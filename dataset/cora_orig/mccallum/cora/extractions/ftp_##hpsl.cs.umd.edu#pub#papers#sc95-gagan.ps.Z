URL: ftp://hpsl.cs.umd.edu/pub/papers/sc95-gagan.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Email: fgagan, saltzg@cs.umd.edu  
Phone: (301)-405-2756  
Title: Interprocedural Compilation of Irregular Applications for Distributed Memory Machines  
Author: Gagan Agrawal and Joel Saltz 
Address: College Park, MD 20742  
Affiliation: UMIACS and Department of Computer Science University of Maryland  
Abstract: Data parallel languages like High Performance Fortran (HPF) are emerging as the architecture independent mode of programming distributed memory parallel machines. In this paper, we present the interprocedural optimizations required for compiling applications having irregular data access patterns, when coded in such data parallel languages. We have developed an Interprocedural Partial Redundancy Elimination (IPRE) algorithm for optimized placement of runtime preprocessing routine and collective communication routines inserted for managing communication in such codes. We also present three new interprocedural optimizations: placement of scatter routines, deletion of data structures and use of coalescing and incremental routines. We then describe how program slicing can be used for further applying IPRE in more complex scenarios. We have done a preliminary implementation of the schemes presented here using the Fortran D compilation system as the necessary infrastructure. We present experimental results from two codes compiled using our system to demonstrate the efficacy of the presented schemes.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal and Joel Saltz. </author> <title> Interprocedural communication optimizations for distributed memory compilation. </title> <booktitle> In Proceedings of the 7th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 283-299, </pages> <month> August </month> <year> 1994. </year> <note> Also available as University of Maryland Technical Report CS-TR-3264. </note>
Reference-contexts: These schemes are closely based upon a classical data flow framework called Partial Redundancy Elimination (PRE) [15, 29]. PRE encompasses traditional optimizations like loop invariant code motion and redundant computation elimination. We have worked on an Interprocedural Partial Redundancy Elimination framework (IPRE) <ref> [1, 2] </ref> as a basis for performing interprocedural placement. In this paper, we discuss various practical aspects in applying interprocedural partial redundancy elimination for placement of communication and communication preprocessing statements. <p> More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [17, 23]. We have extended an existing intraprocedural partial redundancy scheme to be applied interprocedurally <ref> [1, 2] </ref>. In this section, we describe the functionality of the PRE framework, key data flow properties associated with it and briefly sketch how we have extended an existing intraprocedural scheme interprocedurally. Consider any computation of an expression or a call to a pure function. <p> The data flow properties are computed for beginning and end of each edge in the program representation FPR. The details of the data flow analysis required for computing the above properties and then determining placement and deletion based on these has been given elsewhere <ref> [1, 2] </ref>. There are several difficulties in extending the analysis interprocedurally, this includes renaming of influencers across procedure boundaries, saving the calling context of procedures which are called at more than one call sites and further intraprocedural analysis in each procedure to determine final local placement. <p> Second Phase. After the initial pass over all the procedures, we perform the data flow analysis for determining placement. The first step is to generate the full program representation (FPR) using the summary information computed from each procedure <ref> [1] </ref>. The procedure entry nodes are then initialized with the candidates for placement. During the first phase, we have stored the value Global M ax dep, the maximum depth level of any candidate in any procedure.
Reference: [2] <author> Gagan Agrawal, Joel Saltz, and Raja Das. </author> <title> Interprocedural partial redundancy elimination and its application to distributed memory compilation. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation. </booktitle> <publisher> ACM Press, </publisher> <month> June </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: These schemes are closely based upon a classical data flow framework called Partial Redundancy Elimination (PRE) [15, 29]. PRE encompasses traditional optimizations like loop invariant code motion and redundant computation elimination. We have worked on an Interprocedural Partial Redundancy Elimination framework (IPRE) <ref> [1, 2] </ref> as a basis for performing interprocedural placement. In this paper, we discuss various practical aspects in applying interprocedural partial redundancy elimination for placement of communication and communication preprocessing statements. <p> More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [17, 23]. We have extended an existing intraprocedural partial redundancy scheme to be applied interprocedurally <ref> [1, 2] </ref>. In this section, we describe the functionality of the PRE framework, key data flow properties associated with it and briefly sketch how we have extended an existing intraprocedural scheme interprocedurally. Consider any computation of an expression or a call to a pure function. <p> The data flow properties are computed for beginning and end of each edge in the program representation FPR. The details of the data flow analysis required for computing the above properties and then determining placement and deletion based on these has been given elsewhere <ref> [1, 2] </ref>. There are several difficulties in extending the analysis interprocedurally, this includes renaming of influencers across procedure boundaries, saving the calling context of procedures which are called at more than one call sites and further intraprocedural analysis in each procedure to determine final local placement.
Reference: [3] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: We have shown in our previous work how communication preprocessing is useful in regular applications in which data distribution, strides and/or loop bounds are not known at compile-time <ref> [3, 5, 4, 33] </ref> or when the number of processors available for the execution of the program varies at runtime [16]. The rest of the paper is organized as follows. In Section 2, we discuss the basic IPRE framework.
Reference: [4] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Efficient runtime support for parallelizing block structured applications. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 158-167. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: We have shown in our previous work how communication preprocessing is useful in regular applications in which data distribution, strides and/or loop bounds are not known at compile-time <ref> [3, 5, 4, 33] </ref> or when the number of processors available for the execution of the program varies at runtime [16]. The rest of the paper is organized as follows. In Section 2, we discuss the basic IPRE framework.
Reference: [5] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> An integrated runtime and compile-time approach for paral-lelizing structured and block structured applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1995. To appear. Also available as University of Maryland Technical Report CS-TR-3143 and UMIACS-TR-93-94. </note>
Reference-contexts: We have shown in our previous work how communication preprocessing is useful in regular applications in which data distribution, strides and/or loop bounds are not known at compile-time <ref> [3, 5, 4, 33] </ref> or when the number of processors available for the execution of the program varies at runtime [16]. The rest of the paper is organized as follows. In Section 2, we discuss the basic IPRE framework.
Reference: [6] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: A basic block of code in a procedure is a sequence of consecutive statements in a procedure in the flow enters at the beginning and leaves at the end without possibility of branching expect at the end <ref> [6] </ref>. Transparency. Transparency of a basic block with respect to a candidate means that none of the influencers of the candidate are modified in the basic block.
Reference: [7] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M.-Y. Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 15-26, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In the procedure Proc A, the scatter operation can be deleted, since this scatter is subsumed by the scatter done later in Proc B. Scatter operations have also been used by distributed memory compilers in compiling regular applications <ref> [7] </ref>. The HPF/Fortran 90D compiler developed at Syracuse University uses scatter operations (called post-comp writes) whenever the subscript in the left hand side array reference is a complex function of the index variable.
Reference: [8] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dy namics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: We have used two irregular codes in our study, an Euler solver on an unstructured mesh [12], originally developed at ICASE by Mavriplis et al. and a template taken from CHARMM <ref> [8] </ref>, a molecular dynamics code. We used Intel Paragon at Rice University for performing our experiments. The Euler solver we experimented with performs sweeps over an unstructured mesh inside the time step loop. The data parallel loops iterate over both the edges and the faces of the unstructured mesh. <p> The second code we considered was a template taken a molecular dynamics code Charmm <ref> [8, 26] </ref>. The templates we worked with comprised of just 2 procedures, one procedure which computed non-bonded forces between the atoms of the molecule and the other procedure enclosed this procedure in a time step loop.
Reference: [9] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: The total number of nodes in SuperGraph can get very large and consequently the solution may take much longer time to converge. Several ideas in the design of our representation are similar to the ideas used in Callahan's Program Summary Graph <ref> [9] </ref> and Interprocedural Flow Graph used by Soffa et al. [24]. 8 Conclusions In this paper, we have presented interprocedural optimizations for compilation of irregular applications on distributed memory machines. In such applications, runtime preprocessing is used to determine the communication required between the processors.
Reference: [10] <author> A. Choudhary, G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, S. Ranka, and J. Saltz. </author> <title> Software support for irregular and loosely synchronous problems. </title> <booktitle> Computing Systems in Engineering, 3(1-4):43-52, 1992. Papers presented at the Symposium on High-Performance Computing for Flight Vehicles, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Specifically, we concentrate on applications in which data is accessed using indirection arrays. Such codes are common in computational fluid dynamics, molecular dynamics, in particle in cell problems and in numerical simulations <ref> [10] </ref>. The commonly used approach for compiling irregular applications is the inspector/executor model [28]. Conceptually, an inspector or a communication preprocessing statement analyses the indirection array to determine the communication required by a data parallel loop. The results of communication preprocessing is then used to perform the communication.
Reference: [11] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the rn programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: For each such procedure call in the control flow path of candidate, we just examine if any of the variables in the slice is modified by the procedure call <ref> [11] </ref>. If so, we do not consider this candidate for hoisting outside the procedure. When we use slices of the candidates, additional steps are required in final placement of the candidates. In placing the candidate, entire slice corresponding to candidate is placed.
Reference: [12] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <journal> AIAA Journal, </journal> <volume> 32(3) </volume> <pages> 489-496, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: We measure the difference made by performing interprocedural placement of both the communication preprocessing statements and the collective communication statements. We have used two irregular codes in our study, an Euler solver on an unstructured mesh <ref> [12] </ref>, originally developed at ICASE by Mavriplis et al. and a template taken from CHARMM [8], a molecular dynamics code. We used Intel Paragon at Rice University for performing our experiments. The Euler solver we experimented with performs sweeps over an unstructured mesh inside the time step loop.
Reference: [13] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 152-168. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3076 and UMIACS-TR-93-42. </note>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [13, 22, 28, 31, 35] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. It is clear that sophisticated compilation techniques are required for getting optimized performance from irregular codes [13, 23]. <p> For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. It is clear that sophisticated compilation techniques are required for getting optimized performance from irregular codes <ref> [13, 23] </ref>. These techniques have been implemented in prototype compilers for HPF like languages, however the experiences and experimental results reported have been from small code templates. <p> CHAOS/PARTI library provides a rich set of routines for performing the communication preprocessing and optimized communication for such applications [32]. Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications [25] but has more recently been extended to compile irregular applications <ref> [13, 22] </ref>. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [13, 21]. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed [17, 23]. <p> Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications [25] but has more recently been extended to compile irregular applications [13, 22]. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication <ref> [13, 21] </ref>. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed [17, 23]. The key idea underlying these schemes is to do the placement so that redundancies are removed or reduced. <p> This may not be adequate for performing code motion in several irregular applications, especially the ones in which data is accessed using multiple levels of indirection <ref> [13] </ref>. For such codes, IPRE can be performed by using slices of the call to the candidates. Consider the code given in Figure 7. In the procedure Proc B, the array Q is accessed using array R, which is local within procedure Proc B. <p> For this purpose, we use the notion of program (or procedure ) slices. Program Slice. A program (procedure) slice is defined as a program comprising of a set of statements which contribute, either directly or indirectly, to the value of certain variables at a certain point in the program <ref> [13, 14, 34] </ref>. This set of variables and the point in the program is together refered to as the slicing criterion. For our purpose, the slicing criterion used is the set of parameters of the candidate, at the point in the program where the candidate is invoked.
Reference: [14] <author> Raja Das, Joel Saltz, Ken Kennedy, and Paul Havlak. </author> <title> Index array flattening through program transformation. </title> <note> Submitted to PLDI '95, </note> <month> November </month> <year> 1994. </year>
Reference-contexts: For this purpose, we use the notion of program (or procedure ) slices. Program Slice. A program (procedure) slice is defined as a program comprising of a set of statements which contribute, either directly or indirectly, to the value of certain variables at a certain point in the program <ref> [13, 14, 34] </ref>. This set of variables and the point in the program is together refered to as the slicing criterion. For our purpose, the slicing criterion used is the set of parameters of the candidate, at the point in the program where the candidate is invoked.
Reference: [15] <author> D.M. Dhamdhere and H. Patil. </author> <title> An elimination algorithm for bidirectional data flow problems using edge placement. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(2) </volume> <pages> 312-336, </pages> <month> April </month> <year> 1993. </year> <month> 24 </month>
Reference-contexts: Techniques for performing these optimizations within a single procedure are well developed [17, 23]. The key idea underlying these schemes is to do the placement so that redundancies are removed or reduced. These schemes are closely based upon a classical data flow framework called Partial Redundancy Elimination (PRE) <ref> [15, 29] </ref>. PRE encompasses traditional optimizations like loop invariant code motion and redundant computation elimination. We have worked on an Interprocedural Partial Redundancy Elimination framework (IPRE) [1, 2] as a basis for performing interprocedural placement. <p> Partial Redundancy Elimination (PRE) is a unified 2 framework for performing these optimizations intraprocedurally <ref> [15, 29] </ref>. It has been commonly used intraprocedurally for performing optimizations like common subexpression elimination and strength reduction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [17, 23].
Reference: [16] <author> Guy Edjlali, Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Data parallel programming in an adaptive environment. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1995. </year> <note> To appear. Also available as University of Maryland Technical Report CS-TR-3350 and UMIACS-TR-94-109. </note>
Reference-contexts: We have shown in our previous work how communication preprocessing is useful in regular applications in which data distribution, strides and/or loop bounds are not known at compile-time [3, 5, 4, 33] or when the number of processors available for the execution of the program varies at runtime <ref> [16] </ref>. The rest of the paper is organized as follows. In Section 2, we discuss the basic IPRE framework. In Section 3, we present several new optimizations required for compiling irregular applications.
Reference: [17] <author> Manish Gupta, Edith Schonberg, and Harini Srinivasan. </author> <title> A unified data flow framework for optimizing communication. </title> <booktitle> In Proceedings of Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [13, 21]. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed <ref> [17, 23] </ref>. The key idea underlying these schemes is to do the placement so that redundancies are removed or reduced. These schemes are closely based upon a classical data flow framework called Partial Redundancy Elimination (PRE) [15, 29]. <p> It has been commonly used intraprocedurally for performing optimizations like common subexpression elimination and strength reduction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines <ref> [17, 23] </ref>. We have extended an existing intraprocedural partial redundancy scheme to be applied interprocedurally [1, 2]. In this section, we describe the functionality of the PRE framework, key data flow properties associated with it and briefly sketch how we have extended an existing intraprocedural scheme interprocedurally. <p> This is based up Call Graph program abstraction and is targeted more towards flow-insensitive interprocedural analysis. Our implementation uses several facilities available from FIAT as part of the Fortran D infrastructure. Partial redundancy elimination was used interprocedurally by Gupta et al. <ref> [17] </ref> for performing communication optimizations. An interesting feature of their work is to use available section descriptors, which can help with many other optimizations for regular codes. Hanxleden [23] has developed Give-N-Take, a new data placement framework.
Reference: [18] <author> Mary Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: We have chosen a concise full program representation, which will allow efficient data flow analysis, while maintaining sufficient precision to allow useful transformations and to ensure safety and correctness of transformations. 2.1 Program Representation In traditional interprocedural analysis, program is abstracted by a call graph <ref> [18, 19] </ref>. In a call graph G = (V; E), V is the set of procedures and directed edge e = (i; j) (e 2 E) represents a call site in which procedure i invokes procedure j.
Reference: [19] <author> Mary Hall, John M Mellor Crummey, Alan Carle, and Rene G Rodriguez. FIAT: </author> <title> A framework for interpro-cedural analysis and transformations. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 522-545. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: We have chosen a concise full program representation, which will allow efficient data flow analysis, while maintaining sufficient precision to allow useful transformations and to ensure safety and correctness of transformations. 2.1 Program Representation In traditional interprocedural analysis, program is abstracted by a call graph <ref> [18, 19] </ref>. In a call graph G = (V; E), V is the set of procedures and directed edge e = (i; j) (e 2 E) represents a call site in which procedure i invokes procedure j. <p> This abstraction records any loop (s) enclosing a procedure call. Again, this abstraction does not allow to look for redundant communication preprocessing calls or communication in adjacent procedure. Framework for Interprocedural Analysis and Transforms (FIAT) <ref> [19] </ref> has recently been proposed as a general environment for interprocedural analysis. This is based up Call Graph program abstraction and is targeted more towards flow-insensitive interprocedural analysis. Our implementation uses several facilities available from FIAT as part of the Fortran D infrastructure.
Reference: [20] <author> M.W. Hall, S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <pages> pages 522-534. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1992. </year>
Reference-contexts: First Phase. The initial local compilation phase inserts communication preprocessing and communica tion statements based upon intraprocedural analysis [25]. This code generation is based upon reaching decomposition analysis <ref> [20] </ref>. Reaching decomposition analysis propagates information about the distri bution of arrays from calling procedures to callees. <p> Experiments on hand-parallelization of the entire Charmm code [26, 32] have shown a nearly 20% reduction in the communication time, by using coalescing communication routines. 7 Related Work The only other effort on interprocedural analysis for distributed memory compilation is by Hall et al. <ref> [20] </ref>. They have concentrated on flow-insensitive analysis for regular applications, including management of buffer space and propagation of data distribution and data alignment information across procedure boundaries. In this work, Augmented Call Graph (ACG) has been introduced as a new program abstraction.
Reference: [21] <author> R. v. Hanxleden, K. Kennedy, and J. Saltz. </author> <title> Value-based distributions in Fortran D a preliminary report. </title> <type> Technical Report CRPC-TR93365-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> December </month> <year> 1993. </year> <title> Submitted to Journal of Programming Languages Special Issue on Compiling and Run-Time Issues for Distributed Address Space Machines. </title>
Reference-contexts: Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications [25] but has more recently been extended to compile irregular applications [13, 22]. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication <ref> [13, 21] </ref>. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed [17, 23]. The key idea underlying these schemes is to do the placement so that redundancies are removed or reduced.
Reference: [22] <author> Reinhard v. Hanxleden. </author> <title> Handling irregular problems with Fortran D a preliminary report. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year> <note> Also available as CRPC Technical Report CRPC-TR93339-S. </note>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [13, 22, 28, 31, 35] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. It is clear that sophisticated compilation techniques are required for getting optimized performance from irregular codes [13, 23]. <p> CHAOS/PARTI library provides a rich set of routines for performing the communication preprocessing and optimized communication for such applications [32]. Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications [25] but has more recently been extended to compile irregular applications <ref> [13, 22] </ref>. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [13, 21]. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed [17, 23].
Reference: [23] <author> Reinhard von Hanxleden and Ken Kennedy. </author> <title> Give-n-take a balanced code placement framework. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 107-120. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1994. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 29, No. </volume> <pages> 6. </pages>
Reference-contexts: For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. It is clear that sophisticated compilation techniques are required for getting optimized performance from irregular codes <ref> [13, 23] </ref>. These techniques have been implemented in prototype compilers for HPF like languages, however the experiences and experimental results reported have been from small code templates. <p> In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [13, 21]. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. Techniques for performing these optimizations within a single procedure are well developed <ref> [17, 23] </ref>. The key idea underlying these schemes is to do the placement so that redundancies are removed or reduced. These schemes are closely based upon a classical data flow framework called Partial Redundancy Elimination (PRE) [15, 29]. <p> It has been commonly used intraprocedurally for performing optimizations like common subexpression elimination and strength reduction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines <ref> [17, 23] </ref>. We have extended an existing intraprocedural partial redundancy scheme to be applied interprocedurally [1, 2]. In this section, we describe the functionality of the PRE framework, key data flow properties associated with it and briefly sketch how we have extended an existing intraprocedural scheme interprocedurally. <p> Partial redundancy elimination was used interprocedurally by Gupta et al. [17] for performing communication optimizations. An interesting feature of their work is to use available section descriptors, which can help with many other optimizations for regular codes. Hanxleden <ref> [23] </ref> has developed Give-N-Take, a new data placement framework. This framework extends PRE in several ways, including a notion of early and lazy problems, which is used for performing earliest possible placement of sends and latest possible placement of receive operations.
Reference: [24] <author> Mary Jean Harrold and Mary Lou Soffa. </author> <title> Efficient computation of interprocedural definition-use chains. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(2) </volume> <pages> 175-204, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Several ideas in the design of our representation are similar to the ideas used in Callahan's Program Summary Graph [9] and Interprocedural Flow Graph used by Soffa et al. <ref> [24] </ref>. 8 Conclusions In this paper, we have presented interprocedural optimizations for compilation of irregular applications on distributed memory machines. In such applications, runtime preprocessing is used to determine the communication required between the processors.
Reference: [25] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The results of communication preprocessing is then used to perform the communication. CHAOS/PARTI library provides a rich set of routines for performing the communication preprocessing and optimized communication for such applications [32]. Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications <ref> [25] </ref> but has more recently been extended to compile irregular applications [13, 22]. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [13, 21]. An important optimization required for irregular applications is placement of communication preprocessing and communication statements. <p> By scatter, we mean a routine which, after a data parallel loop, updates the off-processor elements modified by the loop. In distributed memory compilation, a commonly used technique for loop iteration partitioning is owner computes rule <ref> [25] </ref>. In this method, each iteration is executed by the processor which owns the left hand side array reference updated by the iteration. <p> In the third phase, each procedure is visited again, and the decisions made about placement of candidates are actually incorporated in the code for each procedure. First Phase. The initial local compilation phase inserts communication preprocessing and communica tion statements based upon intraprocedural analysis <ref> [25] </ref>. This code generation is based upon reaching decomposition analysis [20]. Reaching decomposition analysis propagates information about the distri bution of arrays from calling procedures to callees.
Reference: [26] <author> Yuan-Shin Hwang, Raja Das, Joel Saltz, Bernard Brooks, and Milan Hodoscek. </author> <title> Parallelizing molecular dynamics programs for distributed memory machines: An application of the CHAOS runtime support library. </title> <institution> Technical Report CS-TR-3374 and UMIACS-TR-94-125, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> November </month> <year> 1994. </year> <note> To appear in IEEE Computational Science and Engineering. </note>
Reference-contexts: The second code we considered was a template taken a molecular dynamics code Charmm <ref> [8, 26] </ref>. The templates we worked with comprised of just 2 procedures, one procedure which computed non-bonded forces between the atoms of the molecule and the other procedure enclosed this procedure in a time step loop. <p> V 3 : Interprocedural placement of comm. stmts also further improvement in performance can be achieved by interprocedural optimization of communication statements. Experiments on hand-parallelization of the entire Charmm code <ref> [26, 32] </ref> have shown a nearly 20% reduction in the communication time, by using coalescing communication routines. 7 Related Work The only other effort on interprocedural analysis for distributed memory compilation is by Hall et al. [20].
Reference: [27] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction In recent years, there have been major efforts in developing language and compiler support for programming distributed memory machines. High Performance Fortran (HPF) has been proposed as a Fortran extension for programming these machines <ref> [27] </ref>, many commercial projects are underway to develop compilers for High Performance Fortran. Efforts are also underway in the High Performance Fortran Forum to increase the scope of HPF for compiling a wider range of applications.
Reference: [28] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [13, 22, 28, 31, 35] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. It is clear that sophisticated compilation techniques are required for getting optimized performance from irregular codes [13, 23]. <p> Specifically, we concentrate on applications in which data is accessed using indirection arrays. Such codes are common in computational fluid dynamics, molecular dynamics, in particle in cell problems and in numerical simulations [10]. The commonly used approach for compiling irregular applications is the inspector/executor model <ref> [28] </ref>. Conceptually, an inspector or a communication preprocessing statement analyses the indirection array to determine the communication required by a data parallel loop. The results of communication preprocessing is then used to perform the communication.
Reference: [29] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: Techniques for performing these optimizations within a single procedure are well developed [17, 23]. The key idea underlying these schemes is to do the placement so that redundancies are removed or reduced. These schemes are closely based upon a classical data flow framework called Partial Redundancy Elimination (PRE) <ref> [15, 29] </ref>. PRE encompasses traditional optimizations like loop invariant code motion and redundant computation elimination. We have worked on an Interprocedural Partial Redundancy Elimination framework (IPRE) [1, 2] as a basis for performing interprocedural placement. <p> Partial Redundancy Elimination (PRE) is a unified 2 framework for performing these optimizations intraprocedurally <ref> [15, 29] </ref>. It has been commonly used intraprocedurally for performing optimizations like common subexpression elimination and strength reduction. More recently, it has been used for more complex code placement tasks like placement of communication statements while compiling for parallel machines [17, 23].
Reference: [30] <author> E. Myers. </author> <title> A precise interprocedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 219-230, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Our works differs significantly since we consider interprocedural optimizations and present several new optimizations. Several different program representations have been used for different flow-sensitive interprocedural problems. Myer has suggested concept of SuperGraph <ref> [30] </ref> which is constructed by linking control flow graphs of procedures by inserting edges from call site in the caller to start node in callee. The total number of nodes in SuperGraph can get very large and consequently the solution may take much longer time to converge.
Reference: [31] <author> Ravi Ponnusamy, Joel Saltz, and Alok Choudhary. </author> <title> Runtime-compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 361-370. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3055 and UMIACS-TR-93-32. </note>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [13, 22, 28, 31, 35] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. It is clear that sophisticated compilation techniques are required for getting optimized performance from irregular codes [13, 23].
Reference: [32] <author> Shamik D. Sharma, Ravi Ponnusamy, Bongki Moon, Yuan-Shin Hwang, Raja Das, and Joel Saltz. </author> <title> Run-time and compile-time support for adaptive irregular problems. </title> <booktitle> In Proceedings Supercomputing '94, </booktitle> <pages> pages 97-106. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: The results of communication preprocessing is then used to perform the communication. CHAOS/PARTI library provides a rich set of routines for performing the communication preprocessing and optimized communication for such applications <ref> [32] </ref>. Fortran D compilation system, a prototype compiler for distributed memory machines, initially targeted regular applications [25] but has more recently been extended to compile irregular applications [13, 22]. In compiling irregular applications, the Fortran D compiler inserts calls to CHAOS/PARTI library routines to manage communication [13, 21]. <p> V 3 : Interprocedural placement of comm. stmts also further improvement in performance can be achieved by interprocedural optimization of communication statements. Experiments on hand-parallelization of the entire Charmm code <ref> [26, 32] </ref> have shown a nearly 20% reduction in the communication time, by using coalescing communication routines. 7 Related Work The only other effort on interprocedural analysis for distributed memory compilation is by Hall et al. [20].
Reference: [33] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: We have shown in our previous work how communication preprocessing is useful in regular applications in which data distribution, strides and/or loop bounds are not known at compile-time <ref> [3, 5, 4, 33] </ref> or when the number of processors available for the execution of the program varies at runtime [16]. The rest of the paper is organized as follows. In Section 2, we discuss the basic IPRE framework.
Reference: [34] <author> Mark Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 10 </volume> <pages> 352-357, </pages> <year> 1984. </year>
Reference-contexts: For this purpose, we use the notion of program (or procedure ) slices. Program Slice. A program (procedure) slice is defined as a program comprising of a set of statements which contribute, either directly or indirectly, to the value of certain variables at a certain point in the program <ref> [13, 14, 34] </ref>. This set of variables and the point in the program is together refered to as the slicing criterion. For our purpose, the slicing criterion used is the set of parameters of the candidate, at the point in the program where the candidate is invoked. <p> Computing Slices. Algorithms for computing a slice, given a slicing criterion, have been presented in the literature <ref> [34] </ref>. We make an important difference in the way slices are computed, since we need to accommodate the fact that some of the statements included in the slice may themselves be candidate for the placement.
Reference: [35] <author> Janet Wu, Raja Das, Joel Saltz, Scott Berryman, and Seema Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <journal> IEEE Transactions on Computers, </journal> <note> 1994. To appear. 25 </note>
Reference-contexts: However, significant effort has also been made to compile applications having irregular and/or dynamic data accesses (possibly with the help of additional language support) <ref> [13, 22, 28, 31, 35] </ref>. For such codes, the compiler can analyze the data access pattern and insert appropriate communication and communication preprocessing routines. It is clear that sophisticated compilation techniques are required for getting optimized performance from irregular codes [13, 23].
References-found: 35

