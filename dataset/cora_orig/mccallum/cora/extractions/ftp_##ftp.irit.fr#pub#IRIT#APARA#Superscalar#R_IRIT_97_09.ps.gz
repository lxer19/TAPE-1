URL: ftp://ftp.irit.fr/pub/IRIT/APARA/Superscalar/R_IRIT_97_09.ps.gz
Refering-URL: http://www.irit.fr/ACTIVITES/EQ_APARA/publi_equipe.html
Root-URL: 
Title: Rapport interne IRIT-97-09-R Prediction de l'adresse des lectures pour tolerer la latence des acces memoire
Author: Pham Tuong Hai, Christine Rochange, Pascal Sainrat, Daniel Litaize 
Abstract-found: 0
Intro-found: 1
Reference: <institution> 33 Bibliographie </institution>
Reference: [1] <author> T.M. Austin, D.N. Pnevmatikatos, </author> <title> Streamlining Data Cache Access with Fast Address calculation, </title> <booktitle> 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference: [2] <author> T.M. Austin, G.S. Sohi, </author> <title> Zero-cycle Loads: Microarchitecture Support for Reducing Load Latency, </title> <booktitle> 28th Annual International Symposium on Microar-chitecture, </booktitle> <month> November </month> <year> 1995. </year>
Reference: [3] <author> J-L. Baer, T-F. Chen, </author> <title> An Effective On-Chip Preloading Scheme To Reduce Data Access Penalty, </title> <address> Supercomputing'91, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: This gives an upper bound to the speedup that can be obtained with our scheme. The second case is a simple case which has a relatively small hardware cost. It is described in the next subsections. 5.1 The Prediction Table The prediction mechanism is largely inspired from <ref> [3] </ref>. It consists in a direct-mapped 256-entry table that is indexed by the address of the dispatched instruction. This table keeps the behaviour of loads.
Reference: [4] <author> D. Burger, J. R. Goodman, A. Kagi, </author> <title> Quantifying Memory Bandwidth Limitations of Current and Future Microprocessors, </title> <booktitle> 23eme International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference: [5] <author> M. Franklin, </author> <title> The Multiscalar Architecture, </title> <type> PhD thesis, </type> <institution> University of Wis-consin, Madison, </institution> <year> 1993. </year>
Reference: [6] <author> D.M. Gallagher, W.Y. Chen, S.A. Mahlke, J.C. Gyllenhaal, W-M.W. Hwu, </author> <title> Dynamic Memory Disambiguation Using the Memory Conflict Buffer, </title> <booktitle> 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference: [7] <author> J. L. Hennessy, D. A. Patterson, </author> <title> Architecture des ordinateurs, une approche quantitative, </title> <publisher> International Thomson Publishing, </publisher> <year> 1996. </year>
Reference: [8] <author> A.S. Huang, G. Slavenburg, J.P. Shen, </author> <title> Speculative Disambiguation: A Compilation Technique for Dynamic Memory Disambiguation, </title> <booktitle> 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference: [9] <author> D. Kroft, </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization, </title> <booktitle> 8th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1981. </year>
Reference-contexts: This cache is composed of 4 interleaved banks that allow four simultaneous accesses by the processor (one access per L/S functional unit) provided there is no bank conflict. It is a write-back direct-mapped cache with fetch-on-write and write-allocate policies [13]. As most of caches today, it is also non-blocking <ref> [9] </ref>. We suppose that it can support an infinite number of pending loads since measures show that restricting the number fo pending loads to 2 gives the same performance. The hit access time is 1 cycle.
Reference: [10] <author> S. Jourdan, P. Sainrat, D. Litaize, </author> <title> Exploring Configurations of Functional Units in an Out-Of-Order Superscalar Processor, </title> <booktitle> 25th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: This guarantees the correct result and the possiblity of recovering from mispredictions of branches and from exceptions. The size of the reorder buffer is 96 as defined in <ref> [10] </ref>. Concerning the functional units, we also choose the configuration described as optimal in [10] which is briefly recalled in figure 2.1. <p> This guarantees the correct result and the possiblity of recovering from mispredictions of branches and from exceptions. The size of the reorder buffer is 96 as defined in <ref> [10] </ref>. Concerning the functional units, we also choose the configuration described as optimal in [10] which is briefly recalled in figure 2.1.
Reference: [11] <author> S. Jourdan, P. Sainrat, D. Litaize, </author> <title> An Investigation of the Performance of Various Instruction-Issue Buffer Topologies, </title> <booktitle> 28th Annual International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1995. </year> <month> 35 </month>
Reference-contexts: The sizes of these parts are given in table 2.1 and have been defined as the optimal sizes for our model in <ref> [11] </ref>. Instructions are retired (dequeued from the reorder buffer) in the order of the program and the registers are updated at that time. This guarantees the correct result and the possiblity of recovering from mispredictions of branches and from exceptions.
Reference: [12] <author> S. Jourdan, P. Sainrat, D. Litaize, </author> <title> Tampons d'amor~cage des microproces--seurs superscalaires a execution non ordonnee : une analyse, </title> <address> Renpar'8, </address> <month> May </month> <year> 1996. </year>
Reference: [13] <author> N.P. Jouppi, </author> <title> Cache Write Policies and Performance, </title> <booktitle> 20th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: The memory hierarchy includes a L1 32-KB data cache. This cache is composed of 4 interleaved banks that allow four simultaneous accesses by the processor (one access per L/S functional unit) provided there is no bank conflict. It is a write-back direct-mapped cache with fetch-on-write and write-allocate policies <ref> [13] </ref>. As most of caches today, it is also non-blocking [9]. We suppose that it can support an infinite number of pending loads since measures show that restricting the number fo pending loads to 2 gives the same performance. The hit access time is 1 cycle.
Reference: [14] <author> M. Simone, A. Essen, A. Krishnamoorthy, T. Maruyam, N. Patkar, M. Ra-maswami, M. Shebanow, V. Thirumalaiswamy, D. Tovey, </author> <title> Implementation Trade-offs in Using a Restricted Data Flow Architecture in a High Performance Microprocessor, </title> <booktitle> 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Load execution may be seen as two independent but ordered operations: first, the calculus of the address and second, the access to the memory hierarchy. These two stages may be processed by two different functional units with their own reservation stations as in the Hal microprocessor <ref> [14] </ref>, or by a single functional unit that computes the address followed by a queue for the access to the memory hierarchy.
Reference: [15] <author> A. Seznec, F. Lloansi, </author> <title> About Effective Cache Miss Penalty on Out-Of-Order Superscalar Processors, </title> <type> technical report INRIA 2726, </type> <month> November </month> <year> 1995. </year>
Reference-contexts: This is an essential point since the bandwidth between the L1 and the L2 cache is limited and constitutes a bottleneck as it has been observed in <ref> [15] </ref>. Address prediction generates less unuseful requests to the L2 cache than cache prefetching. Address prediction predicts the address of a load that will be executed when prefetching additionally predicts that this load will be executed in the future.
Reference: [16] <author> M.H. Lipasti, C.B. Wilkerson, J.P. Shen, </author> <title> Value Locality and Load Value Prediction, </title> <booktitle> 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Thus, if the data resides in the cache, it results in a zero-cycle load. This scheme reduces the latency of loads by one cycle. So, it is not effective for long-latency 19 loads. Value prediction <ref> [16] </ref> predicts the value that will be loaded from the memory. It leaves the predicted load in the reservation station until this prediction is verified by accessing to the memory hierarchy.
Reference: [17] <author> R.M. Tomasulo, </author> <title> An Efficient Algorithm for Exploiting Multiple Arithmetic Units, </title> <journal> IBM Journal, </journal> <volume> vol. 11, </volume> <year> 1967. </year> <month> 36 </month>
Reference-contexts: Instructions waiting for their operands do not stall the decoding: the missing operands will be forwarded to the instruction-issue buffer as soon as they are produced by previous instructions. The entries of the instruction-issue buffer are similar to the entries of the reservations stations of the IBM360/91 <ref> [17] </ref> except that they are linked to more than one functional unit. Instructions are waiting in these entries for their operands using a tag to retrieve them. When the operands are ready, the instruction may be issued to a free functional unit and the entry is resetted.
References-found: 18

