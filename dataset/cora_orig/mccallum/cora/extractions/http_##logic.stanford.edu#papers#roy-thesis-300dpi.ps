URL: http://logic.stanford.edu/papers/roy-thesis-300dpi.ps
Refering-URL: http://logic.stanford.edu/papers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: SHARP, RELIABLE PREDICTIONS USING SUPERVISED MIXTURE MODELS  
Author: Howard Scott Roy 
Degree: A DISSERTATION SUBMITTED TO THE DEPARTMENT OF COMPUTER SCIENCE AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY  
Date: March 1995  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> A. Azevedo-Filho and R. Shachter. </author> <title> Laplace's method: Approximations for probabilistic inference in belief networks with continuous variables. </title> <editor> In R. L. de Mantaras and D. Poole, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence: Proceedings of the Tenth Conference, </booktitle> <pages> pages 28-36, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: It is easy to replace MAP induction with more accurate but time consuming approximations to Equation 1, like Laplace's normal approximation <ref> [1] </ref> or Gibb's sampling [15], but this dissertation leaves such extensions to future work. <p> The literature contains many ways to improve on MAP induction, like Laplace's approximation <ref> [1] </ref> or Gibb's sampling [15], that could potentially increase MultiClass's performance at the expense of computation time. The most severe flaw in MultiClass is that its priors do not allow for the possibility that a feature is irrelevant.
Reference: [2] <author> J. M. Bernardo and A. F. M. Smith. </author> <title> Bayesian Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1994. </year>
Reference-contexts: Together, Equations 1 and 2 completely specify Bayesian probabilistic induction. Jaynes's Probability: the Logic of Science is an exceptional introduction [28], as is the new book by Bernardo and Smith <ref> [2] </ref>. The posterior distribution, PrM j DI, indicates which models provide the best description of the database. More generally, as Equation 1 shows, we ideally make predictions by averaging the predictions of all the models we consider, weighted according to their posterior probabilities. <p> pattern holds in higher dimensions: Dirichlet shape 8 &gt; &gt; &gt; : horseshoe; k &lt; 1 uniform; k 1 bell; k &gt; 1 Uninformative priors can be logically deduced in many ways, including group invariance [25], marginalization [26], and arguments that seek to maximize the information content of the data <ref> [2] </ref>. In this instance one can use a marginalization argument, which this dissertation omits, to conclude that k should depend on a deeper hyperparameter ff according to the relation nk ff.
Reference: [3] <author> D. M. Boulton and C. S. Wallace. </author> <title> A program for numerical classification. </title> <journal> The Computer Journal, </journal> <volume> 13(1) </volume> <pages> 63-69, </pages> <year> 1970. </year>
Reference-contexts: AutoClass traces its ancestry to the long history of finite mixture distributions in statistics [53] and to an earlier program, SNOB, developed by Wallace in the early 1970's <ref> [3] </ref>. It is unsupervised, working without any prediction or validation tasks in mind. Its goal is to find a good model of all the effects in a database, thereby providing insight into large, poorly understood domains.
Reference: [4] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Chap-man & Hall, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: In principle, one could provide priors for the hyperpa-rameters and optimize them as part of the search. In practice, one can use a wrapper algorithm like C4.5-AP [34] to automatically find their optimal values using cross validation. <ref> [4] </ref>. 91 Experimental Results and Future Work 92 The user must tell MultiClass how many classes to look for. The program often eliminates unwanted classes by reducing them to zero weight, but this procedure is unreliable.
Reference: [5] <author> W. L. Buntine. </author> <title> Learning classification trees. </title> <editor> In D. J. Hand, editor, </editor> <booktitle> Artificial Intelligence Frontiers in Statistics, </booktitle> <pages> pages 182-201. </pages> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1991. </year> <note> 109 References 110 </note>
Reference-contexts: The decision tree literature, similarly, seems to only periodically rediscover that one can search the space of decision trees using anything other The Problem and the Approach 6 than a greedy hill climbing algorithm. A 2-ply lookahead gives much better performance <ref> [5] </ref>, and the problem seems natural for a general graph search algorithm like A fl .
Reference: [6] <author> W. L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225, </pages> <year> 1994. </year>
Reference-contexts: Wray Buntine and I are developing a general purpose scientific modeling system to bridge this gap [7]. When finished, it will allow researchers to create programs like MultiClass in an afternoon. The research effort, Programming with Probabilities, combines Buntine's graphical modeling language <ref> [6] </ref> with data flow and object oriented modeling lessons learned from the Multi-Class implementation. The goal is to let one specify a mathematical model by drawing graphs, linking together components from a palette, and writing down equations.
Reference: [7] <author> W. L. Buntine and H. S. Roy. </author> <title> Software for data analysis with graphical models. </title> <booktitle> To appear in the proceedings of the International Workshop on Artificial Intelligence and Statistics, </booktitle> <year> 1995. </year>
Reference-contexts: There is currently a gaping chasm between the model level at which one naturally describes a data analysis problem, and the coding level at which one programs. Wray Buntine and I are developing a general purpose scientific modeling system to bridge this gap <ref> [7] </ref>. When finished, it will allow researchers to create programs like MultiClass in an afternoon. The research effort, Programming with Probabilities, combines Buntine's graphical modeling language [6] with data flow and object oriented modeling lessons learned from the Multi-Class implementation.
Reference: [8] <author> P. Cheeseman. </author> <type> Personal communications. </type>
Reference-contexts: Chapter 5 gives a full quantitative account of this phenomenon. V. Thyroid Database The original developers of AutoClass encountered the problem of multiple useful classifications on many data sets <ref> [8] </ref>. Quinlan's thyroid disease database is a good example. The goal with this database is to diagnose various thyroid diseases and prescribe treatment therapies.
Reference: [9] <author> P. Cheeseman et al. </author> <title> AutoClass: A Bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 54-64, </pages> <year> 1988. </year>
Reference-contexts: It is, instead, a map of the potentially treacherous terrain that all data analysis systems must navigate. Existing programs deal with many of the problems in this chapter just fine, but no current program handles all of them. The one system this chapter does frequently mention is AutoClass <ref> [9] </ref>, since it is the limitations of AutoClass that this dissertation primarily looks to understand and overcome. 9 Example Databases 10 II. Iris Database Chapter 1 described the ideal input to a machine learning algorithm, namely an existing object oriented or relational database laid out in multiple tables. <p> A famous example of machine discovery involved an analogous problem, in which the AutoClass program, performing an analysis much like the one suggested by the iris example, discovered new types of stars by spotting patterns in their spectral emissions <ref> [9] </ref>. Example Databases 12 III. Day Camp Database Clusters can appear in discrete and categorical data as well as in real data. <p> Interested readers are encouraged to read Jaynes' entertaining discussion on paradoxes in probability theory. 32 4 Mixture Models I. AutoClass The goal of this dissertation is to improve AutoClass, a Bayesian database analysis system developed over the last six years at NASA's Ames Research Center <ref> [9, 10, 19] </ref>. AutoClass traces its ancestry to the long history of finite mixture distributions in statistics [53] and to an earlier program, SNOB, developed by Wallace in the early 1970's [3]. It is unsupervised, working without any prediction or validation tasks in mind. <p> It is unsupervised, working without any prediction or validation tasks in mind. Its goal is to find a good model of all the effects in a database, thereby providing insight into large, poorly understood domains. It has been successfully applied to many problems, including the IRAS astronomical data <ref> [9] </ref>, protein amino acid sequences [23], and raw English text [51]. AutoClass operates within the density estimation framework of Chapter 3, treating each row in the database as an independent sample from a fixed distribution. Its goal is to reconstruct that distribution.
Reference: [10] <author> P. Cheeseman et al. </author> <title> Bayesian classification. </title> <booktitle> In Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 607-611, </pages> <year> 1988. </year>
Reference-contexts: Interested readers are encouraged to read Jaynes' entertaining discussion on paradoxes in probability theory. 32 4 Mixture Models I. AutoClass The goal of this dissertation is to improve AutoClass, a Bayesian database analysis system developed over the last six years at NASA's Ames Research Center <ref> [9, 10, 19] </ref>. AutoClass traces its ancestry to the long history of finite mixture distributions in statistics [53] and to an earlier program, SNOB, developed by Wallace in the early 1970's [3]. It is unsupervised, working without any prediction or validation tasks in mind.
Reference: [11] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: 0:66 petal length 1:46 0:17 4:91 0:82 0.28 0.83 Conceptually, AutoClass finds this model by doing continuous function optimization over parameterized mixtures of two bivariate normals: Prz j M I w 1 Nz j m 1 1 w 2 Nz j m 2 2 AutoClass uses the EM optimization algorithm <ref> [11, 45] </ref>, but any good optimization algorithm will work, and it is easy to visualize the dynamics of a general gradient search.
Reference: [12] <author> W. Dillon and M. Goldstein. </author> <title> Multivariate Analysis: Methods and Applications. </title> <publisher> John Wiley & Sons, </publisher> <year> 1984. </year>
Reference-contexts: For problems like the iris data where the classes are well separated, the two displays are roughly equivalent, but writing out the class probabilities always exposes the entries for which no single class is clearly indicated. The K-means clustering algorithm <ref> [12, 53] </ref> is closely related to AutoClass. It differs in that it uses hard classifications where every object is assigned to its most probable class. This approach works fine for problems like the iris data, but leads to known biases when the classes are not well separated.
Reference: [13] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Belief nets like those in Figure 7 have a much wider scope than the way that AutoClass uses them. A naive Bayesian classifier, for example, is nothing more than an idiot Bayes model applied to data where the class variable is known rather than hidden <ref> [36, 13] </ref>. V. Prior Probabilities Priors are important to avoid overfitting and ensure that one arrives at sensible, conservative answers when data is scarce. Bayesian philosophy emphasizes that no prior is universally correct, but the mixture prior developed here benefits from a sound rationale and excellent empirical results.
Reference: [14] <author> Y. Freund and D. Haussler. </author> <title> Unsupervised learning of distributions on binary vectors using two layer networks. </title> <booktitle> In Proceedings of the 1991 Conference on Neural Information Processing Systems. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: If there are many, a form of beam search that multiplies one factor at a time and discards all but the W most significant classes may give a practical approximation. Freund and Haussler have explored a special case of Equation 3 they call the harmo-nium <ref> [14] </ref>. This model arises when all inputs and hidden features are binary, in which case it reduces to the Boltzmann machine shown in Figure 5.
Reference: [15] <author> S. Geman and D. Geman. </author> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741, </pages> <year> 1984. </year>
Reference-contexts: It is easy to replace MAP induction with more accurate but time consuming approximations to Equation 1, like Laplace's normal approximation [1] or Gibb's sampling <ref> [15] </ref>, but this dissertation leaves such extensions to future work. <p> The literature contains many ways to improve on MAP induction, like Laplace's approximation [1] or Gibb's sampling <ref> [15] </ref>, that could potentially increase MultiClass's performance at the expense of computation time. The most severe flaw in MultiClass is that its priors do not allow for the possibility that a feature is irrelevant.
Reference: [16] <author> Z. Ghahramani. </author> <title> Factorial learning and the EM algorithm. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7 (NIPS*93). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year> <note> ftp://psyche.mit.edu/pub/zoubin/factorial.ps.Z. References 111 </note>
Reference-contexts: One possibility is a noisy-or model [47], which unfolds the belief net of Figure 1 into the more manageable model of Figure 2 using auxiliary propositional variables. Ghahramani has proposed an additive model for the case in which z consists of only continuous features <ref> [16] </ref>, and Musick has written his thesis about using a general purpose conditional estimation model, like a neural net, to represent the troublesome probability [44]. This dissertation takes a different approach. <p> Physicists have been attacking partition function problems for over a century, and they have developed many ingenious methods, like the mean field approximation mentioned earlier. Ghahramani reports good results using the mean field approximation in his directed graph system <ref> [16] </ref>, so there is a chance it applies to Equation 3. Finally, multiplying out the product in Equation 3 is feasible if there are few enough terms in the resulting sum.
Reference: [17] <author> P. E. Gill, W. Murray, M. A. Saunders, and M. H. Wright. </author> <title> User's guide for NPSOL (version 4:0): A Fortran package for nonlinear programming. </title> <type> Technical Report SOL 86-2, </type> <institution> Stanford Systems Optimization Lab, Stanford, </institution> <address> CA, </address> <year> 1986. </year>
Reference-contexts: MultiClass should be able to find the optimal number of classes with improved priors, but at present it is best to determine the right number through cross validation. II. Optimization MultiClass uses a general purpose quasi-Newton optimization package, NPSOL, developed by the Stanford Systems Optimization Lab <ref> [17] </ref>. This algorithm improves upon gradient descent by incrementally building a Hessian approximation [18] from the gradient information, and its only real disadvantage is that it requires On 2 memory, where n is the number of parameters in the model.
Reference: [18] <author> P. E. Gill, W. Murray, and M. H. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press ltd., </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: II. Optimization MultiClass uses a general purpose quasi-Newton optimization package, NPSOL, developed by the Stanford Systems Optimization Lab [17]. This algorithm improves upon gradient descent by incrementally building a Hessian approximation <ref> [18] </ref> from the gradient information, and its only real disadvantage is that it requires On 2 memory, where n is the number of parameters in the model.
Reference: [19] <author> R. Hanson, J. Stutz, and P. Cheeseman. </author> <title> Bayesian classification with correlation and inheritance. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: In describing the transition between successive versions of AutoClass, the program out of which this dissertation arises, Hanson and his coworkers relate that improving the model space brought orders of magnitude more improvement than tinkering with the search algorithm <ref> [19] </ref>. Chapter 2 will cement this lesson by showing numerous examples where an inadequate model renders sharp predictions impossible. The wise practitioner never forgets: Lesson #2 You find what you look for. So you had better make certain you look for the right thing. <p> Interested readers are encouraged to read Jaynes' entertaining discussion on paradoxes in probability theory. 32 4 Mixture Models I. AutoClass The goal of this dissertation is to improve AutoClass, a Bayesian database analysis system developed over the last six years at NASA's Ames Research Center <ref> [9, 10, 19] </ref>. AutoClass traces its ancestry to the long history of finite mixture distributions in statistics [53] and to an earlier program, SNOB, developed by Wallace in the early 1970's [3]. It is unsupervised, working without any prediction or validation tasks in mind.
Reference: [20] <author> M. Henrion, J. S. Breese, and E. J. Horvitz. </author> <title> Decision analysis and expert systems. </title> <journal> AI Magazine, </journal> <volume> 12(4) </volume> <pages> 64-91, </pages> <month> Winter </month> <year> 1991. </year>
Reference-contexts: Nevertheless, fielded expert systems, like the medical diagnosis system PathFinder <ref> [20] </ref>, have demonstrated success using models close to the idiot Bayes one. The complexity of Figure 6 should leave one skeptical that AutoClass will find all the effects in a database, but there is no doubt that whatever structure it does find is very real.
Reference: [21] <author> G. E. Hinton and T. J. Sejnowski. </author> <title> Learning and relearning in Boltzmann machines. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1, chapter 7, </volume> <pages> pages 282-317. </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The above parameter values therefore lead to a uniform distribution over the possible database rows in which adjacent pairs of columns are identical. Equation 1 is how a Hopfield net models data [22]. It is also how a Boltzmann machine works <ref> [21] </ref>, with the additional complication that some of the z i can be hidden. Unfortunately, there is a reason why AutoClass uses the class description functions it does: they are already about as complicated as they can be while remaining computationally feasible.
Reference: [22] <author> J. J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> In Proceedings of the National Academy of Sciences, USA, </booktitle> <volume> volume 79, </volume> <pages> pages 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: The above parameter values therefore lead to a uniform distribution over the possible database rows in which adjacent pairs of columns are identical. Equation 1 is how a Hopfield net models data <ref> [22] </ref>. It is also how a Boltzmann machine works [21], with the additional complication that some of the z i can be hidden.
Reference: [23] <author> L. Hunter and D. </author> <title> States. Applying Bayesian classification to protein structure. </title> <booktitle> In IEEE Conference on Applications of AI, </booktitle> <year> 1991. </year>
Reference-contexts: Its goal is to find a good model of all the effects in a database, thereby providing insight into large, poorly understood domains. It has been successfully applied to many problems, including the IRAS astronomical data [9], protein amino acid sequences <ref> [23] </ref>, and raw English text [51]. AutoClass operates within the density estimation framework of Chapter 3, treating each row in the database as an independent sample from a fixed distribution. Its goal is to reconstruct that distribution.
Reference: [24] <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: MultiClass resembles the hierarchical mixture of experts model of Jordan and Jacobs [31, 30] and the earlier mixture of experts model on which it is based <ref> [24] </ref>, but it is distinguished by its Bayesian methodology and roots in density estimation. It is unique in its priors, noise class, ability to handle missing data, and ability to deal with categorical, discrete, and continuous features.
Reference: [25] <author> E. T. Jaynes. </author> <title> Prior probabilities. </title> <journal> In IEEE Transactions on Systems Science and Cybernetics, </journal> <pages> pages 227-241. </pages> <publisher> IEEE, </publisher> <year> 1968. </year> <note> Also appears in Jaynes [27], chapter 7. </note>
Reference-contexts: The same pattern holds in higher dimensions: Dirichlet shape 8 &gt; &gt; &gt; : horseshoe; k &lt; 1 uniform; k 1 bell; k &gt; 1 Uninformative priors can be logically deduced in many ways, including group invariance <ref> [25] </ref>, marginalization [26], and arguments that seek to maximize the information content of the data [2]. In this instance one can use a marginalization argument, which this dissertation omits, to conclude that k should depend on a deeper hyperparameter ff according to the relation nk ff.
Reference: [26] <author> E. T. Jaynes. </author> <title> Marginalization and prior probabilities. </title> <editor> In A. Zellner, editor, </editor> <title> Bayesian Analysis in Econometrics and Statistics. </title> <publisher> North-Holland Publishing Company, Amsterdam, Holland, </publisher> <year> 1980. </year> <note> Also appears in Jaynes [27], chapter 12. </note>
Reference-contexts: The same pattern holds in higher dimensions: Dirichlet shape 8 &gt; &gt; &gt; : horseshoe; k &lt; 1 uniform; k 1 bell; k &gt; 1 Uninformative priors can be logically deduced in many ways, including group invariance [25], marginalization <ref> [26] </ref>, and arguments that seek to maximize the information content of the data [2]. In this instance one can use a marginalization argument, which this dissertation omits, to conclude that k should depend on a deeper hyperparameter ff according to the relation nk ff.
Reference: [27] <author> E. T. Jaynes. </author> <title> Papers on Probability, Statistics, and Statistical Physics. </title> <address> D. </address> <publisher> Reidel Publishing Co., Dordrecht, Holland, </publisher> <year> 1989. </year> <note> References 112 </note>
Reference: [28] <author> E. T. Jaynes. </author> <title> Probability Theory: </title> <journal> The Logic of Science. </journal> <note> Fragmentary version of June, 1994. http://omega.albany.edu:8008/JaynesBook.html. </note>
Reference-contexts: Together, Equations 1 and 2 completely specify Bayesian probabilistic induction. Jaynes's Probability: the Logic of Science is an exceptional introduction <ref> [28] </ref>, as is the new book by Bernardo and Smith [2]. The posterior distribution, PrM j DI, indicates which models provide the best description of the database. <p> As Jaynes points out, conditioning on continuous quantities like GRE and GPA is really a limit operation, and in general the kernel function may change depending on how the limit is approached <ref> [28] </ref>. However, the models in this dissertation are all members of the well behaved exponential family, and the limiting processes are the natural ones that lead to Equation 7. Interested readers are encouraged to read Jaynes' entertaining discussion on paradoxes in probability theory. 32 4 Mixture Models I.
Reference: [29] <author> G. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kauf-mann, </publisher> <year> 1994. </year> <month> ftp://starry.Stanford.EDU:pub/ronnyk/ml94.ps. </month>
Reference-contexts: If there were nine classes, for example, hypothyroid might occur 20% of the time in seven of them and 67% in the other two. The process of looking for completely irrelevant features is feature subset selection <ref> [29] </ref>, while partial irrelevance is akin to weight sharing in neural networks. In either case, MultiClass must use graph search to decide which parts of its model to prune.
Reference: [30] <author> M. Jordan and R. Jacobs. </author> <title> Supervised learning and divide-and-conquer: A statistical approach. </title> <editor> In P. Utgoff, editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 159-166. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: MultiClass resembles the hierarchical mixture of experts model of Jordan and Jacobs <ref> [31, 30] </ref> and the earlier mixture of experts model on which it is based [24], but it is distinguished by its Bayesian methodology and roots in density estimation.
Reference: [31] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: MultiClass resembles the hierarchical mixture of experts model of Jordan and Jacobs <ref> [31, 30] </ref> and the earlier mixture of experts model on which it is based [24], but it is distinguished by its Bayesian methodology and roots in density estimation. <p> This restriction limits MultiClass to no more than a few hundred parameters, though the program is written to allow alternative optimization algorithms, like conjugate gradient, that do not have this problem. Jordan and Jacobs have developed an EM algorithm for their HME architecture <ref> [31, 32] </ref> which it would also be desirable to implement. Rumelhart finds that second order optimization algorithms, like NPSOL, are sometimes ironically hindered by their far sighted view of the local topology [49]. They can descend into local minima that a less powerful algorithm, like gradient descent, would never see.
Reference: [32] <author> M. I. Jordan and L. Xu. </author> <title> Convergence properties of the EM approach to learning in mixtures-of-experts architectures. </title> <type> Technical Report 9301, </type> <institution> MIT Computational Cognitive Science, </institution> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: This restriction limits MultiClass to no more than a few hundred parameters, though the program is written to allow alternative optimization algorithms, like conjugate gradient, that do not have this problem. Jordan and Jacobs have developed an EM algorithm for their HME architecture <ref> [31, 32] </ref> which it would also be desirable to implement. Rumelhart finds that second order optimization algorithms, like NPSOL, are sometimes ironically hindered by their far sighted view of the local topology [49]. They can descend into local minima that a less powerful algorithm, like gradient descent, would never see.
Reference: [33] <author> R. E. Kass and A. E. Raftery. </author> <title> Bayes factors and model uncertainty. </title> <type> Technical Report #571, </type> <institution> Department of Statistics, Carnegie Mellon University, </institution> <address> PA, </address> <year> 1993. </year> <note> To appear in Journal of the American Statistical Association. </note>
Reference-contexts: The factor in the denominator, PrD j I, is often called the evidence for the models, or the Bayes factor. It plays an important role when one allows the prior information I to vary, as occurs when considering different model space alternatives <ref> [38, 33] </ref>, but for the purpose of this dissertation it is a fixed normalizing constant. Together, Equations 1 and 2 completely specify Bayesian probabilistic induction. Jaynes's Probability: the Logic of Science is an exceptional introduction [28], as is the new book by Bernardo and Smith [2].
Reference: [34] <author> R. Kohavi and G. John. </author> <title> Automatic parameter selection by minimizing estimated error. </title> <address> ftp://starry.Stanford.EDU:pub/ronnyk/c45ap.ps, </address> <year> 1995. </year>
Reference-contexts: In principle, one could provide priors for the hyperpa-rameters and optimize them as part of the search. In practice, one can use a wrapper algorithm like C4.5-AP <ref> [34] </ref> to automatically find their optimal values using cross validation. [4]. 91 Experimental Results and Future Work 92 The user must tell MultiClass how many classes to look for. The program often eliminates unwanted classes by reducing them to zero weight, but this procedure is unreliable.
Reference: [35] <author> R. Kohavi, G. John, R. Long, D. Manley, and K. Pfleger. MLC++: </author> <title> A machine learning library in C++. </title> <booktitle> In Tools with Artificial Intelligence, </booktitle> <pages> pages 740-743. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year> <month> ftp://starry.Stanford.EDU:pub/ronnyk/mlc/toolsmlc.ps. </month>
Reference-contexts: IV. MLC++ Data Sets The experiments tested MultiClass on the collection of databases distributed with MLC++, a machine learning toolbox being developed at Stanford University <ref> [35] </ref>. The data sets include most of those from the UC Irvine repository [43], the Statlog project [41], and Thrun's monk data sets [52]. They are all classification problems. There are no experiments involving continuous predictions due to a lack of well established benchmarks.
Reference: [36] <author> P. Langley, W. Iba, and K. Thompson. </author> <title> An analysis of Bayesian classifiers. </title> <booktitle> In Tenth National Conference on Artificial Intelligence, </booktitle> <year> 1992. </year>
Reference-contexts: Belief nets like those in Figure 7 have a much wider scope than the way that AutoClass uses them. A naive Bayesian classifier, for example, is nothing more than an idiot Bayes model applied to data where the class variable is known rather than hidden <ref> [36, 13] </ref>. V. Prior Probabilities Priors are important to avoid overfitting and ensure that one arrives at sensible, conservative answers when data is scarce. Bayesian philosophy emphasizes that no prior is universally correct, but the mixture prior developed here benefits from a sound rationale and excellent empirical results.
Reference: [37] <author> D. B. Lenat and E. A. Feigenbaum. </author> <title> On the thresholds of knowledge. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 185-250, </pages> <year> 1991. </year>
Reference-contexts: Lenat and Feigenbaum cite disturbing examples: a medical system issuing absurd prescriptions to a patient whose weight and age are accidentally interchanged, a loan authorization program approving an enterprising teenager claiming to have worked for over 20 years, and others <ref> [37] </ref>. Problems like these arise when an expert system fails to validate the things it is told. Fortunately, an expert system induced directly from a database of examples has access to all Example Databases 17 the information it needs to avoid such mistakes.
Reference: [38] <author> D. J. C. MacKay. </author> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 415-447, </pages> <year> 1992. </year> <note> ftp://131.111.48.8/pub/mackay/README.html. References 113 </note>
Reference-contexts: The factor in the denominator, PrD j I, is often called the evidence for the models, or the Bayes factor. It plays an important role when one allows the prior information I to vary, as occurs when considering different model space alternatives <ref> [38, 33] </ref>, but for the purpose of this dissertation it is a fixed normalizing constant. Together, Equations 1 and 2 completely specify Bayesian probabilistic induction. Jaynes's Probability: the Logic of Science is an exceptional introduction [28], as is the new book by Bernardo and Smith [2].
Reference: [39] <author> O. L. Mangasarian and M. V. Solodov. </author> <title> Backpropagation convergence via deterministic non-monotone perturbed minimization. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: a sampling of how long it needs to find the models in Figure 3 on a relatively slow 16:3 SPECmark machine: iris 1 minute breast-cancer 7 minutes vote 18 minutes australian 1.5 hours german 2.5 hours chess 18 hours These times could be improved by using a stochastic gradient approximation <ref> [39] </ref>, by giving MultiClass successively larger portions of a database until its model stabilizes, or even by simply easing the optimality tolerance of NPSOL.
Reference: [40] <author> J. D. Martin and D. O. Billman. </author> <title> Acquiring and combining overlapping concepts. </title> <journal> Machine Learning, </journal> <volume> 16 </volume> <pages> 1-37, </pages> <year> 1994. </year> <month> ftp://ai.iit.nrc.ca/pub/joel/nov.dvi.ps. </month>
Reference-contexts: It requires us to specify the very quantity we hope to model, Prz j MI, before any calculation can take place. Martin works from this equation in his OLOC system <ref> [40] </ref>, but he only avoids circularity by abducting the most probable class assignments Multiple Hidden Features 65 for each z, rather than summing over all of them.
Reference: [41] <author> D. Mitche, D. J. Spiegelhalter, and C. C. Taylor. </author> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: IV. MLC++ Data Sets The experiments tested MultiClass on the collection of databases distributed with MLC++, a machine learning toolbox being developed at Stanford University [35]. The data sets include most of those from the UC Irvine repository [43], the Statlog project <ref> [41] </ref>, and Thrun's monk data sets [52]. They are all classification problems. There are no experiments involving continuous predictions due to a lack of well established benchmarks.
Reference: [42] <author> T. M. Mitchell. </author> <title> Version spaces: A candidate elimination approach to rule learning. </title> <booktitle> In Proceedings of the Third International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 305-310, </pages> <year> 1977. </year>
Reference-contexts: Only when the posterior distribution is sharply peaked are the two approaches effectively equivalent. Mitchell wrote down Lesson #1 for the machine learning community in 1977, in his work on version space learning <ref> [42] </ref>. It no doubt has a considerably longer history in the field of philosophy. But it is worth continually reemphasizing, since it seems so easy to conflate a model space with the algorithms that search it.
Reference: [43] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases, </title> <year> 1995. </year> <month> ftp://ics.uci.edu:pub/machine-learning-databases. </month>
Reference-contexts: IV. MLC++ Data Sets The experiments tested MultiClass on the collection of databases distributed with MLC++, a machine learning toolbox being developed at Stanford University [35]. The data sets include most of those from the UC Irvine repository <ref> [43] </ref>, the Statlog project [41], and Thrun's monk data sets [52]. They are all classification problems. There are no experiments involving continuous predictions due to a lack of well established benchmarks.
Reference: [44] <author> C. R. Musick Jr. </author> <title> Belief Network Induction. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: Ghahramani has proposed an additive model for the case in which z consists of only continuous features [16], and Musick has written his thesis about using a general purpose conditional estimation model, like a neural net, to represent the troublesome probability <ref> [44] </ref>. This dissertation takes a different approach. It keeps the graph structure of Figure 1, but drops the directionality of the arrows to give the undirected belief net of Figure 3, known as a Multiple Hidden Features 63 Multiple Hidden Features 64 Markov random field.
Reference: [45] <author> R. M. Neal and G. E. Hinton. </author> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <note> Submitted to Biometrika. ftp://cs.toronto.edu/pub/radford/www/publications.html, </note> <year> 1993. </year>
Reference-contexts: 0:66 petal length 1:46 0:17 4:91 0:82 0.28 0.83 Conceptually, AutoClass finds this model by doing continuous function optimization over parameterized mixtures of two bivariate normals: Prz j M I w 1 Nz j m 1 1 w 2 Nz j m 2 2 AutoClass uses the EM optimization algorithm <ref> [11, 45] </ref>, but any good optimization algorithm will work, and it is easy to visualize the dynamics of a general gradient search.
Reference: [46] <author> G. Pagallo and D. Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning Journal, </journal> <volume> 5 </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: A decision tree has trouble with disjunctive rules, because it must replicate each disjunct down multiple paths in the tree <ref> [46] </ref>. MultiClass is closer to a feed forward sigmoid neural net with two hidden layers, where each node in the second layer corresponds to a class in the mixture model. VI. Density vs. Conditional Estimation This section derives the implicit assumption of conditional estimation. A little new notation is required.
Reference: [47] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: IV. Belief Nets Bayesian belief nets are a good language for understanding AutoClass, and for describing hidden feature models in general. A complete account of belief nets can be found in many places, Mixture Models 42 most notably Pearl's book <ref> [47] </ref>. <p> We see at once, though, that this equation is no different than using a single hidden feature that is a crossproduct of all the distinct ones, so clearly we want something different. One possibility is a noisy-or model <ref> [47] </ref>, which unfolds the belief net of Figure 1 into the more manageable model of Figure 2 using auxiliary propositional variables. <p> This model is a maximum entropy model that takes into account correlations among the hidden features and the observables. Pearl's book gives a good introductory description of undirected belief nets <ref> [47] </ref>. The directed and undirected graphs encode different independence relations. In the directed belief net, the hidden classes are all marginally independent, but conditionally dependent given z. This type of graph is typified by the wet grass example of Chapter 4.
Reference: [48] <author> C. Peterson and J. R. Anderson. </author> <title> A mean field theory learing algorithm for neural networks. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 995-1019, </pages> <year> 1987. </year>
Reference-contexts: The Boltzmann machine resorts to numeric integration by stochastic simulation to estimate Z, although Peterson has developed a much improved algorithm that uses a different technique, the mean field approximation of statistical mechanics <ref> [48] </ref>. Equation 1 is easy to normalize when all the z i are continuous, in which case f zj is just an unusual way to parameterize a multivariate normal distribution. In this case continuous integration is considerably easier than discrete summation.
Reference: [49] <author> D. Rumelhart. </author> <type> Personal communications. </type>
Reference-contexts: Jordan and Jacobs have developed an EM algorithm for their HME architecture [31, 32] which it would also be desirable to implement. Rumelhart finds that second order optimization algorithms, like NPSOL, are sometimes ironically hindered by their far sighted view of the local topology <ref> [49] </ref>. They can descend into local minima that a less powerful algorithm, like gradient descent, would never see. Priors smooth the terrain and eliminate most of this effect, but one must still be careful. Figure 1 illustrates a common danger.
Reference: [50] <author> E. Saund. </author> <title> A multiple cause mixture model for unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 51-71, </pages> <year> 1995. </year>
Reference-contexts: They obtain promising results in a handwritten digit recognition problem, where each hidden feature identifies a line or Multiple Hidden Features 68 stroke, as opposed to a complete digit. Saund has looked at similar letter recognition problem using a different multiple cause model <ref> [50] </ref>. His model is less firmly grounded in probability theory, but against that he obtains good results and has no partition function to evaluate. VI. Logical Rules This section shows how hidden feature models naturally extend logical rules.
Reference: [51] <author> H. Schutze. </author> <title> Dimensions of meaning. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <year> 1992. </year> <note> ftp://csli.stanford.edu/pub/prosit/papers. References 114 </note>
Reference-contexts: Its goal is to find a good model of all the effects in a database, thereby providing insight into large, poorly understood domains. It has been successfully applied to many problems, including the IRAS astronomical data [9], protein amino acid sequences [23], and raw English text <ref> [51] </ref>. AutoClass operates within the density estimation framework of Chapter 3, treating each row in the database as an independent sample from a fixed distribution. Its goal is to reconstruct that distribution.
Reference: [52] <author> S. B. Thrun et al. </author> <title> The monk's problemsa performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> CMU School of Computer Science, </institution> <year> 1991. </year>
Reference-contexts: IV. MLC++ Data Sets The experiments tested MultiClass on the collection of databases distributed with MLC++, a machine learning toolbox being developed at Stanford University [35]. The data sets include most of those from the UC Irvine repository [43], the Statlog project [41], and Thrun's monk data sets <ref> [52] </ref>. They are all classification problems. There are no experiments involving continuous predictions due to a lack of well established benchmarks.
Reference: [53] <author> D. M. Titterington, A. F. M. Smith, and U. E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1985. </year>
Reference-contexts: AutoClass The goal of this dissertation is to improve AutoClass, a Bayesian database analysis system developed over the last six years at NASA's Ames Research Center [9, 10, 19]. AutoClass traces its ancestry to the long history of finite mixture distributions in statistics <ref> [53] </ref> and to an earlier program, SNOB, developed by Wallace in the early 1970's [3]. It is unsupervised, working without any prediction or validation tasks in mind. Its goal is to find a good model of all the effects in a database, thereby providing insight into large, poorly understood domains. <p> For problems like the iris data where the classes are well separated, the two displays are roughly equivalent, but writing out the class probabilities always exposes the entries for which no single class is clearly indicated. The K-means clustering algorithm <ref> [12, 53] </ref> is closely related to AutoClass. It differs in that it uses hard classifications where every object is assigned to its most probable class. This approach works fine for problems like the iris data, but leads to known biases when the classes are not well separated.
Reference: [54] <author> G. G. Towell, J. W. Shavlik, and M. O. Noordewier. </author> <title> Refinement of approximately correct domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year>
Reference-contexts: The search will automatically adjust probabilities away from zero for rules like birds fly that are not absolute. The result would be much like Towell's KBANN system <ref> [54] </ref>, but with the potential for considerably greater flexibility, since a multiple hidden feature model can so naturally incorporate a complete set of DNF rules. 6 Supervised Mixture Models I. MultiClass This Chapter describes a new modeling system, MultiClass, that acts as a supervised version of AutoClass.
References-found: 54

