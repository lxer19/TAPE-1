URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1995/umsi-95-13.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1995/
Root-URL: http://www.cs.umn.edu
Title: Approximate Inverse Techniques for Block-Partitioned Matrices  
Author: Edmond Chow and Yousef Saad 
Date: April 13, 1995  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science, and Minnesota Supercomputer Institute University of Minnesota  
Abstract: This paper proposes some preconditioning options when the system matrix is in block-partitioned form. This form may arise naturally, for example from the incompressible Navier-Stokes equations, or may be imposed after a domain decomposition reordering. Approximate inverse techniques are used to generate sparse approximate solutions whenever these are needed in forming the preconditioner. The storage requirements for these preconditioners may be much less than for ILU preconditioners for tough, large-scale CFD problems. The numerical experiments reported show that these preconditioners can help us solve difficult linear systems whose coefficient matrices are highly indefinite. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Alvarado, H. Da~g and M. ten Bruggencate. </author> <title> Block-bordered diagonalization and parallel iterative solvers. </title> <booktitle> Colorado Conference on Iterative Methods, </booktitle> <month> April 5-9, </month> <year> 1994. </year>
Reference-contexts: More recently, Elman and Silvester [13] proposed a few techniques for the specific case of the Stokes and Navier-Stokes problems. A number of variations of Block Block Approximate Inverse Techniques 3 Jacobi preconditioners have also been developed <ref> [1, 9] </ref>. In these techniques the off-block diagonal terms are either neglected or an attempt is made to approximate their effect. This paper explores some preconditioning options when the matrix is expressed in block-partitioned form, either naturally or after some domain decomposition type reordering.
Reference: [2] <author> O. Axelsson. </author> <title> Incomplete block matrix factorization preconditioning methods. </title> <journal> The ultimate answer? J. Comput. Appl. Math., </journal> <volume> 12 & 13 (1985), </volume> <pages> pp. 3-18. </pages>
Reference-contexts: This was later extended to the more general case where the diagonal blocks are arbitrary [4, 17]. In many of these cases, the incomplete block factorizations are developed for matrices arising from the discretization of PDE's <ref> [2, 3, 7, 17, 19] </ref> and utilize approximate inverses when diagonal blocks need to be inverted. More recently, Elman and Silvester [13] proposed a few techniques for the specific case of the Stokes and Navier-Stokes problems.
Reference: [3] <author> O. Axelsson. </author> <title> Iterative Solution Methods. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This was later extended to the more general case where the diagonal blocks are arbitrary [4, 17]. In many of these cases, the incomplete block factorizations are developed for matrices arising from the discretization of PDE's <ref> [2, 3, 7, 17, 19] </ref> and utilize approximate inverses when diagonal blocks need to be inverted. More recently, Elman and Silvester [13] proposed a few techniques for the specific case of the Stokes and Navier-Stokes problems.
Reference: [4] <author> O. Axelsson and B. Polman. </author> <title> On approximate factorization methods for block matrices suitable for vector and parallel processors. </title> <journal> Lin. Alg. Appl., </journal> <volume> 77 (1986), </volume> <pages> pp. 3-26. </pages>
Reference-contexts: The inverses of tridiagonal matrices encountered in the approximations are themselves approximated by tridiagonal matrices, exploiting an exact formula for the inverse of a tridiagonal matrix. This was later extended to the more general case where the diagonal blocks are arbitrary <ref> [4, 17] </ref>. In many of these cases, the incomplete block factorizations are developed for matrices arising from the discretization of PDE's [2, 3, 7, 17, 19] and utilize approximate inverses when diagonal blocks need to be inverted.
Reference: [5] <author> M. W. Benson and P. O. Frederickson. </author> <title> Iterative solution of large sparse linear systems arising in certain multidimensional approximation problems. </title> <journal> Utilitas Math., </journal> <volume> 22 (1982), </volume> <pages> pp. 127-140. </pages>
Reference-contexts: For these algorithms to be practical, they must provide approximations that are sparse. Block Approximate Inverse Techniques 4 A number of techniques have recently been developed to construct a sparse approximate inverse of a matrix, to be used as a preconditioner <ref> [5, 6, 8, 10, 15, 17, 18] </ref>.
Reference: [6] <author> E. Chow and Y. Saad. </author> <title> Approximate inverse preconditioners for general sparse matrices, </title> <type> Technical Report UMSI 94/101, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, Minnesota, </institution> <year> 1994. </year>
Reference-contexts: If the right-hand-side b and the initial guess for x are sparse, this is a very economical method for computing a sparse approximate solution. We have used this technique to construct preconditioners based on approximating the inverse of A directly <ref> [6] </ref>. This paper is organized as follows. In Section 2 we describe the sparse approximate inverse algorithm and some techniques for finding sparse approximate solutions with the Schur complement. Section 3 describes how block-partitioned factorizations may be used as preconditioners. <p> For these algorithms to be practical, they must provide approximations that are sparse. Block Approximate Inverse Techniques 4 A number of techniques have recently been developed to construct a sparse approximate inverse of a matrix, to be used as a preconditioner <ref> [5, 6, 8, 10, 15, 17, 18] </ref>. <p> Such a preconditioner is distinctly easier than most existing preconditioners to construct and apply on a massively parallel computer. Because they do not rely on matrix factorizations, these preconditioners often are complementary to ILU preconditioners <ref> [6, 22] </ref>. Previous approaches select a sparsity pattern for x and then minimize (4) in a least squares sense. In our approach, we minimize (4) with a method that reduces the residual norm at each step, such as Minimal Residual or FGMRES [20], beginning with a sparse initial guess. <p> inverse technique for each column may be generalized to find a sparse approximate solution to the sparse linear problem Ax = b by minimizing min kb Axk 2 (5) possibly with an existing preconditioner M for A. 2.1 Approximate inverse algorithm We describe a modification of the technique reported in <ref> [6] </ref> that guarantees the reduction of the residual norm at each minimal residual step. Starting with a sparse initial guess, the fill-in is increased by one at each iteration. <p> This explains why a transpose initial guess for the approximate inverse combined with self-preconditioning (preconditioning r with the current approximate inverse) is so effective for some problems <ref> [6] </ref>. There are many possibilities for the second stage. We choose to drop one entry in x and introduce one new entry in d if this causes a decrease in the residual norm. The candidate for dropping is the smallest absolute nonzero entry in x. <p> The Schur complement S which appears in the block factorization is approximated by its preconditioner. Approximate inverse techniques <ref> [6] </ref> are used in different ways to approximate either S directly or a part of A 1 . As can be seen by comparing Tables 5 and 10, we can solve more problems with the block approach than with a standard ILU factorization.
Reference: [7] <author> P. Concus, G. H. Golub and G. Meurant. </author> <title> Block preconditioning for the conjugate gradient method. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6 (1985), </volume> <pages> pp. 309-332. </pages>
Reference-contexts: There is a danger, however, that for general matrices, B may be singular after the reordering. Much work has been done on exploiting some form of blocking in conjunction with preconditioning. In one of the earlier papers on the subject, Concus, Golub, and Meurant <ref> [7] </ref> introduce the idea of block preconditioning, designed for block-tridiagonal matrices whose diagonal blocks are tridiagonal. The inverses of tridiagonal matrices encountered in the approximations are themselves approximated by tridiagonal matrices, exploiting an exact formula for the inverse of a tridiagonal matrix. <p> This was later extended to the more general case where the diagonal blocks are arbitrary [4, 17]. In many of these cases, the incomplete block factorizations are developed for matrices arising from the discretization of PDE's <ref> [2, 3, 7, 17, 19] </ref> and utilize approximate inverses when diagonal blocks need to be inverted. More recently, Elman and Silvester [13] proposed a few techniques for the specific case of the Stokes and Navier-Stokes problems. <p> This is particularly the case for block preconditioners for block-tridiagonal matrices <ref> [7, 19] </ref>. For these algorithms to be practical, they must provide approximations that are sparse. Block Approximate Inverse Techniques 4 A number of techniques have recently been developed to construct a sparse approximate inverse of a matrix, to be used as a preconditioner [5, 6, 8, 10, 15, 17, 18].
Reference: [8] <author> J. D. F. Cosgrove, J. C. Daz and A. Griewank. </author> <title> Approximate inverse preconditioning for sparse linear systems. </title> <journal> Intl. J. Comp. Math., </journal> <volume> 44 (1992), </volume> <pages> pp. 91-110. </pages>
Reference-contexts: For these algorithms to be practical, they must provide approximations that are sparse. Block Approximate Inverse Techniques 4 A number of techniques have recently been developed to construct a sparse approximate inverse of a matrix, to be used as a preconditioner <ref> [5, 6, 8, 10, 15, 17, 18] </ref>.
Reference: [9] <author> L. C. Dutto, W. G. Habashi and M. Fortin. </author> <title> Parallelizable block diagonal precondi-tioners for the compressible Navier-Stokes equations, </title> <journal> Comput. Methods Appl. Mech. Engrg., </journal> <volume> 117 (1994), </volume> <pages> pp. 15-47. </pages>
Reference-contexts: More recently, Elman and Silvester [13] proposed a few techniques for the specific case of the Stokes and Navier-Stokes problems. A number of variations of Block Block Approximate Inverse Techniques 3 Jacobi preconditioners have also been developed <ref> [1, 9] </ref>. In these techniques the off-block diagonal terms are either neglected or an attempt is made to approximate their effect. This paper explores some preconditioning options when the matrix is expressed in block-partitioned form, either naturally or after some domain decomposition type reordering.
Reference: [10] <author> T. Huckle and M. Grote. </author> <title> A new approach to parallel preconditioning with sparse approximate inverses. </title> <type> Manuscript SCCM-94-03, </type> <institution> Scientific Computing and Computational Mathematics Program, Stanford University, Stanford, California, </institution> <year> 1994. </year>
Reference-contexts: For these algorithms to be practical, they must provide approximations that are sparse. Block Approximate Inverse Techniques 4 A number of techniques have recently been developed to construct a sparse approximate inverse of a matrix, to be used as a preconditioner <ref> [5, 6, 8, 10, 15, 17, 18] </ref>.
Reference: [11] <author> H. C. Elman. </author> <title> A stability analysis of incomplete LU factorizations. </title> <journal> Math. Comp., </journal> <volume> 47 (1986), </volume> <pages> pp. 191-217. </pages>
Reference-contexts: If A is highly indefinite or has large nonsymmetric parts, an ILU factorization often produces unstable L and U factors, i.e., k (LU ) 1 k can be extremely large, caused by the long recurrences in the forward and backward triangular solves <ref> [11] </ref>. To illustrate this point, we computed for a number of factorizations the rough lower bound k (LU ) 1 k 1 k (LU ) 1 ek 1 ; where e is a vector of all ones.
Reference: [12] <author> H. C. Elman. </author> <title> Multigrid and Krylov subspace methods for the discrete Stokes equations. Proc. Matrix Analysis and Parallel Computing, </title> <institution> Keio University, </institution> <month> March 14-16, </month> <year> 1994, </year> <pages> pp. </pages> <month> 151-164. </month> <title> Block Approximate Inverse Techniques 25 </title>
Reference: [13] <author> H. C. Elman and D. Silvester. </author> <title> Fast nonsymmetric iterations and preconditioning for Navier-Stokes equations. </title> <institution> UMIACS-TR-94-66, Institute for Advanced Computer Studies, University of Maryland, College Park, Maryland, </institution> <year> 1994. </year>
Reference-contexts: In many of these cases, the incomplete block factorizations are developed for matrices arising from the discretization of PDE's [2, 3, 7, 17, 19] and utilize approximate inverses when diagonal blocks need to be inverted. More recently, Elman and Silvester <ref> [13] </ref> proposed a few techniques for the specific case of the Stokes and Navier-Stokes problems. A number of variations of Block Block Approximate Inverse Techniques 3 Jacobi preconditioners have also been developed [1, 9].
Reference: [14] <author> M. Engleman. FIDAP: </author> <title> Examples Manual, Revision 6.0. </title> <booktitle> Fluid Dynamics International, </booktitle> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: Grid n nnz n B n C 32 by 32 961 4681 900 61 64 by 64 3969 19593 3844 125 Table 1: Laplacian test problems. The second set of test matrices were extracted from the example incompressible Navier-Stokes problems in the FIDAP <ref> [14] </ref> package. All problems with zero C submatrix were tested. In the case of transient problems, the matrices are the Jacobians when the Newton iterations had converged. The matrices are reordered so that the continuity equations are ordered last.
Reference: [15] <author> M. Grote and H. D. Simon. </author> <title> Parallel preconditioning and approximate inverses on the Connection Machine. </title> <editor> In R. F. Sincovec, D. E. Keyes, L. R. Petzold and D. A. Reed, eds., </editor> <booktitle> Parallel Processing for Scientific Computing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 519-523. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Pennsylvania, </address> <year> 1993. </year>
Reference-contexts: For these algorithms to be practical, they must provide approximations that are sparse. Block Approximate Inverse Techniques 4 A number of techniques have recently been developed to construct a sparse approximate inverse of a matrix, to be used as a preconditioner <ref> [5, 6, 8, 10, 15, 17, 18] </ref>.
Reference: [16] <author> David E. Keyes and William D. Gropp. </author> <title> A comparison of domain decomposition techniques for elliptic partial differential equations and their parallel implementation. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <note> 8 (1987) pp. s166-s202. </note>
Reference-contexts: the initial guess should make the x-part of the residual vector equal to 0 for the original system (11), i.e., the initial guess is u 0 = x 0 ! with x 0 = B 1 (f F y 0 ) : This result, due to Eisenstat and reported in <ref> [16] </ref>, immediately follows from (16) which shows that the preconditioned matrix has the particular form, (LU ) 1 A = I 0 S S : (17) Thus, if the initial residual has its x-component equal to zero then all iterates will be vectors with y components only, and a GMRES iteration
Reference: [17] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> On a family of two-level preconditionings of the incomplete block factorization type, </title> <journal> Soviet J. Numer. Anal. Math. Model., </journal> <volume> 1 (1986), </volume> <pages> pp. 293-320. </pages>
Reference-contexts: The inverses of tridiagonal matrices encountered in the approximations are themselves approximated by tridiagonal matrices, exploiting an exact formula for the inverse of a tridiagonal matrix. This was later extended to the more general case where the diagonal blocks are arbitrary <ref> [4, 17] </ref>. In many of these cases, the incomplete block factorizations are developed for matrices arising from the discretization of PDE's [2, 3, 7, 17, 19] and utilize approximate inverses when diagonal blocks need to be inverted. <p> This was later extended to the more general case where the diagonal blocks are arbitrary [4, 17]. In many of these cases, the incomplete block factorizations are developed for matrices arising from the discretization of PDE's <ref> [2, 3, 7, 17, 19] </ref> and utilize approximate inverses when diagonal blocks need to be inverted. More recently, Elman and Silvester [13] proposed a few techniques for the specific case of the Stokes and Navier-Stokes problems. <p> For these algorithms to be practical, they must provide approximations that are sparse. Block Approximate Inverse Techniques 4 A number of techniques have recently been developed to construct a sparse approximate inverse of a matrix, to be used as a preconditioner <ref> [5, 6, 8, 10, 15, 17, 18] </ref>.
Reference: [18] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> Factorized sparse approximate inverse precon-ditionings I. </title> <journal> Theory. SIAM J. Matrix Anal. Appl., </journal> <volume> 14 (1993), </volume> <pages> pp. 45-58. </pages>
Reference-contexts: For these algorithms to be practical, they must provide approximations that are sparse. Block Approximate Inverse Techniques 4 A number of techniques have recently been developed to construct a sparse approximate inverse of a matrix, to be used as a preconditioner <ref> [5, 6, 8, 10, 15, 17, 18] </ref>.
Reference: [19] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> Incomplete block factorizations as precondi-tioners for sparse SPD matrices. </title> <note> Research Report EM-RR 6/92, </note> <institution> Elegant Mathematics, Inc. </institution> <address> Bothell, Washington, </address> <year> 1992. </year>
Reference-contexts: This was later extended to the more general case where the diagonal blocks are arbitrary [4, 17]. In many of these cases, the incomplete block factorizations are developed for matrices arising from the discretization of PDE's <ref> [2, 3, 7, 17, 19] </ref> and utilize approximate inverses when diagonal blocks need to be inverted. More recently, Elman and Silvester [13] proposed a few techniques for the specific case of the Stokes and Navier-Stokes problems. <p> This is particularly the case for block preconditioners for block-tridiagonal matrices <ref> [7, 19] </ref>. For these algorithms to be practical, they must provide approximations that are sparse. Block Approximate Inverse Techniques 4 A number of techniques have recently been developed to construct a sparse approximate inverse of a matrix, to be used as a preconditioner [5, 6, 8, 10, 15, 17, 18].
Reference: [20] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 461-469. </pages>
Reference-contexts: Previous approaches select a sparsity pattern for x and then minimize (4) in a least squares sense. In our approach, we minimize (4) with a method that reduces the residual norm at each step, such as Minimal Residual or FGMRES <ref> [20] </ref>, beginning with a sparse initial guess. Sparsity is preserved by dropping elements in the search direction or current solution at each step based on their magnitude or criteria related to the residual norm reduction. <p> In an approach based on the larger system (11) this is not necessary. In fact any iterative process can be used for solving with M S and B provided we use a flexible variant of GMRES such as FGMRES <ref> [20] </ref>. Block Approximate Inverse Techniques 10 Systems involving B may be solved in many ways, depending on their difficulty and what we know about B. If B is known to be well-conditioned, then triangular solves with incomplete LU factors may be sufficient. <p> Finally, in Sections 4.5 and 4.6, we present the results of the new preconditioners on more realistic problems arising from the incompressible Navier-Stokes equations. Linear systems were constructed so that the solution is a vector of all ones. A zero initial guess for right-preconditioned FGMRES <ref> [20] </ref> restarted every 20 iterations was used to solve the systems. The Tables show the number of iterations required to reduce the residual norm by 10 7 . The iterations were stopped when 300 matrix-vector multiplications were reached, indicated by a dagger (y).
Reference: [21] <author> Y. Saad. ILUT: </author> <title> A dual threshold incomplete LU factorization. </title> <journal> Num. Lin. Alg. Appl., </journal> <volume> 1 (1994), </volume> <pages> pp. 387-402. </pages>
Reference: [22] <author> Y. Saad. </author> <title> Preconditioned Krylov subspace methods for CFD applications, Proceedings of the International Workshop on Solution Techniques for Large-Scale CFD Problems, </title> <address> Montreal, </address> <month> Sept. </month> <pages> 26-28, </pages> <year> 1994, </year> <pages> pp. 179-195. </pages>
Reference-contexts: Such a preconditioner is distinctly easier than most existing preconditioners to construct and apply on a massively parallel computer. Because they do not rely on matrix factorizations, these preconditioners often are complementary to ILU preconditioners <ref> [6, 22] </ref>. Previous approaches select a sparsity pattern for x and then minimize (4) in a least squares sense. In our approach, we minimize (4) with a method that reduces the residual norm at each step, such as Minimal Residual or FGMRES [20], beginning with a sparse initial guess.
Reference: [23] <author> Y. Saad. SPARSKIT: </author> <title> a basic tool kit for sparse matrix computations, </title> <type> Version 2. Preprint, </type> <institution> University of Minnesota, Minneapolis, Minnesota, </institution> <year> 1993. </year>
Reference-contexts: The Tables show the number of iterations required to reduce the residual norm by 10 7 . The iterations were stopped when 300 matrix-vector multiplications were reached, indicated by a dagger (y). The codes were written in FORTRAN 77 using many routines from SPARSKIT <ref> [23] </ref>, and run in single precision on a Cray C90 supercomputer. 4.1 Test problems and methods The first set of test problems is a finite difference Laplace equation with Dirichlet boundary conditions. Three different sized grids were used. The matrices were reordered using a domain decomposition reordering with 4 subdomains.
References-found: 23

