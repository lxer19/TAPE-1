URL: http://neural-server.aston.ac.uk/Papers/postscript/NCRG_96_026.ps.Z
Refering-URL: http://www.cs.toronto.edu/~carl/gp.html
Root-URL: 
Email: c.k.i.williams@aston.ac.uk  
Title: Computing with infinite networks  
Author: Christopher K. I. Williams 
Address: B4 7ET, UK  
Affiliation: Neural Computing Research Group Department of Computer Science and Applied Mathematics Aston University, Birmingham  
Date: 1997  
Note: To appear in: Advances in Neural Information Processing Systems 9, eds. M. C. Mozer, M. I. Jordan and T. Petsche. MIT Press,  
Abstract: For neural networks with a wide class of weight-priors, it can be shown that in the limit of an infinite number of hidden units the prior over functions tends to a Gaussian process. In this paper analytic forms are derived for the covariance function of the Gaussian processes corresponding to networks with sigmoidal and Gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units, and shows that, somewhat paradoxically, it may be easier to compute with infinite networks than finite ones.
Abstract-found: 1
Intro-found: 1
Reference: <author> Cressie, N. A. C. </author> <year> (1993). </year> <title> Statistics for Spatial Data. </title> <publisher> Wiley. </publisher>
Reference: <author> Hornik, K. </author> <year> (1993). </year> <title> Some new results on neural network approximation. </title> <booktitle> Neural Networks 6 (8), </booktitle> <pages> 1069-1072. </pages>
Reference: <author> Journel, A. G. and C. J. </author> <month> Huijbregts </month> <year> (1978). </year> <title> Mining Geostatistics. </title> <publisher> Academic Press. </publisher>
Reference: <author> MacKay, D. J. C. </author> <year> (1992). </year> <title> A Practical Bayesian Framework for Backpropagation Networks. </title> <booktitle> Neural Computation 4(3), </booktitle> <pages> 448-472. </pages>
Reference-contexts: Appropriately scaled, the graph of this function is very similar to the tanh function which is more commonly used in the neural networks literature. In calculating V (x; x 0 ) def = E u [h (x; u)h (x 0 ; u)] we make the usual assumptions <ref> (e.g. MacKay, 1992) </ref> that u is drawn from a zero-mean Gaussian distribution with co-variance matrix , i.e. u ~ N (0; ). Let ~ x = (1; x 1 ; : : : ; x d ) be an augmented input vector whose first entry corresponds to the bias.
Reference: <author> Neal, R. M. </author> <year> (1996). </year> <title> Bayesian Learning for Neural Netowrks. </title> <booktitle> Springer. Lecture Notes in Statistics 118. </booktitle>
Reference-contexts: This allows predictions to be made using neural networks with an infinite number of hidden units in time O (n 3 ), where n is the number of training examples 1 . The only alternative currently available is to use Markov Chain Monte Carlo (MCMC) methods <ref> (e.g. Neal, 1996) </ref> for networks with a large (but finite) number of hidden units. However, this is likely to be computationally expensive, and we note possible concerns over the time needed for the Markov chain to reach equilibrium. <p> Let b and the v's have independent zero-mean distributions of variance 2 b and 2 v respec tively, and let the weights u j for each hidden unit be independently and identically distributed. Denoting all weights by w, we obtain <ref> (following Neal, 1996) </ref> E w [f (x)] = 0 (7) b + j v E u [h j (x; u)h j (x 0 ; u)] (8) b + H 2 where equation 9 follows because all of the hidden units are identically distributed. <p> The finite network simulations were carried out using a slightly modified version of Neal's MCMC Bayesian neural networks code <ref> (Neal, 1996) </ref> and the inputs were drawn from a 4 Note that this would require ! 2 ! 1 and hence the Central Limit Theorem would no longer hold, i.e. the process would be non-Gaussian. N (0; 1) distribution.
Reference: <author> Parzen, E. </author> <year> (1962). </year> <title> Stochastic Processes. </title> <publisher> Holden-Day. </publisher>
Reference: <author> Poggio, T. and F. </author> <title> Girosi (1990). Networks for approximation and learning. </title> <booktitle> Proceedings of IEEE 78, </booktitle> <pages> 1481-1497. </pages>
Reference-contexts: They have appeared in a wide variety of contexts including geostatistics where the method is known as "kriging" (Journel and Huijbregts, 1978; Cressie 1993), multidimensional spline smoothing (Wahba, 1990), in the derivation of radial basis function neural networks <ref> (Poggio and Girosi, 1990) </ref> and in the work of Whittle (1963). 3 Covariance functions for Neural Networks Consider a network which takes an input x, has one hidden layer with H units and then linearly combines the outputs of the hidden units with a bias to obtain f (x). <p> Gaussian basis functions are often used in Radial Basis Function (RBF) networks <ref> (e.g. Poggio and Girosi, 1990) </ref>.
Reference: <author> Wahba, G. </author> <year> (1990). </year> <title> Spline Models for Observational Data. </title> <booktitle> Society for Industrial and Applied Mathematics. CBMS-NSF Regional Conference series in applied mathematics. </booktitle>
Reference-contexts: Equations 4 and 5 are the analogue for spatial processes of Wiener-Kolmogorov prediction theory. They have appeared in a wide variety of contexts including geostatistics where the method is known as "kriging" (Journel and Huijbregts, 1978; Cressie 1993), multidimensional spline smoothing <ref> (Wahba, 1990) </ref>, in the derivation of radial basis function neural networks (Poggio and Girosi, 1990) and in the work of Whittle (1963). 3 Covariance functions for Neural Networks Consider a network which takes an input x, has one hidden layer with H units and then linearly combines the outputs of the <p> centered on each of the points, where v is chosen iid from a distribution with mean zero and variance 2 v . 3.3 Comparing covariance functions The priors over functions specified by sigmoidal and Gaussian neural networks differ from covariance functions that are usually employed in the literature, e.g. splines <ref> (Wahba, 1990) </ref>.
Reference: <author> Whittle, P. </author> <year> (1963). </year> <title> Prediction and regulation by linear least-square methods. </title> <publisher> English Universities Press. </publisher>
Reference: <author> Williams, C. K. I. and C. E. </author> <title> Rasmussen (1996). Gaussian processes for regression. </title> <editor> In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pp. 514-520. </pages> <publisher> MIT Press. </publisher>
References-found: 10

