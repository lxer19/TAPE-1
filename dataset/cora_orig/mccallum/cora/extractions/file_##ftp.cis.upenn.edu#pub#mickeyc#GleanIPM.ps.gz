URL: file://ftp.cis.upenn.edu/pub/mickeyc/GleanIPM.ps.gz
Refering-URL: http://www.umiacs.umd.edu/~resnik/cl_colloquium/
Root-URL: 
Email: mickeyc@linc.cis.upenn.edu srini@linc.cis.upenn.edu.  
Phone: Phone: (215) 898-0332 Fax: (215) 573-9247  
Title: Glean: Using Syntactic Information in Document Filtering  Running Title: Glean: Using Syntactic Information in Document Filtering  
Author: R. Chandrasekar B. Srinivas 
Address: Philadelphia, PA 19104 Philadelphia, PA 19104  
Affiliation: Institute for Research in Cognitive Science Institute for Research Center for the Advanced Study of India in Cognitive Science University of Pennsylvania University of Pennsylvania  
Note: Submitted to Info. Proc. Mgmnt.  
Abstract: fl This is a substantially revised version of a paper presented at RIAO'97, in Montreal, Canada, and included in the RIAO'97 proceedings [Chandrasekar and Srinivas, 1997c] Acknowledgments: Much of this work was done when the first author was at IRCS, on leave from NCST, Bombay. We thank the Linguistic Data Consortium, University of Pennsylvania, for providing us access to New York Times (NYT) and Wall Street Journal (WSJ) data for our experiments. This work is partially supported by NSF grant NSF-STC SBR 8920230, ARPA grant N00014-94 and ARO grant DAAH04-94-G0426. We thank the two reviewers who have helped significantly in refining this paper. 
Abstract-found: 1
Intro-found: 1
Reference: [Anick and Vaithyanathan, 1997] <author> Anick, P. G. and Vaithyanathan, S. </author> <year> (1997). </year> <title> Exploiting clustering and phrases for context-based information retrieval. </title> <booktitle> In Proc. ACM SIGIR-97, </booktitle> <pages> pages 314-323. </pages>
Reference-contexts: Other approaches [Korfhage, 1991, Hearst and Pedersen, 1996, Hearst and Karadi, 1997] have provided extensive visualization tools to view and prune the results returned by an IR system. Clustering and phrasal analysis have together been exploited in <ref> [Anick and Vaithyanathan, 1997] </ref>. There have been approaches to improve precision that exploit the structure of documents (based on mark-up languages such as SGML and XML 8 ), instead of the contents of documents.
Reference: [Belkin and Croft, 1992] <author> Belkin, N. and Croft, W. </author> <year> (1992). </year> <title> Information filtering and information retrieval: two sides of the same coin? Communications of the ACM, </title> <type> 35(12). </type>
Reference-contexts: We conclude with a summary in Section 8. But first, we define two terms: The term information filtering has several different interpretations. Many researchers use this term to denote the selective dissemination of information, matched to user's profiles of interest (see for instance, <ref> [Belkin and Croft, 1992] </ref>).
Reference: [Brill, 1994] <author> Brill, E. </author> <year> (1994). </year> <title> Some advances in transformation-based part of speech tagging. </title> <booktitle> In Proceedings of AAAI94. </booktitle>
Reference-contexts: POS taggers use information from a limited context in deciding which tag (s) to choose for each word in a text. As is well known, these taggers (for example, <ref> [Church, 1988, Brill, 1994] </ref>) are quite accurate and of significant utility. The tagger that we use is an N-gram tagger similar to [Church, 1988], and uses the tagset (40 tags) from the Penn TreeBank [Marcus et al., 1993].
Reference: [Byrd et al., 1995] <author> Byrd, R., Ravin, Y., and Prager, J. </author> <year> (1995). </year> <title> Lexical assistance at the information-retrieval user interface. </title> <booktitle> In Fourth Annual Symposium on Document Analysis and Information Retrieval. </booktitle>
Reference-contexts: Latent Semantic Indexing (LSI) [Deerwester et al., 1990] which clusters related documents is another interesting approach to improving IR performance. Query expansion based on linguistic tools such as WordNet [Miller et al., 1990] categories have been suggested [Voorhees, 1993, Voorhees, 1994]. There have been approaches <ref> [Byrd et al., 1995] </ref> that provide assistance in constructing better queries based on domain knowledge. [Grefenstette, 1997] suggests a method using NL tools to attack the problem of short (one-word) queries.
Reference: [Chandrasekar and Srinivas, 1997a] <author> Chandrasekar, R. and Srinivas, B. </author> <year> (1997a). </year> <title> Gleaning information from the Web: Using syntax to filter out irrelevant information. </title> <booktitle> In Proceedings of AAAI 1997 Spring Symposium on NLP on the World Wide Web. </booktitle>
Reference-contexts: This section describes three such experiments, which pose the questions (in the context of information filtering): * Is syntactic information useful? * What granularity of syntactic information is useful? * How much syntactic context is useful? 5.1 The Utility of Syntactic Information The objective in this experiment <ref> [Chandrasekar and Srinivas, 1997a] </ref> is to show that syntactic information improves information filtering performance and to quantitatively measure this performance improvement. 5.1.1 Training: Creating Augmented Patterns The training corpus constituted of a corpus of approximately 52 MB of New York Times (NYT) text data comprising of the August 1995 output.
Reference: [Chandrasekar and Srinivas, 1997b] <author> Chandrasekar, R. and Srinivas, B. </author> <year> (1997b). </year> <title> Using supertags in document filtering: The effect of increased context on information retrieval effectiveness. </title> <booktitle> In Proceedings of Recent Advances in NLP (RANLP) '97, </booktitle> <address> Tzigov Chark. </address>
Reference-contexts: The effect of increasing the window size (for example, to include two chunks on the left) is discussed in the next section. 5.3 Varying Context in Syntactic Patterns In this experiment <ref> [Chandrasekar and Srinivas, 1997b] </ref>, we quantitatively measure the performance difference arising from variations in the size of the context around words of interest. Again, we 12 Submitted to Info.
Reference: [Chandrasekar and Srinivas, 1997c] <author> Chandrasekar, R. and Srinivas, B. </author> <year> (1997c). </year> <title> Using syntactic information in document filtering: A comparative study of part-of-speech tagging and supertag-ging. </title> <booktitle> In Proceedings of RIAO '97, </booktitle> <pages> pages 531-545, </pages> <address> Montreal. </address>
Reference-contexts: This may syntactically correspond to a standard sentence about an appointment event, and so it is not easy to filter such sentences out, without additional information. 5.2 Comparing POS Tagging with Supertagging In the second set of experiments <ref> [Chandrasekar and Srinivas, 1997c] </ref>, we show that richer syntactic information improves performance of the filtering system. We describe experiments where we use techniques discussed in the previous sections to retrieve relevant documents about appointments.
Reference: [Chandrasekar et al., 1997] <author> Chandrasekar, R., Srinivas, B., and Sarkar, A. </author> <year> (1997). </year> <title> Sieving the Web: Improving search precision using information filtering. </title> <note> Submitted for publication. </note>
Reference-contexts: Proc. & Mgmnt. There are several other methods by which retrieval efficiency could be enhanced. For example, we have proposed server-side filtering as an innovative method for improving the efficiency of retrieval on the Web <ref> [Chandrasekar et al., 1997] </ref>. The idea here is to move the filtering code to the site of potentially relevant documents, which would be useful if the filtering code is substantially smaller then the document (s) to be processed at any site.
Reference: [Church, 1988] <author> Church, K. W. </author> <year> (1988). </year> <title> A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. </title> <booktitle> In 2nd Applied Natural Language Processing Conference, </booktitle> <address> Austin, Texas. </address>
Reference-contexts: POS taggers use information from a limited context in deciding which tag (s) to choose for each word in a text. As is well known, these taggers (for example, <ref> [Church, 1988, Brill, 1994] </ref>) are quite accurate and of significant utility. The tagger that we use is an N-gram tagger similar to [Church, 1988], and uses the tagset (40 tags) from the Penn TreeBank [Marcus et al., 1993]. <p> As is well known, these taggers (for example, [Church, 1988, Brill, 1994]) are quite accurate and of significant utility. The tagger that we use is an N-gram tagger similar to <ref> [Church, 1988] </ref>, and uses the tagset (40 tags) from the Penn TreeBank [Marcus et al., 1993]. This tagset distinguishes some morphological information, such as singular and plural nouns (NN and NNS, NNP and NNPS), and different verbal categories (past, participle, continuous: VBD, VBN, VBG respectively) etc.
Reference: [Croft et al., 1991] <author> Croft, B. W., Turtle, H. R., and Lewis, D. D. </author> <year> (1991). </year> <title> The use of phrases and structured queries in information retrieval. </title> <booktitle> In Proceedings of the 14 th Annual International Conference on Research and Development in Information Retrieval (SIGIR '91), </booktitle> <pages> pages 32-45, </pages> <address> Chicago, USA. </address> <note> 18 Submitted to Info. Proc. & Mgmnt. </note>
Reference: [Deerwester et al., 1990] <author> Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. </author> <year> (1990). </year> <title> Indexing by Latent Semantic Analysis. </title> <journal> Journal of the American Socety for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407. </pages>
Reference-contexts: However, improving the precision of an IR system has attracted many novel and varied techniques. Relevance feedback [Rocchio, 1971] is a popular approach which allows the user, through successive interactions, to zero in on what she desires. Latent Semantic Indexing (LSI) <ref> [Deerwester et al., 1990] </ref> which clusters related documents is another interesting approach to improving IR performance. Query expansion based on linguistic tools such as WordNet [Miller et al., 1990] categories have been suggested [Voorhees, 1993, Voorhees, 1994].
Reference: [Doran et al., 1994] <author> Doran, C., Egedi, D., Hockey, B. A., Srinivas, B., and Zaidel, M. </author> <year> (1994). </year> <title> XTAG System A Wide Coverage Grammar for English. </title> <booktitle> In Proceedings of the 17 th International Conference on Computational Linguistics (COLING '94), </booktitle> <address> Kyoto, Japan. </address>
Reference-contexts: and ro bust: a system trained on 1,000,000 words of Wall Street Journal text and tested on 47,000 words 4 A wide-coverage English grammar has been implemented in the LTAG framework and this grammar has been used to parse sentences from the Wall Street Journal, IBM manual and ATIS domains <ref> [Doran et al., 1994] </ref>. 5 Submitted to Info. Proc. & Mgmnt. correctly supertagged 92.2% of these words. For a detailed discussion of the supertagger, refer to [Srinivas, 1997]. by the Governor and the sentence She was appointed by the Governor in 1998.
Reference: [Fagan, 1987] <author> Fagan, J. L. </author> <year> (1987). </year> <title> Experiments in Automatic Phrase Indexing for Document Retrieval: A Comparison of Syntactic and Nonsyntactic Methods. </title> <type> PhD thesis, </type> <institution> Cornell University. TR87-868. </institution>
Reference: [Frakes and Baeza-Yates, 1992] <author> Frakes, W. B. and Baeza-Yates, R. S. </author> <year> (1992). </year> <title> Information Retrieval: Data Structures and Algorithms. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: At the outset, we must note that Glean is a filtering approach atop a standard retrieval engine. A standard retrieval engine might use several techniques such as query expansion, thesauri, morphological analysis and stemming techniques <ref> [Salton, 1989, Salton and McGill, 1983, Frakes and Baeza-Yates, 1992] </ref> to improve its recall. However, improving the precision of an IR system has attracted many novel and varied techniques.
Reference: [Grefenstette, 1997] <author> Grefenstette, G. </author> <year> (1997). </year> <title> SQLET: Short Query Linguistic Expansion Techniques, palliating one-word queries by providing intermediate structure to text. </title> <booktitle> In Proceedings of RIAO '97, </booktitle> <pages> pages 500-509, </pages> <address> Montreal. </address>
Reference-contexts: Query expansion based on linguistic tools such as WordNet [Miller et al., 1990] categories have been suggested [Voorhees, 1993, Voorhees, 1994]. There have been approaches [Byrd et al., 1995] that provide assistance in constructing better queries based on domain knowledge. <ref> [Grefenstette, 1997] </ref> suggests a method using NL tools to attack the problem of short (one-word) queries. Other approaches [Korfhage, 1991, Hearst and Pedersen, 1996, Hearst and Karadi, 1997] have provided extensive visualization tools to view and prune the results returned by an IR system.
Reference: [Hearst and Karadi, 1997] <author> Hearst, M. A. and Karadi, C. </author> <year> (1997). </year> <title> Cat-a-cone: An interactive interface for specifying searches and viewing retrieval results using a large category hierarchy. </title> <booktitle> In Proc. ACM SIGIR-97, </booktitle> <pages> pages 246-255. </pages>
Reference-contexts: There have been approaches [Byrd et al., 1995] that provide assistance in constructing better queries based on domain knowledge. [Grefenstette, 1997] suggests a method using NL tools to attack the problem of short (one-word) queries. Other approaches <ref> [Korfhage, 1991, Hearst and Pedersen, 1996, Hearst and Karadi, 1997] </ref> have provided extensive visualization tools to view and prune the results returned by an IR system. Clustering and phrasal analysis have together been exploited in [Anick and Vaithyanathan, 1997].
Reference: [Hearst and Pedersen, 1996] <author> Hearst, M. A. and Pedersen, J. O. </author> <year> (1996). </year> <title> Reexamining the cluster hypothesis: </title> <booktitle> Scatter/gather on retrieval results. In Proc. ACM SIGIR-96. </booktitle>
Reference-contexts: There have been approaches [Byrd et al., 1995] that provide assistance in constructing better queries based on domain knowledge. [Grefenstette, 1997] suggests a method using NL tools to attack the problem of short (one-word) queries. Other approaches <ref> [Korfhage, 1991, Hearst and Pedersen, 1996, Hearst and Karadi, 1997] </ref> have provided extensive visualization tools to view and prune the results returned by an IR system. Clustering and phrasal analysis have together been exploited in [Anick and Vaithyanathan, 1997].
Reference: [Joshi and Srinivas, 1994] <author> Joshi, A. K. and Srinivas, B. </author> <year> (1994). </year> <title> Disambiguation of Super Parts of Speech (or Supertags): Almost Parsing. </title> <booktitle> In Proceedings of the 17 th International Conference on Computational Linguistics (COLING '94), </booktitle> <address> Kyoto, Japan. </address>
Reference-contexts: Note that the two senses of appointed are labeled with different supertags. 4.2 LTAG and Supertagging The other approach to syntactic annotation is based on Lexicalized Tree Adjoining Grammar (LTAG) [Schabes et al., 1988] 4 and uses the "supertagging" technique <ref> [Joshi and Srinivas, 1994, Srinivas, 1997] </ref> described in this section. Supertags, the primary elements of the LTAG formalism, localize dependencies, including long distance dependencies, by requiring that all and only the dependent elements be present within the same structure. <p> Local statistical information, in the form of a trigram model based on the distribution of su-pertags in an LTAG parsed corpus, can be used to choose the most appropriate supertag for any given word. The process of assigning the best supertag to each word is termed supertag-ging <ref> [Joshi and Srinivas, 1994] </ref>.
Reference: [Katz, 1997] <author> Katz, B. </author> <year> (1997). </year> <title> Annotating the world wide web using natural language. </title> <booktitle> In Proceedings of RIAO '97, </booktitle> <pages> pages 136-155, </pages> <address> Montreal. </address>
Reference: [Korfhage, 1991] <author> Korfhage, R. R. </author> <year> (1991). </year> <title> To see, or not to see is that the query? In Proc. </title> <booktitle> ACM SIGIR-91, </booktitle> <pages> pages 134-141. </pages>
Reference-contexts: There have been approaches [Byrd et al., 1995] that provide assistance in constructing better queries based on domain knowledge. [Grefenstette, 1997] suggests a method using NL tools to attack the problem of short (one-word) queries. Other approaches <ref> [Korfhage, 1991, Hearst and Pedersen, 1996, Hearst and Karadi, 1997] </ref> have provided extensive visualization tools to view and prune the results returned by an IR system. Clustering and phrasal analysis have together been exploited in [Anick and Vaithyanathan, 1997].
Reference: [Marcus et al., 1993] <author> Marcus, M. M., Santorini, B., and Marcinkiewicz, M. A. </author> <year> (1993). </year> <title> Building a Large Annotated Corpus of English: The Penn Treebank. </title> <note> Computational Linguistics, 19.2:313-330. </note>
Reference-contexts: As is well known, these taggers (for example, [Church, 1988, Brill, 1994]) are quite accurate and of significant utility. The tagger that we use is an N-gram tagger similar to [Church, 1988], and uses the tagset (40 tags) from the Penn TreeBank <ref> [Marcus et al., 1993] </ref>. This tagset distinguishes some morphological information, such as singular and plural nouns (NN and NNS, NNP and NNPS), and different verbal categories (past, participle, continuous: VBD, VBN, VBG respectively) etc.
Reference: [Martin et al., 1983] <author> Martin, W., Al, B., and van Sterkenburg, P. </author> <year> (1983). </year> <title> On the processing of a text corpus: from textual data to lexicographical information. </title> <editor> In Hartmann, R., editor, </editor> <booktitle> Lexicography: Principles and Practices, Applied Language Studies Series. </booktitle> <publisher> Academic Press, London. </publisher>
Reference-contexts: In both the selection and filtering tasks, for both the words studied, we find a left context of 2 chunks and a right context of 1 chunk performs the best. Interestingly, similar findings have been reported in the domain of lexicography, in the work of <ref> [Martin et al., 1983] </ref>. the F-scores both for selecting relevant documents, and for filtering irrelevant documents. This optimal value of window size linguistically corresponds to the intuition that the left context contains the information such as relative pronouns and complementizers required to discriminate between embedded and main clauses.
Reference: [Miller et al., 1990] <author> Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D., and Miller, K. </author> <year> (1990). </year> <title> Introduction to WordNet: An on-line lexical database. </title> <journal> International Journal of Lexicography, </journal> <volume> 3(4) </volume> <pages> 235-312. </pages>
Reference-contexts: Latent Semantic Indexing (LSI) [Deerwester et al., 1990] which clusters related documents is another interesting approach to improving IR performance. Query expansion based on linguistic tools such as WordNet <ref> [Miller et al., 1990] </ref> categories have been suggested [Voorhees, 1993, Voorhees, 1994]. There have been approaches [Byrd et al., 1995] that provide assistance in constructing better queries based on domain knowledge. [Grefenstette, 1997] suggests a method using NL tools to attack the problem of short (one-word) queries.
Reference: [MUC, 1996] <author> MUC (1996). </author> <title> Message Understanding Evaluation and Conference: </title> <booktitle> Proceedings of the 6th ARPA Workshop. </booktitle> <address> Morgan Kaufmann. </address> <note> 19 Submitted to Info. Proc. & Mgmnt. </note>
Reference-contexts: This approach is interesting, but time-consuming and may not be scalable. Besides, human annotations are very subjective, and perhaps of limited utility for very large corpus sizes. 7.1 Comparison with MUC and TREC In this section, we contrast our work against tasks in the Message Understanding Conferences (MUC) <ref> [MUC, 1996] </ref> and the Text Retrieval Conferences (TREC) [Voorhees and Harman, 1998]. MUC tasks are typically set up to extract information in the form of a predefined template from an a priori specified set of documents and topics.
Reference: [Ramani and Chandrasekar, 1993] <author> Ramani, S. and Chandrasekar, R. </author> <year> (1993). </year> <title> Glean: a tool for automated information acquisition and maintenance. </title> <type> Technical report, </type> <institution> National Centre for Software Technology, Bombay. </institution>
Reference-contexts: Thus Glean's task, given regions A2 and B2 (as output from a search engine), is to select regions A22 and B22, or identify regions A21 and B21 that have to be filtered out. Such a tool for information filtering, named Glean <ref> [Ramani and Chandrasekar, 1993] </ref>, has been developed.
Reference: [Robison, 1970] <author> Robison, H. R. </author> <year> (1970). </year> <title> Computer-detectable semantic structures. </title> <booktitle> Information Storage and Retrieval, </booktitle> <volume> 6 </volume> <pages> 273-288. </pages> <publisher> Pergamon Press. </publisher>
Reference-contexts: However, these approaches require that such higher level information be used for indexing text; they cannot be used with existing collections without reindexing text, a daunting task when terabytes of data are considered. Robison <ref> [Robison, 1970] </ref> discusses the idea of making semantic distinctions between words by using the fact that some common syntactic units (such as propositions) occur adjacent to the words whose meaning they specify (or `govern').
Reference: [Rocchio, 1971] <author> Rocchio, J. </author> <year> (1971). </year> <title> Relevance Feedback in Information Retrieval. </title> <editor> In Salton, G., editor, </editor> <title> The Smart System Experiments in Automatic Document Processing . Prentice-Hall Inc., </title> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: A standard retrieval engine might use several techniques such as query expansion, thesauri, morphological analysis and stemming techniques [Salton, 1989, Salton and McGill, 1983, Frakes and Baeza-Yates, 1992] to improve its recall. However, improving the precision of an IR system has attracted many novel and varied techniques. Relevance feedback <ref> [Rocchio, 1971] </ref> is a popular approach which allows the user, through successive interactions, to zero in on what she desires. Latent Semantic Indexing (LSI) [Deerwester et al., 1990] which clusters related documents is another interesting approach to improving IR performance.
Reference: [Salton, 1989] <author> Salton, G. </author> <year> (1989). </year> <title> Automatic Text Processing: the transformation, analysis, and retrieval of information by computer. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: At the outset, we must note that Glean is a filtering approach atop a standard retrieval engine. A standard retrieval engine might use several techniques such as query expansion, thesauri, morphological analysis and stemming techniques <ref> [Salton, 1989, Salton and McGill, 1983, Frakes and Baeza-Yates, 1992] </ref> to improve its recall. However, improving the precision of an IR system has attracted many novel and varied techniques.
Reference: [Salton and McGill, 1983] <author> Salton, G. and McGill, M. J. </author> <year> (1983). </year> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: At the outset, we must note that Glean is a filtering approach atop a standard retrieval engine. A standard retrieval engine might use several techniques such as query expansion, thesauri, morphological analysis and stemming techniques <ref> [Salton, 1989, Salton and McGill, 1983, Frakes and Baeza-Yates, 1992] </ref> to improve its recall. However, improving the precision of an IR system has attracted many novel and varied techniques.
Reference: [Schabes et al., 1988] <author> Schabes, Y., Abeille, A., and Joshi, A. K. </author> <year> (1988). </year> <title> Parsing strategies with `lexicalized' grammars: Application to Tree Adjoining Grammars. </title> <booktitle> In Proceedings of the 12 th International Conference on Computational Linguistics (COLING'88), </booktitle> <address> Budapest, Hungary. </address>
Reference-contexts: Note that the two senses of appointed are labeled with different supertags. 4.2 LTAG and Supertagging The other approach to syntactic annotation is based on Lexicalized Tree Adjoining Grammar (LTAG) <ref> [Schabes et al., 1988] </ref> 4 and uses the "supertagging" technique [Joshi and Srinivas, 1994, Srinivas, 1997] described in this section. Supertags, the primary elements of the LTAG formalism, localize dependencies, including long distance dependencies, by requiring that all and only the dependent elements be present within the same structure.
Reference: [Srinivas, 1997] <author> Srinivas, B. </author> <year> (1997). </year> <title> Complexity of Lexical Descriptions and its Relevance to Partial Parsing. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <address> Philadelphia, PA. </address>
Reference-contexts: Note that the two senses of appointed are labeled with different supertags. 4.2 LTAG and Supertagging The other approach to syntactic annotation is based on Lexicalized Tree Adjoining Grammar (LTAG) [Schabes et al., 1988] 4 and uses the "supertagging" technique <ref> [Joshi and Srinivas, 1994, Srinivas, 1997] </ref> described in this section. Supertags, the primary elements of the LTAG formalism, localize dependencies, including long distance dependencies, by requiring that all and only the dependent elements be present within the same structure. <p> Proc. & Mgmnt. correctly supertagged 92.2% of these words. For a detailed discussion of the supertagger, refer to <ref> [Srinivas, 1997] </ref>. by the Governor and the sentence She was appointed by the Governor in 1998. Note that the supertagger distinguishes between the two uses of the word appointed, while the POS tagger does not. There are about 300 supertags used by the supertagger.
Reference: [Strzalkowski, 1994] <author> Strzalkowski, T. </author> <year> (1994). </year> <title> Document indexing and retrieval using linguistic knowledge. </title> <booktitle> In Proceedings of RIAO '94, </booktitle> <pages> pages 131-145, </pages> <address> New York. </address>
Reference-contexts: There have been a different class of approaches that emphasized the use of information such as collocations or phrases ([Fagan, 1987],[Croft et al., 1991]) or grammatical relations derived from a parser <ref> [Strzalkowski, 1994, Strzalkowski et al., 1997] </ref>, for improving the precision of IR systems. However, these approaches require that such higher level information be used for indexing text; they cannot be used with existing collections without reindexing text, a daunting task when terabytes of data are considered.
Reference: [Strzalkowski et al., 1997] <author> Strzalkowski, T., Lin, F., Perez-Carballo, J., and Wang, J. </author> <year> (1997). </year> <title> Building effective queries in natural language information retrieval. </title> <booktitle> In Proc. 5th Conference on Applied Natural Language Processing, </booktitle> <pages> pages 299-306, </pages> <address> Washington. </address> <publisher> ACL. </publisher>
Reference-contexts: There have been a different class of approaches that emphasized the use of information such as collocations or phrases ([Fagan, 1987],[Croft et al., 1991]) or grammatical relations derived from a parser <ref> [Strzalkowski, 1994, Strzalkowski et al., 1997] </ref>, for improving the precision of IR systems. However, these approaches require that such higher level information be used for indexing text; they cannot be used with existing collections without reindexing text, a daunting task when terabytes of data are considered.
Reference: [Voorhees and Harman, 1998] <author> Voorhees, E. and Harman, D., </author> <title> editors (1998). </title> <booktitle> The Sixth Text REtrieval Conference. </booktitle> <institution> Department of Commerce, National Institute of Standards and Technology. </institution> <note> Proc. 6th TREC, </note> <institution> Gaithersburg, MD, </institution> <month> Nov. </month> <pages> 19-21, </pages> <year> 1997. </year>
Reference-contexts: Besides, human annotations are very subjective, and perhaps of limited utility for very large corpus sizes. 7.1 Comparison with MUC and TREC In this section, we contrast our work against tasks in the Message Understanding Conferences (MUC) [MUC, 1996] and the Text Retrieval Conferences (TREC) <ref> [Voorhees and Harman, 1998] </ref>. MUC tasks are typically set up to extract information in the form of a predefined template from an a priori specified set of documents and topics.
Reference: [Voorhees, 1993] <author> Voorhees, E. M. </author> <year> (1993). </year> <title> Using WordNet to disambiguate word senses for text retrieval. </title> <booktitle> In Proc. ACM SIGIR-93, </booktitle> <pages> pages 171-180. </pages>
Reference-contexts: Latent Semantic Indexing (LSI) [Deerwester et al., 1990] which clusters related documents is another interesting approach to improving IR performance. Query expansion based on linguistic tools such as WordNet [Miller et al., 1990] categories have been suggested <ref> [Voorhees, 1993, Voorhees, 1994] </ref>. There have been approaches [Byrd et al., 1995] that provide assistance in constructing better queries based on domain knowledge. [Grefenstette, 1997] suggests a method using NL tools to attack the problem of short (one-word) queries.
Reference: [Voorhees, 1994] <author> Voorhees, E. M. </author> <year> (1994). </year> <title> Query expansion using lexical-semantic relations. </title> <booktitle> In Proc. ACM SIGIR-94. </booktitle> <pages> 20 </pages>
Reference-contexts: Latent Semantic Indexing (LSI) [Deerwester et al., 1990] which clusters related documents is another interesting approach to improving IR performance. Query expansion based on linguistic tools such as WordNet [Miller et al., 1990] categories have been suggested <ref> [Voorhees, 1993, Voorhees, 1994] </ref>. There have been approaches [Byrd et al., 1995] that provide assistance in constructing better queries based on domain knowledge. [Grefenstette, 1997] suggests a method using NL tools to attack the problem of short (one-word) queries.
References-found: 36

