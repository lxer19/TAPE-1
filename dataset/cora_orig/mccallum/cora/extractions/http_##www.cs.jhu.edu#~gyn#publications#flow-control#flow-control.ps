URL: http://www.cs.jhu.edu/~gyn/publications/flow-control/flow-control.ps
Refering-URL: http://www.cs.jhu.edu/~gyn/
Root-URL: http://www.cs.jhu.edu
Email: fkonstant, gyng@cs.jhu.edu  
Title: Flow control considerations in network-based architectures Ph.D. Project report by G.Ngai  
Author: S.Konstantinidou and Grace Ngai) 
Keyword: parallel architecture, communication, workstation clusters.  
Address: Baltimore, MD 21218  
Affiliation: Computer Science Department Johns Hopkins University  
Abstract: In network-based parallel architectures, the issues of fairness, freedom of deadlock due to finite buffers and guaranteed message delivery can affect not only performance but even more importantly, the guarantees that the architecture provides to the users so that they can write correct, race-free portable programs. When blocking communication is assumed, the solutions to these problems can be relatively simple and efficient. When nonblocking communications must be supported for performance, these issues complicate the designs of architectures irrespective of the programming paradigm that the architecture supports and the characteristics of the network. In this paper, we consider blocking and nonblocking communication and the effect of flow control in the latter case, on both performance and correctness. Our experimental results are derived on a cluster of dual processor SMPs connected via FDDI fiber ring. The cluster is built on off-the-shelf hardware components, although communication protocols customized for parallelism, and in particular the remote read/write programming paradigm, have been developed and are executed by one of the processors while the other is dedicated to computation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill, </author> <title> "Weak Ordering ANew Definition", </title> <booktitle> in Proc. 17th International Symposium on Computer Architecture, </booktitle> <pages> pp. 2-14, </pages> <year> 1990. </year>
Reference-contexts: Such solutions are known to improve performance but they complicate the guarantees the architecture provides to the programmers as to when writes to shared data take effect and are visible by every processor <ref> [1] </ref>. Even in this more complicated design, fairness and completion of the requests is guaranteed by the architecture. Furthermore, the sequentiality in the accesses of the bus guarantees that the rate at which processors produce memory requests cannot exceed the rate at which the memory can satisfy them.
Reference: [2] <author> M.A. Blumrich, K. Li, R. Alpert, C. Dubnickin, E.W. Felten, and J. Sandberg. </author> <title> "Virtual memory mapped network interface for the shrimp multicomputer." </title> <booktitle> In Proc. 21st International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: Such novel models include the Cray T3D PUT/GET commands [6] for remote reads and writes, the AP1000 PUT/GET commands [22, ?], the Split-C PUT/GET definitions [8], and the virtual-memory mapped communication of the Shrimp multicomputer <ref> [2] </ref>. In all these examples the user controls the mapping of the data to the processors. Communication is provided with remote read and/or write commands that allow a process to read or write a value in another processor's memory. Thus communication is explicit and one-sided.
Reference: [3] <author> B. </author> <title> Catanzaro, "Multiprocessor System Architectures", Sun Microsystems, </title> <publisher> Inc., </publisher> <address> Mountain View, CA, </address> <year> 1994. </year>
Reference-contexts: Because this solution would keep processors idle, existing bus-based multiprocessor architectures allow for the buffering of memory requests <ref> [3, 23] </ref>. Such solutions are known to improve performance but they complicate the guarantees the architecture provides to the programmers as to when writes to shared data take effect and are visible by every processor [1].
Reference: [4] <author> D. Chaiken, C. Fields, K. Kurihara and A. Agarwal, </author> <title> "Directory-Based Cache Coherence in Large-Scale Multiprocessors." </title> <booktitle> In IEEE Computer, </booktitle> <pages> pages 49-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Note that the system controls the size and location of each message that is sent (which is typically a cache line). In shared memory architectures, the communication coprocessor must perform complex tasks such as maintaining directories of caches, replicated data and implementing the necessary consistency protocols <ref> [4, 16] </ref>. Because the communication in shared memory architectures can be multiphase, that is, in response to one message received many messages may have to be cerated, flow control and in particular buffer management issues are extremely important for correctness.
Reference: [5] <author> A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> "Software vs. hardware shared memory implementations: A case study." </title> <booktitle> In Proc. International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: This includes multistage networks (IBM SP and TMC CM5) [24, 18], two-dimensional grads (Intel Paragon) [21], three-dimensional tori (Cray T3D) [6], a hierarchy of fiber rings (Convex Exemplar) [11] and networks constructed of ATM switching fabric <ref> [20, 5] </ref>. As varied as the characteristics of these interconnects maybe, they have certain similarities. Specifically, the interconnect is not a broadcast medium, and thus broadcasts are expensive operations that cannot be used often, and centralized control of resources is extremely inefficient.
Reference: [6] <author> Cray Research Inc., </author> <title> "The CRAY T3D Massively Parallel Processing System", </title> <note> available on the www at URL http://www.cray.com/PUBLIC/product-info/mpp. </note>
Reference-contexts: Existing network-based architectures have made a wide variety of choices with respect to their interconnect medium. This includes multistage networks (IBM SP and TMC CM5) [24, 18], two-dimensional grads (Intel Paragon) [21], three-dimensional tori (Cray T3D) <ref> [6] </ref>, a hierarchy of fiber rings (Convex Exemplar) [11] and networks constructed of ATM switching fabric [20, 5]. As varied as the characteristics of these interconnects maybe, they have certain similarities. <p> In particular, the fact that the system determines the size and location of each message can result in unnecessary communication (due to false sharing) and inefficient communication (due to small message sizes). Such novel models include the Cray T3D PUT/GET commands <ref> [6] </ref> for remote reads and writes, the AP1000 PUT/GET commands [22, ?], the Split-C PUT/GET definitions [8], and the virtual-memory mapped communication of the Shrimp multicomputer [2]. In all these examples the user controls the mapping of the data to the processors.
Reference: [7] <institution> Cray Research Incorporated. </institution> <note> "SHMEM Technical Note for C." Revision 2.3. </note>
Reference: [8] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krisnamurthy, S. Lumetta, T. von Eicken, and K. Yelick, </author> <title> "Parallel Programming in Split-C", </title> <booktitle> in Proceedings of Supercomputing 93, </booktitle> <pages> pp. 262-273, </pages> <year> 1993. </year>
Reference-contexts: Such novel models include the Cray T3D PUT/GET commands [6] for remote reads and writes, the AP1000 PUT/GET commands [22, ?], the Split-C PUT/GET definitions <ref> [8] </ref>, and the virtual-memory mapped communication of the Shrimp multicomputer [2]. In all these examples the user controls the mapping of the data to the processors. Communication is provided with remote read and/or write commands that allow a process to read or write a value in another processor's memory.
Reference: [9] <author> R. Cypher and S. Konstantinidou, </author> <title> "Bounds on the efficiency of message-passing protocols for parallel computers", </title> <booktitle> in Proc. Fifth ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 173-181, </pages> <year> 1993. </year>
Reference-contexts: Flow control is necessary so that the unanticipated messages do not overflow the system buffers <ref> [9] </ref>. Furthermore, special protocols may have to be implemented to handle long messages and support for complex communication patterns (such as collective communication) may be part of the communication coprocessor as well.
Reference: [10] <author> M. Dubois, C. Scheurich and F. A. Briggs, </author> <title> "Synchronization, Coherence and Event Ordering in Multiprocessors", </title> <booktitle> in IEEE Computer, </booktitle> <pages> 21-2, pp. 9-21, </pages> <year> 1988. </year> <month> 13 </month>
Reference-contexts: In order to achieve scalability, parallel architectures must use a point-to-point network as the interconnect medium. The transition from bus-based to network-based parallel architectures requires that efficient solutions are found for difficult problems, such as data consistency, mutual exclusion and event ordering <ref> [10] </ref>. An instance of a problem for which the solutions are simple in bus-based architectures is the problem of flow control.
Reference: [11] <institution> The Exemplar Systems: SPP1000 Systems Overview. Convex Corporation, </institution> <year> 1994. </year>
Reference-contexts: Existing network-based architectures have made a wide variety of choices with respect to their interconnect medium. This includes multistage networks (IBM SP and TMC CM5) [24, 18], two-dimensional grads (Intel Paragon) [21], three-dimensional tori (Cray T3D) [6], a hierarchy of fiber rings (Convex Exemplar) <ref> [11] </ref> and networks constructed of ATM switching fabric [20, 5]. As varied as the characteristics of these interconnects maybe, they have certain similarities. Specifically, the interconnect is not a broadcast medium, and thus broadcasts are expensive operations that cannot be used often, and centralized control of resources is extremely inefficient.
Reference: [12] <author> K. Hayashi, T. Doi, T. Horie, Y. Koyonagi, O. Shiraki, N. Imamura, T. Shimizu, H. Ishihata and T. Shindo, </author> <title> "AP1000+: Architectural Support of PUT/GET Interface for Parallelizing Compiler", </title> <booktitle> in em Proc. Sixth Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 196-207, </pages> <year> 1994. </year>
Reference: [13] <author> J. Heinlein, K. Gharachorloo, S. Dresser, and A. Gupta, </author> <title> "Integration of message passing and shared memory in the Stanford FLASH multiprocessor", </title> <booktitle> in Proc. Sixth Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 38-50, </pages> <year> 1994. </year>
Reference-contexts: It is generally agreed that both of these programming paradigms have certain limitations <ref> [14, 15, 17, 13] </ref>. The message passing model requires substantial knowledge and effort by the programmer in order to optimize the data placement and communication.
Reference: [14] <author> A.C. Klaiber and H.M. Levy. </author> <title> "A comparison of message passing and shared memory architectures for data parallel programs.", </title> <booktitle> in Proc. 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 94-105, </pages> <year> 1994. </year>
Reference-contexts: It is generally agreed that both of these programming paradigms have certain limitations <ref> [14, 15, 17, 13] </ref>. The message passing model requires substantial knowledge and effort by the programmer in order to optimize the data placement and communication.
Reference: [15] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz and B.H. Lim. </author> <title> "Integrating Message-Passing and Shared-Memory: Early Experience." </title> <booktitle> in Proc. 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <year> 1994. </year>
Reference-contexts: It is generally agreed that both of these programming paradigms have certain limitations <ref> [14, 15, 17, 13] </ref>. The message passing model requires substantial knowledge and effort by the programmer in order to optimize the data placement and communication.
Reference: [16] <author> J. Kuskin, D. Ofelt, M. Heirich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum and J. Hennessy. </author> <title> "The Stan-ford FLASH Multiprocessor." </title> <booktitle> In Proc. 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <year> 1994. </year>
Reference-contexts: Note that the system controls the size and location of each message that is sent (which is typically a cache line). In shared memory architectures, the communication coprocessor must perform complex tasks such as maintaining directories of caches, replicated data and implementing the necessary consistency protocols <ref> [4, 16] </ref>. Because the communication in shared memory architectures can be multiphase, that is, in response to one message received many messages may have to be cerated, flow control and in particular buffer management issues are extremely important for correctness.
Reference: [17] <author> J.R. Larus S. Chandra and A. Rogers. </author> <title> "Where is time spent in message-passing and shared-memory programs?" In Proc. </title> <booktitle> of the 6th ASPLOS Conference, </booktitle> <year> 1994. </year>
Reference-contexts: It is generally agreed that both of these programming paradigms have certain limitations <ref> [14, 15, 17, 13] </ref>. The message passing model requires substantial knowledge and effort by the programmer in order to optimize the data placement and communication.
Reference: [18] <author> C. Leiserson, Z. Abuhamdeh, D. Douglas, C. Feynman, M. Ganmukhi, J. Hill, D. Hillis, B. Kuszmaul, M. St. Pierre, D. Wells, M. Wong, S.W. Yang, and R. Zak, </author> <title> "The Network Architecture of the Connection Machine CM-5", </title> <booktitle> in Proc. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 272-285, </pages> <year> 1992 </year>
Reference-contexts: When the same problems are considered in network-based architectures, the solutions are inherently much more complicated. Existing network-based architectures have made a wide variety of choices with respect to their interconnect medium. This includes multistage networks (IBM SP and TMC CM5) <ref> [24, 18] </ref>, two-dimensional grads (Intel Paragon) [21], three-dimensional tori (Cray T3D) [6], a hierarchy of fiber rings (Convex Exemplar) [11] and networks constructed of ATM switching fabric [20, 5]. As varied as the characteristics of these interconnects maybe, they have certain similarities.
Reference: [19] <author> P. Merlin and P. Schweitzer, </author> <title> "Deadlock-avoidance in store-and-forward networks. 2: Other deadlock types", </title> <journal> IEEE Trans. on Communications, </journal> <volume> vol. 28, no. 3, </volume> <pages> pp. 355-360, </pages> <year> 1980. </year>
Reference-contexts: They are nevertheless extremely important as the guarantees that the architecture provides are essential information for the development of portable, race-free parallel applications irrespective of the chosen programming paradigm. These issues have been considered extensively in the area of communication and computer networks <ref> [19] </ref> as well as bus-based architectures but their importance and effect on the design of network-based parallel architectures is relatively unexplored. Such architectures must rely on end-to-end flow control schemes both for correctness and performance.
Reference: [20] <author> D. Patterson. </author> <title> "A case for NOW." </title> <booktitle> In Proc. Hot Interconnect Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: This includes multistage networks (IBM SP and TMC CM5) [24, 18], two-dimensional grads (Intel Paragon) [21], three-dimensional tori (Cray T3D) [6], a hierarchy of fiber rings (Convex Exemplar) [11] and networks constructed of ATM switching fabric <ref> [20, 5] </ref>. As varied as the characteristics of these interconnects maybe, they have certain similarities. Specifically, the interconnect is not a broadcast medium, and thus broadcasts are expensive operations that cannot be used often, and centralized control of resources is extremely inefficient.
Reference: [21] <author> G. Regnier. </author> <title> "Delta message passing protocol." </title> <booktitle> In Proc. of the First Intel Delta Applications Workshop, </booktitle> <pages> pages 173-178, </pages> <year> 1992. </year>
Reference-contexts: When the same problems are considered in network-based architectures, the solutions are inherently much more complicated. Existing network-based architectures have made a wide variety of choices with respect to their interconnect medium. This includes multistage networks (IBM SP and TMC CM5) [24, 18], two-dimensional grads (Intel Paragon) <ref> [21] </ref>, three-dimensional tori (Cray T3D) [6], a hierarchy of fiber rings (Convex Exemplar) [11] and networks constructed of ATM switching fabric [20, 5]. As varied as the characteristics of these interconnects maybe, they have certain similarities.
Reference: [22] <author> T. Shimizu, T. Horie, and H. Ishihata. </author> <title> "Low-latency Message Communication Support for the AP1000." </title> <booktitle> In Proc. 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 288-297, </pages> <year> 1992. </year>
Reference-contexts: Such novel models include the Cray T3D PUT/GET commands [6] for remote reads and writes, the AP1000 PUT/GET commands <ref> [22, ?] </ref>, the Split-C PUT/GET definitions [8], and the virtual-memory mapped communication of the Shrimp multicomputer [2]. In all these examples the user controls the mapping of the data to the processors.
Reference: [23] <institution> Solaris Multithreaded Programming Guide. Sun Microsystems, </institution> <address> Inc., </address> <publisher> Prentice HAll, </publisher> <year> 1995. </year> <month> 14 </month>
Reference-contexts: Because this solution would keep processors idle, existing bus-based multiprocessor architectures allow for the buffering of memory requests <ref> [3, 23] </ref>. Such solutions are known to improve performance but they complicate the guarantees the architecture provides to the programmers as to when writes to shared data take effect and are visible by every processor [1].
Reference: [24] <author> C.B. Stunkel, D.G. Shea, D.G. Grice, P. Hochschild, and M. Tsao. </author> <title> "The SP1 High--Performance Switch." </title> <booktitle> In Proc. Scalable High Performance Computing Conference, </booktitle> <year> 1994 </year>
Reference-contexts: When the same problems are considered in network-based architectures, the solutions are inherently much more complicated. Existing network-based architectures have made a wide variety of choices with respect to their interconnect medium. This includes multistage networks (IBM SP and TMC CM5) <ref> [24, 18] </ref>, two-dimensional grads (Intel Paragon) [21], three-dimensional tori (Cray T3D) [6], a hierarchy of fiber rings (Convex Exemplar) [11] and networks constructed of ATM switching fabric [20, 5]. As varied as the characteristics of these interconnects maybe, they have certain similarities.
Reference: [25] <author> SunLink FDDI/S 3.0 User's Guide SunSoft, Sun Microsystems, Inc., </author> <title> Part No. </title> <address> 801-7424-10, </address> <year> 1994. </year>
Reference-contexts: system buffers with messages that are waiting to set flags. 3 The experimental testbed In order to study communication protocols for parallel computing and the issue of end-to-end flow control in particular, we use a cluster of eight dual-processor Sparcstation 20s connected via Ethernet and a dedicated fiber FDDI ring <ref> [25] </ref>. System level communication uses the Ethernet network whereas all application communication uses the FDDI ring. Within each node in the system, one of the processors is used for computation while the second processor is used exclusively as the communication coprocessor.
References-found: 25

