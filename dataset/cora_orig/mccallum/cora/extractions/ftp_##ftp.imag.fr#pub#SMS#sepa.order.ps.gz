URL: ftp://ftp.imag.fr/pub/SMS/sepa.order.ps.gz
Refering-URL: http://www-lmc.imag.fr/SMS/publications.html
Root-URL: http://www.imag.fr
Title: Blind Separation of Instantaneous Mixture of Sources based on order statistics  
Author: Dinh Tuan PHAM, Member IEEE 
Keyword: Separation of sources. Contrast. Kullback-Leibner divergence. Entropy. Independence component analysis. Mutual information. Order statistic. Quantile.  
Note: EDICS Classification: SP 3.5, SP 2.5.4  
Address: B.P. 53X, 38041 Grenoble cedex, France  
Affiliation: Laboratory of Modeling and Computation, IMAG C.N.R.S.  
Abstract: In this paper we introduce a novel procedure for separating an instantaneous mixture of source based on the order statistics. The method is derived in a general context of independence component analysis, using a contrast function defined in term of the Kullback-Leibner divergence or of the mutual information. We introduce a discretized form of this contrast permitting its easy estimation through the order statistics. We show that the local contrast property is preserved and also derive a global contrast exploiting only the information of the support of the distribution (in the case this support is finite). Some simulations are given illustrating the good performance of the method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cardoso, J. F., Laheld, B. </author> <title> "Equivariant adaptive source separation" IEEE Trans. </title> <journal> SP 44, </journal> <volume> 12, </volume> <pages> 3017-3030, </pages> <year> 1996. </year>
Reference-contexts: as C d (B) = i=1 l=2 h Q (BX) i (u l ) Q (BX) i (u l1 ) i u l u l1 ln j det Bj (2:2) where fu 1 ; : : : ; u L g is a given set of strictly increasing numbers in <ref> [0; 1] </ref>. The difference between C and C d is that when C is computed numerically, the number of discretizing points could be arbitrarily large, depending on the required accuracy, while C d involves only a fixed and relatively small number of such points. <p> The empirical quantile function of (BX) i at j=(n+1), (1 j n), is simply the j-th order statistic (BX) i (j : n), defined as the j-th smallest among the (BX) i (1); : : : ; (BX) i (n). For arbitrary u 2 <ref> [0; 1] </ref>, the empirical quantile function at u may be taken as the j-th order statistic with j being such that j=(n + 1) is closest to u. <p> We do not suppose u 1 = 0 or u L = 1, to see the effect of choosing them in the interior of <ref> [0; 1] </ref>. Since the scale factor has no effect, one can assume without loss of generality that the sources are uniform over [ 1 2 ; 1 2 ]. <p> The last function admits indeed a stationary point at (b 12 ; b 21 ) = (0; 0) but this is a saddle point and not a minimum. Further, this neighborhood shrinks and tends to disappear as [u 1 ; u L ] increases to <ref> [0; 1] </ref>. In the limiting case where u 1 = 0, u L = 1, we have shown that the criterion admits the identity matrix as a local minimum, but only for L = 2, we have been able to show that it is a global minimum.
Reference: [2] <author> Cardoso, J.P. </author> <title> "Source separation using higher order moments". </title> <booktitle> Proc . ICASSP, </booktitle> <pages> 2109-2112, </pages> <year> 1989 </year>
Reference-contexts: 1 Introduction The problem of separation of sources has been the subject of rapid development in the signal processing literature recently (see for example <ref> [2] </ref> - [5], [7] - [12], [14], [15] : : : ).
Reference: [3] <author> Cardoso, J.P. </author> <title> "Iterative technique for blind source separation using only fourth order cumulants". </title> <booktitle> In Signal Processing VI: Theory and Application (Proc. of EUSIPCO 92), </booktitle> <editor> J. Vandewalle, R. Boite, M. Moonen & A. Oosterlink eds, </editor> <year> 1992, </year> <pages> 739-742. </pages>
Reference: [4] <author> Cardoso, J.P., Souloumiac, A. </author> <title> "An efficient technique for blind separation of complex sources". </title> <booktitle> In Proc. IEEE SP Workshop on Higher-Order-Stat., </booktitle> <address> Lake Tahoe, U.S.A., </address> <year> 1993, </year> <pages> 275-279. </pages>
Reference: [5] <author> Comon, P. </author> <title> "Blind Separation of sources: Problems statement". </title> <booktitle> Signal Processing 24, </booktitle> <year> 1991, </year> <pages> 11-20. </pages>
Reference-contexts: 1 Introduction The problem of separation of sources has been the subject of rapid development in the signal processing literature recently (see for example [2] - <ref> [5] </ref>, [7] - [12], [14], [15] : : : ). We consider here the simplest case where one observes K sequences X 1 (t), : : : , X K (t), each being a linear combination of K independent sources S 1 (t), : : : , S K (t).
Reference: [6] <author> Comon, P. </author> <title> "Independent component analysis, a new concept". </title> <booktitle> Signal Processing 36, </booktitle> <volume> 3, </volume> <year> 1994, </year> <pages> 287-314. </pages>
Reference-contexts: Hence H (BX) depends on B only through the term ln j det Bj and the ICA problem is reduced to the minimization of C (B) = i=1 In the context of separation of sources, the function C is called a contrast (as introduced in <ref> [6] </ref>) meaning that if X = AS with the matrix A invertible and the random vector S having independent components (the sources), then C (B) is minimum if and only if B = A 1 (up to the pre-multiplication by a permuted diagonal matrix). <p> Instead of computing the gradient of C d , it is easier to compute the relative gradient. This concept has been explained in <ref> [6] </ref> and [13]. Briefly, one look for a matrix, denoted by C 0 d (B), so that for any matrix * tending to zero, C d (B + *B) C (B) approximates to tr [*C 0 (B) T ] with an error tending to 0 faster than *.
Reference: [7] <author> Duvaut, P. </author> <title> "Principle des methodes de separation de sources fondees sur les moments d'ordre superieur". Traitement du Signal, </title> <type> 7, </type> <note> 5 (numero special non lineaire non gaussien), </note> <year> 1990, </year> <pages> 407-418. </pages>
Reference-contexts: 1 Introduction The problem of separation of sources has been the subject of rapid development in the signal processing literature recently (see for example [2] - [5], <ref> [7] </ref> - [12], [14], [15] : : : ). We consider here the simplest case where one observes K sequences X 1 (t), : : : , X K (t), each being a linear combination of K independent sources S 1 (t), : : : , S K (t).
Reference: [8] <author> Gaeta, M., Lacoume, J. L. </author> <title> "Source separation without a priori knowledge: the maximum likelihood approach". </title> <booktitle> In Signal Processing V (Proc. of EUSIPCO 90), </booktitle> <editor> L.Tores, E. MasGrau and M. A. Lagunas eds, </editor> <year> 1990, </year> <pages> 621-624. </pages>
Reference: [9] <author> Jutten, C. Herault, J. </author> <title> "Blind separation of sources, Part I: an adaptative algorithm based on neuromimetic structure". </title> <booktitle> Signal Processing 24, </booktitle> <year> 1991, </year> <pages> 1-10. </pages>
Reference: [10] <author> Lacoume, J. L. </author> <title> "Sources identification: a solution based on the cumulants". </title> <booktitle> IEEE workshop on spectrum analysis and modeling. </booktitle> <month> August </month> <year> 1988, </year> <month> Mineapolis. </month>
Reference: [11] <author> Loubaton, Ph., Delfosse, N. </author> <title> "Separation adaptive de sources independantes par une approch de deflation". In XIV Colloque sur le Traitement du Signal et des Images (GRETSI), </title> <address> Juan-Les-Pins, </address> <month> Sept. </month> <year> 1993, </year> <pages> 325-328 </pages>
Reference: [12] <author> Moreau, E., Macchi, O. </author> <title> "New self-adaptive algorithm for sources separation based on constrast functions". </title> <booktitle> In Proc. IEEE SP Workshop on Higher-Order-Stat., </booktitle> <address> Lake Tahoe, U.S.A., </address> <year> 1993, </year> <pages> 215-219. </pages>
Reference-contexts: 1 Introduction The problem of separation of sources has been the subject of rapid development in the signal processing literature recently (see for example [2] - [5], [7] - <ref> [12] </ref>, [14], [15] : : : ). We consider here the simplest case where one observes K sequences X 1 (t), : : : , X K (t), each being a linear combination of K independent sources S 1 (t), : : : , S K (t).
Reference: [13] <author> Pham, D. T. </author> <title> "Blind Separation of Instantaneous Mixture of Sources via an Independent Component Analysis." </title> <journal> IEEE Trans. SP 44, </journal> <volume> 11, </volume> <pages> 2768-2779, </pages> <year> 1996. </year>
Reference-contexts: In this paper we propose a method of separation of source based on the ICA concept. We consider the same contrast function as used in <ref> [13] </ref> to measure the lack of independence between the components of the transformed vector, so that its minimization would provide the separation. But unlike [13] which estimates this contrast directly through density estimation, we rewrite it in term of the quantile function leading to a discretized form which can be easily <p> In this paper we propose a method of separation of source based on the ICA concept. We consider the same contrast function as used in <ref> [13] </ref> to measure the lack of independence between the components of the transformed vector, so that its minimization would provide the separation. But unlike [13] which estimates this contrast directly through density estimation, we rewrite it in term of the quantile function leading to a discretized form which can be easily estimated through the order statistics. We shall show that this form still retain the local contrast property. <p> The problem is to find a linear invertible transformation defined by a matrix B such that the components (BX) 1 , : : : , (BX) K of the transformed vector BX are the most independent possible. As in <ref> [13] </ref>, we take as measure of independence the 2 Kullback-Leibner entropy divergence between the joint probability distribution of the (BX) i and the one they would have if they are independent. <p> Thus our independence measure is actually the mutual entropy between the components of BX. By the same computation as in <ref> [13] </ref>; H (BX) = H (X) + ln j det Bj; H (X) denoting the entropy of X. <p> Instead of computing the gradient of C d , it is easier to compute the relative gradient. This concept has been explained in [6] and <ref> [13] </ref>. Briefly, one look for a matrix, denoted by C 0 d (B), so that for any matrix * tending to zero, C d (B + *B) C (B) approximates to tr [*C 0 (B) T ] with an error tending to 0 faster than *. <p> Hence, one is led to the same formula in <ref> [13] </ref> 2 4 . . . 3 5 I: Let us look at the Hessian of C d . Again we shall actually work with the relative Hessian. <p> "B) T ] tr [*C 0 d (B) T ] with an error tending to zero faster than k"kk*k as ("; *) ! 0, _ C d denoting the ordinary gradient, which is related to the relative gradient by _ C d (B)B T = C 0 d (B) (see <ref> [13] </ref> for more detail). We shall compute the Hessian only at the stationary point A 1 . Although the computation can be done at an arbitrary point, the result is quite complex and is of lesser interest. <p> The above distributions are the same as those considered in <ref> [13] </ref> except that here they are normalized to have unit variance. The results of application of our method are reported in figures 6.1 - 6.3. <p> Since we have taken care to work with normalized sources (that is sources having unit variance), these terms are no other than the contamination coefficients introduced in <ref> [13] </ref> and [14]. More precisely, b 2 12 and b 2 21 represent the fraction of power of the contamination of source 2 to source 1 and of sources 1 to source 2, respectively. <p> : t (k) &gt; t (k i ); s (k) + fi i t (k) &gt; s (k i ) + fi i t (k i )g i K &lt; A.3 Proofs of results To prove the Proposition 3.1 and 3.1, we shall make use of the Lemma A.2 of <ref> [13] </ref>, which we recall here Lemma A.1 Assume that f X is continuously differentiable.
Reference: [14] <author> Pham, D. T., Garat, Ph. </author> <title> "Blind separation of a mixture of independent sources through a quasi-maximum likelihood approach". </title> <journal> IEEE Trans. SP 45, </journal> <volume> 7, </volume> <pages> 1712-1725, </pages> <year> 1997. </year>
Reference-contexts: 1 Introduction The problem of separation of sources has been the subject of rapid development in the signal processing literature recently (see for example [2] - [5], [7] - [12], <ref> [14] </ref>, [15] : : : ). We consider here the simplest case where one observes K sequences X 1 (t), : : : , X K (t), each being a linear combination of K independent sources S 1 (t), : : : , S K (t). <p> Since we have taken care to work with normalized sources (that is sources having unit variance), these terms are no other than the contamination coefficients introduced in [13] and <ref> [14] </ref>. More precisely, b 2 12 and b 2 21 represent the fraction of power of the contamination of source 2 to source 1 and of sources 1 to source 2, respectively.
Reference: [15] <author> Puntonet, C. G., Prieto A., Jutten C, Rodriguez-Alvarez M and Ortega J. </author> <title> "Separation of sources: A geometrical based procedure for reconstsruction of n-valued signal. </title> <booktitle> Singal Processing 46, </booktitle> <volume> 3, </volume> <pages> 267-284, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction The problem of separation of sources has been the subject of rapid development in the signal processing literature recently (see for example [2] - [5], [7] - [12], [14], <ref> [15] </ref> : : : ). We consider here the simplest case where one observes K sequences X 1 (t), : : : , X K (t), each being a linear combination of K independent sources S 1 (t), : : : , S K (t).
Reference: [16] <author> Tong, L., Inouye, Y., Liu, R. </author> <title> "Waveform preserving blind estimation of multiple 29 independent sources." </title> <journal> IEEE Trans. on SP 41, </journal> <volume> 7, </volume> <year> 1993, </year> <pages> 2461-2470. 30 </pages>
References-found: 16

