URL: http://netlib.bell-labs.com/cm/ms/departments/sia/doc/96.9.ps
Refering-URL: http://netlib.bell-labs.com/cm/ms/departments/sia/doc/index.html
Root-URL: 
Title: matrix of leading k eigen vectors of where V is the matrix of k leading
Author: ~(j; m)( O jm jm T O jm jm X ~(j; m)( O jm T O jm X ~(j; m)(V T O jm jm X ~(j; m) O flT jm P jm O flT j;m t= t (j; m)(O t O jm )(O t O jm T j;m t= t (j; m) W = BW = B j;m ~(j; m) O jm O T P P P T P P T W W = U U T W = BW = U U T W = L. E. Baum. [] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi T. Hastie and R. Tibshirani. [] B. H. Juang. B. H. Juang and S. Katagiri. [] K. V. Mardia, J. T. Kent, and J. M. Bibby. Multivariate [] L. R. Rabiner. IEEE, :-, . [] C. Rathinavelu and L. Deng. 
Address: 1994.  Co., New York, 1979.  
Date: (5)  
Affiliation: X  W  AT&T Bell Laboratories,  
Note: jm 1=2 O jm  It is easy to show that this term is minimized when V is the  and jm V T O jm which leads to jm 1=2 V V T 1=2 O jm  jm W 1=2 V V T W 1=2 O jm  jm O jm W:  5. CONCLUSION REFERENCES [1]  J. Roy. Stat. Soc., 39:1-38, 1977. [3]  AT&T Technical Journal, 64:1235-1249, 1985. [5]  Analysis. Academic Press, Harcourt Brace  In Proceedings ICASSP, pages 373-376, 1995.  
Pubnum: Technical report,  
Abstract: that the estimates of all the mean parameters stay within a k-dimensional subspace of the original feature space, i.e., jm = A-jm , where A is any p by k (k p) matrix of full rank, and -jm a k fi 1 vector. For a given , there is only one part in (4) that involves jm . Let where V is a p fi k matrix, jm a k fi 1 vector and (V jU ) a p fi p orthogonal matrix, we then have Furthermore, when is unknown, we can prove that the estimate is obtained similarly by replacing with W in (5), where In summary, we have the explicit solution to the reduced-rank maximization as follows: where U is the matrix of the remaining p k eigenvectors of W 1=2 BW 1=2 . Since V is a p fik matrix, it is apparent that all the ^ jm 's stay in a k dimensional space. If we choose k = p, then V is a p fi p orthogonal matrix, and we have: This paper proposed a new approach of feature space dimensionality reduction within the framework of hidden Markov models. It avoids the difficulty in applying linear discriminant analysis to speech data which relies on precise segmentation information of HMM states. Also, the idea of using constrained maximum likelihood estimation can be extended to guarantee desirable properties of the estimates in HMMs. One important case could be imposing smoothness constraints on the estimates to avoid unstable estimation when training data is not sufficient. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. E. Baum. </author> <title> An inequality and associated maximiza tion technique in statistical estimation for probabilistic functions of markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference: [2] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maxi mum likelihood from incomplete data via the em algorithm. </title> <journal> J. Roy. Stat. Soc., </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference: [3] <author> T. Hastie and R. Tibshirani. </author> <title> Discriminant analysis by gaussian mixtures. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1994. </year>
Reference: [4] <author> B. H. Juang. </author> <title> Maximum likelihood estimation for multi variate observations of markov sources. </title> <journal> AT&T Technical Journal, </journal> <volume> 64 </volume> <pages> 1235-1249, </pages> <year> 1985. </year>
Reference: [5] <author> B. H. Juang and S. Katagiri. </author> <title> Discriminative learning for minimum error classification. </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> 40(12) </volume> <pages> 3043-3054, </pages> <year> 1992. </year>
Reference: [6] <author> K. V. Mardia, J. T. Kent, and J. M. Bibby. </author> <title> Multivariate Analysis. </title> <publisher> Academic Press, Harcourt Brace & Co., </publisher> <address> New York, </address> <year> 1979. </year>
Reference: [7] <author> L. R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <journal> Proc. of the IEEE, </journal> <volume> 77 </volume> <pages> 257-285, </pages> <year> 1989. </year>


References-found: 7

