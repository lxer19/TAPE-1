URL: http://www.cs.berkeley.edu/~edss/252.ps
Refering-URL: http://www.cs.berkeley.edu/~edss/
Root-URL: 
Title: Cache Behaviour of the SPEC95 Benchmark Suite  
Author: Sanjoy Dasgupta and Edouard Servan-Schreiber 
Date: May 13, 1996  
Abstract: Previous cache studies are becoming rather outdated, since they deal with older benchmarks (such as those in SPEC92) and smaller cache configurations than are now feasible. We remedy this situation by performing a battery of cache tests on several interesting programs in the SPEC95 suite. The applications of chief interest to us are those that are represent current trends in computing, such as ijpeg, perl and database programs. We try out a variety of key cache configurations and do some analyses of second-level caches. The results generally corroborate older studies, with some subtle and interesting differences. Some conclusions are: (1) a block size of 32 bytes remains optimal for first-level caches; (2) LRU buys little more than random replacement; (3) going beyond 2-associativity results in little improvement; (4) the 2:1 "cache rule-of-thumb" is highly application-dependent; (5) second-level caches do not need to be very large, and there is little point in pushing their associativity beyond 2-way or making their replacement policy LRU; and (6) second-level caches may warrant blocks of 64 bytes or more, depending upon their size. We also found some idiosyncrasies of different classes of programs; for instance, the optimal block size for our database application vortex was lower than that for our other benchmarks, indicating highly non-sequential accesses. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.L. Hennessy, </author> <title> D.P. Patterson. Computer Architecture: A Quantitative Approach, 2nd edition. </title> <publisher> Morgan Kaufmann Books, </publisher> <year> 1995. </year>
Reference-contexts: Several papers [6, 7, 8] have attempted to model hit times and hit time ratios. We will be crude and follow the example on p.420 of <ref> [1] </ref> which mentions that two-way associativity increases the hit time by 1%. Let us assume doubling the cache size does the same.
Reference: [2] <author> J.D. Gee, M.D. Hill, D.N. Pnevmatikatos, A.J. Smith. </author> <title> Cache Performance of SPEC92 Benchmark Suite. </title> <journal> IEEE Micro, </journal> <volume> 13:4, </volume> <year> 1993. </year>
Reference-contexts: In keeping with current trends, the SPEC95 benchmark features perl, ijpeg, and a database package, vortex. Its other components include a series of floating-point routines similar to those in SPEC92, and various odds and ends such as a simulator m88ksim. A definitive cache study <ref> [2] </ref> was performed a few years ago on SPEC92, 2 and illustrated many interesting features of cache behaviour. At the end of this report was a recommendation that such a study be carried out for all future SPEC benchmarks as well. <p> We have focused our analysis on the applications we thought would be most interesting, such as perl, ijpeg, vortex, and compress. We also have also included a floating point routine, su2cor, which is centered on matrix multiplication and has the reputation (from the SPEC92 cache study <ref> [2] </ref>) of being particularly badly behaved among the SPEC floating point routines. Our simulations were carried out on a series of SUN workstations, using the tracer shade. The starting point was to determine whether the miss rate of any given application ever stabilised, and if so, when it did. <p> fixed global miss rate (that is to say, for a fixed second-level cache size), the performance of different first-level cache sizes fluctuated in a interesting convex manner. 2 Related Work A very comprehensive and definitive study of cache behaviour was carried out in 1993 by Gee, Hill, Pnevmatikatos, and Smith <ref> [2] </ref>. They tested the entire SPEC92 benchmark on DECstations using the Pixie tracer and their own cache simulator Tycho. The configurations tested included all that could be considered reasonable in 1993, except that second-level caches were not discussed. <p> Multimedia applications are continuously struggling with the enormous size of their data which needs to be transmitted at very fast rates, leading to a wider use of compression algorithms. These reasons, coupled with the fact that we wanted to avoid simply redoing the work of <ref> [2] </ref>, led us to focus on the following four applications: compress Lempel-Ziv compression and decompression in memory ijpeg Graphic compression and decompression perl Manipulates strings (anagrams) and prime numbers in Perl vortex A database program As for floating point applications, we decided not to focus on them as the SPEC95 suite <p> But if the branching factor of an application is not abnormally high, there is no reason to expect a very different behavior at the instruction level 11 from the new applications. This view is supported in <ref> [2] </ref> where the authors state that instruction misses is a very small portion of all misses, and therefore the data access pattern is really what drives the cache behavior. <p> The rest of the experiments can be divided into two categories: single-level cache, and second-level cache experiments. 4.3.1 Single-Level Caches For single-level caches, we mostly wanted to confirm the data of <ref> [2] </ref> and see how the new applications differed. This led us to the following set of experiments: Cache Size: Check if the power law of cache sizes vs. miss rates still applied, given a fixed block size and associativity. <p> Finally, ijpeg's miss rate did not change over the entire course of measurement, indicating that our subsequent results for this program were especially valid. 5.2 Single Level Results In this suite of results, our main goal was to check whether the results of the SPEC92 study <ref> [2] </ref> on optimal first-level cache parameters continued to be valid in today's circumstances. We started, therefore, with the most basic analysis: comparing the miss rates for different cache sizes, and fixing the other cache parameters to values suggested in [2], namely direct-mapping and 32-byte blocks. <p> goal was to check whether the results of the SPEC92 study <ref> [2] </ref> on optimal first-level cache parameters continued to be valid in today's circumstances. We started, therefore, with the most basic analysis: comparing the miss rates for different cache sizes, and fixing the other cache parameters to values suggested in [2], namely direct-mapping and 32-byte blocks. The results can be seen in figure 2. <p> The trends exhibited in all this data match those in <ref> [2] </ref>, although our miss rates are consistently lower. <p> It is approximately true for gcc and su2cor, but for vortex, a 128K direct-mapped cache is far worse than a 64K 2-way associative cache. 5.2.2 Block Size A block size of 32 bytes seemed to be the suggestion of the SPEC92 study <ref> [2] </ref>. <p> This subset included a database program, two compression algorithms, a perl script, and one floating point application (su2cor). We attempted to confirm the results on single-level caches obtained in <ref> [2] </ref> for these untested applications, and then performed several experiments with two-level caches as they seem to be becoming increasingly popular.
Reference: [3] <author> R.E. Kessler, M.D. Hill, and D.A. Wood. </author> <title> A Comparison of Trace-Sampling Techniques for Multi-Megabyte Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43:6, </volume> <year> 1994. </year> <month> 29 </month>
Reference-contexts: A valid study must therefore involve tens of billions of data references, which are extremely expensive to store and use. Thus the question arises of trying to pick representative subsets of these references. A few papers by Hill, Kessler, and Wood <ref> [3, 4] </ref> suggest two techniques: time sampling and set sampling. The former samples the behaviour of the entire cache during certain stretches of time; the latter samples the behaviour of a few of the sets in the cache for the entire running of the program.
Reference: [4] <author> D.A. Wood, M.D. Hill, and R.E. Kessler. </author> <title> A Model for Estimating Trace--Sample Miss Ratios. </title> <booktitle> ACM Sigmetrics, </booktitle> <year> 1991. </year>
Reference-contexts: A valid study must therefore involve tens of billions of data references, which are extremely expensive to store and use. Thus the question arises of trying to pick representative subsets of these references. A few papers by Hill, Kessler, and Wood <ref> [3, 4] </ref> suggest two techniques: time sampling and set sampling. The former samples the behaviour of the entire cache during certain stretches of time; the latter samples the behaviour of a few of the sets in the cache for the entire running of the program.
Reference: [5] <author> A. Borg, R.E. Kessler, and D.W. Wall: </author> <title> Generation and analysis of very long address traces. </title> <booktitle> Proc. 17th Annual Int'l Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: Another aspect of cache simulation which has received some attention is that of sampling to avoid processing billions of references. The necessity of collecting this much data is shown in a paper by Borg, Kessler, and Wall <ref> [5] </ref>, in which various applications are seen to take very long to stabilise in their miss rate behaviour. A valid study must therefore involve tens of billions of data references, which are extremely expensive to store and use. <p> This would give us a good idea of how long we needed to run simulations for each 13 individual application. From <ref> [5] </ref>, we expected that su2cor would take a while to settle down into its main (matrix manipulation) loop, and this latency period turned out to be 4 billion references. vortex, a database program with potentially chaotic behaviour, showed a similar trend.
Reference: [6] <author> S. Przybylski, M. Horowitz, J. Hennessy: </author> <title> Performance Tradeoffs in Cache Design. </title> <booktitle> Proc. 15th Annual Int'l Symposium on Computer Architecture, </booktitle> <year> 1988. </year>
Reference-contexts: Several papers <ref> [6, 7, 8] </ref> have attempted to model hit times and hit time ratios. We will be crude and follow the example on p.420 of [1] which mentions that two-way associativity increases the hit time by 1%. Let us assume doubling the cache size does the same. <p> However, configuration changes can only be properly evaluated in terms of AMAT. And for this we need a more precise model of hit times in different cache configurations, although a model of hit time ratios between two configurations would be sufficient (such as in <ref> [6] </ref>).
Reference: [7] <author> S.J.E. Wilton, </author> <title> N.P Jouppi: Tradeoffs in Two-Level On-Chip Caching. </title> <booktitle> Proc. 20th Annual Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: For instance, how much longer does it take to access a 16K data cache than a 4K cache? Such questions are tackled in two papers by Wilton and Jouppi <ref> [7, 8] </ref>, which propose a very comprehensive analytical model of the access times for caches of varying sizes, block lengths, and associativities. <p> Several papers <ref> [6, 7, 8] </ref> have attempted to model hit times and hit time ratios. We will be crude and follow the example on p.420 of [1] which mentions that two-way associativity increases the hit time by 1%. Let us assume doubling the cache size does the same.
Reference: [8] <author> S.J.E. Wilton, </author> <title> N.P. Jouppi: An Enhanced Access and Cycle Time Model for On-Chip Caches. </title> <note> WRL Research Report 93/5, </note> <institution> DEC Western Research Lab, </institution> <year> 1994. </year>
Reference-contexts: For instance, how much longer does it take to access a 16K data cache than a 4K cache? Such questions are tackled in two papers by Wilton and Jouppi <ref> [7, 8] </ref>, which propose a very comprehensive analytical model of the access times for caches of varying sizes, block lengths, and associativities. <p> Several papers <ref> [6, 7, 8] </ref> have attempted to model hit times and hit time ratios. We will be crude and follow the example on p.420 of [1] which mentions that two-way associativity increases the hit time by 1%. Let us assume doubling the cache size does the same.
Reference: [9] <institution> M.D. Hill. </institution> <note> Private Communications. 30 </note>
References-found: 9

