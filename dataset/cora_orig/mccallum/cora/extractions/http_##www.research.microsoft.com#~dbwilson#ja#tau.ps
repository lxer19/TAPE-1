URL: http://www.research.microsoft.com/~dbwilson/ja/tau.ps
Refering-URL: http://www.research.microsoft.com/~dbwilson/ja/
Root-URL: http://www.research.microsoft.com
Email: dbwilson@mit.edu  
Title: Generating Random Spanning Trees More Quickly than the Cover Time  
Author: David Bruce Wilson 
Address: Cambridge, Massachusetts 02139  
Affiliation: Department of Mathematics, Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> David Aldous. </author> <title> On simulating a Markov chain stationary distribution when transition probabilities are unknown, 1994. </title> <type> Preprint. </type>
Reference-contexts: They give a general procedure, which given n and a Markov chain on n states, simulates the Markov chain for a while, stops after finite time, Exact Sampling Algorithm Expected running time Asmussen, Glynn, Thorisson [4] finite time Aldous <ref> [1] </ref> (" bias in sample) 81t =" 2 Lovasz, Winkler [22] O (hT 0 mix n log n) Propp, Wilson [24] (requires monotonicity) O (T mix log l) Wilson, Propp [26] 15T c or O (T mix n log n) Fill [15] (not yet analyzed) This paper 22t (t h T <p> See [3] for background on the Markov chain parameters. and then outputs a random state distributed exactly according to . However, they did not gives bounds on the running time, and described the procedure as more of a possibility result than an algorithm to run. Aldous <ref> [1] </ref> described an efficient procedure for sampling within O (t =" 2 ) time from an unknown Markov chain that can be simulated, but with some bias " in the samples.
Reference: [2] <author> David J. Aldous. </author> <title> A random walk construction of uniform spanning trees and uniform labelled trees. </title> <journal> SIAM Journal of Discrete Mathematics, </journal> <volume> 3(4) </volume> <pages> 450-465, </pages> <year> 1990. </year>
Reference-contexts: For some graphs the best determinant algorithm will be faster, and for others the random-walk algorithms will be faster, but Broder argues that for most graphs, the random-walk algorithms will be faster [6]. Broder [6] and Aldous <ref> [2] </ref> found the random-walk algorithm (described in the introduction) for randomly generating spanning trees after discussing the Matrix Tree Theorem with Diaconis. The algorithm works for undirected graphs and runs within the cover time of the random walk. <p> is the mean hitting time of G, for Eulerian graphs the time bound is the maximum Tree Algorithm Expected running time Guenoche [16] / Kulkarni [19] O (n 3 m) Colbourn, Day, Nel [8] O (n 3 ) Colbourn, Myrvold, Neufeld [10] M (n) = O (n 2:376 ) Aldous <ref> [2] </ref> / Broder [6] O ( T c ) (undirected) Kandel, Matias, Unger, Winkler [18] O ( T c ) (Eulerian) Wilson, Propp [26] O ( T c ) (any graph) This paper O (t ) (undirected) This paper O ( h) (Eulerian) This paper O (~t ) (any graph) n
Reference: [3] <author> David J. Aldous and James A. Fill. </author> <title> Reversible Markov Chains and Random Walks on Graphs. </title> <note> Book in preparation, </note> <year> 1995. </year>
Reference-contexts: The result is a random spanning tree. Aldous calls this theorem "the most often rediscovered result in probability theory" <ref> [3] </ref>; [6] includes a nice proof. For undirected graphs and Eulerian graphs, ~ is just the uniform distribution on vertices. <p> See <ref> [3] </ref> for background on the Markov chain parameters. and then outputs a random state distributed exactly according to . However, they did not gives bounds on the running time, and described the procedure as more of a possibility result than an algorithm to run. <p> Proof of Theorem 2: What is the expected number of times that RandomSuccessor (u) is called? Since the order of cycle popping is irrelevant, we may assume that the first trajectory starts at u. It is a standard result (see <ref> [3] </ref>) that the expected number of times the random walk started at u returns to u before reaching r is given by (u)[E u T r + E r T u ], where the "return" at time 0 is included, and E i T j is the expected number of steps
Reference: [4] <author> Stren Asmussen, Peter W. Glynn, and Hermann Thorisson. </author> <title> Stationary detection in the initial transient problem. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 2(2) </volume> <pages> 130-157, </pages> <year> 1992. </year>
Reference-contexts: Despite much work at determining how long is "long enough" [12] [17] [14] [21] [25] [13], this remains an arduous task. Asmussen, Glynn, and Thorisson <ref> [4] </ref> addressed the problem of not knowing when to stop running the chain. They give a general procedure, which given n and a Markov chain on n states, simulates the Markov chain for a while, stops after finite time, Exact Sampling Algorithm Expected running time Asmussen, Glynn, Thorisson [4] finite time <p> and Thorisson <ref> [4] </ref> addressed the problem of not knowing when to stop running the chain. They give a general procedure, which given n and a Markov chain on n states, simulates the Markov chain for a while, stops after finite time, Exact Sampling Algorithm Expected running time Asmussen, Glynn, Thorisson [4] finite time Aldous [1] (" bias in sample) 81t =" 2 Lovasz, Winkler [22] O (hT 0 mix n log n) Propp, Wilson [24] (requires monotonicity) O (T mix log l) Wilson, Propp [26] 15T c or O (T mix n log n) Fill [15] (not yet analyzed) This paper
Reference: [5] <author> Bela Bollobas. </author> <title> Graph Theory: An Introductory Course. </title> <publisher> Springer-Verlag, </publisher> <year> 1979. </year> <note> Graduate texts in mathematics, #63. </note>
Reference-contexts: In this section we compare the results of this paper to previous work. 2.1. Random tree generation The first algorithms for generating random spanning trees were based on the Matrix Tree Theorem, which allows one to compute the number of spanning trees by evaluating a determinant (see <ref> [5, ch. 2, thm. 8] </ref>).
Reference: [6] <author> Andrei Broder. </author> <title> Generating random spanning trees. </title> <booktitle> In Foundations of Computer Science, </booktitle> <pages> pages 442-447, </pages> <year> 1989. </year>
Reference-contexts: The result is a random spanning tree. Aldous calls this theorem "the most often rediscovered result in probability theory" [3]; <ref> [6] </ref> includes a nice proof. For undirected graphs and Eulerian graphs, ~ is just the uniform distribution on vertices. <p> A number of other algorithms are based on random walks on the graph. For some graphs the best determinant algorithm will be faster, and for others the random-walk algorithms will be faster, but Broder argues that for most graphs, the random-walk algorithms will be faster <ref> [6] </ref>. Broder [6] and Aldous [2] found the random-walk algorithm (described in the introduction) for randomly generating spanning trees after discussing the Matrix Tree Theorem with Diaconis. The algorithm works for undirected graphs and runs within the cover time of the random walk. <p> A number of other algorithms are based on random walks on the graph. For some graphs the best determinant algorithm will be faster, and for others the random-walk algorithms will be faster, but Broder argues that for most graphs, the random-walk algorithms will be faster <ref> [6] </ref>. Broder [6] and Aldous [2] found the random-walk algorithm (described in the introduction) for randomly generating spanning trees after discussing the Matrix Tree Theorem with Diaconis. The algorithm works for undirected graphs and runs within the cover time of the random walk. <p> The algorithm works for undirected graphs and runs within the cover time of the random walk. The cover time T c of a simple undirected graph is bounded by O (n 3 ), and is often as small as O (n log n); see <ref> [6] </ref> and references contained therein. Broder also described an algorithm for the approximate random generation of ar-borescences from a directed graph. When the out-degree is regular, the running time is O (T c ), but for general simple directed graphs, Broder's algorithm takes O (nT c ) time. <p> hitting time of G, for Eulerian graphs the time bound is the maximum Tree Algorithm Expected running time Guenoche [16] / Kulkarni [19] O (n 3 m) Colbourn, Day, Nel [8] O (n 3 ) Colbourn, Myrvold, Neufeld [10] M (n) = O (n 2:376 ) Aldous [2] / Broder <ref> [6] </ref> O ( T c ) (undirected) Kandel, Matias, Unger, Winkler [18] O ( T c ) (Eulerian) Wilson, Propp [26] O ( T c ) (any graph) This paper O (t ) (undirected) This paper O ( h) (Eulerian) This paper O (~t ) (any graph) n = number of <p> The mean and maximum hitting times are always less than the cover time. Broder described a simple directed graph on n vertices which has an exponentially large cover time <ref> [6] </ref>. It is noteworthy that the mean hitting time of Broder's graph is linear in n. Even for undirected graphs these times can be substantially smaller than the cover time.
Reference: [7] <author> Robert Burton and Robin Pemantle. </author> <title> Local characteristics, entropy and limit theorems for spanning trees and domino tilings via transfer-impedances. </title> <journal> The Annals of Probability, </journal> <volume> 21(3) </volume> <pages> 1329-1371, </pages> <year> 1993. </year>
Reference-contexts: Then the cycle-erased trajectory gets added to the current tree. It has been known for some time that the path from a vertex to the root of a random spanning tree is a loop-erased random walk (see e.g. [23] and <ref> [7] </ref>), but this is the first time that anyone has used this fact to make a provably efficient algorithm. See [20] for background on loop-erased random walks. Theorem 1 RandomTreeWithRoot (r) returns a random spanning tree rooted at r.
Reference: [8] <author> Charles J. Colbourn, Robert P. J. Day, and Louis D. Nel. </author> <title> Unranking and ranking spanning trees of a graph. </title> <journal> Journal of Algorithms, </journal> <volume> 10 </volume> <pages> 271-286, </pages> <year> 1989. </year>
Reference-contexts: This algorithm was optimized for the generation of many random spanning trees to make it more suitable for Monte Carlo studies [9]. Colbourn, Day, and Nel <ref> [8] </ref> reduced the time spent computing determinants to get an O (n 3 ) algorithm for random spanning trees. Colbourn, Myrvold, and Neufeld [10] simplified this algorithm, and showed how to sample random arborescences in the time required to multiply n fi n matrices, currently O (n 2:376 ) [11]. <p> For undirected graphs the time bound is the mean hitting time of G, for Eulerian graphs the time bound is the maximum Tree Algorithm Expected running time Guenoche [16] / Kulkarni [19] O (n 3 m) Colbourn, Day, Nel <ref> [8] </ref> O (n 3 ) Colbourn, Myrvold, Neufeld [10] M (n) = O (n 2:376 ) Aldous [2] / Broder [6] O ( T c ) (undirected) Kandel, Matias, Unger, Winkler [18] O ( T c ) (Eulerian) Wilson, Propp [26] O ( T c ) (any graph) This paper O
Reference: [9] <author> Charles J. Colbourn, Bradley M. Debroni, and Wendy J. Myrvold. </author> <title> Estimating the coefficients of the reliability polynomial. </title> <journal> Congressus Numeran-tium, </journal> <volume> 62 </volume> <pages> 217-223, </pages> <year> 1988. </year>
Reference-contexts: This algorithm was optimized for the generation of many random spanning trees to make it more suitable for Monte Carlo studies <ref> [9] </ref>. Colbourn, Day, and Nel [8] reduced the time spent computing determinants to get an O (n 3 ) algorithm for random spanning trees.
Reference: [10] <author> Charles J. Colbourn, Wendy J. Myrvold, and Eu-gene Neufeld. </author> <title> Two algorithms for unranking ar-borescences. </title> <journal> Journal of Algorithms, </journal> <note> 1995. To appear. </note>
Reference-contexts: This algorithm was optimized for the generation of many random spanning trees to make it more suitable for Monte Carlo studies [9]. Colbourn, Day, and Nel [8] reduced the time spent computing determinants to get an O (n 3 ) algorithm for random spanning trees. Colbourn, Myrvold, and Neufeld <ref> [10] </ref> simplified this algorithm, and showed how to sample random arborescences in the time required to multiply n fi n matrices, currently O (n 2:376 ) [11]. A number of other algorithms are based on random walks on the graph. <p> For undirected graphs the time bound is the mean hitting time of G, for Eulerian graphs the time bound is the maximum Tree Algorithm Expected running time Guenoche [16] / Kulkarni [19] O (n 3 m) Colbourn, Day, Nel [8] O (n 3 ) Colbourn, Myrvold, Neufeld <ref> [10] </ref> M (n) = O (n 2:376 ) Aldous [2] / Broder [6] O ( T c ) (undirected) Kandel, Matias, Unger, Winkler [18] O ( T c ) (Eulerian) Wilson, Propp [26] O ( T c ) (any graph) This paper O (t ) (undirected) This paper O ( h)
Reference: [11] <author> Don Coppersmith and Shmuel Winograd. </author> <title> Matrix multiplication via arithmetic progressions. </title> <journal> Journal of Symbolic Computation, </journal> <volume> 9 </volume> <pages> 251-280, </pages> <year> 1990. </year>
Reference-contexts: Colbourn, Myrvold, and Neufeld [10] simplified this algorithm, and showed how to sample random arborescences in the time required to multiply n fi n matrices, currently O (n 2:376 ) <ref> [11] </ref>. A number of other algorithms are based on random walks on the graph. For some graphs the best determinant algorithm will be faster, and for others the random-walk algorithms will be faster, but Broder argues that for most graphs, the random-walk algorithms will be faster [6].
Reference: [12] <author> Persi Diaconis. </author> <title> Group Representations in Probability and Statistics. </title> <institution> Institute of Mathematical Statistics, </institution> <year> 1988. </year>
Reference-contexts: The final state will be sampled from a probability distribution that can be made arbitrarily close to , if the chain is run for long enough. Despite much work at determining how long is "long enough" <ref> [12] </ref> [17] [14] [21] [25] [13], this remains an arduous task. Asmussen, Glynn, and Thorisson [4] addressed the problem of not knowing when to stop running the chain.
Reference: [13] <author> Persi Diaconis and Laurent Saloff-Coste. </author> <title> What do we know about the Metropolis algorithm? In Symposium on the Theory of Computing, </title> <address> pages 112-129, </address> <year> 1995. </year>
Reference-contexts: The final state will be sampled from a probability distribution that can be made arbitrarily close to , if the chain is run for long enough. Despite much work at determining how long is "long enough" [12] [17] [14] [21] [25] <ref> [13] </ref>, this remains an arduous task. Asmussen, Glynn, and Thorisson [4] addressed the problem of not knowing when to stop running the chain.
Reference: [14] <author> Persi Diaconis and Daniel Stroock. </author> <title> Geometric bounds for eigenvalues of Markov chains. </title> <journal> The Annals of Applied Probability, </journal> <volume> 1(1) </volume> <pages> 36-61, </pages> <year> 1991. </year>
Reference-contexts: The final state will be sampled from a probability distribution that can be made arbitrarily close to , if the chain is run for long enough. Despite much work at determining how long is "long enough" [12] [17] <ref> [14] </ref> [21] [25] [13], this remains an arduous task. Asmussen, Glynn, and Thorisson [4] addressed the problem of not knowing when to stop running the chain.
Reference: [15] <author> James A. Fill, </author> <year> 1995. </year> <type> Personal communication. </type>
Reference-contexts: running time Asmussen, Glynn, Thorisson [4] finite time Aldous [1] (" bias in sample) 81t =" 2 Lovasz, Winkler [22] O (hT 0 mix n log n) Propp, Wilson [24] (requires monotonicity) O (T mix log l) Wilson, Propp [26] 15T c or O (T mix n log n) Fill <ref> [15] </ref> (not yet analyzed) This paper 22t (t h T c ) n = number of states l = length of longest chain (monotone Markov chains only). <p> They used this same technique to obtain unbiased samples from Markov chains with huge state spaces (e.g. 2 35;000;000 states) provided that the Markov chain has a certain structure called monotonicity [24]. Recently Fill <ref> [15] </ref> has found another exact sampling method which may be applied to either moderate-sized general Markov chains or huge Markov chains with special structure. His method requires the ability to simulate the reverse Markov chain. Because the algorithm is new, the running time has not yet been determined. <p> A preliminary analysis of Fill's general Markov chain procedure suggests that the algorithm given here is faster <ref> [15] </ref>. Aldous's O (t =" 2 ) approx imate sampling algorithm suggested that an O (t ) exact sampling algorithm should be possible, but several changes in the algorithm were necessary, and the analysis is completely different. 3.
Reference: [16] <author> A. Guenoche. </author> <title> Random spanning tree. </title> <journal> Journal of Algorithms, </journal> <volume> 4 </volume> <pages> 214-220, </pages> <year> 1983. </year> <note> In French. </note>
Reference-contexts: Random tree generation The first algorithms for generating random spanning trees were based on the Matrix Tree Theorem, which allows one to compute the number of spanning trees by evaluating a determinant (see [5, ch. 2, thm. 8]). Guenoche <ref> [16] </ref> and Kulkarni [19] gave one such algorithm that runs in O (n 3 m) time 1 , where 1 Guenoche used m n 2 and stated the running time as O (n 5 ). n is the number of vertices and m is the number of edges. <p> For undirected graphs the time bound is the mean hitting time of G, for Eulerian graphs the time bound is the maximum Tree Algorithm Expected running time Guenoche <ref> [16] </ref> / Kulkarni [19] O (n 3 m) Colbourn, Day, Nel [8] O (n 3 ) Colbourn, Myrvold, Neufeld [10] M (n) = O (n 2:376 ) Aldous [2] / Broder [6] O ( T c ) (undirected) Kandel, Matias, Unger, Winkler [18] O ( T c ) (Eulerian) Wilson, Propp
Reference: [17] <author> Mark Jerrum and Alistair Sinclair. </author> <title> Approximating the permanent. </title> <journal> SIAM Journal on Computing, </journal> <volume> 18(6) </volume> <pages> 1149-1178, </pages> <year> 1989. </year>
Reference-contexts: The final state will be sampled from a probability distribution that can be made arbitrarily close to , if the chain is run for long enough. Despite much work at determining how long is "long enough" [12] <ref> [17] </ref> [14] [21] [25] [13], this remains an arduous task. Asmussen, Glynn, and Thorisson [4] addressed the problem of not knowing when to stop running the chain.
Reference: [18] <author> D. Kandel, Y. Matias, R. Unger, and P. Winkler. </author> <title> Shu*ing biological sequences, 1995. </title> <type> Preprint. </type>
Reference-contexts: Broder also described an algorithm for the approximate random generation of ar-borescences from a directed graph. When the out-degree is regular, the running time is O (T c ), but for general simple directed graphs, Broder's algorithm takes O (nT c ) time. Kandel, Matias, Unger, and Winkler <ref> [18] </ref> extended the Aldous-Broder algorithm to sample arborescences of a directed Eule-rian graph (i.e., one in which in-degree equals out-degree at each node) within the cover time. Wilson and Propp [26] gave an algorithm for sampling random arborescences from a general directed graph within 18 cover times. <p> the maximum Tree Algorithm Expected running time Guenoche [16] / Kulkarni [19] O (n 3 m) Colbourn, Day, Nel [8] O (n 3 ) Colbourn, Myrvold, Neufeld [10] M (n) = O (n 2:376 ) Aldous [2] / Broder [6] O ( T c ) (undirected) Kandel, Matias, Unger, Winkler <ref> [18] </ref> O ( T c ) (Eulerian) Wilson, Propp [26] O ( T c ) (any graph) This paper O (t ) (undirected) This paper O ( h) (Eulerian) This paper O (~t ) (any graph) n = number of vertices m = number of edges M (n) = time to
Reference: [19] <author> V. G. Kulkarni. </author> <title> Generating random combinatorial objects. </title> <journal> Journal of Algorithms, </journal> <volume> 11(2) </volume> <pages> 185-207, </pages> <year> 1990. </year>
Reference-contexts: Random tree generation The first algorithms for generating random spanning trees were based on the Matrix Tree Theorem, which allows one to compute the number of spanning trees by evaluating a determinant (see [5, ch. 2, thm. 8]). Guenoche [16] and Kulkarni <ref> [19] </ref> gave one such algorithm that runs in O (n 3 m) time 1 , where 1 Guenoche used m n 2 and stated the running time as O (n 5 ). n is the number of vertices and m is the number of edges. <p> For undirected graphs the time bound is the mean hitting time of G, for Eulerian graphs the time bound is the maximum Tree Algorithm Expected running time Guenoche [16] / Kulkarni <ref> [19] </ref> O (n 3 m) Colbourn, Day, Nel [8] O (n 3 ) Colbourn, Myrvold, Neufeld [10] M (n) = O (n 2:376 ) Aldous [2] / Broder [6] O ( T c ) (undirected) Kandel, Matias, Unger, Winkler [18] O ( T c ) (Eulerian) Wilson, Propp [26] O (
Reference: [20] <author> Gregory F. Lawler. </author> <title> Intersections of Random Walks. </title> <publisher> Birkhauser, </publisher> <year> 1991. </year>
Reference-contexts: See <ref> [20] </ref> for background on loop-erased random walks. Theorem 1 RandomTreeWithRoot (r) returns a random spanning tree rooted at r. The proofs of this and subsequent theorems in the introduction are in x3. Suppose that what we want is a random spanning tree without prescribing the root.
Reference: [21] <author> Laszlo Lovasz and Miklos Simonovits. </author> <title> On the randomized complexity of volume and diameter. </title> <booktitle> In Foundations of Computer Science, </booktitle> <pages> pages 482-491, </pages> <year> 1992. </year>
Reference-contexts: The final state will be sampled from a probability distribution that can be made arbitrarily close to , if the chain is run for long enough. Despite much work at determining how long is "long enough" [12] [17] [14] <ref> [21] </ref> [25] [13], this remains an arduous task. Asmussen, Glynn, and Thorisson [4] addressed the problem of not knowing when to stop running the chain.
Reference: [22] <author> Laszlo Lovasz and Peter Winkler. </author> <title> Exact mixing in an unknown Markov chain. </title> <journal> Electronic Journal of Combinatorics, </journal> <volume> 2, </volume> <year> 1995. </year> <note> Paper #R15. </note>
Reference-contexts: They give a general procedure, which given n and a Markov chain on n states, simulates the Markov chain for a while, stops after finite time, Exact Sampling Algorithm Expected running time Asmussen, Glynn, Thorisson [4] finite time Aldous [1] (" bias in sample) 81t =" 2 Lovasz, Winkler <ref> [22] </ref> O (hT 0 mix n log n) Propp, Wilson [24] (requires monotonicity) O (T mix log l) Wilson, Propp [26] 15T c or O (T mix n log n) Fill [15] (not yet analyzed) This paper 22t (t h T c ) n = number of states l = length <p> Aldous [1] described an efficient procedure for sampling within O (t =" 2 ) time from an unknown Markov chain that can be simulated, but with some bias " in the samples. Lovasz and Winkler <ref> [22] </ref> found the first provably polynomial time procedure for obtaining unbiased samples by observing an unknown Markov chain.
Reference: [23] <author> Robin Pemantle. </author> <title> Choosing a spanning tree for the integer lattice uniformly. </title> <journal> The Annals of Probability, </journal> <volume> 19(4) </volume> <pages> 1559-1574, </pages> <year> 1991. </year>
Reference-contexts: Then the cycle-erased trajectory gets added to the current tree. It has been known for some time that the path from a vertex to the root of a random spanning tree is a loop-erased random walk (see e.g. <ref> [23] </ref> and [7]), but this is the first time that anyone has used this fact to make a provably efficient algorithm. See [20] for background on loop-erased random walks. Theorem 1 RandomTreeWithRoot (r) returns a random spanning tree rooted at r.
Reference: [24] <author> James G. Propp and David B. Wilson. </author> <title> Exact sampling with coupled Markov chains and applications to statistical mechanics. Random Structures and Algorithms, </title> <note> 1996. To appear. </note>
Reference-contexts: a Markov chain on n states, simulates the Markov chain for a while, stops after finite time, Exact Sampling Algorithm Expected running time Asmussen, Glynn, Thorisson [4] finite time Aldous [1] (" bias in sample) 81t =" 2 Lovasz, Winkler [22] O (hT 0 mix n log n) Propp, Wilson <ref> [24] </ref> (requires monotonicity) O (T mix log l) Wilson, Propp [26] 15T c or O (T mix n log n) Fill [15] (not yet analyzed) This paper 22t (t h T c ) n = number of states l = length of longest chain (monotone Markov chains only). <p> They used this same technique to obtain unbiased samples from Markov chains with huge state spaces (e.g. 2 35;000;000 states) provided that the Markov chain has a certain structure called monotonicity <ref> [24] </ref>. Recently Fill [15] has found another exact sampling method which may be applied to either moderate-sized general Markov chains or huge Markov chains with special structure. His method requires the ability to simulate the reverse Markov chain.
Reference: [25] <author> Alistair Sinclair. </author> <title> Algorithms for Random Generation and Counting: A Markov Chain Approach. </title> <publisher> Birkhauser, </publisher> <year> 1993. </year>
Reference-contexts: The final state will be sampled from a probability distribution that can be made arbitrarily close to , if the chain is run for long enough. Despite much work at determining how long is "long enough" [12] [17] [14] [21] <ref> [25] </ref> [13], this remains an arduous task. Asmussen, Glynn, and Thorisson [4] addressed the problem of not knowing when to stop running the chain.
Reference: [26] <author> David B. Wilson and James G. Propp. </author> <title> How to get an exact sample from a generic Markov chain and sample a random spanning tree from a directed graph, both within the cover time. </title> <booktitle> In Symposium on Discrete Algorithms, </booktitle> <year> 1996. </year>
Reference-contexts: Kandel, Matias, Unger, and Winkler [18] extended the Aldous-Broder algorithm to sample arborescences of a directed Eule-rian graph (i.e., one in which in-degree equals out-degree at each node) within the cover time. Wilson and Propp <ref> [26] </ref> gave an algorithm for sampling random arborescences from a general directed graph within 18 cover times. Most of this running time is spent just picking the root, which must be distributed according to the stationary distribution of the random walk. <p> / Kulkarni [19] O (n 3 m) Colbourn, Day, Nel [8] O (n 3 ) Colbourn, Myrvold, Neufeld [10] M (n) = O (n 2:376 ) Aldous [2] / Broder [6] O ( T c ) (undirected) Kandel, Matias, Unger, Winkler [18] O ( T c ) (Eulerian) Wilson, Propp <ref> [26] </ref> O ( T c ) (any graph) This paper O (t ) (undirected) This paper O ( h) (Eulerian) This paper O (~t ) (any graph) n = number of vertices m = number of edges M (n) = time to multiply two n fi n matrices = stationary probability <p> for a while, stops after finite time, Exact Sampling Algorithm Expected running time Asmussen, Glynn, Thorisson [4] finite time Aldous [1] (" bias in sample) 81t =" 2 Lovasz, Winkler [22] O (hT 0 mix n log n) Propp, Wilson [24] (requires monotonicity) O (T mix log l) Wilson, Propp <ref> [26] </ref> 15T c or O (T mix n log n) Fill [15] (not yet analyzed) This paper 22t (t h T c ) n = number of states l = length of longest chain (monotone Markov chains only). <p> the stationary distribution, then the Lovasz-Winkler algorithm runs in O (hT 0 mix n log n) O (h 2 n log n) time. (The param eter T 0 mix is a measure of how long the Markov chain takes to randomize, and is defined as t (2) Wilson and Propp <ref> [26] </ref> described another sampling procedure which runs within the cover time of the random walk, using a technique called "coupling-from-the-past".
References-found: 26

