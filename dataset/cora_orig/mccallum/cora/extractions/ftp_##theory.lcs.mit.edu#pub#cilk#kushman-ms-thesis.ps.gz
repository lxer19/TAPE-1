URL: ftp://theory.lcs.mit.edu/pub/cilk/kushman-ms-thesis.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~cilk/abstracts/kushman-ms-thesis.html
Root-URL: 
Title: Performance Nonmonotonicities: A Case Study of the UltraSPARC Processor  
Author: by Nathaniel A. Kushman Volker Strumpen Arthur C. Smith 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degrees of Bachelor of Science in Electrical Engineering and Computer Science and Master of Engineering in Electrical Engineering and Computer Science at the  c Nathaniel A. Kushman, MCMXCVIII. All rights reserved. The author hereby grants to MIT permission to reproduce and distribute publicly paper and electronic copies of this thesis document in whole or in part, and to grant others the right to do so. Author  Certified by  Postdoctoral Associate Thesis Supervisor Certified by Charles E. Leiserson Professor Thesis Supervisor Accepted by  Chairman, Department Committee on Graduate Students  
Date: June 1998  June 1, 1998  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Glenn Ammons, Thomas Ball, and James R. Larus. </author> <title> Exploiting hardware performance counters with flow and context sensitive profiling. </title> <booktitle> In Proceedings of the 1997 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Hardware counters have been around for several years [21, 26]. Today, almost all mainstream microprocessors implement hardware counters. Additionally, many vendors have either developed special software to use the counters [2, 3] or have integrated their use into their existing profiling packages <ref> [1, 28] </ref>. Some vendors do not distribute commercial packages, but use proprietary software for tuning benchmarks and commercial software packages. <p> For this reason, hardware counters are usually used in conjunction with other techniques such as path profiling and continuous profiling <ref> [1, 2, 3] </ref>. Work has been done to show the utility of hardware counters in improving the interaction between the hardware and the software. To my knowledge, however, none of this work has included any discussion of processor performance anomalies.
Reference: [2] <author> Jennifer M. Anderson, Lance M. Berc, Jeffery Dean, Sanjay Ghemawat, Monika R. Hen zinger, Shun-Tak A. Leung, Richard L. Sites, Mark T. Vandevoorde, Carl A. Waldspurger, and William E. Weihl. </author> <title> Continuous profiling: </title> <booktitle> Where have all the cycles gone? In Proceedings of the 16th ACM Symposium on Operating Systems Principles, </booktitle> <address> St. Malo. France, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: Hardware counters have been around for several years [21, 26]. Today, almost all mainstream microprocessors implement hardware counters. Additionally, many vendors have either developed special software to use the counters <ref> [2, 3] </ref> or have integrated their use into their existing profiling packages [1, 28]. Some vendors do not distribute commercial packages, but use proprietary software for tuning benchmarks and commercial software packages. <p> For this reason, hardware counters are usually used in conjunction with other techniques such as path profiling and continuous profiling <ref> [1, 2, 3] </ref>. Work has been done to show the utility of hardware counters in improving the interaction between the hardware and the software. To my knowledge, however, none of this work has included any discussion of processor performance anomalies.
Reference: [3] <author> Mark Atkins and Ramesh Subramaniam. </author> <title> PC software performance tuning. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 47-54, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Hardware counters have been around for several years [21, 26]. Today, almost all mainstream microprocessors implement hardware counters. Additionally, many vendors have either developed special software to use the counters <ref> [2, 3] </ref> or have integrated their use into their existing profiling packages [1, 28]. Some vendors do not distribute commercial packages, but use proprietary software for tuning benchmarks and commercial software packages. <p> For this reason, hardware counters are usually used in conjunction with other techniques such as path profiling and continuous profiling <ref> [1, 2, 3] </ref>. Work has been done to show the utility of hardware counters in improving the interaction between the hardware and the software. To my knowledge, however, none of this work has included any discussion of processor performance anomalies.
Reference: [4] <author> David H. Bailey. </author> <title> RISC microprocessors and scientific computing. </title> <booktitle> In Proceedings of the Confer ence on High Performance Computing and Communications, </booktitle> <pages> pages 645-654, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Bailey <ref> [4] </ref> shows that caching causes machines to exhibit performance discontinuities. Four examples of performance discontinuities are presented in this thesis. Each of them is also an example of a performance nonmonotonicity. This thesis focuses on avoiding performance nonmonotonicities.
Reference: [5] <author> L. A. Belady, R. A. Nelson, and G. S. Shedler. </author> <title> An anomaly in space-time characteristics of certain program running in a paging machine. </title> <journal> Communications of the ACM, </journal> <volume> 12(6) </volume> <pages> 349-353, </pages> <month> June </month> <year> 1969. </year>
Reference-contexts: Performance nondeterminism is often observed in timeshared environments, even if the system is used by only one user. The operating system, and specifically, the virtual memory page mapping algorithms are often identified as the cause of this nondeterminism <ref> [5] </ref>. I have found, however, that the processor itself also exhibits performance nondeterminism. Two of the performance nonmonotonicities identified on the UltraSPARC are also examples of performance nondeterminism. I shall present algorithms to conceal the performance nondeterminisms, but I was unable to determine their cause.
Reference: [6] <author> Bryan Black and John Paul Shen. </author> <title> Calibration of microprocessor performance models. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 59-65, </pages> <month> May </month> <year> 1998. </year>
Reference-contexts: Therefore, hardware designers use performance models to assist them in making difficult design decisions. Unfortunately, the validity of these models is debatable. If these models are in fact invalid, as argued by Black and Shen <ref> [6] </ref>, a huge potential for performance anomalies exists. These performance anomalies are the focus of this thesis. <p> Currently, architectural designers spend a significant amount of time and energy to avoid performance bottlenecks, making use of performance modeling techniques to aid them in their design decisions <ref> [6, 7, 16] </ref>. Errors in these models, however, make it difficult to design an architecture for performance. Performance models used by hardware designers are attempts to extract the information that affects performance from the design. Unfortunately, there is no rigorous method for finding errors in these performance models. <p> Unfortunately, this method of validation by inspection is error-prone, and research is being done to develop better methods for validating performance models. Even more rigorous methods of validation, such as those proposed by Black and Shen <ref> [6] </ref>, rely on developer-written test suites that may not test the model effectively. Improved methods for validation of performance models may eventually reduce the number of performance problems.
Reference: [7] <author> Pradip Bose and Thomas M. Conte. </author> <title> Performance analysis and its impact on design. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 41-49, </pages> <month> May </month> <year> 1998. </year>
Reference-contexts: Currently, architectural designers spend a significant amount of time and energy to avoid performance bottlenecks, making use of performance modeling techniques to aid them in their design decisions <ref> [6, 7, 16] </ref>. Errors in these models, however, make it difficult to design an architecture for performance. Performance models used by hardware designers are attempts to extract the information that affects performance from the design. Unfortunately, there is no rigorous method for finding errors in these performance models.
Reference: [8] <author> Doug Burger and James R. Goodman. </author> <title> Billion transistor architectures. </title> <journal> IEEE Computer, </journal> <volume> 30(9), </volume> <month> September </month> <year> 1997. </year>
Reference-contexts: Users are left with little or no information about the cause of these performance problems, and no systematic way to avoid them exists to date. Additionally, since the introduction of RISC, microprocessor complexity has been increasing steadily. If this increase continues to the one-billion transistor microprocessors of the future <ref> [8] </ref>, I fear that the performance problems caused by the complexity of current microprocessors will increase without bound. I propose the following resolution to this dilemma. The Resolution Unless processors are designed without performance anomalies, a method to prevent programmers from observing these anomalies is desired.
Reference: [9] <author> Philip Heidelberger and Stephen Lavenberg. </author> <title> Computer performance evaluation methodology. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33(12):1195-1220, </volume> <month> December </month> <year> 1984. </year>
Reference-contexts: The performance of a computer system is represented using a small number of 16 measurements. The mathematical model then uses these measurements to predict the performance of an application on the given system <ref> [9, 19] </ref>. Toledo [25] presents a model where a small amount of dynamic information is used to produce a more accurate model. Performance modeling techniques are focused on two areas: software design and hardware design.
Reference: [10] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Francisco, CA, </address> <year> 1996. </year> <month> 49 </month>
Reference-contexts: Introduction In early computers, most instructions were executed in the same amount of time, and the overall execution time could be estimated by counting the number of instructions executed <ref> [10] </ref>. As machines became more complex, however, this situation changed quickly. Machines were pipelined, and dependencies between instructions became important. When caches were developed, the layout of instructions and data in memory became important. With the introduction of superscalar processors, evaluating processor performance has become problematic.
Reference: [11] <author> Michael Krech, </author> <month> February </month> <year> 1998. </year> <note> http://www.intelligentfirm.com. </note>
Reference-contexts: Programming for performance is even more difficult if the relationship of work and execution time contains discontinuous performance nonmonotonicities. An example of nonmonotonic performance in the Intel Pentium is presented by Krech <ref> [11] </ref>. Krech's example of nonmonotonicity was exhibited when performing a particular sequence of memory reads. <p> I believe the performance gains presented in this thesis justify the additional effort for implementing a binary restructurer. The algorithms presented in this thesis conceal performance anomalies of the SUN UltraSPARC, and are presumably not applicable to other processors. Krech <ref> [11] </ref> has shown that similar performance anomalies exist on the Pentium, however, and I believe that the design of all modern microprocessors includes performance nonmonotonicities. I hope that my explanation of the performance anomalies on the UltraSPARC sparks the discovery of performance anomalies on other processors.
Reference: [12] <author> James R. Larus and Eric Schnarr. EEL: </author> <title> Machine-independent executable editing. </title> <booktitle> In Proceed ings of the SIGPLAN '95 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 291-300, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The engineering effort required to write a code restructuring tool may outweigh its possible benefits. Several tools have been introduced to aid in the process of developing code restructuring tools <ref> [12, 17, 22] </ref>. These tools provide a high-level abstraction of the binary code which greatly simplifies the process of writing a code restructurer.
Reference: [13] <author> Peter S. Magnusson and Johan Montelius. </author> <title> Performance debugging and tuning using an instruction-set simulator. </title> <type> Technical Report SICS-T-97/02-SE, </type> <institution> Swedish Institute of Computer Science, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: If not enough information can be obtained by running a program directly on the hardware, the architecture is simulated with sufficient detail to obtain the desired information. Architecture simulation ranges from the Instruction Set Architecture (ISA) level <ref> [13] </ref> down to the transistor level [14]. Generally, the more detail the simulation provides, the worse the performance. The Stanford SIMOS project [18] provides an excellent example of the trade-off between detail and speed. <p> First, it is the main approach used to evaluate hardware design decisions during the design phase. Second, simulators for existing architectures are used for performance tuning of programs that run on these machines. At the Swedish Institute of Computer Science, the SimICS system <ref> [13] </ref> has been used to show that architecture simulators, when used for performance tuning, can achieve up to an order of magnitude 15 speedup on some specific applications.
Reference: [14] <author> Giovanni De Micheli. </author> <title> Synthesis and Optimization of Digital Circuits. </title> <publisher> McGraw-Hill, </publisher> <year> 1994. </year>
Reference-contexts: If not enough information can be obtained by running a program directly on the hardware, the architecture is simulated with sufficient detail to obtain the desired information. Architecture simulation ranges from the Instruction Set Architecture (ISA) level [13] down to the transistor level <ref> [14] </ref>. Generally, the more detail the simulation provides, the worse the performance. The Stanford SIMOS project [18] provides an excellent example of the trade-off between detail and speed. This simulator provides several different modes, each with a distinct locale in the trade-off between detail and speed.
Reference: [15] <institution> Sun Microelectronics. </institution> <note> STP1030BGA UltraSPARC-I User's Manual. </note> <institution> Sun Microsystems, </institution> <address> Moun tainview, CA, </address> <year> 1996. </year>
Reference-contexts: Hardware performance monitoring is accomplished through hardware counters. These counters accumulate the number of times that events occur within the architecture. The countable events include cache-misses, branch mispredictions, decoded instructions, retired instructions, and other events that might indicate the hardware is not performing optimally <ref> [15] </ref>. Hardware counters have been around for several years [21, 26]. Today, almost all mainstream microprocessors implement hardware counters. Additionally, many vendors have either developed special software to use the counters [2, 3] or have integrated their use into their existing profiling packages [1, 28]. <p> Each of these anomalies was caused by a specific feature of the architecture: next field predictors, fetching logic, grouping logic, and branch prediction logic. I created a set of microbenchmarks for each feature. I use information from the UltraSPARC User's Manual <ref> [15] </ref>, and from the execution times of these microbenchmarks to identify the cause of each anomaly. foo: retl nop .LL7: call foo,0 add %l0,-1,%l0 bg .LL7 a microbenchmark. The example from Chapter 1 is used to show the process of creating microbenchmarks. <p> The next field contains the index of the I-cache line and the associativity number (or way) of the I-cache line that should be fetched next <ref> [15, p. 258] </ref>. The 2-cycle stall caused when the NFP is mispredicted produces the most pronounced performance anomaly. <p> equal to 20, 24, or 28, then three, two or one instruction (s) 27 A br L2: ! 32-byte aligned nop01 nop03 nop05 B br L3: ! 32-byte aligned nop08 nop10 nop12 C L3: cmp %l0,0 bg L1: !32-byte aligned add %l0,-1,%l0 respectively will be added to the instruction buffer" <ref> [15] </ref>, rather than four. I define an execution group to be a group of up to 4 consecutive instructions that can be scheduled for execution during a single cycle. Execution groups that cross I-cache (32-byte) boundaries expose the fetching limitation. <p> Consequently, during each cycle the instructions of one of the execution groups, A, B, or C, must be loaded from the I-cache. The branch target prediction mechanism of the UltraSPARC allows it to predict the target of branches before they are passed to the grouping logic <ref> [15, pg. 13] </ref>. I assume the branches of the code fragment are always predicted correctly, since each iteration of the loop executes in 3 cycles. Realigning the code in the I-cache, we produce the instruction sequence shown in Figure 3-12. <p> Each integer unit can only execute a subset of the integer instructions, however, and each floating-point unit can only execute a subset of the floating-point instructions <ref> [15] </ref>. This design restricts which instructions of an instruction stream can be scheduled for execution during the same cycle. The grouping logic ensures that none of the restrictions are violated when instructions are passed to the execution units. <p> All instructions following the group break are scheduled for execution in later cycles. The grouping logic itself imposes a constraint on the instruction stream, as stated in the UltraSPARC-I User's Manual <ref> [15] </ref>: "UltraSPARC-I can execute up to 4 instructions per cycle. The first 3 instructions in a group occupy slots that are interchangeable with respect to resources. [...] The fourth slot can only be used for PC-based branches or for floating-point instructions." I call this the grouping constraint. <p> This table shows that each iteration of the loop in Figure 3-13 can be executed in 3 cycles, whereas each iteration of the code in Figure 3-14 takes 4 cycles. 3.5 Branch Prediction Logic A 2-bit branch prediction mechanism is used on the UltraSPARC <ref> [15] </ref>. A single 2-bit predictor is associated with every two instructions. The branch prediction logic exhibits a performance anomaly called odd-fetch which is explained in the Ultra-SPARC User's Manual [15]: "When the target of a branch is word one or word three of an I-cache line, and the 4th instruction to <p> code in Figure 3-14 takes 4 cycles. 3.5 Branch Prediction Logic A 2-bit branch prediction mechanism is used on the UltraSPARC <ref> [15] </ref>. A single 2-bit predictor is associated with every two instructions. The branch prediction logic exhibits a performance anomaly called odd-fetch which is explained in the Ultra-SPARC User's Manual [15]: "When the target of a branch is word one or word three of an I-cache line, and the 4th instruction to be fetched is a branch, the branch prediction bits from the wrong pair of instructions are used." 33 nop ! 32-byte aligned L1: cmp %g1, 0 add %g5,1,%g5 sub
Reference: [16] <author> Matt Reilly and John Edmondson. </author> <title> Performance simulation of an Alpha microprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 50-58, </pages> <month> May </month> <year> 1998. </year>
Reference-contexts: There may be multiple outstanding memory references, and the dependencies between instructions affect their execution sequence. These processors are so complex that it is difficult to predict their performance. Because of this complexity, significant time and effort in the design of a processor is spent on performance modeling <ref> [16] </ref>. The effect of individual design decisions on the overall performance is often unclear without tools to aid the designers' intuitions. Therefore, hardware designers use performance models to assist them in making difficult design decisions. Unfortunately, the validity of these models is debatable. <p> Currently, architectural designers spend a significant amount of time and energy to avoid performance bottlenecks, making use of performance modeling techniques to aid them in their design decisions <ref> [6, 7, 16] </ref>. Errors in these models, however, make it difficult to design an architecture for performance. Performance models used by hardware designers are attempts to extract the information that affects performance from the design. Unfortunately, there is no rigorous method for finding errors in these performance models.
Reference: [17] <author> Ted Romer, Geoff Voelker, Dennis Lee, Alec Wolman, Wayne Wong, Hank Levy, Brad Chen, and Brian Bershad. </author> <title> Instrumentation and optimization of Win32/Intel executables using Etch. </title> <booktitle> In Proceedings of the USENIX Windows NT Workshop, </booktitle> <pages> pages 1-7, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: Code restructuring is useful in circumstances where the original source code is not readily available. Often, older binaries are restructured to perform better on a new implementation of the same ISA <ref> [17] </ref>. For example, binaries compiled for the Pentium may be restructured to be optimized for the Pentium Pro [20]. Analogously, restructuring can be used to reschedule executables to conceal performance anomalies. A disadvantage of code restructurers is that their construction involves significant implementation effort. <p> The engineering effort required to write a code restructuring tool may outweigh its possible benefits. Several tools have been introduced to aid in the process of developing code restructuring tools <ref> [12, 17, 22] </ref>. These tools provide a high-level abstraction of the binary code which greatly simplifies the process of writing a code restructurer.
Reference: [18] <author> Mendel Rosenblum, Stephen A. Herrod, Emmett Witchel, and Anoop Gupta. </author> <title> Complete com puter simulation: The SimOS approach. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <month> Fall </month> <year> 1995. </year>
Reference-contexts: Architecture simulation ranges from the Instruction Set Architecture (ISA) level [13] down to the transistor level [14]. Generally, the more detail the simulation provides, the worse the performance. The Stanford SIMOS project <ref> [18] </ref> provides an excellent example of the trade-off between detail and speed. This simulator provides several different modes, each with a distinct locale in the trade-off between detail and speed.
Reference: [19] <author> Rafael Saavedra-Barrera, Alan Smith, and Eugene Miya. </author> <title> Machine characterization based on an abstract high-level language machine. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1659-1679, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The performance of a computer system is represented using a small number of 16 measurements. The mathematical model then uses these measurements to predict the performance of an application on the given system <ref> [9, 19] </ref>. Toledo [25] presents a model where a small amount of dynamic information is used to produce a more accurate model. Performance modeling techniques are focused on two areas: software design and hardware design.
Reference: [20] <author> Eric Schnarr and James R. Larus. </author> <title> Instruction scheduling and executable editing. </title> <booktitle> In Proceedings of the Workshop on Compiler Support for System Software, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Code restructuring is useful in circumstances where the original source code is not readily available. Often, older binaries are restructured to perform better on a new implementation of the same ISA [17]. For example, binaries compiled for the Pentium may be restructured to be optimized for the Pentium Pro <ref> [20] </ref>. Analogously, restructuring can be used to reschedule executables to conceal performance anomalies. A disadvantage of code restructurers is that their construction involves significant implementation effort. Unfortunately, the process of restructuring code is not as simple as disassembling the code, rearranging instructions, and then reassembling it.
Reference: [21] <author> Ashok Singhal and Aaron J. Goldberg. </author> <title> Architectural support for performance tuning: A case study on the SPARCcenter 2000. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: These counters accumulate the number of times that events occur within the architecture. The countable events include cache-misses, branch mispredictions, decoded instructions, retired instructions, and other events that might indicate the hardware is not performing optimally [15]. Hardware counters have been around for several years <ref> [21, 26] </ref>. Today, almost all mainstream microprocessors implement hardware counters. Additionally, many vendors have either developed special software to use the counters [2, 3] or have integrated their use into their existing profiling packages [1, 28].
Reference: [22] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of SIGPLAN '94, </booktitle> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The engineering effort required to write a code restructuring tool may outweigh its possible benefits. Several tools have been introduced to aid in the process of developing code restructuring tools <ref> [12, 17, 22] </ref>. These tools provide a high-level abstraction of the binary code which greatly simplifies the process of writing a code restructurer.
Reference: [23] <author> Sun Microsystems. </author> <title> The UltraSPARC processor technology white paper. </title> <address> http://www.sun.com/microelectronics/whitepapers/UltraSPARCtechnology/. </address>
Reference-contexts: of assembly code requires access to the assembly code produced by the compiler, but simplifies the implementation effort considerably. 18 Chapter 3 UltraSPARC Case Study To show that performance anomalies constitute a serious problem on commercial microprocessors, I performed a case study of the UltraSPARC Microprocessor, developed by Sun Microsystems <ref> [23] </ref>. The UltraSPARC is an implementation of the SPARC-V9 ISA 1 [27]. Through this study, I identified four performance anomalies caused by the design of the architecture.
Reference: [24] <institution> The Standard Performance Evaluation Corporation. </institution> <note> http://www.specbench.org/osg/spec95. 50 </note>
Reference-contexts: The algorithms for the restructuring of assembly code, are the second contribution of this thesis. In Chapter 4, I show how they were used to obtain speedups of up to 9% on the SPECint benchmarks <ref> [24] </ref>. Finally, in Chapter 5, I conclude. 13 14 Chapter 2 Related Work Little research has been focused on the problem of processor performance anomalies. In this Chapter, I discuss five approaches to assessing hardware performance problems. <p> This tool implements the algorithms described in Chapter 3 for concealing performance anomalies. I show that using this tool during the compilation of the SPECint benchmarks <ref> [24] </ref> provides speedups of up 2.2% on average across compilers 1 . I first describe the experimental setup, and then present and discuss the performance gains of restructuring. The Experiment The instruction sequences of the SPECint benchmarks were restructured at the assembly-code level.
Reference: [25] <author> Sivan Avraham Toledo. </author> <title> Quantitative Performance Modeling of Scientific Computations. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: I employ an iterative approach that uses the data derived from microbenchmarks in conjunction with information provided by the microprocessor vendor to create a model of the machine. In contrast to the performance models created in the past <ref> [25] </ref>, I create a very detailed model of particular aspects of the architecture. This model allows me to focus specifically on the architectural features which cause performance anomalies. I attempt to verify the accuracy of my model through the use of microbenchmarks. <p> The performance of a computer system is represented using a small number of 16 measurements. The mathematical model then uses these measurements to predict the performance of an application on the given system [9, 19]. Toledo <ref> [25] </ref> presents a model where a small amount of dynamic information is used to produce a more accurate model. Performance modeling techniques are focused on two areas: software design and hardware design. Performance models help a programmer discover what parts of the code are causing the hardware to stall.
Reference: [26] <author> G. Michael Uhler, Debra Bernstein, Larry L. Biro, John F. Brown III, John H. Edmondson, Jeffrey D. Pickholtz, and Rebecca L. Stamm. </author> <title> The NVAX and NVAX+ high-performance VAX microprocessors. </title> <journal> Digital Technical Journal, </journal> <volume> 4(3) </volume> <pages> 1-19, </pages> <month> summer </month> <year> 1992. </year>
Reference-contexts: These counters accumulate the number of times that events occur within the architecture. The countable events include cache-misses, branch mispredictions, decoded instructions, retired instructions, and other events that might indicate the hardware is not performing optimally [15]. Hardware counters have been around for several years <ref> [21, 26] </ref>. Today, almost all mainstream microprocessors implement hardware counters. Additionally, many vendors have either developed special software to use the counters [2, 3] or have integrated their use into their existing profiling packages [1, 28].
Reference: [27] <author> David L. Weaver and Tom Germond. </author> <title> The SPARC Architecture Manual. </title> <publisher> PTR Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1994. </year>
Reference-contexts: The UltraSPARC is an implementation of the SPARC-V9 ISA 1 <ref> [27] </ref>. Through this study, I identified four performance anomalies caused by the design of the architecture. Each of these anomalies was caused by a specific feature of the architecture: next field predictors, fetching logic, grouping logic, and branch prediction logic. I created a set of microbenchmarks for each feature.
Reference: [28] <author> Marco Zagha, Brond Larson, Steve Turner, and Marty Itzkowitz. </author> <title> Performance analysis using the MIPS R10000 performance counters. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <month> November </month> <year> 1996. </year> <month> 51 </month>
Reference-contexts: Hardware counters have been around for several years [21, 26]. Today, almost all mainstream microprocessors implement hardware counters. Additionally, many vendors have either developed special software to use the counters [2, 3] or have integrated their use into their existing profiling packages <ref> [1, 28] </ref>. Some vendors do not distribute commercial packages, but use proprietary software for tuning benchmarks and commercial software packages.
References-found: 28

