URL: http://www.cs.helsinki.fi/research/pmdm/datamining/pubs/vldb96.ps.gz
Refering-URL: http://www.cs.helsinki.fi/research/pmdm/publications/
Root-URL: 
Email: Hannu.Toivonen@Helsinki.FI  
Title: Sampling Large Databases for Association Rules  
Author: Hannu Toivonen 
Address: FIN-00014 University of Helsinki, Finland  
Affiliation: University of Helsinki, Department of Computer Science  
Abstract: Discovery of association rules is an important database mining problem. Current algorithms for finding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very significant for very large databases. We present new algorithms that reduce the database activity considerably. The idea is to pick a random sample, to find using this sample all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and in those rare cases where our sampling method does not produce all association rules, the missing rules can be found in a second pass. Our experiments show that the proposed algorithms can find association rules very efficiently in only one database pass.
Abstract-found: 1
Intro-found: 1
Reference: [AIS93] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of ACM SIGMOD Conference on Management of Data (SIGMOD'93), </booktitle> <pages> pages 207 - 216, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For instance, supermarkets store electronic copies of millions of receipts, and banks and credit card companies maintain extensive transaction histories. The goal in database mining is to analyze these large data sets and to discover patterns or regularities that are useful for decision support. Discovery of association rules <ref> [AIS93] </ref> is an interesting subfield of database mining. The motivation for searching association rules has come from the desire to analyze large amounts of supermarket basket data. <p> The discovery of association rules can be divided into two phases <ref> [AIS93] </ref>. First, discover all frequent itemsets X R. Then, for each frequent X, test for all non-empty subsets Y X if the rule X n Y ) Y holds with sufficient confidence. <p> For all but the smallest m, efficient methods for locating the frequent sets are needed. For the rest of this article, we consider the task of discovering all frequent sets. 3 Previous Work on Association Rules Since the introduction of the problem of mining association rules <ref> [AIS93] </ref> several generate-and-test type of algorithms have been proposed for the task of discovering frequent sets. An efficient breadth-first or level-wise method for generating candidate sets, i.e., potentially frequent sets, has been presented in [AS94, MTV94, AMS + 96]. <p> An efficient breadth-first or level-wise method for generating candidate sets, i.e., potentially frequent sets, has been presented in [AS94, MTV94, AMS + 96]. This method|also called Apriori|is the core of all known algorithms except the original one <ref> [AIS93] </ref> and its variation for SQL [HS93], which have been shown to be inferior to the level-wise method [AS94, MTV94, AMS + 96]. An alternative strategy for the database pass, using inverted structures and a general purpose DBMS, has been considered in [HKMT95].
Reference: [AMS + 96] <author> Rakesh Agrawal, Heikki Mannila, Ra-makrishnan Srikant, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Fast discovery of association rules. </title> <editor> In Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307 - 328. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: An efficient breadth-first or level-wise method for generating candidate sets, i.e., potentially frequent sets, has been presented in <ref> [AS94, MTV94, AMS + 96] </ref>. This method|also called Apriori|is the core of all known algorithms except the original one [AIS93] and its variation for SQL [HS93], which have been shown to be inferior to the level-wise method [AS94, MTV94, AMS + 96]. <p> method for generating candidate sets, i.e., potentially frequent sets, has been presented in <ref> [AS94, MTV94, AMS + 96] </ref>. This method|also called Apriori|is the core of all known algorithms except the original one [AIS93] and its variation for SQL [HS93], which have been shown to be inferior to the level-wise method [AS94, MTV94, AMS + 96]. An alternative strategy for the database pass, using inverted structures and a general purpose DBMS, has been considered in [HKMT95]. The most efficient algorithm so far, in particular in terms of database operations, is Partition [SON95]. <p> A connectionist approach to mining rules is presented in [LSL95]. 3.1 Level-Wise Algorithms Algorithms for discovering frequent sets are based on the observation that if a set is not frequent then its supersets can not be frequent. All current algorithms <ref> [MTV94, AS94, HKMT95, HF95, PCY95, SA95, AMS + 96] </ref> start on level 1 by evaluating the frequencies of singleton itemsets. On level k, candidate itemsets X of size k are generated such that all subsets of X are frequent. <p> We first consider how accurate the frequencies computed from a random sample are. As has been noted before, samples of reasonable size provide good approximations for frequent sets <ref> [MTV94, AMS + 96] </ref>. Related work on using a sample for approximately verifying the truth of sentences of tuple calculus is considered in [KM94]. 5.1 Accuracy and Sample Size We consider the absolute error of the estimated frequency. <p> We present two new variants of algorithms using sampling and give experimental results. 6.1 Test Organization We used three synthetic data sets from [AS94] in our tests. These databases model supermarket basket data, and they have been used as benchmarks for several association rule algorithms <ref> [AS94, HKMT95, PCY95, SON95, AMS + 96] </ref>. The central properties of the data sets are the following. There are jRj = 1; 000 attributes, and the average number T of attributes per row is 5, 10, and 20. The number jrj of rows is approximately 100; 000.
Reference: [AS92] <author> Noga Alon and Joel H. Spencer. </author> <title> The Probabilistic Method. </title> <publisher> John Wiley Inc., </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Proof. We have P r [e (X; s) &gt; "] = P r [jfr (X; s) fr (X)j jsj &gt; " jsj]: The Chernoff bounds <ref> [AS92] </ref> give an upper bound 2e 2 (" jsj) 2 =jsj = ffi for the probability. Table 1 gives values for the sufficient sample size jsj, for " = 0:01; 0:001 and ffi = 0:01; 0:001; 0:0001. With the tolerable error " around 0.01, samples of a reasonable size suffice.
Reference: [AS94] <author> Rakesh Agrawal and Ramakrishnan Srik-ant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In Proceedings of the Twentieth International Conference on Very Large Data Bases (VLDB'94), </booktitle> <pages> pages 487 - 499, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: An efficient breadth-first or level-wise method for generating candidate sets, i.e., potentially frequent sets, has been presented in <ref> [AS94, MTV94, AMS + 96] </ref>. This method|also called Apriori|is the core of all known algorithms except the original one [AIS93] and its variation for SQL [HS93], which have been shown to be inferior to the level-wise method [AS94, MTV94, AMS + 96]. <p> method for generating candidate sets, i.e., potentially frequent sets, has been presented in <ref> [AS94, MTV94, AMS + 96] </ref>. This method|also called Apriori|is the core of all known algorithms except the original one [AIS93] and its variation for SQL [HS93], which have been shown to be inferior to the level-wise method [AS94, MTV94, AMS + 96]. An alternative strategy for the database pass, using inverted structures and a general purpose DBMS, has been considered in [HKMT95]. The most efficient algorithm so far, in particular in terms of database operations, is Partition [SON95]. <p> A connectionist approach to mining rules is presented in [LSL95]. 3.1 Level-Wise Algorithms Algorithms for discovering frequent sets are based on the observation that if a set is not frequent then its supersets can not be frequent. All current algorithms <ref> [MTV94, AS94, HKMT95, HF95, PCY95, SA95, AMS + 96] </ref> start on level 1 by evaluating the frequencies of singleton itemsets. On level k, candidate itemsets X of size k are generated such that all subsets of X are frequent. <p> We present two new variants of algorithms using sampling and give experimental results. 6.1 Test Organization We used three synthetic data sets from <ref> [AS94] </ref> in our tests. These databases model supermarket basket data, and they have been used as benchmarks for several association rule algorithms [AS94, HKMT95, PCY95, SON95, AMS + 96]. The central properties of the data sets are the following. <p> We present two new variants of algorithms using sampling and give experimental results. 6.1 Test Organization We used three synthetic data sets from [AS94] in our tests. These databases model supermarket basket data, and they have been used as benchmarks for several association rule algorithms <ref> [AS94, HKMT95, PCY95, SON95, AMS + 96] </ref>. The central properties of the data sets are the following. There are jRj = 1; 000 attributes, and the average number T of attributes per row is 5, 10, and 20. The number jrj of rows is approximately 100; 000. <p> The number jrj of rows is approximately 100; 000. The average size I of maximal frequent sets is 2, 4, and 6. Table 2 summarizes the parameters for the data sets; see <ref> [AS94] </ref> for details of the data generation.
Reference: [FPSSU96] <editor> Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy, editors. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Database mining, or knowledge discovery in databases (KDD), has in the recent years attracted a lot of interest in the database community (for overviews, see <ref> [FPSSU96, PSF91] </ref>).
Reference: [HF95] <author> Jiawei Han and Yongjian Fu. </author> <title> Discovery of multiple-level association rules from large databases. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 420 - 431, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: The most efficient algorithm so far, in particular in terms of database operations, is Partition [SON95]. We review the level-wise method and the Partition algorithm below. Page 2 Other work related to association rules includes the problem of mining rules with generalizations <ref> [HF95, HKMT95, SA95] </ref>, management of large amounts of discovered association rules [KMR + 94, TKR + 95], and a theoretical analysis of an algorithm for a class of KDD problems including the discovery of frequent sets [MT96]. <p> A connectionist approach to mining rules is presented in [LSL95]. 3.1 Level-Wise Algorithms Algorithms for discovering frequent sets are based on the observation that if a set is not frequent then its supersets can not be frequent. All current algorithms <ref> [MTV94, AS94, HKMT95, HF95, PCY95, SA95, AMS + 96] </ref> start on level 1 by evaluating the frequencies of singleton itemsets. On level k, candidate itemsets X of size k are generated such that all subsets of X are frequent.
Reference: [HKMT95] <author> Marcel Holsheimer, Martin Kersten, Heikki Mannila, and Hannu Toivonen. </author> <title> A perspective on databases and data mining. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD'95), </booktitle> <pages> pages 150 - 155, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: An alternative strategy for the database pass, using inverted structures and a general purpose DBMS, has been considered in <ref> [HKMT95] </ref>. The most efficient algorithm so far, in particular in terms of database operations, is Partition [SON95]. We review the level-wise method and the Partition algorithm below. <p> The most efficient algorithm so far, in particular in terms of database operations, is Partition [SON95]. We review the level-wise method and the Partition algorithm below. Page 2 Other work related to association rules includes the problem of mining rules with generalizations <ref> [HF95, HKMT95, SA95] </ref>, management of large amounts of discovered association rules [KMR + 94, TKR + 95], and a theoretical analysis of an algorithm for a class of KDD problems including the discovery of frequent sets [MT96]. <p> A connectionist approach to mining rules is presented in [LSL95]. 3.1 Level-Wise Algorithms Algorithms for discovering frequent sets are based on the observation that if a set is not frequent then its supersets can not be frequent. All current algorithms <ref> [MTV94, AS94, HKMT95, HF95, PCY95, SA95, AMS + 96] </ref> start on level 1 by evaluating the frequencies of singleton itemsets. On level k, candidate itemsets X of size k are generated such that all subsets of X are frequent. <p> We present two new variants of algorithms using sampling and give experimental results. 6.1 Test Organization We used three synthetic data sets from [AS94] in our tests. These databases model supermarket basket data, and they have been used as benchmarks for several association rule algorithms <ref> [AS94, HKMT95, PCY95, SON95, AMS + 96] </ref>. The central properties of the data sets are the following. There are jRj = 1; 000 attributes, and the average number T of attributes per row is 5, 10, and 20. The number jrj of rows is approximately 100; 000.
Reference: [HS92] <author> Peter J. Haas and Arun N. Swami. </author> <title> Sequential sampling procedures for query size estimation. </title> <booktitle> In Proceedings of ACM SIGMOD Conference on Management of Data (SIG-MOD'92), </booktitle> <pages> pages 341 - 350, </pages> <address> San Diego, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: For the design and analysis of sampling methods see, e.g, [OR89]. The related problem of sampling for query estimation is considered in more detail in <ref> [HS92] </ref>. An alternative for randomly drawing each row in separation is, of course, to draw whole blocks of rows to the sample. Depending on how randomly the rows have been assigned to the blocks, this method can give very good or very bad results.
Reference: [HS93] <author> Maurice Houtsma and Arun Swami. </author> <title> Set-oriented mining of association rules. </title> <type> Research Report RJ 9567, </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, California, </address> <month> Octo-ber </month> <year> 1993. </year>
Reference-contexts: An efficient breadth-first or level-wise method for generating candidate sets, i.e., potentially frequent sets, has been presented in [AS94, MTV94, AMS + 96]. This method|also called Apriori|is the core of all known algorithms except the original one [AIS93] and its variation for SQL <ref> [HS93] </ref>, which have been shown to be inferior to the level-wise method [AS94, MTV94, AMS + 96]. An alternative strategy for the database pass, using inverted structures and a general purpose DBMS, has been considered in [HKMT95].
Reference: [KM94] <author> Jyrki Kivinen and Heikki Mannila. </author> <title> The power of sampling in knowledge discovery. </title> <booktitle> In Proceedings of the Thirteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS'94), </booktitle> <pages> pages 77 - 85, </pages> <address> Minneapolis, MN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: As has been noted before, samples of reasonable size provide good approximations for frequent sets [MTV94, AMS + 96]. Related work on using a sample for approximately verifying the truth of sentences of tuple calculus is considered in <ref> [KM94] </ref>. 5.1 Accuracy and Sample Size We consider the absolute error of the estimated frequency.
Reference: [KMR + 94] <author> Mika Klemettinen, Heikki Mannila, Pirjo Ronkainen, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> In Proceedings of the Third International Conference on Information and Knowledge Management (CIKM'94), </booktitle> <pages> pages 401 - 407, </pages> <address> Gaithersburg, MD, </address> <month> November </month> <year> 1994. </year> <note> ACM. Page 11 </note>
Reference-contexts: We review the level-wise method and the Partition algorithm below. Page 2 Other work related to association rules includes the problem of mining rules with generalizations [HF95, HKMT95, SA95], management of large amounts of discovered association rules <ref> [KMR + 94, TKR + 95] </ref>, and a theoretical analysis of an algorithm for a class of KDD problems including the discovery of frequent sets [MT96].
Reference: [LSL95] <author> Hongjun Lu, Rudy Setiono, and Huan Liu. Neurorule: </author> <title> A connectionist approach to data mining. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 478 - 489, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: A connectionist approach to mining rules is presented in <ref> [LSL95] </ref>. 3.1 Level-Wise Algorithms Algorithms for discovering frequent sets are based on the observation that if a set is not frequent then its supersets can not be frequent.
Reference: [MT96] <author> Heikki Mannila and Hannu Toivonen. </author> <title> On an algorithm for finding all interesting sentences. </title> <booktitle> In Cybernetics and Systems, Volume II, The Thirteenth European Meeting on Cybernetics and Systems Research, </booktitle> <pages> pages 973 - 978, </pages> <address> Vienna, Austria, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Other work related to association rules includes the problem of mining rules with generalizations [HF95, HKMT95, SA95], management of large amounts of discovered association rules [KMR + 94, TKR + 95], and a theoretical analysis of an algorithm for a class of KDD problems including the discovery of frequent sets <ref> [MT96] </ref>. A connectionist approach to mining rules is presented in [LSL95]. 3.1 Level-Wise Algorithms Algorithms for discovering frequent sets are based on the observation that if a set is not frequent then its supersets can not be frequent. <p> Given a collection S P (R) of sets, closed with respect to the set inclusion relation, the negative border Bd (S) of S consists of the minimal itemsets X R not in S <ref> [MT96] </ref>. The collection of all frequent sets is always closed with respect to set inclusion. <p> Of particular importance is the fact that the negative border needs to be evaluated, in order to be sure that no frequent sets are missed <ref> [MT96] </ref>. 4.2 Frequent Set Discovery Using Sampling We now apply the concept of negative border to using sampling for finding frequent sets.
Reference: [MTV94] <author> Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Efficient algorithms for discovering association rules. In Knowledge Discovery in Databases, </title> <booktitle> Papers from the 1994 AAAI Workshop (KDD'94), </booktitle> <pages> pages 181 - 192, </pages> <address> Seattle, Washington, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Association rules have been used to discover regularities in other databases as well: university course enrollment data has been analyzed to find combinations of courses taken by the same students, and alarms that occur close to each other in time have been searched in telecommunication alarm data <ref> [MTV94] </ref>. The size of the data collection has an essential role in database mining. Large data sets are necessary for reliable results; unfortunately, however, the efficiency of the mining algorithms depends heavily on the database. <p> An efficient breadth-first or level-wise method for generating candidate sets, i.e., potentially frequent sets, has been presented in <ref> [AS94, MTV94, AMS + 96] </ref>. This method|also called Apriori|is the core of all known algorithms except the original one [AIS93] and its variation for SQL [HS93], which have been shown to be inferior to the level-wise method [AS94, MTV94, AMS + 96]. <p> method for generating candidate sets, i.e., potentially frequent sets, has been presented in <ref> [AS94, MTV94, AMS + 96] </ref>. This method|also called Apriori|is the core of all known algorithms except the original one [AIS93] and its variation for SQL [HS93], which have been shown to be inferior to the level-wise method [AS94, MTV94, AMS + 96]. An alternative strategy for the database pass, using inverted structures and a general purpose DBMS, has been considered in [HKMT95]. The most efficient algorithm so far, in particular in terms of database operations, is Partition [SON95]. <p> A connectionist approach to mining rules is presented in [LSL95]. 3.1 Level-Wise Algorithms Algorithms for discovering frequent sets are based on the observation that if a set is not frequent then its supersets can not be frequent. All current algorithms <ref> [MTV94, AS94, HKMT95, HF95, PCY95, SA95, AMS + 96] </ref> start on level 1 by evaluating the frequencies of singleton itemsets. On level k, candidate itemsets X of size k are generated such that all subsets of X are frequent. <p> We first consider how accurate the frequencies computed from a random sample are. As has been noted before, samples of reasonable size provide good approximations for frequent sets <ref> [MTV94, AMS + 96] </ref>. Related work on using a sample for approximately verifying the truth of sentences of tuple calculus is considered in [KM94]. 5.1 Accuracy and Sample Size We consider the absolute error of the estimated frequency.
Reference: [OR89] <author> Frank Olken and Doron Rotem. </author> <title> Random sampling from B + trees. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Very Large Data Bases (VLDB'89), </booktitle> <pages> pages 269 - 277, </pages> <address> Amsterdam, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: For instance, if the database has 10 million rows, a disk block contains on average 100 rows, and the sample size is 20,000, then the sampling phase could read up to 20 % of the database. For the design and analysis of sampling methods see, e.g, <ref> [OR89] </ref>. The related problem of sampling for query estimation is considered in more detail in [HS92]. An alternative for randomly drawing each row in separation is, of course, to draw whole blocks of rows to the sample.
Reference: [PCY95] <author> Jong Soo Park, Ming-Syan Chen, and Philip S. Yu. </author> <title> An effective hash-based algorithm for mining association rules. </title> <booktitle> In Proceedings of ACM SIGMOD Conference on Management of Data (SIGMOD'95), </booktitle> <pages> pages 175 - 186, </pages> <address> San Jose, California, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: A connectionist approach to mining rules is presented in [LSL95]. 3.1 Level-Wise Algorithms Algorithms for discovering frequent sets are based on the observation that if a set is not frequent then its supersets can not be frequent. All current algorithms <ref> [MTV94, AS94, HKMT95, HF95, PCY95, SA95, AMS + 96] </ref> start on level 1 by evaluating the frequencies of singleton itemsets. On level k, candidate itemsets X of size k are generated such that all subsets of X are frequent. <p> We present two new variants of algorithms using sampling and give experimental results. 6.1 Test Organization We used three synthetic data sets from [AS94] in our tests. These databases model supermarket basket data, and they have been used as benchmarks for several association rule algorithms <ref> [AS94, HKMT95, PCY95, SON95, AMS + 96] </ref>. The central properties of the data sets are the following. There are jRj = 1; 000 attributes, and the average number T of attributes per row is 5, 10, and 20. The number jrj of rows is approximately 100; 000.
Reference: [PSF91] <editor> Gregory Piatetsky-Shapiro and William J. Frawley, editors. </editor> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Database mining, or knowledge discovery in databases (KDD), has in the recent years attracted a lot of interest in the database community (for overviews, see <ref> [FPSSU96, PSF91] </ref>).
Reference: [SA95] <author> Ramakrishnan Srikant and Rakesh Agrawal. </author> <title> Mining generalized association rules. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 407 - 419, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: The most efficient algorithm so far, in particular in terms of database operations, is Partition [SON95]. We review the level-wise method and the Partition algorithm below. Page 2 Other work related to association rules includes the problem of mining rules with generalizations <ref> [HF95, HKMT95, SA95] </ref>, management of large amounts of discovered association rules [KMR + 94, TKR + 95], and a theoretical analysis of an algorithm for a class of KDD problems including the discovery of frequent sets [MT96]. <p> A connectionist approach to mining rules is presented in [LSL95]. 3.1 Level-Wise Algorithms Algorithms for discovering frequent sets are based on the observation that if a set is not frequent then its supersets can not be frequent. All current algorithms <ref> [MTV94, AS94, HKMT95, HF95, PCY95, SA95, AMS + 96] </ref> start on level 1 by evaluating the frequencies of singleton itemsets. On level k, candidate itemsets X of size k are generated such that all subsets of X are frequent.
Reference: [SON95] <author> Ashok Savasere, Edward Omiecinski, and Shamkant Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 432 - 444, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: Association rule algorithms require multiple passes over the whole database, and subsequently the database size is by far the most influential factor of the execution time for very large databases. Recent research has managed to reduce the disk I/O activity to two full scans over the database <ref> [SON95] </ref>. We present algorithms that make only one full pass over the database. The idea is to pick a random sample, use it to determine all association rules that probably hold in the whole database, and then to verify the results with the rest of the database. <p> An alternative strategy for the database pass, using inverted structures and a general purpose DBMS, has been considered in [HKMT95]. The most efficient algorithm so far, in particular in terms of database operations, is Partition <ref> [SON95] </ref>. We review the level-wise method and the Partition algorithm below. <p> are thus K or K + 1 passes over the database, where K is the size of the largest frequent set. 2 Sometimes, if there are only few candidates in the last iterations, candidates can be generated and tested for several levels at once. 3.2 Partition Algorithm The Partition algorithm <ref> [SON95] </ref> reduces the database activity: it computes all frequent sets in two passes over the database. The algorithm works also in the level-wise manner, but the idea is to partition the database to sections small enough to be handled in main memory. <p> We present two new variants of algorithms using sampling and give experimental results. 6.1 Test Organization We used three synthetic data sets from [AS94] in our tests. These databases model supermarket basket data, and they have been used as benchmarks for several association rule algorithms <ref> [AS94, HKMT95, PCY95, SON95, AMS + 96] </ref>. The central properties of the data sets are the following. There are jRj = 1; 000 attributes, and the average number T of attributes per row is 5, 10, and 20. The number jrj of rows is approximately 100; 000.
Reference: [TKR + 95] <author> Hannu Toivonen, Mika Klemettinen, Pirjo Ronkainen, Kimmo Hatonen, and Heikki Mannila. </author> <title> Pruning and grouping of discovered association rules. </title> <booktitle> In Workshop Notes of the ECML-95 Workshop on Statistics, Machine Learning, and Knowledge Discovery in Databases, </booktitle> <pages> pages 47 - 52, </pages> <address> Heraklion, Crete, Greece, </address> <month> April </month> <year> 1995. </year> <pages> Page 12 </pages>
Reference-contexts: We review the level-wise method and the Partition algorithm below. Page 2 Other work related to association rules includes the problem of mining rules with generalizations [HF95, HKMT95, SA95], management of large amounts of discovered association rules <ref> [KMR + 94, TKR + 95] </ref>, and a theoretical analysis of an algorithm for a class of KDD problems including the discovery of frequent sets [MT96].
References-found: 20

