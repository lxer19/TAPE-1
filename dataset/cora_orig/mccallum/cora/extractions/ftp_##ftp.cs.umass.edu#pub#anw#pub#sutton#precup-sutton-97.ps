URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/precup-sutton-97.ps
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: dprecup@cs.umass.edu  rich@cs.umass.edu  
Title: Exponentiated Gradient Methods for Reinforcement Learning  
Author: Doina Precup Richard S. Sutton 
Address: Amherst, MA 01003  Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  Department of Computer Science University of Massachusetts  
Abstract: This paper introduces and evaluates a natural extension of linear exponentiated gradient methods that makes them applicable to reinforcement learning problems. Just as these methods speed up supervised learning, we find that they can also increase the efficiency of reinforcement learning. Comparisons are made with conventional reinforcement learning methods on two test problems using CMAC function approximators and replacing traces. On a small prediction task, exponentiated gradient methods showed no improvement, but on a larger control task (Mountain Car) they improved the learning speed by approximately 25%. A more detailed analysis suggests that the difference may be due to the distribution of irrelevant features.
Abstract-found: 1
Intro-found: 1
Reference: <author> Albus, J. S. </author> <year> (1981). </year> <title> Brain, </title> <journal> behaviour and robotics, </journal> <volume> chapter 6, </volume> <pages> pp. 139-176. </pages> <publisher> Byte Books. </publisher>
Reference: <author> Kivinen, J., & Warmuth, M. </author> <title> K (1994). Exponentiated gradient versus gradient descent for linear predictors, </title> <type> Technical Report UCSC-CRL-94-16, </type> <institution> CSE Dept., University of California, Santa Cruz. </institution>
Reference: <author> Lewis, D. D., Schapire, R. E., Callan, J. P., & Papka, R. </author> <year> (1996). </year> <title> Training algorithms for linear text classifiers. </title> <booktitle> Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. </booktitle> <pages> 298-315). </pages> <address> Zurich, Switzerland. </address>
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 285-318. </pages>
Reference: <author> Miller, W. T., Glanz, F. H., & Kraft, L. G. </author> <year> (1990). </year> <title> CMAC: An associative neural network alternative to backpropagation. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78, </volume> <pages> 1561-1567. </pages>
Reference: <author> Papka, R., Callan, J. P., & Barto, A. G. </author> <year> (1996). </year> <title> Text-based information retrieval using exponentiated gradient descent, </title> <note> to appear in Neural Information Processing Systems, </note> <year> 1996. </year>
Reference: <author> Precup, D., & Sutton, R. S. </author> <year> (1996). </year> <title> Empirical comparison of gradient descent and exponentiated gradient descent in supervised and reinforcement learning, </title> <type> Technical Report 96-70, </type> <institution> CS Dept., University of Massachusetts, Amherst. </institution>
Reference-contexts: The normalization linearly rescales all weights so that they sum to a constant: 2flN X w t (i) = W: Normalization introduces a supplementary parameter, W , which needs to be tuned. Our initial experiments found normalization to be sometimes counterproductive and never very helpful <ref> (Precup & Sutton, 1996) </ref>. We did not use it in the experiments presented here. 3 EXTENSION TO REINFORCEMENT LEARNING It is natural to extend the EG idea to reinforcement learning in two steps.
Reference: <author> Rummery, G. A., & Niranjan, M. </author> <year> (1994). </year> <title> On-line Q-learning using connectionist systems, </title> <institution> Technical report CUED/F-INFENG/TR 166 Cambridge University Engineering Department. </institution>
Reference: <author> Singh, S.P., & Sutton, R.S. </author> <year> (1996). </year> <title> Reinforcement learning with replacing eligibility traces. </title> <booktitle> Machine Learning , 22, </booktitle> <pages> 123-158. </pages>
Reference-contexts: In our experiments we found that EG worked much more reliably with replacing traces <ref> (Singh & Sutton, 1996) </ref> than with conventional accumulating traces. Replacing traces are defined by e t (i) = 1 if x t (i) = 1 where e t (i) is the ith component of e t . <p> The software we used was derived from that provided by Mahadevan. We applied the Sarsa and EG-Sarsa algorithms, with replacing traces, as described earlier. The details of mapping the algorithms onto the mountain car task were exactly as described in <ref> (Singh & Sutton, 1996) </ref>. This task has a continuous state space, with two state variables: position and velocity. We used a set of three CMACs, one for each action. Each CMAC had 5 tilings with random offsets.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: The prediction at time t is denoted ^y t and is formed as a linear function of x t and a real-valued parameter vector, w t , just as in the supervised case. The standard learning rule for this problem is linear TD () <ref> (Sutton, 1988) </ref>. It is given by w t+1 := w t + ffffi t e t ; (5) where ffi t is the temporal-difference error at time t: ffi t = r t+1 + flw T and e t is a vector of eligibility traces.
Reference: <author> Sutton, R.S. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> Advances in Neural Information Processing Systems 8 (pp. </booktitle> <address> 1038-1044,). </address> <publisher> MIT Press. </publisher>
Reference-contexts: EG methods have several characteristics that make them appealing for reinforcement learning. First, they are online methods, which is required for reinforcement learning. Second, many reinforcement learning tasks with continuous state spaces have been tackled using sparse coarse coded function approximators <ref> (e.g., Sutton, 1996, Watkins, 1989, Rummery and Ni-ranjan, 1994) </ref>. EG methods may be able to discriminate irrelevant features quickly in this type of input representation. Third, online reinforcement learning is particularly sensitive to the speed of learning, and EG methods can potentially improve it. <p> The normalization linearly rescales all weights so that they sum to a constant: 2flN X w t (i) = W: Normalization introduces a supplementary parameter, W , which needs to be tuned. Our initial experiments found normalization to be sometimes counterproductive and never very helpful <ref> (Precup & Sutton, 1996) </ref>. We did not use it in the experiments presented here. 3 EXTENSION TO REINFORCEMENT LEARNING It is natural to extend the EG idea to reinforcement learning in two steps. <p> In our experiments we found that EG worked much more reliably with replacing traces <ref> (Singh & Sutton, 1996) </ref> than with conventional accumulating traces. Replacing traces are defined by e t (i) = 1 if x t (i) = 1 where e t (i) is the ith component of e t . <p> The software we used was derived from that provided by Mahadevan. We applied the Sarsa and EG-Sarsa algorithms, with replacing traces, as described earlier. The details of mapping the algorithms onto the mountain car task were exactly as described in <ref> (Singh & Sutton, 1996) </ref>. This task has a continuous state space, with two state variables: position and velocity. We used a set of three CMACs, one for each action. Each CMAC had 5 tilings with random offsets.
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning with delayed rewards. </title> <type> Doctoral dissertation, </type> <institution> Psychology Department, Cambridge University. </institution>
References-found: 12

