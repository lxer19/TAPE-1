URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-94-46.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: Estimation of Distributed Parameters by Multiresolution Optimization  
Author: by Koji Amakawa Alex T. Pang Suresh K. Lodha Manfred K. Warmuth Dean 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in  The dissertation of Koji Amakawa is approved:  
Date: December 1994  
Affiliation: University of California Santa Cruz  Computer and Information Sciences  of Graduate Studies and Research  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K. Amakawa and A. Pang. </author> <title> An inverse method of estimating parameter distributions based on a heart muscle model. </title> <booktitle> In Proceedings of Computers in Cardiology 1992, </booktitle> <pages> pages 47-50. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: An example of this type is estimation of the diffusion coefficient distribution of a heart muscle tissue from its electrical activities <ref> [1] </ref>. This dissertation deals with such estimation problems for parameters distributed over a multi-dimensional field. Assumed as a basic property of the problems is that the parameter is continuous almost everywhere in the field. <p> This is fine with most neural networks, because such a function usually has only one input, e.g. the sum of several signals. However, this assumption may not work in the case of parameter estimation, because the variable V (t) j can be a function of two or more inputs <ref> [1] </ref>. Viewing back propagation in a general manner as above eliminates such unnecessary restrictions and allows us to use the technique for general parameter estimation problems. 3.2 When the Parameter is Constant In the previous section, we treated the distributed parameter as independent in each layer.
Reference: [2] <author> H.T. Banks and K. Kunisch. </author> <title> Estimation Techniques for Distributed Parameter Systems. </title> <publisher> Birkhauser, </publisher> <year> 1989. </year>
Reference-contexts: a better r in the vicinity of the present r, and global search methods that explore much wider regions than the vicinity of the present r. 4 1.2 Local Search Methods Among various local optimization techniques, efficient and therefore often used are the conjugate gradient method and the quasi-Newton family <ref> [2, 25, 30, 37] </ref>. The conjugate gradient method may be more suitable if the distributed parameter to be estimated has a large number of elements, because it requires less memory than the quasi-Newton family and yet its convergence speed is comparable as follows.
Reference: [3] <author> M. Bertero and E.R. Pike, </author> <title> editors. Inverse Problems in Scattering and Imaging: </title> <booktitle> Proceedings of a NATO Advanced Research Workshop held at Cape Cod, </booktitle> <address> USA, 14-19 April 1991. Adam Hilger, </address> <year> 1992. </year>
Reference-contexts: In this case, the distributed parameter that represents the interior of the system has to be estimated from certain quantities measured on the exterior. Some examples of this type are image reconstruction problems in ultrasound, light scattering, impedance, diffuse tomography and biomagnetism <ref> [3, 12, 24, 35] </ref>. In another case, one may want to determine the aspects of a system, which are in themselves difficult to measure directly, by inferring these aspects from other more accessible quantities which may actually be in the same physical position.
Reference: [4] <author> C. Chavent and J. Liu. </author> <title> Multiscale parametrization for the estimation of a diffusion coefficient in elliptic and parabolic problems. </title> <booktitle> In Fifth IFAC Symposium on Control of Distributed Parameter Systems, Perpignan, France, </booktitle> <pages> pages 193-202, </pages> <year> 1989. </year>
Reference-contexts: Therefore, a search can be faster with a smaller dimension of the search space. It was found that there were other researchers who conceived the same idea of using multiresolution optimization for parameter estimation <ref> [4, 23] </ref>. They used the Haar wavelet [5, 6, 31] as a multiresolution scheme combined with a quasi-Newton method (the BFGS algorithm) to estimate a distributed parameter of elliptic and parabolic models. It was shown that the multiresolution algorithm performed better than an ordinary single-resolution method.
Reference: [5] <author> C.K. Chui. </author> <title> An Introduction to Wavelets. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Therefore, a search can be faster with a smaller dimension of the search space. It was found that there were other researchers who conceived the same idea of using multiresolution optimization for parameter estimation [4, 23]. They used the Haar wavelet <ref> [5, 6, 31] </ref> as a multiresolution scheme combined with a quasi-Newton method (the BFGS algorithm) to estimate a distributed parameter of elliptic and parabolic models. It was shown that the multiresolution algorithm performed better than an ordinary single-resolution method.
Reference: [6] <author> I. Daubechies. </author> <title> Ten Lectures on Wavelets. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1992. </year>
Reference-contexts: Therefore, a search can be faster with a smaller dimension of the search space. It was found that there were other researchers who conceived the same idea of using multiresolution optimization for parameter estimation [4, 23]. They used the Haar wavelet <ref> [5, 6, 31] </ref> as a multiresolution scheme combined with a quasi-Newton method (the BFGS algorithm) to estimate a distributed parameter of elliptic and parabolic models. It was shown that the multiresolution algorithm performed better than an ordinary single-resolution method. <p> In step 2, R 0;0 ; R 0;1 and R 1;1 are estimated, and so on. In general, step M estimates R 0;0 and the difference information R 0;1 ; ; R M1;1 to reconstruct the parameter distribution in resolution 2 M . 2.4.2 For n-dimensional Field Although Daubechies <ref> [6] </ref> shows construction of the two-dimensional wavelet transform, higher-dimensional wavelet transforms are referred to as simply "analogous". In this section, we define a general n-dimensional Haar wavelet transform "analogous" to her two-dimensional wavelet structure.
Reference: [7] <editor> L. Davis, editor. </editor> <booktitle> The Handbook of Genetic Algorithms. </booktitle> <publisher> Van Nostrand Reinhold, </publisher> <year> 1991. </year>
Reference-contexts: The temperature is initially set high and gradually decreased based on a certain annealing schedule. Accordingly, the probability of accepting uphill moves is initially high and gradually decreased. Genetic algorithms <ref> [7, 8, 10, 11, 43] </ref> also perform global minimization by somewhat random moves, but in a totally different way. The search starts with a population of random chromosomes (candidates of the best estimate of the parameters) and improves the population by creating new offspring by genetic operations.
Reference: [8] <author> L. Davis and M. Steenstrup. </author> <title> Genetic algorithms and simulated annealing: An overview. </title> <editor> In L. Davis, editor, </editor> <title> Genetic Algorithms and Simulated Annealing. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1987. </year>
Reference-contexts: The following two methods, which are the only general global optimization methods to date other than a brute force search, take this approach. Simulated annealing <ref> [8, 20, 39] </ref>, which was invented from analogy to physical annealing in which a material is heated to a high temperature and slowly cooled down until it rests in the most stable phase, allows cost-increasing moves with a probability controlled by the so-called temperature. <p> The temperature is initially set high and gradually decreased based on a certain annealing schedule. Accordingly, the probability of accepting uphill moves is initially high and gradually decreased. Genetic algorithms <ref> [7, 8, 10, 11, 43] </ref> also perform global minimization by somewhat random moves, but in a totally different way. The search starts with a population of random chromosomes (candidates of the best estimate of the parameters) and improves the population by creating new offspring by genetic operations.
Reference: [9] <author> P. DuChateau and D. Zachmann. </author> <title> Applied Partial Differential Equations. </title> <publisher> Harper & Row, </publisher> <year> 1989. </year>
Reference-contexts: To compensate for this, we assume from now on that the values of J i;j at the corners are halved before this equation and those below are used. 4.2 Forward Solution Numerical methods to solve elliptic difference equations such as (4.9) are classified into direct methods and iterative methods <ref> [9, 13, 27, 30, 36] </ref>. Direct methods directly solve the finite difference equations of the matrix form Av = b with respect to the column vector v. A serious problem with direct methods is that a large amount of memory is required.
Reference: [10] <author> D.E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The temperature is initially set high and gradually decreased based on a certain annealing schedule. Accordingly, the probability of accepting uphill moves is initially high and gradually decreased. Genetic algorithms <ref> [7, 8, 10, 11, 43] </ref> also perform global minimization by somewhat random moves, but in a totally different way. The search starts with a population of random chromosomes (candidates of the best estimate of the parameters) and improves the population by creating new offspring by genetic operations.
Reference: [11] <author> J.J. Grefenstette. </author> <title> Optimization of control parameters for genetic algorithms. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 16(1) </volume> <pages> 122-128, </pages> <year> 1986. </year>
Reference-contexts: The temperature is initially set high and gradually decreased based on a certain annealing schedule. Accordingly, the probability of accepting uphill moves is initially high and gradually decreased. Genetic algorithms <ref> [7, 8, 10, 11, 43] </ref> also perform global minimization by somewhat random moves, but in a totally different way. The search starts with a population of random chromosomes (candidates of the best estimate of the parameters) and improves the population by creating new offspring by genetic operations. <p> Similarly, in the case of a genetic algorithm, the population size, i.e. the number of chromosomes, and other control parameters determine the accuracy and efficiency of the estimation <ref> [11] </ref>. Another problem with genetic algorithms is that a vast amount of memory may be needed because many estimates (chromosomes) have to be stored. 1.4 Proposed Method: Multiresolution Local Search This dissertation proposes, develops and tests multiresolution local search methods.
Reference: [12] <author> F.A. Grunbaum. </author> <title> Diffuse tomography: the isotropic case. Inverse Problems, </title> <booktitle> 8 </booktitle> <pages> 409-419, </pages> <year> 1992. </year>
Reference-contexts: In this case, the distributed parameter that represents the interior of the system has to be estimated from certain quantities measured on the exterior. Some examples of this type are image reconstruction problems in ultrasound, light scattering, impedance, diffuse tomography and biomagnetism <ref> [3, 12, 24, 35] </ref>. In another case, one may want to determine the aspects of a system, which are in themselves difficult to measure directly, by inferring these aspects from other more accessible quantities which may actually be in the same physical position.
Reference: [13] <author> C.A. Hall and T.A. Porsching. </author> <title> Numerical Analysis of Partial Differential Equations. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: To compensate for this, we assume from now on that the values of J i;j at the corners are halved before this equation and those below are used. 4.2 Forward Solution Numerical methods to solve elliptic difference equations such as (4.9) are classified into direct methods and iterative methods <ref> [9, 13, 27, 30, 36] </ref>. Direct methods directly solve the finite difference equations of the matrix form Av = b with respect to the column vector v. A serious problem with direct methods is that a large amount of memory is required.
Reference: [14] <author> J. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: the gradient r r H 4.4 Adjust r based on r flt r H 5 Output r 17 Furthermore, it can possibly open up new ways of multiresolution optimization by introducing various types of filters for the gradient. 2.2 The Conjugate Gradient Method This dissertation employs the conjugate gradient method <ref> [14, 25, 30, 37] </ref> for local search. The reason is that this method is suitable for estimating a large number of unknowns because its required storage is smaller than that of the quasi-Newton family, which is another powerful method, and yet its convergence speed is comparable (see Section 1.2). <p> Fortunately, we can utilize the so-called back propagation to calculate the gradient very efficiently. Its computation time is only of the same order of the forward calculation time. 3.1 The Back-Propagation Algorithm The back-propagation algorithm <ref> [14, 15, 33, 32] </ref> is a scheme for solving learning problems of artificial neural networks where a cost function that represents learning errors is to be minimized.
Reference: [15] <author> G. Hinton. </author> <title> Connectionist learning procedures. </title> <type> Technical Report CMU-CS-87-115(version 2), </type> <institution> Carnegie-Mellon University, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: Fortunately, we can utilize the so-called back propagation to calculate the gradient very efficiently. Its computation time is only of the same order of the forward calculation time. 3.1 The Back-Propagation Algorithm The back-propagation algorithm <ref> [14, 15, 33, 32] </ref> is a scheme for solving learning problems of artificial neural networks where a cost function that represents learning errors is to be minimized.
Reference: [16] <author> P. Hua, E.J. Woo, J.G. Webster, and W.J. Tompkins. </author> <title> Improved methods to determine optimal currents in electrical impedance tomography. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <volume> 11(4) </volume> <pages> 488-495, </pages> <year> 1992. </year> <month> 125 </month>
Reference: [17] <author> D. Isaacson. </author> <title> Distinguishability of conductivities by electric current computed tomography. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <volume> MI-5(2):91-95, </volume> <year> 1986. </year>
Reference: [18] <author> R.C. James. </author> <title> Mathematics Dictionary. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> fifth edition, </address> <year> 1992. </year>
Reference-contexts: The sign of the exponent of e can be reversed as the following <ref> [18, 30] </ref>. R k = j=0 i2kj r j = N k=0 N (j = 0; ; N 1) (2.33) The effect is simply that the parameter distribution is looked at in the reverse order as shown below.
Reference: [19] <author> S.M. Kay. </author> <title> Modern Spectral Estimation: Theory and Application. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Any such definition can be used as long as the transform and the inverse transform are self-consistent. The following definitions in which the inverse DFT, instead of the DFT, has the constant 1=N seem to be used in most of the literature, e.g. <ref> [19, 40] </ref>.
Reference: [20] <author> S. Kirkpatrick, C.D. Gelatt Jr., </author> <title> and M.P. Vecchi. Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680, </pages> <year> 1983. </year>
Reference-contexts: The following two methods, which are the only general global optimization methods to date other than a brute force search, take this approach. Simulated annealing <ref> [8, 20, 39] </ref>, which was invented from analogy to physical annealing in which a material is heated to a high temperature and slowly cooled down until it rests in the most stable phase, allows cost-increasing moves with a probability controlled by the so-called temperature.
Reference: [21] <author> P. Kraniauskas. </author> <title> Transforms in Signals and Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Their results were quite promising, although the models tested were only one-dimensional and the number of elements of the distributed parameter was limited to 32. 8 This dissertation develops methods that use the discrete Fourier transform <ref> [21, 30, 40] </ref> as well as the Haar wavelet transform for a multi-dimensional field. <p> Let R = (R 0 ; R 1 ; ; R N1 ) be the Fourier coefficients of the parameter r. One definition of the one-dimensional DFT from r to R is the following <ref> [21] </ref> : 23 1 N1 X r j e i2kj The corresponding inverse DFT, which reconstructs the distributed parameter r from the Fourier coefficients R, is defined by: r j = k=0 i2jk With this set of definitions, the coefficient R 0 for the frequency zero is equal to the average
Reference: [22] <author> G.A. Kyriacou, C.S. Koukourlis, and J.N. Sahalos. </author> <title> A reconstruction algorithm of electrical impedance tomography with optimal configuration of the driven electrodes. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <volume> 12(3) </volume> <pages> 430-438, </pages> <year> 1993. </year>
Reference: [23] <author> Jun Liu. </author> <title> A multiresolution method for distributed parameter estimation. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(2) </volume> <pages> 389-405, </pages> <year> 1993. </year>
Reference-contexts: Therefore, a search can be faster with a smaller dimension of the search space. It was found that there were other researchers who conceived the same idea of using multiresolution optimization for parameter estimation <ref> [4, 23] </ref>. They used the Haar wavelet [5, 6, 31] as a multiresolution scheme combined with a quasi-Newton method (the BFGS algorithm) to estimate a distributed parameter of elliptic and parabolic models. It was shown that the multiresolution algorithm performed better than an ordinary single-resolution method.
Reference: [24] <author> A.K. Louis. </author> <title> Medical imaging: state of the art and future development. Inverse Problems, </title> <booktitle> 8 </booktitle> <pages> 709-738, </pages> <year> 1992. </year>
Reference-contexts: In this case, the distributed parameter that represents the interior of the system has to be estimated from certain quantities measured on the exterior. Some examples of this type are image reconstruction problems in ultrasound, light scattering, impedance, diffuse tomography and biomagnetism <ref> [3, 12, 24, 35] </ref>. In another case, one may want to determine the aspects of a system, which are in themselves difficult to measure directly, by inferring these aspects from other more accessible quantities which may actually be in the same physical position.
Reference: [25] <author> D.G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley Publishing, </publisher> <address> second edition, </address> <year> 1984. </year>
Reference-contexts: a better r in the vicinity of the present r, and global search methods that explore much wider regions than the vicinity of the present r. 4 1.2 Local Search Methods Among various local optimization techniques, efficient and therefore often used are the conjugate gradient method and the quasi-Newton family <ref> [2, 25, 30, 37] </ref>. The conjugate gradient method may be more suitable if the distributed parameter to be estimated has a large number of elements, because it requires less memory than the quasi-Newton family and yet its convergence speed is comparable as follows. <p> If the number of coefficients to be estimated is N L with frequency limit L and the cost function is quadratic with respect to them, it takes O (N L ) local search iterations to converge in the conjugate gradient method <ref> [25, 30] </ref>. <p> the gradient r r H 4.4 Adjust r based on r flt r H 5 Output r 17 Furthermore, it can possibly open up new ways of multiresolution optimization by introducing various types of filters for the gradient. 2.2 The Conjugate Gradient Method This dissertation employs the conjugate gradient method <ref> [14, 25, 30, 37] </ref> for local search. The reason is that this method is suitable for estimating a large number of unknowns because its required storage is smaller than that of the quasi-Newton family, which is another powerful method, and yet its convergence speed is comparable (see Section 1.2).
Reference: [26] <author> T. Murai and Y. Kagawa. </author> <title> Electrical impedance computed tomography based on a finite element method. </title> <journal> IEEE Transactions on Biomedical Engineering, </journal> <volume> BME-32(3):177-184, </volume> <year> 1985. </year>
Reference: [27] <author> S. Nakamura. </author> <title> Applied Numerical Methods with Software. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: To compensate for this, we assume from now on that the values of J i;j at the corners are halved before this equation and those below are used. 4.2 Forward Solution Numerical methods to solve elliptic difference equations such as (4.9) are classified into direct methods and iterative methods <ref> [9, 13, 27, 30, 36] </ref>. Direct methods directly solve the finite difference equations of the matrix form Av = b with respect to the column vector v. A serious problem with direct methods is that a large amount of memory is required. <p> Although there are methods that utilize the fact that the matrix A is a sparse matrix whose elements are non-zero only in a band 2N x + 1 elements wide, their storage requirements are still large, e.g. 77 x N y ) with a typical method for sparse matrices <ref> [27] </ref>. For this reason, we choose an iterative method here that only needs memory of size O (N x N y ). <p> Because of this, the SOR does not have the two-cyclic property of the Jacobi-iterative method. However, this updating scheme also makes it difficult to vectorize/parallelize the SOR. 78 More suitable for massively parallel computers and vectorized supercomputers than the SOR is the Extrapolated Jacobi-iterative (EJ) method <ref> [27] </ref> as follows: V i;j = a C b i;j a E V i+1;j + a W V i1;j + a S V i;j+1 + a N V i;j1 + (1 )V i;j where is the extrapolation parameter (1 &lt; &lt; 2).
Reference: [28] <author> J.C. Newell, D.G. Gisser, and D. Isaacson. </author> <title> An electric current tomograph. </title> <journal> IEEE Transactions on Biomedical Engineering, </journal> <volume> 35(10) </volume> <pages> 828-833, </pages> <year> 1988. </year>
Reference: [29] <author> K. Paulson, W. Lionheart, and M. Pidcock. </author> <title> Optimal experiments in electrical impedance tomography. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <volume> 12(4) </volume> <pages> 681-686, </pages> <year> 1993. </year>
Reference-contexts: It might be because the image reconstruction of electrical impedance tomography is such a highly nonlinear, ill-posed problem <ref> [29] </ref> that even an excellent estimation algorithm has trouble with it. For example, two or more different resistivity distributions can produce very similar voltage distributions on the external boundary [41].
Reference: [30] <author> W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> second edition, </address> <year> 1992. </year>
Reference-contexts: a better r in the vicinity of the present r, and global search methods that explore much wider regions than the vicinity of the present r. 4 1.2 Local Search Methods Among various local optimization techniques, efficient and therefore often used are the conjugate gradient method and the quasi-Newton family <ref> [2, 25, 30, 37] </ref>. The conjugate gradient method may be more suitable if the distributed parameter to be estimated has a large number of elements, because it requires less memory than the quasi-Newton family and yet its convergence speed is comparable as follows. <p> Their results were quite promising, although the models tested were only one-dimensional and the number of elements of the distributed parameter was limited to 32. 8 This dissertation develops methods that use the discrete Fourier transform <ref> [21, 30, 40] </ref> as well as the Haar wavelet transform for a multi-dimensional field. <p> If the number of coefficients to be estimated is N L with frequency limit L and the cost function is quadratic with respect to them, it takes O (N L ) local search iterations to converge in the conjugate gradient method <ref> [25, 30] </ref>. <p> the gradient r r H 4.4 Adjust r based on r flt r H 5 Output r 17 Furthermore, it can possibly open up new ways of multiresolution optimization by introducing various types of filters for the gradient. 2.2 The Conjugate Gradient Method This dissertation employs the conjugate gradient method <ref> [14, 25, 30, 37] </ref> for local search. The reason is that this method is suitable for estimating a large number of unknowns because its required storage is smaller than that of the quasi-Newton family, which is another powerful method, and yet its convergence speed is comparable (see Section 1.2). <p> The Polak-Ribiere formula is said to be usually superior to the Fletcher-Reeves formula <ref> [30] </ref>. 2.2.2 When Parameters are Complex Let us consider the case where the cost H is a function of complex numbers. This consideration is necessary for estimating the Fourier coefficients, which are complex 19 in general. <p> The sign of the exponent of e can be reversed as the following <ref> [18, 30] </ref>. R k = j=0 i2kj r j = N k=0 N (j = 0; ; N 1) (2.33) The effect is simply that the parameter distribution is looked at in the reverse order as shown below. <p> A parameter r distributed in an n-dimensional field can be described by an n-dimensional array fr j 1 j 2 j n g where j m is an integer from 0 to N m 1. The n-dimensional DFT is defined by combining n one-dimensional DFTs <ref> [30] </ref>. <p> To compensate for this, we assume from now on that the values of J i;j at the corners are halved before this equation and those below are used. 4.2 Forward Solution Numerical methods to solve elliptic difference equations such as (4.9) are classified into direct methods and iterative methods <ref> [9, 13, 27, 30, 36] </ref>. Direct methods directly solve the finite difference equations of the matrix form Av = b with respect to the column vector v. A serious problem with direct methods is that a large amount of memory is required. <p> Back propagation is executed once per line minimization to obtain the gradient to determine the line search direction. The line minimization algorithm in the conjugate gradient method is based on so-called Brent's method <ref> [30] </ref> that uses the parabolic interpolation for searching a minimum whenever appropriate and the golden section search otherwise. All the parameters and the variables are calculated in double precision. <p> This may explain 120 why the method with the Fourier transform performed better, if slightly, than the method with the wavelet transform in the experiments. The wavelet transform is said to be better than the Fourier transform in applications other than parameter estimation, such as image compression <ref> [30] </ref>. The reason is that each wavelet coefficient represents localized information, while each Fourier coefficient represents information of the whole field.
Reference: [31] <author> O. Rioul and M. Vetterli. </author> <title> Wavelets and signal processing. </title> <journal> IEEE SP Magazine, </journal> <pages> pages 14-38, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Therefore, a search can be faster with a smaller dimension of the search space. It was found that there were other researchers who conceived the same idea of using multiresolution optimization for parameter estimation [4, 23]. They used the Haar wavelet <ref> [5, 6, 31] </ref> as a multiresolution scheme combined with a quasi-Newton method (the BFGS algorithm) to estimate a distributed parameter of elliptic and parabolic models. It was shown that the multiresolution algorithm performed better than an ordinary single-resolution method.
Reference: [32] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations, chapter Learning Internal Representations by Error Propagation, </title> <address> pages 318-362. </address> <publisher> MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1986. </year>
Reference-contexts: Fortunately, we can utilize the so-called back propagation to calculate the gradient very efficiently. Its computation time is only of the same order of the forward calculation time. 3.1 The Back-Propagation Algorithm The back-propagation algorithm <ref> [14, 15, 33, 32] </ref> is a scheme for solving learning problems of artificial neural networks where a cost function that represents learning errors is to be minimized.
Reference: [33] <editor> D.E. Rumelhart, J.L. McClelland, </editor> <booktitle> and the PDP Research Group. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1986. </year>
Reference-contexts: Fortunately, we can utilize the so-called back propagation to calculate the gradient very efficiently. Its computation time is only of the same order of the forward calculation time. 3.1 The Back-Propagation Algorithm The back-propagation algorithm <ref> [14, 15, 33, 32] </ref> is a scheme for solving learning problems of artificial neural networks where a cost function that represents learning errors is to be minimized.
Reference: [34] <author> P.C. Sabatier, </author> <title> editor. Inverse Methods in Action: </title> <booktitle> Proceedings of the Multicenten-nials Meeting on Inverse Problems, </booktitle> <address> Montpellier, 1989. </address> <publisher> Springer-Verlag, </publisher> <year> 1990. </year> <month> 126 </month>
Reference-contexts: words, a parameter 2 element is expected to have a value close to its neighbors' in most of the regions in the field. 1.1 Parameter Estimation by Numerical Optimization Numerical optimization techniques are often employed to solve parameter estimation problems, especially when the mathematical model of the phenomenon is nonlinear <ref> [34, 37] </ref>. The common framework of these techniques is to optimize the unknown parameter distribution so that the simulated results of the phenomenon approach the given data. Calculating the resulting state of the model variables based on given parameter values is called the direct problem or the forward problem.
Reference: [35] <author> J.R. Singer, F.A. Grunbaum, P. Kohn, and J.P. Zubelli. </author> <title> Image reconstruction of the interior of bodies that diffuse radiation. </title> <journal> Science, </journal> <volume> 248 </volume> <pages> 990-993, </pages> <year> 1990. </year>
Reference-contexts: In this case, the distributed parameter that represents the interior of the system has to be estimated from certain quantities measured on the exterior. Some examples of this type are image reconstruction problems in ultrasound, light scattering, impedance, diffuse tomography and biomagnetism <ref> [3, 12, 24, 35] </ref>. In another case, one may want to determine the aspects of a system, which are in themselves difficult to measure directly, by inferring these aspects from other more accessible quantities which may actually be in the same physical position.
Reference: [36] <author> J.C. Strikwerda. </author> <title> Finite Difference Schemes and Partial Differential Equations. </title> <publisher> Wadsworth & Brooks/Cole Advanced Books & Software, </publisher> <year> 1989. </year>
Reference-contexts: To compensate for this, we assume from now on that the values of J i;j at the corners are halved before this equation and those below are used. 4.2 Forward Solution Numerical methods to solve elliptic difference equations such as (4.9) are classified into direct methods and iterative methods <ref> [9, 13, 27, 30, 36] </ref>. Direct methods directly solve the finite difference equations of the matrix form Av = b with respect to the column vector v. A serious problem with direct methods is that a large amount of memory is required.
Reference: [37] <author> A. Tarantola. </author> <title> Inverse Problem Theory. </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1987. </year>
Reference-contexts: words, a parameter 2 element is expected to have a value close to its neighbors' in most of the regions in the field. 1.1 Parameter Estimation by Numerical Optimization Numerical optimization techniques are often employed to solve parameter estimation problems, especially when the mathematical model of the phenomenon is nonlinear <ref> [34, 37] </ref>. The common framework of these techniques is to optimize the unknown parameter distribution so that the simulated results of the phenomenon approach the given data. Calculating the resulting state of the model variables based on given parameter values is called the direct problem or the forward problem. <p> a better r in the vicinity of the present r, and global search methods that explore much wider regions than the vicinity of the present r. 4 1.2 Local Search Methods Among various local optimization techniques, efficient and therefore often used are the conjugate gradient method and the quasi-Newton family <ref> [2, 25, 30, 37] </ref>. The conjugate gradient method may be more suitable if the distributed parameter to be estimated has a large number of elements, because it requires less memory than the quasi-Newton family and yet its convergence speed is comparable as follows. <p> the gradient r r H 4.4 Adjust r based on r flt r H 5 Output r 17 Furthermore, it can possibly open up new ways of multiresolution optimization by introducing various types of filters for the gradient. 2.2 The Conjugate Gradient Method This dissertation employs the conjugate gradient method <ref> [14, 25, 30, 37] </ref> for local search. The reason is that this method is suitable for estimating a large number of unknowns because its required storage is smaller than that of the quasi-Newton family, which is another powerful method, and yet its convergence speed is comparable (see Section 1.2).
Reference: [38] <author> M. Tasto and H. Schomberg. </author> <title> Object reconstruction from projections and some nonlinear extensions. </title> <editor> In C.H. Chen, editor, </editor> <booktitle> Pattern Recognition and Signal Processing, NATO Advanced Study Institutes Series, </booktitle> <pages> pages 485-503. </pages> <address> Sijthoff & Noordhoff, </address> <year> 1978. </year>
Reference: [39] <author> P.J.M. van Laarhoven and E.H.L. Aarts. </author> <title> Simulated Annealing: Theory and Applications. </title> <address> D. </address> <publisher> Reidel Publishing, </publisher> <year> 1986. </year>
Reference-contexts: The following two methods, which are the only general global optimization methods to date other than a brute force search, take this approach. Simulated annealing <ref> [8, 20, 39] </ref>, which was invented from analogy to physical annealing in which a material is heated to a high temperature and slowly cooled down until it rests in the most stable phase, allows cost-increasing moves with a probability controlled by the so-called temperature.
Reference: [40] <author> J.S. Walker. </author> <title> Fast Fourier Transforms. </title> <publisher> CRC Press, </publisher> <year> 1991. </year>
Reference-contexts: Their results were quite promising, although the models tested were only one-dimensional and the number of elements of the distributed parameter was limited to 32. 8 This dissertation develops methods that use the discrete Fourier transform <ref> [21, 30, 40] </ref> as well as the Haar wavelet transform for a multi-dimensional field. <p> Any such definition can be used as long as the transform and the inverse transform are self-consistent. The following definitions in which the inverse DFT, instead of the DFT, has the constant 1=N seem to be used in most of the literature, e.g. <ref> [19, 40] </ref>.
Reference: [41] <author> J.G. Webster, </author> <title> editor. Electrical Impedance Tomography. </title> <booktitle> The Adam Hilger Series on Biomedical Engineering. Adam Hilger, </booktitle> <year> 1990. </year>
Reference-contexts: The other class applies voltage and measures the resulting current on the boundary. The former (current injection) generally allows more accurate measurements than the latter (voltage input method) because the contact impedance between the electrodes and the object surface is negligible for a current source <ref> [41] </ref>. We choose here the current injection method used by most EIT research groups. In all of the following experiments, voltage/current measurements are computer-simulated ones. <p> In all of the following experiments, voltage/current measurements are computer-simulated ones. If the medium is a disk and its resistivity distribution is homogeneous, injecting currents that vary as a cosine curve along the boundary gives a perfectly uniform current distribution and hence maximizes distinguishability <ref> [41] </ref>. <p> It might be because the image reconstruction of electrical impedance tomography is such a highly nonlinear, ill-posed problem [29] that even an excellent estimation algorithm has trouble with it. For example, two or more different resistivity distributions can produce very similar voltage distributions on the external boundary <ref> [41] </ref>. This means that there can be local minima that are almost as good as the global minimum in terms of the cost. It is also possible that the existence of discontinuities in the true resistivity distributions (Pattern A and Pattern B) disturbed the estimation.
Reference: [42] <author> E.J. Woo, P. Hua, and J.G. Webster. </author> <title> A robust image reconstruction algorithm and its parallel implementation in electrical impedance tomography. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <volume> 12(2) </volume> <pages> 137-146, </pages> <year> 1993. </year>
Reference: [43] <author> A.H. Wright. </author> <title> Genetic algorithms for real parameter optimization. </title> <editor> In J.E. Rawlins, editor, </editor> <booktitle> Foundations of Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1991. </year>
Reference-contexts: The temperature is initially set high and gradually decreased based on a certain annealing schedule. Accordingly, the probability of accepting uphill moves is initially high and gradually decreased. Genetic algorithms <ref> [7, 8, 10, 11, 43] </ref> also perform global minimization by somewhat random moves, but in a totally different way. The search starts with a population of random chromosomes (candidates of the best estimate of the parameters) and improves the population by creating new offspring by genetic operations.
Reference: [44] <author> T.J. Yorkey, J.G. Webster, and W.J. Tompkins. </author> <title> Comparing reconstruction algorithms for electrical impedance tomography. </title> <journal> IEEE Transactions on Biomedical Engineering, </journal> <volume> BME-34(11):843-852, </volume> <year> 1987. </year>
Reference: [45] <author> T.J. Yorkey, J.G. Webster, and W.J. Tompkins. </author> <title> An improved perturbation technique for electrical impedance imaging with some criticisms. </title> <journal> IEEE Transactions on Biomedical Engineering, </journal> <volume> BME-34(11):898-901, </volume> <year> 1987. </year>
References-found: 45

