URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/95.tr602.Customized_dynamic_load_balancing.ps.gz
Refering-URL: http://www.cs.rochester.edu/research/cashmere/98-03_NSF_report/bib.html
Root-URL: 
Title: Customized Dynamic Load Balancing for a Network of Workstations  
Author: Mohammed Javeed Zaki, Wei Li, Srinivasan Parthasarathy 
Keyword: Customized Dynamic Load Balancing, Network of workstations, Performance Modeling, Compile/Run-Time system.  
Note: supported this work. This work was supported in part by an NSF Research Initiation Award (CCR-9409120) and ARPA contract F19628-94-C-0057.  
Address: Rochester, New York 14627  
Affiliation: The University of Rochester Computer Science Department  The University of Rochester Computer Science Department  
Pubnum: Technical Report 602  
Email: fzaki,wei,srinig@cs.rochester.edu  
Date: December 1995  
Abstract: Load balancing involves assigning to each processor, work proportional to its performance, minimizing the execution time of the program. Although static load balancing can solve many problems (e.g., those caused by processor heterogeneity and non-uniform loops) for most regular applications, the transient external load due to multiple-users on a network of workstations necessitates a dynamic approach to load balancing. In this paper we examine the behavior of global vs local, and centralized vs distributed, load balancing strategies. We show that different schemes are best for different applications under varying program and system parameters. Therefore, customized load balancing schemes become essential for good performance. We present a hybrid compile-time and run-time modeling and decision process which selects (customizes) the best scheme, along with automatic generation of parallel code with calls to a runtime library for load balancing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.N.C. Arabe, A. Beguelin, B. Lowekamp, E. Seligman, M. Starkey, and P. Stephan. Dome: </author> <title> parallel programming in a heterogeneous multi-user environment. </title> <type> CMU-CS-95-137 30786, </type> <institution> Carnegie Mellon Univ - Sch. of Computer Science, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: The approach is global distributed, where the processor's load is given as the average computation time per virtual processor, and load balancing involves periodic information exchanges. Dome <ref> [1] </ref> implements a global central scheme and a local distributed scheme. The performance metric used is the rate at which the processors execute the dome program, and load balancing involves periodic exchanges.
Reference: [2] <author> R. D. Blumofe and D. S. Park. </author> <title> Scheduling large-scale parallel computations on network of workstations. </title> <booktitle> 3rd IEEE Intl. Symposium on High-Performance Distributed Computing, </booktitle> <month> april </month> <year> 1994. </year>
Reference-contexts: Siegell [21] also presented a global centralized scheme, with periodic information exchanges, and where the performance metric is the iterations done per second. The main contribution of this paper was the methodology for automatic generation of parallel programs with dynamic load balancing. In Phish <ref> [2] </ref>, a local distributed receiver-initiated scheme is described, where the processor requesting more tasks, called the thief, chooses a victim at random from which to steal more work. If the current victim cannot satisfy the request, another victim is selected. CHARM [20] implements a two phased scheme.
Reference: [3] <author> A. L. Cheung and A. P. Reeves. </author> <title> High performance computing on a cluster of workstations. </title> <booktitle> Proc. of the 1st Int. Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 152-160, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: For UMA (Uniform Memory Access) parallel machines, usually loop iterations can be scheduled in block or cyclic fashion [17]. For NUMA (Non-Uniform Memory Access) parallel machines, loop scheduling has to take data distribution into account [11]. Static scheduling algorithms for heterogeneous programs, processors, and network were proposed in <ref> [3, 4] </ref>.
Reference: [4] <author> M. Cierniak, W. Li, and M. J. Zaki. </author> <title> Loop scheduling for heterogeneity. </title> <booktitle> In 4th IEEE Intl. Symposium on High-Performance Distributed Computing, also as URCS-TR 540, </booktitle> <institution> CS Dept., Univ. of Rochester, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: For UMA (Uniform Memory Access) parallel machines, usually loop iterations can be scheduled in block or cyclic fashion [17]. For NUMA (Non-Uniform Memory Access) parallel machines, loop scheduling has to take data distribution into account [11]. Static scheduling algorithms for heterogeneous programs, processors, and network were proposed in <ref> [3, 4] </ref>. <p> We transform this triangular loop into a uniform loop using the 18 bitonic scheduling technique <ref> [4] </ref>, i.e., by combining iterations i and n (n + 1)=2 i + 1 into one iteration. Note that the number of iterations for loop 2 is given as n (n + 1)=4, and the work is also linear in the array size.
Reference: [5] <author> D. L. Eager and J. Zahorjan. </author> <title> Adaptive guided self-scheduling. </title> <type> Technical Report 92-01-01, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Factoring [9], allocates iterations in batches, which are partitioned equally among all the processors. The current size of a batch is exactly half the size of the previous batch. Other schemes in this model are adaptive guided self-scheduling <ref> [5] </ref>, which includes a random back-off to avoid contention for the task queue, and assigns iterations in an interleaved fashion to avoid imbalance; trapezoidal self-scheduling [23], which linearly decreases the number of iterations allocated to each processor; tapering [14], which is suitable for irregular loops, and uses execution profiles to select
Reference: [6] <author> L. Kipp (ed.). </author> <title> Perfect Benchmarks Documentation, </title> <type> Suite 1. </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: The data size for each array is given as Z = R C, X = R R2, and Y = R2 C. * TRFD: TRFD is part of the Perfect Benchmark application suite <ref> [6] </ref>. It simulates the computational aspects of two-electron integral transformations. We used a modified version of TRFD, in the C programming language, which was enhanced to exploit the parallelism. 6.1 Network Characterization The network characterization is done off-line.
Reference: [7] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The compiler, however, generates code to handle this at run-time. The compiler also helps to generate symbolic cost functions for the iteration cost and communication cost. 5.2 Code Generation For the source-to-source code translation from a sequential program to a parallel program using PVM <ref> [7] </ref> for message passing, with DLB library calls, we use the Stanford University Intermediate Format (SUIF) [24] compiler. <p> Applications used C as the source code language, and were run on dedicated machines, i.e., there were no other users on the machines. External load was simulated within our programs as described in section 4. PVM <ref> [7] </ref> was used to parallelize the applications. PVM (Parallel Virtual Machine), is a message passing software system mainly intended for network based distributed computing on heterogeneous serial and parallel computers. PVM supports heterogeneity at the application, machine and network level, and supports coarse grain parallelism in the application.
Reference: [8] <author> A. S. Grimshaw, J. B. Weissman, E. A. West, and E. C. Loyot. Metasystems: </author> <title> An approach combining parallel processing and heterogeneous distributed computing systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(3) </volume> <pages> 257-270, </pages> <year> 1994. </year>
Reference-contexts: For NUMA (Non-Uniform Memory Access) parallel machines, loop scheduling has to take data distribution into account [11]. Static scheduling algorithms for heterogeneous programs, processors, and network were proposed in [3, 4]. In <ref> [8] </ref> another static scheme for heterogeneous systems was proposed, which uses a run-time system to select the appropriate number of processors, and the ratios to allocate work to the processors. 2.2 Dynamic Scheduling The dynamic scheduling strategies fall under different models, which include the task queue model, diffusion model, and schemes
Reference: [9] <author> S.F. Hummel, E. Schonberg, and L.E. Flynn. </author> <title> Factoring: a practical and robust method for scheduling parallel loops. </title> <journal> Communications of the ACM, </journal> <volume> 35 </volume> <pages> 90-101, </pages> <month> aug </month> <year> 1992. </year>
Reference-contexts: Although the large chunk sizes in the beginning reduce synchronization, they can cause serious imbalances in non-uniform loops. Moreover, this scheme degenerates to the case of self-scheduling towards the end due to small chunk sizes. Factoring <ref> [9] </ref>, allocates iterations in batches, which are partitioned equally among all the processors. The current size of a batch is exactly half the size of the previous batch.
Reference: [10] <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11 </volume> <pages> 1001-16, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Once the processors have finished their assigned portion, more work is obtained from this queue. The simplest approach in this model is self-scheduling [22], where each processor is allocated only one iteration at a time, which leads to high synchronization cost. To alleviate this problem fixed-size chunking was proposed <ref> [10] </ref>, where each processor is allocated K iterations instead of one. The potential for load imbalance, however, is still present. In guided self-scheduling [18], the chunk size is changed at run-time. Each processor is assigned 1=P -th of the remaining loop iterations, where P denotes the number of processors.
Reference: [11] <author> W. Li and K. Pingali. </author> <title> Access normalization: Loop restructuring for NUMA compilers. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 353-375, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: For UMA (Uniform Memory Access) parallel machines, usually loop iterations can be scheduled in block or cyclic fashion [17]. For NUMA (Non-Uniform Memory Access) parallel machines, loop scheduling has to take data distribution into account <ref> [11] </ref>. Static scheduling algorithms for heterogeneous programs, processors, and network were proposed in [3, 4].
Reference: [12] <author> F.C.H. Lin and R.M. Keller. </author> <title> The gradient model load balancing method. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-13:32-38, </volume> <month> jan </month> <year> 1987. </year>
Reference-contexts: Diffusion Model Other approaches include diffusion models with all the work initially distributed, and with work movement between adjacent processors if an imbalance is detected between their load and their neighbor's. An example is the gradient model <ref> [12] </ref> approach. Predicting the Future Another approach is to predict future performance based on past information. For example, in Data parallel C [19], loop iterations are mapped to virtual processors, and these virtual processors are assigned to the physical processors based on past load behavior.
Reference: [13] <author> Liu and et al._ </author> <title> Scheduling parallel loops with variable length iteration execution times on parallel computers. </title> <booktitle> Proc. 5th Intl. Conf. Parallel and Distributed Computing and System, </booktitle> <month> oct </month> <year> 1992. </year> <month> 22 </month>
Reference-contexts: task queue, and assigns iterations in an interleaved fashion to avoid imbalance; trapezoidal self-scheduling [23], which linearly decreases the number of iterations allocated to each processor; tapering [14], which is suitable for irregular loops, and uses execution profiles to select a chunk size that minimizes the load imbalance; safe self-scheduling <ref> [13] </ref>, which uses a static phase where each processor is allocated a chunk of iterations, and a dynamic phase during which the processors are self scheduled to even out the load imbalances; and affinity scheduling [15] which takes processor affinity into account while scheduling.
Reference: [14] <author> S. Lucco. </author> <title> A dynamic scheduling method for irregular parallel programs. </title> <booktitle> ACM SIG--PLAN'92 Conf. Programming Language Design and Implementation, </booktitle> <pages> pages 200-211, </pages> <year> 1992. </year>
Reference-contexts: Other schemes in this model are adaptive guided self-scheduling [5], which includes a random back-off to avoid contention for the task queue, and assigns iterations in an interleaved fashion to avoid imbalance; trapezoidal self-scheduling [23], which linearly decreases the number of iterations allocated to each processor; tapering <ref> [14] </ref>, which is suitable for irregular loops, and uses execution profiles to select a chunk size that minimizes the load imbalance; safe self-scheduling [13], which uses a static phase where each processor is allocated a chunk of iterations, and a dynamic phase during which the processors are self scheduled to even
Reference: [15] <author> E.P. Markatos and T.J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: uses execution profiles to select a chunk size that minimizes the load imbalance; safe self-scheduling [13], which uses a static phase where each processor is allocated a chunk of iterations, and a dynamic phase during which the processors are self scheduled to even out the load imbalances; and affinity scheduling <ref> [15] </ref> which takes processor affinity into account while scheduling. Under this scheme, all the processors are initially assigned an 2 equal chunk taking into account data reuse and locality.
Reference: [16] <author> H. Nishikawa and P. Steenkiste. </author> <title> A general architecture for load balancing in a distributed-memory environment. </title> <booktitle> 13th IEEE International Conference on Distributed Computing, </booktitle> <month> may </month> <year> 1993. </year>
Reference-contexts: In the local schemes, instead of random selection of a processor from which to request more work, work is exchanged among all the neighbors (the number of neighbors is selected statically). These strategies are explained in more detail in the next section. In <ref> [16] </ref>, an approach was presented, where a user specifies homogeneous load balancers for different tasks within a heterogeneous application. They also present a global load balancer that handles the interactions among the different homogeneous load balancers.
Reference: [17] <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: For UMA (Uniform Memory Access) parallel machines, usually loop iterations can be scheduled in block or cyclic fashion <ref> [17] </ref>. For NUMA (Non-Uniform Memory Access) parallel machines, loop scheduling has to take data distribution into account [11]. Static scheduling algorithms for heterogeneous programs, processors, and network were proposed in [3, 4].
Reference: [18] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: a practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: To alleviate this problem fixed-size chunking was proposed [10], where each processor is allocated K iterations instead of one. The potential for load imbalance, however, is still present. In guided self-scheduling <ref> [18] </ref>, the chunk size is changed at run-time. Each processor is assigned 1=P -th of the remaining loop iterations, where P denotes the number of processors. Although the large chunk sizes in the beginning reduce synchronization, they can cause serious imbalances in non-uniform loops.
Reference: [19] <author> N. Nedeljkovic M. J. Quinn. </author> <title> Data-parallel programming on a network of heterogeneous workstations. </title> <booktitle> Proc. of the 1st Int. Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 152-160, </pages> <month> sep </month> <year> 1992. </year>
Reference-contexts: An example is the gradient model [12] approach. Predicting the Future Another approach is to predict future performance based on past information. For example, in Data parallel C <ref> [19] </ref>, loop iterations are mapped to virtual processors, and these virtual processors are assigned to the physical processors based on past load behavior. The approach is global distributed, where the processor's load is given as the average computation time per virtual processor, and load balancing involves periodic information exchanges.
Reference: [20] <author> V. A. Saletore, J. Jacob, and M. Padala. </author> <title> Parallel computations on the charm heterogeneous workstation cluster. </title> <booktitle> 3rd IEEE Intl. Symposium on High-Performance Distributed Computing, </booktitle> <month> april </month> <year> 1994. </year>
Reference-contexts: In Phish [2], a local distributed receiver-initiated scheme is described, where the processor requesting more tasks, called the thief, chooses a victim at random from which to steal more work. If the current victim cannot satisfy the request, another victim is selected. CHARM <ref> [20] </ref> implements a two phased scheme. Initially, in the static phase, work is assigned to the processors proportional to their speed, and inversely proportional to the load on the processor. The dynamic phase implements a local distributed receiver-initiated scheme.
Reference: [21] <author> B.S. Siegell. </author> <title> Automatic generation of parallel programs with dynamic load balancing for a network of workstations. </title> <type> CMU-CS-95-168 30880, </type> <institution> Carnegie Mellon Univ. - Sch. of Computer Science, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Dome [1] implements a global central scheme and a local distributed scheme. The performance metric used is the rate at which the processors execute the dome program, and load balancing involves periodic exchanges. Siegell <ref> [21] </ref> also presented a global centralized scheme, with periodic information exchanges, and where the performance metric is the iterations done per second. The main contribution of this paper was the methodology for automatic generation of parallel programs with dynamic load balancing.
Reference: [22] <author> P. Tang and P.-C. Yew. </author> <title> Processor self-scheduling for multiple nested parallel loops. </title> <booktitle> In Proc. of '86 International Conference On Parallel Processing, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: In these schemes there is a central task queue of loop iterations. Once the processors have finished their assigned portion, more work is obtained from this queue. The simplest approach in this model is self-scheduling <ref> [22] </ref>, where each processor is allocated only one iteration at a time, which leads to high synchronization cost. To alleviate this problem fixed-size chunking was proposed [10], where each processor is allocated K iterations instead of one. The potential for load imbalance, however, is still present.
Reference: [23] <author> T.H. Tzen and L.M. Ni. </author> <title> Trapeziod self-scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Trans. Parallel Distributed Systems, </journal> <volume> 4 </volume> <pages> 87-98, </pages> <month> jan </month> <year> 1993. </year>
Reference-contexts: The current size of a batch is exactly half the size of the previous batch. Other schemes in this model are adaptive guided self-scheduling [5], which includes a random back-off to avoid contention for the task queue, and assigns iterations in an interleaved fashion to avoid imbalance; trapezoidal self-scheduling <ref> [23] </ref>, which linearly decreases the number of iterations allocated to each processor; tapering [14], which is suitable for irregular loops, and uses execution profiles to select a chunk size that minimizes the load imbalance; safe self-scheduling [13], which uses a static phase where each processor is allocated a chunk of iterations,
Reference: [24] <author> R. Wilson, R. French, C. Wilson, S. Amarasinghe, J. Anderson, S. Tjiang, S. Liao, C. Tseng, M. Hall, M. Lam, and J. Hennessy. </author> <title> An overview of the suif compiler system. </title>
Reference-contexts: The compiler also helps to generate symbolic cost functions for the iteration cost and communication cost. 5.2 Code Generation For the source-to-source code translation from a sequential program to a parallel program using PVM [7] for message passing, with DLB library calls, we use the Stanford University Intermediate Format (SUIF) <ref> [24] </ref> compiler. The input to the compiler consists of the sequential version of the code, with annotations to indicate the data decomposition for the shared arrays, and to indicate the loops which have to be load balanced.
Reference: [25] <author> M. Zaki, W. Li, and M. Cierniak. </author> <title> Performance impact of processor and memory heterogeneity in a network of machines. </title> <booktitle> In the Proceedings of the Fourth Heterogeneous Computing Workshop, </booktitle> <address> Santa Barbara, California, </address> <month> April </month> <year> 1995. </year> <month> 23 </month>
Reference-contexts: This number is specified by the user, and is denoted as P . 7 Processor Speeds: This specifies the ratio of a processor's performance w.r.t a base processor. Since this ratio is application specific <ref> [25] </ref>, we can obtain this by a profiling run. We may also try to predict this at compile-time. The speed for processor i is denoted as S i .
References-found: 25

