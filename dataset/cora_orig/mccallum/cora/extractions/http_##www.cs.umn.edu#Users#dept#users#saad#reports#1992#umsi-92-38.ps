URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1992/umsi-92-38.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1992/
Root-URL: http://www.cs.umn.edu
Title: ILUT: A DUAL THRESHOLD INCOMPLETE LU FACTORIZATION  
Author: YOUCEF SAAD 
Keyword: Key words: Preconditioning; Incomplete LU; Threshold strategies.  
Address: 200 Union Street S.E. Minneapolis, MN. 55455  
Affiliation: University of Minnesota Computer Science Department  
Abstract: In this paper we describe an Incomplete LU factorization technique based on a strategy which combines two heuristics. This ILUT factorization extends the usual ILU(0) factorization without using the concept of level of fill-in. There are two traditional ways of developing incomplete factorization preconditioners. The first uses a symbolic factorization approach in which a level of fill is attributed to each fill-in element using only the graph of the matrix. Then each fill-in that is introduced is dropped whenever its level of fill exceeds a certain threshold. The second class of methods consists of techniques derived from modifications of a given direct solver by including a drop-off rule, based on the numerical size of the fill-ins introduced. traditionally referred to as threshold preconditioners. The first type of approach may not be reliable for indefinite problems, since it does not consider numerical values. The second is often far more expensive than the standard ILU(0). The strategy we propose is a compromise between these two extremes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. C. Anderson. </author> <title> Parallel implementation of preconditioned conjugate gradient methods for solving sparse systems of linear equations. </title> <type> Technical Report 805, </type> <institution> CSRD, University of Illinois, Urbana, IL, </institution> <year> 1988. </year> <type> MS Thesis. </type>
Reference-contexts: The experiments have been performed on a CRAY 2. In order to make the experiments realistic, we implemented the level-scheduling technique for backward and forward solution using the jagged diagonal data structure. For details, see <ref> [2, 1] </ref>. We have tested the preconditioned on the matrix obtained from a natural ordering as well as from permuting the matrix into the red-black ordering. 6.1 Natural ordering We experimented with two ways of varying the parameters t and p.
Reference: [2] <author> E. C. Anderson and Y. Saad. </author> <title> Solving sparse triangular systems on parallel computers. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1 </volume> <pages> 73-96, </pages> <year> 1989. </year>
Reference-contexts: The experiments have been performed on a CRAY 2. In order to make the experiments realistic, we implemented the level-scheduling technique for backward and forward solution using the jagged diagonal data structure. For details, see <ref> [2, 1] </ref>. We have tested the preconditioned on the matrix obtained from a natural ordering as well as from permuting the matrix into the red-black ordering. 6.1 Natural ordering We experimented with two ways of varying the parameters t and p.
Reference: [3] <author> O. Axelsson and V. A. Barker. </author> <title> Finite Element Solution of Boundary Value Problems. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida, </address> <year> 1984. </year>
Reference-contexts: Note that these modifications aim at preventing elements generated in position (i; j i ) from ever being dropped and that there may be many alternative strategies that can lead to the same property. Following Axelsson and Barker <ref> [3] </ref>, we will call an ^ M matrix a matrix H whose entries h ij satisfy the following three conditions: h ii &gt;0 for 1 i&lt;N and h NN 0 (5) N X h ij &lt; 0; for 1 i&lt;N: (7) The third condition is simply a requirement that there be <p> An ^ M matrix H is said to be diagonally dominant if all its rows are diagonally dominant. In the following we establish an existence result for ILUT that is similar to Theorem 1.14 in Axelsson and Barker <ref> [3] </ref> for ILU (0). The underlying assumption is that an ILUT strategy is used with modification 2.
Reference: [4] <author> E. F. D'Azevedo, F. A. Forsyth, and W. P. Tang. </author> <title> Ordering methods for preconditioned conjugate gradient methods applied to unstructured grid problems. </title> <note> SIAM J. Mat. Comput., 1992. to appear. </note>
Reference-contexts: Then each fill-in that is introduced and whose level exceeds a certain threshold is dropped. The second common approach is to modify a given direct solver by including a drop-off rule, based on the numerical size of the fill-ins introduced <ref> [8, 10, 5, 4, 15, 14] </ref>. The level-of-fill approach may not be reliable for some indefinite problems, since the strategy ignores the numerical values.
Reference: [5] <author> E. F. D'Azevedo, F. A. Forsyth, and W. P. Tang. </author> <title> Towards a cost effective ILU preconditioner with high level fill. </title> <journal> BIT, </journal> <note> 1992. to appear. </note>
Reference-contexts: Then each fill-in that is introduced and whose level exceeds a certain threshold is dropped. The second common approach is to modify a given direct solver by including a drop-off rule, based on the numerical size of the fill-ins introduced <ref> [8, 10, 5, 4, 15, 14] </ref>. The level-of-fill approach may not be reliable for some indefinite problems, since the strategy ignores the numerical values.
Reference: [6] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1986. </year>
Reference-contexts: This enables efficient sparse vector updates in line 7 <ref> [6] </ref>. The vector row fills with nonzero elements after the completion of each outer loop i so that it is necessary to zero-out the elements that were just filled in during the course of this loop, as is done in line 13.
Reference: [7] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM trans. Math. Soft., </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: The parameters and their meanings are identical. We do not plot the analogue of that the better performance is reached for the more accurate factorizations. 6.3 Tests with Harwell-Boeing matrices Some of the matrices in the Harwell-Boeing collection <ref> [7] </ref> can cause serious challenges to iterative methods. We have performed a number of tests of ILUT on some of these matrices and made the following observation: 1. <p> Sherman3. Sherman3 is a matrix of size N = 5; 005 which has a total of N Z = 20; 033 nonzero elements. This is in fact a 7-point matrix on a three-dimensional grid of size 35 fi 11 fi 13 which arises from a reservoir simulation problem <ref> [7] </ref>. In Again, t is of the form t = t 0 =2 k , with t 0 = 0:1 and the x-axis shows the power k. The ILU (0) factorization is computable but ILU (0)-GMRES (10) does not converge for this problem. Pores 2.
Reference: [8] <author> K. Gallivan, A. Sameh, and Z. Zlatev. </author> <title> A parallel hybrid sparse linear system solver. </title> <booktitle> Computing Systems in Engineering, </booktitle> <address> 1(2-4):183-195, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Then each fill-in that is introduced and whose level exceeds a certain threshold is dropped. The second common approach is to modify a given direct solver by including a drop-off rule, based on the numerical size of the fill-ins introduced <ref> [8, 10, 5, 4, 15, 14] </ref>. The level-of-fill approach may not be reliable for some indefinite problems, since the strategy ignores the numerical values.
Reference: [9] <author> J. A. Meijerink and H. A. van der Vorst. </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Math. Comp., </journal> <volume> 31(137) </volume> <pages> 148-162, </pages> <year> 1977. </year>
Reference-contexts: 1; : : : ; n and if (i; j) 2 N Z (A), Do: compute u ij = u ij l ik u kj . endfor endfor endfor The existence of the above ILU (0) factorization is guaranteed under a few simple and restrictive assumptions on the coefficient matrix <ref> [9] </ref>. In some cases, one can put the factorization in the form A = (D E)D 1 (D F ); in which E and F are the strict lower and strict upper triangular parts of A, and D is some diagonal matrix. <p> The accuracy of this incomplete factorization may be insufficient to yield an adequate rate of convergence. The idea of high accuracy ILU factorization has been proposed by Meijerink and Van Der Vorst <ref> [9] </ref> at least for structured matrices arising from 5-point and 7-point matrices. Thus, ILU (1) allows the `first order fill-ins' which are located in specific diagonals, to be kept. One problem with this is that the definition does not extend to general sparse matrices.
Reference: [10] <author> O. Osterby and Z. Zlatev. </author> <title> Direct methods for sparse matrices. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Then each fill-in that is introduced and whose level exceeds a certain threshold is dropped. The second common approach is to modify a given direct solver by including a drop-off rule, based on the numerical size of the fill-ins introduced <ref> [8, 10, 5, 4, 15, 14] </ref>. The level-of-fill approach may not be reliable for some indefinite problems, since the strategy ignores the numerical values.
Reference: [11] <author> Y. Saad. </author> <title> Preconditioning techniques for indefinite and nonsymmetric linear systems. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 24 </volume> <pages> 89-105, </pages> <year> 1988. </year>
Reference-contexts: Note that no pivoting is ever performed. Partial pivoting may be incorporated at the expense of substantially complicating the code. In <ref> [11] </ref> we implemented and tested a version of ILUT which performs column pivoting, allowing to use essentially the same simple data structure as with the non-pivoting code. An additional permutation array is required to record the corresponding permutation of the variables. <p> We found from recent experience with pivoting, that the rewards gained from pivoting are variable. If a matrix is not close to being diagonally dominant, it may be preferable to use an incomplete factorization that is derived from a direct solver, or a different iterative technique altogether <ref> [11] </ref>, such as one based on preconditioning the normal equations. Another issue that is not addressed here is that of using various standard permutations employed in the context of direct solvers to reduce fill-in. The two most popular of these are the minimal degree ordering and the nested dissection ordering.
Reference: [12] <author> Y. Saad. </author> <title> Supercomputer implementations of preconditioned Krylov subspace methods. </title> <type> Technical Report 91-311, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, Minnesota, </institution> <year> 1991. </year>
Reference-contexts: On the other hand if we use ILUT with slightly more fill-in, then the performance starts improving again. In fact an interesting observation we made in <ref> [12] </ref> is that the performance of ILUT for the Red-Black ordering eventually outperforms that of ILUT for the natural ordering using the same parameters p and t . 4 Analysis Existence theorems for the ILUT factorization are similar to those of other incomplete factorizations.
Reference: [13] <author> J. W. Watts-III. </author> <title> A conjugate gradient truncated direct method for the iterative solution of the reservoir simulation pressure equation. </title> <journal> Society of Petroleum Engineer Journal, </journal> <volume> 21 </volume> <pages> 345-353, </pages> <year> 1981. </year>
Reference-contexts: Thus, ILU (1) allows the `first order fill-ins' which are located in specific diagonals, to be kept. One problem with this is that the definition does not extend to general sparse matrices. Later, Watts <ref> [13] </ref> generalized this definition by introducing the concept of level of fill-in. The idea is quite simple. Any nonzero element in A is initially given a level of fill-in equal to 0.
Reference: [14] <author> D. P. Young, R. G. Melvin, F. T. Johnson, J. E. Bussoletti, L. B. Wigton, and S. S. Samant. </author> <title> Application of sparse matrix solvers as effective preconditioners. </title> <journal> SIAM J. Scient. Stat. Comput., </journal> <volume> 10 </volume> <pages> 1186-1199, </pages> <year> 1989. </year>
Reference-contexts: Then each fill-in that is introduced and whose level exceeds a certain threshold is dropped. The second common approach is to modify a given direct solver by including a drop-off rule, based on the numerical size of the fill-ins introduced <ref> [8, 10, 5, 4, 15, 14] </ref>. The level-of-fill approach may not be reliable for some indefinite problems, since the strategy ignores the numerical values.
Reference: [15] <author> Z. Zlatev. </author> <title> Use of iterative refinement in the solution of sparse linear systems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19 </volume> <pages> 381-399, </pages> <year> 1982. </year>
Reference-contexts: Then each fill-in that is introduced and whose level exceeds a certain threshold is dropped. The second common approach is to modify a given direct solver by including a drop-off rule, based on the numerical size of the fill-ins introduced <ref> [8, 10, 5, 4, 15, 14] </ref>. The level-of-fill approach may not be reliable for some indefinite problems, since the strategy ignores the numerical values.
References-found: 15

