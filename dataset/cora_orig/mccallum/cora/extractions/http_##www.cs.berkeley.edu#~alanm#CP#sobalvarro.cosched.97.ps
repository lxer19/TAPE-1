URL: http://www.cs.berkeley.edu/~alanm/CP/sobalvarro.cosched.97.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Title: Dynamic Coscheduling on Workstation Clusters  
Author: Patrick G. Sobalvarro Scott Pakin William E. Weihl Andrew A. Chien 
Note: Submitted for Publication  
Date: March 19, 1997  
Affiliation: 1 DEC Systems Research Center 2 University of Illinois at Urbana-Champaign  
Abstract: Coscheduling has been shown to be a critical factor in achieving efficient parallel execution in time- shared environments [11, 18, 4]. However, the most common approach, gang scheduling, has limitations in scaling, can compromise good interactive response, and requires that communicating processes be identified in advance. We explore a technique called dynamic coscheduling (DCS) which produces emergent coscheduling of the processes constituting a parallel job. Experiments are performed in a workstation environment with high performance networks and autonomous timesharing schedulers for each CPU. The results demonstrate that DCS can achieve effective, robust coscheduling for a range of workloads and background loads. Empirical comparisons to implicit scheduling and uncoordinated scheduling are presented. Under spin-block synchronization, DCS reduces job response times by up to 20% over implicit scheduling while maintaining fairness; and under spinning synchronization, DCS reduces job response times by up to two decimal orders of magnitude over uncoordinated scheduling. The results suggest that DCS is a promising avenue for achieving coordinated parallel scheduling in an environment that coexists with autonomous node schedulers.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. </author> <title> Myrinet|a gigabit-per-second local-area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year> <note> Available from http://www.myri.com/research/publications/Hot.ps. </note>
Reference-contexts: Fast Messages is a high- performance messaging layer that bypasses the operating system to provide direct access to an underlying Myricom Myrinet <ref> [1] </ref> and thereby achieve high performance. Details of FM can be found in [12, 14].
Reference: [2] <author> Rohit Chandra, Scott Devine, Ben Verghese, Anoop Gupta, and Mendel Rosenblum. </author> <title> Scheduling and page migra-tion for multiprocessor compute servers. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 12-24, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: Ousterhout provided a basic framework and presents a number of ways to achieve coscheduling while optimizing for system utilization [11]. Later work on the DASH multiprocessor <ref> [7, 2] </ref> demonstrated that coscheduling could be used to achieve efficiencies comparable to batch scheduling, while providing more flexible resource sharing. <p> Later work on the DASH multiprocessor [7, 2] demonstrated that coscheduling could be used to achieve efficiencies comparable to batch scheduling, while providing more flexible resource sharing. Specifically, coscheduling and process control (dynamic space-partitioning) performed similarly in the experiments described in <ref> [2] </ref>. 2.2 Detecting Threads Requiring Coscheduling Coscheduling implies coordinated scheduling of clusters of threads; identification of such clusters has been pursued through both explicit and implicit approaches. The shared memory workloads described in [7, 2] are parallel jobs which consist of thread collections, explicitly indicating which threads should be coscheduled. <p> Specifically, coscheduling and process control (dynamic space-partitioning) performed similarly in the experiments described in [2]. 2.2 Detecting Threads Requiring Coscheduling Coscheduling implies coordinated scheduling of clusters of threads; identification of such clusters has been pursued through both explicit and implicit approaches. The shared memory workloads described in <ref> [7, 2] </ref> are parallel jobs which consist of thread collections, explicitly indicating which threads should be coscheduled. A variety of implicit schemes which do not require explicit programmer annotation have been explored. On distributed memory systems, the need for coscheduling has typically been associated with communication [16, 5, 3, 15].
Reference: [3] <author> Andrea C. Dusseau, Remzi H. Arpaci, and David E. Culler. </author> <title> Effective distributed scheduling of parallel workloads. </title> <booktitle> In ACM SIGMETRICS '96 Conference on the Measurement and Modeling of Computer Systems, </booktitle> <year> 1996. </year> <note> Available from http://www.cs.berkeley.edu/~dusseau/Papers/sigmetrics96.ps. </note>
Reference-contexts: 1 Introduction Coordinated scheduling of parallel jobs across the nodes of a multiprocessor is well-known to produce benefits in both system and individual job efficiency <ref> [11, 17, 4, 16, 3] </ref>. Without coordinated scheduling, the processes constituting a parallel job suffer high communication latencies because of processor thrashing [11]. <p> These results indicate that dynamic coscheduling, spin- block, and the combination of dynamic coscheduling with spin-block synchronization can effectively achieve coscheduling. The effectiveness of spin-block has been previously documented in <ref> [3] </ref> (where it was called implicit scheduling), and our measurements confirm their results. In addition, our work demonstrates that DCS achieves coscheduling with both spinning and spin-block synchronization, where implicit scheduling requires processes to block awaiting message arrivals for coscheduling to happen. <p> A variety of implicit schemes which do not require explicit programmer annotation have been explored. On distributed memory systems, the need for coscheduling has typically been associated with communication <ref> [16, 5, 3, 15] </ref>. Feitelson's Runtime Activity Working Set Identification (RAWSI) monitors the communication between processes or threads 3 to determine their rate of communication. Working sets of processes (which require coscheduling) are identified based on their rate of communication. <p> Working sets of processes (which require coscheduling) are identified based on their rate of communication. RAWSI collects the information and uses a coordinated global mechanism to decide on a schedule. Both our dynamic coscheduling approach [16] and implicit scheduling <ref> [3] </ref> detect threads requiring coscheduling through their communication. However, neither system explicitly identifies the sets of processes to be coscheduled. 2.3 Mechanisms for Achieving Coscheduling Once thread clusters have been identified, a mechanism for coscheduling must be used. <p> An earlier paper on dynamic coscheduling [16] detailed analysis and simulation of an integrated coscheduling technique. Subsequently, Dusseau's implicit scheduling was evaluated via simulation in <ref> [3] </ref>, using synthetic single-program, multiple data applications. Because dynamic coscheduling is discussed extensively in the remainder of the paper, we only describe implicit scheduling here. Implicit scheduling uses spin-block synchronization primitives and the priority boost given by the SVR4 scheduler to threads which block on input/output to produce coscheduling. <p> It is noted in [11] that a two-context-switch maximum spin time is competitive, and in <ref> [3] </ref> it is argued that two context-switch times might be required for a processor to respond to a message if the message arrives at the beginning of a context switch to a process that is not the one to which the message is directed. 3.2.3 Experimental Platform Our experimental platform consists <p> We explore the surprising reasons for this the following discussion. Spin-block synchronization in combination with the Solaris 2.4 scheduler was reported to achieve coschedul- ing <ref> [3] </ref> (for a synthetic bulk-synchronous workload). <p> However, based on the results reported in <ref> [3] </ref> and our own work, it seems clear that the SVR4 approach is a very useful one for coscheduling. 14 For example, DCS can schedule on message arrival programs that are not on sleep queues (due to polling, infrequent communication, asynchronous communication, or one-sided data movement). <p> Such broadening would enable evaluation of a wide range of issues, including the viability of the epoch number mechanism [16] for multitasking parallel jobs. Such efforts are already underway as part of the (name omitted for blind reviewing) Project. Most of the workloads used to study coscheduling <ref> [3, 15] </ref> are quite regular in structure, communication, and computation. Such regularity often fails to exercise the behavioral richness of dynamic systems.
Reference: [4] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Distributed hierarchical control for parallel processing. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Coordinated scheduling of parallel jobs across the nodes of a multiprocessor is well-known to produce benefits in both system and individual job efficiency <ref> [11, 17, 4, 16, 3] </ref>. Without coordinated scheduling, the processes constituting a parallel job suffer high communication latencies because of processor thrashing [11]. <p> In many systems, particularly those with shared memory, a gang scheduler which has the capability to achieve coordinated context switches across processors has been assumed <ref> [7, 11, 4, 5] </ref>. Such systems replace the basic process scheduler in the operating system, and schedule the related threads across the processing nodes.
Reference: [5] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Coscheduling based on run-time identification of activity working sets. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 23(2) </volume> <pages> 135-160, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: A variety of implicit schemes which do not require explicit programmer annotation have been explored. On distributed memory systems, the need for coscheduling has typically been associated with communication <ref> [16, 5, 3, 15] </ref>. Feitelson's Runtime Activity Working Set Identification (RAWSI) monitors the communication between processes or threads 3 to determine their rate of communication. Working sets of processes (which require coscheduling) are identified based on their rate of communication. <p> In many systems, particularly those with shared memory, a gang scheduler which has the capability to achieve coordinated context switches across processors has been assumed <ref> [7, 11, 4, 5] </ref>. Such systems replace the basic process scheduler in the operating system, and schedule the related threads across the processing nodes.
Reference: [6] <author> Richard B. Gillett. </author> <title> Memory Channel network for PCI. </title> <journal> IEEE Micro, </journal> <volume> 16(1) </volume> <pages> 12-18, </pages> <month> February </month> <year> 1996. </year> <note> Available from http://www.computer.org/pubs/micro/web/m1gil.pdf. </note>
Reference-contexts: With clusters connected by high performance networks that achieve latencies in the range of tens of microseconds <ref> [12, 19, 20, 8, 6] </ref>, scheduling and context switching latency can increase communication latency by several orders of magnitude.
Reference: [7] <author> Anoop Gupta, Andrew Tucker, and Shigeru Urushibara. </author> <title> The impact of operating system scheduling poli-cies and synchronization methods on the performance of parallel applications. </title> <booktitle> In ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year> <note> Available from http://xenon.stanford.edu/~tucker/papers/sigmetrics.ps. </note>
Reference-contexts: Ousterhout provided a basic framework and presents a number of ways to achieve coscheduling while optimizing for system utilization [11]. Later work on the DASH multiprocessor <ref> [7, 2] </ref> demonstrated that coscheduling could be used to achieve efficiencies comparable to batch scheduling, while providing more flexible resource sharing. <p> Specifically, coscheduling and process control (dynamic space-partitioning) performed similarly in the experiments described in [2]. 2.2 Detecting Threads Requiring Coscheduling Coscheduling implies coordinated scheduling of clusters of threads; identification of such clusters has been pursued through both explicit and implicit approaches. The shared memory workloads described in <ref> [7, 2] </ref> are parallel jobs which consist of thread collections, explicitly indicating which threads should be coscheduled. A variety of implicit schemes which do not require explicit programmer annotation have been explored. On distributed memory systems, the need for coscheduling has typically been associated with communication [16, 5, 3, 15]. <p> In many systems, particularly those with shared memory, a gang scheduler which has the capability to achieve coordinated context switches across processors has been assumed <ref> [7, 11, 4, 5] </ref>. Such systems replace the basic process scheduler in the operating system, and schedule the related threads across the processing nodes.
Reference: [8] <author> D. B. Gustavson. </author> <title> The scalable coherent interface and related standards projects. </title> <journal> IEEE Micro, </journal> <volume> 12(1), </volume> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: With clusters connected by high performance networks that achieve latencies in the range of tens of microseconds <ref> [12, 19, 20, 8, 6] </ref>, scheduling and context switching latency can increase communication latency by several orders of magnitude.
Reference: [9] <institution> Sun Microsystems Inc. </institution> <note> ts dptbl(4) manual page. SunOS 5.4 Manual. Section 4. </note>
Reference-contexts: With clusters connected by high performance networks that achieve latencies in the range of tens of microseconds [12, 19, 20, 8, 6], scheduling and context switching latency can increase communication latency by several orders of magnitude. For example, under Solaris, CPU quanta vary from 20 ms to 200 ms <ref> [9] </ref>; consequently uncoordinated scheduling can increase best-case latencies (~ 10 microseconds) by three to four orders of magnitude, nullifying many benefits of fast communication subsystems. Uncoordinated scheduling can also decrease the efficiency of resource utilization.
Reference: [10] <author> Mario Lauria and Andrew Chien. </author> <title> MPI-FM: High performance MPI on workstation clusters. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <year> 1997. </year>
Reference-contexts: Details of FM can be found in [12, 14]. We also employ an implementation of the Message Passing Interface (MPI) built atop FM, called MPI-FM <ref> [10] </ref>, for some of the benchmarks. 3.2.2 Implementing Spin-block in Fast Messages Fast Messages was enhanced with a spin-block mechanism to support our experimentation. Adding a spin- block communication primitive required changes to the FM firmware, the network device driver, and the FM libraries. <p> The parallel job is a SOR kernel, a two-dimensional Laplace's equation solver on a 128 fi 128 element matrix (written in FORTRAN on an FM-based implementation of MPI <ref> [10] </ref>). The sequential jobs are GNU tar, archiving and compressing a collection of 97 files, totalling 2.1 MB, and Ghostscript, a PostScript interpreter on a 1.7 MB, 103-page PostScript file.
Reference: [11] <author> John K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of the 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: 1 Introduction Coordinated scheduling of parallel jobs across the nodes of a multiprocessor is well-known to produce benefits in both system and individual job efficiency <ref> [11, 17, 4, 16, 3] </ref>. Without coordinated scheduling, the processes constituting a parallel job suffer high communication latencies because of processor thrashing [11]. <p> 1 Introduction Coordinated scheduling of parallel jobs across the nodes of a multiprocessor is well-known to produce benefits in both system and individual job efficiency [11, 17, 4, 16, 3]. Without coordinated scheduling, the processes constituting a parallel job suffer high communication latencies because of processor thrashing <ref> [11] </ref>. While multiprocessor systems typically address these problems with a mix of batch, gang, and timesharing scheduling (based on kernel scheduler changes), the problem is more difficult for shared workstation clusters in which stock operating systems kernels must be run. <p> Finally, Section 6 briefly summarizes our results. 2 Related Work There has been a wide variety of work on coscheduling, beginning with Ousterhout's seminal paper <ref> [11] </ref> which identified the need. <p> Ousterhout provided a basic framework and presents a number of ways to achieve coscheduling while optimizing for system utilization <ref> [11] </ref>. Later work on the DASH multiprocessor [7, 2] demonstrated that coscheduling could be used to achieve efficiencies comparable to batch scheduling, while providing more flexible resource sharing. <p> In many systems, particularly those with shared memory, a gang scheduler which has the capability to achieve coordinated context switches across processors has been assumed <ref> [7, 11, 4, 5] </ref>. Such systems replace the basic process scheduler in the operating system, and schedule the related threads across the processing nodes. <p> It is noted in <ref> [11] </ref> that a two-context-switch maximum spin time is competitive, and in [3] it is argued that two context-switch times might be required for a processor to respond to a message if the message arrives at the beginning of a context switch to a process that is not the one to which
Reference: [12] <author> Scott Pakin, Vijay Karamcheti, and Andrew A. Chien. </author> <title> Fast Messages (FM): Efficient, portable communication for workstation clusters and massively-parallel processors. </title> <journal> IEEE Concurrency, </journal> <year> 1997. </year>
Reference-contexts: With clusters connected by high performance networks that achieve latencies in the range of tens of microseconds <ref> [12, 19, 20, 8, 6] </ref>, scheduling and context switching latency can increase communication latency by several orders of magnitude. <p> This approach can achieve coscheduling without changes to the operating system scheduler or applications programs. Our implementation of dynamic coscheduling is based on the Illinois Fast Messages communication layer which delivers low latency and high bandwidth user-space to user-space communication <ref> [12, 13] </ref>. We augmented this system with blocking communication primitives, and implemented dynamic coscheduling with changes to a device driver, network interface card firmware, and the communication library. <p> Fast Messages is a high- performance messaging layer that bypasses the operating system to provide direct access to an underlying Myricom Myrinet [1] and thereby achieve high performance. Details of FM can be found in <ref> [12, 14] </ref>. We also employ an implementation of the Message Passing Interface (MPI) built atop FM, called MPI-FM [10], for some of the benchmarks. 3.2.2 Implementing Spin-block in Fast Messages Fast Messages was enhanced with a spin-block mechanism to support our experimentation.
Reference: [13] <author> Scott Pakin, Mario Lauria, Matt Buchanan, Kay Hane, Louis Giannini, Jane Prusakova, and Andrew Chien. </author> <title> Fast Messages 2.0 User Documentation, </title> <month> October </month> <year> 1996. </year>
Reference-contexts: This approach can achieve coscheduling without changes to the operating system scheduler or applications programs. Our implementation of dynamic coscheduling is based on the Illinois Fast Messages communication layer which delivers low latency and high bandwidth user-space to user-space communication <ref> [12, 13] </ref>. We augmented this system with blocking communication primitives, and implemented dynamic coscheduling with changes to a device driver, network interface card firmware, and the communication library.
Reference: [14] <author> Scott Pakin, Mario Lauria, and Andrew Chien. </author> <title> High performance messaging on workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Supercomputing, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/ papers/myrinet-fm-sc95.ps. </note>
Reference-contexts: The details of any such implementation embody only a specific instance of a dynamic coscheduling. 4 3.2 Implementation Context 3.2.1 Fast Messages and Myrinet Our dynamic coscheduling prototype is implemented under Illinois Fast Messages (FM), a user-level messaging layer developed at the University of Illinois at Urbana-Champaign <ref> [14] </ref>. Fast Messages is a high- performance messaging layer that bypasses the operating system to provide direct access to an underlying Myricom Myrinet [1] and thereby achieve high performance. Details of FM can be found in [12, 14]. <p> Fast Messages is a high- performance messaging layer that bypasses the operating system to provide direct access to an underlying Myricom Myrinet [1] and thereby achieve high performance. Details of FM can be found in <ref> [12, 14] </ref>. We also employ an implementation of the Message Passing Interface (MPI) built atop FM, called MPI-FM [10], for some of the benchmarks. 3.2.2 Implementing Spin-block in Fast Messages Fast Messages was enhanced with a spin-block mechanism to support our experimentation.
Reference: [15] <author> Patrick G. Sobalvarro. </author> <title> Demand-based Coscheduling of Parallel Jobs on Multiprogrammed Multiprocessors. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1997. </year> <month> 16 </month>
Reference-contexts: The approach to coordinated scheduling that we use is a form of demand-based coscheduling called dynamic coscheduling (DCS) <ref> [16, 15] </ref>, which achieves coordination by observing the communication between threads. This is a bottom-up, emergent scheduling approach that exploits the key observation that only those threads which are communicating need be coscheduled. This approach can achieve coscheduling without changes to the operating system scheduler or applications programs. <p> A variety of implicit schemes which do not require explicit programmer annotation have been explored. On distributed memory systems, the need for coscheduling has typically been associated with communication <ref> [16, 5, 3, 15] </ref>. Feitelson's Runtime Activity Working Set Identification (RAWSI) monitors the communication between processes or threads 3 to determine their rate of communication. Working sets of processes (which require coscheduling) are identified based on their rate of communication. <p> While this appears quite similar to implicit scheduling for the particular case of bulk synchronous jobs using spin-block synchronization, we believe dynamic coscheduling can be used to achieve coordinated scheduling in a broader range of cases. 3 Dynamic Coscheduling 3.1 Overview Demand-based coscheduling <ref> [16, 15] </ref> exploits communication between processes to deduce which processes should be coscheduled and to effect coscheduling. It is effective because of the key observation: the communicating (or synchronizing) processes are the ones that need be coscheduled. <p> The device driver was modified to add a call puts the caller to sleep, waiting for a message. Finally, the FM libraries were modified to integrate these changes. We chose our maximum spin time of 1600 sec based on the empirical evidence of experiments described in <ref> [15] </ref>, in which the maximum delay we saw for response in the case where a context switch was required was approximately 1500 sec. 1600 sec is also slightly greater than twice the mean context-switch time plus the message round-trip time. <p> A simple illustration of the DCS implemenation is shown in Figure 1. Our description of the implementation is necessarily brief; further detail can be found in <ref> [15] </ref>. 3.3.1 Monitoring Communication/Thread Activity Communication and thread monitoring is performance on the network interface card. Myricom's network interface card provides a programmable processor, running the Fast Messages firmware. We modified the FM firmware to monitor the ongoing communication and thread activity. <p> However, because the scheduling quanta are 20 milliseconds or larger, this approximation is sufficient. The mechanisms used for dealing with cases where the information is inaccurate are described in <ref> [15] </ref>. 6 if (running_LWP != FM_LWP) - if (fair to preempt) - for each kernel thread belonging to FM_LWP - raise priority to maximum for user mode; - preempt currently running thread; - 3.3.2 Causing Scheduling Decisions Scheduling decisions are effected by a device driver for the Myrinet network interface card <p> This can be seen from both the reduced job response times and the fact that fairness could be achieved even under spin-block synchronization. More detailed evidence, in the form of histograms of message round-trip times, is presented in <ref> [15] </ref>. The unmodified Solaris 2.4 scheduler also achieved some coscheduling under spin-block synchronization, as is discussed at greater length in Section 5. However, the unmodified Solaris 2.4 scheduler (hNo DCS, spin onlyi) failed to achieve coscheduling under spinning synchronization, as indicated by increased CPU and response times. <p> However, we have tracked this anomaly down an extra 5 microseconds latency being required for each message transmission, because of several extra instructions performed on message receipt under DCS in the Lanai control program's main loop <ref> [15] </ref> . 7 The effect is not apparent for the Barrier benchmark and Mixed Workload benchmark, because additional computation is performed which allows the overhead to be overlapped. 5 Discussion In this section we examine the mechanisms responsible for our experimental results. <p> In effect, the coscheduling allows the use of synchronization strategies previously only viable on batch scheduled or dedicated machines. However, our experiments also show that spinning synchronization under DCS is less efficient than spin-block synchronization (further data are reported in <ref> [15] </ref>). We believe that this problem is exacerbated in Solaris 2.4 7 This effect would be reduced with faster network controllers. 13 disabled. <p> DCS can also be used to coschedule sets of threads, whereas the SVR4 and spin-block combination is only applicable for single thread coscheduling. 5.3 Directions for Future Work Experiments that vary the granularity of communication <ref> [15] </ref> indicate that spin-block message receipt paired with DCS or the unmodified Solaris 2.4 scheduler (with priority boosts on process wakeup) is less successful at coscheduling as the frequency of communication decreases. <p> Such broadening would enable evaluation of a wide range of issues, including the viability of the epoch number mechanism [16] for multitasking parallel jobs. Such efforts are already underway as part of the (name omitted for blind reviewing) Project. Most of the workloads used to study coscheduling <ref> [3, 15] </ref> are quite regular in structure, communication, and computation. Such regularity often fails to exercise the behavioral richness of dynamic systems.
Reference: [16] <author> Patrick G. Sobalvarro and William E. Weihl. </author> <title> Demand-based coscheduling of parallel jobs on multiprogrammed multiprocessors. </title> <booktitle> In Proceedings of the Parallel Job Scheduling Workshop at IPPS '95, </booktitle> <year> 1995. </year> <note> Available from http://www.psg.lcs.mit.edu/~pgs/papers/jsw-for-springer.ps. Also appears in Springer-Verlag Lecture Notes in Computer Science, Vol. 949. </note>
Reference-contexts: 1 Introduction Coordinated scheduling of parallel jobs across the nodes of a multiprocessor is well-known to produce benefits in both system and individual job efficiency <ref> [11, 17, 4, 16, 3] </ref>. Without coordinated scheduling, the processes constituting a parallel job suffer high communication latencies because of processor thrashing [11]. <p> The approach to coordinated scheduling that we use is a form of demand-based coscheduling called dynamic coscheduling (DCS) <ref> [16, 15] </ref>, which achieves coordination by observing the communication between threads. This is a bottom-up, emergent scheduling approach that exploits the key observation that only those threads which are communicating need be coscheduled. This approach can achieve coscheduling without changes to the operating system scheduler or applications programs. <p> A variety of implicit schemes which do not require explicit programmer annotation have been explored. On distributed memory systems, the need for coscheduling has typically been associated with communication <ref> [16, 5, 3, 15] </ref>. Feitelson's Runtime Activity Working Set Identification (RAWSI) monitors the communication between processes or threads 3 to determine their rate of communication. Working sets of processes (which require coscheduling) are identified based on their rate of communication. <p> Working sets of processes (which require coscheduling) are identified based on their rate of communication. RAWSI collects the information and uses a coordinated global mechanism to decide on a schedule. Both our dynamic coscheduling approach <ref> [16] </ref> and implicit scheduling [3] detect threads requiring coscheduling through their communication. However, neither system explicitly identifies the sets of processes to be coscheduled. 2.3 Mechanisms for Achieving Coscheduling Once thread clusters have been identified, a mechanism for coscheduling must be used. <p> Further, such systems have typically not explicitly identified sets of processes to be coscheduled, but rather integrate the detection of a coscheduling requirement 3 Feitelson's system actually addresses both distributed and shared memory systems. 3 with actions to produce effective coscheduling. An earlier paper on dynamic coscheduling <ref> [16] </ref> detailed analysis and simulation of an integrated coscheduling technique. Subsequently, Dusseau's implicit scheduling was evaluated via simulation in [3], using synthetic single-program, multiple data applications. Because dynamic coscheduling is discussed extensively in the remainder of the paper, we only describe implicit scheduling here. <p> While this appears quite similar to implicit scheduling for the particular case of bulk synchronous jobs using spin-block synchronization, we believe dynamic coscheduling can be used to achieve coordinated scheduling in a broader range of cases. 3 Dynamic Coscheduling 3.1 Overview Demand-based coscheduling <ref> [16, 15] </ref> exploits communication between processes to deduce which processes should be coscheduled and to effect coscheduling. It is effective because of the key observation: the communicating (or synchronizing) processes are the ones that need be coscheduled. <p> It is effective because of the key observation: the communicating (or synchronizing) processes are the ones that need be coscheduled. Thus, demand-based coscheduling produces emergent coscheduling without requiring explicit identification by programmer of the computations that need be coscheduled. Dynamic coscheduling <ref> [16] </ref> is a type of demand-based coscheduling in which scheduling decisions are driven directly by message arrivals. If an arriving message is directed to a process that isn't running, a scheduling decision is made. <p> This decision can be based on a wide variety of factors (e.g., system load, last time run, etc.), and is generally designed to maximize coscheduling performance while ensuring fairness of CPU allocation. Previously published modeling and simulation results <ref> [16] </ref> indicate that dynamic coscheduling produces robust coscheduling. Thus, the key elements of dynamic coscheduling are: 1. Monitoring communication/thread activity 2. Causing scheduling decisions 3. Making a decision whether to preempt The latter two points are intimately tied to how the operating system operates. <p> The equalization mechanism described in <ref> [16] </ref> used detailed CPU time numbers. <p> Our current efforts in the context of priority-decay schedulers focus on obtaining more accurate estimates of recent run time to implement an equalization criterion <ref> [16] </ref>. The experiments described in this paper have two key limitations: a single parallel job and a cluster of only seven nodes were used in all benchmarks. Future DCS experiments must include multiple parallel jobs, larger workloads, and larger configurations. <p> Future DCS experiments must include multiple parallel jobs, larger workloads, and larger configurations. Such broadening would enable evaluation of a wide range of issues, including the viability of the epoch number mechanism <ref> [16] </ref> for multitasking parallel jobs. Such efforts are already underway as part of the (name omitted for blind reviewing) Project. Most of the workloads used to study coscheduling [3, 15] are quite regular in structure, communication, and computation. Such regularity often fails to exercise the behavioral richness of dynamic systems.
Reference: [17] <author> Andrew Tucker. </author> <title> Efficient scheduling on multiprogrammed shared-memory multiprocessors. </title> <type> Technical Report CSL-TR-94-601, </type> <institution> Stanford University Department of Computer Science, </institution> <month> November </month> <year> 1993. </year> <note> Available from http://elib.stanford.edu/Dienst/UI/2.0/Describe/stanford.cs/CSL-TR-94-601. </note>
Reference-contexts: 1 Introduction Coordinated scheduling of parallel jobs across the nodes of a multiprocessor is well-known to produce benefits in both system and individual job efficiency <ref> [11, 17, 4, 16, 3] </ref>. Without coordinated scheduling, the processes constituting a parallel job suffer high communication latencies because of processor thrashing [11].
Reference: [18] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM SIGOPS Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-186, </pages> <year> 1989. </year> <note> Available from http://xenon.stanford.edu/~tucker/papers/sosp.ps. </note>
Reference: [19] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: a mechanism for integrated commu-nication and computation. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: With clusters connected by high performance networks that achieve latencies in the range of tens of microseconds <ref> [12, 19, 20, 8, 6] </ref>, scheduling and context switching latency can increase communication latency by several orders of magnitude.
Reference: [20] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www.cs.cornell.edu/Info/Projects/ATM/sosp.ps. </note>
Reference-contexts: With clusters connected by high performance networks that achieve latencies in the range of tens of microseconds <ref> [12, 19, 20, 8, 6] </ref>, scheduling and context switching latency can increase communication latency by several orders of magnitude.
Reference: [21] <author> Carl A. Waldspurger. </author> <title> Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1995. </year> <note> MIT/LCS/TR-667. 17 </note>
Reference-contexts: An automatic mechanism is clearly required for any robust coscheduling system. Obviously, the widespread use of priority-decay schedulers complicates the situation | stride schedulers <ref> [21] </ref>) or other proportional share schedulers would provide a better platform for DCS, by separating the concepts of execution order and processor share. Our current efforts in the context of priority-decay schedulers focus on obtaining more accurate estimates of recent run time to implement an equalization criterion [16].
References-found: 21

