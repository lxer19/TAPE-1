URL: ftp://ftp.cs.city.ac.uk/papers/92/cs92-7.ps.Z
Refering-URL: http://www.cs.umd.edu/~keleher/bib/dsmbiblio/node5.html
Root-URL: 
Title: A DVSM server for MESHIX  
Author: Ashley Saulsbury and Tom Stiemerling 
Note: A subsequent document will describe performance testing of the system.  
Address: 180 Queens Gate, London SW7 2BZ, UK.  Northampton Square, London EC1V 0HB, UK.  
Affiliation: Department of Computing, Imperial College,  Computer Science Department, City University,  
Abstract: This report describes the implementation of distributed virtual shared memory (DVSM) on the Topsy multicomputer. The Topsy machine is a distributed memory multiprocessor based on MC68030 nodes connected by a custom circuit-switched mesh interconnection network (MeshNet), and runs the Meshix operating system which is Unix System V compatible. The DVSM allows distributed processes to share a paged virtual memory region, whose coherence is maintained by user-level servers using the dynamic distributed manager algorithm. The DVSM implementation is described at the user, server and kernel level, and an overview of the relevant parts of the Meshix operating systems is also given. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Winterbottom and P. Osmon, "Topsy: </author> <title> an extensible UNIX multicomputer," </title> <booktitle> in Proceed ings of UK IT90 Conference, </booktitle> <publisher> (Southampton University), </publisher> <pages> pp. 164-176, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: 1 Introduction In this report we describe the design and implementation of distributed virtual shared memory (DVSM) on the Topsy multicomputer <ref> [1] </ref>. DVSM provides the functionality of shared memory within the virtual memory structure of loosely coupled microprocessors which have their own independent local memory. The operating system for Topsy is Meshix [2], which is interface compatible with System V Unix (release 3.0). <p> Pre-fetching of cache lines is also supported. A prototype implementation has been constructed using Silicon Graphics R3000-based multiprocessors as the clusters. 3 An overview of the TOPSY machine The Topsy machine is a distributed memory multicomputer built at City University <ref> [1] </ref>, and consists of processor-memory nodes connected by a network. Topsy is intended to be scalable to any size (although present networking hardware limits it to 256 nodes) through its use of distributed data management strategies.
Reference: [2] <author> P. Winterbottom and T. Wilkinson, "MESHIX: </author> <title> a UNIX like operating system for distributed machines," </title> <booktitle> in UKUUG Summer Conference Proceedings, </booktitle> <pages> pp. 237-246, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: DVSM provides the functionality of shared memory within the virtual memory structure of loosely coupled microprocessors which have their own independent local memory. The operating system for Topsy is Meshix <ref> [2] </ref>, which is interface compatible with System V Unix (release 3.0). The DVSM allows distributed Unix processes to share paged virtual memory regions transparently across the machine. Coherence of the DVSM region is maintained by user-level server processes based on the dynamic distributed manager algorithm of Li [3]. <p> The network, named MeshNet [34], provides multiple point-to-point connections of 10 Mbytes per channel. All communications between nodes of the machine takes place through the network. 4 An overview of the MESHIX operating system Meshix <ref> [2] </ref> is a distributed, micro-kernel based implementation of System V Unix 1 . The kernel functionality has been separated into a number of privileged server processes executing concurrently on each node of the system.
Reference: [3] <author> K. Li and P. Hudak, </author> <title> "Memory coherence in shared virtual memory systems," </title> <journal> ACM Trans actions on Computer Systems, </journal> <volume> vol. 7, </volume> <pages> pp. 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The DVSM allows distributed Unix processes to share paged virtual memory regions transparently across the machine. Coherence of the DVSM region is maintained by user-level server processes based on the dynamic distributed manager algorithm of Li <ref> [3] </ref>. The report is arranged as follows. In Section 2 we review previous work on distributed shared memory. In Section 3 we describe the organisation of the Topsy multicomputer, and the Meshix operating system. <p> For example, assuming user-level DVSM servers, then the distribution of the servers, the ownership algorithm and how updates are propagated, must be chosen <ref> [14, 15, 3] </ref>. There can either be a single central server or a server distributed on every node. With a central server all requests for pages must go to that server, which would soon be a bottleneck. <p> Almost every project uses its own acronym for DVSM, and so we use this to cover all the other descriptions, such as DSM, SVM, DVSM, etc. 2.6.1 IVY K.Li and P.Hudak. Yale University. 1986 <ref> [16, 3] </ref>. IVY is a DVSM programming environment for a network of Apollo workstations. IVY provides a thread based library with procedures for process control and memory management. <p> Each server process handles the DVSM requests made by processes on its local node, and communicates with the servers on the other nodes to satisfy these requests. The DVSM servers use the well known dynamic distributed manager algorithm of Li <ref> [3] </ref> to maintain strong coherence of the DVSM. Processes which make use of the DVSM access it as if it were part of their normal data space. No special function calls are required to interact with the DVSM, apart from initially attaching a process to it. <p> In this manner an ownership message incorrectly sent to the wrong node is forwarded on until it reaches the actual owning node. The limit for the number of messages to locate the page owner in an N node system is therefore N 1 <ref> [3] </ref>. 5.1.4 Invalidation When there are copies of a page besides the owning node, then the page on the owning node is write protected. Therefore when a process on the owning node tries to write to a page a write fault is generated. <p> Having received all the acknowledgements, the server on the owning node makes the page writable and re-schedules the faulted process to complete the write. 2 Here we differ from the mechanism of Li <ref> [3] </ref>, in which the new owner invalidates the page copies after having been passed the copy set. <p> This implementation is based on user-level server processes distributed across the machine, which maintain coherence of the paged DVSM region using invalidation <ref> [3] </ref>. The DVSM servers communicate with the operating system using an external-pager interface. Simple spin-lock synchronisation primitives are provided for synchronisation within the DVSM, and virtual memory paging of the DVSM region was integrated into the existing Meshix paging mechanism.
Reference: [4] <author> M.-C. Tam, J. Smith, and D. Farber, </author> <title> "A taxonomy-based comparison of several distributed shared memory systems," </title> <journal> ACM Operating Systems Review, </journal> <volume> vol. 24, </volume> <pages> pp. 40-67, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Research into DVSM is very popular, and over 15 different systems have been reported in the literature (see review articles <ref> [4, 5] </ref>). <p> Synchronisation is accomplished using explicit locking of the shared memory and event count structures. 2.6.2 MemNet G.Delp and D.Farber. University of Delaware. 1986 <ref> [17, 4] </ref>. Memnet is a hardware DVSM system. A global address space is shared across a custom token-ring network and used for inter-node communication, replacing the conventional message-based communication. The shared address space is part of each nodes physical address space, and shared in 32-byte pages.
Reference: [5] <author> B. Nitzberg and V. Lo, </author> <title> "Distributed shared memory: A survey of issues and algorithms," </title> <journal> IEEE Computer, </journal> <volume> vol. 24, </volume> <month> August </month> <year> 1991. </year>
Reference-contexts: Research into DVSM is very popular, and over 15 different systems have been reported in the literature (see review articles <ref> [4, 5] </ref>). <p> Other models are processor [11], weak [12] and release coherence [13]. The definitions of these models are rather complex and subtle, and we will not define them fully here. The short summary below is from <ref> [5] </ref>. * In processor coherence writes are seen in order on one node, but the order of writes between nodes may be different. * In weak coherence the consistency must be enforced by the programmer using explicit synchronisation mechanisms which are sequentially coherent. * In release coherence the programmer again must
Reference: [6] <author> T. Wilkinson, T. Stiemerling, P. Osmon, A. Saulsbury, and P. Kelly, "Angel: </author> <title> a proposed multiprocessor operating system," </title> <booktitle> in European Workshops on Parallel Computing 92, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: Systems in which the DVSM management has been incorporated into the operating system kernel, and is the basis for all virtual memory management, are also being researched. Examples are Clouds and Angel <ref> [6] </ref>. Such systems should have higher performance than server implementations due to the reduced message and domain-crossing overheads. There are also hardware DVSM systems in which hardware support is provided for DVSM management. Examples are PLUS, DASH and MemNet.
Reference: [7] <author> J. Archibald and J. Baer, </author> <title> "Cache coherance protocols: Evaluation using a multiprocessor simulation model," </title> <journal> ACM Transactions of computer systems, </journal> <pages> pp. 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: If a program assumes a certain coherence model then the data dependences of the program may be violated if it is executed on a machine that does not guarantee that model. Similar issues are found in cache coherent shared memory multiprocessor systems <ref> [7] </ref>. If strict (or strong) coherence is used, then a read must return the value most recently written to a memory location. The processor caches in bus-based shared memory multiprocessors are generally kept strictly coherent, using bus-snooping for example [8].
Reference: [8] <author> M. Vernon, E. Lazowska, and J. Zahorjan, </author> <title> "An accurate and efficient performance analysis technique for multiprocessor snooping cache-consistency protocols," </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pp. 308-315, </pages> <year> 1988. </year>
Reference-contexts: If strict (or strong) coherence is used, then a read must return the value most recently written to a memory location. The processor caches in bus-based shared memory multiprocessors are generally kept strictly coherent, using bus-snooping for example <ref> [8] </ref>. A DVSM can be kept strict by making sure that a write is propagated to all copies before any further reads (or writes) are allowed. Having a strict DVSM allows programs from shared memory machines to be ported with no change.
Reference: [9] <author> L. Lamport, </author> <title> "How to make a multiprocessor computer that correctly executes multiproces sor programs," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 28, </volume> <pages> pp. 690-691, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: A DVSM can be kept strict by making sure that a write is propagated to all copies before any further reads (or writes) are allowed. Having a strict DVSM allows programs from shared memory machines to be ported with no change. In sequential coherence <ref> [9] </ref>, the order of writes must be equivalent to some potential sequential ordering when the program is executed on a multiprogrammed uniprocessor. Shared memory multiprocessors using multi-stage networks are sequentially consistent, since the exact order of writes cannot be guaranteed (a property of the network) [10].
Reference: [10] <author> M. Dubois, C. Scheurich, and F. Briggs, </author> <title> "Memory access buffering in multiprocessors," </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pp. 434-442, </pages> <year> 1986. </year>
Reference-contexts: In sequential coherence [9], the order of writes must be equivalent to some potential sequential ordering when the program is executed on a multiprogrammed uniprocessor. Shared memory multiprocessors using multi-stage networks are sequentially consistent, since the exact order of writes cannot be guaranteed (a property of the network) <ref> [10] </ref>. Other models are processor [11], weak [12] and release coherence [13]. The definitions of these models are rather complex and subtle, and we will not define them fully here.
Reference: [11] <author> J. Goodman, M. Vernon, and P. Woest, </author> <title> "Efficient synchronisation primitives for large-scale cache coherent multiprocessors," </title> <booktitle> in Proceedings 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Shared memory multiprocessors using multi-stage networks are sequentially consistent, since the exact order of writes cannot be guaranteed (a property of the network) [10]. Other models are processor <ref> [11] </ref>, weak [12] and release coherence [13]. The definitions of these models are rather complex and subtle, and we will not define them fully here.
Reference: [12] <author> A. Adve and M. Hill, </author> <title> "Weak ordering | a new definition," </title> <booktitle> in Proceedings of the Interna tional Symposium on Computer Architecture, </booktitle> <pages> pp. 2-14, </pages> <year> 1990. </year>
Reference-contexts: Shared memory multiprocessors using multi-stage networks are sequentially consistent, since the exact order of writes cannot be guaranteed (a property of the network) [10]. Other models are processor [11], weak <ref> [12] </ref> and release coherence [13]. The definitions of these models are rather complex and subtle, and we will not define them fully here.
Reference: [13] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy, </author> <title> "Memory consistency and event ordering in scalable shared-memory multiprocessors," </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pp. 15-26, </pages> <year> 1990. </year>
Reference-contexts: Shared memory multiprocessors using multi-stage networks are sequentially consistent, since the exact order of writes cannot be guaranteed (a property of the network) [10]. Other models are processor [11], weak [12] and release coherence <ref> [13] </ref>. The definitions of these models are rather complex and subtle, and we will not define them fully here. <p> A fixed distributed ownership strategy with write-invalidation is used to maintain strict coherence. Swapping of pages to other nodes is supported. A separate token-passing synchronisation mechanism is used. 2.6.13 DASH D.Lenoski, K.Ghacherloo, W-D.Weber, A.Gupta and J.Hennessey. Stanford University. 1991 <ref> [13, 33] </ref>. DASH is a cache-coherent scalable multiprocessor design. The processors are grouped into clusters on a shared bus, and the clusters are connected by a mesh network. Coherence in a cluster is maintained using bus-snooping, and a directory coherence protocol is used to keep the clusters coherent.
Reference: [14] <author> M. Stumm and S. Zhou, </author> <title> "Algorithms implementing distributed shared memory," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 54-64, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For example, assuming user-level DVSM servers, then the distribution of the servers, the ownership algorithm and how updates are propagated, must be chosen <ref> [14, 15, 3] </ref>. There can either be a single central server or a server distributed on every node. With a central server all requests for pages must go to that server, which would soon be a bottleneck.
Reference: [15] <author> R. Kessler and M. Livny, </author> <title> "An analysis of distributed shared memory algorithms," </title> <booktitle> in Proceed ings of the 1989 International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 498-505, </pages> <year> 1989. </year>
Reference-contexts: For example, assuming user-level DVSM servers, then the distribution of the servers, the ownership algorithm and how updates are propagated, must be chosen <ref> [14, 15, 3] </ref>. There can either be a single central server or a server distributed on every node. With a central server all requests for pages must go to that server, which would soon be a bottleneck.
Reference: [16] <author> K. Li, "IVY: </author> <title> a shared virtual memory system for parallel computing," </title> <booktitle> in Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. 94-101, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Almost every project uses its own acronym for DVSM, and so we use this to cover all the other descriptions, such as DSM, SVM, DVSM, etc. 2.6.1 IVY K.Li and P.Hudak. Yale University. 1986 <ref> [16, 3] </ref>. IVY is a DVSM programming environment for a network of Apollo workstations. IVY provides a thread based library with procedures for process control and memory management.
Reference: [17] <author> G. Delp and D. Farber, "MemNet: </author> <title> an experiment on high-speed memory mapped network interface," </title> <type> Tech. Rep. </type> <institution> 85-11-IR, Computer Science Department, University of Delaware, </institution> <year> 1986. </year>
Reference-contexts: Synchronisation is accomplished using explicit locking of the shared memory and event count structures. 2.6.2 MemNet G.Delp and D.Farber. University of Delaware. 1986 <ref> [17, 4] </ref>. Memnet is a hardware DVSM system. A global address space is shared across a custom token-ring network and used for inter-node communication, replacing the conventional message-based communication. The shared address space is part of each nodes physical address space, and shared in 32-byte pages.
Reference: [18] <author> R. Campbell, V. Russo, and G. Johnston, </author> <title> "The design of a multiprocessor operating sys tem," </title> <booktitle> in Proceedings of the USENIX C++ Workshop, </booktitle> <pages> pp. 109-123, </pages> <year> 1987. </year>
Reference-contexts: Each node is connected to the network by a Memnet device which is used to broadcast read requests, and to snoop for writes to cached pages which are then invalidated, similar to a shared memory bus. 2.6.3 Choices R.Campbell, V.Russo and G.Johnston. University of Illinois (U-C). 1988 <ref> [18] </ref> Choices is an object-oriented multiprocessor operating system. Memory objects are used to define protection domains, and each memory object has an associated memory object cache which handles paging of the object. A DVSM external pager has been implemented as a Choices memory object cache on networked 4-processor Encore Multimaxes.
Reference: [19] <author> N. Accetta, W. Bolosky, D. Golub, R. Rashid, A. Tevanian, and M. Young, </author> <title> "MACH: A new kernel foundation for UNIX development," </title> <booktitle> in USENIX Summer Conference, </booktitle> <month> July </month> <year> 1986. </year>
Reference-contexts: A dynamic distributed ownership strategy with write-invalidation is used to maintain strict coherence. Explicit lock variables in the DVSM are used for synchronisation. 2.6.4 Mach A.Forin, J.Barrera, M.Young and R.Rashid. Carnegie Mellon University. 1988 <ref> [19, 20, 21] </ref>. Mach is a distributed Unix compatible operating system. The kernel interface allows external pagers to manage memory objects. A DVSM external pager has been implemented which uses a dynamic distributed ownership strategy and write-invalidation to maintain strict page coherence. Support for heterogeneous systems and fault-tolerance is included.
Reference: [20] <author> A. Forin, J. Barrera, M. Young, and R. Rashid, </author> <title> "Design, implementation, and performance evaluation of a distributed shared memory server for Mach," </title> <type> Tech. Rep. </type> <institution> CMU-CS-88-165, Carnegie-Mellon University Computer Science Department, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: A dynamic distributed ownership strategy with write-invalidation is used to maintain strict coherence. Explicit lock variables in the DVSM are used for synchronisation. 2.6.4 Mach A.Forin, J.Barrera, M.Young and R.Rashid. Carnegie Mellon University. 1988 <ref> [19, 20, 21] </ref>. Mach is a distributed Unix compatible operating system. The kernel interface allows external pagers to manage memory objects. A DVSM external pager has been implemented which uses a dynamic distributed ownership strategy and write-invalidation to maintain strict page coherence. Support for heterogeneous systems and fault-tolerance is included. <p> Once the page is again resident, all those waiting on it are woken. 5 The DVSM strategy We now describe the Topsy DVSM server. In implementing DVSM on Topsy we have used an external-pager approach, similar to Mach <ref> [20] </ref>. Every node on the Topsy machine has a user-level DVSM server process executing on it. Each server process handles the DVSM requests made by processes on its local node, and communicates with the servers on the other nodes to satisfy these requests. <p> An external-pager interface had to be added to the Meshix kernel first, though. The functionality currently provided by this interface was defined by the requirements of the DVSM server, and it is therefore not a general virtual memory interface such as those provided by Mach <ref> [20] </ref> or Chorus [36]. 5.1 Dynamic distributed manager algorithm The dynamic distributed manager algorithm is so-called because the manager (or server) controlling a page in the DVSM can vary dynamically, and the servers are distributed around the machine.
Reference: [21] <author> A. Forin, J. Barrera, and R. Sanzi, </author> <title> "The shared memory server," </title> <booktitle> in Proceedings of the USENIX|Winter 1989 Technical Conference, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: A dynamic distributed ownership strategy with write-invalidation is used to maintain strict coherence. Explicit lock variables in the DVSM are used for synchronisation. 2.6.4 Mach A.Forin, J.Barrera, M.Young and R.Rashid. Carnegie Mellon University. 1988 <ref> [19, 20, 21] </ref>. Mach is a distributed Unix compatible operating system. The kernel interface allows external pagers to manage memory objects. A DVSM external pager has been implemented which uses a dynamic distributed ownership strategy and write-invalidation to maintain strict page coherence. Support for heterogeneous systems and fault-tolerance is included.
Reference: [22] <author> S. Zhou, M. Stumm, K. Li, and D. Wortman, </author> <title> "Heterogeneous distributed shared memory," </title> <type> Tech. Rep., </type> <institution> Computer Systems Research Institute, Toronto University, </institution> <year> 1988. </year>
Reference-contexts: Support for heterogeneous systems and fault-tolerance is included. A separate pager has also been implemented which maps files directly into virtual memory. 4 2.6.5 Mermaid S.Zhou, M.Stumm, K.Li and D.Wortman. Toronto University. 1988 <ref> [22] </ref>. Mermaid is a prototype heterogeneous DVSM programming environment running on networked Sun 3 and DEC Firefly workstations. Strict coherence of the DSM is maintained by user-level servers on each workstation using a fixed-distributed ownership strategy and broadcast write-invalidation.
Reference: [23] <author> K. Li and R. Schaefer, </author> <title> "A Hypercube Shared Virtual Memory System," </title> <booktitle> in Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. 125-132, </pages> <year> 1989. </year>
Reference-contexts: Pages transferred between different computer types are first be converted to the appropriate format. A separate, message based, distributed thread synchronisation module provides semaphore and event synchronisation. 2.6.6 SHIVA K.Li and P.Schaefer. Princeton University. 1989 <ref> [23] </ref>. SHIVA is DVSM programming environment on an iPSC/2 hypercube containing i386 processors (up to 128 nodes). Fixed distributed servers maintain strict coherence using write-invalidation, and run on top of a small message-passing kernel on each node. Swapping of shared memory pages onto backing store and other nodes is supported.
Reference: [24] <author> R. Minnich and D. Farber, </author> <title> "The Mether system: distributed shared memory for SunOS 4.0," </title> <booktitle> in Proceedings of the USENIX|Summer 1989 Technical Conference, </booktitle> <pages> pp. 51-60, </pages> <year> 1989. </year>
Reference-contexts: A semaphore migrates around nodes, and a mechanism similar to dynamic distributed server is used to find the semaphore. 2.6.7 Mether R.Minnich and D.Farber. University of Pennsylvania. 1989 <ref> [24, 25] </ref>. Mether is a server based DVSM system implemented on networked Sun workstations (running SunOs 4.0). The mmap call is used to map a special shared file into processes address space. A kernel driver is used for network page transmission and a user-level server maintains coherence of the pages.
Reference: [25] <author> R. Minnich and D. Farber, </author> <title> "Reducing host load, network load, and latency in a distributed shared memory," </title> <booktitle> in Proceedings of the 1990 International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 468-475, </pages> <year> 1990. </year>
Reference-contexts: A semaphore migrates around nodes, and a mechanism similar to dynamic distributed server is used to find the semaphore. 2.6.7 Mether R.Minnich and D.Farber. University of Pennsylvania. 1989 <ref> [24, 25] </ref>. Mether is a server based DVSM system implemented on networked Sun workstations (running SunOs 4.0). The mmap call is used to map a special shared file into processes address space. A kernel driver is used for network page transmission and a user-level server maintains coherence of the pages.
Reference: [26] <author> B. Fleisch, </author> <title> "Distributed shared memory in a loosely coupled distributed system," </title> <booktitle> in Pro ceedings of the Symposium on Operating Systems Principles, </booktitle> <pages> pp. 317-327, </pages> <year> 1988. </year>
Reference-contexts: A variety of coherence mechanisms are supported including a data-driven as well as a demand-driven protocol. Incoherent copies of pages are allowed and a short page size (32 bytes) can be used as the unit of coherence. 2.6.8 Mirage B.D.Fleisch and G.J.Popek. UCLA. 1989 <ref> [26, 27] </ref>. Mirage is a kernel DVSM system, which aims to extend the System V IPC shared memory interface transparently across a network. A prototype system has been implemented on networked VAX-11's running the LOCUS distributed operating system.
Reference: [27] <author> B. Fleisch and G. Popek, </author> <title> "Mirage: a coherent distributed shared memory design," </title> <booktitle> in Pro ceedings of the Symposium on Operating Systems Principles, </booktitle> <pages> pp. 211-223, </pages> <year> 1989. </year>
Reference-contexts: A variety of coherence mechanisms are supported including a data-driven as well as a demand-driven protocol. Incoherent copies of pages are allowed and a short page size (32 bytes) can be used as the unit of coherence. 2.6.8 Mirage B.D.Fleisch and G.J.Popek. UCLA. 1989 <ref> [26, 27] </ref>. Mirage is a kernel DVSM system, which aims to extend the System V IPC shared memory interface transparently across a network. A prototype system has been implemented on networked VAX-11's running the LOCUS distributed operating system.
Reference: [28] <author> R. Bisiani and M. Ravishankar, </author> <title> "PLUS: a distributed shared-memory system," </title> <booktitle> in Proceed ings of the International Symposium on Computer Architecture, </booktitle> <pages> pp. 115-124, </pages> <year> 1990. </year>
Reference-contexts: An attempt is made to reduce the effect of thrashing by allowing processes a variable time window in which to access a page. Synchronisation was provided by using the test&set instruction on a shared variable. 2.6.9 PLUS R.Bisiani and M.Ravishankar. Carnegie Mellon University. 1990 <ref> [28] </ref>. PLUS is a distributed memory multiprocessor system with hardware DVSM control. Nodes are connected by a mesh network which is interfaced to a memory-coherence manager on each node. The manager implements a non-demand, write-update coherence protocol for shared pages.
Reference: [29] <author> R. Ananthanarayanan, S. Menon, A. Mohindra, and U. Ramachandran, </author> <title> "On the Integration of Distributed Shared Memory with Virtual Memory Management," </title> <type> Tech. Rep. </type> <institution> GIT-CC-90/40, College of Computing, Georgia Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: The manager implements a non-demand, write-update coherence protocol for shared pages. Special delayed synchronisation primitives are provided, including simple test-and-set's up to more complex queuing operations. A prototype system with MC88000 processors has been constructed. 5 2.6.10 Clouds R.Ananthanarayanan, S.Menon, A.Mohindra and U.Ramachandran. Georgia Institute of Technology. 1990 <ref> [29] </ref>. Clouds is an object-oriented distributed operating system. Clouds provides the abstraction of a single distributed shared global virtual address space. DVSM is therefore supported by the Clouds kernel, and integrated into the virtual memory management of Clouds.
Reference: [30] <author> J. Carter, J. Bennett, and W. Zwaenepoel, "Munin: </author> <title> distributed shared memory based on type-specific memory coherence," </title> <booktitle> in Proceedings of the 2nd Symposium on the Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 168-176, </pages> <year> 1990. </year>
Reference-contexts: Both strict and weak coherence are maintained using fixed ownership and a release protocol, which combines data transfer and synchronisation. Clouds has been implemented on networked (Ethernet) Sun workstations. 2.6.11 Munin J.Carter, J.Bennett and W.Zwaenepoel. Rice University. 1990 <ref> [30, 31] </ref> Munin is a DVSM system and programming environment for distributed memory systems, and is distinguished by it's support for type-specific memory coherence. By annotating shared memory programs the required coherence protocol can be used for each shared datum.
Reference: [31] <author> J. Carter, J. Bennett, and W. Zwaenepoel, </author> <title> "Implementation and performance of Munin," </title> <booktitle> in Proceedings of the Symposium on Operating Systems Principles, </booktitle> <pages> pp. 152-164, </pages> <year> 1991. </year>
Reference-contexts: Both strict and weak coherence are maintained using fixed ownership and a release protocol, which combines data transfer and synchronisation. Clouds has been implemented on networked (Ethernet) Sun workstations. 2.6.11 Munin J.Carter, J.Bennett and W.Zwaenepoel. Rice University. 1990 <ref> [30, 31] </ref> Munin is a DVSM system and programming environment for distributed memory systems, and is distinguished by it's support for type-specific memory coherence. By annotating shared memory programs the required coherence protocol can be used for each shared datum.
Reference: [32] <author> Z. Lahjomri and T. Priol, "KOAN: </author> <title> a Shared Virtual Memory for the iPSC/2 hypercube," </title> <type> Tech. Rep. 597, </type> <institution> IRISA, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Release consistency is supported using a delayed update queue in which multiple updates are merged until they are propagated when convenient. A prototype Munin system has been implemented on networked Sun workstations, using the V kernel to provide low level services. 2.6.12 KOAN Z. Lahjomri and T.Priol. IRISA. 1991 <ref> [32] </ref>. KOAN is a DVSM implementation on an iPSC/2 hypercube. The DVSM system interfaces to the NX/2 kernel on each node, and no other thread and memory management facilities are supplied. A fixed distributed ownership strategy with write-invalidation is used to maintain strict coherence.
Reference: [33] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, and J. Hennessy, </author> <title> "Overview and status of the Stanford DASH multiprocessor," </title> <booktitle> in Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pp. 102-108, </pages> <year> 1991. </year>
Reference-contexts: A fixed distributed ownership strategy with write-invalidation is used to maintain strict coherence. Swapping of pages to other nodes is supported. A separate token-passing synchronisation mechanism is used. 2.6.13 DASH D.Lenoski, K.Ghacherloo, W-D.Weber, A.Gupta and J.Hennessey. Stanford University. 1991 <ref> [13, 33] </ref>. DASH is a cache-coherent scalable multiprocessor design. The processors are grouped into clusters on a shared bus, and the clusters are connected by a mesh network. Coherence in a cluster is maintained using bus-snooping, and a directory coherence protocol is used to keep the clusters coherent.
Reference: [34] <author> T. Wilkinson, A. Whitcroft, P. Winterbottom, and P. Osmon, </author> <title> "The Meshnet Multi-stage Communications Network," </title> <type> Tech. Rep., </type> <institution> City University Computer Science Department, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Message system, context switch, scheduling, Device driver processes System server processes User processes Meshix The nodes are connected by a circuit-switched network with a toroidal topology. The network, named MeshNet <ref> [34] </ref>, provides multiple point-to-point connections of 10 Mbytes per channel. All communications between nodes of the machine takes place through the network. 4 An overview of the MESHIX operating system Meshix [2] is a distributed, micro-kernel based implementation of System V Unix 1 .
Reference: [35] <author> M. Bach, </author> <title> The Design of the UNIX Operating System. </title> <publisher> Prentice Hall, Inc, </publisher> <year> 1986. </year> <month> 26 </month>
Reference-contexts: In a similar way to the message passing primitives, the migrate system call uses a relative address to specify the destination node for the process being exec'ed. 4.4 Memory management Meshix provides the standard Unix virtual memory model <ref> [35] </ref>. In implementation it is different from those of either System V or Berkeley Unix, because of the separation of the filesystem from the process management section of the kernel.
Reference: [36] <author> V. Abrossimov, M. Rozier, and M. Gien, </author> <title> "Virtual Memory Management in Chorus," </title> <type> Tech. Rep. </type> <institution> CS/TR-89-30.1, Chorus systemes, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: An external-pager interface had to be added to the Meshix kernel first, though. The functionality currently provided by this interface was defined by the requirements of the DVSM server, and it is therefore not a general virtual memory interface such as those provided by Mach [20] or Chorus <ref> [36] </ref>. 5.1 Dynamic distributed manager algorithm The dynamic distributed manager algorithm is so-called because the manager (or server) controlling a page in the DVSM can vary dynamically, and the servers are distributed around the machine.
Reference: [37] <institution> UNIX System V Release 3. AT&T, </institution> <year> 1986. </year>
Reference-contexts: To allow programs written for other shared memory interfaces to be ported onto Topsy it is necessary either to rewrite the relevant parts of the program, or to provide an alternative (compatible) shared memory interface 3 . An example is the Unix System V shared memory library <ref> [37] </ref>, which is part of a larger IPC facility. The System V shared memory calls are similar to the one's described above (since these are derived from the System V calls), but differ in a number of details.
Reference: [38] <author> J. Crammond, </author> <title> "The Abstract Machine and Implementation of Parallel Parlog," </title> <type> Tech. Rep., </type> <institution> Department of Computing, Imperial College, </institution> <month> July </month> <year> 1990. </year> <month> 27 </month>
Reference-contexts: A subsequent report will include a performance analysis of the DVSM implementation, and suggest some ways of increasing performance. Further development to the DVSM system is likely to be application driven. Colleagues have expressed interest in porting some applications onto the DVSM, such as a parallel Parlog system (JAM <ref> [38] </ref>) and the Postgres database system.
References-found: 38

