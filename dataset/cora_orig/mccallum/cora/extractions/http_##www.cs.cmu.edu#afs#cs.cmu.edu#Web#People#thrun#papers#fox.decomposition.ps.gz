URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/fox.decomposition.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/full.html
Root-URL: http://www.cs.cmu.edu
Title: LEARNING BY ERROR-DRIVEN DECOMPOSITION  
Author: Dieter Fox Volker Heinze Knut Moller Sebastian Thrun yz Gerd Veenker 
Address: D 5300 Bonn 1, FR GERMANY D 5205 St. Augustin, FR GERMANY  
Affiliation: yComputer Science Department zGerman National Research Center Bonn University, Romerstr. 164 for Computer Science, Postfach 1240  
Date: June 1991  
Note: To appear in: O. Simula (ed.) Proceedings of the International Conference on Artificial Neural Networks, ICANN-91, Elsevier Science Publishers,  
Abstract: In this paper we describe a new selforganizing decomposition technique for learning high-dimensional mappings. Problem decomposition is performed in an error-driven manner, such that the resulting subtasks (patches) are equally well approximated. Our method combines an unsupervised learning scheme (Feature Maps [Koh84]) with a nonlinear approximator (Backpropagation [RHW86]). The resulting learning system is more stable and effective in changing environments than plain backpropagation and much more powerful than extended feature maps as proposed by [RS88, RMS89]. Extensions of our method give rise to active exploration strategies for autonomous agents facing unknown environments. The appropriateness of our general purpose method will be demonstrated with an ex ample from mathematical function approximation.
Abstract-found: 1
Intro-found: 1
Reference: [BH89] <author> E. Baum and D. Haussler. </author> <title> What size net gives valid generalization. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <pages> pages 81-89, </pages> <publisher> IEEE, Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference-contexts: Some approaches have already been proposed to eliminate the need for repeated trials with different architectures, such as the theoretical results on minimal hidden layer size <ref> [BH89] </ref> and constructive/destructive algorithms [FL90, Cha89]. In this paper we propose a radically different approach. Instead of constructing an architecture that fits a problem or function, we are decomposing the problem by splitting its input domain. Subdivision of input space is accomplished by an extended feature map.
Reference: [BP84] <author> M. Bercovier and T. Pat. </author> <title> A C 0 finite element method for the analysis of inextensible pipe lines. </title> <journal> Computers and Structures, </journal> <volume> 18(6) </volume> <pages> 1019-1023, </pages> <year> 1984. </year>
Reference-contexts: Our approach can be seen as an application of the devide and conquer-principle (or problem decomposition) that has a long tradition in computer science [HS78] and artificial neural network learning [JJB90, MT90]. Its usefulness is widely accepted (e.g. finite element method <ref> [BP84] </ref> in numerical mathematics, patching [Far90] in geometric modeling). But traditional methods suffer from the fact that either the decomposition has to be given in advance or is performed in a strict, predetermined manner (e.g. splitting inhalves). We introduce an error-driven technique to automatically derive a suitable task decomposition.
Reference: [Cha89] <author> Y. Chauvin. </author> <title> A back-propagation algorithm with optimal use of hidden units. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <pages> pages 519-526, </pages> <publisher> IEEE, Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference-contexts: Some approaches have already been proposed to eliminate the need for repeated trials with different architectures, such as the theoretical results on minimal hidden layer size [BH89] and constructive/destructive algorithms <ref> [FL90, Cha89] </ref>. In this paper we propose a radically different approach. Instead of constructing an architecture that fits a problem or function, we are decomposing the problem by splitting its input domain. Subdivision of input space is accomplished by an extended feature map.
Reference: [Far90] <author> G. Farin. </author> <title> Curves and Surfaces for Computer Aided Geometric Design. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1990. </year>
Reference-contexts: Our approach can be seen as an application of the devide and conquer-principle (or problem decomposition) that has a long tradition in computer science [HS78] and artificial neural network learning [JJB90, MT90]. Its usefulness is widely accepted (e.g. finite element method [BP84] in numerical mathematics, patching <ref> [Far90] </ref> in geometric modeling). But traditional methods suffer from the fact that either the decomposition has to be given in advance or is performed in a strict, predetermined manner (e.g. splitting inhalves). We introduce an error-driven technique to automatically derive a suitable task decomposition.
Reference: [FL90] <author> S. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <pages> pages 524-532, </pages> <publisher> IEEE, Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Some approaches have already been proposed to eliminate the need for repeated trials with different architectures, such as the theoretical results on minimal hidden layer size [BH89] and constructive/destructive algorithms <ref> [FL90, Cha89] </ref>. In this paper we propose a radically different approach. Instead of constructing an architecture that fits a problem or function, we are decomposing the problem by splitting its input domain. Subdivision of input space is accomplished by an extended feature map.
Reference: [HS78] <author> E. Horowitz and S. Sahni. </author> <title> Fundamentals of computer algorithms. </title> <publisher> Computer Science Press, </publisher> <year> 1978. </year>
Reference-contexts: Our approach can be seen as an application of the devide and conquer-principle (or problem decomposition) that has a long tradition in computer science <ref> [HS78] </ref> and artificial neural network learning [JJB90, MT90]. Its usefulness is widely accepted (e.g. finite element method [BP84] in numerical mathematics, patching [Far90] in geometric modeling).
Reference: [JJB90] <author> R. A. Jacobs, M. I. Jordan, and A. G. Barto. </author> <title> Task decomposition through competition in a modular connectionist architecture: The what and where vision task. </title> <type> Technical Report COINS TR 90-27, </type> <institution> Dept. of Brain and Cognitive Science, Massachusetts Institute of Technology, Cambridge,MA, </institution> <year> 1990. </year>
Reference-contexts: Our approach can be seen as an application of the devide and conquer-principle (or problem decomposition) that has a long tradition in computer science [HS78] and artificial neural network learning <ref> [JJB90, MT90] </ref>. Its usefulness is widely accepted (e.g. finite element method [BP84] in numerical mathematics, patching [Far90] in geometric modeling). But traditional methods suffer from the fact that either the decomposition has to be given in advance or is performed in a strict, predetermined manner (e.g. splitting inhalves).
Reference: [Koh84] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer, </publisher> <address> Berlin New York, </address> <year> 1984. </year>
Reference-contexts: 2 + y 2 ) 2)e (x 2 +y 2 ) We start with approximation by a discrete step function using an extended feature map algorithm. 2.1 Approximation with a step function In order to approximate a multidimensional function, we first introduce a technique which is related to feature maps <ref> [Koh84, RMS89] </ref>. In this approach the input space is devided into small regions, each of which is represented by a feature vector.
Reference: [Moe91] <author> K. Moller. </author> <title> Error-driven decomposition. </title> <type> Technical Report, </type> <institution> Bonn University, </institution> <note> in preparation, </note> <year> 1991. </year>
Reference-contexts: Plot of approximation (tss=1.24). In figure 3 results after 900,000 presentations are shown using this update rule. Please note the distribution of units that is driven by the approximation error. A discussion of convergence properties of this method may be found in <ref> [Moe91] </ref>. So far we presented an error-driven decomposition method whose approximation capabilities are limited due to it's stepwise manner. We will extend this to a piecewise linear and even nonlinear approximation. 3 Linear and nonlinear approximation A straight forward nonlinear approximation can be tried with plain backpropagation networks 4 .
Reference: [MT90] <author> K. Moller and S. Thrun. </author> <title> Task modularization by network modulation. </title> <editor> In J. Rault, editor, </editor> <volume> Neuro-Nimes '90, </volume> <pages> pages 419-432, </pages> <year> 1990. </year>
Reference-contexts: Our approach can be seen as an application of the devide and conquer-principle (or problem decomposition) that has a long tradition in computer science [HS78] and artificial neural network learning <ref> [JJB90, MT90] </ref>. Its usefulness is widely accepted (e.g. finite element method [BP84] in numerical mathematics, patching [Far90] in geometric modeling). But traditional methods suffer from the fact that either the decomposition has to be given in advance or is performed in a strict, predetermined manner (e.g. splitting inhalves).
Reference: [RHW86] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. I + II, </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: 1 Introduction Artificial neural networks have been found useful in widespread applications. One of the important problems concerning real-world applications is the slowness and instability of existing learning algorithms such as backpropagation <ref> [RHW86] </ref>. This slowness (which increases drastically with network size) is due to the fact, that * all of the weights in the network are changed at once.
Reference: [RS88] <author> H. Ritter and K. Schulten. </author> <title> Extending Kohonen's self organizing mapping algorithm to learn ballistic movements. </title> <editor> In R. Eckmiller and C. von der Malsburg, editors, </editor> <booktitle> Neural Computers, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference: [RMS89] <author> H. Ritter, T. Martinez, and K. Schulten. </author> <title> Topology-conserving maps for learning visuo-motor-coordination. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(3) </volume> <pages> 159-168, </pages> <year> 1989. </year>
Reference-contexts: 2 + y 2 ) 2)e (x 2 +y 2 ) We start with approximation by a discrete step function using an extended feature map algorithm. 2.1 Approximation with a step function In order to approximate a multidimensional function, we first introduce a technique which is related to feature maps <ref> [Koh84, RMS89] </ref>. In this approach the input space is devided into small regions, each of which is represented by a feature vector. <p> Change stepsize *(t) and neighborhood function h ij (t), such that these are monotonically decreasing. 5. Goto step 1. An extension of this algorithm was used by Ritter, Martinez and Schulten <ref> [RMS89] </ref> to perform function approximation. They assigned to every node in the FM-grid a set of output parameters that are adapted over time to minimize the LMS difference of the output value and the desired k-dimensional f (x).
Reference: [TML90] <author> S. Thrun, K. Moller, and A. Linden. </author> <title> Adaptive look-ahead planning. </title> <editor> In G. Dorffner, editor, </editor> <booktitle> Konnektion-ismus in Artificial Intelligence, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: to backpropagation is sped up by using many small networks in a modular fashion rather than a single large one. * Execution speed is enhanced by modularization, and it is possible to perform faster (gradient directed) search in input space, as it is used by a planning method described in <ref> [TML90, TML91] </ref>. * The problem of determining the number of hidden units is circumvented. The system performs an adaptive resource allocation in a way such that "difficult" parts of a function attract more networks (or hidden units) than easier ones.
Reference: [TML91] <author> S. Thrun, K. Moller, and A. Linden. </author> <title> Planning with an adaptive world model. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <publisher> IEEE, Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year> <note> to appear. </note>
Reference-contexts: to backpropagation is sped up by using many small networks in a modular fashion rather than a single large one. * Execution speed is enhanced by modularization, and it is possible to perform faster (gradient directed) search in input space, as it is used by a planning method described in <ref> [TML90, TML91] </ref>. * The problem of determining the number of hidden units is circumvented. The system performs an adaptive resource allocation in a way such that "difficult" parts of a function attract more networks (or hidden units) than easier ones.
Reference: [TM91] <author> S. Thrun and K. Moller. </author> <title> On planning and exploration in non-discrete environments. </title> <type> Technical Report, </type> <institution> GMD, </institution> <month> February, </month> <year> 1991. </year>
Reference-contexts: It is straight forward how our learning method can be used to generate input for those areas where knowledge is still inadequate <ref> [TM91] </ref>. * easier relearning, i.e. in case that the learning system is facing a constantly changing environment fast relearning is necessary. Unfortunately pure backpropagation is not well suited for online training with open training sets. Our decomposition allows to keep certain changes local.
References-found: 16

