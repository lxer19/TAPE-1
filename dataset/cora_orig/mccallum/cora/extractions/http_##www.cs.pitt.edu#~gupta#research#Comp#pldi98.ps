URL: http://www.cs.pitt.edu/~gupta/research/Comp/pldi98.ps
Refering-URL: http://www.cs.pitt.edu/~gupta/research/optimization.html
Root-URL: 
Email: fbodik,gupta,soffag@cs.pitt.edu  
Title: Complete Removal of Redundant Expressions  
Author: Rastislav Bodk Rajiv Gupta Mary Lou Soffa 
Keyword: partial redundancy elimination, control flow restructuring, speculative execution, demand-driven frequency data-flow analysis, profile-guided optimization.  
Address: Pittsburgh Pittsburgh, PA 15260  
Affiliation: Dept. of Computer Science University of  
Abstract: Partial redundancy elimination (PRE), the most important component of global optimizers, generalizes the removal of common subexpressions and loop-invariant computations. Because existing PRE implementations are based on code motion, they fail to completely remove the redundancies. In fact, we observed that 73% of loop-invariant statements cannot be eliminated from loops by code motion alone. In dynamic terms, traditional PRE eliminates only half of redundancies that are strictly partial. To achieve a complete PRE, control flow restructuring must be applied. However, the resulting code duplication may cause code size explosion. This paper focuses on achieving a complete PRE while incurring an acceptable code growth. First, we present an algorithm for complete removal of partial redundancies, based on the integration of code motion and control flow restructuring. In contrast to existing complete techniques, we resort to restructuring merely to remove obstacles to code motion, rather than to carry out the actual optimization. Guiding the optimization with a profile enables additional code growth reduction through selecting those duplications whose cost is justified by sufficient execution-time gains. The paper develops two methods for determining the optimization benefit of restructuring a program region, one based on path-profiles and the other on data-flow frequency analysis. Furthermore, the abstraction underlying the new PRE algorithm enables a simple formulation of speculative code motion guaranteed to have positive dynamic improvements. Finally, we show how to balance the three transformations (code motion, restructuring, and speculation) to achieve a near-complete PRE with very little code growth. We also present algorithms for efficiently computing dynamic benefits. In particular, using an elimination-style data-flow framework, we derive a demand-driven frequency analyzer whose cost can be controlled by permitting a bounded degree of conservative imprecision in the solution. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1986. </year>
Reference-contexts: In some cases, a simple transformation may help. For example, [a + b] (but not [c + d]) can be optimized by peeling off one loop iteration and performing the traditional LI <ref> [1] </ref>, producing the program Figure 1 (b). In while-loops, LI can often be enabled with more economical do-until conversion. The example presented does not allow this transformation because the loop exit does not post-dominate the loop entry. <p> Finally, we compute AV some which characterizes some-paths availability and also encapsulates CMP detection: AV some , AVAIL 6= No. The pair of solutions (AV all ; AV some ) can be directly mapped to the desired solution of AVAIL. The GEN and KILL sets <ref> [1] </ref> for the AV some problem are given below. The value of the initial guess is false, the meet operator is the bit-wise or.
Reference: [2] <author> F. E. Allen and J. Cocke. </author> <title> A program data flow analysis procedure. </title> <journal> CACM, </journal> <volume> 19(3) </volume> <pages> 137-147, </pages> <month> March </month> <year> 1976. </year>
Reference-contexts: We overcome this limitation by designing the demand-driven analyzer based upon elimination data-flow methods [28] whose time complexity is independent of the lattice shape. We have developed a demand-driven analysis framework motivated by the Allen-Cocke interval elimination solver <ref> [2] </ref>. Next, using the framework, a demand-driven algorithm for general frequency data-flow analysis was derived [8]. We present here the frequency solver spe cialized for the problem of availability. Definitions. <p> This information is used to develop a version of the analyzer that approximates frequency by disregarding low-contribution nodes with the goal of further restricting analysis scope. The exhaustive interval data-flow analysis <ref> [2] </ref> computes x n for all n as follows. First, loop headers are identified to partition the graph into hierarchic acyclic subregions, called intervals. Second, forward substitution of equations is per formed within each interval to express each node solution in terms of its loop header. <p> The second and third step remove cyclic dependences from all innermost loops in EG; they are repeated until all nesting levels are processed and all solutions are expressed in terms of the start node, which is then propagated to all previously reduced equations in the final propagation phase <ref> [2] </ref>. The demand-driven interval analysis substitutes only those equations and reduces only those intervals on which the desired x e n is transitively dependent. To find the relevant equations, we back-substitute equations (flow functions) into the right-hand side of x e n along the EG edges. <p> The edges are added to the exploded graph on-line, whenever a new EG node is visited, by first computing the flow function of the node and then inserting its predecessors into the graph. As in <ref> [2] </ref>, we define an EG interval to be a set of nodes dominated by the sink of any back-edge. In an irreducible EG, a back-edge is each loop edge sinking onto a loop entry node. <p> A forward substitution on the subgraph will yield solutions for all subgraph nodes which can be cached in case they are later desired (worst-case running time). This step corresponds to the propagation phase in <ref> [2] </ref>, and to caching in [18, 29]. The framework instance calculates the probability of expression e being available at the exit of node n during the execution: x e n = p (AVAIL out [n; e] = Must) 2 R. <p> Since real programs have loop nesting level bound by a small con stant, the expected complexity is (N S), as in <ref> [2] </ref>.
Reference: [3] <author> Glenn Ammons, Thomas Ball, and James R. Larus. </author> <title> Exploiting hardware performance counters with flow and context sensitive profiling. </title> <booktitle> In Proceedings of the ACM SIGPLAN '97 Conf. on Prog. Language Design and Impl., </booktitle> <pages> pages 85-96, </pages> <year> 1997. </year>
Reference-contexts: An alternative is to use path profiles <ref> [3, 7] </ref>, which are convenient for establishing cost-benefit optimization trade-offs [4, 19, 20].
Reference: [4] <author> Glenn Ammons and James L. Larus. </author> <title> Improving data-flow analysis with path profiles. </title> <booktitle> In Proceedings of the ACM SIG-PLAN '98 Conference on Programming Language Design and Implementation, </booktitle> <year> 1998. </year>
Reference-contexts: In Figure 1 (c), for example, ComPRE resulted in a loop with two distinct entry nodes. Even though PoePRE preserves reducibility on the same loop (Figure 1 (b)), like other restructuring-based optimizations <ref> [4, 10, 30] </ref>, it is also plagued by introducing irreducibility. One way to deal with the problem is to perform all opti mizations that presuppose single-entry loops prior to PRE. However, many algorithms for scheduling (which should follow PRE) rely on reducibility. <p> The third algorithm integrates both restructuring and speculation and selects a profitable subgraph of the CMP for each. While profitably balancing the cost and benefit under a given profile is NP-hard, the empirically small number of hot program paths promises an efficient algorithm <ref> [4, 19] </ref>. <p> An alternative is to use path profiles [3, 7], which are convenient for establishing cost-benefit optimization trade-offs <ref> [4, 19, 20] </ref>. To arrive at the value of the region benefit with a path profile, it is sufficient to sum the frequencies of Must-Must paths, which are paths that cross any region entry edge that is Must-available and any exit edge that is Must-anticipated. <p> An alternative scenario is more attractive, however. When a low-weight node is selected in line 3, we throw it away. We can keep disregarding such nodes until their total weights exceed *. In essence, this approach performs analysis along hot paths <ref> [4] </ref>, and on-line region formation [21]. The idea of terminating the analysis before it could find the precise solution was first applied in the implementation of interprocedural branch elimination [10]. Stopping after visiting a thousand nodes resulted in two magnitudes of analysis speedup, while most optimization opportunities were still discovered. <p> However, using only code motion, they fail to completely exploit the additional reuse opportunities. Thus, the transformations presented here are applicable in other styles of PRE as well, for example in elimination of loads. Ammons and Larus <ref> [4] </ref> developed a constant propagation optimization based on restructuring, namely on peeling of hot paths.
Reference: [5] <author> Joel Auslander, Matthai Philipose, Craig Chambers, Su-san J. Eggers, and Brian N. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 149-159, </pages> <month> 21- May </month> <year> 1996. </year>
Reference-contexts: Approximation permits the analyzer to avoid exploring program paths guaranteed to provide insignificant contribution (frequency-wise) to the overall solution. Besides PRE, the demand-driven approximate frequency analysis is applicable in in-terprocedural branch correlation analysis [10] and dynamic optimizations <ref> [5] </ref>. Let us illustrate our PRE algorithms on the loop in Figure 1 (a). Assume no statement in the loop defines variables a, b, c, or d. Although the computations [a+b] and [c+d] are loop-invariant, removing them from the loop with code motion is not possible. <p> Besides PRE, the analyzer is suitable for optimizations where acceptable running time must be maintained by restricting analysis scope, as in run-time optimizations <ref> [5] </ref> or interprocedural branch removal [10]. Frequency analysis computes the probability that a data-flow fact will occur during execution. Therefore, the probability "lattice" is an infinite chain of real numbers.
Reference: [6] <author> A. Ayers, R. Schooler, and R. Gottlieb. </author> <title> Aggressive inlining. </title> <booktitle> In Proceedings of the ACM SIGPLAN '97 Conf. on Prog. Language Design and Impl., </booktitle> <pages> pages 134-145, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: We have chosen this form of T in order to avoid modeling complex interactions among compiler stages. In the implementation, T is supplemented with a code growth budget (for example, in <ref> [6] </ref>, code is allowed to grow by about 20%). First, we present an algorithm for computing the optimization benefit Rem (R).
Reference: [7] <author> Thomas Ball and James R. Larus. </author> <title> Efficient path profiling. </title> <booktitle> In 29th Annual IEEE/ACM International Symposium on Microarchitecture, </booktitle> <pages> pages 46-57, </pages> <year> 1996. </year>
Reference-contexts: The run-time benefit corresponds to the cumulative execution frequency of control flow paths that will permit value reuse after the restructuring. We describe how this benefit can be obtained either using edge profiles, path-profiles <ref> [7] </ref>, or through data-flow frequency analysis [27]. As another contribution, we reduce the cost of frequency analysis by presenting a frequency analyzer derived from a new demand-driven data-flow analysis framework. Based on interval analysis, the framework enables formulation of analyzers whose time complexity is independent of the lattice size. <p> Finally, to support profile guiding, we show how an estimate of the run-time gain thwarted by a CMP region can be obtained using edge profiles, frequency analysis [27], or path profiles <ref> [7] </ref>. 3.1 Selective Restructuring We model the profitability of duplicating a CMP region R with a cost-benefit threshold predicate T (R), which holds if the region optimization benefit exceeds a constant multiple of the region size. <p> An alternative is to use path profiles <ref> [3, 7] </ref>, which are convenient for establishing cost-benefit optimization trade-offs [4, 19, 20].
Reference: [8] <author> Rastislav Bodik. </author> <title> Path-Sensitive Compilation. </title> <type> PhD thesis, </type> <institution> University of Pittsburgh, </institution> <note> in preparation. </note>
Reference-contexts: We have developed a demand-driven analysis framework motivated by the Allen-Cocke interval elimination solver [2]. Next, using the framework, a demand-driven algorithm for general frequency data-flow analysis was derived <ref> [8] </ref>. We present here the frequency solver spe cialized for the problem of availability. Definitions. <p> It is likely that path recombination can be integrated with code motion, as presented in this paper, to further reduce the code growth. In a global view, we have identified four main issues with path-sensitive program optimizations <ref> [8] </ref>: a) solving non-distributive problems without conservative approximation (e.g. non-linear constant propagation), b) collecting distinct opportunities (e.g., variable has different constant along each path), c) exploiting distinct opportunities (e.g., enabling folding of path-dependent constants through restructuring), and d) directing the analysis effort towards hot paths.
Reference: [9] <author> Rastislav Bodik and Sadun Anik. </author> <title> Path-sensitive value-flow analysis. </title> <booktitle> In Conference Record of the 25th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1998. </year>
Reference-contexts: The graph EG is called an exploded graph [22]. The data flow problems underlying ComPRE are separable, hence x e n only depends on x e m . In value-based PRE <ref> [9] </ref>, constant propagation [29], and branch correlation analysis [10], edges (x d m ; x e n ), d 6= e, may exist, complicating the analysis. The analyzer presented here handles such general exploded graphs. Requirements. The demand-driven analyzer grew out of four specific design requirements: 1. Demand-driven. <p> While PRE is significantly improved through effective program transformations presented in this paper, a large orthogonal potential lies in detecting more redundancies. Some techniques have used powerful analysis to uncover more value reuse than the traditional PRE analysis <ref> [9, 11] </ref>. However, using only code motion, they fail to completely exploit the additional reuse opportunities. Thus, the transformations presented here are applicable in other styles of PRE as well, for example in elimination of loads. <p> Our approach is to reserve restructuring for the actual transformation. This implies a different overall strategy: a) we solve non-distributive problems precisely along all paths by customizing the data-flow name space <ref> [9] </ref>, b) we collect distinct opportunities through demand-driven analysis as in branch elimination [10], which is itself a form of constant propa-gation, c) we exploit all profitable opportunities with economical transformations, and d) avoid infrequent program regions using the approximation frequency analysis (the last three presented in this paper). 7 Acknowledgments
Reference: [10] <author> Rastislav Bodik, Rajiv Gupta, and Mary Lou Soffa. </author> <title> Inter-procedural conditional branch elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '97 Conf. on Prog. Language Design and Impl., </booktitle> <pages> pages 146-158, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Approximation permits the analyzer to avoid exploring program paths guaranteed to provide insignificant contribution (frequency-wise) to the overall solution. Besides PRE, the demand-driven approximate frequency analysis is applicable in in-terprocedural branch correlation analysis <ref> [10] </ref> and dynamic optimizations [5]. Let us illustrate our PRE algorithms on the loop in Figure 1 (a). Assume no statement in the loop defines variables a, b, c, or d. Although the computations [a+b] and [c+d] are loop-invariant, removing them from the loop with code motion is not possible. <p> In Figure 1 (c), for example, ComPRE resulted in a loop with two distinct entry nodes. Even though PoePRE preserves reducibility on the same loop (Figure 1 (b)), like other restructuring-based optimizations <ref> [4, 10, 30] </ref>, it is also plagued by introducing irreducibility. One way to deal with the problem is to perform all opti mizations that presuppose single-entry loops prior to PRE. However, many algorithms for scheduling (which should follow PRE) rely on reducibility. <p> Besides PRE, the analyzer is suitable for optimizations where acceptable running time must be maintained by restricting analysis scope, as in run-time optimizations [5] or interprocedural branch removal <ref> [10] </ref>. Frequency analysis computes the probability that a data-flow fact will occur during execution. Therefore, the probability "lattice" is an infinite chain of real numbers. <p> The graph EG is called an exploded graph [22]. The data flow problems underlying ComPRE are separable, hence x e n only depends on x e m . In value-based PRE [9], constant propagation [29], and branch correlation analysis <ref> [10] </ref>, edges (x d m ; x e n ), d 6= e, may exist, complicating the analysis. The analyzer presented here handles such general exploded graphs. Requirements. The demand-driven analyzer grew out of four specific design requirements: 1. Demand-driven. <p> In essence, this approach performs analysis along hot paths [4], and on-line region formation [21]. The idea of terminating the analysis before it could find the precise solution was first applied in the implementation of interprocedural branch elimination <ref> [10] </ref>. Stopping after visiting a thousand nodes resulted in two magnitudes of analysis speedup, while most optimization opportunities were still discovered. <p> Our approach is to reserve restructuring for the actual transformation. This implies a different overall strategy: a) we solve non-distributive problems precisely along all paths by customizing the data-flow name space [9], b) we collect distinct opportunities through demand-driven analysis as in branch elimination <ref> [10] </ref>, which is itself a form of constant propa-gation, c) we exploit all profitable opportunities with economical transformations, and d) avoid infrequent program regions using the approximation frequency analysis (the last three presented in this paper). 7 Acknowledgments We are indebted to the elcor and Impact compiler teams for providing their
Reference: [11] <author> Preston Briggs and Keith D. Cooper. </author> <title> Effective partial redundancy elimination. </title> <booktitle> In Proceedings of the Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 159-170, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: PRE is attractive because by targeting statements that are redundant only along some execution paths, it subsumes and generalizes two important value-reuse techniques: global common subexpression elimination and loop-invariant code motion. Consequently, PRE serves as a unified value-reuse optimizer. Most PRE algorithms employ code motion <ref> [11, 12, 14, 15, 16, 17, 24, 26] </ref>, a program transformation that reorders instructions without changing the shape of the control flow graph. Unfortunately, code-motion alone fails to remove routine redundancies. <p> In the long history of PRE research and implementation, three distinct transformations can be identified. The seminal paper by Morel and Renviose [26] and its derivations <ref> [11, 14, 15, 16, 17, 24] </ref> employ pure, non-speculative code motion. Second, the complete PRE by Steffen [30] is based on control flow restructuring. <p> While PRE is significantly improved through effective program transformations presented in this paper, a large orthogonal potential lies in detecting more redundancies. Some techniques have used powerful analysis to uncover more value reuse than the traditional PRE analysis <ref> [9, 11] </ref>. However, using only code motion, they fail to completely exploit the additional reuse opportunities. Thus, the transformations presented here are applicable in other styles of PRE as well, for example in elimination of loads.
Reference: [12] <author> F. Chow, S. Chan, R. Kennedy, S.-M. Liu, R. Lo, and P. Tu. </author> <title> A new algorithm for partial redundancy elimination based on SSA form. </title> <booktitle> In Proceedings of the ACM SIGPLAN '97 Conf. on Prog. Language Design and Impl., </booktitle> <pages> pages 273-286, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: PRE is attractive because by targeting statements that are redundant only along some execution paths, it subsumes and generalizes two important value-reuse techniques: global common subexpression elimination and loop-invariant code motion. Consequently, PRE serves as a unified value-reuse optimizer. Most PRE algorithms employ code motion <ref> [11, 12, 14, 15, 16, 17, 24, 26] </ref>, a program transformation that reorders instructions without changing the shape of the control flow graph. Unfortunately, code-motion alone fails to remove routine redundancies.
Reference: [13] <author> P. Cousot and R. Cousot. </author> <title> Abstract intrepretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Conference Record of the 4th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Such an approach fails on infinite-height lattices where CFG loops keep always iterating towards a better approximation of the solution. Note that breaking each control flow cycle by inserting a widening operator <ref> [13] </ref> does not appear to resolve the problem because widening is a local adjustment primarily intended to approximate the solution. Therefore, in frequency analysis, too many iterations would be required to achieve an acceptable approximation.
Reference: [14] <author> Dhanajay M. Dhamdhere, Barry K. Rosen, and Kenneth F. Zadeck. </author> <title> How to analyze large programs efficiently and informatively. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 212-223, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: PRE is attractive because by targeting statements that are redundant only along some execution paths, it subsumes and generalizes two important value-reuse techniques: global common subexpression elimination and loop-invariant code motion. Consequently, PRE serves as a unified value-reuse optimizer. Most PRE algorithms employ code motion <ref> [11, 12, 14, 15, 16, 17, 24, 26] </ref>, a program transformation that reorders instructions without changing the shape of the control flow graph. Unfortunately, code-motion alone fails to remove routine redundancies. <p> In the long history of PRE research and implementation, three distinct transformations can be identified. The seminal paper by Morel and Renviose [26] and its derivations <ref> [11, 14, 15, 16, 17, 24] </ref> employ pure, non-speculative code motion. Second, the complete PRE by Steffen [30] is based on control flow restructuring.
Reference: [15] <author> Dhananjay M. Dhamdhere. </author> <title> Practical adaptation of the global optimization algorithm of Morel and Renvoise. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 291-294, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: PRE is attractive because by targeting statements that are redundant only along some execution paths, it subsumes and generalizes two important value-reuse techniques: global common subexpression elimination and loop-invariant code motion. Consequently, PRE serves as a unified value-reuse optimizer. Most PRE algorithms employ code motion <ref> [11, 12, 14, 15, 16, 17, 24, 26] </ref>, a program transformation that reorders instructions without changing the shape of the control flow graph. Unfortunately, code-motion alone fails to remove routine redundancies. <p> In the long history of PRE research and implementation, three distinct transformations can be identified. The seminal paper by Morel and Renviose [26] and its derivations <ref> [11, 14, 15, 16, 17, 24] </ref> employ pure, non-speculative code motion. Second, the complete PRE by Steffen [30] is based on control flow restructuring.
Reference: [16] <author> K. Drechsler and M. Stadel. </author> <title> A solution to a problem with Morel and Renvoise's "global optimization by suppression of partial redundancies". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 635-640, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: PRE is attractive because by targeting statements that are redundant only along some execution paths, it subsumes and generalizes two important value-reuse techniques: global common subexpression elimination and loop-invariant code motion. Consequently, PRE serves as a unified value-reuse optimizer. Most PRE algorithms employ code motion <ref> [11, 12, 14, 15, 16, 17, 24, 26] </ref>, a program transformation that reorders instructions without changing the shape of the control flow graph. Unfortunately, code-motion alone fails to remove routine redundancies. <p> In the long history of PRE research and implementation, three distinct transformations can be identified. The seminal paper by Morel and Renviose [26] and its derivations <ref> [11, 14, 15, 16, 17, 24] </ref> employ pure, non-speculative code motion. Second, the complete PRE by Steffen [30] is based on control flow restructuring.
Reference: [17] <author> K. Drechsler and M. Stadel. </author> <title> A variation of Knoop, Ruthing, and Steffen's lazy code motion. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 28(5) </volume> <pages> 635-640, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: PRE is attractive because by targeting statements that are redundant only along some execution paths, it subsumes and generalizes two important value-reuse techniques: global common subexpression elimination and loop-invariant code motion. Consequently, PRE serves as a unified value-reuse optimizer. Most PRE algorithms employ code motion <ref> [11, 12, 14, 15, 16, 17, 24, 26] </ref>, a program transformation that reorders instructions without changing the shape of the control flow graph. Unfortunately, code-motion alone fails to remove routine redundancies. <p> The result is an algorithm for PRE that is complete (i.e., it exploits all opportunities for value reuse) and minimizes the code growth necessary to achieve the code motion. * We show that restricting the algorithm to code motion produces the traditional code-motion PRE <ref> [17, 24] </ref>. * Profile-guided techniques for limiting the code growth through integration of selective duplication and specu lative code motion are developed. * We develop a demand-driven frequency analyzer based on a new elimination data-flow analysis framework. * The notion of approximate data-flow information is de fined and used to improve <p> In this section, we show that our complete algorithm can be naturally constrained by prohibiting the restructuring, and that such modification results in the same optimization as the optimal motion-only PRE <ref> [17, 24] </ref>. In comparison to ComPRE, the constrained CM-PRE algorithm bypasses the CMP removal; the last step (transformation) is unchanged (Figure 3). <p> Consequently, we can substitute ANTIC 6= Must with :AN all , where AN all is defined analogously to AV all . As a result, we obtain the same implementation complexity as the algorithms in <ref> [17, 24] </ref>: three data-flow problems must be solved, each requiring one bit of solution per expression. In conclusion, the CMP region is a convenient abstraction for terminating hoisting when it would unnecessarily extend the live ranges. <p> In the long history of PRE research and implementation, three distinct transformations can be identified. The seminal paper by Morel and Renviose [26] and its derivations <ref> [11, 14, 15, 16, 17, 24] </ref> employ pure, non-speculative code motion. Second, the complete PRE by Steffen [30] is based on control flow restructuring.
Reference: [18] <author> Evelyn Duesterwald, Rajiv Gupta, and Mary Lou Soffa. </author> <title> A practical framework for demand-driven interprocedural data flow analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 19(6) </volume> <pages> 992-1030, </pages> <month> November </month> <year> 1997. </year>
Reference-contexts: Based on interval analysis, the framework enables formulation of analyzers whose time complexity is independent of the lattice size. This is a requirement of frequency analysis whose lattice is of infinite-height. Due to this requirement, existing demand frameworks are unable to produce a frequency analyzer <ref> [18, 22, 29] </ref>. Furthermore, we introduce the notion of approximate data-flow frequency information, which conservatively underestimates the meet-over-all-paths solution, keeping the imprecision within a given degree. Approximation permits the analyzer to avoid exploring program paths guaranteed to provide insignificant contribution (frequency-wise) to the overall solution. <p> Frequency analysis computes the probability that a data-flow fact will occur during execution. Therefore, the probability "lattice" is an infinite chain of real numbers. Because existing demand-driven analysis frameworks are built on iterative approaches, they only permit lattices of finite size <ref> [18] </ref> or finite height [22, 29] and hence cannot derive a frequency analyzer. We overcome this limitation by designing the demand-driven analyzer based upon elimination data-flow methods [28] whose time complexity is independent of the lattice shape. <p> A forward substitution on the subgraph will yield solutions for all subgraph nodes which can be cached in case they are later desired (worst-case running time). This step corresponds to the propagation phase in [2], and to caching in <ref> [18, 29] </ref>. The framework instance calculates the probability of expression e being available at the exit of node n during the execution: x e n = p (AVAIL out [n; e] = Must) 2 R.
Reference: [19] <author> R. Gupta, D. Berson, and J.Z. Fang. </author> <title> Resource-sensitive profile-directed data flow analysis for code optimization. </title> <booktitle> In 30th Annual IEEE/ACM International Symposium on Mi-croarchitecture, </booktitle> <pages> pages 358-368, </pages> <month> December </month> <year> 1997. </year>
Reference-contexts: The third algorithm integrates both restructuring and speculation and selects a profitable subgraph of the CMP for each. While profitably balancing the cost and benefit under a given profile is NP-hard, the empirically small number of hot program paths promises an efficient algorithm <ref> [4, 19] </ref>. <p> An alternative is to use path profiles [3, 7], which are convenient for establishing cost-benefit optimization trade-offs <ref> [4, 19, 20] </ref>. To arrive at the value of the region benefit with a path profile, it is sufficient to sum the frequencies of Must-Must paths, which are paths that cross any region entry edge that is Must-available and any exit edge that is Must-anticipated. <p> These are precisely the paths along which value reuse exists but is blocked by the region. While there is an exponential number of profiled acyclic paths, only 5.4% of procedures execute more than 50 distinct paths in spec95 <ref> [19] </ref>. This number drops to 1.3% when low-frequency paths accounting for 5% of total frequency are removed. Since we can afford to approximate by disregarding these infrequent paths, summing individual path frequencies constitutes a feasible algorithm for many CMP regions.
Reference: [20] <author> R. Gupta, D. Berson, and J.Z. Fang. </author> <title> Path profile guided partial redundancy elimination using speculation. </title> <booktitle> In IEEE International Conference on Computer Languages, </booktitle> <month> May </month> <year> 1998. </year>
Reference-contexts: The former restriction can be relaxed for instruction that cannot except, leading to safe speculation. New processor generations will support control-speculative instructions which will suppress raising the exception until the generated value is eventually used, allowing unsafe speculation [25]. The latter problem is solved in <ref> [20] </ref>, where an aggressive code-motion PRE navigated by path profiles is developed. The goal is to allow speculative hoisting, but only into such paths on which dynamic impairment would not outweigh the benefit of elimi nating the computation from its original position. <p> Next, we utilize the CMP region to determine i) the profitability of speculative code motion and ii) the positions of speculative insertion points that minimize live ranges of temporary variables. Figure 6 illustrates the principle of speculative PRE <ref> [20] </ref>. Instead of duplicating the CMP region, we hoist the expression into all No-available entry edges. This makes all exits fully available, enabling complete removal of original computations along the Must exits. In our example, [a + b] is moved into the No-available region entry edge e 2 . <p> An alternative is to use path profiles [3, 7], which are convenient for establishing cost-benefit optimization trade-offs <ref> [4, 19, 20] </ref>. To arrive at the value of the region benefit with a path profile, it is sufficient to sum the frequencies of Must-Must paths, which are paths that cross any region entry edge that is Must-available and any exit edge that is Must-anticipated. <p> This becomes more intuitive once we realize that without control flow restructuring, one is restricted to consider only an individual edge (but not a path) for expression insertion and removal. To compare the CMP-based partial speculation with the speculative PRE in <ref> [20] </ref>, we show how to efficiently compute the benefit by defining the CMP region and how to apply edge profiles with the same precision as path profiles. In acyclic code, we achieve the same precision; in cyclic code, we are more precise in the presence of loop-carried reuse. <p> Second, the complete PRE by Steffen [30] is based on control flow restructuring. Third, navigated by path profile information, Gupta et al apply speculative code motion in order to avoid code-motion obstacles by controlled im pairment of some paths <ref> [20] </ref>. In this work, we defined the code-motion-preventing (CMP) region, which is a CFG subgraph localizing adverse effects of control flow on the desired value reuse. The notion of the CMP is applied to enhance and integrate the three existing PRE transformations in the following ways, 1.
Reference: [21] <author> Richard E. Hank, Wen-Mei W. Hwu, and B. Ramakrishna Rau. </author> <title> Region-based compilation: An introduction and motivation. </title> <booktitle> In 28th Annual IEEE/ACM International Symposium on Microarchitecture, </booktitle> <address> Ann Arbor, Michigan, </address> <year> 1995. </year>
Reference-contexts: An alternative scenario is more attractive, however. When a low-weight node is selected in line 3, we throw it away. We can keep disregarding such nodes until their total weights exceed *. In essence, this approach performs analysis along hot paths [4], and on-line region formation <ref> [21] </ref>. The idea of terminating the analysis before it could find the precise solution was first applied in the implementation of interprocedural branch elimination [10]. Stopping after visiting a thousand nodes resulted in two magnitudes of analysis speedup, while most optimization opportunities were still discovered.
Reference: [22] <author> Susan Horwitz, Thomas Reps, and Mooly Sagiv. </author> <title> Demand Interprocedural Dataflow Analysis. </title> <booktitle> In Proceedings of the Third ACM SIGSOFT Symposium on the Foundations of Software Engineering, </booktitle> <pages> pages 104-115, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Based on interval analysis, the framework enables formulation of analyzers whose time complexity is independent of the lattice size. This is a requirement of frequency analysis whose lattice is of infinite-height. Due to this requirement, existing demand frameworks are unable to produce a frequency analyzer <ref> [18, 22, 29] </ref>. Furthermore, we introduce the notion of approximate data-flow frequency information, which conservatively underestimates the meet-over-all-paths solution, keeping the imprecision within a given degree. Approximation permits the analyzer to avoid exploring program paths guaranteed to provide insignificant contribution (frequency-wise) to the overall solution. <p> Frequency analysis computes the probability that a data-flow fact will occur during execution. Therefore, the probability "lattice" is an infinite chain of real numbers. Because existing demand-driven analysis frameworks are built on iterative approaches, they only permit lattices of finite size [18] or finite height <ref> [22, 29] </ref> and hence cannot derive a frequency analyzer. We overcome this limitation by designing the demand-driven analyzer based upon elimination data-flow methods [28] whose time complexity is independent of the lattice shape. We have developed a demand-driven analysis framework motivated by the Allen-Cocke interval elimination solver [2]. <p> The graph EG is called an exploded graph <ref> [22] </ref>. The data flow problems underlying ComPRE are separable, hence x e n only depends on x e m . In value-based PRE [9], constant propagation [29], and branch correlation analysis [10], edges (x d m ; x e n ), d 6= e, may exist, complicating the analysis.
Reference: [23] <author> Johan Janssen and Henk Corporaal. </author> <title> Controlled node splitting. </title> <booktitle> In Compiler Construction, 6th International Conference, volume 1060 of Springer Lecture Notes in Computer Science, </booktitle> <pages> pages 44-58, </pages> <address> Sweden, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: However, many algorithms for scheduling (which should follow PRE) rely on reducibility. After ComPRE, a reducible graph can be obtained with additional code duplication. An effective algorithm for normalizing irreducible programs is given in <ref> [23] </ref>. To suppress an unnecessary invocation of the algorithm, we can employ a simple test of whether irreducibility may be created after a region duplication. The test is based upon examining only the CMP entry and exit edges, rather than the entire program. <p> Consider the loop in Figure 4 (a). Two of the three exits of CMP [a + b] fall into the loop. After restructuring, they will become loop entries, as shown in Figure 1 (c). Rather than applying a global algorithm like <ref> [23] </ref>, a straightforward approach to make the affected loop reducible is to peel off a part of its body. The goal is to extend the replication scope so that the region exits sink onto a single loop node, which will then become the new loop entry.
Reference: [24] <author> Jens Knoop, Oliver Ruthing, and Bernhard Steffen. </author> <title> Optimal code motion: </title> <journal> Theory and practice. ACM Trans. on Progr. Languages and Systems, </journal> <volume> 16(4) </volume> <pages> 1117-1155, </pages> <year> 1994. </year>
Reference-contexts: PRE is attractive because by targeting statements that are redundant only along some execution paths, it subsumes and generalizes two important value-reuse techniques: global common subexpression elimination and loop-invariant code motion. Consequently, PRE serves as a unified value-reuse optimizer. Most PRE algorithms employ code motion <ref> [11, 12, 14, 15, 16, 17, 24, 26] </ref>, a program transformation that reorders instructions without changing the shape of the control flow graph. Unfortunately, code-motion alone fails to remove routine redundancies. <p> Unfortunately, code-motion alone fails to remove routine redundancies. In practice, one half of computations that are strictly partially redundant (not redundant along some paths) are left unoptimized due to code-motion obstacles. In theory, even the optimal code-motion algorithm <ref> [24] </ref> breaks down on loop invariants in while-loops, unless supported by explicit do-until conversion. Recently, Steffen demonstrated that control flow restructuring can remove from the program all redundant computations, including conditional branches [30]. While his property-oriented expansion algorithm (Poe) is complete, it causes unnecessary code duplication. <p> This computation cannot be moved out of the loop because it would be executed on the path En; O; P; Ex, which does not execute [a + b] in the original program. Because this could slow down the program and create spurious exceptions, PRE disallows such unsafe code motion <ref> [24] </ref>. The desired optimization is only possible if the CFG is restructured. The PoePRE algorithm [30] would produce the program in Figure 1 (b), which was created by duplicating each node on which the value of [a+b] was available only on a subset of incoming paths. <p> The result is an algorithm for PRE that is complete (i.e., it exploits all opportunities for value reuse) and minimizes the code growth necessary to achieve the code motion. * We show that restricting the algorithm to code motion produces the traditional code-motion PRE <ref> [17, 24] </ref>. * Profile-guided techniques for limiting the code growth through integration of selective duplication and specu lative code motion are developed. * We develop a demand-driven frequency analyzer based on a new elimination data-flow analysis framework. * The notion of approximate data-flow information is de fined and used to improve <p> The smallest set of motion-blocking nodes is identified by solving the problems of availability and anticipabil-ity on an expressive lattice. We also show that when control flow restructuring is disabled, ComPRE becomes equivalent to the optimal code-motion PRE algorithm <ref> [24] </ref>. An expression is partially redundant if its value is computed on some incoming control flow path by a previous expression. <p> In this section, we show that our complete algorithm can be naturally constrained by prohibiting the restructuring, and that such modification results in the same optimization as the optimal motion-only PRE <ref> [17, 24] </ref>. In comparison to ComPRE, the constrained CM-PRE algorithm bypasses the CMP removal; the last step (transformation) is unchanged (Figure 3). <p> First, computations are hoisted as high as possible, maximizing the removal of redundancies. Later, the placement is corrected through the computation of delayability <ref> [24] </ref>. Our formulation specifies the optimal placement directly, as we never hoist into paths where a blocking CMP will be subsequently encountered. <p> Consequently, we can substitute ANTIC 6= Must with :AN all , where AN all is defined analogously to AV all . As a result, we obtain the same implementation complexity as the algorithms in <ref> [17, 24] </ref>: three data-flow problems must be solved, each requiring one bit of solution per expression. In conclusion, the CMP region is a convenient abstraction for terminating hoisting when it would unnecessarily extend the live ranges. <p> In conclusion, the CMP region is a convenient abstraction for terminating hoisting when it would unnecessarily extend the live ranges. It also provides an intuitive way of explaining the shortest-live-range solution without applying the corrective step based on delayability <ref> [24] </ref>. Furthermore, the CMP-based, motion-only solution can be implemented as efficiently as existing shortest-live-range algorithms. 2.2 Reducible Restructuring Duplicating a CMP region may destroy reducibility of the control flow graph. In Figure 1 (c), for example, ComPRE resulted in a loop with two distinct entry nodes. <p> In the long history of PRE research and implementation, three distinct transformations can be identified. The seminal paper by Morel and Renviose [26] and its derivations <ref> [11, 14, 15, 16, 17, 24] </ref> employ pure, non-speculative code motion. Second, the complete PRE by Steffen [30] is based on control flow restructuring.
Reference: [25] <author> S. A. Mahlke, W. Y. Chen, R. A. Bringmann, R. E. Hank, W.M. W. Hwu, B. R. Rau, and M. S. Schlansker. </author> <title> Sentinel scheduling for VLIW and superscalar processors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 376-408, </pages> <year> 1993. </year>
Reference-contexts: The former restriction can be relaxed for instruction that cannot except, leading to safe speculation. New processor generations will support control-speculative instructions which will suppress raising the exception until the generated value is eventually used, allowing unsafe speculation <ref> [25] </ref>. The latter problem is solved in [20], where an aggressive code-motion PRE navigated by path profiles is developed. The goal is to allow speculative hoisting, but only into such paths on which dynamic impairment would not outweigh the benefit of elimi nating the computation from its original position.
Reference: [26] <author> E. Morel and C. Renviose. </author> <title> Global optimization by supression of partial redundancies. </title> <journal> CACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction Partial redundancy elimination (PRE) is a widely used and effective optimization aimed at removing program statements that are redundant due to recomputing previously produced values <ref> [26] </ref>. PRE is attractive because by targeting statements that are redundant only along some execution paths, it subsumes and generalizes two important value-reuse techniques: global common subexpression elimination and loop-invariant code motion. Consequently, PRE serves as a unified value-reuse optimizer. <p> PRE is attractive because by targeting statements that are redundant only along some execution paths, it subsumes and generalizes two important value-reuse techniques: global common subexpression elimination and loop-invariant code motion. Consequently, PRE serves as a unified value-reuse optimizer. Most PRE algorithms employ code motion <ref> [11, 12, 14, 15, 16, 17, 24, 26] </ref>, a program transformation that reorders instructions without changing the shape of the control flow graph. Unfortunately, code-motion alone fails to remove routine redundancies. <p> Second, no computation is inserted where its value is available along any incoming path. Hence, no additional computations can be removed. fl Within the domain of the Morel and Renviose code-motion transformation, where PRE is accomplished by hoisting optimization candidates (but not other statements) <ref> [26] </ref>, ComPRE achieves minimum code growth. 1 This follows from the fact that after CMP restructuring, no program node can be removed or merged with some other node without destroying any value reuse, as shown by the following observations. <p> In the long history of PRE research and implementation, three distinct transformations can be identified. The seminal paper by Morel and Renviose <ref> [26] </ref> and its derivations [11, 14, 15, 16, 17, 24] employ pure, non-speculative code motion. Second, the complete PRE by Steffen [30] is based on control flow restructuring.
Reference: [27] <author> G. Ramalingam. </author> <title> Data flow frequency analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conf. on Progr. Language Design and Implementation, </booktitle> <pages> pages 267-277, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The run-time benefit corresponds to the cumulative execution frequency of control flow paths that will permit value reuse after the restructuring. We describe how this benefit can be obtained either using edge profiles, path-profiles [7], or through data-flow frequency analysis <ref> [27] </ref>. As another contribution, we reduce the cost of frequency analysis by presenting a frequency analyzer derived from a new demand-driven data-flow analysis framework. Based on interval analysis, the framework enables formulation of analyzers whose time complexity is independent of the lattice size. <p> Finally, to support profile guiding, we show how an estimate of the run-time gain thwarted by a CMP region can be obtained using edge profiles, frequency analysis <ref> [27] </ref>, or path profiles [7]. 3.1 Selective Restructuring We model the profitability of duplicating a CMP region R with a cost-benefit threshold predicate T (R), which holds if the region optimization benefit exceeds a constant multiple of the region size. <p> The benefit of duplicating the region R is thus the sum of all exit edge benefits Rem (R) = a=(n;m)2Exits Must (R) ex (a):p (AVAIL out [n; e] = Must): The probability p is computed from an edge profile using frequency analysis <ref> [27] </ref>. In the frequency domain, the probability of each data-flow fact occurring, rather than the fact's mere boolean meet-over-all-paths existence, is computed by incorporating the execution probabilities of control flow edges into the data-flow system. <p> Overall, ComPRE created about three times less code growth than PoePRE. 5 Demand-Driven Frequency Analysis Not amenable to bit-vector representation, frequency analysis <ref> [27] </ref> is an expensive component of profile-guided opti-mizers. We have shown that ComPRE allows restricting the scope of frequency analysis within the CMP region without a loss of accuracy. <p> Let p (a) denote the probability of edge a being taken, given its sink node is executed. We relate the edge probability to the sink (rather than the source, as in exhaustive analysis <ref> [27] </ref>) because the demand solver proceeds in the backward direction. The frequency flow function returns probability 1 when the node computes the expression e and 0 when it kills the expression. Otherwise, the sum of probabilities on predecessors weighted by edge execution probabilities is returned.
Reference: [28] <author> Barbara G. Ryder and Marvin C. Paull. </author> <title> Elimination algorithms for data flow analysis. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(3) </volume> <pages> 277-316, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Because existing demand-driven analysis frameworks are built on iterative approaches, they only permit lattices of finite size [18] or finite height [22, 29] and hence cannot derive a frequency analyzer. We overcome this limitation by designing the demand-driven analyzer based upon elimination data-flow methods <ref> [28] </ref> whose time complexity is independent of the lattice shape. We have developed a demand-driven analysis framework motivated by the Allen-Cocke interval elimination solver [2]. Next, using the framework, a demand-driven algorithm for general frequency data-flow analysis was derived [8]. <p> We believe that existing demand methods can be extended to operate in a structural manner, enabling the application of loop-breaking rules. This would make the methods reminiscent of the elimination algorithms <ref> [28] </ref>. 6 Conclusion and Related Work The focus of this paper is to improve program transformations that constitute value-reuse optimizations commonly known as Partial Redundancy Elimination (PRE). In the long history of PRE research and implementation, three distinct transformations can be identified.
Reference: [29] <author> Mooly Sagiv, Thomas Reps, and Susan Horwitz. </author> <title> Precise in-terprocedural dataflow analysis with applications to constant propagation. </title> <booktitle> Theoretical Computer Science, </booktitle> <address> 167(1-2):131-170, </address> <year> 1996. </year>
Reference-contexts: Based on interval analysis, the framework enables formulation of analyzers whose time complexity is independent of the lattice size. This is a requirement of frequency analysis whose lattice is of infinite-height. Due to this requirement, existing demand frameworks are unable to produce a frequency analyzer <ref> [18, 22, 29] </ref>. Furthermore, we introduce the notion of approximate data-flow frequency information, which conservatively underestimates the meet-over-all-paths solution, keeping the imprecision within a given degree. Approximation permits the analyzer to avoid exploring program paths guaranteed to provide insignificant contribution (frequency-wise) to the overall solution. <p> Frequency analysis computes the probability that a data-flow fact will occur during execution. Therefore, the probability "lattice" is an infinite chain of real numbers. Because existing demand-driven analysis frameworks are built on iterative approaches, they only permit lattices of finite size [18] or finite height <ref> [22, 29] </ref> and hence cannot derive a frequency analyzer. We overcome this limitation by designing the demand-driven analyzer based upon elimination data-flow methods [28] whose time complexity is independent of the lattice shape. We have developed a demand-driven analysis framework motivated by the Allen-Cocke interval elimination solver [2]. <p> The graph EG is called an exploded graph [22]. The data flow problems underlying ComPRE are separable, hence x e n only depends on x e m . In value-based PRE [9], constant propagation <ref> [29] </ref>, and branch correlation analysis [10], edges (x d m ; x e n ), d 6= e, may exist, complicating the analysis. The analyzer presented here handles such general exploded graphs. Requirements. The demand-driven analyzer grew out of four specific design requirements: 1. Demand-driven. <p> A forward substitution on the subgraph will yield solutions for all subgraph nodes which can be cached in case they are later desired (worst-case running time). This step corresponds to the propagation phase in [2], and to caching in <ref> [18, 29] </ref>. The framework instance calculates the probability of expression e being available at the exit of node n during the execution: x e n = p (AVAIL out [n; e] = Must) 2 R. <p> Since real programs have loop nesting level bound by a small con stant, the expected complexity is (N S), as in [2]. Although most existing demand-driven data-flow algorithms ([18, 22], <ref> [29] </ref> in particular) can be viewed (like ours) to operate on the principle of back-substituting flow functions into the right-hand side of the target variable, they do not focus on specifying a profitable order of substitutions (unlike ours) but rely instead on finding the fixed point iteratively.
Reference: [30] <author> Bernhard Steffen. </author> <title> Property oriented expansion. </title> <booktitle> In Proc. Int. Static Analysis Symposium (SAS'96), volume 1145 of LNCS, </booktitle> <pages> pages 22-41, </pages> <address> Germany, September 1996. </address> <publisher> Springer. </publisher>
Reference-contexts: In theory, even the optimal code-motion algorithm [24] breaks down on loop invariants in while-loops, unless supported by explicit do-until conversion. Recently, Steffen demonstrated that control flow restructuring can remove from the program all redundant computations, including conditional branches <ref> [30] </ref>. While his property-oriented expansion algorithm (Poe) is complete, it causes unnecessary code duplication. <p> Because this could slow down the program and create spurious exceptions, PRE disallows such unsafe code motion [24]. The desired optimization is only possible if the CFG is restructured. The PoePRE algorithm <ref> [30] </ref> would produce the program in Figure 1 (b), which was created by duplicating each node on which the value of [a+b] was available only on a subset of incoming paths. While [a + b] is fully optimized, the scope of restructuring is unnecessarily large. <p> To compare ComPRE with a restructuring-only PRE, we consider PoePRE, a version of Steffen's complete algorithm <ref> [30] </ref> that includes minimization of duplicated nodes but is restricted in that only expressions are eliminated (as is the case in ComPRE). Elimination is carried out using a temporary, as in Step 3. Theorem 2 ComPRE does not create more new nodes than PoePRE. Proof outline. <p> In Figure 1 (c), for example, ComPRE resulted in a loop with two distinct entry nodes. Even though PoePRE preserves reducibility on the same loop (Figure 1 (b)), like other restructuring-based optimizations <ref> [4, 10, 30] </ref>, it is also plagued by introducing irreducibility. One way to deal with the problem is to perform all opti mizations that presuppose single-entry loops prior to PRE. However, many algorithms for scheduling (which should follow PRE) rely on reducibility. <p> In the long history of PRE research and implementation, three distinct transformations can be identified. The seminal paper by Morel and Renviose [26] and its derivations [11, 14, 15, 16, 17, 24] employ pure, non-speculative code motion. Second, the complete PRE by Steffen <ref> [30] </ref> is based on control flow restructuring. Third, navigated by path profile information, Gupta et al apply speculative code motion in order to avoid code-motion obstacles by controlled im pairment of some paths [20].
References-found: 30

