URL: http://www.isi.edu/teamcore/tambe/papers/96/AT/adaptive.ps
Refering-URL: http://www.isi.edu/teamcore/tambe/agent.html
Root-URL: http://www.isi.edu
Email: ftambe,johnson,sheng@isi.edu  
Title: Adaptive Agent Tracking in Real-world Multi-Agent Domains: A Preliminary Report  
Author: Milind Tambe, Lewis Johnson and Wei-Min Shen 
Date: August 26, 1996  
Address: 4676 Admiralty Way, Marina del Rey, CA 90292  
Affiliation: Information Sciences Institute and Computer Science Department University of Southern California  
Abstract: Intelligent interaction in multi-agent domains frequently requires an agent to track other agents' mental states: their current goals, beliefs, and intentions. Accuracy in this agent tracking task is critically dependent on the accuracy of the tracker's (tracking agent's) model of the trackee (tracked agent). Unfortunately, in real-world situations, model imperfections arise due to the tracker's resource and information constraints, as well as due to trackees' dynamic behavior modification. While such model imperfections are unavoidable, a tracker must nonetheless attempt to be adaptive in its agent tracking. This article identifies key issues in adaptive agent tracking and presents an approach called DEFT. At its core, DEFT is based on discrimination-based learning. The main idea is to identify the deficiency of a model based on tracking failures, and revise the model by using features that are critical in discriminating successful and failed tracking episodes. Because in real-world situations the set of candidate discriminating features is very large, DEFT relies on knowledge-based focusing to limit the discrimination to those features that it determines were relevant in successful tracking episodes with an autonomous explanation capability as a major source of this knowledge. This article reports on experiments with an implementation of key aspects of DEFT in a complex synthetic air-to-air combat domain. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Anderson, C. F. Boyle, A. T. Corbett, and M. W. Lewis. </author> <title> Cognitive modeling and intelligent tutoring. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 7-49, </pages> <year> 1990. </year>
Reference-contexts: In the arena of education, intelligent tutors, whether in the form of standard intelligent tutoring systems <ref> [1] </ref> or as participants in virtual environments (e.g., a virtual guide in a virtual historical setting [18] or virtual instructor in a training environment [8, 22]), must interact with students in real-time. <p> Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments [3, 6]. Such real-time interaction is also seen in robotic environments [10]. In all these environments, agent tracking is a key capability required for intelligent interaction <ref> [38, 36, 40, 21, 1] </ref>. It involves monitoring other agents' observable actions and inferring their mental state their goals, beliefs, intentions and tracking this over time. This capability is closely related to plan recognition [9, 31], which involves recognizing agents' plans based on observations of their actions. <p> In particular, successful agent tracking requires that a tracker (tracking agent) use a specified model of the trackee (tracked agent), and the trackee's currently observed actions, to accurately infer and track its mental state. For instance, one typical agent tracking approach is model tracing <ref> [1] </ref>, where the tracker relies on an executable specification of the trackee's model. By executing this model, and matching the model's predictions with actual observations, the trackee's current goals, beliefs and intentions are tracked. <p> The limitations of the straightforward approach are identified, followed by the augmentations in DEFT that address these limitations. 3.1 RESC: An Approach to Real-time Dynamic Agent Tracking The RESC (REal-time Situated Commitments) approach to agent tracking [38] builds on model tracing <ref> [1, 40] </ref>. Here, a tracker executes a model of the trackee (the agent being tracked), matching the model's predictions with observations of the trackee's actions. One key innovation in RESC is the use of commitments.
Reference: [2] <author> D. Angluin. </author> <title> Learning regular sets from queries and counter-examples. </title> <journal> Information and Computation, </journal> <volume> 75(2) </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: In addition, we use explanation for focusing on features that are critical for the purpose of discrimination. The work described in this paper is also related to the work of learning finite state machines <ref> [23, 2, 28] </ref> and reinforcement learning [34, 41]. However, the states in the Soar agents have a great number of internal structures and features, while state-machine-based learning assumes states are atomic. This difference has two implications.
Reference: [3] <author> J. Bates, A. B. Loyall, and W. S. Reilly. </author> <title> Integrating reactivity, goals and emotions in a broad agent. </title> <type> Technical Report CMU-CS-92-142, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1992. </year> <month> 17 </month>
Reference-contexts: Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments <ref> [3, 6] </ref>. Such real-time interaction is also seen in robotic environments [10]. In all these environments, agent tracking is a key capability required for intelligent interaction [38, 36, 40, 21, 1].
Reference: [4] <author> R. B. Calder, J. E. Smith, A. J. Courtemanche, J. M. F. Mar, and A. Z. Ceranowicz. </author> <title> Modsaf behavior simulation and control. </title> <booktitle> In Proceedings of the Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <year> 1993. </year>
Reference: [5] <author> J. Cremer, J. Kearney, Y. Papelis, and R. Romano. </author> <title> The software architecture for scenario control in the Iowa driving simulator. </title> <booktitle> In Proceedings of the Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <year> 1994. </year>
Reference-contexts: Many of these multi-agent domains are dynamic and real-time, requiring the interaction to be flexible and reactive. For instance, in the arena of training, there is a recent thrust on dynamic, real-time interactive simulations e.g., realistic traffic environments <ref> [5] </ref>, or realistic combat environments [37] where intelligent agents may interact with tens or hundreds of collaborative and non-collaborative participants (agents and humans).
Reference: [6] <author> B. Hayes-Roth, L. Brownston, and R. V. Gen. </author> <title> Multiagent collaobration in directed improvisation. </title> <booktitle> In Proceedings of the International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <year> 1995. </year>
Reference-contexts: Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments <ref> [3, 6] </ref>. Such real-time interaction is also seen in robotic environments [10]. In all these environments, agent tracking is a key capability required for intelligent interaction [38, 36, 40, 21, 1].
Reference: [7] <author> W.L. Johnson. </author> <title> Agents that learn to explain themselves. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 1257-1263, </pages> <address> Seattle, WA, </address> <month> August </month> <year> 1994. </year> <booktitle> AAAI, </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: To address these difficulties, DEFT adopts a knowledge-based approach to focus discrimination on features that it determines were critical in the successful tracking episodes while relying on an autonomous explanation capability <ref> [7] </ref> to play a major role in this focusing. In short, DEFT brings together three separate threads of research: agent tracking [38], autonomous explanation [7] and discrimination-based learning [29]. DEFT is applied in a real-world synthetic air-combat environment, to enable pilot agents to adapt models of their adversaries. <p> adopts a knowledge-based approach to focus discrimination on features that it determines were critical in the successful tracking episodes while relying on an autonomous explanation capability <ref> [7] </ref> to play a major role in this focusing. In short, DEFT brings together three separate threads of research: agent tracking [38], autonomous explanation [7] and discrimination-based learning [29]. DEFT is applied in a real-world synthetic air-combat environment, to enable pilot agents to adapt models of their adversaries. <p> Therefore, nose-off angle should be considered as one of the relevant features for discrimination-based learning. Debrief <ref> [7] </ref> provides a means for analyzing successful tracking decisions in order to determine relevant features. Debrief is a package implemented in Soar which, when incorporated with Soar-based agents, enables them to explain their reasoning to people. <p> Steps 5, 6 and 7 require are critical in DEFT they are the heart of its focusing and mandate the application of an autonomous explanation capability. To this end, the the RESC agents developed for tracking [38, 35] were integrated with the Debrief explanation capability <ref> [7] </ref>. (As mentioned earlier, these agents are based on the Soar integrated architecture [16], and use chunking, a form of EBL, as the basis of all of their learning [11]).
Reference: [8] <author> W.L. Johnson. </author> <title> Pedagogical agents for virtual learning environments. </title> <booktitle> In Proceedings of the International Conference on Computers in Education, </booktitle> <pages> pages 41-48, </pages> <address> Singapore, 1995. </address> <publisher> AACE. </publisher>
Reference-contexts: In the arena of education, intelligent tutors, whether in the form of standard intelligent tutoring systems [1] or as participants in virtual environments (e.g., a virtual guide in a virtual historical setting [18] or virtual instructor in a training environment <ref> [8, 22] </ref>), must interact with students in real-time. Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments [3, 6]. Such real-time interaction is also seen in robotic environments [10]. <p> Debrief is a package implemented in Soar which, when incorporated with Soar-based agents, enables them to explain their reasoning to people. Debrief has been used in synthetic forces and in the pedagogical agent called STEVE being developed by the Virtual Environments for Training Project <ref> [8, 22] </ref>. Debrief comprises three main capabilities. * An episodic memory enables the agent to recall previous decisions and the circumstances in which they were made.
Reference: [9] <author> A. Kautz and J. F. Allen. </author> <title> Generalized plan recognition. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 32-37. </pages> <address> Menlo Park, Calif.: </address> <publisher> AAAI press, </publisher> <year> 1986. </year>
Reference-contexts: In all these environments, agent tracking is a key capability required for intelligent interaction [38, 36, 40, 21, 1]. It involves monitoring other agents' observable actions and inferring their mental state their goals, beliefs, intentions and tracking this over time. This capability is closely related to plan recognition <ref> [9, 31] </ref>, which involves recognizing agents' plans based on observations of their actions. One key difference is that plan-recognition efforts generally assume that agents are executing plans that rigidly prescribe the actions to be performed. Agent tracking, in contrast, involves recognizing a broader mix of goal-driven and reactive behaviors.
Reference: [10] <author> Y. Kuniyoshi, S. Rougeaux, M. Ishii, N. Kita, S. Sakane, and M. Kakikura. </author> <title> Cooperation by observation: the framework and the basic task pattern. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments [3, 6]. Such real-time interaction is also seen in robotic environments <ref> [10] </ref>. In all these environments, agent tracking is a key capability required for intelligent interaction [38, 36, 40, 21, 1]. It involves monitoring other agents' observable actions and inferring their mental state their goals, beliefs, intentions and tracking this over time.
Reference: [11] <author> J. E. Laird, P. S. Rosenbloom, and A. Newell. </author> <title> Chunking in soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: All of the agents discussed in this article are based on the Soar integrated architecture [16]. We will assume some familiarity with Soar's problem-solving, which involves applying an operator hierarchy to a state to reach a desired state. For learning, Soar relies on chunking <ref> [11] </ref>, a form of explanation-based learning (EBL)[15], and its operation will be explained in a following section. The initial implementation of key elements of DEFT in Soar has shown some promising results; although many issues remain open for future research. <p> To this end, the the RESC agents developed for tracking [38, 35] were integrated with the Debrief explanation capability [7]. (As mentioned earlier, these agents are based on the Soar integrated architecture [16], and use chunking, a form of EBL, as the basis of all of their learning <ref> [11] </ref>). Figure 6 shows a snapshot of the Debrief user interface listing some of the 4 Chunks from steps 6 and 7 were in reality hand-edited to eliminate some of their overspecificity; otherwise they would not fire appropriately in step 5.
Reference: [12] <author> R. L. Lewis. </author> <title> An architecturally-based theory of human sentence comprehension. </title> <booktitle> In Proceedings of the Annual Conference of the Cognitive Science Society, </booktitle> <year> 1993. </year>
Reference-contexts: Should this commitment lead to a tracking error, a real-time repair mechanism is invoked. RESC is thus a repair-based approach to tracking (like repair-based approaches to constraint satisfaction [14] and natural language understanding <ref> [12] </ref>). A second key technique in RESC leads to its situatedness, i.e., responsiveness to the present. To track the trackee's dynamic behaviors, it is necessary to execute the trackee's model so it is responsive to the changing world situation.
Reference: [13] <author> Pazzani M. and Kibler D. </author> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9(1) </volume> <pages> 57-94, </pages> <year> 1991. </year>
Reference-contexts: First, the number of states is so extremely large that it is beyond the scale of most state-machine-based learning approaches. Second, the structured states offer advantages when discriminating instances and thus learning can proceed rapidly. In terms of using knowledge to guide the learning process, theory revision (e.g. FOCL <ref> [13] </ref>) is another large body of research that is related to our work. Given a set of positive and negative examples and an approximate theory T , a theory revision system can produce a revised theory RT of T such that RT will correctly classify the given examples.
Reference: [14] <author> S. Minton, M. D. Johnston, A. Philips, and P. Laird. </author> <title> Solving large-scale constraint satisfaction and scheduling problems using a heuristic repair method. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: Therefore, RESC commits to one, heuristically selected, execution path through the model, which provides a constraining context for its continued interpretations. Should this commitment lead to a tracking error, a real-time repair mechanism is invoked. RESC is thus a repair-based approach to tracking (like repair-based approaches to constraint satisfaction <ref> [14] </ref> and natural language understanding [12]). A second key technique in RESC leads to its situatedness, i.e., responsiveness to the present. To track the trackee's dynamic behaviors, it is necessary to execute the trackee's model so it is responsive to the changing world situation.
Reference: [15] <author> T. M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: This requirement is very likely to undermine most of the standard learning methods, for example, decision tree induction [20] or relational concept learning [19]. On the other hand, unlike most knowledge-rich speed-up learning, such as the traditional explanation-based learning <ref> [15] </ref>, model adaptation in agent tracking must be at the knowledge level the tracker agent must take some inductive leap to mend the model which may not be entirely explanable by the knowledge that is available. Given these constraints, a discrimination-based method offers itself as one viable choice.
Reference: [16] <author> A. Newell. </author> <title> Unified Theories of Cognition. </title> <publisher> Harvard Univ. Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: All of the agents discussed in this article are based on the Soar integrated architecture <ref> [16] </ref>. We will assume some familiarity with Soar's problem-solving, which involves applying an operator hierarchy to a state to reach a desired state. For learning, Soar relies on chunking [11], a form of explanation-based learning (EBL)[15], and its operation will be explained in a following section. <p> To this end, the the RESC agents developed for tracking [38, 35] were integrated with the Debrief explanation capability [7]. (As mentioned earlier, these agents are based on the Soar integrated architecture <ref> [16] </ref>, and use chunking, a form of EBL, as the basis of all of their learning [11]).
Reference: [17] <author> J. D. Pearson and J. E. Laird. </author> <title> Toward incremental knowledge correction for agents in complex environments. </title> <journal> Machine Intelligence, </journal> <volume> 15, </volume> <year> 1996. </year>
Reference-contexts: For example, there is less need for a detailed, runnable trackee model in domains where the trackee is perfectly willing to tell you what his/her goals and intentions are. Similar to the approach described in this paper, Pearson and Laird <ref> [17] </ref> also studied the problem of correcting an existing model using discriminations.
Reference: [18] <author> K. Pimentel and K. Teixeira. </author> <title> Virtual reality: Through the new looking glass. Windcrest/McGraw-Hill, Blue Ridge Summit, </title> <address> PA, </address> <year> 1994. </year> <month> 18 </month>
Reference-contexts: In the arena of education, intelligent tutors, whether in the form of standard intelligent tutoring systems [1] or as participants in virtual environments (e.g., a virtual guide in a virtual historical setting <ref> [18] </ref> or virtual instructor in a training environment [8, 22]), must interact with students in real-time. Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments [3, 6]. Such real-time interaction is also seen in robotic environments [10].
Reference: [19] <author> R. J. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: This requirement is very likely to undermine most of the standard learning methods, for example, decision tree induction [20] or relational concept learning <ref> [19] </ref>.
Reference: [20] <author> R. J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: This requirement is very likely to undermine most of the standard learning methods, for example, decision tree induction <ref> [20] </ref> or relational concept learning [19].
Reference: [21] <author> A. S. Rao. </author> <title> Means-end plan recognition: Towards a theory of reactive recognition. </title> <booktitle> In Proceedings of the International Conference on Knowledge Representation and Reasoning (KR-94), </booktitle> <year> 1994. </year>
Reference-contexts: Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments [3, 6]. Such real-time interaction is also seen in robotic environments [10]. In all these environments, agent tracking is a key capability required for intelligent interaction <ref> [38, 36, 40, 21, 1] </ref>. It involves monitoring other agents' observable actions and inferring their mental state their goals, beliefs, intentions and tracking this over time. This capability is closely related to plan recognition [9, 31], which involves recognizing agents' plans based on observations of their actions.
Reference: [22] <author> J. Rickel and W.L. Johnson. </author> <title> Pedagogical agents for immersive training environments. </title> <booktitle> Accepted for publication in the Proceedings of the First International Conference on Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: In the arena of education, intelligent tutors, whether in the form of standard intelligent tutoring systems [1] or as participants in virtual environments (e.g., a virtual guide in a virtual historical setting [18] or virtual instructor in a training environment <ref> [8, 22] </ref>), must interact with students in real-time. Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments [3, 6]. Such real-time interaction is also seen in robotic environments [10]. <p> Debrief is a package implemented in Soar which, when incorporated with Soar-based agents, enables them to explain their reasoning to people. Debrief has been used in synthetic forces and in the pedagogical agent called STEVE being developed by the Virtual Environments for Training Project <ref> [8, 22] </ref>. Debrief comprises three main capabilities. * An episodic memory enables the agent to recall previous decisions and the circumstances in which they were made.
Reference: [23] <author> R.L. Rivest and R.E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <booktitle> Information and Computation, </booktitle> <year> 1993. </year>
Reference-contexts: In addition, we use explanation for focusing on features that are critical for the purpose of discrimination. The work described in this paper is also related to the work of learning finite state machines <ref> [23, 2, 28] </ref> and reinforcement learning [34, 41]. However, the states in the Soar agents have a great number of internal structures and features, while state-machine-based learning assumes states are atomic. This difference has two implications.
Reference: [24] <author> P. S. Rosenbloom, J. E. Laird, A. Newell, , and R. McCarl. </author> <title> A preliminary analysis of the soar architecture as a basis for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 47(1-3):289-325, </volume> <year> 1991. </year>
Reference-contexts: Skipping to the final subgoal, maintain-heading enables the tracker to maintain its heading, as seen in Figure 1-b. trackee to be currently executing. Dashed lines are unselected alternative operators. The tracker's dynamic behavior in this environment is supported by Soar's architectural mechanisms for flexible operator selection and reactive termination <ref> [24] </ref>. The tracker reuses this architecture in tracking. Thus, the tracker uses a hierarchy such as the one in Figure 3-b in tracking.
Reference: [25] <author> W.M. Shen. </author> <title> Complementary discrimination learning: A duality between generalization and discrimination. </title> <booktitle> In Proceedings of Eighth National Conference on Artificial Intelligence. </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The LIVE framework has been applied successfully to learning many different types of concepts and action models, including Boolean concepts <ref> [25] </ref>, decision lists [26], first-order action and prediction rules [27], recursive theoretical terms (hidden variables) [30], and finite-state machines [28]. In addition, the LIVE style of learning is incremental, and the training examples can be either perfect or noisy.
Reference: [26] <author> W.M. Shen. </author> <title> Complementary discrimination learning with decision lists. </title> <booktitle> In Proceedings of Tenth National Conference on Artificial Intelligence. </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: The LIVE framework has been applied successfully to learning many different types of concepts and action models, including Boolean concepts [25], decision lists <ref> [26] </ref>, first-order action and prediction rules [27], recursive theoretical terms (hidden variables) [30], and finite-state machines [28]. In addition, the LIVE style of learning is incremental, and the training examples can be either perfect or noisy. Compared to other incremental learning algorithms, LIVE assumes less biases in the learning process.
Reference: [27] <author> W.M. Shen. </author> <title> Discovery as autonomous learning from the environment. </title> <journal> Machine Learning, </journal> <volume> 12 </volume> <pages> 143-165, </pages> <year> 1993. </year>
Reference-contexts: backtracking, a repair-based approach that attempts the generation a new matching operator hierarchy without re-examining past states (see [38] for more details). 3.2 Applying Discrimination-based Learning to RESC To apply discrimination-based learning in service of adaptiveness in agent tracking, we have chosen the approach of autonomous learning from the environment <ref> [27, 29] </ref>. At the center of this approach is the LIVE framework of predict-surprise-revise. In simpler domains, the approach works as follows. The prediction made by a model about some other agents, or the environment is compared and is verified or falsified by the actual observations. <p> The LIVE framework has been applied successfully to learning many different types of concepts and action models, including Boolean concepts [25], decision lists [26], first-order action and prediction rules <ref> [27] </ref>, recursive theoretical terms (hidden variables) [30], and finite-state machines [28]. In addition, the LIVE style of learning is incremental, and the training examples can be either perfect or noisy. Compared to other incremental learning algorithms, LIVE assumes less biases in the learning process.
Reference: [28] <author> W.M. Shen. </author> <title> Learning finite state automata using local distinguishing experiments. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <address> Chambery, France, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The LIVE framework has been applied successfully to learning many different types of concepts and action models, including Boolean concepts [25], decision lists [26], first-order action and prediction rules [27], recursive theoretical terms (hidden variables) [30], and finite-state machines <ref> [28] </ref>. In addition, the LIVE style of learning is incremental, and the training examples can be either perfect or noisy. Compared to other incremental learning algorithms, LIVE assumes less biases in the learning process. <p> In addition, we use explanation for focusing on features that are critical for the purpose of discrimination. The work described in this paper is also related to the work of learning finite state machines <ref> [23, 2, 28] </ref> and reinforcement learning [34, 41]. However, the states in the Soar agents have a great number of internal structures and features, while state-machine-based learning assumes states are atomic. This difference has two implications.
Reference: [29] <author> W.M. Shen. </author> <title> Autonomous Learning from the Environment. </title> <editor> W. H. </editor> <publisher> Freeman, Computer Science Press, </publisher> <year> 1994. </year>
Reference-contexts: In short, DEFT brings together three separate threads of research: agent tracking [38], autonomous explanation [7] and discrimination-based learning <ref> [29] </ref>. DEFT is applied in a real-world synthetic air-combat environment, to enable pilot agents to adapt models of their adversaries. While our analysis focuses on this one environment, given its real-world character, we expect that its lessons will generalize to some of the other multi-agent environments mentioned earlier. <p> backtracking, a repair-based approach that attempts the generation a new matching operator hierarchy without re-examining past states (see [38] for more details). 3.2 Applying Discrimination-based Learning to RESC To apply discrimination-based learning in service of adaptiveness in agent tracking, we have chosen the approach of autonomous learning from the environment <ref> [27, 29] </ref>. At the center of this approach is the LIVE framework of predict-surprise-revise. In simpler domains, the approach works as follows. The prediction made by a model about some other agents, or the environment is compared and is verified or falsified by the actual observations.
Reference: [30] <author> W.M. Shen and H.A. Simon. </author> <title> Fitness requirements for scientific theories containing recursive theoretical terms. </title> <journal> British Journal for the Philosophy of Science, </journal> <pages> pages 641-652, </pages> <year> 1993. </year>
Reference-contexts: The LIVE framework has been applied successfully to learning many different types of concepts and action models, including Boolean concepts [25], decision lists [26], first-order action and prediction rules [27], recursive theoretical terms (hidden variables) <ref> [30] </ref>, and finite-state machines [28]. In addition, the LIVE style of learning is incremental, and the training examples can be either perfect or noisy. Compared to other incremental learning algorithms, LIVE assumes less biases in the learning process.
Reference: [31] <author> F. Song and R. Cohen. </author> <title> Temporal reasoning during plan recognition. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence. </booktitle> <address> Menlo Park, Calif.: </address> <publisher> AAAI press, </publisher> <year> 1991. </year>
Reference-contexts: In all these environments, agent tracking is a key capability required for intelligent interaction [38, 36, 40, 21, 1]. It involves monitoring other agents' observable actions and inferring their mental state their goals, beliefs, intentions and tracking this over time. This capability is closely related to plan recognition <ref> [9, 31] </ref>, which involves recognizing agents' plans based on observations of their actions. One key difference is that plan-recognition efforts generally assume that agents are executing plans that rigidly prescribe the actions to be performed. Agent tracking, in contrast, involves recognizing a broader mix of goal-driven and reactive behaviors.
Reference: [32] <author> The DIS steering committee. </author> <title> The dis vision: A map to the future of distributed simulation. </title> <type> Technical Report IST-SP-94-01, </type> <institution> Institute for simulation and training, University of Central Florida, </institution> <address> Orlando, </address> <month> May </month> <year> 1994. </year>
Reference: [33] <author> P. Stone and M. Veloso. </author> <title> Multiagent systems: A survey from a machine learning perspective. </title> <journal> IEEE Transaction on Data and Knowledge Engineering, </journal> <note> (Submitted), 1997. 19 </note>
Reference-contexts: In such cases, analysis of additional tracking success and failure 15 episodes may be essential to narrow down the differences an important issue for future work. 7 Related Work Research in multi-agent systems has experienced tremendous growth in the recent years. In a survey paper by Stone and Veloso <ref> [33] </ref>, multi-agent systems are classified along the dimensions of agent heterogeneity and communications. In this taxonomy, our adaptive tracking agents can in principle be viewed as heterogeneous and communicating although in this particular implementation we only used homogeneous agent models and ignored the communication between agents.
Reference: [34] <author> R. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Machine Learning Conference, </booktitle> <year> 1990. </year>
Reference-contexts: In addition, we use explanation for focusing on features that are critical for the purpose of discrimination. The work described in this paper is also related to the work of learning finite state machines [23, 2, 28] and reinforcement learning <ref> [34, 41] </ref>. However, the states in the Soar agents have a great number of internal structures and features, while state-machine-based learning assumes states are atomic. This difference has two implications. First, the number of states is so extremely large that it is beyond the scale of most state-machine-based learning approaches.
Reference: [35] <author> M. Tambe. </author> <title> Recursive agent and agent-group tracking in a real-time dynamic environment. </title> <booktitle> In Proceedings of the International Conference on Multi-agent systems (ICMAS), </booktitle> <year> 1995. </year>
Reference-contexts: Steps 5, 6 and 7 require are critical in DEFT they are the heart of its focusing and mandate the application of an autonomous explanation capability. To this end, the the RESC agents developed for tracking <ref> [38, 35] </ref> were integrated with the Debrief explanation capability [7]. (As mentioned earlier, these agents are based on the Soar integrated architecture [16], and use chunking, a form of EBL, as the basis of all of their learning [11]).
Reference: [36] <author> M. Tambe. </author> <title> Tracking dynamic team activity. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI), </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments [3, 6]. Such real-time interaction is also seen in robotic environments [10]. In all these environments, agent tracking is a key capability required for intelligent interaction <ref> [38, 36, 40, 21, 1] </ref>. It involves monitoring other agents' observable actions and inferring their mental state their goals, beliefs, intentions and tracking this over time. This capability is closely related to plan recognition [9, 31], which involves recognizing agents' plans based on observations of their actions. <p> With respect to other less harmful imperfections, a flexible tracking strategy one that can work with an imperfect model of the trackee would appear to be a more fruitful approach. Such a strategy would need the capability to switch inferences dynamically in real-time <ref> [38, 36] </ref>. For instance, if the tracker is not sure if the trackee is performing an offensive maneuver or a defensive maneuver, it may first assume that the maneuver is offensive (worst-case scenario), and then flexibly modify this assumption as soon as warranted by further observations.
Reference: [37] <author> M. Tambe, W. L. Johnson, R. Jones, F. Koss, J. E. Laird, P. S. Rosenbloom, and K. Schwamb. </author> <title> Intelligent agents for interactive simulation environments. </title> <journal> AI Magazine, </journal> <volume> 16(1), </volume> <month> Spring </month> <year> 1995. </year>
Reference-contexts: Many of these multi-agent domains are dynamic and real-time, requiring the interaction to be flexible and reactive. For instance, in the arena of training, there is a recent thrust on dynamic, real-time interactive simulations e.g., realistic traffic environments [5], or realistic combat environments <ref> [37] </ref> where intelligent agents may interact with tens or hundreds of collaborative and non-collaborative participants (agents and humans). <p> This was intended to confuse the participating intelligent pilot agents, and indeed it did <ref> [37] </ref>. Unable to track this changed missile firing tactic, intelligent pilot agents got shot down. Of course, human pilots are bound to come up with novel variations on known maneuvers, and intelligent agents cannot be expected to anticipate them. <p> Rules learned in steps 6 and 7 are used in step 8 to focus the comparison with the situation recalled in step 5. 4 6 Experimental Results We have implemented some key aspects of the above approach in an intelligent pilot agent for the air-combat simulation environment <ref> [37] </ref>. Specifically, we have an agent that implements steps 4-8 in Figure 5, while relying on simple heuristics in step 4. Steps 5, 6 and 7 require are critical in DEFT they are the heart of its focusing and mandate the application of an autonomous explanation capability.
Reference: [38] <author> M. Tambe and P. S. Rosenbloom. RESC: </author> <title> An approach for real-time, dynamic agent tracking. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <year> 1995. </year>
Reference-contexts: Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments [3, 6]. Such real-time interaction is also seen in robotic environments [10]. In all these environments, agent tracking is a key capability required for intelligent interaction <ref> [38, 36, 40, 21, 1] </ref>. It involves monitoring other agents' observable actions and inferring their mental state their goals, beliefs, intentions and tracking this over time. This capability is closely related to plan recognition [9, 31], which involves recognizing agents' plans based on observations of their actions. <p> In short, DEFT brings together three separate threads of research: agent tracking <ref> [38] </ref>, autonomous explanation [7] and discrimination-based learning [29]. DEFT is applied in a real-world synthetic air-combat environment, to enable pilot agents to adapt models of their adversaries. <p> Agent tracking is of course one key aspect of such an intelligent interaction <ref> [38] </ref>. Certainly, an adversary will not communicate information regarding its goals and plans to an agent voluntarily such information must be inferred via tracking. <p> It needs to infer a missile firing from the trackee's observable maneuvers, even though those are often ambiguous. Nonetheless, given a reasonably accurate model of the trackee, the tracker agent can hope to address such ambiguity in real-time <ref> [38] </ref>. Given adaptiveness on part of the trackee, however, the problem becomes much more difficult. The tracker cannot necessarily assume its model of the trackee is accurate, or that it will stay accurate over time. <p> With respect to other less harmful imperfections, a flexible tracking strategy one that can work with an imperfect model of the trackee would appear to be a more fruitful approach. Such a strategy would need the capability to switch inferences dynamically in real-time <ref> [38, 36] </ref>. For instance, if the tracker is not sure if the trackee is performing an offensive maneuver or a defensive maneuver, it may first assume that the maneuver is offensive (worst-case scenario), and then flexibly modify this assumption as soon as warranted by further observations. <p> This is followed by a discussion of the straightforward application of discrimination-based learning. The limitations of the straightforward approach are identified, followed by the augmentations in DEFT that address these limitations. 3.1 RESC: An Approach to Real-time Dynamic Agent Tracking The RESC (REal-time Situated Commitments) approach to agent tracking <ref> [38] </ref> builds on model tracing [1, 40]. Here, a tracker executes a model of the trackee (the agent being tracked), matching the model's predictions with observations of the trackee's actions. One key innovation in RESC is the use of commitments. <p> RESC's primary repair mechanism 7 to recover from such failures is current-state backtracking, a repair-based approach that attempts the generation a new matching operator hierarchy without re-examining past states (see <ref> [38] </ref> for more details). 3.2 Applying Discrimination-based Learning to RESC To apply discrimination-based learning in service of adaptiveness in agent tracking, we have chosen the approach of autonomous learning from the environment [27, 29]. At the center of this approach is the LIVE framework of predict-surprise-revise. <p> Steps 5, 6 and 7 require are critical in DEFT they are the heart of its focusing and mandate the application of an autonomous explanation capability. To this end, the the RESC agents developed for tracking <ref> [38, 35] </ref> were integrated with the Debrief explanation capability [7]. (As mentioned earlier, these agents are based on the Soar integrated architecture [16], and use chunking, a form of EBL, as the basis of all of their learning [11]).
Reference: [39] <author> M. Tambe and P. S. Rosenbloom. </author> <title> Architectures for agents that track other agents in multi-agent worlds. </title> <booktitle> In Intelligent Agents, Volume II: Lecture Notes in Artificial Intelligence 1037. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1996. </year>
Reference-contexts: This transition from step 1 to step 2 requires generalized methods for blame assignment. In standard discrimination-based learning, the next step would be essentially step 8, that is to identify a previously successful agent tracking episode and compare with the current failure episode. 3 For interested readers, <ref> [39] </ref> provides details; specifically, RESC is based on a modified version of the Soar architecture, where these architectural modifications are included in the form of Soar rules. 11 DEFT's key additions are steps 3 through 7, which help focus the comparison.
Reference: [40] <author> B. Ward. ET-Soar: </author> <title> Toward an ITS for Theory-Based Representations. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon Univ., </institution> <year> 1991. </year>
Reference-contexts: Similarly, in the arena of entertainment, recent work has focused on real-time, dynamic interactivity among multiple agents within virtual reality environments [3, 6]. Such real-time interaction is also seen in robotic environments [10]. In all these environments, agent tracking is a key capability required for intelligent interaction <ref> [38, 36, 40, 21, 1] </ref>. It involves monitoring other agents' observable actions and inferring their mental state their goals, beliefs, intentions and tracking this over time. This capability is closely related to plan recognition [9, 31], which involves recognizing agents' plans based on observations of their actions. <p> The limitations of the straightforward approach are identified, followed by the augmentations in DEFT that address these limitations. 3.1 RESC: An Approach to Real-time Dynamic Agent Tracking The RESC (REal-time Situated Commitments) approach to agent tracking [38] builds on model tracing <ref> [1, 40] </ref>. Here, a tracker executes a model of the trackee (the agent being tracked), matching the model's predictions with observations of the trackee's actions. One key innovation in RESC is the use of commitments.
Reference: [41] <author> C. Watkins and P Dayan. </author> <title> Technical note: </title> <journal> q-learning. Machine Learning, </journal> <volume> 8(3/4), </volume> <year> 1992. </year> <month> 20 </month>
Reference-contexts: In addition, we use explanation for focusing on features that are critical for the purpose of discrimination. The work described in this paper is also related to the work of learning finite state machines [23, 2, 28] and reinforcement learning <ref> [34, 41] </ref>. However, the states in the Soar agents have a great number of internal structures and features, while state-machine-based learning assumes states are atomic. This difference has two implications. First, the number of states is so extremely large that it is beyond the scale of most state-machine-based learning approaches.
References-found: 41

