URL: http://www.cs.unm.edu/~bap/papers/ieee-dynnn-draft.ps.gz
Refering-URL: http://www.cs.unm.edu/~bap/publications.html
Root-URL: http://www.cs.unm.edu
Title: Gradient Calculations for Dynamic Recurrent Neural Networks: A Survey  
Author: Barak A. Pearlmutter 
Keyword: Recurrent neural networks, backpropagation through time, real time recurrent learning, trajectory learning.  
Note: DRAFT OF July 20, 1995FOR IEEE TRANSACTIONS ON NEURAL NETWORKS 1  
Abstract: We survey learning algorithms for recurrent neural networks with hidden units, and put the various techniques into a common framework. We discuss fixedpoint learning algorithms, namely recurrent backpropagation and deterministic Boltzmann Machines, and non-fixedpoint algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an online technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. We discuss advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones, continue with some "tricks of the trade" for training, using, and simulating continuous time and recurrent neural networks. We present some simulations, and at the end, address issues of computational complexity and learning speed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. L. McClelland, D. E. Rumelhart, and G. E. Hinton, </author> <title> "The appeal of parallel distributed processing", </title> <editor> In Rumelhart et al. </editor> <volume> [151]. </volume>
Reference: [2] <author> D. Marr and T. Poggio, </author> <title> "Cooperative computation of stereo disparity", </title> <journal> Science, </journal> <volume> vol. 194, </volume> <pages> pp. 283-287, </pages> <year> 1976. </year>
Reference: [3] <author> G. Marr, D. Palm and T. Poggio, </author> <title> "Analysis of a cooperative stereo algorithm", </title> <journal> Biological Cybernetics, </journal> <volume> vol. 28, </volume> <pages> pp. 223-229, </pages> <year> 1978. </year>
Reference: [4] <author> Geoffrey E. Hinton, </author> <title> Relaxation and its role in vision, </title> <type> PhD thesis, </type> <institution> University of Edinburgh, </institution> <year> 1977, </year> <booktitle> Described in [152, </booktitle> <pages> pages 408-430]. </pages>
Reference: [5] <author> L. S. Davis and A. Rosenfeld, </author> <title> "Cooperating processes for low-level vision: A survey", </title> <journal> Artificial Intelligence, </journal> <volume> vol. 3, </volume> <pages> pp. 245-264, </pages> <year> 1981. </year>

Reference: [7] <author> Geoffrey E. Hinton, </author> <title> "Using relaxation to find a puppet", </title> <booktitle> in Proceedings of the A.I.S.B. Summer Conference, </booktitle> <institution> University of Edinburgh, </institution> <month> July </month> <year> 1976. </year>
Reference: [8] <author> D. L. Waltz and J. B. Pollack, </author> <title> "Massively parallel parsing: A strongly interactive model of natural language interpretation", </title> <journal> Cognitive Science, </journal> <volume> vol. 9, </volume> <pages> pp. 51-74, </pages> <year> 1985. </year>
Reference: [9] <author> Ning Qian and Terrence J. Sejnowski, </author> <title> "Learning to solve random-dot stereograms of dense and transparent surfaces with recurrent backpropagation", </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> David S. Touretzky, Geof-frey E. Hinton, and Terrence J. Sejnowski, Eds. </editor> <year> 1989, </year> <pages> pp. 435-443, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, redoing the training with weight symmetry enforced, however, caused the network to learn not only the training data but also to do well on these untrained completions. <ref> [9] </ref> successfully applied the [72], [73] recurrent backprop 3 A unit with external input could be pushed outside the [0,1] bounds of the range of the oe () used. agation learning procedure to learning weights for a relaxation procedure for dense stereo disparity problems with transparent surfaces.
Reference: [10] <author> O. Nerrand, P. Roussel-Ragot, G. Dreyfus L. Personnaz, and S. Marcos, </author> <title> "Neural networks and non-linear adaptive filtering: Unifying concepts and new algorithms", </title> <journal> Neural Computation, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 165-197, </pages> <year> 1993. </year>
Reference: [11] <author> T. W. Karjala, D. M. Himmelblau, and R. Miikkulainen, </author> <title> "Data rectification using recurrent (Elman) neural networks", </title> <booktitle> In IJCNN92'Baltimore [153], </booktitle> <pages> pp. 901-905. </pages>
Reference: [12] <author> Raymond L. Watrous, Bruce Laedendorf, and Gary M. Kuhn, </author> <title> "Complete gradient optimization of a recurrent network applied to BDG descrimination", </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 87, no. 3, </volume> <pages> pp. 1301-1309, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: Given modifiable time delays, we would like to be able to learn appropriate values for them, which can be accomplished using gradient descent by @E = t 0 dy i (t o ij )dt: (41) <ref> [12] </ref> applied recurrent networks with immutable time delays in the domain of speech. Feedforward networks with immutable time delays (TDNNs) have been applied with great success in the same domain by Lang et al. [22].
Reference: [13] <author> P. Poddar and K.P. Unnikrishnan, </author> <title> "Nonlinear prediction of speech signals using memory neuron networks", </title> <editor> In Juang et al. </editor> <volume> [31], </volume> <pages> pp. 395-404. </pages>
Reference: [14] <author> Dario Albesano adn Roberto Gemello and Franco Mana, </author> <title> "Word recognition with recurrent network automata", </title> <booktitle> In IJCNN92'Baltimore [153], </booktitle> <pages> pp. 308-313. </pages>
Reference: [15] <author> Shawn R. Lockery, Yan Fang, and Terrence J. Sejnowski, </author> <title> "A dynamic neural network model of sensorimotor transformations in the leech", </title> <journal> Neural Computation, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 274-282, </pages> <year> 1990. </year>
Reference: [16] <author> Kenji Doya, M. E. T. Boyle, and A. I. Selverston, </author> <title> "Mapping between neural and physical activities of the lobster gastric mill system", </title> <editor> In Hanson et al. </editor> <volume> [154], </volume> <pages> pp. 913-920. </pages>
Reference: [17] <author> Kenji Doya, A. I. Selverston, and P. F. Rowat, </author> <title> "A hodgkin-huxley type neuron model that learns slow non-spike oscillation", </title> <editor> In Cowan et al. </editor> <volume> [155]. </volume>
Reference: [18] <editor> Yves Chauvin and David E. Rumelhart, Eds., </editor> <title> Back-propagation: Theory, Architectures and Applications, </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1995. </year>
Reference: [19] <author> Paul R. Gorman and Terrence J. Sejnowski, </author> <title> "Analysis of hidden units in a layered network trained to classify sonar targets", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 1, no. 1, </volume> <pages> pp. 75-89, </pages> <year> 1988. </year>
Reference: [20] <author> Kevin Lang and Geoffrey Hinton, </author> <title> "The development of the time-delay neural network architecture for speech recognition", </title> <type> Tech. Rep. </type> <institution> CMU-CS-88-152, Department of Computer Science, Carnegie Mellon University, </institution> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: In the presence of time delays, it is reasonable to have more than one connection between a single pair of units, with different time delays along the different connections. Such "time delay neural networks" have proven useful in the domain of speech recognition <ref> [20] </ref>, [22], [21], [115]. Having more than one connection from one unit to another requires us to modify our notation somewhat; weights and time delays are modified to take a single index, and we introduce some external apparatus to specify the source and destination of each connection.
Reference: [21] <author> Alex Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang, </author> <title> "Phoneme recognition using time-delay networks", </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 37, no. 3, </volume> <pages> pp. 328-339, </pages> <year> 1989. </year>
Reference-contexts: In the presence of time delays, it is reasonable to have more than one connection between a single pair of units, with different time delays along the different connections. Such "time delay neural networks" have proven useful in the domain of speech recognition [20], [22], <ref> [21] </ref>, [115]. Having more than one connection from one unit to another requires us to modify our notation somewhat; weights and time delays are modified to take a single index, and we introduce some external apparatus to specify the source and destination of each connection.
Reference: [22] <author> Kevin J. Lang, Geoffrey E. Hinton, and Alex Waibel, </author> <title> "A time-delay neural network architecture for isolated word recognition", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 3, no. 1, </volume> <pages> pp. 23-43, </pages> <year> 1990. </year>
Reference-contexts: Feedforward networks with immutable time delays (TDNNs) have been applied with great success in the same domain by Lang et al. <ref> [22] </ref>. A variant of TDNNs which learn the time delays was explored by Bodenhausen et al. [113]. The synapses in their networks, rather than having point taps, have gaussian envelopes whose widths and centers were both learned. <p> In the presence of time delays, it is reasonable to have more than one connection between a single pair of units, with different time delays along the different connections. Such "time delay neural networks" have proven useful in the domain of speech recognition [20], <ref> [22] </ref>, [21], [115]. Having more than one connection from one unit to another requires us to modify our notation somewhat; weights and time delays are modified to take a single index, and we introduce some external apparatus to specify the source and destination of each connection.
Reference: [23] <author> Kevin J. Lang and Geoffrey E. Hinton, </author> <title> "Dimensionality reduction and prior knowledge in e-set recognition", </title> <booktitle> In Touretzky [156], </booktitle> <pages> pp. 178-185. </pages>
Reference-contexts: The techniques we will discuss here, like those of section II, are quite general purpose: they can accommodate hidden units as well as various architectural embellishments, such as second-order connections [93], [34], [94], [44], weight sharing <ref> [23] </ref>, [35], and in general any of the architectureal modifications made to neural networks to customize them for their problem domain. We will consider two major gradient calculation techniques, and then a few more derived from them.
Reference: [24] <author> D. W. Tank and J. J. </author> <title> Hopfield, "Concentrating information in time: Analog neural networks with applications to speech recognition problems", </title> <booktitle> In Caudill and Butler [157], </booktitle> <pages> pp. 455-468. </pages>
Reference: [25] <author> Shawn P. Day and Michael R. Davenport, </author> <title> "Continuous-time temporal backpropagation with adaptable time delays", </title> <journal> IEEE Transactions on Neural Networks, </journal> , <volume> no. 2, </volume> <pages> pp. 348-354, </pages> <year> 1993, </year> <note> Ftp archive.cis.ohio-state.edu: /pub/neuroprose/day.temporal.ps.Z. </note>
Reference-contexts: There are continuous-time feed-forward learning algorithms that are as efficient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections [77], [78], [79], [80], <ref> [25] </ref> or for a survey, [81]. Later, we will describe a number of training procedures that, for a price in space or time, do not rely on such restrictions and can be applied to training networks to exhibit desired limit cycles, or particular detailed temporal behavior. C. Continuous vs. <p> A continuous time feed-forward network with learned time delays was successfully applied to a difficult time-series prediction task by Day and Davenport <ref> [25] </ref>. In the sections on time constants and delays, we have carried out the derivative derivations for BPTT. All the other techniques also remain applicable to this case, with straightforward derivations. The analogous derivations for RTRL are carried out in [76].
Reference: [26] <author> M. I. Jordan, </author> <title> "Generic constraints on underspecified target trajectories", </title> <booktitle> In IJCNN89 [158]. </booktitle>
Reference: [27] <editor> Applications of Artificial Neural Networks, </editor> <booktitle> number 1294 in APIE Proceedings Series, </booktitle> <address> Orlando, Florida, </address> <month> Apr. 18-20 </month> <year> 1990. </year>
Reference: [28] <author> A. S. Weigend, D. E. Rumelhart, and B. A. Huberman, </author> <title> "Generalization by weight-elimination with application to forecasting", </title> <editor> In Lippmann et al. </editor> <volume> [159], </volume> <pages> pp. 875-882. </pages>
Reference-contexts: For instance, adding P i is equivalent to assuming that the weights are Gaussian distributed. Not adding such a term is equivalent to assuming that the a priori distribution on what the weights will turn out to be is flat|not a totally unreasonable prior <ref> [28] </ref>, [130]. However, we have added some new sorts of parameters, namely time constants and time delays, here represented generically by the vector T . These are scale parameters, which differ from positional parameters in a number of ways.
Reference: [29] <author> Bert de Vries and Jose C. Principe, </author> <title> "A theory for neural networks with time delays", </title> <editor> In Lippmann et al. </editor> <volume> [159], </volume> <pages> pp. 162-168. </pages>
Reference-contexts: The synapses in their networks, rather than having point taps, have gaussian envelopes whose widths and centers were both learned. Similar synaptic architectures using alpha function envelopes (which obviate the need for a history buffer) whose parameters were learned were proposed and used in systems without hidden units [114], <ref> [29] </ref>. A continuous time feed-forward network with learned time delays was successfully applied to a difficult time-series prediction task by Day and Davenport [25]. In the sections on time constants and delays, we have carried out the derivative derivations for BPTT.
Reference: [30] <author> A.D. Back and A.C. Tsoi, </author> <title> "FIR and IIR synapses, a new neural network architecture for time series modelling", </title> <journal> Neural Computation, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 337-350, </pages> <year> 1991. </year>
Reference: [31] <author> B.H. Juang, S.Y. Kung, and C A. Camm, Eds., </author> <title> Neural Networks for Signal Processing: </title> <booktitle> Proceedings of the 1991 IEEE Workshop. </booktitle> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference: [32] <author> D. Hush and B. </author> <title> Horne, "Progress in supervised neural networks", </title> <journal> IEEE Signal Processing Magazine, </journal> <volume> vol. 10, no. 1, </volume> <pages> pp. 8-39, </pages> <year> 1993. </year>
Reference: [33] <author> B. de Vries and J. Principe, </author> <title> "The gamma model|a new neural network for temporal processing", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, no. 4, </volume> <pages> pp. 565-576, </pages> <year> 1992. </year>
Reference: [34] <author> T. Maxwell, C. L. Giles, Y. C. Lee, and H. H. Chen, </author> <title> "Nonlinear dynamics of artificial neural systems", </title> <booktitle> In Denker [160], </booktitle> <pages> pp. 299-304. </pages>
Reference-contexts: The techniques we will discuss here, like those of section II, are quite general purpose: they can accommodate hidden units as well as various architectural embellishments, such as second-order connections [93], <ref> [34] </ref>, [94], [44], weight sharing [23], [35], and in general any of the architectureal modifications made to neural networks to customize them for their problem domain. We will consider two major gradient calculation techniques, and then a few more derived from them.
Reference: [35] <author> Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, </author> <title> "Backpropagation applied to handwritten zip code recognition", </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 541-551, </pages> <year> 1989. </year>
Reference-contexts: The techniques we will discuss here, like those of section II, are quite general purpose: they can accommodate hidden units as well as various architectural embellishments, such as second-order connections [93], [34], [94], [44], weight sharing [23], <ref> [35] </ref>, and in general any of the architectureal modifications made to neural networks to customize them for their problem domain. We will consider two major gradient calculation techniques, and then a few more derived from them.
Reference: [36] <author> M. Kawato, T. Setoyama, and R. Suzuki, </author> <title> "Feedback error learning of movement by multi-layer neural networks", </title> <booktitle> in Proceedings of the International Neural Networks Society First Annual Meeting, </booktitle> <year> 1988, </year> <note> p. 342. </note>
Reference: [37] <author> M. I. Jordan and R. A. Jacobs, </author> <title> "Learning to control an unstable system with forward modeling", </title> <note> In Touretzky [156]. </note>
Reference: [38] <author> K.S. Narendra and K. Parthasarathy, </author> <title> "Identification and control of dynamical systems using neural networks", </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 1, </volume> <pages> pp. 4-27, </pages> <month> Mar. </month> <year> 1990. </year>
Reference: [39] <author> W. Thomas Miller, III, Richard S. Sutton, and Paul J. Werbos, Eds., </author> <title> Neural Networks for Control, </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> 18 DRAFT OF July 20, 1995FOR IEEE TRANSACTIONS ON NEURAL NETWORKS </note>
Reference: [40] <author> H. T. Siegelmann and E. D. Sontag, </author> <title> "Turing computability with neural networks", </title> <journal> Applied Mathematics Letters, </journal> <volume> vol. 4, no. 6, </volume> <pages> pp. 77-80, </pages> <year> 1991. </year>
Reference: [41] <author> J. Kilian and H. T. Siegelmann, </author> <title> "Computability with the classical sigmoid", </title> <booktitle> in Sixth Annual ACM Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, CA, </address> <month> July </month> <year> 1993, </year> <pages> pp. 137-143. </pages>
Reference: [42] <author> H. T. Siegelmann and E. D. Sontag, </author> <title> "Analog computation via neural networks", </title> <booktitle> in The Second Israel Symposium on Theory of Computing and Systems, </booktitle> <address> Natanya, Israel, </address> <month> June </month> <year> 1993, </year> <note> To appear in Theoretical Computer Science. </note>
Reference: [43] <author> Axel Cleeremans, David Servan-Schreiber, and James McClel-land, </author> <title> "Finite state automata and simple recurrent networks", </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 372-381, </pages> <year> 1989. </year>
Reference-contexts: This question can only be answered relative to a particular task. For instance, [119] finds some problems amenable to the history cutoff, but resorts to full fledged backpropagation through time for other tasks. <ref> [43] </ref> describe a regular language token prediction task which is difficult for Elman nets when the transition probabili PEARLMUTTER: GRADIENT CALCULATIONS FOR DYNAMIC RECURRENT NEURAL NETWORKS: A SURVEY 13 Fig. 9.
Reference: [44] <author> Raymond L. Watrous and G. M. Kuhn, </author> <title> "Induction of finite-state automata using second-order recurrent networks", </title> <editor> In Moody et al. </editor> <volume> [161], </volume> <pages> pp. 309-316. </pages>
Reference-contexts: The techniques we will discuss here, like those of section II, are quite general purpose: they can accommodate hidden units as well as various architectural embellishments, such as second-order connections [93], [34], [94], <ref> [44] </ref>, weight sharing [23], [35], and in general any of the architectureal modifications made to neural networks to customize them for their problem domain. We will consider two major gradient calculation techniques, and then a few more derived from them.
Reference: [45] <author> C. L. Giles, C. B. Miller, D. Chen, G. Z. Sun, H. H. Chen, and Y. C. Lee, </author> <title> "Extracting and learning an unknown grammar with recurrent neural networks", </title> <editor> In Moody et al. </editor> <volume> [161], </volume> <pages> pp. 317-324. </pages>
Reference: [46] <author> Michael C. Mozer and Sreerupa Das, </author> <title> "A connectionist symbol manipulator that discovers the structure of context-free languages", </title> <editor> In Hanson et al. </editor> <volume> [154], </volume> <pages> pp. 863-870. </pages>
Reference: [47] <author> Sreerupa Das, C. Lee Giles, and Guo-Zheng Sun, </author> <title> "Using prior knowledge in a NNPDA to learn context-free languages", </title> <editor> In Hanson et al. </editor> <volume> [154], </volume> <pages> pp. 65-72. </pages>
Reference: [48] <author> John F. Kolen, </author> <title> "Fool's gold: Extracting finite state machines from recurrent network dynamics", </title> <note> In NIPS*93 [155], Ftp archive.cis.ohio-state.edu: /pub/neuroprose/kolen.foolsgold.ps.Z. </note>
Reference: [49] <author> Sreerupa Das and Michael C. Mozer, </author> <title> "A unified gradient-descent/clustering architecture for finite state machine induction", </title> <editor> In Cowan et al. </editor> <volume> [155]. </volume>
Reference: [50] <author> Dana Angluin, </author> <title> "Learning regular sets from queries and counterexamples", </title> <journal> Information and Computation, </journal> <volume> vol. 75, </volume> <pages> pp. 87-106, </pages> <year> 1987. </year>
Reference: [51] <author> Kevin J. Lang, </author> <title> "Random dfa's can be approximately learned from sparse uniform examples", </title> <booktitle> in Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, PA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 45-52. </pages>
Reference: [52] <author> Josef Hochreiter, "Untersuchungen zu dynamischen neuronalen netzen", </author> <year> 1991, </year> <institution> Diplomarbeit, Institut fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. </institution>
Reference-contexts: D. Time Constants A major advantage of temporally continuous networks is that one can add additional parameters that control the temporal bahavior in ways known to relate to natural tasks. An example of this is time constants, which were learned in the context of neural networks in [79], [53], <ref> [52] </ref>.
Reference: [53] <author> Michael C. Mozer, </author> <title> "Induction of multiscale temporal structure", </title> <editor> In Moody et al. </editor> <volume> [161], </volume> <pages> pp. 275-282. </pages>
Reference-contexts: D. Time Constants A major advantage of temporally continuous networks is that one can add additional parameters that control the temporal bahavior in ways known to relate to natural tasks. An example of this is time constants, which were learned in the context of neural networks in [79], <ref> [53] </ref>, [52].
Reference: [54] <author> Jurgen H. Schmidhuber, </author> <title> "Learning complex, extended sequences using the principle of history compression", </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 2, </volume> <pages> pp. 234-242, </pages> <year> 1992. </year>
Reference: [55] <author> Jurgen H. Schmidhuber, </author> <title> "Learning unambiguous reduced sequence descriptions", </title> <editor> In Moody et al. </editor> <volume> [161], </volume> <pages> pp. 291-298. </pages>
Reference: [56] <author> Geoffrey E. Hinton, </author> <title> "Learning distributed representations of concepts", </title> <booktitle> in Proceedings of the Eighth Annual Cognitive Science Conference. 1986, </booktitle> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: [57] <author> T.J. Sejnowski, P.K. Kienker, and G. Hinton, </author> <title> "Learning symmetry groups with hidden units: Beyond the perceptron", </title> <journal> Physica D, </journal> <volume> vol. 22, </volume> <pages> pp. 260-275, </pages> <year> 1986. </year>
Reference: [58] <author> David H. Ackley, Geoffrey E. Hinton, and Terrence J. Se-jnowski, </author> <title> "A learning algorithm for Boltzmann Machines", </title> <journal> Cognitive Science, </journal> <volume> vol. 9, </volume> <pages> pp. 147-169, </pages> <year> 1985. </year>
Reference-contexts: D. Deterministic Boltzmann Machines The Mean Field form of the stochastic Boltzmann Machine learning rule, or MFT Boltzmann Machines, [91] have been shown to descend an error functional [74]. Stochastic Boltzmann Machines themselves <ref> [58] </ref> are beyond our scope here; instead, we give only the probabilistic interpretation of MFT Boltzmann Machines, without derivation. <p> On the other hand, we can turn the logic of section V around. Consider a difficult constraint satisfaction task of the sort that neural networks are sometimes applied to, such as the traveling salesman problem [149]. Two competing techniques for such problems are simulated annealing [150], <ref> [58] </ref> and mean field theory [92].
Reference: [59] <author> David E. Rumelhart, Geoffrey E. Hinton, and R. J. Williams, </author> <title> "Learning internal representations by error propagation", </title> <editor> In Rumelhart et al. </editor> <volume> [151]. </volume>
Reference-contexts: There is as yet no theoretical explanation for this phenomenon, and it has not been replicated with larger networks. One algorithm that is capable of learning fixedpoints, but does not require the network being trained to settle to a fixedpoint in order to operate, is backpropagation through time <ref> [59] </ref>. This has been used by Nowlan to train a constraint satisfaction network for the eight queens problem, where shaping was used to gradually train a discrete time 2 Technically, these algorithms only require that a fixedpoint be reached, not that it be stable. <p> C. Recurrent Backpropagation It was shown independently by Pineda [72] and Alemeida [73] that the error backpropagation algorithm [61], <ref> [59] </ref>, [60] is a special case of a more general error gradient computation procedure. <p> Below, we derive a technique for computing @E (y)=@w ij efficiently, thus al lowing us to do gradient descent in the weights so as to minimize E. Backpropagation through time has been used to train discrete time networks to perform a variety of tasks <ref> [59] </ref>, [89]. Here, we will derive the continuous time version of backpropagation through time, as in [96], and use it in two toy domains.
Reference: [60] <author> Paul J. Werbos, </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences, </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference-contexts: C. Recurrent Backpropagation It was shown independently by Pineda [72] and Alemeida [73] that the error backpropagation algorithm [61], [59], <ref> [60] </ref> is a special case of a more general error gradient computation procedure. <p> The backpropagation equations are x i = j y i = oe (x i ) + I i (6) X w ij z j + e i (7) @w ij where z i is the ordered partial derivative of E with respect to y i as defined in <ref> [60] </ref>, E is an error measure over y (t 1 ), and e i = @E=@y i (t 1 ) is the simple derivative of E with respect to the final state of a unit.
Reference: [61] <author> David B. Parker, "Learning-logic", </author> <type> Tech. Rep. </type> <institution> TR-47, MIT Center for Research in Computational Economics and Management Science, </institution> <year> 1985. </year>
Reference-contexts: C. Recurrent Backpropagation It was shown independently by Pineda [72] and Alemeida [73] that the error backpropagation algorithm <ref> [61] </ref>, [59], [60] is a special case of a more general error gradient computation procedure.
Reference: [62] <author> R. Howard, </author> <title> Dynamic Programming and Karkhov Processes, </title> <publisher> MIT Press, </publisher> <year> 1960. </year>
Reference-contexts: We will consider two major gradient calculation techniques, and then a few more derived from them. The first is the obvious extension of backpropagation through time (BPTT) to continuous time [95], [96], <ref> [62] </ref>. A. Backpropagation Through Time The fixedpoint learning procedures discussed above are unable to learn non-fixedpoint attractors, or to produce desired temporal behavior over a bounded interval, or even to learn to reach their fixedpoints quickly. Here, we turn to a learning procedure suitable for such non-fixedpoint situations.
Reference: [63] <author> D.S. Touretzky and D.A. Pomerleau, </author> <title> "What's hidden in the hidden layers?", </title> <journal> BYTE, </journal> <pages> pp. 227-233, </pages> <month> Aug. </month> <year> 1989. </year>
Reference: [64] <author> B. Widrow and M. Hoff, </author> <title> "Adaptive switching circuits", in Western Electronic Show and Convention, </title> <booktitle> Convention Record. 1960, </booktitle> <volume> vol. 4, </volume> <pages> pp. 96-104, </pages> <booktitle> Institute of Radio Engineers (now IEEE). </booktitle>
Reference: [65] <author> Yann LeCun, Ido Kanter, and Sara A. Solla, </author> <title> "Second order properties of error surfaces: Learning time and generalization", </title> <editor> In Lippmann et al. </editor> <volume> [159], </volume> <pages> pp. 918-924. </pages>
Reference-contexts: The latter symmetric squashing function is usually preferable, as it leads to a better conditioned Hessian, which speeds gradient descent <ref> [65] </ref>.
Reference: [66] <author> B. Baird, </author> <title> "A learning rule for CAM storage of continuous periodic sequences", </title> <booktitle> In IJCNN90 II [162], </booktitle> <pages> pp. 493-498. </pages>
Reference: [67] <author> Bill Baird and Frank Eeckman, </author> <title> "CAM storage of analog patterns and continuous sequences with 3n 2 weights", </title> <editor> In Lipp-mann et al. </editor> <volume> [159], </volume> <pages> pp. 91-97. </pages>
Reference: [68] <author> C. A. Skarda and W. J. Freeman, </author> <title> "How brains make chaos in order to make sense of the world", </title> <journal> Brain and Behavioral Science, </journal> <volume> vol. 10, </volume> <month> Nov. </month> <year> 1987. </year>
Reference: [69] <author> W. J. Freeman, </author> <title> "Simulation of chaotic EEG patterns with a dynamic model of the olfactory system", </title> <journal> Biological Cybernetics, </journal> <volume> vol. 56, </volume> <pages> pp. 139, </pages> <year> 1987. </year>
Reference: [70] <author> J. P. Crutchfield and B. S. McNamara, </author> <title> "Equations of motion from a data series", </title> <journal> Complex Systems, </journal> <volume> vol. 1, </volume> <pages> pp. 417-452, </pages> <year> 1987. </year>
Reference: [71] <author> Alan Lapedes and Robert Farber, </author> <title> "Nonlinear signal processing using neural networks: Prediction and system modelling", </title> <type> Tech. Rep. </type> <institution> LA-UR-87-2662, Theoretical Division, Los Alamos National Laboratory, </institution> <address> Los Alamos, NM, </address> <year> 1987. </year>
Reference: [72] <author> Fernando Pineda, </author> <title> "Generalization of back-propagation to recurrent neural networks", </title> <journal> Physical Review Letters, </journal> <volume> vol. 19, no. 59, </volume> <pages> pp. 2229-2232, </pages> <year> 1987. </year>
Reference-contexts: C. Recurrent Backpropagation It was shown independently by Pineda <ref> [72] </ref> and Alemeida [73] that the error backpropagation algorithm [61], [59], [60] is a special case of a more general error gradient computation procedure. <p> However, redoing the training with weight symmetry enforced, however, caused the network to learn not only the training data but also to do well on these untrained completions. [9] successfully applied the <ref> [72] </ref>, [73] recurrent backprop 3 A unit with external input could be pushed outside the [0,1] bounds of the range of the oe () used. agation learning procedure to learning weights for a relaxation procedure for dense stereo disparity problems with transparent surfaces.
Reference: [73] <author> L. B. Almeida, </author> <title> "A learning rule for asynchronous perceptrons with feedback in a combinatorial environment", </title> <booktitle> In Caudill and Butler [157], </booktitle> <pages> pp. 609-618. </pages>
Reference-contexts: C. Recurrent Backpropagation It was shown independently by Pineda [72] and Alemeida <ref> [73] </ref> that the error backpropagation algorithm [61], [59], [60] is a special case of a more general error gradient computation procedure. <p> However, redoing the training with weight symmetry enforced, however, caused the network to learn not only the training data but also to do well on these untrained completions. [9] successfully applied the [72], <ref> [73] </ref> recurrent backprop 3 A unit with external input could be pushed outside the [0,1] bounds of the range of the oe () used. agation learning procedure to learning weights for a relaxation procedure for dense stereo disparity problems with transparent surfaces.
Reference: [74] <author> Geoffrey E. Hinton, </author> <title> "Deterministic Boltzmann learning performs steepest descent in weight-space", </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 143-150, </pages> <year> 1989. </year>
Reference-contexts: D. Deterministic Boltzmann Machines The Mean Field form of the stochastic Boltzmann Machine learning rule, or MFT Boltzmann Machines, [91] have been shown to descend an error functional <ref> [74] </ref>. Stochastic Boltzmann Machines themselves [58] are beyond our scope here; instead, we give only the probabilistic interpretation of MFT Boltzmann Machines, without derivation.
Reference: [75] <author> Pierre Baldi and Fernando Pineda, </author> <title> "Contrastive learning and neural oscillations", </title> <journal> Neural Computation, </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 526-545, </pages> <year> 1991. </year>
Reference: [76] <author> Barak A. Pearlmutter, </author> <title> "Dynamic recurrent neural networks", </title> <type> Tech. Rep. </type> <institution> CMU-CS-90-196, Carnegie Mellon University School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: One is direct, using the calculus of variations [98]. The other is to take the continuous time equations, approximate them by difference equations, precisely calculate the adjoint equations for this discrete time system, and then approximate back to get the continuous time adjoint equations, as in <ref> [76] </ref>. An advantage of the latter approach is that, when simulating on a digital computer, one actually simulates the difference equations. The derivation ensures that the simulated adjoint difference equations are the precise adjoints to the simulated forward difference equations, so the computed derivatives contain no approximation errors. B. <p> online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], [102], [103]; for reviews see also [81], <ref> [76] </ref>, [104]. It is called by various researchers forward propagation, forward perturbation, or real time recurrent learning, RTRL. <p> In the sections on time constants and delays, we have carried out the derivative derivations for BPTT. All the other techniques also remain applicable to this case, with straightforward derivations. The analogous derivations for RTRL are carried out in <ref> [76] </ref>. However, we will not here simulate networks with modifiable time delays. An interesting class of architectures would have the state of one unit modulate the time delay along some arbitrary link in the network or the time constant of some other unit. <p> make sense. 14 DRAFT OF July 20, 1995FOR IEEE TRANSACTIONS ON NEURAL NETWORKS The error is as usual, with the caveat that errors are to be measured before output units are forced, not after. [123] report that their teacher forcing technique radically reduced training time for their recurrent networks, although <ref> [76] </ref> reports difficulties when teacher forcing was used networks with a larger number of hidden units. Williams and Zipser's application of teacher forcing to their networks is dependent on discrete time steps, so applying teacher forcing to temporally continuous networks requires a different approach.
Reference: [77] <author> Marco Gori, Yoshua Bengio, and Renato de Mori, </author> <title> "BPS: A learning algorithm for capturing the dynamic nature of speech", </title> <booktitle> In IJCNN89 [158], </booktitle> <pages> pp. 417-423. </pages>
Reference-contexts: There are continuous-time feed-forward learning algorithms that are as efficient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections <ref> [77] </ref>, [78], [79], [80], [25] or for a survey, [81]. Later, we will describe a number of training procedures that, for a price in space or time, do not rely on such restrictions and can be applied to training networks to exhibit desired limit cycles, or particular detailed temporal behavior. <p> This makes the technique inapplicable for online learning, in which each pattern is seen only once. C. Feedforward Networks with State It is noteworthy that that the same basic mathematical technique of forward propagation can be applied to networks with a restricted architecture, feedforward networks whose units have state <ref> [77] </ref>, [78], [80]. This is the same as requiring the w ij matrix to be triangular, but allowing non-zero diagonal terms. If we let the fl quantities be ordered derivatives, as in standard backpropagation, then this simplified architecture reduces the computational burden substantially.
Reference: [78] <author> Gary Kuhn, </author> <title> "A first look at phonetic discrimination using connectionist models with recurrent links", </title> <note> SCIMP working paper 82018, </note> <institution> Institute for Defense Analysis, Princeton, </institution> <address> New Jersey, </address> <month> Apr. </month> <year> 1987. </year>
Reference-contexts: There are continuous-time feed-forward learning algorithms that are as efficient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections [77], <ref> [78] </ref>, [79], [80], [25] or for a survey, [81]. Later, we will describe a number of training procedures that, for a price in space or time, do not rely on such restrictions and can be applied to training networks to exhibit desired limit cycles, or particular detailed temporal behavior. C. <p> C. Feedforward Networks with State It is noteworthy that that the same basic mathematical technique of forward propagation can be applied to networks with a restricted architecture, feedforward networks whose units have state [77], <ref> [78] </ref>, [80]. This is the same as requiring the w ij matrix to be triangular, but allowing non-zero diagonal terms. If we let the fl quantities be ordered derivatives, as in standard backpropagation, then this simplified architecture reduces the computational burden substantially.
Reference: [79] <author> Michael C. Mozer, </author> <title> "A focused backpropagation algorithm for temporal pattern recognition", </title> <journal> Complex Systems, </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 349-381, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: There are continuous-time feed-forward learning algorithms that are as efficient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections [77], [78], <ref> [79] </ref>, [80], [25] or for a survey, [81]. Later, we will describe a number of training procedures that, for a price in space or time, do not rely on such restrictions and can be applied to training networks to exhibit desired limit cycles, or particular detailed temporal behavior. C. <p> D. Time Constants A major advantage of temporally continuous networks is that one can add additional parameters that control the temporal bahavior in ways known to relate to natural tasks. An example of this is time constants, which were learned in the context of neural networks in <ref> [79] </ref>, [53], [52].
Reference: [80] <author> Tadasu Uchiyama, Katsunori Shimohara, and Yukio Tokunaga, </author> <title> "A modified leaky integrator network for temporal pattern recognition", </title> <booktitle> In IJCNN89 [158], </booktitle> <pages> pp. 469-475. </pages>
Reference-contexts: There are continuous-time feed-forward learning algorithms that are as efficient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections [77], [78], [79], <ref> [80] </ref>, [25] or for a survey, [81]. Later, we will describe a number of training procedures that, for a price in space or time, do not rely on such restrictions and can be applied to training networks to exhibit desired limit cycles, or particular detailed temporal behavior. C. Continuous vs. <p> C. Feedforward Networks with State It is noteworthy that that the same basic mathematical technique of forward propagation can be applied to networks with a restricted architecture, feedforward networks whose units have state [77], [78], <ref> [80] </ref>. This is the same as requiring the w ij matrix to be triangular, but allowing non-zero diagonal terms. If we let the fl quantities be ordered derivatives, as in standard backpropagation, then this simplified architecture reduces the computational burden substantially.
Reference: [81] <author> Barak A. Pearlmutter, </author> <title> "Two new learning procedures for recurrent networks", </title> <journal> Neural Network Review, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 99-101, </pages> <year> 1990. </year>
Reference-contexts: learning algorithms that are as efficient in both time and space as algorithms for pure feedforward networks, but are applicable only when w is upper-triangular but not necessarily zero-diagonal, in other words, when the network is feedforward except for recurrent self-connections [77], [78], [79], [80], [25] or for a survey, <ref> [81] </ref>. Later, we will describe a number of training procedures that, for a price in space or time, do not rely on such restrictions and can be applied to training networks to exhibit desired limit cycles, or particular detailed temporal behavior. C. Continuous vs. <p> An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], [102], [103]; for reviews see also <ref> [81] </ref>, [76], [104]. It is called by various researchers forward propagation, forward perturbation, or real time recurrent learning, RTRL.
Reference: [82] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Verrerling, </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: However, regarding the discrete time network running on the computer as a simulation of a continuous time network has a number of advantages. First, more sophisticated and faster simulation techniques than simple first order difference equations can be used <ref> [82] </ref>.
Reference: [83] <author> M. A. Cohen and Steven Grossberg, </author> <title> "Stability of global pattern formation and parallel memory storage by competitive neural networks", </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> vol. 13, </volume> <pages> pp. 815-826, </pages> <year> 1983. </year>
Reference-contexts: linear conditions on the weights, such as zero-diagonal symmetry (w ij = w ji , w ii = 0) guaran tee that the Lyopunov function L = i;j X (y i log y i + (1 y i ) log (1 y i )) decreases until a fixedpoint is reached <ref> [83] </ref>.
Reference: [84] <author> Geoffrey E. Hinton and Terrence J. Sejnowski, </author> <title> "Optimal perceptual inference", </title> <booktitle> in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, </booktitle> <address> Washington DC, </address> <month> June </month> <year> 1983, </year> <journal> IEEE Computer Society, </journal> <pages> pp. 448-453. </pages>
Reference-contexts: This weight symmetry condition arises naturally if weights are considered to be Bayesian constraints, as in Boltz mann Machines <ref> [84] </ref>. * A unique fixedpoint is reached regardless of initial con ditions if P ij &lt; max (oe 0 ) where max (oe 0 ) is the maximal value of oe 0 (x) for any x [85], but in practice much weaker bounds on the weights seem to suffice, as indicated
Reference: [85] <author> Amir F. Atiya, </author> <title> "Learning on a general network", </title> <booktitle> In Anderson [163], </booktitle> <pages> pp. 22-30. </pages>
Reference-contexts: naturally if weights are considered to be Bayesian constraints, as in Boltz mann Machines [84]. * A unique fixedpoint is reached regardless of initial con ditions if P ij &lt; max (oe 0 ) where max (oe 0 ) is the maximal value of oe 0 (x) for any x <ref> [85] </ref>, but in practice much weaker bounds on the weights seem to suffice, as indicated by empirical studies of the dynamics of networks with random weights [86]. * Other empirical studies indicate that applying fixed-point learning algorithms stabilizes networks, causing them to exhibit asymptotic fixedpoint behavior [87], [88].
Reference: [86] <author> Steve Renals and Richard Rohwer, </author> <title> "A study of network dynamics", </title> <journal> Journal of Statistical Physics, </journal> <volume> vol. 58, </volume> <pages> pp. 825-848, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: if P ij &lt; max (oe 0 ) where max (oe 0 ) is the maximal value of oe 0 (x) for any x [85], but in practice much weaker bounds on the weights seem to suffice, as indicated by empirical studies of the dynamics of networks with random weights <ref> [86] </ref>. * Other empirical studies indicate that applying fixed-point learning algorithms stabilizes networks, causing them to exhibit asymptotic fixedpoint behavior [87], [88]. There is as yet no theoretical explanation for this phenomenon, and it has not been replicated with larger networks.
Reference: [87] <author> Robert B. Allen and Joshua Alspector, </author> <title> "Learning of stable states in stochastic asymmetric networks", </title> <type> Tech. Rep. </type> <institution> TM-ARH-015240, Bell Communications Research, Morristown, NJ, </institution> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: (x) for any x [85], but in practice much weaker bounds on the weights seem to suffice, as indicated by empirical studies of the dynamics of networks with random weights [86]. * Other empirical studies indicate that applying fixed-point learning algorithms stabilizes networks, causing them to exhibit asymptotic fixedpoint behavior <ref> [87] </ref>, [88]. There is as yet no theoretical explanation for this phenomenon, and it has not been replicated with larger networks.
Reference: [88] <author> Conrad C. Galland and Geoffrey E. Hinton, </author> <title> "Deterministic Boltzmann learning in networks with asymmetric connectivity", </title> <type> Tech. Rep. </type> <institution> CRG-TR-89-6, University of Toronto Department of Computer Science, </institution> <year> 1989. </year> <title> PEARLMUTTER: GRADIENT CALCULATIONS FOR DYNAMIC RECURRENT NEURAL NETWORKS: A SURVEY 19 </title>
Reference-contexts: for any x [85], but in practice much weaker bounds on the weights seem to suffice, as indicated by empirical studies of the dynamics of networks with random weights [86]. * Other empirical studies indicate that applying fixed-point learning algorithms stabilizes networks, causing them to exhibit asymptotic fixedpoint behavior [87], <ref> [88] </ref>. There is as yet no theoretical explanation for this phenomenon, and it has not been replicated with larger networks. One algorithm that is capable of learning fixedpoints, but does not require the network being trained to settle to a fixedpoint in order to operate, is backpropagation through time [59]. <p> Although weight symmetry is assumed in the definition of energy which is used in the definition of probability, and is thus fundamental to these mathematics, it seems that in practice weight asymmetry can be tolerated in large networks <ref> [88] </ref>. This makes MFT Boltzmann Machines the most biologically plausible of the various learning procedures we discuss, but it is difficult to see how it would be possible to extend them to learning more complex phenomena, like limit cycles or paths through state space.
Reference: [89] <author> Steven J. Nowlan, </author> <title> "Gain variation in recurrent error propagation networks", </title> <journal> Complex Systems, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 305-320, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: However, it is unlikely (with probability zero) that a network will converge to an unstable fixedpoint, and in practice the posibility of convergence to unstable fixedpoints can be safely ignored. network without hidden units to exhibit the desired at-tractors <ref> [89] </ref>. However, the other fixedpoint algorithms we will consider take advantage of the special properties of a fixedpoint to simplify the learning algorithm. B. Problems with Fixedpoints Even when it can be guaranteed that a network settles to a fixedpoint, fixedpoint learning algorithms can still run into trouble. <p> Below, we derive a technique for computing @E (y)=@w ij efficiently, thus al lowing us to do gradient descent in the weights so as to minimize E. Backpropagation through time has been used to train discrete time networks to perform a variety of tasks [59], <ref> [89] </ref>. Here, we will derive the continuous time version of backpropagation through time, as in [96], and use it in two toy domains.
Reference: [90] <author> Mary B. Ottaway, Patrice Y. Simard, and Dana H. Ballard, </author> <title> "Fixed point analysis for recurrent neural networks", </title> <booktitle> in Advances in Neural Information Processing Systems I, </booktitle> <editor> David S. Touretzky, Ed. </editor> <booktitle> 1989, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: And if (6) is satisfied, and assuming we can find z i that satisfy (7), then (8) will give us the derivatives we seek, even in the presence of recurrent connections. (For a simple task, <ref> [90] </ref> reports that reaching the precise fixed-point is not crucial to learning.) One way to compute a fixedpoint for (6) is to relax to a solution.
Reference: [91] <author> C. Peterson and J. R. Anderson, </author> <title> "A mean field theory learning algorithm for neural nets", </title> <journal> Complex Systems, </journal> <volume> vol. 1, </volume> <year> 1987. </year>
Reference-contexts: D. Deterministic Boltzmann Machines The Mean Field form of the stochastic Boltzmann Machine learning rule, or MFT Boltzmann Machines, <ref> [91] </ref> have been shown to descend an error functional [74]. Stochastic Boltzmann Machines themselves [58] are beyond our scope here; instead, we give only the probabilistic interpretation of MFT Boltzmann Machines, without derivation. <p> This learning rule (14) is a version of Hebb's rule in which the sign of synaptic modification is alternated, positive during the "waking" phase and negative during the "hallucinating" phase. Even before the learning rule was rigorously justified, deterministic Boltzmann Machines were applied to a number of tasks [92], <ref> [91] </ref>. Although weight symmetry is assumed in the definition of energy which is used in the definition of probability, and is thus fundamental to these mathematics, it seems that in practice weight asymmetry can be tolerated in large networks [88].
Reference: [92] <author> C. Peterson and James R. Anderson, </author> <title> "A mean field theory learning algorithm for neural networks", </title> <type> Tech. Rep. </type> <institution> EI-259-87, MCC, </institution> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: This learning rule (14) is a version of Hebb's rule in which the sign of synaptic modification is alternated, positive during the "waking" phase and negative during the "hallucinating" phase. Even before the learning rule was rigorously justified, deterministic Boltzmann Machines were applied to a number of tasks <ref> [92] </ref>, [91]. Although weight symmetry is assumed in the definition of energy which is used in the definition of probability, and is thus fundamental to these mathematics, it seems that in practice weight asymmetry can be tolerated in large networks [88]. <p> Consider a difficult constraint satisfaction task of the sort that neural networks are sometimes applied to, such as the traveling salesman problem [149]. Two competing techniques for such problems are simulated annealing [150], [58] and mean field theory <ref> [92] </ref>. By providing a network with a noise source which can be modulated (by second order connections, say) we could see if the learning algorithm constructs a network that makes use of the noise to generate networks that do simulated annealing, or if pure gradient descent techniques are evolved.
Reference: [93] <author> Geoffrey E. Hinton and Kevin J. Lang, </author> <title> "Shape recognition and illusory conjunctions", </title> <booktitle> in the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <address> Los Angeles, </address> <month> Aug. </month> <journal> 1985, </journal> <volume> vol. 1, </volume> <pages> pp. 252-259, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The techniques we will discuss here, like those of section II, are quite general purpose: they can accommodate hidden units as well as various architectural embellishments, such as second-order connections <ref> [93] </ref>, [34], [94], [44], weight sharing [23], [35], and in general any of the architectureal modifications made to neural networks to customize them for their problem domain. We will consider two major gradient calculation techniques, and then a few more derived from them.
Reference: [94] <author> T. J. Sejnowski, </author> <title> "Higher-order Boltzmann machines", </title> <booktitle> In Denker [160], </booktitle> <pages> pp. 398-403. </pages>
Reference-contexts: The techniques we will discuss here, like those of section II, are quite general purpose: they can accommodate hidden units as well as various architectural embellishments, such as second-order connections [93], [34], <ref> [94] </ref>, [44], weight sharing [23], [35], and in general any of the architectureal modifications made to neural networks to customize them for their problem domain. We will consider two major gradient calculation techniques, and then a few more derived from them.
Reference: [95] <author> Paul J. Werbos, </author> <title> "Backpropagation through time: what it does and how to do it", </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 78, </volume> <pages> pp. 1550-1560, </pages> <year> 1990. </year>
Reference-contexts: We will consider two major gradient calculation techniques, and then a few more derived from them. The first is the obvious extension of backpropagation through time (BPTT) to continuous time <ref> [95] </ref>, [96], [62]. A. Backpropagation Through Time The fixedpoint learning procedures discussed above are unable to learn non-fixedpoint attractors, or to produce desired temporal behavior over a bounded interval, or even to learn to reach their fixedpoints quickly. Here, we turn to a learning procedure suitable for such non-fixedpoint situations.
Reference: [96] <author> Barak Pearlmutter, </author> <title> "Learning state space trajectories in recurrent neural networks", </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 263-269, </pages> <year> 1989. </year>
Reference-contexts: We will consider two major gradient calculation techniques, and then a few more derived from them. The first is the obvious extension of backpropagation through time (BPTT) to continuous time [95], <ref> [96] </ref>, [62]. A. Backpropagation Through Time The fixedpoint learning procedures discussed above are unable to learn non-fixedpoint attractors, or to produce desired temporal behavior over a bounded interval, or even to learn to reach their fixedpoints quickly. Here, we turn to a learning procedure suitable for such non-fixedpoint situations. <p> Backpropagation through time has been used to train discrete time networks to perform a variety of tasks [59], [89]. Here, we will derive the continuous time version of backpropagation through time, as in <ref> [96] </ref>, and use it in two toy domains.
Reference: [97] <author> Paul J. Werbos, </author> <title> "Generalization of backpropagation with application to a recurrent gas market model", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 339-356, </pages> <year> 1988. </year>
Reference-contexts: Intuitively, e i (t) measures how much a small change to y i at time t affects E if every thing else is left unchanged. As usual in backpropagation, let us define ~z i (t) = @ ~y i (t) where the @ + denotes the ordered derivative of <ref> [97] </ref>, with variables ordered here by time and not unit index. Intuitively, ~z i (t) measures how much a small change to ~y i at time t affects E when this change is propagated forward through time and influences the remainder of the trajectory, as in figure 7.
Reference: [98] <author> Arthur E. Bryson, Jr., </author> <title> "A steepest ascent method for solving optimum programming problems", </title> <journal> Journal of Applied Mechanics, </journal> <volume> vol. 29, no. 2, </volume> <pages> pp. 247, </pages> <year> 1962. </year>
Reference-contexts: Since we wish to know the effect of making this infinitesimal change to w ij throughout time, we integrate over the entire interval, yielding @E = t 0 One can also derive (26), (27) and (37) using the calculus of variations and Lagrange multipliers, as in optimal control theory <ref> [98] </ref>, [99]. In fact, the idea of using gradient descent to optimize complex systems was explored by control theorists in the late 1950s. <p> As t 1 ! 1, (26) and (27) reduce to the recurrent backpropagation equations (9) and (8), so in this sense backpropagation through time is a generalization of recurrent backpropagation. There are two ways to go about finding such derivations. One is direct, using the calculus of variations <ref> [98] </ref>. The other is to take the continuous time equations, approximate them by difference equations, precisely calculate the adjoint equations for this discrete time system, and then approximate back to get the continuous time adjoint equations, as in [76].
Reference: [99] <author> Stuart E. Dreyfus, </author> <title> Dynamic Programming and the Calculus of Variations, </title> <booktitle> vol. 21 of Mathematics in science and engineering, </booktitle> <publisher> Academic Press, </publisher> <year> 1965. </year>
Reference-contexts: Since we wish to know the effect of making this infinitesimal change to w ij throughout time, we integrate over the entire interval, yielding @E = t 0 One can also derive (26), (27) and (37) using the calculus of variations and Lagrange multipliers, as in optimal control theory [98], <ref> [99] </ref>. In fact, the idea of using gradient descent to optimize complex systems was explored by control theorists in the late 1950s.
Reference: [100] <author> Paul J. Werbos, </author> <title> "Applications of advances in nonlinear sensitivity analysis", in System Modeling and Optimization: </title> <booktitle> Proceedings of the 10th IFIP Conference, </booktitle> <editor> R. F. Drenick and F. Kozin, Eds., </editor> <address> (New York, Aug 31-Sep 4, </address> <year> 1981), 1982, </year> <booktitle> number 38 in Lecture Notes in Control and Information Sciences, </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: B. Real Time Recurrent Learning An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times <ref> [100] </ref>, [101], [102], [103]; for reviews see also [81], [76], [104]. It is called by various researchers forward propagation, forward perturbation, or real time recurrent learning, RTRL.
Reference: [101] <author> A. J. Robinson and F. Fallside, </author> <title> "Static and dynamic error propagation networks with application to speech coding", </title> <booktitle> In Anderson [163], </booktitle> <pages> pp. 632-641. </pages>
Reference-contexts: B. Real Time Recurrent Learning An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], <ref> [101] </ref>, [102], [103]; for reviews see also [81], [76], [104]. It is called by various researchers forward propagation, forward perturbation, or real time recurrent learning, RTRL.
Reference: [102] <author> Michael Gherrity, </author> <title> "A learning algorithm for analog, fully recurrent neural networks", </title> <booktitle> In IJCNN89 [158], </booktitle> <pages> pp. 643-644. </pages>
Reference-contexts: B. Real Time Recurrent Learning An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], <ref> [102] </ref>, [103]; for reviews see also [81], [76], [104]. It is called by various researchers forward propagation, forward perturbation, or real time recurrent learning, RTRL.
Reference: [103] <author> Ronald J. Williams and David Zipser, </author> <title> "A learning algorithm for continually running fully recurrent neural networks", </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 270-280, </pages> <year> 1989. </year>
Reference-contexts: Real Time Recurrent Learning An online, exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], [102], <ref> [103] </ref>; for reviews see also [81], [76], [104]. It is called by various researchers forward propagation, forward perturbation, or real time recurrent learning, RTRL.
Reference: [104] <author> K. S. Narendra and K. Parthasarathy, </author> <title> "Gradient methods for the optimization of dynamical systems containing neural networks", </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 2, no. 2, </volume> <pages> pp. 252-262, </pages> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: exact, and stable, but computationally expensive, procedure for determining the derivatives of functions of the states of a dynamic system with respect to that system's internal parameters has been discovered and applied to recurrent neural networks a number of times [100], [101], [102], [103]; for reviews see also [81], [76], <ref> [104] </ref>. It is called by various researchers forward propagation, forward perturbation, or real time recurrent learning, RTRL. Like BPTT, the technique was known and applied to other sorts of systems since the 1950s; for a hook into this literature see [105], [106] or the closely related Extended Kalman Filter [107].
Reference: [105] <author> D. H. Jacobson, </author> <title> "New second order and first order algorithm for determining optimal control: A differential dynamic programming approach", </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> vol. 2, </volume> <year> 1968. </year>
Reference-contexts: It is called by various researchers forward propagation, forward perturbation, or real time recurrent learning, RTRL. Like BPTT, the technique was known and applied to other sorts of systems since the 1950s; for a hook into this literature see <ref> [105] </ref>, [106] or the closely related Extended Kalman Filter [107].
Reference: [106] <author> R. E. Bellman, </author> <title> Methods of Nonlinear Analysis: Volume II, </title> <publisher> Academic Press, </publisher> <year> 1973. </year>
Reference-contexts: It is called by various researchers forward propagation, forward perturbation, or real time recurrent learning, RTRL. Like BPTT, the technique was known and applied to other sorts of systems since the 1950s; for a hook into this literature see [105], <ref> [106] </ref> or the closely related Extended Kalman Filter [107].
Reference: [107] <editor> A. Gelb et al., Eds., </editor> <title> Applied Optimal Estimation, </title> <publisher> MIT Press, </publisher> <year> 1974. </year>
Reference-contexts: It is called by various researchers forward propagation, forward perturbation, or real time recurrent learning, RTRL. Like BPTT, the technique was known and applied to other sorts of systems since the 1950s; for a hook into this literature see [105], [106] or the closely related Extended Kalman Filter <ref> [107] </ref>. <p> F. Teacher Forcing, RTRL, and the Kalman Filter [125], [126] have pointed out that RTRL is related to a version of the [127] filter, in the extension that allows it to apply to nonlinear systems, namely the extended Kalman filter (EKF) [128], <ref> [107] </ref>, [129]. The EKF has time and space complexity of the same order as those of RTRL.
Reference: [108] <author> David Zipser, </author> <title> "Subgrouping reduces complexity and speeds up learning in recurrent networks", </title> <booktitle> In Touretzky [156], </booktitle> <pages> pp. 638-641. </pages>
Reference-contexts: This approach, in particular ignoring the coupling terms which relate the states of units in one module to weights in another, has been explored by Zipser <ref> [108] </ref>. Another is to use BPTT with a history cutoff of k units of time, termed BPTT (k) by Williams and Peng [109], and make a small weight change each timestep.
Reference: [109] <author> Ronald J. Williams and Jing Peng, </author> <title> "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", </title> <journal> Neural Computation, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 490-501, </pages> <year> 1990. </year>
Reference-contexts: This approach, in particular ignoring the coupling terms which relate the states of units in one module to weights in another, has been explored by Zipser [108]. Another is to use BPTT with a history cutoff of k units of time, termed BPTT (k) by Williams and Peng <ref> [109] </ref>, and make a small weight change each timestep. This obviates the need for epochs, resulting in a purely online technique, and is probably the best technique for most practical problems.
Reference: [110] <author> R. J. Williams and D. Zipser, </author> <title> "Gradient-based learning algorithms for recurrent networks and their computational complexity", </title> <note> In Chauvin and Rumelhart [18], Also published as [164]. </note>
Reference-contexts: This requires O (s 1 n 2 m + nm) time per step, on average, and O (nm + sm) space. Choosing s = n makes this O (nm) time and O (nm) space, which dominates RTRL. This technique has been discovered independently a number of times <ref> [110] </ref>, [111]. Finally, one can note that, although the forward equations for y are nonlinear, and therefore require numeric integration, the backwards equations for z in BPTT are linear.
Reference: [111] <author> Jurgen H. Schmidhuber, </author> <title> "A fixed size storage O(n 3 ) time complexity learning algorithm for fully recurrent continually running networks", </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 2, </volume> <pages> pp. 243-248, </pages> <year> 1992. </year>
Reference-contexts: This requires O (s 1 n 2 m + nm) time per step, on average, and O (nm + sm) space. Choosing s = n makes this O (nm) time and O (nm) space, which dominates RTRL. This technique has been discovered independently a number of times [110], <ref> [111] </ref>. Finally, one can note that, although the forward equations for y are nonlinear, and therefore require numeric integration, the backwards equations for z in BPTT are linear.
Reference: [112] <author> Guo-Zheng Sun, Hsing-Hen Chen, and Yee-Chun Lee, </author> <title> "Green's function method for fast on-line learning algorithm of recurrent neural networks", </title> <editor> In Moody et al. </editor> <volume> [161], </volume> <pages> pp. 333-340. </pages>
Reference-contexts: Since the dE=dw terms are linear integrations of the z, this means that they are linear functions of the external inputs, namely the e i terms. As shown by Sun et al. <ref> [112] </ref>, this allows one, during the forward pass, to compute a matrix relating the external error signal to the elements of r w , allowing a fully online algorithm with O (nm) time and space complexity. D.
Reference: [113] <author> U. Bodenhausen, </author> <title> "Learning internal representations of pattern sequences in a neural network with adaptive time-delays", </title> <booktitle> In IJCNN90 II [162]. </booktitle>
Reference-contexts: Feedforward networks with immutable time delays (TDNNs) have been applied with great success in the same domain by Lang et al. [22]. A variant of TDNNs which learn the time delays was explored by Bodenhausen et al. <ref> [113] </ref>. The synapses in their networks, rather than having point taps, have gaussian envelopes whose widths and centers were both learned.
Reference: [114] <author> D. W. Tank and J. J. </author> <title> Hopfield, "Neural computation by time compression", </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> vol. 84, </volume> <pages> pp. 1896-1900, </pages> <year> 1987. </year>
Reference-contexts: The synapses in their networks, rather than having point taps, have gaussian envelopes whose widths and centers were both learned. Similar synaptic architectures using alpha function envelopes (which obviate the need for a history buffer) whose parameters were learned were proposed and used in systems without hidden units <ref> [114] </ref>, [29]. A continuous time feed-forward network with learned time delays was successfully applied to a difficult time-series prediction task by Day and Davenport [25]. In the sections on time constants and delays, we have carried out the derivative derivations for BPTT.
Reference: [115] <author> R. L. Watrous, </author> <title> Speech Recognition Using Connectionist Networks, </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: In the presence of time delays, it is reasonable to have more than one connection between a single pair of units, with different time delays along the different connections. Such "time delay neural networks" have proven useful in the domain of speech recognition [20], [22], [21], <ref> [115] </ref>. Having more than one connection from one unit to another requires us to modify our notation somewhat; weights and time delays are modified to take a single index, and we introduce some external apparatus to specify the source and destination of each connection.
Reference: [116] <author> B. J. Norris, M. J. Coleman, and M. P. Nusbaum, </author> <title> "Distinct responses of electrically-coupled pacemaker neurons to activation of a modulatory projection neuron", </title> <journal> Society for Neuroscience Abstracts, </journal> <volume> vol. 20, no. 18.6, </volume> <pages> pp. 23, </pages> <year> 1994. </year>
Reference-contexts: This was intended to model a controlled modulation of a central pattern generator from tonic modulatory input, as in the lobster stomatagastric gangleon <ref> [116] </ref>.
Reference: [117] <author> Patrice Y. Simard, Jean Pierre Rayzs, and Bernard Victorri, </author> <title> "Shaping the state space landscape in recurrent networks", </title> <editor> In Lippmann et al. </editor> <volume> [159], </volume> <pages> pp. 105-112. </pages>
Reference-contexts: The diverging lines seem to be caused by this circuitry being activated and exerting a strong influence on the output units while the circuitry itself deactivates. In fact, <ref> [117] </ref> developed a technique for learning the local maximum eigenvalue of the transfer function, optionally projecting out directions whose eigenvalues are not of interest. This technique, which explicitly modulates the behavior we only measured above, has not yet been applied in a control domain. VI.
Reference: [118] <author> J.L. Elman, </author> <title> "Finding structure in time", </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference-contexts: This technique, which explicitly modulates the behavior we only measured above, has not yet been applied in a control domain. VI. Other Non-fixedpoint Techniques A. "Elman Nets" <ref> [118] </ref> considers a version of backpropagation through time in discrete time in which the temporal history is cut off. Typically, only one or two timesteps are preserved, at the discretion of the architect.
Reference: [119] <author> Jeffrey L. Elman, </author> <title> "Finding structure in time", </title> <type> Tech. Rep. </type> <institution> CRL-8801, Center for Research in Language, UCSD, </institution> <month> Apr. </month> <year> 1988. </year>
Reference-contexts: The real question with Elman networks is whether the contribution to the error from the history that has been cut off is significant. This question can only be answered relative to a particular task. For instance, <ref> [119] </ref> finds some problems amenable to the history cutoff, but resorts to full fledged backpropagation through time for other tasks. [43] describe a regular language token prediction task which is difficult for Elman nets when the transition probabili PEARLMUTTER: GRADIENT CALCULATIONS FOR DYNAMIC RECURRENT NEURAL NETWORKS: A SURVEY 13 Fig. 9.
Reference: [120] <author> Yann LeCun, </author> <title> "Une procedure d'apprentissage pour reseau a seuil assymetrique", </title> <booktitle> in Cognitiva 85: </booktitle> <institution> A la Frontiere de l'Intelligence Artificielle des Sciences de la Connaissance des Neurosciences, </institution> <address> Paris 1985, </address> <year> 1985, </year> <pages> pp. 599-604, </pages> <address> CESTA, Paris. </address>
Reference-contexts: The perturbations in the circle network (left) were uniform in 0:1, and in the figure eight network (right) in 0:05. ties are equal, but find that breaking this symmetry allows these nets to learn the task. B. The Moving Targets Method <ref> [120] </ref>, [121], [122] propose a moving targets learning algorithm. Such an algorithm maintains a target value for each hidden unit at each point in time. These target values are typically initialized either randomly, or to the units' initial untrained behavior. In learning, two phases alternate.
Reference: [121] <author> T. Grossman, R. Meir, and E. Domany, </author> <title> "Learning by choice of internal representations", </title> <journal> Complex Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 555-575, </pages> <year> 1989. </year>
Reference-contexts: The perturbations in the circle network (left) were uniform in 0:1, and in the figure eight network (right) in 0:05. ties are equal, but find that breaking this symmetry allows these nets to learn the task. B. The Moving Targets Method [120], <ref> [121] </ref>, [122] propose a moving targets learning algorithm. Such an algorithm maintains a target value for each hidden unit at each point in time. These target values are typically initialized either randomly, or to the units' initial untrained behavior. In learning, two phases alternate.
Reference: [122] <author> Richard Rohwer, </author> <title> "The "moving targets" training algorithm", </title> <booktitle> In Touretzky [156], </booktitle> <pages> pp. 558-565. </pages>
Reference-contexts: The perturbations in the circle network (left) were uniform in 0:1, and in the figure eight network (right) in 0:05. ties are equal, but find that breaking this symmetry allows these nets to learn the task. B. The Moving Targets Method [120], [121], <ref> [122] </ref> propose a moving targets learning algorithm. Such an algorithm maintains a target value for each hidden unit at each point in time. These target values are typically initialized either randomly, or to the units' initial untrained behavior. In learning, two phases alternate.
Reference: [123] <author> Ronald J. Williams and David Zipser, </author> <title> "A learning algorithm for continually running fully recurrent neural networks", </title> <type> Tech. Rep. ICS Report 8805, </type> <address> UCSD, La Jolla, CA 92093, </address> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: This favorable computational complexity makes it of practical significance even for large feedforward recurrent networks. But these feedforward networks are outside the scope of this paper. D. Teacher Forcing In Continuous Time <ref> [123] </ref> coin the term teacher forcing,, which consists of jamming the desired output values into output units as the network runs. Thus, the teacher forces the output units to have the correct states, even as the network runs, and hence the name. <p> only then does the concept of changing the state of an output unit each time step make sense. 14 DRAFT OF July 20, 1995FOR IEEE TRANSACTIONS ON NEURAL NETWORKS The error is as usual, with the caveat that errors are to be measured before output units are forced, not after. <ref> [123] </ref> report that their teacher forcing technique radically reduced training time for their recurrent networks, although [76] reports difficulties when teacher forcing was used networks with a larger number of hidden units. <p> a network which, although when being forced has little error, when running free rapidly drifts far from the desired trajectory, in a qualitative sense, as reported by Williams and Zipser for some cases where oscillations trained with teacher forcing exhibited radically and systematically lower frequency and amplitude when running free <ref> [123] </ref>. E. Jordan Nets [124] used a backpropagation network with the outputs clocked back to the inputs to generate temporal sequences.
Reference: [124] <author> Michael I. Jordan, </author> <title> "Attractor dynamics and parallelism in a connectionist sequential machine", </title> <booktitle> in Proceedings of Ninth Annual Conference of the Cognitive Science Society. </booktitle> <year> 1986, </year> <pages> pp. 531-546, </pages> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: E. Jordan Nets <ref> [124] </ref> used a backpropagation network with the outputs clocked back to the inputs to generate temporal sequences.
Reference: [125] <author> M. B. Matthews, </author> <title> "Neural network nonlinear adaptive filtering using the extended Kalman filter algorithm", </title> <booktitle> in Proceedings of the International Neural Networks Conference, </booktitle> <address> Paris, France, </address> <month> July </month> <year> 1990, </year> <journal> vol. </journal> <volume> 1, </volume> <pages> pp. 115-119. </pages>
Reference-contexts: F. Teacher Forcing, RTRL, and the Kalman Filter <ref> [125] </ref>, [126] have pointed out that RTRL is related to a version of the [127] filter, in the extension that allows it to apply to nonlinear systems, namely the extended Kalman filter (EKF) [128], [107], [129].
Reference: [126] <author> Ronald J. Williams, </author> <title> "Training recurrent networks using the extended Kalman filter", </title> <booktitle> In IJCNN92'Baltimore [153], </booktitle> <pages> pp. 241-250. </pages>
Reference-contexts: F. Teacher Forcing, RTRL, and the Kalman Filter [125], <ref> [126] </ref> have pointed out that RTRL is related to a version of the [127] filter, in the extension that allows it to apply to nonlinear systems, namely the extended Kalman filter (EKF) [128], [107], [129]. The EKF has time and space complexity of the same order as those of RTRL.
Reference: [127] <author> R. E. </author> <title> Kalman, "A new approach to linear filtering and prediction problems", </title> <journal> Trans. ASME Journal of Basic Engineering, </journal> <volume> vol. 82, no. 1, </volume> <pages> pp. 35-45, </pages> <month> Mar. </month> <year> 1960. </year>
Reference-contexts: F. Teacher Forcing, RTRL, and the Kalman Filter [125], [126] have pointed out that RTRL is related to a version of the <ref> [127] </ref> filter, in the extension that allows it to apply to nonlinear systems, namely the extended Kalman filter (EKF) [128], [107], [129]. The EKF has time and space complexity of the same order as those of RTRL.
Reference: [128] <author> R. K. Mahra, </author> <title> "On the identification of variances and adaptive Kalman filtering", </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> vol. AC-15, no. 2, </volume> <pages> pp. 175-184, </pages> <month> Apr. </month> <year> 1970. </year>
Reference-contexts: F. Teacher Forcing, RTRL, and the Kalman Filter [125], [126] have pointed out that RTRL is related to a version of the [127] filter, in the extension that allows it to apply to nonlinear systems, namely the extended Kalman filter (EKF) <ref> [128] </ref>, [107], [129]. The EKF has time and space complexity of the same order as those of RTRL.
Reference: [129] <author> B. D. O. Anderson and J. B. Moore, </author> <title> Optimal Filtering, </title> <publisher> Prentice-Hall, </publisher> <year> 1979. </year>
Reference-contexts: F. Teacher Forcing, RTRL, and the Kalman Filter [125], [126] have pointed out that RTRL is related to a version of the [127] filter, in the extension that allows it to apply to nonlinear systems, namely the extended Kalman filter (EKF) [128], [107], <ref> [129] </ref>. The EKF has time and space complexity of the same order as those of RTRL.
Reference: [130] <author> S. J. Nowlan and G. E. Hinton, </author> <title> "Adaptive soft weight tying using gaussian mixtures", </title> <editor> In Moody et al. </editor> <volume> [161], </volume> <pages> pp. 993-1000. </pages>
Reference-contexts: For instance, adding P i is equivalent to assuming that the weights are Gaussian distributed. Not adding such a term is equivalent to assuming that the a priori distribution on what the weights will turn out to be is flat|not a totally unreasonable prior [28], <ref> [130] </ref>. However, we have added some new sorts of parameters, namely time constants and time delays, here represented generically by the vector T . These are scale parameters, which differ from positional parameters in a number of ways.
Reference: [131] <author> John Skilling, Ed., </author> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Also, the flat prior is no longer the appropriate zero-knowledge prior. All these problems can be solved in a single stroke by noting that the correct zero-knowledge hypothesis for scale parameters is not flat in their values, but rather flat in their log values <ref> [131] </ref>. In practice, This corresponds to doing gradient descent in L T = log T rather than in T itself; in other words, to not manipulating T directly but rather using L T = jdE=dL T .
Reference: [132] <author> P. F. Rowat and A. I. Selverston, </author> <title> "Learning algorithms for oscillatory networks with gap junctions and membrane currents", Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> vol. 2, no. 1, </volume> <pages> pp. 17-42, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: This last property led to the independent invention and use of this technique by <ref> [132] </ref>. In addition, weight decay of scale parameters becomes simpler, as decaying L T towards zero corresponds to decaying T towards one, which is a reasonable target. Of course, a constant factor can be inserted to make the decay towards some other a priori most likely value.
Reference: [133] <author> J. G. Blom, J. M. Sanz-Serna, and Jan G. Verwer, </author> <title> On Simple Moving Grid Methods for One-Dimensional Evolutionary Partial Differential Equations, </title> <publisher> Stichting Mathematisch Centrum, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1986. </year>
Reference-contexts: VIII. Summary and Conclusion A. Complexity Comparison Consider a network with n units and m weights which is run for s time steps (variable grid methods <ref> [133] </ref> would re 16 DRAFT OF July 20, 1995FOR IEEE TRANSACTIONS ON NEURAL NETWORKS duce s by dynamically varying t) where s = (t 1 t 0 )=t. Additionally, assume that the computation of each e i (t) is O (1) and that the network is not partitioned.
Reference: [134] <author> Robert A. Jacobs, </author> <title> "Increased rates of convergence through learning rate adaptation", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 1, no. 4, </volume> <pages> pp. 295-307, </pages> <year> 1988. </year>
Reference-contexts: One technique that has proven useful in this particular situation is that of <ref> [134] </ref> which was applied by Fang and Sejnowski to the single figure eight problem perturbed in figure 9 with great success by [135]. For a modern variant of this technique which is suitable to online pattern presentation, see [136], [137], [138].
Reference: [135] <author> Yan Fang and Terrence J. Sejnowski, </author> <title> "Faster learning for dynamic recurrent backpropagation", </title> <journal> Neural Computation, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 270-273, </pages> <year> 1990. </year>
Reference-contexts: One technique that has proven useful in this particular situation is that of [134] which was applied by Fang and Sejnowski to the single figure eight problem perturbed in figure 9 with great success by <ref> [135] </ref>. For a modern variant of this technique which is suitable to online pattern presentation, see [136], [137], [138].
Reference: [136] <author> Richard S. Sutton, </author> <title> "Gain adaptation beats least squares?", </title> <booktitle> in Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <year> 1992, </year> <pages> pp. 161-166. </pages>
Reference-contexts: For a modern variant of this technique which is suitable to online pattern presentation, see <ref> [136] </ref>, [137], [138].
Reference: [137] <author> Richard S. Sutton, </author> <title> "Adapting bias by gradient descent: An incremental version of delta-bar-delta", </title> <booktitle> in Proceedings of the National Conference on Artificial Intelligence AAAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: For a modern variant of this technique which is suitable to online pattern presentation, see [136], <ref> [137] </ref>, [138].
Reference: [138] <author> Mark A. Gluck, Paul T. Glauthier, and Richard S. Sutton, </author> <title> "Adaptation of cue-specific learning rates in network models of human category learning", </title> <booktitle> in Proceedings of the Fourteenth 20 DRAFT OF July 20, 1995FOR IEEE TRANSACTIONS ON NEURAL NETWORKS Annual Meeting of the Cognitive Science Society, Bloomington, IN, </booktitle> <year> 1992. </year>
Reference-contexts: For a modern variant of this technique which is suitable to online pattern presentation, see [136], [137], <ref> [138] </ref>.
Reference: [139] <author> Douglass J. Wilde and Charles S. Beightler, </author> <title> Foundations of Optimization, </title> <publisher> Prentice-Hall, </publisher> <year> 1967. </year>
Reference-contexts: Since the acceleration of convergence in these gradient systems is such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see <ref> [139, page 304] </ref> and [140], [141], [142], [143], [144], [145], [146], [147], [148]. C.
Reference: [140] <author> B. Widrow, J. M. McCool, M. G. Larimore, and C. R. Johnson, Jr., </author> <title> "Stationary and nonstationary learning characteristics of the LMS adaptive filter", </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 64, </volume> <pages> pp. 1151-1162, </pages> <year> 1976. </year>
Reference-contexts: of convergence in these gradient systems is such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and <ref> [140] </ref>, [141], [142], [143], [144], [145], [146], [147], [148]. C. Prospects and Future Work Control domains are the most natural application for continous time recurrent networks, but signal processing and speech generation (and recognition using generative techniques) are also domains to which this type of network might be naturally applied.
Reference: [141] <author> Bernard Widrow and Samuel D. Stearns, </author> <title> Adaptive Signal Processing, </title> <booktitle> Prentice-Hall signal processing series. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1985. </year>
Reference-contexts: convergence in these gradient systems is such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and [140], <ref> [141] </ref>, [142], [143], [144], [145], [146], [147], [148]. C. Prospects and Future Work Control domains are the most natural application for continous time recurrent networks, but signal processing and speech generation (and recognition using generative techniques) are also domains to which this type of network might be naturally applied.
Reference: [142] <author> S. Thomas Alexander, </author> <title> Adaptive Signal Processing, </title> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: in these gradient systems is such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and [140], [141], <ref> [142] </ref>, [143], [144], [145], [146], [147], [148]. C. Prospects and Future Work Control domains are the most natural application for continous time recurrent networks, but signal processing and speech generation (and recognition using generative techniques) are also domains to which this type of network might be naturally applied.
Reference: [143] <author> David B. Parker, </author> <title> "Optimal algorithms for adaptive networks: Second order back propagation, second order direct propagation, and second order Hebbian learning", </title> <booktitle> In Caudill and Butler [157], </booktitle> <pages> pp. 593-600. </pages>
Reference-contexts: these gradient systems is such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and [140], [141], [142], <ref> [143] </ref>, [144], [145], [146], [147], [148]. C. Prospects and Future Work Control domains are the most natural application for continous time recurrent networks, but signal processing and speech generation (and recognition using generative techniques) are also domains to which this type of network might be naturally applied.
Reference: [144] <author> Raymond Watrous, </author> <title> "Learning algorithms for connectionist networks: Applied gradient methods of nonlinear optimization", </title> <booktitle> In Caudill and Butler [157], </booktitle> <pages> pp. 619-627. </pages>
Reference-contexts: gradient systems is such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and [140], [141], [142], [143], <ref> [144] </ref>, [145], [146], [147], [148]. C. Prospects and Future Work Control domains are the most natural application for continous time recurrent networks, but signal processing and speech generation (and recognition using generative techniques) are also domains to which this type of network might be naturally applied.
Reference: [145] <author> J. J. Shynk and S. Roy, </author> <title> "The LMS algorithm with momentum updating", </title> <booktitle> in Proceedings of the IEEE International Symposium on Circuits and Systems, </booktitle> <month> June 6-9 </month> <year> 1988, </year> <pages> pp. 2651-2654. </pages>
Reference-contexts: systems is such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and [140], [141], [142], [143], [144], <ref> [145] </ref>, [146], [147], [148]. C. Prospects and Future Work Control domains are the most natural application for continous time recurrent networks, but signal processing and speech generation (and recognition using generative techniques) are also domains to which this type of network might be naturally applied.
Reference: [146] <author> Gerald Tesauro, Yu He, and Subutai Ahmad, </author> <title> "Asymptotic convergence of backpropagation", </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 382-391, </pages> <year> 1989. </year>
Reference-contexts: is such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and [140], [141], [142], [143], [144], [145], <ref> [146] </ref>, [147], [148]. C. Prospects and Future Work Control domains are the most natural application for continous time recurrent networks, but signal processing and speech generation (and recognition using generative techniques) are also domains to which this type of network might be naturally applied.
Reference: [147] <author> Mehmet Ali Tugay and Yalcin Tanik, </author> <title> "Properties of the momentum LMS algorithm", </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 18, no. 2, </volume> <pages> pp. 117-127, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: such an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and [140], [141], [142], [143], [144], [145], [146], <ref> [147] </ref>, [148]. C. Prospects and Future Work Control domains are the most natural application for continous time recurrent networks, but signal processing and speech generation (and recognition using generative techniques) are also domains to which this type of network might be naturally applied.
Reference: [148] <author> Barak A. Pearlmutter, </author> <title> "Gradient descent: Second-order momentum and saturating error", </title> <editor> In Moody et al. </editor> <volume> [161], </volume> <pages> pp. 887-894. </pages>
Reference-contexts: an important issue, it can be helpful to know some of the techniques used to analyze the limitations of convergence under various conditions in systems of this sort, and of some other techniques for accelerating their convergence; see [139, page 304] and [140], [141], [142], [143], [144], [145], [146], [147], <ref> [148] </ref>. C. Prospects and Future Work Control domains are the most natural application for continous time recurrent networks, but signal processing and speech generation (and recognition using generative techniques) are also domains to which this type of network might be naturally applied.
Reference: [149] <author> J. J. Hopfield and D. W. Tank, </author> <title> "`Neural' computation of decisions in optimization problems", </title> <journal> Biological Cybernetics, </journal> <volume> vol. 52, </volume> <pages> pp. 141-152, </pages> <year> 1985. </year>
Reference-contexts: On the other hand, we can turn the logic of section V around. Consider a difficult constraint satisfaction task of the sort that neural networks are sometimes applied to, such as the traveling salesman problem <ref> [149] </ref>. Two competing techniques for such problems are simulated annealing [150], [58] and mean field theory [92].
Reference: [150] <author> S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi, </author> <title> "Optimization by simulated annealing", </title> <journal> Science, </journal> <volume> vol. 220, </volume> <pages> pp. 671-680, </pages> <year> 1983. </year>
Reference-contexts: On the other hand, we can turn the logic of section V around. Consider a difficult constraint satisfaction task of the sort that neural networks are sometimes applied to, such as the traveling salesman problem [149]. Two competing techniques for such problems are simulated annealing <ref> [150] </ref>, [58] and mean field theory [92].
Reference: [151] <author> D. E. Rumelhart, J. L. McClelland, </author> <title> and the PDP research group., </title> <editor> Eds., </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundations, </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference: [152] <author> D. H. Ballard and C. M. Brown, </author> <title> Computer Vision, </title> <address> Printice-Hall, </address> <year> 1982. </year>
Reference: [153] <institution> International Joint Conference on Neural Networks, Balti-more, MD, </institution> <month> Apr. </month> <year> 1992. </year> <note> IEEE Press. </note>
Reference: [154] <editor> Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, Eds., </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference: [155] <editor> Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, Eds., </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: [156] <editor> David S. Touretzky, Ed., </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [157] <author> Maureen Caudill and Charles Butler, </author> <title> Eds., </title> <booktitle> IEEE First International Conference on Neural Networks, </booktitle> <address> San Diego, CA, </address> <month> June 21-24 </month> <year> 1987. </year>
Reference: [158] <institution> International Joint Conference on Neural Networks, </institution> <address> Washing-ton DC, June 18-22 1989. </address> <publisher> IEEE Press. </publisher>
Reference: [159] <editor> Richard P. Lippmann, John E. Moody, and David S. Touretzky, Eds., </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [160] <editor> John S. Denker, Ed., </editor> <booktitle> Snowbird Conference on Neural Networks for Computing, number 151 in AIP conference proceedings. American Institute of Physics, </booktitle> <year> 1986. </year>
Reference: [161] <editor> John E. Moody, Steven J. Hanson, and Richard P. Lippmann, Eds., </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference: [162] <institution> International Joint Conference on Neural Networks, </institution> <address> San Diego, CA, June 1990. </address> <publisher> IEEE Press. </publisher>
Reference: [163] <editor> Dana Z. Anderson, Ed., </editor> <booktitle> Neural Information Processing Systems, </booktitle> <address> New York, New York, </address> <year> 1988. </year> <journal> American Institute of Physics. </journal>
Reference: [164] <author> R. J. Williams and D. Zipser, </author> <title> "Gradient-based learning algorithms for recurrent connectionist networks", </title> <type> Tech. Rep. </type> <institution> NU-CCS-90-9, College of Computer Science, Northeastern University, </institution> <address> Boston, MA, </address> <year> 1990, </year> <note> Also published as [110]. </note> <author> Barak A. </author> <title> Pearlmutter has been a connectionist for over a decade, and will receive his PhD in Computer Science from Carnegie Mel-lon University really really soon (really mom, I promise.) When paging through IEEE transactions, he reads the bio's first. He can slice a hippocampus, train an artificial neural network, implement Scheme, and get a giga-ohm seal on a cultured frog neuron with a single suck. </title>
References-found: 163

