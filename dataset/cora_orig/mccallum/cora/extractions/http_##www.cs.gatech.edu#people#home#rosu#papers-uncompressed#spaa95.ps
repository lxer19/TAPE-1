URL: http://www.cs.gatech.edu/people/home/rosu/papers-uncompressed/spaa95.ps
Refering-URL: http://www.cs.gatech.edu/people/home/rosu/cv/cv.html
Root-URL: 
Email: bruck@systems.caltech.edu dolev@cs.huji.ac.il  fho,strongg@almaden.ibm.com rosu@cs.cornell.edu  
Phone: 116-81  
Title: Efficient Message Passing Interface (MPI) for Parallel Computing on Clusters of Workstations  
Author: Jehoshua Bruck Danny Dolev Ching-Tien Ho Marcel-Catalin Ro~su Ray Strong 
Note: Supported in part by the NSF Young Investigator Award CCR-9457811, by the Sloan Research Fellowship, by a grant from the IBM  and by a grant from the AT&T Foundation.  
Address: Mail Code  Pasadena, CA 91125 Jerusalem, Israel  650 Harry Road  San Jose, CA 95120 Ithaca, NY 14853  San Jose, California  
Affiliation: California Institute of Technology Institute of CS  Hebrew University  IBM Almaden Research Center Department of Computer Science  Cornell University  Almaden Research Center,  
Abstract: Parallel computing on clusters of workstations and personal computers has very high potential, since it leverages existing hardware and software. Parallel programming environments offer the user a convenient way to express parallel computation and communication. In fact, recently, a Message Passing Interface (MPI) has been proposed as an industrial standard for writing "portable" message-passing parallel programs. The communication part of MPI consists of the usual point-to-point communication as well as collective communication. However, existing implementations of programming environments for clusters are built on top of a point-to-point communication layer (send and receive) over local area networks (LANs) and, as a result, suffer from poor performance in the collective communication part. In this paper, we present an efficient design and implementation of the collective communication part in MPI that is optimized for clusters of workstations. Our system consists of two main components: the MPI-CCL layer that includes the collective communication functionality of MPI and a User-level Reliable Transport Protocol (URTP) that interfaces with the LAN Data-link layer and leverages the fact that the LAN is a broadcast medium. Our system is integrated with the operating system via an efficient kernel extension mechanism that we developed. The kernel extension significantly improves the performance of our implementation as it can handle part of the communication overhead without involving user space. We have implemented our system on a collection of IBM RS/6000 workstations connected via a 10Mbit Ethernet LAN. Our performance measurements are taken from typical sci entific programs that run in a parallel mode by means of the MPI. The hypothesis behind our design is that system's performance will be bounded by interactions between the kernel and user space rather than by the bandwidth delivered by the LAN Data-Link Layer. Our results indicate that the performance of our MPI Broadcast (on top of Ethernet) is about twice as fast as a recently published software implementation of broadcast on top of ATM. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Amir, D. Dolev, S. Kramer and D. Malki, "Tran-sis: </author> <title> A communication subs-system for high availability," </title> <booktitle> Proceedings of the 22nd International Symposium on Fault-Tolerant Computing, IEEE, </booktitle> <pages> pp. 76-84, </pages> <year> 1992. </year>
Reference-contexts: For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. We note there have been many existing systems, such as <ref> [4, 20, 1] </ref>, that provide a reliable transport protocol and other services for distributed computing. Our URTP protocol distinguishes itself from previous ones because it is targeted for supporting parallel computing using MPI programs and can take advantage of the global program semantics derived from our MPI-CCL implementation.
Reference: [2] <author> V. Bala, J. Bruck, R. Bryant, R. Cypher, P. de Jong, P. Elustondo, D. Frye, A. Ho, C.T. Ho, G. Irwin, S. Kipnis, R. Lawrence, and M. Snir, </author> <title> "The IBM External User Interface for Scalable Parallel Systems", </title> <journal> Parallel Computing, </journal> <volume> Vol. 20, No. 4, </volume> <pages> pp. 445-462, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: For example, a multicast is specified as a broadcast to a particular process group. The performance of a parallel program depends on an efficient implementation of point-to-point as well as collective communication. In existing parallel programming environments, such as PVM, EXPRESS and IBM's MPL <ref> [12, 9, 2] </ref>, for Local Area Networks (LANs), collective communication routines are implemented on top of point-to-point communication. As a result, these environments suffer from poor collective communication performance.
Reference: [3] <author> V. Bala, J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.T. Ho, S. Kipnis, and M. Snir, </author> <title> "CCL: A portable and tunable collective communication library for scalable parallel computers", </title> <booktitle> International Parallel Processing Symposium, </booktitle> <pages> pp. 835-844, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: One of the key components of MPI is the collective communication subset that allows users to conveniently call library routines for various "global" communication operations, like broadcast, scatter and gather. All MPI collective communication routines are implicitly defined with respect to a process group <ref> [3] </ref> which specifies an ordered set of processors within which the collective communication will be performed. For example, a multicast is specified as a broadcast to a particular process group. The performance of a parallel program depends on an efficient implementation of point-to-point as well as collective communication.
Reference: [4] <author> K. Birman, R. Cooper, T. A. Joseph, K. Marzullo, M. Makpangou, K. Kane, F. Schmuck and M. Wood, </author> <title> The ISIS System Manual, </title> <institution> Dept. of Computer Science, Cor-nell University, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. We note there have been many existing systems, such as <ref> [4, 20, 1] </ref>, that provide a reliable transport protocol and other services for distributed computing. Our URTP protocol distinguishes itself from previous ones because it is targeted for supporting parallel computing using MPI programs and can take advantage of the global program semantics derived from our MPI-CCL implementation.
Reference: [5] <author> G.M. Brown, M.G. Gouda, and R.E. Miller "Block Acknowledgement: </author> <title> Redesigning the Window Protocol", </title> <booktitle> In Proc. ACM SIGCOMM'89, </booktitle> <address> Austin, Texas. </address>
Reference-contexts: A packet can be acknowledged as soon as it reaches a URTP buffer. If the number of received and unacknowledged packets reaches a threshold then an ACK packet will be sent to the sender (as in <ref> [5] </ref>). As already evident, the receive call has a rather unusual semantics: data is not returned in a buffer supplied by the MPI-CCL layer. A pointer to a buffer in the receiving buffer pool is returned instead.
Reference: [6] <author> J. Bruck, D. Dolev, C.T. Ho, R. Orni, and R. Strong, "PCODE: </author> <title> An Efficient and Reliable Collective Communication Protocol for Unreliable Broadcast Domains", </title> <journal> IBM Research Report, </journal> <volume> RJ 9895, </volume> <month> September, </month> <year> 1994. </year> <booktitle> Also in Proceedings of International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Our URTP protocol distinguishes itself from previous ones because it is targeted for supporting parallel computing using MPI programs and can take advantage of the global program semantics derived from our MPI-CCL implementation. In a previous paper describing an early version of reliable transport protocol with the same target <ref> [6] </ref>, we used a different protocol with no kernel extension and did not integrate the protocol with MPI. Our performance improvement over [6] is described in Section 4.3. The rest of the paper is organized as follows. In Section 2 we describe our system architecture. <p> In a previous paper describing an early version of reliable transport protocol with the same target <ref> [6] </ref>, we used a different protocol with no kernel extension and did not integrate the protocol with MPI. Our performance improvement over [6] is described in Section 4.3. The rest of the paper is organized as follows. In Section 2 we describe our system architecture. In Section 3 we present the design, prototype implementation and performance measurement of our URTP protocol. <p> receive takes as argument one explicit source processor. 1 In the most strict definition, we say a global program is correct if, for each instruction counter, there is at most one multicast and exactly the processors in the target set issue receive with the source matching the multicast source. (See <ref> [6] </ref> for a more formal and detailed definition.) 1 That is, the wildcard source is not allowed. Note that the skip call, the instruction counter and the re-striction to one multicast call per instruction call are all introduced for the purpose of specification. <p> The hardware implementation of [14] is faster than ours as would be expected from the higher bandwidth of the ATM equipment. As another comparison, our broadcast of a 1 Kbyte message on 8 processors takes about 3.87 msecs as compared to 8.9 msecs based on TCP/IP measured in <ref> [6] </ref>. 4.3 MPI Allgather Let q = dM=me be the number of packets, per processor, that need to be multicast. There are two possible algorithms for MPI Allgather. In Algorithm 1, each processor takes turn as a broadcaster for each packet while all other processors receive. <p> Note that on 8 workstations, MPI Allgather runs about 6.1 msecs and 12.7 msecs, for 32 bytes and 1K byte messages, respectively. As a comparison, a hand-coded all-to-all broadcast based on the PCODE protocol in <ref> [6] </ref> runs about 9.0 msecs and 16.7 msecs for 20 bytes and 1 Kbyte messages, respectively, on faster (100 MHz clock) workstations. Figure 8 shows the same measured times for up to 1 Kbyte messages, but with a linear scale on the Y axis.
Reference: [7] <author> J.B. Carter and W. Zwaenepoel, </author> <title> "Optimistic Implementation of Bulk Data Transfer Protocols" In Performance Evaluation Review, </title> <journal> Vol. </journal> <volume> 17, No. 1, </volume> <month> May </month> <year> 1989, </year> <pages> Pages 61-69. </pages>
Reference-contexts: buffer and second from the kernel buffer to the URTP buffer. (The assembled message will be further copied from URTP buffers into the user's buffer in the MPI program by the MPI-CCL layer.) A protocol using an average of little more than one data copy per message is described in <ref> [7] </ref> but it works only for large data transfers. URTP is intended to be used for both small and large data transfers. In addition, URTP has the following mechanisms: 1. A REQ packet is a point-to-point communication requesting a specific packet from a source.
Reference: [8] <author> D.R. Cheriton and W. Zwaenepoel, </author> <title> "Distributed Process Groups in the V Kernel", </title> <journal> In ACM Transactions on Computer Systems, </journal> <volume> Vol. 3, No. 2, </volume> <month> May </month> <year> 1985, </year> <pages> Pages 77-107. </pages>
Reference: [9] <author> Parasoft Corporation, </author> <title> Express version 1.0: A communication environment for parallel computers, </title> <year> 1988. </year>
Reference-contexts: For example, a multicast is specified as a broadcast to a particular process group. The performance of a parallel program depends on an efficient implementation of point-to-point as well as collective communication. In existing parallel programming environments, such as PVM, EXPRESS and IBM's MPL <ref> [12, 9, 2] </ref>, for Local Area Networks (LANs), collective communication routines are implemented on top of point-to-point communication. As a result, these environments suffer from poor collective communication performance.
Reference: [10] <author> M. J. Fischer, N. A. Lynch, and M. S. Paterson, </author> <title> "Impossibility of Distributed Consensus with One Faulty Process," </title> <note> JACM 32 (1985) 373-382. </note>
Reference-contexts: In fact the problem is harder than the consensus problem because it cannot be solved, even when it is guaranteed that no process will fail (cf. <ref> [10] </ref>). Fortunately, we do not need to solve this problem because we are assured that all the useful work of the application has been performed if only one processor successfully returns from the MPI Barrier call.
Reference: [11] <author> H. Franke, P. Hochschild, P. Pattnaik, and M. Snir, </author> <title> MPI-F: An Efficient Implementation of MPI on IBM-SP1, </title> <booktitle> Proceedings of 1994 International Conference on Parallel Processing, </booktitle> <volume> Vol. III, </volume> <pages> pp. 197-201, </pages> <month> August, </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Recently, a Message Passing Interface (MPI) [15] has been proposed as an industrial standard for writing "portable" message-passing parallel programs and many MPI implementations are already available (e.g., <ref> [11] </ref>). One of the key components of MPI is the collective communication subset that allows users to conveniently call library routines for various "global" communication operations, like broadcast, scatter and gather.
Reference: [12] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, B. Manchek, and V. Sunderam, </author> <title> PVM: Parallel Virtual Machine|A User's Guide and Tutorial for Network Parallel Computing, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: For example, a multicast is specified as a broadcast to a particular process group. The performance of a parallel program depends on an efficient implementation of point-to-point as well as collective communication. In existing parallel programming environments, such as PVM, EXPRESS and IBM's MPL <ref> [12, 9, 2] </ref>, for Local Area Networks (LANs), collective communication routines are implemented on top of point-to-point communication. As a result, these environments suffer from poor collective communication performance.
Reference: [13] <author> J. N. Gray, </author> <booktitle> "Notes on Database Operating Systems," Operating Systems: an advanced course, Lecture Notes in Computer Science 60, </booktitle> <publisher> Springer Verlag (1978) 393-481. </publisher>
Reference-contexts: This termination problem is a variant of the well-known Two Generals Problem, which is unsolvable in the presence of the possibility of message loss <ref> [13, 19] </ref>. In fact the problem is harder than the consensus problem because it cannot be solved, even when it is guaranteed that no process will fail (cf. [10]).
Reference: [14] <author> C. Huang, E. P. Kasten, and P. K. McKinley, </author> <title> "Design and Implementation of Multicast Operations for ATM-Based High Performance Computing", </title> <booktitle> Proceedings of Supercomputing 94 conference, </booktitle> <pages> pp. 164-173, </pages> <address> Washing-ton D.C., </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Our results indicate that the performance of our MPI Broadcast (on top of Ethernet) is about twice as fast as the software implementation of broadcast on top of ATM that is presented in <ref> [14] </ref>. For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. <p> (on top of Ethernet) is about twice as fast as the software implementation of broadcast on top of ATM that is presented in <ref> [14] </ref>. For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. We note there have been many existing systems, such as [4, 20, 1], that provide a reliable transport protocol and other services for distributed computing. <p> From the figure, broadcasting one packet takes about 1 to 4 msecs. The performance of our MPI Broadcast (on top of Ethernet) is about twice as fast as the implementation of broadcast on top of ATM that is presented in <ref> [14] </ref>. For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. The hardware implementation of [14] is faster than ours as would be expected from the higher bandwidth of the ATM equipment. <p> Broadcast (on top of Ethernet) is about twice as fast as the implementation of broadcast on top of ATM that is presented in <ref> [14] </ref>. For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. The hardware implementation of [14] is faster than ours as would be expected from the higher bandwidth of the ATM equipment. <p> is about twice as fast as the implementation of broadcast on top of ATM that is presented in <ref> [14] </ref>. For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. The hardware implementation of [14] is faster than ours as would be expected from the higher bandwidth of the ATM equipment.
Reference: [15] <author> Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Recently, a Message Passing Interface (MPI) <ref> [15] </ref> has been proposed as an industrial standard for writing "portable" message-passing parallel programs and many MPI implementations are already available (e.g., [11]). <p> The timing is very flat for up to 8 processors. 4.6 MPI Initialization and Termination According to MPI specification, MPI Init () must be called before any MPI routines and MPI Finalize () must be called after any MPI routines. Also, a system-defined communicator (see <ref> [15] </ref> for details) called MPI COMM WORLD is defined after MPI Init call. From MPI COMM WORLD, one can derive the processor group of the "MPI world", denoted MPI GROUP WORLD in the paper.
Reference: [16] <author> J.C. Mogul, R.F. Rashid, and M.J. Accetta, </author> <title> "The Packet Filter: An Efficient Mechanism for User-level Network Code", </title> <booktitle> In Proceedings of the 11th Symposium on Operating System Principles, ACM SIGOPS, </booktitle> <address> Austin, Texas, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: As a result URTP is implemented as a combination of a kernel extension and a user-level library. Most of the protocol code is in the user-level library. This decision made the implementation easier without a significant performance degradation (see <ref> [16] </ref>). The kernel extension part enables fast processing (dropping) of multicast packets at processors that are not part of the target set and also reduces the inter-processor communication overhead between processes. 3.1 Protocol Description The issues and requirements that we consider while designing the protocol are as follows: 1.
Reference: [17] <author> R. B. Morris, Y. Tsuji, and P. Carnevali, </author> <title> "Adaptive Solution Strategy for Solving Large Systems of p-type Finite Element Equations", </title> <journal> Int. J. Numer. Methods Eng., </journal> <volume> Vol 33, </volume> <pages> 2059-2071, </pages> <year> 1992. </year>
Reference-contexts: We have ported two sequential programs into parallel MPI programs. The first one is PolyFEM, a simulation and modeling program that uses p-type finite-element-method algorithms for elasticity modeling <ref> [17] </ref>. We have isolated and paral-lelized the portion of the PolyFEM solver that takes most of the running time on large problems. This subapplication involves iteratively multiplying a vector by a large sparse matrix.
Reference: [18] <author> D. Patterson et al., </author> <title> "A Case for Networks of Workstations (NOW)", </title> <booktitle> Symposium Record of Hot Interconnects II, </booktitle> <pages> pp. 24-39, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: In particular, we demonstrate the implementation on a traditional 10Mbit Ethernet-based LAN. We note here that the ideas presented in this paper can be easily extended to any Network of Workstations (NOW) <ref> [18] </ref> that provides an un- reliable broadcast transport protocol (e.g. ATM).
Reference: [19] <author> R. Strong, D. Dolev, and F. Cristian, </author> <title> "A Unified Theory of Distributed Coordination with Communication Uncertainty," IBM Research Report RJ7727, </title> <booktitle> 1990, in the Proceedings of the 28th Allerton Conference, </booktitle> <address> Aller-ton, </address> <month> September, </month> <year> 1990. </year>
Reference-contexts: This termination problem is a variant of the well-known Two Generals Problem, which is unsolvable in the presence of the possibility of message loss <ref> [13, 19] </ref>. In fact the problem is harder than the consensus problem because it cannot be solved, even when it is guaranteed that no process will fail (cf. [10]).

References-found: 19

