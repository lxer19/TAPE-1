URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1998/tr-98-033.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1998.html
Root-URL: http://www.icsi.berkeley.edu
Email: hidber@icsi.berkeley.edu  
Title: Online Association Rule Mining  
Author: Christian Hidber 
Date: September 1998  
Address: Berkeley  
Affiliation: International Computer Science Institute,  
Pubnum: TR-98-033  
Abstract: We present a novel algorithm to compute large itemsets online. The user is free to change the support threshold any time during the first scan of the transaction sequence. The algorithm maintains a superset of all large itemsets and for each itemset a shrinking, deterministic interval on its support. After at most 2 scans the algorithm terminates with the precise support for each large itemset. Typically our algorithm is by an order of magnitude more memory efficient than Apriori or DIC. 1 
Abstract-found: 1
Intro-found: 1
Reference: [AIS93] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pages 207-216, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Mining for association rules is a form of data mining introduced in <ref> [AIS93] </ref>. The prototypical example is based on a list of purchases in a store. An association rule for this list is a rule such as "85% of all customers who buy product A and B also buy product C and D".
Reference: [AS94] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules. </title> <booktitle> In Proc. of the 20th Int'l Conf. on Very Large Databases, </booktitle> <address> Santiago, Chile, </address> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: In Appendix C we give a formal proof of correctness for PhaseI. In Appendix D we introduce a forward pruning technique for PhaseII and prove its correctness. 3 Related Work Most large itemset computation algorithms are related to the Apriori algorithm due to Agrawal & Srikant, c.f. <ref> [AS94] </ref>. See [AY98] for a survey of large itemset computation algorithms. Apriori exploits the observation that all subsets of a large itemset are large themselves. It is a multi-pass algorithm, where in the k-th pass all large itemsets 2 of cardinality k are computed. <p> V ; begin V := PhaseI ( T , ); V := PhaseII ( V , T , ); return V ; end; Figure 8 7 Implementation To asses the performance we tested Carma along with Apriori and DIC on synthetic data generated by the IBM test data generator, c.f. <ref> [AS94] </ref> 1 . We illustrate our findings on the synthetic dataset with 100'000 transactions of an average size of 10 items chosen from 10'000 items and an average large itemset size of 4 (T10.I4.100K with 10K items). For runs on further datasets see Appendix A. <p> Our implementation of Carma diverges from the pseudo-code given in Subsection 5.2 only in that we perform the PhaseI incrementation and insertion step simultaneously, enumerating the subsets of a scanned transaction once. Apriori and DIC were implemented as described in <ref> [AS94] </ref> and [BMUT97] respec tively. For DIC we chose a blocksize of 15000, which we found to be fastest. 1 http://www.almaden.ibm.com/cs/quest/syndata.html 12 7.2 Relative Performance To compare Carma with Apriori and DIC we ran all three algorithms on a range of datasets and (constant) support thresholds.
Reference: [AY97] <author> Charu C. Aggarwal and Philip S. Yu. </author> <title> Online generation of association rules. </title> <type> Technical Report RC 20899 (92609), </type> <institution> IBM Research Division, T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Find all large itemsets. 2. For each large itemset Z, find all subsets X, such that the confidence of X ) ZnX is greater or equal to the confidence threshold. We address the first step, since the second step can already be computed online, c.f. <ref> [AY97] </ref>. Existing large itemset computation algorithms have an o*ine or batch behaviour: given the user-specified support threshold, the transaction sequence is scanned and rescanned, often several times, and eventually all large itemsets are produced. However, the user does not know, in general, an appropriate support threshold in advance. <p> These algorithms require a rescan of the full transaction sequence whenever an itemset becomes large due to an insertion. Carma, in contrast, requires a rescan only if the user needs the precise support of the additional large itemsets, instead of the continuously shrinking support intervals provided by PhaseI. In <ref> [AY97] </ref> an Online Analytical Processing (OLAP)-style algorithm is proposed to compute association rules. The general idea is to precompute all large itemsets relative to some support threshold s using a traditional algorithm. The association rules are then generated online relative to an interactively specified confidence threshold and support threshold s. <p> Carma overcomes these difficulties by bringing the large itemset computation itself online. Thus, combining Carma's large itemset computation with the online rule generation suggested in <ref> [AY97] </ref> brings both steps online, not requiring any precomputation. 4 Sketch of the Algorithm Carma uses distinct algorithms, called PhaseI and PhaseII, for the first and second scan of the transaction sequence. In this section, we give a sketch of both algorithms.
Reference: [AY98] <author> Charu C. Aggarwal and Philip S. Yu. </author> <title> Mining large itemsets for association rules. </title> <journal> Bulletin of the IEEE Computer Society Technical Comittee on Data Engineering, </journal> <pages> pages 23-31, </pages> <month> March </month> <year> 1998. </year>
Reference-contexts: In Appendix C we give a formal proof of correctness for PhaseI. In Appendix D we introduce a forward pruning technique for PhaseII and prove its correctness. 3 Related Work Most large itemset computation algorithms are related to the Apriori algorithm due to Agrawal & Srikant, c.f. [AS94]. See <ref> [AY98] </ref> for a survey of large itemset computation algorithms. Apriori exploits the observation that all subsets of a large itemset are large themselves. It is a multi-pass algorithm, where in the k-th pass all large itemsets 2 of cardinality k are computed.
Reference: [Bay98] <author> R. J. Bayardo Jr. </author> <title> Efficiently mining long patterns from databases. </title> <booktitle> In Proc. of the 1998 ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 85-93, </pages> <address> Seattle, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: This is important for Carma, since whenever Carma inserts a new itemset v, it accesses all its maximal subsets to compute maxM issed (v). We represent the lattice structure by associating to each itemset the set of all further items appearing in any of its supersets, c.f. <ref> [Bay98] </ref>. As in the case of a hashtree, we need only one hashtable access to pass from an itemset to one of its minimal supersets. Thus we can enumerate all subsets of a scanned transaction, which are contained in the lattice, as quickly as in a hashtree.
Reference: [BMUT97] <author> Sergey Brin, Rajeev Motwani, Jeffrey D. Ullman, and Shalom Tsur. </author> <title> Dynamic itemset counting and implication rules for market basket data. </title> <booktitle> 28 In Proceedings of the ACM SIGMOD International Conference on Man--agement of Data, volume 26,2 of SIGMOD Record, </booktitle> <pages> pages 255-264, </pages> <address> New York, May 13th-15th 1997. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Discovering such customer buying patterns is useful for customer segmentation, cross-marketing, catalog design and product placement. We give a problem description which follows <ref> [BMUT97] </ref>. The support of an itemset (set of items) in a transaction sequence is the fraction of all transactions containing the itemset. An itemset is called large if its support is greater or equal to a user-specified support threshold, otherwise it is called small. <p> In the second pass, the actual support of each set in the superset is computed. After removing all small itemsets, Partition produces the set of all large itemsets. In contrast to Apriori, the DIC (Dynamic Itemset Counting) algorithm counts itemsets of different cardinality simultaneously, c.f. <ref> [BMUT97] </ref>. The transaction sequence is partioned into blocks. The itemsets are stored in a lattice which is initialized by all singleton sets. While a block is scanned, the count (number of occurences) of each itemset in the lattice is adjusted. <p> Our implementation of Carma diverges from the pseudo-code given in Subsection 5.2 only in that we perform the PhaseI incrementation and insertion step simultaneously, enumerating the subsets of a scanned transaction once. Apriori and DIC were implemented as described in [AS94] and <ref> [BMUT97] </ref> respec tively. For DIC we chose a blocksize of 15000, which we found to be fastest. 1 http://www.almaden.ibm.com/cs/quest/syndata.html 12 7.2 Relative Performance To compare Carma with Apriori and DIC we ran all three algorithms on a range of datasets and (constant) support thresholds.
Reference: [CHNW96] <author> D. Cheung, J. Han, V. Ng, and C.Y. Wong. </author> <title> Maintenance of discovered association rules in large databases: An incremental updating technique. </title> <booktitle> In Proc. of 1996 Int'l Conf. on Data Engineering (ICDE'96), </booktitle> <address> New Orleans, Louisiana, USA, </address> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Carma, in contrast, deterministically computes all large itemsets along with the precise support for each itemset. Several algorithms based on Apriori were proposed to update a previously computed set of large itemsets due to insertion or deletion of transactions, c.f. <ref> [CHNW96, CLK97, TBAR97] </ref>. These algorithms require a rescan of the full transaction sequence whenever an itemset becomes large due to an insertion.
Reference: [CLK97] <author> David W. L. Cheung, S.D. Lee, and Benjamin Kao. </author> <title> A general incremental technique for maintaining discovered association rules. </title> <booktitle> In Proceedings of the Fifth International Conference On Database Systems For Advanced Applications, </booktitle> <pages> pages 185-194, </pages> <address> Melbourne, Australia, </address> <month> March </month> <year> 1997. </year>
Reference-contexts: Carma, in contrast, deterministically computes all large itemsets along with the precise support for each itemset. Several algorithms based on Apriori were proposed to update a previously computed set of large itemsets due to insertion or deletion of transactions, c.f. <ref> [CHNW96, CLK97, TBAR97] </ref>. These algorithms require a rescan of the full transaction sequence whenever an itemset becomes large due to an insertion.
Reference: [Hel96] <author> Joseph M. Hellerstein. </author> <title> The case for online aggregation. </title> <type> Technical Report UCB//CSD-96-908, </type> <institution> EECS Computer Science Divison, University of California at Berkeley, </institution> <year> 1996. </year>
Reference-contexts: However, the user does not know, in general, an appropriate support threshold in advance. An inappropriate choice yields, after a long wait, either too many or too few large itemsets, which often results in useless or misleading association rules. Inspired by online aggregation, c.f. <ref> [Hel96, HHW97] </ref>, our goal is to overcome these difficulties by bringing large itemset computation online. We consider an algorithm to be online if: 1) it gives continuous feedback, 2) it is user controllable during processing and 3) it yields a deterministic and accurate result.
Reference: [HHW97] <author> Joseph M. Hellerstein, Peter J. Haas, and Helen J. Wang. </author> <title> Online aggregation. </title> <booktitle> SIGMOD '97, </booktitle> <year> 1997. </year>
Reference-contexts: However, the user does not know, in general, an appropriate support threshold in advance. An inappropriate choice yields, after a long wait, either too many or too few large itemsets, which often results in useless or misleading association rules. Inspired by online aggregation, c.f. <ref> [Hel96, HHW97] </ref>, our goal is to overcome these difficulties by bringing large itemset computation online. We consider an algorithm to be online if: 1) it gives continuous feedback, 2) it is user controllable during processing and 3) it yields a deterministic and accurate result.
Reference: [SON95] <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In Proceedings of the Very Large Data Base Conference, </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: It is a multi-pass algorithm, where in the k-th pass all large itemsets 2 of cardinality k are computed. Hence Apriori needs up to c + 1 scans of the database where c is the maximal cardinality of a large itemset. In <ref> [SON95] </ref> a 2-pass algorithm called Partition is introduced. The general idea is to partition the database into blocks such that each block fits into main-memory. In the first pass, each block is loaded into memory and all large itemsets, with respect to that block, are computed using Apriori.
Reference: [TBAR97] <author> Shiby Thomas, Sreenath Bodagala, Khaled Alsabti, and Sanjay Ranka. </author> <title> An efficient algorithm for the incremental updation of association rules in large databases. </title> <booktitle> In Proceedings of the 3rd International conference on Knowledge Discovery and Data Mining (KDD 97), </booktitle> <address> New Port Beach, California, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: Carma, in contrast, deterministically computes all large itemsets along with the precise support for each itemset. Several algorithms based on Apriori were proposed to update a previously computed set of large itemsets due to insertion or deletion of transactions, c.f. <ref> [CHNW96, CLK97, TBAR97] </ref>. These algorithms require a rescan of the full transaction sequence whenever an itemset becomes large due to an insertion.
Reference: [Toi96] <author> Hannu Toivonen. </author> <title> Sampling large databases for association rules. </title> <editor> In T. M. Vijayaraman, Alejandro P. Buchmann, C. Mohan, and Nandlal L. Sarda, editors, VLDB'96, </editor> <booktitle> Proceedings of 22th International Conference on Very Large Data Bases, </booktitle> <address> Mumbai (Bombay), India, September 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Carma, in contrast: 1) allows the user to change the support threshold at any time, 2) gives continuous feedback and 3) requires at most two scans of the transaction sequence. Random sampling algorithms have been suggested as well, c.f. <ref> [Toi96, ZPLO96] </ref>. The general idea is to take a random sample of suitable size from the transaction sequence and compute the large itemsets using Apriori or Partition with respect to that sample.
Reference: [ZPLO96] <author> Mohammed Javeed Zaki, Srinivasan Parthasarathy, Wei Li, and Mit-sunori Ogihara. </author> <title> Evaluation of sampling for data mining of association rules. </title> <type> Technical Report 617, </type> <institution> Computer Science Dept., U. Rochester, </institution> <month> May </month> <year> 1996. </year> <month> 29 </month>
Reference-contexts: Carma, in contrast: 1) allows the user to change the support threshold at any time, 2) gives continuous feedback and 3) requires at most two scans of the transaction sequence. Random sampling algorithms have been suggested as well, c.f. <ref> [Toi96, ZPLO96] </ref>. The general idea is to take a random sample of suitable size from the transaction sequence and compute the large itemsets using Apriori or Partition with respect to that sample.
References-found: 14

