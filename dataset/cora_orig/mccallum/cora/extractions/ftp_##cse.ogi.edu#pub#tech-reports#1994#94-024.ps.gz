URL: ftp://cse.ogi.edu/pub/tech-reports/1994/94-024.ps.gz
Refering-URL: ftp://cse.ogi.edu/pub/tech-reports/README.html
Root-URL: http://www.cse.ogi.edu
Title: Processor Virtualization and Migration for PVM  
Author: Steve W. Otto 
Abstract: This paper describes research underway to define and develop the next generation of PVM (Parallel Virtual Machine). Future versions of PVM will be modular and open so as to allow interoperability with other packages, such as distributed scheduling systems. We concentrate on one aspect of the work: providing virtualization of processors and transparent migration mechanisms within the message-passing programming model. Work migration is a key ingredient to allow good scheduling on a large, busy system. Two migration systems will be described. The first is Migratable PVM (MPVM), which allows transparent migration at process granularity amongst homogeneous groups of processors. The system is functional and has run realistic applications. The second system is a multi-threaded version of PVM, where threads are disjoint and do not share data spaces. This again allows transparent migration. Local communication speeds and context-switch times are improved over process-level MPVM. Performance figures and semantic restrictions of both packages are given.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. L. Beguelin, J. J. Dongarra, A. Geist, R. J. Manchek, S. W. Otto, and J. Walpole, </author> <title> PVM: Experiences, current status and future direction, </title> <booktitle> in Supercomputing'93 Proceedings, </booktitle> <year> 1993, </year> <pages> pp. 765-6. </pages>
Reference-contexts: Ridge National Laboratory, Carnegie Mellon University, and Oregon Graduate Institute have begun research to enhance the functionality of PVM, with emphasis on: migration capability and interfaces to distributed schedulers and resource managers; tools for program development and performance monitoring; and a new interface adding multimedia and visualization capabilities to PVM <ref> [1] </ref>. Some of this functionality will be extended to include MPI, the standardized message-passing interface [7]. fl Dept of Computer Science and Engineering, Oregon Graduate Institute of Science & Technology, 20000 NW Walker Road, P. O.
Reference: [2] <author> A. L. Beguelin, J. J. Dongarra, A. Geist, R. J. Manchek, and V. S. Sunderam, </author> <title> Heterogeneous network computing, </title> <booktitle> in Sixth SIAM Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: The PVM (Parallel Virtual Machine) system virtualizes a heterogeneous collection of computers into a distributed-memory, message-passing, parallel computer <ref> [2, 6] </ref>. PVM allows concurrent processing on heterogeneous systems interconnected by fast networks and has demonstrated the technical and economic viability of this computing model.
Reference: [3] <author> P. Bernstein, </author> <title> Middleware: An architecture for distributed system services, </title> <type> Tech. Rep. </type> <institution> CRL93/6, Cambridge Research Lab, Digital Equipment Corp., </institution> <month> March </month> <year> 1993. </year> <note> Submitted for publication. </note>
Reference-contexts: The smaller boxes such as VS (visualization system) are existing modules that we cannot modify. Note that this picture illustrates the software architecture and not processes several of the modules here are distributed and correspond to many executing processes. 1.2 The CPE as Middleware The CPE kernel is "middleware" <ref> [3] </ref>, providing a well-defined, virtual environment through which concurrent applications access system resources. Besides message passing, the CPE kernel also provides its own process control abstractions, internal interfaces to other CPE modules, and external interfaces to the operating system, file system, windowing system and visualization environment.
Reference: [4] <author> A. P. Black and J. Walpole, </author> <title> Objects to the rescue!, </title> <type> tech. rep., </type> <institution> Oregon Graduate Institute of Science & Technology, </institution> <month> May </month> <year> 1994. </year> <note> Position paper for SIGOPS 1994 European Workshop. </note>
Reference-contexts: Similarly, the CPE kernel will export an explicit scheduling interface such that resource allocation can be done globally rather than on a per application basis. Due to the severe constraints of portability across operating systems, creating efficient and functional middleware such as the CPE is a challenge. See <ref> [4] </ref> for some thoughts on what operating systems should provide to support such systems. 1.3 Distributed Scheduler and Authentication Current PVM includes many process creation and management tasks that are more properly left to a separate scheduling system.
Reference: [5] <author> J. Casas, R. Konuru, S. W. Otto, R. Prouty, and J. Walpole, </author> <title> Adaptive load migration systems for PVM, </title> <type> tech. rep., </type> <month> March </month> <year> 1994. </year> <note> Submitted to Supercomputing `94. </note>
Reference-contexts: The remainder of this paper describes two processor virtualization and migration systems that we have built. The first system, Migratable PVM (MPVM), uses Unix processes as its virtual processors (as does conventional PVM) and allows the transparent migration of these processes <ref> [5] </ref>. The processes of a PVM application can be suspended on one workstation and subsequently resumed on another workstation without any help from the application program. The package is source-code compatible with PVM requiring no more than re-compilation and re-linking of PVM applications.
Reference: [6] <author> J. J. Dongarra, A. Geist, R. J. Manchek, and V. S. Sunderam, </author> <title> Integrated PVM framework supports heterogeneous network computing, </title> <booktitle> Computers in Physics, </booktitle> <year> (1993). </year>
Reference-contexts: The PVM (Parallel Virtual Machine) system virtualizes a heterogeneous collection of computers into a distributed-memory, message-passing, parallel computer <ref> [2, 6] </ref>. PVM allows concurrent processing on heterogeneous systems interconnected by fast networks and has demonstrated the technical and economic viability of this computing model.
Reference: [7] <author> M. P. I. Forum, </author> <title> MPI: A message-passing interface standard, </title> <institution> computer Science Dept. </institution> <type> Technical Report CS-94-230, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> April </month> <year> 1994. </year> <journal> (To appear in the International Journal of Supercomputer Applications, </journal> <volume> Volume 8, Number 3/4, </volume> <year> 1994). </year>
Reference-contexts: Some of this functionality will be extended to include MPI, the standardized message-passing interface <ref> [7] </ref>. fl Dept of Computer Science and Engineering, Oregon Graduate Institute of Science & Technology, 20000 NW Walker Road, P. O. Box 91000, Portland OR 97291-1000, otto@cse.ogi.edu 1 2 Otto 1.1 An Open Framework The new version of PVM will form the "kernel" of the CPE.
Reference: [8] <author> R. Konuru, J. Casas, S. W. Otto, R. Prouty, and J. Walpole, </author> <title> A user-level process package for PVM, </title> <booktitle> in 1994 Scalable High-Performance Computing Conference, IEEE, </booktitle> <month> May </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Migration events are initiated and controlled by a global scheduler that is external to the application. The second system, UPVM, is a virtual processor package that supports multi-threading and transparent migration for PVM applications <ref> [8] </ref>. The virtual processors are called User Level Processes (ULPs) and can be thought of as light-weight, Unix-like processes that are independently migratable. <p> This time is equivalent to the obtrusiveness time plus the restart time. 4 User-level Process PVM (UPVM) UPVM is a package that supports multi-threading and transparent migration for PVM applications <ref> [8] </ref>. Though the MPVM package gives a transparent migration capability, UPVM provides a set of "smaller" entities than processes to migrate, allowing load redistribution at a finer granularity. Context switching and on-processor (local) communication times are also greatly improved over MPVM.
Reference: [9] <author> M. Litzkow, M. Livny, and M. </author> <title> Mutka, Condor | a hunter of idle workstations, </title> <booktitle> in Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <address> San Jose, CA, </address> <month> June </month> <year> 1988, </year> <journal> IEEE, </journal> <pages> pp. 104-111. </pages>
Reference-contexts: Subsequently, the system was ported to the SPARC architecture running SUNOS 4.X. The implementation tries to be machine-independent. The migration mechanism, however, is somewhat machine and operating system dependent. We have attempted to limit the dependence on the OS by using generic features found in most versions of Unix <ref> [9, 10] </ref>. As long as a process can take a snap-shot of its register context and determine the extents of its writable data, heap, and stack space at run-time, porting is not difficult.
Reference: [10] <author> M. Litzkow and M. Solomon, </author> <title> Supporting checkpoint and process migration outside the unix kernal, </title> <booktitle> in Usenix Winter Conference, </booktitle> <year> 1992. </year>
Reference-contexts: Subsequently, the system was ported to the SPARC architecture running SUNOS 4.X. The implementation tries to be machine-independent. The migration mechanism, however, is somewhat machine and operating system dependent. We have attempted to limit the dependence on the OS by using generic features found in most versions of Unix <ref> [9, 10] </ref>. As long as a process can take a snap-shot of its register context and determine the extents of its writable data, heap, and stack space at run-time, porting is not difficult.
Reference: [11] <author> B. C. Neuman, Prospero: </author> <title> A tool for organizing Internet resources, </title> <journal> Electronic Networking: Research, Applications and Policy, </journal> <month> 2 </month> <year> (1992). </year>
Reference-contexts: Distributed schedulers in a large-scale heterogeneous environments require additional functionality to access resources in multiple administrative domains. Rather than build authentication and accounting functionality directly into the scheduler, however, we will use existing systems such as the Prospero resource manager <ref> [11, 12] </ref>. 2 Migration Processor virtualization is an attractive goal because it frees application programmers from the burden of managing physical processor location and availability. Virtual processors (VPs) allow programmers to think solely in terms of the parallelism within their application.
Reference: [12] <author> B. C. Neuman and S. Rao, </author> <title> Resource management for distributed parallel systems, </title> <booktitle> in Proceedings of the 2nd Internationational Symposium on High Performance Distributed Computing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: Distributed schedulers in a large-scale heterogeneous environments require additional functionality to access resources in multiple administrative domains. Rather than build authentication and accounting functionality directly into the scheduler, however, we will use existing systems such as the Prospero resource manager <ref> [11, 12] </ref>. 2 Migration Processor virtualization is an attractive goal because it frees application programmers from the burden of managing physical processor location and availability. Virtual processors (VPs) allow programmers to think solely in terms of the parallelism within their application.
Reference: [13] <author> J. K. Ousterhout, </author> <title> Why aren't operating systems getting faster as fast as hardware?, </title> <booktitle> in Proceedings of the Summer 1990 USENIX Conference, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> pp. 247-256. </pages>
Reference-contexts: Type Cost (micro-seconds) Ratio ULP switch 4.74 7.30 UNIX switch 195.00 300.46 Isolating the process context switch cost in a portable manner is extremely difficult, since there is no equivalent of a yield-to-another-process system call on UNIX. Our solution to this problem was to use Ousterhout's context switch benchmark <ref> [13] </ref>. In this case, we calculate half the time taken by two UNIX processes to alternately read and write one byte from a pair of pipes.
References-found: 13

