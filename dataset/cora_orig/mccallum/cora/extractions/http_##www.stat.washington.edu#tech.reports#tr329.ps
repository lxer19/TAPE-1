URL: http://www.stat.washington.edu/tech.reports/tr329.ps
Refering-URL: http://www.stat.washington.edu/fraley/resources.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: How Many Clusters? Which Clustering Method? Answers Via Model-Based Cluster Analysis 1  
Author: C. Fraley and A. E. Raftery 
Note: 1 Funded by the Office of Naval Research under contracts N00014-96-1-0192 and N00014-96-1 0330.  
Date: February 27, 1998  
Address: Box 354322 Seattle, WA 98195-4322 USA  
Affiliation: Department of Statistics University of Washington  
Pubnum: Technical Report No. 329  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Murtagh, F. and Raftery, A. E. </author> <year> (1984). </year> <title> Fitting straight lines to point patterns. </title> <journal> Pattern Recognition, </journal> <volume> 17, </volume> <pages> 479-483. </pages>
Reference-contexts: At each stage, the splitting or merging is chosen so as to optimize some criterion. Conven 2 tional agglomerative hierarchical methods use heuristic criteria (e.g. sum of squares, single and complete link (nearest and farthest neighbor)) [13]. A maximum-likelihood criterion is used for merging groups in model-based methods <ref> [1] </ref>, [2]. Relocation methods move observations iteratively from one group to another, starting from an initial partition. The number of groups has to be specified in advance and typically does not change during the course of the iteration, only their composition. <p> We are mainly concerned with the case where f k (x i j k ) is multivariate normal (Gaussian), a model that has been used with considerable success in a number of applications <ref> [1] </ref>, [2], [5], [3], [4], [6]. <p> are spherical and have equal volumes; k = = DAD T , in which all clusters have the same shape, volume and orientation (Friedman and Rubin [16]); unconstrained k , which is the most general model (Scott and Symons [17]); and k = D k AD k (Murtagh and Raftery <ref> [1] </ref>), in which only the orientations of the clusters may differ. Table 1 shows the geometric interpretation of the various parameterizations discussed in [2]. A more extensive set of models within the same framework is treated in [5]. <p> Table 1: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], <ref> [1] </ref>, [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical <p> Orientation Reference I Spherical fixed fixed NA [15], <ref> [1] </ref>, [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], [2]. At each stage, a pair of clusters is merged so as to maximize the resulting likelihood. <p> [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable <ref> [1] </ref>, [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], [2]. At each stage, a pair of clusters is merged so as to maximize the resulting likelihood. <p> The algorithms used for EM and for computing BIC monitor an estimate of the reciprocal condition number (smallest to 9 largest eigenvalue ratio) of the covariances. This latter quantity falls in the range <ref> [0; 1] </ref>, and values near zero imply ill-conditioning [31]. Computations are less reliable for ill-conditioned problems, and as a consequence may cause anomalies before reaching the point of actual failure.
Reference: [2] <author> Banfield, J. D. and Raftery, A. E. </author> <year> (1993). </year> <title> Model-based Gaussian and non-Gaussian clustering. </title> <journal> Biometrics, </journal> <volume> 49, </volume> <pages> 803-821. </pages>
Reference-contexts: At each stage, the splitting or merging is chosen so as to optimize some criterion. Conven 2 tional agglomerative hierarchical methods use heuristic criteria (e.g. sum of squares, single and complete link (nearest and farthest neighbor)) [13]. A maximum-likelihood criterion is used for merging groups in model-based methods [1], <ref> [2] </ref>. Relocation methods move observations iteratively from one group to another, starting from an initial partition. The number of groups has to be specified in advance and typically does not change during the course of the iteration, only their composition. <p> We are mainly concerned with the case where f k (x i j k ) is multivariate normal (Gaussian), a model that has been used with considerable success in a number of applications [1], <ref> [2] </ref>, [5], [3], [4], [6]. <p> The covariances k determine their other geometric characteristics. Banfield and Raftery <ref> [2] </ref> developed a model-based framework for clustering by parame terizing the covariance matrix in terms of its eigenvalue decomposition k = k D k A k D T where D k is the orthogonal matrix of eigenvectors, A k is a diagonal matrix whose elements are proportional to eigenvalues of k <p> Table 1 shows the geometric interpretation of the various parameterizations discussed in <ref> [2] </ref>. A more extensive set of models within the same framework is treated in [5]. Table 1: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical <p> geometric interpretation of the various parameterizations discussed in <ref> [2] </ref>. A more extensive set of models within the same framework is treated in [5]. Table 1: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k <p> Table 1: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery <ref> [2] </ref>. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable <p> Table 1: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery <ref> [2] </ref>. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can <p> The models shown here are those discussed in Banfield and Raftery <ref> [2] </ref>. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering <p> discussed in Banfield and Raftery <ref> [2] </ref>. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], [2]. <p> Reference I Spherical fixed fixed NA [15], [1], <ref> [2] </ref>, [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], [2]. At each stage, a pair of clusters is merged so as to maximize the resulting likelihood. <p> I Spherical variable fixed NA <ref> [2] </ref>, [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], [2]. At each stage, a pair of clusters is merged so as to maximize the resulting likelihood. <p> k D k A k D k Elliptical variable variable variable [17], <ref> [2] </ref>, [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], [2]. At each stage, a pair of clusters is merged so as to maximize the resulting likelihood. <p> For a recent review of Bayes factors emphasizing the underlying concepts and scientific applications, see Kass and Raftery [23]. The Bayes factor is the posterior odds for one model against the other assuming neither is favored a priori. Banfield and Raftery <ref> [2] </ref> used a heuristically derived approximation to twice the log Bayes factor called the `AWE' to determine the number of clusters in hierarchical clustering based on the classification likelihood. <p> In the clinical classification, each of the two long extensions roughly represents a cluster, while the third cluster is concentrated closer to the origin. This poor performance of single link and, to a lesser extent of k-means, has also been found in other examples (e.g. <ref> [2] </ref>). They work well (as do most other methods) when clusters are well separated, but not when clusters overlap or intersect. 8 It is important, however, to distinguish between single-link clustering and nearest-neighbor classification. <p> The two clusters together give an accurate reconstruction of the actual minefield. It should be noted that the method is sensitive to the value of V , the assumed volume of the data region. Here it is clear that V is the area of the image; Banfield and Raftery <ref> [2] </ref> and Dasgupta and Raftery [3] similarly used the volume of the smallest hyperrectangle with sides parallel to the axes that contains all the data points. However, this value could overestimate V in many cases. <p> One way around this is to determine the structure of some subset of the data according to the strategy given here, and classify the remaining observations via supervised classification or discriminant analysis <ref> [2] </ref>. Bensmail and Celeux [33] have developed a method for regularized discriminant analysis based on the full range of parameterizations of Gaussian mixtures (3). Alternatively, fast methods for determining an initial coarse partition can be used to reduce computational requirements.
Reference: [3] <author> Dasgupta, A. and Raftery, A. E. </author> <year> (1998). </year> <title> Detecting Features in Spatial Point Processes with Clutter via Model-Based Clustering. </title> <journal> Journal of the American Statistical Association. </journal> <note> (to appear). </note>
Reference-contexts: We are mainly concerned with the case where f k (x i j k ) is multivariate normal (Gaussian), a model that has been used with considerable success in a number of applications [1], [2], [5], <ref> [3] </ref>, [4], [6]. <p> The BIC can be used to compare models with differing parameterizations, differing numbers of components, or both. Although the regularity conditions for BIC do not hold for mixture models, there is considerable theoretical and practical support for its use in this context [25], [26], <ref> [3] </ref>, [4], [6]. <p> The EM algorithm can refine parititions when started sufficiently close to the optimal value. Dasgupta and Raftery <ref> [3] </ref> were able to obtain good results in a number of examples by using the partitions produced by model-based hierarchical agglomeration as starting values for an EM algorithm for constant-shape Gaussian models, together with the BIC to determine the number of clusters. <p> 0:33 0:020 0:0060 0:0064 0:0067 10 7 10 7 10 32 10 32 k 0:0025 0:0048 0:0044 0:0049 0:0072 0:0070 0:0070 0:0017 0:0024 k 0:0025 0:0035 0:0070 0:0065 0:0065 0:0063 0:0046 0:0039 0:0027 3.2 Minefield Detection in the Presence of Noise minefield data (Muise and Smith [32] | see also <ref> [3] </ref>). The data arise from the processing of a series of images taken by a reconnaissance aircraft in which a large number of points are identified as representing possible mines, but many of these are in fact false positives (noise). <p> It should be noted that the method is sensitive to the value of V , the assumed volume of the data region. Here it is clear that V is the area of the image; Banfield and Raftery [2] and Dasgupta and Raftery <ref> [3] </ref> similarly used the volume of the smallest hyperrectangle with sides parallel to the axes that contains all the data points. However, this value could overestimate V in many cases.
Reference: [4] <author> Campbell, J. G., Fraley, C., Murtagh, F., and Raftery, A. E. </author> <year> (1998). </year> <title> Linear Flaw Detection in Woven Textiles using Model-Based Clustering. </title> <journal> Pattern Recognition Letters. </journal> <note> (to appear). </note>
Reference-contexts: We are mainly concerned with the case where f k (x i j k ) is multivariate normal (Gaussian), a model that has been used with considerable success in a number of applications [1], [2], [5], [3], <ref> [4] </ref>, [6]. <p> The BIC can be used to compare models with differing parameterizations, differing numbers of components, or both. Although the regularity conditions for BIC do not hold for mixture models, there is considerable theoretical and practical support for its use in this context [25], [26], [3], <ref> [4] </ref>, [6].
Reference: [5] <author> Celeux, G. and Govaert, G. </author> <year> (1995). </year> <title> Gaussian parsimonious clustering models. </title> <journal> Pattern Recognition, </journal> <volume> 28, </volume> <pages> 781-793. </pages>
Reference-contexts: We are mainly concerned with the case where f k (x i j k ) is multivariate normal (Gaussian), a model that has been used with considerable success in a number of applications [1], [2], <ref> [5] </ref>, [3], [4], [6]. <p> Table 1 shows the geometric interpretation of the various parameterizations discussed in [2]. A more extensive set of models within the same framework is treated in <ref> [5] </ref>. Table 1: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], [5] k I Spherical variable fixed NA [2], <p> within the same framework is treated in <ref> [5] </ref>. Table 1: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed <p> The models shown here are those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], <ref> [5] </ref> k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be <p> The models shown here are those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], <ref> [5] </ref> k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], <p> in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], <ref> [5] </ref> k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], [2]. <p> I Spherical fixed fixed NA [15], [1], [2], <ref> [5] </ref> k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], [2]. At each stage, a pair of clusters is merged so as to maximize the resulting likelihood. <p> Spherical variable fixed NA [2], <ref> [5] </ref> DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], [2]. At each stage, a pair of clusters is merged so as to maximize the resulting likelihood. <p> Celeux and Govaert <ref> [5] </ref> detail both the E and M steps for the case of multivariate normal mixture models parameterized via the eigenvalue decomposition in (3). Under mild conditions, the iteration converges to a local maximum of the loglikelihood (Wu [20]).
Reference: [6] <author> Mukerjee, S., Feigelson, E. D., Babu, G. J., Murtagh, F., Fraley, C., and Raftery, A. </author> <year> (1998). </year> <title> Three Types of Gamma Ray Bursts. </title> <type> Technical report, </type> <institution> Pennsylvania State University, Department of Astronomy and Astrophysics. </institution>
Reference-contexts: We are mainly concerned with the case where f k (x i j k ) is multivariate normal (Gaussian), a model that has been used with considerable success in a number of applications [1], [2], [5], [3], [4], <ref> [6] </ref>. <p> The BIC can be used to compare models with differing parameterizations, differing numbers of components, or both. Although the regularity conditions for BIC do not hold for mixture models, there is considerable theoretical and practical support for its use in this context [25], [26], [3], [4], <ref> [6] </ref>.
Reference: [7] <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood for incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39, </volume> <pages> 341-353. </pages>
Reference-contexts: The most common relocation method | k-means (Hartigan and Wong [14]) | uses heuristics for reducing the within-group sums of squares. For clustering via mixture models, relocation techniques are usually based on the EM algorithm <ref> [7] </ref> (see section 2.3). 2.2 Probability Models for Cluster Analysis In model-based clustering, it is assumed that the data are generated by a mixture of underlying probability distributions; each component of the mixture represents a different group or cluster. <p> Fraley [18] developed efficient algorithms for hierarchical clustering with the various parameterizations (3) of Gaussian mixture models. 2.3 EM Algorithms for Clustering Iterative relocation methods for clustering via mixture models are possible through EM and related techniques [12]. The EM algorithm <ref> [7] </ref>, [19] is a general approach to maximum likelihood in the presence of missing data.
Reference: [8] <author> Reaven, G. M. and Miller, R. G. </author> <year> (1979). </year> <title> An attempt to define the nature of chemical diabetes using a multidimensional analysis. </title> <journal> Diabetologia, </journal> <volume> 16, </volume> <pages> 17-24. </pages>
Reference-contexts: introductions include Hartigan [9], Gordon [10], Murtagh [11], McLachlan and Basford [12], 1 <ref> [8] </ref> using single link or nearest neighbor, k-means, and the unconstrained model-based approach. Filled symbols represent misclassified observations. and Kaufman and Rouseeuw [13]. Clustering methods range from those that are largely heuristic to more formal procedures based on statistical models. <p> The BIC is then used to select the best model representing the data. 3 Examples 3.1 Diabetes Diagnosis In this section we illustrate the model-based approach to clustering using a three-dimensional data set involving 145 observations used for diabetes diagnosis (Reaven and Miller <ref> [8] </ref>). Figure 2 is a pairs plot showing the clinical classification, which partitions the data into three groups. The clusters are overlapping and far from spherical in shape. As a result, conventional clustering precedures do not work well for this application.
Reference: [9] <author> Hartigan, J. A. </author> <year> (1975). </year> <title> Clustering Algorithms. </title> <publisher> Wiley. </publisher>
Reference-contexts: introductions include Hartigan <ref> [9] </ref>, Gordon [10], Murtagh [11], McLachlan and Basford [12], 1 [8] using single link or nearest neighbor, k-means, and the unconstrained model-based approach. Filled symbols represent misclassified observations. and Kaufman and Rouseeuw [13]. Clustering methods range from those that are largely heuristic to more formal procedures based on statistical models.
Reference: [10] <author> Gordon, A. D. </author> <year> (1981). </year> <title> Classification: Methods for the Exploratory Analysis of Multi-variate Data. </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: introductions include Hartigan [9], Gordon <ref> [10] </ref>, Murtagh [11], McLachlan and Basford [12], 1 [8] using single link or nearest neighbor, k-means, and the unconstrained model-based approach. Filled symbols represent misclassified observations. and Kaufman and Rouseeuw [13]. Clustering methods range from those that are largely heuristic to more formal procedures based on statistical models.
Reference: [11] <author> Murtagh, F. </author> <year> (1985). </year> <title> Multidimensional Clustering Algorithms, </title> <booktitle> volume 4 of CompStat Lectures. </booktitle> <publisher> Physica-Verlag. </publisher>
Reference-contexts: introductions include Hartigan [9], Gordon [10], Murtagh <ref> [11] </ref>, McLachlan and Basford [12], 1 [8] using single link or nearest neighbor, k-means, and the unconstrained model-based approach. Filled symbols represent misclassified observations. and Kaufman and Rouseeuw [13]. Clustering methods range from those that are largely heuristic to more formal procedures based on statistical models.
Reference: [12] <author> McLachlan, G. J. and Basford, K. E. </author> <year> (1988). </year> <title> Mixture Models : Inference and Applications to Clustering. </title> <publisher> Marcel Dekker. </publisher>
Reference-contexts: introductions include Hartigan [9], Gordon [10], Murtagh [11], McLachlan and Basford <ref> [12] </ref>, 1 [8] using single link or nearest neighbor, k-means, and the unconstrained model-based approach. Filled symbols represent misclassified observations. and Kaufman and Rouseeuw [13]. Clustering methods range from those that are largely heuristic to more formal procedures based on statistical models. <p> Fraley [18] developed efficient algorithms for hierarchical clustering with the various parameterizations (3) of Gaussian mixture models. 2.3 EM Algorithms for Clustering Iterative relocation methods for clustering via mixture models are possible through EM and related techniques <ref> [12] </ref>. The EM algorithm [7], [19] is a general approach to maximum likelihood in the presence of missing data.
Reference: [13] <author> Kaufman, L. and Rousseeuw, P. J. </author> <year> (1990). </year> <title> Finding Groups in Data. </title> <publisher> Wiley. </publisher> <pages> 13 </pages>
Reference-contexts: introductions include Hartigan [9], Gordon [10], Murtagh [11], McLachlan and Basford [12], 1 [8] using single link or nearest neighbor, k-means, and the unconstrained model-based approach. Filled symbols represent misclassified observations. and Kaufman and Rouseeuw <ref> [13] </ref>. Clustering methods range from those that are largely heuristic to more formal procedures based on statistical models. They usually follow either a hierarchical strategy, or one in which observations are relocated among tentative clusters. <p> At each stage, the splitting or merging is chosen so as to optimize some criterion. Conven 2 tional agglomerative hierarchical methods use heuristic criteria (e.g. sum of squares, single and complete link (nearest and farthest neighbor)) <ref> [13] </ref>. A maximum-likelihood criterion is used for merging groups in model-based methods [1], [2]. Relocation methods move observations iteratively from one group to another, starting from an initial partition.
Reference: [14] <author> Hartigan, J. A. and Wong, M. A. </author> <year> (1979). </year> <title> Algorithm AS 136 : A K-means clustering algorithm. </title> <journal> Applied Statistics, </journal> <volume> 28, </volume> <pages> 100-108. </pages>
Reference-contexts: Relocation methods move observations iteratively from one group to another, starting from an initial partition. The number of groups has to be specified in advance and typically does not change during the course of the iteration, only their composition. The most common relocation method | k-means (Hartigan and Wong <ref> [14] </ref>) | uses heuristics for reducing the within-group sums of squares.
Reference: [15] <author> Ward, J. H. </author> <year> (1963). </year> <title> Hierarchical groupings to optimize an objective function. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58, </volume> <pages> 234-244. </pages>
Reference-contexts: This approach subsumes several earlier proposals based on Gaussian mixtures: k = I gives the sum of squares criterion, long known as a heuristic (Ward <ref> [15] </ref>), in which clusters are spherical and have equal volumes; k = = DAD T , in which all clusters have the same shape, volume and orientation (Friedman and Rubin [16]); unconstrained k , which is the most general model (Scott and Symons [17]); and k = D k AD k <p> Table 1: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA <ref> [15] </ref>, [1], [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k
Reference: [16] <author> Friedman, H. P. and Rubin, J. </author> <year> (1967). </year> <title> On some invariant criteria for grouping data. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 62, </volume> <pages> 1159-1178. </pages>
Reference-contexts: proposals based on Gaussian mixtures: k = I gives the sum of squares criterion, long known as a heuristic (Ward [15]), in which clusters are spherical and have equal volumes; k = = DAD T , in which all clusters have the same shape, volume and orientation (Friedman and Rubin <ref> [16] </ref>); unconstrained k , which is the most general model (Scott and Symons [17]); and k = D k AD k (Murtagh and Raftery [1]), in which only the orientations of the clusters may differ. Table 1 shows the geometric interpretation of the various parameterizations discussed in [2]. <p> The models shown here are those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed <ref> [16] </ref>, [17], [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative
Reference: [17] <author> Scott, A. J. and Symons, M. J. </author> <year> (1971). </year> <title> Clustering methods based on likelihood ratio criteria. </title> <journal> Biometrics, </journal> <volume> 27, </volume> <pages> 387-397. </pages>
Reference-contexts: criterion, long known as a heuristic (Ward [15]), in which clusters are spherical and have equal volumes; k = = DAD T , in which all clusters have the same shape, volume and orientation (Friedman and Rubin [16]); unconstrained k , which is the most general model (Scott and Symons <ref> [17] </ref>); and k = D k AD k (Murtagh and Raftery [1]), in which only the orientations of the clusters may differ. Table 1 shows the geometric interpretation of the various parameterizations discussed in [2]. A more extensive set of models within the same framework is treated in [5]. <p> The models shown here are those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], <ref> [17] </ref>, [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical <p> those discussed in Banfield and Raftery [2]. k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [15], [1], [2], [5] k I Spherical variable fixed NA [2], [5] DAD Elliptical fixed fixed fixed [16], <ref> [17] </ref>, [2], [5] k D k A k D k Elliptical variable variable variable [17], [2], [5] D k AD k Elliptical fixed fixed variable [1], [2], [5] k D k AD k Elliptical variable fixed variable [2], [5] The classification likelihood can be used as the basis for agglomerative hierarchical clustering [1], [2].
Reference: [18] <author> Fraley, C. </author> <year> (1998). </year> <title> Algorithms for Model-Based Gaussian Hierarchical Clustering. </title> <journal> SIAM Journal on Scientific Computing. </journal> <note> (to appear). </note>
Reference-contexts: At each stage, a pair of clusters is merged so as to maximize the resulting likelihood. Fraley <ref> [18] </ref> developed efficient algorithms for hierarchical clustering with the various parameterizations (3) of Gaussian mixture models. 2.3 EM Algorithms for Clustering Iterative relocation methods for clustering via mixture models are possible through EM and related techniques [12]. <p> with the following exceptions: the constant variance ( or DAD T ) and constant shape methods (D k AD T k and k D k AD T were started with the hierarchical clustering solution for the unconstrained classification likelihood since these models do not admit fast algorithms for hierarchical clustering <ref> [18] </ref>. Of note is that no values of the BIC are given in Figure 3 for the spherial, varying volumes model for 9 clusters and for the unconstrained model for 8 and 9 clusters. <p> A better solution might be to use the volume of the convex hull of the data, although this may not be practical to compute in higher dimensions. 4 Software Software implementing state-of-the-art algorithms for hierarchical clustering based on the the various parameterizations of Gaussian mixtures <ref> [18] </ref> through the internet | for details see http://www.stat.washington.edu/fraley/software.html. It is designed to interface with the commercial interactive software S-PLUS 2 . An earlier version is included in the S-PLUS package as the function mclust; an upgrade to the new version is planned for the next release.
Reference: [19] <author> McLachlan, G. J. and Krishnan, T. </author> <year> (1997). </year> <title> The EM Algorithm and Extensions. </title> <publisher> Wiley. </publisher>
Reference-contexts: Fraley [18] developed efficient algorithms for hierarchical clustering with the various parameterizations (3) of Gaussian mixture models. 2.3 EM Algorithms for Clustering Iterative relocation methods for clustering via mixture models are possible through EM and related techniques [12]. The EM algorithm [7], <ref> [19] </ref> is a general approach to maximum likelihood in the presence of missing data.
Reference: [20] <author> Wu, C. F. J. </author> <year> (1983). </year> <title> On convergence properties of the EM algorithm for Gaussian mixtures. </title> <journal> Annals of Statistics, </journal> <volume> 11, </volume> <pages> 95-103. </pages>
Reference-contexts: Celeux and Govaert [5] detail both the E and M steps for the case of multivariate normal mixture models parameterized via the eigenvalue decomposition in (3). Under mild conditions, the iteration converges to a local maximum of the loglikelihood (Wu <ref> [20] </ref>). The model and classification can be considered to be an accurate representation of the data if all of the z ik are close to either 0 or 1 at a local maximum.
Reference: [21] <author> Bensmail, H., Celeux, G., Raftery, A. E., and Robert, C. P. </author> <year> (1997). </year> <title> Inference in model-based cluster analysis. </title> <journal> Statistics and Computing, </journal> <volume> 7, </volume> <pages> 1-10. </pages>
Reference-contexts: Moreover, for each observation i, (1 max k z ik ) is a measure of uncertainty in the associated classification (Bensmail et al. <ref> [21] </ref>). The EM algorithm for clustering has a number of limitations. First, the asymptotic rate of convergence can be very slow. This does not appear to be a problem in practice for well-separated mixtures when started with reasonable values. <p> Principal curve clustering in the presence of noise using BIC is discussed in Stanford and Raftery [41]. In situations where the BIC is not definitive, more computationally intensive Bayesian analysis may provide a solution. Bensmail et al. <ref> [21] </ref> showed that exact Bayesian inference via Gibbs sampling, with calculations of Bayes factors using the Laplace-Metropolis estimator works well in several real and simulated examples.
Reference: [22] <author> Celeux, G. and Govaert, G. </author> <year> (1992). </year> <title> A Classification EM Algorithm for Clustering and two stochastic versions. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 14, </volume> <pages> 315-332. </pages>
Reference-contexts: A number of variants of the EM algorithm for clustering presented above have been studied. The classification EM or CEM algorithm (Celeux and Govaert <ref> [22] </ref>) converts the ^z ik from the E step to a discrete classification before performing the M-step. The standard k-means algorithm can be shown to be a version of the CEM algorithm corresponding to the uniform spherical Gaussian model k = I [22]. 2.4 Bayesian Model Selection in Clustering One advantage <p> classification EM or CEM algorithm (Celeux and Govaert <ref> [22] </ref>) converts the ^z ik from the E step to a discrete classification before performing the M-step. The standard k-means algorithm can be shown to be a version of the CEM algorithm corresponding to the uniform spherical Gaussian model k = I [22]. 2.4 Bayesian Model Selection in Clustering One advantage of the mixture-model approach to clustering is that it allows the use of approximate Bayes factors to compare models.
Reference: [23] <author> Kass, R. E. and Raftery, A. E. </author> <year> (1995). </year> <title> Bayes Factors. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 90, </volume> <pages> 773-795. </pages>
Reference-contexts: Before approximate Bayes factors were applied in this context, no systematic approach existed for determining both the intrinsic number of groups within a data set and the clustering method. For a recent review of Bayes factors emphasizing the underlying concepts and scientific applications, see Kass and Raftery <ref> [23] </ref>. The Bayes factor is the posterior odds for one model against the other assuming neither is favored a priori. <p> A standard convention for calibrating BIC differences is that differences of less than 2 correspond to weak evidence, differences between 2 and 6 to positive evidence, differences between 6 and 10 to strong evidence, and differences greater than 10 to very strong evidence (Jeffreys [27] | see also <ref> [23] </ref>). 2.5 Model-Based Strategy for Clustering In practice, agglomerative hierarchical clustering based on mixture models often gives good, but suboptimal partitions. The EM algorithm can refine parititions when started sufficiently close to the optimal value. <p> This gives a matrix of BIC values corresponding to each possible combination of parameterization and number of clusters. 1 Kass and Raftery <ref> [23] </ref> and other writers define BIC as minus the value given here, in which case the smaller (more negative) the BIC, the stronger the evidence for the model.
Reference: [24] <author> Schwarz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6, </volume> <pages> 461-464. </pages>
Reference-contexts: When EM is used to find the maximum mixture likelihood, a more reliable approximation to twice the log Bayes factor called the 5 Bayesian Information Criterion or `BIC' (Schwarz <ref> [24] </ref>) is applicable: 2 log p (x j M) + const: 2l M (x; ^ ) m M log (n) BIC; where p (x j M) is the (integrated) loglikelihood of the data for the model M, l M (x; ^ ) is the maximized mixture likelihood for the model, and
Reference: [25] <author> Leroux, M. </author> <year> (1992). </year> <title> Consistent Estimation of a Mixing Distribution. </title> <journal> The Annals of Statistics, </journal> <volume> 20, </volume> <pages> 1350-1360. </pages>
Reference-contexts: The BIC can be used to compare models with differing parameterizations, differing numbers of components, or both. Although the regularity conditions for BIC do not hold for mixture models, there is considerable theoretical and practical support for its use in this context <ref> [25] </ref>, [26], [3], [4], [6].
Reference: [26] <author> Roeder, K. and Wasserman, L. </author> <year> (1997). </year> <title> Practical Bayesian Density Estimation using Mixtures of Normals. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 92, </volume> <pages> 894-902. </pages>
Reference-contexts: The BIC can be used to compare models with differing parameterizations, differing numbers of components, or both. Although the regularity conditions for BIC do not hold for mixture models, there is considerable theoretical and practical support for its use in this context [25], <ref> [26] </ref>, [3], [4], [6].
Reference: [27] <author> Jeffreys, H. </author> <year> (1961). </year> <title> Theory of Probability. </title> <publisher> Clarendon, 3rd edition. </publisher>
Reference-contexts: A standard convention for calibrating BIC differences is that differences of less than 2 correspond to weak evidence, differences between 2 and 6 to positive evidence, differences between 6 and 10 to strong evidence, and differences greater than 10 to very strong evidence (Jeffreys <ref> [27] </ref> | see also [23]). 2.5 Model-Based Strategy for Clustering In practice, agglomerative hierarchical clustering based on mixture models often gives good, but suboptimal partitions. The EM algorithm can refine parititions when started sufficiently close to the optimal value.
Reference: [28] <author> Allard, D. and Fraley, C. </author> <year> (1997). </year> <title> Nonparametric maximum likelihood estimation of features in spatial point processes using Vorono tessellation. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 92, </volume> <pages> 1485-1493. </pages>
Reference-contexts: An observation contributes 1=V if it belongs to the noise; otherwise it contributes a Gaussian term. The basic model-based procedure for noisy data is as follows. First, it is necessary to obtain an initial estimate of the noise. Possible approaches to denoising include the method of Allard and Fraley <ref> [28] </ref> which uses Vorono tessellation, and the nearest-neighbor method of Byers and Raftery [29]. Next, hierarchical clustering is applied to the denoised data.
Reference: [29] <author> Byers, S. and Raftery, A. E. </author> <year> (1998). </year> <title> Nearest Neighbor Clutter Removal for Estimating Features in Spatial Point Processes. </title> <journal> Journal of the American Statistical Association. </journal> <note> (to appear). </note>
Reference-contexts: The basic model-based procedure for noisy data is as follows. First, it is necessary to obtain an initial estimate of the noise. Possible approaches to denoising include the method of Allard and Fraley [28] which uses Vorono tessellation, and the nearest-neighbor method of Byers and Raftery <ref> [29] </ref>. Next, hierarchical clustering is applied to the denoised data. In a final step, EM based on the augmented model (6) is applied to the entire data set with the Gaussian components initialized with the hierarchical clustering partitions, and the noise component initialized with the result of the denoising procedure. <p> The goals are to determine whether the image contains one or more minefields, and to give the location of any minefields that may be present. The initial denoising for Figure 4 was carried out using the NNclean procedure for nearest-neighbor denoising <ref> [29] </ref>. The BIC is clearly maximized at a value of 3 (2 clusters plus noise), and favors the constant-shape, equal-volume model. The two clusters together give an accurate reconstruction of the actual minefield.
Reference: [30] <author> Ripley, B. D. </author> <year> (1994). </year> <title> Neural Networks and Related Methods for Classification. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 56, </volume> <pages> 409-456. 14 </pages>
Reference-contexts: Nearest-neighbor classification assigns a data point to the same group as the point in the training set nearest to it. It often works very well (e.g. Ripley <ref> [30] </ref>), but its success depends entirely on the available training set. equal and varying volumes, constant variance, unconstrained variance, and constant shape models with equal and varying volumes). The first local maximum (in this case also the methods applied to the diabetes data.
Reference: [31] <author> Golub, G. H. and Van Loan, C. F. </author> <year> (1996). </year> <title> Matrix Computations. </title> <publisher> Johns Hopkins, 3rd edition. </publisher>
Reference-contexts: The algorithms used for EM and for computing BIC monitor an estimate of the reciprocal condition number (smallest to 9 largest eigenvalue ratio) of the covariances. This latter quantity falls in the range [0; 1], and values near zero imply ill-conditioning <ref> [31] </ref>. Computations are less reliable for ill-conditioned problems, and as a consequence may cause anomalies before reaching the point of actual failure.
Reference: [32] <author> Muise, R. and Smith, C. </author> <year> (1991). </year> <title> Nonparametric Minefield Detection and Localization. </title> <type> Technical Report CSS-TM-591-91, </type> <institution> Coastal Systems Station, </institution> <address> Panama City, Florida. </address>
Reference-contexts: 1:0 1:0 0:0 k 0:33 0:020 0:0060 0:0064 0:0067 10 7 10 7 10 32 10 32 k 0:0025 0:0048 0:0044 0:0049 0:0072 0:0070 0:0070 0:0017 0:0024 k 0:0025 0:0035 0:0070 0:0065 0:0065 0:0063 0:0046 0:0039 0:0027 3.2 Minefield Detection in the Presence of Noise minefield data (Muise and Smith <ref> [32] </ref> | see also [3]). The data arise from the processing of a series of images taken by a reconnaissance aircraft in which a large number of points are identified as representing possible mines, but many of these are in fact false positives (noise).
Reference: [33] <author> Bensmail, H. and Celeux, G. </author> <year> (1996). </year> <title> Regularized Gaussian discriminant analysis through eigenvalue decomposition. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 91, </volume> <pages> 1743-1748. </pages>
Reference-contexts: One way around this is to determine the structure of some subset of the data according to the strategy given here, and classify the remaining observations via supervised classification or discriminant analysis [2]. Bensmail and Celeux <ref> [33] </ref> have developed a method for regularized discriminant analysis based on the full range of parameterizations of Gaussian mixtures (3). Alternatively, fast methods for determining an initial coarse partition can be used to reduce computational requirements.
Reference: [34] <author> Posse, C. </author> <year> (1998). </year> <title> Hierarchical model-based clustering for large data sets. </title> <type> Technical report, </type> <institution> University of Minnesota, School of Statistics. </institution> <note> (in preparation). </note>
Reference-contexts: Bensmail and Celeux [33] have developed a method for regularized discriminant analysis based on the full range of parameterizations of Gaussian mixtures (3). Alternatively, fast methods for determining an initial coarse partition can be used to reduce computational requirements. Posse <ref> [34] </ref> suggested the minimum spanning tree for this purpose, and has shown that it works well in practice.
Reference: [35] <author> Hastie, T. and Stuetzle, W. </author> <year> (1989). </year> <title> Principal Curves. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84, </volume> <pages> 502-516. </pages>
Reference-contexts: This example also suggests that nonlinear features can be well represented in the present framework as piecewise linear features, using several groups. When features are strongly curvilinear, curves about which groups are centered can be modeled using principal curves (Hastie and Stuezle <ref> [35] </ref>). Clustering about principal curves has been successfully applied to automatic identifcation of ice-floe contours [36], [37], tracking of ice floes [38], and modeling ice-floe leads [39]. Initial estimation of ice-floe outlines is accomplished by means of mathematical morphology (e.g. [40]).
Reference: [36] <author> Banfield, J. D. and Raftery, A. E. </author> <year> (1992). </year> <title> Ice Floe Identification in Satellite Images using Mathematical Morphology and Clustering about Principle Curves. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 87, </volume> <pages> 7-16. </pages>
Reference-contexts: When features are strongly curvilinear, curves about which groups are centered can be modeled using principal curves (Hastie and Stuezle [35]). Clustering about principal curves has been successfully applied to automatic identifcation of ice-floe contours <ref> [36] </ref>, [37], tracking of ice floes [38], and modeling ice-floe leads [39]. Initial estimation of ice-floe outlines is accomplished by means of mathematical morphology (e.g. [40]). Principal curve clustering in the presence of noise using BIC is discussed in Stanford and Raftery [41].
Reference: [37] <author> Banfield, J. D. and Raftery, A. E. </author> <year> (1992). </year> <title> Identifying Ice Floes in Satellite Images. </title> <journal> Naval Research Reviews, </journal> <volume> 43, </volume> <pages> 2-18. </pages>
Reference-contexts: When features are strongly curvilinear, curves about which groups are centered can be modeled using principal curves (Hastie and Stuezle [35]). Clustering about principal curves has been successfully applied to automatic identifcation of ice-floe contours [36], <ref> [37] </ref>, tracking of ice floes [38], and modeling ice-floe leads [39]. Initial estimation of ice-floe outlines is accomplished by means of mathematical morphology (e.g. [40]). Principal curve clustering in the presence of noise using BIC is discussed in Stanford and Raftery [41].
Reference: [38] <author> Banfield, J. D. </author> <year> (1991). </year> <title> Automated Tracking of Ice Floes : A Statistical Approach. </title> <journal> IEEE Transactions on Geoscience and Remote Sensing, </journal> <volume> 29, </volume> <pages> 905-911. </pages>
Reference-contexts: When features are strongly curvilinear, curves about which groups are centered can be modeled using principal curves (Hastie and Stuezle [35]). Clustering about principal curves has been successfully applied to automatic identifcation of ice-floe contours [36], [37], tracking of ice floes <ref> [38] </ref>, and modeling ice-floe leads [39]. Initial estimation of ice-floe outlines is accomplished by means of mathematical morphology (e.g. [40]). Principal curve clustering in the presence of noise using BIC is discussed in Stanford and Raftery [41].
Reference: [39] <author> Banfield, J. D. </author> <year> (1992). </year> <title> Skeletal Modeling of Ice Leads. </title> <journal> IEEE Transactions on Geoscience and Remote Sensing, </journal> <volume> 30, </volume> <pages> 918-923. </pages>
Reference-contexts: When features are strongly curvilinear, curves about which groups are centered can be modeled using principal curves (Hastie and Stuezle [35]). Clustering about principal curves has been successfully applied to automatic identifcation of ice-floe contours [36], [37], tracking of ice floes [38], and modeling ice-floe leads <ref> [39] </ref>. Initial estimation of ice-floe outlines is accomplished by means of mathematical morphology (e.g. [40]). Principal curve clustering in the presence of noise using BIC is discussed in Stanford and Raftery [41]. In situations where the BIC is not definitive, more computationally intensive Bayesian analysis may provide a solution.
Reference: [40] <author> Heijmans, H. J. A. M. </author> <year> (1995). </year> <title> Mathematical Morphology: A Modern Approach in Image Processing Based on Algebra and Geometry. </title> <journal> SIAM Review, </journal> <volume> 37, </volume> <pages> 1-36. </pages>
Reference-contexts: Clustering about principal curves has been successfully applied to automatic identifcation of ice-floe contours [36], [37], tracking of ice floes [38], and modeling ice-floe leads [39]. Initial estimation of ice-floe outlines is accomplished by means of mathematical morphology (e.g. <ref> [40] </ref>). Principal curve clustering in the presence of noise using BIC is discussed in Stanford and Raftery [41]. In situations where the BIC is not definitive, more computationally intensive Bayesian analysis may provide a solution.
Reference: [41] <author> Stanford, D. and Raftery, A. E. </author> <year> (1997). </year> <title> Principal Curve Clustering with Noise. </title> <type> Technical Report 317, </type> <institution> University of Washington, Department of Statistics. </institution>
Reference-contexts: Initial estimation of ice-floe outlines is accomplished by means of mathematical morphology (e.g. [40]). Principal curve clustering in the presence of noise using BIC is discussed in Stanford and Raftery <ref> [41] </ref>. In situations where the BIC is not definitive, more computationally intensive Bayesian analysis may provide a solution. Bensmail et al. [21] showed that exact Bayesian inference via Gibbs sampling, with calculations of Bayes factors using the Laplace-Metropolis estimator works well in several real and simulated examples.
Reference: [42] <author> Cheeseman, P. and Stutz, J. </author> <year> (1995). </year> <title> Bayesian Classification (AutoClass): Theory and Results. </title> <editor> In Fayyad, U., Piatesky-Shapiro, G., Smyth, P., and Uthurusamy, R., editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Bensmail et al. [21] showed that exact Bayesian inference via Gibbs sampling, with calculations of Bayes factors using the Laplace-Metropolis estimator works well in several real and simulated examples. Other model-based clustering methodologies include Cheeseman and Stutz <ref> [42, 43] </ref>, implemented in the AutoClass software, and McLachlan et al. [44], implemented in the MIXFIT software. Both rely on the EM algorithm with the unconstrained multivariate normal distribution as a model for each cluster rather than the larger class of models considered here.
Reference: [43] <author> Stutz, J. and Cheeseman, P. </author> <year> (1995). </year> <title> AutoClass A BayesiaB Approach to Classification. </title> <editor> In Skilling, J. and Sibisi, S., editors, </editor> <title> Maximum Entropy and bayesian Methods, </title> <address> Cambridge 1994. </address> <publisher> Kluwer. </publisher>
Reference-contexts: Bensmail et al. [21] showed that exact Bayesian inference via Gibbs sampling, with calculations of Bayes factors using the Laplace-Metropolis estimator works well in several real and simulated examples. Other model-based clustering methodologies include Cheeseman and Stutz <ref> [42, 43] </ref>, implemented in the AutoClass software, and McLachlan et al. [44], implemented in the MIXFIT software. Both rely on the EM algorithm with the unconstrained multivariate normal distribution as a model for each cluster rather than the larger class of models considered here.
Reference: [44] <author> McLachlan, G. J., Peel, D., Basford, K. E., and Adams, P. </author> <year> (1997). </year> <note> User's Guide to MIXFIT Version 1.1. </note> <institution> University of Queensland. </institution>
Reference-contexts: Bensmail et al. [21] showed that exact Bayesian inference via Gibbs sampling, with calculations of Bayes factors using the Laplace-Metropolis estimator works well in several real and simulated examples. Other model-based clustering methodologies include Cheeseman and Stutz [42, 43], implemented in the AutoClass software, and McLachlan et al. <ref> [44] </ref>, implemented in the MIXFIT software. Both rely on the EM algorithm with the unconstrained multivariate normal distribution as a model for each cluster rather than the larger class of models considered here. Neither has a provision for modeling noise or outliers.
Reference: [45] <author> Chickering, D. M. and Heckerman, D. </author> <year> (1996). </year> <title> Efficient Approximations for the Marginal Likelihood of Bayesian Networks with Hidden Variables. </title> <type> Technical Report MSR-TR-96-08, </type> <institution> Microsoft Research. </institution> <month> 15 </month>
Reference-contexts: Neither has a provision for modeling noise or outliers. As in our approach, AutoClass uses 12 approximate Bayes factors to choose the number of clusters (see also Chickering and Hecker--man <ref> [45] </ref>), although their approximation differs from the BIC. MIXFIT determines the number of clusters by resampling. Options for initializing EM in MIXFIT include the most common heuristic hierarchical clustering methods, as well as k-means, whereas we use the hierarchical clustering solution for the appropriate Gaussian mixture model as an initial value.
References-found: 45

