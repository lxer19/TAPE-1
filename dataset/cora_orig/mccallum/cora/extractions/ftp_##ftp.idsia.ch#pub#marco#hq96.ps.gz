URL: ftp://ftp.idsia.ch/pub/marco/hq96.ps.gz
Refering-URL: http://www.idsia.ch/reports.html
Root-URL: 
Title: HQ-LEARNING: DISCOVERING MARKOVIAN SUBGOALS FOR NON-MARKOVIAN REINFORCEMENT LEARNING  
Author: Marco Wiering Jurgen Schmidhuber 
Keyword: reinforcement learning, hierarchical Q-learning, POMDPs, non-Markovian interfaces, subgoal learning.  
Address: Corso Elvezia 36, CH-6900-Lugano, Switzerland  
Affiliation: IDSIA,  
Pubnum: Technical Report IDSIA-95-96  
Email: marco,juergen@idsia.ch  
Web: http://www.idsia.ch  
Date: October 9, 1996  
Abstract: To solve partially observable Markov decision problems, we introduce HQ-learning, a hierarchical extension of Q-learning. HQ-learning is based on an ordered sequence of subagents, each learning to identify and solve a Markovian subtask of the total task. Each agent learns (1) an appropriate subgoal (though there is no intermediate, external reinforcement for "good" subgoals), and (2) a Markovian policy, given a particular subgoal. Our experiments demonstrate: (a) The system can easily solve tasks standard Q-learning cannot solve at all. (b) It can solve partially observable mazes with more states than those used in most previous POMDP work. (c) It can quickly solve complex tasks that require manipulation of the environment to free a blocked path to the goal.
Abstract-found: 1
Intro-found: 1
Reference: <author> Caironi, P. V. C. and Dorigo, M. </author> <year> (1994). </year> <title> Training Q-agents. </title> <type> Technical Report IRIDIA-94-14, </type> <institution> Universite Libre de Bruxelles. </institution>
Reference: <author> Cliff, D. and Ross, S. </author> <year> (1994). </year> <title> Adding temporary memory to ZCS. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 3 </volume> <pages> 101-150. </pages>
Reference: <author> Cohn, D. A. </author> <year> (1994). </year> <title> Neural network exploration using optimal experiment design. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 679-686. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dayan, P. and Hinton, G. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 271-278. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Digney, B. </author> <year> (1996). </year> <title> Emergent hierarchical control structures: Learning reactive/hierarchical relationships in reinforcement environments. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> &gt;From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 363-372. </pages> <publisher> MIT Press, Bradford Books. </publisher>
Reference: <author> Fedorov, V. V. </author> <year> (1972). </year> <title> Theory of optimal experiments. </title> <publisher> Academic Press. 11 Hochreiter, </publisher> <editor> S. and Schmidhuber, J. </editor> <year> (1996). </year> <title> Long short term memory. </title> <note> Submitted to Neural Com--putation. </note>
Reference: <author> Humphrys, M. </author> <year> (1996). </year> <title> Action selection methods using reinforcement learning. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> &gt;From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 135-144. </pages> <publisher> MIT Press, Bradford Books. </publisher>
Reference: <author> Jaakkola, T., Singh, S. P., and Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 345-352. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Jordan, M. I. and Rumelhart, D. E. </author> <year> (1990). </year> <title> Supervised learning with a distal teacher. </title> <type> Technical Report Occasional Paper #40, </type> <institution> Center for Cog. Sci., Massachusetts Institute of Technology. </institution>
Reference: <author> Kaelbling, L., Littman, M., and Cassandra, A. </author> <year> (1995). </year> <title> Planning and acting in partially observable stochastic domains. </title> <type> Technical report, </type> <institution> Brown University, Providence RI. </institution>
Reference-contexts: Eventually the memory bit won't remain switched off. Q-learning, for example, tends 9 to fail to reliably set memory bits because learning the Q-values for changing a bit depends on luck <ref> (L. Kaelbling, personal communication, 1995) </ref>. HQ-learning, however, does not depend on long traces of memory bit resets. Its memory is embodied solely in the active agent number, which is rarely incremented during a trial. This makes it much more stable. Multiple Q-learners.
Reference: <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference-contexts: Both identifier and controller are adaptive. One limitation of the method is that the agent must have access to the external Markov model; this is not necessary for HQ-learning. Levin Search. Wiering and Schmidhuber (1996) use Levin search (LS) through program space <ref> (Levin 1973) </ref> to discover programs computing solutions for large POMDPs. LS is of interest because of its amazing theoretical properties: for a broad class of search problems, it has the optimal order of computational complexity.
Reference: <author> Lin, L. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh. </institution>
Reference: <author> Littman, M. </author> <year> (1994). </year> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In D. Cliff, P. Husbands, J. A. M. and Wilson, S. W., editors, </editor> <booktitle> Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats 3, </booktitle> <pages> pages 297-305. </pages> <publisher> MIT Press/Bradford Books. </publisher>
Reference: <author> Littman, M., Cassandra, A., and Kaelbling, L. </author> <year> (1995). </year> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 362-370. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Section 4 briefly reviews related work that has mainly been tested on small problems, as most previous methods do not scale up very well <ref> (Littman 1995) </ref>. Section 5 concludes and lists directions for future research. 2 HQ-learning POMDP specification. System life is separable into "trials". A trial consists of at most T max discrete time steps t = 1; 2; 3; : : :,T.
Reference: <author> McCallum, R. A. </author> <year> (1993). </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Amherst, MA. </address>
Reference: <author> McCallum, R. A. </author> <year> (1996). </year> <title> Learning to use selective attention and short-term memory in sequential tasks. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> &gt;From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 315-324. </pages> <publisher> MIT Press, Bradford Books. </publisher>
Reference: <author> Moore, A. and Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130. </pages>
Reference: <author> Nguyen and Widrow, B. </author> <year> (1989). </year> <title> The truck backer-upper: An example of self learning in neural networks. </title> <booktitle> In IEEE/INNS International Joint Conference on Neural Networks, Washington, D.C., </booktitle> <volume> volume 1, </volume> <pages> pages 357-364. </pages>
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas 78712. </institution>
Reference: <author> Schmidhuber, J. </author> <year> (1991a). </year> <title> Curious model-building control systems. </title> <booktitle> In Proc. International Joint Conference on Neural Networks, Singapore, </booktitle> <volume> volume 2, </volume> <pages> pages 1458-1463. </pages> <publisher> IEEE. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1991b). </year> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. 12 Schmidhuber, </publisher> <editor> J., Zhao, J., and Wiering, M. </editor> <year> (1996). </year> <title> Simple principles of metalearning. </title> <type> Technical Report IDSIA-69-96, </type> <institution> IDSIA. </institution>
Reference: <author> Schraudolph, N. N., Dayan, P., and Sejnowski, T. J. </author> <year> (1994). </year> <title> Temporal difference learning of position evaluation in the game of go. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 817-824. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco. </address>
Reference: <author> Singh, S. </author> <year> (1992). </year> <title> The efficient learning of multiple task sequences. </title> <editor> In Moody, J., Hanson, S., and Lippman, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 251-258, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Storck, J., Hochreiter, S., and Schmidhuber, J. </author> <year> (1995). </year> <title> Reinforcement driven information acquisition in non-deterministic environments. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Paris, </booktitle> <volume> volume 2, </volume> <pages> pages 159-164. </pages> <address> EC2 & Cie, Paris. </address>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: 1 Introduction The problem. If a learner's optimal next action always depends only on its current input, we speak of a Markovian interface between learner and environment. The most widely used reinforcement learning (RL) algorithms, such as TD () <ref> (Sutton 1988) </ref> and Q-learning (Watkins 1989; Watkins and Dayan 1992), depend on Markovian interfaces; they fail if the problem requires memory of previous events.
Reference: <author> Tham, C. </author> <year> (1995). </year> <title> Reinforcement learning of multiple tasks using a hierarchical CMAC architecture. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 15(4) </volume> <pages> 247-274. </pages>
Reference: <author> Thrun, S. </author> <year> (1992). </year> <title> Efficient exploration in reinforcement learning. </title> <type> Technical Report CMU-CS-92-102, </type> <institution> Carnegie-Mellon University. </institution>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College London. </institution>
Reference: <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292. </pages>
Reference: <author> Whitehead, S. </author> <year> (1992). </year> <title> Reinforcement Learning for the adaptive control of perception and action. </title> <type> PhD thesis, </type> <institution> University of Rochester. </institution>
Reference: <author> Wiering, M. and Schmidhuber, J. </author> <year> (1996). </year> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 534-542. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Wilson, S. </author> <year> (1996a). </year> <title> Explore/exploit strategies in autonomy. </title> <editor> In Meyer, J. A. and Wilson, S. W., editors, </editor> <booktitle> Proc. of the Fourth International Conference on Simulation of Adaptive Behavior: From Animals to Animats 4, </booktitle> <pages> pages 325-332. </pages> <publisher> MIT Press/Bradford Books. </publisher>
Reference: <author> Wilson, S. </author> <year> (1996b). </year> <title> Generalization in XCS. </title> <booktitle> In ICML'96 Workshop on Evolutionary Computing and Machine Learning. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Zhao, J. and Schmidhuber, J. </author> <year> (1996). </year> <title> Incremental self-improvement for life-time multi-agent reinforcement learning. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 516-525. </pages> <publisher> MIT Press, Bradford Books. </publisher> <pages> 13 </pages>
References-found: 34

