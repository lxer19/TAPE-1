URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P477.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: Massively Parallel Self-Consistent-Field Calculations  
Author: Jeffrey L. Tilson 
Date: October 29, 1994  
Address: Argonne, IL 60439  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: The advent of supercomputers with many computational nodes each with its own independent memory makes possible extremely fast computations. Our work, as part of the U.S. High Performance Computing and Communications Program (HPCCP), is focused on the development of electronic structure techniques for the solution of Grand Challenge-size molecules containing hundreds of atoms. Our efforts have resulted in a fully scalable Direct-SCF program that is portable and efficient. This code, named NWCHEM, is built around a distributed-data model. This distributed data is managed by a software package called Global Arrays developed within the HPCCP. We present performance results for Direct-SCF calculations of interest to the consortium. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Roothaan, </author> <title> C., </title> <journal> Reviews of Modern Physics, </journal> <volume> 23, 69, </volume> <year> 1951. </year>
Reference-contexts: This collaboration has operated through the Department of Energy under the auspices of the High Performance Computing and Communications Initiative. This paper reports the results of one particular MPP adaptation, that of the self-consistent field (SCF) <ref> [1, 2] </ref> electronic structure method. The results to date suggest that efficient coding for MPP technology can qualitatively change the size of the molecule that can be treated by the SCF method. <p> F AO 1 X i ae g AO j ae (8) where the integrals are now over the AO functions. 4 The optimized MOs are determined by finding the optimal coefficients, C, that satisfy Roothaans <ref> [1] </ref> nonorthogonal matrix eigenvalue problem F C = SC*: (10) The eigenvalues, *, can be interpreted as the set of individual electron energies. In the limit of a complete basis, the true Hartree-Fock limit is attained. Equations 2-10 give us a prescription for solving the SCF problem. 1.
Reference: [2] <author> Almlof, J., Faegri, K., and Korsell, K., J. </author> <title> Comp. </title> <journal> Chem, </journal> <volume> 3, 385, </volume> <year> 1982. </year>
Reference-contexts: This collaboration has operated through the Department of Energy under the auspices of the High Performance Computing and Communications Initiative. This paper reports the results of one particular MPP adaptation, that of the self-consistent field (SCF) <ref> [1, 2] </ref> electronic structure method. The results to date suggest that efficient coding for MPP technology can qualitatively change the size of the molecule that can be treated by the SCF method.
Reference: [3] <author> Harrison, R. J., and Shepard, R., </author> <title> Annual Review of Physical Chemistry, </title> <note> to appear 1994. </note>
Reference-contexts: The SCF approach is also typical of other more sophisticated methods in its use of large data structures and irregular data access patterns. Because of its importance, others have developed various parallel MPP SCF codes (see <ref> [3] </ref> and references therein). However, the code reported here is (in our opinion) the most scalable SCF code currently available. 2 SCF Wavefunctions The SCF wavefunction is constructed from an antisymmetrized product of single particle functions. This is the molecular orbital (MO) approximation. <p> The diagonalization step takes on a much greater importance in a parallel environment, often becoming the computational bottleneck. 3 Parallel SCF In this section we describe our fully scalable SCF program named NWCHEM. A description of NWCHEM is available in a recent review <ref> [3] </ref> on parallel SCF programs and algorithms. Here we summarize the important points of our scalable distributed-data SCF algorithm. A fully scalable, parallel direct-SCF algorithm must parallelize both the AO F construction and diagonalization steps.
Reference: [4] <author> Foster, I., T., Tilson, J., L., Shepard, R. L., Wagner, A. F., Harrison, R. J., Kendall, R. A., Littlefield, R. L. </author> <note> submitted J. Comp. </note> <institution> Chem., </institution> <year> 1994. </year>
Reference-contexts: This technique, unfortunately, shifts the computational bottleneck from the highly parallel integral generation step to the diagonalization and is limited by the amount of memory on one node. 3.2 Distributed-Data Model Several models of scalable Fock matrix construction algorithms have been previously analyzed <ref> [4] </ref>. The resulting program has been thoroughly discussed in [5]. We summarize the important parallel details here. To develop a fully scalable parallel SCF program requires efficiently distributing matrix data throughout the aggregate memory of the parallel computer. This process eliminates the memory restrictions of the replicated-data model algorithm.
Reference: [5] <author> Harrison, R. J., Guest, M. F., Kendall, R. A., Bernholdt, D. E., Wong, A. T., Stave, M., Anchell, J., Hess, A. C., Littlefield, R. L., Fann, G. L., Nieplocha, J., Thomas, G. S., Elwood, D., Tilson, J., Shepard, R. L., Wagner, A. F., Foster, I., T., Lusk, E., and Stevens, R. </author> <note> submitted J. Comp. Chem., 1994. 10 </note>
Reference-contexts: The resulting program has been thoroughly discussed in <ref> [5] </ref>. We summarize the important parallel details here. To develop a fully scalable parallel SCF program requires efficiently distributing matrix data throughout the aggregate memory of the parallel computer. This process eliminates the memory restrictions of the replicated-data model algorithm.
Reference: [6] <author> Harrison, R., Theo. </author> <title> Chim. </title> <journal> Acta., </journal> <volume> 84, 363, </volume> <year> 1993. </year>
Reference-contexts: The simplicity of this algorithm is complicated by the varying data requirements for different integral blocks. Our algorithm performs these communications with a library of routines called Global Arrays. These Global Arrays support a lightweight one-sided communications model, thereby greatly simplifying development of our scalable program <ref> [6, 7] </ref>. The scalable construction of the F matrix requires that the integrals be allocated dynamically and that nonlocal data requirements be satisfied without unduly synchronizing the computational progress. These requirements are difficult to satisfy by using a traditional point-to-point communications scheme.
Reference: [7] <author> Nieplocha, J., Harrison, R., J., Littlefield, R., J. </author> <title> For submission to Supercomputing 1994, </title> <year> 1994 </year>
Reference-contexts: The simplicity of this algorithm is complicated by the varying data requirements for different integral blocks. Our algorithm performs these communications with a library of routines called Global Arrays. These Global Arrays support a lightweight one-sided communications model, thereby greatly simplifying development of our scalable program <ref> [6, 7] </ref>. The scalable construction of the F matrix requires that the integrals be allocated dynamically and that nonlocal data requirements be satisfied without unduly synchronizing the computational progress. These requirements are difficult to satisfy by using a traditional point-to-point communications scheme.
Reference: [8] <author> Littlefield, R., and Maschhoff, K., </author> <title> Theor. </title> <journal> Chim. Acta, </journal> <volume> 84, 457, </volume> <year> 1993. </year>
Reference-contexts: Once the F matrix is constructed, the optimum orbitals must be generated. We have the capability to perform the generalized eigenvalue analysis in parallel. The scalability, however, is much worse than construction of the F matrix because of the nature of the diagonalization algorithm. <ref> [8, 9] </ref> This fact led us to investigate and include alternative schemes as suggested by Shepard [10] . These techniques are all second-order convergence techniques that try to find the minimum SCF energy within the space of parameters, C. A recent paper [11] compares various techniques for direct-SCF calculations.
Reference: [9] <author> Kendall, R., A., Harrison, R., J., Littlefield, R., J., Guest, M., F. </author> <title> Reviews in Computational Chemistry, </title> <publisher> VCH Publishers, Inc., </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Once the F matrix is constructed, the optimum orbitals must be generated. We have the capability to perform the generalized eigenvalue analysis in parallel. The scalability, however, is much worse than construction of the F matrix because of the nature of the diagonalization algorithm. <ref> [8, 9] </ref> This fact led us to investigate and include alternative schemes as suggested by Shepard [10] . These techniques are all second-order convergence techniques that try to find the minimum SCF energy within the space of parameters, C. A recent paper [11] compares various techniques for direct-SCF calculations.
Reference: [10] <author> Shepard, R., </author> <title> Theor. </title> <journal> Chim. Acta, </journal> <volume> 84, 343, </volume> <year> 1993. </year>
Reference-contexts: We have the capability to perform the generalized eigenvalue analysis in parallel. The scalability, however, is much worse than construction of the F matrix because of the nature of the diagonalization algorithm. [8, 9] This fact led us to investigate and include alternative schemes as suggested by Shepard <ref> [10] </ref> . These techniques are all second-order convergence techniques that try to find the minimum SCF energy within the space of parameters, C. A recent paper [11] compares various techniques for direct-SCF calculations. Shepard and Tilson are exploring the use of a simultaneous vector expansion method for overlapping computational effort.

References-found: 10

