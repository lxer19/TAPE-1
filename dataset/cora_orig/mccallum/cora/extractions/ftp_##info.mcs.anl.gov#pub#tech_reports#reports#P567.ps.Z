URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P567.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts96.htm
Root-URL: http://www.mcs.anl.gov
Title: A High-Performance, Portable Implementation of the MPI Message Passing Interface Standard  
Author: William Gropp Ewing Lusk Nathan Doss Anthony Skjellum 
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  Department of Computer Science NSF Engineering Research Center for CFS Mississippi State University  
Abstract: MPI (Message Passing Interface) is a specification for a standard library for message passing that was defined by the MPI Forum, a broadly based group of parallel computer vendors, library writers, and applications specialists. Multiple implementations of MPI have been developed. In this paper, we describe MPICH, unique among existing implementations in its design goal of combining portability with high performance. We document its portability and performance and describe the architecture by which these features are simultaneously achieved. We also discuss the set of tools that accompany the free distribution of MPICH, which constitute the beginnings of a portable parallel programming environment. A project of this scope inevitably imparts lessons about parallel computing, the specification being followed, the current hardware and software environment for parallel computing, and project management; we describe those we have learned. Finally, we discuss future developments for MPICH, including those necessary to accommodate extensions to the MPI Standard now being contemplated by the MPI Forum. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Barnett, S. Gupta, D. Payne, L. Shuler, R. van de Geijn, and J. Watts. </author> <title> Interproces-sor collective communications library (intercomm). </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 356-364. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Each line is identified with its process number (rank in MPI_COMM_WORLD). Since stdout from all processes is collected, even on a network of workstations, all output comes out on the console. A sample is shown here. ... <ref> [1] </ref> Starting MPI_Bcast... [0] Starting MPI_Bcast... [0] Ending MPI_Bcast [2] Starting MPI_Bcast... [2] Ending MPI_Bcast [1] Ending MPI_Bcast ... logging The logging library uses the mpe logging routines described in Section 6.1 to write a logfile with events for entry to and exit from each MPI function. <p> Since stdout from all processes is collected, even on a network of workstations, all output comes out on the console. A sample is shown here. ... <ref> [1] </ref> Starting MPI_Bcast... [0] Starting MPI_Bcast... [0] Ending MPI_Bcast [2] Starting MPI_Bcast... [2] Ending MPI_Bcast [1] Ending MPI_Bcast ... logging The logging library uses the mpe logging routines described in Section 6.1 to write a logfile with events for entry to and exit from each MPI function. <p> Better collective algorithms As mentioned in Section 5.3, the current collective operations are implemented in a straightforward way. We would like to incorporate some of the ideas in <ref> [1] </ref> for improved performance. Thread safety The MPI specification is thread-safe, and considerable effort has gone into providing for thread safety in MPICH, but this has not been seriously tested. The primary obstacle here is the availability of a test suite for thread safety of MPI oper ations.
Reference: [2] <editor> Nanette J. Boden et al. Myrinet|a Gigabit-per-Second Local-Area Network. IEEE-Micro, </editor> <volume> 15(1) </volume> <pages> 29-36, </pages> <year> 1995. </year>
Reference-contexts: Each line is identified with its process number (rank in MPI_COMM_WORLD). Since stdout from all processes is collected, even on a network of workstations, all output comes out on the console. A sample is shown here. ... [1] Starting MPI_Bcast... [0] Starting MPI_Bcast... [0] Ending MPI_Bcast <ref> [2] </ref> Starting MPI_Bcast... [2] Ending MPI_Bcast [1] Ending MPI_Bcast ... logging The logging library uses the mpe logging routines described in Section 6.1 to write a logfile with events for entry to and exit from each MPI function. <p> Each line is identified with its process number (rank in MPI_COMM_WORLD). Since stdout from all processes is collected, even on a network of workstations, all output comes out on the console. A sample is shown here. ... [1] Starting MPI_Bcast... [0] Starting MPI_Bcast... [0] Ending MPI_Bcast <ref> [2] </ref> Starting MPI_Bcast... [2] Ending MPI_Bcast [1] Ending MPI_Bcast ... logging The logging library uses the mpe logging routines described in Section 6.1 to write a logfile with events for entry to and exit from each MPI function. <p> the Parsytec machine; * NEC SX-4 and Cenju-3; * Microsoft Windows NT, both for multiprocessor servers and across the many different kinds of networks that NT will support; and * Network protocols that are more efficient than TCP/IP, both standard (for ex ample, MessageWay [10]) and proprietary (for example, Myrinet <ref> [2] </ref>). Parallel I/O We have recently begun a project to determine whether the concepts of the ADI can be extended to include parallel I/O.
Reference: [3] <author> David Brightwell. </author> <title> The Design and Implementation of a Datagram Protocol (UDP) Device for MPICH. </title> <type> Master's thesis, </type> <month> December </month> <year> 1995. </year> <institution> Dept. of Computer Science. </institution>
Reference-contexts: It will be lighter weight than p4 and will support dynamic process management, which p4 does not. RDP/UDP device We are working on a reliable data protocol device approach, built on UDP/IP (User datagram protocol), which extends and leverages the initial work done by D. Brightwell <ref> [3] </ref>. Multiprotocol support Currently MPICH can use only one of its "devices" at a time.
Reference: [4] <author> Ronald Brightwell and Anthony Skjellum. </author> <title> Design and Implementation Study of MPICH for the Cray T3D. </title> <note> In preparation, </note> <month> February </month> <year> 1996. </year>
Reference-contexts: In this category are the Intel Paragon, IBM SP2, Meiko CS-2, Thinking Machines CM-5, NCube-2, and Cray T3D. (Although the Cray T3D provides some hardware that allows one to treat it as a shared-memory machine, it falls primarily into this category; see <ref> [4] </ref>.) Details of how MPICH is implemented on each of these machines are given in Section 4, and performance results for the Paragon and SP2 are given in Section 3.2. 3.1.2 Exploiting Shared-Memory Architectures A number of architectures support a shared-memory programming model, in which a memory location can be both
Reference: [5] <author> R. Alasdair A. Bruce, James G. Mills, and A. Gordon Smith. </author> <title> CHIMP/MPI user guide. </title> <type> Technical Report EPCC-KTP-CHIMP-V2-USER 1.2, </type> <institution> Edinburgh Parallel Computing Centre, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Some of these systems are as follows: * LAM [7] is available from the Ohio Supercomputer Center and runs on heterogeneous networks of Sun, DEC, SGI, IBM, and HP workstations. * CHIMP-MPI <ref> [5] </ref> is available from the Edinburgh Parallel Computing Center and runs on Sun, SGI, DEC, IBM, and HP workstations, the Meiko Computing Surface ma chines, and the Fujitsu AP-1000.
Reference: [6] <author> Greg Burns and Raja Daoud. </author> <title> MPI Cubix|collective POSIX I/O operations for MPI. </title> <type> Technical Report OSC-TR-1995-10, </type> <institution> Ohio Supercomputer Center, </institution> <year> 1995. </year> <month> 38 </month>
Reference-contexts: Such functionality has the added benefit of enhancing the ability of third parties to provide add-on tools for both C and Fortran users, without working with inside knowledge of the MPICH implementation (for instance, see <ref> [6] </ref>). The second level of "vertical" support is to allow a C routine to transmit data to a Fortran routine.
Reference: [7] <author> Greg Burns, Raja Daoud, and James Vaigl. LAM: </author> <title> An open cluster environment for MPI. </title> <editor> In John W. Ross, editor, </editor> <booktitle> Proceedings of Supercomputing Symposium '94, </booktitle> <pages> pages 379-386. </pages> <institution> University of Toronto, </institution> <year> 1994. </year>
Reference-contexts: Like MPICH, their initial versions were built on existing portable message-passing systems. They differ from MPICH in that they focus on the workstation environment, where software performance is necessarily limited by Unix socket functionality. Some of these systems are as follows: * LAM <ref> [7] </ref> is available from the Ohio Supercomputer Center and runs on heterogeneous networks of Sun, DEC, SGI, IBM, and HP workstations. * CHIMP-MPI [5] is available from the Edinburgh Parallel Computing Center and runs on Sun, SGI, DEC, IBM, and HP workstations, the Meiko Computing Surface ma chines, and the Fujitsu
Reference: [8] <author> Ralph Butler and Ewing Lusk. </author> <title> Monitors, messages, and clusters: The p4 parallel programming system. </title> <journal> Parallel Computing, </journal> <volume> 20 </volume> <pages> 547-564, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These systems prefigured in various ways the portability, performance, and some of the other features of MPICH. Although most of that original code has been extensively reworked, MPICH still owes some of its design to those earlier systems, which we briefly describe here. P4 <ref> [8] </ref> is a third-generation parallel programming library, including both message-passing and shared-memory components, portable to a great many parallel computing environments, including heterogeneous networks. Although p4 contributed much of the code for TCP/IP networks and shared-memory multiprocessors for the early versions of MPICH, most of that has been rewritten.
Reference: [9] <author> Lyndon J. Clarke, Robert A. Fletcher, Shari M. Trewin, R. Alasdair A. Bruce, A. Gor-don Smith, and Simon R. Chapple. </author> <title> Reuse, portability and parallel libraries. </title> <booktitle> In Proceedings of IFIP WG10.3|Programming Environments for Massively Parallel Distributed Systems, </booktitle> <year> 1994. </year>
Reference-contexts: It is based on CHIMP <ref> [9] </ref>. * At the Technical University of Munich, research has been done on a system for check pointing message-passing jobs, including MPI.
Reference: [10] <author> Danny Cohen and Craig Lund. </author> <title> Proposed Standard for the MessageWay Inter-SAN Routing. IETF Working Group Document, </title> <month> September </month> <year> 1995. </year>
Reference-contexts: These include * the Parsytec machine; * NEC SX-4 and Cenju-3; * Microsoft Windows NT, both for multiprocessor servers and across the many different kinds of networks that NT will support; and * Network protocols that are more efficient than TCP/IP, both standard (for ex ample, MessageWay <ref> [10] </ref>) and proprietary (for example, Myrinet [2]). Parallel I/O We have recently begun a project to determine whether the concepts of the ADI can be extended to include parallel I/O.
Reference: [11] <author> Peter Corbett, Dror Feitelson, Yarsun Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, and Parkson Wong. </author> <title> MPI-IO: A parallel file I/O interface for MPI, version 0.3. </title> <type> Technical Report NAS-95-002, </type> <institution> NAS, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Parallel I/O We have recently begun a project to determine whether the concepts of the ADI can be extended to include parallel I/O. If this proves successful, we will include an experimental implementation of parts of MPI-IO <ref> [11, 12] </ref> into MPICH. 36 9.4 MPI-2 In March 1995, the MPI Forum resumed meeting, with many of its original participants, to consider extensions to the original MPI Standard.
Reference: [12] <author> Peter Corbett, Yarsun Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, Parkson Wong, and Dror Feitelson. </author> <title> MPI-IO: A parallel file I/O interface for MPI, </title> <note> version 0.4. http://lovelace.nas.nasa.gov/MPI-IO, December 1995. </note>
Reference-contexts: Parallel I/O We have recently begun a project to determine whether the concepts of the ADI can be extended to include parallel I/O. If this proves successful, we will include an experimental implementation of parts of MPI-IO <ref> [11, 12] </ref> into MPICH. 36 9.4 MPI-2 In March 1995, the MPI Forum resumed meeting, with many of its original participants, to consider extensions to the original MPI Standard.
Reference: [13] <author> Remy Evard. </author> <title> Managing the ever-growing to do list. </title> <booktitle> In USENIX Proceedings of the Eighth Large Installation Systems Administration Conference, </booktitle> <pages> pages 111-116, </pages> <year> 1994. </year>
Reference-contexts: We needed a system that would allow all of the developers to keep track of reports, including what had been done (dialog with problem submitter, answers, etc.). It also had to be simple for users to use and for us to install. We chose the req system <ref> [13] </ref>. This system allows users to send mail (without any format restrictions) to mpi-bugs@mcs.anl.gov; the bug report is kept in a separate system as well as being forwarded to a list of developers. Both GUI and command-line access to the bug reports is provided.
Reference: [14] <author> S. I. Feldman and P. J. Weinberger. </author> <title> A portable Fortran 77 compiler. In UNIX Time Sharing System Programmer's Manual, </title> <type> volume 2. </type> <institution> AT&T Bell Laboratories, </institution> <address> tenth edition, </address> <year> 1990. </year>
Reference-contexts: Using the automatic tool also simplifies updating all of the interfaces when a system with a previously unknown Fortran-C interface is encountered. This situation arose the first time we ported MPICH to a system that used the program f2c <ref> [14] </ref> as a way to provide a Fortran compiler; f2c generates unusual external names for Fortran routine names. We needed only to rerun bfort to update the Fortran interfaces.
Reference: [15] <institution> American National Standard Programming Language Fortran. ANSI X3.9-1978. </institution>
Reference-contexts: Another issue is the Fortran bindings of all of the routines that take buffers. These buffers can be of any Fortran datatype (e.g., INTEGER, REAL, or CHARACTER). This was common practice in most previous message-passing systems but is in violation of the Fortran Standard <ref> [15] </ref>. The MPI Forum voted to follow standard practice. In most cases, Fortran compilers pass all items by reference, and few complain when a routine is called with different datatypes. <p> While C makes few statements about the length of datatypes (for example, sizeof (int) and sizeof (float) are unrelated), Fortran defines the ratios of the sizes of the numeric datatypes. Specifically, the sizes of INTEGER and REAL data are the same, and are half the size of DOUBLE PRECISION <ref> [15] </ref>. This is important in Fortran 77, where there is no memory allocation in the language and programmers often have to reuse data areas for different types of data. Further, using 64-bit IEEE floating point for DOUBLE PRECISION requires that INTEGER be 32 bits.
Reference: [16] <author> Message Passing Interface Forum. </author> <title> Document for a standard message-passing interface. </title> <type> Technical Report Technical Report No. </type> <institution> CS-93-214 (revised), University of Tennessee, </institution> <month> April </month> <year> 1994. </year> <note> Available on netlib. </note>
Reference-contexts: During the next eighteen months the MPI Forum met regularly, and Version 1.0 of the MPI Standard was completed in May 1994 <ref> [16, 36] </ref>. Some clarifications and refinements were made in the spring of 1995, and Version 1.1 of the MPI Standard is now available [17]. For a detailed presentation of the Standard itself, see [42]; for a tutorial approach to MPI, see [29].
Reference: [17] <author> The MPI Forum. </author> <title> The MPI message-passing interface standard. </title> <note> http://www.mcs.anl.gov/mpi/standard.html, May 1995. </note>
Reference-contexts: During the next eighteen months the MPI Forum met regularly, and Version 1.0 of the MPI Standard was completed in May 1994 [16, 36]. Some clarifications and refinements were made in the spring of 1995, and Version 1.1 of the MPI Standard is now available <ref> [17] </ref>. For a detailed presentation of the Standard itself, see [42]; for a tutorial approach to MPI, see [29]. In this paper we assume that the reader is relatively familiar with the MPI specification, but we provide a brief overview in Section 2.2.
Reference: [18] <author> Ian Foster, Carl Kesselman, and Steven Tuecke. </author> <title> The Nexus task-parallel runtime system. </title> <booktitle> In Proc. 1st Intl. Workshop on Parallel Processing,, </booktitle> <pages> pages 467-462. </pages> <publisher> Tata McGraw Hill, </publisher> <year> 1994. </year>
Reference-contexts: Brightwell [3]. Multiprotocol support Currently MPICH can use only one of its "devices" at a time. Although two of those devices, the one based on Nexus <ref> [18] </ref> and the one based on p4, are to a certain extent multiprotocol devices, we need a general mechanism for allowing multiple devices to be active at the same time. We are designing such a mechanism now.
Reference: [19] <author> Hubertus Franke, Peter Hochschild, Pratap Pattnaik, Jean-Pierre Prost, and Marc Snir. </author> <title> MPI on IBM SP1/SP2: Current status and future directions. </title> <type> Technical report, </type> <institution> IBM T. J. Watson Research Center, </institution> <year> 1995. </year>
Reference-contexts: It is not restricted to noncommercial use. This approach has worked well, and vendor implementations are now appearing, many incorporating major portions of MPICH code. * IBM obtained an explicit license for MPICH and collaborated with us in testing and debugging early versions. During this time, MPI-F <ref> [19] </ref> appeared. This IBM implementation does not use the ADI, but maps MPI functions directly onto an internal IBM abstract device interface.
Reference: [20] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Bob Manchek, and Vaidy Sunderam. </author> <title> PVM: Parallel Virtual Machine|A User's Guide and Tutorial for Network Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year> <month> 39 </month>
Reference-contexts: It is based on CHIMP [9]. * At the Technical University of Munich, research has been done on a system for check pointing message-passing jobs, including MPI. See [43] and [44]. * Unify [45], available from Mississippi State University, layers MPI on a version of PVM <ref> [20] </ref> that has been modified to support contexts and static groups. Unify allows mixed MPI and PVM calls in the same program.
Reference: [21] <author> William Gropp. </author> <title> Users manual for bfort: Producing Fortran interfaces to C source code. </title> <type> Technical Report ANL/MCS-TM-208, </type> <institution> Argonne National Laboratory, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Rather than manually create each interface routine, we used a program that had been developed at Argonne for just this purpose. The program, bfort <ref> [21] </ref>, reads the C source file and uses structured comments to identify routines for which to generate interfaces. Special options allow it to handle opaque types, choose how to handle C pointers, and provide name mapping. In many cases, this was all that was necessary to create the Fortran interfaces.
Reference: [22] <author> William Gropp. </author> <title> Users manual for doctext: Producing documentation from C source code. </title> <type> Technical Report ANL/MCS-TM-206, </type> <institution> Argonne National Laboratory, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: These difficulties might well never be encountered by systems administrators who merely install MPICH. 22 An important but frequently overlooked part of a software project (particular for re-search software) is the generation of documentation, particularly Unix-style man pages. 2 We use a tool called doctext <ref> [22] </ref> that generates man pages (as well as WWW and LaTeX documentation) directly from simple, structured comments in the source code. Using this tool allowed us to deliver MPICH with complete documentation from the beginning.
Reference: [23] <author> William Gropp, Edward Karrels, and Ewing Lusk. </author> <title> MPE graphics|scalable X11 graphics in MPI. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 49-54. </pages> <address> Mississippi State University, </address> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: It is not the case that the various processes communicate with one process that draws on the display; rather, the display is shared by all the processes. This library is described in <ref> [23] </ref>. Logging One of the most common tools for analyzing parallel program performance is a time-stamped event trace file. The MPE library provides simple calls to produce such a file.
Reference: [24] <author> William Gropp and Ewing Lusk. </author> <title> An abstract device definition to support the implementation of a high-level point-to-point message-passing interface. </title> <type> Preprint MCS-P342-1193, </type> <institution> Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: As an example, we present in Section 4.3 a case study showing how MPICH was quickly ported and then incrementally tuned for peak performance on SGI shared-memory systems. The central mechanism for achieving the goals of portability and performance is a specification we call the abstract device interface (ADI) <ref> [24] </ref>. All MPI functions are implemented in terms of the macros and functions that make up the ADI. All such code is portable. Hence, MPICH contains many implementations of the ADI, which provide portability, ease of implementation, and an incremental approach to trading portability for performance. <p> The MPICH ADI provides all of these functions; however, many message-passing hardware systems may not provide list management or elaborate data-transfer abilities. These functions are emulated through the use of auxiliary routines, described in <ref> [24] </ref>. The abstract device interface is a set of function definitions (which may be realized as either C functions or macro definitions) in terms of which the user-callable standard MPI functions may be expressed. As such, it provides the message-passing protocols that distinguish MPICH from other implementations of MPI. <p> In particular, the ADI layer contains the code for packetizing messages and attaching header information, managing multiple buffering policies, matching posted receives with incoming messages or queuing them if 13 necessary, and handling heterogeneous communications. For details of the exact interface and the algorithms used, see <ref> [24] </ref>. MPI point-to-point The Channel Interface The Abstract Device Interface MPI_Reduce MPI_Isend (implementations of the channel interface) SGI (3)T3DMeiko MPID_Post_Send SGI (4) NX MPID_SendControl A diagram of the upper layers of MPICH, showing the ADI, is shown in Figure 7. Sample functions at each layer are shown on the left.
Reference: [25] <author> William Gropp and Ewing Lusk. </author> <title> Installation guide for mpich, a portable implementation of MPI. </title> <type> Technical Report ANL-96/5, </type> <institution> Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: After being built and tested, MPICH can be installed in a publicly available location such as /usr/local with make install. Painless building and installation has become one of our pet goals for MPICH. 5.10 Documentation MPICH comes with both an installation guide <ref> [25] </ref> and a user's guide [27]. Although there is some overlap, and therefore some duplication, we consider separating them to be a better approach than combining them.
Reference: [26] <author> William Gropp and Ewing Lusk. </author> <title> Scalable Unix tools on parallel processors. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 56-62. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: This latter option is useful in locating "lost" messages. Details on all of these commands can be found in the user's guide [27]. 6.5 Network Management Tools Although not strictly part of MPICH itself, the Scalable Unix Tools (SUT) <ref> [26] </ref> are a useful part of the MPICH programming environment on workstation clusters. Basically, SUT implements parallel versions of common Unix commands such as ls, ps, cp, or rm. Perhaps the most useful is a cross between find and ps that we call pfps (parallel find in the process space). <p> Details can be found in <ref> [26] </ref>. 27 6.6 Example Programs MPICH comes with a fairly rich collection of example programs to illustrate its features.
Reference: [27] <author> William Gropp and Ewing Lusk. </author> <title> User's guide for mpich, a portable implementation of MPI. </title> <type> Technical Report ANL-96/6, </type> <institution> Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: After being built and tested, MPICH can be installed in a publicly available location such as /usr/local with make install. Painless building and installation has become one of our pet goals for MPICH. 5.10 Documentation MPICH comes with both an installation guide [25] and a user's guide <ref> [27] </ref>. Although there is some overlap, and therefore some duplication, we consider separating them to be a better approach than combining them. <p> Thus the work required by a user to add a new profiling library is reduced to writing individual MPI_Init and MPI_Finalize routines and one template routine. The libraries described above in Section 6.3.1 are all produced in this way. Details of how to use wrappergen can be found in <ref> [27] </ref>. 6.4 Useful Commands Aspects of the environment required for correct compilation and linking are encapsulated in the Makefiles produced when the user runs configure. Users may set up Makefile for their own applications by copying one from an MPI examples directory and modifying it as needed. <p> This latter option is useful in locating "lost" messages. Details on all of these commands can be found in the user's guide <ref> [27] </ref>. 6.5 Network Management Tools Although not strictly part of MPICH itself, the Scalable Unix Tools (SUT) [26] are a useful part of the MPICH programming environment on workstation clusters. Basically, SUT implements parallel versions of common Unix commands such as ls, ps, cp, or rm.
Reference: [28] <author> William Gropp and Ewing Lusk. </author> <title> MPICH working note: Creating a new MPICH device using the channel interface. </title> <type> Technical Report ANL/MCS-TM-213, </type> <institution> Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: All such code is portable. Hence, MPICH contains many implementations of the ADI, which provide portability, ease of implementation, and an incremental approach to trading portability for performance. One implementation of the ADI is in terms of a lower level (yet still portable) interface we call the channel interface <ref> [28] </ref>. The channel interface can be extremely small (five functions at minimum) and provides the quickest way to port MPICH to a new environment. Such a port can then be expanded gradually to include specialized implementation of more of the ADI functionality. <p> Although many implementations are possible, the specification can be done with a small number of definitions. The channel interface, described in more detail in <ref> [28] </ref>, consists of only five required functions. Three routines send and receive envelope (or control) information: MPID_SendControl, 1 MPID_RecvAnyControl, and MPID_ControlMsgAvail; two routines send and receive data: MPID_SendChannel and MPID_RecvFromChannel.
Reference: [29] <author> William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Some clarifications and refinements were made in the spring of 1995, and Version 1.1 of the MPI Standard is now available [17]. For a detailed presentation of the Standard itself, see [42]; for a tutorial approach to MPI, see <ref> [29] </ref>. In this paper we assume that the reader is relatively familiar with the MPI specification, but we provide a brief overview in Section 2.2. The project to provide a portable implementation of MPI began at the same time as the MPI definition process itself. <p> A number of simple examples illustrate specific features of the MPI Standard (topologies, for example) and have been developed for use in classes and tutorials. Many of the examples from <ref> [29] </ref> are included. For all of these examples, configure prepares the appropriate Makefiles, but they have to be individually built as the user wishes.
Reference: [30] <author> William D. Gropp and Ewing Lusk. </author> <title> A test implementation of the MPI draft message-passing standard. </title> <type> Technical Report ANL-92/47, </type> <institution> Argonne National Laboratory, </institution> <month> De-cember </month> <year> 1992. </year>
Reference-contexts: The first version of MPICH, in fact, implemented the prespecification described in [46] within a few days. The speed with which this version was completed was due to the existing portable systems p4 and Chameleon. This first MPICH, which offered quite reasonable performance and portability, is described in <ref> [30] </ref>. Starting in spring 1993, this implementation was gradually modified to provide increased performance and portability. At the same time the system was greatly expanded to include all of the MPI specification.
Reference: [31] <author> William D. Gropp and Barry Smith. </author> <title> Chameleon parallel programming tools users manual. </title> <type> Technical Report ANL-93/23, </type> <institution> Argonne National Laboratory, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: P4 remains one of the "devices" on which MPICH can be built (see Section 4), but in most cases more customized alternatives are available. Chameleon <ref> [31] </ref> is a high-performance portability package for message passing on parallel supercomputers. It is implemented as a thin layer (mostly C macros) over vendor message-passing systems (Intel's NX, TMC's CMMD, IBM's MPL) for performance and over publicly available systems (p4 and PVM) for portability. <p> By implementing the channel interface in terms of Chameleon <ref> [31] </ref> macros, we provide portability to a number of systems at one stroke, with no additional overhead, since Chameleon macros are resolved at compile time. Chameleon macros exist for most vendor message-passing systems, and also for p4, which in turn is portable to very many systems.
Reference: [32] <author> M. T. Heath and J. A. Etheridge. </author> <title> Visualizing the performance of parallel programs. </title> <journal> IEEE Software, </journal> <volume> 8(5) </volume> <pages> 29-39, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: A number of tools developed by various groups do this. One of the earliest of these was upshot [33]. Since then upshot has been reimplemented in Tcl/Tk, and this version [34] is distributed with MPICH. It can read log files generated either by Paragraph <ref> [32] </ref> or by the mpe logging routines, which are in turn used by the logging profiling library.
Reference: [33] <author> Virginia Herrarte and Ewing Lusk. </author> <title> Studying parallel program behavior with upshot. </title> <type> Technical Report ANL-91/15, </type> <institution> Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: It also automatically handles the misalignment and drift of clocks on multiple processors, if the system does not provide a synchronized clock. The logfile format is that of upshot <ref> [33] </ref>. This is the library for a user who wishes to define his own events and program states. Automatic generation of events by MPI routines is described in Section 6.3.1. <p> A number of tools developed by various groups do this. One of the earliest of these was upshot <ref> [33] </ref>. Since then upshot has been reimplemented in Tcl/Tk, and this version [34] is distributed with MPICH. It can read log files generated either by Paragraph [32] or by the mpe logging routines, which are in turn used by the logging profiling library.
Reference: [34] <author> Edward Karrels and Ewing Lusk. </author> <title> Performance analysis of MPI programs. </title> <editor> In Jack J. Dongarra and Bernard Tourancheau, editors, </editor> <booktitle> Environments and Tools for Parallel Scientific Computing, </booktitle> <pages> pages 195-200. </pages> <publisher> SIAM, </publisher> <year> 1994. </year>
Reference-contexts: Further description of these libraries can be found in <ref> [34] </ref>. 6.3.2 Upshot One of the most useful tools for understanding parallel program behavior is a graphical display of parallel timelines with colored bars to indicate the state of each process at any given time. A number of tools developed by various groups do this. <p> A number of tools developed by various groups do this. One of the earliest of these was upshot [33]. Since then upshot has been reimplemented in Tcl/Tk, and this version <ref> [34] </ref> is distributed with MPICH. It can read log files generated either by Paragraph [32] or by the mpe logging routines, which are in turn used by the logging profiling library.
Reference: [35] <author> Donald Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol. 3. </volume> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: These implementations follow system-specific strategies; for 19 example, the Convex SPP collective routines makes use both of shared memory and of the memory hierarchies in the SPP. 5.4 Attributes Attribute caching on communicators is implemented by using a height-balanced tree (HBT or AVL tree) <ref> [35] </ref>. Each communicator has an HBT associated with it, although initially the HBT may be an empty or null tree. Caching an attribute on a communicator is simply an insertion into the HBT; retrieving an attribute is simply searching the tree and returning the cached attribute.
Reference: [36] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <journal> International Journal of Supercomputer Applications, </journal> 8(3/4):165-414, 1994. <volume> 40 </volume>
Reference-contexts: During the next eighteen months the MPI Forum met regularly, and Version 1.0 of the MPI Standard was completed in May 1994 <ref> [16, 36] </ref>. Some clarifications and refinements were made in the spring of 1995, and Version 1.1 of the MPI Standard is now available [17]. For a detailed presentation of the Standard itself, see [42]; for a tutorial approach to MPI, see [29].
Reference: [37] <author> Joerg Meyer. </author> <title> Message passing interface for Microsoft Windows 3.1. </title> <type> Master's thesis, </type> <institution> University of Nebraska at Omaha, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: An important family of non-Unix operating systems is supported by Microsoft. MPICH has been ported to Windows 3.1 (where it simulates multiprocessing on a single processor); the system is called WinMPI <ref> [37, 38] </ref>. 3.2 Performance of MPICH The MPI specification was designed to allow high performance in the sense that semantic restrictions on optimization were avoided wherever user convenience would not be severely impacted.
Reference: [38] <author> Joerg Meyer and Hesham El-Rewini. WinMPI: </author> <title> Message passing interface for Microsoft Windows 3.1. </title> <address> /ftp://csftp.unomaha.edu/pub/rewini/WinMPI/, </address> <year> 1994. </year>
Reference-contexts: An important family of non-Unix operating systems is supported by Microsoft. MPICH has been ported to Windows 3.1 (where it simulates multiprocessing on a single processor); the system is called WinMPI <ref> [37, 38] </ref>. 3.2 Performance of MPICH The MPI specification was designed to allow high performance in the sense that semantic restrictions on optimization were avoided wherever user convenience would not be severely impacted.
Reference: [39] <institution> Para//ab, University of Bergen. </institution> <note> Programmer's Guide to MPI for Dolphin's SBus-to-SCI adapters, version 1.0 edition, </note> <month> November </month> <year> 1995. </year> <note> Available at ftp://ftp.ii.uib.no/pub/incoming/progr-guide-v10.ps. </note>
Reference-contexts: SCI A specialized implementation of the channel interface has been developed for an implementation of the Scalable Coherent Interface [40] from Dolphin Interconnect Solutions, which provides portability to a number of systems that use it <ref> [39] </ref>. Contrary to some descriptions of MPICH that have appeared elsewhere, MPICH has never relied on the p4 version of the channel interface for portability to massively parallel processors. From the beginning, the MPP (IBM SP, Intel Paragon, TMC CM-5) versions used the macros provided by Chameleon.
Reference: [40] <institution> IEEE standard for scalable coherent interface (SCI) (1-55937-222-2) [SH15255]. IEEE 596-1992, </institution> <year> 1993. </year>
Reference-contexts: This approach takes advantage of particular memory models and operating system features that the shared-memory implementation of the channel interface does not assume are present. SCI A specialized implementation of the channel interface has been developed for an implementation of the Scalable Coherent Interface <ref> [40] </ref> from Dolphin Interconnect Solutions, which provides portability to a number of systems that use it [39]. Contrary to some descriptions of MPICH that have appeared elsewhere, MPICH has never relied on the p4 version of the channel interface for portability to massively parallel processors.
Reference: [41] <author> Anthony Skjellum, Steven G. Smith, Nathan E. Doss, Alvin P. Leung, and Manfred Morari. </author> <title> The design and evolution of Zipcode. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 565-596, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It is implemented as a thin layer (mostly C macros) over vendor message-passing systems (Intel's NX, TMC's CMMD, IBM's MPL) for performance and over publicly available systems (p4 and PVM) for portability. A substantial amount of Chameleon technology is incorporated into MPICH (as detailed in Section 4). Zipcode <ref> [41] </ref> is a portable system for writing scalable libraries. It contributed several concepts to the design of the MPI Standard|in particular contexts, groups, and mailers (the equivalent of MPI communicators).
Reference: [42] <author> Marc Snir, Steve W. Otto, Steven Huss-Lederman, David W. Walker, and Jack Don-garra. </author> <title> MPI: The Complete Reference. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Some clarifications and refinements were made in the spring of 1995, and Version 1.1 of the MPI Standard is now available [17]. For a detailed presentation of the Standard itself, see <ref> [42] </ref>; for a tutorial approach to MPI, see [29]. In this paper we assume that the reader is relatively familiar with the MPI specification, but we provide a brief overview in Section 2.2.
Reference: [43] <author> Georg Stellner. </author> <title> Cocheck: Checkpointing and process migration for MPI. </title> <booktitle> In Proceedings of the IPPS. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: It is based on CHIMP [9]. * At the Technical University of Munich, research has been done on a system for check pointing message-passing jobs, including MPI. See <ref> [43] </ref> and [44]. * Unify [45], available from Mississippi State University, layers MPI on a version of PVM [20] that has been modified to support contexts and static groups. Unify allows mixed MPI and PVM calls in the same program.
Reference: [44] <author> Georg Stellner and Jim Pruyne. </author> <title> Resource Management and Checkpointing for PVM. </title> <booktitle> In Proceedings of the 2nd European PVM Users' Group Meeting, </booktitle> <pages> pages 131-136, </pages> <address> Lyon, </address> <month> September </month> <year> 1995. </year> <note> Editions Hermes. </note>
Reference-contexts: It is based on CHIMP [9]. * At the Technical University of Munich, research has been done on a system for check pointing message-passing jobs, including MPI. See [43] and <ref> [44] </ref>. * Unify [45], available from Mississippi State University, layers MPI on a version of PVM [20] that has been modified to support contexts and static groups. Unify allows mixed MPI and PVM calls in the same program.
Reference: [45] <author> Paula L. Vaughan, Anthony Skjellum, Donna S. Reese, and Fei Chen Cheng. </author> <title> Migrating from PVM to MPI, part I: The Unify System. </title> <booktitle> In Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 488-495, </pages> <address> McLean, Virginia, </address> <month> February </month> <year> 1995. </year> <booktitle> IEEE Computer Society Technical Committee on Computer Architecture, </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: It is based on CHIMP [9]. * At the Technical University of Munich, research has been done on a system for check pointing message-passing jobs, including MPI. See [43] and [44]. * Unify <ref> [45] </ref>, available from Mississippi State University, layers MPI on a version of PVM [20] that has been modified to support contexts and static groups. Unify allows mixed MPI and PVM calls in the same program.
Reference: [46] <author> David Walker. </author> <title> Standards for message passing in a distributed memory environment. </title> <type> Technical report, </type> <institution> Oak Ridge National Laboratory, </institution> <month> August </month> <year> 1992. </year> <month> 41 </month>
Reference-contexts: The purpose was to quickly expose problems that the specification might pose for implementors and to provide early experimenters with an opportunity to try ideas being proposed for MPI before they became fixed. The first version of MPICH, in fact, implemented the prespecification described in <ref> [46] </ref> within a few days. The speed with which this version was completed was due to the existing portable systems p4 and Chameleon. This first MPICH, which offered quite reasonable performance and portability, is described in [30].
References-found: 46

