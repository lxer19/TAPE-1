URL: http://www.almaden.ibm.com/cs/quest/papers/tkde93.ps
Refering-URL: http://www.almaden.ibm.com/cs/quest/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Database Mining: A Performance Perspective  
Author: Rakesh Agrawal Tomasz Imielinski Arun Swami 
Address: 650 Harry Road San Jose, CA 95120-6099  
Affiliation: IBM Almaden Research Center  
Abstract: We present our perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology. We describe three classes of database mining problems involving classification, associations, and sequences, and argue that these problems can be uniformly viewed as requiring discovery of rules embedded in massive data. We describe a model and some basic operations for the process of rule discovery. We show how the database mining problems we consider map to this model and how they can be solved by using the basic operations we propose. We give an example of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm not only is efficient in discovering classification rules but also has accuracy comparable to ID3, one of the current best classifiers. Index Terms. database mining, knowledge discovery, classification, associations, se quences, decision trees fl Current address: Computer Science Department, Rutgers University, New Brunswick, NJ 08903
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rakesh Agrawal, Sakti Ghosh, Tomasz Imielinski, Bala Iyer, and Arun Swami, </author> <title> "An Interval Classifier for Database Mining Applications", </title> <booktitle> VLDB 92 , Vancouver, </booktitle> <address> British Columbia, Canada, </address> <year> 1992, </year> <pages> 560-573. </pages>
Reference-contexts: Other applications involving classification include credit approval, treatment-appropriateness determination, etc. A variation of the classification problem is the BestN problem <ref> [1] </ref>. A company may be interested in finding the best N candidates to whom a ski package should be mailed. First a small number of ski packages are mailed to a selected sample of the population and then a profile of likely positive respondents is obtained. <p> In the following we often use the term "attribute" for the accessor method associated with the attribute. * Classification: We consider classifiers based on decision trees (see [7] [6] [11] for an overview.) We refer the reader to <ref> [1] </ref> for a discussion on why these classifiers (as opposed to, for example, neural nets [10]) are more appropriate for database mining applications. 7 The target set of strings includes all strings corresponding to the paths from the root to the leaves of the classification tree. <p> Combination is performed on new strings generated through an extension by a continuous-valued attribute. ID3 [14] and CART [2] use binary splitting [13] for this purpose, whereas IC <ref> [1] </ref> partitions the domain of a continuous attribute into intervals. In the Filter operation, entropy [13] is computed for each attribute added, and only the strings containing the attribute that has the highest value of information gain [13] are retained and included in the next seed set. <p> We refer to this classifier as CDP , for classifier with dynamic pruning. CDP uses the binary partitioning of continuous attributes, proposed in ID3 [14] and CART [2], in the Combine operation. It uses the dynamic pruning scheme of IC <ref> [1] </ref> in the Filter operation. <p> We believe that database mining is an important new application area for databases, combining commercial interest with intriguing research questions. 7 Appendix: Experimental Methodology We used the evaluation methodology and the synthetic database proposed in <ref> [1] </ref> to assess the accuracy characteristics of CDP . Every tuple in this database has the nine attributes given in Table 1. Attributes elevel, car, and zipcode are categorical attributes, all others are non-categorical attributes. Attribute values were randomly generated. <p> Function 6 involves ranges on a linear function of two non-categorical attributes. Functions 7 through 9 are linear functions and function 10 is a nonlinear function of attribute values. These functions are listed in Section 7. Note that these functions are a superset of the functions used in <ref> [1] </ref>. 18 For every experiment, we generated a training set and a test data set. Tuples in the training set were assigned the group label by first generating the tuple and then applying the classification function on the tuple to determine the group to which the tuple belongs.
Reference: [2] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: Given a string s in the seed, we Generate all extensions of this string by adding all possible (attribute, value) pairs to it. Combination is performed on new strings generated through an extension by a continuous-valued attribute. ID3 [14] and CART <ref> [2] </ref> use binary splitting [13] for this purpose, whereas IC [1] partitions the domain of a continuous attribute into intervals. <p> We also describe the implementation of each of the basic operations. We refer to this classifier as CDP , for classifier with dynamic pruning. CDP uses the binary partitioning of continuous attributes, proposed in ID3 [14] and CART <ref> [2] </ref>, in the Combine operation. It uses the dynamic pruning scheme of IC [1] in the Filter operation. <p> This operation is repeated for all tuples. 5.1.2 Combine Let f ~s a g be the set of new strings generated by extending a seed s with (a, value) pairs of a continuous attribute a. We use binary partitioning, as proposed in ID3 [14] and CART <ref> [2] </ref>, to determine a value u that partitions the range of atomic values of a into two intervals i 1 and i 2 such that the information gain is maximized. The interval i 1 is given by a &lt; u and the interval i 2 is given by a u. <p> Counts for Group B for strings (age=25) and (zipcode=95120) are incremented by 1. This process continues till all the tuples in the database have been exhausted. CDP now combines the new strings for the continuous attributes using binary partitioning, as proposed in ID3 [14] and CART <ref> [2] </ref>. In this example, age is the only continuous attribute. We omit the details of partitioning and present only the results. We find that the partitioning replaces all the age strings with two strings: (20 age &lt; 59.5) and (59.5 age &lt; 80).
Reference: [3] <author> R. Brice and W. Alexander, </author> <title> "Finding Interesting Things in Lots of Data", </title> <booktitle> 23rd Hawaii International Conference on System Sciences, Kona, Hawaii, </booktitle> <month> January </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Database technology has been used with great success in traditional business data processing. There is an increasing desire to use this technology in new application domains. One such application domain that is likely to acquire considerable significance in the near future is database mining [12] <ref> [3] </ref> [5] [8] [9] [11] [15] [16] [18] [19]. An increasing number of organizations are creating ultra large data bases (measured in gigabytes and even terabytes) of business data, such as consumer data, transaction histories, sales records, etc. Such data forms a potential gold mine of valuable business information.
Reference: [4] <author> Wray Buntine, </author> <title> About the IND Tree Package, </title> <institution> NASA Ames Research Center, Moffett Field, California, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: To assess the accuracy of the rules discovered by CDP , we compared it with ID3. We used the IND tree package <ref> [4] </ref> from the NASA Ames Research Center for this empirical evaluation. IND implements C4, which is a more recent, improved version of ID3. The experimental methodology, data sets, and the classification functions used for the experiments are described in the Appendix. for the CDP algorithm.
Reference: [5] <editor> Wray Buntine and Matha Del Alto (Editors), </editor> <title> Collected Notes on the Workshop for Pattern Discovery in Large Databases, </title> <type> Technical Report FIA-91-07, </type> <institution> NASA Ames Research Center, Moffett Field, California, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Database technology has been used with great success in traditional business data processing. There is an increasing desire to use this technology in new application domains. One such application domain that is likely to acquire considerable significance in the near future is database mining [12] [3] <ref> [5] </ref> [8] [9] [11] [15] [16] [18] [19]. An increasing number of organizations are creating ultra large data bases (measured in gigabytes and even terabytes) of business data, such as consumer data, transaction histories, sales records, etc. Such data forms a potential gold mine of valuable business information.
Reference: [6] <author> Philip Andrew Chou, </author> <title> "Application of Information Theory to Pattern Recognition and the Design of Decision Trees and Trellises", </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, California, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: These classes certainly do not exhaust all database mining applications, but do capture an interesting subset of them. In Section 3, we will present a unifying framework for studying and solving these problems. 2.1 Classification The classification problem <ref> [6] </ref> [10] [11] [18] involves finding rules that partition the given data into disjoint groups. As an example of a classification problem, consider the store location problem. <p> In the following we often use the term "attribute" for the accessor method associated with the attribute. * Classification: We consider classifiers based on decision trees (see [7] <ref> [6] </ref> [11] for an overview.) We refer the reader to [1] for a discussion on why these classifiers (as opposed to, for example, neural nets [10]) are more appropriate for database mining applications. 7 The target set of strings includes all strings corresponding to the paths from the root to the <p> CDP uses the binary partitioning of continuous attributes, proposed in ID3 [14] and CART [2], in the Combine operation. It uses the dynamic pruning scheme of IC [1] in the Filter operation. CDP belongs to the class of tree-classifiers [7] <ref> [6] </ref> [11], and hence can be used to generate rules that can easily be translated into SQL queries for efficient interfacing with relational databases [20]. 10 5.1 Basic Operations in CDP We present the implementation of the basic rule discovery operations in CDP .
Reference: [7] <author> G. R. Dattatreya and L. N. Kanal, </author> <title> "Decision Trees in Pattern Recognition", In Progress in Pattern Recognition 2 , L. </title> <editor> N. Kanal and A. Rosenfeld (Editors), </editor> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <year> 1985. </year>
Reference-contexts: In the following we often use the term "attribute" for the accessor method associated with the attribute. * Classification: We consider classifiers based on decision trees (see <ref> [7] </ref> [6] [11] for an overview.) We refer the reader to [1] for a discussion on why these classifiers (as opposed to, for example, neural nets [10]) are more appropriate for database mining applications. 7 The target set of strings includes all strings corresponding to the paths from the root to <p> CDP uses the binary partitioning of continuous attributes, proposed in ID3 [14] and CART [2], in the Combine operation. It uses the dynamic pruning scheme of IC [1] in the Filter operation. CDP belongs to the class of tree-classifiers <ref> [7] </ref> [6] [11], and hence can be used to generate rules that can easily be translated into SQL queries for efficient interfacing with relational databases [20]. 10 5.1 Basic Operations in CDP We present the implementation of the basic rule discovery operations in CDP .
Reference: [8] <author> J. Han, Y. Cai, and N. Cercone, </author> <title> "Knowledge Discovery in Databases: An Attribute-Oriented Approach", </title> <address> VLDB-92 , Vancouver, British Columbia, Canada, </address> <year> 1992, </year> <pages> 547-559. </pages>
Reference-contexts: 1 Introduction Database technology has been used with great success in traditional business data processing. There is an increasing desire to use this technology in new application domains. One such application domain that is likely to acquire considerable significance in the near future is database mining [12] [3] [5] <ref> [8] </ref> [9] [11] [15] [16] [18] [19]. An increasing number of organizations are creating ultra large data bases (measured in gigabytes and even terabytes) of business data, such as consumer data, transaction histories, sales records, etc. Such data forms a potential gold mine of valuable business information.
Reference: [9] <author> Ravi Krishnamurthy and Tomasz Imielinski, </author> <title> "Practitioner Problems in Need of Database Research: Research Directions in Knowledge Discovery", </title> <booktitle> SIGMOD Record , Vol. </booktitle> <volume> 20, No. 3, </volume> <month> Sept. </month> <year> 1991, </year> <pages> 76-78. </pages>
Reference-contexts: 1 Introduction Database technology has been used with great success in traditional business data processing. There is an increasing desire to use this technology in new application domains. One such application domain that is likely to acquire considerable significance in the near future is database mining [12] [3] [5] [8] <ref> [9] </ref> [11] [15] [16] [18] [19]. An increasing number of organizations are creating ultra large data bases (measured in gigabytes and even terabytes) of business data, such as consumer data, transaction histories, sales records, etc. Such data forms a potential gold mine of valuable business information. <p> We describe a model and some basic operations for the process of rule discovery. We also show how these database mining problems map to this model and how they can be solved by using the basic operations we propose. Our view of database mining complements the perspective presented in <ref> [9, 19] </ref>. Both of these papers argue for an iterative process for mining with a human in the loop. The user begins with a hypothesis and uses data to refute or confirm the hypothesis. <p> The hypothesis is refined, depending on the response and this process continues until a satisfactory theory has been obtained. The emphasis in [19] is on having a declarative language that makes it easier to formulate and revise hypotheses. The emphasis in <ref> [9] </ref> is on providing a large bandwidth between the machine and human so that user-interest is maintained between successive iterations. Although we do not discuss this aspect in detail in this paper, we admit the possibility of human intervention in the mining process.
Reference: [10] <author> Richard P. Lippmann, </author> <title> "An Introduction to Computing with Neural Nets", </title> <journal> IEEE ASSP Magazine, </journal> <month> April </month> <year> 1987, </year> <pages> 4-22. </pages>
Reference-contexts: These classes certainly do not exhaust all database mining applications, but do capture an interesting subset of them. In Section 3, we will present a unifying framework for studying and solving these problems. 2.1 Classification The classification problem [6] <ref> [10] </ref> [11] [18] involves finding rules that partition the given data into disjoint groups. As an example of a classification problem, consider the store location problem. <p> we often use the term "attribute" for the accessor method associated with the attribute. * Classification: We consider classifiers based on decision trees (see [7] [6] [11] for an overview.) We refer the reader to [1] for a discussion on why these classifiers (as opposed to, for example, neural nets <ref> [10] </ref>) are more appropriate for database mining applications. 7 The target set of strings includes all strings corresponding to the paths from the root to the leaves of the classification tree.
Reference: [11] <author> David J. Lubinsky, </author> <title> "Discovery from Databases: A Review of AI and Statistical Techniques", </title> <booktitle> IJCAI-89 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Detroit, </address> <month> August </month> <year> 1989, </year> <pages> 204-218. 21 </pages>
Reference-contexts: There is an increasing desire to use this technology in new application domains. One such application domain that is likely to acquire considerable significance in the near future is database mining [12] [3] [5] [8] [9] <ref> [11] </ref> [15] [16] [18] [19]. An increasing number of organizations are creating ultra large data bases (measured in gigabytes and even terabytes) of business data, such as consumer data, transaction histories, sales records, etc. Such data forms a potential gold mine of valuable business information. <p> These classes certainly do not exhaust all database mining applications, but do capture an interesting subset of them. In Section 3, we will present a unifying framework for studying and solving these problems. 2.1 Classification The classification problem [6] [10] <ref> [11] </ref> [18] involves finding rules that partition the given data into disjoint groups. As an example of a classification problem, consider the store location problem. <p> In the following we often use the term "attribute" for the accessor method associated with the attribute. * Classification: We consider classifiers based on decision trees (see [7] [6] <ref> [11] </ref> for an overview.) We refer the reader to [1] for a discussion on why these classifiers (as opposed to, for example, neural nets [10]) are more appropriate for database mining applications. 7 The target set of strings includes all strings corresponding to the paths from the root to the leaves <p> CDP uses the binary partitioning of continuous attributes, proposed in ID3 [14] and CART [2], in the Combine operation. It uses the dynamic pruning scheme of IC [1] in the Filter operation. CDP belongs to the class of tree-classifiers [7] [6] <ref> [11] </ref>, and hence can be used to generate rules that can easily be translated into SQL queries for efficient interfacing with relational databases [20]. 10 5.1 Basic Operations in CDP We present the implementation of the basic rule discovery operations in CDP .
Reference: [12] <author> Tarek M. Anwar, Howard W. Beck, and Shamkant B. Navathe, </author> <title> "Knowledge Mining by Imprecise Querying: A Classification-Based Approach", </title> <booktitle> IEEE 8th Int'l Conf. on Data Engineering, </booktitle> <address> Phoenix, Arizona, </address> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Database technology has been used with great success in traditional business data processing. There is an increasing desire to use this technology in new application domains. One such application domain that is likely to acquire considerable significance in the near future is database mining <ref> [12] </ref> [3] [5] [8] [9] [11] [15] [16] [18] [19]. An increasing number of organizations are creating ultra large data bases (measured in gigabytes and even terabytes) of business data, such as consumer data, transaction histories, sales records, etc. Such data forms a potential gold mine of valuable business information.
Reference: [13] <author> J. Ross Quinlan, </author> <title> "Induction of Decision Trees", </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <year> 1986, </year> <pages> 81-106. </pages>
Reference-contexts: Given a string s in the seed, we Generate all extensions of this string by adding all possible (attribute, value) pairs to it. Combination is performed on new strings generated through an extension by a continuous-valued attribute. ID3 [14] and CART [2] use binary splitting <ref> [13] </ref> for this purpose, whereas IC [1] partitions the domain of a continuous attribute into intervals. In the Filter operation, entropy [13] is computed for each attribute added, and only the strings containing the attribute that has the highest value of information gain [13] are retained and included in the next <p> Combination is performed on new strings generated through an extension by a continuous-valued attribute. ID3 [14] and CART [2] use binary splitting <ref> [13] </ref> for this purpose, whereas IC [1] partitions the domain of a continuous attribute into intervals. In the Filter operation, entropy [13] is computed for each attribute added, and only the strings containing the attribute that has the highest value of information gain [13] are retained and included in the next seed set. <p> [14] and CART [2] use binary splitting <ref> [13] </ref> for this purpose, whereas IC [1] partitions the domain of a continuous attribute into intervals. In the Filter operation, entropy [13] is computed for each attribute added, and only the strings containing the attribute that has the highest value of information gain [13] are retained and included in the next seed set. IC also computes for each expanded string an expansion merit, and a string is filtered out if its expansion merit is below an acceptable level. <p> The algorithm is trying to determine what characterizes group 0 (G = 0) and group 1 (G = 1). The two attributes a and b are categorical attributes. a b G 0 1 1 1 1 0 A string is put in the target set if the information gain <ref> [13] </ref> computed for the string is 1. In the Filter operation a single attribute is selected for extension and strings obtained by extending other attributes are eliminated. We start with the empty string. <p> The number of strings actually present depends on the number of distinct values present in the database. 5.1.3 Filter We retain strings corresponding to the attribute that maximizes the information gain ratio <ref> [13] </ref> and eliminate all other strings. Let the database D of n objects contain n k objects of group G k . <p> Thus, at the end of Combine operation, we have 16 strings: 2 corresponding to age, 9 corresponding to zipcode, and 5 corresponding to elevel. CDP now performs Filter operation, and eliminates all strings corresponding to attributes other than the one that maximizes the information gain ratio <ref> [13] </ref>. <p> Finally, we gave a concrete example of an algorithm suitable for discovering classification rules, and described the efficient implementation of the basic operations for this algorithm. This algorithm not only is efficient in discovering classification rules but also has accuracy comparable to the well known classification algorithm ID3 <ref> [13] </ref>. 16 17 The work reported in this paper has been done in the context of the Quest project at the IBM Almaden Research Center. In Quest, we are exploring the various aspects of the database mining problem.
Reference: [14] <author> J. Ross Quinlan, </author> <title> "Simplifying Decision Trees", </title> <journal> Int. J. Man-Machine Studies, </journal> <volume> 27, </volume> <year> 1987, </year> <pages> 221-234. </pages>
Reference-contexts: We show that the classifier so obtained is not only efficient but has a classification accuracy comparable to the well-known classifier ID3 <ref> [14] </ref>. We present our conclusions and directions for future work in Section 6. 2 2 Database Mining Problems We present three classes of database mining problems that we have identified by examining some of the often cited applications of database mining. <p> Given a string s in the seed, we Generate all extensions of this string by adding all possible (attribute, value) pairs to it. Combination is performed on new strings generated through an extension by a continuous-valued attribute. ID3 <ref> [14] </ref> and CART [2] use binary splitting [13] for this purpose, whereas IC [1] partitions the domain of a continuous attribute into intervals. <p> A large value for the waste ratio indicates generally poor performance due to possibly unnecessary additional work. For instance, we may build a classification tree either by building first a complete tree and then pruning it (as it is the case for instance for ID3 <ref> [14] </ref>) or take a dynamic approach and expand the tree only until estimated errors are reduced by a certain amount. The latter method will have a much better waste ratio since it will not generate and measure many strings that would be pruned later. <p> We also describe the implementation of each of the basic operations. We refer to this classifier as CDP , for classifier with dynamic pruning. CDP uses the binary partitioning of continuous attributes, proposed in ID3 <ref> [14] </ref> and CART [2], in the Combine operation. It uses the dynamic pruning scheme of IC [1] in the Filter operation. <p> This operation is repeated for all tuples. 5.1.2 Combine Let f ~s a g be the set of new strings generated by extending a seed s with (a, value) pairs of a continuous attribute a. We use binary partitioning, as proposed in ID3 <ref> [14] </ref> and CART [2], to determine a value u that partitions the range of atomic values of a into two intervals i 1 and i 2 such that the information gain is maximized. <p> Counts for Group B for strings (age=25) and (zipcode=95120) are incremented by 1. This process continues till all the tuples in the database have been exhausted. CDP now combines the new strings for the continuous attributes using binary partitioning, as proposed in ID3 <ref> [14] </ref> and CART [2]. In this example, age is the only continuous attribute. We omit the details of partitioning and present only the results. We find that the partitioning replaces all the age strings with two strings: (20 age &lt; 59.5) and (59.5 age &lt; 80). <p> An alternative would have been to expand all the strings fully and then prune them as is the case, for instance, in ID3 <ref> [14] </ref>. However, this approach will exhibit a bad waste ratio. Hence, if the classification accuracy is similar, using dynamic pruning is a winner from the computational perspective.
Reference: [15] <editor> G. Piatetsky-Shapiro and W. Frawley (Editors), </editor> <booktitle> Proceedings of IJCAI-89 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Detroit, Michigan, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: There is an increasing desire to use this technology in new application domains. One such application domain that is likely to acquire considerable significance in the near future is database mining [12] [3] [5] [8] [9] [11] <ref> [15] </ref> [16] [18] [19]. An increasing number of organizations are creating ultra large data bases (measured in gigabytes and even terabytes) of business data, such as consumer data, transaction histories, sales records, etc. Such data forms a potential gold mine of valuable business information.
Reference: [16] <editor> G. Piatetsky-Shapiro (Editor), </editor> <booktitle> Proceedings of AAAI-91 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Anaheim, California, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: There is an increasing desire to use this technology in new application domains. One such application domain that is likely to acquire considerable significance in the near future is database mining [12] [3] [5] [8] [9] [11] [15] <ref> [16] </ref> [18] [19]. An increasing number of organizations are creating ultra large data bases (measured in gigabytes and even terabytes) of business data, such as consumer data, transaction histories, sales records, etc. Such data forms a potential gold mine of valuable business information.
Reference: [17] <author> G. Piatetsky-Shapiro, </author> <title> Discovery, Analysis, and Presentation of Strong Rules, </title> <booktitle> In [18], </booktitle> <pages> 229-248. </pages>
Reference-contexts: This intervention can be in the form of domain knowledge to guide the mining process, or additional knowledge as the rules are mined. There has been work on quantifying the "usefulness" or "interestingness" of a rule <ref> [17] </ref>. These ideas may be built as filters on top of the kernel of the rule discovery techniques. The rest of the paper is organized as follows. In Section 2, we present three classes of database mining problems involving classification, associations, and sequences.
Reference: [18] <editor> G. Piatetsky-Shapiro (Editor), </editor> <title> Knowledge Discovery in Databases, </title> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: There is an increasing desire to use this technology in new application domains. One such application domain that is likely to acquire considerable significance in the near future is database mining [12] [3] [5] [8] [9] [11] [15] [16] <ref> [18] </ref> [19]. An increasing number of organizations are creating ultra large data bases (measured in gigabytes and even terabytes) of business data, such as consumer data, transaction histories, sales records, etc. Such data forms a potential gold mine of valuable business information. <p> These classes certainly do not exhaust all database mining applications, but do capture an interesting subset of them. In Section 3, we will present a unifying framework for studying and solving these problems. 2.1 Classification The classification problem [6] [10] [11] <ref> [18] </ref> involves finding rules that partition the given data into disjoint groups. As an example of a classification problem, consider the store location problem.
Reference: [19] <author> Shalom Tsur, </author> <title> "Data Dredging", </title> <journal> IEEE Data Engineering Bulletin, </journal> <volume> 13, 4, </volume> <month> December </month> <year> 1990, </year> <pages> 58-63. </pages>
Reference-contexts: There is an increasing desire to use this technology in new application domains. One such application domain that is likely to acquire considerable significance in the near future is database mining [12] [3] [5] [8] [9] [11] [15] [16] [18] <ref> [19] </ref>. An increasing number of organizations are creating ultra large data bases (measured in gigabytes and even terabytes) of business data, such as consumer data, transaction histories, sales records, etc. Such data forms a potential gold mine of valuable business information. <p> We describe a model and some basic operations for the process of rule discovery. We also show how these database mining problems map to this model and how they can be solved by using the basic operations we propose. Our view of database mining complements the perspective presented in <ref> [9, 19] </ref>. Both of these papers argue for an iterative process for mining with a human in the loop. The user begins with a hypothesis and uses data to refute or confirm the hypothesis. <p> The user begins with a hypothesis and uses data to refute or confirm the hypothesis. The hypothesis is refined, depending on the response and this process continues until a satisfactory theory has been obtained. The emphasis in <ref> [19] </ref> is on having a declarative language that makes it easier to formulate and revise hypotheses. The emphasis in [9] is on providing a large bandwidth between the machine and human so that user-interest is maintained between successive iterations.
Reference: [20] <author> Jeffrey D. Ullman, </author> <title> Principles of Database and Knowledge-Base Systems, Volume I, </title> <publisher> Computer Science Press, </publisher> <year> 1988. </year> <month> 22 </month>
Reference-contexts: It uses the dynamic pruning scheme of IC [1] in the Filter operation. CDP belongs to the class of tree-classifiers [7] [6] [11], and hence can be used to generate rules that can easily be translated into SQL queries for efficient interfacing with relational databases <ref> [20] </ref>. 10 5.1 Basic Operations in CDP We present the implementation of the basic rule discovery operations in CDP . Starting with an empty string in the seed set, these operations are performed in sequence until the seed set become empty.
References-found: 20

