URL: ftp://info.mcs.anl.gov/pub/tech_reports/ip/denupd.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/otc/InteriorPoint/abstracts/Andersen.html
Root-URL: http://www.mcs.anl.gov
Email: E-mail: kda@beta.dou.dk  
Title: A modified Schur complement method for handling dense columns in interior point methods for linear programming  
Author: Knud D. Andersen 
Keyword: Key words: Linear programming, interior-point methods, Cholesky factorization.  
Date: October 7, 1995  
Address: Quinton Lodge, Binswood Avenue, Leamington Spa, Warwickshire, CV32 5TH, UK  
Affiliation: Dash Associates Ltd  
Abstract: The main computational work in interior-point methods for linear programming (LP) is to solve a least-squares problem. The normal equations are often used, but if the LP constraint matrix contains a nearly dense column the normal equations matrix will be nearly dense. Assuming that the non-dense part of the constraint matrix is of full rank, the Schur complement can be used to handle dense columns. In this paper we propose a modified Schur complement method that relaxes this assumption. Encouraging numerical results are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Adler, N. Karmarkar, M.G.C. Resende, and G. Veiga. </author> <title> An implementation of Karmarkar's algorithm for linear programming. </title> <journal> Math. Programming, </journal> <volume> 44 </volume> <pages> 297-235, </pages> <year> 1989. </year>
Reference: [2] <author> E.D. Andersen. </author> <title> Finding all linearly dependent rows in large-scale linear programming. </title> <type> Technical report, </type> <year> 1994. </year>
Reference-contexts: the number of columns in V including columns in W , jW j is the number of columns in F , m is the number of rows in A, NSU stands for No Schur Update and WSU for With Schur Update, CPU is total time in cpu seconds including cross-over <ref> [2] </ref> to a basic feasible solution and presolving [3], and ITER is the total number of iterations. We see in Table 1 that V is sparse.
Reference: [3] <author> E.D. Andersen and K.D. Andersen. </author> <title> Presolving in linear programming. </title> <type> Preprint 35, </type> <institution> Dept. of Math. and Computer Sci., Odense University, </institution> <year> 1993. </year>
Reference-contexts: in W , jW j is the number of columns in F , m is the number of rows in A, NSU stands for No Schur Update and WSU for With Schur Update, CPU is total time in cpu seconds including cross-over [2] to a basic feasible solution and presolving <ref> [3] </ref>, and ITER is the total number of iterations. We see in Table 1 that V is sparse. For p2, the upper bound on the number of nonzeros in V is 15763 fi 51 = 803913 but there are only 29646 nonzeros, a density of 4%.
Reference: [4] <author> E.D. Andersen, K.D. Andersen, and X. Xu. </author> <title> Improved Cholesky decompositions for work-stations and vector computers. </title> <type> Technical report, </type> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: One reason to such a fixed pivot order is sparsity considerations. The method described here is related. 3 Implementation details For general implementation details of a sparse Cholesky factorization, see <ref> [4] </ref>. We will only give implementation details related to the modified Schur complement method in Figure 2.. 3 Normally one would assume that V is fully dense. This is true if the columns in E are fully dense. <p> Both codes use the same Cholesky factorization, to be described in <ref> [4] </ref>. The codes are written in ANSI C and have been ported to several different computers including PC's, HP 9000/715 workstation, SUN SPARCstation 10, different SGI workstations, and a CONVEX C3240 vector computer. All testing presented here was done on a CONVEX C3240 at Odense University.
Reference: [5] <author> K.D. Andersen. </author> <title> An efficient Newton barrier method for minimizing a sum of Euclidean norms. </title> <type> Preprint 42, </type> <institution> Dept. of Math. and Computer Sci., Odense University, </institution> <year> 1993. </year> <note> To appear in SIAM J. on Optim. </note>
Reference-contexts: Andersen). The codes are OPTMZE, a primal-dual interior-point code for linear programming (This code is part of the XPRESS modelling system from Dash Associates Ltd), and GOPT (Global Optimizer), a Newton method for minimizing the sum of L1 or L2 norms subject to linear constraints <ref> [5] </ref>. Both codes use the same Cholesky factorization, to be described in [4]. The codes are written in ANSI C and have been ported to several different computers including PC's, HP 9000/715 workstation, SUN SPARCstation 10, different SGI workstations, and a CONVEX C3240 vector computer.
Reference: [6] <author> I.C. Choi, C.L. Monma, and D.F. Shanno. </author> <title> Further development of a primal-dual interior point method. </title> <journal> ORSA J. on Comput., </journal> (2):304-311, 1990. 
Reference: [7] <author> D.M. Gay. </author> <title> Electronic mail distribution of linear programming test problems. </title> <journal> COAL Newsletter, </journal> <volume> 13 </volume> <pages> 10-12, </pages> <year> 1985. </year>
Reference-contexts: All testing presented here was done on a CONVEX C3240 at Odense University. The method was tested on all problems with dense columns that we had access to. This includes some problems from the netlib test set <ref> [7] </ref> and some from another source. The method was also tested on two minimum sum of norms (MSN) problem using GOPT. These are problems "ssu" and "cl399".
Reference: [8] <author> P.E. Gill, W. Murray, M.A. Saunders, J.A. Tomlin, and M.H. Wright. </author> <title> On the projected Newton barrier methods for linear programming and an equivalence to Karmarkar's projective method. </title> <journal> Math. Programming, </journal> <volume> 36 </volume> <pages> 183-209, </pages> <year> 1986. </year>
Reference: [9] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The John Hopkins University Press, </publisher> <address> Balti-more and London, 2nd edition, </address> <year> 1989. </year>
Reference: [10] <author> M.T. Heath. </author> <title> Some extensions of an algorithm for sparse linear least squares problems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 3(2) </volume> <pages> 223-237, </pages> <year> 1982. </year>
Reference: [11] <author> I.J. Lustig, R.E. Marsten, and D.F. Shanno. </author> <title> Computational experience with a primal-dual interior point method for linear programming. </title> <journal> Linear Algebra Appl., </journal> <volume> 20 </volume> <pages> 191-222, </pages> <year> 1991. </year>
Reference: [12] <author> I.J. Lustig, R.E. Marsten, and D.F. Shanno. </author> <title> On implementing Mehrotra's predictor-corrector interior-point method for linear programming. </title> <journal> SIAM J. on Optim., </journal> <volume> 2(3) </volume> <pages> 435-449, </pages> <year> 1992. </year>
Reference: [13] <author> A. Marxen. </author> <title> Primal barrier methods for linear programming. </title> <type> Report SOL 89-6, </type> <institution> Dept. of Operations Research, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1989. </year>
Reference: [14] <author> M.A. Saunders. </author> <title> Major Cholesky would feel proud. </title> <journal> ORSA J. on Comput., </journal> <volume> 6(1) </volume> <pages> 94-105, </pages> <year> 1994. </year>
Reference: [15] <author> G. Stewart. </author> <title> Modifying Pivot Elements in Gaussian Elimination. </title> <journal> Math. Comp., </journal> <volume> 28(126) </volume> <pages> 537-642, </pages> <year> 1974. </year>
Reference-contexts: If a diagonal element is small we add something to it and introduce a new column in F . It has been pointed out to us of J. Gondizio that Stewart <ref> [15] </ref> describes how to handle null pivots exact due to a priory fixed pivot order in Gaussian Elimination on a square matrix of full rank. One reason to such a fixed pivot order is sparsity considerations.
Reference: [16] <author> R.J. Vanderbei. </author> <title> Splitting dense columns in sparse linear systems. </title> <journal> Linear Algebra Appl., </journal> <volume> 152 </volume> <pages> 107-117, </pages> <year> 1991. </year>
References-found: 16

