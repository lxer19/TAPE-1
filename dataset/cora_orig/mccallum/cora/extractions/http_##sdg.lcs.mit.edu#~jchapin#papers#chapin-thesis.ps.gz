URL: http://sdg.lcs.mit.edu/~jchapin/papers/chapin-thesis.ps.gz
Refering-URL: http://sdg.lcs.mit.edu/~jchapin/publications.html
Root-URL: 
Abstract: HIVE: OPERATING SYSTEM FAULT CONTAINMENT FOR SHARED-MEMORY MULTIPROCESSORS John Chapin Technical Report No. CSL-TR-97-712 January 1997 This research has been supported by DARPA contract DABT63-94-C-0054. Author also acknowledges support from the Fannie and John Hertz Foundation. 
Abstract-found: 1
Intro-found: 1
Reference: [ACC+93] <author> S. Arevalo, J. Carretero, J.L. Castellanos, and F. Barco. </author> <title> A fault-tolerant server on MACH. </title> <booktitle> Nineteenth EUROMICRO Symposium on Microprocessing and Microprogramming, </booktitle> <pages> pp. </pages> <address> 793800 (Barcelona, Spain, </address> <month> September 69, </month> <year> 1993). </year> <journal> Available as Microprocessing & Microprogramming, </journal> <volume> vol. 38, no. 1, </volume> <month> September </month> <year> 1993. </year>
Reference-contexts: Finally, there has been significant work on improving the reliability of applications despite the potential failure of the systems they run on. Common approaches include checkpointing [LiS92, LNP94, PBK+95] and replication of processes on top of microkernels <ref> [LiR93, ACC+93] </ref>.
Reference: [ADN+96] <author> T.E. Anderson, M.D. Dahlin, J.M. Neefe, D.A. Patterson, D.S. Roselli, and R.Y. Wang. </author> <title> Serverless network file systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 14, no. 1, </volume> <pages> pp. 4179, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: It must do all this while tolerating the loss of any cells in the system. File systems of this nature are just now emerging in the research community <ref> [ADN+96] </ref> and will require significant further development before they can be widely used.
Reference: [AGH+89] <author> F. Armand, M. Gien, F. Herrmann, and M. Rozier. </author> <title> Distributing UNIX brings it back to its original virtues. </title> <booktitle> USENIX Workshop on Distributed and Multiprocessor Systems, </booktitle> <pages> pp. </pages> <address> 153174 (Fort Lauderdale, FL, </address> <month> October 56, </month> <year> 1989). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1989. </year>
Reference-contexts: Work on Sprite includes mechanisms for reducing the recovery time of distributed file system state [Bak94] which can be applied to reducing the pause times required to recover consistency of distributed state in a multicellular kernel. Other single-system image distributed systems include MOSIX [BGW93], Chorus/MIX <ref> [AGH+89] </ref>, and Solaris MC [KBM+96]. Solaris MC demonstrates techniques for efficiently implementing a single network identity for the separate kernels with respect to internal processes and external clients. Resource sharing: The classic resource sharing problem for distributed systems is process migration, implemented in the above single-system image systems and others.
Reference: [AhG91] <author> I. Ahmad and A. Ghafoor. </author> <title> Semi-distributed load balancing for massively parallel multicomputer systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 17, no. 10, </volume> <pages> pp. 9871004, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In [UKG+95], the Toronto researchers identify several previous proposals for improving scalability through approaches similar to multicellular structuring: <ref> [AhG91, CGB91, FeR90, ZhB91] </ref>. Hurricane was the first complete operating system implementation based on these ideas, but neither it nor the previous proposals investigate the reliability benefits of the architecture as Hive does. Many of the mechanisms in Hive parallel similar mechanisms in Hurricane and Tornado.
Reference: [BaB95] <author> J.M. Barton and N. Bitar. </author> <title> A scalable multi-discipline, multiple-processor scheduling framework for IRIX. </title> <booktitle> Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pp. </pages> <address> 4569 (Santa Barbara, CA, April 25, 1995). Berlin, Germany: </address> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Mechanisms used to achieve efficient performance from UNIX SVR4 on small-scale parallel systems are described in [SPY+93, Pea92, CBB+91, CHS91]. Mechanisms in the Sun Microsystems versions of UNIX SVR4 are described in [EKB+92]. Work on the Silicon Graphics version is described in <ref> [BaB95] </ref>. These efforts are all focused on achieving sufficient parallelization to avoid synchronization bottlenecks on four- or eight-processor systems. The largest-scale systems constructed to date that provide standard SMP semantics are non-shared-memory multicomputers.
Reference: [Bak94] <author> M. Baker. </author> <title> Fast Crash Recovery in Distributed File Systems. </title> <type> Doctoral dissertation, technical report CSD-94-787, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: The later versions include process migration and distributed shared memory. Sprite [OCD+88] implements process migration and a high-performance SSI distributed file system. Work on Sprite includes mechanisms for reducing the recovery time of distributed file system state <ref> [Bak94] </ref> which can be applied to reducing the pause times required to recover consistency of distributed state in a multicellular kernel. Other single-system image distributed systems include MOSIX [BGW93], Chorus/MIX [AGH+89], and Solaris MC [KBM+96].
Reference: [Bar81] <author> J.F. Bartlett. </author> <title> A NonStop kernel. </title> <booktitle> Eighth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. </pages> <address> 2229 (Pacific Grove, CA, </address> <month> December </month> <year> 1981). </year> <title> Available as Operating Systems Review, </title> <journal> vol. </journal> <volume> 15, no. 5, </volume> <month> December </month> <year> 1981. </year> <month> 128 </month>
Reference-contexts: Other large applications, such as multimedia and web servers, naturally split into multiple processes where each services a client or group of clients. Nondecomposable applications that require high availability can be restructured as process pairs <ref> [Bar81, SiS92] </ref>. A system with fault containment will support such applications well if it exposes sufficient control over resource allocation that the processes of the pair can avoid common points of failure.
Reference: [Bea79] <author> M.D. Beaudry. </author> <title> A statistical analysis of failures in the SLAC computing center. </title> <booktitle> Digest of papers, COMPCON, Spring 79, </booktitle> <pages> pp. </pages> <address> 4952 (San Francisco, February 26March 1, 1979). New York: </address> <publisher> IEEE, </publisher> <year> 1979. </year>
Reference-contexts: Systematic efforts to understand the types of errors that affect operating systems started with studies of IBM mainframes, since these were the dominant computing platform for commercial use. An ongoing study of systems at Stanford produced <ref> [Bea79] </ref> [VeI84, MoA87].
Reference: [BGW93] <author> A. Barak, S. Guday, and R. Wheeler. </author> <title> The MOSIX Distributed Operating System: Load Balancing for UNIX. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Work on Sprite includes mechanisms for reducing the recovery time of distributed file system state [Bak94] which can be applied to reducing the pause times required to recover consistency of distributed state in a multicellular kernel. Other single-system image distributed systems include MOSIX <ref> [BGW93] </ref>, Chorus/MIX [AGH+89], and Solaris MC [KBM+96]. Solaris MC demonstrates techniques for efficiently implementing a single network identity for the separate kernels with respect to internal processes and external clients.
Reference: [BSS+96] <author> D.L. Black, R.D. Smith, S.J. Sears, and R.W. Dean. FLIPC: </author> <title> a low latency messaging system for distributed real time environments. </title> <booktitle> USENIX 1996 Annual Technical Conference, </booktitle> <pages> pp. </pages> <address> 229238 (San Diego, CA, </address> <month> January 2226, </month> <year> 1996). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1996. </year>
Reference-contexts: The Hive RPC subsystem and the SIPS mechanism closely resemble other low-latency message delivery systems such as active messages [vEC+92], U-Net [vEB+95], and FLIPC <ref> [BSS+96] </ref>. The key difference from these previous systems is that SIPS is implemented directly by the memory controller and thus can take advantage of optimizations unavailable to the other systems.
Reference: [BuB93] <author> R.W. Buskens and R.P. Bianchini. </author> <title> Distributed on-line diagnosis in the presence of arbitrary faults. </title> <booktitle> Twenty-Third International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 470479 (Toulouse, France, June 2224, 1993). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Theoretical work on this problem is surveyed in [Dah87] and [LeS94]. Distributed implementations, which do not require a separate fault-free control processor to execute the diagnosis algorithm, include [Dah86] and <ref> [BuB93] </ref>. The algorithm used in Hive is simpler and less efficient than some known algorithms, so it should be possible to reduce the null recovery latency and the chance of incorrect diagnosis by using a more sophisticated approach.
Reference: [CaS82] <author> X. Castillo and D.P. Siewiorek. </author> <title> A workload dependent software reliability prediction model. </title> <booktitle> Twelfth International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 279286 (Santa Monica, CA, June 1982). Long Beach, CA: </address> <publisher> IEEE Computer Society, </publisher> <year> 1982. </year>
Reference-contexts: This leads to an increased number of dynamic interactions between processes, increasing the potential for stimulating any latent parallelism-related faults in the operating system code. The relationship between system workload and software error rate is well-documented <ref> [CaS82, MoA87] </ref>.
Reference: [CBB+91] <author> M. Campbell, R. Barton, J. Browning, D. Cervenka, B. Curry, T. Davis, T. Edmonds, R. Holt, J. Slice, T. Smith, and R. Wescott. </author> <title> The parallelization of UNIX System V Release 4.0. </title> <booktitle> Winter 1991 USENIX Conference, </booktitle> <pages> pp. </pages> <address> 307323 (Dallas, TX, </address> <month> January 2125, </month> <year> 1991). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1991. </year>
Reference-contexts: Mechanisms used to achieve efficient performance from UNIX SVR4 on small-scale parallel systems are described in <ref> [SPY+93, Pea92, CBB+91, CHS91] </ref>. Mechanisms in the Sun Microsystems versions of UNIX SVR4 are described in [EKB+92]. Work on the Silicon Graphics version is described in [BaB95]. These efforts are all focused on achieving sufficient parallelization to avoid synchronization bottlenecks on four- or eight-processor systems.
Reference: [CBR95] <author> R. Chillarege, S. Biyani, and J. Rosenthal. </author> <title> Measurement of failure rate in widely distributed software. </title> <booktitle> Twenty-Fifth International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 424433 (Pasadena, CA, June 2730, 1995). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: Since this data was collected from VAXclusters, which are more tightly coupled than the separate machines on a standard LAN, the software error rate is probably higher than would be observed on independent machines. <ref> [CBR95] </ref> analyzes problems reported to IBMs service organization about two releases of a large IBM operating system. This data covers software errors only, not hardware errors, and includes errors that did not lead to system failures.
Reference: [CDV+94] <author> R. Chandra, S. Devine, B. Verghese, A. Gupta, and M. Rosenblum. </author> <title> Scheduling and page migration for multiprocessor compute servers. </title> <booktitle> Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. </pages> <address> 1224 (San Jose, CA, </address> <month> October 47, </month> <year> 1994). </year> <journal> Available as SIGPLAN Notices, </journal> <volume> vol. 29, no. 11, </volume> <month> November </month> <year> 1994. </year> <note> References 129 </note>
Reference-contexts: Common approaches include checkpointing [LiS92, LNP94, PBK+95] and replication of processes on top of microkernels [LiR93, ACC+93]. Scalability: Few researchers have investigated the techniques required to support general-purpose workloads on large-scale shared-memory multiprocessors, because these machines have only recently become available for general-purpose use. <ref> [CDV+94] </ref> and [VDG+96] investigate algorithms for page replication and migration and how these must work together with processor scheduling to reduce memory system costs. [CHR+95] characterizes the performance bottlenecks of an SMP kernel running on a CC-NUMA system. [SDH+96] describes a new file system created by Silicon Graphics to support general-purpose <p> Even without changing the overall algorithm, the simple ood algorithm used for reliable broadcast could be improved to terminate early if no cells have failed. Mechanisms for doing this are described in <ref> [CDV+94] </ref>. The Hive diagnosis algorithm makes strong assumptions about the observability of errors. In particular, it does not behave predictably with respect to software faults that cause intermittent errors.
Reference: [CGB91] <author> D.R. Cheriton, H.A. Goosen, and P.D. Boyle. </author> <title> Paradigm: a highly scalable shared-memory multicomputer architecture. </title> <journal> Computer, </journal> <volume> vol. 24, no. 2, </volume> <pages> pp. 3346, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: In [UKG+95], the Toronto researchers identify several previous proposals for improving scalability through approaches similar to multicellular structuring: <ref> [AhG91, CGB91, FeR90, ZhB91] </ref>. Hurricane was the first complete operating system implementation based on these ideas, but neither it nor the previous proposals investigate the reliability benefits of the architecture as Hive does. Many of the mechanisms in Hive parallel similar mechanisms in Hurricane and Tornado.
Reference: [ChB94] <author> R. Chillarege and S. Biyani. </author> <title> Identifying risk using ODC based growth models. </title> <booktitle> Fifth International Symposium on Software Reliability Engineering, </booktitle> <pages> pp. </pages> <address> 282288 (Monterey, CA, November 69, 1994). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Unlike other kernel software architectures that assume that the operating system is correct, the multicellular kernel architecture provides reliability with respect to operating system software errors, which is important because most failures observed in the field are caused by software errors <ref> [Gra90, CVJ92, ChB94] </ref>. A multicellular architecture improves scalability because few kernel data structures are shared by processes running on different cells. <p> The software error rate dominates the hardware error rate on current systems <ref> [Gra90, CVJ92, ChB94] </ref> and is likely to do so even for machines several times larger than the largest current multiprocessor. Reducing the rate of software errors may provide sufficient reliability improvements to eliminate the need for more radical changes.
Reference: [ChC96] <author> J. Christmansson and R. Chillarege. </author> <title> Generation of an error set that emulates software faults based on field data. </title> <booktitle> Twenty-Sixth International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 304313 (Sendai, Japan, June 2527, 1996). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: The most relevant field failure data for Hives error model is provided by <ref> [ChC96] </ref>. This study attempts to develop a statistically valid error model, using field data from a large IBM operating system. <p> Reliability prediction: Many of the papers discussed above, especially those written or advised by R. K. Iyer, provide mathematical models that assist in predicting reliability from failure data. Notable examples include [LeI92, LeI93, TaI92b, TaI93]. <ref> [ChC96] </ref> considers how to design experiments so that reliability prediction based on fault injection studies will be statistically valid, while [EIP+91] uses data from system test of a UNIX-based AT&T network management system to analyze the validity of various mathematical models.
Reference: [CHR+95] <author> J. Chapin, S.A. Herrod, M. Rosenblum, and A. Gupta. </author> <title> Memory system performance of UNIX on CC-NUMA multiprocessors. </title> <booktitle> 1995 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. </pages> <address> 113 (Ottawa, Ontario, Canada, </address> <month> May 1519, </month> <year> 1995). </year> <title> Available as Performance Evaluation Review, </title> <journal> vol. </journal> <volume> 23, no. 1, </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: Rather than giving complete details of the study here, I highlight a few selected points. Interested readers may refer to <ref> [CHR+95] </ref> for details and much more data. SMP OS limitations 15 2.2.1 Experimental setup Detailed trace data was collected from a DASH configuration with eight clusters (Figure 2.1). Each cluster is a slightly modified Silicon Graphics POWER Station 4D/340, which is a bus-based multiprocessor with four 33-MHz MIPS R3000 processors. <p> Few researchers have investigated the techniques required to support general-purpose workloads on large-scale shared-memory multiprocessors, because these machines have only recently become available for general-purpose use. [CDV+94] and [VDG+96] investigate algorithms for page replication and migration and how these must work together with processor scheduling to reduce memory system costs. <ref> [CHR+95] </ref> characterizes the performance bottlenecks of an SMP kernel running on a CC-NUMA system. [SDH+96] describes a new file system created by Silicon Graphics to support general-purpose workloads that access very large amounts of data. 10.2 Multiprocessor operating systems Multicellular architecture: The scalability benefits of the multicellular architecture for multiprocessor operating
Reference: [CHS91] <author> M. Campbell, R. Holt, and J. </author> <title> Slice. Lock granularity tuning mechanisms in SVR4/MP. Symposium on Experiences with Distributed and Multiprocessor Systems, </title> <booktitle> SEDMS II, </booktitle> <pages> pp. </pages> <address> 221228 (Atlanta, GA, March 2122, 1991). Berkeley, CA: </address> <publisher> USENIX Association, </publisher> <year> 1991. </year>
Reference-contexts: Mechanisms used to achieve efficient performance from UNIX SVR4 on small-scale parallel systems are described in <ref> [SPY+93, Pea92, CBB+91, CHS91] </ref>. Mechanisms in the Sun Microsystems versions of UNIX SVR4 are described in [EKB+92]. Work on the Silicon Graphics version is described in [BaB95]. These efforts are all focused on achieving sufficient parallelization to avoid synchronization bottlenecks on four- or eight-processor systems.
Reference: [ChT96] <author> T.D. Chandra and S. Toueg. </author> <title> Unreliable failure detectors for reliable distributed systems. </title> <journal> Journal of the ACM, </journal> <volume> vol. 43, no. 2, </volume> <pages> pp. 225267, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: In particular, it does not behave predictably with respect to software faults that cause intermittent errors. Algorithms are known that can tolerate a wider range of faults. [LeS94] surveys system-level diagnosis algorithms in which the test results are only probabilistically correct, while <ref> [ChT96] </ref> considers the general problem of consensus and voting when only weak failure detection is possible. 10.4 Error model and reliability prediction Field failure data: Studies on the errors that affect systems in the field help the design of Hive and similar systems in two ways.
Reference: [CIR+93] <author> R.H. Campbell, N. Islam, D. Raila, and P. Madany. </author> <title> Designing and implementing Choices: an object-oriented system in C++. </title> <journal> Communications of the ACM, </journal> <volume> vol. 36, no. 9, </volume> <pages> pp. 117126, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: All of these techniques could be applied to the individual cells of a multicellular kernel to improve reliability. One large area of work has been software engineering techniques. Object-oriented designs such as Choices <ref> [CIR+93] </ref> and Spring [MGH+94] improve the modularity of the system by adding strong internal interfaces, so the operating system is easier to understand and maintain and hence should have fewer software faults over time than a traditional monolithic kernel.
Reference: [CNC+96] <author> P. Chen, W. Ng, S. Chandra, C. Aycock, G. Rajamani, and D. Lowell. </author> <title> The Rio file cache: surviving operating system crashes. </title> <booktitle> Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. </pages> <address> 7483 (Cambridge, MA, </address> <month> October 15, </month> <year> 1996). </year> <title> Available as Operating Systems Review, </title> <journal> vol. </journal> <volume> 30, no. 5, </volume> <month> October </month> <year> 1996. </year> <month> 130 </month>
Reference-contexts: SimOS only implements 32-bit virtual and 29-bit physical addresses. These are significantly smaller than the 64-bit virtual and 40-bit physical addresses of FLASH. Peter Chen has argued that the 64-bit virtual address space of a next-generation processor reduces the probability of wild writes <ref> [CNC+96] </ref>. If so, the inaccuracy caused by SimOS strengthens the primary experimental results in this dissertation, which argue that Hive successfully provides fault containment despite the possibility of wild writes. Table 5.4. Percent of kernel time spent on level 1 cache misses. <p> Since SimOS is deterministic and all 1000 experiments start from the same checkpoint, the corrupted instruction is guaranteed to be executed at least once during the parallel make run. Instruction corruption uses the methodology developed in <ref> [CNC+96] </ref>. Figure 6.4 shows the decision tree used to determine how the instruction is corrupted. If the instruction is a branch and the decision tree randomly decides to change the destination register, the sense of the branch is inverted, simulating an off-by-one bug or inverted conditional. <p> If the machine had been configured with 512 MB of memory, the system would have had a wild write frequency of 3%. It is clear that wild writes are a nontrivial problem. It is interesting to compare these results to <ref> [CNC+96] </ref>, in which a similar fault injection methodology led to memory corruption in 11 out of 650 stimulated crashes of Digital UNIX running on an Alpha workstation with 128 MB of memory. Hive shows nearly the same rate, 8 firewall violations out of 612 stimulated cell failures. <p> This rate would only become significant for very large multiprocessors that are partitioned into minimally-sized cells. Fault injection: In addition to field failure data, researchers have also used fault injection studies to examine the detailed effects of errors. Work in the RIO project reported in <ref> [CNC+96] </ref> uses instruction corruption to study the probability of data integrity violations due to wild writes in Digital UNIX. As described in Section 6.5.4, these studies match the results of similar experiments on Hive quite closely. [KIT93] reports the results of similar experiments using SunOS 4.1.2. <p> More fundamentally, this research demonstrates that the traditional assumptions about the inherent unreliability of shared memory systems are incorrect. Hive draws a fault containment boundary inside the shared memory boundary and gains reliability without sacrificing resource sharing. In this regard, Hive is part of a widespread research effort <ref> [CNC+96, Gil96, WLA+93] </ref> to reevaluate the fundamental ways in which shared memory can be used. References 127
Reference: [CVJ92] <author> R. Cramp, M.A. Vouk, and W. Jones. </author> <title> On operational availability of a large software-based telecommunications system. </title> <booktitle> Third International Symposium on Software Reliability Engineering, </booktitle> <pages> pp. </pages> <address> 358366 (Research Triangle Park, NC, October 7 10, 1992). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: Unlike other kernel software architectures that assume that the operating system is correct, the multicellular kernel architecture provides reliability with respect to operating system software errors, which is important because most failures observed in the field are caused by software errors <ref> [Gra90, CVJ92, ChB94] </ref>. A multicellular architecture improves scalability because few kernel data structures are shared by processes running on different cells. <p> The software error rate dominates the hardware error rate on current systems <ref> [Gra90, CVJ92, ChB94] </ref> and is likely to do so even for machines several times larger than the largest current multiprocessor. Reducing the rate of software errors may provide sufficient reliability improvements to eliminate the need for more radical changes.
Reference: [Dah86] <author> A.T. Dahbura. </author> <title> An efficient algorithm for identifying the most likely fault set in a probabilistically diagnosable system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, no. 4, </volume> <pages> pp. 354356, </pages> <month> April </month> <year> 1986. </year>
Reference-contexts: Theoretical work on this problem is surveyed in [Dah87] and [LeS94]. Distributed implementations, which do not require a separate fault-free control processor to execute the diagnosis algorithm, include <ref> [Dah86] </ref> and [BuB93]. The algorithm used in Hive is simpler and less efficient than some known algorithms, so it should be possible to reduce the null recovery latency and the chance of incorrect diagnosis by using a more sophisticated approach.
Reference: [Dah87] <author> A.T. Dahbura. </author> <title> System-level diagnosis: a perspective for the third decade. Princeton Workshop on Algorithm, </title> <booktitle> Architecture, and Technology Issues for Models of Concurrent Computation, </booktitle> <pages> pp. </pages> <address> 411434 (Princeton, NJ, </address> <note> September 30October 1, 1987). Available as S.K. </note> <editor> Tewksbury, B.W. Dickinson, and S.C. Schwartz, eds., </editor> <booktitle> Concurrent Computations: Algorithms, Architecture, and Technology. </booktitle> <address> New York: </address> <publisher> Plenum Press, </publisher> <year> 1987. </year>
Reference-contexts: The challenge of determining the liveset is the classic system-level diagnosis problem: given a set of test results from cells testing each other, compute the set of faulty cells. Theoretical work on this problem is surveyed in <ref> [Dah87] </ref> and [LeS94]. Distributed implementations, which do not require a separate fault-free control processor to execute the diagnosis algorithm, include [Dah86] and [BuB93].
Reference: [Dat96] <author> Data General Corporation. </author> <title> NUMA: delivering the next level of commodity SMP performance. Document 4855, VIEWPOINT no. </title> <type> 5, </type> <year> 1996. </year>
Reference-contexts: This exibility offers the opportunity to design new hardware and operating system software features that work together to provide novel system functionality. 1.2 Motivation for a new operating system The developers of FLASH and most other scalable CC-NUMA machines (including those manufactured by Silicon Graphics [Sil96], Sequent [LoC96], Data General <ref> [Dat96] </ref>, and Hewlett-Packard [HP95]) intend them to be general-purpose multiprocessors.
Reference: [DWA+94] <author> M.D. Dahlin, R.Y. Wang, T.E. Anderson, and D.A. Patterson. </author> <title> Cooperative caching: using remote client memory to improve file system performance. </title> <booktitle> First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pp. </pages> <address> 267280 (Monterey, CA, </address> <month> November 1417, </month> <year> 1994). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1994. </year>
Reference-contexts: The high amount and cost of memory in desktop workstations has also stimulated development of systems that use the memory of other systems on a local-area network as paging devices. Apollo DOMAIN was an early system to implement this functionality [LLD+83]. More recent systems, including cooperative caching <ref> [DWA+94] </ref> and GMS [FMP+95], have implemented policies to globally optimize the file pages cached across the machines of the system.
Reference: [EIP+91] <author> W.K. Ehrlich, A. Iannino, B.S. Prasanna, J.P. Stampfel, and J.R. Wu. </author> <title> How faults cause software failures: implications for software reliability engineering. </title> <booktitle> Second International Symposium on Software Reliability Engineering, </booktitle> <pages> pp. </pages> <address> 233241 (Austin, TX, May 1718, 1991). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: K. Iyer, provide mathematical models that assist in predicting reliability from failure data. Notable examples include [LeI92, LeI93, TaI92b, TaI93]. [ChC96] considers how to design experiments so that reliability prediction based on fault injection studies will be statistically valid, while <ref> [EIP+91] </ref> uses data from system test of a UNIX-based AT&T network management system to analyze the validity of various mathematical models. Finally, [Ham92] provides a non-mathematical introduction to the requirements for effectively predicting software reliability, while [Ham96] provides a more detailed methodology.
Reference: [EKB+92] <author> J.R. Eykholt, S.R. Kleiman, S. Barton, R. Faulkner, A. Shivalingiah, M. Smith, D. Stein, J. Voll, M. Weeks, and D. Williams. </author> <title> Beyond multiprocessing: multithreading the SunOS kernel. </title> <booktitle> Summer 1992 USENIX Conference, </booktitle> <pages> pp. </pages> <address> 1118 (San Antonio, TX, June 812, 1992). Berkeley, CA: </address> <publisher> USENIX Association, </publisher> <year> 1992. </year>
Reference-contexts: Mechanisms used to achieve efficient performance from UNIX SVR4 on small-scale parallel systems are described in [SPY+93, Pea92, CBB+91, CHS91]. Mechanisms in the Sun Microsystems versions of UNIX SVR4 are described in <ref> [EKB+92] </ref>. Work on the Silicon Graphics version is described in [BaB95]. These efforts are all focused on achieving sufficient parallelization to avoid synchronization bottlenecks on four- or eight-processor systems. The largest-scale systems constructed to date that provide standard SMP semantics are non-shared-memory multicomputers.
Reference: [FeR90] <author> D.G. Feitelson and L. Rudolph. </author> <title> Distributed hierarchical control for parallel processing. </title> <journal> Computer, </journal> <volume> vol. 23, no. 5, </volume> <pages> pp. 6577, </pages> <month> May </month> <year> 1990. </year> <note> References 131 </note>
Reference-contexts: In [UKG+95], the Toronto researchers identify several previous proposals for improving scalability through approaches similar to multicellular structuring: <ref> [AhG91, CGB91, FeR90, ZhB91] </ref>. Hurricane was the first complete operating system implementation based on these ideas, but neither it nor the previous proposals investigate the reliability benefits of the architecture as Hive does. Many of the mechanisms in Hive parallel similar mechanisms in Hurricane and Tornado.
Reference: [FMP+95] <author> M.J. Feeley, W.E. Morgan, F.H. Pighin, A.R. Karlin, H.M. Levy, and C.A. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. </pages> <address> 201212 (Copper Mountain Resort, CO, </address> <month> December 36, </month> <year> 1995). </year> <title> Available as Operating Systems Review, </title> <journal> vol. </journal> <volume> 29, no. 5, </volume> <month> December </month> <year> 1995. </year>
Reference-contexts: Apollo DOMAIN was an early system to implement this functionality [LLD+83]. More recent systems, including cooperative caching [DWA+94] and GMS <ref> [FMP+95] </ref>, have implemented policies to globally optimize the file pages cached across the machines of the system.
Reference: [Gal96] <author> M. Galles. </author> <title> Scalable pipelined interconnect for distributed endpoint routing: the SGI SPIDER chip. </title> <booktitle> Hot Interconnects Symposium IV (Stanford, </booktitle> <address> CA, </address> <month> August </month> <year> 1996). </year>
Reference-contexts: Network recovery is complicated enough that FLASH implements it with code that runs on the main compute processor, in the same way as memory system recovery. More information: See [TBG+] for more details on memory system and network recovery and <ref> [Gal96] </ref> for normal operation of the network and data link levels. Hive prototype 39 Chapter 4 Hive prototype The architecture described in the previous chapter is a system design. This chapter describes the current Hive prototype that implements the design and some of FLASH features that support it. <p> instruction cache 32 kilobytes 2-way associative 64-byte lines Primary data cache 32 kilobytes 2-way associative 32-byte lines Unified secondary cache 1 megabyte 2-way associative 128-byte lines 15 disks Accurate model of HP 97560 drive [KTR94] 256 MB memory 8 FLASH nodes, 32 megabytes each Mesh interconnect 400 MHz SPIDER routers <ref> [Gal96] </ref> 8 MAGIC controllers 100 MHz 1 megabyte direct-mapped data cache 16 kilobyte 2-way associative instruction cache 50 5.3 Reliability experiments compilation, the files to be compiled are chosen from among the source files in the distribution to minimize the length difference between the shortest and longest source files. <p> This includes significant amounts of microcode and careful physical design but only a small amount of custom logic. The routers contain logic dedicated to recovering from hardware errors, but this is not considered custom because it was designed without knowledge of the Hive memory fault model <ref> [Gal96] </ref>. 110 9.1 Hardware support Software fault containment: The memory system provides the firewall (Section 3.2.3, Section 6.2.1, and Section 7.1.5). The firewall includes a small amount of custom logic plus data storage of 64 bits per 4 kilobyte page (0.2% memory overhead).
Reference: [Gil96] <author> R.B. Gillett. </author> <title> Memory channel network for PCI. </title> <journal> IEEE Micro, </journal> <volume> vol. 16, no. 1, </volume> <pages> pp. 1218, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: More fundamentally, this research demonstrates that the traditional assumptions about the inherent unreliability of shared memory systems are incorrect. Hive draws a fault containment boundary inside the shared memory boundary and gains reliability without sacrificing resource sharing. In this regard, Hive is part of a widespread research effort <ref> [CNC+96, Gil96, WLA+93] </ref> to reevaluate the fundamental ways in which shared memory can be used. References 127
Reference: [GLL+90] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. </pages> <address> 1526 (Seattle, WA, May 2831, 1990). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1990. </year>
Reference-contexts: TIMEOUT; while (timeout-- &gt;= 0) unsigned short a = p-&gt;v; if (a & 0x1) continue; *local_copy = *remote_struct; if (a == p-&gt;v) return SUCCESS; -return ERROR; The only subtlety of this algorithm is the placement of the hardware store barriers, which are needed on machines with relaxed memory consistency models <ref> [GLL+90] </ref>. Hive does not use the store barriers. The base IRIX implementation is not well-labeled so FLASH must run in sequential consistency mode to execute Hive correctly. When implementing Hive, an unexpected usage pattern for the publishers lock appeared.
Reference: [Gra90] <author> J. Gray. </author> <title> A census of Tandem system availability between 1985 and 1990. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> vol. 39, no. 4, </volume> <pages> pp. 409418, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Unlike other kernel software architectures that assume that the operating system is correct, the multicellular kernel architecture provides reliability with respect to operating system software errors, which is important because most failures observed in the field are caused by software errors <ref> [Gra90, CVJ92, ChB94] </ref>. A multicellular architecture improves scalability because few kernel data structures are shared by processes running on different cells. <p> The software error rate dominates the hardware error rate on current systems <ref> [Gra90, CVJ92, ChB94] </ref> and is likely to do so even for machines several times larger than the largest current multiprocessor. Reducing the rate of software errors may provide sufficient reliability improvements to eliminate the need for more radical changes.
Reference: [GrC96] <author> M. Greenwald and D. Cheriton. </author> <title> The synergy between non-blocking synchronization and operating system structure. </title> <booktitle> Second USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pp. </pages> <address> 123136 (Seattle, WA, </address> <month> October 28 31, </month> <year> 1996). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1996. </year>
Reference-contexts: For example, cells could publish translations for their internal virtual addresses, making it efficient for remote cells to walk virtual pointer chains (as long as virtual translations used in remotely-read data structures are valid for minimum time durations, which can be implemented using techniques for type-stable memory management <ref> [GrC96] </ref>). In contrast, neither of the two more-aggressive experiments with kernel-level memory sharing appears to be successful. The remote process creation subsystem could not reuse local process creation code, and the anonymous memory manager does not on average save any significant time.
Reference: [Ham92] <author> D. Hamlet. </author> <title> Are we testing for true reliability? IEEE Software, </title> <journal> vol. </journal> <volume> 9, no. 4, </volume> <pages> pp. 2127, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Notable examples include [LeI92, LeI93, TaI92b, TaI93]. [ChC96] considers how to design experiments so that reliability prediction based on fault injection studies will be statistically valid, while [EIP+91] uses data from system test of a UNIX-based AT&T network management system to analyze the validity of various mathematical models. Finally, <ref> [Ham92] </ref> provides a non-mathematical introduction to the requirements for effectively predicting software reliability, while [Ham96] provides a more detailed methodology. Conclusions 125 Chapter 11 Conclusions The goal of this research has been to improve the reliability and scalability of large multiprocessors used as general-purpose compute servers.
Reference: [Ham96] <author> D. Hamlet. </author> <title> Predicting dependability by testing. </title> <booktitle> 1996 International Symposium on Software Testing and Analysis, </booktitle> <pages> pp. </pages> <address> 8491 (San Diego, CA, </address> <month> January 810, </month> <year> 1996). </year> <booktitle> Available as SIGSOFT Software Engineering Notes, </booktitle> <volume> vol. 21, no. 3, </volume> <month> May </month> <year> 1996. </year>
Reference-contexts: Finally, [Ham92] provides a non-mathematical introduction to the requirements for effectively predicting software reliability, while <ref> [Ham96] </ref> provides a more detailed methodology. Conclusions 125 Chapter 11 Conclusions The goal of this research has been to improve the reliability and scalability of large multiprocessors used as general-purpose compute servers. Contributions: This dissertation has made three contributions towards this goal.
Reference: [HJK93] <author> Y. Huang, P. Jalote, and C. Kintala. </author> <title> Two techniques for transient software error recovery. Hardware and Software Architectures for Fault Tolerance, pp. </title> <address> 159170 (Le Mont Saint Michel, France, June 1993). Berlin, Germany: </address> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Another approach, implemented in MVS/XA and Fault-Tolerant Mach [RSS93], is to abort the operation in progress when an error is detected and 118 10.2 Multiprocessor operating systems retry it. <ref> [HJK93] </ref> provides an excellent analysis of the reasons why an aborted operation frequently succeeds when retried. Finally, there has been significant work on improving the reliability of applications despite the potential failure of the systems they run on.
Reference: [HJL93] <author> R. Horst, D. Jewett, and D. Lenoski. </author> <title> The risk of data corruption in microprocessor-based systems. </title> <booktitle> Twenty-Third International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 576585 (Toulouse, France, June 2224, 1993). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year> <month> 132 </month>
Reference-contexts: The study breaks these and other coarse categories down into a joint distribution of specific fault and error types. Finally, several studies examine the frequency of types of errors that are outside Hives error model. <ref> [HJL93] </ref> considers the possibility of non-fail-fast errors in microprocessors, concluding that a data integrity violation occurs once per month in a population of 10,000 processors built with 1990-era technology.
Reference: [HKO+94] <author> M. Heinrich, J. Kuskin, D. Ofelt, J. Heinlein, J. Baxter, J.P. Singh, R. Simoni, K. Gharachorloo, D. Nakahira, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The performance impact of exibility in the Stanford FLASH multiprocessor. </title> <booktitle> Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. </pages> <address> 274284 (San Jose, CA, </address> <month> October 4 7, </month> <year> 1994). </year> <journal> Available as SIGPLAN Notices, </journal> <volume> vol. 29, no. 11, </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: Simulation model: The experiments use the Mipsy processor model, configured with the same caches as the MIPS R10000 used in FLASH. Memory access latencies are computed using the reference memory system model developed by the FLASH hardware team, called Flashlite <ref> [HKO+94] </ref>.
Reference: [HP95] <author> HP-Convex Technology Center. </author> <title> Convex Examplar scalable computing systems feature message passing and shared memory for programming exibility. </title> <publisher> Press release, </publisher> <month> November 22, </month> <year> 1995. </year>
Reference-contexts: the opportunity to design new hardware and operating system software features that work together to provide novel system functionality. 1.2 Motivation for a new operating system The developers of FLASH and most other scalable CC-NUMA machines (including those manufactured by Silicon Graphics [Sil96], Sequent [LoC96], Data General [Dat96], and Hewlett-Packard <ref> [HP95] </ref>) intend them to be general-purpose multiprocessors.
Reference: [Iye95] <author> R. Iyer. </author> <title> Experimental evaluation. </title> <booktitle> Twenty-Fifth International Symposium on Fault-Tolerant Computing, Special Issue, </booktitle> <pages> pp. </pages> <address> 115132 (Pasadena, CA, June 2730, 1995). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: There have been other studies on failure rates in the field, but the systems examined differ more from the SMP operating systems currently used by commercial multiprocessor vendors than those investigated in these studies. <ref> [Iye95] </ref> provides a survey of the literature. 2.1.2 Failure rates of large machines Scaling to larger systems will cause the MTBF of SMP operating systems to decrease significantly.
Reference: [Jew91] <author> D. Jewett. </author> <title> Integrity S2: a fault-tolerant Unix platform. </title> <booktitle> Twenty-First International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 512519 (Montreal, Quebec, Canada, June 2527, 1991). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Assuming that software faults remain in the system and therefore software errors must be tolerated, improving reliability requires adding code that checks and repairs internal data structures as in the Tandem Nonstop-UX kernel <ref> [Jew91] </ref>. If done thoroughly, this approach promises to add comparable or greater complexity than 114 9.4 Limitations of the architecture integrating these same data structures into a distributed single system image, with the disadvantage that only the specific errors checked for can be tolerated. <p> Another area of work has been techniques for avoiding the reboot that is required to recover from software errors in standard SMP kernels. Both the Nonstop-UX kernel for Tandem Integrity S2 <ref> [Jew91] </ref> and the IBM MVS/XA system [MoA87] provide data-structure-specific repair routines invoked when an inconsistency is detected. <p> The separate kernels use a heartbeat protocol and a distributed consensus mechanism to agree on which processors are alive. Reliability with respect to software faults is provided by a high number of assertions that shut down a kernel when it detects any internal inconsistency. The Tandem Nonstop-UX kernel <ref> [Jew91, TIY+95] </ref> runs as a uniprocessor UNIX kernel on top of triply-redundant hardware. The redundant hardware masks most hardware failures, while the operating system detects software errors using assertions and recovers using forward recovery routines.
Reference: [Joh89] <author> B. Johnson. </author> <title> Design and Analysis of Fault-Tolerant Digital Systems. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: effect that increases time spent spinning on locks at user level, indicating that it is important to spread the kernel workload evenly across the multiple cells when supporting parallel applications that synchronize frequently. 10 1.10 Terminology 1.10 Terminology This dissertation follows the standard terminology used to describe dependable computing systems <ref> [Joh89] </ref>. A fault is a latent problem such as a mistake in a line of code or a short circuit on a chip.
Reference: [KBM+96] <author> Y.A. Khalidi, J.M. Bernabeu, V. Matena, K. Shirriff, and M. Thadani. </author> <title> Solaris MC: a multicomputer OS. </title> <booktitle> USENIX 1996 Annual Technical Conference, </booktitle> <pages> pp. </pages> <address> 191203 (San Diego, CA, </address> <month> January 2226, </month> <year> 1996). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1996. </year>
Reference-contexts: I discuss each of these parts of the system in turn. 3.3.1 Distributed system At the distributed system level, Hive is similar to previous single-system image distributed systems such as Sprite [OCD+88], Locus [PoW85], and Solaris MC <ref> [KBM+96] </ref>. Hive is implemented using techniques borrowed from these systems: The cells communicate primarily through remote procedure calls. Hive architecture 27 The cells cooperate in a shared-namespace distributed file system with each acting as both client and server. Process management is distributed so users and applications see a single-system image. <p> Work on Sprite includes mechanisms for reducing the recovery time of distributed file system state [Bak94] which can be applied to reducing the pause times required to recover consistency of distributed state in a multicellular kernel. Other single-system image distributed systems include MOSIX [BGW93], Chorus/MIX [AGH+89], and Solaris MC <ref> [KBM+96] </ref>. Solaris MC demonstrates techniques for efficiently implementing a single network identity for the separate kernels with respect to internal processes and external clients. Resource sharing: The classic resource sharing problem for distributed systems is process migration, implemented in the above single-system image systems and others.
Reference: [KEM+78] <author> D. Katsuki, E. Elsam, W. Mann, E. Roberts, J. Robinson, F. Skowronski, and E. Wolf. </author> <title> Pluribusan operational fault-tolerant multiprocessor. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 66, no. 10, </volume> <pages> pp. 11461159, </pages> <month> October </month> <year> 1978. </year>
Reference-contexts: These techniques could be applied to the internal data structures of a modern SMP kernel to improve its reliability with respect to software faults. At the same time that the C.mmp project was active at Carnegie-Mellon University, BBN built the Pluribus multiprocessor as a network switch for the ARPANET <ref> [KEM+78] </ref>. Although not a general-purpose multiprocessor, Pluribus is notable for its focus on fault containment implemented in software as a reliability mechanism. The operating system partitions the machine, run a separate instance of the operating system in each partition, and uses a consensus mechanism to integrate or exclude partitions.
Reference: [KIT93] <author> W.-I. Kao, R.K. Iyer, and D. Tang. </author> <title> FINE: a fault injection and monitoring environment for tracing the UNIX system behavior under faults. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 19, no. 11, </volume> <pages> pp. 11051118, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Work in the RIO project reported in [CNC+96] uses instruction corruption to study the probability of data integrity violations due to wild writes in Digital UNIX. As described in Section 6.5.4, these studies match the results of similar experiments on Hive quite closely. <ref> [KIT93] </ref> reports the results of similar experiments using SunOS 4.1.2.
Reference: [KOH+94] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> Twenty-First Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. </pages> <address> 302313 (Chicago, April 1821, 1994). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year> <note> References 133 </note>
Reference-contexts: The name arises because unlike bus-based machines, where all memory is accessible in uniform time, the physically-distributed memory of a CC-NUMA machine makes some addresses slower and some faster to access from the perspective of any given processor. Like other CC-NUMA multiprocessors, FLASH uses a directory-based cache coherence protocol <ref> [KOH+94] </ref>. The core of the FLASH design is a programmable protocol processor in each node that implements the complex algorithms required for directory-based cache coherence. The protocol processor chip is called MAGIC (Memory and General Interconnect Controller).
Reference: [Kri95] <author> O. Krieger. </author> <title> HFS: A Flexible File System for Shared-Memory Multiprocessors. </title> <type> Doctoral dissertation, </type> <institution> Department of Electrical and Computer Engineering, University of Toronto, </institution> <year> 1995. </year>
Reference-contexts: Their initial work on the Hurricane operating system <ref> [UKG+95, Kri95] </ref> running on the Hector multiprocessor [VSL+91] has been followed by current work on the Tornado operating system [PGK+95] running on the NUMAchine multiprocessor [VBS+95]. In [UKG+95], the Toronto researchers identify several previous proposals for improving scalability through approaches similar to multicellular structuring: [AhG91, CGB91, FeR90, ZhB91].
Reference: [KTR94] <author> D. Kotz, S. Toh, and S. Radhakrishnan. </author> <title> A detailed simulation of the HP 97560 disk drive. </title> <type> Technical report PCS-TR94-20, </type> <institution> Dartmouth College, </institution> <year> 1994. </year>
Reference-contexts: Component Characteristics 8 processors 200 MHz MIPS R4000 1 CPI when no cache misses Primary instruction cache 32 kilobytes 2-way associative 64-byte lines Primary data cache 32 kilobytes 2-way associative 32-byte lines Unified secondary cache 1 megabyte 2-way associative 128-byte lines 15 disks Accurate model of HP 97560 drive <ref> [KTR94] </ref> 256 MB memory 8 FLASH nodes, 32 megabytes each Mesh interconnect 400 MHz SPIDER routers [Gal96] 8 MAGIC controllers 100 MHz 1 megabyte direct-mapped data cache 16 kilobyte 2-way associative instruction cache 50 5.3 Reliability experiments compilation, the files to be compiled are chosen from among the source files in
Reference: [LeI92] <author> I. Lee and K. Iyer. </author> <title> Analysis of software halts in the Tandem GUARDIAN operating system. </title> <booktitle> Third International Symposium on Software Reliability Engineering, </booktitle> <pages> pp. </pages> <address> 227236 (Research Triangle Park, NC, October 710, 1992). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: Tandem operating systems have also been studied extensively because of their intended use in environments with high availability requirements. <ref> [LeI92] </ref> and [LeI93] examine the Guardian90 operating system, while [TIY+95] examines the Nonstop-UX operating system. [LeI93] and [TIY+95] are particularly interesting because they provide detailed information on the types and locations of programming mistakes in the operating system code. [LeI93] notes that timing errors are the most significant single cause of <p> Reliability prediction: Many of the papers discussed above, especially those written or advised by R. K. Iyer, provide mathematical models that assist in predicting reliability from failure data. Notable examples include <ref> [LeI92, LeI93, TaI92b, TaI93] </ref>. [ChC96] considers how to design experiments so that reliability prediction based on fault injection studies will be statistically valid, while [EIP+91] uses data from system test of a UNIX-based AT&T network management system to analyze the validity of various mathematical models.
Reference: [LeI93] <author> I. Lee and R.K. Iyer. </author> <title> Faults, symptoms, and software fault tolerance in the Tandem GUARDIAN90 operating system. </title> <booktitle> Twenty-Third International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 2029 (Toulouse, France, June 2224, 1993). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: This was true both for the number of faults in the code and for the number of failures resulting from those faults <ref> [LeI93] </ref>. SMP OS limitations 13 As a general-purpose machine grows in size, not only does the operating system require increased internal parallelism, but more applications run at the same time. <p> Tandem operating systems have also been studied extensively because of their intended use in environments with high availability requirements. [LeI92] and <ref> [LeI93] </ref> examine the Guardian90 operating system, while [TIY+95] examines the Nonstop-UX operating system. [LeI93] and [TIY+95] are particularly interesting because they provide detailed information on the types and locations of programming mistakes in the operating system code. [LeI93] notes that timing errors are the most significant single cause of system failures <p> Tandem operating systems have also been studied extensively because of their intended use in environments with high availability requirements. [LeI92] and <ref> [LeI93] </ref> examine the Guardian90 operating system, while [TIY+95] examines the Nonstop-UX operating system. [LeI93] and [TIY+95] are particularly interesting because they provide detailed information on the types and locations of programming mistakes in the operating system code. [LeI93] notes that timing errors are the most significant single cause of system failures in the field, while missing operations (pointer or variable initialization, data update or <p> because of their intended use in environments with high availability requirements. [LeI92] and <ref> [LeI93] </ref> examine the Guardian90 operating system, while [TIY+95] examines the Nonstop-UX operating system. [LeI93] and [TIY+95] are particularly interesting because they provide detailed information on the types and locations of programming mistakes in the operating system code. [LeI93] notes that timing errors are the most significant single cause of system failures in the field, while missing operations (pointer or variable initialization, data update or message send) are the most common type of low-level programming mistake. [TIY+95] observes that pointer manipulation errors and missing checks for illegal data values <p> Reliability prediction: Many of the papers discussed above, especially those written or advised by R. K. Iyer, provide mathematical models that assist in predicting reliability from failure data. Notable examples include <ref> [LeI92, LeI93, TaI92b, TaI93] </ref>. [ChC96] considers how to design experiments so that reliability prediction based on fault injection studies will be statistically valid, while [EIP+91] uses data from system test of a UNIX-based AT&T network management system to analyze the validity of various mathematical models.
Reference: [LeS94] <author> S. Lee and K. G. Shin. </author> <title> Probabilistic diagnosis of multiprocessor systems. </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 26, no. 1, </volume> <pages> pp. 121139, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The challenge of determining the liveset is the classic system-level diagnosis problem: given a set of test results from cells testing each other, compute the set of faulty cells. Theoretical work on this problem is surveyed in [Dah87] and <ref> [LeS94] </ref>. Distributed implementations, which do not require a separate fault-free control processor to execute the diagnosis algorithm, include [Dah86] and [BuB93]. <p> Mechanisms for doing this are described in [CDV+94]. The Hive diagnosis algorithm makes strong assumptions about the observability of errors. In particular, it does not behave predictably with respect to software faults that cause intermittent errors. Algorithms are known that can tolerate a wider range of faults. <ref> [LeS94] </ref> surveys system-level diagnosis algorithms in which the test results are only probabilistically correct, while [ChT96] considers the general problem of consensus and voting when only weak failure detection is possible. 10.4 Error model and reliability prediction Field failure data: Studies on the errors that affect systems in the field help
Reference: [LiR93] <author> J. Lipkis and M. Rozier. </author> <title> Fault tolerance enablers in the CHORUS microkernel. Hardware and Software Architectures for Fault Tolerance, pp. </title> <address> 182190 (Le Mont Saint Michel, France, June 1993). Berlin, Germany: </address> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Finally, there has been significant work on improving the reliability of applications despite the potential failure of the systems they run on. Common approaches include checkpointing [LiS92, LNP94, PBK+95] and replication of processes on top of microkernels <ref> [LiR93, ACC+93] </ref>.
Reference: [LiS92] <author> M. Litzkow and M. Solomon. </author> <title> Supporting checkpointing and process migration outside the UNIX kernel. </title> <booktitle> Winter 1992 USENIX Conference, </booktitle> <pages> pp. </pages> <address> 283290 (San Francisco, </address> <month> January </month> <year> 2024, 1992). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1992. </year>
Reference-contexts: Batch processes such as engineering and scientific simulation or graphics rendering tend to do all their I/O to files or directly to humans (e.g. graphical output and computation steering). Checkpointing such applications is straightforward, and various user-level checkpointing libraries have been written that require little effort to exploit <ref> [LiS92, PBK+95] </ref>. <p> Finally, there has been significant work on improving the reliability of applications despite the potential failure of the systems they run on. Common approaches include checkpointing <ref> [LiS92, LNP94, PBK+95] </ref> and replication of processes on top of microkernels [LiR93, ACC+93]. <p> Solaris MC demonstrates techniques for efficiently implementing a single network identity for the separate kernels with respect to internal processes and external clients. Resource sharing: The classic resource sharing problem for distributed systems is process migration, implemented in the above single-system image systems and others. Condor <ref> [LiS92] </ref> provides checkpointing and process migration on top of UNIX without kernel changes, while [MZD+93] describes a process migration mechanism on top of Mach that is transparent to applications. [Nut94] surveys systems that provide migration.
Reference: [LLD+83] <author> P. Leach, P. Levine, B. Douros, J. Hamilton, D. Nelson, and B. Stumpf. </author> <title> The architecture of an integrated local network. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 1, no. 5, </volume> <pages> pp. 842857, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: The high amount and cost of memory in desktop workstations has also stimulated development of systems that use the memory of other systems on a local-area network as paging devices. Apollo DOMAIN was an early system to implement this functionality <ref> [LLD+83] </ref>. More recent systems, including cooperative caching [DWA+94] and GMS [FMP+95], have implemented policies to globally optimize the file pages cached across the machines of the system.
Reference: [LLG+92] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M.S. Lam. </author> <title> The Stanford Dash multiprocessor. </title> <journal> Computer, </journal> <volume> vol. 25, no. 3, </volume> <pages> pp. 6379, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: This section reports data on these memory system costs and the problems that SMP operating systems face in reducing them. The data comes from a study of an SMP operating system, IRIX 5.2, running on a 32-processor CC-NUMA machine, the Stanford DASH <ref> [LLG+92] </ref>. IRIX has been parallelized to run efficiently on large bus-based multiprocessors (SGI Challenge machines support up to 36 MIPS processors), so its memory system behavior is not skewed by bad synchronization behavior at this system size.
Reference: [LNP94] <author> K. Li, J.F. Naughton, and J.S. Plank. </author> <title> Low-latency, concurrent checkpointing for parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 5, no. 8, </volume> <pages> pp. 874879, </pages> <month> August </month> <year> 1994. </year> <month> 134 </month>
Reference-contexts: Checkpointing such applications is straightforward, and various user-level checkpointing libraries have been written that require little effort to exploit [LiS92, PBK+95]. Operating system support can also be useful for checkpointing <ref> [LNP94, SiS92] </ref> and could easily be integrated into a multicellular kernel. 1.7 Success conditions Although fault containment appears to be useful for a wide range of general-purpose applications, this is not sufficient by itself to justify the immense investment required to adopt a new kernel architecture. <p> Finally, there has been significant work on improving the reliability of applications despite the potential failure of the systems they run on. Common approaches include checkpointing <ref> [LiS92, LNP94, PBK+95] </ref> and replication of processes on top of microkernels [LiR93, ACC+93].
Reference: [LoC96] <author> T. Lovett and R. Clapp. STiNG: </author> <title> a CC-NUMA computer system for the commercial marketplace. </title> <booktitle> Twenty-Third Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. </pages> <address> 308317 (Philadelphia, PA, </address> <month> May 2224, </month> <year> 1996). </year> <title> Available as Computer Architecture News, </title> <journal> vol. </journal> <volume> 24, no. 2, </volume> <month> May </month> <year> 1996. </year>
Reference-contexts: This exibility offers the opportunity to design new hardware and operating system software features that work together to provide novel system functionality. 1.2 Motivation for a new operating system The developers of FLASH and most other scalable CC-NUMA machines (including those manufactured by Silicon Graphics [Sil96], Sequent <ref> [LoC96] </ref>, Data General [Dat96], and Hewlett-Packard [HP95]) intend them to be general-purpose multiprocessors.
Reference: [LSP82] <author> L. Lamport, R. Shostak, and M. Pease. </author> <title> The Byzantine Generals problem. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 382401, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: However, before a cell reaches that point it may issue wild writes to arbitrary addresses or send corrupt messages to other cells. 22 3.2 Fault containment architecture In a key simplification of the error model, cells are assumed not to exhibit Byzantine faults <ref> [LSP82] </ref>. In particular, the sanity-checks that cells apply to messages and data received from other cells are assumed to detect all incorrect messages. Furthermore, a cell that appears failed to one correct cell will appear failed to all correct cells that subsequently test it.
Reference: [Lyn96] <author> N. Lynch. </author> <title> Distributed Algorithms. </title> <address> San Francisco: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: The success of the fault injection experiments described in Chapter 6 suggests that these assumptions are reasonable. These assumptions free Hive from the performance impact and implementation complexity of Byzantine fault-tolerant distributed algorithms <ref> [Lyn96] </ref>. The assumptions are strong enough that intermittent faults can cause incorrect behavior; if this turns out to be a significant problem in practice, algorithms are known that enable weakening the assumptions without implementing full Byzantine fault tolerance (see Chapter 10). <p> These votes are then intersected to produce agreement. The distributed algorithm used is a fault-tolerant ood algorithm, in which each cell broadcasts its current understanding of the intersection as many times as there are cells in the system <ref> [Lyn96] </ref>. When the liveset process completes agreement, each cell independently compares the new liveset to the previous one to compute the die set, the set of cells that have failed.
Reference: [MaF90] <author> R.A. Maxion and F.E. Feather. </author> <title> A case study of Ethernet anomalies in a distributed computing environment. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> vol. 39, no. 4, </volume> <pages> pp. 433443, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: This suggests that more sophisticated data checking mechanisms may be required for very large multiprocessors, although future microprocessors are likely to incorporate more internal redundancy and integrity checks which reduce the error rate. <ref> [MaF90] </ref> examines cases in which an operating system begins babbling, that is, ooding the network and thus degrading the performance of the system. Such a failure would bypass the recovery mechanisms 124 10.4 Error model and reliability prediction in Hive.
Reference: [MGH+94] <author> J.G. Mitchell, J.J. Gibbons, G. Hamilton, P.B. Kessler, Y.A. Khalidi, P. Kougiouris, P.W. Madany, M.N. Nelson, </author> <title> M.L. Powell, and S.R. Radia. An overview of the Spring system. </title> <booktitle> COMPCON, Spring 94, </booktitle> <pages> pp. </pages> <address> 122131 (San Francisco, February 28March 4, 1994). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: All of these techniques could be applied to the individual cells of a multicellular kernel to improve reliability. One large area of work has been software engineering techniques. Object-oriented designs such as Choices [CIR+93] and Spring <ref> [MGH+94] </ref> improve the modularity of the system by adding strong internal interfaces, so the operating system is easier to understand and maintain and hence should have fewer software faults over time than a traditional monolithic kernel.
Reference: [MoA87] <author> S. Mourad and D. Andrews. </author> <title> On the reliability of the IBM MVS/XA operating system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. SE-13, no. 10, </volume> <pages> pp. 11351139, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: This leads to an increased number of dynamic interactions between processes, increasing the potential for stimulating any latent parallelism-related faults in the operating system code. The relationship between system workload and software error rate is well-documented <ref> [CaS82, MoA87] </ref>. <p> Another area of work has been techniques for avoiding the reboot that is required to recover from software errors in standard SMP kernels. Both the Nonstop-UX kernel for Tandem Integrity S2 [Jew91] and the IBM MVS/XA system <ref> [MoA87] </ref> provide data-structure-specific repair routines invoked when an inconsistency is detected. <p> Systematic efforts to understand the types of errors that affect operating systems started with studies of IBM mainframes, since these were the dominant computing platform for commercial use. An ongoing study of systems at Stanford produced [Bea79] <ref> [VeI84, MoA87] </ref>.
Reference: [MZD+93] <author> D.S. Milojicic, W. Zint, A. Dangel, and P. Giese. </author> <title> Task migration on the top of the Mach microkernel. </title> <booktitle> USENIX Mach III Symposium, </booktitle> <pages> pp. </pages> <address> 273289 (Santa Fe, NM, </address> <month> April </month> <year> 1921, 1993). </year> <institution> Berkley, CA: USENIX Association, </institution> <year> 1993. </year>
Reference-contexts: Resource sharing: The classic resource sharing problem for distributed systems is process migration, implemented in the above single-system image systems and others. Condor [LiS92] provides checkpointing and process migration on top of UNIX without kernel changes, while <ref> [MZD+93] </ref> describes a process migration mechanism on top of Mach that is transparent to applications. [Nut94] surveys systems that provide migration.
Reference: [Nut94] <author> M. Nuttall. </author> <title> A brief survey of systems providing process or object migration facilities. </title> <journal> Operating Systems Review, </journal> <volume> vol. 28, no. 4, </volume> <pages> pp. 6480, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Condor [LiS92] provides checkpointing and process migration on top of UNIX without kernel changes, while [MZD+93] describes a process migration mechanism on top of Mach that is transparent to applications. <ref> [Nut94] </ref> surveys systems that provide migration. The high amount and cost of memory in desktop workstations has also stimulated development of systems that use the memory of other systems on a local-area network as paging devices. Apollo DOMAIN was an early system to implement this functionality [LLD+83].
Reference: [OCD+88] <author> J.K. Ousterhout, A.R. Cherenson, F. Douglis, M.N. Nelson, and B.B. Welch. </author> <title> The Sprite network operating system. </title> <journal> Computer, </journal> <volume> vol. 21, no. 2, </volume> <pages> pp. 2336, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The cell boundaries must not add high performance overheads when resources are shared. Single-system image: The cells must cooperate to present a standard SMP OS interface to applications and users. Many of the problems faced in implementing a multicellular kernel also arise in single-system-image distributed systems such as Sprite <ref> [OCD+88] </ref> and Locus [PoW85]. <p> Finally, add the resource sharing features required to achieve performance competitive with the original SMP operating system. I discuss each of these parts of the system in turn. 3.3.1 Distributed system At the distributed system level, Hive is similar to previous single-system image distributed systems such as Sprite <ref> [OCD+88] </ref>, Locus [PoW85], and Solaris MC [KBM+96]. Hive is implemented using techniques borrowed from these systems: The cells communicate primarily through remote procedure calls. Hive architecture 27 The cells cooperate in a shared-namespace distributed file system with each acting as both client and server. <p> The Locus work was initially commercialized in AIX/TCF from IBM [WaP89] and later as a portable SSI layer that is one component of OSF/1 AD TNC [ZRB+93]. The later versions include process migration and distributed shared memory. Sprite <ref> [OCD+88] </ref> implements process migration and a high-performance SSI distributed file system. Work on Sprite includes mechanisms for reducing the recovery time of distributed file system state [Bak94] which can be applied to reducing the pause times required to recover consistency of distributed state in a multicellular kernel.
Reference: [PBK+95] <author> J.S. Plank, M. Beck, G. Kingsley, and K. Li. Libckpt: </author> <title> transparent checkpointing under Unix. </title> <booktitle> 1995 USENIX Technical Conference, </booktitle> <pages> pp. </pages> <address> 213224 (New Orleans, LA, </address> <month> January 16 20, </month> <year> 1995). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1995. </year>
Reference-contexts: Batch processes such as engineering and scientific simulation or graphics rendering tend to do all their I/O to files or directly to humans (e.g. graphical output and computation steering). Checkpointing such applications is straightforward, and various user-level checkpointing libraries have been written that require little effort to exploit <ref> [LiS92, PBK+95] </ref>. <p> Finally, there has been significant work on improving the reliability of applications despite the potential failure of the systems they run on. Common approaches include checkpointing <ref> [LiS92, LNP94, PBK+95] </ref> and replication of processes on top of microkernels [LiR93, ACC+93].
Reference: [Pea92] <author> J.K. Peacock. </author> <title> File system multithreading in System V Release 4 MP. </title> <booktitle> Summer 1992 USENIX Conference, </booktitle> <pages> pp. </pages> <address> 1929 (San Antonio, TX, June 812, 1992). Berkeley, CA: </address> <publisher> USENIX Association, </publisher> <year> 1992. </year> <note> References 135 </note>
Reference-contexts: Mechanisms used to achieve efficient performance from UNIX SVR4 on small-scale parallel systems are described in <ref> [SPY+93, Pea92, CBB+91, CHS91] </ref>. Mechanisms in the Sun Microsystems versions of UNIX SVR4 are described in [EKB+92]. Work on the Silicon Graphics version is described in [BaB95]. These efforts are all focused on achieving sufficient parallelization to avoid synchronization bottlenecks on four- or eight-processor systems.
Reference: [PGK+95] <author> E. Parsons, B. Gamsa, O. Krieger, and M. Stumm. </author> <title> (De-)clustering objects for multiprocessor system software. </title> <booktitle> Fourth International Workshop on Object-Orientation in Operating Systems, </booktitle> <pages> pp. </pages> <address> 7281 (Lund, Sweden, August 1415, 1995). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: Their initial work on the Hurricane operating system [UKG+95, Kri95] running on the Hector multiprocessor [VSL+91] has been followed by current work on the Tornado operating system <ref> [PGK+95] </ref> running on the NUMAchine multiprocessor [VBS+95]. In [UKG+95], the Toronto researchers identify several previous proposals for improving scalability through approaches similar to multicellular structuring: [AhG91, CGB91, FeR90, ZhB91].
Reference: [PoW85] <author> G. Popek and B. Walker, eds. </author> <title> The LOCUS Distributed System Architecture. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: Single-system image: The cells must cooperate to present a standard SMP OS interface to applications and users. Many of the problems faced in implementing a multicellular kernel also arise in single-system-image distributed systems such as Sprite [OCD+88] and Locus <ref> [PoW85] </ref>. <p> I discuss each of these parts of the system in turn. 3.3.1 Distributed system At the distributed system level, Hive is similar to previous single-system image distributed systems such as Sprite [OCD+88], Locus <ref> [PoW85] </ref>, and Solaris MC [KBM+96]. Hive is implemented using techniques borrowed from these systems: The cells communicate primarily through remote procedure calls. Hive architecture 27 The cells cooperate in a shared-namespace distributed file system with each acting as both client and server. <p> Network partitions: The fault model implies that hardware faults do not lead to network partitions. Although software techniques for managing network partitions are known <ref> [PoW85] </ref>, enough separate links cross each slice of the FLASH interconnect that the implementation complexity of these software techniques is not justified. A network partition will cause Hive to fail. 6.3.3 Limitations of the FLASH implementation FLASH does not implement the full memory fault model. <p> Single-system image: Locus <ref> [PoW85] </ref> pioneered single-system image support for distributed systems. In addition to a shared filesystem and distributed process management, it demonstrates techniques for coping with network partitions.
Reference: [RAA+88] <author> M. Rozier, V. Abrossimov, F. Armand, I. Boule, M. Gien, M. Guillemont, F. Herrmann, C. Kaiser, S. Langlois, P. Leonard, and W. Neuhauser. </author> <title> CHORUS distributed operating systems. </title> <journal> Computing Systems, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 305370, </pages> <month> Fall </month> <year> 1988. </year>
Reference-contexts: An orthogonal approach has been to partition the system into a microkernel and a set of services running in independent address spaces, making the system easier to test and debug. Examples of this approach include Hydra [WLH81], Mach [RJO+89], and Chorus <ref> [RAA+88] </ref>. Finally, automated testing [SaH94] can increase the fraction of faults that are found and fixed during operating system development. Another area of work has been techniques for avoiding the reboot that is required to recover from software errors in standard SMP kernels.
Reference: [RBD+] <author> M. Rosenblum, E. Bugnion, S. Devine, and S. Herrod. </author> <title> Using the SimOS machine simulator to study complex computer systems. </title> <publisher> ACM TOMACS, in press. </publisher>
Reference-contexts: Hive is implemented as an extensive modification of the IRIX 5.2 code base. Overview 9 1.9 Experimental evaluation Because the FLASH machine is not yet operational, the experiments reported in this dissertation use the SimOS machine simulator <ref> [RHW+95, WiR96, RBD+] </ref>. SimOS offers a choice of processor and memory system models that trade off between the speed and accuracy of simulation. <p> The first section of the chapter describes SimOS. The second gives information about the performance experiments. The third describes the reliability experiments. The final section analyzes how using simulation affects the experimental results. 5.1 The SimOS simulation environment SimOS <ref> [RHW+95, WiR96, RBD+] </ref> is a machine simulation environment that simulates the hardware of uniprocessor and multiprocessor computer systems in enough detail to boot, run, and study a commercial operating system. 1 Specifically, SimOS provides simulators of processors, caches, memory systems, and a number of different I/O devices including SCSI disks, ethernet
Reference: [RHW+95] <author> M. Rosenblum, S.A. Herrod, E. Witchel, and A. Gupta. </author> <title> Complete computer system simulation: the SimOS approach. </title> <journal> IEEE Parallel & Distributed Technology: Systems & Applications, </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 3443, </pages> <month> Winter </month> <year> 1995. </year>
Reference-contexts: Hive is implemented as an extensive modification of the IRIX 5.2 code base. Overview 9 1.9 Experimental evaluation Because the FLASH machine is not yet operational, the experiments reported in this dissertation use the SimOS machine simulator <ref> [RHW+95, WiR96, RBD+] </ref>. SimOS offers a choice of processor and memory system models that trade off between the speed and accuracy of simulation. <p> The first section of the chapter describes SimOS. The second gives information about the performance experiments. The third describes the reliability experiments. The final section analyzes how using simulation affects the experimental results. 5.1 The SimOS simulation environment SimOS <ref> [RHW+95, WiR96, RBD+] </ref> is a machine simulation environment that simulates the hardware of uniprocessor and multiprocessor computer systems in enough detail to boot, run, and study a commercial operating system. 1 Specifically, SimOS provides simulators of processors, caches, memory systems, and a number of different I/O devices including SCSI disks, ethernet <p> The processor simulator called Mipsy models a static-pipeline processor (one instruction per cycle, one outstanding cache miss at a time) while the simulator called MXS models a dynamic-pipeline processor (out-of-order and speculative execution, multiple outstanding cache misses). 1. The description in Section 5.1 is largely taken from <ref> [RHW+95] </ref>. 48 5.1 The SimOS simulation environment Checkpoints: SimOS can save the entire state of the simulated machine at any time during execution. This saved state, which includes the contents of all registers, main memory, and I/O devices, can then be restored at a later time. <p> Simulator validation: Validating the accuracy of these statistics is less of an issue for this dissertation than for most other simulation-based studies, because performance is not the focus of the results. However, SimOS has been partially validated as reported in <ref> [RHW+95] </ref>. As for correctness of execution, the applications are unmodified from and generate the same output data as the code that runs on real IRIX systems, and it is difficult to imagine the kernel code successfully completing these complex workloads if SimOS did not execute it correctly. <p> The effects on measured operating system performance of using the simpler processor model while keeping the rest of the hardware configuration unchanged are examined in detail in <ref> [RHW+95] </ref>. Using a make workload on a uniprocessor version of IRIX 5.3, that study found that the dynamically-scheduled processor hides about 50% of the kernel level 1 cache miss time experienced by the static-pipeline processor, but only about 15% of the kernel level 2 cache miss time. <p> The memory system used in <ref> [RHW+95] </ref> satisfies level 2 cache misses in a uniform 300 nsec, which is substantially faster than FLASH. This suggests that changing from a static-pipeline processor to a dynamically-scheduled processor will have a minor impact on kernel level 2 cache miss time in FLASH. <p> This could add 10 msec or more if many lines are cached exclusively. The total cost is far too expensive to pay on each change in TLB mappings, which can occur as often as once every 1000 user instructions in some workloads <ref> [RHW+95] </ref>. Hive uses a more relaxed policy. Write access to a page is granted to all processors of a cell when any process on that cell faults the page into a writable portion of its address space. <p> These benefits appear when the number of processors per cell drops to two or one. There is no fundamental reason (at least on FLASH) that two processors per cell are better than four. However, a uniprocessor kernel is substantially more efficient than a multiprocessor kernel <ref> [RHW+95] </ref>.
Reference: [RJO+89] <author> R. Rashid, D. Julin, D. Orr, R. Sanzi, R. Baron, A. Forin, D. Golub, and M. Jones. </author> <title> Mach: a system software kernel. </title> <booktitle> COMPCON, Spring 89, </booktitle> <pages> pp. </pages> <address> 176178 (San Francisco, February 27March 3, 1989). Washington, DC: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1989. </year>
Reference-contexts: An orthogonal approach has been to partition the system into a microkernel and a set of services running in independent address spaces, making the system easier to test and debug. Examples of this approach include Hydra [WLH81], Mach <ref> [RJO+89] </ref>, and Chorus [RAA+88]. Finally, automated testing [SaH94] can increase the fraction of faults that are found and fixed during operating system development. Another area of work has been techniques for avoiding the reboot that is required to recover from software errors in standard SMP kernels.
Reference: [RMS96] <author> M.Z. Rela, H. Madeira, and J.G. Silva. </author> <title> Experimental evaluation of the fail-silent behaviour in programs with consistency checks. </title> <booktitle> Twenty-Sixth International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 394403 (Sendai, Japan, June 2527, 1996). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: The Esprit project on Fault Tolerant Massively Parallel Systems has supported several error injection studies that considered questions relevant to improving the reliability of operating systems. <ref> [RMS96] </ref> uses pin-level error injection, while [SCM+96] uses internal debugging features of the PowerPC 601 to emulate errors in architecturally hidden processor functional units. These studies demonstrate a significant chance that naive applications will produce incorrect results when errors are injected into one of the systems processors.
Reference: [RSS93] <author> M. Russinovich, Z. Segall, </author> <title> and D.P. Siewiorek. Application transparent fault management in fault tolerant Mach. </title> <booktitle> Twenty-Third International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 1019 (Toulouse, France, June 2224, 1993). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Both the Nonstop-UX kernel for Tandem Integrity S2 [Jew91] and the IBM MVS/XA system [MoA87] provide data-structure-specific repair routines invoked when an inconsistency is detected. Another approach, implemented in MVS/XA and Fault-Tolerant Mach <ref> [RSS93] </ref>, is to abort the operation in progress when an error is detected and 118 10.2 Multiprocessor operating systems retry it. [HJK93] provides an excellent analysis of the reasons why an aborted operation frequently succeeds when retried.
Reference: [SaH94] <author> S. Sankar and R. Hayes. </author> <title> ADLan interface definition language for specifying and testing software. </title> <booktitle> ACM Workshop on Interface Definition Languages, </booktitle> <pages> pp. </pages> <address> 1321 (Portland, OR, </address> <month> January 20, </month> <year> 1994). </year> <journal> Available as SIGPLAN Notices, </journal> <volume> vol. 29, no. 8, </volume> <month> August </month> <year> 1994. </year> <month> 136 </month>
Reference-contexts: An orthogonal approach has been to partition the system into a microkernel and a set of services running in independent address spaces, making the system easier to test and debug. Examples of this approach include Hydra [WLH81], Mach [RJO+89], and Chorus [RAA+88]. Finally, automated testing <ref> [SaH94] </ref> can increase the fraction of faults that are found and fixed during operating system development. Another area of work has been techniques for avoiding the reboot that is required to recover from software errors in standard SMP kernels.
Reference: [SCM+96] <author> J.G. Silva, J. Carreira, H. Madeira, D. Costa, and P. Moreira. </author> <title> Experimental assessment of parallel systems. </title> <booktitle> Twenty-Sixth International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 415424 (Sendai, Japan, June 2527, 1996). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: The Esprit project on Fault Tolerant Massively Parallel Systems has supported several error injection studies that considered questions relevant to improving the reliability of operating systems. [RMS96] uses pin-level error injection, while <ref> [SCM+96] </ref> uses internal debugging features of the PowerPC 601 to emulate errors in architecturally hidden processor functional units. These studies demonstrate a significant chance that naive applications will produce incorrect results when errors are injected into one of the systems processors.
Reference: [SDH+96] <author> A. Sweeney, D. Doucette, W. Hu, C. Anderson, M. Nishimoto, and G. Peck. </author> <title> Scalability in the XFS file system. </title> <booktitle> USENIX 1996 Annual Technical Conference, </booktitle> <pages> pp. </pages> <address> 1 14 (San Diego, CA, </address> <month> January 2226, </month> <year> 1996). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1996. </year>
Reference-contexts: multiprocessors, because these machines have only recently become available for general-purpose use. [CDV+94] and [VDG+96] investigate algorithms for page replication and migration and how these must work together with processor scheduling to reduce memory system costs. [CHR+95] characterizes the performance bottlenecks of an SMP kernel running on a CC-NUMA system. <ref> [SDH+96] </ref> describes a new file system created by Silicon Graphics to support general-purpose workloads that access very large amounts of data. 10.2 Multiprocessor operating systems Multicellular architecture: The scalability benefits of the multicellular architecture for multiprocessor operating systems have been investigated in an ongoing project at the University of Toronto, which
Reference: [SGK+85] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and implementation of the Sun network filesystem. </title> <booktitle> Summer 1985 USENIX Conference, </booktitle> <pages> pp. </pages> <address> 119130 (Portland, OR, </address> <month> June </month> <year> 1985). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1985. </year>
Reference-contexts: Hive prototype 45 4.4 File system The intercell file system in Hive is called cell-NFS. It is a variant of the standard NFS distributed file system <ref> [SGK+85] </ref>. This file system provides functionality sufficient for the prototype but is not designed to provide the functionality needed for a production-quality multicellular kernel. Name space: In cell-NFS each cell is the file server for a portion of the file system name space.
Reference: [Sil96] <author> Silicon Graphics Inc. </author> <title> Origin family of technical and enterprise servers. </title> <publisher> Press release, </publisher> <month> October 7, </month> <year> 1996. </year>
Reference-contexts: This exibility offers the opportunity to design new hardware and operating system software features that work together to provide novel system functionality. 1.2 Motivation for a new operating system The developers of FLASH and most other scalable CC-NUMA machines (including those manufactured by Silicon Graphics <ref> [Sil96] </ref>, Sequent [LoC96], Data General [Dat96], and Hewlett-Packard [HP95]) intend them to be general-purpose multiprocessors.
Reference: [SiS92] <author> D. Siewiorek and R. Swarz. </author> <title> Reliable computer systems: design and evaluation (second edition). </title> <address> Burlington, MA: </address> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: Other large applications, such as multimedia and web servers, naturally split into multiple processes where each services a client or group of clients. Nondecomposable applications that require high availability can be restructured as process pairs <ref> [Bar81, SiS92] </ref>. A system with fault containment will support such applications well if it exposes sufficient control over resource allocation that the processes of the pair can avoid common points of failure. <p> Checkpointing such applications is straightforward, and various user-level checkpointing libraries have been written that require little effort to exploit [LiS92, PBK+95]. Operating system support can also be useful for checkpointing <ref> [LNP94, SiS92] </ref> and could easily be integrated into a multicellular kernel. 1.7 Success conditions Although fault containment appears to be useful for a wide range of general-purpose applications, this is not sufficient by itself to justify the immense investment required to adopt a new kernel architecture. <p> If agreement fails, i.e. if some cell in the liveset has an incorrect liveset, the system is prone to significant corruption and should shut down immediately (this is the well-known split-brain syndrome <ref> [SiS92] </ref>). Evaluation: It was straightforward to integrate information dissemination through shared memory (using the publishers lock and the careful reference protocol) into the design of the fault containment subsystem. In some cases it significantly simplified the implementation, as in the use of the alive field. <p> Finally, the transaction processing multiprocessors built by Tandem set the standard for commercial high-availability systems, combining redundant fail-fast hardware with fault-tolerant software. Tandem systems use two operating systems. The Guardian system <ref> [SiS92] </ref> runs a distributed system internal to a non-shared-memory multiprocessor. The separate kernels use a heartbeat protocol and a distributed consensus mechanism to agree on which processors are alive.
Reference: [SPY+93] <author> S. Saxena, J.K. Peacock, F. Yang, V. Verma, and M. Krishnan. </author> <title> Pitfalls in multithreading SVR4 STREAMS and other weightless processes. </title> <booktitle> Winter 1993 USENIX Conference, </booktitle> <pages> pp. </pages> <address> 8596 (San Diego, CA, </address> <month> January 2529, </month> <year> 1993). </year> <institution> Berkley, CA: USENIX Association, </institution> <year> 1993. </year>
Reference-contexts: Mechanisms used to achieve efficient performance from UNIX SVR4 on small-scale parallel systems are described in <ref> [SPY+93, Pea92, CBB+91, CHS91] </ref>. Mechanisms in the Sun Microsystems versions of UNIX SVR4 are described in [EKB+92]. Work on the Silicon Graphics version is described in [BaB95]. These efforts are all focused on achieving sufficient parallelization to avoid synchronization bottlenecks on four- or eight-processor systems.
Reference: [SuC91] <author> M. Sullivan and R. Chillarege. </author> <title> Software defects and their impact on system availabilitya study of field failures in operating systems. </title> <booktitle> Twenty-First International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. </pages> <address> 29 (Montreal, Quebec, Canada, June 25 27, 1991). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: (2) the recovery routines in the operating system successfully handle errors Related work 123 about half the time, with the exception of timing and I/O errors that are rarely recoverable; and (3) the rate of software errors is closely linked to the intensity of the interactive workload of the system. <ref> [SuC91] </ref> is a more recent study on IBM mainframes that uses error logs to study the probability of wild writes.
Reference: [TaI92a] <author> D. Tang and R.K. Iyer. </author> <title> Analysis of the VAX/VMS error logs in multicomputer environmentsa case study of software dependability. </title> <booktitle> Third International Symposium on Software Reliability Engineering, </booktitle> <pages> pp. </pages> <address> 216226 (Research Triangle Park, NC, October 710, 1992). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: Unfortunately, no information on the failure rate of multiprocessors running current SMP operating systems is available. However, several published studies that combine uniprocessors and multiprocessors give hints about the magnitude of the rate. 2.1.1 System failures in practice <ref> [TaI92a] </ref> uses automatically-maintained error logs to analyze failures in VAXcluster systems running VMS. Over 30 machine-years of data shows an overall mean time between failures (MTBF) of 20 machine days. This rate is high due to the immaturity of the operating system at the start of the data collection period.
Reference: [TaI92b] <author> D. Tang and R.K. Iyer. </author> <title> Analysis and modeling of correlated failures in multicomputer systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 41, no. 5, </volume> <pages> pp. 567577, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Reliability prediction: Many of the papers discussed above, especially those written or advised by R. K. Iyer, provide mathematical models that assist in predicting reliability from failure data. Notable examples include <ref> [LeI92, LeI93, TaI92b, TaI93] </ref>. [ChC96] considers how to design experiments so that reliability prediction based on fault injection studies will be statistically valid, while [EIP+91] uses data from system test of a UNIX-based AT&T network management system to analyze the validity of various mathematical models.
Reference: [TaI93] <author> D. Tang and R.K. Iyer. </author> <title> Dependability measurement and modeling of a multicomputer system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 42, no. 1, </volume> <pages> pp. 6275, </pages> <month> January </month> <year> 1993. </year> <note> References 137 </note>
Reference-contexts: Reliability prediction: Many of the papers discussed above, especially those written or advised by R. K. Iyer, provide mathematical models that assist in predicting reliability from failure data. Notable examples include <ref> [LeI92, LeI93, TaI92b, TaI93] </ref>. [ChC96] considers how to design experiments so that reliability prediction based on fault injection studies will be statistically valid, while [EIP+91] uses data from system test of a UNIX-based AT&T network management system to analyze the validity of various mathematical models.
Reference: [TBG+] <author> D. Teodosiu, J. Baxter, K. Govil, J. Chapin, M. Rosenblum, and M. Horowitz. </author> <title> Hardware fault containment in scalable shared-memory multiprocessors. </title> <publisher> In press. </publisher>
Reference-contexts: Network recovery is complicated enough that FLASH implements it with code that runs on the main compute processor, in the same way as memory system recovery. More information: See <ref> [TBG+] </ref> for more details on memory system and network recovery and [Gal96] for normal operation of the network and data link levels. Hive prototype 39 Chapter 4 Hive prototype The architecture described in the previous chapter is a system design.
Reference: [TeT87] <author> A. Tevanian. </author> <title> Architecture-Independent Virtual Memory Management for Parallel and Distributed Environments: The Mach Approach. </title> <type> Doctoral dissertation, technical report CMU-CS-88-106, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <year> 1987. </year>
Reference-contexts: The anonymous memory manager is responsible for determining which page should be used to satisfy a page fault to a swap region. In IRIX, the anonymous manager tracks swap pages using a copy-on-write tree, similar to the MACH approach <ref> [TeT87] </ref>. An anonymous page is allocated when a process writes to a page of its address space that is shared copy-on-write with its parent. The new page is recorded at the current leaf of the copy-on-write tree (Figure 7.4a).
Reference: [TGH92] <author> J. Torrellas, A. Gupta, and J. Hennessy. </author> <title> Characterizing the caching and synchronization performance of a multiprocessor operating system. </title> <booktitle> Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. </pages> <address> 162174 (Boston, </address> <month> October 1215, </month> <year> 1992). </year> <journal> Available as SIGPLAN Notices, </journal> <volume> vol. 27, no. 9, </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: A familiar example of this architectural characteristic is the synchronization behavior of SMP kernels. For example, in IRIX 4 running on four processors the global run queue lock is the only one with substantial contention <ref> [TGH92] </ref>. The developers made significant algorithmic changes to Table 2.1. Data cache hotspots in IRIX over 22.8 seconds of execution on DASH.
Reference: [TIY+95] <author> A. Thakur, R.K. Iyer, L. Young, and I. Lee. </author> <title> Analysis of failures in the Tandem NonStop-UX Operating System. </title> <booktitle> Sixth International Symposium on Software Reliability Engineering, </booktitle> <pages> pp. </pages> <address> 4050 (Toulouse, France, October 2427, 1995). Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: The separate kernels use a heartbeat protocol and a distributed consensus mechanism to agree on which processors are alive. Reliability with respect to software faults is provided by a high number of assertions that shut down a kernel when it detects any internal inconsistency. The Tandem Nonstop-UX kernel <ref> [Jew91, TIY+95] </ref> runs as a uniprocessor UNIX kernel on top of triply-redundant hardware. The redundant hardware masks most hardware failures, while the operating system detects software errors using assertions and recovers using forward recovery routines. <p> Tandem operating systems have also been studied extensively because of their intended use in environments with high availability requirements. [LeI92] and [LeI93] examine the Guardian90 operating system, while <ref> [TIY+95] </ref> examines the Nonstop-UX operating system. [LeI93] and [TIY+95] are particularly interesting because they provide detailed information on the types and locations of programming mistakes in the operating system code. [LeI93] notes that timing errors are the most significant single cause of system failures in the field, while missing operations (pointer <p> Tandem operating systems have also been studied extensively because of their intended use in environments with high availability requirements. [LeI92] and [LeI93] examine the Guardian90 operating system, while <ref> [TIY+95] </ref> examines the Nonstop-UX operating system. [LeI93] and [TIY+95] are particularly interesting because they provide detailed information on the types and locations of programming mistakes in the operating system code. [LeI93] notes that timing errors are the most significant single cause of system failures in the field, while missing operations (pointer or variable initialization, data update or message send) <p> the types and locations of programming mistakes in the operating system code. [LeI93] notes that timing errors are the most significant single cause of system failures in the field, while missing operations (pointer or variable initialization, data update or message send) are the most common type of low-level programming mistake. <ref> [TIY+95] </ref> observes that pointer manipulation errors and missing checks for illegal data values are the most common low-level mistakes in Nonstop-UX. The most relevant field failure data for Hives error model is provided by [ChC96].
Reference: [UKG+94] <author> R.C. Unrau, O. Krieger, B. Gamsa, and M. Stumm. </author> <title> Experiences with locking in a NUMA multiprocessor operating system kernel. </title> <booktitle> First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pp. </pages> <address> 139152 (Monterey, CA, </address> <month> November 1417, </month> <year> 1994). </year> <institution> Berkeley, CA: USENIX Association, </institution> <year> 1994. </year>
Reference-contexts: First, Hurricane and Tornado are built from the ground up as multicellular kernels rather than being modified from an existing SMP kernel, so their implementations are both more modular and more exible. For example, Hurricane includes a single system-wide locking protocol that makes the interaction between kernels more regular <ref> [UKG+94] </ref>. Second, since reliability is not an issue, the boundaries Related work 119 between kernels are much more exible. For example, different resources such as memory and processors can be managed at different cluster sizes simultaneously.
Reference: [UKG+95] <author> R.C. Unrau, O. Krieger, B. Gamsa, and M. Stumm. </author> <title> Hierarchical clustering: A structure for scalable multiprocessor operating system design. </title> <journal> Journal of Supercomputing, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 105134, </pages> <year> 1995. </year>
Reference-contexts: Their initial work on the Hurricane operating system <ref> [UKG+95, Kri95] </ref> running on the Hector multiprocessor [VSL+91] has been followed by current work on the Tornado operating system [PGK+95] running on the NUMAchine multiprocessor [VBS+95]. In [UKG+95], the Toronto researchers identify several previous proposals for improving scalability through approaches similar to multicellular structuring: [AhG91, CGB91, FeR90, ZhB91]. <p> Their initial work on the Hurricane operating system [UKG+95, Kri95] running on the Hector multiprocessor [VSL+91] has been followed by current work on the Tornado operating system [PGK+95] running on the NUMAchine multiprocessor [VBS+95]. In <ref> [UKG+95] </ref>, the Toronto researchers identify several previous proposals for improving scalability through approaches similar to multicellular structuring: [AhG91, CGB91, FeR90, ZhB91]. Hurricane was the first complete operating system implementation based on these ideas, but neither it nor the previous proposals investigate the reliability benefits of the architecture as Hive does.
Reference: [VBS+95] <author> Z. Vranesic, S. Brown, M. Stumm, S. Caranci, A. Grbic, R. Grindley, M. Gusat, O. Krieger, G. Lemieux, K. Loveless, N. Manjikian, Z. Zilic, T. Abdelrahman, B. Gamsa, P. Pereira, K. Sevcik, A. Elkateeb, and S. Srbljic. </author> <title> The NUMAchine multiprocessor. </title> <type> Unpublished white paper, </type> <institution> Department of Electrical and Computer Engineering, University of Toronto, </institution> <month> June 28, </month> <year> 1995. </year>
Reference-contexts: Their initial work on the Hurricane operating system [UKG+95, Kri95] running on the Hector multiprocessor [VSL+91] has been followed by current work on the Tornado operating system [PGK+95] running on the NUMAchine multiprocessor <ref> [VBS+95] </ref>. In [UKG+95], the Toronto researchers identify several previous proposals for improving scalability through approaches similar to multicellular structuring: [AhG91, CGB91, FeR90, ZhB91].
Reference: [VDG+96] <author> B. Verghese, S. Devine, A. Gupta, and M. Rosenblum. </author> <title> Operating system support for improving data locality on CC-NUMA compute servers. </title> <booktitle> Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. </pages> <address> 279289 (Cambridge, MA, </address> <month> October 15, </month> <year> 1996). </year> <title> Available as Operating Systems Review, </title> <journal> vol. </journal> <volume> 30, no. 5, </volume> <month> October </month> <year> 1996. </year> <month> 138 </month>
Reference-contexts: Common approaches include checkpointing [LiS92, LNP94, PBK+95] and replication of processes on top of microkernels [LiR93, ACC+93]. Scalability: Few researchers have investigated the techniques required to support general-purpose workloads on large-scale shared-memory multiprocessors, because these machines have only recently become available for general-purpose use. [CDV+94] and <ref> [VDG+96] </ref> investigate algorithms for page replication and migration and how these must work together with processor scheduling to reduce memory system costs. [CHR+95] characterizes the performance bottlenecks of an SMP kernel running on a CC-NUMA system. [SDH+96] describes a new file system created by Silicon Graphics to support general-purpose workloads that
Reference: [vEB+95] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> a user-level network interface for parallel and distributed computing. </title> <booktitle> Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. </pages> <address> 4053 (Copper Mountain Resort, CO, </address> <month> December 36, </month> <year> 1995). </year> <title> Available as Operating Systems Review, </title> <journal> vol. </journal> <volume> 29, no. 5, </volume> <month> December </month> <year> 1995. </year>
Reference-contexts: The Hive RPC subsystem and the SIPS mechanism closely resemble other low-latency message delivery systems such as active messages [vEC+92], U-Net <ref> [vEB+95] </ref>, and FLIPC [BSS+96]. The key difference from these previous systems is that SIPS is implemented directly by the memory controller and thus can take advantage of optimizations unavailable to the other systems.
Reference: [vEC+92] <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> Nineteenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. </pages> <address> 256266 (Gold Coast, Queensland, Australia, </address> <month> May </month> <year> 1921, 1992). </year> <title> Available as Computer Architecture News, </title> <journal> vol. </journal> <volume> 20, no. 2, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: The Hive RPC subsystem and the SIPS mechanism closely resemble other low-latency message delivery systems such as active messages <ref> [vEC+92] </ref>, U-Net [vEB+95], and FLIPC [BSS+96]. The key difference from these previous systems is that SIPS is implemented directly by the memory controller and thus can take advantage of optimizations unavailable to the other systems.
Reference: [VeI84] <author> P. Velardi and R.K. Iyer. </author> <title> A study of software failures and recovery in the MVS operating system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-33, no. 7, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: Systematic efforts to understand the types of errors that affect operating systems started with studies of IBM mainframes, since these were the dominant computing platform for commercial use. An ongoing study of systems at Stanford produced [Bea79] <ref> [VeI84, MoA87] </ref>.
Reference: [VSL+91] <author> Z.G. Vranesic, M. Stumm, D.M. Lewis, and R. White. Hector: </author> <title> a hierarchically structured shared-memory multiprocessor. </title> <journal> Computer, </journal> <volume> vol. 24, no. 1, </volume> <pages> pp. 7279, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Their initial work on the Hurricane operating system [UKG+95, Kri95] running on the Hector multiprocessor <ref> [VSL+91] </ref> has been followed by current work on the Tornado operating system [PGK+95] running on the NUMAchine multiprocessor [VBS+95]. In [UKG+95], the Toronto researchers identify several previous proposals for improving scalability through approaches similar to multicellular structuring: [AhG91, CGB91, FeR90, ZhB91].
Reference: [WaP89] <author> B. Walker and J. Popek. </author> <title> Distributed UNIX transparency: goals, benefits, and the TCF example. </title> <booktitle> Winter 1989 Uniforum conference (January 1989). </booktitle> <address> Santa Clara, CA: Uniforum, </address> <year> 1989. </year>
Reference-contexts: Single-system image: Locus [PoW85] pioneered single-system image support for distributed systems. In addition to a shared filesystem and distributed process management, it demonstrates techniques for coping with network partitions. The Locus work was initially commercialized in AIX/TCF from IBM <ref> [WaP89] </ref> and later as a portable SSI layer that is one component of OSF/1 AD TNC [ZRB+93]. The later versions include process migration and distributed shared memory. Sprite [OCD+88] implements process migration and a high-performance SSI distributed file system.
Reference: [WiR96] <author> E. Witchel and M. Rosenblum. Embra: </author> <title> fast and exible machine simulation. </title> <booktitle> 1996 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. </pages> <address> 6879 (Philadelphia, PA, </address> <month> May 2326, </month> <year> 1996). </year> <title> Available as Performance Evaluation Review, </title> <journal> vol. </journal> <volume> 24, no. 1, </volume> <month> May </month> <year> 1996. </year>
Reference-contexts: Hive is implemented as an extensive modification of the IRIX 5.2 code base. Overview 9 1.9 Experimental evaluation Because the FLASH machine is not yet operational, the experiments reported in this dissertation use the SimOS machine simulator <ref> [RHW+95, WiR96, RBD+] </ref>. SimOS offers a choice of processor and memory system models that trade off between the speed and accuracy of simulation. <p> The first section of the chapter describes SimOS. The second gives information about the performance experiments. The third describes the reliability experiments. The final section analyzes how using simulation affects the experimental results. 5.1 The SimOS simulation environment SimOS <ref> [RHW+95, WiR96, RBD+] </ref> is a machine simulation environment that simulates the hardware of uniprocessor and multiprocessor computer systems in enough detail to boot, run, and study a commercial operating system. 1 Specifically, SimOS provides simulators of processors, caches, memory systems, and a number of different I/O devices including SCSI disks, ethernet
Reference: [WLA+93] <author> R. Wahbe, S. Lucco, T.E. Anderson, and S.L. Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. </pages> <address> 203216 (Ashville, NC, </address> <month> December 58, </month> <year> 1993). </year> <title> Available as Operating Systems Review, </title> <journal> vol. </journal> <volume> 27, no. 5, </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: More fundamentally, this research demonstrates that the traditional assumptions about the inherent unreliability of shared memory systems are incorrect. Hive draws a fault containment boundary inside the shared memory boundary and gains reliability without sacrificing resource sharing. In this regard, Hive is part of a widespread research effort <ref> [CNC+96, Gil96, WLA+93] </ref> to reevaluate the fundamental ways in which shared memory can be used. References 127
Reference: [WLH81] <author> W. Wulf, R. Levin, and S. Harbison. HYDRA/C.mmp, </author> <title> an experimental computer system. </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: An orthogonal approach has been to partition the system into a microkernel and a set of services running in independent address spaces, making the system easier to test and debug. Examples of this approach include Hydra <ref> [WLH81] </ref>, Mach [RJO+89], and Chorus [RAA+88]. Finally, automated testing [SaH94] can increase the fraction of faults that are found and fixed during operating system development. Another area of work has been techniques for avoiding the reboot that is required to recover from software errors in standard SMP kernels. <p> C.mmp scaled to 16 processors, which was large from the perspective of reliability issues given the low-integration technology from which it was constructed. The developers found it necessary to add novel reliability-oriented features to the Hydra operating system to achieve adequate MTBF <ref> [WLH81] </ref>. Hydra includes mechanisms to tolerate both hardware and software faults. At the hardware level, it has a watchdog mechanism to detect fail-stop processor faults and a resource exploration phase at reboot time that can avoid bad memory pages.
Reference: [WOT+95] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: characterization and methodological considerations. </title> <booktitle> Twenty-Second Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. </pages> <address> 2436 (Santa Margherita Ligure, Italy, June 2224, 1995). New York: </address> <publisher> ACM, </publisher> <year> 1995. </year> <note> References 139 </note>
Reference-contexts: Performance experiments: The performance experiments use microbenchmarks and three high-level workloads: pmake, raytrace, and ocean. Pmake is a parallel make, which stresses the system in ways characteristic of general-purpose use. Raytrace and ocean are parallel scientific applications from the Splash-2 benchmark suite <ref> [WOT+95] </ref>. The use of simulation limits the performance experiments to small system sizes. The largest system studied has only eight processors and 256 megabytes of memory. <p> This avoids hiding operating system performance differences in the idle time that occurs waiting for the last compilation to complete. The other two workloads, ocean and raytrace, are parallel scientific applications from the Splash-2 benchmark <ref> [WOT+95] </ref>. These applications stress Hives resource sharing mechanisms. Ocean uses a 258 by 258 grid and a 1400 second interval. Raytrace uses the teapot data set and four antialias rays per pixel.
Reference: [ZhB91] <author> S. Zhou and T. Brecht. </author> <title> Processor pool-based scheduling for large-scale NUMA multiprocessors. </title> <booktitle> 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. </pages> <address> 133142 (San Diego, CA, </address> <month> May 2124, </month> <year> 1991). </year> <title> Available as Performance Evaluation Review, </title> <journal> vol. </journal> <volume> 19, no. 1, </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: In [UKG+95], the Toronto researchers identify several previous proposals for improving scalability through approaches similar to multicellular structuring: <ref> [AhG91, CGB91, FeR90, ZhB91] </ref>. Hurricane was the first complete operating system implementation based on these ideas, but neither it nor the previous proposals investigate the reliability benefits of the architecture as Hive does. Many of the mechanisms in Hive parallel similar mechanisms in Hurricane and Tornado.

References-found: 108

