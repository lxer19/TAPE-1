URL: http://www.cs.ucdavis.edu/~pandey/qos.paper1.ps
Refering-URL: http://www.cs.ucdavis.edu/~pandey/qos.html
Root-URL: http://www.cs.ucdavis.edu
Note: ACM COPYRIGHT NOTICE  
Abstract: Copyright 199x by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> NCSA Web Server Source. </institution> <note> ftp://ftp.ncsa.uiuc.edu/ Web/httpd/Unix/ ncsa httpd/httpd 1.5.2a/httpd 1.5.2a-export source.tar.Z. </note>
Reference-contexts: The QoS Web Server is implemented in terms of a set of components: a WWW server, a communications server and a centralized quality of service daemon (qosd). The WWW server is a modified version of the stand alone NCSA httpd WWW server <ref> [1] </ref>. It is used to handle individual HTTP requests. The modification in the NCSA server involves adding a check to ensure that a request is served only if the quality of service constraints are not violated.
Reference: [2] <institution> Hypertext Transfer Protocol (HTTP): A protocol for networked information. </institution> <note> http://www.w3.org/hypertext/WWW/Protocols, 1995. </note>
Reference-contexts: 1 Introduction With the advent of the WWW [13], there has been a fundamental shift in the way information is exchanged among systems connected to the Internet. Three elements [26] of the WWW make this possible: a uniform naming mechanism (URL) for identifying resources, a protocol (HTTP) <ref> [2] </ref> for transferring information, and the client-server based architecture [17]. A client such as a browser uses the URL of a resource to locate an HTTP server that provides the resource. It then requests for services associated with the resource.
Reference: [3] <author> Almeida, J., Dabu, M., Manikutty, A., and Cao, P. </author> <title> Providing Differentiated Levels of Service in Web Content Hosting. </title> <type> Tech. rep., </type> <institution> University of Wisconsin-Madison, </institution> <year> 1998. </year>
Reference-contexts: work, on the other hand, addresses additional issues in the design of HTTP servers: * Should the server accept a request? * If so, how much resources should be allocated to the request? There has been some work that looks at the notion of quality of service for HTTP servers. <ref> [3] </ref> proposes a notion of quality of service by associating priorities with requests from different sites. The HTTP server schedules requests according to priorities, thereby ensuring that preferred sites (with higher priority) are allocated resources before other sites. Our work differs in many ways: first, the focus in [3] is on <p> HTTP servers. <ref> [3] </ref> proposes a notion of quality of service by associating priorities with requests from different sites. The HTTP server schedules requests according to priorities, thereby ensuring that preferred sites (with higher priority) are allocated resources before other sites. Our work differs in many ways: first, the focus in [3] is on proposing techniques for structuring single host HTTP servers in order to improve the response times of high priority requests. Our work primarily involves distributed HTTP servers.
Reference: [4] <author> Andersen, D., Yang, T., Egecioglu, O., Ibarra, O., and Smith, T. SWEB: </author> <title> Towards a Scalable World Wide Web Server on Multicomputers. </title> <booktitle> In Proceedings of 10 the Tenth IEEE International Symposium on Parallel Processing (1996), IEEE Computer Society, </booktitle> <pages> pp. 139-148. </pages>
Reference-contexts: Much of the work has focussed on addressing the performance limiting behaviors [22] of HTTP servers. The research has, thus, focussed on developing techniques (such as information caching [7, 20, 9, 23] and distribution, partitioning [16] of server load across clients and servers, and parallelization <ref> [15, 4, 14, 18] </ref> of HTTP servers over SMPs and workstations) for eliminating performance bottlenecks arising due to the lack of sufficient CPU, disk, and network To appear in the Proceedings of the Seventeenth Annual SIGACT-SIGOPS Symposium on Principles of Distributed Computing, June 1998, Puerto Vallarta, Mexico. bandwidths as well as <p> Much of the HTTP server work has focussed on developing variations of HTTP server architectures that reduce the CPU, network, and disk bottleneck. We will focus only on the distributed HTTP server work <ref> [15, 14, 4] </ref> because of the similarity in the issues addressed by these approaches and our approach. The focus in the distributed server research has been on using the resources of distributed hosts to increase the throughput of HTTP servers.
Reference: [5] <author> Banatre, M., Issamy, V., Leleu, F., and Char-piot, B. </author> <title> Providing Quality of Service over the Web: A Newspaper-based Approach. </title> <booktitle> In Proceedings of the Sixth International World Wide Web Conference (1997). </booktitle>
Reference-contexts: Our work primarily involves distributed HTTP servers. Second, our notion of quality of service is more general in that we not only allow a site to specify priorities but also allow it to specify resource usage constraints on a group of requests. In <ref> [5] </ref>, a notion of quality of service is proposed with respect to the content.
Reference: [6] <author> Banga, G., and Druschel, P. </author> <title> Measuring the Capacity of a Web Server. </title> <booktitle> In Proceedings of the USENIX Symposium on Internet Technologies and Systems,Monterey, </booktitle> <address> California, USA (December 1997). </address>
Reference-contexts: An HTTP server, therefore, can be thought of as a runtime system that manages executions of various method invocations. Traditional HTTP servers do not distinguish among different method invocations. Each method invocation is serviced in the order it is received (unless it is dropped due to resource contentions <ref> [6] </ref>). The QoS model here allows one to specify priority relationships among method invocations. Further, a site may specify a set of resource usage constraints for controlling the amount of server resources allocated to requests.
Reference: [7] <author> Bestavros, A. </author> <title> WWW Traffic Reduction and Load Balancing Through Server-Based Caching. </title> <booktitle> IEEE Con-currency (1997), </booktitle> <pages> 56-67. </pages>
Reference-contexts: The architecture of HTTP servers has been studied in great detail and different variations of HTTP servers have been proposed. Much of the work has focussed on addressing the performance limiting behaviors [22] of HTTP servers. The research has, thus, focussed on developing techniques (such as information caching <ref> [7, 20, 9, 23] </ref> and distribution, partitioning [16] of server load across clients and servers, and parallelization [15, 4, 14, 18] of HTTP servers over SMPs and workstations) for eliminating performance bottlenecks arising due to the lack of sufficient CPU, disk, and network To appear in the Proceedings of the Seventeenth
Reference: [8] <author> Brisco, T. </author> <title> DNS Support for Load Balancing. Network Working Group, </title> <note> RFC 1794 http://andrew2.andrew.cmu.edu/rc/rfc1794.html. </note>
Reference-contexts: We assume that a client can send a request to any of the HTTP servers directly by using one of the routing mechanisms (such as the Domain Name Server's redirection <ref> [8] </ref>, ONE-IP mechanism [10] and router-based redirection [11]). ... ... ... s 2 s 4 QoS Web Server Messages Q Q Q The primary goal of a QoS Web Server is to serve a file request only if servicing the requests does not violate the quality of service constraint that a
Reference: [9] <author> Caughey, S., Ingham, D. B., and Little, M. C. </author> <title> Flexible Open Caching for the Web. </title> <booktitle> In Proceedings of the Sixth International World Wide Web Conference (1997). </booktitle>
Reference-contexts: The architecture of HTTP servers has been studied in great detail and different variations of HTTP servers have been proposed. Much of the work has focussed on addressing the performance limiting behaviors [22] of HTTP servers. The research has, thus, focussed on developing techniques (such as information caching <ref> [7, 20, 9, 23] </ref> and distribution, partitioning [16] of server load across clients and servers, and parallelization [15, 4, 14, 18] of HTTP servers over SMPs and workstations) for eliminating performance bottlenecks arising due to the lack of sufficient CPU, disk, and network To appear in the Proceedings of the Seventeenth
Reference: [10] <author> Damani, O. P., Chung, P. E., Huang, Y., Kintala, C., and Wang, Y. ONE-IP: </author> <title> Techniques for Hosting a Service on a Cluster of Machines. </title> <booktitle> In Proceedings of the Sixth International World Wide Web Conference (1997). </booktitle>
Reference-contexts: We assume that a client can send a request to any of the HTTP servers directly by using one of the routing mechanisms (such as the Domain Name Server's redirection [8], ONE-IP mechanism <ref> [10] </ref> and router-based redirection [11]). ... ... ... s 2 s 4 QoS Web Server Messages Q Q Q The primary goal of a QoS Web Server is to serve a file request only if servicing the requests does not violate the quality of service constraint that a site imposes.
Reference: [11] <author> Dias, D., Kish, W., Mukherjee, R., and Tewari, R. </author> <title> A Scalable and Highly Available Server. </title> <booktitle> In COM-PCON (1996), </booktitle> <pages> pp. 85-92. </pages>
Reference-contexts: We assume that a client can send a request to any of the HTTP servers directly by using one of the routing mechanisms (such as the Domain Name Server's redirection [8], ONE-IP mechanism [10] and router-based redirection <ref> [11] </ref>). ... ... ... s 2 s 4 QoS Web Server Messages Q Q Q The primary goal of a QoS Web Server is to serve a file request only if servicing the requests does not violate the quality of service constraint that a site imposes.
Reference: [12] <author> Eriksson, P. </author> <title> The PHTTPD World Wide Web Server. </title> <note> http://www.signum.se/phttpd. [13] et al., </note> <author> T. B.-L. </author> <title> The World-Wide Web. </title> <type> CACM 37, </type> <month> 8 (August </month> <year> 1994), </year> <pages> 76-82. </pages>
Reference-contexts: These workstations are connected on a local area network. For the purpose of comparing results, we created a benchmark program based on ptester, a HTTP retrieval benchmark program included in the phttpd package <ref> [12] </ref>. The benchmark program takes as input a trace of requests and times, and uses the trace to send requests to the QoS Web Server. We generate traces that reflect specific or random mixes of various requests for different pages.
Reference: [14] <author> Garland, M., Grassia, S., Monroe, R., and Puri, S. </author> <title> Implementing Distributed Server Groups for the World Wide Web. </title> <type> Tech. Rep. </type> <institution> CMU-CS-95-114, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: Much of the work has focussed on addressing the performance limiting behaviors [22] of HTTP servers. The research has, thus, focussed on developing techniques (such as information caching [7, 20, 9, 23] and distribution, partitioning [16] of server load across clients and servers, and parallelization <ref> [15, 4, 14, 18] </ref> of HTTP servers over SMPs and workstations) for eliminating performance bottlenecks arising due to the lack of sufficient CPU, disk, and network To appear in the Proceedings of the Seventeenth Annual SIGACT-SIGOPS Symposium on Principles of Distributed Computing, June 1998, Puerto Vallarta, Mexico. bandwidths as well as <p> Much of the HTTP server work has focussed on developing variations of HTTP server architectures that reduce the CPU, network, and disk bottleneck. We will focus only on the distributed HTTP server work <ref> [15, 14, 4] </ref> because of the similarity in the issues addressed by these approaches and our approach. The focus in the distributed server research has been on using the resources of distributed hosts to increase the throughput of HTTP servers.
Reference: [15] <author> Katz, E., Butler, M., and McGrath, R. </author> <title> A Scalable HTTP Server: The NCSA Prototype. </title> <booktitle> In Proceedings of the First International Conference on the World-Wide Web (May 1994). </booktitle>
Reference-contexts: Much of the work has focussed on addressing the performance limiting behaviors [22] of HTTP servers. The research has, thus, focussed on developing techniques (such as information caching [7, 20, 9, 23] and distribution, partitioning [16] of server load across clients and servers, and parallelization <ref> [15, 4, 14, 18] </ref> of HTTP servers over SMPs and workstations) for eliminating performance bottlenecks arising due to the lack of sufficient CPU, disk, and network To appear in the Proceedings of the Seventeenth Annual SIGACT-SIGOPS Symposium on Principles of Distributed Computing, June 1998, Puerto Vallarta, Mexico. bandwidths as well as <p> Much of the HTTP server work has focussed on developing variations of HTTP server architectures that reduce the CPU, network, and disk bottleneck. We will focus only on the distributed HTTP server work <ref> [15, 14, 4] </ref> because of the similarity in the issues addressed by these approaches and our approach. The focus in the distributed server research has been on using the resources of distributed hosts to increase the throughput of HTTP servers.
Reference: [16] <author> Kong, K., and Ghosal, D. Pseudo-serving: </author> <title> A User-Responsible Paradigm for Internet Access. </title> <booktitle> In Proceedings of the Sixth International World Wide Web Conference (1997). </booktitle>
Reference-contexts: Much of the work has focussed on addressing the performance limiting behaviors [22] of HTTP servers. The research has, thus, focussed on developing techniques (such as information caching [7, 20, 9, 23] and distribution, partitioning <ref> [16] </ref> of server load across clients and servers, and parallelization [15, 4, 14, 18] of HTTP servers over SMPs and workstations) for eliminating performance bottlenecks arising due to the lack of sufficient CPU, disk, and network To appear in the Proceedings of the Seventeenth Annual SIGACT-SIGOPS Symposium on Principles of Distributed
Reference: [17] <author> Kwan, T. T., McGrath, R. E., and Reed, D. A. </author> <title> NCSA's World Wide Web Server: Design and Performance. </title> <booktitle> IEEE Computer (November 1995), </booktitle> <pages> 68-74. </pages>
Reference-contexts: Three elements [26] of the WWW make this possible: a uniform naming mechanism (URL) for identifying resources, a protocol (HTTP) [2] for transferring information, and the client-server based architecture <ref> [17] </ref>. A client such as a browser uses the URL of a resource to locate an HTTP server that provides the resource. It then requests for services associated with the resource.
Reference: [18] <author> Liu, Y., Dantzig, P., Wu, C. E., Challenger, J., and Ni, L. M. </author> <title> A Distributed Web Server and Its Performance Analysis on Multiple Platforms. </title> <booktitle> In Proceedings of the The Sixteenth International Conference on Distributed Computing Systems (1996), </booktitle> <pages> pp. 665-672. </pages>
Reference-contexts: Much of the work has focussed on addressing the performance limiting behaviors [22] of HTTP servers. The research has, thus, focussed on developing techniques (such as information caching [7, 20, 9, 23] and distribution, partitioning [16] of server load across clients and servers, and parallelization <ref> [15, 4, 14, 18] </ref> of HTTP servers over SMPs and workstations) for eliminating performance bottlenecks arising due to the lack of sufficient CPU, disk, and network To appear in the Proceedings of the Seventeenth Annual SIGACT-SIGOPS Symposium on Principles of Distributed Computing, June 1998, Puerto Vallarta, Mexico. bandwidths as well as
Reference: [19] <author> Melamed, A. S. </author> <title> Performance Analysis of Unix-based Network File Systems. </title> <booktitle> IEEE Micro (February 1987), </booktitle> <pages> 25-38. </pages>
Reference-contexts: Note that a machine's ability to serve a specific bandwidth depends on a number of factors: CPU speed, local CPU load factor, file server's capacity, file server's load factor, and local area network characteristics. In <ref> [19] </ref>, an analytical model is created for evaluating the cost associated with accessing remote files, whereas in [25], the experimental technique used in the NFS benchmark (LAD-DIS) for evaluating the performance behavior of NSF servers is described.
Reference: [20] <author> Nabeshima, M. </author> <title> The Japan Cache Project: An Experiment on Domain Cache. </title> <booktitle> In Proceedings of the Sixth International World Wide Web Conference (1997). </booktitle>
Reference-contexts: The architecture of HTTP servers has been studied in great detail and different variations of HTTP servers have been proposed. Much of the work has focussed on addressing the performance limiting behaviors [22] of HTTP servers. The research has, thus, focussed on developing techniques (such as information caching <ref> [7, 20, 9, 23] </ref> and distribution, partitioning [16] of server load across clients and servers, and parallelization [15, 4, 14, 18] of HTTP servers over SMPs and workstations) for eliminating performance bottlenecks arising due to the lack of sufficient CPU, disk, and network To appear in the Proceedings of the Seventeenth
Reference: [21] <author> Nahrestedt, K., and Smith, J. M. </author> <title> The QoS Broker. </title> <booktitle> IEEE MultiMedia Magazine (Spring 1995), </booktitle> <pages> 53-67. </pages>
Reference-contexts: In [5], a notion of quality of service is proposed with respect to the content. However, there is not support for any notion of quality of service with respect to resource usage, throughput or response time. 5.2 Quality of service in Distributed Systems The notion of quality of service <ref> [21] </ref> has been studied in great detail within the context of networking [21] and multimedia [24]. <p> However, there is not support for any notion of quality of service with respect to resource usage, throughput or response time. 5.2 Quality of service in Distributed Systems The notion of quality of service <ref> [21] </ref> has been studied in great detail within the context of networking [21] and multimedia [24].
Reference: [22] <author> Prefect, F., Doan, L., Gold, S., Wicki, T., and Wilcke, W. </author> <title> Performance Limiting Factors in HTTP (Web) Server Operations. </title> <booktitle> In COMPCON (1996), IEEE, </booktitle> <pages> pp. 267-272. </pages>
Reference-contexts: The architecture of HTTP servers has been studied in great detail and different variations of HTTP servers have been proposed. Much of the work has focussed on addressing the performance limiting behaviors <ref> [22] </ref> of HTTP servers.
Reference: [23] <author> Scheuermann, P., Shim, J., and Vingralek, R. </author> <title> A Case for Delay-Conscious Caching of Web Documents. </title> <booktitle> In Proceedings of the Sixth International World Wide Web Conference (1997). </booktitle>
Reference-contexts: The architecture of HTTP servers has been studied in great detail and different variations of HTTP servers have been proposed. Much of the work has focussed on addressing the performance limiting behaviors [22] of HTTP servers. The research has, thus, focussed on developing techniques (such as information caching <ref> [7, 20, 9, 23] </ref> and distribution, partitioning [16] of server load across clients and servers, and parallelization [15, 4, 14, 18] of HTTP servers over SMPs and workstations) for eliminating performance bottlenecks arising due to the lack of sufficient CPU, disk, and network To appear in the Proceedings of the Seventeenth
Reference: [24] <author> Vogel, A., Kerherv e, B., von Bochmann, G., and Gecsei, J. </author> <title> Distributed Multimedia and QoS: A Survey. </title> <booktitle> IEEE MultiMedia 2, 2 (1995), </booktitle> <pages> 10-19. </pages>
Reference-contexts: Section 5 contains a comparison of our work with related work. Finally, Section 6 summarizes the results and discusses future work. 2 A Quality of Service Model for HTTP Servers The notion of quality of service has been addressed in great detail in the network and multi-media community <ref> [24] </ref>. Within a client-server framework, we can think of quality of service as a quantification of level of services that a server can guarantee its clients. <p> However, there is not support for any notion of quality of service with respect to resource usage, throughput or response time. 5.2 Quality of service in Distributed Systems The notion of quality of service [21] has been studied in great detail within the context of networking [21] and multimedia <ref> [24] </ref>.
Reference: [25] <author> Wittle, M., and Keith, B. E. LADDIS: </author> <title> The Next Generation of NFS File Server Benchmarking. </title> <booktitle> In Proceedings of the USENIX Summer 1993 Technical Conference (June 1993), Usenix. </booktitle>
Reference-contexts: In [19], an analytical model is created for evaluating the cost associated with accessing remote files, whereas in <ref> [25] </ref>, the experimental technique used in the NFS benchmark (LAD-DIS) for evaluating the performance behavior of NSF servers is described. Both of these techniques can be extended to construct a resource model for the QoS Web Server. Currently, we are using a simple experimental technique for constructing the resource model.
Reference: [26] <author> Yeager, N. J., and McGrath, R. </author> <title> Web Server Technology: The Advanced Guide for World Wide Web Information. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction With the advent of the WWW [13], there has been a fundamental shift in the way information is exchanged among systems connected to the Internet. Three elements <ref> [26] </ref> of the WWW make this possible: a uniform naming mechanism (URL) for identifying resources, a protocol (HTTP) [2] for transferring information, and the client-server based architecture [17]. A client such as a browser uses the URL of a resource to locate an HTTP server that provides the resource.
Reference: [27] <author> Zinky, J. A., Bakken, D. E., and Schantz, R. D. </author> <title> Architectural support for Quality of Service for CORBA Objects. </title> <booktitle> Theory and Practice of Object Systems 3, 1 (1997), </booktitle> <pages> 1-20. 11 </pages>
Reference-contexts: In <ref> [27] </ref> mechanisms for specifying service guarantees with method invocations of CORBA objects is presented. Our work is similar to these works in that we also associate quality of service with resources in order to schedule resources.
References-found: 26

