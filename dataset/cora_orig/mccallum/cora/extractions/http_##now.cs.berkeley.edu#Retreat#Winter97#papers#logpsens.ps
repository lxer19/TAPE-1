URL: http://now.cs.berkeley.edu/Retreat/Winter97/papers/logpsens.ps
Refering-URL: http://now.cs.berkeley.edu/Retreat/Winter97/papers.html
Root-URL: 
Title: Effects of Communication Latency, Overhead, and Bandwidth in a Cluster Architecture  
Author: Richard P. Martin, Amin M. Vahdat, David E. Culler and Thomas E. Anderson 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Abstract: This work provides a systematic study of the impact of communication performance on parallel applications in a high performance cluster of workstations. We develop an experimental system in which the communication latency, overhead, and bandwidth can be independently varied to observe the effects on applications demonstrating a wide range of architectural requirements. Our results indicate that efforts to bring cluster communication performance in line with the performance of more tightly integrated parallel machines have resulted in significantly improved application performance. We show that applications demonstrate strong sensitivity to overhead and message bandwidth, slowing down by up to a factor of 30 on sixteen processors when overhead is increased by 100 s. Surprisingly, many of our benchmark applications are tolerant of increased latency and lower bulk message bandwidth. Finally, applications demonstrate a linear dependence to both overhead and gap, indicating that further improvements in communication performance will continue to improve application performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Binchini, David Chaiken, Kik Johnson, David Kranz, John Kubiatowicz, Ben Hong Lim, Ken Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedingsof the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller <ref> [31, 1, 19] </ref>, incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and
Reference: [2] <author> Albert Alexandrov, Mihai Ionescu, Klaus E. Schauser, and Chris Scheiman. LogGP: </author> <title> Incorporating Long Messages into the LogP model One step closer towards a realistic model for parallel computation. </title> <booktitle> In 7th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: This four-parameter characterization of communication performance is based on the LogGP model <ref> [14, 2] </ref>, the framework for our systematic investigation of the communication design space. By adjust Page 1 ing these parameters, we can observe changes in the exe-cution time of applications on a spectrum of systems ranging from the current high-performance cluster to conventional LAN-based clusters. <p> For bulk transfers, it is useful to extend the model with an additional gap parameter, G, which specifies the time-per-byte, or the reciprocal of the bulk transfer bandwidth <ref> [2] </ref>. In many machines, this is determined by the DMA rate to or from the network interface, rather than the network link bandwidth. The LogGP characteristics for our cluster is summarized in Table 1. <p> While the programming model does not provide automatic replication with cache coherence, a number of the applications perform their own caching, as described below. The language has been ported to many platforms <ref> [42, 33, 2, 41] </ref>. The sources for the applications, compiler, and communication layer were obtained from the publically available site 1 . 3.2 Technique The key experimental innovation is to modify the communication layer so that it can emulate a system with arbitrary overhead, gap, or latency. <p> On reception of a state description, a processor first checks if the state has been reached before. If the state is new, the processor adds it to the work queue to be validated against an assertion list. * Bulk Radix Sort: This version of the radix sort <ref> [2] </ref> was restructured to use bulk messages. After the the global histogram phase, all keys are sent to their destination processor in one bulk message.
Reference: [3] <author> Thomas E. Anderson, David E. Culler, David A. Patterson, </author> <title> and the NOW Team. A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: Tempest [37]), or a cleaner communication interface (eg., T3E vs T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters in line with that of the more tightly integrated parallel machines <ref> [3, 34, 41, 10, 20] </ref>. Moving forward from this rich body of research alternatives, a crucial question to answer is how much do the improvements in communication performance actually improve application performance.
Reference: [4] <author> Remzi H. Arpaci, David E. Culler, Arvind Krishnamurthy, Steve Steinberg, and Kathy Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <booktitle> In Proceedings Page 15 of the 22nd International Symposium on Computer Archi--tecture, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller <ref> [25, 4, 28, 40] </ref> or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, <p> controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support <ref> [36, 28, 4] </ref>, supporting reflective memory operations [6, 20], and providing lean This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, the AT&T Foundation, Digital Equipment Corporation, Exabyte, Hewlett Packard, Intel, IBM, Microsoft, Mitsubishi, Siemens Corporation, Sun
Reference: [5] <author> E. Barton, J. Crownie, and M. McLaren. </author> <title> Message Passing on the Meiko CS-2. </title> <booktitle> In Parallel Computing, </booktitle> <volume> volume 20, </volume> <pages> pages 497-507, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors <ref> [36, 5, 7] </ref>, providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, the AT&T Foundation, Digital Equipment Corporation,
Reference: [6] <author> M.A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E.W. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus <ref> [30, 6] </ref>, providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, <p> cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations <ref> [6, 20] </ref>, and providing lean This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, the AT&T Foundation, Digital Equipment Corporation, Exabyte, Hewlett Packard, Intel, IBM, Microsoft, Mitsubishi, Siemens Corporation, Sun Microsystems, and Xerox.
Reference: [7] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. </author> <title> MyrinetA Gigabet-per-Second Local-Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-38, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors <ref> [36, 5, 7] </ref>, providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, the AT&T Foundation, Digital Equipment Corporation,
Reference: [8] <author> S. Borkar. </author> <title> Supporting Systolic and Memory Communication in iWarp. </title> <booktitle> In The 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 70-81, </pages> <address> Seattle, WA, USA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor <ref> [8, 35, 39, 22, 21, 16, 10, 9] </ref>, integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by
Reference: [9] <author> John B. Carter, Al Davis, Ravindra Kuramkote, Chen-Chi Kuo, Leigh B. Stoller, and Mark Swanson. </author> <title> Avalanche: A communication and memory architecture for scalable parallel computing. </title> <type> Technical Report UUCS-95-022, </type> <institution> University of Utah, </institution> <note> SoftwarePractice and Experience 1995. </note>
Reference-contexts: These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor <ref> [8, 35, 39, 22, 21, 16, 10, 9] </ref>, integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by <p> Ongoing architectural efforts which integrate the network interface closer to the processor, either by placing the network interface onto the memory bus [10, 37], into the memory controller [28], or by minimizing the number of stores and loads required to launch a communication event <ref> [9] </ref> will continue to improve application performance.
Reference: [10] <author> D. Chiou, B.S. Ang, Arvind, M.J. Beckerle, G.A. Boughton, R. Greiner, J.E. Hicks, and J.C. Hoe. StarT-NG: </author> <title> Delivering Seamless Parallel Computing. </title> <booktitle> In EURO-PAR'95 Conference, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor <ref> [8, 35, 39, 22, 21, 16, 10, 9] </ref>, integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by <p> Tempest [37]), or a cleaner communication interface (eg., T3E vs T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters in line with that of the more tightly integrated parallel machines <ref> [3, 34, 41, 10, 20] </ref>. Moving forward from this rich body of research alternatives, a crucial question to answer is how much do the improvements in communication performance actually improve application performance. <p> The results suggest that there is considerable additional gain to be obtained by further reducing the communication overheads in clusters. Ongoing architectural efforts which integrate the network interface closer to the processor, either by placing the network interface onto the memory bus <ref> [10, 37] </ref>, into the memory controller [28], or by minimizing the number of stores and loads required to launch a communication event [9] will continue to improve application performance.
Reference: [11] <author> David E Culler, Andrea Dusseau, Seth C. Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Ketherine Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedingsof Supercomputing'93, </booktitle> <pages> pages 262-273, </pages> <year> 1993. </year>
Reference-contexts: Groups of five machines are Page 3 connected to a level-1 switch and each of the seven level-1 switches connects to all three level-2 switches. All but one of the applications is written in an SPMD model using Split-C <ref> [11] </ref>, a parallel extension of the C programming language that provides a global address space on distributed memory machines. Split-C (version 961015) is based on GCC (version 2.6.3) and Generic Active Messages (version 961015), which is the base communication layer throughout the study. <p> For our input size, the local sort time is dominated by the distribution of keys to their proper destinations. For our input of 2 million keys/processor Sample sort spends 85% of the time in the two communication phases. * Em3D: Em3D <ref> [11] </ref> is the kernel of an application that models propagation of electromagnetic waves through objects in three dimensions. It first spreads an irregular bipartite graph over all the processors.
Reference: [12] <author> David E Culler, Richard M. Karp, David A. Patterson, A. Sahay, Klaus E. Schauser, Eric Santos, R. Subramonian, and Thorsten von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 262-273, </pages> <year> 1993. </year>
Reference-contexts: However, it is also important that the communication cost model not be too deeply wedded to a specific machine implementation. The LogP model <ref> [12] </ref> provides such a middle ground by characterizing the performance of the key resources, but not their structure. A distributed-memory multiprocessor in which processors physically communicate by point-to-point messages is characterized by four parameters (illustrated in Figure 1).
Reference: [13] <author> David E. Culler, Lok Tin Liu, Richard P. Martin, and Chad O. Yoshikawa. </author> <title> Assessing Fast Network Interfaces. </title> <booktitle> In IEEE Micro, </booktitle> <volume> volume 16, </volume> <pages> pages 35-43, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: The LogGP characteristics for our cluster is summarized in Table 1. For reference, we also provide measured LogP characteristics for two tightly integrated parallel processors, the Intel Paragon and Meiko CS-2 <ref> [13] </ref>. <p> Such a calibration can be obtained by running a set of Active Message micro-benchmarks, described in <ref> [13] </ref>.
Reference: [14] <author> David E. Culler, David A. Patterson, and Robdert Wilen-sky. </author> <title> The Distributed Supercomputer: The Viable MPP is a Building full of Workstations. Project Proposal at U.C. </title> <address> Berkeley, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This four-parameter characterization of communication performance is based on the LogGP model <ref> [14, 2] </ref>, the framework for our systematic investigation of the communication design space. By adjust Page 1 ing these parameters, we can observe changes in the exe-cution time of applications on a spectrum of systems ranging from the current high-performance cluster to conventional LAN-based clusters.
Reference: [15] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Atchitectural Requirements of Parallel Scientific Applications with Explict Communication. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In principle, the wind tunnel provides enough power to systematically determine application sensitivity to the LogP parameters within a given protocol, although the small local memory of the underlying CM5 may limit the study to small data sets. Cypher <ref> [15] </ref> described the characteristics of a set of substantial message passing applications also showing that application behavior varies widely. The applications were developed in the context of fairly heavy weight message passing libraries and are more heavily biased to bulk transfers.
Reference: [16] <author> W. J. Dally, J. S. Keen, and M. D. Noakes. </author> <title> The J-Machine Architecture and Evaluation. </title> <booktitle> In COMPCON, </booktitle> <pages> pages 183-188, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor <ref> [8, 35, 39, 22, 21, 16, 10, 9] </ref>, integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by
Reference: [17] <author> D.L. Dill, A. Drexler, A.J. Hu, and C.H Yang. </author> <title> Protocol Verification as a Hardware Design Aid. </title> <booktitle> In International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <year> 1992. </year>
Reference-contexts: The dark spots in are visible from multiple points in the scene. * Parallel Mur: In this parallel version of a popular protocol verification tool <ref> [17] </ref>, the exponential space of all reachable protocol states are explored to catch protocol bugs. Each processor maintains a work queue of unexplored states. A hash function maps states to owning processors. When a new state is discovered, it is sent to the proper processor.
Reference: [18] <author> Andrea C. Dusseau, David E. Culler, Klaus E. Schauser, and Richard P. Martin. </author> <title> Fast Parallel Sorting Under LogP: Experience with the CM-5. </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> volume 7, </volume> <pages> pages 791-805, </pages> <year> 1996. </year>
Reference-contexts: Each application is discussed briefly below. * Radix Sort: sorts a large collection of 32-bit keys spread over the processors, and is thoroughly analyzed in <ref> [18] </ref>. It progresses as two iterations of three phases. First, each processor determines the local rank for one digit of its keys. Second, the global rank of each key is calculated from local histograms. Finally, each processor uses the global histogram to distribute the keys to the proper location.
Reference: [19] <author> S. Frank, H. Burkhard II, and J. Rthnie. </author> <title> The KSR 1: Bridging the Gap Between Shared Memory and MPPs. </title> <booktitle> In COM-PCON, </booktitle> <pages> pages 285-294, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller <ref> [31, 1, 19] </ref>, incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and
Reference: [20] <author> Richard B. Gillett. </author> <title> Memory Channel Network for PCI. </title> <booktitle> In IEEE Micro, </booktitle> <volume> volume 16, </volume> <pages> pages 12-18, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations <ref> [6, 20] </ref>, and providing lean This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, the AT&T Foundation, Digital Equipment Corporation, Exabyte, Hewlett Packard, Intel, IBM, Microsoft, Mitsubishi, Siemens Corporation, Sun Microsystems, and Xerox. <p> Tempest [37]), or a cleaner communication interface (eg., T3E vs T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters in line with that of the more tightly integrated parallel machines <ref> [3, 34, 41, 10, 20] </ref>. Moving forward from this rich body of research alternatives, a crucial question to answer is how much do the improvements in communication performance actually improve application performance.
Reference: [21] <author> V. G. Grafe and J. E. Hoch. </author> <title> The Epsilon-2 Hybrid Dataflow Architecture. </title> <booktitle> In COMPCON, </booktitle> <pages> pages 88-93, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor <ref> [8, 35, 39, 22, 21, 16, 10, 9] </ref>, integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by
Reference: [22] <author> J. R. Gurd, C. C. Kerkham, and I. Watson. </author> <title> The Machester Prototype Dataflow Computer. </title> <journal> In Communications of the ACM, </journal> <volume> volume 28, </volume> <pages> pages 34-52, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor <ref> [8, 35, 39, 22, 21, 16, 10, 9] </ref>, integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by
Reference: [23] <author> Mark Heinrich, Jeffrey Kuskin, David Ofelt, John Hein-lein, Joel Baxter, Jaswinder Pal Singh, Richard Simoni, Kourosh Gharachorloo, David Nakahira, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-285, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: These issues have driven a number of efforts to develop powerful simulators [37, 38], as well as to develop flexible hardware prototypes <ref> [23] </ref>. The drawback of a real system is that it is most suited to investigate design points that are slower in some sense than the base hardware.
Reference: [24] <author> Chris Holt, Mark Heinrich, Jaswinder Pal Singh, Edward Rothberg, and John Hennessy. </author> <title> The Effects of Latency, Occupancy, and Bandwidth in Distributed Shared Memory Multiprocessors. </title> <type> Technical Report CSL-TR-95-660, </type> <institution> Stan-ford University, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: This result suggests that in some cases, rather than making a significant investment to double a machine's processing capacity, the investment may be better directed toward improving the performance of the communication system. 6 Related Work The Flash team recently conducted a study with very similar goals under simulation <ref> [24] </ref>. This study focuses on understanding the performance requirements for a communication controller for a cache-coherent distributed memory machine.
Reference: [25] <author> E. D. Brooks III, B. C. Gorda, K. H. Warren, </author> <title> and T.S. Welcome. BBN TC2000 Architecture and Programming Models. </title>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller <ref> [25, 4, 28, 40] </ref> or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28,
Reference: [26] <author> Jonathan Kay and Joseph Pasquale. </author> <title> The Importance of Non-Data-Touching Overheads in TCP/IP. </title> <booktitle> In Proceedings of the 1993 SIGCOMM, </booktitle> <pages> pages 259-268, </pages> <address> San Francisco, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: As overhead is increased, the system becomes similar to a very good LAN implementation. Currently, 100 s of overhead with latency and gap values similar to our network is approximately characteristic of very good TCP/IP protocol stacks coupled with a high speed LAN <ref> [26, 27, 41] </ref>. At this extreme, applications slow down from 2x to almost 30x. Clearly, efforts to reduce cluster communication overhead have been successful. Further, all but one of our applications demonstrate a linear dependence to overhead, suggesting that further reduction in overhead will continue to yield improved performance.
Reference: [27] <author> Kimberly Keeton, David A. Patterson, and Thomas E. An-derson. </author> <title> LogP Quantified: The Case for Low-Overhead Local Area Networks. In Hot Interconnects III, </title> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: As overhead is increased, the system becomes similar to a very good LAN implementation. Currently, 100 s of overhead with latency and gap values similar to our network is approximately characteristic of very good TCP/IP protocol stacks coupled with a high speed LAN <ref> [26, 27, 41] </ref>. At this extreme, applications slow down from 2x to almost 30x. Clearly, efforts to reduce cluster communication overhead have been successful. Further, all but one of our applications demonstrate a linear dependence to overhead, suggesting that further reduction in overhead will continue to yield improved performance.
Reference: [28] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Hein-lein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stan-ford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller <ref> [25, 4, 28, 40] </ref> or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, <p> controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support <ref> [36, 28, 4] </ref>, supporting reflective memory operations [6, 20], and providing lean This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, the AT&T Foundation, Digital Equipment Corporation, Exabyte, Hewlett Packard, Intel, IBM, Microsoft, Mitsubishi, Siemens Corporation, Sun <p> The results suggest that there is considerable additional gain to be obtained by further reducing the communication overheads in clusters. Ongoing architectural efforts which integrate the network interface closer to the processor, either by placing the network interface onto the memory bus [10, 37], into the memory controller <ref> [28] </ref>, or by minimizing the number of stores and loads required to launch a communication event [9] will continue to improve application performance.
Reference: [29] <author> Alvin R. Lebeck and David A. Wood. </author> <title> Dynamic Self-Invalidation: Reducing Coherence Overhead in Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year> <pages> Page 16 </pages>
Reference-contexts: The Flash hardware prototype should provide an ideal testbed to perform a systematic in situ study like the one reported here in the DSM context. The Wind Tunnel team has explored a number of cooperative shared memory design points relative to the Tempest interface through simulation and prototyping <ref> [37, 29] </ref>, focused primarily on protocols. In principle, the wind tunnel provides enough power to systematically determine application sensitivity to the LogP parameters within a given protocol, although the small local memory of the underlying CM5 may limit the study to small data sets.
Reference: [30] <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Ganmukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D.S. Wells, M. C. Wond, S. Yang, and R. Zak. </author> <title> The Network Atchitecture of the CM-5. </title> <booktitle> In Symposium on Prallel and Distributed Algorithms, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus <ref> [30, 6] </ref>, providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO,
Reference: [31] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller <ref> [31, 1, 19] </ref>, incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and
Reference: [32] <author> Steven S. Lumetta, Arvind Krishnamurthy, and David E. Culler. </author> <title> Towards Modeling the Performance of a Fast Connected Components Algorithm on Parallel Machines. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: During the interaction phase, the processors cache oct-tree nodes owned by remote processors in a software managed cache. Communication is generally balanced, with the diagonal strip in Figure 4 resulting from frequent barriers. * Connected Components: All connected components of a graph are spread across all processors <ref> [32] </ref>. Each processor then performs a connected components on its local subgraph to collapse portions of its components into representative nodes. Next, the graph is globally adjusted to point remote edges (crossing processor boundaries) at the respective representative nodes. Finally, a global phase successively merges components between neighboring processors.
Reference: [33] <author> Richard P. Martin. HPAM: </author> <title> An Active Message Layer for a Network of Workstations. </title> <booktitle> In Proceedings of the 2nd Hot Interconnects Conference, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: While the programming model does not provide automatic replication with cache coherence, a number of the applications perform their own caching, as described below. The language has been ported to many platforms <ref> [42, 33, 2, 41] </ref>. The sources for the applications, compiler, and communication layer were obtained from the publically available site 1 . 3.2 Technique The key experimental innovation is to modify the communication layer so that it can emulate a system with arbitrary overhead, gap, or latency.
Reference: [34] <author> Scott Pakin, Mario Lauria, and Andrew Chien. </author> <title> High Performance Messaging on Workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Supercomputing '95, </booktitle> <address> San Diego, California, </address> <year> 1995. </year>
Reference-contexts: Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship. The authors can be contacted at frmartin, vahdat, culler, teag@cs.berkeley.edu. communication software layers <ref> [42, 41, 34] </ref>. Recently, we have seen a shift to designs that accept a reduction in communication performance to obtain greater generality (e.g., Flash vs Dash), greater opportunity for specialization (e.g. Tempest [37]), or a cleaner communication interface (eg., T3E vs T3D). <p> Tempest [37]), or a cleaner communication interface (eg., T3E vs T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters in line with that of the more tightly integrated parallel machines <ref> [3, 34, 41, 10, 20] </ref>. Moving forward from this rich body of research alternatives, a crucial question to answer is how much do the improvements in communication performance actually improve application performance.
Reference: [35] <author> Greg M. Papadopoulos and David E. Culler. Monsoon: </author> <title> An Explicit Token-Store Architecture. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 82-91, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor <ref> [8, 35, 39, 22, 21, 16, 10, 9] </ref>, integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by
Reference: [36] <author> Paul Pierce and Greg Regnier. </author> <title> The Paragon Implementation of the NX Message Passing Interface. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 184-190, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors <ref> [36, 5, 7] </ref>, providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, the AT&T Foundation, Digital Equipment Corporation, <p> controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support <ref> [36, 28, 4] </ref>, supporting reflective memory operations [6, 20], and providing lean This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, the AT&T Foundation, Digital Equipment Corporation, Exabyte, Hewlett Packard, Intel, IBM, Microsoft, Mitsubishi, Siemens Corporation, Sun
Reference: [37] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The authors can be contacted at frmartin, vahdat, culler, teag@cs.berkeley.edu. communication software layers [42, 41, 34]. Recently, we have seen a shift to designs that accept a reduction in communication performance to obtain greater generality (e.g., Flash vs Dash), greater opportunity for specialization (e.g. Tempest <ref> [37] </ref>), or a cleaner communication interface (eg., T3E vs T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters in line with that of the more tightly integrated parallel machines [3, 34, 41, 10, 20]. <p> These issues have driven a number of efforts to develop powerful simulators <ref> [37, 38] </ref>, as well as to develop flexible hardware prototypes [23]. The drawback of a real system is that it is most suited to investigate design points that are slower in some sense than the base hardware. <p> The Flash hardware prototype should provide an ideal testbed to perform a systematic in situ study like the one reported here in the DSM context. The Wind Tunnel team has explored a number of cooperative shared memory design points relative to the Tempest interface through simulation and prototyping <ref> [37, 29] </ref>, focused primarily on protocols. In principle, the wind tunnel provides enough power to systematically determine application sensitivity to the LogP parameters within a given protocol, although the small local memory of the underlying CM5 may limit the study to small data sets. <p> The results suggest that there is considerable additional gain to be obtained by further reducing the communication overheads in clusters. Ongoing architectural efforts which integrate the network interface closer to the processor, either by placing the network interface onto the memory bus <ref> [10, 37] </ref>, into the memory controller [28], or by minimizing the number of stores and loads required to launch a communication event [9] will continue to improve application performance.
Reference: [38] <author> Mendel Rosenblum, Stephen A. Herrod, Emmett Witchel, and Anoop Gupta. </author> <title> Complete Computer Simulation: The SimOS Approach. </title> <booktitle> In IEEE Parallel and Distributed Technology, </booktitle> <month> Fall </month> <year> 1995. </year>
Reference-contexts: These issues have driven a number of efforts to develop powerful simulators <ref> [37, 38] </ref>, as well as to develop flexible hardware prototypes [23]. The drawback of a real system is that it is most suited to investigate design points that are slower in some sense than the base hardware.
Reference: [39] <author> S. Sakai, Y. Yamaguchi, K. Hiraki, Y. Kodama, and T. Yuba. </author> <title> An Architecture of a Dataflow Single Chip Processor. </title> <booktitle> In Proc. of the 16th Annual Int. Symp. on Comp. Arch., </booktitle> <pages> pages 46-53, </pages> <address> Jerusalem, Israel, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller [25, 4, 28, 40] or the cache controller [31, 1, 19], incorporating messaging deep into the processor <ref> [8, 35, 39, 22, 21, 16, 10, 9] </ref>, integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28, 4], supporting reflective memory operations [6, 20], and providing lean This work was supported in part by
Reference: [40] <author> Steven L. Scott. </author> <title> Synchronization and Communication in the T3E Multiprocessor. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a huge space of alternatives, ranging from integrating message transactions into the memory controller <ref> [25, 4, 28, 40] </ref> or the cache controller [31, 1, 19], incorporating messaging deep into the processor [8, 35, 39, 22, 21, 16, 10, 9], integrating the network interface on the memory bus [30, 6], providing dedicated message processors [36, 5, 7], providing various kinds of bulk transfer support [36, 28,
Reference: [41] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the Fifteenth SOSP, </booktitle> <pages> pages 40-53, </pages> <address> Copper Mountain, CO, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship. The authors can be contacted at frmartin, vahdat, culler, teag@cs.berkeley.edu. communication software layers <ref> [42, 41, 34] </ref>. Recently, we have seen a shift to designs that accept a reduction in communication performance to obtain greater generality (e.g., Flash vs Dash), greater opportunity for specialization (e.g. Tempest [37]), or a cleaner communication interface (eg., T3E vs T3D). <p> Tempest [37]), or a cleaner communication interface (eg., T3E vs T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters in line with that of the more tightly integrated parallel machines <ref> [3, 34, 41, 10, 20] </ref>. Moving forward from this rich body of research alternatives, a crucial question to answer is how much do the improvements in communication performance actually improve application performance. <p> While the programming model does not provide automatic replication with cache coherence, a number of the applications perform their own caching, as described below. The language has been ported to many platforms <ref> [42, 33, 2, 41] </ref>. The sources for the applications, compiler, and communication layer were obtained from the publically available site 1 . 3.2 Technique The key experimental innovation is to modify the communication layer so that it can emulate a system with arbitrary overhead, gap, or latency. <p> As overhead is increased, the system becomes similar to a very good LAN implementation. Currently, 100 s of overhead with latency and gap values similar to our network is approximately characteristic of very good TCP/IP protocol stacks coupled with a high speed LAN <ref> [26, 27, 41] </ref>. At this extreme, applications slow down from 2x to almost 30x. Clearly, efforts to reduce cluster communication overhead have been successful. Further, all but one of our applications demonstrate a linear dependence to overhead, suggesting that further reduction in overhead will continue to yield improved performance.
Reference: [42] <author> Thorsten von Eicken, David E. Culler, Steh C. Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship. The authors can be contacted at frmartin, vahdat, culler, teag@cs.berkeley.edu. communication software layers <ref> [42, 41, 34] </ref>. Recently, we have seen a shift to designs that accept a reduction in communication performance to obtain greater generality (e.g., Flash vs Dash), greater opportunity for specialization (e.g. Tempest [37]), or a cleaner communication interface (eg., T3E vs T3D). <p> While the programming model does not provide automatic replication with cache coherence, a number of the applications perform their own caching, as described below. The language has been ported to many platforms <ref> [42, 33, 2, 41] </ref>. The sources for the applications, compiler, and communication layer were obtained from the publically available site 1 . 3.2 Technique The key experimental innovation is to modify the communication layer so that it can emulate a system with arbitrary overhead, gap, or latency.
Reference: [43] <author> Steven Cameron Woo, Moriwoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year> <pages> Page 17 </pages>
Reference-contexts: Also, we are able to run applications on realistic input sizes, so we escape the difficulties of attempting to size the machine parameters down to levels appropriate for the small problems feasible on a simulator and then extrapolating to the real case <ref> [43] </ref>. These issues have driven a number of efforts to develop powerful simulators [37, 38], as well as to develop flexible hardware prototypes [23]. The drawback of a real system is that it is most suited to investigate design points that are slower in some sense than the base hardware. <p> Em3d is bulk synchronous, write-based and contains relatively short computation steps. The locality of connectivity in the graph is indicated by the dark swath in Figure 4c. * Barnes: Our implementation of this hierarchical N-Body force calculation is similar to the version in the SPLASH benchmark suite <ref> [43] </ref>. However, the main data structure, a spatial oct-tree, is replicated in software rather than hardware. Each timestep consists of two phases, a tree construction phase and an interaction phase among the simulated bodies. Updates of the oct-tree are synchronized through blocking locks.
References-found: 43

