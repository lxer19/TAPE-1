URL: http://www.cse.ogi.edu/~denni/Publications/mlpvslvq.ps.Z
Refering-URL: http://www.cse.ogi.edu/~denni/publications.html
Root-URL: http://www.cse.ogi.edu
Title: LU TP  Pattern Discrimination Using Feed-Forward Networks a Benchmark Study of Scaling Behaviour  
Author: Thorsteinn Rognvaldsson 
Note: Solvegatan 14 A, S-223 62 Lund, Sweden Submitted to Neural Computation 1 denni@thep.lu.se  
Affiliation: Department of Theoretical Physics, University of Lund,  
Date: June 1992  
Pubnum: 92-18  
Abstract: The discrimination powers of Multilayer perceptron (MLP) and Learning Vector Quantisation (LVQ) networks are compared for overlapping Gaussian distributions. It is shown, both analytically and with Monte Carlo studies, that the MLP network handles high dimensional problems in a more efficient way than LVQ. This is mainly due to the sigmoidal form of the MLP transfer function, but also to the the fact that the MLP uses hyper-planes more efficiently. Both algorithms are equally robust to limited training sets and the learning curves fall off like 1=M, where M is the training set size, which is compared to theoretical predictions from statistical estimates and Vapnik-Chervonenkis bounds. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> D. Rumelhart and J. McLelland (eds.) </editor> <booktitle> Parallel Distributed Processing (Vol. </booktitle> <volume> 1), </volume> <publisher> MIT Press (1986) </publisher>
Reference-contexts: Introduction The task of discriminating between different classes of input patterns has proven to be well suited for artificial neural networks (ANN). Standard methods, like making cuts or discriminant analysis, are repeatedly being outperformed by non-linear ANN algorithms, where the most extensively used algorithms are the feed-forward Multilayer Perceptron (MLP) <ref> [1] </ref> and the Learning Vector Quantisation (LVQ) [2]. Both algorithms have shown good discrimination and generalisation ability, although some confusion prevails on their performance on realistic large sized problems, especially concerning their parsimony in parameters to fit data an important issue when algorithms are transferred to hardware.
Reference: [2] <author> T. Kohonen, </author> <title> Self-organization and Associative Memory, 3 rd ed., </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg (1990) </address>
Reference-contexts: Standard methods, like making cuts or discriminant analysis, are repeatedly being outperformed by non-linear ANN algorithms, where the most extensively used algorithms are the feed-forward Multilayer Perceptron (MLP) [1] and the Learning Vector Quantisation (LVQ) <ref> [2] </ref>. Both algorithms have shown good discrimination and generalisation ability, although some confusion prevails on their performance on realistic large sized problems, especially concerning their parsimony in parameters to fit data an important issue when algorithms are transferred to hardware.
Reference: [3] <author> See e.g. R. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons (1973). </publisher>
Reference-contexts: Two classes are sufficient since the results carry over to problems with more classes. The problem is designed to resemble "real-life" situations with many input nodes and overlapping distributions, making the classification fuzzy. Discrimination is thus only possible down to a minimum error; the Bayes limit <ref> [3] </ref>. It is found, in contrast to previous results [4, 5], that the MLP is more efficient than the LVQ algorithm on heavily overlapping distributions. The sensitivities of the two algorithms to limited training data are also examined and compared to theoretical predictions.
Reference: [4] <author> T. Kohonen, G. Barna and R. Chrisley, </author> <title> "Statistical Pattern Recognition with Neural Networks: Benchmarking Studies", </title> <booktitle> IEEE 2 nd Int. Conf. on Neural Networks I, </booktitle> <pages> 61-68, </pages> <address> San Diego, CA: </address> <publisher> SOS Printing (1988) </publisher>
Reference-contexts: The problem is designed to resemble "real-life" situations with many input nodes and overlapping distributions, making the classification fuzzy. Discrimination is thus only possible down to a minimum error; the Bayes limit [3]. It is found, in contrast to previous results <ref> [4, 5] </ref>, that the MLP is more efficient than the LVQ algorithm on heavily overlapping distributions. The sensitivities of the two algorithms to limited training data are also examined and compared to theoretical predictions. <p> It is found, in contrast to previous results [4, 5], that the MLP is more efficient than the LVQ algorithm on heavily overlapping distributions. The sensitivities of the two algorithms to limited training data are also examined and compared to theoretical predictions. The Problem The problem <ref> [4] </ref> consists of two overlapping Gaussian distributions, P 1 and P 2 , of dimensionality d and normalised to unity with standard deviations 1 = 1:0 and 2 = 2:0. <p> Two versions are generated; one where the distributions have the same mean, referred to as the "hard" case, and one where their means are separated by a distance ~, referred to as the "easy" case (notation follows <ref> [4] </ref>): P 1 (~r) = ( 1 2) d exp [ 2 2 ] (1) p (~r ~ ~) 2 2 where ~ ~ = ~ 0 for the "hard" case, and ~ ~ = (2:32; 0; 0; : : : ; 0) for the "easy" case. <p> LVQ updating was then applied for an additional 150 epochs, while the learning rate was lowered geometrically from = 0:1 down to = 0:001. One epoch corresponded to 1000 patterns. This choice of parameters and initialisation was made to match those of ref. <ref> [4] </ref>. The MLP networks had d input units and one hidden layer with N MLP hidden units, where N MLP 2 fd; d + 2; d + 4; : : : ; d + 2 11g. <p> Conclusions and Discussion The results demonstrate that the MLP architecture, with sigmoidal transfer functions, is superior to LVQ for discriminating between heavily overlapping distributions with convex borders. This is in contrast to previous results comparing LVQ and MLP architectures <ref> [4, 5] </ref>, but in agreement with results for Boltzmann [4] and Mean Field machines [11]. The MLP method is highly efficient and the more viable alternative for problems with many inputs. Furthermore, both algorithms are equally robust to limited training data and their learning curves follow a 1=M behaviour. <p> Conclusions and Discussion The results demonstrate that the MLP architecture, with sigmoidal transfer functions, is superior to LVQ for discriminating between heavily overlapping distributions with convex borders. This is in contrast to previous results comparing LVQ and MLP architectures [4, 5], but in agreement with results for Boltzmann <ref> [4] </ref> and Mean Field machines [11]. The MLP method is highly efficient and the more viable alternative for problems with many inputs. Furthermore, both algorithms are equally robust to limited training data and their learning curves follow a 1=M behaviour.
Reference: [5] <author> G. Barna and K. Kaski, </author> <title> "Stochastic vs. Deterministic Neural Networks for Pattern Recognition", </title> <journal> Physica Scripta T33, </journal> <month> 110-115 </month> <year> (1990) </year>
Reference-contexts: The problem is designed to resemble "real-life" situations with many input nodes and overlapping distributions, making the classification fuzzy. Discrimination is thus only possible down to a minimum error; the Bayes limit [3]. It is found, in contrast to previous results <ref> [4, 5] </ref>, that the MLP is more efficient than the LVQ algorithm on heavily overlapping distributions. The sensitivities of the two algorithms to limited training data are also examined and compared to theoretical predictions. <p> Conclusions and Discussion The results demonstrate that the MLP architecture, with sigmoidal transfer functions, is superior to LVQ for discriminating between heavily overlapping distributions with convex borders. This is in contrast to previous results comparing LVQ and MLP architectures <ref> [4, 5] </ref>, but in agreement with results for Boltzmann [4] and Mean Field machines [11]. The MLP method is highly efficient and the more viable alternative for problems with many inputs. Furthermore, both algorithms are equally robust to limited training data and their learning curves follow a 1=M behaviour. <p> Allowing direct input to output connections in the MLP would increase the discrimination power further [6, 11]. The LVQ algorithm can be augmented with more advanced evaluations of the output signals <ref> [5, 13] </ref> and thus be made to perform better. With such superstructures one can achieve discrimination performances comparable to the MLP, but at the price of many more parameters. Acknowledgements This work has benefited from discussions with Bo Soderberg. 8
Reference: [6] <author> E. Sontag, </author> <title> "Feedforward Nets for Interpolation and Classification", </title> <note> to appear in Journal of Computer and System Sciences, </note> <year> (1992) </year>
Reference-contexts: Hence, an MLP with N MLP hidden units has N = N MLP . A smooth sigmoidal transfer function allows the MLP to "cut corners" in the polyhedron, and the generalisation error subsequently scales better than (9). How much better is difficult to predict, but analytical results <ref> [6] </ref> imply that a sigmoid allows a decrease of the hidden units with at least a factor of two, compared to a Heaviside. <p> The reason the MLP performs well already for low N MLP is clearly its sigmoidal transfer function, making it possible to smooth the corners of the polyhedron. Allowing direct input to output connections in the MLP would increase the discrimination power further <ref> [6, 11] </ref>. The LVQ algorithm can be augmented with more advanced evaluations of the output signals [5, 13] and thus be made to perform better. With such superstructures one can achieve discrimination performances comparable to the MLP, but at the price of many more parameters.
Reference: [7] <author> J. MacQueen. </author> <title> "Some Methods for Classification and Analysis of Multivariate Observations", </title> <editor> in L. M. LeCam and J. Neyman (eds.), </editor> <booktitle> Proc. 5th Berkeley Symposium on Math. </booktitle> <editor> Stat. and Prob., U. </editor> <publisher> California Press, </publisher> <address> Berkeley (1967). </address>
Reference-contexts: The LVQ networks had d input units and N LV Q processing units, where N LV Q 2 fd; d + 4; d + 8; : : : ; d + 4 11g. The weights were initialised with the on-line k-means clustering algorithm <ref> [7] </ref> for a duration of 150 epochs, with randomly selected starting points. LVQ updating was then applied for an additional 150 epochs, while the learning rate was lowered geometrically from = 0:1 down to = 0:001. One epoch corresponded to 1000 patterns.
Reference: [8] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Pattern Recognition in High Energy Physics with Artificial Neural Networks - JETNET 2.0", </title> <journal> Computer Physics Communications 70, </journal> <month> 167-182 </month> <year> (1992) </year>
Reference-contexts: For each value of M the number of epochs was changed to keep the total number of presentations constant at 4 10 5 for the MLP and 3 10 5 for the LVQ networks. All simulations were performed with the network program package JETNET 2.0 <ref> [8] </ref>. 3 The "fan-in" of a unit is the number of units feeding to it. 5 (a) MLP and (b) LVQ on the "hard" task, and (c) MLP and (d) LVQ on the "easy" task.
Reference: [9] <author> See e.g. D. Cohn and G. Tesauro, </author> <title> "How Tight are the Vapnik-Chervonenkis Bounds?", </title> <booktitle> Neural Computation 4, </booktitle> <month> 249-269 </month> <year> (1992) </year>
Reference-contexts: The worst case upper bound generalisation error for this kind of binary classification problem is expected to be E d V C =M , where d V C is the Vapnik-Chervonenkis dimension of the network <ref> [9] </ref>. Figure 4 shows that this bound is indeed well above the actual learning curves if the number of weights is used as an approximate value of d V C .
Reference: [10] <author> D. Schwartz, V. Samalam, S. Solla and J. Denker, </author> <title> "Exhaustive Learning", </title> <booktitle> Neural Computation 2, </booktitle> <month> 374-385 </month> <year> (1990) </year>
Reference-contexts: The learning curves are also well described by E / 1=(M + M 0 ), predicted by statistical learning theories for problems with continuous generalisation spectrum <ref> [10] </ref>, and the algorithms are equally robust to limited training sets. An attempt was also made to estimate the prior generalisation spectrum. <p> The dotted lines are the curves numerically calculated from the prior generalisation spectrum. where hg M i 0 is the M th moment of the prior generalisation distribution <ref> [10] </ref>. The resulting curves (dotted lines in fig. 4) do not follow the true learning curves. Statistical errors, however, are large already for lg M 1:5 and higher statistics was not pursued due to the CPU consumption involved.
Reference: [11] <author> C. Peterson and E. Hartman, </author> <title> "Explorations of the Mean Field Theory Learning Algorithm", </title> <booktitle> Neural Networks 2, </booktitle> <month> 475-494 </month> <year> (1989) </year>
Reference-contexts: This is in contrast to previous results comparing LVQ and MLP architectures [4, 5], but in agreement with results for Boltzmann [4] and Mean Field machines <ref> [11] </ref>. The MLP method is highly efficient and the more viable alternative for problems with many inputs. Furthermore, both algorithms are equally robust to limited training data and their learning curves follow a 1=M behaviour. <p> The reason the MLP performs well already for low N MLP is clearly its sigmoidal transfer function, making it possible to smooth the corners of the polyhedron. Allowing direct input to output connections in the MLP would increase the discrimination power further <ref> [6, 11] </ref>. The LVQ algorithm can be augmented with more advanced evaluations of the output signals [5, 13] and thus be made to perform better. With such superstructures one can achieve discrimination performances comparable to the MLP, but at the price of many more parameters.
Reference: [12] <author> M. Richard and R. Lippmann, </author> <title> "Neural Networks Estimate Bayesian a posteriori Probabilities", </title> <booktitle> in Neural Computation 3, </booktitle> <month> 461-483 </month> <year> (1991) </year>
Reference-contexts: An MLP with n output units can be used for a n-class problem. 7 The output values will then correspond to the Bayesian a posteriori likelihoods that the input pattern belongs to the specific class <ref> [12] </ref>. A discrimination decision can therefore be made from these output values and the number of hidden units will in the worst case just be multiplied by n.
Reference: [13] <author> I. Scabai, F. Czako and Z. Fodor, </author> <title> "Combined Neural Network - QCD Classifier for Quark and Gluon Jet Separation", </title> <journal> Nuclear Physics B374, </journal> <month> 288-308 </month> <year> (1992) </year> <month> 9 </month>
Reference-contexts: Allowing direct input to output connections in the MLP would increase the discrimination power further [6, 11]. The LVQ algorithm can be augmented with more advanced evaluations of the output signals <ref> [5, 13] </ref> and thus be made to perform better. With such superstructures one can achieve discrimination performances comparable to the MLP, but at the price of many more parameters. Acknowledgements This work has benefited from discussions with Bo Soderberg. 8
References-found: 13

