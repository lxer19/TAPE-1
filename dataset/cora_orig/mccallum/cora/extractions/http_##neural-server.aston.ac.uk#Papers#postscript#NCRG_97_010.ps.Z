URL: http://neural-server.aston.ac.uk/Papers/postscript/NCRG_97_010.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00355.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Probabilistic Principal Component Analysis  
Author: Michael E. Tipping Christopher M. Bishop 
Note: Submitted for publication.  
Address: Birmingham B4 7ET United Kingdom  
Affiliation: Neural Computing Research Group Dept of Computer Science Applied Mathematics Aston University  
Pubnum: Technical Report NCRG/97/010  
Email: M.E.Tipping@aston.ac.uk  C.M.Bishop@aston.ac.uk  
Phone: Tel: +44 (0)121 333 4631  
Date: September 4, 1997  
Web: http://www.ncrg.aston.ac.uk/  
Abstract: Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss the advantages conveyed by the definition of a probability density function for PCA. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, T. W. </author> <year> (1963). </year> <title> Asymptotic theory for principal component analysis. </title> <journal> Annals of Mathematical Statistics 34, </journal> <pages> 122-148. </pages>
Reference: <author> Bartholomew, D. J. </author> <year> (1987). </year> <title> Latent Variable Models and Factor Analysis. </title> <address> London: Charles Griffin & Co. </address> <publisher> Ltd. </publisher>
Reference-contexts: Generally, q &lt; d such that the latent variables offer a more parsimonious description of the data. By defining a prior distribution over x, equation (1) induces a corresponding distribution in the data space and the model parameters may be determined by maximum-likelihood. In standard factor analysis <ref> (Bartholomew 1987) </ref> the mapping y (x; ) is linear: t = Wx + + *; (2) where the latent variables x ~ N (0; I) have a unit isotropic Gaussian distribution.
Reference: <author> Basilevsky, A. </author> <year> (1994). </year> <title> Statistical Factor Analysis and Related Methods. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference: <author> Chatfield, C. and A. J. </author> <title> Collins (1980). Introduction to Multivariate Analysis. </title> <publisher> London: Chapman & Hall. </publisher>
Reference-contexts: In the latter method, considerable care must be taken in the choice of the number of factors q. An inappropriate choice can easily give misleading results, and some practitioners have been quite emphatic in their warnings <ref> (notably Chatfield and Collins 1980, chapter 5) </ref>. A major problem is that if the observations can be explained sufficiently by, say, two factors, a model which attempts to identify only a single factor may often fail to find either of the sufficient two, but may instead find a third alternative.
Reference: <author> Dempster, A. P., N. M. Laird, and D. B. </author> <title> Rubin (1977). Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B 39 (1), </volume> <pages> 1-38. </pages>
Reference: <author> Hotelling, H. </author> <year> (1933). </year> <title> Analysis of a complex of statistical variables into principal components. </title> <journal> Journal of Educational Psychology 24, </journal> <pages> 417-441. </pages>
Reference-contexts: Examples of its many applications include data compression, image processing, visualization, exploratory data analysis, pattern recognition and time series prediction. The most common derivation of PCA is in terms of a standardised linear projection which max-imises the variance in the projected space <ref> (Hotelling 1933) </ref>. For a set of observed d-dimensional data vectors ft n g; n 2 f1 : : : N g, the q principal axes w j , j 2 f1 : : : qg, are those orthonormal axes onto which the retained variance under projection is maximal.
Reference: <author> Jolliffe, I. T. </author> <year> (1986). </year> <title> Principal Component Analysis. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: 1 Introduction Principal component analysis (PCA) <ref> (Jolliffe 1986) </ref> is a well-established technique for dimension reduction, and a chapter on the subject may be found in practically every text on multivariate analysis. Examples of its many applications include data compression, image processing, visualization, exploratory data analysis, pattern recognition and time series prediction.
Reference: <author> Krzanowski, W. J. and F. H. C. </author> <title> Marriott (1994). Multivariate Analysis Part I: Distributions, Ordination and Inference. </title> <publisher> London: Edward Arnold. </publisher>
Reference: <author> Pearson, K. </author> <year> (1901). </year> <title> On lines and planes of closest fit to systems of points in space. </title> <journal> The London, Edinburgh and Dublin Philosophical Magazine and Journal of Science, </journal> <volume> Sixth Series 2, </volume> <pages> 559-572. </pages>
Reference: <author> Rao, C. R. </author> <year> (1955). </year> <title> Estimation and tests of significance in factor analysis. </title> <type> Psychometrika 20, </type> <pages> 93-111. </pages>
Reference-contexts: This implies that the diagonal elements of the error matrix in factor analysis above should be identical. Indeed, the similarity between the factor loadings and the principal axes has often been observed in situations in which the elements of are approximately equal <ref> (Rao 1955) </ref>. Basilevsky (1994) further notes that when the model WW T + 2 I is exact, and therefore equal to S, the factor loadings are identifiable and can be determined analytically through eigen-decomposition of S, without resort to iteration.
Reference: <author> Rubin, D. B. and D. T. </author> <title> Thayer (1982). EM algorithms for ML factor analysis. </title> <booktitle> Psychome-trika 47 (1), </booktitle> <pages> 69-76. </pages>
Reference: <author> Tipping, M. E. and C. M. </author> <title> Bishop (1997a). Hierarchical models for data visualization. </title> <booktitle> In Proceedings of the IEE Fifth International Conference on Artificial Neural Networks, </booktitle> <address> Cambridge, </address> <pages> pp. 70-75. </pages> <address> London: </address> <publisher> IEE. </publisher>
Reference: <author> Tipping, M. E. and C. M. </author> <title> Bishop (1997b). Mixtures of principal component analysers. </title> <booktitle> In Proceedings of the IEE Fifth International Conference on Aritificial Neural Networks, </booktitle> <address> Cambridge, </address> <pages> pp. </pages> <month> 13-18. </month> <title> London: IEE. 8 Probabilistic Principal Component Analysis </title>
References-found: 14

