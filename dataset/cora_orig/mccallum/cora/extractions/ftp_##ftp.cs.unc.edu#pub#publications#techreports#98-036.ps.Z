URL: ftp://ftp.cs.unc.edu/pub/publications/techreports/98-036.ps.Z
Refering-URL: http://www.cs.unc.edu/Info/Publications/PHDAbstracts.html
Root-URL: http://www.cs.unc.edu
Title: Merging Real and Virtual Environments with Video See-Through Head-Mounted Displays  
Author: by Michael A. Bajura Advisor: Henry Fuchs Reader: Gary Bishop Reader: James Coggins Turner Whitted Jonathan Marshall 
Degree: A Dissertation submitted to the faculty of The  in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the  Approved by:  
Date: 1997  
Address: Chapel Hill  
Affiliation: University of North Carolina at Chapel Hill  Department of Computer Science.  
Abstract-found: 0
Intro-found: 1
Reference: [Adachi et al., 1995] <author> Adachi, Y., Kumano, T., and Ogino, K. </author> <year> (1995). </year> <title> Intermediate representation for stiff virtual objects. </title> <booktitle> In Proceedings of IEEE Virtual Reality Annual International Symposium (VRAIS), </booktitle> <pages> pages 203-210. </pages>
Reference-contexts: The ability to synthesize extremely complex and detailed images at interactive rates is now commonplace. Head-mounted displays and systems for tracking head and body positions are commercial products. Force feedback systems are an active area of research <ref> [Adachi et al., 1995] </ref> [Buttolo and Hannaford, 1995] and are already viable commercial products [PHANToM, 1997]. It seems that the ultimate display problem is well on its way to being solved... Or is it? Many of the original problems present in Sutherland's early system [Sutherland, 1968] remain.
Reference: [Adelstein et al., 1992] <author> Adelstein, B., Johnston, E., and Ellis, S. </author> <year> (1992). </year> <title> A testbed for characterizing dynamic response of virtual environment spatial sensors. </title> <booktitle> In Proceedings 5th Annual ACM Symposium on User Interface Software and Technology (UIST), </booktitle> <pages> pages 15-22, </pages> <address> Monderey, CA. </address> <publisher> ACM SIGGRAPH/SIGCHI. </publisher>
Reference-contexts: Position and orientation errors cause misregistration in all cases, while temporal errors cause misregistration only during user movement. Temporal errors are caused by a delay in sensing and reporting tracking information to the computer graphics system and the computer graphics system's delay in generating the appropriate virtual images <ref> [Adelstein et al., 1992] </ref>. 4. The virtual Camera-to-Image mapping doesn't accurately model the real camera. The Camera-to-Image mapping abstraction is that any real camera can be modelled by an idealized pinhole camera with a particular center of projection, viewing direction, field of view, and distortion function.
Reference: [Akeley, 1993] <author> Akeley, K. </author> <year> (1993). </year> <title> Reality engine graphics. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 109-116. </pages>
Reference-contexts: Recently progress has been made toward rendering more complex images at increasing rates [Molnar et al., 1992] <ref> [Akeley, 1993] </ref>. Unfortunately the emphasis in these systems has been on total system throughput, often at the expense of image-generation latency and flexibility of camera models.
Reference: [Ascension, 1997] <author> Ascension (1997). </author> <title> Flock of Birds magnetic tracking system. </title> <institution> Ascension Technology Corporation, </institution> <address> P.O. Box 527, Burlington, VT 05402, (http://www.ascension-tech.com). </address>
Reference-contexts: A few technologies which have been employed for this are: mechanical linkages [Faro, 1997], magnetic sensors [Polhemus, 1997] <ref> [Ascension, 1997] </ref>, optical sensors [Ward et al., 1992], and acoustic methods [SAC, 1994]. Bhatnagar [Bhatnagar, 1993] provides a survey of these methods. Magnetic tracking systems are attractive because no line-of-sight or mechanical tether is needed.
Reference: [Azarbayejani and Pentland, 1994] <author> Azarbayejani, A. and Pentland, A. </author> <year> (1994). </year> <title> Recursive estimation of motion, structure, and focal length. </title> <type> Technical Report Perceptual Computing Technical Report #243, </type> <note> Submitted to IEEE PAMI July 1994, </note> <institution> MIT Media Laboratory. </institution>
Reference-contexts: Optimal (Kalman) filtering is typically used for this process. The advantage of an incremental approach is that a partial solution is available at intermediate steps. Several authors [Matthies et al., 1989] [Weng et al., 1993] [Broida and Chellappa, 1986] [Thomas and Oliensis, 1992] <ref> [Azarbayejani and Pentland, 1994] </ref> [Bouguet and Perona, 1995] [Morita and Kanade, 1994] report varying results using Kalman filters. Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed. <p> Several authors [Matthies et al., 1989] [Weng et al., 1993] [Broida and Chellappa, 1986] [Thomas and Oliensis, 1992] <ref> [Azarbayejani and Pentland, 1994] </ref> [Bouguet and Perona, 1995] [Morita and Kanade, 1994] report varying results using Kalman filters. Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed.
Reference: [Azuma and Bishop, 1994] <author> Azuma, R. and Bishop, G. </author> <year> (1994). </year> <title> Improving static and dynamic registration in an optical see-through hmd. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 197-204. </pages>
Reference-contexts: This occurs when the delay in the tracking/image-generation loop is different from the delay in the real-world view/image-generation loop <ref> [Azuma and Bishop, 1994] </ref> (note that there are two paths back to the user (figure 2.3)). For example, a user with a 45 degree field-of-view rotating his head at 90 degrees/second with a delay of 100 msec sees an error of 9 degrees or 20% of his field-of-view. <p> A slow update rate or significant latency can cause virtual objects inserted into a natural scene to appear to swim about during user movement. Some success at effectively reducing the latency of tracking systems has been achieved with predictive tracking schemes using optimal filters and accelerometers <ref> [Azuma and Bishop, 1994] </ref> [Azuma and Bishop, 1995]. The drawback with this approach is that tracking error grows rapidly with larger prediction intervals and input signal frequencies. <p> Some success has been achieved in effectively speeding up synthetic images by using predictive tracking schemes <ref> [Azuma and Bishop, 1994] </ref> [Azuma and Bishop, 1995], but again the tracking error increases as position estimates are projected further into the future. The advantage of the video see-through method is complete control over the composite images seen by the user. <p> This trade-off is not possible with optically based AR systems which allow the user to see his surroundings directly. Some success at improving registration error has been achieved with autocalibration approaches [Gottschalk and Hughes, 1993] and predictive tracking techniques <ref> [Azuma and Bishop, 1994] </ref> [List, 1984] which use a state estimate to help predict current measurements. <p> Since feedback can compensate for tracking errors, in essence becoming part of the tracking system itself, less accurate and less expensive tracking systems may be feasible. Optical tracking systems <ref> [Azuma and Bishop, 1994] </ref> could be designed to use stationary cameras to track a user's position while cameras on the user's head could look outward to determine the user's orientation.
Reference: [Azuma and Bishop, 1995] <author> Azuma, R. and Bishop, G. </author> <year> (1995). </year> <title> A frequency-domain analysis of head-motion prediction. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 401-408. </pages>
Reference-contexts: Some success at effectively reducing the latency of tracking systems has been achieved with predictive tracking schemes using optimal filters and accelerometers [Azuma and Bishop, 1994] <ref> [Azuma and Bishop, 1995] </ref>. The drawback with this approach is that tracking error grows rapidly with larger prediction intervals and input signal frequencies. <p> Some success has been achieved in effectively speeding up synthetic images by using predictive tracking schemes [Azuma and Bishop, 1994] <ref> [Azuma and Bishop, 1995] </ref>, but again the tracking error increases as position estimates are projected further into the future. The advantage of the video see-through method is complete control over the composite images seen by the user.
Reference: [Bajura, 1993] <author> Bajura, M. </author> <year> (1993). </year> <title> Camera calibration for video see-through head-mounted display. </title> <type> Technical Report TR93-048, </type> <institution> UNC Chapel Hill. </institution>
Reference-contexts: This is done by operating the AR system and using manual feedback to converge on a solution. Optional compensation for non-linear lens distortion in the Camera-to-Image mapping is measured by examining a distorted camera image and finding a 2D warp function which converts that image into an undistorted one <ref> [Bajura, 1993] </ref>. If non-linear lens distortion is not considered, a best-fit calibration solution by matching field of view is possible even for distorting lenses. fixture is used to represent a fixed position and orientation which are measured relative to the tracking system origin.
Reference: [Bajura et al., 1992] <author> Bajura, M., Fuchs, H., and Ohbuchi, R. </author> <year> (1992). </year> <title> Merging virtual objects with the real world: Seeing ultrasound imagery within the patient. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 203-210. </pages> <booktitle> Published as Proc. ACM SIGGRAPH '92, Computer Graphics, </booktitle> <volume> volume 26, number 2. </volume> <pages> 80 </pages>
Reference-contexts: Another type of SE is an augmented-reality (AR) system where a user is presented with images of his natural surroundings which are augmented with computer-generated images. For example, these could be 3-D medical data sets of a patient which are anatomically registered <ref> [Bajura et al., 1992] </ref> [State et al., 1996b] or 3-D assembly directions for complex machinery [Feiner et al., 1993] [Feiner et al., 1992]. <p> delay presents a more convincing reality than one achievable with less delay in the real-world scene and with incorrect registration, as in optical see-through systems. 2.3 Other AR Systems Augmented-reality systems based on the current model (figure 2.3) have been used to visualize ultrasound medical data within a patient 3 <ref> [Bajura et al., 1992] </ref>, to provide 3D graphical annotations for machinery 4 [Feiner et al., 1993] [Feiner et al., 1992], and to assist manual manufacturing 5 [Caudell and Mizell, 1992]. These systems suffer from the weaknesses outlined in section 2.1.1. <p> Other applications include interactive 3D illustrations for constructing and for maintaining complex machinery [Feiner et al., 1992] [Feiner et al., 1993] [Caudell and Mizell, 1992] and in-patient visualization of medical data, e.g., ultrasound <ref> [Bajura et al., 1992] </ref>. In all these applications it is vitally necessary for computer-generated objects 24 and real-world objects to be visually registered with respect to each other in every image the user sees.
Reference: [Bajura and Neumann, 1995a] <author> Bajura, M. and Neumann, U. </author> <year> (1995a). </year> <title> Dynamic registration correc-tion in augmented-reality systems. </title> <booktitle> In Proceedings of IEEE Virtual Reality Annual International Symposium (VRAIS), </booktitle> <pages> pages 189-196. </pages>
Reference-contexts: The locations of these points in the user's field of view are used to correct various errors in an AR system <ref> [Bajura and Neumann, 1995a] </ref> [Bajura and Neumann, 1995b]. <p> In addition to the trackable features as defined by Tomasi, Forstner identifies lines and edges in static figures. Real-time tracking of simple features in video images has been demonstrated in several applications. A few recent examples are listed here. Bajura <ref> [Bajura and Neumann, 1995a] </ref> demonstrates real-time tracking of LEDs by threshold detection. Blake [Blake and Isard, 1994] demonstrates incremental real-time tracking of curved silhouettes suitable for recognizing hand and lip movement. <p> Further advances in processing power will soon make more general real-time tracking algorithms possible. 21 Chapter 3 Correcting Registration Error This chapter is a reformatted copy of a paper which appeared in the proceedings of the Virtual Reality Annual International Symposium (VRAIS) 1995 <ref> [Bajura and Neumann, 1995a] </ref>. It describes an experiment in which known features are tracked in real-time in an augmented-reality environment. The features are used to correct the apparent positions of virtual objects appearing in the natural scene.
Reference: [Bajura and Neumann, 1995b] <author> Bajura, M. and Neumann, U. </author> <year> (1995b). </year> <title> Dynamic registration correction in video-based augmented reality systems. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 15(5) </volume> <pages> 52-60. </pages>
Reference-contexts: The locations of these points in the user's field of view are used to correct various errors in an AR system [Bajura and Neumann, 1995a] <ref> [Bajura and Neumann, 1995b] </ref>. <p> It can be computed in real time incrementally or by O (n log n) algorithms. 4.6 Correcting Camera Position Using precalibrated features (fiducials) to correct for errors in camera pose has already been demonstrated <ref> [Bajura and Neumann, 1995b] </ref> [State et al., 1996a]. The contribution here is to use features which are not precalibrated to correct camera pose. This is possible under the assumption that the camera's error is normally distributed over time.
Reference: [Besl, 1989] <author> Besl, P. J. </author> <year> (1989). </year> <title> Active optical range imaging sensors. In Sanz, </title> <editor> J. L. C., editor, </editor> <booktitle> Advances in Machine Vision, chapter 1, </booktitle> <pages> pages 1-63. </pages> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: Active strategies use some form of structured light to illuminate a scene which is scanned by a sensor. Examples of active methods are light stripe (triangulation) [Cyberware, 1997] [Gruss et al., 1992], moire (interference), and radar (time-of-flight). Besl <ref> [Besl, 1989] </ref> provides a good overview of active sensing strategies. These methods are unattractive to AR systems because they would involve adding light to the natural scene which may be objectionable and adding hardware which may not be needed.
Reference: [Bhatnagar, 1993] <author> Bhatnagar, D. K. </author> <year> (1993). </year> <title> Position trackers for head-mounted display systems: A survey. </title> <type> Technical Report TR93-010, </type> <institution> University of North Carolina at Chapel Hill. </institution>
Reference-contexts: A few technologies which have been employed for this are: mechanical linkages [Faro, 1997], magnetic sensors [Polhemus, 1997] [Ascension, 1997], optical sensors [Ward et al., 1992], and acoustic methods [SAC, 1994]. Bhatnagar <ref> [Bhatnagar, 1993] </ref> provides a survey of these methods. Magnetic tracking systems are attractive because no line-of-sight or mechanical tether is needed.
Reference: [Bishop, 1984] <author> Bishop, G. </author> <year> (1984). </year> <title> Self-Tracker: A Smart Optical Sensor on Silicon. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, Department of Computer Science. </institution>
Reference-contexts: The ultimate goal for a tracking system is to operate in an unknown environment strictly from visual input: Gary Bishop's Self-Tracker <ref> [Bishop, 1984] </ref>. The ultimate goal of depth recovery is to accurately recover both an unknown scene's structure and a camera's motion within it solely from camera images (the structure-from-motion problem).
Reference: [Blake and Isard, 1994] <author> Blake, A. and Isard, M. </author> <year> (1994). </year> <title> 3d position, attitude and shape input using video tracking of hands and lips. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 185-192. </pages>
Reference-contexts: Real-time tracking of simple features in video images has been demonstrated in several applications. A few recent examples are listed here. Bajura [Bajura and Neumann, 1995a] demonstrates real-time tracking of LEDs by threshold detection. Blake <ref> [Blake and Isard, 1994] </ref> demonstrates incremental real-time tracking of curved silhouettes suitable for recognizing hand and lip movement. The ultrasound visualization project at the University of North Carolina has achieved good results in incrementally tracking color dot targets [State et al., 1996a] under a variety of lighting conditions.
Reference: [Bolles et al., 1987] <author> Bolles, R. C., Baker, H. H., and Marimont, D. H. </author> <year> (1987). </year> <title> Epipolar-plane image analysis: An approach to determining structure from motion. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 7-55. </pages>
Reference-contexts: This can speed up the search for correspondences. Stereo can be thought of as a special case of this method with only two images. Two examples of finding depth from several images are described by Kanade [Kanade et al., 1992] and Bolles <ref> [Bolles et al., 1987] </ref>. By constraining camera motion to be linear, perpendicular to viewing direction, and parallel to image scan lines, image correspondences are more easily located in successive images on identical image scan lines. Feature redundancy in multiple images increases accuracy of the solution.
Reference: [Bouguet and Perona, 1995] <author> Bouguet, J. Y. and Perona, P. </author> <year> (1995). </year> <title> Visual navigation using a single camera. </title> <booktitle> In International Conference on Computer Vision (ICCV), </booktitle> <pages> pages 645-652. </pages> <publisher> IEEE. </publisher>
Reference-contexts: Optimal (Kalman) filtering is typically used for this process. The advantage of an incremental approach is that a partial solution is available at intermediate steps. Several authors [Matthies et al., 1989] [Weng et al., 1993] [Broida and Chellappa, 1986] [Thomas and Oliensis, 1992] [Azarbayejani and Pentland, 1994] <ref> [Bouguet and Perona, 1995] </ref> [Morita and Kanade, 1994] report varying results using Kalman filters. Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed. <p> Morita [Morita and Kanade, 1994] presents a sequential method based on the batch method reported by Tomasi [Tomasi and Kanade, 1992] [Tomasi and Kanade, 1991a] [Tomasi and Kanade, 1991b] which achieves nearly the same results as Tomasi but retains the same limitations. Bouguet <ref> [Bouguet and Perona, 1995] </ref> achieves impressive results for recovering the structure of a hallway and relative camera position for an 8000 frame round-trip image sequence. 20 2.4.4 Locating and Tracking Features in Image Sequences To recover structure from motion in an image sequence, Tomasi and Shi [Tomasi and Kanade, 1991b] [Shi
Reference: [Broida and Chellappa, 1986] <author> Broida, T. J. and Chellappa, R. </author> <year> (1986). </year> <title> Estimation of object motion parameters from noisy images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-8(1):90-99. </journal>
Reference-contexts: Optimal (Kalman) filtering is typically used for this process. The advantage of an incremental approach is that a partial solution is available at intermediate steps. Several authors [Matthies et al., 1989] [Weng et al., 1993] <ref> [Broida and Chellappa, 1986] </ref> [Thomas and Oliensis, 1992] [Azarbayejani and Pentland, 1994] [Bouguet and Perona, 1995] [Morita and Kanade, 1994] report varying results using Kalman filters. Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed.
Reference: [Buttolo and Hannaford, 1995] <author> Buttolo, P. and Hannaford, B. </author> <year> (1995). </year> <title> Pen-based force display for precision manipulation in virtual environments. </title> <booktitle> In Proceedings of IEEE Virtual Reality Annual International Symposium (VRAIS), </booktitle> <pages> pages 217-224. </pages>
Reference-contexts: The ability to synthesize extremely complex and detailed images at interactive rates is now commonplace. Head-mounted displays and systems for tracking head and body positions are commercial products. Force feedback systems are an active area of research [Adachi et al., 1995] <ref> [Buttolo and Hannaford, 1995] </ref> and are already viable commercial products [PHANToM, 1997]. It seems that the ultimate display problem is well on its way to being solved... Or is it? Many of the original problems present in Sutherland's early system [Sutherland, 1968] remain.
Reference: [Caudell and Mizell, 1992] <author> Caudell, T. P. and Mizell, D. W. </author> <year> (1992). </year> <title> Augmented reality: An application of heads-up display technology to manual manufacturing processes. </title> <booktitle> In Proc. Hawaii Intl. Conference on System Sciences (HICCS) 1992., </booktitle> <volume> volume 2, </volume> <pages> pages 659-669. </pages>
Reference-contexts: Other AR Systems Augmented-reality systems based on the current model (figure 2.3) have been used to visualize ultrasound medical data within a patient 3 [Bajura et al., 1992], to provide 3D graphical annotations for machinery 4 [Feiner et al., 1993] [Feiner et al., 1992], and to assist manual manufacturing 5 <ref> [Caudell and Mizell, 1992] </ref>. These systems suffer from the weaknesses outlined in section 2.1.1. Mellor [Mellor, 1995a] [Mellor, 1995b] reports using known fiducial marks in an augmented-reality scene to register anatomical data relative to those marks. <p> Figure 3.1 illustrates an application of this powerful visualization tool where a user can visualize an as-yet unbuilt building in its proposed natural setting. Other applications include interactive 3D illustrations for constructing and for maintaining complex machinery [Feiner et al., 1992] [Feiner et al., 1993] <ref> [Caudell and Mizell, 1992] </ref> and in-patient visualization of medical data, e.g., ultrasound [Bajura et al., 1992]. In all these applications it is vitally necessary for computer-generated objects 24 and real-world objects to be visually registered with respect to each other in every image the user sees.
Reference: [Chen, 1995] <author> Chen, S. E. </author> <year> (1995). </year> <title> Quicktime vr an image-based approach to virtual environment navigation. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 29-38. </pages> <publisher> ACM. </publisher> <pages> 81 </pages>
Reference-contexts: directly generates reprojections of a scene from existing projections 2 Sutherland's system could transform and display up to 3000 line segments at 30 frames per second while the Pixel-Planes 5 system at UNC [Fuchs et al., 1989] requires 54 ms to render a single triangle [Mine and Bishop, 1993] 10 <ref> [Chen, 1995] </ref> [McMillan and Bishop, 1995] [Levoy and Hanrahan, 1996] [Gortler et al., 1996]. Most high-performance image-generation systems have been optimized to render distortion-free images with an ideal pinhole camera model. <p> A ground-plane constraint tracks the position of people moving in the environment and serves to position them relative to computer-generated life forms on a virtual plane. An emerging technique for rendering images from recorded images of 3-D environments called image-based rendering, pioneered by Chen <ref> [Chen, 1995] </ref>, McMillan [McMillan and Bishop, 1995], Levoy [Levoy and Hanrahan, 1996], and Gortler [Gortler et al., 1996] holds some promise for augmented-reality applications.
Reference: [Cyberware, 1997] <institution> Cyberware (1997). Cyberware Scanners. Cyberware Inc., </institution> <address> 2110 Del Monte Avenue, Monterey, CA 93940 (http://www.cyberware.com). </address>
Reference-contexts: Broadly speaking, methods for sensing depth remotely are either active or passive. Active strategies use some form of structured light to illuminate a scene which is scanned by a sensor. Examples of active methods are light stripe (triangulation) <ref> [Cyberware, 1997] </ref> [Gruss et al., 1992], moire (interference), and radar (time-of-flight). Besl [Besl, 1989] provides a good overview of active sensing strategies.
Reference: [Deriche, 1990] <author> Deriche, R. </author> <year> (1990). </year> <title> Fast algorithms for low-level vision. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(1) </volume> <pages> 78-87. </pages>
Reference-contexts: The cost of computing this function is the cost of computing its underlying scaled derivative functions plus the cost of computing the function itself. Differentiation is implemented as a two-pass recursive filter by Deriche <ref> [Deriche, 1990] </ref> which very nearly approximates a derivative of gaussian convolution kernel. The filter is applied twice to compute second order derivatives.
Reference: [Digital, 1997] <institution> Digital (1997). Alpha Microprocessors. Digital Equipment Corporation, </institution> <note> (http://www.digital.com/info/semiconductor). </note>
Reference-contexts: Real-time update of 200 points at 30 Hz (= 6000 updates/sec) is well within the computational power of a modern 200 MHz processor. 1 Digital Equipment Corporation's Alpha processor is available at 500 Mhz <ref> [Digital, 1997] </ref> 57 Surface generation by Delunnay triangulation is computed in O (n log n) time, where n is the number of points in the triangulation [O'Rourke, 1994] [Fortune, 1987].
Reference: [Durlach and Mavor, 1994] <author> Durlach, N. I. and Mavor, A. S., </author> <title> editors (1994). Virtual Reality: Scientific and Technological Challenges. </title> <publisher> National Academy Press, </publisher> <address> Washington, D.C. </address>
Reference: [Edwards et al., 1993] <author> Edwards, E., Rolland, J., and Keller, K. </author> <year> (1993). </year> <title> Video see-through design for merging of real and virtual environments. </title> <booktitle> In Proceedings of IEEE Virtual Reality Annual International Symposium (VRAIS), </booktitle> <pages> pages 223-233, </pages> <address> Seattle, WA. </address>
Reference-contexts: Important optical design issues include field-of-view, angular resolution, and distortion. Robinett [Robinett and Rolland, 1991] and Edwards <ref> [Edwards et al., 1993] </ref> demonstrate it is also important to match a user's convergence and inter-pupillary distance. In an optical see-through system, half-silvered mirrors are integrated with the optics to allow the user to directly view his natural surroundings while viewing the reflection of an image display. <p> A stereo 25 system would add a second video camera which would be treated as a second independent monocular system. Constructing a binocular HMD which presents correct stereopsis is problem addressed in <ref> [Edwards et al., 1993] </ref>. World Origin Origin-to-Head Head-to-Camera Origin-to-Object Camera-to-Image The apparent registration between real and virtual objects depends on how accurately the virtual camera models the real one. Figure 3.4 shows a detailed transformation model for the virtual camera. <p> It should be noted that this model does not address the problem of correcting for distortion in the HMD optics which is a separate problem from generating correctly registered images <ref> [Edwards et al., 1993] </ref>. Image registration error in combined real and virtual images is caused by the following types of errors: 1. The tracking system's origin is not aligned with the world coordinate system origin.
Reference: [Ens and Lawrence, 1993] <author> Ens, J. and Lawrence, P. </author> <year> (1993). </year> <title> An investigation of methods for determining depth from focus. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(2) </volume> <pages> 97-108. </pages>
Reference-contexts: Because of depth-of-field effects, features further from the focal plane of an imaging system are blurrier than features closer to the focal plane. By relating the amount of blur (defocus) of a particular feature to its depth, a range map can be computed <ref> [Ens and Lawrence, 1993] </ref>[Nayar et al., 1995]. This can be done in parallel for each point in an image. Nayar [Nayar et al., 1995] reports generating depth estimates at 512 fi 480 resolution at 30 Hz with an accuracy to 0.3%. <p> The chief drawback of the depth-from-focus/defocus approach is that the effective baseline of the imaging system is no larger than the aperture of the camera being used, effectively limiting the accuracy and working 17 volume of this method. In a demonstration system, Ens <ref> [Ens and Lawrence, 1993] </ref> reports 10% RMS accuracy over a range of 80 to 95 cm from the camera, or 1.7% RMS accuracy relative to the camera. Nayar reports 0.3% RMS accuracy over an approximately 30 cm working range. This is not enough working volume for a usable AR application.
Reference: [Faro, 1997] <institution> Faro (1997). FaroArm. Faro Technologies, Inc., Industrial Division, 125 Technology Park, Lake Mary, </institution> <address> FL 32746 (http://www.faro.com). </address>
Reference-contexts: A few technologies which have been employed for this are: mechanical linkages <ref> [Faro, 1997] </ref>, magnetic sensors [Polhemus, 1997] [Ascension, 1997], optical sensors [Ward et al., 1992], and acoustic methods [SAC, 1994]. Bhatnagar [Bhatnagar, 1993] provides a survey of these methods. Magnetic tracking systems are attractive because no line-of-sight or mechanical tether is needed. <p> The Real-World-View is captured by a Panasonic PG-KS102 color CCD video camera with a Cosmicar 12.4 mm f =1:8 C-Mount lens. The camera is mounted on a FARO <ref> [Faro, 1997] </ref> mechanical tracking arm which is the Tracking System. As the experimental system operates, points visible in the Real-World-View are tracked over time. Their 2-D image positions are combined with arm 3-D tracking information to locate their 3-D coordinates. <p> A.5 Experimental Results This section describes alibration results from applying this method to a Panasonic GP-KS102 color CCD video camera with a Cosmicar 12.5 mm f =1:8 C-mount lens (approx. 28 degree field-of-view) rigidly attached to a FARO <ref> [Faro, 1997] </ref> 3-D digitizing mechanical tracking arm with approximately 1 m working radius.
Reference: [Faugeras et al., 1992] <author> Faugeras, O., Loung, Q., and Maybank, S. </author> <year> (1992). </year> <title> Camera self-calibration: Theory and experiments. </title> <booktitle> In International Conference on Computer Vision (ICCV), </booktitle> <pages> pages 321-334. </pages> <publisher> IEEE. </publisher>
Reference-contexts: In addition to computing parameters for a camera model, algorithms based on 4-tuple data must also implicitly solve for the unknown relative 3-D (X; Y; Z) spatial positions of each feature. To date 4-tuple methods have not incorporated lens distortion models <ref> [Faugeras et al., 1992] </ref> [Liu et al., 1990] [Luong and Faugeras, 1993] [Maybank and Faugeras, 1992] [Shashua, 1994]. Several authors have studied methods of camera modeling based on the (X; Y; Z; U; V ) 5-tuple approach. <p> This is not yet dependent on any information about the camera. This could be a useful function to correct for errors in camera calibration approaches which assume projective distortion only, particu larly those based on 4-tuple data <ref> [Faugeras et al., 1992] </ref> [Liu et al., 1990] [Luong and Faugeras, 1993] [Maybank and Faugeras, 1992] [Shashua, 1994]. This function could also be used to map distorted images into undistorted ones for registration correction in augmented-reality systems.
Reference: [Feiner et al., 1992] <author> Feiner, S., Macintyre, B., and Seligmann, D. </author> <year> (1992). </year> <title> Annotating the real world with knowledge-based graphics on a see-through head-mounted display. </title> <booktitle> In Proc. Graphics Interface 1992, </booktitle> <pages> pages 78-85. </pages> <booktitle> Canadian Information Proc. </booktitle> <publisher> Soc. </publisher>
Reference-contexts: For example, these could be 3-D medical data sets of a patient which are anatomically registered [Bajura et al., 1992] [State et al., 1996b] or 3-D assembly directions for complex machinery [Feiner et al., 1993] <ref> [Feiner et al., 1992] </ref>. AR systems share the same basic technology of VE systems except that the user is able to "see through" his head-mounted display (HMD) into his natural environment with the use of either half-silvered mirrors (optical see-through), or video cameras (video see-through). <p> and with incorrect registration, as in optical see-through systems. 2.3 Other AR Systems Augmented-reality systems based on the current model (figure 2.3) have been used to visualize ultrasound medical data within a patient 3 [Bajura et al., 1992], to provide 3D graphical annotations for machinery 4 [Feiner et al., 1993] <ref> [Feiner et al., 1992] </ref>, and to assist manual manufacturing 5 [Caudell and Mizell, 1992]. These systems suffer from the weaknesses outlined in section 2.1.1. Mellor [Mellor, 1995a] [Mellor, 1995b] reports using known fiducial marks in an augmented-reality scene to register anatomical data relative to those marks. <p> Figure 3.1 illustrates an application of this powerful visualization tool where a user can visualize an as-yet unbuilt building in its proposed natural setting. Other applications include interactive 3D illustrations for constructing and for maintaining complex machinery <ref> [Feiner et al., 1992] </ref> [Feiner et al., 1993] [Caudell and Mizell, 1992] and in-patient visualization of medical data, e.g., ultrasound [Bajura et al., 1992].
Reference: [Feiner et al., 1993] <author> Feiner, S., Macintyre, B., and Seligmann, D. </author> <year> (1993). </year> <title> Knowlege-based augmented reality. </title> <journal> Communications of the ACM, </journal> <volume> 36(7) </volume> <pages> 53-61. </pages>
Reference-contexts: For example, these could be 3-D medical data sets of a patient which are anatomically registered [Bajura et al., 1992] [State et al., 1996b] or 3-D assembly directions for complex machinery <ref> [Feiner et al., 1993] </ref> [Feiner et al., 1992]. AR systems share the same basic technology of VE systems except that the user is able to "see through" his head-mounted display (HMD) into his natural environment with the use of either half-silvered mirrors (optical see-through), or video cameras (video see-through). <p> in the real-world scene and with incorrect registration, as in optical see-through systems. 2.3 Other AR Systems Augmented-reality systems based on the current model (figure 2.3) have been used to visualize ultrasound medical data within a patient 3 [Bajura et al., 1992], to provide 3D graphical annotations for machinery 4 <ref> [Feiner et al., 1993] </ref> [Feiner et al., 1992], and to assist manual manufacturing 5 [Caudell and Mizell, 1992]. These systems suffer from the weaknesses outlined in section 2.1.1. Mellor [Mellor, 1995a] [Mellor, 1995b] reports using known fiducial marks in an augmented-reality scene to register anatomical data relative to those marks. <p> Figure 3.1 illustrates an application of this powerful visualization tool where a user can visualize an as-yet unbuilt building in its proposed natural setting. Other applications include interactive 3D illustrations for constructing and for maintaining complex machinery [Feiner et al., 1992] <ref> [Feiner et al., 1993] </ref> [Caudell and Mizell, 1992] and in-patient visualization of medical data, e.g., ultrasound [Bajura et al., 1992]. In all these applications it is vitally necessary for computer-generated objects 24 and real-world objects to be visually registered with respect to each other in every image the user sees.
Reference: [Forstner, 1994] <author> Forstner, W. </author> <year> (1994). </year> <title> A framework for low level feature extraction. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 383-394. </pages>
Reference-contexts: The drawback with this approach is that it is only possible to compute the affine registration in windows with sufficient visual texture | probably the equivalent of locating enough individual features by Tomasi's method to define a local affine transformation from individual features. Forstner <ref> [Forstner, 1994] </ref> presents a scale-space approach to feature extraction based on local statistics of the image function. The criteria used are based on the same squared gradient information used by Tomasi [Tomasi and Kanade, 1991b] to select features.
Reference: [Fortune, 1987] <author> Fortune, S. </author> <year> (1987). </year> <title> A sweeping algorithm for vornoi diagrams. </title> <journal> Algorithmica, </journal> <volume> 2(2) </volume> <pages> 153-74. </pages>
Reference-contexts: well within the computational power of a modern 200 MHz processor. 1 Digital Equipment Corporation's Alpha processor is available at 500 Mhz [Digital, 1997] 57 Surface generation by Delunnay triangulation is computed in O (n log n) time, where n is the number of points in the triangulation [O'Rourke, 1994] <ref> [Fortune, 1987] </ref>. Interactions with the computed surface (approximately n = 200 triangles) can be simulated in real-time by ray-tracing trajectories to intersect the surface and computing cut-plane intersections.
Reference: [Fuchs et al., 1989] <author> Fuchs, H., Poulton, J., Eyles, J., Greer, T., Goldfeather, J., Ellsworth, D., Mol-nar, S., Turk, G., Tebbs, B., and Israel, L. </author> <year> (1989). </year> <title> Pixel-planes 5: A heterogeneous multiprocessor graphics system using processor-enhanced memories. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 79-88. 82 </pages>
Reference-contexts: This falls under the research area of image-based rendering which directly generates reprojections of a scene from existing projections 2 Sutherland's system could transform and display up to 3000 line segments at 30 frames per second while the Pixel-Planes 5 system at UNC <ref> [Fuchs et al., 1989] </ref> requires 54 ms to render a single triangle [Mine and Bishop, 1993] 10 [Chen, 1995] [McMillan and Bishop, 1995] [Levoy and Hanrahan, 1996] [Gortler et al., 1996]. Most high-performance image-generation systems have been optimized to render distortion-free images with an ideal pinhole camera model. <p> The head tracking system is an Ascension A Flock of Birds magnetic tracking system. The delay and unwarp, image feature tracker, and graphics system are different software modules which utilize separate portions of the Pixel-Planes 5 graphics multicom-puter at UNC <ref> [Fuchs et al., 1989] </ref>. Video is input to the Pixel-Planes 5 system via a real-time video digitizer and output via a standard double-buffered frame buffer.
Reference: [Ganapathy, 1984a] <author> Ganapathy, S. </author> <year> (1984a). </year> <title> Decomposition of transformation matrices for robot vision. </title> <journal> Pattern Recognition Letters, </journal> <volume> 2(6) </volume> <pages> 401-412. </pages>
Reference-contexts: Several authors have studied methods of camera modeling based on the (X; Y; Z; U; V ) 5-tuple approach. Sutherland [Sutherland, 1974] describes a linear-projection (non-distorting) camera model which can be computed as the solution to a linear system of equations. Ganapathy <ref> [Ganapathy, 1984a] </ref> shows how this transformation can be decomposed into camera parameters such as position, orientation, and field-of-view but does not account for general projective distortions. 64 Tsai [Tsai, 1987] and Weng [Weng et al., 1992] add non-linear distortion terms and non-linear mini- mization to essentially the same equations used by
Reference: [Ganapathy, 1984b] <author> Ganapathy, S. </author> <year> (1984b). </year> <title> Real-time motion tracking using a single camera. </title> <type> Technical Report 11358-841105-21-TM, </type> <institution> AT&T Bell Labs. </institution>
Reference-contexts: If the feature positions aren't degenerate, the camera position and orientation can be recovered by non-linear methods with a minimum of 4 points and by linear methods with a minimum of 6 points [Horaud et al., 1989] <ref> [Ganapathy, 1984b] </ref>. Trying to correct the camera position this way isn't practical for at least three reasons. First, there is no way to guarantee enough features will be visible in every image. Second, these solution methods are highly sensitive to noise and spatial feature distribution.
Reference: [GeneralReality, 1997] <author> GeneralReality (1997). </author> <title> CyberEye Head-Mounted Displays. General Reality Company, </title> <address> 124 Race St., San Jose, CA 95216, (http://www.genreality.com). </address>
Reference-contexts: A mounting fixture for the head: Helmets are frequently used as mounting fixtures. There must be space for image displays and associated optics, optional video cameras, and a tracking system if needed. Some HMDs are compact and light enough to be mounted on eyeglass-style frames <ref> [GeneralReality, 1997] </ref>. Unfortunately these frames often lack the rigidity needed for precise tracking and modeling. 2.1.6 Modeling the Camera The two requirements for modeling a real camera attached to a tracking system are: 1.
Reference: [Gortler et al., 1996] <author> Gortler, S. J., Grueszczuk, R., Szeliski, R., and Cohen, M. F. </author> <year> (1996). </year> <title> The lumigraph. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 43-54. </pages>
Reference-contexts: Sutherland's system could transform and display up to 3000 line segments at 30 frames per second while the Pixel-Planes 5 system at UNC [Fuchs et al., 1989] requires 54 ms to render a single triangle [Mine and Bishop, 1993] 10 [Chen, 1995] [McMillan and Bishop, 1995] [Levoy and Hanrahan, 1996] <ref> [Gortler et al., 1996] </ref>. Most high-performance image-generation systems have been optimized to render distortion-free images with an ideal pinhole camera model. This can be a problem for AR applications when trying to match rendered images with the distortion properties of a real lens system. <p> An emerging technique for rendering images from recorded images of 3-D environments called image-based rendering, pioneered by Chen [Chen, 1995], McMillan [McMillan and Bishop, 1995], Levoy [Levoy and Hanrahan, 1996], and Gortler <ref> [Gortler et al., 1996] </ref> holds some promise for augmented-reality applications.
Reference: [Gottschalk and Hughes, 1993] <author> Gottschalk, S. and Hughes, J. </author> <year> (1993). </year> <title> Autocalibration for virtual environments tracking hardware. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 65-72. </pages>
Reference-contexts: For applications which can tolerate minimal delays, potentially perfect registration can be achieved. This trade-off is not possible with optically based AR systems which allow the user to see his surroundings directly. Some success at improving registration error has been achieved with autocalibration approaches <ref> [Gottschalk and Hughes, 1993] </ref> and predictive tracking techniques [Azuma and Bishop, 1994] [List, 1984] which use a state estimate to help predict current measurements.
Reference: [Gruss et al., 1992] <author> Gruss, A., Tada, S., and Kanade, T. </author> <year> (1992). </year> <title> A vlsi smart sensor for fast range imaging. </title> <booktitle> In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems. </booktitle>
Reference-contexts: Broadly speaking, methods for sensing depth remotely are either active or passive. Active strategies use some form of structured light to illuminate a scene which is scanned by a sensor. Examples of active methods are light stripe (triangulation) [Cyberware, 1997] <ref> [Gruss et al., 1992] </ref>, moire (interference), and radar (time-of-flight). Besl [Besl, 1989] provides a good overview of active sensing strategies. These methods are unattractive to AR systems because they would involve adding light to the natural scene which may be objectionable and adding hardware which may not be needed.
Reference: [Hecht, 1987] <author> Hecht, E. </author> <year> (1987). </year> <note> Optics. Addison-Wesley, second edition edition. </note>
Reference-contexts: Positive (pincushion) and negative (barrel) distortion are typically caused by lenses with different magnification at different points across their face <ref> [Hecht, 1987] </ref>. Images are still in focus because an aperature limits the light on different parts of the image plane to specific areas of the lens.
Reference: [Hildebrand and Kothe, 1993] <author> Hildebrand, A. and Kothe, U. </author> <year> (1993). </year> <title> SMART: system for segmentation matching and reconstructi on. </title> <booktitle> SPIE: OE/Aerospace and Remote Sensing, State-of-the- Art Mapping, </booktitle> <year> 1943. </year>
Reference-contexts: These shape and motion solutions are not well suited to AR applications which typically require estimating distance at arm's length over a similar range, violating both the assumption of orthographic projection and the range of good approximation for the perspective approximation to orthographic projection. Hildebrand <ref> [Hildebrand and Kothe, 1993] </ref> describes a method which combines multiple two-frame solutions from a group of frames for an initial estimate of camera position and object structure which is improved by iteration.
Reference: [Holloway, 1995] <author> Holloway, R. </author> <year> (1995). </year> <title> Registration Errors in Augmented Reality Sytems. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill. </institution>
Reference-contexts: When images from the real camera and virtual camera are combined, the camera registration error becomes apparent. This is the chief problem with designing and building AR systems with the current AR system model. As demonstrated by Holloway <ref> [Holloway, 1995] </ref>, any errors in tracking, camera, or system calibration are amplified through an AR system creating image registration errors between the synthetic world model and actual world view. <p> A wider angle lens would also increase the limit. Holloway <ref> [Holloway, 1995, pages 131-132] </ref> reports mean rotation of 20 degrees/second with peaks as high as 50 degrees/second in the heads of test subjects wearing HMDs. <p> A similar dynamic constraint exists for camera (head) translation although at arm's length rotation is the dominant source of image shift <ref> [Holloway, 1995] </ref>. As features are tracked over time, knowledge of the camera's position can be used to match feature peaks. This is done by searching for feature matches in the metric image based on where features ought to appear given their previously estimated 3-D positions.
Reference: [Horaud et al., 1989] <author> Horaud, R., Conio, B., and Leboulleux, O. </author> <year> (1989). </year> <title> An analytic solution for the perspective 4-point problem. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 47 </volume> <pages> 33-44. </pages>
Reference-contexts: If the feature positions aren't degenerate, the camera position and orientation can be recovered by non-linear methods with a minimum of 4 points and by linear methods with a minimum of 6 points <ref> [Horaud et al., 1989] </ref> [Ganapathy, 1984b]. Trying to correct the camera position this way isn't practical for at least three reasons. First, there is no way to guarantee enough features will be visible in every image. Second, these solution methods are highly sensitive to noise and spatial feature distribution.
Reference: [Janin et al., 1993] <author> Janin, A. L., Mizell, D. W., and Caudell, T. P. </author> <year> (1993). </year> <title> Calibration of head-mounted displays for augmented reality applications. </title> <booktitle> In Proceedings of IEEE Virtual Reality Annual International Symposium (VRAIS). </booktitle>
Reference-contexts: Using a more accurate measuring device to measure the Head-to-Camera transformation would not eliminate errors in the AR system because camera position would still be a function of the tracking system which reports the Origin-to-Head transformation <ref> [Janin et al., 1993] </ref>. 3.5.3 Correcting Registration Error The image registration model of matching a point on each object makes it difficult to determine which particular errors are causing misregistration.
Reference: [Kaiser, 1997] <author> Kaiser (1997). </author> <title> Full Immersion Head Mounted Display System. Kaiser Electro-Optics, </title> <publisher> Inc., Mr. Douglas DeFoe, </publisher> <address> (619)438-9255 x205, keo@cts.net (http://esto.sysplan.com/ETO/Displays/HMD/). </address>
Reference-contexts: Field-sequential displays achieve significantly greater resolution over LCDs by time-multiplexing color signals instead of simultaneously displaying separate color elements. Recent effort has been made to produce tessellated displays which combine several smaller displays into a larger one creating a larger field-of-view with increased resolution <ref> [Kaiser, 1997] </ref>. 2. The optical systems for viewing the displays: Optical systems are usually independent for each image display for each eye.
Reference: [Kanade, 1996a] <author> Kanade, T. </author> <year> (1996a). </year> <title> Comments from seminar. </title> <address> Chapel Hill. </address>
Reference-contexts: This helps avoid introducing erroneous folds in the reconstructed surface. This is arguably not the best or most accurate surface reconstruction possible. A simple modification is to avoid tessellating large triangles which can prevent construction of some false geometry, particularly around the borders of the reconstructed surface <ref> [Kanade, 1996a] </ref>. Other more involved methods for interpolating surface data include using multiresolution wavelet transforms [Yaou and Chang, 1994], grid-based hierarchies [Terzopoulos, 1988], surface patches [Liao and Medioni, 1996], and constrained Delunnay triangulation. A simple 2-D Delunnay triangulation approximation is reasonable in certain environments and is sufficient for certain applications.
Reference: [Kanade, 1996b] <author> Kanade, T. </author> <year> (1996b). </year> <title> A stereo machine for video-rate dense depth mapping and its new applications. </title> <booktitle> In Proceedings: 1996 ARPA Image Understanding Workshop, </booktitle> <pages> pages 805-811. </pages> <booktitle> ARPA. </booktitle> <pages> 83 </pages>
Reference-contexts: Ravela [Ravela et al., 1996] tracks known features on a known object through different poses of that object, effectively repositioning the camera at every step. These approaches are similar to the new AR model in figure 2.5 but without a tracking system. Kanade <ref> [Kanade, 1996b] </ref> and Wloka [Wloka and Anderson, 1995] report using real-time and near real-time depth-from-stereo, respectively, to generate depth maps relative to a viewing position. These depth maps are then used to combine synthetic images with real ones from video cameras. Kanade [Kanade, 1996b] calls this operation z-keying, after the z-buffering <p> Kanade <ref> [Kanade, 1996b] </ref> and Wloka [Wloka and Anderson, 1995] report using real-time and near real-time depth-from-stereo, respectively, to generate depth maps relative to a viewing position. These depth maps are then used to combine synthetic images with real ones from video cameras. Kanade [Kanade, 1996b] calls this operation z-keying, after the z-buffering computer graphics rendering algorithm and the chroma-keying method used in video production. <p> Composite images are then formed from the corresponding real and synthetic images pixel by pixel. Pixels are copied from the synthetic image if a synthetic object is visible and not obscured by the surface mesh. Otherwise pixels are copied directly from the real image. Kanade <ref> [Kanade, 1996b] </ref> calls this operation z-keying, after the chroma-keying technique currently used in video production. There are two alternatives for rendering synthetic images depending on how the camera model is applied: 1. Ray-trace the synthetic images with the complete camera model.
Reference: [Kanade et al., 1992] <author> Kanade, T., Okutomi, M., and Nakahara, T. </author> <year> (1992). </year> <title> A multiple-baseline stereo method. </title> <booktitle> In Proceedings of the Darpa Image Understand Workshop, </booktitle> <pages> pages 409-426. </pages>
Reference-contexts: This can speed up the search for correspondences. Stereo can be thought of as a special case of this method with only two images. Two examples of finding depth from several images are described by Kanade <ref> [Kanade et al., 1992] </ref> and Bolles [Bolles et al., 1987]. By constraining camera motion to be linear, perpendicular to viewing direction, and parallel to image scan lines, image correspondences are more easily located in successive images on identical image scan lines.
Reference: [Lee and Cooper, 1993] <author> Lee, C.-Y. and Cooper, D. B. </author> <year> (1993). </year> <title> Structure from motion: A region based approach using affine transformations and moment invariants. </title> <booktitle> In IEEE Intl. Conference on Robotics and Automation, </booktitle> <volume> volume 3, </volume> <pages> pages 120-127. </pages> <publisher> IEEE. </publisher>
Reference-contexts: Shi [Shi and Tomasi, 1994] addresses this idea by estimating an affine motion field for each feature. When a feature's correlation with its original view (after correction for motion and distortion) falls below a given threshold, Shi and Tomasi consider correspondence to be lost. Lee <ref> [Lee and Cooper, 1993] </ref> describes a structure from motion algorithm in which larger windows in an image sequence are registered by affine transformations between a window's original viewing and the current image. The transformations of the individual windows instead of points are then used to recover structure.
Reference: [Levoy and Hanrahan, 1996] <author> Levoy, M. and Hanrahan, P. </author> <year> (1996). </year> <title> Light field rendering. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics. ACM. </booktitle>
Reference-contexts: from existing projections 2 Sutherland's system could transform and display up to 3000 line segments at 30 frames per second while the Pixel-Planes 5 system at UNC [Fuchs et al., 1989] requires 54 ms to render a single triangle [Mine and Bishop, 1993] 10 [Chen, 1995] [McMillan and Bishop, 1995] <ref> [Levoy and Hanrahan, 1996] </ref> [Gortler et al., 1996]. Most high-performance image-generation systems have been optimized to render distortion-free images with an ideal pinhole camera model. This can be a problem for AR applications when trying to match rendered images with the distortion properties of a real lens system. <p> An emerging technique for rendering images from recorded images of 3-D environments called image-based rendering, pioneered by Chen [Chen, 1995], McMillan [McMillan and Bishop, 1995], Levoy <ref> [Levoy and Hanrahan, 1996] </ref>, and Gortler [Gortler et al., 1996] holds some promise for augmented-reality applications.
Reference: [Lewis, 1986] <author> Lewis, F. L. </author> <year> (1986). </year> <title> Optimal Estimation (with an introduction to stochastic control theory). </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: Recognizing them in real-time by image-space parallelism is not as straightforward as detecting local point-like features. It would be possible to use lines in the reconstruction process and to help segment the generated surfaces but that subject is not pursued here. 4.3 3-D Reconstruction Method Incremental optimal (Kalman) filtering <ref> [Lewis, 1986] </ref> is used to reconstruct 3-D feature positions from their 2-D positions in the image sequence. This is fairly straightforward because the tracking arm provides a good estimate for the camera position for each image. <p> The common factor K 3fi3 in equations 4.20 and 4.21 is referred to as the Kalman gain <ref> [Lewis, 1986, page 70] </ref>. x old and x new are the old and updated feature position estimates, respectively. z is the estimated mean of the measurement seen by the camera.
Reference: [Liang et al., 1991] <author> Liang, J., Shaw, C., and Green, M. </author> <year> (1991). </year> <title> On temporal-spatial realism in the virtual reality environment. </title> <booktitle> In Proceedings 4th Annual ACM Symposium on User Interface Software and Technology (UIST), </booktitle> <pages> pages 19-25, </pages> <address> Hilton Head, SC. </address> <publisher> ACM SIGGRAPH/SIGCHI. </publisher>
Reference-contexts: Third, tracking system data has more error in rotation than in translation. This is because HMD wearers typically rotate their heads faster than they translate them and the head tracking system used incurs significant delays in reporting measurements (temporal error) <ref> [Liang et al., 1991] </ref>. In the experimental system, camera orientation error is adjusted by considering only one "reference" feature position and rotating the virtual camera to align that position.
Reference: [Liao and Medioni, 1996] <author> Liao, C.-W. and Medioni, G. </author> <year> (1996). </year> <title> Surface approximation and segmentation of objects with unknown topology. </title> <booktitle> In Image Understanding Workshop, </booktitle> <pages> pages 1033-1040. ARPA. </pages>
Reference-contexts: Other more involved methods for interpolating surface data include using multiresolution wavelet transforms [Yaou and Chang, 1994], grid-based hierarchies [Terzopoulos, 1988], surface patches <ref> [Liao and Medioni, 1996] </ref>, and constrained Delunnay triangulation. A simple 2-D Delunnay triangulation approximation is reasonable in certain environments and is sufficient for certain applications.
Reference: [List, 1984] <author> List (1984). </author> <title> Nonlinear prediction of head movements for helmet-mounted displays. </title> <type> Technical Report AFHRL-TP-83-45, </type> <institution> Williams AFB, AZ. </institution>
Reference-contexts: This trade-off is not possible with optically based AR systems which allow the user to see his surroundings directly. Some success at improving registration error has been achieved with autocalibration approaches [Gottschalk and Hughes, 1993] and predictive tracking techniques [Azuma and Bishop, 1994] <ref> [List, 1984] </ref> which use a state estimate to help predict current measurements. However these ap 28 proaches still suffer from the "open loop" requirement for perfect tracking and calibration. 3.5 The Experimental System This section describes an experimental AR system which corrects image registration error on a frame-by-frame basis.
Reference: [Liu et al., 1990] <author> Liu, Y., Huang, T. S., and Faugeras, O. D. </author> <year> (1990). </year> <title> Determination of camera location from 2-d to 3-d line and point correspondences. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(1) </volume> <pages> 28-37. </pages>
Reference-contexts: In addition to computing parameters for a camera model, algorithms based on 4-tuple data must also implicitly solve for the unknown relative 3-D (X; Y; Z) spatial positions of each feature. To date 4-tuple methods have not incorporated lens distortion models [Faugeras et al., 1992] <ref> [Liu et al., 1990] </ref> [Luong and Faugeras, 1993] [Maybank and Faugeras, 1992] [Shashua, 1994]. Several authors have studied methods of camera modeling based on the (X; Y; Z; U; V ) 5-tuple approach. <p> This is not yet dependent on any information about the camera. This could be a useful function to correct for errors in camera calibration approaches which assume projective distortion only, particu larly those based on 4-tuple data [Faugeras et al., 1992] <ref> [Liu et al., 1990] </ref> [Luong and Faugeras, 1993] [Maybank and Faugeras, 1992] [Shashua, 1994]. This function could also be used to map distorted images into undistorted ones for registration correction in augmented-reality systems.
Reference: [Livingston and State, ] <author> Livingston, M. A. and State, A. </author> <title> Magnetic tracker calibration for improved augmented reality registration. To appear in Presence: Teleoperators and Virtual Environments. </title>
Reference-contexts: Although the theoretical tracking accuracy of these systems is quite good, in practice their accuracy 1 Usually the user's Head-Mounted Display is tracked, not the actual pose of the user's head 9 has been disappointingly low in magnetically distorted lab environments <ref> [Livingston and State, ] </ref>. Mechanical arms and optical sensors have achieved better accuracy than magnetic systems at the expense of mechanical or line-of-sight restrictions. The important operating characteristics of a tracking system are accuracy, resolution, update rate, latency (or lag), and working volume.
Reference: [Longuet-Higgins, 1981] <author> Longuet-Higgins, H. C. </author> <year> (1981). </year> <title> A computer algorithm for reconstructing a scene from two projections. </title> <journal> Nature, </journal> <volume> 293 </volume> <pages> 133-135. </pages>
Reference-contexts: Longuet-Higgins <ref> [Longuet-Higgins, 1981] </ref> [Longuet-Higgins, 1984] and Tsai [Tsai and Huang, 1984] describe non-linear methods for recovering the relative orientation between two frames from this "essential matrix." Once relative orientation is known, the structure of the features can be computed. The chief drawback to this method is noise sensitivity to input data.
Reference: [Longuet-Higgins, 1984] <author> Longuet-Higgins, H. C. </author> <year> (1984). </year> <title> The reconstruction of a scene from two projections configurations that defeat the 8-point algorithm. </title> <booktitle> In Proceedings of 1st Conference on Artificial Intelligence Applications, </booktitle> <pages> pages 395-397. </pages>
Reference-contexts: Longuet-Higgins [Longuet-Higgins, 1981] <ref> [Longuet-Higgins, 1984] </ref> and Tsai [Tsai and Huang, 1984] describe non-linear methods for recovering the relative orientation between two frames from this "essential matrix." Once relative orientation is known, the structure of the features can be computed. The chief drawback to this method is noise sensitivity to input data.
Reference: [Lucas and Kanade, 1981] <author> Lucas, B. D. and Kanade, T. </author> <year> (1981). </year> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Seventh International Joint Conference on Artificial Intelligence IJCAI-81, </booktitle> <volume> volume 2, </volume> <pages> pages 674-679. </pages> <booktitle> The International Joint Conferences on Artificial Intelligence. </booktitle> <pages> 84 </pages>
Reference-contexts: Tomasi [Tomasi and Kanade, 1991b] demonstrates that trackable features can be recognized by local statistics based on squared image gradient values by implementing a tracking algorithm based on Lucas' <ref> [Lucas and Kanade, 1981] </ref> method of registering windows by shifting them relative to one another to maximize a correlation metric. Lucas suggests that parameters for rotation, scale, and shearing could also be used to register feature windows. <p> An alternative approach which uses a stereo image sequence is proposed in section 4.3. Shi and Tomasi [Shi and Tomasi, 1994] [Tomasi and Kanade, 1991b] describe a method based on Lucas' earlier work <ref> [Lucas and Kanade, 1981] </ref> for identifying feature windows based on local image gradient information and then tracking the movement of those windows over time. As feature windows deform in successive images, parameters for window movement and deformation are estimated.
Reference: [Luong and Faugeras, 1993] <author> Luong, Q.-T. and Faugeras, O. </author> <year> (1993). </year> <title> Self-calibration of a stereo rig from unknown camera motions and point correspondences. </title> <type> Technical Report 2014, </type> <institution> Institut National de Recherche en Informatique et en Automatique (INRIA). </institution>
Reference-contexts: In addition to computing parameters for a camera model, algorithms based on 4-tuple data must also implicitly solve for the unknown relative 3-D (X; Y; Z) spatial positions of each feature. To date 4-tuple methods have not incorporated lens distortion models [Faugeras et al., 1992] [Liu et al., 1990] <ref> [Luong and Faugeras, 1993] </ref> [Maybank and Faugeras, 1992] [Shashua, 1994]. Several authors have studied methods of camera modeling based on the (X; Y; Z; U; V ) 5-tuple approach. <p> This is not yet dependent on any information about the camera. This could be a useful function to correct for errors in camera calibration approaches which assume projective distortion only, particu larly those based on 4-tuple data [Faugeras et al., 1992] [Liu et al., 1990] <ref> [Luong and Faugeras, 1993] </ref> [Maybank and Faugeras, 1992] [Shashua, 1994]. This function could also be used to map distorted images into undistorted ones for registration correction in augmented-reality systems.
Reference: [Matthies et al., 1989] <author> Matthies, L., Kanade, T., and Szeliski, R. </author> <year> (1989). </year> <title> Kalman filter-based algorithms for estimating depth from image sequences. </title> <journal> International Journal of Computer Vision, </journal> <volume> 3 </volume> <pages> 209-236. </pages>
Reference-contexts: Like the batch method, the initial guess and parameterization selection are important for converging on the correct solution. Optimal (Kalman) filtering is typically used for this process. The advantage of an incremental approach is that a partial solution is available at intermediate steps. Several authors <ref> [Matthies et al., 1989] </ref> [Weng et al., 1993] [Broida and Chellappa, 1986] [Thomas and Oliensis, 1992] [Azarbayejani and Pentland, 1994] [Bouguet and Perona, 1995] [Morita and Kanade, 1994] report varying results using Kalman filters.
Reference: [Maybank and Faugeras, 1992] <author> Maybank, S. J. and Faugeras, O. D. </author> <year> (1992). </year> <title> A theory of self-calibration of a moving camera. </title> <journal> International Journal of Computer Vision, </journal> <volume> 8(2) </volume> <pages> 123-151. </pages>
Reference-contexts: To date 4-tuple methods have not incorporated lens distortion models [Faugeras et al., 1992] [Liu et al., 1990] [Luong and Faugeras, 1993] <ref> [Maybank and Faugeras, 1992] </ref> [Shashua, 1994]. Several authors have studied methods of camera modeling based on the (X; Y; Z; U; V ) 5-tuple approach. Sutherland [Sutherland, 1974] describes a linear-projection (non-distorting) camera model which can be computed as the solution to a linear system of equations. <p> This is not yet dependent on any information about the camera. This could be a useful function to correct for errors in camera calibration approaches which assume projective distortion only, particu larly those based on 4-tuple data [Faugeras et al., 1992] [Liu et al., 1990] [Luong and Faugeras, 1993] <ref> [Maybank and Faugeras, 1992] </ref> [Shashua, 1994]. This function could also be used to map distorted images into undistorted ones for registration correction in augmented-reality systems.
Reference: [McMillan and Bishop, 1995] <author> McMillan, L. and Bishop, G. </author> <year> (1995). </year> <title> Plenoptic modeling: an image-based rendering system. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 39-46. </pages> <publisher> ACM. </publisher>
Reference-contexts: reprojections of a scene from existing projections 2 Sutherland's system could transform and display up to 3000 line segments at 30 frames per second while the Pixel-Planes 5 system at UNC [Fuchs et al., 1989] requires 54 ms to render a single triangle [Mine and Bishop, 1993] 10 [Chen, 1995] <ref> [McMillan and Bishop, 1995] </ref> [Levoy and Hanrahan, 1996] [Gortler et al., 1996]. Most high-performance image-generation systems have been optimized to render distortion-free images with an ideal pinhole camera model. <p> A ground-plane constraint tracks the position of people moving in the environment and serves to position them relative to computer-generated life forms on a virtual plane. An emerging technique for rendering images from recorded images of 3-D environments called image-based rendering, pioneered by Chen [Chen, 1995], McMillan <ref> [McMillan and Bishop, 1995] </ref>, Levoy [Levoy and Hanrahan, 1996], and Gortler [Gortler et al., 1996] holds some promise for augmented-reality applications.
Reference: [MediaLab, 1996] <author> MediaLab (1996). </author> <title> ALIVE: Artificial Life Interactice Video Environment. </title> <publisher> MIT Media Lab, </publisher> <address> (http://lcs.www.media.mit.edu/projects/alive/ 8/28/96). </address>
Reference-contexts: The accuracy of both of these approaches is limited because correspondence is computed only to single pixel accuracy. This imposes a measurement discretization equal to the number of pixels disparity throughout the working volume. The MIT Alive project <ref> [MediaLab, 1996] </ref> generates images of computer-generated artificial life interacting with people in a natural setting. A ground-plane constraint tracks the position of people moving in the environment and serves to position them relative to computer-generated life forms on a virtual plane.
Reference: [Mellor, 1995a] <author> Mellor, J. </author> <year> (1995a). </year> <title> Enhanced reality visualization in a surgical environment. </title> <type> Master's thesis, </type> <institution> Massachusetts Istitute of Technology. </institution>
Reference-contexts: These systems suffer from the weaknesses outlined in section 2.1.1. Mellor <ref> [Mellor, 1995a] </ref> [Mellor, 1995b] reports using known fiducial marks in an augmented-reality scene to register anatomical data relative to those marks. The effective position of the camera is computed dynamically to be consistent with the observed positions of fiducial marks.
Reference: [Mellor, 1995b] <author> Mellor, J. </author> <year> (1995b). </year> <title> Realtime camera calibration for enhanced reality visualization. </title> <editor> In Ayache, N., editor, </editor> <booktitle> Proceedings of the first international converence on Computer vision, Virtual reality, and Robotics in Medicine (CVRMed), </booktitle> <address> Nice France. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: These systems suffer from the weaknesses outlined in section 2.1.1. Mellor [Mellor, 1995a] <ref> [Mellor, 1995b] </ref> reports using known fiducial marks in an augmented-reality scene to register anatomical data relative to those marks. The effective position of the camera is computed dynamically to be consistent with the observed positions of fiducial marks. <p> This case describes systems by Mellor <ref> [Mellor, 1995b] </ref>, Uenohara [Uenohara and Kanade, 1995], Ravela [Ravela et al., 1996], and State [State et al., 1996a] which identify known fiducial marks to reposition a virtual camera. 3. Both feature positions and camera positions are unknown: This defines the classic non-linear structure-from-motion problem. <p> T low is typically set to a value high enough to disambiguate a fiducial point from its local background. T high is typically set to the maximum pixel value although some authors claim that lowering this ceiling can improve noise immunity <ref> [Mellor, 1995b] </ref>. This accuracy of this centroid calculation depends on: * The distorting properties of the imaging system. Given an arbitrary image distortion, the centroid of a distorted fiducial is not the same as the distorted position of the fiducial's actual center.
Reference: [Mine and Bishop, 1993] <author> Mine, M. and Bishop, G. </author> <year> (1993). </year> <title> Just-in-time pixels. </title> <type> Technical Report TR93-005, </type> <institution> UNC Chapel Hill. </institution>
Reference-contexts: Unfortunately the emphasis in these systems has been on total system throughput, often at the expense of image-generation latency and flexibility of camera models. Because of the tradeoff of latency for image complexity and because of the delay incurred by rendering images as discrete frames <ref> [Mine and Bishop, 1993] </ref>, the delay in image generation for synthetic-reality systems has remained largely unchanged or has even grown worse since Suther-land's pioneering work [Sutherland, 1968] 2 . <p> area of image-based rendering which directly generates reprojections of a scene from existing projections 2 Sutherland's system could transform and display up to 3000 line segments at 30 frames per second while the Pixel-Planes 5 system at UNC [Fuchs et al., 1989] requires 54 ms to render a single triangle <ref> [Mine and Bishop, 1993] </ref> 10 [Chen, 1995] [McMillan and Bishop, 1995] [Levoy and Hanrahan, 1996] [Gortler et al., 1996]. Most high-performance image-generation systems have been optimized to render distortion-free images with an ideal pinhole camera model.
Reference: [Molnar et al., 1992] <author> Molnar, S., Eyles, J., and Poulton, J. </author> <year> (1992). </year> <title> Pixelflow: High-speed rendering using image composition. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 231-240. </pages> <booktitle> ACM. Published as Computer Graphics, Proceedings of Siggraph, </booktitle> <volume> volume 26, number 2. </volume>
Reference-contexts: Recently progress has been made toward rendering more complex images at increasing rates <ref> [Molnar et al., 1992] </ref> [Akeley, 1993]. Unfortunately the emphasis in these systems has been on total system throughput, often at the expense of image-generation latency and flexibility of camera models.
Reference: [Morita and Kanade, 1994] <author> Morita, T. and Kanade, T. </author> <year> (1994). </year> <title> A sequential factorization method for recovering shape and motion from image streams. </title> <type> Technical Report CMU-CS-94-167, CMU. </type>
Reference-contexts: The advantage of an incremental approach is that a partial solution is available at intermediate steps. Several authors [Matthies et al., 1989] [Weng et al., 1993] [Broida and Chellappa, 1986] [Thomas and Oliensis, 1992] [Azarbayejani and Pentland, 1994] [Bouguet and Perona, 1995] <ref> [Morita and Kanade, 1994] </ref> report varying results using Kalman filters. Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed. Morita [Morita and Kanade, 1994] presents a sequential method based on the batch method reported by Tomasi [Tomasi and Kanade, 1992] [Tomasi and <p> 1989] [Weng et al., 1993] [Broida and Chellappa, 1986] [Thomas and Oliensis, 1992] [Azarbayejani and Pentland, 1994] [Bouguet and Perona, 1995] <ref> [Morita and Kanade, 1994] </ref> report varying results using Kalman filters. Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed. Morita [Morita and Kanade, 1994] presents a sequential method based on the batch method reported by Tomasi [Tomasi and Kanade, 1992] [Tomasi and Kanade, 1991a] [Tomasi and Kanade, 1991b] which achieves nearly the same results as Tomasi but retains the same limitations.
Reference: [Mundy and Zisserman, 1992] <author> Mundy, J. L. and Zisserman, A. </author> <year> (1992). </year> <title> Geometric invariance in computer vision, </title> <booktitle> chapter Appendix, </booktitle> <pages> pages 463-519. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: ! &lt; 2 linear projection with an &lt; 3 ! &lt; 2 linear projection is simply another &lt; 3 ! &lt; 2 linear projection: To see this, recall how to express a linear projection from 3-D homogeneous coordinates into 2-D homogeneous coordinates with a 3 fi 4 linear projection matrix <ref> [Mundy and Zisserman, 1992] </ref>: LP 3fi4 6 6 6 4 Y W 7 7 7 5 2 6 4 lp 2;1 lp 2;2 lp 2;3 lp 2;4 3 7 5 6 6 6 4 Y W 7 7 7 5 2 6 4 V 3 7 5 Recall also that a
Reference: [Nayar et al., 1995] <author> Nayar, S. K., Watanabe, M., and Noguchi, M. </author> <year> (1995). </year> <title> Real-time focus range sensor. </title> <booktitle> In International Conference on Computer Vision (ICCV), </booktitle> <pages> pages 995-1001. </pages> <publisher> IEEE. </publisher>
Reference-contexts: By relating the amount of blur (defocus) of a particular feature to its depth, a range map can be computed [Ens and Lawrence, 1993]<ref> [Nayar et al., 1995] </ref>. This can be done in parallel for each point in an image. Nayar [Nayar et al., 1995] reports generating depth estimates at 512 fi 480 resolution at 30 Hz with an accuracy to 0.3%. Another "pixel-by-pixel" approach is to adjust the imaging system sequentially for each point in an image until it is in focus.
Reference: [O'Neill, 1966] <author> O'Neill, B. </author> <year> (1966). </year> <title> Elementary Differential Geometry. </title> <publisher> Academic Press Inc., </publisher> <address> Orlando, Florida 32887. </address> <month> 85 </month>
Reference-contexts: The metric function is computed from a curvature matrix formed by second-order derivatives of the image function. The eigenvalues of this matrix are the principal curvatures at each image point <ref> [O'Neill, 1966] </ref>. 2 @ 2 I @ 2 I @ 2 I @ 2 I 3 5 = Curvature Matrix: (4.2) The eigenvalues (or principal curvatures) e 1 and e 2 of the curvature matrix are computed by: radical = v u @ 2 u @ 2 I 2 @ 2
Reference: [O'Rourke, 1994] <author> O'Rourke, J. </author> <year> (1994). </year> <title> Computational Geometry in C. </title> <publisher> Cambridge ; New York : Cambridge University Press. </publisher>
Reference-contexts: updates/sec) is well within the computational power of a modern 200 MHz processor. 1 Digital Equipment Corporation's Alpha processor is available at 500 Mhz [Digital, 1997] 57 Surface generation by Delunnay triangulation is computed in O (n log n) time, where n is the number of points in the triangulation <ref> [O'Rourke, 1994] </ref> [Fortune, 1987]. Interactions with the computed surface (approximately n = 200 triangles) can be simulated in real-time by ray-tracing trajectories to intersect the surface and computing cut-plane intersections.
Reference: [Pausch et al., 1992] <author> Pausch, R., Crea, T., and Conway, M. </author> <year> (1992). </year> <title> A literature survey for virtual environments: Military flight simulator visual systems and simulator sickness. Presence, </title> <type> 1(3). </type>
Reference-contexts: The inconsistent sensory cues created by this time delay can cause disorientation or even nausea if they are excessive and can limit the effectiveness of an application <ref> [Pausch et al., 1992] </ref>. Pausch [Pausch et al., 1992] cites unpublished work by Lilienthal claiming that update delays should be limited to less than 35 msec because "the motion cues may give the impression of motion in one direction while the delayed visual cues give the impression of motion in another <p> The inconsistent sensory cues created by this time delay can cause disorientation or even nausea if they are excessive and can limit the effectiveness of an application <ref> [Pausch et al., 1992] </ref>. Pausch [Pausch et al., 1992] cites unpublished work by Lilienthal claiming that update delays should be limited to less than 35 msec because "the motion cues may give the impression of motion in one direction while the delayed visual cues give the impression of motion in another direction." * In AR systems:
Reference: [Peuchot, 1993] <author> Peuchot, B. </author> <year> (1993). </year> <title> Camera virtual equivalent model 0.01 pixel detectors. </title> <journal> Computerized Medical Imaging and Graphics, </journal> 17(4/5):289-294. 
Reference-contexts: Error Set 0 Max. Error (pixels) x 10 -3 Width (pixels) 0.00 10.00 20.00 30.00 40.00 50.00 60.00 70.00 80.00 90.00 Figure A.2: Maximum error for 1D Center of Mass Computation 78 This result is consistent with Peuchot's <ref> [Peuchot, 1993] </ref> result of locating points to within 1% of a pixel in static 2-D plane measurements and Stone's [Stone, 1989] result that fitting a gaussian function to an (expected) gaussian symmetric measurement gives similar accuracy to computing a centroid, provided the signal levels are high enough. 79
Reference: [PHANToM, 1997] <institution> PHANToM (1997). PHANToM Haptic Interface. SensAble Technologies, Inc., 26 Landsdowne Street, University Park at MIT, </institution> <address> Cambridge, MA 02139 Phone: (617) 621-0150 FAX: (617) 621-0135, email: sensable@sensable.com (http://www.sensable.com). </address>
Reference-contexts: Head-mounted displays and systems for tracking head and body positions are commercial products. Force feedback systems are an active area of research [Adachi et al., 1995] [Buttolo and Hannaford, 1995] and are already viable commercial products <ref> [PHANToM, 1997] </ref>. It seems that the ultimate display problem is well on its way to being solved... Or is it? Many of the original problems present in Sutherland's early system [Sutherland, 1968] remain. The delay between a user's head motion and corresponding image display remains largely unchanged.
Reference: [Poelman and Kanade, 1992] <author> Poelman, C. J. and Kanade, T. </author> <year> (1992). </year> <title> A paraperspective factorization method for shape and motion recovery. </title> <type> Technical Report CMU-CS-92-208, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Shape and motion are decomposed into a product of shape and rotation matrices which is found by singular value decomposition analysis. This method achieves impressive results and is extended to a perspective approximation which preserves this factorization property <ref> [Poelman and Kanade, 1992] </ref>. This approximation is good when the range of depth estimation remains small compared to the distance between the camera and range of estimation even if the range of estimation undergoes significant changes in depth over an image sequence.
Reference: [Polhemus, 1997] <institution> Polhemus (1997). FASTRACK(R) and ISOTRACK(R) magnetic tracking systems. </institution> <address> Polhemus, P.O. Box 560, Colchester, VT 05446, (http://www.polhemus.com). </address>
Reference-contexts: A few technologies which have been employed for this are: mechanical linkages [Faro, 1997], magnetic sensors <ref> [Polhemus, 1997] </ref> [Ascension, 1997], optical sensors [Ward et al., 1992], and acoustic methods [SAC, 1994]. Bhatnagar [Bhatnagar, 1993] provides a survey of these methods. Magnetic tracking systems are attractive because no line-of-sight or mechanical tether is needed.
Reference: [Qiu and Ma, 1995] <author> Qiu, M. and Ma, S. </author> <year> (1995). </year> <title> The nonparametric approach for camera calibration. </title> <booktitle> In International Conference on Computer Vision (ICCV), </booktitle> <pages> pages 224-229. </pages> <publisher> IEEE. </publisher>
Reference-contexts: Often the form of the distortion is modeled as a symmetric function at a particular image location, which may or may not be the same as the projec tive center point [Willson and Shafer, 1993]. Wei [Wei and Ma, 1993] and Qiu <ref> [Qiu and Ma, 1995] </ref> use more general models for lens distortion similar to the method described here. We review the standard 5-tuple approach and its problems by starting with the linear-projection model and adding non linear-projection terms to it.
Reference: [Ravela et al., 1996] <author> Ravela, S., Draper, B., Lim, J., and Weiss, R. </author> <year> (1996). </year> <title> Tracking object motion across aspect changes for augmented reality. </title> <booktitle> In Proceedings: 1996 ARPA Image Understanding Workshop, </booktitle> <pages> pages 1345-1352. ARPA. </pages>
Reference-contexts: This method works well for displaying registered data when the fiducial marks are visible and positioned nearby the registered data. Uenohara [Uenohara and Kanade, 1995] reports similar results using a template matching scheme to locate features on known objects. Ravela <ref> [Ravela et al., 1996] </ref> tracks known features on a known object through different poses of that object, effectively repositioning the camera at every step. These approaches are similar to the new AR model in figure 2.5 but without a tracking system. <p> This case describes systems by Mellor [Mellor, 1995b], Uenohara [Uenohara and Kanade, 1995], Ravela <ref> [Ravela et al., 1996] </ref>, and State [State et al., 1996a] which identify known fiducial marks to reposition a virtual camera. 3. Both feature positions and camera positions are unknown: This defines the classic non-linear structure-from-motion problem.
Reference: [Regan and Pose, 1994] <author> Regan, M. and Pose, R. </author> <year> (1994). </year> <title> Priority rendering with a virtual reality address recalculation pipeline. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 155-162. </pages>
Reference-contexts: However, we are near the point where greater improvements in VR and AR experiences will be made by reducing image-generation latency rather than by increasing image complexity. Regan and Pose <ref> [Regan and Pose, 1994] </ref> describe a system for reprojecting images immediately prior to display on a HMD which significantly shortens head rotation latency.
Reference: [Robinett and Rolland, 1991] <author> Robinett, W. and Rolland, J. </author> <year> (1991). </year> <title> A computational model for the stereoscopic optics of a head-mounted display. </title> <type> Technical Report TR91-009, </type> <institution> University of North Carolina. </institution>
Reference-contexts: An important perceptual problem is that objects in the field-of-view are all seen at the same accommodation distance from the user's eye (visible in the display), which is not true of natural vision <ref> [Robinett and Rolland, 1991] </ref>. Important optical design issues include field-of-view, angular resolution, and distortion. Robinett [Robinett and Rolland, 1991] and Edwards [Edwards et al., 1993] demonstrate it is also important to match a user's convergence and inter-pupillary distance. <p> An important perceptual problem is that objects in the field-of-view are all seen at the same accommodation distance from the user's eye (visible in the display), which is not true of natural vision <ref> [Robinett and Rolland, 1991] </ref>. Important optical design issues include field-of-view, angular resolution, and distortion. Robinett [Robinett and Rolland, 1991] and Edwards [Edwards et al., 1993] demonstrate it is also important to match a user's convergence and inter-pupillary distance. <p> Designing the appropriate optical systems for distortion-free viewing with correct stereopsis on a HMD and/or finding the appropriate pre-distortion functions to achieve this are left as separate problems <ref> [Robinett and Rolland, 1991] </ref>[Edwards et al., 1993]. 12 3. A mounting fixture for the head: Helmets are frequently used as mounting fixtures. There must be space for image displays and associated optics, optional video cameras, and a tracking system if needed.
Reference: [SAC, 1994] <institution> SAC (1994). Model GP-12 3-D Digitizer. Science Accessories Corporation, </institution> <address> 2 Research Drive, P.O. Box 825, Shelton, CT 06484-0825, phone: (203)925-1661 fax: </address> <pages> (203)929-9636. </pages>
Reference-contexts: A few technologies which have been employed for this are: mechanical linkages [Faro, 1997], magnetic sensors [Polhemus, 1997] [Ascension, 1997], optical sensors [Ward et al., 1992], and acoustic methods <ref> [SAC, 1994] </ref>. Bhatnagar [Bhatnagar, 1993] provides a survey of these methods. Magnetic tracking systems are attractive because no line-of-sight or mechanical tether is needed.
Reference: [Shashua, 1994] <author> Shashua, A. </author> <year> (1994). </year> <title> Projective structure from uncalibrated images: Structure from motion and recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(8) </volume> <pages> 778-790. </pages>
Reference-contexts: To date 4-tuple methods have not incorporated lens distortion models [Faugeras et al., 1992] [Liu et al., 1990] [Luong and Faugeras, 1993] [Maybank and Faugeras, 1992] <ref> [Shashua, 1994] </ref>. Several authors have studied methods of camera modeling based on the (X; Y; Z; U; V ) 5-tuple approach. Sutherland [Sutherland, 1974] describes a linear-projection (non-distorting) camera model which can be computed as the solution to a linear system of equations. <p> This could be a useful function to correct for errors in camera calibration approaches which assume projective distortion only, particu larly those based on 4-tuple data [Faugeras et al., 1992] [Liu et al., 1990] [Luong and Faugeras, 1993] [Maybank and Faugeras, 1992] <ref> [Shashua, 1994] </ref>. This function could also be used to map distorted images into undistorted ones for registration correction in augmented-reality systems.
Reference: [Shekhar and Chellappa, 1992] <author> Shekhar, C. and Chellappa, R. </author> <year> (1992). </year> <title> Passive ranging using a moving camera. </title> <journal> Journal of Robotic Systems, </journal> <volume> 9(6) </volume> <pages> 729-752. 86 </pages>
Reference-contexts: Feature redundancy in multiple images increases accuracy of the solution. These formulations are not suitable to head-mounted display applications because of the requirement for linear camera motion perpendicular to camera viewing direction. Shekhar <ref> [Shekhar and Chellappa, 1992] </ref> uses a partially-tracked camera and an extended Kal-man filter to find feature positions and improve camera motion parameters. This method is 18 similar to a tracked head-mounted display except that the camera position is not as well known and is estimated by a filtering process. <p> In theory this means that computing camera pose from the 3-D positions of well-known features is potentially more accurate than the camera pose as reported by the tracking arm. This idea is explored more generally by <ref> [Shekhar and Chellappa, 1992] </ref>. A method for computing camera pose from a set of fiducials is presented in appendix A. Assuming enough accurate feature positions are visible, camera pose can be computed directly by equation A.15.
Reference: [Shi and Tomasi, 1994] <author> Shi, J. and Tomasi, C. </author> <year> (1994). </year> <title> Good features to track. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 593-600. </pages>
Reference-contexts: [Bouguet and Perona, 1995] achieves impressive results for recovering the structure of a hallway and relative camera position for an 8000 frame round-trip image sequence. 20 2.4.4 Locating and Tracking Features in Image Sequences To recover structure from motion in an image sequence, Tomasi and Shi [Tomasi and Kanade, 1991b] <ref> [Shi and Tomasi, 1994] </ref> define a feature simply as a point which can be tracked reliably from one image to another. <p> Lucas suggests that parameters for rotation, scale, and shearing could also be used to register feature windows. Shi <ref> [Shi and Tomasi, 1994] </ref> addresses this idea by estimating an affine motion field for each feature. When a feature's correlation with its original view (after correction for motion and distortion) falls below a given threshold, Shi and Tomasi consider correspondence to be lost. <p> In addition, even a working stereo approach would not solve all problems because optimal estimation and world-model reconstruction would still be needed to improve accuracy and perform occlusion. An alternative approach which uses a stereo image sequence is proposed in section 4.3. Shi and Tomasi <ref> [Shi and Tomasi, 1994] </ref> [Tomasi and Kanade, 1991b] describe a method based on Lucas' earlier work [Lucas and Kanade, 1981] for identifying feature windows based on local image gradient information and then tracking the movement of those windows over time. <p> The amount of drift varies with the scale of the underlying functions used. 43 4.2.1 Metric Function The function used for feature selection is similar to that used by Tomasi and Shi [Tomasi and Kanade, 1991b] <ref> [Shi and Tomasi, 1994] </ref> who define a feature as something which can be tracked reliably from one image to another. Considering an image as a height surface, their criteria for correlating feature windows require that a feature point have high curvature in both principal directions.
Reference: [Siegel and Leary, 1992] <author> Siegel, M. and Leary, M. </author> <year> (1992). </year> <title> Range from focus pixel-by-pixel. </title> <booktitle> In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <pages> pages 2035-2040. </pages>
Reference-contexts: Another "pixel-by-pixel" approach is to adjust the imaging system sequentially for each point in an image until it is in focus. The setting of the lens system needed to focus each point is used to estimate that point's distance from the camera <ref> [Siegel and Leary, 1992] </ref>. Unfortunately, this pixel-by-pixel approach is impractical because of its sequential nature. The best characteristic of the depth-from-focus/defocus approach is that only one camera or optical axis is used, avoiding both correspondence and occlusion problems.
Reference: [Skifstad, 1991] <author> Skifstad, K. D. </author> <year> (1991). </year> <title> High-Speed Range Estimation Based on Intensity Gradient Analysis. </title> <booktitle> Springer Series in Perception Engineering. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This method is 18 similar to a tracked head-mounted display except that the camera position is not as well known and is estimated by a filtering process. Skifstad <ref> [Skifstad, 1991] </ref> demonstrates a mobile robot which estimates depth in its environment by using its own measured motion to position successive camera images and to restrict the search for object features to specific image areas. 2.
Reference: [State et al., 1996a] <author> State, A., Hirota, G., Chen, D., Garrett, W., and Livingston, M. </author> <year> (1996a). </year> <title> Superior augmented reality registration by integrating landmark tracking and magnetic tracking. </title> <booktitle> In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 429-438. </pages>
Reference-contexts: The locations of these points in the user's field of view are used to correct various errors in an AR system [Bajura and Neumann, 1995a] [Bajura and Neumann, 1995b]. Applications of this technique have yielded encouraging results in the 3-D ultrasound visualization research at the University of North Carolina <ref> [State et al., 1996a] </ref> [State et al., 1996b]. * Chapter 4 describes a method for detecting surface geometry in the user's natural environment to generate proper occlusion cues and to perform collision detection. <p> This case describes systems by Mellor [Mellor, 1995b], Uenohara [Uenohara and Kanade, 1995], Ravela [Ravela et al., 1996], and State <ref> [State et al., 1996a] </ref> which identify known fiducial marks to reposition a virtual camera. 3. Both feature positions and camera positions are unknown: This defines the classic non-linear structure-from-motion problem. Because both camera and feature positions are unknown, shape and motion recovery are possible only up to a scale factor. <p> Blake [Blake and Isard, 1994] demonstrates incremental real-time tracking of curved silhouettes suitable for recognizing hand and lip movement. The ultrasound visualization project at the University of North Carolina has achieved good results in incrementally tracking color dot targets <ref> [State et al., 1996a] </ref> under a variety of lighting conditions. It appears that many of the various non real-time incremental tracking algorithms are limited only by processing speed from being real-time algorithms. The simple real-time tracking algorithms demonstrated to date have become possible only because of increases in processing power. <p> It can be computed in real time incrementally or by O (n log n) algorithms. 4.6 Correcting Camera Position Using precalibrated features (fiducials) to correct for errors in camera pose has already been demonstrated [Bajura and Neumann, 1995b] <ref> [State et al., 1996a] </ref>. The contribution here is to use features which are not precalibrated to correct camera pose. This is possible under the assumption that the camera's error is normally distributed over time. The filtering process which reconstructs 3-D feature positions in some sense averages out the camera's error. <p> This approach is also not ideal from the viewpoint of signal processing and optimally filtered estimation. It minimizes the error seen in composite images by adding noise to the camera position estimate. State <ref> [State et al., 1996a] </ref> presents several approaches for correcting camera position depending on the number of features visible. This system corrects camera pose by performing rotations about the camera's center of projection to align points in a central window of each image.
Reference: [State et al., 1996b] <author> State, A., Livingston, M., Garrett, W., Hirota, G., Whitton, M., Pisano, E., and Fuchs, H. </author> <year> (1996b). </year> <title> Technologies for augmented-reality systems: </title> <booktitle> Realizing ultrasound-guided needle biopsies. In Proceedings of ACM Siggraph, Computer Graphics, </booktitle> <pages> pages 439-446. </pages>
Reference-contexts: Another type of SE is an augmented-reality (AR) system where a user is presented with images of his natural surroundings which are augmented with computer-generated images. For example, these could be 3-D medical data sets of a patient which are anatomically registered [Bajura et al., 1992] <ref> [State et al., 1996b] </ref> or 3-D assembly directions for complex machinery [Feiner et al., 1993] [Feiner et al., 1992]. <p> Applications of this technique have yielded encouraging results in the 3-D ultrasound visualization research at the University of North Carolina [State et al., 1996a] <ref> [State et al., 1996b] </ref>. * Chapter 4 describes a method for detecting surface geometry in the user's natural environment to generate proper occlusion cues and to perform collision detection. This more difficult than correcting image registration (as in Chapter 3) because it requires constructing a model of scene geometry.
Reference: [Stone, 1989] <author> Stone, R. C. </author> <year> (1989). </year> <title> A comparison of igital centering algorithms. </title> <journal> The Astronomical Journal, </journal> <volume> 97(4) </volume> <pages> 1227-1237. </pages>
Reference-contexts: 10 -3 Width (pixels) 0.00 10.00 20.00 30.00 40.00 50.00 60.00 70.00 80.00 90.00 Figure A.2: Maximum error for 1D Center of Mass Computation 78 This result is consistent with Peuchot's [Peuchot, 1993] result of locating points to within 1% of a pixel in static 2-D plane measurements and Stone's <ref> [Stone, 1989] </ref> result that fitting a gaussian function to an (expected) gaussian symmetric measurement gives similar accuracy to computing a centroid, provided the signal levels are high enough. 79
Reference: [Strang, 1988] <author> Strang, G. </author> <year> (1988). </year> <title> Linear Algebra and Its Applications. </title> <publisher> Harcourt Brace Janovich, </publisher> <address> Orlando, FL, </address> <note> 3rd edition. </note>
Reference-contexts: If no previous estimate is available, an arm's length guess of 50 cm in front of the camera is used. The covariance matrix representing a ray's probability distribution is formed by a similarity transformation <ref> [Strang, 1988] </ref>. <p> performed by first factoring CP 3fi4 into P ROJ 3fi4 and a residual (F OO 3fi3 ): CP 3fi4 = F OO 3fi3 ffi P ROJ 3fi4 (A.32) Next F OO 3fi3 is decomposed into an orthonormal ROT 3fi3 matrix and an upper triangular SKEW 3fi3 matrix by QR factorization <ref> [Strang, 1988] </ref>: F OO 3fi3 = SKEW 3fi3 ffi ROT 3fi3 (A.33) If the skew between the u and v image planes, s uv , is a problem (u and v image planes not orthogonal), the SKEW 3fi3 matrix can be further factored into: 2 6 4 0 s v v
Reference: [Sutherland, 1965] <author> Sutherland, I. E. </author> <year> (1965). </year> <title> The ultimate display. </title> <booktitle> In Proc. IFIP Congress, </booktitle> <volume> volume 65, </volume> <pages> pages 506-508. </pages>
Reference-contexts: It would be possible to immerse a user in a synthetic environment modeled after his own environment but this modeling task is not always practical. 3 Ivan Sutherland is often credited with the original concept of a synthetic-environment system in his visionary paper The Ultimate Display <ref> [Sutherland, 1965] </ref>. 1 completely synthetic experience, VE systems become dominant for three main reasons: 1. Given the same underlying technology, a AR system exposes errors in tracking, registration accuracy, and system delay which are not as apparent in a similar VE system, 2. <p> contains conclusions and directions for future work in this area. * Appendix A covers camera calibration which is used in Chapter 4. 4 Chapter 2 Background The idea of AR as a dynamic immersive mode of interaction is usually attributed to Ivan Sutherland from his visionary paper The Ultimate Display <ref> [Sutherland, 1965] </ref>: The ultimate display would, of course, be a room within which the computer can control the existence of matter. Since then much progress has been made, in both small and large steps, toward this goal.
Reference: [Sutherland, 1968] <author> Sutherland, I. E. </author> <year> (1968). </year> <title> A head-mounted three dimensional display. </title> <booktitle> In Fall Joint Computer Conference, AFIPS Conference Proceedings, </booktitle> <volume> volume 33, </volume> <pages> pages 757-764. </pages>
Reference-contexts: It is interesting to note that Ivan Sutherland's 3 original synthetic-environment system was actually an AR, or see-through, system with which the user could view computer-generated images superimposed on views of his natural environment <ref> [Sutherland, 1968] </ref>. <p> It seems that the ultimate display problem is well on its way to being solved... Or is it? Many of the original problems present in Sutherland's early system <ref> [Sutherland, 1968] </ref> remain. The delay between a user's head motion and corresponding image display remains largely unchanged. Accurate registration between computer-generated imagery and real imagery is still elusive. <p> Because of the tradeoff of latency for image complexity and because of the delay incurred by rendering images as discrete frames [Mine and Bishop, 1993], the delay in image generation for synthetic-reality systems has remained largely unchanged or has even grown worse since Suther-land's pioneering work <ref> [Sutherland, 1968] </ref> 2 . This fact has been widely tolerated because the improvements in image complexity and rendering models have been so dramatic and possibly also because there has not been sufficient (economic) incentive to focus on low-latency systems.
Reference: [Sutherland, 1974] <author> Sutherland, I. E. </author> <year> (1974). </year> <title> Three-dimensional data input by tablet. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 62(4) </volume> <pages> 453-461. </pages>
Reference-contexts: To date 4-tuple methods have not incorporated lens distortion models [Faugeras et al., 1992] [Liu et al., 1990] [Luong and Faugeras, 1993] [Maybank and Faugeras, 1992] [Shashua, 1994]. Several authors have studied methods of camera modeling based on the (X; Y; Z; U; V ) 5-tuple approach. Sutherland <ref> [Sutherland, 1974] </ref> describes a linear-projection (non-distorting) camera model which can be computed as the solution to a linear system of equations. <p> shows how this transformation can be decomposed into camera parameters such as position, orientation, and field-of-view but does not account for general projective distortions. 64 Tsai [Tsai, 1987] and Weng [Weng et al., 1992] add non-linear distortion terms and non-linear mini- mization to essentially the same equations used by Sutherland <ref> [Sutherland, 1974] </ref>. The problem with these later approaches is that they fail to separate the lens distortion function from the inherent linear-projection function of a camera system. This makes it difficult to know whether their non linear minimization solutions are the correct ones. <p> X lp 3;1 + U Y lp 3;2 + U Z lp 3;3 + U lp 3;4 ) = 0 (A.4) With at least 8 separate (X; Y; Z; U; V ) data points, LP 3fi4 can be found by directly by least squares methods from equations A.4 and A.5 <ref> [Sutherland, 1974] </ref>.
Reference: [Szeliski and Kang, 1993a] <author> Szeliski, R. and Kang, S. B. </author> <year> (1993a). </year> <title> Recovering 3d shape and motion from image streams using non-linear least squares. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 752-753. </pages>
Reference-contexts: This method achieves the best answer in situations where all data are available simultaneously [Weng et al., 1993]. Getting a good initial guess can be important to speed up minimization or ensure that the correct solution is found. Examples of this method are by Szeliski <ref> [Szeliski and Kang, 1993a] </ref> [Szeliski and Kang, 1993b], and Weng [Weng et al., 1993]. Tomasi [Tomasi and Kanade, 1992] [Tomasi and Kanade, 1991a] [Tomasi and Kanade, 1991b] describes a linear solution for factorizing shape and motion 19 under the assumption of orthographic projection.
Reference: [Szeliski and Kang, 1993b] <author> Szeliski, R. and Kang, S. B. </author> <year> (1993b). </year> <title> Recovering 3d shape and motion from image streams using non-linear least squares. </title> <type> Technical report, </type> <institution> Digital Equipment Corporation Cambridge Research Lab. </institution>
Reference-contexts: This method achieves the best answer in situations where all data are available simultaneously [Weng et al., 1993]. Getting a good initial guess can be important to speed up minimization or ensure that the correct solution is found. Examples of this method are by Szeliski [Szeliski and Kang, 1993a] <ref> [Szeliski and Kang, 1993b] </ref>, and Weng [Weng et al., 1993]. Tomasi [Tomasi and Kanade, 1992] [Tomasi and Kanade, 1991a] [Tomasi and Kanade, 1991b] describes a linear solution for factorizing shape and motion 19 under the assumption of orthographic projection.
Reference: [Terzopoulos, 1988] <author> Terzopoulos, D. </author> <year> (1988). </year> <title> The computation of visible-surface representations. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(4) </volume> <pages> 417-438. 87 </pages>
Reference-contexts: A simple modification is to avoid tessellating large triangles which can prevent construction of some false geometry, particularly around the borders of the reconstructed surface [Kanade, 1996a]. Other more involved methods for interpolating surface data include using multiresolution wavelet transforms [Yaou and Chang, 1994], grid-based hierarchies <ref> [Terzopoulos, 1988] </ref>, surface patches [Liao and Medioni, 1996], and constrained Delunnay triangulation. A simple 2-D Delunnay triangulation approximation is reasonable in certain environments and is sufficient for certain applications.
Reference: [Thomas and Oliensis, 1992] <author> Thomas, J. I. and Oliensis, J. </author> <year> (1992). </year> <title> Recursive multi-frame structure frommotion incorporating motion error. </title> <booktitle> In Darpa Image Understanding Workshop. </booktitle>
Reference-contexts: Optimal (Kalman) filtering is typically used for this process. The advantage of an incremental approach is that a partial solution is available at intermediate steps. Several authors [Matthies et al., 1989] [Weng et al., 1993] [Broida and Chellappa, 1986] <ref> [Thomas and Oliensis, 1992] </ref> [Azarbayejani and Pentland, 1994] [Bouguet and Perona, 1995] [Morita and Kanade, 1994] report varying results using Kalman filters. Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed.
Reference: [Tomasi and Kanade, 1991a] <author> Tomasi, C. and Kanade, T. </author> <year> (1991a). </year> <title> Shape and motion from image streams: A factorization method 2. point features in 3D motion. </title> <type> Technical Report CMU-CS-91-105, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Getting a good initial guess can be important to speed up minimization or ensure that the correct solution is found. Examples of this method are by Szeliski [Szeliski and Kang, 1993a] [Szeliski and Kang, 1993b], and Weng [Weng et al., 1993]. Tomasi [Tomasi and Kanade, 1992] <ref> [Tomasi and Kanade, 1991a] </ref> [Tomasi and Kanade, 1991b] describes a linear solution for factorizing shape and motion 19 under the assumption of orthographic projection. Shape and motion are decomposed into a product of shape and rotation matrices which is found by singular value decomposition analysis. <p> Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed. Morita [Morita and Kanade, 1994] presents a sequential method based on the batch method reported by Tomasi [Tomasi and Kanade, 1992] <ref> [Tomasi and Kanade, 1991a] </ref> [Tomasi and Kanade, 1991b] which achieves nearly the same results as Tomasi but retains the same limitations.
Reference: [Tomasi and Kanade, 1991b] <author> Tomasi, C. and Kanade, T. </author> <year> (1991b). </year> <title> Shape and motion from image streams: A factorization method part 2. detection and tracking of point features. </title> <type> Technical Report CMU-CS-91-132, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Examples of this method are by Szeliski [Szeliski and Kang, 1993a] [Szeliski and Kang, 1993b], and Weng [Weng et al., 1993]. Tomasi [Tomasi and Kanade, 1992] [Tomasi and Kanade, 1991a] <ref> [Tomasi and Kanade, 1991b] </ref> describes a linear solution for factorizing shape and motion 19 under the assumption of orthographic projection. Shape and motion are decomposed into a product of shape and rotation matrices which is found by singular value decomposition analysis. <p> Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed. Morita [Morita and Kanade, 1994] presents a sequential method based on the batch method reported by Tomasi [Tomasi and Kanade, 1992] [Tomasi and Kanade, 1991a] <ref> [Tomasi and Kanade, 1991b] </ref> which achieves nearly the same results as Tomasi but retains the same limitations. <p> Bouguet [Bouguet and Perona, 1995] achieves impressive results for recovering the structure of a hallway and relative camera position for an 8000 frame round-trip image sequence. 20 2.4.4 Locating and Tracking Features in Image Sequences To recover structure from motion in an image sequence, Tomasi and Shi <ref> [Tomasi and Kanade, 1991b] </ref> [Shi and Tomasi, 1994] define a feature simply as a point which can be tracked reliably from one image to another. Tomasi [Tomasi and Kanade, 1991b] demonstrates that trackable features can be recognized by local statistics based on squared image gradient values by implementing a tracking algorithm <p> round-trip image sequence. 20 2.4.4 Locating and Tracking Features in Image Sequences To recover structure from motion in an image sequence, Tomasi and Shi <ref> [Tomasi and Kanade, 1991b] </ref> [Shi and Tomasi, 1994] define a feature simply as a point which can be tracked reliably from one image to another. Tomasi [Tomasi and Kanade, 1991b] demonstrates that trackable features can be recognized by local statistics based on squared image gradient values by implementing a tracking algorithm based on Lucas' [Lucas and Kanade, 1981] method of registering windows by shifting them relative to one another to maximize a correlation metric. <p> Forstner [Forstner, 1994] presents a scale-space approach to feature extraction based on local statistics of the image function. The criteria used are based on the same squared gradient information used by Tomasi <ref> [Tomasi and Kanade, 1991b] </ref> to select features. In addition to the trackable features as defined by Tomasi, Forstner identifies lines and edges in static figures. Real-time tracking of simple features in video images has been demonstrated in several applications. A few recent examples are listed here. <p> In addition, even a working stereo approach would not solve all problems because optimal estimation and world-model reconstruction would still be needed to improve accuracy and perform occlusion. An alternative approach which uses a stereo image sequence is proposed in section 4.3. Shi and Tomasi [Shi and Tomasi, 1994] <ref> [Tomasi and Kanade, 1991b] </ref> describe a method based on Lucas' earlier work [Lucas and Kanade, 1981] for identifying feature windows based on local image gradient information and then tracking the movement of those windows over time. <p> The amount of drift varies with the scale of the underlying functions used. 43 4.2.1 Metric Function The function used for feature selection is similar to that used by Tomasi and Shi <ref> [Tomasi and Kanade, 1991b] </ref> [Shi and Tomasi, 1994] who define a feature as something which can be tracked reliably from one image to another. Considering an image as a height surface, their criteria for correlating feature windows require that a feature point have high curvature in both principal directions.
Reference: [Tomasi and Kanade, 1992] <author> Tomasi, C. and Kanade, T. </author> <year> (1992). </year> <title> Shape and motion from image streams: A factorization method 2. full report on the orthographic case [parts 2,8,10]. </title> <type> Technical Report CMU-CS-92-104, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Getting a good initial guess can be important to speed up minimization or ensure that the correct solution is found. Examples of this method are by Szeliski [Szeliski and Kang, 1993a] [Szeliski and Kang, 1993b], and Weng [Weng et al., 1993]. Tomasi <ref> [Tomasi and Kanade, 1992] </ref> [Tomasi and Kanade, 1991a] [Tomasi and Kanade, 1991b] describes a linear solution for factorizing shape and motion 19 under the assumption of orthographic projection. Shape and motion are decomposed into a product of shape and rotation matrices which is found by singular value decomposition analysis. <p> Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed. Morita [Morita and Kanade, 1994] presents a sequential method based on the batch method reported by Tomasi <ref> [Tomasi and Kanade, 1992] </ref> [Tomasi and Kanade, 1991a] [Tomasi and Kanade, 1991b] which achieves nearly the same results as Tomasi but retains the same limitations.
Reference: [Tsai, 1987] <author> Tsai, R. Y. </author> <year> (1987). </year> <title> A versatile camera calibration technique for high-accuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses. </title> <journal> IEEE Journal of Robotics and Automation, RA-3(4):323-344. </journal>
Reference-contexts: Ganapathy [Ganapathy, 1984a] shows how this transformation can be decomposed into camera parameters such as position, orientation, and field-of-view but does not account for general projective distortions. 64 Tsai <ref> [Tsai, 1987] </ref> and Weng [Weng et al., 1992] add non-linear distortion terms and non-linear mini- mization to essentially the same equations used by Sutherland [Sutherland, 1974]. <p> The homogeneous coordinate of any 3-D coordinate is [X; Y; Z; W = 1] T . 65 When there is non-linear projective distortion in the lens system, equations A.2 and A.3 are modified by adding image-space distortion functions <ref> [Tsai, 1987] </ref> [Weng et al., 1992]: U + f U (U; V ) = lp 3;1 X + lp 3;2 Y + lp 3;3 Z + lp3; 4 V + f V (U; V ) = lp 3;1 X + lp 3;2 Y + lp 3;3 Z + lp3; 4 where
Reference: [Tsai and Huang, 1984] <author> Tsai, R. Y. and Huang, T. S. </author> <year> (1984). </year> <title> Uniqueness and estimation of three-dimensional motion parameters of rigid objects with curved surfaces. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-6(1):13-26. </journal>
Reference-contexts: Longuet-Higgins [Longuet-Higgins, 1981] [Longuet-Higgins, 1984] and Tsai <ref> [Tsai and Huang, 1984] </ref> describe non-linear methods for recovering the relative orientation between two frames from this "essential matrix." Once relative orientation is known, the structure of the features can be computed. The chief drawback to this method is noise sensitivity to input data.
Reference: [Uenohara and Kanade, 1995] <author> Uenohara, M. and Kanade, T. </author> <year> (1995). </year> <title> Vision-based object registration for real-time image overlay. </title> <booktitle> In Proceedings of the first international converence on Computer vision, Virtual reality, and Robotics in Medicine (CVRMed), </booktitle> <address> Nice France. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The effective position of the camera is computed dynamically to be consistent with the observed positions of fiducial marks. This method works well for displaying registered data when the fiducial marks are visible and positioned nearby the registered data. Uenohara <ref> [Uenohara and Kanade, 1995] </ref> reports similar results using a template matching scheme to locate features on known objects. Ravela [Ravela et al., 1996] tracks known features on a known object through different poses of that object, effectively repositioning the camera at every step. <p> This case describes systems by Mellor [Mellor, 1995b], Uenohara <ref> [Uenohara and Kanade, 1995] </ref>, Ravela [Ravela et al., 1996], and State [State et al., 1996a] which identify known fiducial marks to reposition a virtual camera. 3. Both feature positions and camera positions are unknown: This defines the classic non-linear structure-from-motion problem.
Reference: [Ward et al., 1992] <author> Ward, M., Azuma, R., Bennet, R., Gottschalk, S., and Fuchs, H. </author> <year> (1992). </year> <title> A demonstrated optical tracker with scalable work area for head-mounted display systems. </title> <booktitle> In Proceedings of Symposium on Interactive 3D Graphics, </booktitle> <address> Cambridge, Mass., </address> <pages> pages 43-52. </pages>
Reference-contexts: A few technologies which have been employed for this are: mechanical linkages [Faro, 1997], magnetic sensors [Polhemus, 1997] [Ascension, 1997], optical sensors <ref> [Ward et al., 1992] </ref>, and acoustic methods [SAC, 1994]. Bhatnagar [Bhatnagar, 1993] provides a survey of these methods. Magnetic tracking systems are attractive because no line-of-sight or mechanical tether is needed.
Reference: [Watson and Hodges, 1995] <author> Watson, B. A. and Hodges, L. F. </author> <year> (1995). </year> <title> Using texture maps to correct for optical distortion in head-mounted displays. </title> <booktitle> In Proceedings of IEEE Virtual Reality Annual International Symposium (VRAIS), </booktitle> <pages> pages 172-178. </pages>
Reference-contexts: To match image distortion in both the real and computer-generated image feedback paths (figure 2.3), either the image-generation system must generate distorted images or the real-world view must be distorted to match the pinhole camera model of the image-generation system. Real-time distortion correction has been demonstrated on commercial hardware <ref> [Watson and Hodges, 1995] </ref> by texture-mapping rendered images onto a warped polygon mesh and rendering a second image of that mesh. Unfortunately this solution adds a cycle of delay to the rendering process.
Reference: [Wei and Ma, 1993] <author> Wei, G. and Ma, S. </author> <year> (1993). </year> <title> A complete two-plane camera calibration method and experimental comparisons. </title> <booktitle> In International Conference on Computer Vision (ICCV), </booktitle> <pages> pages 439-446. </pages> <publisher> IEEE. </publisher>
Reference-contexts: Often the form of the distortion is modeled as a symmetric function at a particular image location, which may or may not be the same as the projec tive center point [Willson and Shafer, 1993]. Wei <ref> [Wei and Ma, 1993] </ref> and Qiu [Qiu and Ma, 1995] use more general models for lens distortion similar to the method described here. We review the standard 5-tuple approach and its problems by starting with the linear-projection model and adding non linear-projection terms to it.
Reference: [Weng et al., 1993] <author> Weng, J., Ahuja, N., and Huang, T. S. </author> <year> (1993). </year> <title> Optimal motion and structure estimation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(9) </volume> <pages> 864-884. 88 </pages>
Reference-contexts: Because all data are considered simultaneously, this method is much less sensitive to noise than the two frame method. This method achieves the best answer in situations where all data are available simultaneously <ref> [Weng et al., 1993] </ref>. Getting a good initial guess can be important to speed up minimization or ensure that the correct solution is found. Examples of this method are by Szeliski [Szeliski and Kang, 1993a] [Szeliski and Kang, 1993b], and Weng [Weng et al., 1993]. <p> answer in situations where all data are available simultaneously <ref> [Weng et al., 1993] </ref>. Getting a good initial guess can be important to speed up minimization or ensure that the correct solution is found. Examples of this method are by Szeliski [Szeliski and Kang, 1993a] [Szeliski and Kang, 1993b], and Weng [Weng et al., 1993]. Tomasi [Tomasi and Kanade, 1992] [Tomasi and Kanade, 1991a] [Tomasi and Kanade, 1991b] describes a linear solution for factorizing shape and motion 19 under the assumption of orthographic projection. <p> Like the batch method, the initial guess and parameterization selection are important for converging on the correct solution. Optimal (Kalman) filtering is typically used for this process. The advantage of an incremental approach is that a partial solution is available at intermediate steps. Several authors [Matthies et al., 1989] <ref> [Weng et al., 1993] </ref> [Broida and Chellappa, 1986] [Thomas and Oliensis, 1992] [Azarbayejani and Pentland, 1994] [Bouguet and Perona, 1995] [Morita and Kanade, 1994] report varying results using Kalman filters. Azarbayejani [Azarbayejani and Pentland, 1994] claims a unique parameterization including camera focal length improves solution accuracy and speed.
Reference: [Weng et al., 1992] <author> Weng, J., Cohen, P., and Herniou, M. </author> <year> (1992). </year> <title> Camera calibration with dis-tortion models and accuracy evaluation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(10) </volume> <pages> 965-980. </pages>
Reference-contexts: Figure 3.7 is the distorted image output from the camera. Figure 3.8 is a corrected version of the same image. The correction is a radial distortion at the image center which accounts for most of the image distortion <ref> [Weng et al., 1992] </ref>. The important point about calibration is that it is difficult to do accurately, particularly when the tracking system used has noticeable tracking error throughout its working volume. <p> Ganapathy [Ganapathy, 1984a] shows how this transformation can be decomposed into camera parameters such as position, orientation, and field-of-view but does not account for general projective distortions. 64 Tsai [Tsai, 1987] and Weng <ref> [Weng et al., 1992] </ref> add non-linear distortion terms and non-linear mini- mization to essentially the same equations used by Sutherland [Sutherland, 1974]. The problem with these later approaches is that they fail to separate the lens distortion function from the inherent linear-projection function of a camera system. <p> The homogeneous coordinate of any 3-D coordinate is [X; Y; Z; W = 1] T . 65 When there is non-linear projective distortion in the lens system, equations A.2 and A.3 are modified by adding image-space distortion functions [Tsai, 1987] <ref> [Weng et al., 1992] </ref>: U + f U (U; V ) = lp 3;1 X + lp 3;2 Y + lp 3;3 Z + lp3; 4 V + f V (U; V ) = lp 3;1 X + lp 3;2 Y + lp 3;3 Z + lp3; 4 where f U
Reference: [Willson and Shafer, 1993] <author> Willson, R. G. and Shafer, S. A. </author> <year> (1993). </year> <booktitle> What is the center of the image? In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 670-671. </pages>
Reference-contexts: This makes it difficult to know whether their non linear minimization solutions are the correct ones. Often the form of the distortion is modeled as a symmetric function at a particular image location, which may or may not be the same as the projec tive center point <ref> [Willson and Shafer, 1993] </ref>. Wei [Wei and Ma, 1993] and Qiu [Qiu and Ma, 1995] use more general models for lens distortion similar to the method described here. We review the standard 5-tuple approach and its problems by starting with the linear-projection model and adding non linear-projection terms to it.
Reference: [Wloka and Anderson, 1995] <author> Wloka, M. M. and Anderson, B. G. </author> <year> (1995). </year> <title> Resolving occlusion in augmented reality. </title> <booktitle> In ACM Symposium on Interactive 3D Graphics, </booktitle> <pages> pages 5-12. </pages> <publisher> ACM. </publisher>
Reference-contexts: Ravela [Ravela et al., 1996] tracks known features on a known object through different poses of that object, effectively repositioning the camera at every step. These approaches are similar to the new AR model in figure 2.5 but without a tracking system. Kanade [Kanade, 1996b] and Wloka <ref> [Wloka and Anderson, 1995] </ref> report using real-time and near real-time depth-from-stereo, respectively, to generate depth maps relative to a viewing position. These depth maps are then used to combine synthetic images with real ones from video cameras.
Reference: [Yaou and Chang, 1994] <author> Yaou, M.-H. and Chang, W.-T. </author> <year> (1994). </year> <title> Fast surface interpolation using multiresolution wavelet transform. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(7) </volume> <pages> 673-688. 89 </pages>
Reference-contexts: A simple modification is to avoid tessellating large triangles which can prevent construction of some false geometry, particularly around the borders of the reconstructed surface [Kanade, 1996a]. Other more involved methods for interpolating surface data include using multiresolution wavelet transforms <ref> [Yaou and Chang, 1994] </ref>, grid-based hierarchies [Terzopoulos, 1988], surface patches [Liao and Medioni, 1996], and constrained Delunnay triangulation. A simple 2-D Delunnay triangulation approximation is reasonable in certain environments and is sufficient for certain applications.
References-found: 114

