URL: ftp://ftp.eecs.umich.edu/people/fessler/ps/94,tsp,sage.ps.Z
Refering-URL: http://www.eecs.umich.edu/~fessler/papers/jour.html
Root-URL: http://www.cs.umich.edu
Email: Email: fessler@umich.edu,  
Phone: Voice: 313-763-1434, FAX: 313-764-8041,  
Title: Space-Alternating Generalized Expectation-Maximization Algorithm  
Author: Jeffrey A. Fessler and Alfred O. Hero 
Web: WWW: http://www.eecs.umich.edu/~fessler/  
Address: Ann Arbor, MI 48109-2122  
Affiliation: Dept. of Electrical Engineering and Computer Science 4240 EECS Bldg., University of Michigan,  
Date: 2664-77, OCT. 1994 1  
Note: IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 42, NO. 10, PP.  
Abstract: The expectation-maximization (EM) method can facilitate maximizing likelihood functions that arise in statistical estimation problems. In the classical EM paradigm, one iteratively maximizes the conditional log-likelihood of a single unobservable complete data space, rather than maximizing the intractable likelihood function for the measured or incomplete data. EM algorithms update all parameters simultaneously, which has two drawbacks: 1) slow convergence, and 2) difficult maximization steps due to coupling when smoothness penalties are used. This paper describes the space-alternating generalized EM (SAGE) method, which updates the parameters sequentially by alternating between several small hidden-data spaces defined by the algorithm designer. We prove that the sequence of estimates monotonically increases the penalized-likelihood objective, we derive asymptotic convergence rates, and we provide sufficient conditions for monotone convergence in norm. Two signal processing applications illustrate the method: estimation of superimposed signals in Gaussian noise, and image reconstruction from Poisson measurements. In both applications, our SAGE algorithms easily accommodate smoothness penalties, and converge faster than the EM algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A P Dempster, N M Laird, and D B Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Stat. Soc. Ser. B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Rather than requiring a strict maximization in (5), one could settle simply for local maxima [4], or for mere increases in S , in analogy with GEM algorithms <ref> [1] </ref>. These generalizations provide the opportunity to further refine the tradeoff between convergence rate and computation per-iteration. D. Choosing Index Sets To implement a SAGE algorithm, one must choose a sequence of index sets S i ; i = 0; 1; : : :. <p> E. Monotonicity Let S and X S respectively denote an index set and hidden data space used in a SAGE algorithm. Under mild regularity conditions <ref> [1, 4] </ref>, one can apply Bayes' Theorem to (3) to see that Q S ( S ; ) = f (xjY = y; ) log f (x; S ; ~ S ) dx where L ( S ; ~ S ) = log f (y; S ; ~ S ); 4 <p> Using these definitions and Jensen's inequality <ref> [1] </ref>, one can easily show that H S ( S ; ) H S ( S ; ); 8 S ; 8 ; (11) from which the following theorem follows directly. Theorem 1: Let i denote the sequence of estimates gen erated by a SAGE algorithm (5). <p> F. Convergence For a well behaved objective , the monotonicity property ensures that the sequence f i g will not diverge, but it does not guarantee convergence even to a local maxi mum of . (Some EM algorithms have fixed points that are not local maxima <ref> [1, 31] </ref>.) Therefore, in the appendices we provide additional theorems that give sufficient conditions for convergence in norm, and that characterize the asymptotic convergence rate. <p> The benefits of par-allelization must be weighed against the convergence rates for each application. It is probably no coincidence that the applications we put forth are ones in which the terminology "incomplete-data" and "complete-date" as introduced in <ref> [1] </ref> are somewhat unnatural. In most of the statistical applications discussed in [1], there is a clearly identifiable portion of the data that is "missing," and hence one natural complete-data space. <p> The benefits of par-allelization must be weighed against the convergence rates for each application. It is probably no coincidence that the applications we put forth are ones in which the terminology "incomplete-data" and "complete-date" as introduced in <ref> [1] </ref> are somewhat unnatural. In most of the statistical applications discussed in [1], there is a clearly identifiable portion of the data that is "missing," and hence one natural complete-data space. In contrast, there is nothing really "incomplete" about tomographic measurements; the problem is simply that the log-likelihood is difficult to maximize. <p> First note that from (10) one can show that (r 110 H S )( ^ S ; ^ ) = (r 200 H S )( ^ S ; ^ ); (cf (3.16) of <ref> [1] </ref>). For an index set S, define F S then from (10) one can see that the matrix F S X jy is the conditional Fisher information of X S for S given Y = y and given all of the other parameters ~ S . <p> For an EM algorithm, this spectral radius increases towards 1 as the complete-data becomes more informative, i.e., as its Fisher information increases <ref> [1, 4, 5] </ref>. In this section we demonstrate that a similar relationship holds for the convergence rate of a SAGE algorithm.
Reference: [2] <author> M Feder and E Weinstein. </author> <title> Parameter estimation of superimposed signals using the EM algorithm. </title> <journal> IEEE Tr. Acoust. Sp. Sig. Proc., </journal> <volume> 36(4) </volume> <pages> 477-489, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: IV. Example 2 Linear Gaussian Measurements The Poisson problem has important practical applications, but the nonlinearity of the algorithms complicates a formal analysis of the convergence rates. In this section, we analyze the problem of estimating superimposed linear signals in Gaussian noise <ref> [2, 9] </ref>: Y = a 1 1 + + a p p + * = A + *; (20) where A = [a 1 : : : a p ], and * is additive zero-mean Gaussian noise with covariance , i.e. * ~ N (0; ). <p> = (A 0 1 A + P ) 1 A 0 1 y: (21) If A is large, or if positivity constraints on are appropriate, then (21) is impractical and iterative methods may be useful. (One can also think of (20) as a linearization of the more interesting nonlinear problem <ref> [2] </ref>.) We present the linear version here since we can derive exact expressions for the convergence rates. We first present admissible hidden-data spaces for this problem, derive EM and SAGE algorithms, and then prove that the SAGE algorithm converges faster. <p> We first present admissible hidden-data spaces for this problem, derive EM and SAGE algorithms, and then prove that the SAGE algorithm converges faster. Since the mean of Y is linear in , the conventional complete-data <ref> [2, 9] </ref> for the EM algorithm for this problem is also linear in . <p> EM Algorithm The ordinary EM algorithm <ref> [2, 9] </ref> is based on the following choices for the complete-data space: S = f1; : : :; pg, B = diag fa k g, C = 1 p I p , G = (1 0 ~ B = ~ G = W = 0, where diag fg denotes a diagonal <p> This depends of course on how difficult the M-step is; in the nonlinear case discussed in <ref> [2] </ref>, the M-step is presumably fairly difficult, so parallelization may be advantageous. Equations (26) and (27) help one examine these types of tradeoffs. V.
Reference: [3] <author> K Lange and R Carson. </author> <title> EM reconstruction algorithms for emission and transmission tomography. </title> <journal> J. Comp. Assisted Tomo., </journal> <volume> 8(2) </volume> <pages> 306-316, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: However, those authors recently concluded [16] that a new simultaneous-update algorithm by De Pierro [17] is preferable. Those methods use the same complete-data space as in the conventional EM algorithm for image reconstruction <ref> [3] </ref>, so the convergence rate is still slow. Since the E-step for image reconstruction naturally decomposes into p separate calculations (one for each element of ), it is natural to update individual pixels (8). <p> FESSLER AND HERO: SAGE 5 III. Example 1 Linear Poisson Measurements The EM method has been used for over a decade to compute ML estimates of radionuclide distributions from to-mographic data, such as that measured by positron emission tomography (PET) <ref> [3, 32] </ref>. In this section we present a brief review of the classical EM algorithm for this problem, and then introduce two SAGE algorithms. The second SAGE algorithm is based on a new hidden-data space, and converges faster than even an accelerated EM algorithm. <p> Assume the variates N nk have independent Poisson distributions: N nk ~ Poissonfa nk k g; (12) where the a nk are nonnegative constants that characterize the system <ref> [3] </ref>. The detectors record emissions from several source locations, so at best one would observe only the sums P p k=1 N nk , rather than each N nk . <p> Thus, Y n ~ Poissonf p X a nk k + r n g: (14) Given realizations fy n g of fY n g, the log-likelihood for this problem is given by <ref> [3] </ref>: log f (y; ) = n=1 where y n () = k=1 We would like to compute the ML estimate ^ from y. <p> Unfortunately, this equation has no analytical solution. A line-search method would require multiple evaluations of (15), which would be expensive| hence the popularity of EM-type algorithms <ref> [3] </ref> that require no line searches. The complete-data space for the classical EM algorithm [3] for this problem is the set of unobservable random vari-ates X 1 = ffN nk g k=1 ; fR n gg N For this complete-data space, the Q function (3) becomes [3, eqn. (4)]: N X <p> Unfortunately, this equation has no analytical solution. A line-search method would require multiple evaluations of (15), which would be expensive| hence the popularity of EM-type algorithms <ref> [3] </ref> that require no line searches. The complete-data space for the classical EM algorithm [3] for this problem is the set of unobservable random vari-ates X 1 = ffN nk g k=1 ; fR n gg N For this complete-data space, the Q function (3) becomes [3, eqn. (4)]: N X p X where [3] N nk = EfN nk jY = y; i g <p> The complete-data space for the classical EM algorithm [3] for this problem is the set of unobservable random vari-ates X 1 = ffN nk g k=1 ; fR n gg N For this complete-data space, the Q function (3) becomes <ref> [3, eqn. (4)] </ref>: N X p X where [3] N nk = EfN nk jY = y; i g = i Maximizing Q 1 (; i ) analytically leads to the following al gorithm: ML-EM Algorithm for Poisson Data for i = 0; 1; : : : f p X a <p> The complete-data space for the classical EM algorithm <ref> [3] </ref> for this problem is the set of unobservable random vari-ates X 1 = ffN nk g k=1 ; fR n gg N For this complete-data space, the Q function (3) becomes [3, eqn. (4)]: N X p X where [3] N nk = EfN nk jY = y; i g = i Maximizing Q 1 (; i ) analytically leads to the following al gorithm: ML-EM Algorithm for Poisson Data for i = 0; 1; : : : f p X a nk i for k = 1; : : <p> This EM algorithm converges globally <ref> [3, 5] </ref> but slowly. The root-convergence factor is very close to 1 (even if p = 1 [5]), since the complete-data space is considerably more informative than the measurements [5, 8, 30]. <p> Thus, the aggregate of all p of the hidden-data spaces is not an admissible hidden-data space for the entire parameter vec tor . Using a similar derivation as in <ref> [3] </ref> (see [8, 30] for details), one can show: Q S k N X (a nk ( k + z k ) + Z nk log (a nk ( k + z k )); where Z nk = EfZ nk jY = y; i g = ( i Maximizing Q S <p> See [30] for a global convergence proof for ML-SAGE-1 and ML-SAGE-2 similar to the proofs in <ref> [3, 17] </ref>. The reader may wonder whether one can also find a better complete-data space for the classical EM algorithm.
Reference: [4] <author> A O Hero and J A Fessler. </author> <title> Asymptotic convergence properties of EM-type algorithms. </title> <type> Technical Report 282, </type> <institution> Comm. and Sign. Proc. Lab., Dept. of EECS, Univ. of Michigan, </institution> <address> Ann Arbor, MI, 48109-2122, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Thus, grouped coordinate-ascent [27] is a spe cial case of the SAGE method, which one can use with index sets S for which ( S ; i ~ S ) is easily maximized. Rather than requiring a strict maximization in (5), one could settle simply for local maxima <ref> [4] </ref>, or for mere increases in S , in analogy with GEM algorithms [1]. These generalizations provide the opportunity to further refine the tradeoff between convergence rate and computation per-iteration. D. <p> E. Monotonicity Let S and X S respectively denote an index set and hidden data space used in a SAGE algorithm. Under mild regularity conditions <ref> [1, 4] </ref>, one can apply Bayes' Theorem to (3) to see that Q S ( S ; ) = f (xjY = y; ) log f (x; S ; ~ S ) dx where L ( S ; ~ S ) = log f (y; S ; ~ S ); 4 <p> Appendix 1 Monotone Convergence in Norm Because the SAGE "algorithm" is so general, a single convergence theorem statement/proof cannot possibly cover all cases of interest (see for example the variety of special cases considered for the classical EM algorithm in [40].) Here we adopt the Taylor expansion approach of <ref> [4] </ref> since it directly illuminates the convergence rate properties and prescribes a region of monotone convergence in norm. However, this general approach has the drawback that it assumes the fixed point lies in the interior of fi. <p> By continuity of the derivatives of S k , one can show <ref> [4] </ref> that the root convergence factor of the subsequence iK is governed by the spectral radius (M S K ( ^ S 1 ; ^ )): Since the spectral radius is bounded above by any matrix norm, the root convergence factor is bounded above by ff. 2 Appendix 2: R + <p> For an EM algorithm, this spectral radius increases towards 1 as the complete-data becomes more informative, i.e., as its Fisher information increases <ref> [1, 4, 5] </ref>. In this section we demonstrate that a similar relationship holds for the convergence rate of a SAGE algorithm.
Reference: [5] <author> J A Fessler, N H Clinthorne, and W L Rogers. </author> <title> On complete data spaces for PET reconstruction algorithms. </title> <journal> IEEE Tr. Nuc. Sci., </journal> <volume> 40(4) </volume> <pages> 1055-61, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: This last point is subtle, but is perhaps one of the most important conclusions of our analyses since it emphasizes the need for careful design of the hidden-data spaces. Less informative hidden-data spaces yield faster convergence, but more informative hidden-data spaces may yield easier M-steps <ref> [5, 8, 30] </ref>. FESSLER AND HERO: SAGE 5 III. Example 1 Linear Poisson Measurements The EM method has been used for over a decade to compute ML estimates of radionuclide distributions from to-mographic data, such as that measured by positron emission tomography (PET) [3, 32]. <p> This EM algorithm converges globally <ref> [3, 5] </ref> but slowly. The root-convergence factor is very close to 1 (even if p = 1 [5]), since the complete-data space is considerably more informative than the measurements [5, 8, 30]. <p> This EM algorithm converges globally [3, 5] but slowly. The root-convergence factor is very close to 1 (even if p = 1 <ref> [5] </ref>), since the complete-data space is considerably more informative than the measurements [5, 8, 30]. We now derive two SAGE algorithms for this problem, both of which use individual pixels for the index sets: S i = fkg, where k = 1+(i modulo p). <p> This EM algorithm converges globally [3, 5] but slowly. The root-convergence factor is very close to 1 (even if p = 1 [5]), since the complete-data space is considerably more informative than the measurements <ref> [5, 8, 30] </ref>. We now derive two SAGE algorithms for this problem, both of which use individual pixels for the index sets: S i = fkg, where k = 1+(i modulo p). <p> For an EM algorithm, this spectral radius increases towards 1 as the complete-data becomes more informative, i.e., as its Fisher information increases <ref> [1, 4, 5] </ref>. In this section we demonstrate that a similar relationship holds for the convergence rate of a SAGE algorithm.
Reference: [6] <author> J A Fessler and A O Hero. </author> <title> Complete-data spaces and generalized EM algorithms. </title> <booktitle> In Proc. IEEE Conf. Acoust. Speech Sig. Proc., </booktitle> <volume> volume 4, </volume> <pages> pages 1-4, </pages> <year> 1993. </year>
Reference: [7] <author> D G Politte and D L Snyder. </author> <title> Corrections for accidental coincidences and attenuation in maximum-likelihood image reconstruction for positron-emission tomography. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 10(1) </volume> <pages> 82-89, </pages> <month> March </month> <year> 1991. </year>
Reference: [8] <author> J A Fessler and A O Hero. </author> <title> New complete-data spaces and faster algorithms for penalized-likelihood emission tomography. </title> <booktitle> In Proc. IEEE Nuc. Sci. Symp. Med. Im. Conf., </booktitle> <volume> volume 3, </volume> <pages> pages 1897-1901, </pages> <year> 1993. </year>
Reference-contexts: Since the E-step for image reconstruction naturally decomposes into p separate calculations (one for each element of ), it is natural to update individual pixels (8). By using the less informative hidden-data spaces described in Section III, we show in <ref> [8, 30] </ref> that the SAGE algorithm converges faster than the GEM algorithm of Hebert and Leahy [10], which in turn is faster than the new method of De Pierro [17]. Thus, for image reconstruction it appears that (8) is best for serial computers. <p> This last point is subtle, but is perhaps one of the most important conclusions of our analyses since it emphasizes the need for careful design of the hidden-data spaces. Less informative hidden-data spaces yield faster convergence, but more informative hidden-data spaces may yield easier M-steps <ref> [5, 8, 30] </ref>. FESSLER AND HERO: SAGE 5 III. Example 1 Linear Poisson Measurements The EM method has been used for over a decade to compute ML estimates of radionuclide distributions from to-mographic data, such as that measured by positron emission tomography (PET) [3, 32]. <p> The second SAGE algorithm is based on a new hidden-data space, and converges faster than even an accelerated EM algorithm. For simplicity we focus in this paper on ML estimation; the penalized version is described in <ref> [8, 30] </ref>. Assume that a radionuclide distribution can be dis-cretized into p pixels with emission rates = [ 0 ; : : : ; p ] 0 . <p> This EM algorithm converges globally [3, 5] but slowly. The root-convergence factor is very close to 1 (even if p = 1 [5]), since the complete-data space is considerably more informative than the measurements <ref> [5, 8, 30] </ref>. We now derive two SAGE algorithms for this problem, both of which use individual pixels for the index sets: S i = fkg, where k = 1+(i modulo p). <p> We found that ML-SAGE-1 converges somewhat faster than ML-EM for well conditioned problems, but the difference is minimal for poorly conditioned problems. The reason is that X S k is still overly informative since the background events are isolated from the parameter being updated (cf (12) and (13)) <ref> [8, 30] </ref>. Therefore, we now introduce a new less informative hidden-data space that associates some of the uncertainty of the background events R n with the particular parameter k as it is updated [8, 30]. <p> overly informative since the background events are isolated from the parameter being updated (cf (12) and (13)) <ref> [8, 30] </ref>. Therefore, we now introduce a new less informative hidden-data space that associates some of the uncertainty of the background events R n with the particular parameter k as it is updated [8, 30]. Whereas the ordinary complete-data space has some intuitive relationship with the underlying image formation physics, this new hidden-data space was developed from a statistical perspective on the problem and its Fisher infor mation. <p> Thus, the aggregate of all p of the hidden-data spaces is not an admissible hidden-data space for the entire parameter vec tor . Using a similar derivation as in [3] (see <ref> [8, 30] </ref> for details), one can show: Q S k N X (a nk ( k + z k ) + Z nk log (a nk ( k + z k )); where Z nk = EfZ nk jY = y; i g = ( i Maximizing Q S k (; <p> A uniform field of random coincidences was added, reflecting a scan with 9% of the total counts due to ran-doms (i.e., P N P N n=1 y n ()), a typical fraction for a PET study. Futher details can be found in <ref> [8, 30] </ref>, including comparisons over a large range of r n 's. Also shown in Fig. 2 is the LINU unbounded line-search acceleration algorithm described by Kaufman [23]. <p> EM update is simultaneous, one must distribute the background events among all pixels, so the terms z k are reduced by a factor of roughly p Since p p is in the hundreds, the change in convergence rate is insignificant, which is consistent with the small reduction in Fisher information <ref> [8, 30] </ref>. Other simultaneous updates [17] similarly do not improve much [30]. Apparently one benefits most from this less informative hidden-data space by using a SAGE method with the parameters grouped into many small index sets.
Reference: [9] <author> P J Green. </author> <title> On use of the EM algorithm for penalized likelihood estimation. </title> <journal> J. Royal Stat. Soc. Ser. B, </journal> <volume> 52(3) </volume> <pages> 443-452, </pages> <year> 1990. </year>
Reference-contexts: IV. Example 2 Linear Gaussian Measurements The Poisson problem has important practical applications, but the nonlinearity of the algorithms complicates a formal analysis of the convergence rates. In this section, we analyze the problem of estimating superimposed linear signals in Gaussian noise <ref> [2, 9] </ref>: Y = a 1 1 + + a p p + * = A + *; (20) where A = [a 1 : : : a p ], and * is additive zero-mean Gaussian noise with covariance , i.e. * ~ N (0; ). <p> For simplicity we consider a quadratic penalty P () = 1 2 0 P so the penalized-likelihood objective function is: () = 2 1 0 P : Such objective functions arise in many inverse problems <ref> [9] </ref>. <p> We first present admissible hidden-data spaces for this problem, derive EM and SAGE algorithms, and then prove that the SAGE algorithm converges faster. Since the mean of Y is linear in , the conventional complete-data <ref> [2, 9] </ref> for the EM algorithm for this problem is also linear in . <p> EM Algorithm The ordinary EM algorithm <ref> [2, 9] </ref> is based on the following choices for the complete-data space: S = f1; : : :; pg, B = diag fa k g, C = 1 p I p , G = (1 0 ~ B = ~ G = W = 0, where diag fg denotes a diagonal
Reference: [10] <author> T Hebert and R Leahy. </author> <title> A Bayesian reconstruction algorithm for emission tomography using a Markov random field prior. </title> <booktitle> In Proc. SPIE 1092, Med. Im. III: Im. Proc., </booktitle> <pages> pages 458-4662, </pages> <year> 1989. </year>
Reference-contexts: By using the less informative hidden-data spaces described in Section III, we show in [8, 30] that the SAGE algorithm converges faster than the GEM algorithm of Hebert and Leahy <ref> [10] </ref>, which in turn is faster than the new method of De Pierro [17]. Thus, for image reconstruction it appears that (8) is best for serial computers.
Reference: [11] <author> T Hebert and R Leahy. </author> <title> A generalized EM algorithm for 3-d Bayesian reconstruction from Poisson data using Gibbs priors. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 8(2) </volume> <pages> 194-202, </pages> <month> June </month> <year> 1989. </year>
Reference: [12] <author> J A Fessler, N H Clinthorne, and W L Rogers. </author> <title> Regularized emission image reconstruction using imperfect side information. </title> <journal> IEEE Tr. Nuc. Sci., </journal> <volume> 39(5) </volume> <pages> 1464-71, </pages> <month> October </month> <year> 1992. </year>
Reference: [13] <author> K Lange. </author> <title> Convergence of EM image reconstruction algorithms with Gibbs smoothing. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 9(4) </volume> <pages> 439-446, </pages> <address> De-cember 1990. Corrections, </address> <month> June </month> <year> 1991. </year>
Reference: [14] <author> B W Silverman, M C Jones, J D Wilson, and D W Nychka. </author> <title> A smoothed EM approach to indirect estimation problems, with particular reference to stereology and emission tomography. </title> <journal> J. Royal Stat. Soc. Ser. B, </journal> <volume> 52(2) </volume> <pages> 271-324, </pages> <year> 1990. </year>
Reference: [15] <author> A R De Pierro. </author> <title> A generalization of the EM algorithm for maximum likelihood estimates from incomplete data. </title> <type> Technical Report MIPG119, </type> <institution> Med. Im. Proc. Group, Dept. of Radiol., Univ. of Pennsylvania, </institution> <month> February </month> <year> 1987. </year>
Reference-contexts: These four choices lead to different tradeoffs between convergence rate and ability to parallelize. A "red-black" grouping was used in a modified EM algorithm in <ref> [15] </ref> to address the M-step coupling introduced by the smoothness penalties. However, those authors recently concluded [16] that a new simultaneous-update algorithm by De Pierro [17] is preferable. <p> VI. Acknowledgement The first author gratefully acknowledges helpful discussions on the superimposed signals application with Yoram Bresler and Sze-Fong Yau. The authors thank the reviewers for helpful suggestions, including references to [18] and <ref> [15] </ref>.
Reference: [16] <author> G T Herman, </author> <title> A R De Pierro, and N Gai. On methods for maximum a posteriori image reconstruction with a normal prior. </title> <journal> J. Visual Comm. Im. Rep., </journal> <volume> 3(4) </volume> <pages> 316-324, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: These four choices lead to different tradeoffs between convergence rate and ability to parallelize. A "red-black" grouping was used in a modified EM algorithm in [15] to address the M-step coupling introduced by the smoothness penalties. However, those authors recently concluded <ref> [16] </ref> that a new simultaneous-update algorithm by De Pierro [17] is preferable. Those methods use the same complete-data space as in the conventional EM algorithm for image reconstruction [3], so the convergence rate is still slow.
Reference: [17] <author> A R De Pierro. </author> <title> A modified expectation maximization algorithm for penalized likelihood estimation in emission tomography. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 14(1) </volume> <pages> 132-137, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: A "red-black" grouping was used in a modified EM algorithm in [15] to address the M-step coupling introduced by the smoothness penalties. However, those authors recently concluded [16] that a new simultaneous-update algorithm by De Pierro <ref> [17] </ref> is preferable. Those methods use the same complete-data space as in the conventional EM algorithm for image reconstruction [3], so the convergence rate is still slow. <p> By using the less informative hidden-data spaces described in Section III, we show in [8, 30] that the SAGE algorithm converges faster than the GEM algorithm of Hebert and Leahy [10], which in turn is faster than the new method of De Pierro <ref> [17] </ref>. Thus, for image reconstruction it appears that (8) is best for serial computers. As noted by the reviewers, for image restoration problems with spatially-invariant systems, one can compute the E-step of the conventional EM algorithm using fast Fourier transforms (FFTs). <p> See [30] for a global convergence proof for ML-SAGE-1 and ML-SAGE-2 similar to the proofs in <ref> [3, 17] </ref>. The reader may wonder whether one can also find a better complete-data space for the classical EM algorithm. <p> Other simultaneous updates <ref> [17] </ref> similarly do not improve much [30]. Apparently one benefits most from this less informative hidden-data space by using a SAGE method with the parameters grouped into many small index sets. An alternative to SAGE is the coordinate-wise sequential Newton-Raphson updates recently proposed by Bouman and Sauer [19].
Reference: [18] <author> M Abdalla and J W Kay. </author> <title> Edge-preserving image restoration. </title> <editor> In P Barone, A Frigessi, and M Piccioni, editors, </editor> <title> Stochastic Models, Statistical Methods, and Algorithms in Im. Analysis, </title> <booktitle> volume 74 of Lecture Notes in Statistics, </booktitle> <pages> pages 1-13. </pages> <publisher> Springer, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: TRANSACTIONS ON SIGNAL PROCESSING, VOL. 42, NO. 10, PP. 2664-77, OCT. 1994 for which there are at least four natural choices for the index sets: 1) the entire image, 2) individual pixels, i.e., S i = f1 + (i modulo p)g; (8) (this was used in the ICM-EM algorithm of <ref> [18] </ref>), 3) grouping by rows or by columns, and 4) "red-black" type order-ings. These four choices lead to different tradeoffs between convergence rate and ability to parallelize. A "red-black" grouping was used in a modified EM algorithm in [15] to address the M-step coupling introduced by the smoothness penalties. <p> ML-SAGE-1 is the unregularized special case of the "ICM-EM" algorithm of <ref> [18] </ref>; a local convergence result for ICM-EM was mentioned in [18]. We found that ML-SAGE-1 converges somewhat faster than ML-EM for well conditioned problems, but the difference is minimal for poorly conditioned problems. <p> ML-SAGE-1 is the unregularized special case of the "ICM-EM" algorithm of <ref> [18] </ref>; a local convergence result for ICM-EM was mentioned in [18]. We found that ML-SAGE-1 converges somewhat faster than ML-EM for well conditioned problems, but the difference is minimal for poorly conditioned problems. <p> VI. Acknowledgement The first author gratefully acknowledges helpful discussions on the superimposed signals application with Yoram Bresler and Sze-Fong Yau. The authors thank the reviewers for helpful suggestions, including references to <ref> [18] </ref> and [15].
Reference: [19] <author> C Bouman and K Sauer. </author> <title> Fast numerical methods for emission and transmission tomographic reconstruction. </title> <booktitle> In Proc. 27th Conf. Info. Sci. Sys., </booktitle> <publisher> Johns Hopkins, </publisher> <pages> pages 611-616, </pages> <year> 1993. </year>
Reference-contexts: Apparently one benefits most from this less informative hidden-data space by using a SAGE method with the parameters grouped into many small index sets. An alternative to SAGE is the coordinate-wise sequential Newton-Raphson updates recently proposed by Bouman and Sauer <ref> [19] </ref>. That method is not guaranteed to be monotonic, but when it converges it might do so somewhat faster than SAGE since it is even greedier.
Reference: [20] <author> C A Bouman and K Sauer. </author> <title> A unified approach to statistical tomography using coordinate descent optimization. </title> <journal> IEEE Tr. Im. Proc., </journal> <volume> 5(3) </volume> <pages> 480-92, </pages> <month> March </month> <year> 1996. </year>
Reference: [21] <author> T A Louis. </author> <title> Finding the observed information matrix when using the EM algorithm. </title> <journal> J. Royal Stat. Soc. Ser. B, </journal> <volume> 44(2) </volume> <pages> 226-233, </pages> <year> 1982. </year>
Reference: [22] <author> R M Lewitt and G Muehllehner. </author> <title> Accelerated iterative reconstruction for positron emission tomography based on the EM algorithm for maximum likelihood estimation. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 5(1) </volume> <pages> 16-22, </pages> <month> March </month> <year> 1986. </year>
Reference: [23] <author> L Kaufman. </author> <title> Implementing and accelerating the EM algorithm for positron emission tomography. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 6(1) </volume> <pages> 37-51, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: Futher details can be found in [8, 30], including comparisons over a large range of r n 's. Also shown in Fig. 2 is the LINU unbounded line-search acceleration algorithm described by Kaufman <ref> [23] </ref>. <p> However, for the few cases we have tested, we have not observed any improvement in convergence rates using multiple sub-iterations. Although further investigation of the tradeoffs available is needed, including comparisons with possibly super-linear methods such as preconditioned conjugate gradient <ref> [23, 34] </ref>, it appears that the statistical perspective inherent to the SAGE method is a useful addition to conventional numerical tools. IV. Example 2 Linear Gaussian Measurements The Poisson problem has important practical applications, but the nonlinearity of the algorithms complicates a formal analysis of the convergence rates.
Reference: [24] <author> I Meilijson. </author> <title> A fast improvement to the EM algorithm on its own terms. </title> <journal> J. Royal Stat. Soc. Ser. B, </journal> <volume> 51(1) </volume> <pages> 127-138, </pages> <year> 1989. </year>
Reference: [25] <author> M Jamshidian and R I Jennrich. </author> <title> Conjugate Gradient Acceleration of the EM Algorithm. </title> <journal> J. Am. Stat. Ass., </journal> <volume> 88(421) </volume> <pages> 221-228, </pages> <year> 1993. </year> <journal> 14 IEEE TRANSACTIONS ON SIGNAL PROCESSING, </journal> <volume> VOL. 42, NO. 10, </volume> <pages> PP. 2664-77, </pages> <month> OCT. </month> <title> 1994 ~ S S - X S - C - Y Fig. 1. Representing the observed data Y as the output of a possibly noisy channel C whose input is the hidden-data X S </title> . 
Reference: [26] <author> K Sauer and C Bouman. </author> <title> A local update strategy for iterative reconstruction from projections. </title> <journal> IEEE Tr. Sig. Proc., </journal> <volume> 41(2) </volume> <pages> 534-548, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: sooner than both ML-EM and ML-LINU algorithm 4 . 4 Fast convergence is clearly desirable for regularized objective functions, but we advise caution when using "stopping rules" in conjunction with coordinate-based algorithms for the unregularized case, since for such algorithms the high spatial frequencies converge faster than the low frequencies <ref> [26] </ref>.
Reference: [27] <author> W H Press, B P Flannery, </author> <title> S A Teukolsky, and W T Vetterling. Numerical recipes in C. </title> <publisher> Cambridge Univ. Press, </publisher> <year> 1988. </year>
Reference-contexts: Note that if for some index set S one chooses X S = Y , then for that S one sees from (3) and (4) that S ( S ; i ) = ( S ; i ~ S ). Thus, grouped coordinate-ascent <ref> [27] </ref> is a spe cial case of the SAGE method, which one can use with index sets S for which ( S ; i ~ S ) is easily maximized.
Reference: [28] <author> M Segal and E Weinstein. </author> <title> The cascade EM algorithm. </title> <journal> Proc. IEEE, </journal> <volume> 76(10) </volume> <pages> 1388-1390, </pages> <month> October </month> <year> 1988. </year>
Reference: [29] <author> J A Fessler. </author> <title> Penalized weighted least-squares image reconstruction for positron emission tomography. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 13(2) </volume> <pages> 290-300, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: In the few examples we have tried in image reconstruction, the additional greediness was not beneficial. (This is consistent with the benefits of under-relaxation for coordinate-ascent analyzed in <ref> [29] </ref>.) In other applications however, such sub-iterations may improve the convergence rate, and may be computa-tionally advantageous over line-search methods that require analogous sub-iterations applied directly to . 4 IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 42, NO. 10, PP. 2664-77, OCT. 1994 for which there are at least four natural choices <p> The image was a 80fi110 discretization of a central slice of the digital 3D Hoffman brain phantom (2mm pixel size). The sino-gram size was 70 radial bins (3mm wide) by 100 angles. A 900000 count noisy projection was generated using (6mm wide) strip-integrals for fa nk g <ref> [29] </ref> including the effects of nonuniform head attenuation and nonuniform detector efficiency. A uniform field of random coincidences was added, reflecting a scan with 9% of the total counts due to ran-doms (i.e., P N P N n=1 y n ()), a typical fraction for a PET study.
Reference: [30] <author> J A Fessler and A O Hero. </author> <title> Space-alternating generalized EM algorithms for penalized maximum-likelihood image reconstruction. </title> <type> Technical Report 286, </type> <institution> Comm. and Sign. Proc. Lab., Dept. of EECS, Univ. of Michigan, </institution> <address> Ann Arbor, MI, 48109-2122, </address> <month> Febru-ary </month> <year> 1994. </year>
Reference-contexts: Since the E-step for image reconstruction naturally decomposes into p separate calculations (one for each element of ), it is natural to update individual pixels (8). By using the less informative hidden-data spaces described in Section III, we show in <ref> [8, 30] </ref> that the SAGE algorithm converges faster than the GEM algorithm of Hebert and Leahy [10], which in turn is faster than the new method of De Pierro [17]. Thus, for image reconstruction it appears that (8) is best for serial computers. <p> This last point is subtle, but is perhaps one of the most important conclusions of our analyses since it emphasizes the need for careful design of the hidden-data spaces. Less informative hidden-data spaces yield faster convergence, but more informative hidden-data spaces may yield easier M-steps <ref> [5, 8, 30] </ref>. FESSLER AND HERO: SAGE 5 III. Example 1 Linear Poisson Measurements The EM method has been used for over a decade to compute ML estimates of radionuclide distributions from to-mographic data, such as that measured by positron emission tomography (PET) [3, 32]. <p> The second SAGE algorithm is based on a new hidden-data space, and converges faster than even an accelerated EM algorithm. For simplicity we focus in this paper on ML estimation; the penalized version is described in <ref> [8, 30] </ref>. Assume that a radionuclide distribution can be dis-cretized into p pixels with emission rates = [ 0 ; : : : ; p ] 0 . <p> This EM algorithm converges globally [3, 5] but slowly. The root-convergence factor is very close to 1 (even if p = 1 [5]), since the complete-data space is considerably more informative than the measurements <ref> [5, 8, 30] </ref>. We now derive two SAGE algorithms for this problem, both of which use individual pixels for the index sets: S i = fkg, where k = 1+(i modulo p). <p> We found that ML-SAGE-1 converges somewhat faster than ML-EM for well conditioned problems, but the difference is minimal for poorly conditioned problems. The reason is that X S k is still overly informative since the background events are isolated from the parameter being updated (cf (12) and (13)) <ref> [8, 30] </ref>. Therefore, we now introduce a new less informative hidden-data space that associates some of the uncertainty of the background events R n with the particular parameter k as it is updated [8, 30]. <p> overly informative since the background events are isolated from the parameter being updated (cf (12) and (13)) <ref> [8, 30] </ref>. Therefore, we now introduce a new less informative hidden-data space that associates some of the uncertainty of the background events R n with the particular parameter k as it is updated [8, 30]. Whereas the ordinary complete-data space has some intuitive relationship with the underlying image formation physics, this new hidden-data space was developed from a statistical perspective on the problem and its Fisher infor mation. <p> Thus, the aggregate of all p of the hidden-data spaces is not an admissible hidden-data space for the entire parameter vec tor . Using a similar derivation as in [3] (see <ref> [8, 30] </ref> for details), one can show: Q S k N X (a nk ( k + z k ) + Z nk log (a nk ( k + z k )); where Z nk = EfZ nk jY = y; i g = ( i Maximizing Q S k (; <p> A uniform field of random coincidences was added, reflecting a scan with 9% of the total counts due to ran-doms (i.e., P N P N n=1 y n ()), a typical fraction for a PET study. Futher details can be found in <ref> [8, 30] </ref>, including comparisons over a large range of r n 's. Also shown in Fig. 2 is the LINU unbounded line-search acceleration algorithm described by Kaufman [23]. <p> See <ref> [30] </ref> for a global convergence proof for ML-SAGE-1 and ML-SAGE-2 similar to the proofs in [3, 17]. The reader may wonder whether one can also find a better complete-data space for the classical EM algorithm. <p> EM update is simultaneous, one must distribute the background events among all pixels, so the terms z k are reduced by a factor of roughly p Since p p is in the hundreds, the change in convergence rate is insignificant, which is consistent with the small reduction in Fisher information <ref> [8, 30] </ref>. Other simultaneous updates [17] similarly do not improve much [30]. Apparently one benefits most from this less informative hidden-data space by using a SAGE method with the parameters grouped into many small index sets. <p> Other simultaneous updates [17] similarly do not improve much <ref> [30] </ref>. Apparently one benefits most from this less informative hidden-data space by using a SAGE method with the parameters grouped into many small index sets. An alternative to SAGE is the coordinate-wise sequential Newton-Raphson updates recently proposed by Bouman and Sauer [19].
Reference: [31] <author> R Boyles. </author> <title> On the convergence of the EM algorithm. </title> <journal> J. Royal Stat. Soc. Ser. B, </journal> <volume> 45(1) </volume> <pages> 47-50, </pages> <year> 1983. </year>
Reference-contexts: F. Convergence For a well behaved objective , the monotonicity property ensures that the sequence f i g will not diverge, but it does not guarantee convergence even to a local maxi mum of . (Some EM algorithms have fixed points that are not local maxima <ref> [1, 31] </ref>.) Therefore, in the appendices we provide additional theorems that give sufficient conditions for convergence in norm, and that characterize the asymptotic convergence rate.
Reference: [32] <author> L A Shepp and Y Vardi. </author> <title> Maximum likelihood reconstruction for emission tomography. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 1(2) </volume> <pages> 113-122, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: FESSLER AND HERO: SAGE 5 III. Example 1 Linear Poisson Measurements The EM method has been used for over a decade to compute ML estimates of radionuclide distributions from to-mographic data, such as that measured by positron emission tomography (PET) <ref> [3, 32] </ref>. In this section we present a brief review of the classical EM algorithm for this problem, and then introduce two SAGE algorithms. The second SAGE algorithm is based on a new hidden-data space, and converges faster than even an accelerated EM algorithm.
Reference: [33] <author> C L Byrne. </author> <title> Iterative image reconstruction algorithms based on cross-entropy minimization. </title> <journal> IEEE Tr. Im. Proc., </journal> <volume> 2(1) </volume> <pages> 96-103, </pages> <month> January </month> <year> 1993. </year> <title> Erratum and addendum: </title> <journal> vol. </journal> <volume> 4, no. 2, </volume> <pages> pp. 226-227, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: was also considerably easier to implement than the bent-line LINU method.) The convergence in norm given by Theorem 3 of Appendix 1 is inapplicable to this Poisson example when the ML estimate has components that are zero, i.e., when the ML estimate lies on the boundary of the nonnegative or-thant <ref> [33] </ref>. See [30] for a global convergence proof for ML-SAGE-1 and ML-SAGE-2 similar to the proofs in [3, 17]. The reader may wonder whether one can also find a better complete-data space for the classical EM algorithm.
Reference: [34] <author> L Kaufman. </author> <title> Maximum likelihood, least squares, and penalized least squares for PET. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 12(2) </volume> <pages> 200-214, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: However, for the few cases we have tested, we have not observed any improvement in convergence rates using multiple sub-iterations. Although further investigation of the tradeoffs available is needed, including comparisons with possibly super-linear methods such as preconditioned conjugate gradient <ref> [23, 34] </ref>, it appears that the statistical perspective inherent to the SAGE method is a useful addition to conventional numerical tools. IV. Example 2 Linear Gaussian Measurements The Poisson problem has important practical applications, but the nonlinearity of the algorithms complicates a formal analysis of the convergence rates.
Reference: [35] <author> J A Fessler. </author> <title> Object-based 3-D reconstruction of arterial trees from a few projections. </title> <type> PhD thesis, </type> <institution> Stanford Univ., Stanford, </institution> <address> CA., </address> <month> August </month> <year> 1990. </year>
Reference-contexts: j : Thus, F S k X = a 0 k 1 a k , which is p times less informative than the EM case, which associates only a fraction 1=p of the noise covariance with each signal. (This provides a statistical interpretation of the modified EM algorithms used in <ref> [35, 36] </ref>.) The above choice for the hidden-data space corresponds to B = a j , C = , ~ B = W = 0, G = I, and ~ G = [a 1 : : : a k1 a k+1 : : : a p ], which substituted into (22)
Reference: [36] <author> D Chazan, Y Stettiner, and D Malah. </author> <title> Optimal multi-path estimation using the EM algorithm for co-channel speech separation. </title> <booktitle> In Proc. IEEE Conf. Acoust. Speech Sig. Proc., </booktitle> <volume> volume 2, </volume> <pages> pages 728-731, </pages> <year> 1993. </year>
Reference-contexts: j : Thus, F S k X = a 0 k 1 a k , which is p times less informative than the EM case, which associates only a fraction 1=p of the noise covariance with each signal. (This provides a statistical interpretation of the modified EM algorithms used in <ref> [35, 36] </ref>.) The above choice for the hidden-data space corresponds to B = a j , C = , ~ B = W = 0, G = I, and ~ G = [a 1 : : : a k1 a k+1 : : : a p ], which substituted into (22)
Reference: [37] <author> D M Young. </author> <title> Iterative solution of large linear systems. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Since an orthogonal projection is nonexpansive, kM k k T 1, which confirms condition 2 of Definition 3. To confirm condition 3, rewrite the SAGE algorithm using (24) as: (i+1)p ^ = [I (D H + L H ) 1 H]( ip ^ ); which is the Gauss-Siedel iteration <ref> [37, p. 72] </ref>. <p> To confirm condition 3, rewrite the SAGE algorithm using (24) as: (i+1)p ^ = [I (D H + L H ) 1 H]( ip ^ ); which is the Gauss-Siedel iteration [37, p. 72]. Condition 3 follows from <ref> [37, p. 109] </ref> since kI (D H + L H ) 1 Hk T = kM p M 1 k T &lt; 1: FESSLER AND HERO: SAGE 9 C.2 EM Algorithm One can use (21) and (23) to show that i+1 ^ = M ( i ^ ); for the EM <p> EM Algorithm One can use (21) and (23) to show that i+1 ^ = M ( i ^ ); for the EM algorithm, where (cf (37)) M = I (pD F + P ) 1 H : Thus the EM algorithm is closely related to the simultaneous overrelaxation (JOR) iteration <ref> [37, p. 72] </ref>. To establish that kM k T &lt; 1 for T = H 1=2 using Theorem 4, we must show that S + S 0 &gt; H, where in this case S = pD F + P . <p> The root-convergence factor [41] of the subsequence f iK g 1 i=0 is given by the spectral radius M S K ( ^ S 1 ; ^ ) ; (35) which is bounded above by ff &lt; 1. Note that by the equivalence of matrix norms <ref> [37, p. 29] </ref>, monotone convergence with respect to the norm k k T implies convergence with respect to any other norm, although probably non-monotonically. <p> We can thus establish that kM S K M S 1 k T &lt; 1 by using the following "splitting matrix" theorem (p. 79 of <ref> [37] </ref> Theorem 4: If H is positive definite and S is invertible, then kI S 1 Hk T &lt; 1 1 S + S 0 &gt; H: (42) From (40), for a SAGE algorithm S = ~ D H + ~ D F + ~ L H , so in light
Reference: [38] <author> I Ziskind and M Wax. </author> <title> Maximum likelihood localization of multiple sources by alternating projection. </title> <journal> IEEE Tr. Acoust. Sp. Sig. Proc., </journal> <volume> 36(10) </volume> <pages> 1553-1560, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Thus this SAGE 10 IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 42, NO. 10, PP. 2664-77, OCT. 1994 algorithm is closely related to the method of alternating projections <ref> [38, 39] </ref>. In particular, if P = 0 and the columns of A are orthogonal, then SAGE = 0 whereas EM 1 1=p, i.e., the SAGE algorithm converges in one iteration, while EM converges very slowly.
Reference: [39] <author> S Kayalar and H L Weinert. </author> <title> Error bounds for the method of alternating projections. </title> <journal> Math Control Signals Systems, </journal> <volume> 1 </volume> <pages> 43-59, </pages> <year> 1988. </year>
Reference-contexts: Thus this SAGE 10 IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 42, NO. 10, PP. 2664-77, OCT. 1994 algorithm is closely related to the method of alternating projections <ref> [38, 39] </ref>. In particular, if P = 0 and the columns of A are orthogonal, then SAGE = 0 whereas EM 1 1=p, i.e., the SAGE algorithm converges in one iteration, while EM converges very slowly.
Reference: [40] <author> C F J Wu. </author> <title> On the convergence properties of the EM algorithm. </title> <journal> Ann. Stat., </journal> <volume> 11(1) </volume> <pages> 95-103, </pages> <year> 1983. </year>
Reference-contexts: Appendix 1 Monotone Convergence in Norm Because the SAGE "algorithm" is so general, a single convergence theorem statement/proof cannot possibly cover all cases of interest (see for example the variety of special cases considered for the classical EM algorithm in <ref> [40] </ref>.) Here we adopt the Taylor expansion approach of [4] since it directly illuminates the convergence rate properties and prescribes a region of monotone convergence in norm. However, this general approach has the drawback that it assumes the fixed point lies in the interior of fi.
Reference: [41] <author> J M Ortega and W C Rheinboldt. </author> <title> Iterative solution of nonlinear equations in several variables. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Therefore, k iK ^ k T converges monotonically to zero with at least linear rate. 2. The root-convergence factor <ref> [41] </ref> of the subsequence f iK g 1 i=0 is given by the spectral radius M S K ( ^ S 1 ; ^ ) ; (35) which is bounded above by ff &lt; 1.
Reference: [42] <author> E Polak. </author> <title> Computational methods in optimization: a unified approach. </title> <publisher> Academic Press, </publisher> <address> Orlando, </address> <year> 1971. </year> <title> Fig. 2. Comparison of log-likelihood increase log f (y; i ) log f(y; 0 ) vs iteration i for ML-EM, ML-LINU, and ML-SAGE-2 algorithms, for image reconstruction from PET measurements with 9% random coincidences. </title> <address> ML-SAGE-2 clearly reaches the asymptote sooner. Fig. </address> <month> 3. </month> <title> Comparison of root-convergence factors for conventional EM algorithm and proposed SAGE algorithm versus complementary angle between subspaces of superimposed signals. The SAGE algorithm has a significantly improved convergence rate. </title>
Reference-contexts: Define z = 4 S 3 2 ^ S ^ ~ S 5 ; d (z) = d ( S ; ) = (r 0 S )(z); then by assumption (iv) we can apply the Taylor formula with remainder <ref> [42] </ref> to expand d (z) about ^ z: d (z) = d ( ^ z) + 0 Since ^ is a fixed point of the SAGE algorithm, by assumption (iii) and (iv), d ( ^ z) = 0.
References-found: 42

