URL: ftp://ftp.cnl.salk.edu/pub/wiskott/publications/WisSej97a-NeuralMapFormation-TRINC9701.ps.gz
Refering-URL: http://www.cnl.salk.edu/~wiskott/Abstracts/WisSej97a.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: wiskott@salk.edu  
Phone: 3  
Title: Objective Functions for Neural Map Formation  
Author: Laurenz Wiskott and Terrence Sejnowski 
Web: http://www.cnl.salk.edu/CNL  
Address: San Diego, CA 92186-5800  La Jolla, CA 92093  
Affiliation: 2 Howard Hughes Medical Institute The Salk Institute for Biological Studies  Department of Biology University of California, San Diego  
Abstract: Institute for Neural Computation Technical Report Series, No. INC-9701, January 1997. University of California, San Diego. La Jolla, CA 92093. Abstract Computational models of neural map formation can be considered on at least three different levels of abstraction: detailed models including neural activity dynamics, weight dynamics which abstract from the the neural activity dynamics by an adiabatic approximation, and objective functions from which weight dynamics may be derived as gradient flows. In this paper we present an example of how an objective function can be derived from detailed non-linear neural dynamics. A systematic investigation reveals how different weight dynamics introduced previously can be derived from objective functions generated from a few prototypical terms. This includes dynamic link matching as a special case of neural map formation. We focus in particular on the role of coordinate transformations to derive different weight dynamics from the same objective function. Coordinate transformations are also important in deriving normalization rules from constraints. Several examples illustrate how objective functions can help in understanding, generating, and comparing different models of neural map formation. The techniques used in this analysis may also be useful in investigating other types of neural dynamics.
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S. </author> <year> (1977). </year> <title> Dynamics of pattern formation in lateral-inhibition type neural fields. </title> <journal> Biol. Cybern., </journal> <volume> 27 </volume> <pages> 77-87. </pages>
Reference: <author> Amari, S. </author> <year> (1980). </year> <title> Topographic organization of nerve fields. </title> <journal> Bulletin of Mathematical Biology, </journal> <volume> 42 </volume> <pages> 339-364. </pages>
Reference-contexts: The original equations are listed as well as the classification in terms of growth rules and normalization rules listed in Table 1. Detailed comments for these models and the model in <ref> (Amari, 1980) </ref> follow below. The latter is not listed in Table 2 because it cannot be interpreted within our objective function framework. <p> Similar models were proposed earlier in (Swin-dale, 1980), though not derived from a receptive-field model, and more recently in (Tanaka, 1991). These approaches and their relationships to our objective functions need to be investigated more systematically. A neural map formation of <ref> (Amari, 1980) </ref> could not be formulated within the objective function framework presented here (cf. Sec. 6.2).
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> An information-maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 1129-1159. </pages>
Reference-contexts: It would be interesting as a next step to consider third-order terms in the objective function and the conditions under which they can be derived from detailed neural dynamics. There may also be an interesting relationship to recent advances in algorithms for independent component analysis <ref> (Bell & Sejnowski, 1995) </ref>, which can be derived from a maximum entropy method and is dominated by higher-order correlations.
Reference: <author> Bienenstock, E. and von der Malsburg, C. </author> <year> (1987). </year> <title> A neural network for invariant pattern recognition. </title> <journal> Europhysics Letters, </journal> <volume> 4(1) </volume> <pages> 121-126. </pages>
Reference: <author> Dirac, P. A. M. </author> <year> (1996). </year> <title> General Theory of Relativity. Princeton landmarks in physics. </title> <publisher> Princeton University Press, </publisher> <address> 41 William Street, Princeton, NJ 08540. </address>
Reference-contexts: This effect of coordinate transformations is known from the general theory of relativity and tensor analysis <ref> (e.g. Dirac, 1996) </ref>. The gradient of a potential (or objective function) is a covariant vector, which adds the 8 factor dw i =dv i through the transformation from W to V .
Reference: <author> Ermentrout, G. B. and Cowan, J. D. </author> <year> (1979). </year> <title> A mathematical theory of visual hallucination patterns. </title> <journal> Biological Cybernetics, </journal> <volume> 34(3) </volume> <pages> 137-150. </pages>
Reference: <author> Erwin, E., Obermayer, K., and Schulten, K. </author> <year> (1995). </year> <title> Models of orientation and ocular dominance columns in the visual cortex: A critical comparison. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 425-468. </pages>
Reference: <author> Ginzburg, I. and Sompolinsky, H. </author> <year> (1994). </year> <title> Theory of correlations in stochastic neural networks. </title> <journal> Physical Review E, </journal> <volume> 50(4) </volume> <pages> 3171-3191. </pages> <note> 25 Goodhill, </note> <author> G. J. </author> <year> (1993). </year> <title> Topography and ocular dominance: A model exploring positive correlations. </title> <journal> Biol. Cybern., </journal> <volume> 69 </volume> <pages> 109-118. </pages>
Reference: <author> Goodhill, G. J., Finch, S., and Sejnowski, T. J. </author> <year> (1996). </year> <title> Optimizing cortical mappings. </title> <editor> In Touret-zky, D., Mozer, M., and Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 330-336, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> H aussler, A. F. and von der Malsburg, C. </author> <year> (1983). </year> <title> Development of retinotopic projections | An analytical treatment. </title> <journal> J. Theor. Neurobiol., </journal> <volume> 2 </volume> <pages> 47-73. </pages>
Reference-contexts: A suitable objective function is H (w) = 2 ij (cf. Eq. 15), since it yields _w i = @H (w)=@w i . A dynamics that cannot be generated by an objective function directly is _w i = w i j as used in <ref> (H aussler & von der Malsburg, 1983) </ref>, since for i 6= j we obtain @ _w i =@w j = w i D ij 6= w j D ji = @ _w j =@w i , and _w i is not curl-free. <p> However, difficulties arise when interfering constraints are combined, i.e. different constraints that affect the same weights. This type of formulation is required for certain types of analyses <ref> (e.g. H aussler & von der Malsburg, 1983) </ref>. <p> The limitation constraint can be waived for systems with positive weights and multiplicative normalization rules (Konen & von der Malsburg, 1993; Obermayer et al., 1990; von der Malsburg, 1973). Only few systems contain the linear term L, which can be used for dynamic link matching. In <ref> (H aussler & von der Malsburg, 1983) </ref> the linear term was introduced for analytical convenience and does not differentiate between different links.
Reference: <author> Horst, R., Pardalos, P. M., and Thoai, N. V. </author> <year> (1995). </year> <title> Introduction to Global Optimization, volume 3 of Nonconvex Optimization and Its Applications. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, The Netherlands. </address>
Reference-contexts: These problems are known to be NP-complete. However, there is a large literature on algorithms that efficiently solve special cases or that find good approximate solutions in polynomial time <ref> (e.g. Horst et al., 1995) </ref>. Many related objective functions are only defined for maps for which each input neuron terminates on exactly one output neuron with weight 1, which makes the index t = t () a function of index .
Reference: <author> Kohonen, T. </author> <year> (1982). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biol. Cybern., </journal> <volume> 43 </volume> <pages> 59-69. </pages>
Reference-contexts: Although they may be conceptionally similar to those based on neural activities, they can differ significantly in the detailed mathematical formulation. Nor do we consider models that treat the input layer as a low-dimensional space, say 2-dimensional for the retina, from which input vectors are drawn, <ref> (e.g. Kohonen, 1982) </ref>. The output neurons then receive only two synapses per neuron, one for each input dimension. The dynamic link matching model (e.g. Bienenstock & von der Malsburg, 1987; Konen et al., 1994) is a form of neural map formation that has been developed for pattern recognition. <p> Appendix A Probabilistic Blob Model Consider the activity model of Obermayer et al. (1990) as an abstraction of the neural activity dynamics in Section 2.1 (Eqs. 1, 2). Obermayer et al. use a high-dimensional version of the self-organizing feature map algorithm <ref> (Kohonen, 1982) </ref>. A blob B 0 0 is located at a random position 0 in the input layer and the input i t 0 ( 0 ) received by the output neurons is calculated as in Equation (7).
Reference: <author> Konen, W., Maurer, T., and von der Malsburg, C. </author> <year> (1994). </year> <title> A fast dynamic link matching algorithm for invariant pattern recognition. </title> <booktitle> Neural Networks, </booktitle> 7(6/7):1019-1030. 
Reference: <author> Konen, W. and von der Malsburg, C. </author> <year> (1993). </year> <title> Learning to generalize from single examples in the dynamic link architecture. </title> <journal> Neural Computation, </journal> <volume> 5(5) </volume> <pages> 719-735. </pages>
Reference-contexts: Coordinate transformations can be used to generate different models that are equivalent in terms of their objective function. Consider the system in <ref> (Konen & von der Malsburg, 1993) </ref>.
Reference: <author> Lades, M., Vorbr uggen, J. C., Buhmann, J., Lange, J., von der Malsburg, C., W urtz, R. P., and Konen, W. </author> <year> (1993). </year> <title> Distortion invariant object recognition in the dynamic link architecture. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(3) </volume> <pages> 300-311. </pages>
Reference-contexts: The first term now directly favors links with high similarity values. This may be advantageous because it allows better control over the influence of the topography vs. the feature similarity term. Furthermore, this objective function is more closely related to the similarity function of elastic graph matching in <ref> (Lades et al., 1993) </ref>, which has been developed as an algorithmic abstraction of dynamic link matching (see Sec. 6.7). 6.6 Soft vs. Hard Competitive Normalization Miller & MacKay (1994) have analyzed the role of normalization rules for neural map formation. <p> Thus, the objective function Q is the typical term for preserving topography in other contexts as well. Elastic graph matching is an algorithmic counterpart to dynamic link matching and has been used for applications such as object and face recognition <ref> (Lades et al., 1993) </ref>.
Reference: <author> Linsker, R. </author> <year> (1986). </year> <title> From basic network principles to neural architecture: Emergence of orientation columns. </title> <journal> Ntl. Acad. Sci. USA, </journal> <volume> 83 </volume> <pages> 8779-8783. </pages>
Reference-contexts: The difference for the latter would be an additional constant, which can always be compensated for in the growth rule. The correlation model in <ref> (Linsker, 1986) </ref> differs from the linear one introduced here in two respects. The input (Eq. 20) has an additional constant term and correlations are defined by subtracting positive constants from the activities. However, it can be shown that correlations in the model in (Linsker, 1986) are a linear combination of a <p> The correlation model in <ref> (Linsker, 1986) </ref> differs from the linear one introduced here in two respects. The input (Eq. 20) has an additional constant term and correlations are defined by subtracting positive constants from the activities. However, it can be shown that correlations in the model in (Linsker, 1986) are a linear combination of a constant and the terms of Equations (22, 23). 4 Objective Functions In general, there is no systematic way of finding an objective function for a particular dynamical system, but it is possible to determine whether there exists an objective function. <p> As von der Malsburg (1995) has pointed out, this is appropriate only for a subset of phenomena of neural map formation, such as retinotopy and ocular dominance. Although orientation tuning can arise by spontaneous symmetry breaking <ref> (e.g. Linsker, 1986) </ref>, a full understanding of the self-organization of orientation selectivity and other phenomena may require taking higher-order correlations into account.
Reference: <author> MacKay, D. J. C. and Miller, K. D. </author> <year> (1990). </year> <title> Analysis of Linsker's simulations of hebbian rules. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 173-187. </pages>
Reference-contexts: An important tool for both methods is the objective function (or energy function) from which the dynamics can be generated as a gradient flow. The objective value (or energy) can be used to estimate which weight configurations would be more likely to arise from the dynamics <ref> (e.g. MacKay & Miller, 1990) </ref>. In computer simulations the objective function is maximized (or the energy function is minimized) numerically in order to find stable solutions of the dynamics (e.g. Linsker, 1986; Bienenstock & von der Malsburg, 1987). <p> The role of parameters and effective lateral connectivities might be investigated analytically for a variety of models by means of objective functions, similar to the approach sketched in Section 6.3 or the one taken in <ref> (MacKay & Miller, 1990) </ref>. We have considered here only three levels of abstraction: detailed neural dynamics, abstract weight dynamics, and the objective function. There exist even higher levels of abstraction and the relationship between our objective functions and these more abstract models should be explored.
Reference: <author> Meister, M., Wong, R. O. L., Baylor, D. A., and Shatz, C. J. </author> <year> (1991). </year> <title> Synchronous bursts of action potentials in ganglion cells of the developing mammalian retina. </title> <journal> Science, </journal> <volume> 252 </volume> <pages> 939-943. </pages>
Reference-contexts: A solution to this problem might be found by examining propagating activity patterns in the input as well as the output layer, such as traveling waves (Triesch, 1995) or running blobs (Wiskott & von der Malsburg, 1996). Waves and blobs of activity have been observed in the developing retina <ref> (Meister et al., 1991) </ref>. If the waves or blobs have the same intrinsic velocity in the two layers, they would tend to generate metric maps, regardless of the scaling factor induced by the normalization rules.
Reference: <author> Miller, K. D. </author> <year> (1990). </year> <title> Derivation of linear Hebbian equations from nonlinear Hebbian model of synaptic plasticity. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 321-333. </pages>
Reference-contexts: An important tool for both methods is the objective function (or energy function) from which the dynamics can be generated as a gradient flow. The objective value (or energy) can be used to estimate which weight configurations would be more likely to arise from the dynamics <ref> (e.g. MacKay & Miller, 1990) </ref>. In computer simulations the objective function is maximized (or the energy function is minimized) numerically in order to find stable solutions of the dynamics (e.g. Linsker, 1986; Bienenstock & von der Malsburg, 1987). <p> The role of parameters and effective lateral connectivities might be investigated analytically for a variety of models by means of objective functions, similar to the approach sketched in Section 6.3 or the one taken in <ref> (MacKay & Miller, 1990) </ref>. We have considered here only three levels of abstraction: detailed neural dynamics, abstract weight dynamics, and the objective function. There exist even higher levels of abstraction and the relationship between our objective functions and these more abstract models should be explored.
Reference: <author> Miller, K. D., Keller, J. B., and Stryker, M. P. </author> <year> (1989). </year> <title> Ocular dominance column development: Analysis and simulation. </title> <journal> Science, </journal> <volume> 245 </volume> <pages> 605-245. </pages>
Reference-contexts: A more systematic treatment of the normalization rules could replace this inconsistent rule. Another inconsistency is that weights that reach their upper or lower limit become frozen, i.e. fixed at the limit value. With some exception this seems to have little effect on the resulting maps <ref> (Miller et al., 1989, Note 23.) </ref>. Thus this model has only two minor inconsistencies, which could be modified to make the system consistent. Limitation constraints enter the weight dynamics in two forms, I ff and I ff . <p> Goodhill (1993): This model is based on an algorithmic blob model and the linear correlation model is only an approximation (cf. Appendix A). As the model in <ref> (Miller et al., 1989) </ref> this model uses an inconsistent normalization rule as a backup and it freezes weights that reach their upper or lower limit. In addition it uses an inconsistent normalization rule for the input neurons.
Reference: <author> Miller, K. D. and MacKay, D. J. C. </author> <year> (1994). </year> <title> The role of constraints in Hebbian learning. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 100-126. </pages>
Reference: <author> Nowlan, S. J. </author> <year> (1990). </year> <title> Maximum likelihood competitive learning. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 574-582, </pages> <address> San Mateo, CA 94403. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Obermayer, K., Ritter, H., and Schulten, K. </author> <year> (1990). </year> <title> Large-scale simulations of self-organizing neural networks on parallel computers: Application to biological modelling. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 381-404. </pages>
Reference-contexts: Although it is plausible that such a probabilistic blob location could be approximated by noise in the output layer, it is difficult to develop a concrete model. For a similar but more algorithmic activity model <ref> (Obermayer et al., 1990) </ref> an exact noise model for the probabilistic blob location can be formulated (see Appendix A).
Reference: <author> Sejnowski, T. J. </author> <year> (1976). </year> <title> On the stochastic dynamics of neuronal interaction. </title> <journal> Biol. Cybern., </journal> <volume> 22 </volume> <pages> 203-211. </pages>
Reference: <author> Sejnowski, T. J. </author> <year> (1977). </year> <title> Storing covariance with nonlinearly interacting neurons. </title> <journal> J. Math. Biology, </journal> <volume> 4 </volume> <pages> 303-321. </pages>
Reference-contexts: = t 0 0 X A ij w j : (22) Assuming a linear correlation function (ha ; ff (a 0 + a 00 )i = ffha ; a 0 i + ffha ; a 00 i with a real constant ff) such as the average product or the covariance <ref> (Sejnowski, 1977) </ref>, the correlation between input and output neurons is ha t ; a i = t 0 0 X D ij w j ; (23) Note that i = f; t g, j = f 0 ; t 0 g, A ij = A ji = D tt 0 A
Reference: <author> Swindale, N. V. </author> <year> (1980). </year> <title> A model for the formation of ocular domance stripes. </title> <journal> Proc. R. Soc. Lond. B, </journal> <volume> 208 </volume> <pages> 243-264. </pages>
Reference: <author> Swindale, N. V. </author> <year> (1996). </year> <title> The development of topography in the visual cortex: A review of models. Network: </title> <journal> Comput. in Neural Syst., </journal> <volume> 7(2) </volume> <pages> 161-247. </pages> <note> 26 Tanaka, </note> <author> S. </author> <year> (1990). </year> <title> Theory of self-organization of cortical maps: Mathematical framework. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 625-640. </pages>
Reference: <author> Tanaka, S. </author> <year> (1991). </year> <title> Theory of ocular dominance column formation. </title> <journal> Biol. Cybern., </journal> <volume> 64 </volume> <pages> 263-272. </pages>
Reference-contexts: The only variables of this energy function were the orientations of the receptive fields, an abstraction from the connectivity. Similar models were proposed earlier in (Swin-dale, 1980), though not derived from a receptive-field model, and more recently in <ref> (Tanaka, 1991) </ref>. These approaches and their relationships to our objective functions need to be investigated more systematically. A neural map formation of (Amari, 1980) could not be formulated within the objective function framework presented here (cf. Sec. 6.2).

Reference: <author> Whitelaw, D. J. and Cowan, J. D. </author> <year> (1981). </year> <title> Specificity and plasticity of retinotectal connections: A computational model. </title> <journal> J. Neuroscience, </journal> <volume> 1(12) </volume> <pages> 1369-1387. </pages>
Reference: <author> Willshaw, D. J. and von der Malsburg, C. </author> <year> (1976). </year> <title> How patterned neural connections can be set up by self-organization. </title> <booktitle> Proc. R. </booktitle> <publisher> Soc. </publisher> <address> London, B194:431-445. </address>
Reference: <author> Wiskott, L. and von der Malsburg, C. </author> <year> (1996). </year> <title> Face recognition by dynamic link matching. </title> <editor> In Sirosh, J., Miikkulainen, R., and Choe, Y., editors, </editor> <title> Lateral Interactions in the Cortex: Structure and Function, </title> <booktitle> chapter 11. The UTCS Neural Networks Research Group, </booktitle> <address> Austin, TX, </address> <note> http://www.cs.utexas.edu/users/nn/web-pubs/htmlbook96/. Electronic book, ISBN 0-9647060-0-8. 27 </note>
Reference-contexts: Only two types of constraints have been used, limitation I and normalization N (or Z). These are not the only ways in which constraints could be enforced. In <ref> (Wiskott & von der Malsburg, 1996) </ref>, for example, a normalization rule is used that affects a group of weights if single weights grow beyond their limits. <p> A solution to this problem might be found by examining propagating activity patterns in the input as well as the output layer, such as traveling waves (Triesch, 1995) or running blobs <ref> (Wiskott & von der Malsburg, 1996) </ref>. Waves and blobs of activity have been observed in the developing retina (Meister et al., 1991).
References-found: 31

