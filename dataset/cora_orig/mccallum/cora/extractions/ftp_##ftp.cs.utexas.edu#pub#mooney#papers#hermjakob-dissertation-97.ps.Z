URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/hermjakob-dissertation-97.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/nl.html
Root-URL: http://www.cs.utexas.edu
Title: by  
Author: Ulf Hermjakob 
Date: 1997  
Note: Copyright  
Abstract-found: 0
Intro-found: 1
Reference: <author> Allen, J. F. </author> <year> (1995). </year> <booktitle> Natural Language Understanding (2nd Ed.). </booktitle> <address> Benjamin/Cummings, Menlo Park, CA. </address>
Reference-contexts: In natural language understanding, the structural descriptions built by the parser typically include a tree reflecting the syntactic structure of the text, but can also contain semantic and other information. A good introductory textbook on natural language understanding is <ref> (Allen, 1995) </ref>. 1 (13) The statue of liberty in New York, which was sent to the United States as a gift from France, was dedicated in 1886. (14) The airline said that it would report a loss for the first quarter, but that it would be less than $100 million.
Reference: <author> Aone, C., & Bennett, S. W. </author> <year> (1995). </year> <title> Evaluating automated and manual acquisition of anaphora resolution strategies. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 122-129 Cambridge, MA. </address>
Reference-contexts: motivated representation which may or may not involve the direct application of contrastive linguistic knowledge." 2 A number of researchers have already applied machine learning techniques to various NLP tasks like accent restoration by Yarowsky (1994), who achieves 99% accuracy, relative pronoun disambiguation (Cardie, 1992, 1993, 1996), (Japanese) anaphora resolution <ref> (Aone & Bennett, 1995) </ref>, part of speech tagging (Weischedel & al., 1993; Brill, 1995; Daelemans, Zavrel, Berck, & Gillis, 1996), where tagging accuracies between 96% and 97% are achieved, cue phrase classification (Siegel & McKeown, 1994; Litman, 1996), and word sense disambiguation (Yarowsky, 1992, 1995; Ng & Lee, 1996; Mooney, 1996). <p> Relatively simple heuristics were sufficient to cover all anaphora cases in the 48 training sentences. In a more advanced anaphora resolver, these heuristics have to be elaborated further. With enough complexity, machine learning might again prove useful in deciding which antecedent to pick. Previous work in this area includes <ref> (Aone & Bennett, 1995) </ref>, which describes an anaphora resolution system trained on examples from Japanese newspaper articles. 5.5 Parsing Safeguards The parser contains a few safeguards that suppress the attempt to execute unexecutable parse actions, detect potential endless loops and handle incomplete parses.
Reference: <author> Bates, M. </author> <year> (1978). </year> <title> The theory and practise of augmented transition networks. </title> <editor> In Bloc, L. (Ed.), </editor> <title> Natural Language Communication with Computers. </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference-contexts: Traditional approaches try to master the complexity of parse grammars with hand-crafted rules, as in augmented transition networks <ref> (Bates, 1978) </ref>, definite clause grammars (Pereira & Warren, 1980), lexical functional grammars (Kaplan & Bresnan, 1982), functional unification grammars (Kay, 1982), tree-adjoining grammars 3 (Joshi, 1985), or generalized phrase structure grammars (Gazdar, Klein, Pullum, & Sag, 1982).
Reference: <author> Berger, A., & al. </author> <year> (1994). </year> <title> The Candide system for machine translation. </title> <booktitle> In Proceedings of ARPA Workshop on Human Language Technologies. </booktitle>
Reference-contexts: Due to lexical restrictions, our average sentence length (17.1) is below the one used in SPATTER and BLD for a similar comparable sentence length range. While the Penn Treebank leaves many phrases such as "the New York Stock Exchange" 4 In <ref> (Berger & al., 1994) </ref>, the authors report that their "in-house evaluation methodology consists of fully-automatic translation of 100 sentences of 15 words or less". 103 System Spatter Spatter BLD Contex Contex Training Sentences 40,000 40,000 40,000 64 256 Background Knowledge little little little much much Test Sentence Length Range 4-25 4-40
Reference: <author> Black, E., Jelineck, F., Lafferty, J., Magerman, D., Mercer, R., & Roukos, S. </author> <year> (1993). </year> <title> Towards history-based grammars: Using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 31-37 Columbus, Ohio. </address>
Reference-contexts: Black, Laf-ferty, and Roukos (1992) show how, using a treebank, the various parse rules of a hand-coded broad-coverage context-free phrase-structure grammar can automatically be assigned probabilities in order to better identify the parse tree that is most likely correct. Their "history-based grammar" (HBG) system <ref> (Black, Jelineck, Lafferty, Magerman, Mercer, & Roukos, 1993) </ref> enhances the probabilistic context-free grammar approach by providing more detailed (incl. semantic) linguistic information to resolve ambiguity. Finally, the Candide system (Brown & et al., 1990; Berger & al., 1994) presents a statistical approach to machine translation.
Reference: <author> Black, E., Lafferty, J., & Roukos, S. </author> <year> (1992). </year> <title> Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 185-192 Newark, Delaware. </address>
Reference: <author> Brill, E. </author> <year> (1995). </year> <title> Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. </title> <journal> Computational Linguistics, </journal> <volume> 21 (4), </volume> <pages> 543-565. </pages>
Reference: <author> Brown, P., & et al. </author> <year> (1990). </year> <title> A statistical approach to machine translation. </title> <journal> Computational Linguistics, </journal> <volume> 16 (2), </volume> <pages> 79-85. </pages>
Reference: <author> Cardie, C. </author> <year> (1992). </year> <title> Learning to disambiguate relative pronouns. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 38-43 San Jose, CA. </address>
Reference-contexts: syntax] and generation, and an intermediate linguistically motivated representation which may or may not involve the direct application of contrastive linguistic knowledge." 2 A number of researchers have already applied machine learning techniques to various NLP tasks like accent restoration by Yarowsky (1994), who achieves 99% accuracy, relative pronoun disambiguation <ref> (Cardie, 1992, 1993, 1996) </ref>, (Japanese) anaphora resolution (Aone & Bennett, 1995), part of speech tagging (Weischedel & al., 1993; Brill, 1995; Daelemans, Zavrel, Berck, & Gillis, 1996), where tagging accuracies between 96% and 97% are achieved, cue phrase classification (Siegel & McKeown, 1994; Litman, 1996), and word sense disambiguation (Yarowsky, 1992,
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 25-32 San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cardie, C. </author> <year> (1996). </year> <title> Automating feature set selection for case-based learning of linguistic knowledge. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pp. 113-126. </pages>
Reference: <author> Chomsky, N. </author> <year> (1988). </year> <title> Lectures on Government and Binding, the Pisa lectures. </title> <publisher> Floris Publications, </publisher> <address> Dordrecht, The Netherlands and Providence, Rhode Island. </address>
Reference: <author> Collins, M. J. </author> <year> (1996). </year> <title> A new statistical parser based on bigram lexical dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 184-191 Santa Cruz, CA. </address>
Reference-contexts: Why then not learn everything, and do without any background knowledge? Statistical approaches such as SPATTER (Magerman, 1995) or the Bigram Lexical Dependency based parser <ref> (Collins, 1996) </ref> produce parsers on very limited linguistic background information and Inductive Logic Programming systems such as CHILL (Zelle & Mooney, 1994; Zelle, 1995) have even generated linguistically relevant categories such as animate. <p> 1 scale compared to .743 for Systran) and that Candide could at least serve as a time-saving tool to produce translations that can be manually post-edited faster than a manual translation from scratch would take. 10.2.5 Bigram Lexical Dependencies Another treebank based statistical parser, the bigram lexical dependencies (BLD) system <ref> (Collins, 1996) </ref>, is based on probabilities between head-words in the parse tree, enhanced by additional distance features that check for order and adjacency of dependency candidates as well as intervening verbs, and intervening or surrounding commas.
Reference: <author> Daelemans, W., Zavrel, J., Berck, P., & Gillis, S. </author> <year> (1996). </year> <title> MBT: A memory-based part of speech tagger-generator. </title> <booktitle> In Proceedings of the Fourth Workshop on Very Large Corpora. </booktitle>
Reference: <author> Engel, U. </author> <year> (1988). </year> <title> Deutsche Grammatik. </title> <publisher> Julius Groos Verlag, </publisher> <address> Heidelberg. </address>
Reference: <author> Francis, W., & Kucera, H. </author> <year> (1982). </year> <title> Frequency Analysis of English Usage: Lexicon and Grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston. </address>
Reference-contexts: Many of the statistical approaches have been made possible by the increased availability of large corpora of natural language data like the Brown corpus <ref> (Francis & Kucera, 1982) </ref>, which consists of about a million words, all labeled with their parts of speech, or the 4.5 million word Penn Treebank (Marcus, Santorini, & Marcinkiewicz, 1993), in which more than half of the sentences have been annotated for their skeletal syntactic structure.
Reference: <author> Frederking, R., & al. </author> <year> (1994). </year> <title> The Pangloss Mark III machine translation system. </title> <booktitle> In Proceedings of the 1st AMTA Conference. </booktitle>
Reference-contexts: While for example the originally extremely probabilistic Candide system (Brown & et al., 1990; Berger & al., 1994) has been augmented by a number of manual heuristics, the originally fairly manual-knowledge-based Pangloss Mark I/II/III system <ref> (Frederking & al., 1994) </ref> has incorporated less dogmatic example-based and transfer-based approaches.
Reference: <author> Gale, W. A., & Church, K. W. </author> <year> (1991). </year> <title> A program for aligning sentences in bilingual corpora. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 177-184. </pages>
Reference-contexts: The bilingual dictionary is a prime candidate for automated acquisition. Bilingual corpora like the Canadian Hansards (parliamentary proceedings) or articles from the Scientific American along with their German translations in Spektrum der Wissenschaft can be exploited to identify co-occurrences of sentences <ref> (Gale & Church, 1991) </ref>, words and expressions. Kay and Roescheisen (1993) even have been able to extract a word alignment table containing German/English word pairs based solely on the internal evidence of the bilingual corpus.
Reference: <author> Gazdar, G., Klein, E., Pullum, G. K., & Sag, I. </author> <year> (1982). </year> <title> Generalized Phrase Structure Grammar. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Traditional approaches try to master the complexity of parse grammars with hand-crafted rules, as in augmented transition networks (Bates, 1978), definite clause grammars (Pereira & Warren, 1980), lexical functional grammars (Kaplan & Bresnan, 1982), functional unification grammars (Kay, 1982), tree-adjoining grammars 3 (Joshi, 1985), or generalized phrase structure grammars <ref> (Gazdar, Klein, Pullum, & Sag, 1982) </ref>. The manual construction of broad-coverage grammars turned out to be much more difficult than expected, if not impossible.
Reference: <author> Goodman, J. </author> <year> (1996). </year> <title> Parsing algorithms and metrics. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 177-183 Santa Cruz, CA. </address>
Reference-contexts: The results of the 17 sub-tests of each series are then averaged. Such a test is also referred to as a 17-fold cross-validation. The following standard <ref> (Goodman, 1996) </ref> evaluation criteria are used: 64 Precision = C system N system Labeled precision = L system N system Recall = C system N logged Labeled recall = L system N logged where N system = number of constituents in system parse N logged = number of constituents in logged
Reference: <author> Hermjakob, U., & Mooney, R. J. </author> <year> (1997). </year> <title> Learning parse and translation decisions from examples with rich context. </title> <booktitle> In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics Madrid, </booktitle> <address> Spain. </address>
Reference: <author> Hornby, A. S. </author> <year> (1974). </year> <title> Oxford Advanced Learner's Dictionary of Current English. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> England. </address>
Reference: <author> Joshi, A. </author> <year> (1985). </year> <title> Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions. </title> <editor> In Dowty, D. R., Karttunen, L., & Zwicky, A. (Eds.), </editor> <booktitle> Natural Language Parsing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> New York. </address>
Reference-contexts: Traditional approaches try to master the complexity of parse grammars with hand-crafted rules, as in augmented transition networks (Bates, 1978), definite clause grammars (Pereira & Warren, 1980), lexical functional grammars (Kaplan & Bresnan, 1982), functional unification grammars (Kay, 1982), tree-adjoining grammars 3 <ref> (Joshi, 1985) </ref>, or generalized phrase structure grammars (Gazdar, Klein, Pullum, & Sag, 1982). The manual construction of broad-coverage grammars turned out to be much more difficult than expected, if not impossible.
Reference: <author> Kaplan, R. M., & Bresnan, J. </author> <year> (1982). </year> <title> Lexical-functional grammar. </title> <editor> In Bresnan, J. (Ed.), </editor> <title> The Mental Representation of Grammatical Relations. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Traditional approaches try to master the complexity of parse grammars with hand-crafted rules, as in augmented transition networks (Bates, 1978), definite clause grammars (Pereira & Warren, 1980), lexical functional grammars <ref> (Kaplan & Bresnan, 1982) </ref>, functional unification grammars (Kay, 1982), tree-adjoining grammars 3 (Joshi, 1985), or generalized phrase structure grammars (Gazdar, Klein, Pullum, & Sag, 1982). The manual construction of broad-coverage grammars turned out to be much more difficult than expected, if not impossible.
Reference: <author> Kay, M. </author> <year> (1982). </year> <title> Parsing in functional unification grammar. </title> <editor> In Dowty, D. R., Karttunen, L., & Zwicky, A. (Eds.), </editor> <booktitle> Natural Language Parsing, </booktitle> <pages> pp. 251-278. </pages> <publisher> Cambridge University Press, </publisher> <address> New York. </address>
Reference-contexts: Traditional approaches try to master the complexity of parse grammars with hand-crafted rules, as in augmented transition networks (Bates, 1978), definite clause grammars (Pereira & Warren, 1980), lexical functional grammars (Kaplan & Bresnan, 1982), functional unification grammars <ref> (Kay, 1982) </ref>, tree-adjoining grammars 3 (Joshi, 1985), or generalized phrase structure grammars (Gazdar, Klein, Pullum, & Sag, 1982). The manual construction of broad-coverage grammars turned out to be much more difficult than expected, if not impossible.
Reference: <author> Kay, M., & Roescheisen, M. </author> <year> (1993). </year> <title> Text-translation alignment. </title> <journal> Computational Linguistics, </journal> <volume> 19 (1), </volume> <pages> 121-142. </pages>
Reference: <author> Lasnik, H., & Uriagereka, J. </author> <year> (1988). </year> <title> A Course in GB Syntax, Lectures on Binding and Empty Categories. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts and London, England. </address>
Reference: <author> Lederer, H. </author> <year> (1969). </year> <title> Reference Grammar of the German Language. </title> <publisher> Charles Scribner's Sons, </publisher> <address> New York. </address>
Reference: <author> Litman, D. J. </author> <year> (1996). </year> <title> Cue phrase classification using machine learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 5, </volume> <pages> 53-95. </pages>
Reference: <author> Magerman, D. M. </author> <year> (1995). </year> <title> Statistical decision-tree models for parsing. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 276-283 Cambridge, MA. </address>
Reference-contexts: Why then not learn everything, and do without any background knowledge? Statistical approaches such as SPATTER <ref> (Magerman, 1995) </ref> or the Bigram Lexical Dependency based parser (Collins, 1996) produce parsers on very limited linguistic background information and Inductive Logic Programming systems such as CHILL (Zelle & Mooney, 1994; Zelle, 1995) have even generated linguistically relevant categories such as animate.
Reference: <author> Magerman, D. M. </author> <year> (1994). </year> <title> Natural Lagnuage Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. thesis, </type> <institution> Stanford University. </institution>
Reference: <author> Manning, C. D. </author> <year> (1993). </year> <title> Automatic acquisition of a large subcategorization dictionary from corpora. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 235-242 Columbus, Ohio. </address>
Reference-contexts: It is however important to distinguish this case from a situation where a match has been found, but no role is available for a specific potential phrase component. Using techniques described in <ref> (Manning, 1993) </ref>, we could at least partially automate the acquisition of subcategorization entries from corpora. 11.2.3 Incomplete Knowledge Base When unknown words get assigned a part of speech, they also need to be assigned a semantic concept from the knowledge base.
Reference: <author> Marcus, M. </author> <year> (1980). </year> <title> A Theory of Syntactic Recognition for Natural Language. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference-contexts: Compared to the original shift-reduce parser <ref> (Marcus, 1980) </ref>, our parser includes additional types of operations and allows additional operation parameters, so that the parser can produce a deeper analysis of the sentence. The parse tree integrates semantic information, phrase-structure and case-frames. <p> A sentence is processed `left-to-right' in a single pass, integrating part-of-speech selection and syntactic and semantic analysis. As the basic mechanism for parsing text we therefore choose a shift-reduce type parser <ref> (Marcus, 1980) </ref>. It breaks parsing into an ordered sequence of small and manageable parse actions such as shift and reduce. <p> The results are compared in section 6.4.4. 5.9 Garden Paths Garden path sentences are sentences that initially mislead the reader in their syntactic analysis. Classical examples <ref> (Marcus, 1980) </ref> are: 1. The horse raced past the barn fell. 2. Cotton clothing is made of grows in Mississippi.
Reference: <author> Marcus, M., Santorini, B., & Marcinkiewicz, M. </author> <year> (1993). </year> <title> Building a large annotated corpus of English: The Penn treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19 (2), </volume> <pages> 313-330. </pages>
Reference-contexts: Many of the statistical approaches have been made possible by the increased availability of large corpora of natural language data like the Brown corpus (Francis & Kucera, 1982), which consists of about a million words, all labeled with their parts of speech, or the 4.5 million word Penn Treebank <ref> (Marcus, Santorini, & Marcinkiewicz, 1993) </ref>, in which more than half of the sentences have been annotated for their skeletal syntactic structure. Newer treebank-based probabilistic approaches (Magerman, 1995; Collins, 1996) have produced encouraging results. <p> * adding background knowledge, in particular a KB and a subcategorization table; * building a much richer internal structure, including both phrase-structure and case-frame information; * providing morphological pre-processing; * introducing a sophisticated machine learning component. 10.2 Parsers Learning From Treebank Examples Several researchers have now used the Penn Treebank <ref> (Marcus et al., 1993) </ref> to develop parsers. The Penn Treebank is a corpus of over 4.5 million words of American English that has been annotated with part of speech. In addition, over half of it has been annotated for skeletal syntactic structure.
Reference: <author> Matsumoto, Y., Kurohashi, S., Utsuro, T., Myoki, Y., & Nagao, M. </author> <year> (1994). </year> <title> Japanese morphological analysis system JUMAN manual, version 2.0 (in Japanese). </title> <type> Technical report NAIST-IS-TR94025, </type> <institution> Nara Institute of Science and Technology, Nara, </institution> <address> Japan. </address>
Reference-contexts: Yet more difficult, in languages such as Japanese, where words are generally not separated by spaces, sophisticated segmentation tools, such as JUMAN <ref> (Matsumoto, Kurohashi, Utsuro, Myoki, & Nagao, 1994) </ref>, have to be used. <p> work in the Knowledge Based Natural Language (KBNL) group at the Microelectronics and Computer Technology Corporation (MCC) in Austin, Texas, where the author reimplemented major parts of a Lisp parser in C++, achieving a speed-up of one to two orders of magnitude, and speeding up the JUMAN Japanese language segmenter <ref> (Matsumoto et al., 1994) </ref> by a factor of 60, mostly by caching information that was used repeatedly. 112 Chapter 12 Conclusions Guided by the goal to advance the technology of robust and efficient parsers and machine translation systems for free text, we try to bridge the gap between the typically hard-to-scale
Reference: <author> Mooney, R. J. </author> <year> (1996). </year> <title> Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pp. </pages> <address> 82-91 Philadelphia, PA. </address>
Reference: <author> Ng, H. T., & Lee, H. B. </author> <year> (1996). </year> <title> Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 40-47 Santa Cruz, CA. </address>
Reference: <author> Nirenburg, S., Carbonell, J., Tomita, M., & Goodman, K. </author> <year> (1992). </year> <title> Machine Translation: A Knowledge-Based Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Interlingua Approach The overall system architecture follows the classical subdivision of parsing, transfer and generation, thereby rejecting a pure interlingua approach (see figure 2.1). As the authors and interlingua proponents of <ref> (Nirenburg, Carbonell, Tomita, & Goodman, 1992) </ref> point out, the major distinction between the interlingua- and transfer-based systems is the "attitude toward comprehensive analysis 6 of meaning".
Reference: <author> Nirenburg, S. (Ed.). </author> <year> (1987). </year> <title> Machine Translation: Theoretical and Methodological Issues. </title> <publisher> Cam-bridge University Press, </publisher> <address> Cambridge, England. </address>
Reference: <author> Pereira, F. C. N., & Warren, D. H. D. </author> <year> (1980). </year> <title> Definite clause grammars for language analysis a survey of the formalisms and a comparison with augmented transition networks. </title> <journal> Artificial Intelligence, </journal> <volume> 13, 3, </volume> <pages> 231-278. </pages>
Reference-contexts: Traditional approaches try to master the complexity of parse grammars with hand-crafted rules, as in augmented transition networks (Bates, 1978), definite clause grammars <ref> (Pereira & Warren, 1980) </ref>, lexical functional grammars (Kaplan & Bresnan, 1982), functional unification grammars (Kay, 1982), tree-adjoining grammars 3 (Joshi, 1985), or generalized phrase structure grammars (Gazdar, Klein, Pullum, & Sag, 1982). The manual construction of broad-coverage grammars turned out to be much more difficult than expected, if not impossible.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 81-106. </pages> <note> 162 Quinlan, </note> <author> J. R. </author> <year> (1987). </year> <title> Generating production rules from decision trees. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 304-307 Milan, Italy. </address>
Reference-contexts: Our decision structure is a hybrid that combines elements of decision trees, decision lists and decision hierarchies. 5.8.1 Review of Decision Trees and Lists Based on a training set of classified examples, algorithms like ID3 <ref> (Quinlan, 1986) </ref> and C4.5 (Quin-lan, 1993) build so-called decision trees that can classify further (yet unclassified) cases. Consider 51 52 the small training set with 14 examples in table 5.2. In each example, the weather is classified as "Play" or "Don't Play".
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo,CA. </address>
Reference-contexts: Play overcast 83 78 false Play overcast 64 65 true Play overcast 81 75 false Play rain 71 80 true Don't Play rain 65 70 true Don't Play rain 75 80 false Play rain 68 80 false Play rain 70 96 false Play Table 5.2: A small training set; from <ref> (Quinlan, 1993) </ref>. Given the examples of table 5.2, the program C4.5 builds the decision tree shown in figure 5.5. <p> Table 5.3: Definition of gain ratio. For a detailed motivation and explanation of the gain ratio, see <ref> (Quinlan, 1993) </ref>, pp. 20 ff. the example set into subsets that are to be as homogeneous as possible with respect to their classes.
Reference: <author> Quirk, R., Greenbaum, S., Leech, G., & Svartvik, J. </author> <year> (1985). </year> <title> A Comprehensive Grammar of the English Language. </title> <publisher> Longman, </publisher> <address> London and New York. </address>
Reference: <author> Radford, A. </author> <year> (1988). </year> <title> Transformational Grammar. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Eng-land. </address>
Reference-contexts: These techniques automatically derive parse grammars and other classification structures from examples. 2 An anaphor is an expression which cannot have independent reference, but refers to another expression, the so-called antecedent <ref> (Radford, 1988) </ref>. Examples of anaphora in sentences 13&14 are the pronouns "which" and "it" (both occurrences), which refer to the antecents "the statue of liberty", "the airline", and "a loss".
Reference: <author> Riloff, E. </author> <year> (1996). </year> <title> An empirical study of automated dictionary construction for information extraction in three domains. </title> <journal> Artificial Intelligence, </journal> <volume> 85, </volume> <pages> 101-134. </pages>
Reference: <author> Rivest, R. L. </author> <year> (1987). </year> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 229-246. </pages>
Reference-contexts: All examples for `overcast' are classified as `Play', so the subtree for `overcast' is a single node marked `Play'. The other two example subsets have to be split one more time before all example subsets share the same class. Alternatively, decision list algorithms <ref> (Rivest, 1987) </ref> generate a list of conjunctive rules, where rules are tested in order and the first one that matches an instance is used to classify it. 5.8.2 Decision Hierarchies Our first extension to the standard algorithms for decision trees and decision lists is the concept of a decision hierarchy.
Reference: <author> Siegel, E. V., & McKeown, K. R. </author> <year> (1994). </year> <title> Emergent linguistic rules from inducing decision trees: Disambiguating discourse clue words. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Simmons, R. F., & Yu, Y. </author> <year> (1992). </year> <title> The acquisition and use of context dependent grammars for English. </title> <journal> Computational Linguistics, </journal> <volume> 18 (4), </volume> <pages> 391-418. </pages>
Reference-contexts: We now describe related empirical work on parsing. 10.1 Simmons and Yu Our basic deterministic parsing and interactive training paradigm is based on <ref> (Simmons & Yu, 1992) </ref>.
Reference: <author> Slocum, J. (Ed.). </author> <year> (1988). </year> <title> Machine Translation Systems. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference: <author> Smadja, F., McKeown, K. R., & Hatzivassiloglou, V. </author> <year> (1996). </year> <title> Translating collocations for bilingual lexicons: A statistical approach. </title> <journal> Computational Linguistics, </journal> <volume> 22 (1), </volume> <pages> 1-38. </pages>
Reference-contexts: Kay and Roescheisen (1993) even have been able to extract a word alignment table containing German/English word pairs based solely on the internal evidence of the bilingual corpus. As for the monolingual lexicon, alignment programs could propose likely candidates to a supervisor for approval or rejection. The Champollion program <ref> (Smadja, McKeown, & Hatzivassiloglou, 1996) </ref> for example, using aligned text from the Canadian Hansards and given a multi-word English collocation, can identify the equivalent collocation in French with a precision of up to 78%. Champollion is limited to a statistical analysis of sets of words.
Reference: <author> Somers, H. S. </author> <year> (1993). </year> <title> Current research in machine translation. </title> <journal> Machine Translation, </journal> <volume> 7 (4), </volume> <pages> 231-246. </pages>
Reference: <author> Tanenhaus, M. K., & al. </author> <year> (1996). </year> <title> Eye movements and spoken language comprehension. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 48-54. </pages>
Reference-contexts: Since during the training phase, the system is to be guided by a human supervisor, it is extremely important that the parsing process has a very transparent control structure that is intuitive to a human. As recent work in cognitive science, e.g. <ref> (Tanenhaus & al., 1996) </ref>, has confirmed again, the human mind performs a continuous and deep interpretation of natural language input. A sentence is processed `left-to-right' in a single pass, integrating part-of-speech selection and syntactic and semantic analysis.
Reference: <author> Tomita, M. </author> <year> (1986). </year> <title> Efficient Parsing for Natural Language. </title> <publisher> Kluwer Academic Publishers, Boston. </publisher> <editor> van Riemsdijk, H. C., & Williams, E. </editor> <year> (1986). </year> <title> Introduction to the Theory of Grammar. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts and London, England. </address> <month> Webster's </month> <year> (1994). </year> <title> Webster's Encyclopedic Unabridged Dictionary of the English Language. </title> <publisher> Gramercy Books, </publisher> <address> New York/Avenel. </address>
Reference-contexts: The traditional approach of trying to master the complexity of parse grammars with hand-crafted rules turned out to be much more difficult than expected, if not impossible. Many parsers leave ambiguity largely unresolved and just focus on an efficient administration of ambiguity. The Tomita parser <ref> (Tomita, 1986) </ref> for example, when running the sentence "Labels can be assigned to a particular instruction step in a source program to identify that step as an entry point for use in subsequent instructions." on a relatively moderate sized grammar with 220 hand-coded rules returns a parse forest representing 309 parsing
Reference: <author> Weischedel, R., & al. </author> <year> (1993). </year> <title> Coping with ambiguity and unknown words through probabilistic models. </title> <journal> Computational Linguistics, </journal> <volume> 19 (2), </volume> <pages> 359-382. </pages>
Reference: <author> Wermter, S., & Weber, V. </author> <year> (1997). </year> <title> Screen: Learning a flat syntactic and semantic spoken language analysis using artificial neural networks. </title> <journal> Journal of Artificial Intelligence Reserach, </journal> <volume> 6, </volume> <pages> 35-85. </pages>
Reference-contexts: Februar ich ich in der Schule." Table 11.1: Parse tree and subsequent translation of an ill-formed sentence. The sentence is a `noisy' variation of "On February 27 I was in school.", both based on a German sentence pair from <ref> (Wermter & Weber, 1997) </ref>. As usual, the parse tree is printed with decreasing detail at lower levels. The parse tree is somewhat pathological (see section 5.5 for an explanation of CONC etc.), but adequately preserves the "healthy" parts of the input sentence.
Reference: <author> Wu, D., & Xia, X. </author> <year> (1995). </year> <title> Large-scale automatic extraction of an English-Chinese translation lexicon. </title> <journal> Machine Translation, </journal> <volume> 9 (3-4), </volume> <pages> 285-313. </pages> <address> 163 Yarowsky, D. </address> <year> (1992). </year> <title> Word-sense disambiguation using statistical methods of Roget's categories trained on large corpora. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Computational Linguistics, </booktitle> <pages> pp. 454-460. </pages>
Reference: <author> Yarowsky, D. </author> <year> (1994). </year> <title> Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 88-95 Las Cruces, NM. </address>
Reference: <author> Yarowsky, D. </author> <year> (1995). </year> <title> Unsupervised word sense disambiguation rivaling supervised methods. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 189-196. </pages>
Reference: <author> Zelle, J. M. </author> <year> (1995). </year> <title> Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. </title> <type> Ph.D. thesis, </type> <institution> University of Texas, Austin, TX. </institution> <note> Also appears as Artificial Intelligence Laboratory Technical Report AI 96-249. </note>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994). </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 343-351 New Brunswick, NJ. </address> <month> 164 </month>
References-found: 60

