URL: http://polaris.cs.uiuc.edu/reports/1274.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: on Use of Multiprocessors in Meteorology, European Centre for Medium Range Weather  
Author: DeRose, K. Gallivan and E. Gallopoulos 
Address: 1308 West Main Street Urbana, Illinois 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Date: Nov. 1992.  Dec. 1992  
Note: To appear in Parallel Supercomputing in Atmospheric Science, G.-R. Hoffmann and T. Kauranne eds., World Scientific, 1993. Paper from Fifth ECMWF Workshop  L.  
Pubnum: Forecasts,  CSRD Report No. 1274  
Abstract: Status Report: Parallel Ocean Circulation on Cedar 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Bryan. </author> <title> A numerical method for the study of the circulation of the world ocean. </title> <journal> Journal of Computational Physics, </journal> <volume> 4 </volume> <pages> 347-376, </pages> <year> 1969. </year>
Reference: [2] <author> M. D. Cox. </author> <title> A primitive equation, 3-dimensional model of the ocean. </title> <type> Technical Report 1, </type> <institution> Geophysical Fluid Dynamics Laboratory/NOAA, Princeton University, </institution> <address> Prince-ton, NJ 08542, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: 1 Introduction and background The ocean circulation model (OCM) used in this work is based on a basic model of the Geophysical Fluid Dynamics Laboratory (GFDL) <ref> [2] </ref>, as adapted in the Istituto per lo Studio delle Metodologie Geofisiche Ambientali (IMGA-CNR) to the Mediterranean basin geometry [11]. This model, from now on referred to as GFDL-IMGA, simulates the basic aspects of large-scale, baroclinic ocean circulation, including treatment of irregular bottom topography.
Reference: [3] <author> L. DeRose. </author> <title> Parallel ocean circulation modeling on Cedar. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> December </month> <year> 1991. </year>
Reference: [4] <author> L. DeRose, K. Gallivan, and E. Gallopoulos. </author> <title> Trace analysis of the GFDL ocean circulation model: A preliminary study. </title> <type> Technical Report 863, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: In <ref> [4] </ref>, we explored the mapping of the program to the Alliant FX/8 which is a single cluster of Cedar. In [6] and [5] we described optimizations for the multicluster architecture, with particular emphasis on the baroclinic, three dimensional part of the code.
Reference: [5] <author> L. DeRose, K. Gallivan, and E. Gallopoulos. </author> <title> Experiments with an ocean circulation model on CEDAR. </title> <booktitle> In Proc. 1992 ACM Int'l. Conference on Supercomputing, </booktitle> <pages> pages 397-408, </pages> <address> Washington D.C., </address> <month> July </month> <year> 1992. </year> <note> Also CSRD TR-1200, </note> <month> Feb. </month> <year> 1992. </year> <month> 14 </month>
Reference-contexts: In [4], we explored the mapping of the program to the Alliant FX/8 which is a single cluster of Cedar. In [6] and <ref> [5] </ref> we described optimizations for the multicluster architecture, with particular emphasis on the baroclinic, three dimensional part of the code. The current report outlines the status of our work with emphasis on recent developments. <p> The current report outlines the status of our work with emphasis on recent developments. A final report will be available soon; in the meantime we recommend that this report is read together with <ref> [5] </ref>, the latter providing important background information which, in the interest of brevity, is either omitted or mentioned only briefly. An extensive list of references and a discussion of related work can also be found in [5]. <p> available soon; in the meantime we recommend that this report is read together with <ref> [5] </ref>, the latter providing important background information which, in the interest of brevity, is either omitted or mentioned only briefly. An extensive list of references and a discussion of related work can also be found in [5]. The model equations, written in spherical coordinates (; ; z), describe velocity, tracers (temperature and salinity) in a three dimensional ocean basin. The numerical scheme uses second order finite differences for space and leapfrog timestepping, occasionally mixed with a forward Euler step. <p> Cray SSD, Cedar global memory. The finite difference scheme used in the model naturally defines a workspace consisting of data from a small number of slabs. The steps leading to the results described in <ref> [5] </ref> are summarized in Figure 1. In particular the first major phase was based on the following design principles: 1. The slab organization was to be preserved and partitionings for multicluster paral lelism were to be built as extensions of the slabs. 2. <p> As will be seen, and unlike its predecessors, this report discusses several modifications to the code, which are close to the 2 limits of these design principles. For reasons outlined in <ref> [5] </ref>, we have still opted not to discuss modifications to the barotropic phase. 2 The computational environment: Cedar We provide a short summary of the architecture, referring the reader to [8] for a more detailed description. <p> A design decision is how to partition the computational domain across the processors. This decision can be critical to the effectiveness of distributed memory processors, with volume/area/perimeter arguments being made to support particular strategies on grounds of minimized communication to operation ratios. As discussed in detail in <ref> [5] </ref> our original design explored three data partitioning strategies. The first two partitionings, referred to as row and column are one dimensional as they cut across a single ( and respectively) grid dimension. The third partitioning is two-dimensional, as it cuts across two dimensions of the grid. <p> This important issue is discussed in Section 7. We also note interesting recent work on this topic in the context of problems of ocean modeling for distributed memory machines discussed in [12]. 5 Summary of results In <ref> [5] </ref> we experimented mainly with data sets P 8 L 16 and P 4 L 8 . The results for P 8 L 16 are summarized in Table 2. <p> These represent a significant advancement on the results of <ref> [5] </ref>. In particular, we note that there is evidence that improvements that are presently being implemented should result a code that exploits effectively the different views of Cedar.
Reference: [6] <author> L. DeRose, K. Gallivan, E. Gallopoulos, and A. Navarra. </author> <title> Parallel ocean circulation modeling on Cedar. </title> <editor> In D. C. Sorensen, editor, </editor> <booktitle> Proc. Fifth SIAM Conf. Parallel Processing for Scientific Computing, </booktitle> <pages> pages 401-405, </pages> <address> Philadelphia, </address> <month> March </month> <year> 1991. </year> <note> SIAM. Also CSRD TR-1124. </note>
Reference-contexts: In [4], we explored the mapping of the program to the Alliant FX/8 which is a single cluster of Cedar. In <ref> [6] </ref> and [5] we described optimizations for the multicluster architecture, with particular emphasis on the baroclinic, three dimensional part of the code. The current report outlines the status of our work with emphasis on recent developments.
Reference: [7] <author> J. Hoeflinger. </author> <title> Cedar Fortran programmer's handbook. </title> <type> Technical Report 1157, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Programs are mostly written in Cedar Fortran <ref> [7] </ref>, a language resembling Fortran 77, with vector constructs such as those proposed for the Fortran 90 standard and has extensions for memory allocation, concurrency control, multitasking and synchronization.
Reference: [8] <author> J. Konicek, T. Tilton, A. Veidenbaum, C. Zhu, E. Davidson, R. Downing, M. Haney, M. Sharma, P. Yew, P. Farmwald, D. Kuck, D. Lavery, R. Lindsey, D. Pointer, J. Andrews, T. Beck, T. Murphy, S. Turner, and N. Warter. </author> <title> The organization of the Cedar system. </title> <booktitle> In Proc. 1991 Int'l Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 49-56, </pages> <address> St. Charles, IL, </address> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: For reasons outlined in [5], we have still opted not to discuss modifications to the barotropic phase. 2 The computational environment: Cedar We provide a short summary of the architecture, referring the reader to <ref> [8] </ref> for a more detailed description. The Cedar system was developed at the Center for Supercomputing Research and Development of the University of Illinois. Its main characteristic are the hierarchical organization of its computational capabilities and memory system.
Reference: [9] <author> C. Leith. </author> <title> The Livermore model of parallel computation. Talk presented during the Computer and Atmospheric Sciences (CAS'92) Conference, </title> <address> Santa Fe, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The three smaller models, however, clearly have difficulty amortizing the architectural overheads effectively and in some cases actually degrade in performance as the number of processors increases. 2 Paraphrasing C. Leith <ref> [9] </ref>: The big problem in parallel computing is not how large of a problem you can do efficiently, but how small of a problem you can do efficiently. 7 Table 2: GC model P 8 L 16 CEs/ Number of Clusters cluster 1 2 3 4 (row) (col) (row) (col) (row)
Reference: [10] <author> R. McGrath and P. Emrath. </author> <title> Using memory in the Cedar system. </title> <type> Technical Report 655, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1987. </year>
Reference: [11] <author> N. Pinardi and A. Navarra. </author> <title> A brief review of global Mediterranean wind-driven general circulation experiments. </title> <type> Technical Report 132, </type> <institution> IMGA-CNR, Modena Italy, </institution> <year> 1988. </year>
Reference-contexts: 1 Introduction and background The ocean circulation model (OCM) used in this work is based on a basic model of the Geophysical Fluid Dynamics Laboratory (GFDL) [2], as adapted in the Istituto per lo Studio delle Metodologie Geofisiche Ambientali (IMGA-CNR) to the Mediterranean basin geometry <ref> [11] </ref>. This model, from now on referred to as GFDL-IMGA, simulates the basic aspects of large-scale, baroclinic ocean circulation, including treatment of irregular bottom topography. It is used in climate studies and also to study the development of mid-ocean eddies.
Reference: [12] <author> R. Wait and T. J. Harding. </author> <title> Numerical software for 3D hydrodynamic modelling using transputer arrays. </title> <booktitle> In Parallel Supercomputing in Atmospheric Science, these proceedings. </booktitle> <volume> 15 0 5 10 15 20 25 30 10 20 30 Number of CEs S P 1:4 L 8 P 4 L 8 16 </volume>
Reference-contexts: A related design issue is that of data partitioning to achieve load balance. This important issue is discussed in Section 7. We also note interesting recent work on this topic in the context of problems of ocean modeling for distributed memory machines discussed in <ref> [12] </ref>. 5 Summary of results In [5] we experimented mainly with data sets P 8 L 16 and P 4 L 8 . The results for P 8 L 16 are summarized in Table 2.
References-found: 12

