URL: ftp://ftp.cs.washington.edu/pub/orca/papers/lcpc94.ps
Refering-URL: http://www.cs.washington.edu/research/zpl/papers/abstracts/lcpc94.html
Root-URL: 
Affiliation: University of Washington  
Date: 361-375, 1994.  
Note: Appears in "Proceedings of the Seventh International Workshop on Languages and Compilers for Parallel Computing," pages  
Abstract: SIMPLE Performance Results in ZPL Abstract. This paper presents performance results for ZPL programs running on the Kendall Square Research KSR-2 and the Intel Paragon. Because ZPL is a data parallel language based on the Phase Abstractions programming model, these results complement earlier claims that the Phase Abstractions model can lead to portability across MIMD computers. The ZPL language and selected aspects of the compilation strategy are briefly described, and performance results are compared against hand-coded programs.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Gail Alverson, William Griswold, Calvin Lin, David Notkin, and Lawrence Snyder. </author> <title> Abstractions for portable, scalable parallel programming. </title> <type> Technical Report 93-12-09, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <note> submitted to IEEE Trans. on Parallel and Distributed Systems, </note> <year> 1993. </year>
Reference-contexts: This section briefly reviews the salient features of the Phase Abstractions. A more complete description can be found elsewhere <ref> [1, 3, 14] </ref>. The Phase Abstractions programming model identifies three levels of parallel programming: the X level, the Y level, and the Z level. The Z level specifies a program's overall control logic and the sequential invocation of phases to solve the overall problem. <p> Logically, the ZPL program should be thought of as a sequence of phase invocations, with the ZPL compiler producing the X and Y level code that implements these phases. In the parlance of the Phase Abstractions, ZPL's arrays program Jacobi; config var N: integer = 100; direction east = <ref> [0, 1] </ref>; west = [0,-1]; north = [-1,0]; south = [1, 0]; region R = [1..N, 1..N]; constant epsilon: real = .01; var A, Temp: [R] real; error : real; procedure Jacobi (); begin [east of R] A := 0.0; [west of R] A := 0.0; [north of R] A := <p> In the parlance of the Phase Abstractions, ZPL's arrays program Jacobi; config var N: integer = 100; direction east = [0, 1]; west = [0,-1]; north = [-1,0]; south = <ref> [1, 0] </ref>; region R = [1..N, 1..N]; constant epsilon: real = .01; var A, Temp: [R] real; error : real; procedure Jacobi (); begin [east of R] A := 0.0; [west of R] A := 0.0; [north of R] A := 0.0; [south of R] A := 1.0; [R] A :=
Reference: 2. <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN'93 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference: 3. <author> William Griswold, Gail Harrison, David Notkin, and Lawrence Snyder. </author> <title> Scalable abstractions for parallel programming. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, 1990. </booktitle> <address> Charleston, South Carolina. </address>
Reference-contexts: This section briefly reviews the salient features of the Phase Abstractions. A more complete description can be found elsewhere <ref> [1, 3, 14] </ref>. The Phase Abstractions programming model identifies three levels of parallel programming: the X level, the Y level, and the Z level. The Z level specifies a program's overall control logic and the sequential invocation of phases to solve the overall problem.
Reference: 4. <author> R. E. Hiromoto, O. M. Lubeck, and J. Moore. </author> <title> Experiences with the Denelcor HEP. </title> <booktitle> In Parallel Computing, </booktitle> <pages> pages 1 197-206, </pages> <year> 1984. </year>
Reference-contexts: a compiler can perform the necessary translations to achieve portability for the SIMPLE program. 1 This research was supported in part by Office of Naval Research Contract N00014 92-J-1824 and NSF Contract CDA-9211095 2 Figure 1 compares against results from Hiromoto et al.'s hand coded results on the Denelcor HEP <ref> [4] </ref> and Pingali and Rogers' compiled Id program on the iPSC/2 [11].
Reference: 5. <author> H. T. Kung and C.E. Leiserson. </author> <title> Introduction to VLSI Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1980. </year> <editor> Section 8.3, by C. Mead and L. </editor> <address> Conway. </address>
Reference-contexts: ZPL is a subset of Orca C that aims to provide conciseness, convenience, and clarity for 3 In sophisticated computations a section can comprise multiple code ensembles|for example, Kung and Leiserson's systolic matrix multiplication algorithm <ref> [5] </ref> ideally consists of two processes per processor, one to move data and one to compute new local products|and may comprise multiple port ensembles. data parallel computations. By addressing a restricted domain the language and compiler design are simplified and good performance can be attained.
Reference: 6. <author> Jinling Lee, Calvin Lin, and Lawrence Snyder. </author> <title> Programming SIMPLE for parallel portability. </title> <editor> In Uptal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 84-98. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction In 1991 the authors claimed that programs written in languages founded on the CTA machine model and the Phase Abstractions programming model would be portable across the major families of MIMD parallel computers <ref> [6, 8] </ref>. The evidence offered to support this claim was the observed performance of the SIMPLE computational fluid dynamics program on five parallel machines that represented the MIMD machines then available: the Sequent Symmetry, BBN Butterfly, Intel iPSC/2, nCUBE/7 and a Transputer array machine; see Figure 1. <p> SIMPLE results. coded version written in C with messages passing <ref> [6] </ref>. Ten iterations were run for a problem with 256fi256 data points (all C programs were compiled with optimizations at level -O2).
Reference: 7. <author> Calvin Lin and Lawrence Snyder. </author> <title> A comparison of programming models for shared memory multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages II 163-180, </pages> <year> 1990. </year>
Reference-contexts: Interestingly, these trends are completely reversed from the Paragon. Further analysis is needed to explain this behavior and an anomaly at P=8, where the ZPL program is slightly faster. 5.2 Jacobi Results In addition to SIMPLE, we also compare a ZPL implementation of Jacobi against a hand-coded message-passing implementation <ref> [7] </ref> for a 512 fi 512 problem that converged at 279 iterations. (See Figure 4.) At P=1 the ZPL program is 5.2% slower than hand-coded. For more than one processor the overhead is between 5.4% (P=4) and 1.8% (P=16).
Reference: 8. <author> Calvin Lin and Lawrence Snyder. </author> <title> A portable implementation of SIMPLE. </title> <journal> Inter--national Journal of Parallel Programming, </journal> <volume> 20(5) </volume> <pages> 363-401, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction In 1991 the authors claimed that programs written in languages founded on the CTA machine model and the Phase Abstractions programming model would be portable across the major families of MIMD parallel computers <ref> [6, 8] </ref>. The evidence offered to support this claim was the observed performance of the SIMPLE computational fluid dynamics program on five parallel machines that represented the MIMD machines then available: the Sequent Symmetry, BBN Butterfly, Intel iPSC/2, nCUBE/7 and a Transputer array machine; see Figure 1. <p> describing all aspects of a section through the common notion of an ensemble, all aspects of parallelism can scale in a coherent fashion. 3 The ZPL Language For the sake of performance, low level languages can be built upon the Phase Abstractions, and, in fact, such languages have been proposed <ref> [8, 9] </ref>: Orca C is a low level non-shared memory language that extends the programming model with modest conveniences. Thus, Orca C is a MIMD language that gives programmers control over the performance-sensitive aspects of their programs.
Reference: 9. <author> Calvin Lin and Lawrence Snyder. </author> <title> Data ensembles in Orca C. </title> <editor> In Uptal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 112-123. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: describing all aspects of a section through the common notion of an ensemble, all aspects of parallelism can scale in a coherent fashion. 3 The ZPL Language For the sake of performance, low level languages can be built upon the Phase Abstractions, and, in fact, such languages have been proposed <ref> [8, 9] </ref>: Orca C is a low level non-shared memory language that extends the programming model with modest conveniences. Thus, Orca C is a MIMD language that gives programmers control over the performance-sensitive aspects of their programs.
Reference: 10. <author> Calvin Lin and Lawrence Snyder. ZPL: </author> <title> An array sublanguage. </title> <editor> In Uptal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 96-114. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: For example, maxn A reduces A to its largest element. ZPL has several other interesting features that, although worthy of mention, are rather involved and not needed in the programs discussed in the experiments below. These are described in the literature <ref> [10, 15] </ref>. The above summary of ZPL should allow the Jacobi program of Figure 2 to be easily understood. The program begins with declarations, first for the region R, then for two array variables, A and Temp, then for a scalar, error, and finally for a set of directions.
Reference: 11. <author> Keshav Pingali and Anne Rogers. </author> <title> Compiler parallelization of SIMPLE for a distributed memory machine. </title> <type> Technical Report 90-1084, </type> <institution> Cornell University, </institution> <year> 1990. </year>
Reference-contexts: the SIMPLE program. 1 This research was supported in part by Office of Naval Research Contract N00014 92-J-1824 and NSF Contract CDA-9211095 2 Figure 1 compares against results from Hiromoto et al.'s hand coded results on the Denelcor HEP [4] and Pingali and Rogers' compiled Id program on the iPSC/2 <ref> [11] </ref>.
Reference: 12. <author> Constantine Polychronopolous, Milind Girkar, Mohammad Reza Haghighat, Chia Ling Lee, Bruce Leung, and Dale Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 39-48, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: that produces identical object code for both the shared address space KSR-2 and the non-shared memory Intel Paragon. 5 4 The decomposition can be changed through Orca C's Y level ensemble declarations. 5 We currently perform no machine-specific optimizations. 4 The ZPL Compiler We have modified the Parafrase-2 source-to-source translator <ref> [12] </ref> to compile ZPL programs to SPMD C code with calls to machine-specific communication routines. We rely on each machine's native C compiler to complete the translation to machine code.
Reference: 13. <author> Lawrence Snyder. </author> <title> Type architecture, shared memory and the corollary of modest potential. </title> <booktitle> In Annual Review of Computer Science, </booktitle> <pages> pages I:289-318, </pages> <year> 1986. </year>
Reference-contexts: This is a necessity because ZPL is a sublanguage of an explicitly parallel language. The parallelism is formulated in terms of the CTA <ref> [13] </ref>, an abstract machine on which the program is logically thought to be executed. Programmers, knowing the general characteristics of the parallel execution, can thus assess the performance implications of alternate program implementations.
Reference: 14. <author> Lawrence Snyder. </author> <booktitle> Foundations of practical parallel programming languages. In Proceedings of the Second International Conference of the Austrian Center for Parallel Computation. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: This section briefly reviews the salient features of the Phase Abstractions. A more complete description can be found elsewhere <ref> [1, 3, 14] </ref>. The Phase Abstractions programming model identifies three levels of parallel programming: the X level, the Y level, and the Z level. The Z level specifies a program's overall control logic and the sequential invocation of phases to solve the overall problem.
Reference: 15. <author> Lawrence Snyder. </author> <title> A ZPL programming guide. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: For example, maxn A reduces A to its largest element. ZPL has several other interesting features that, although worthy of mention, are rather involved and not needed in the programs discussed in the experiments below. These are described in the literature <ref> [10, 15] </ref>. The above summary of ZPL should allow the Jacobi program of Figure 2 to be easily understood. The program begins with declarations, first for the region R, then for two array variables, A and Temp, then for a scalar, error, and finally for a set of directions.
Reference: 16. <author> Reinhard v. Hanxleden and Ken Kennedy. </author> <title> A code placement framework and its application to communication generation. </title> <type> Technical Report CRPC-TR93337, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> October </month> <year> 1993. </year>
References-found: 16

