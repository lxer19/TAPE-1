URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-9/s2k-92-09.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-9/
Root-URL: http://www.cs.berkeley.edu
Email: marti@cs.berkeley.edu  
Title: Automatic Acquisition of Hyponyms from Large Text Corpora  
Author: Marti A. Hearst and 
Address: 571 Evans Hall  Berkeley, CA 94720  
Affiliation: Computer Science Division,  University of California, Berkeley  Xerox Palo Alto Research Center  
Date: July 1992  
Note: To Appear in the Proceedings of the Fourteenth International Conference on Computational Linguistics, Nantes France,  
Abstract: We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ahlswede, T. & M. </author> <title> Evens (1988). Parsing vs. text processing in the analysis of dictionary definitions. </title> <booktitle> Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 217-224. </pages>
Reference-contexts: For example, (Alshawi 1987), in interpreting LDOCE definitions, uses a hierarchy of patterns which consist mainly of part-of-speech indicators and wildcard characters. (Markowitz et al. 1986), (Jensen & Binot 1987), and (Nakamura & Nagao 1988) also use pattern recognition to extract semantic relations such as taxonomy from various dictionaries. <ref> (Ahlswede & Evens 1988) </ref> compares an approach based on parsing Webster's 7th definitions with one based on pattern recognition, and finds that for finding simple semantic relations, pattern recognition is far more accurate and efficient than parsing.
Reference: <author> Alshawi, H. </author> <year> (1987). </year> <title> Processing dictionary definitions with phrasal pattern hierarchies. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 13(3) </volume> <pages> 195-202. </pages>
Reference-contexts: This approach is similar in spirit to the pattern-based interpretation techniques being used in MRD processing. For example, <ref> (Alshawi 1987) </ref>, in interpreting LDOCE definitions, uses a hierarchy of patterns which consist mainly of part-of-speech indicators and wildcard characters. (Markowitz et al. 1986), (Jensen & Binot 1987), and (Nakamura & Nagao 1988) also use pattern recognition to extract semantic relations such as taxonomy from various dictionaries. (Ahlswede & Evens 1988)
Reference: <author> Batali, J. </author> <year> (1991). </year> <title> Automatic Acquisition and Use of Some of the Knowledge in Physics Texts. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Artificial Intelligence Laboratory. </institution>
Reference-contexts: Since we are trying to acquire lexical information our parsing mechanism should not be one that requires extensive lexical information. In order to detect the lexico-syntactic patterns, we use a unification-based constituent analyzer (taken from <ref> (Batali 1991) </ref>), which builds on the output of a part-of-speech tagger (Cutting et al. 1991). (All code described in this report is written in Common Lisp and run on Sun SparcSta-tions.) We wrote grammar rules for the constituent analyzer to recognize the pattern in (1a).
Reference: <author> Brent, M. R. </author> <year> (1991). </year> <title> Automatic acquisition of subcategorization frames from untagged, free-text corpora. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. </booktitle>
Reference-contexts: Work on acquisition of syntactic information from text corpora includes Brent's <ref> (Brent 1991) </ref> verb subcategorization frame recognition technique and Smadja's (Smadja & McKeown 1990) collocation acquisition algorithm. (Calzolari & Bindi 1990) use corpus-based statistical association ratios to determine lexical information such as prepositional complementation relations, modification relations, and significant compounds.
Reference: <author> Calzolari, N. & R. </author> <month> Bindi </month> <year> (1990). </year> <title> Acquisition of lexical information from a large textual italian corpus. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Computational Linguistics, </booktitle> <publisher> Helsinki. </publisher>
Reference-contexts: Work on acquisition of syntactic information from text corpora includes Brent's (Brent 1991) verb subcategorization frame recognition technique and Smadja's (Smadja & McKeown 1990) collocation acquisition algorithm. <ref> (Calzolari & Bindi 1990) </ref> use corpus-based statistical association ratios to determine lexical information such as prepositional complementation relations, modification relations, and significant compounds. Our methodology is similar to Brent's in its effort to distinguish clear pieces of evidence from ambiguous ones.
Reference: <author> Coates-Stephens, S. </author> <year> (1991). </year> <title> Coping with lexical inadequacy the automatic acquisition of proper nouns from news text. </title> <booktitle> In The Proceedings of the 7th Annual Conference of the UW Centre for the New OED and Text Research: Using Corpora, </booktitle> <pages> pages 154-169, </pages> <publisher> Oxford. </publisher>
Reference: <author> Cutting, D., J. Kupiec, J. Pedersen, & P. </author> <title> Sibun (1991). A practical part-of-speech tagger. </title> <booktitle> Submitted to The 3rd Conference on Applied Natural Language Processing. Grolier (1990). </booktitle> <publisher> Academic American Encyclopedia. Grolier Electronic Publishing, Danbury, Con-necticut. </publisher>
Reference-contexts: Since we are trying to acquire lexical information our parsing mechanism should not be one that requires extensive lexical information. In order to detect the lexico-syntactic patterns, we use a unification-based constituent analyzer (taken from (Batali 1991)), which builds on the output of a part-of-speech tagger <ref> (Cutting et al. 1991) </ref>. (All code described in this report is written in Common Lisp and run on Sun SparcSta-tions.) We wrote grammar rules for the constituent analyzer to recognize the pattern in (1a). As mentioned above, in this experiment we are detecting only unmodified nouns.
Reference: <author> Hearst, M. A. </author> <year> (1991). </year> <title> Noun homograph disambiguation using local context in large text corpora. </title> <booktitle> In The Proceedings of the 7th Annual Conference of the UW Centre for the New OED and Text Research: Using Corpora, </booktitle> <pages> pages 1-22, </pages> <publisher> Oxford. </publisher>
Reference-contexts: Say the hyponym in question has only one sense, but the hypernym has several. Then the task is simplified to determining which sense of the hypernym to link the hyponym to. We can then make use of a lexical disambiguation algorithm, e.g., <ref> (Hearst 1991) </ref>, to determine which sense of the hypernym is being used in the sample sentence.
Reference: <author> Hindle, D. </author> <year> (1990). </year> <title> Noun classification from predicate-argument structures. </title> <booktitle> Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 268-275. </pages>
Reference-contexts: Note also that a term like "broken bone" is not likely to appear in a dictionary or lexicon, although it is a common locution. Semantic Relatedness Information. There has recently been work in the detection of semantically related nouns via, for example, shared argument structures <ref> (Hindle 1990) </ref>, and shared dictionary definition context (Wilks et al. 1990). These approaches attempt to infer relationships among lexical terms by looking at very large text samples and determining which ones are related in a statistically significant way.
Reference: <author> Jacobs, P. & U. </author> <title> Zernik (1988). Acquiring lexical knowledge from text: A case study. </title> <booktitle> In Proceedings of AAAI88, </booktitle> <pages> pages 739-744. </pages>
Reference-contexts: is able to fill in many kinds of frame roles, it requires a parser that produces a detailed structure, and it requires a domain-dependent knowlege base/lexicon. (Velardi & Pazienza 1989) makes use of hand-coded selection restriction and conceptual relation rules in order to assign case roles to lexical items, and <ref> (Jacobs & Zernik 1988) </ref> uses extensive domain knowledge to fill in missing category information for unknown words.
Reference: <author> Jensen, K. & J.-L. </author> <title> Binot (1987). Disambiguating prepositional phrase attachments by using on-line dictionary definitions. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 13(3) </volume> <pages> 251-260. </pages>
Reference-contexts: This approach is similar in spirit to the pattern-based interpretation techniques being used in MRD processing. For example, (Alshawi 1987), in interpreting LDOCE definitions, uses a hierarchy of patterns which consist mainly of part-of-speech indicators and wildcard characters. (Markowitz et al. 1986), <ref> (Jensen & Binot 1987) </ref>, and (Nakamura & Nagao 1988) also use pattern recognition to extract semantic relations such as taxonomy from various dictionaries. (Ahlswede & Evens 1988) compares an approach based on parsing Webster's 7th definitions with one based on pattern recognition, and finds that for finding simple semantic relations, pattern
Reference: <author> Markowitz, J., T. Ahlswede, & M. </author> <title> Evens (1986). Semantically significant patterns in dictionary definitions. </title> <booktitle> Proceedings of the 24th Annual Meeting 7 of the Association for Computational Linguistics, </booktitle> <pages> pages 112-119. </pages>
Reference-contexts: This approach is similar in spirit to the pattern-based interpretation techniques being used in MRD processing. For example, (Alshawi 1987), in interpreting LDOCE definitions, uses a hierarchy of patterns which consist mainly of part-of-speech indicators and wildcard characters. <ref> (Markowitz et al. 1986) </ref>, (Jensen & Binot 1987), and (Nakamura & Nagao 1988) also use pattern recognition to extract semantic relations such as taxonomy from various dictionaries. (Ahlswede & Evens 1988) compares an approach based on parsing Webster's 7th definitions with one based on pattern recognition, and finds that for finding
Reference: <author> Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, & K. J. </author> <title> Miller (1990). Introduction to WordNet: An on-line lexical database. </title> <journal> Journal of Lexicography, </journal> <volume> 3(4) </volume> <pages> 235-244. </pages>
Reference-contexts: We use the term hyponym similarly to the sense used in <ref> (Miller et al. 1990) </ref>: a concept represented by a lexical item L 0 is said to be a hyponym of the concept represented by a lexical item L 1 if native speakers of English accept sentences constructed from the frame An L 0 is a (kind of) L 1 . <p> The approach presented in this paper, using the algorithm sketched in the previous subsection, is potentially extensible. 3 Incorporating Results into WordNet To validate this acquisition method, we compared the results of a restricted version of the algorithm with information found in WordNet. 2 WordNet <ref> (Miller et al. 1990) </ref> is a hand-built online thesaurus whose organization is modeled after the results of psycholinguistic research. To use the authors' words, Wordnet "... is an attempt to organize lexical information in terms of word meanings, rather than word forms.
Reference: <author> Morris, J. & G. </author> <title> Hirst (1991). Lexical cohesion computed by thesaural relations as an indicator of the structure of text. </title> <journal> Computational Linguistics, </journal> <volume> 17(1) </volume> <pages> 21-48. </pages>
Reference-contexts: Connecting terms whose expressions are quite disparate but whose meanings are similar should be useful for improved synonym expansion in information retrieval and for finding chains of semantically related phrases, as used in the approach to recognition of topic boundaries of <ref> (Morris & Hirst 1991) </ref>. We observe that terms that occur in a list are often related semantically, whether they occur in a hyponymy relation or not. In the next section we outline a way to discover these lexico-syntactic patterns as well as illustrate those we have found.
Reference: <author> Nakamura, J. & M. </author> <title> Nagao (1988). Extraction of semantic information from an ordinary english dictionary and its evaluation. </title> <booktitle> In Proceedings of the Twelfth International Conference on Computational Linguistics, </booktitle> <pages> pages 459-464, </pages> <address> Budapest. </address>
Reference-contexts: This approach is similar in spirit to the pattern-based interpretation techniques being used in MRD processing. For example, (Alshawi 1987), in interpreting LDOCE definitions, uses a hierarchy of patterns which consist mainly of part-of-speech indicators and wildcard characters. (Markowitz et al. 1986), (Jensen & Binot 1987), and <ref> (Nakamura & Nagao 1988) </ref> also use pattern recognition to extract semantic relations such as taxonomy from various dictionaries. (Ahlswede & Evens 1988) compares an approach based on parsing Webster's 7th definitions with one based on pattern recognition, and finds that for finding simple semantic relations, pattern recognition is far more accurate
Reference: <author> Smadja, F. A. & K. R. </author> <title> McKeown (1990). Automatically extracting and representing collocations for language generation. </title> <booktitle> In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 252-259. </pages>
Reference-contexts: Work on acquisition of syntactic information from text corpora includes Brent's (Brent 1991) verb subcategorization frame recognition technique and Smadja's <ref> (Smadja & McKeown 1990) </ref> collocation acquisition algorithm. (Calzolari & Bindi 1990) use corpus-based statistical association ratios to determine lexical information such as prepositional complementation relations, modification relations, and significant compounds. Our methodology is similar to Brent's in its effort to distinguish clear pieces of evidence from ambiguous ones.
Reference: <author> Velardi, P. & M. T. </author> <title> Pazienza (1989). </title> <booktitle> Computer aided interpretation of lexical cooccurrences. Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 185-192. </pages>
Reference-contexts: FUNES differs quite strongly from our approach in that, because it is able to fill in many kinds of frame roles, it requires a parser that produces a detailed structure, and it requires a domain-dependent knowlege base/lexicon. <ref> (Velardi & Pazienza 1989) </ref> makes use of hand-coded selection restriction and conceptual relation rules in order to assign case roles to lexical items, and (Jacobs & Zernik 1988) uses extensive domain knowledge to fill in missing category information for unknown words.
Reference: <author> Wilks, Y. A., D. C. Fass, C. ming Guo, J. E. Mc-Donald, T. Plate, & B. M. </author> <month> Slator </month> <year> (1990). </year> <title> Providing machine tractable dictionary tools. </title> <journal> Journal of Machine Translation, </journal> <volume> 2. </volume> <pages> 8 </pages>
Reference-contexts: Semantic Relatedness Information. There has recently been work in the detection of semantically related nouns via, for example, shared argument structures (Hindle 1990), and shared dictionary definition context <ref> (Wilks et al. 1990) </ref>. These approaches attempt to infer relationships among lexical terms by looking at very large text samples and determining which ones are related in a statistically significant way.
References-found: 18

