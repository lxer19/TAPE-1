URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/96-05.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Title: DEGRADED TEXT RECOGNITION USING VISUAL AND LINGUISTIC CONTEXT  
Author: by Tao Hong 
Degree: A dissertation submitted to the Faculty of the Graduate School of the  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  
Date: September, 1995  
Affiliation: State University of New York at Buffalo  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. S. Huang A. B. Wang and K. C. Fan. </author> <title> Optical recognition of handwriting chinese characters by partial matching. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis (ICDAR-93), </booktitle> <pages> pages 822-826, </pages> <year> 1993. </year>
Reference-contexts: Generally, similarities at the lexicographical level and the visual level are highly consistent. As we observed, if character categories share the same lexicographical element in a region, their image instances are usually similar visually in that region. There are previous studies on Chinese character recognition using radical-based partial matching <ref> [1, 168] </ref>. Although radicals are well-defined lexicographically, it is time-consuming to manually locate all possible character radicals and describe the structure of each Chinese character according to the radical set. The goal of our work is to conduct a visual similarity analysis among Chinese characters at the image level.
Reference: [2] <author> T. Akiyama and N. Hagita. </author> <title> Automated entry system for printed documents. </title> <journal> Pattern Recognition, </journal> <volume> Vol. 23, No. 11 </volume> <pages> 1141-1154, </pages> <year> 1990. </year>
Reference-contexts: To represent the stroke structure of a Chinese character, its feature vector usually has many dimensions. For some types of structural features such as local stroke direction (LSD) <ref> [2, 133] </ref>, geometric information is retained as in the original image. We use LSD features for our experiments.
Reference: [3] <author> J. Allen. </author> <title> Natural Language Understanding, second edition. </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1995. </year>
Reference-contexts: Different theories have been proposed to model syntactic, semantic, pragmatic, and discourse aspects of language <ref> [3, 162] </ref>. Natural language processing systems usually contain several essential parts, such as a lexicon, a grammar, and a parser. A lexicon contains knowledge about words in a language. Words are the units on which more complex language structures, such as phrases, sentences and discourses, are based.
Reference: [4] <author> H. Alshawi. </author> <title> Qualitative and quantitative methods of speech translation. In Proceedings of the Workshop: The Balancing Act Combining Symbolic and Statistical Approaches to Language, </title> <month> July 1 </month> <year> 1994. </year>
Reference-contexts: Many speech recognition systems use statistical language models exclusively, whereas many natural language understanding systems are symbolic. In recent years, there have been many efforts to combine the two approaches in tasks like speech understanding, text understanding and machine translation <ref> [4, 100, 103, 114, 146] </ref>. In a natural language processing system built with a symbolic approach, linguistic knowl BACKGROUND 30 edge may be represented by rules. The rules are often written manually, usually with a large human effort.
Reference: [5] <author> S.C. Bagley and G.E. Kopec. </author> <title> Editing images of text. </title> <journal> CACM, </journal> <year> 1994. </year>
Reference-contexts: are visually similar (character segmentation and recognition for those words will be easier if we consider that they have to be interpreted as the same word; and (c) examples of partial similarties which can be found between word images. 12.6 Summary Image-based document retrieval [21, 71, 145] and text editing <ref> [5] </ref> are two interesting applications in which visual and linguistic information can also be utilized. The objective of visual text recognition is to correctly transform an arbitrary image of text into its symbolic equivalent.
Reference: [6] <author> H.S. Baird. </author> <title> Anatomy of a versatile page reader. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1059-1065, </pages> <year> 1992. </year>
Reference-contexts: Chapter 2 Background Visual text recognition is an application area of image processing, pattern recognition and natural language processing. In this chapter, research in several related areas is surveyed. 2.1 Text Recognition To automatically recognize the text in a document image, the first step is page layout analysis <ref> [6, 19, 109, 155] </ref>. The process of layout analysis detects and extracts text blocks and text lines. Each text line can be further segmented into a sequence of word images. Because of the space between words in machine-printed English text, it is not difficult to find word boundaries. <p> As a general framework to resolve ambiguity at several levels, a lattice has been identified as a useful structure to keep records of possible segmentation points and character alternatives according to their geometric positions. Various lattice-based methods have been applied <ref> [6, 135, 150, 154] </ref>. In previous studies, each entry in a lattice is only a representation at the character level. Sponsored by ARPA (Advanced Research Projects Agency), RAF Technology, Inc. is developing ILLUMINATOR, a new framework for document decomposition, recognition, storage and interchange [119].
Reference: [7] <author> H.S. Baird. </author> <title> Document image defect models. </title> <editor> In H. S. Baird, H. Bunke, and K. Ya-mamoto, editors, </editor> <booktitle> Structural Document Image Analysis, </booktitle> <pages> pages 546-556. </pages> <publisher> Springer Ver-lag, </publisher> <address> N.Y., </address> <year> 1992. </year> <note> 212 BIBLIOGRAPHY 213 </note>
Reference-contexts: Characters may become blurred, distorted, touch together, or split into smaller pieces. To characterize different sources of degradation on document pages, document degradation models (DDM) have been proposed <ref> [7, 8, 81, 80] </ref>. Tools based on those DDMs and defect images generated by the methods can be found in the UW English Document Images Database, which is available in CDROM form [112]. Those parameterized models can quantitatively describe local or global image defects. <p> The visual similarity measure discussed in this thesis works well for document images which are degraded by random noise or other uniformly distributed noise. For images with artifacts, such as character skew and perspective distortion <ref> [7, 81] </ref>, more complex similarity measures can be developed. Future Directions 203 12.4 A Unified Approach toward Text Recognition Character segmentation, character recognition (OCR) and postprocessing are three major components of a text recognition system.
Reference: [8] <author> H.S. Baird. </author> <title> Document image defect models and their uses. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93, </booktitle> <pages> pages 62-67, </pages> <year> 1993. </year>
Reference-contexts: While very high accuracy has been achieved on high-quality character images, the performance of OCR on poor-quality character images is still not satisfactory <ref> [8, 111] </ref>. Improving performance on poor quality documents is a challenging problem to which much OCR research is now devoted. New feature representations and new classification methods have been proposed [51, 111]. <p> Characters may become blurred, distorted, touch together, or split into smaller pieces. To characterize different sources of degradation on document pages, document degradation models (DDM) have been proposed <ref> [7, 8, 81, 80] </ref>. Tools based on those DDMs and defect images generated by the methods can be found in the UW English Document Images Database, which is available in CDROM form [112]. Those parameterized models can quantitatively describe local or global image defects. <p> Given a high quality text page, current commercial document recognition systems can recognize the words on the page at a high correct rate [22, 125, 126]. However, given a degraded text page, such as a multiple-generation photocopy or facsimile, their performance usually drops abruptly <ref> [8, 111] </ref>. image can be read correctly by a human but an OCR system makes a significant number of mistakes. The recognition results generated by a commercial OCR system are also shown in the same figure.
Reference: [9] <author> H.S. Baird and G. Nagy. </author> <title> A self-correcting 100-font classifier. </title> <booktitle> In Proceedings of the Conference on Document Recognition of 1994 S&T/SPIE Symposium, </booktitle> <month> February 6-10 </month> <year> 1994. </year>
Reference-contexts: Given a character image, a character recognition algorithm generates decisions about the character's BACKGROUND 12 identity. This can be done by template matching or structural analysis. To recognize any of the hundreds of typefaces in common use, current omni-font OCR engines take the structural analysis approach <ref> [9, 11, 79] </ref>. Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes [79, 106, 143]. <p> The role of training sets for classification has been investigated and it has been suggested that the quality of training sets, rather than classification methodology, is the determining factor in achieving higher accuracy [52]. By taking advantage of local typeface homogeneity, OCR accuracy can be improved <ref> [9] </ref>. The combination of several classifiers has been shown as a way to make recognition more accurate [50, 164]. Besides problems of isolated character classification, improper character segmentation has been identified as one of the major sources of incorrect recognition [18]. <p> Syntactic, semantic and pragmatic constraints also cause many words, such as so-called "function words" and "content words," to appear with high frequency in a natural language text. Although there are thousands of fonts, there are usually only several of them used in a real document <ref> [9] </ref>. For those words printed in the same font, typesetting parameters, such as letter space, are usually consistent within a page or an article [91]. <p> The approach proposed here is an extension of the character-based clustering and deciphering algorithms used previously [16, 18, 108]. The concept of a self-teaching OCR system has also been used in a character classifier that automatically adapts itself to a single font <ref> [9] </ref>. The underlying assumption that a given document is printed primarily in a OCR POSTPROCESSING USING VISUAL CONTEXT 97 small number of fonts is also utilized in the algorithm proposed here. The rest of this chapter discusses the proposed algorithm. The procedure for computing visual inter-word relations is discussed.
Reference: [10] <author> H.S. Baird and K. Thompson. </author> <title> Reading chess. </title> <journal> IEEE Transactions on pattern analysis and machine intelligence, </journal> <volume> 12(6) </volume> <pages> 552-559, </pages> <year> 1990. </year>
Reference-contexts: Previous approaches have utilized local word-to-word transitions [68], statistical part-of-speech transitions [70], definitional overlap [40], word collocation constraints [130], transitions between hypertags [141], N-best strategies [31] and semantic constraints from a limited domain, such as chess <ref> [10] </ref>. Post-editing through approximation and global correction was demonstrated as a useful method in which a string with errors can be corrected because it is very close to some well-recognized words (centroids) in the text [148, 149].
Reference: [11] <author> M. Bokser. </author> <booktitle> Omnidocument technologies. Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1066-1078, </pages> <year> 1992. </year>
Reference-contexts: Given a character image, a character recognition algorithm generates decisions about the character's BACKGROUND 12 identity. This can be done by template matching or structural analysis. To recognize any of the hundreds of typefaces in common use, current omni-font OCR engines take the structural analysis approach <ref> [9, 11, 79] </ref>. Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes [79, 106, 143]. <p> In this situation, several characters may be segmented as one character image or one character image may be split into two or more pieces. Many methods have been developed to improve character segmentation accuracy. Many current methods are based on connected component analysis, aspect ratio estimation, or profile analysis <ref> [11, 35, 79, 93] </ref>. <p> But it was also reported that the accuracy of the OCR systems declined dramatically when the images are degraded or the resolution of the images was reduced from 300 to 200dpi. Current OCR systems use lexical constraints and domain knowledge to detect and correct character recognition errors <ref> [79, 11] </ref>.
Reference: [12] <author> R.D. Borsley. </author> <title> Syntactic Theory: a Unified Approach. </title> <editor> Edward Arnold, </editor> <year> 1991. </year>
Reference-contexts: The structural descriptions are necessary for further processing, for example, for semantic interpretation. In the last three decades, many frameworks have been developed to model the syntactic phenomena of English <ref> [12, 116, 136, 157] </ref>. Among them are context-free grammars (CFG), transformational grammars (TG), lexical functional grammars (LFG), generalized phrase structure grammars (GPSG) and government-binding (GB) theory. In an NLP system, a grammar is usually represented as a rule base.
Reference: [13] <author> T. Briscoe and J. Carroll. </author> <title> Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 25-60, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Because syntactic phenomena are complex, a grammar with comprehensive coverage can be difficult to obtain. Many parsing methods have been developed in the last thirty years, such as ATN parsers [163], generalized LR parsers <ref> [13, 151] </ref>, deterministic parsers [101], unification algorithms [83] and chart parsers [82, 166]. Among these techniques, chart parsing is quite popular. The chart in a chart parser is a data structure that stores the information about syntactic structures (or subtrees) already derived from an input sentence.
Reference: [14] <author> P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. Lafferty, and R. Mercer. </author> <title> A statistical approach to machine translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2) </volume> <pages> 79-85, </pages> <year> 1990. </year>
Reference-contexts: intelligence and computational linguistics has shown that statistical methods can achieve promising performance in the tasks of lexical acquisition [75], tagging [25], parsing [42, 99, 100] and sense disambiguation [140] and that these informa BACKGROUND 26 tion sources can be used in systems for natural language processing [161], machine translation <ref> [14] </ref>, information retrieval [88], and speech recognition [160]. The statistical information is usually collected by training on a large text corpus. This approach is also called corpus-based natural language processing. The Brown Corpus is an English text corpus with about one million words [89].
Reference: [15] <author> Thomas H. Carr. </author> <title> Perceiving visual language. </title> <editor> In L. Kaufman and J. Thomas, editors, </editor> <title> Handbook of Perception and Human Performance, page Chapter 29. </title> <address> New York, </address> <publisher> Wiley, </publisher> <year> 1992. </year>
Reference-contexts: decoder estimates the message, by finding the a posteriori most probable path (so-called "maximum a posteriori (MAP)") through the combined source and channel models using a Viterbi-like dynamic programming algorithm. 2.1.3 Reading by Hypothesis Generation and Testing Based on previous studies of how people read and the psychology of reading <ref> [15, 113, 120] </ref>, a computational theory for the visual recognition of words of text was proposed [69]. The theory includes three stages: hypothesis generation, hypothesis testing, and global contextual analysis. Hypothesis generation uses gross visual features to provide expectations about word identities.
Reference: [16] <author> R.G. Casey. </author> <title> Text OCR by solving a cryptogram. </title> <booktitle> In IEEE Proceedings, </booktitle> <pages> pages 349-351, </pages> <year> 1986. </year> <note> BIBLIOGRAPHY 214 </note>
Reference-contexts: inherit the identity of the cluster, there are at least two applicable methods: one is simply to apply a character recognition algorithm; another is to use a deciphering algorithm in which the characters are treated as substitution ciphers and their true values can be solved by a dictionary-based deciphering algorithm <ref> [16, 108, 39] </ref>. <p> After clustering character images in a text page, a dictionary-based deciphering algorithm can be applied to find the best interpretation of each cluster so that the character images can be recognized without using OCR <ref> [16, 108] </ref>. By clustering visually equivalent word images, many function words and content words inside the text page can be found [72, 84]. This chapter studies visual contextual constraints more extensively. English is an alphabetic language. <p> This is then used in a self-teaching OCR system to recognize the rest of the text in the document. The approach proposed here is an extension of the character-based clustering and deciphering algorithms used previously <ref> [16, 18, 108] </ref>. The concept of a self-teaching OCR system has also been used in a character classifier that automatically adapts itself to a single font [9].
Reference: [17] <author> R.G. Casey. </author> <title> Character segmentation in document OCR: Progress and hope. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 13-40, </pages> <month> April 24-26 </month> <year> 1995. </year>
Reference-contexts: Besides problems of isolated character classification, improper character segmentation has been identified as one of the major sources of incorrect recognition [18]. As an important preprocessing step, character segmentation partitions word images into sequences of character images so that OCR techniques can be applied <ref> [17] </ref>. The performance of a text recognition system can depend heavily upon the performance of its character segmentation step. The simplest method of character segmentation is the use of the small space between characters as a segmentation point.
Reference: [18] <author> R.G. Casey and G. Nagy. </author> <title> Recursive segmentation and classification of composite character patterns. </title> <booktitle> In Proc. 6th ICPR, </booktitle> <pages> pages 1023-1026, </pages> <year> 1982. </year>
Reference-contexts: The combination of several classifiers has been shown as a way to make recognition more accurate [50, 164]. Besides problems of isolated character classification, improper character segmentation has been identified as one of the major sources of incorrect recognition <ref> [18] </ref>. As an important preprocessing step, character segmentation partitions word images into sequences of character images so that OCR techniques can be applied [17]. The performance of a text recognition system can depend heavily upon the performance of its character segmentation step. <p> Many current methods are based on connected component analysis, aspect ratio estimation, or profile analysis [11, 35, 79, 93]. There are also some methods that integrate character segmentation with character recognition under the belief that segmentation decisions are tentative until confirmed by the successful recognition of segmented image pieces <ref> [18, 154] </ref>. 2.1.1.2 Commercial OCR Systems Since 1992, the Information Science Research Institute (ISRI) of University of Nevada has conducted an annual test of OCR systems [125, 126, 124]. <p> This is then used in a self-teaching OCR system to recognize the rest of the text in the document. The approach proposed here is an extension of the character-based clustering and deciphering algorithms used previously <ref> [16, 18, 108] </ref>. The concept of a self-teaching OCR system has also been used in a character classifier that automatically adapts itself to a single font [9].
Reference: [19] <author> R.G. Casey and K.Y. Wong. </author> <title> Document-analysis systems and techniques. </title> <editor> In R. Kasturi and M. Trivedi, editors, </editor> <booktitle> Image Analysis and Applications, </booktitle> <pages> pages 1-35. </pages> <year> 1990. </year>
Reference-contexts: Chapter 2 Background Visual text recognition is an application area of image processing, pattern recognition and natural language processing. In this chapter, research in several related areas is surveyed. 2.1 Text Recognition To automatically recognize the text in a document image, the first step is page layout analysis <ref> [6, 19, 109, 155] </ref>. The process of layout analysis detects and extracts text blocks and text lines. Each text line can be further segmented into a sequence of word images. Because of the space between words in machine-printed English text, it is not difficult to find word boundaries.
Reference: [20] <author> J.S. Chang, Y.F. Luo, and K.Y. Su. Gpsm: </author> <title> A generalised probabilistic semantic model for ambiguity resolution. </title> <booktitle> In Proceedings of the 30nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <month> June 28 - July 2 </month> <year> 1992. </year>
Reference-contexts: Systems using this approach are more robust, and easier to build and maintain. Statistical language models have been applied to help symbolic systems in many important tasks, such as lexical acquisition, part-of-speech tagging, sentence parsing, sense disambiguation and discourse analysis <ref> [20, 98, 123, 122, 139] </ref>. 2.2.4 The Noisy Channel Model for Speech Recognition, OCR and Spelling Correction The noisy channel model developed in Information Theory was originally used to model communication along a noisy channel such as a telephone line.
Reference: [21] <author> F.R. Chen, L.D. Wilcox, and D.S. Bloomberg. </author> <title> Detecting and locating partially specified keywords in scanned images using hidden markov models. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93, </booktitle> <pages> pages 133-138, </pages> <year> 1993. </year>
Reference-contexts: items of the word "Lloyd" which are visually similar (character segmentation and recognition for those words will be easier if we consider that they have to be interpreted as the same word; and (c) examples of partial similarties which can be found between word images. 12.6 Summary Image-based document retrieval <ref> [21, 71, 145] </ref> and text editing [5] are two interesting applications in which visual and linguistic information can also be utilized. The objective of visual text recognition is to correctly transform an arbitrary image of text into its symbolic equivalent.
Reference: [22] <author> S. Chen, S. Subramaniam, R.M. Haralick, and I.T. Phillips. </author> <title> Performance evaluation of two OCR systems. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 299-317, </pages> <year> 1993. </year>
Reference-contexts: Given a high quality text page, current commercial document recognition systems can recognize the words on the page at a high correct rate <ref> [22, 125, 126] </ref>. However, given a degraded text page, such as a multiple-generation photocopy or facsimile, their performance usually drops abruptly [8, 111]. image can be read correctly by a human but an OCR system makes a significant number of mistakes.
Reference: [23] <author> L.F. Chien, K.J. Chen, and L.S. Lee. </author> <title> An Augmented Chart Data Structure with Efficient Word Lattice Parsing Scheme in Speech Recognition Application. </title> <booktitle> In COLING 1990, </booktitle> <year> 1990. </year>
Reference-contexts: Augmented with semantic and pragmatic knowledge, a lattice parser can select candidates which are not only a grammatical sequence, but also a meaningful one. Lattice parsers were designed for word candidate selection in speech recognition <ref> [23, 48, 152] </ref>. For example, "I saw the man" can be genegrated by a lattice parser as the best word sequence for the word lattice in Figure 2.5. <p> Besides phonemic and phonologic knowledge, syntactic, semantic, prosodic, pragmatic knowledge sources and knowledge about the real world are applied to speech recognition. Under the approaches of statistical language models and structural analysis, methods such as the N-best search [134], HMM [67] and lattice parsing <ref> [23, 48, 152] </ref> are designed to select word candidates from multiple choices generated by acoustic-phonetic analysis. 2.3 Conclusions Text recognition is not only an image processing and pattern recognition task, but also a natural language processing task. <p> Statistical methods can be easily incorporated in a chart parser [100]. A chart parser can be extended to a lattice parser which allows for several word candidates at the same position, and therefore can be directly used for speech recognition <ref> [23, 152] </ref> and visual text recognition. The parser chooses the words on a path through the lattice that correspond to a legal sentence with the highest probability of being correct, given the sentences represented in the lattice.
Reference: [24] <author> B.H. Chou and J.S. Chang. </author> <title> Language models for chinese character recognition. </title> <booktitle> In Proceedings of ROCLING V, </booktitle> <pages> pages 259-286, </pages> <year> 1986. </year>
Reference-contexts: Linguistic context can be applied to choose decisions for those character images which have competitive candidates generated by a Chinese character classifier. Like that for English text recognition, linguistic knowledge sources useful here include word dictionaries, statistical language models, language syntax and semantics <ref> [24, 43, 78] </ref>. Character recognition results for a Chinese sentence are shown in Figure 10.9 (a). The translation of the sentence is "He congratulates your success" There are six Chinese characters in the sentence. For each character, three character candidates are provided by a classifier.
Reference: [25] <author> K.W. Church. </author> <title> A Stochastic Part Program and Noun Phrase Parser for Unrestricted Text. </title> <booktitle> In Second Conference on Appled Natural Language Processing, </booktitle> <year> 1988. </year> <note> BIBLIOGRAPHY 215 </note>
Reference-contexts: In the section, we will study methodologies in areas of natural language processing and techniques developed for applications. 2.2.1 Statistical Language Modeling Research from artificial intelligence and computational linguistics has shown that statistical methods can achieve promising performance in the tasks of lexical acquisition [75], tagging <ref> [25] </ref>, parsing [42, 99, 100] and sense disambiguation [140] and that these informa BACKGROUND 26 tion sources can be used in systems for natural language processing [161], machine translation [14], information retrieval [88], and speech recognition [160]. The statistical information is usually collected by training on a large text corpus.
Reference: [26] <author> K.W. Church, W. Gale, P. Hank, and D. Hindle. </author> <title> Word association norms, mutual information, and lexicography. </title> <journal> Computational Linguistics, </journal> <volume> 16(1) </volume> <pages> 22-29, </pages> <year> 1990. </year>
Reference-contexts: In the last three decades, several more large English corpora have been built, such as the AP Corpus, the British National Corpus and the Penn Treebank [28]. The most typical statistical information directly extracted from a large corpus is word collocation, which has a simple format <ref> [26, 27] </ref>. <p> It might also be some idiosyncratic relation which exists but is difficult to explain (for example, the word pairs &lt; strong; tea &gt; and &lt; powerf ul; car &gt;). As Church pointed out, there are a variety of interesting linguistic phenomena can be identified by word collocation information <ref> [26] </ref>. Word collocation can be used as a contextual constraint for word candidate selection in BACKGROUND 27 visual text recognition and speech recognition [26]. <p> As Church pointed out, there are a variety of interesting linguistic phenomena can be identified by word collocation information <ref> [26] </ref>. Word collocation can be used as a contextual constraint for word candidate selection in BACKGROUND 27 visual text recognition and speech recognition [26]. <p> Chapter 7 A Relaxation Algorithm Using Word Collocation 7.1 Introduction Word collocation data is one source of information that has been investigated in computational linguistics and that has been proposed as a useful tool to post-process word recognition results <ref> [26, 130] </ref>. Word collocation refers to the likelihood that two words co-occur within a fixed distance of one another. <p> Another is local word collocation in which the identity of a word can be used to predict the identity of other nearby words. Local word collocation data can be collected by training on large text corpora <ref> [26] </ref>. The global structural constraints are exploited with a chart parsing model. The chart data structure provides a flexible framework for parsing [82, 166]. Statistical methods can be easily incorporated in a chart parser [100]. <p> Currently, there are two approaches, the statistical approach and the structural approach, towards the problem of candidate selection. In the statistical approach, language models such as a Hidden Markov Model and word collocation can be utilized for candidate selection <ref> [26, 58, 70] </ref>. In the structural approach, lattice parsing techniques have been developed for candidate selection [62, 152]. The contextual constraints considered in a statistical language model, such as word collocation, are local constraints.
Reference: [27] <author> K.W. Church, W. Gale, P. Hank, and D. Hindle. </author> <title> Using statistics in lexical analysis. In Uri Zernik, editor, Lexical Acquisition: Exploitung On-Line Resources to Build a Lexicon, </title> <address> pages 115-164. </address> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <year> 1991. </year>
Reference-contexts: In the last three decades, several more large English corpora have been built, such as the AP Corpus, the British National Corpus and the Penn Treebank [28]. The most typical statistical information directly extracted from a large corpus is word collocation, which has a simple format <ref> [26, 27] </ref>.
Reference: [28] <author> K.W. Church and R.L. Mercer. </author> <title> Introduction to the special issue on computational linguistics using large corpora. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 1-24, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: The Brown Corpus was built in the mid 1960s. In the last three decades, several more large English corpora have been built, such as the AP Corpus, the British National Corpus and the Penn Treebank <ref> [28] </ref>. The most typical statistical information directly extracted from a large corpus is word collocation, which has a simple format [26, 27]. <p> The model can be applied to recognition applications such as speech recognition, OCR and spelling correction <ref> [28] </ref>. A BACKGROUND 31 recognition system can be considered as a noisy channel. Its input is a sequence of good text (I ), and its output is usually a sequence of corrupted text (O).
Reference: [29] <editor> P.R. Cohen and E.A. Feigenbaum, editors. </editor> <booktitle> The Handbook of Artificial Intelligence, </booktitle> <volume> Volume 3. </volume> <publisher> William Kaufmann, INC., </publisher> <year> 1982. </year>
Reference-contexts: Another is the so-called "structural/symbolic approach." 3.1.4.1 Postprocessing Using Statistical Language Model Word collocation is a simple but powerful statistical language model. By formalizing the FRAMEWORK 42 word candidate selection problem as an instance of a constraint satisfaction problem [97], we proposed relaxation (pp. 292-300 in <ref> [29] </ref>) for word candidate selection. The probabilistic relaxation algorithm uses word collocation trained from large text corpora to re-evaluate the confidence scores of word candidates and to re-rank them based on their new confidence scores.
Reference: [30] <author> T.H. Cormen, C.E. Leiserson, and R.L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Just like what happened in string matching, much of the computation for matching two slightly shifted regions with a pattern is redundant so it can be avoided. By generalizing fast algorithms designed for string matching (see the Rabin-Karp algorithm, the Knuth-Morris-Pratt algorithm and the Boyer-moore algorithm in <ref> [30] </ref>), fast algorithms for image matching can also be proposed. Another promising direction to improve efficiency is to use inter-word relations at the symbolic level to guide the computation of visual relations. <p> To improve efficiency, in this thesis, we proposed to use feature-matching and OCR results to guide image matching. Other information, such as connected component information and image profiles, can also be used to speed up image matching Similar to algorithms for string matching <ref> [30] </ref>, fast algorithms for image matching can also be proposed. The visual similarity measure discussed in this thesis works well for document images which are degraded by random noise or other uniformly distributed noise.
Reference: [31] <author> D.A. Dahl, L.M. Norton, and S.L. Taylor. </author> <title> Improving OCR accuracy with linguistic knowledge. </title> <booktitle> In Proceedings of the Second Annual Symposium on Document Anaysis and Information Retrieval, </booktitle> <pages> pages 169-177, </pages> <address> Las Vegas, Nevada, </address> <month> April 26-28 </month> <year> 1993. </year>
Reference-contexts: To select a candidate for each word image, those constraints can play a crucial role. Previous approaches have utilized local word-to-word transitions [68], statistical part-of-speech transitions [70], definitional overlap [40], word collocation constraints [130], transitions between hypertags [141], N-best strategies <ref> [31] </ref> and semantic constraints from a limited domain, such as chess [10]. Post-editing through approximation and global correction was demonstrated as a useful method in which a string with errors can be corrected because it is very close to some well-recognized words (centroids) in the text [148, 149].
Reference: [32] <author> B.V. Dasarathy. </author> <title> Nearest Neighbor(NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes [79, 106, 143]. Many classification methods, such as nearest-neighbor classifiers <ref> [32] </ref>, Bayesian classifiers [34], neural network classifiers [94, 132], decision tree classifiers [105] and syntactic classifiers [41], have been designed to classify character images. While very high accuracy has been achieved on high-quality character images, the performance of OCR on poor-quality character images is still not satisfactory [8, 111].
Reference: [33] <author> G. DeSilva and J.J. Hull. </author> <title> Proper noun location in document images. </title> <journal> Pattern Recognition, </journal> <pages> pages 311-320, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: In clusters 41, 43, 59, and 66, the correct choice was among the decisions in the cluster. It just was not chosen by the postprocessing algorithm. This could be solved by including more proper nouns in the dictionary or perhaps detecting proper nouns a-priori <ref> [33] </ref> and recognizing them with a specialized form of divide-and-conquer. Cluster 52 would be best solved by the second form of this strategy. The remainder of the errors illustrate the need for improving the rejection strategy or that some errors are probably non-recoverable.
Reference: [34] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes [79, 106, 143]. Many classification methods, such as nearest-neighbor classifiers [32], Bayesian classifiers <ref> [34] </ref>, neural network classifiers [94, 132], decision tree classifiers [105] and syntactic classifiers [41], have been designed to classify character images. While very high accuracy has been achieved on high-quality character images, the performance of OCR on poor-quality character images is still not satisfactory [8, 111].
Reference: [35] <author> D.G. Elliman and I.T. Lancaster. </author> <title> A review of segmentation and contextual analysis techniques for text recognition. </title> <journal> Pattern Recognition, </journal> 23(3/4):337-346, 1990. BIBLIOGRAPHY <volume> 216 </volume>
Reference-contexts: In this situation, several characters may be segmented as one character image or one character image may be split into two or more pieces. Many methods have been developed to improve character segmentation accuracy. Many current methods are based on connected component analysis, aspect ratio estimation, or profile analysis <ref> [11, 35, 79, 93] </ref>. <p> Contextual constraints which have been utilized successfully in practical systems are limited to the word-level <ref> [35, 54, 90] </ref>. The lexical constraint is the one that is used most BACKGROUND 15 widely. Methods of dictionary look-up [158], confusion-matrix-based transformation and string editing [36, 79], probabilistic relaxation [44], and character n-gram frequency analysis [74, 142] have been developed to exploit the lexical constraint efficiently.
Reference: [36] <author> J. Esakov, D.P. Lopresti, J.S. Sandberg, and J. Zhou. </author> <title> Issues in automatic OCR error classification. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 401-412, </pages> <year> 1993. </year>
Reference-contexts: Contextual constraints which have been utilized successfully in practical systems are limited to the word-level [35, 54, 90]. The lexical constraint is the one that is used most BACKGROUND 15 widely. Methods of dictionary look-up [158], confusion-matrix-based transformation and string editing <ref> [36, 79] </ref>, probabilistic relaxation [44], and character n-gram frequency analysis [74, 142] have been developed to exploit the lexical constraint efficiently. Such methods work well on non-word error detection.
Reference: [37] <author> L.J. Evett, C.J. Wells, F.G. Keenan, T. Rose, and R.J. Whitrow. </author> <title> Using linguistic information to aid handwriting recognition. </title> <booktitle> In Proceedings of the International Workshop on Frontiers in Handwriting Recognition, </booktitle> <pages> pages 303-311, </pages> <year> 1991. </year>
Reference-contexts: Future Directions 208 12.5 Handwritten Text Recognition Handwritten text recognition is a similar, but more difficult task. Techniques designed here for machine-printed text recognition can be adapted to word candidate selection for this task. Significant research has been conducted on using linguistic contextual information to improve recognition performance <ref> [37, 156, 141, 159] </ref>. Because word segmentation sometimes is not an easy task, many possible word boundaries can be generated and the structure of a word lattice may become more complex: positions of word candidates may overlap (see the example inFigure 12.5).
Reference: [38] <author> Inc. ExperVision. </author> <title> TypeReader professional for Windows, user's guide. </title> <year> 1995. </year>
Reference-contexts: In OCR systems such as Caere's OmniPage, BACKGROUND 14 Calera's WordScan Plus, ExpertVision's TypeReader and XIS's TextBridge, for the characters or words on which the recognition results are not confident, suspicion marks are provided so that the user can verify and edit them manually with a graphical interface <ref> [147, 38] </ref> (see at the original word image and its linguistic context. marked word in the OCR result, the verifier can display the original word image so that the user can verify the recognition result easily.
Reference: [39] <author> C. Fang and J. J. Hull. </author> <title> A modified character level deciphering algorithm for ocr in degraded documents. </title> <booktitle> In Proceedings of the Conference on Document Recognition, 1995 SPIE Symposium(SPIE95), </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: inherit the identity of the cluster, there are at least two applicable methods: one is simply to apply a character recognition algorithm; another is to use a deciphering algorithm in which the characters are treated as substitution ciphers and their true values can be solved by a dictionary-based deciphering algorithm <ref> [16, 108, 39] </ref>.
Reference: [40] <author> P. Filipski and J.J. Hull. </author> <title> Keyword selection from word recognition results using definitional overlap. </title> <booktitle> In Third Symposium on Document Analysis and Information Retrieval, </booktitle> <month> April 11-13 </month> <year> 1994. </year>
Reference-contexts: Passage-level postprocessing is usually formalized as a candidate selection problem in its simplest case. To select a candidate for each word image, those constraints can play a crucial role. Previous approaches have utilized local word-to-word transitions [68], statistical part-of-speech transitions [70], definitional overlap <ref> [40] </ref>, word collocation constraints [130], transitions between hypertags [141], N-best strategies [31] and semantic constraints from a limited domain, such as chess [10].
Reference: [41] <author> K.S. Fu. </author> <title> Syntactic Pattern Recognition and Applications. </title> <address> Preentice-Hall, Engelwood Cliffs, N.J., </address> <year> 1982. </year>
Reference-contexts: Many classification methods, such as nearest-neighbor classifiers [32], Bayesian classifiers [34], neural network classifiers [94, 132], decision tree classifiers [105] and syntactic classifiers <ref> [41] </ref>, have been designed to classify character images. While very high accuracy has been achieved on high-quality character images, the performance of OCR on poor-quality character images is still not satisfactory [8, 111].
Reference: [42] <author> T. Fujisaki, F. Jelinek, J. Cooke an E. Black, and T. Nishino. </author> <title> A Probabilistic Parsing Method for Sentence Disambiguation. </title> <booktitle> In International Parsing Workshop '89, </booktitle> <year> 1989. </year>
Reference-contexts: In the section, we will study methodologies in areas of natural language processing and techniques developed for applications. 2.2.1 Statistical Language Modeling Research from artificial intelligence and computational linguistics has shown that statistical methods can achieve promising performance in the tasks of lexical acquisition [75], tagging [25], parsing <ref> [42, 99, 100] </ref> and sense disambiguation [140] and that these informa BACKGROUND 26 tion sources can be used in systems for natural language processing [161], machine translation [14], information retrieval [88], and speech recognition [160]. The statistical information is usually collected by training on a large text corpus.
Reference: [43] <author> H. Fujisawa and K. Marukawa. </author> <title> Full-text search and document recognition of japanese text. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 55-80, </pages> <month> April 24-26 </month> <year> 1995. </year>
Reference-contexts: Linguistic context can be applied to choose decisions for those character images which have competitive candidates generated by a Chinese character classifier. Like that for English text recognition, linguistic knowledge sources useful here include word dictionaries, statistical language models, language syntax and semantics <ref> [24, 43, 78] </ref>. Character recognition results for a Chinese sentence are shown in Figure 10.9 (a). The translation of the sentence is "He congratulates your success" There are six Chinese characters in the sentence. For each character, three character candidates are provided by a classifier.
Reference: [44] <author> A. Goshtasby and R.W. Ehrich. </author> <title> Contextual word recognition using probabilistic relaxation labeling. </title> <journal> Pattern Recognition, </journal> <volume> 21(5) </volume> <pages> 455-462, </pages> <year> 1988. </year>
Reference-contexts: Contextual constraints which have been utilized successfully in practical systems are limited to the word-level [35, 54, 90]. The lexical constraint is the one that is used most BACKGROUND 15 widely. Methods of dictionary look-up [158], confusion-matrix-based transformation and string editing [36, 79], probabilistic relaxation <ref> [44] </ref>, and character n-gram frequency analysis [74, 142] have been developed to exploit the lexical constraint efficiently. Such methods work well on non-word error detection.
Reference: [45] <author> J. Guo and H.C. Liu. </author> <title> Ph a Chinese corpus for pinyin-hanzi transcription. </title> <type> Technical report, ISS Technical Report, </type> <institution> National University of Singapore, </institution> <year> 1992. </year> <note> BIBLIOGRAPHY 217 </note>
Reference-contexts: A large Chinese lexicon has been edited. It has about 34,000 words which are assigned POS tags. Statistical data have been collected from two large Chinese corpora, the PH corpus and the HXWZ corpus. The PH corpus includes about 8000 news articles with about 4 million characters <ref> [45] </ref>. The HXWZ corpus is collected from an online Chinese journal "Hua Xia Wen Zhai" (Chinese News Digest) which publishes weekly, since April 1991. Each issue of HXWZ has about 20,000 characters. Statistical data include: character frequency, word frequency, character collocation and word collocation.
Reference: [46] <author> J. A. Hartigan. </author> <title> Clustering Algorithms. </title> <publisher> John Wiley & Sons, </publisher> <year> 1975. </year>
Reference-contexts: In this way, the speed of image matching operations can be improved significantly. 4.2.2 Find Word Equivalence by Image Clustering If two word images hold a type-1 relation at image level, they are images which match with each other. Clustering is the grouping of similar objects <ref> [46] </ref>. A word image clustering algorithm is proposed to extract this kind of relation from a text page (see Figure 4.4). After image clustering, any two word images from a cluster are identified to hold the type-1 relation at the image level.
Reference: [47] <author> P. Heeman and J. Allen. </author> <title> Detecting and correcting speech repair. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <month> June 27-30 </month> <year> 1994. </year>
Reference-contexts: A parsing process sometimes may fail because of the incomplete grammar used in the parser or the ungrammatical constructions within the text or speech to be processed. Methods for repairing parser failures, such as partial parsing, have been reported <ref> [47, 56, 102, 128] </ref>. 2.2.3 Integration of Statistical and Structural Approaches Statistical and structural approaches have often been applied to different tasks. Many speech recognition systems use statistical language models exclusively, whereas many natural language understanding systems are symbolic.
Reference: [48] <author> C. Hemphill and J. Picone. </author> <title> Chart Parsing of Stochastic Spoken Language Model. </title> <booktitle> In Speech and Natural Language Workshop, </booktitle> <year> 1989. </year>
Reference-contexts: Augmented with semantic and pragmatic knowledge, a lattice parser can select candidates which are not only a grammatical sequence, but also a meaningful one. Lattice parsers were designed for word candidate selection in speech recognition <ref> [23, 48, 152] </ref>. For example, "I saw the man" can be genegrated by a lattice parser as the best word sequence for the word lattice in Figure 2.5. <p> The acoustic ambiguity may be caused by uncertainty of word boundaries, phonetic ambiguity, syllable omissions and other missing information. Researchers believe that acoustic ambiguity can only be resolved through the use of higher sources of knowledge and NLP techniques. There have been efforts to integrate speech recognition with NLP <ref> [48, 76, 137, 160, 165] </ref>. NLP can play two roles. The first role is to interpret the meaning of an utterance. The second, more subtle role, is to reduce acoustic ambiguity by "understanding" the utter BACKGROUND 33 ance. <p> Besides phonemic and phonologic knowledge, syntactic, semantic, prosodic, pragmatic knowledge sources and knowledge about the real world are applied to speech recognition. Under the approaches of statistical language models and structural analysis, methods such as the N-best search [134], HMM [67] and lattice parsing <ref> [23, 48, 152] </ref> are designed to select word candidates from multiple choices generated by acoustic-phonetic analysis. 2.3 Conclusions Text recognition is not only an image processing and pattern recognition task, but also a natural language processing task.
Reference: [49] <author> D. Hindle and M. Rooth. </author> <title> Structural ambiguity and lexical relations. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 103-120, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: For example, "Fruit flies like apples" is an ambiguous sentence. Prepositional phrase (PP) attachment and conjunction scoping are two typical sources of syntactic ambiguity. By using semantic information, such as common sense knowledge and domain knowledge, many ambiguities can be resolved <ref> [49, 56, 57] </ref>. Because of the nature of natural language processing, parsing methods developed so BACKGROUND 29 far sometimes are not accurate, robust, or efficient [153], especially when the length of the sentence to be parsed is long (i.e., on average more than 25 words).
Reference: [50] <author> T.K. Ho. </author> <title> A Theory of Multiple Classifier Systems And Its Application to Visual Word Recognition. </title> <type> PhD thesis, </type> <institution> Computer Science Department of SUNY at Buffalo, </institution> <year> 1992. </year>
Reference-contexts: By taking advantage of local typeface homogeneity, OCR accuracy can be improved [9]. The combination of several classifiers has been shown as a way to make recognition more accurate <ref> [50, 164] </ref>. Besides problems of isolated character classification, improper character segmentation has been identified as one of the major sources of incorrect recognition [18]. As an important preprocessing step, character segmentation partitions word images into sequences of character images so that OCR techniques can be applied [17].
Reference: [51] <author> T.K. Ho and H.S. Baird. </author> <title> Perfect metrics. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93, </booktitle> <pages> pages 593-597, </pages> <year> 1993. </year>
Reference-contexts: Improving performance on poor quality documents is a challenging problem to which much OCR research is now devoted. New feature representations and new classification methods have been proposed <ref> [51, 111] </ref>. The role of training sets for classification has been investigated and it has been suggested that the quality of training sets, rather than classification methodology, is the determining factor in achieving higher accuracy [52]. By taking advantage of local typeface homogeneity, OCR accuracy can be improved [9].
Reference: [52] <author> T.K. Ho and H.S. Baird. </author> <title> Asymptotic accuracy of two-class discrimination. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 275-288, </pages> <year> 1994. </year>
Reference-contexts: New feature representations and new classification methods have been proposed [51, 111]. The role of training sets for classification has been investigated and it has been suggested that the quality of training sets, rather than classification methodology, is the determining factor in achieving higher accuracy <ref> [52] </ref>. By taking advantage of local typeface homogeneity, OCR accuracy can be improved [9]. The combination of several classifiers has been shown as a way to make recognition more accurate [50, 164].
Reference: [53] <author> T.K. Ho and H.S. Baird. </author> <title> Evaluation of OCR accuracy using synthetic data. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 413-422, </pages> <month> April 24-26 </month> <year> 1995. </year>
Reference-contexts: Those parameterized models can quantitatively describe local or global image defects. They can be used to evaluate the performance of a recognition system for a continuum of degradation levels <ref> [53] </ref>. Another use of those models is to develop new systems which can explicitly tolerate the noise represented by those models. To describe the capability of a new method or a new system on degraded text recognition, simply showing its performance on testing samples may not be enough.
Reference: [54] <author> T.K. Ho, J.J. Hull, </author> <title> and S.N. Srihari. Word recognition with multi-level contextual knowledge. </title> <booktitle> In Proceedings of the First International Conference on Document Analysis (ICDAR-91), </booktitle> <pages> pages 905-915, </pages> <year> 1991. </year>
Reference-contexts: Contextual constraints which have been utilized successfully in practical systems are limited to the word-level <ref> [35, 54, 90] </ref>. The lexical constraint is the one that is used most BACKGROUND 15 widely. Methods of dictionary look-up [158], confusion-matrix-based transformation and string editing [36, 79], probabilistic relaxation [44], and character n-gram frequency analysis [74, 142] have been developed to exploit the lexical constraint efficiently.
Reference: [55] <author> T.K. Ho, J.J. Hull, </author> <title> and S.N. Srihari. A word shape analysis approach to lexicon based word recognition. </title> <journal> Pattern Recognition letters, </journal> <volume> 13 </volume> <pages> 821-826, </pages> <year> 1992. </year> <note> BIBLIOGRAPHY 218 </note>
Reference-contexts: Based on DAFS, ILLUMINATOR also provides tools which make OCR postprocessing more efficient and accurate [117, 119]. 2.1.1.4 The Whole Word Recognition Approach Lexical constraints can be directly exploited in the process of word recognition by applying the word-shape analysis method <ref> [55] </ref>. In this way, character segmentation is bypassed. Given a word image, its word-shape feature will be directly matched with word-shape features of prototypes for words in a lexicon (see Figure 2.3). The first n best matches will be given as BACKGROUND 17 the recognition result. <p> Each candidate is associated with a confidence score. A typical OCR algorithm, which includes the steps of character segmentation and character recognition, plus word-level postprocessing, can be used as a word recognizer. Word recognition also can be performed by word-shape analysis under the holistic approach <ref> [55] </ref>. 3.1.2 Visual Contextual Analysis The visual contextual analysis stage examines visual context by computing visual inter-word relations. Visual inter-word relations are phenomena which can be observed from text pages. Inside a text page, we can find many word images that share visual similarity. <p> Times Roman font in postscript with the Unix command ditroff. The postscript files were then converted into raster images with the ghostscript system. Neighborhoods were generated for each word by first calculating a feature vector for the word known as the stroke direction feature vector <ref> [55] </ref>. The neighborhoods for each dictionary word were then calculated by computing the Euclidean distance between its feature vector and the feature vectors of all the other dictionary words and sorting the result. <p> After printing those sentences with ditroff in a 12 point font on paper using a laser printer, text images were created by digitizing the printouts. The text images were segmented into sentences, and further into word images. Using a word recognition program which is based on word shape analysis <ref> [55] </ref>, the top ten word candidates were generated for each word image (for the frequent function words such as "a," "the" and "of," and punctuation marks, the recognizer generates just one word candidate). The outline of word shape analysis algorithm is shown in Figure. 2.3.
Reference: [56] <author> J.R. Hobbs, D.E. Appelt, J.Bear, and M. Tyson. </author> <title> Robust Processing of Real-World Natural Language Texts. </title> <booktitle> In Proceedings of 3rd Conference on Applied Natural Language Processing, </booktitle> <pages> pages 186-182, </pages> <year> 1992. </year>
Reference-contexts: For example, "Fruit flies like apples" is an ambiguous sentence. Prepositional phrase (PP) attachment and conjunction scoping are two typical sources of syntactic ambiguity. By using semantic information, such as common sense knowledge and domain knowledge, many ambiguities can be resolved <ref> [49, 56, 57] </ref>. Because of the nature of natural language processing, parsing methods developed so BACKGROUND 29 far sometimes are not accurate, robust, or efficient [153], especially when the length of the sentence to be parsed is long (i.e., on average more than 25 words). <p> A parsing process sometimes may fail because of the incomplete grammar used in the parser or the ungrammatical constructions within the text or speech to be processed. Methods for repairing parser failures, such as partial parsing, have been reported <ref> [47, 56, 102, 128] </ref>. 2.2.3 Integration of Statistical and Structural Approaches Statistical and structural approaches have often been applied to different tasks. Many speech recognition systems use statistical language models exclusively, whereas many natural language understanding systems are symbolic. <p> A large-scale English lexicon which supports the parsing of unrestricted English text is difficult to build. Parsing methods developed so far are still not accurate and efficient, especially when a sentence is long (more than 25 words on average) <ref> [56] </ref>. Parsing unrestricted English texts also requires a comprehensive English grammar which can cover most syntactic phenomena in unrestricted text.
Reference: [57] <author> J.R. Hobbs and J. Bear. </author> <title> Two Principles of Parse Preference. </title> <booktitle> In COLING-90, </booktitle> <year> 1990. </year>
Reference-contexts: For example, "Fruit flies like apples" is an ambiguous sentence. Prepositional phrase (PP) attachment and conjunction scoping are two typical sources of syntactic ambiguity. By using semantic information, such as common sense knowledge and domain knowledge, many ambiguities can be resolved <ref> [49, 56, 57] </ref>. Because of the nature of natural language processing, parsing methods developed so BACKGROUND 29 far sometimes are not accurate, robust, or efficient [153], especially when the length of the sentence to be parsed is long (i.e., on average more than 25 words).
Reference: [58] <author> T. Hong. </author> <title> Integration of visual inter-word constraints and linguistic knowledge in degraded text recognition. </title> <booktitle> In Proceedings of 32nd Annual Meeting of Association for Computational Linguistics, Student Sessions, </booktitle> <pages> pages 328-330, </pages> <month> 27-30 June </month> <year> 1994. </year>
Reference-contexts: The method works only for a portion of the words in a document page. But the approach can be extended to the more general situation. Word image equivalence is only one of several visual inter-word relations that we have observed <ref> [58] </ref>. The other types of relations concern partial similarity. <p> Currently, there are two approaches, the statistical approach and the structural approach, towards the problem of candidate selection. In the statistical approach, language models such as a Hidden Markov Model and word collocation can be utilized for candidate selection <ref> [26, 58, 70] </ref>. In the structural approach, lattice parsing techniques have been developed for candidate selection [62, 152]. The contextual constraints considered in a statistical language model, such as word collocation, are local constraints. <p> The principle of consistency between visual inter-word relations and symbolic inter-word relations was described. Therefore, visual inter-word relations can be used as contextual constraints to interpret related word images systematically <ref> [58] </ref>. CONCLUSION 200 4. A new algorithm was designed to exploit visual inter-word relations for character segmentation of degraded text pages [61]. 5. New techniques for OCR postprocessing were proposed [59, 60]. It is observed that current commercial OCR systems often recognize almost equivalent image patterns as different string patterns. <p> A lattice parser was proposed to take advantage of syntactic constraints for word candidate selection. They were integrated to achieve better performance. 7. A computational framework in which visual contextual constraints and different linguistic constraints can be exploited for passage-level postprocessing was proposed <ref> [58, 64] </ref>. Unlike the traditional approach of contextual analysis, which is limited to symbolic level processing and uses only an intra-word linguistic context, visual and linguistic inter-word constraints are integrated. The interaction between language-level analysis and image data distinguishes this from work in other areas such as speech recognition.
Reference: [59] <author> T. Hong and J. J. Hull. </author> <title> Improving OCR performance with word image equivalence. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 177-190, </pages> <month> April 24-26 </month> <year> 1995. </year>
Reference-contexts: Therefore, visual inter-word relations can be used as contextual constraints to interpret related word images systematically [58]. CONCLUSION 200 4. A new algorithm was designed to exploit visual inter-word relations for character segmentation of degraded text pages [61]. 5. New techniques for OCR postprocessing were proposed <ref> [59, 60] </ref>. It is observed that current commercial OCR systems often recognize almost equivalent image patterns as different string patterns. In the techniques proposed, consistency analysis of inter-word relations at the image level and the symbolic level is applied to detect and correct potential OCR errors. 6.
Reference: [60] <author> T. Hong and J. J. Hull. </author> <title> Visual inter-word relations and their use for OCR postpro-cessing. </title> <booktitle> In Proceedings of the Third International Conference on Document Analysis (ICDAR-95), </booktitle> <month> August 14-16 </month> <year> 1995. </year>
Reference-contexts: In addition to traditional word-level post-processing, passage-level postprocessing is identified as a necessary stage to improve recognition performance for degraded text. Visual context, as well as symbolic and linguistic context, can be used in passage-level postprocessing. 2. A new set of visual inter-word relations was defined <ref> [60] </ref>. Those visual relations reflect typographical constraints inside a document page and are tolerant to uniform noise. In a document image written in normal English, many word images have visual inter word relationships to each other [61]. 3. <p> Therefore, visual inter-word relations can be used as contextual constraints to interpret related word images systematically [58]. CONCLUSION 200 4. A new algorithm was designed to exploit visual inter-word relations for character segmentation of degraded text pages [61]. 5. New techniques for OCR postprocessing were proposed <ref> [59, 60] </ref>. It is observed that current commercial OCR systems often recognize almost equivalent image patterns as different string patterns. In the techniques proposed, consistency analysis of inter-word relations at the image level and the symbolic level is applied to detect and correct potential OCR errors. 6.
Reference: [61] <author> T. Hong and J. J. Hull. </author> <title> Visual inter-word relations and their use in character segmentation. </title> <booktitle> In Proceedings of the Conference on Document Recognition, 1995 SPIE Symposium(SPIE95), </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: Here we limit our discussion to their uses for character segmentation, font detection and candidate verification and selection. 4.3.1 Character Segmentation A novel method has been designed for character segmentation that utilizes visual inter-word constraints from a text image to recursively split word images into smaller image pieces <ref> [61] </ref>. This method is applicable to machine-printed English text where the same spacing is used between identical pairs of characters. The intuition behind the method comes from the observation, that, inside an English text (with reasonable length), there are usually many word images that are visually similar. <p> A new set of visual inter-word relations was defined [60]. Those visual relations reflect typographical constraints inside a document page and are tolerant to uniform noise. In a document image written in normal English, many word images have visual inter word relationships to each other <ref> [61] </ref>. 3. The principle of consistency between visual inter-word relations and symbolic inter-word relations was described. Therefore, visual inter-word relations can be used as contextual constraints to interpret related word images systematically [58]. CONCLUSION 200 4. <p> Therefore, visual inter-word relations can be used as contextual constraints to interpret related word images systematically [58]. CONCLUSION 200 4. A new algorithm was designed to exploit visual inter-word relations for character segmentation of degraded text pages <ref> [61] </ref>. 5. New techniques for OCR postprocessing were proposed [59, 60]. It is observed that current commercial OCR systems often recognize almost equivalent image patterns as different string patterns.
Reference: [62] <author> T. Hong and J.J. Hull. </author> <title> Text recognition enhancement with a probabilistic lattice chart parser. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis (ICDAR-93), </booktitle> <year> 1993. </year>
Reference-contexts: In this case all three words should be rejected. Clusters 68 and 79 contain alternative decisions that are legal dictionary words. The best way to make these choices is probably to employ a different form of postprocessing such as word transition probabilities [63, 64] or parsing <ref> [62] </ref>. Thus, there is the potential, with further development of the algorithm proposed in this chapter, to correct all but four of the errors. <p> In the statistical approach, language models such as a Hidden Markov Model and word collocation can be utilized for candidate selection [26, 58, 70]. In the structural approach, lattice parsing techniques have been developed for candidate selection <ref> [62, 152] </ref>. The contextual constraints considered in a statistical language model, such as word collocation, are local constraints. For a word image, a candidate will be selected according to the candidate information from its neighboring word images in a fixed window size. <p> In the techniques proposed, consistency analysis of inter-word relations at the image level and the symbolic level is applied to detect and correct potential OCR errors. 6. Different linguistic constraints were explored to solve the problem of word candidate selection for degraded text recognition <ref> [62, 63] </ref>. A new relaxation algorithm that uses word collocation data collected from large text corpora was proposed and tested. A lattice parser was proposed to take advantage of syntactic constraints for word candidate selection. They were integrated to achieve better performance. 7.
Reference: [63] <author> T. Hong and J.J. Hull. </author> <title> Degraded text recognition using word collocation. </title> <booktitle> In Proceedings of the Conference on Document Recognition of 1994 IS&T/SPIE Symposium, </booktitle> <pages> pages 334-342, </pages> <month> February 6-10 </month> <year> 1994. </year> <note> BIBLIOGRAPHY 219 </note>
Reference-contexts: In this case all three words should be rejected. Clusters 68 and 79 contain alternative decisions that are legal dictionary words. The best way to make these choices is probably to employ a different form of postprocessing such as word transition probabilities <ref> [63, 64] </ref> or parsing [62]. Thus, there is the potential, with further development of the algorithm proposed in this chapter, to correct all but four of the errors. <p> This method of calculating the score for w ij at time k + 1 is an improvement over a previous approach that did not incorporate recognition confidence <ref> [63] </ref>. This measure uses the top-ranked choice of adjacent words to adjust the ranking in each neighborhood. Repeated applications of this measure effectively propagates results across a sentence. <p> In the techniques proposed, consistency analysis of inter-word relations at the image level and the symbolic level is applied to detect and correct potential OCR errors. 6. Different linguistic constraints were explored to solve the problem of word candidate selection for degraded text recognition <ref> [62, 63] </ref>. A new relaxation algorithm that uses word collocation data collected from large text corpora was proposed and tested. A lattice parser was proposed to take advantage of syntactic constraints for word candidate selection. They were integrated to achieve better performance. 7.
Reference: [64] <author> T. Hong and J.J. Hull. </author> <title> Degraded text recognition using word collocation and visual inter-word constraints. </title> <booktitle> In Proceedings of the Forth Conference of Applied Natural Language Processing(ANLP94), </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: In this case all three words should be rejected. Clusters 68 and 79 contain alternative decisions that are legal dictionary words. The best way to make these choices is probably to employ a different form of postprocessing such as word transition probabilities <ref> [63, 64] </ref> or parsing [62]. Thus, there is the potential, with further development of the algorithm proposed in this chapter, to correct all but four of the errors. <p> A lattice parser was proposed to take advantage of syntactic constraints for word candidate selection. They were integrated to achieve better performance. 7. A computational framework in which visual contextual constraints and different linguistic constraints can be exploited for passage-level postprocessing was proposed <ref> [58, 64] </ref>. Unlike the traditional approach of contextual analysis, which is limited to symbolic level processing and uses only an intra-word linguistic context, visual and linguistic inter-word constraints are integrated. The interaction between language-level analysis and image data distinguishes this from work in other areas such as speech recognition.
Reference: [65] <author> T. Hong, S. W. Lam, J. J. Hull, and S. N. Srihari. </author> <title> The design of a nearest-neighbor classifier and its use for japanese character recognition. </title> <booktitle> In Proceedings of the Third International Conference on Document Analysis (ICDAR-95), </booktitle> <month> August 14-16 </month> <year> 1995. </year>
Reference-contexts: Instead, the OCR may generate a candidate list for each Chinese character (see Figure 10.8 for the top5 candidate lists of three Chinese characters) <ref> [65] </ref>. <p> The methodology was generalized to other alphabetic languages and ideographic lan guages, such as Chinese and Japanese <ref> [65, 66] </ref>. Chapter 12 Future Directions Some interesting further studies can be conducted.
Reference: [66] <author> T. Hong, S. W. Lam, J. J. Hull, and S. N. Srihari. </author> <title> Visual similarity analysis of chinese characters and its uses in japanese OCR. </title> <booktitle> In Proceedings of the Conference on Document Recognition, 1995 SPIE Symposium(SPIE95), </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: The methodology was generalized to other alphabetic languages and ideographic lan guages, such as Chinese and Japanese <ref> [65, 66] </ref>. Chapter 12 Future Directions Some interesting further studies can be conducted.
Reference: [67] <author> X.D. Huang, Y. Ariki, and M.A. Jack. </author> <title> Hidden Markov Models for Speech Recognition. </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: Besides phonemic and phonologic knowledge, syntactic, semantic, prosodic, pragmatic knowledge sources and knowledge about the real world are applied to speech recognition. Under the approaches of statistical language models and structural analysis, methods such as the N-best search [134], HMM <ref> [67] </ref> and lattice parsing [23, 48, 152] are designed to select word candidates from multiple choices generated by acoustic-phonetic analysis. 2.3 Conclusions Text recognition is not only an image processing and pattern recognition task, but also a natural language processing task.
Reference: [68] <author> J.J. Hull. </author> <title> Inter-word constraints in visual word recognition. </title> <booktitle> In Proceedings of the Conference of the Canadian Society for Computational Studies of Intelligence, </booktitle> <month> May 21-23 </month> <year> 1986. </year>
Reference-contexts: Passage-level postprocessing is usually formalized as a candidate selection problem in its simplest case. To select a candidate for each word image, those constraints can play a crucial role. Previous approaches have utilized local word-to-word transitions <ref> [68] </ref>, statistical part-of-speech transitions [70], definitional overlap [40], word collocation constraints [130], transitions between hypertags [141], N-best strategies [31] and semantic constraints from a limited domain, such as chess [10].
Reference: [69] <author> J.J. Hull. </author> <title> A Computational Theory of Visual Word Recognition. </title> <type> PhD thesis, </type> <institution> Computer Science Department of SUNY at Buffalo, </institution> <year> 1987. </year>
Reference-contexts: (MAP)") through the combined source and channel models using a Viterbi-like dynamic programming algorithm. 2.1.3 Reading by Hypothesis Generation and Testing Based on previous studies of how people read and the psychology of reading [15, 113, 120], a computational theory for the visual recognition of words of text was proposed <ref> [69] </ref>. The theory includes three stages: hypothesis generation, hypothesis testing, and global contextual analysis. Hypothesis generation uses gross visual features to provide expectations about word identities. Hypothesis testing integrates the information determined by hypothesis generation with more detailed features that are extracted from a word image.
Reference: [70] <author> J.J. Hull. </author> <title> Incorporation of a markov model of language syntax in a text recognition algorithm. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 174-185, </pages> <month> March 16-18 </month> <year> 1992. </year>
Reference-contexts: Passage-level postprocessing is usually formalized as a candidate selection problem in its simplest case. To select a candidate for each word image, those constraints can play a crucial role. Previous approaches have utilized local word-to-word transitions [68], statistical part-of-speech transitions <ref> [70] </ref>, definitional overlap [40], word collocation constraints [130], transitions between hypertags [141], N-best strategies [31] and semantic constraints from a limited domain, such as chess [10]. <p> Currently, there are two approaches, the statistical approach and the structural approach, towards the problem of candidate selection. In the statistical approach, language models such as a Hidden Markov Model and word collocation can be utilized for candidate selection <ref> [26, 58, 70] </ref>. In the structural approach, lattice parsing techniques have been developed for candidate selection [62, 152]. The contextual constraints considered in a statistical language model, such as word collocation, are local constraints.
Reference: [71] <author> J.J. Hull. </author> <title> Document image matching and retrieval with multiple distortion-invariant descriptors. </title> <booktitle> In IAPR Workshop On Document Analysis Systems, </booktitle> <pages> pages 383-400, </pages> <month> Oct. 18-20 </month> <year> 1994. </year> <note> BIBLIOGRAPHY 220 </note>
Reference-contexts: items of the word "Lloyd" which are visually similar (character segmentation and recognition for those words will be easier if we consider that they have to be interpreted as the same word; and (c) examples of partial similarties which can be found between word images. 12.6 Summary Image-based document retrieval <ref> [21, 71, 145] </ref> and text editing [5] are two interesting applications in which visual and linguistic information can also be utilized. The objective of visual text recognition is to correctly transform an arbitrary image of text into its symbolic equivalent.
Reference: [72] <author> J.J. Hull, S. Khoubyari, and T.K. Ho. </author> <title> Visual global context: Word image matching in a methodology for degraded text recognition. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 26-39, </pages> <month> March 16-18 </month> <year> 1992. </year>
Reference-contexts: If there are several degraded word images in a cluster, the image quality of the prototype for the cluster usually is much better than that of the original noisy word images, so it is easier for a recognition algorithm to recognize the prototype than the original word images <ref> [72, 84] </ref>. The method has been applied successfully to tasks such as content word detection and font identification [85]. In the word image clustering method, one type of visual inter-word constraint theword equivalence constraint- is taken into consideration. <p> By clustering visually equivalent word images, many function words and content words inside the text page can be found <ref> [72, 84] </ref>. This chapter studies visual contextual constraints more extensively. English is an alphabetic language. In an English document page, there are usually several hundred words, or several thousand characters, which are printed in a limited number of type-faces. Among these word images, many share certain kinds of visual similarity. <p> Neighborhood Size by Relax ation The word recognition system generates the top-n candidates for each word image except for frequent function words, such as "a," "the" and "of." For the function words, we assume LATTICE PARSING 136 they can be recognized correctly by a word image matching and clustering procedure <ref> [72] </ref>. Thus, the function words are "islands" or words with identities that the parser can rely on. The next step is to apply a relaxation algorithm as a filter to reduce the top-n word candidates at each location to the top-m, where m &lt; n.
Reference: [73] <author> J.J. Hull, S. Khoubyari, and T.K. Ho. </author> <title> Word image matching as a technique for degraded text recognition. </title> <booktitle> In Proceedings of 11th International Conference on Pattern Recognition, </booktitle> <year> 1992. </year>
Reference-contexts: Image Equivalence for Error Correction 82 5.2 Image Equivalence and Word Image Clustering The visual similarity between two binary images of the same size can be measured quantitatively by how the images match at the pixel level <ref> [73] </ref>. Let A and B be two m fi n binary images. Inside an image, "1" and "0" denote "black" and "white" pixel respectively.
Reference: [74] <author> J.J. Hull and S.N. Srihari. </author> <title> Experiments in text recognition with binary n-gram and viterbi algorithms. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-4(5):520-530, </volume> <year> 1982. </year>
Reference-contexts: The lexical constraint is the one that is used most BACKGROUND 15 widely. Methods of dictionary look-up [158], confusion-matrix-based transformation and string editing [36, 79], probabilistic relaxation [44], and character n-gram frequency analysis <ref> [74, 142] </ref> have been developed to exploit the lexical constraint efficiently. Such methods work well on non-word error detection. If a string generated by an OCR for a word image does not appear in a dictionary, which can be very large for wide coverage, a non-word error can be detected.
Reference: [75] <author> P. Jacobs. </author> <title> Making sense of lexical acquisition. In Uri Zernik, editor, Lexical Acquisition: Exploitung On-Line Resources to Build a Lexicon, </title> <address> pages 29-44. </address> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <year> 1991. </year>
Reference-contexts: In the section, we will study methodologies in areas of natural language processing and techniques developed for applications. 2.2.1 Statistical Language Modeling Research from artificial intelligence and computational linguistics has shown that statistical methods can achieve promising performance in the tasks of lexical acquisition <ref> [75] </ref>, tagging [25], parsing [42, 99, 100] and sense disambiguation [140] and that these informa BACKGROUND 26 tion sources can be used in systems for natural language processing [161], machine translation [14], information retrieval [88], and speech recognition [160]. <p> Therefore, lexical knowledge is necessary for any natural language processing system. In recent years, there have been many efforts to create machine-readable and machine-accessible lexical knowledge bases in computational linguistics <ref> [75] </ref>. It is almost impossible to build a large-scale machine-accessible lexicon manually. To overcome the lexical bottleneck, research has proceeded in two directions. One is to compile a machine-accessible lexicon based on on-line lexicons. Another is to extract lexical knowledge from large corpora by using statistical methods.
Reference: [76] <author> F. Jelinek. </author> <title> Self-organized language modeling for speech recognition. </title> <editor> In K. F. Lee, editor, </editor> <title> Reading of Speech Recognition. </title>
Reference-contexts: The acoustic ambiguity may be caused by uncertainty of word boundaries, phonetic ambiguity, syllable omissions and other missing information. Researchers believe that acoustic ambiguity can only be resolved through the use of higher sources of knowledge and NLP techniques. There have been efforts to integrate speech recognition with NLP <ref> [48, 76, 137, 160, 165] </ref>. NLP can play two roles. The first role is to interpret the meaning of an utterance. The second, more subtle role, is to reduce acoustic ambiguity by "understanding" the utter BACKGROUND 33 ance.
Reference: [77] <author> M.A. Jones, G.A. Story, and B.W. Ballard. </author> <title> Integrating multiple knowledge sources in a bayesian OCR post-processor. </title> <booktitle> In Proceedings of ICDAR-91, </booktitle> <pages> pages 925-933, 91. </pages>
Reference-contexts: There are many contextual constraints that can be utilized if we consider information about the relations among words and the structure of documents <ref> [77, 144] </ref>. Passage-level postprocessing is usually formalized as a candidate selection problem in its simplest case. To select a candidate for each word image, those constraints can play a crucial role.
Reference: [78] <author> Y. Tanabe K. Seino and K. Sakai. </author> <title> A linguistic post-processing based on word occurrence probability. </title> <editor> In S. Impedovo and J.C. Simon, editors, </editor> <title> From Pixels to Features III: </title> <booktitle> Frintiers in Handwriting Recognition, </booktitle> <pages> pages 191-199. </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, </address> <year> 1992. </year>
Reference-contexts: Linguistic context can be applied to choose decisions for those character images which have competitive candidates generated by a Chinese character classifier. Like that for English text recognition, linguistic knowledge sources useful here include word dictionaries, statistical language models, language syntax and semantics <ref> [24, 43, 78] </ref>. Character recognition results for a Chinese sentence are shown in Figure 10.9 (a). The translation of the sentence is "He congratulates your success" There are six Chinese characters in the sentence. For each character, three character candidates are provided by a classifier.
Reference: [79] <author> S. Kahan, T. Pavlidis, and H.S. Baird. </author> <title> On the recognition of printed characters of any font and size. </title> <journal> IEEE Transactions on pattern analysis and machine intelligence, </journal> <volume> 9(2), </volume> <year> 1987. </year> <note> BIBLIOGRAPHY 221 </note>
Reference-contexts: One is the analytical approach, or "character-based word recognition"; another is the holistic approach, or "word shape recognition" [144]. 2.1.1.1 The OCR Approach By taking the analytical approach, word recognition can be accomplished by a three-stage process: character segmentation, character recognition, and postprocessing <ref> [79] </ref>. The approach is traditionally called OCR (Optical Character Recognition), which has a long history and is the dominant paradigm in current text recognition research [106]. Character recognition is the key component of OCR-based word recognition. <p> Given a character image, a character recognition algorithm generates decisions about the character's BACKGROUND 12 identity. This can be done by template matching or structural analysis. To recognize any of the hundreds of typefaces in common use, current omni-font OCR engines take the structural analysis approach <ref> [9, 11, 79] </ref>. Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes [79, 106, 143]. <p> To recognize any of the hundreds of typefaces in common use, current omni-font OCR engines take the structural analysis approach [9, 11, 79]. Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes <ref> [79, 106, 143] </ref>. Many classification methods, such as nearest-neighbor classifiers [32], Bayesian classifiers [34], neural network classifiers [94, 132], decision tree classifiers [105] and syntactic classifiers [41], have been designed to classify character images. <p> In this situation, several characters may be segmented as one character image or one character image may be split into two or more pieces. Many methods have been developed to improve character segmentation accuracy. Many current methods are based on connected component analysis, aspect ratio estimation, or profile analysis <ref> [11, 35, 79, 93] </ref>. <p> But it was also reported that the accuracy of the OCR systems declined dramatically when the images are degraded or the resolution of the images was reduced from 300 to 200dpi. Current OCR systems use lexical constraints and domain knowledge to detect and correct character recognition errors <ref> [79, 11] </ref>. <p> Contextual constraints which have been utilized successfully in practical systems are limited to the word-level [35, 54, 90]. The lexical constraint is the one that is used most BACKGROUND 15 widely. Methods of dictionary look-up [158], confusion-matrix-based transformation and string editing <ref> [36, 79] </ref>, probabilistic relaxation [44], and character n-gram frequency analysis [74, 142] have been developed to exploit the lexical constraint efficiently. Such methods work well on non-word error detection.
Reference: [80] <author> T. Kanungo, H.S. Baird, and R.M. Haralick. </author> <title> Validation and estimation of document degradation models. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 217-228, </pages> <month> April 24-26 </month> <year> 1995. </year>
Reference-contexts: Characters may become blurred, distorted, touch together, or split into smaller pieces. To characterize different sources of degradation on document pages, document degradation models (DDM) have been proposed <ref> [7, 8, 81, 80] </ref>. Tools based on those DDMs and defect images generated by the methods can be found in the UW English Document Images Database, which is available in CDROM form [112]. Those parameterized models can quantitatively describe local or global image defects.
Reference: [81] <author> T. Kanungo, R.M. Haralick, and I. Phillips. </author> <title> Global and local document degradation models. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93, </booktitle> <pages> pages 730-734, </pages> <year> 1993. </year>
Reference-contexts: Characters may become blurred, distorted, touch together, or split into smaller pieces. To characterize different sources of degradation on document pages, document degradation models (DDM) have been proposed <ref> [7, 8, 81, 80] </ref>. Tools based on those DDMs and defect images generated by the methods can be found in the UW English Document Images Database, which is available in CDROM form [112]. Those parameterized models can quantitatively describe local or global image defects. <p> On average, about sixty percent of words in a page are repeated at least once in some other position. Noise was added to the original images to simulate the effect of a multiple generation photocopy. We used the University of Washington document degradation model (DDM) <ref> [81] </ref> to add two different levels of local noise, which are denoted dd1 and dd2. <p> Six page images (listed in Table 6.1) were used to test the system. These were scanned at 300 ppi and the binary image produced by the scanning hardware was used. Uniform noise was added to each image using the documentation degradation model (DDM) package from the University of Washington <ref> [81] </ref>. The parameter set for DDM was (820; 0:0; 1:0; 1:0; 1:0; 1:0; 3). The accuracy of Caere's AnyFont OCR package on the original pages is very high, more than 98% correct at the word level. After adding uniform noise with DDM, the word correct rate dropped to 73.5%. <p> The visual similarity measure discussed in this thesis works well for document images which are degraded by random noise or other uniformly distributed noise. For images with artifacts, such as character skew and perspective distortion <ref> [7, 81] </ref>, more complex similarity measures can be developed. Future Directions 203 12.4 A Unified Approach toward Text Recognition Character segmentation, character recognition (OCR) and postprocessing are three major components of a text recognition system.
Reference: [82] <author> M. Kay. </author> <title> Algorithm Schemata and Data Structures in Syntactic Processing. </title> <type> Technical Report 80-12, CSL, </type> <month> October </month> <year> 1980. </year>
Reference-contexts: Because syntactic phenomena are complex, a grammar with comprehensive coverage can be difficult to obtain. Many parsing methods have been developed in the last thirty years, such as ATN parsers [163], generalized LR parsers [13, 151], deterministic parsers [101], unification algorithms [83] and chart parsers <ref> [82, 166] </ref>. Among these techniques, chart parsing is quite popular. The chart in a chart parser is a data structure that stores the information about syntactic structures (or subtrees) already derived from an input sentence. With such a data structure, the same structure will be derived only once. <p> Local word collocation data can be collected by training on large text corpora [26]. The global structural constraints are exploited with a chart parsing model. The chart data structure provides a flexible framework for parsing <ref> [82, 166] </ref>. Statistical methods can be easily incorporated in a chart parser [100]. A chart parser can be extended to a lattice parser which allows for several word candidates at the same position, and therefore can be directly used for speech recognition [23, 152] and visual text recognition.
Reference: [83] <author> M. Kay. </author> <title> Parsing in Functional Unification Grammar. </title> <editor> In Barbara J. Grosz, Karen Sparck Jones, and Bonnie Lynn Webber, editors, </editor> <booktitle> Reading in Natural Language Processing, </booktitle> <pages> pages 35-70. </pages> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <year> 1986. </year>
Reference-contexts: Because syntactic phenomena are complex, a grammar with comprehensive coverage can be difficult to obtain. Many parsing methods have been developed in the last thirty years, such as ATN parsers [163], generalized LR parsers [13, 151], deterministic parsers [101], unification algorithms <ref> [83] </ref> and chart parsers [82, 166]. Among these techniques, chart parsing is quite popular. The chart in a chart parser is a data structure that stores the information about syntactic structures (or subtrees) already derived from an input sentence.
Reference: [84] <author> S. Khoubyari and J.J. Hull. </author> <title> Keyword location in noisy document images. </title> <booktitle> In Proceedings of the Second Annual Symposium on Document Anaysis and Information Retrieval, </booktitle> <address> Las Vegas, Nevada, </address> <month> April 26-28 </month> <year> 1993. </year>
Reference-contexts: If there are several degraded word images in a cluster, the image quality of the prototype for the cluster usually is much better than that of the original noisy word images, so it is easier for a recognition algorithm to recognize the prototype than the original word images <ref> [72, 84] </ref>. The method has been applied successfully to tasks such as content word detection and font identification [85]. In the word image clustering method, one type of visual inter-word constraint theword equivalence constraint- is taken into consideration. <p> By clustering visually equivalent word images, many function words and content words inside the text page can be found <ref> [72, 84] </ref>. This chapter studies visual contextual constraints more extensively. English is an alphabetic language. In an English document page, there are usually several hundred words, or several thousand characters, which are printed in a limited number of type-faces. Among these word images, many share certain kinds of visual similarity. <p> Given a sequence of word images from a text page, the visual equivalence among the word images can be computed by word image clustering <ref> [84] </ref>. After image clustering, images in the same cluster are visually equivalent.
Reference: [85] <author> S. Khoubyari and J.J. Hull. </author> <title> Font identification using visual global context. </title> <booktitle> In Proceedings of the Conference on Document Recognition of 1994 IS&T/SPIE Symposium, </booktitle> <address> San Jose, CA, </address> <month> February 6-10 </month> <year> 1994. </year>
Reference-contexts: The method has been applied successfully to tasks such as content word detection and font identification <ref> [85] </ref>. In the word image clustering method, one type of visual inter-word constraint theword equivalence constraint- is taken into consideration. <p> to test obtained image pieces so that only those image pieces that produce high confidence recognition results are used to split other images. 4.3.2 Font Information Extraction Previous methods for font detection usually depended on a font library which recorded the prototypes of characters or highly-frequent words in different fonts <ref> [85] </ref>. The method here does not require a font library. In this method, visual inter-word constraints would be used to cluster the word images which are in the same font.
Reference: [86] <author> S. Khoubyari and J.J. Hull. </author> <title> Font and function word identification in document recognition. Computer Vision, Graphics, </title> <booktitle> and Image Processing: Image Understanding, </booktitle> <year> 1995. </year> <note> to appear. </note>
Reference-contexts: This suggests that the relaxation algorithm should be used as part of a larger text recognition system and that this system should include a preprocessing step, such as <ref> [86] </ref>, that detects function words and recognizes them separately. The relaxation algorithm would then be applied to the other words in the text. Since these will be the information-bearing words, the relaxation approach will be more likely to perform correctly.
Reference: [87] <author> G. E. Kopec and P. A. Chou. </author> <title> Document image decoding using markov sources. </title> <journal> PAMI, </journal> <note> 1994. BIBLIOGRAPHY 222 </note>
Reference-contexts: Document image decoding using Markov sources is another promising approach that uses visual context <ref> [87] </ref>. Under this approach, a document image is assumed to be generated by an image generator and a noisy channel. A document image generator is a Markov source (stochastic finite-state automaton) which combines a message source with an imager.
Reference: [88] <author> R. Krovetz. </author> <title> Lexical acquisition and information retrieval. In Uri Zernik, editor, Lexical Acquisition : Exploiting on-line Resources to Build a Lexicon, </title> <address> pages 45-64. </address> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale,NJ, </address> <year> 1991. </year>
Reference-contexts: linguistics has shown that statistical methods can achieve promising performance in the tasks of lexical acquisition [75], tagging [25], parsing [42, 99, 100] and sense disambiguation [140] and that these informa BACKGROUND 26 tion sources can be used in systems for natural language processing [161], machine translation [14], information retrieval <ref> [88] </ref>, and speech recognition [160]. The statistical information is usually collected by training on a large text corpus. This approach is also called corpus-based natural language processing. The Brown Corpus is an English text corpus with about one million words [89]. It has both a raw and a tagged version.
Reference: [89] <author> H. Kucera and W. N. Francis. </author> <title> Computational Analysis of Present-day American English. </title> <publisher> Brown University Press, </publisher> <year> 1967. </year>
Reference-contexts: The statistical information is usually collected by training on a large text corpus. This approach is also called corpus-based natural language processing. The Brown Corpus is an English text corpus with about one million words <ref> [89] </ref>. It has both a raw and a tagged version. In the tagged version, each word is labeled with its part-of-speech (POS) tag. A corpus can be further annotated at the syntactic or semantic level, for example, each sentence can be associated with a parse tree. <p> To determine the number of pattern-occurrence relations that exist in a normal English text, an experiment was conducted with ideal ASCII input. Five articles were randomly selected from the Brown Corpus <ref> [89] </ref> as test samples for this analysis. They are denoted as A06, G02, J42, N01, and R07. Each of them contains about 2,000 words. The simulation result showed that almost all the words in those samples could be split into individual VISUAL RELATIONS 62 characters using the proposed approach. <p> The data used in the experiments were generated from the Brown Corpus and Penn Treebank databases. These corpora together contain over four million words of running text. The Brown corpus is divided into 500 samples of approximately 2000 words each <ref> [89] </ref>. The part of the Penn Treebank used here is the collection of articles from the Wall Street Journal that contain about three million words. Five articles were randomly selected from the Brown Corpus as test samples. They are A06, G02, J42, N01 and R07. <p> In this example, they are correct (see Figure 8.12). 8.7 Experimental Results We chose three texts from the Brown Corpus and two texts from the Penn Treebank as testing samples. The Brown Corpus is a collection of 500 samples of English texts, each of which contains approximately 2000 words <ref> [89] </ref>. A06 is a collection of six short articles from the Newark Evening News. G02 is from an article, "Toward a Concept of National Responsibility," from The Yale Review. <p> In a normal English text, there are many occurrences of the same words. Language statistics indicate that function words such as "the," "of," "to" and "and" account for about 30 percent of words in a English text <ref> [89] </ref>. Those words which are related to the topics of the text usually also appear with high frequency inside the text. Usually, the main body of a text is prepared in the same font type. <p> The word collocation data we used was trained on the Penn Treebank and the Brown Corpus after removing the five testing samples from the Brown Corpus. The Brown corpus is divided into 500 samples of approximately 2000 words each <ref> [89] </ref>. The part of the Penn Treebank used here is the collection of articles from the Wall Street Journal that contains three million words. We used the frequency of a word pair to measure its collocation strength. There are a total of 1,200,000 word pairs after training.
Reference: [90] <author> K. Kukich. </author> <title> Techniques for automatically correcting words in text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Contextual constraints which have been utilized successfully in practical systems are limited to the word-level <ref> [35, 54, 90] </ref>. The lexical constraint is the one that is used most BACKGROUND 15 widely. Methods of dictionary look-up [158], confusion-matrix-based transformation and string editing [36, 79], probabilistic relaxation [44], and character n-gram frequency analysis [74, 142] have been developed to exploit the lexical constraint efficiently. <p> For a degraded text page, if a text recognition system just outputs the candidates with the highest scores as the recognition result, like current OCR systems, manual effort is needed to detect and correct those errors. One technique for improving postprocessing performance is called context-dependent word correction <ref> [90] </ref>. Passage-level linguistic contextual constraints, above the word level, are utilized in this process. Over the last decade, much research has been conducted in the area. According to the ranges of context to be taken into consideration, we can separate contextual postprocessing into several levels (see Table 2.1). <p> Application Input Output Speech writer rider Recognition here hear Optical all a1l (A-one-L) Character of of Recognition form farm Spelling government goverment Correction occurred occured commercial commerical Table 2.2: Examples of channel confusions in different applications (Church et al 1993) In her survey paper <ref> [90] </ref>, Kukich reviewed the techniques for automatic word correction in spell checking and text recognition. Automatic word correction research was viewed as BACKGROUND 32 focusing on three progressively more difficult problems: (1) non-word error detection; (2) isolated-word error correction; and (3) context-dependent word correction.
Reference: [91] <author> R. Labuz. </author> <title> Typography and Typesetting. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Although there are thousands of fonts, there are usually only several of them used in a real document [9]. For those words printed in the same font, typesetting parameters, such as letter space, are usually consistent within a page or an article <ref> [91] </ref>. The typographic characteristics of document images makes it possible to detect the visual similarity between words. 3.1.3 Postprocessing Using Visual Constraints The goal of text recognition is to transform word images in a text page into their equivalent strings.
Reference: [92] <author> K.F. Lee. </author> <title> Automatic Speech Recognition: The Development of the SPHINX System. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference-contexts: But speech recognition at the word level is inadequate. Error rates are fairly high even for the best systems such as SPHINX <ref> [92] </ref>. The SPHINX system has an error rate of 29.4 percent for speaker independent, continuous speech recognition. Normal continuous speech is usually filled with acoustic ambiguity at the level of pure acoustic-phonetic analysis.
Reference: [93] <author> S. Liang, M. Ahmadi, and M. Shridhar. </author> <title> Segmentation of touching characters in printed document recognition. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93, </booktitle> <pages> pages 569-572, </pages> <year> 1993. </year>
Reference-contexts: In this situation, several characters may be segmented as one character image or one character image may be split into two or more pieces. Many methods have been developed to improve character segmentation accuracy. Many current methods are based on connected component analysis, aspect ratio estimation, or profile analysis <ref> [11, 35, 79, 93] </ref>.
Reference: [94] <author> P.P. Lippman. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 4 </volume> <pages> 4-22, </pages> <year> 1987. </year>
Reference-contexts: Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes [79, 106, 143]. Many classification methods, such as nearest-neighbor classifiers [32], Bayesian classifiers [34], neural network classifiers <ref> [94, 132] </ref>, decision tree classifiers [105] and syntactic classifiers [41], have been designed to classify character images. While very high accuracy has been achieved on high-quality character images, the performance of OCR on poor-quality character images is still not satisfactory [8, 111].
Reference: [95] <author> D. Lopresti and J. Zhou. </author> <title> Using consensus sequence voting to correct OCR errors. </title> <booktitle> In IAPR Workshop On Document Analysis Systems, </booktitle> <pages> pages 191-202, </pages> <month> Oct. 18-20 </month> <year> 1994. </year>
Reference-contexts: Consensus voting methods have been applied to improve recognition accuracy by combining outputs of the same OCR on different scanned versions of the same page <ref> [95] </ref>. 2.1.5 Postprocessing Using Passage-level Linguistic Context Isolated word recognition is error-prone, especially when the input page is seriously degraded. Word-level postprocessing is not sufficient to detect and correct many recognition errors.
Reference: [96] <author> K. Lunde. </author> <title> Understanding Japanese Information Processing. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1993. </year>
Reference-contexts: In this chapter, we discuss briefly the uses of visual and symbolic/linguistic context for Chinese/Japanese OCR postprocess-ing. 10.1 Visual Similarity Analysis of Chinese Characters The Chinese and Japanese character sets are large since both utilize thousands of Chinese characters <ref> [96] </ref>.
Reference: [97] <author> A.K. Mackworth. </author> <title> Constraint Satisfaction. </title> <editor> In S. Shapiro, editor, </editor> <booktitle> Encyclopedia of Artificial Intelligence, </booktitle> <pages> pages 285-293. </pages> <year> 1992. </year>
Reference-contexts: Another is the so-called "structural/symbolic approach." 3.1.4.1 Postprocessing Using Statistical Language Model Word collocation is a simple but powerful statistical language model. By formalizing the FRAMEWORK 42 word candidate selection problem as an instance of a constraint satisfaction problem <ref> [97] </ref>, we proposed relaxation (pp. 292-300 in [29]) for word candidate selection. The probabilistic relaxation algorithm uses word collocation trained from large text corpora to re-evaluate the confidence scores of word candidates and to re-rank them based on their new confidence scores.
Reference: [98] <author> D.M. Magerman. </author> <title> Natural Language Parsing as Statistical Pattern Recognition. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1994. </year> <note> BIBLIOGRAPHY 223 </note>
Reference-contexts: Systems using this approach are more robust, and easier to build and maintain. Statistical language models have been applied to help symbolic systems in many important tasks, such as lexical acquisition, part-of-speech tagging, sentence parsing, sense disambiguation and discourse analysis <ref> [20, 98, 123, 122, 139] </ref>. 2.2.4 The Noisy Channel Model for Speech Recognition, OCR and Spelling Correction The noisy channel model developed in Information Theory was originally used to model communication along a noisy channel such as a telephone line.
Reference: [99] <author> D.M. Magerman and M.P. Marcus. </author> <title> Parsing a Natural Language Using Mutual Information Statistics. </title> <booktitle> In Proceeding of AAAI-90, </booktitle> <year> 1990. </year>
Reference-contexts: In the section, we will study methodologies in areas of natural language processing and techniques developed for applications. 2.2.1 Statistical Language Modeling Research from artificial intelligence and computational linguistics has shown that statistical methods can achieve promising performance in the tasks of lexical acquisition [75], tagging [25], parsing <ref> [42, 99, 100] </ref> and sense disambiguation [140] and that these informa BACKGROUND 26 tion sources can be used in systems for natural language processing [161], machine translation [14], information retrieval [88], and speech recognition [160]. The statistical information is usually collected by training on a large text corpus.
Reference: [100] <author> D.M. Magerman and M.P. Marcus. Pearl: </author> <title> A Probabilistic Chart Parser. </title> <booktitle> In Proceedings of 5th Conference of the European Chapter of the Association for Computational Linguistics, </booktitle> <year> 1991. </year>
Reference-contexts: In the section, we will study methodologies in areas of natural language processing and techniques developed for applications. 2.2.1 Statistical Language Modeling Research from artificial intelligence and computational linguistics has shown that statistical methods can achieve promising performance in the tasks of lexical acquisition [75], tagging [25], parsing <ref> [42, 99, 100] </ref> and sense disambiguation [140] and that these informa BACKGROUND 26 tion sources can be used in systems for natural language processing [161], machine translation [14], information retrieval [88], and speech recognition [160]. The statistical information is usually collected by training on a large text corpus. <p> Many speech recognition systems use statistical language models exclusively, whereas many natural language understanding systems are symbolic. In recent years, there have been many efforts to combine the two approaches in tasks like speech understanding, text understanding and machine translation <ref> [4, 100, 103, 114, 146] </ref>. In a natural language processing system built with a symbolic approach, linguistic knowl BACKGROUND 30 edge may be represented by rules. The rules are often written manually, usually with a large human effort. <p> Local word collocation data can be collected by training on large text corpora [26]. The global structural constraints are exploited with a chart parsing model. The chart data structure provides a flexible framework for parsing [82, 166]. Statistical methods can be easily incorporated in a chart parser <ref> [100] </ref>. A chart parser can be extended to a lattice parser which allows for several word candidates at the same position, and therefore can be directly used for speech recognition [23, 152] and visual text recognition.
Reference: [101] <author> M. Marcus. </author> <title> A Theory of Syntactic Recognition for Natural Language. </title> <publisher> MIT Press, </publisher> <year> 1980. </year>
Reference-contexts: Because syntactic phenomena are complex, a grammar with comprehensive coverage can be difficult to obtain. Many parsing methods have been developed in the last thirty years, such as ATN parsers [163], generalized LR parsers [13, 151], deterministic parsers <ref> [101] </ref>, unification algorithms [83] and chart parsers [82, 166]. Among these techniques, chart parsing is quite popular. The chart in a chart parser is a data structure that stores the information about syntactic structures (or subtrees) already derived from an input sentence.
Reference: [102] <author> D.D. McDonald. </author> <title> Robust Partial-Parsing Through Incremental, Multi-Algorithm Processing. </title> <editor> In Paul S. Jacobs, editor, </editor> <booktitle> Text-Based intelligent Systems. </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1991. </year>
Reference-contexts: A parsing process sometimes may fail because of the incomplete grammar used in the parser or the ungrammatical constructions within the text or speech to be processed. Methods for repairing parser failures, such as partial parsing, have been reported <ref> [47, 56, 102, 128] </ref>. 2.2.3 Integration of Statistical and Structural Approaches Statistical and structural approaches have often been applied to different tasks. Many speech recognition systems use statistical language models exclusively, whereas many natural language understanding systems are symbolic.
Reference: [103] <author> M. Meteer and H. Gish. </author> <title> Integrating symbolic and statistical approaches in speech and natural language applications. In Proceedings of the Workshop: The Balancing Act Combining Symbolic and Statistical Approaches to Language, </title> <month> July 1 </month> <year> 1994. </year>
Reference-contexts: Many speech recognition systems use statistical language models exclusively, whereas many natural language understanding systems are symbolic. In recent years, there have been many efforts to combine the two approaches in tasks like speech understanding, text understanding and machine translation <ref> [4, 100, 103, 114, 146] </ref>. In a natural language processing system built with a symbolic approach, linguistic knowl BACKGROUND 30 edge may be represented by rules. The rules are often written manually, usually with a large human effort.
Reference: [104] <author> G.A. Miller and C. Fellbaum. </author> <title> Semantic network of english. </title> <journal> Cognition, </journal> <volume> 41, </volume> <year> 1991. </year>
Reference-contexts: We adopted the tagsets of those two corpora with some modifications. We further sub-categorized verbs into 15 subclasses by the type of argument the verb takes (see Table. 8.1). The information was extracted based on verb frames provided by WordNet <ref> [104] </ref>.
Reference: [105] <author> B.M.E. Moret. </author> <title> Decision trees and diagrams. </title> <journal> ACM Computing Surveys, </journal> <volume> 14 </volume> <pages> 593-623, </pages> <month> December </month> <year> 1982. </year>
Reference-contexts: Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes [79, 106, 143]. Many classification methods, such as nearest-neighbor classifiers [32], Bayesian classifiers [34], neural network classifiers [94, 132], decision tree classifiers <ref> [105] </ref> and syntactic classifiers [41], have been designed to classify character images. While very high accuracy has been achieved on high-quality character images, the performance of OCR on poor-quality character images is still not satisfactory [8, 111].
Reference: [106] <author> S. Mori, C.Y. Suen, and K. Yamamoto. </author> <title> Historical review of OCR research and development. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1029-1058, </pages> <year> 1992. </year>
Reference-contexts: The approach is traditionally called OCR (Optical Character Recognition), which has a long history and is the dominant paradigm in current text recognition research <ref> [106] </ref>. Character recognition is the key component of OCR-based word recognition. Given a character image, a character recognition algorithm generates decisions about the character's BACKGROUND 12 identity. This can be done by template matching or structural analysis. <p> To recognize any of the hundreds of typefaces in common use, current omni-font OCR engines take the structural analysis approach [9, 11, 79]. Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes <ref> [79, 106, 143] </ref>. Many classification methods, such as nearest-neighbor classifiers [32], Bayesian classifiers [34], neural network classifiers [94, 132], decision tree classifiers [105] and syntactic classifiers [41], have been designed to classify character images.
Reference: [107] <author> G. Nagy. </author> <booktitle> At the frontiers of OCR. Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1093-1100, </pages> <year> 1992. </year>
Reference-contexts: BACKGROUND 18 2.1.2 Text Recognition by Decoding Based on Visual Constraints Although text recognition is based on word recognition, it is not equivalent to the task of isolated word recognition. Text recognition can take advantage of the typographic uniformity of paragraphs, or other layout components <ref> [107] </ref>. To utilize visual constraints in a text passage, image clustering techniques have been introduced for text recognition. There are two basic classes of methods used for word image or character image based clustering.
Reference: [108] <author> G. Nagy, S. Seth, and K. Einspahr. </author> <title> Decoding substitution ciphers by means of word matching with application to OCR. </title> <journal> PAMI, </journal> (9):710-715, 1987. 
Reference-contexts: inherit the identity of the cluster, there are at least two applicable methods: one is simply to apply a character recognition algorithm; another is to use a deciphering algorithm in which the characters are treated as substitution ciphers and their true values can be solved by a dictionary-based deciphering algorithm <ref> [16, 108, 39] </ref>. <p> After clustering character images in a text page, a dictionary-based deciphering algorithm can be applied to find the best interpretation of each cluster so that the character images can be recognized without using OCR <ref> [16, 108] </ref>. By clustering visually equivalent word images, many function words and content words inside the text page can be found [72, 84]. This chapter studies visual contextual constraints more extensively. English is an alphabetic language. <p> This is then used in a self-teaching OCR system to recognize the rest of the text in the document. The approach proposed here is an extension of the character-based clustering and deciphering algorithms used previously <ref> [16, 18, 108] </ref>. The concept of a self-teaching OCR system has also been used in a character classifier that automatically adapts itself to a single font [9].
Reference: [109] <author> G. Nagy, S. Seth, and M. Viswanathan. </author> <title> A prototype document image analysis system for technical journals. </title> <journal> Computer, </journal> <volume> 25(7) </volume> <pages> 10-24, </pages> <year> 1992. </year> <note> BIBLIOGRAPHY 224 </note>
Reference-contexts: Chapter 2 Background Visual text recognition is an application area of image processing, pattern recognition and natural language processing. In this chapter, research in several related areas is surveyed. 2.1 Text Recognition To automatically recognize the text in a document image, the first step is page layout analysis <ref> [6, 19, 109, 155] </ref>. The process of layout analysis detects and extracts text blocks and text lines. Each text line can be further segmented into a sequence of word images. Because of the space between words in machine-printed English text, it is not difficult to find word boundaries.
Reference: [110] <author> A. Newell, J. Barnett, J.W. Forgie, C. Green, D. Klatt, J.C.R Licklider, J. Munson, D.R. Reddy, and W.A. Woods. </author> <title> Speech Understanding Systems Final Report of a Study Group. </title> <publisher> North-Holland/American Elsevier, </publisher> <year> 1973. </year>
Reference-contexts: Since the 1980s, many experiments have been carried out using statistical language models and natural language processing tools. 2.2.5 Speech Recognition Speech recognition is similar to visual text recognition. The goal of automatic speech recognition is to develop techniques and systems that enable computers to accept speech input <ref> [110, 121] </ref>. Speech recognition at the phoneme and word level has used Hidden-Markov-Models (HMM) [115] and other statistical methods successfully. Practical systems have been built for speaker-independent, continuous, large vocabulary (1,000 words or more) speech recognition in a specified domain, such as an airline travel information service (ATIS) [169].
Reference: [111] <author> T. Pavlidis. </author> <title> Problems in the recognition of poorly printed text. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 162-173, </pages> <year> 1992. </year>
Reference-contexts: While very high accuracy has been achieved on high-quality character images, the performance of OCR on poor-quality character images is still not satisfactory <ref> [8, 111] </ref>. Improving performance on poor quality documents is a challenging problem to which much OCR research is now devoted. New feature representations and new classification methods have been proposed [51, 111]. <p> Improving performance on poor quality documents is a challenging problem to which much OCR research is now devoted. New feature representations and new classification methods have been proposed <ref> [51, 111] </ref>. The role of training sets for classification has been investigated and it has been suggested that the quality of training sets, rather than classification methodology, is the determining factor in achieving higher accuracy [52]. By taking advantage of local typeface homogeneity, OCR accuracy can be improved [9]. <p> Given a high quality text page, current commercial document recognition systems can recognize the words on the page at a high correct rate [22, 125, 126]. However, given a degraded text page, such as a multiple-generation photocopy or facsimile, their performance usually drops abruptly <ref> [8, 111] </ref>. image can be read correctly by a human but an OCR system makes a significant number of mistakes. The recognition results generated by a commercial OCR system are also shown in the same figure.
Reference: [112] <author> I.T. Phillips, S. Chen, J. Ha, and R.M. Haralick. </author> <title> English document database design and implementation methodology. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 65-104, </pages> <year> 1993. </year>
Reference-contexts: To characterize different sources of degradation on document pages, document degradation models (DDM) have been proposed [7, 8, 81, 80]. Tools based on those DDMs and defect images generated by the methods can be found in the UW English Document Images Database, which is available in CDROM form <ref> [112] </ref>. Those parameterized models can quantitatively describe local or global image defects. They can be used to evaluate the performance of a recognition system for a continuum of degradation levels [53].
Reference: [113] <author> A. Pollatsek and K. Rayner. </author> <title> Reading. </title> <editor> In M. Posner, editor, </editor> <booktitle> Foundation Of Cognitive Science. </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: decoder estimates the message, by finding the a posteriori most probable path (so-called "maximum a posteriori (MAP)") through the combined source and channel models using a Viterbi-like dynamic programming algorithm. 2.1.3 Reading by Hypothesis Generation and Testing Based on previous studies of how people read and the psychology of reading <ref> [15, 113, 120] </ref>, a computational theory for the visual recognition of words of text was proposed [69]. The theory includes three stages: hypothesis generation, hypothesis testing, and global contextual analysis. Hypothesis generation uses gross visual features to provide expectations about word identities.
Reference: [114] <author> P. Price. </author> <title> Combining linguistic with statistical methods in automatic speech understanding. In Proceedings of the Workshop: The Balancing Act Combining Symbolic and Statistical Approaches to Language, </title> <month> July 1 </month> <year> 1994. </year>
Reference-contexts: Many speech recognition systems use statistical language models exclusively, whereas many natural language understanding systems are symbolic. In recent years, there have been many efforts to combine the two approaches in tasks like speech understanding, text understanding and machine translation <ref> [4, 100, 103, 114, 146] </ref>. In a natural language processing system built with a symbolic approach, linguistic knowl BACKGROUND 30 edge may be represented by rules. The rules are often written manually, usually with a large human effort.
Reference: [115] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <year> 1986. </year>
Reference-contexts: The goal of automatic speech recognition is to develop techniques and systems that enable computers to accept speech input [110, 121]. Speech recognition at the phoneme and word level has used Hidden-Markov-Models (HMM) <ref> [115] </ref> and other statistical methods successfully. Practical systems have been built for speaker-independent, continuous, large vocabulary (1,000 words or more) speech recognition in a specified domain, such as an airline travel information service (ATIS) [169]. But speech recognition at the word level is inadequate.
Reference: [116] <author> A. Radford. </author> <title> Transformational Grammar: A First Course. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: Words are the units on which more complex language structures, such as phrases, sentences and discourses, are based. Any description about higher-level structures, either syntactic structures or semantic structures, must be projected from the lexical descriptions of the involved words (Chapter 7 in <ref> [116] </ref>). Therefore, lexical knowledge is necessary for any natural language processing system. In recent years, there have been many efforts to create machine-readable and machine-accessible lexical knowledge bases in computational linguistics [75]. It is almost impossible to build a large-scale machine-accessible lexicon manually. <p> The structural descriptions are necessary for further processing, for example, for semantic interpretation. In the last three decades, many frameworks have been developed to model the syntactic phenomena of English <ref> [12, 116, 136, 157] </ref>. Among them are context-free grammars (CFG), transformational grammars (TG), lexical functional grammars (LFG), generalized phrase structure grammars (GPSG) and government-binding (GB) theory. In an NLP system, a grammar is usually represented as a rule base.
Reference: [117] <author> Inc. RAF Technology. </author> <title> Programmer's guide to the dafs library, release 0.7. </title> <type> Technical report, </type> <month> December 9 </month> <year> 1994. </year>
Reference-contexts: The Document Attribute Format Specification (DAFS), is being jointly defined by ARPA and RAF, and being promoted by ARPA as a standard [118]. Based on DAFS, ILLUMINATOR also provides tools which make OCR postprocessing more efficient and accurate <ref> [117, 119] </ref>. 2.1.1.4 The Whole Word Recognition Approach Lexical constraints can be directly exploited in the process of word recognition by applying the word-shape analysis method [55]. In this way, character segmentation is bypassed.
Reference: [118] <author> Inc. RAF Technology. Dafs: </author> <title> Document attribute format specification. </title> <type> Technical report, </type> <month> January 5 </month> <year> 1995. </year>
Reference-contexts: It provides a format for breaking down documents into standardized entities, defining entities boundaries and attributes and tagging or labeling their contents and attributes. The Document Attribute Format Specification (DAFS), is being jointly defined by ARPA and RAF, and being promoted by ARPA as a standard <ref> [118] </ref>. Based on DAFS, ILLUMINATOR also provides tools which make OCR postprocessing more efficient and accurate [117, 119]. 2.1.1.4 The Whole Word Recognition Approach Lexical constraints can be directly exploited in the process of word recognition by applying the word-shape analysis method [55]. In this way, character segmentation is bypassed.
Reference: [119] <institution> Inc. RAF Technology. </institution> <note> Illuminator user's manual, release 0.9. Technical report, May 31 1995. BIBLIOGRAPHY 225 </note>
Reference-contexts: Various lattice-based methods have been applied [6, 135, 150, 154]. In previous studies, each entry in a lattice is only a representation at the character level. Sponsored by ARPA (Advanced Research Projects Agency), RAF Technology, Inc. is developing ILLUMINATOR, a new framework for document decomposition, recognition, storage and interchange <ref> [119] </ref>. It provides a format for breaking down documents into standardized entities, defining entities boundaries and attributes and tagging or labeling their contents and attributes. The Document Attribute Format Specification (DAFS), is being jointly defined by ARPA and RAF, and being promoted by ARPA as a standard [118]. <p> The Document Attribute Format Specification (DAFS), is being jointly defined by ARPA and RAF, and being promoted by ARPA as a standard [118]. Based on DAFS, ILLUMINATOR also provides tools which make OCR postprocessing more efficient and accurate <ref> [117, 119] </ref>. 2.1.1.4 The Whole Word Recognition Approach Lexical constraints can be directly exploited in the process of word recognition by applying the word-shape analysis method [55]. In this way, character segmentation is bypassed.
Reference: [120] <author> K. Rayner and A. Pollatsek. </author> <title> Psychology of Reading. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: decoder estimates the message, by finding the a posteriori most probable path (so-called "maximum a posteriori (MAP)") through the combined source and channel models using a Viterbi-like dynamic programming algorithm. 2.1.3 Reading by Hypothesis Generation and Testing Based on previous studies of how people read and the psychology of reading <ref> [15, 113, 120] </ref>, a computational theory for the visual recognition of words of text was proposed [69]. The theory includes three stages: hypothesis generation, hypothesis testing, and global contextual analysis. Hypothesis generation uses gross visual features to provide expectations about word identities.
Reference: [121] <author> D.R. Reddy. </author> <title> Speech recognition by machine: a review. </title> <booktitle> In Proceedings of ICASSP 1992, </booktitle> <year> 1992. </year>
Reference-contexts: Since the 1980s, many experiments have been carried out using statistical language models and natural language processing tools. 2.2.5 Speech Recognition Speech recognition is similar to visual text recognition. The goal of automatic speech recognition is to develop techniques and systems that enable computers to accept speech input <ref> [110, 121] </ref>. Speech recognition at the phoneme and word level has used Hidden-Markov-Models (HMM) [115] and other statistical methods successfully. Practical systems have been built for speaker-independent, continuous, large vocabulary (1,000 words or more) speech recognition in a specified domain, such as an airline travel information service (ATIS) [169].
Reference: [122] <author> S. Reichardson. </author> <title> Bootstrapping statistical processing into a rule-based natural language parser. In Proceedings of the Workshop: The Balancing Act Combining Symbolic and Statistical Approaches to Language, </title> <month> July 1 </month> <year> 1994. </year>
Reference-contexts: Systems using this approach are more robust, and easier to build and maintain. Statistical language models have been applied to help symbolic systems in many important tasks, such as lexical acquisition, part-of-speech tagging, sentence parsing, sense disambiguation and discourse analysis <ref> [20, 98, 123, 122, 139] </ref>. 2.2.4 The Noisy Channel Model for Speech Recognition, OCR and Spelling Correction The noisy channel model developed in Information Theory was originally used to model communication along a noisy channel such as a telephone line.
Reference: [123] <author> P.S. </author> <title> Resnik. Selection and Information: A Class-Based Approach to Lexical Relationships. </title> <type> PhD thesis, </type> <institution> Computer and Information Science Department of UPENN, </institution> <year> 1993. </year>
Reference-contexts: Systems using this approach are more robust, and easier to build and maintain. Statistical language models have been applied to help symbolic systems in many important tasks, such as lexical acquisition, part-of-speech tagging, sentence parsing, sense disambiguation and discourse analysis <ref> [20, 98, 123, 122, 139] </ref>. 2.2.4 The Noisy Channel Model for Speech Recognition, OCR and Spelling Correction The noisy channel model developed in Information Theory was originally used to model communication along a noisy channel such as a telephone line.
Reference: [124] <author> S.V. Rice, F.R. Jenkins, and T.A. Nartker. </author> <booktitle> The fourth annual test of OCR accuracy. In 1995 Annual Report of ISRI, </booktitle> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <pages> pages 11-50, </pages> <year> 1995. </year>
Reference-contexts: integrate character segmentation with character recognition under the belief that segmentation decisions are tentative until confirmed by the successful recognition of segmented image pieces [18, 154]. 2.1.1.2 Commercial OCR Systems Since 1992, the Information Science Research Institute (ISRI) of University of Nevada has conducted an annual test of OCR systems <ref> [125, 126, 124] </ref>. Given a binary image of any document page, an OCR system, also called as "page reader", identifies the machine-printed characters on the page. In a test, the accuracy of each system is measured by comparing its OCR result with the truth text.
Reference: [125] <author> S.V. Rice, J. Kanai, and T.A. Nartker. </author> <title> An evaluation of OCR accuracy. </title> <booktitle> In 1993 Annual Report of ISRI, </booktitle> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <pages> pages 9-34, </pages> <year> 1993. </year>
Reference-contexts: integrate character segmentation with character recognition under the belief that segmentation decisions are tentative until confirmed by the successful recognition of segmented image pieces [18, 154]. 2.1.1.2 Commercial OCR Systems Since 1992, the Information Science Research Institute (ISRI) of University of Nevada has conducted an annual test of OCR systems <ref> [125, 126, 124] </ref>. Given a binary image of any document page, an OCR system, also called as "page reader", identifies the machine-printed characters on the page. In a test, the accuracy of each system is measured by comparing its OCR result with the truth text. <p> Given a high quality text page, current commercial document recognition systems can recognize the words on the page at a high correct rate <ref> [22, 125, 126] </ref>. However, given a degraded text page, such as a multiple-generation photocopy or facsimile, their performance usually drops abruptly [8, 111]. image can be read correctly by a human but an OCR system makes a significant number of mistakes.
Reference: [126] <author> S.V. Rice, J. Kanai, and T.A. Nartker. </author> <booktitle> The third annual test of OCR accuracy. In 1994 Annual Report of ISRI, </booktitle> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <pages> pages 11-40, </pages> <year> 1994. </year>
Reference-contexts: integrate character segmentation with character recognition under the belief that segmentation decisions are tentative until confirmed by the successful recognition of segmented image pieces [18, 154]. 2.1.1.2 Commercial OCR Systems Since 1992, the Information Science Research Institute (ISRI) of University of Nevada has conducted an annual test of OCR systems <ref> [125, 126, 124] </ref>. Given a binary image of any document page, an OCR system, also called as "page reader", identifies the machine-printed characters on the page. In a test, the accuracy of each system is measured by comparing its OCR result with the truth text. <p> Given a high quality text page, current commercial document recognition systems can recognize the words on the page at a high correct rate <ref> [22, 125, 126] </ref>. However, given a degraded text page, such as a multiple-generation photocopy or facsimile, their performance usually drops abruptly [8, 111]. image can be read correctly by a human but an OCR system makes a significant number of mistakes.
Reference: [127] <author> E.S. Ristad. </author> <title> Computational structure of gpsg models. </title> <journal> Linguistics and Philosophy, </journal> <volume> 13 </volume> <pages> 521-587, </pages> <year> 1990. </year>
Reference-contexts: The large rule base is mainly the result of verb sub-categorization. Although there are many rules, it was not difficult to write them manually because most rules can be derived formally from a small set of meta rules, such as those used in Generalized Phrase Structure Grammar (GPSG) models <ref> [127] </ref>. The original re-write rules are transformed into Chomsky normal form (CNF) by introducing new intermediate nonterminals. The number of intermediate nonterminals to be introduced can be minimized by an optimization process. After translation, the equivalent CNF grammar contains 7183 rules.
Reference: [128] <author> C.P. Rose and A. Waibel. </author> <title> Recoving from parser failure: A hybrid statistical/symbolic approach. In Proceedings of the Workshop: The Balancing Act Combining Symbolic and Statistical Approaches to Language, </title> <month> July 1 </month> <year> 1994. </year>
Reference-contexts: A parsing process sometimes may fail because of the incomplete grammar used in the parser or the ungrammatical constructions within the text or speech to be processed. Methods for repairing parser failures, such as partial parsing, have been reported <ref> [47, 56, 102, 128] </ref>. 2.2.3 Integration of Statistical and Structural Approaches Statistical and structural approaches have often been applied to different tasks. Many speech recognition systems use statistical language models exclusively, whereas many natural language understanding systems are symbolic.
Reference: [129] <author> T.G. Rose and L.J. Evett. </author> <title> Text recognition using collocations and domain codes. </title> <booktitle> In Proceedings of the Workshop on Very Large Corpora:Academic and Industrial Perspectives, </booktitle> <pages> pages 65-73, </pages> <year> 1993. </year> <note> BIBLIOGRAPHY 226 </note>
Reference-contexts: Previous work in using word collocation data to post-process word recognition results has shown the usefulness of this data <ref> [129] </ref>. This technique used local collocation data about words that co-occur next to each other to improve recognition performance. A disadvantage of this approach was that it did not allow for successful results on one word to influence the results on another word.
Reference: [130] <author> T.G. Rose, L.J. Evett, and R.J. Whitrow. </author> <title> The Use of Semantic Information as an Aid to Handwriting Recognition. </title> <booktitle> In Proceedings of the First International Conference on Document Analysis (ICDAR-91), </booktitle> <pages> pages 629-637, </pages> <year> 1991. </year>
Reference-contexts: Passage-level postprocessing is usually formalized as a candidate selection problem in its simplest case. To select a candidate for each word image, those constraints can play a crucial role. Previous approaches have utilized local word-to-word transitions [68], statistical part-of-speech transitions [70], definitional overlap [40], word collocation constraints <ref> [130] </ref>, transitions between hypertags [141], N-best strategies [31] and semantic constraints from a limited domain, such as chess [10]. <p> Chapter 7 A Relaxation Algorithm Using Word Collocation 7.1 Introduction Word collocation data is one source of information that has been investigated in computational linguistics and that has been proposed as a useful tool to post-process word recognition results <ref> [26, 130] </ref>. Word collocation refers to the likelihood that two words co-occur within a fixed distance of one another.
Reference: [131] <author> A. Rosenfeld, R.A. Hummel, </author> <title> and S.W. Zucker. Scene labeling by relaxation operations. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-6(6):420-433, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: Relaxation has been widely used to solve problems where the search for a globally optimal solution can be broken down into a series of local problems, each of which contributes to the overall result <ref> [131] </ref>. The rest of the chapter discusses the algorithm in more detail. An experimental analysis is discussed in which the algorithm is applied to improving text recognition results that are less than 60% correct. The correct rate is effectively improved to 83% or better in all cases.
Reference: [132] <author> D.E. Rumelhart and J.L. McClelland. </author> <title> An interactive activation model of context effects in letter perception. </title> <journal> Psychological Review, </journal> <volume> 89(1) </volume> <pages> 60-94, 82. </pages>
Reference-contexts: Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes [79, 106, 143]. Many classification methods, such as nearest-neighbor classifiers [32], Bayesian classifiers [34], neural network classifiers <ref> [94, 132] </ref>, decision tree classifiers [105] and syntactic classifiers [41], have been designed to classify character images. While very high accuracy has been achieved on high-quality character images, the performance of OCR on poor-quality character images is still not satisfactory [8, 111].
Reference: [133] <author> K. Yamamoto S. Mori and M. </author> <title> Yasuda. </title> <journal> Research on machine recognition of hand-printed characters. IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6(4):386-405, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: To represent the stroke structure of a Chinese character, its feature vector usually has many dimensions. For some types of structural features such as local stroke direction (LSD) <ref> [2, 133] </ref>, geometric information is retained as in the original image. We use LSD features for our experiments.
Reference: [134] <author> R. Schartz and S. Austin. </author> <title> Efficient, high-performance algorithms for n-best search. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop 1990, </booktitle> <pages> pages 6-11, </pages> <year> 1990. </year>
Reference-contexts: Besides phonemic and phonologic knowledge, syntactic, semantic, prosodic, pragmatic knowledge sources and knowledge about the real world are applied to speech recognition. Under the approaches of statistical language models and structural analysis, methods such as the N-best search <ref> [134] </ref>, HMM [67] and lattice parsing [23, 48, 152] are designed to select word candidates from multiple choices generated by acoustic-phonetic analysis. 2.3 Conclusions Text recognition is not only an image processing and pattern recognition task, but also a natural language processing task.
Reference: [135] <author> J. Schurmann, N. Bartneck, T. Bayer, J. Franke, E. Mandler, and M. Oberlander. </author> <title> Document analysis from pixels to contents. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1101-1119, </pages> <year> 1992. </year>
Reference-contexts: As a general framework to resolve ambiguity at several levels, a lattice has been identified as a useful structure to keep records of possible segmentation points and character alternatives according to their geometric positions. Various lattice-based methods have been applied <ref> [6, 135, 150, 154] </ref>. In previous studies, each entry in a lattice is only a representation at the character level. Sponsored by ARPA (Advanced Research Projects Agency), RAF Technology, Inc. is developing ILLUMINATOR, a new framework for document decomposition, recognition, storage and interchange [119].
Reference: [136] <author> P. Sells. </author> <title> Lectures on Contempary Syntactic Theories: An introduction to Government-Binding Theory, Generalized Phrase Structure Grammar and Lexical-Functional Grammar. </title> <address> CSLI:Stanford, </address> <year> 1985. </year>
Reference-contexts: The structural descriptions are necessary for further processing, for example, for semantic interpretation. In the last three decades, many frameworks have been developed to model the syntactic phenomena of English <ref> [12, 116, 136, 157] </ref>. Among them are context-free grammars (CFG), transformational grammars (TG), lexical functional grammars (LFG), generalized phrase structure grammars (GPSG) and government-binding (GB) theory. In an NLP system, a grammar is usually represented as a rule base.
Reference: [137] <author> S. Seneff. </author> <title> Robust parsing for spoken language systems. </title> <booktitle> In Proceedings of ICASSP 1992, </booktitle> <year> 1992. </year>
Reference-contexts: The acoustic ambiguity may be caused by uncertainty of word boundaries, phonetic ambiguity, syllable omissions and other missing information. Researchers believe that acoustic ambiguity can only be resolved through the use of higher sources of knowledge and NLP techniques. There have been efforts to integrate speech recognition with NLP <ref> [48, 76, 137, 160, 165] </ref>. NLP can play two roles. The first role is to interpret the meaning of an utterance. The second, more subtle role, is to reduce acoustic ambiguity by "understanding" the utter BACKGROUND 33 ance.
Reference: [138] <author> A.W. Senior. </author> <title> Off-line Cursive Handwritting Recognition Using Recurrent Neural Networks. </title> <type> PhD thesis, </type> <institution> Engineering Department of Cambridge University, </institution> <year> 1994. </year> <note> BIBLIOGRAPHY 227 </note>
Reference-contexts: The word lattice parser can be generalized to handle these cases. Visual contextual information is also applicable. Figure 12.6 is a portion of a handwritten text page <ref> [138] </ref>. Because people write with a consistent style, visual similarities between word images can be observed (see Figure 12.7 for examples). Like what we demonstrated for machine-printed text recognition, visual inter-word relations can be useful to improve handwriting recognition performance.
Reference: [139] <author> E.V. Siegel and K.R. McKeown. </author> <title> Emergent linguistic rules from inducing decision trees: Disambiguation discourse clue words. </title> <booktitle> In Proceedings of AAAI, </booktitle> <year> 1994. </year>
Reference-contexts: Systems using this approach are more robust, and easier to build and maintain. Statistical language models have been applied to help symbolic systems in many important tasks, such as lexical acquisition, part-of-speech tagging, sentence parsing, sense disambiguation and discourse analysis <ref> [20, 98, 123, 122, 139] </ref>. 2.2.4 The Noisy Channel Model for Speech Recognition, OCR and Spelling Correction The noisy channel model developed in Information Theory was originally used to model communication along a noisy channel such as a telephone line.
Reference: [140] <author> B. Slator. </author> <title> Using context for sense preference. </title> <editor> In U. Zernik, editor, </editor> <title> Lexical Acquisition: Exploitung On-Line Resources to Build a Lexicon, </title> <address> pages 65-96. </address> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <year> 1991. </year>
Reference-contexts: will study methodologies in areas of natural language processing and techniques developed for applications. 2.2.1 Statistical Language Modeling Research from artificial intelligence and computational linguistics has shown that statistical methods can achieve promising performance in the tasks of lexical acquisition [75], tagging [25], parsing [42, 99, 100] and sense disambiguation <ref> [140] </ref> and that these informa BACKGROUND 26 tion sources can be used in systems for natural language processing [161], machine translation [14], information retrieval [88], and speech recognition [160]. The statistical information is usually collected by training on a large text corpus.
Reference: [141] <author> R.K. Srihari, S. Ng, M. Baltus, and J. Kud. </author> <title> Use of language models in on-line sentence/phrase recognition. </title> <booktitle> In Third International Workshop on Frontiers in Handwriting Recognition, </booktitle> <pages> pages 284-294, </pages> <month> May 25-27 </month> <year> 1993. </year>
Reference-contexts: To select a candidate for each word image, those constraints can play a crucial role. Previous approaches have utilized local word-to-word transitions [68], statistical part-of-speech transitions [70], definitional overlap [40], word collocation constraints [130], transitions between hypertags <ref> [141] </ref>, N-best strategies [31] and semantic constraints from a limited domain, such as chess [10]. <p> Future Directions 208 12.5 Handwritten Text Recognition Handwritten text recognition is a similar, but more difficult task. Techniques designed here for machine-printed text recognition can be adapted to word candidate selection for this task. Significant research has been conducted on using linguistic contextual information to improve recognition performance <ref> [37, 156, 141, 159] </ref>. Because word segmentation sometimes is not an easy task, many possible word boundaries can be generated and the structure of a word lattice may become more complex: positions of word candidates may overlap (see the example inFigure 12.5).
Reference: [142] <author> S.N. Srihari. </author> <title> Computer Text Recognition and Error Correction. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1984. </year>
Reference-contexts: The lexical constraint is the one that is used most BACKGROUND 15 widely. Methods of dictionary look-up [158], confusion-matrix-based transformation and string editing [36, 79], probabilistic relaxation [44], and character n-gram frequency analysis <ref> [74, 142] </ref> have been developed to exploit the lexical constraint efficiently. Such methods work well on non-word error detection. If a string generated by an OCR for a word image does not appear in a dictionary, which can be very large for wide coverage, a non-word error can be detected.
Reference: [143] <author> S.N. Srihari. </author> <title> High-performance reading machines. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1120-1132, </pages> <year> 1992. </year>
Reference-contexts: To recognize any of the hundreds of typefaces in common use, current omni-font OCR engines take the structural analysis approach [9, 11, 79]. Different features, including various statistical and structural aspects, have been invented to concisely describe the structure of characters in various typefaces and font sizes <ref> [79, 106, 143] </ref>. Many classification methods, such as nearest-neighbor classifiers [32], Bayesian classifiers [34], neural network classifiers [94, 132], decision tree classifiers [105] and syntactic classifiers [41], have been designed to classify character images.
Reference: [144] <author> S.N. Srihari. </author> <title> From pixels to paragraphs: the use of models in text recognition. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 47-64, </pages> <year> 1993. </year>
Reference-contexts: Given a word image, there are basically two approaches towards isolated word recognition. One is the analytical approach, or "character-based word recognition"; another is the holistic approach, or "word shape recognition" <ref> [144] </ref>. 2.1.1.1 The OCR Approach By taking the analytical approach, word recognition can be accomplished by a three-stage process: character segmentation, character recognition, and postprocessing [79]. <p> There are many contextual constraints that can be utilized if we consider information about the relations among words and the structure of documents <ref> [77, 144] </ref>. Passage-level postprocessing is usually formalized as a candidate selection problem in its simplest case. To select a candidate for each word image, those constraints can play a crucial role.
Reference: [145] <author> G.A. Story, L.O'Gorman, D. Fox, L.L. Schaper, and H.V. Jagadish. </author> <title> The rightpages image-based electronic library for alerting and browsing. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 17-26, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: items of the word "Lloyd" which are visually similar (character segmentation and recognition for those words will be easier if we consider that they have to be interpreted as the same word; and (c) examples of partial similarties which can be found between word images. 12.6 Summary Image-based document retrieval <ref> [21, 71, 145] </ref> and text editing [5] are two interesting applications in which visual and linguistic information can also be utilized. The objective of visual text recognition is to correctly transform an arbitrary image of text into its symbolic equivalent.
Reference: [146] <author> T. Strzalkowski and B. Vauthey. </author> <title> Information retrieval using robust natural language processing. </title> <booktitle> In Proceedings of the 30nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <month> June 28 - July 2 </month> <year> 1992. </year>
Reference-contexts: Many speech recognition systems use statistical language models exclusively, whereas many natural language understanding systems are symbolic. In recent years, there have been many efforts to combine the two approaches in tasks like speech understanding, text understanding and machine translation <ref> [4, 100, 103, 114, 146] </ref>. In a natural language processing system built with a symbolic approach, linguistic knowl BACKGROUND 30 edge may be represented by rules. The rules are often written manually, usually with a large human effort.
Reference: [147] <institution> Calera Recognition Systems. </institution> <note> WordScan Plus, user's guide. </note> <year> 1994. </year>
Reference-contexts: In OCR systems such as Caere's OmniPage, BACKGROUND 14 Calera's WordScan Plus, ExpertVision's TypeReader and XIS's TextBridge, for the characters or words on which the recognition results are not confident, suspicion marks are provided so that the user can verify and edit them manually with a graphical interface <ref> [147, 38] </ref> (see at the original word image and its linguistic context. marked word in the OCR result, the verifier can display the original word image so that the user can verify the recognition result easily.
Reference: [148] <author> K. Taghva, J. Borsack, and A. Condit. </author> <title> An expert system for automatically correcting OCR output. </title> <booktitle> In Proceedings of the Conference on Document Recognition of 1994 IS&T/SPIE Symposium, </booktitle> <pages> pages 270-278, </pages> <month> February 6-10 </month> <year> 1994. </year> <note> BIBLIOGRAPHY 228 </note>
Reference-contexts: Post-editing through approximation and global correction was demonstrated as a useful method in which a string with errors can be corrected because it is very close to some well-recognized words (centroids) in the text <ref> [148, 149] </ref>. Other techniques developed in natural language understanding (NLU), such as parsing, can be integrated with text recognition. However this is sometimes difficult because some NLU techniques were developed for restricted domains and cannot process unrestricted English text accurately, efficiently and robustly.
Reference: [149] <author> K. Taghva, J. Borsack, A. Condit, and S. Erva. </author> <title> The effects of noisy data on text retrieval. </title> <booktitle> In 1993 Annual Report of ISRI, </booktitle> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <pages> pages 71-82, </pages> <year> 1993. </year>
Reference-contexts: Post-editing through approximation and global correction was demonstrated as a useful method in which a string with errors can be corrected because it is very close to some well-recognized words (centroids) in the text <ref> [148, 149] </ref>. Other techniques developed in natural language understanding (NLU), such as parsing, can be integrated with text recognition. However this is sometimes difficult because some NLU techniques were developed for restricted domains and cannot process unrestricted English text accurately, efficiently and robustly.
Reference: [150] <author> C.C. Tappert, C.Y. Suen, and T. Wakahara. </author> <title> The state of the art in on-line handwriting recognition. </title> <journal> IEEE Transactions on pattern analysis and machine intelligence, </journal> <volume> 12(8) </volume> <pages> 787-808, </pages> <year> 1990. </year>
Reference-contexts: As a general framework to resolve ambiguity at several levels, a lattice has been identified as a useful structure to keep records of possible segmentation points and character alternatives according to their geometric positions. Various lattice-based methods have been applied <ref> [6, 135, 150, 154] </ref>. In previous studies, each entry in a lattice is only a representation at the character level. Sponsored by ARPA (Advanced Research Projects Agency), RAF Technology, Inc. is developing ILLUMINATOR, a new framework for document decomposition, recognition, storage and interchange [119]. <p> Appropriate thresholds are incorporated in the algorithm so that character strings not in the dictionary may also be output. This approach is similar to some methods used in cursive script recognition <ref> [150] </ref>. The primary difference is that the algorithm proposed here learns the character image information it uses from the input page rather than from a previous training step. An example of re-recognition is shown in Figure 6.6. Overall there are five complete paths that cover the entire word image.
Reference: [151] <author> M. Tomita. </author> <title> Efficient Parsing For Natural Language. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1985. </year>
Reference-contexts: Because syntactic phenomena are complex, a grammar with comprehensive coverage can be difficult to obtain. Many parsing methods have been developed in the last thirty years, such as ATN parsers [163], generalized LR parsers <ref> [13, 151] </ref>, deterministic parsers [101], unification algorithms [83] and chart parsers [82, 166]. Among these techniques, chart parsing is quite popular. The chart in a chart parser is a data structure that stores the information about syntactic structures (or subtrees) already derived from an input sentence.
Reference: [152] <author> M. Tomita. </author> <title> An Efficient Word Lattice Parsing Algorithm for Continuous Speech Recognition. </title> <booktitle> In Proceedings of the International Conference on Acoustic, Speech and Signal Processing, </booktitle> <year> 1986. </year>
Reference-contexts: This places restrictions on applying NLP techniques in many practical tasks, such as unrestricted text analysis, because sentences are usually long in texts from books, newspapers, journals, or technical reports. Methods for sentence parsing can be generalized to word lattice parsing. As defined by M. Tomita <ref> [152] </ref>, a word lattice is a set of hypothesised words fW 1 ; :::; W n g. <p> Augmented with semantic and pragmatic knowledge, a lattice parser can select candidates which are not only a grammatical sequence, but also a meaningful one. Lattice parsers were designed for word candidate selection in speech recognition <ref> [23, 48, 152] </ref>. For example, "I saw the man" can be genegrated by a lattice parser as the best word sequence for the word lattice in Figure 2.5. <p> Besides phonemic and phonologic knowledge, syntactic, semantic, prosodic, pragmatic knowledge sources and knowledge about the real world are applied to speech recognition. Under the approaches of statistical language models and structural analysis, methods such as the N-best search [134], HMM [67] and lattice parsing <ref> [23, 48, 152] </ref> are designed to select word candidates from multiple choices generated by acoustic-phonetic analysis. 2.3 Conclusions Text recognition is not only an image processing and pattern recognition task, but also a natural language processing task. <p> Statistical methods can be easily incorporated in a chart parser [100]. A chart parser can be extended to a lattice parser which allows for several word candidates at the same position, and therefore can be directly used for speech recognition <ref> [23, 152] </ref> and visual text recognition. The parser chooses the words on a path through the lattice that correspond to a legal sentence with the highest probability of being correct, given the sentences represented in the lattice. <p> In the statistical approach, language models such as a Hidden Markov Model and word collocation can be utilized for candidate selection [26, 58, 70]. In the structural approach, lattice parsing techniques have been developed for candidate selection <ref> [62, 152] </ref>. The contextual constraints considered in a statistical language model, such as word collocation, are local constraints. For a word image, a candidate will be selected according to the candidate information from its neighboring word images in a fixed window size.
Reference: [153] <author> M. Tomita. </author> <title> Why parsing technologies? In Masaru Tomita, editor, </title> <booktitle> Current Issues in Parsing Technology, </booktitle> <pages> pages 1-9. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: By using semantic information, such as common sense knowledge and domain knowledge, many ambiguities can be resolved [49, 56, 57]. Because of the nature of natural language processing, parsing methods developed so BACKGROUND 29 far sometimes are not accurate, robust, or efficient <ref> [153] </ref>, especially when the length of the sentence to be parsed is long (i.e., on average more than 25 words). This places restrictions on applying NLP techniques in many practical tasks, such as unrestricted text analysis, because sentences are usually long in texts from books, newspapers, journals, or technical reports.
Reference: [154] <author> S. Tsujimoto and H. Asada. </author> <title> Resolving ambiguity in segmenting touching characters. </title> <booktitle> In 1st Int. Conf. on Document Analysis and Recognition, </booktitle> <pages> pages 701-709, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Many current methods are based on connected component analysis, aspect ratio estimation, or profile analysis [11, 35, 79, 93]. There are also some methods that integrate character segmentation with character recognition under the belief that segmentation decisions are tentative until confirmed by the successful recognition of segmented image pieces <ref> [18, 154] </ref>. 2.1.1.2 Commercial OCR Systems Since 1992, the Information Science Research Institute (ISRI) of University of Nevada has conducted an annual test of OCR systems [125, 126, 124]. <p> As a general framework to resolve ambiguity at several levels, a lattice has been identified as a useful structure to keep records of possible segmentation points and character alternatives according to their geometric positions. Various lattice-based methods have been applied <ref> [6, 135, 150, 154] </ref>. In previous studies, each entry in a lattice is only a representation at the character level. Sponsored by ARPA (Advanced Research Projects Agency), RAF Technology, Inc. is developing ILLUMINATOR, a new framework for document decomposition, recognition, storage and interchange [119].
Reference: [155] <author> S. Tsujimoto and H. Asada. </author> <title> Major components of a complete text reading system. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1133-1149, </pages> <year> 1992. </year>
Reference-contexts: Chapter 2 Background Visual text recognition is an application area of image processing, pattern recognition and natural language processing. In this chapter, research in several related areas is surveyed. 2.1 Text Recognition To automatically recognize the text in a document image, the first step is page layout analysis <ref> [6, 19, 109, 155] </ref>. The process of layout analysis detects and extracts text blocks and text lines. Each text line can be further segmented into a sequence of word images. Because of the space between words in machine-printed English text, it is not difficult to find word boundaries.
Reference: [156] <author> R.K. Srihari V. Govindaraju and S.N. Srihari. </author> <title> Handwritten text recognition. </title> <booktitle> In IAPR Workshop On Document Analysis Systems, </booktitle> <pages> pages 157-174, </pages> <month> Oct. 18-20 </month> <year> 1994. </year>
Reference-contexts: Future Directions 208 12.5 Handwritten Text Recognition Handwritten text recognition is a similar, but more difficult task. Techniques designed here for machine-printed text recognition can be adapted to word candidate selection for this task. Significant research has been conducted on using linguistic contextual information to improve recognition performance <ref> [37, 156, 141, 159] </ref>. Because word segmentation sometimes is not an easy task, many possible word boundaries can be generated and the structure of a word lattice may become more complex: positions of word candidates may overlap (see the example inFigure 12.5).
Reference: [157] <author> T. Wasow. </author> <title> Grammatical theory. </title> <editor> In M. Posner, editor, </editor> <booktitle> Foundation of Cognitive Science, </booktitle> <pages> pages 161-205. </pages> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: The structural descriptions are necessary for further processing, for example, for semantic interpretation. In the last three decades, many frameworks have been developed to model the syntactic phenomena of English <ref> [12, 116, 136, 157] </ref>. Among them are context-free grammars (CFG), transformational grammars (TG), lexical functional grammars (LFG), generalized phrase structure grammars (GPSG) and government-binding (GB) theory. In an NLP system, a grammar is usually represented as a rule base.
Reference: [158] <author> C. J. Wells, L. J. Evett, P. E. Whitby, and R. J. Whitrow. </author> <title> Fast dictionary look-up for contextual word recognition. </title> <journal> Pattern Recognition, </journal> <volume> 23(5) </volume> <pages> 501-508, </pages> <year> 1990. </year> <note> BIBLIOGRAPHY 229 </note>
Reference-contexts: Contextual constraints which have been utilized successfully in practical systems are limited to the word-level [35, 54, 90]. The lexical constraint is the one that is used most BACKGROUND 15 widely. Methods of dictionary look-up <ref> [158] </ref>, confusion-matrix-based transformation and string editing [36, 79], probabilistic relaxation [44], and character n-gram frequency analysis [74, 142] have been developed to exploit the lexical constraint efficiently. Such methods work well on non-word error detection.
Reference: [159] <author> C.J. Wells, L.J. Evett, and R.J. Whitrow. </author> <title> Word Look-Up for Script Recognition - choosing a candidate. </title> <booktitle> In Proceedings of the First International Conference on Document Analysis (ICDAR-91), </booktitle> <pages> pages 620-628, </pages> <year> 1991. </year>
Reference-contexts: Future Directions 208 12.5 Handwritten Text Recognition Handwritten text recognition is a similar, but more difficult task. Techniques designed here for machine-printed text recognition can be adapted to word candidate selection for this task. Significant research has been conducted on using linguistic contextual information to improve recognition performance <ref> [37, 156, 141, 159] </ref>. Because word segmentation sometimes is not an easy task, many possible word boundaries can be generated and the structure of a word lattice may become more complex: positions of word candidates may overlap (see the example inFigure 12.5).
Reference: [160] <author> G.M. White. </author> <title> Natural language understanding and speech recognition. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 72-82, </pages> <year> 1990. </year>
Reference-contexts: statistical methods can achieve promising performance in the tasks of lexical acquisition [75], tagging [25], parsing [42, 99, 100] and sense disambiguation [140] and that these informa BACKGROUND 26 tion sources can be used in systems for natural language processing [161], machine translation [14], information retrieval [88], and speech recognition <ref> [160] </ref>. The statistical information is usually collected by training on a large text corpus. This approach is also called corpus-based natural language processing. The Brown Corpus is an English text corpus with about one million words [89]. It has both a raw and a tagged version. <p> The acoustic ambiguity may be caused by uncertainty of word boundaries, phonetic ambiguity, syllable omissions and other missing information. Researchers believe that acoustic ambiguity can only be resolved through the use of higher sources of knowledge and NLP techniques. There have been efforts to integrate speech recognition with NLP <ref> [48, 76, 137, 160, 165] </ref>. NLP can play two roles. The first role is to interpret the meaning of an utterance. The second, more subtle role, is to reduce acoustic ambiguity by "understanding" the utter BACKGROUND 33 ance.
Reference: [161] <author> Y. Wilks, L. Guthrie, J. Guthrie, and J. Cowie. </author> <title> Combining weak methods in large-scale text processing. </title> <editor> In Paul S. Jacobs, editor, </editor> <booktitle> Text-Based intelligent Systems. </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1991. </year>
Reference-contexts: Research from artificial intelligence and computational linguistics has shown that statistical methods can achieve promising performance in the tasks of lexical acquisition [75], tagging [25], parsing [42, 99, 100] and sense disambiguation [140] and that these informa BACKGROUND 26 tion sources can be used in systems for natural language processing <ref> [161] </ref>, machine translation [14], information retrieval [88], and speech recognition [160]. The statistical information is usually collected by training on a large text corpus. This approach is also called corpus-based natural language processing. The Brown Corpus is an English text corpus with about one million words [89].
Reference: [162] <author> T. Winograd. </author> <title> Language as a Cognitive Process,Volume I: Syntax. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1983. </year>
Reference-contexts: Different theories have been proposed to model syntactic, semantic, pragmatic, and discourse aspects of language <ref> [3, 162] </ref>. Natural language processing systems usually contain several essential parts, such as a lexicon, a grammar, and a parser. A lexicon contains knowledge about words in a language. Words are the units on which more complex language structures, such as phrases, sentences and discourses, are based.
Reference: [163] <author> W.A. Woods. </author> <title> Tansition Network Grammars for Natural Language Analysis. </title> <editor> In Bar-bara J. Grosz, Karen Sparck Jones, and Bonnie Lynn Webber, editors, </editor> <booktitle> Reading in Natural Language Processing, </booktitle> <pages> pages 71-88. </pages> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <year> 1986. </year>
Reference-contexts: Because syntactic phenomena are complex, a grammar with comprehensive coverage can be difficult to obtain. Many parsing methods have been developed in the last thirty years, such as ATN parsers <ref> [163] </ref>, generalized LR parsers [13, 151], deterministic parsers [101], unification algorithms [83] and chart parsers [82, 166]. Among these techniques, chart parsing is quite popular. The chart in a chart parser is a data structure that stores the information about syntactic structures (or subtrees) already derived from an input sentence.
Reference: [164] <author> L. Xu, A. Krzyzak, and C. Y. Suen. </author> <title> Method of combining multiple classifiers and their application to handwritten character recognition. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-22:418-435, </volume> <year> 1992. </year>
Reference-contexts: By taking advantage of local typeface homogeneity, OCR accuracy can be improved [9]. The combination of several classifiers has been shown as a way to make recognition more accurate <ref> [50, 164] </ref>. Besides problems of isolated character classification, improper character segmentation has been identified as one of the major sources of incorrect recognition [18]. As an important preprocessing step, character segmentation partitions word images into sequences of character images so that OCR techniques can be applied [17].
Reference: [165] <author> S.R. Young, A.G. Hauptmann, W.H. Ward, E.T. Smith, and P. Werner. </author> <title> High level knowledge sources in usable speech recognition systems. </title> <journal> Communications of the ACM, </journal> <volume> 32(2) </volume> <pages> 183-194, </pages> <year> 1989. </year>
Reference-contexts: The acoustic ambiguity may be caused by uncertainty of word boundaries, phonetic ambiguity, syllable omissions and other missing information. Researchers believe that acoustic ambiguity can only be resolved through the use of higher sources of knowledge and NLP techniques. There have been efforts to integrate speech recognition with NLP <ref> [48, 76, 137, 160, 165] </ref>. NLP can play two roles. The first role is to interpret the meaning of an utterance. The second, more subtle role, is to reduce acoustic ambiguity by "understanding" the utter BACKGROUND 33 ance.
Reference: [166] <author> D.H. Younger. </author> <title> Recognition and Parsing of Context-Free Language in Time n 3 . Information and Control, </title> <booktitle> 10 </booktitle> <pages> 189-208, </pages> <year> 1967. </year>
Reference-contexts: Because syntactic phenomena are complex, a grammar with comprehensive coverage can be difficult to obtain. Many parsing methods have been developed in the last thirty years, such as ATN parsers [163], generalized LR parsers [13, 151], deterministic parsers [101], unification algorithms [83] and chart parsers <ref> [82, 166] </ref>. Among these techniques, chart parsing is quite popular. The chart in a chart parser is a data structure that stores the information about syntactic structures (or subtrees) already derived from an input sentence. With such a data structure, the same structure will be derived only once. <p> Local word collocation data can be collected by training on large text corpora [26]. The global structural constraints are exploited with a chart parsing model. The chart data structure provides a flexible framework for parsing <ref> [82, 166] </ref>. Statistical methods can be easily incorporated in a chart parser [100]. A chart parser can be extended to a lattice parser which allows for several word candidates at the same position, and therefore can be directly used for speech recognition [23, 152] and visual text recognition.
Reference: [167] <author> U. Zernik. </author> <title> Introduction. In Uri Zernik, editor, Lexical Acquisition: Exploitung OnLine Resources to Build a Lexicon, </title> <address> pages 1-26. </address> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <year> 1991. </year> <note> BIBLIOGRAPHY 230 </note>
Reference-contexts: Section 8 contains conclusions and future directions. 8.3 About the English lexicon It is time-consuming to manually build a large-scale English lexicon that can support an unrestricted text understanding system. Automatic acquisition of lexical knowledge is a promising approach towards solving this problem <ref> [167] </ref>. Following this approach, we built an English lexicon by training on English text corpora that contain more than three million LATTICE PARSING 134 words. The corpora used are the entire Brown Corpus and part of the Penn Treebank.
Reference: [168] <author> X. Z. Zhang. </author> <title> Techniques for Chinese Character Recognition. </title> <institution> Qinghua University, </institution> <year> 1992. </year>
Reference-contexts: Although there are more than ten thousand Chinese characters in use, lexicographical researchers estimate that there are only about 800 character elements and no more than 100 such elements are frequently used <ref> [168] </ref>. Many Chinese characters share a similar character element at the lexicographical level. If they are printed in the same font, their images will be partially similar. Figure 10.1 shows three sets of characters which have the same left, right or bottom part respectively. are eight character images. <p> Generally, similarities at the lexicographical level and the visual level are highly consistent. As we observed, if character categories share the same lexicographical element in a region, their image instances are usually similar visually in that region. There are previous studies on Chinese character recognition using radical-based partial matching <ref> [1, 168] </ref>. Although radicals are well-defined lexicographically, it is time-consuming to manually locate all possible character radicals and describe the structure of each Chinese character according to the radical set. The goal of our work is to conduct a visual similarity analysis among Chinese characters at the image level.
Reference: [169] <author> V. Zue, J. Glass, D. Goodine, H. Leung, M. McCandless, M. Phillips, J. Polifroni, and S. Seneff. </author> <title> Recent progress on the voyager system. </title> <booktitle> In Speech and Natural Language Workshop '90, </booktitle> <pages> pages 206-211, </pages> <year> 1990. </year>
Reference-contexts: Speech recognition at the phoneme and word level has used Hidden-Markov-Models (HMM) [115] and other statistical methods successfully. Practical systems have been built for speaker-independent, continuous, large vocabulary (1,000 words or more) speech recognition in a specified domain, such as an airline travel information service (ATIS) <ref> [169] </ref>. But speech recognition at the word level is inadequate. Error rates are fairly high even for the best systems such as SPHINX [92]. The SPHINX system has an error rate of 29.4 percent for speaker independent, continuous speech recognition.
References-found: 169

