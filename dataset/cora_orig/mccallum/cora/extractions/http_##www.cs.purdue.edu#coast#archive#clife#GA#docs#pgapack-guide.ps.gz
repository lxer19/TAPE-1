URL: http://www.cs.purdue.edu/coast/archive/clife/GA/docs/pgapack-guide.ps.gz
Refering-URL: http://www.cs.purdue.edu/coast/archive/clife/GA/docs/
Root-URL: http://www.cs.purdue.edu
Title: Distribution Category:  Users Guide to the PGAPack Parallel Genetic Algorithm Library  
Author: Mathematics and by David Levine 
Note: This work was supported in part by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Date: January 31, 1996  
Web: ANL-95/18  
Address: 9700 South Cass Avenue Argonne, IL 60439  
Affiliation: Computer Science (UC-405) ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> MPICH World Wide Web home page. </institution> <note> Available by anonymous ftp from ftp.mcs.anl.gov in directory pub/mpi, file mpich.tar.Z, or on the World Wide Web at http://www.mcs.anl.gov/home/lusk/mpich/index.html, June 1995. </note>
Reference-contexts: If you do not have a native version of MPI for your computer, several machine-independent implementations are available. Most of the testing and development of PGAPack was done by using the MPICH implementation of MPI which is freely available <ref> [1] </ref>. 2.3 Structure of the Distribution Directory The PGAPack distribution contains the following files and subdirectories: * CHANGES: Changes new to this release of PGAPack. * COPYRIGHT: The usage terms. * README: General instructions, including how to build and install PGAPack. * configure.in: The "source code" for the configure script. * <p> This example uses the mpirun script that is distributed with the MPICH implementation <ref> [1] </ref>. Other MPI implementations may have other ways to specify the number of processes to use. 6 2.6 Mailing Lists, Web Page, and Bug Reporting To join the PGAPack mailing list to receive announcements of new versions, enhancements, and bug fixes, send electronic mail to pgapack@mcs.anl.gov. <p> This may be done using the function PGASetRandomSeed to initialize the random number generator with the same seed each time, for example, PGASetRandomSeed (ctx,1). PGARandom01 (ctx,0) will return a random number generated uniformly on <ref> [0; 1] </ref>. If the second argument is not 0, it will be used to reseed the random number sequence. PGARandomFlip flips a biased coin. For example, PGARandomFlip (ctx,.7) will return PGA TRUE approximately 70% of the time. PGARandomInterval (-10,30) will return an integer value generated uniformly on [10; 30]. <p> The example shows the use of a custom mutation function with an integer data type. The PGASetUserFunction function specifies that this function, MyMutation, will be called when the mutation operator is applied, rather than the default mutation operator. MyMutation generates a random integer on the interval <ref> [1; L] </ref>. 7.3 Example Problem: Fortran 34 #include &lt;pgapack.h&gt; double evaluate (PGAContext *ctx, int p, int pop); int myMutation (PGAContext *ctx, int p, int pop, double pm); int main ( int argc, char **argv ) - PGAContext *ctx; int i, maxiter; ctx = PGACreate (&argc, argv, PGA_DATATYPE_INTEGER, 10, PGA_MAXIMIZE); PGASetUserFunction (ctx, <p> See pgapack.h for details one the * fields (under PGAIndividual) */ MPI_Address (&P-&gt;evalfunc, &displs [0]); counts [0] = 2; types [0] = MPI_DOUBLE; /* Next, we have an integer, evaluptodate. */ MPI_Address (&P-&gt;evaluptodate, &displs <ref> [1] </ref>); counts [1] = 1; types [1] = MPI_INT; /* Finally, we have the actual user-defined string. */ MPI_Address (S-&gt;t, &displs [2]); counts [2] = 6; types [2] = MPI_DOUBLE; MPI_Address (S-&gt;sc, &displs [3]); counts [3] = 40; types [3] = MPI_INT; MPI_Type_struct (4, counts, displs, types, &DT_PGAIndividual); MPI_Type_commit (&DT_PGAIndividual); return <p> See pgapack.h for details one the * fields (under PGAIndividual) */ MPI_Address (&P-&gt;evalfunc, &displs [0]); counts [0] = 2; types [0] = MPI_DOUBLE; /* Next, we have an integer, evaluptodate. */ MPI_Address (&P-&gt;evaluptodate, &displs <ref> [1] </ref>); counts [1] = 1; types [1] = MPI_INT; /* Finally, we have the actual user-defined string. */ MPI_Address (S-&gt;t, &displs [2]); counts [2] = 6; types [2] = MPI_DOUBLE; MPI_Address (S-&gt;sc, &displs [3]); counts [3] = 40; types [3] = MPI_INT; MPI_Type_struct (4, counts, displs, types, &DT_PGAIndividual); MPI_Type_commit (&DT_PGAIndividual); return (DT_PGAIndividual); - <p> See pgapack.h for details one the * fields (under PGAIndividual) */ MPI_Address (&P-&gt;evalfunc, &displs [0]); counts [0] = 2; types [0] = MPI_DOUBLE; /* Next, we have an integer, evaluptodate. */ MPI_Address (&P-&gt;evaluptodate, &displs <ref> [1] </ref>); counts [1] = 1; types [1] = MPI_INT; /* Finally, we have the actual user-defined string. */ MPI_Address (S-&gt;t, &displs [2]); counts [2] = 6; types [2] = MPI_DOUBLE; MPI_Address (S-&gt;sc, &displs [3]); counts [3] = 40; types [3] = MPI_INT; MPI_Type_struct (4, counts, displs, types, &DT_PGAIndividual); MPI_Type_commit (&DT_PGAIndividual); return (DT_PGAIndividual); - 43 Chapter 9 Hill-Climbing <p> For example, to set the crossover probability to 0.6, use call PGASetCrossoverProb (ctx, 0.6d0), or double precision pc pc = 0.6 call PGASetCrossoverProb (ctx, pc) * Gene indices are [0; L 1] in C, and <ref> [1; L] </ref> in Fortran, where L is the string length. * Population member indices are [0; N 1] in C, and [1; N ] in Fortran, where N is the population size. * Fortran does not support command line arguments (Section 5.13). * Fortran allows custom usage with native data types <p> probability to 0.6, use call PGASetCrossoverProb (ctx, 0.6d0), or double precision pc pc = 0.6 call PGASetCrossoverProb (ctx, pc) * Gene indices are [0; L 1] in C, and [1; L] in Fortran, where L is the string length. * Population member indices are [0; N 1] in C, and <ref> [1; N ] </ref> in Fortran, where N is the population size. * Fortran does not support command line arguments (Section 5.13). * Fortran allows custom usage with native data types (Chapter 7), but not with new data types (Chap ter 8). * In the MPICH implementation of MPI, the Fortran and <p> PGASetPrintOptions Print online statistics PGA FALSE PGASetPrintOptions Print best string PGA FALSE PGASetPrintOptions Print worst string PGA FALSE PGASetPrintOptions Print Hamming distance PGA FALSE PGASetPrintOptions Randomly initialize population PGA TRUE PGASetRandomInitFlag Probability of initializing a bit to one 0.5 PGASetBinaryInitProb How to initialize real strings Range PGASetrealInitRange Real initialization range <ref> [0; 1] </ref> PGASetRealInitRange How to initialize integer strings Permutation PGASetIntegerInitPermute Integer initialization range [0; L 1] PGASetIntegerInitPermute How to initialize character strings PGA CINIT LOWER PGASetCharacterInitFlag Seed random number with clock PGA TRUE PGASetRandomSeed Default MPI communicator MPI COMM WORLD PGASetCommunicator L is the string length 56 Appendix B Function Bindings
Reference: [2] <author> J. Baker. </author> <title> Reducing bias and inefficiency in the selection algorithm. </title> <editor> In J. Grefenstette, editor, </editor> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms and Their Applications, </booktitle> <pages> pages 14-21, </pages> <address> Hillsdale, New Jersey, 1987. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: The second argument must be one of PGA FITNESS RAW, PGA FITNESS RANKING, or PGA FITNESS NORMAL, for the identity, linear ranking, or linear normalization, respectively. A linear rank fitness function <ref> [2, 10] </ref> is given by M in + (M ax M in) rank (p) 1 N 1 where rank (p) is the index of string p in a list sorted in order of decreasing evaluation function value, and N is the population size. <p> the * fields (under PGAIndividual) */ MPI_Address (&P-&gt;evalfunc, &displs [0]); counts [0] = 2; types [0] = MPI_DOUBLE; /* Next, we have an integer, evaluptodate. */ MPI_Address (&P-&gt;evaluptodate, &displs [1]); counts [1] = 1; types [1] = MPI_INT; /* Finally, we have the actual user-defined string. */ MPI_Address (S-&gt;t, &displs <ref> [2] </ref>); counts [2] = 6; types [2] = MPI_DOUBLE; MPI_Address (S-&gt;sc, &displs [3]); counts [3] = 40; types [3] = MPI_INT; MPI_Type_struct (4, counts, displs, types, &DT_PGAIndividual); MPI_Type_commit (&DT_PGAIndividual); return (DT_PGAIndividual); - 43 Chapter 9 Hill-Climbing and Hybridization Hill-climbing heuristics attempt to improve a solution by moving to a better neighbor <p> fields (under PGAIndividual) */ MPI_Address (&P-&gt;evalfunc, &displs [0]); counts [0] = 2; types [0] = MPI_DOUBLE; /* Next, we have an integer, evaluptodate. */ MPI_Address (&P-&gt;evaluptodate, &displs [1]); counts [1] = 1; types [1] = MPI_INT; /* Finally, we have the actual user-defined string. */ MPI_Address (S-&gt;t, &displs <ref> [2] </ref>); counts [2] = 6; types [2] = MPI_DOUBLE; MPI_Address (S-&gt;sc, &displs [3]); counts [3] = 40; types [3] = MPI_INT; MPI_Type_struct (4, counts, displs, types, &DT_PGAIndividual); MPI_Type_commit (&DT_PGAIndividual); return (DT_PGAIndividual); - 43 Chapter 9 Hill-Climbing and Hybridization Hill-climbing heuristics attempt to improve a solution by moving to a better neighbor solution. <p> MPI_Address (&P-&gt;evalfunc, &displs [0]); counts [0] = 2; types [0] = MPI_DOUBLE; /* Next, we have an integer, evaluptodate. */ MPI_Address (&P-&gt;evaluptodate, &displs [1]); counts [1] = 1; types [1] = MPI_INT; /* Finally, we have the actual user-defined string. */ MPI_Address (S-&gt;t, &displs <ref> [2] </ref>); counts [2] = 6; types [2] = MPI_DOUBLE; MPI_Address (S-&gt;sc, &displs [3]); counts [3] = 40; types [3] = MPI_INT; MPI_Type_struct (4, counts, displs, types, &DT_PGAIndividual); MPI_Type_commit (&DT_PGAIndividual); return (DT_PGAIndividual); - 43 Chapter 9 Hill-Climbing and Hybridization Hill-climbing heuristics attempt to improve a solution by moving to a better neighbor solution.
Reference: [3] <author> L. Davis. </author> <title> Handbook of Genetic Algorithms. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Traditionally, strings created through recombination first undergo crossover and then mutation. Some practitioners <ref> [3] </ref> have argued that these two operators should be separate. By default, PGAPack applies mutation only to strings that did not undergo crossover. This is equivalent to setting PGASetMutationOrCrossoverFlag (ctx,PGA TRUE). To have strings undergo both crossover and mutation, on should use PGASetMutationAndCrossoverFlag (ctx,PGA TRUE). <p> = 2; types [0] = MPI_DOUBLE; /* Next, we have an integer, evaluptodate. */ MPI_Address (&P-&gt;evaluptodate, &displs [1]); counts [1] = 1; types [1] = MPI_INT; /* Finally, we have the actual user-defined string. */ MPI_Address (S-&gt;t, &displs [2]); counts [2] = 6; types [2] = MPI_DOUBLE; MPI_Address (S-&gt;sc, &displs <ref> [3] </ref>); counts [3] = 40; types [3] = MPI_INT; MPI_Type_struct (4, counts, displs, types, &DT_PGAIndividual); MPI_Type_commit (&DT_PGAIndividual); return (DT_PGAIndividual); - 43 Chapter 9 Hill-Climbing and Hybridization Hill-climbing heuristics attempt to improve a solution by moving to a better neighbor solution. <p> types [0] = MPI_DOUBLE; /* Next, we have an integer, evaluptodate. */ MPI_Address (&P-&gt;evaluptodate, &displs [1]); counts [1] = 1; types [1] = MPI_INT; /* Finally, we have the actual user-defined string. */ MPI_Address (S-&gt;t, &displs [2]); counts [2] = 6; types [2] = MPI_DOUBLE; MPI_Address (S-&gt;sc, &displs <ref> [3] </ref>); counts [3] = 40; types [3] = MPI_INT; MPI_Type_struct (4, counts, displs, types, &DT_PGAIndividual); MPI_Type_commit (&DT_PGAIndividual); return (DT_PGAIndividual); - 43 Chapter 9 Hill-Climbing and Hybridization Hill-climbing heuristics attempt to improve a solution by moving to a better neighbor solution. <p> /* Next, we have an integer, evaluptodate. */ MPI_Address (&P-&gt;evaluptodate, &displs [1]); counts [1] = 1; types [1] = MPI_INT; /* Finally, we have the actual user-defined string. */ MPI_Address (S-&gt;t, &displs [2]); counts [2] = 6; types [2] = MPI_DOUBLE; MPI_Address (S-&gt;sc, &displs <ref> [3] </ref>); counts [3] = 40; types [3] = MPI_INT; MPI_Type_struct (4, counts, displs, types, &DT_PGAIndividual); MPI_Type_commit (&DT_PGAIndividual); return (DT_PGAIndividual); - 43 Chapter 9 Hill-Climbing and Hybridization Hill-climbing heuristics attempt to improve a solution by moving to a better neighbor solution. Whenever the neighboring solution is better than the current solution, it replaces the current solution.
Reference: [4] <author> M. Flynn. </author> <title> Some computer organizations and their effectiveness. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 21 </volume> <pages> 948-960, </pages> <year> 1972. </year>
Reference-contexts: PGAUpdateGeneration (PGAContext *ctx, MPI Comm comm) void PGAUsage (PGAContext *ctx) Fortran 77 Bindings Use the rules defined in Chapter 11 (and the machine-specific idiosyncrasies noted in Appendix D) to determine the Fortran bindings. 63 Appendix C Parallelism Background Parallel Computer Taxonomy Traditionally, parallel computers are classified according to Flynn's taxonomy <ref> [4] </ref>. Flynn's classification distinguishes parallel computers according to the number of instruction streams and data operands being computed on simultaneously. Flynn's single-instruction single-data (SISD) model is the traditional sequential computer. A single program counter fetches instructions from memory. The instructions are executed on scalar operands.
Reference: [5] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 8(3/4), </volume> <year> 1994. </year>
Reference-contexts: If you wish only to build a sequential version of PGAPack this is all that is required. To build a parallel version, you must have an implementation of the Message Passing Interface (MPI) <ref> [5, 6] </ref> for the parallel computer or workstation network you are running on. If you do not have a native version of MPI for your computer, several machine-independent implementations are available. <p> If the user called MPI Init, the user must also call MPI Finalize before exiting. We elaborate here on the MPI Bcast function because of its practical value in the model of parallel I/O shown. For more detailed discussion of MPI concepts and functions, the user should consult <ref> [5, 6] </ref>. <p> A set of interface functions allows most user-level PGAPack functions to be called from Fortran. All message-passing calls follow the Message Passing Interface (MPI) standard <ref> [5, 6] </ref>. Nonoperative versions of the basic MPI functions used in the examples are supplied if the user does not provide an MPI implementation for their machine. These routines simply return and provide no parallel functionality. <p> x [6]; int sc [40]; PGAIndividual *ind; ligand *lig; lig = (ligand *)PGAGetIndividual (ctx, p, pop)-&gt;chrom; for (i = 0; i &lt; 6; i++) for (i = 0; i &lt; 40; i++) return ( energy (x,sc) ); - 42 MPI_Datatype BuildDT (PGAContext *ctx, int p, int pop) - int counts <ref> [5] </ref>; MPI_Aint displs [5]; MPI_Datatype types [5]; MPI_Datatype DT_PGAIndividual; PGAIndividual *P; ligand *S; P = PGAGetIndividual (ctx, p, pop); S = (ligand *)P-&gt;chrom; /* Build the MPI datatype. <p> sc [40]; PGAIndividual *ind; ligand *lig; lig = (ligand *)PGAGetIndividual (ctx, p, pop)-&gt;chrom; for (i = 0; i &lt; 6; i++) for (i = 0; i &lt; 40; i++) return ( energy (x,sc) ); - 42 MPI_Datatype BuildDT (PGAContext *ctx, int p, int pop) - int counts <ref> [5] </ref>; MPI_Aint displs [5]; MPI_Datatype types [5]; MPI_Datatype DT_PGAIndividual; PGAIndividual *P; ligand *S; P = PGAGetIndividual (ctx, p, pop); S = (ligand *)P-&gt;chrom; /* Build the MPI datatype. <p> *ind; ligand *lig; lig = (ligand *)PGAGetIndividual (ctx, p, pop)-&gt;chrom; for (i = 0; i &lt; 6; i++) for (i = 0; i &lt; 40; i++) return ( energy (x,sc) ); - 42 MPI_Datatype BuildDT (PGAContext *ctx, int p, int pop) - int counts <ref> [5] </ref>; MPI_Aint displs [5]; MPI_Datatype types [5]; MPI_Datatype DT_PGAIndividual; PGAIndividual *P; ligand *S; P = PGAGetIndividual (ctx, p, pop); S = (ligand *)P-&gt;chrom; /* Build the MPI datatype. Every user defined function needs these. * The first two calls are stuff that is internal to PGAPack, but * the user still must include it. <p> This communicator contains all processes that were created when MPI was initialized. For most users this is all they have to know about communicators. Simply supply MPI COMM WORLD whenever a communicator is required as an argument. For more sophisticated use, users are referred to <ref> [5, 6] </ref>. Parallel I/O The issue of parallel I/O is independent of PGAPack. However, since we assume many PGAPack users will wish to do I/O we address this topic. A primary consideration has to do with whether one or all processors do I/O.
Reference: [6] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> USING MPI Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, </address> <year> 1994. </year>
Reference-contexts: If you wish only to build a sequential version of PGAPack this is all that is required. To build a parallel version, you must have an implementation of the Message Passing Interface (MPI) <ref> [5, 6] </ref> for the parallel computer or workstation network you are running on. If you do not have a native version of MPI for your computer, several machine-independent implementations are available. <p> If the user called MPI Init, the user must also call MPI Finalize before exiting. We elaborate here on the MPI Bcast function because of its practical value in the model of parallel I/O shown. For more detailed discussion of MPI concepts and functions, the user should consult <ref> [5, 6] </ref>. <p> A set of interface functions allows most user-level PGAPack functions to be called from Fortran. All message-passing calls follow the Message Passing Interface (MPI) standard <ref> [5, 6] </ref>. Nonoperative versions of the basic MPI functions used in the examples are supplied if the user does not provide an MPI implementation for their machine. These routines simply return and provide no parallel functionality. <p> (PGAContext *, int, int, double); void Crossover (PGAContext *, int, int, int, int, int, int); void WriteString (PGAContext *, FILE *, int, int); void CopyString (PGAContext *, int, int, int, int); int DuplicateString (PGAContext *, int, int, int, int); MPI_Datatype BuildDT (PGAContext *, int, int); typedef struct - double t <ref> [6] </ref>; /* ligand translation and rotation */ int sc [40]; /* ligand sidechain rotations */ ligand; int main (int argc, char **argv) - PGAContext *ctx; int maxiter; ctx = PGACreate (&argc, argv, PGA_DATATYPE_USER, 46, PGA_MINIMIZE); PGASetRandomSeed (ctx, 1); PGASetMaxGAIterValue (ctx, 5000); PGASetUserFunction (ctx, PGA_USERFUNCTION_CREATESTRING, CreateString); PGASetUserFunction (ctx, PGA_USERFUNCTION_MUTATION, Mutation); PGASetUserFunction (ctx, <p> int DuplicateString (PGAContext *ctx, int p1, int pop1, int p2, int pop2) - void *a, *b; a = PGAGetIndividual (ctx, p1, pop1)-&gt;chrom; b = PGAGetIndividual (ctx, p2, pop2)-&gt;chrom; return (!memcmp (a, b, sizeof (ligand))); - 41 double Evaluate (PGAContext *ctx, int p, int pop) - int i, j; double x <ref> [6] </ref>; int sc [40]; PGAIndividual *ind; ligand *lig; lig = (ligand *)PGAGetIndividual (ctx, p, pop)-&gt;chrom; for (i = 0; i &lt; 6; i++) for (i = 0; i &lt; 40; i++) return ( energy (x,sc) ); - 42 MPI_Datatype BuildDT (PGAContext *ctx, int p, int pop) - int counts [5]; MPI_Aint <p> This communicator contains all processes that were created when MPI was initialized. For most users this is all they have to know about communicators. Simply supply MPI COMM WORLD whenever a communicator is required as an argument. For more sophisticated use, users are referred to <ref> [5, 6] </ref>. Parallel I/O The issue of parallel I/O is independent of PGAPack. However, since we assume many PGAPack users will wish to do I/O we address this topic. A primary consideration has to do with whether one or all processors do I/O.
Reference: [7] <author> J. Holland. </author> <booktitle> Adaption in Natural and Artificial Systems. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: The first, the generational replacement genetic algorithm (GRGA), replaces the entire population each generation and is the traditional genetic algorithm <ref> [7] </ref>. The second, the steady-state genetic algorithm (SSGA), typically replaces only a few strings each generation and is a more recent development [9, 10, 11]. PGAPack supports both GRGA and SSGA and variants in between via parameterized population replacement.
Reference: [8] <author> W. Spears and K. DeJong. </author> <title> On the virtues of parameterized uniform crossover. </title> <editor> In R. Belew and L. Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 230-236, </pages> <address> San Mateo, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The default is two-point crossover. By default the crossover rate is 0.85. It may be set to 0.6 by PGASetCrossoverProb (ctx, 0.6), for example. Uniform crossover is parameterized by p u , the probability of swapping two parent bit values <ref> [8] </ref>. By default, p u = 0:5. The function call PGASetUniformCrossoverProb (ctx, 0.7) will set p u = 0:7. 5.7 Mutation The mutation rate is the probability that a gene will undergo mutation. The mutation rate is independent of the datatype used.
Reference: [9] <author> G. Syswerda. </author> <title> Uniform crossover in genetic algorithms. </title> <editor> In J. Schaffer, editor, </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 2-9, </pages> <address> San Mateo, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The first, the generational replacement genetic algorithm (GRGA), replaces the entire population each generation and is the traditional genetic algorithm [7]. The second, the steady-state genetic algorithm (SSGA), typically replaces only a few strings each generation and is a more recent development <ref> [9, 10, 11] </ref>. PGAPack supports both GRGA and SSGA and variants in between via parameterized population replacement.
Reference: [10] <author> D. Whitley. </author> <title> The GENITOR algorithm and selection pressure: Why rank-based allocation of reproductive trials is best. </title> <editor> In J. Schaffer, editor, </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 116-121, </pages> <address> San Mateo, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The first, the generational replacement genetic algorithm (GRGA), replaces the entire population each generation and is the traditional genetic algorithm [7]. The second, the steady-state genetic algorithm (SSGA), typically replaces only a few strings each generation and is a more recent development <ref> [9, 10, 11] </ref>. PGAPack supports both GRGA and SSGA and variants in between via parameterized population replacement. <p> The second argument must be one of PGA FITNESS RAW, PGA FITNESS RANKING, or PGA FITNESS NORMAL, for the identity, linear ranking, or linear normalization, respectively. A linear rank fitness function <ref> [2, 10] </ref> is given by M in + (M ax M in) rank (p) 1 N 1 where rank (p) is the index of string p in a list sorted in order of decreasing evaluation function value, and N is the population size. <p> If the second argument is not 0, it will be used to reseed the random number sequence. PGARandomFlip flips a biased coin. For example, PGARandomFlip (ctx,.7) will return PGA TRUE approximately 70% of the time. PGARandomInterval (-10,30) will return an integer value generated uniformly on <ref> [10; 30] </ref>. PGARandomUniform (ctx,-50.,50.) will return a real value generated uniformly randomly on the interval [-50,50].
Reference: [11] <author> D. Whitley and J. Kauth. </author> <title> GENITOR: A different genetic algorithm. </title> <booktitle> In Rocky Mountain Conference on Artificial Intelligence, </booktitle> <pages> pages 118-130, </pages> <address> Denver, </address> <year> 1988. </year> <month> 73 </month>
Reference-contexts: The first, the generational replacement genetic algorithm (GRGA), replaces the entire population each generation and is the traditional genetic algorithm [7]. The second, the steady-state genetic algorithm (SSGA), typically replaces only a few strings each generation and is a more recent development <ref> [9, 10, 11] </ref>. PGAPack supports both GRGA and SSGA and variants in between via parameterized population replacement.
References-found: 11

