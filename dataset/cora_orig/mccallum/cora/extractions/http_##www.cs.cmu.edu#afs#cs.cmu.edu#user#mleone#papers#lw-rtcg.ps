URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mleone/papers/lw-rtcg.ps
Refering-URL: http://www.cs.washington.edu/research/projects/unisw/DynComp/www/Related/papers.html
Root-URL: 
Email: fmleone,petelg@cs.cmu.edu  
Title: Lightweight Run-Time Code Generation  
Author: Mark Leone Peter Lee 
Address: Pittsburgh, Pennsylvania 15213 USA  
Affiliation: Carnegie Mellon University  
Abstract: The cost of performing optimization at run time is of paramount importance, since it must be repaid by improved performance in order to obtain an overall speedup. This paper describes a lightweight approach to run-time code generation, called deferred compilation, in which compile-time specialization is employed to reduce the cost of optimizing and generating code at run time. Implementation strategies developed for a prototype compiler are discussed, and the results of preliminary experiments demonstrating significant overall speedup are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [App87] <author> Andrew W. Appel. </author> <title> Re-opening closures. </title> <type> Technical Report CS-TR-079-87, </type> <institution> Department of Computer Science, Princeton University, </institution> <year> 1987. </year>
Reference-contexts: It may be profitable to generate optimized code for f (x) if it will be applied many times. Run-time code generation can therefore be viewed as an alternative to the conventional implementation of closures; this idea has also been described by Appel <ref> [App87] </ref> and Feeley and Lapalme [FL92]. Stages of computation also arise from conventional iter-ation constructs such as loop nests.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ull-man. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: unknown: fun raise exp bases = map (power exp) bases and power exp base = if exp = 0 then 1 else base * power (exp - 1) base 1.2.1 Staging Transformations Various techniques such as staging transformation [JS86, Han91], program bifurcation [Mog89, DBV91], fold/unfold transformations [BD77], or loop-invariant removal <ref> [ASU86] </ref> might be employed at compile time to "hoist" computations that do not depend on base out of its scope.
Reference: [BD77] <author> R. M. Burstall and John Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: exponent that is statically unknown: fun raise exp bases = map (power exp) bases and power exp base = if exp = 0 then 1 else base * power (exp - 1) base 1.2.1 Staging Transformations Various techniques such as staging transformation [JS86, Han91], program bifurcation [Mog89, DBV91], fold/unfold transformations <ref> [BD77] </ref>, or loop-invariant removal [ASU86] might be employed at compile time to "hoist" computations that do not depend on base out of its scope.
Reference: [BD91] <author> Anders Bondorf and Olivier Danvy. </author> <title> Automatic auto-projection of recursive equations with global variables and abstract data types. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 16(2) </volume> <pages> 151-195, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: An aggressive heuristic currently guides inlining: a call to a function with both "early" and "late" formal parameters is marked for run-time inlining if it does not appear in a branch of a conditional controlled by a late-stage value <ref> [BD91] </ref>. Each function that might be inlined at run time is compiled into a specialized code generator that does not emit procedure linkage code, but instead emits optimized code directly into the code being generated for the calling context. <p> Although the inlining strategy implemented by Fabius preserves the termination behavior of programs <ref> [BD91] </ref>, it remains to be seen whether the increased time and space requirements of such aggressive run-time inlining are manageable in large applications. 3.4 Specialization In some contexts it is impractical to inline a function yet still desirable to specialize it based upon the results of early computations. <p> Specialization also permits run-time-optimized code to be reused rather than regenerated, which saves both space and time. Determining where specialization will be beneficial is a difficult problem. Fabius currently implements the aggressive heuristic employed in <ref> [BD91] </ref>: all functions with both "early" and "late" arguments that are not inlined are specialized. Such functions are compiled into specialized code generators, parameterized by the values of the early arguments, which generate optimized functions at run time. <p> These code generators are memoized, so that previously optimized code is reused whenever possible. Run-time memoization on structured data can be quite expensive [Mal93], so Fabius uses pointer equality. Although this form of specialization yields significant speedups for some examples, it does not always terminate <ref> [BD91] </ref>. Preliminary experiments also indicate that the strategy is too aggressive in practice, since some functions do not benefit significantly from specialization. 4 Results Preliminary experiments with Fabius are encouraging. As an example we return to the matrix multiplication algorithm that was introduced in Section 2.
Reference: [BHOS76] <author> Lennart Beckman, Anders Haraldson, Osten Oskars-son, and Erik Sandewall. </author> <title> A partial evaluator, and its use as a programming tool. </title> <journal> Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 319-357, </pages> <year> 1976. </year>
Reference-contexts: The interpretational overhead present in this compilation cannot be statically eliminated. Fabius can be viewed as a manually derived implementation of comix ([[comix]]; [[comix]]). This bears strong similarity to the notion of hand-writing cogen <ref> [BHOS76, EH80, 5 HL91] </ref>. How then does deferred compilation differ from par-tial evaluation? Perhaps the most fundamental difference, discussed in Section 2.2, is that deferred compilation is not driven by an externally imposed division of program inputs, but rather by the staging inherent a program.
Reference: [CH84] <author> Frederick Chow and John Hennessy. </author> <title> Register allocation by priority-based coloring. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 222-232. </pages> <booktitle> SIGPLAN Notices, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: For example in dotprod, computations involving the two vectors are textually adjacent, but because they belong to different stages the same register can be assigned to both vectors. Existing register allocation algorithms based on graph coloring <ref> [Cha82, CH84] </ref> can be adapted to deferred compilation by simply modifying the construction of the interference graph. An interference graph contains nodes representing the lifetime ranges of variables and edges indicating where these ranges intersect.
Reference: [Cha82] <author> Gregory J. Chaitin. </author> <title> Register allocation and spilling via graph coloring. </title> <journal> SIGPLAN Notices, </journal> <volume> 17(6) </volume> <pages> 98-105, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: For example in dotprod, computations involving the two vectors are textually adjacent, but because they belong to different stages the same register can be assigned to both vectors. Existing register allocation algorithms based on graph coloring <ref> [Cha82, CH84] </ref> can be adapted to deferred compilation by simply modifying the construction of the interference graph. An interference graph contains nodes representing the lifetime ranges of variables and edges indicating where these ranges intersect.
Reference: [CHK93] <author> Keith D. Cooper, Mary W. Hall, and Ken Kennedy. </author> <title> A methodology for procedure cloning. </title> <journal> Computer Languages, </journal> <volume> 19(2) </volume> <pages> 105-117, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Run-time code generation can reduce the dynamic frequency of loop-invariant operations with less overhead. In addition, it can optimize non-loop-invariant computations using information that is not available at compile time. 1.2.2 Static Specialization Alternatively, specialization [JGS93], driving [Tur86], or procedure cloning <ref> [CHK93] </ref> might be employed at compile time to transform power into the following function: 2 fun powgen exp = nth exp [fn base =&gt; 1, fn base =&gt; base, fn base =&gt; base * base, fn base =&gt; base * base * base, . . . &gt; &gt; &gt; = k
Reference: [CKP93] <author> Andrew A. Chien, Vijay Karamcheti, and John Plevyak. </author> <title> The Concert system | compiler and runtime support for efficient, fine-grained concurrent object-oriented programs. </title> <type> Technical Report R-93-1815, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Run-time compilation is employed in SELF [HU94, DC94, CU91], a compiler for a classless object-oriented language, using a general-purpose intermediate representation. Run-time optimizations are obtained automatically by simply deferring the bulk of compilation to run time. Similar approaches have been implemented for Smalltalk [DS84] and concurrent object-oriented languages <ref> [CKP93] </ref>. In SELF the primary run-time optimizations are inlining and a form of specialization in which methods are customized to reduce the cost of dynamic type dispatch.
Reference: [Con93] <author> Charles Consel. </author> <title> Polyvariant binding-time analysis for applicative languages. </title> <booktitle> In Proceedings of the Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 66-77. </pages> <institution> Association for Computing Machinery, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Fabius generates native code for the MIPS R2000. The three major phases of compilation are as follows: * Staging analysis identifies computation stages at which it may be profitable to perform run-time code generation. In a process similar to binding-time analysis <ref> [JSS89, Con93] </ref>, subexpressions of the program are annotated to indicate whether they belong to early or late stages of computation. * Register allocation assigns registers to program variables and intermediate values. <p> The staging analysis also labels the recursive application as an early computation, indicating that the function should be inlined at run time (see Section 3.3). In the case of just two stages, this labeling of early and late computations is very similar to a binding-time analysis and annotation <ref> [JSS89, Con93] </ref>. There is a subtle difference between the two, however.
Reference: [CPW93] <author> Charles Consel, Calton Pu, and Jonathan Walpole. </author> <title> Incremental partial evaluation: The key to high performance, modularity and portability in operating systems. </title> <booktitle> In Proceedings of the Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 44-46. </pages> <institution> Association for Computing Machinery, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Detecting program stages is a difficult problem. Syntactic features of programming languages often provide clear indications of stages that can be subjected to deferred compilation. Applications of curried functions are an obvious candidate for deferred compilation, as are nested loop constructs. Consel, Pu, and Walpole <ref> [CPW93] </ref> recently proposed the use of a multi-level programming language to allow programmers to express invariants that will become established during the execution of a large system.
Reference: [CU91] <author> Craig Chambers and David Ungar. </author> <title> Making pure object-oriented languages practical. </title> <booktitle> In OOPSLA '91 Conference Proceedings. SIGPLAN Notices 26(11) </booktitle> <pages> 1-15, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Engler and Proebsting [EP93] have investigated using the lcc compiler's intermediate representation for run-time code generation. Intermediate code constructed using ad hoc methods can be compiled by the lcc back end at run time and then directly executed. Run-time compilation is employed in SELF <ref> [HU94, DC94, CU91] </ref>, a compiler for a classless object-oriented language, using a general-purpose intermediate representation. Run-time optimizations are obtained automatically by simply deferring the bulk of compilation to run time. Similar approaches have been implemented for Smalltalk [DS84] and concurrent object-oriented languages [CKP93]. <p> The cost of run-time compilation is reduced by delaying the compilation of infrequently executed methods 3 and applying aggressive optimizations only to frequently executed methods using dynamic recompilation. Nevertheless, compilation is time consuming: the SELF91 run-time compiler is about as fast as an optimizing C compiler <ref> [CU91] </ref>. 2 Deferred Compilation Deferred compilation employs compile-time specialization to reduce the cost of run-time code generation. No intermediate representation of a program is processed at run time; instead, portions of a program are compiled into code that is "hard-wired" to perform optimizations and generate native code at run time. <p> A key aspect of deferred compilation is that the input x is not known until run time, so 3 Chambers and Ungar <ref> [CU91] </ref> originally coined the term "deferred compilation" to describe this strategy. some amount of specialization must be carried out at run time.
Reference: [DBV91] <author> Anne De Niel, Eddy Bevers, and Karel De Vlam-inck. </author> <title> Program bifurcation for a polymorphically typed functional language. </title> <booktitle> In Proceedings of the Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 142-153. </pages> <booktitle> SIGPLAN Notices, </booktitle> <month> September </month> <year> 1991. </year>
Reference-contexts: function with a fixed exponent that is statically unknown: fun raise exp bases = map (power exp) bases and power exp base = if exp = 0 then 1 else base * power (exp - 1) base 1.2.1 Staging Transformations Various techniques such as staging transformation [JS86, Han91], program bifurcation <ref> [Mog89, DBV91] </ref>, fold/unfold transformations [BD77], or loop-invariant removal [ASU86] might be employed at compile time to "hoist" computations that do not depend on base out of its scope.
Reference: [DC94] <author> Jeffrey Dean and Craig Chambers. </author> <title> Towards better inlining decisions using inlining trials. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <month> June </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Engler and Proebsting [EP93] have investigated using the lcc compiler's intermediate representation for run-time code generation. Intermediate code constructed using ad hoc methods can be compiled by the lcc back end at run time and then directly executed. Run-time compilation is employed in SELF <ref> [HU94, DC94, CU91] </ref>, a compiler for a classless object-oriented language, using a general-purpose intermediate representation. Run-time optimizations are obtained automatically by simply deferring the bulk of compilation to run time. Similar approaches have been implemented for Smalltalk [DS84] and concurrent object-oriented languages [CKP93]. <p> Online strategies that employ run-time information to guide optimization may be necessary. For example, online strategies are employed in the Cecil and SELF compilers to determine where method specialization [DCG94] and inlining <ref> [DC94] </ref> should be applied at run time. Currently Fabius relies on programmer hints to determine where run-time code generation will be profitable.
Reference: [DCG94] <author> Jeffrey Dean, Craig Chambers, and David Grove. </author> <title> Identifying profitable specialization in object-oriented languages. </title> <type> Technical Report 94-02-05, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> February </month> <year> 1994. </year> <month> 9 </month>
Reference-contexts: Online strategies that employ run-time information to guide optimization may be necessary. For example, online strategies are employed in the Cecil and SELF compilers to determine where method specialization <ref> [DCG94] </ref> and inlining [DC94] should be applied at run time. Currently Fabius relies on programmer hints to determine where run-time code generation will be profitable.
Reference: [DH88] <author> Jack W. Davidson and Anne M. Holler. </author> <title> A study of a C function inliner. </title> <journal> Software | Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 775-790, </pages> <year> 1988. </year>
Reference-contexts: efficient in contexts that would otherwise require the register shu*ing described above. 3.3 Inlining and Loop Unrolling Inlining and loop unrolling are valuable optimizations in conventional compilers because they yield increased opportunities for optimization, eliminate the overhead of function calls, and improve the amortization of computations such as range checks <ref> [DH88] </ref>. The extent to which these optimizations may be performed at compile time is rather limited, however. Loop bounds are usually unknown, and a loop can only be unrolled a fixed number of times.
Reference: [DS84] <author> L. Peter Deutsch and Allan M. Schiffman. </author> <title> Efficient implementation of the Smalltalk-80 system. </title> <booktitle> In Conference Record of the 11th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <address> Salt Lake City, </address> <pages> pages 297-302, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Run-time compilation is employed in SELF [HU94, DC94, CU91], a compiler for a classless object-oriented language, using a general-purpose intermediate representation. Run-time optimizations are obtained automatically by simply deferring the bulk of compilation to run time. Similar approaches have been implemented for Smalltalk <ref> [DS84] </ref> and concurrent object-oriented languages [CKP93]. In SELF the primary run-time optimizations are inlining and a form of specialization in which methods are customized to reduce the cost of dynamic type dispatch.
Reference: [EH80] <author> Par Emanuelson and Anders Haraldsson. </author> <title> On compiling embedded languages in LISP. </title> <booktitle> In ACM Conference on Lisp and Functional Programming, Stanford, Cal-ifornia, </booktitle> <pages> pages 208-215, </pages> <year> 1980. </year>
Reference-contexts: The interpretational overhead present in this compilation cannot be statically eliminated. Fabius can be viewed as a manually derived implementation of comix ([[comix]]; [[comix]]). This bears strong similarity to the notion of hand-writing cogen <ref> [BHOS76, EH80, 5 HL91] </ref>. How then does deferred compilation differ from par-tial evaluation? Perhaps the most fundamental difference, discussed in Section 2.2, is that deferred compilation is not driven by an externally imposed division of program inputs, but rather by the staging inherent a program.
Reference: [EP93] <author> Dawson R. Engler and Todd A. Proebsting. </author> <title> DCG: An efficient, retargetable dynamic code generation system. </title> <note> In preparation, </note> <month> November </month> <year> 1993. </year>
Reference-contexts: Keppel, Eggers, and Henry [KEH93, KEH91] have explored the tradeoff between run-time code generation costs and code quality by implementing a template compiler and a more general intermediate-representation compiler for several applications. Engler and Proebsting <ref> [EP93] </ref> have investigated using the lcc compiler's intermediate representation for run-time code generation. Intermediate code constructed using ad hoc methods can be compiled by the lcc back end at run time and then directly executed.
Reference: [FL92] <author> Marc Feeley and Guy Lapalme. </author> <title> Closure generation based on viewing lambda as epsilon plus compile. </title> <journal> Computer Languages, </journal> <volume> 17(4) </volume> <pages> 251-267, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: It may be profitable to generate optimized code for f (x) if it will be applied many times. Run-time code generation can therefore be viewed as an alternative to the conventional implementation of closures; this idea has also been described by Appel [App87] and Feeley and Lapalme <ref> [FL92] </ref>. Stages of computation also arise from conventional iter-ation constructs such as loop nests. Values computed in an outer loop are usually fixed for the duration of inner loops, and substantial benefits might be obtained by optimizing inner loops for each iteration of an outer loop.
Reference: [Han91] <author> John Hannan. </author> <title> Staging transformations for abstract machines. </title> <booktitle> In Proceedings of the Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 130-141. </pages> <booktitle> SIGPLAN Notices, </booktitle> <year> 1991. </year>
Reference-contexts: repeatedly calls an exponentiation function with a fixed exponent that is statically unknown: fun raise exp bases = map (power exp) bases and power exp base = if exp = 0 then 1 else base * power (exp - 1) base 1.2.1 Staging Transformations Various techniques such as staging transformation <ref> [JS86, Han91] </ref>, program bifurcation [Mog89, DBV91], fold/unfold transformations [BD77], or loop-invariant removal [ASU86] might be employed at compile time to "hoist" computations that do not depend on base out of its scope.
Reference: [HL91] <author> N. Carsten Kehler Holst and John Launchbury. </author> <title> Handwriting cogen to avoid problems with static typing. </title> <booktitle> In Draft Proceedings, Fourth Annual Glasgow Workshop on Functional Programming, Skye, Scotland, </booktitle> <pages> pages 210-218. </pages> <address> Glasgow University, </address> <year> 1991. </year>
Reference: [Hol88] <author> N. Carsten Kehler Holst. </author> <title> Language triplets: The AMIX approach. </title> <editor> In D. Bjtrner, A.P. Ershov, and N.D. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 167-185. </pages> <publisher> North-Holland, </publisher> <month> October </month> <year> 1988. </year>
Reference-contexts: One system that comes close to this goal is AMIX, a self-applicable partial evaluator for a first-order functional language whose target is an abstract stack machine <ref> [Hol88] </ref>. AMIX's abstract machine code is a relatively high-level language, however, and the cost of compiling it to native code at run time would be substantial. The interpretational overhead present in this compilation cannot be statically eliminated. Fabius can be viewed as a manually derived implementation of comix ([[comix]]; [[comix]]).
Reference: [HU94] <author> Urs Holzle and David Ungar. </author> <title> Optimizing dynamically-dispatched calls with run-time type feedback. </title> <booktitle> In ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Engler and Proebsting [EP93] have investigated using the lcc compiler's intermediate representation for run-time code generation. Intermediate code constructed using ad hoc methods can be compiled by the lcc back end at run time and then directly executed. Run-time compilation is employed in SELF <ref> [HU94, DC94, CU91] </ref>, a compiler for a classless object-oriented language, using a general-purpose intermediate representation. Run-time optimizations are obtained automatically by simply deferring the bulk of compilation to run time. Similar approaches have been implemented for Smalltalk [DS84] and concurrent object-oriented languages [CKP93].
Reference: [JGS93] <author> Neil D. Jones, Carsten K. Gomard, and Peter Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <publisher> Prentice-Hall, </publisher> <year> 1993. </year>
Reference-contexts: Run-time code generation can reduce the dynamic frequency of loop-invariant operations with less overhead. In addition, it can optimize non-loop-invariant computations using information that is not available at compile time. 1.2.2 Static Specialization Alternatively, specialization <ref> [JGS93] </ref>, driving [Tur86], or procedure cloning [CHK93] might be employed at compile time to transform power into the following function: 2 fun powgen exp = nth exp [fn base =&gt; 1, fn base =&gt; base, fn base =&gt; base * base, fn base =&gt; base * base * base, . . <p> Our goal is to make run-time code generation lightweight and largely automatic without greatly limiting the range of optimizations that may be applied at run time. There are close connections between deferred compilation and partial evaluation <ref> [JGS93] </ref>.
Reference: [JS86] <author> Ulrik Jtrring and William L. Scherlis. </author> <title> Compilers and staging transformations. </title> <booktitle> In Conference Record of the 13th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 86-96, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: repeatedly calls an exponentiation function with a fixed exponent that is statically unknown: fun raise exp bases = map (power exp) bases and power exp base = if exp = 0 then 1 else base * power (exp - 1) base 1.2.1 Staging Transformations Various techniques such as staging transformation <ref> [JS86, Han91] </ref>, program bifurcation [Mog89, DBV91], fold/unfold transformations [BD77], or loop-invariant removal [ASU86] might be employed at compile time to "hoist" computations that do not depend on base out of its scope.
Reference: [JSS89] <author> Neil D. Jones, Peter Sestoft, and Harald Stndergaard. </author> <title> Mix: A self-applicable partial evaluator for experiments in compiler generation. </title> <journal> LISP and Symbolic Computation, </journal> <volume> 2(1) </volume> <pages> 9-50, </pages> <year> 1989. </year>
Reference-contexts: Fabius generates native code for the MIPS R2000. The three major phases of compilation are as follows: * Staging analysis identifies computation stages at which it may be profitable to perform run-time code generation. In a process similar to binding-time analysis <ref> [JSS89, Con93] </ref>, subexpressions of the program are annotated to indicate whether they belong to early or late stages of computation. * Register allocation assigns registers to program variables and intermediate values. <p> The staging analysis also labels the recursive application as an early computation, indicating that the function should be inlined at run time (see Section 3.3). In the case of just two stages, this labeling of early and late computations is very similar to a binding-time analysis and annotation <ref> [JSS89, Con93] </ref>. There is a subtle difference between the two, however.
Reference: [KEH91] <author> David Keppel, Susan J. Eggers, and Robert R. Henry. </author> <title> A case for runtime code generation. </title> <type> Technical Report 91-11-04, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Key benefits of run-time code generation are that specialization occurs "on demand" and that code space can be reused. 1.3 Run-Time Code Generation Run-time code generation has a long history; a useful summary can be found in <ref> [KEH91] </ref>. However, it has not been widely adopted, primarily because it is difficult to automate and because the cost of optimization is often recovered only for large input sizes. The cost of run-time code generation is often reduced using templates. <p> For example, instruction scheduling across template boundaries is difficult to achieve. Using a more general intermediate representation permits a wider range of optimizations and results in higher-quality code, but at higher cost. Keppel, Eggers, and Henry <ref> [KEH93, KEH91] </ref> have explored the tradeoff between run-time code generation costs and code quality by implementing a template compiler and a more general intermediate-representation compiler for several applications. Engler and Proebsting [EP93] have investigated using the lcc compiler's intermediate representation for run-time code generation. <p> Most modern architectures prefetch instructions into an instruction cache, and many do not automatically invalidate cache entries when memory writes occur. Portions of the instruction cache may therefore need 8 to be flushed whenever new code is generated at run time <ref> [KEH91] </ref>. Fortunately the regularity of code-space allocation and initialization simplifies amortizing the cost of such operations. Fabius aligns each newly allocated code object to a boundary that the MIPS instruction prefetcher is guaranteed not to have crossed while executing previously generated code, thus avoiding the invalidation of cached instructions.
Reference: [KEH93] <author> David Keppel, Susan J. Eggers, and Robert R. Henry. </author> <title> Evaluating runtime-compiled value-specific optimizations. </title> <type> Technical Report 93-11-02, </type> <institution> Department of Computer Science and Engineering, University of Washing-ton, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Templates are employed in the Synthesis kernel [Mas92, MP89] to reduce the overhead of kernel calls and context switches. Other applications that benefit significantly from run-time template compilation include decompression and cache simulation <ref> [KEH93] </ref> and the bitblt graphics primitive [PLR85]. Although run-time template compilation is fast, the range of optimizations that may be applied at run time is limited. For example, instruction scheduling across template boundaries is difficult to achieve. <p> For example, instruction scheduling across template boundaries is difficult to achieve. Using a more general intermediate representation permits a wider range of optimizations and results in higher-quality code, but at higher cost. Keppel, Eggers, and Henry <ref> [KEH93, KEH91] </ref> have explored the tradeoff between run-time code generation costs and code quality by implementing a template compiler and a more general intermediate-representation compiler for several applications. Engler and Proebsting [EP93] have investigated using the lcc compiler's intermediate representation for run-time code generation.
Reference: [Kep91] <author> David Keppel. </author> <title> A portable interface for on-the-fly instruction space modification. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 86-95, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: For example Keppel reports that flushing the instruction cache on a DECstation 5000/200 requires a kernel trap plus approximately 0.8 nanoseconds per byte flushed <ref> [Kep91] </ref>.
Reference: [Mal93] <author> Karoline Malmkjr. </author> <title> Towards efficient partial evaluation. </title> <booktitle> In Proceedings of the Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 33-43. </pages> <institution> Association for Computing Machinery, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Such functions are compiled into specialized code generators, parameterized by the values of the early arguments, which generate optimized functions at run time. These code generators are memoized, so that previously optimized code is reused whenever possible. Run-time memoization on structured data can be quite expensive <ref> [Mal93] </ref>, so Fabius uses pointer equality. Although this form of specialization yields significant speedups for some examples, it does not always terminate [BD91].
Reference: [Mas92] <author> Henry Massalin. </author> <title> Synthesis: An Efficient Implementation of Fundamental Operating System Services. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <year> 1992. </year>
Reference-contexts: An exception called Nth is raised if i &gt; n, in which case powgen returns a closure containing the value of exp and the code of an unspecialized exponentiation function. 2 cost; for example, loop unrolling can be implemented by concatenating templates. Templates are employed in the Synthesis kernel <ref> [Mas92, MP89] </ref> to reduce the overhead of kernel calls and context switches. Other applications that benefit significantly from run-time template compilation include decompression and cache simulation [KEH93] and the bitblt graphics primitive [PLR85]. <p> Readers familiar with partial evaluation may notice that dotgen is a generating extension for dotprod. Specialized code generators can also be viewed as executable data structures <ref> [Mas92] </ref> in which interpretational overhead is eliminated by merging code and data. dotgen effectively performs "constant" propagation, conditional folding, and inlining at run time.
Reference: [Mog89] <author> Torben Mogensen. </author> <title> Separating binding times in language specifications. </title> <booktitle> In Fourth International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <address> London, England, </address> <month> Septem-ber </month> <year> 1989, </year> <pages> pages 14-25. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: function with a fixed exponent that is statically unknown: fun raise exp bases = map (power exp) bases and power exp base = if exp = 0 then 1 else base * power (exp - 1) base 1.2.1 Staging Transformations Various techniques such as staging transformation [JS86, Han91], program bifurcation <ref> [Mog89, DBV91] </ref>, fold/unfold transformations [BD77], or loop-invariant removal [ASU86] might be employed at compile time to "hoist" computations that do not depend on base out of its scope.
Reference: [MP89] <author> Henry Massalin and Calton Pu. </author> <title> Threads and input/output in the Synthesis kernel. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 191-201, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: An exception called Nth is raised if i &gt; n, in which case powgen returns a closure containing the value of exp and the code of an unspecialized exponentiation function. 2 cost; for example, loop unrolling can be implemented by concatenating templates. Templates are employed in the Synthesis kernel <ref> [Mas92, MP89] </ref> to reduce the overhead of kernel calls and context switches. Other applications that benefit significantly from run-time template compilation include decompression and cache simulation [KEH93] and the bitblt graphics primitive [PLR85].
Reference: [PLR85] <author> Rob Pike, Bart Locanthi, and John Reiser. </author> <title> Hardware/software trade-offs for bitmap graphics on the Blit. </title> <journal> Software | Practice and Experience, </journal> <volume> 15(2) </volume> <pages> 131-151, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: Templates are employed in the Synthesis kernel [Mas92, MP89] to reduce the overhead of kernel calls and context switches. Other applications that benefit significantly from run-time template compilation include decompression and cache simulation [KEH93] and the bitblt graphics primitive <ref> [PLR85] </ref>. Although run-time template compilation is fast, the range of optimizations that may be applied at run time is limited. For example, instruction scheduling across template boundaries is difficult to achieve.
Reference: [Tur86] <author> Valentin F. Turchin. </author> <title> The concept of a supercompiler. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(3) </volume> <pages> 292-325, </pages> <year> 1986. </year> <month> 10 </month>
Reference-contexts: Run-time code generation can reduce the dynamic frequency of loop-invariant operations with less overhead. In addition, it can optimize non-loop-invariant computations using information that is not available at compile time. 1.2.2 Static Specialization Alternatively, specialization [JGS93], driving <ref> [Tur86] </ref>, or procedure cloning [CHK93] might be employed at compile time to transform power into the following function: 2 fun powgen exp = nth exp [fn base =&gt; 1, fn base =&gt; base, fn base =&gt; base * base, fn base =&gt; base * base * base, . . . &gt;
References-found: 36

