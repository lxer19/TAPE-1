URL: http://www.research.att.com/~mkearns/papers/lower.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Title: A General Lower Bound on the Number of Examples Needed for Learning  
Author: Andrzej Ehrenfeucht David Haussler Michael Kearns Leslie Valiant 
Note: 0 A. Ehrenfeucht was supported by NSF grant MCS-8305245, D. Haussler by ONR grant N00014-86-K-0454, M. Kearns by ONR grant N00014-86-K-0454 and an A.T. T. Bell Laboratories Scholarship, and L. Valiant by grants ONR-N00014-85-K-0445, NSF-DCR-8600379, DAAL03-86-K-0171 and by the SERC of the U.K. Part of this research was done while M. Kearns was visiting  
Affiliation: University of Colorado  U.C. Santa Cruz  Harvard University  Harvard University  U.C. Santa Cruz.  
Abstract: We prove a lower bound of ( 1 * ln 1 ffi + VCdim(C) * ) on the number of random examples required for distribution-free learning of a concept class C, where VCdim(C) is the Vapnik-Chervonenkis dimension and * and ffi are the accuracy and confidence parameters. This improves the previous best lower bound of ( 1 * ln 1 ffi + VCdim(C)), and comes close to the known general upper bound of O( 1 ffi + VCdim(C) * ln 1 * ) for consistent algorithms. We show that for many interesting concept classes, including kCNF and kDNF, our bound is actually tight to within a constant factor. 
Abstract-found: 1
Intro-found: 1
Reference: [AL86] <author> Angluin, D., P. Laird, </author> <title> "Identifying kCNF formulas from noisy examples", </title> <booktitle> Machine Learning 2(4), </booktitle> <year> 1988, </year> <pages> pp. 319-342. </pages>
Reference-contexts: These examples are drawn randomly according to a fixed but arbitrary probability distribution. From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. <ref> [AL86] </ref>, [BEHW86,87a,87b], [BI88], [KLPV87], [KL87], [KV88], [LMR88], [N87], [PV86], [R87], [S88], [V85], [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not.
Reference: [AV79] <author> Angluin, D., L.G. Valiant, </author> <title> "Fast probabilistic algorithms for Hamiltonian circuits and matchings", </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 18, </volume> <year> 1979, </year> <pages> pp. 155-193. </pages>
Reference-contexts: Lemma 3 Let ffi 1 100 . For m = d Proof: We will use the following fact from probability theory (Proposition 2.4, <ref> [AV79] </ref>): For 0 p 1 and m; r positive integers, let GE (p; m; r) denote the probability of at least r successes in m independent trials of a Bernoulli variable with probability of success p. Fact.
Reference: [BH88] <author> Baum, E., D. Haussler, </author> <title> "What size net gives valid generalization?", </title> <booktitle> to appear in I.E.E.E. Conference on Neural Information Processing Systems, </booktitle> <address> Denver, CO, </address> <year> 1988. </year>
Reference-contexts: In [H88] other applications of Corollary 5 to learning algorithms in Artificial Intelligence domains are given. Further consequences of Corollary 5 for learning algorithms on feedforward neural networks of linear threshold functions are discussed in <ref> [BH88] </ref>. 10 5 Open Problems Disregarding computational resources, does there always exist an (*; ffi)-learning algorithm for C using sample size O ( 1 * ln 1 VCdim (C) * ) ? It is shown in [HLW87] that the upper bound of O ( 1 * ln 1 VCdim (C) *
Reference: [BI88] <author> Benedek, G.M., A. Itai, </author> <title> "Learnability by fixed distributions", </title> <booktitle> First Workshop on Computational Learning Theory, </booktitle> <publisher> M.I.T., </publisher> <year> 1988. </year>
Reference-contexts: These examples are drawn randomly according to a fixed but arbitrary probability distribution. From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], <ref> [BI88] </ref>, [KLPV87], [KL87], [KV88], [LMR88], [N87], [PV86], [R87], [S88], [V85], [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not.
Reference: [BEHW86] <author> Blumer, A., A. Ehrenfeucht, D. Haussler, M. Warmuth, </author> <title> "Classifying learnable geometric concepts with the Vapnik-Chervonenkis dimension", </title> <booktitle> 18th ACM Symposium on the Theory of Computing, </booktitle> <address> Berkeley, CA, </address> <year> 1986, </year> <pages> pp. 273-282. </pages>
Reference: [BEHW87a] <author> Blumer, A., A. Ehrenfeucht, D. Haussler, M. Warmuth, </author> <title> "Occam's Razor", </title> <journal> Information Processing Letters, </journal> <volume> 24, </volume> <year> 1987, </year> <pages> pp. 377-380. </pages>
Reference: [BEHW87b] <author> Blumer, A., A. Ehrenfeucht, D. Haussler, M. Warmuth, </author> <title> "Learnability and the Vapnik-Chervonenkis dimension", </title> <type> technical report UCSC-CRL-87-20, </type> <institution> U.C. Santa Cruz, </institution> <year> 1987, </year> <note> and to appear in JACM. </note>
Reference: [H88] <author> Haussler, D., </author> <title> "Quantifying Inductive Bias: AI Learning Algorithms and Valiant's Model", </title> <journal> Artificial Intelligence, </journal> <volume> 36, </volume> <year> 1988, </year> <pages> pp. 177-221. </pages>
Reference-contexts: This bound is off from optimal by a factor of at most O (ln 1 * ). In <ref> [H88] </ref> other applications of Corollary 5 to learning algorithms in Artificial Intelligence domains are given.
Reference: [HLW87] <author> Haussler, D., N. Littlestone, M. Warmuth, </author> <title> "Predicting f0; 1g-functions on randomly 14 drawn points", </title> <booktitle> to appear in 29th I.E.E.E. Symposium on Foundations of Computer Science, </booktitle> <address> White Plains, NY, </address> <year> 1988. </year>
Reference-contexts: Results in [L88] and <ref> [HLW87] </ref> can be combined to show that there is an efficient (*; ffi)-learning algorithm for C using sample size O ( lln n * ln 1 ffi ). <p> for learning algorithms on feedforward neural networks of linear threshold functions are discussed in [BH88]. 10 5 Open Problems Disregarding computational resources, does there always exist an (*; ffi)-learning algorithm for C using sample size O ( 1 * ln 1 VCdim (C) * ) ? It is shown in <ref> [HLW87] </ref> that the upper bound of O ( 1 * ln 1 VCdim (C) * ln 1 * ) given in Theorem 6 for arbitrary consistent algorithms using hypotheses in C cannot be improved, i.e. for all d 1 there are concept classes C with V Cdim (C) = d with
Reference: [HW87] <author> Haussler, D., E. Welzl, </author> <title> "Epsilon-nets and simplex range queries", </title> <journal> Discrete Computational Geometry, </journal> <volume> 2, </volume> <year> 1987, </year> <pages> pp. 127-151. </pages>
Reference-contexts: We now turn our attention to examples where the concept class is defined over a continuous domain. Linear Separators (Half-spaces): Let C be the class of all half-spaces (open or closed) in Eu-clidean n-dimensional space E n . Then VCdim (C) = n + 1 (see e.g. [WD81] or <ref> [HW87] </ref>), and an efficient consistent algorithm for C using hypotheses in C can be implemented using linear programming (see [K84], [K79], see [BEHW86,87b]).
Reference: [K84] <author> Karmarkar, N., </author> <title> "A new polynomial-time algorithm for linear programming", </title> <journal> Combinatorica, </journal> <volume> 4, </volume> <year> 1984, </year> <pages> pp. 373-395. </pages>
Reference-contexts: Then VCdim (C) = n + 1 (see e.g. [WD81] or [HW87]), and an efficient consistent algorithm for C using hypotheses in C can be implemented using linear programming (see <ref> [K84] </ref>, [K79], see [BEHW86,87b]). By Theorem 6 (i) this algorithm requires sample size O ( 1 * ln 1 * ln 1 * ), which is within a factor of O (ln 1 * ) of optimal by Corollary 5.
Reference: [KL87] <author> Kearns, M., M. Li, </author> <title> "Learning in the presence of malicious errors", </title> <booktitle> 20th ACM Symposium on the Theory of Computing, </booktitle> <address> Chicago, IL, </address> <year> 1988, </year> <pages> pp. 267-280. </pages>
Reference-contexts: From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], [BI88], [KLPV87], <ref> [KL87] </ref>, [KV88], [LMR88], [N87], [PV86], [R87], [S88], [V85], [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not. <p> Hence, VCdim (C) jSj = n + 1. Corollary 5 then gives a lower bound of ( 1 * ln 1 * ) on the number of examples needed to learn symmetric functions, proving that the algorithm given in <ref> [KL87] </ref> has optimal sample complexity. k-term DNF: For constant k, a k-term DNF formulae is one of the form T 1 + + T k , where each T i is a monomial whose length is not restricted.
Reference: [KLPV87] <author> Kearns, M., M. Li, L. Pitt, L. Valiant, </author> <title> "On the learnability of Boolean formulae", </title> <booktitle> 19th ACM Symposium on the Theory of Computing, </booktitle> <address> New York, NY, </address> <year> 1987, </year> <pages> pp. 285-295. </pages>
Reference-contexts: These examples are drawn randomly according to a fixed but arbitrary probability distribution. From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], [BI88], <ref> [KLPV87] </ref>, [KL87], [KV88], [LMR88], [N87], [PV86], [R87], [S88], [V85], [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not. <p> We will call * the accuracy parameter and ffi the confidence parameter. We note that our results also hold in the model where distributions over 3 the positive and negative examples are distinguished (e.g. [V84], <ref> [KLPV87] </ref>), with only minor modifications to the proof. If C is a concept class over domain X, and W X, we say that W is shattered by C if for every W 0 W , there exists a c 2 C such that W 0 = c " W . <p> Note that the hypothesis produced by this learning algorithm is not in the class C, but in kCNF. It is shown in [PV86] (see also <ref> [KLPV87] </ref>) that learning k-term DNF using hypothesis space k-term DNF is NP-hard. There is an algorithm using hypothesis space k-term DNF, but it is an exhaustive-search algorithm requiring superpolynomial time. <p> Furthermore, by the above proof the image decision lists are linearly separable in p (n)-dimensional space, so VCdim (kDL) = O (p (n)) = O (n k ). Similar transformation techniques can be found in <ref> [KLPV87] </ref> and [L88].
Reference: [KV88] <author> Kearns, M., L.G. Valiant, </author> <title> "Learning Boolean formulae or finite automata is as hard as factoring", </title> <type> technical report TR-14-88, </type> <institution> Aiken Computation Laboratory, Harvard University, </institution> <year> 1988. </year>
Reference-contexts: From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], [BI88], [KLPV87], [KL87], <ref> [KV88] </ref>, [LMR88], [N87], [PV86], [R87], [S88], [V85], [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not.
Reference: [K79] <author> Khachiyan, L.G., </author> <title> "A polynomial algorithm for linear programming", </title> <journal> Doklady Akademiia Nauk SSSR, </journal> <volume> 244:S, </volume> <year> 1979, </year> <pages> pp. 191-194. </pages>
Reference-contexts: Then VCdim (C) = n + 1 (see e.g. [WD81] or [HW87]), and an efficient consistent algorithm for C using hypotheses in C can be implemented using linear programming (see [K84], <ref> [K79] </ref>, see [BEHW86,87b]). By Theorem 6 (i) this algorithm requires sample size O ( 1 * ln 1 * ln 1 * ), which is within a factor of O (ln 1 * ) of optimal by Corollary 5.
Reference: [L88] <author> Littlestone, N., </author> <title> "Learning quickly when irrelevant attributes abound: a new linear threshold algorithm", </title> <booktitle> Machine Learning 2(4), </booktitle> <year> 1988, </year> <pages> pp. 245-318. </pages>
Reference-contexts: Results in <ref> [L88] </ref> and [HLW87] can be combined to show that there is an efficient (*; ffi)-learning algorithm for C using sample size O ( lln n * ln 1 ffi ). <p> Furthermore, by the above proof the image decision lists are linearly separable in p (n)-dimensional space, so VCdim (kDL) = O (p (n)) = O (n k ). Similar transformation techniques can be found in [KLPV87] and <ref> [L88] </ref>.
Reference: [LMR88] <author> Linial, N., Y. Mansour, R.L. Rivest, </author> <title> "Results on learnability and the Vapnik-Chervonenkis dimension", </title> <booktitle> First Workshop on Computational Learning Theory, </booktitle> <publisher> M.I.T., </publisher> <year> 1988, </year> <booktitle> and to appear in 29th I.E.E.E. Symposium on Foundations of Computer Science, </booktitle> <address> White Plains, N.Y., </address> <year> 1988. </year>
Reference-contexts: From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], [BI88], [KLPV87], [KL87], [KV88], <ref> [LMR88] </ref>, [N87], [PV86], [R87], [S88], [V85], [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not. Little attention has been paid to determining precise complexity bounds (both upper and lower) for classes already known to be learnable in polynomial time.
Reference: [N87] <author> Natarajan, B.K., </author> <title> "On learning Boolean functions", </title> <booktitle> 19th A.C.M. Symposium on the Theory of Computing, </booktitle> <address> New York, NY, </address> <year> 1987, </year> <pages> pp. 296-304. </pages>
Reference-contexts: From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], [BI88], [KLPV87], [KL87], [KV88], [LMR88], <ref> [N87] </ref>, [PV86], [R87], [S88], [V85], [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not. Little attention has been paid to determining precise complexity bounds (both upper and lower) for classes already known to be learnable in polynomial time.
Reference: [PV86] <author> Pitt, L., L.G. Valiant, </author> <title> "Computational limitations on learning from examples", </title> <type> technical report TR-05-86, </type> <institution> Aiken Computation Laboratory, Harvard University, </institution> <year> 1986. </year> <month> 15 </month>
Reference-contexts: From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], [BI88], [KLPV87], [KL87], [KV88], [LMR88], [N87], <ref> [PV86] </ref>, [R87], [S88], [V85], [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not. Little attention has been paid to determining precise complexity bounds (both upper and lower) for classes already known to be learnable in polynomial time. <p> Thus Corollary 5 gives a lower bound of O ( 1 * ln 1 * ) on the sample size required for learning C. However, the best known efficient algorithm for learning C (given in <ref> [PV86] </ref>) uses the algorithm of [V84] for k-CNF, and thus needs sample size ( 1 * ln 1 * ). Note that the hypothesis produced by this learning algorithm is not in the class C, but in kCNF. It is shown in [PV86] (see also [KLPV87]) that learning k-term DNF using <p> known efficient algorithm for learning C (given in <ref> [PV86] </ref>) uses the algorithm of [V84] for k-CNF, and thus needs sample size ( 1 * ln 1 * ). Note that the hypothesis produced by this learning algorithm is not in the class C, but in kCNF. It is shown in [PV86] (see also [KLPV87]) that learning k-term DNF using hypothesis space k-term DNF is NP-hard. There is an algorithm using hypothesis space k-term DNF, but it is an exhaustive-search algorithm requiring superpolynomial time.
Reference: [R87] <author> Rivest, R., </author> <title> "Learning decision lists", </title> <booktitle> Machine Learning 2(3), </booktitle> <year> 1987, </year> <pages> pp. 229-246. </pages>
Reference-contexts: From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], [BI88], [KLPV87], [KL87], [KV88], [LMR88], [N87], [PV86], <ref> [R87] </ref>, [S88], [V85], [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not. Little attention has been paid to determining precise complexity bounds (both upper and lower) for classes already known to be learnable in polynomial time. <p> We apply this result to show that the existing algorithms for learning monomials, kDNF formulae, kCNF formulae and symmetric functions all use the optimal number of examples (within a constant factor). By similar methods, we prove that the algorithm given in <ref> [R87] </ref> for learning decision lists on n variables uses a sample size that is at most a logarithmic factor off optimal, and give an alternative analysis of this algorithm that yields a small improvement in its sample size. <p> The value of L (~v) for ~v 2 f0; 1g n is defined as follows: let 1 i m be the least value such that M i (~v) = 1. Then L (~v) = b i (or 0 if no such i exists). In <ref> [R87] </ref> it is shown that the concept class represented by 9 k-Decision Lists properly includes the kCNF and kDNF functions, and an efficient consistent algo-rithm for learning k-Decision Lists using k-Decision List hypotheses is given that uses sample size O ( 1 ffi + n k * lnn). <p> Thus, the sample size of the algorithm of <ref> [R87] </ref> is at most O (lnn) above the optimal. Furthermore, the upper bound on VCdim (C) yields an alternative analysis of this algorithm: by applying Theorem 6 (i), we see that in fact a sample of size O ( 1 ffi + n k * ) also suffices. <p> If it is decided at run time which log factor is smaller, then we have shown that the sample complexity of the algorithm of <ref> [R87] </ref> is in fact O ( 1 * ln 1 * min (ln 1 a factor of min (ln 1 * ; lnn) above optimal. We now turn our attention to examples where the concept class is defined over a continuous domain. <p> Then VCdim (kDL) = fi (n k ). Proof: The lower bound on the dimension follows easily from the fact that kDL contains the class of kDNF functions <ref> [R87] </ref>, thus VCdim (kDL) VCdim (kDNF) = fi (n k ). For the upper bound, we begin by proving that VCdim (1DL) = O (n). We then give a simple transformation that proves the theorem for arbitrary k.
Reference: [S88] <author> Shvayster, H., </author> <title> "Non-learnable classes of Boolean formulae that are closed under variable permutation", </title> <booktitle> First Workshop on Computational Learning Theory, </booktitle> <publisher> M.I.T., </publisher> <year> 1988. </year>
Reference-contexts: From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], [BI88], [KLPV87], [KL87], [KV88], [LMR88], [N87], [PV86], [R87], <ref> [S88] </ref>, [V85], [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not. Little attention has been paid to determining precise complexity bounds (both upper and lower) for classes already known to be learnable in polynomial time.
Reference: [V84] <author> Valiant, L.G., </author> <title> "A theory of the learnable", </title> <journal> Comm. ACM, </journal> <volume> 27(11) 1984, </volume> <pages> pp. 1134-1142. </pages>
Reference-contexts: 1 Introduction In <ref> [V84] </ref>, a stochastic model of machine learning from examples based on computational complexity was introduced. Informally, this model can be described as follows: positive and negative examples of some unknown target concept, chosen from a concept class C, are presented to a learning algorithm. <p> We also show that some existing algorithms for concept classes over continuous domains use a sample size that is within a multiplicative logarithmic factor of optimal. The lower bound we prove is information-theoretic in the sense that no algorithm in the learning model of <ref> [V84] </ref>, even one with infinite computational resources, can learn from fewer examples. It comes within a multiplicative log factor of the information-theoretic upper bound on the number of examples needed by any algorithm that always produces consistent hypotheses in the target concept class [V82] [BEHW86,87b]. <p> It comes within a multiplicative log factor of the information-theoretic upper bound on the number of examples needed by any algorithm that always produces consistent hypotheses in the target concept class [V82] [BEHW86,87b]. The outline of the paper is as follows: in Section 2, we define the model of <ref> [V84] </ref>, and give relevant notation. Section 3 presents our main result, the lower bound. In Section 4 we apply the lower bound to obtain tight and nearly tight bounds on the sample complexity for learning several well-studied concept classes. <p> We will call * the accuracy parameter and ffi the confidence parameter. We note that our results also hold in the model where distributions over 3 the positive and negative examples are distinguished (e.g. <ref> [V84] </ref>, [KLPV87]), with only minor modifications to the proof. <p> Thus, we have that VCdim (C) jSj = n, so by Corollary 5 the sample complexity of learning C is ( 1 * ln 1 * ). On the other hand, in <ref> [V84] </ref> an efficient consistent algorithm for C using hypothesis space C is given. Since jCj = 3 n , we have by Theorem 6 (ii) that C is learnable with sample complexity O ( 1 ffi + n * ), which is within a constant factor of the lower bound. <p> By Corollary 5, we have a lower bound of ( 1 ffi + n k * ) on the number of examples needed to learn kDNF. Since lnjCj = O (n k ) and <ref> [V84] </ref> gives a consistent algorithm for kDNF using kDNF hypotheses that runs in time polynomial in the length of the sample, this lower bound proves that the algorithm of [V84] is optimal in 8 terms of the number of examples used by Theorem 6 (ii). <p> Since lnjCj = O (n k ) and <ref> [V84] </ref> gives a consistent algorithm for kDNF using kDNF hypotheses that runs in time polynomial in the length of the sample, this lower bound proves that the algorithm of [V84] is optimal in 8 terms of the number of examples used by Theorem 6 (ii). By duality, we have an analogous result for the class kCNF of conjunctions of clauses, where each clause is a disjunction of at most k literals. <p> Thus Corollary 5 gives a lower bound of O ( 1 * ln 1 * ) on the sample size required for learning C. However, the best known efficient algorithm for learning C (given in [PV86]) uses the algorithm of <ref> [V84] </ref> for k-CNF, and thus needs sample size ( 1 * ln 1 * ). Note that the hypothesis produced by this learning algorithm is not in the class C, but in kCNF.
Reference: [V85] <author> Valiant, L.G., </author> <booktitle> "Learning disjunctions of conjunctions" Proc. 9th IJCAI, </booktitle> <address> Los Angeles, CA, </address> <year> 1985, </year> <pages> pp. 560-566. </pages>
Reference-contexts: From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], [BI88], [KLPV87], [KL87], [KV88], [LMR88], [N87], [PV86], [R87], [S88], <ref> [V85] </ref>, [VL88]) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not. Little attention has been paid to determining precise complexity bounds (both upper and lower) for classes already known to be learnable in polynomial time.
Reference: [V82] <author> Vapnik, V.N., </author> <title> "Estimation of dependences based on empirical data", </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: It comes within a multiplicative log factor of the information-theoretic upper bound on the number of examples needed by any algorithm that always produces consistent hypotheses in the target concept class <ref> [V82] </ref> [BEHW86,87b]. The outline of the paper is as follows: in Section 2, we define the model of [V84], and give relevant notation. Section 3 presents our main result, the lower bound. <p> In this section we apply Theorem 1 to obtain lower bounds on the sample complexity for a variety of concept classes. These bounds obtained are tight within a constant factor in many important cases. We begin by recalling results of [BEHW86,87a,87b], derived from <ref> [V82] </ref>, that bound the sample complexity of algorithms that produce hypotheses in the target class C that are consistent with the examples they have seen. <p> If A always outputs a hypothesis h 2 H 2 X , then we say that A uses hypothesis space H. Theorem 6 <ref> [V82] </ref> [BEHW86,87a,87b] Let A be a consistent algorithm for C using hypothesis space C, and let 0 &lt; *; ffi &lt; 1.
Reference: [VC71] <author> Vapnik, V.N., A.Ya. Chervonenkis, </author> <title> "On the uniform convergence of relative frequencies of events to their probabilities", </title> <journal> Th. Prob. and its Appl., </journal> <volume> 16(2), </volume> <year> 1971, </year> <pages> pp. 264-280. </pages>
Reference: [VL88] <author> Vitter, J.S., J. Lin, </author> <title> "Learning in parallel", </title> <booktitle> First Workshop on Computational Learning Theory, </booktitle> <publisher> M.I.T., </publisher> <year> 1988. </year>
Reference-contexts: From the examples drawn, the learning algorithm must, with high probability, produce a hypothesis concept that is a good approximation to the target. Most of the recent research in this model (see e.g. [AL86], [BEHW86,87a,87b], [BI88], [KLPV87], [KL87], [KV88], [LMR88], [N87], [PV86], [R87], [S88], [V85], <ref> [VL88] </ref>) has emphasized the broad distinction between those classes that are learnable in polynomial time and those that are not. Little attention has been paid to determining precise complexity bounds (both upper and lower) for classes already known to be learnable in polynomial time.
Reference: [WD81] <author> Wencour, </author> <title> R.S., R.M. Dudley, "Some special Vapnik-Chervonenkis classes", </title> <journal> Discrete Math, </journal> <volume> 33, </volume> <year> 1981, </year> <pages> pp. 313-318. </pages>
Reference-contexts: We now turn our attention to examples where the concept class is defined over a continuous domain. Linear Separators (Half-spaces): Let C be the class of all half-spaces (open or closed) in Eu-clidean n-dimensional space E n . Then VCdim (C) = n + 1 (see e.g. <ref> [WD81] </ref> or [HW87]), and an efficient consistent algorithm for C using hypotheses in C can be implemented using linear programming (see [K84], [K79], see [BEHW86,87b]).
References-found: 27

