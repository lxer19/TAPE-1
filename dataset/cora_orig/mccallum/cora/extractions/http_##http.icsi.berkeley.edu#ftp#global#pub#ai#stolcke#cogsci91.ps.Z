URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/stolcke/cogsci91.ps.Z
Refering-URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/stolcke/
Root-URL: http://http.icsi.berkeley.edu
Email: stolcke@icsi.berkeley.edu  
Title: Syntactic Category Formation with Vector Space Grammars symbolic formalisms to continuous representations is a promising
Author: Andreas Stolcke 
Address: Berkeley, CA 94720  1947 Center St., Berkeley, CA 94704  
Affiliation: Computer Science Division University of California  International Computer Science Institute  
Note: In: Proc. 13th Ann. Conf. Cognitive Science Soc., Chicago, Ill., August 1991, pp. 908-912  More generally, it is argued that the conversion of  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Elman, J. L. </author> <year> 1988. </year> <title> Finding Structure in Time. </title> <type> CRL Technical Report 8801, </type> <institution> Center for Research in Language, University of California at San Diego, La Jolla, Calif. </institution>
Reference: <author> Fass, L. F. </author> <year> 1983. </year> <title> Learning Context-Free Languages from their Structured Sentences. </title> <journal> ACM SIGACT News 15(3). </journal>
Reference-contexts: The kinds of structures available to the learning algorithm are familiar from Levy and Joshi's (1978) skeletal structural descriptions, and have been shown to be sufficient for syntax learning <ref> (Fass 1983) </ref>. These learnability results, however, use automata induction techniques with very complex data structures (equivalence classes of trees structures), and are therefore not directly comparable to the methods employed here.
Reference: <author> Feldman, J. A., Lakoff, G., Stolcke, A., and Weber, S. H. </author> <year> 1990. </year> <title> Miniature Language Acquisition: A touchstone for cognitive science. </title> <booktitle> In Proceedings of the 12th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 686-693, </pages> <publisher> MIT, </publisher> <address> Cambridge, Mass. </address>
Reference: <author> Holland, J. </author> <year> 1975. </year> <booktitle> Adaption in natural and artificial systems. </booktitle>
Reference-contexts: Many rules never get applied and never learn to be useful as a result. To counteract this tendency we have recently modified the learning algorithm to incorporate an idea from learning in genetic systems <ref> (Holland 1975) </ref>. In the modified learning schedule, rules that never are used are periodically eliminated from the rule set and replaced by copies (`clones') of rules that are heavily used.
Reference: <institution> Ann Arbor, Mich.: University of Michigan Press. </institution>
Reference: <author> Langacker, R. </author> <year> 1985. </year> <booktitle> Foundations of Cognitive Grammar. </booktitle> <volume> Vol. 1: </volume> <booktitle> Theoretical Prerequisites. </booktitle> <publisher> Stanford: Stanford University Press. </publisher>
Reference-contexts: This fundamental `iconic relationship' between syntax and concepts is understood by some linguists as the very essence of language <ref> (Langacker 1985) </ref>. A learner could capitalize on this principle if one assumes that certain general cognitive capacities are available prior to syntax learning. For the purpose of this paper, then, we will assume that a learning system has access to phrase-bracketing information from independent sources.
Reference: <author> Magerman, D. M., and Marcus, M. P. </author> <year> 1990. </year> <title> Parsing a Natural Language Using Mutual Information Statistics. </title> <booktitle> In Proceedings of the 8th National Conference on Artificial Intelligence, </booktitle> <address> Boston, Mass. </address>
Reference-contexts: It has been shown that there are very effective statistical methods to find phrase boundaries (without phrase type classification) in text <ref> (Magerman & Marcus 1990) </ref>. Secondly, psycholinguistic data indicates that humans can learn language structures successfully only when they can draw from a rich set of universal intra- and extra-sentential cues to induce phrase structure independently (Morgan, Meier, & Newport 1987; Morgan, Meier, & Newport 1989).
Reference: <author> Morgan, J. L. </author> <year> 1986. </year> <title> From Simple Input to Complex Grammar. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference: <author> Morgan, J. L., Meier, R. P., and Newport, E. L. </author> <year> 1987. </year> <title> Structural Packaging in the Input to Language Learning: Contributions of Prosodic and Morphological Marking of Phrases to the Acquisition of Language. </title> <booktitle> Cognitive Psychology 19 </booktitle> <pages> 498-550. </pages>
Reference: <author> Morgan, J. L., Meier, R. P., and Newport, E. L. </author> <year> 1989. </year> <title> Facilitating the Acquisition of Syntax with Cross-Sentential Cues to Phrase Structure. </title> <booktitle> Journal of Memory and Language 28 </booktitle> <pages> 360-374. </pages>
Reference: <author> Pinker, S. </author> <year> 1989. </year> <title> Language Acquisition. </title> <editor> In Posner, M. I., ed., </editor> <booktitle> Foundations of Cognitive Science. </booktitle> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The need for negative examples is the most bothersome problem if one is looking for a plausible mechanism for natural language acquisition (and widely acknowledged as a major challenge for many theories of acquisition, see, e.g., <ref> (Pinker 1989) </ref>). Although our current system is certainly too impoverished to claim to be a model of natural language acquisition (it handles only syntax, for one thing), it would be nice to obviate the need for negative examples.
Reference: <author> Pollack, J. B. </author> <year> 1990. </year> <title> Recursive Distributed Representations. </title> <booktitle> Artificial Intelligence 46 </booktitle> <pages> 77-105. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> 1986. </year> <title> Learning Internal Representations by Error Propagation. </title> <editor> In (Rumelhart, McClelland, </editor> & <booktitle> The PDP Research Group 1986), </booktitle> <pages> 318-362. </pages>
Reference-contexts: Introduction Connectionism, and especially Parallel Distributed Processing (PDP) has developed an array of models of learning systems (backpropagation, Boltzmann machines, competitive learning <ref> (Rumelhart, McClelland, & The PDP Research Group 1986) </ref>). These models typically operate on representations at a rather low and unstructured level (unit activations, bit vectors, micro-features) relative to the structures used in traditional linguistic descriptions (trees and graphs, case frames, grammar rules, stacks). <p> Without loss of generality we can fix S throughout training to be a particular vector, e.g., the unit vector (1; 0; : : : ; 0). The second idea adapted from connectionist learning methods is that of error backpropagation <ref> (Rumelhart, Hinton, & Williams 1986) </ref>. At the root node we can immediately compute an error term for the discrepancy between the desired output and the actual output.
Reference: <editor> Rumelhart, D. E., McClelland, J. L., </editor> <booktitle> and The PDP Research Group 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> vol. 1: </volume> <booktitle> Foundations. </booktitle> <address> Cambridge, Mass.: </address> <publisher> Bradford Books (MIT Press). </publisher>
Reference-contexts: Introduction Connectionism, and especially Parallel Distributed Processing (PDP) has developed an array of models of learning systems (backpropagation, Boltzmann machines, competitive learning <ref> (Rumelhart, McClelland, & The PDP Research Group 1986) </ref>). These models typically operate on representations at a rather low and unstructured level (unit activations, bit vectors, micro-features) relative to the structures used in traditional linguistic descriptions (trees and graphs, case frames, grammar rules, stacks). <p> Without loss of generality we can fix S throughout training to be a particular vector, e.g., the unit vector (1; 0; : : : ; 0). The second idea adapted from connectionist learning methods is that of error backpropagation <ref> (Rumelhart, Hinton, & Williams 1986) </ref>. At the root node we can immediately compute an error term for the discrepancy between the desired output and the actual output.
Reference: <author> Rumelhart, D. E., and Zipser, D. </author> <year> 1985. </year> <title> Feature Discovery by Competitive Learning. </title> <booktitle> Cognitive Science 9 </booktitle> <pages> 75-112. </pages> <note> Reprinted in (Rumelhart, </note> <editor> McClelland, </editor> & <booktitle> The PDP Research Group 1986), </booktitle> <pages> pp. 151-193. </pages>
Reference-contexts: Only the rules selected at some node will later participate in the learning process. Since only the currently best rules get selected the whole process strongly resembles the method of competitive learning <ref> (Rumelhart & Zipser 1985) </ref>. By working from the terminal nodes to the root we arrive at a category label for the entire string.
Reference: <author> Smolensky, P. </author> <year> 1987. </year> <title> On variable binding and the representation of symbolic structures in connectionist systems. </title> <type> Technical Report CU-CS-355-87, </type> <institution> University of Colorado, Boulder, Colo. </institution>
Reference: <author> Stolcke, A. </author> <year> 1991. </year> <title> Vector Space Grammars and Grammatical Category Acquisition. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, Calif. </institution> <note> in preparation. </note>
Reference-contexts: The computation of error derivatives is straightforward because of the simple linear operations used in equation 3 but omitted here for lack of space (see <ref> (Stolcke 1991) </ref>). Derivatives for each category vector are then added up and multiplied by some constant (the `learning rate') to give the adjustment to be applied to that category. All rules are updated accordingly, all categories are rescaled to unit-length, and the next training example is processed.
Reference: <author> Zadeh, L. A. </author> <year> 1972. </year> <title> Fuzzy Languages and their Relation to Human and Machine Intelligence. </title> <booktitle> In Proceedings of the Conference on Man and Computer, </booktitle> <address> Bordeaux, France, </address> <month> June </month> <year> 1970, </year> <pages> 130-165. </pages> <address> Basel: </address> <publisher> S. Karger. </publisher>
Reference-contexts: Again, continuity and differentiability are typically not found in traditional linguistic constructs, which tend to be inherently discrete (an exception are Fuzzy Languages <ref> (Zadeh 1972) </ref>). It seems desirable, then, to investigate ways to combine connectionist (usually vector-based) representa tions with structures and concepts developed in traditional theories, especially in cases where those theories have a strong empirical or intuitive appeal.
References-found: 18

