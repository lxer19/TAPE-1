URL: http://www.cs.princeton.edu/~durand/papers/ieeeTPDS96.ps
Refering-URL: http://www.cs.princeton.edu/~durand/
Root-URL: http://www.cs.princeton.edu
Title: Load Balancing Performance of Dynamic Scheduling on NUMA Multiprocessors  
Author: M. D. Durand T. Montaut L. Kervella W. Jalby 
Keyword: Dynamic scheduling, load balancing, memory performance, self-scheduling, NUMA multi processors.  
Note: Appeared in IEEE Transactionson Parallel and Distributed Systems, Nov. 1996,p. 1201 1214 Durand's contribution to this work was performed in part at IRISA and Ecoles des Mines de Paris and supported by the North Atlantic Treaty Organization under a Grant awarded in 1990. Jalby's contribution to this work was performed in part at IRISA.  
Date: September 9, 1997  
Address: Versailles, France  
Affiliation: Computer and Information Science Department University of Pennsylvania  IRISA Rennes, France  Laboratoire MASI Universite de  
Abstract: Self-scheduling is a method for task scheduling in parallel programs, in which each processor acquires a new block of tasks for execution whenever it becomes idle. To get the best performance, the block size must be chosen to balance the scheduling overhead against the load imbalance. To determine the best block size, a better understanding of the role of load imbalance in self-scheduling performance is needed. In this paper we study the effect of memory contention on task duration distributions and, hence, load balancing in self-scheduling on a Non Uniform Memory Access (NUMA) machine. Experimental studies on a BBN TC2000 are used to reveal the strengths and weaknesses of analytical performance models to predict running time and optimal block size. The models are shown to be very accurate for small block sizes. However, the models fail when the block size is large due to a previously unrecognized source of load imbalance. We extend the analytical models to address this failure. The implications for the construction of compilers and runtime systems are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAuliffe, E. A. Melton, V. A. Norton, and J. Weiss, </author> <title> "The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture.," </title> <booktitle> in Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pp. 764-771, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: 1 Introduction Non-Uniform Memory Access (NUMA) machines are designed to provide the scalability of a distributed memory machine with the ease of programming associated with the shared memory model. Many research and commercial NUMA multiprocessors are currently being designed and built, including RP3 <ref> [1] </ref>, Ultracomputer [2], DASH [3], Hector [4], KSR [5], Cedar [6] and the BBN Butterfly machines [7]. On these machines, each processing element includes a processor and some local memory. The processors are linked by an interconnection network.
Reference: [2] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. M. e, L. Rudolph, and M. Snir, </author> <title> "The NYU Ultracomputer - Designing an MIMD Shared Memory Parallel Computer," </title> <journal> IEEE Transactions on Computers, </journal> <pages> pp. 175-189, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: 1 Introduction Non-Uniform Memory Access (NUMA) machines are designed to provide the scalability of a distributed memory machine with the ease of programming associated with the shared memory model. Many research and commercial NUMA multiprocessors are currently being designed and built, including RP3 [1], Ultracomputer <ref> [2] </ref>, DASH [3], Hector [4], KSR [5], Cedar [6] and the BBN Butterfly machines [7]. On these machines, each processing element includes a processor and some local memory. The processors are linked by an interconnection network.
Reference: [3] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy, </author> <title> "The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor," </title> <booktitle> in Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <address> (Seattle, Washington), </address> <pages> pp. 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Non-Uniform Memory Access (NUMA) machines are designed to provide the scalability of a distributed memory machine with the ease of programming associated with the shared memory model. Many research and commercial NUMA multiprocessors are currently being designed and built, including RP3 [1], Ultracomputer [2], DASH <ref> [3] </ref>, Hector [4], KSR [5], Cedar [6] and the BBN Butterfly machines [7]. On these machines, each processing element includes a processor and some local memory. The processors are linked by an interconnection network.
Reference: [4] <author> D. M. L. Z.G. Vranesic, M. Stumm and R. White, "Hector: </author> <title> A Hierarchically Structured Shared Memory Multiprocessor," </title> <journal> Computer, </journal> <volume> vol. 24, </volume> <pages> pp. 73-82, </pages> <month> Jan. </month> <year> 1991. </year> <note> [5] "KSR Parallel Programming," </note> <year> 1991. </year>
Reference-contexts: 1 Introduction Non-Uniform Memory Access (NUMA) machines are designed to provide the scalability of a distributed memory machine with the ease of programming associated with the shared memory model. Many research and commercial NUMA multiprocessors are currently being designed and built, including RP3 [1], Ultracomputer [2], DASH [3], Hector <ref> [4] </ref>, KSR [5], Cedar [6] and the BBN Butterfly machines [7]. On these machines, each processing element includes a processor and some local memory. The processors are linked by an interconnection network.
Reference: [6] <author> D. Gajski, D. Kuck, D. Lawrie, and A. Sameh, </author> <title> "Cedar A large Scale Multiprocessor," </title> <booktitle> in Proceedings of the 1983 International Conference on Parallel Processing, </booktitle> <pages> pp. 524-9, </pages> <month> Aug. </month> <year> 1983. </year> <title> [7] "Inside the TC2000," </title> <year> 1990. </year>
Reference-contexts: Many research and commercial NUMA multiprocessors are currently being designed and built, including RP3 [1], Ultracomputer [2], DASH [3], Hector [4], KSR [5], Cedar <ref> [6] </ref> and the BBN Butterfly machines [7]. On these machines, each processing element includes a processor and some local memory. The processors are linked by an interconnection network. A global address space allows the programmer to view the physically distinct local memories as a single shared memory.
Reference: [8] <author> S. F. Hummel, E. Schonberg, and L. E. Flynn, </author> <title> "Factoring: A Practical and Robust Method for Scheduling Parallel Loops," </title> <booktitle> in 1991 Supercomputing Conference, </booktitle> <pages> pp. 610-619, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Together with fixed block size self-scheduling, they form a family of algorithms with similar structure and behavior. A better understanding of one member of the family can yield new insights into others. For example, the mathematical analysis that led to the development of the decreasing block size scheme Factoring <ref> [8] </ref> was based on an analytical model of fixed block size self-scheduling [9]. 2 The problem in designing a decreasing block size self-scheduling algorithm is to determine how large the first block allocated should be and at what rate to reduce the size of subsequent blocks so as to minimize overhead <p> The size of the first block and the rate of decrease depend on the values of the parameters. TSS is also designed to have faster queue accesses than GSS. A third method, Factoring <ref> [8] </ref>, is based on a mathematical analysis to determine the expected amount of work that can be assigned to each block and still, with high probability, leave enough tasks to smooth over the uneveness in finishing times. <p> How to choose the best values of the TSS parameters is not well understood. There is also experimental evidence to suggest that if the block size is chosen correctly, fixed block size self-scheduling can perform comparably to decreasing block size self-scheduling. For example, Hummel et al. <ref> [8] </ref> and Tzen and Ni [11] compared Factoring and TSS, respectively, with self-scheduling using K = 1 ("self scheduling" or SS) and K = N=p ("chunk or static scheduling" or CSS). <p> A 30% performance improvement in the entire workload can substantially improve the throughput on the entire machine. Our results are important for the development of algorithms with decreasing block size as well. As seen in the work of Hummel et al. <ref> [8] </ref>, an understanding of the variance of task durations on NUMA machines is necessary to determine the optimal rate of decrease. 12 7 Acknowledgments We wish to thank to Christine Fricker, Susan Flynn Hummel, Annie Morin, Philippe Robert, Gerardo Rubino and Paul Tukey for helpful discussions.
Reference: [9] <author> C. Kruskal and A. Weiss, </author> <title> "Allocating Independent Subtasks on Parallel Processors," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 11, </volume> <pages> pp. 1001-16, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: A better understanding of one member of the family can yield new insights into others. For example, the mathematical analysis that led to the development of the decreasing block size scheme Factoring [8] was based on an analytical model of fixed block size self-scheduling <ref> [9] </ref>. 2 The problem in designing a decreasing block size self-scheduling algorithm is to determine how large the first block allocated should be and at what rate to reduce the size of subsequent blocks so as to minimize overhead without incurring a load imbalance. <p> the durations of its constituent tasks, the distribution function of the block duration is simply the K-fold convolution of the distribution function of the tasks, G = F flK . 3.1 Kruskal and Weiss The earliest and most comprehensive model of fixed-size block scheduling was presented by Kruskal and Weiss <ref> [9] </ref>. Their analysis is based on splitting the program execution into two distinct phases and estimating the running time of each phase separately. This allows the authors to analyze the body of the computation and the load imbalance at the end separately.
Reference: [10] <author> C. Polychronopoulos and D. Kuck, </author> <title> "Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 36, no. 12, </volume> <pages> pp. 1425-39, </pages> <year> 1987. </year> <note> Special Issue on Supercomputing. </note>
Reference-contexts: In Guided Self-Scheduling (GSS) <ref> [10] </ref>, each time a processor becomes idle, it acquires 1=pth of the remaining tasks, where p is the number of processors.
Reference: [11] <author> T. H. Tzen and L. M. Ni, </author> <title> "Trapezoid Self-Scheduling: A Practical Scheduling Scheme for Parallel Compilers," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 4, </volume> <pages> pp. 87-98, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: For some other distributions, particularly those with decreasing task durations, GSS allots too much work at the beginning of the schedule, leaving too few tasks to smooth out unevenness at the end. To address this problem, Trapezoidal Self-Scheduling (TSS) <ref> [11] </ref> uses a parameterized scheme where the block size decreases linearly rather than exponentially. The size of the first block and the rate of decrease depend on the values of the parameters. TSS is also designed to have faster queue accesses than GSS. <p> There is also experimental evidence to suggest that if the block size is chosen correctly, fixed block size self-scheduling can perform comparably to decreasing block size self-scheduling. For example, Hummel et al. [8] and Tzen and Ni <ref> [11] </ref> compared Factoring and TSS, respectively, with self-scheduling using K = 1 ("self scheduling" or SS) and K = N=p ("chunk or static scheduling" or CSS). <p> In all experiments, either SS or CSS ran as well as or better than GSS. In <ref> [11] </ref>, one of the fixed block methods (either CSS or SS) performed comparably to TSS and better than GSS in all experiments except one.
Reference: [12] <author> E. Markatos and T. LeBlanc, </author> <title> "Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors," </title> <booktitle> in Proceedings of the 1992 Supercomputing Conference, (Minneapolis), </booktitle> <pages> pp. 104-13, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: A simplified version of Factoring that is independent of and has been shown to work well in practise. Factoring is the only scheme supported by a comprehensive mathematical model. In addition, two decreasing block size algorithms have been presented that take data locality into account, Affinity Scheduling <ref> [12] </ref> (AFS) and Locality-based Dynamic Scheduling [13] (LDS). In both schemes, a subset of the tasks is placed in a local queue on each processor. The data needed by those tasks is stored on the same processor.
Reference: [13] <author> H. Li, S. Tandri, M. Stumm, and K. Sevcik, </author> <title> "Locality and Loop Scheduling on NUMA Multiprocessors," </title> <booktitle> in Proceedings of the 1993 International Conference on Parallel Processing, pp. </booktitle> <volume> II 140 II 127, </volume> <month> Aug. </month> <year> 1993. </year> <month> 13 </month>
Reference-contexts: Factoring is the only scheme supported by a comprehensive mathematical model. In addition, two decreasing block size algorithms have been presented that take data locality into account, Affinity Scheduling [12] (AFS) and Locality-based Dynamic Scheduling <ref> [13] </ref> (LDS). In both schemes, a subset of the tasks is placed in a local queue on each processor. The data needed by those tasks is stored on the same processor. The processor executes its local tasks using a decreasing block size scheme until they are exhausted.
Reference: [14] <author> R. H. Thomas and W. Crowther, </author> <title> "The Uniform System: An approach to runtime support for large scale shared memory parallel processors.," </title> <booktitle> in Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pp. 245-254, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Fixed block size self-scheduling offers a number of other advantages. It is easier to implement and easier to analyze theoretically. It has already been implemented in runtime systems on a number of multiprocessors <ref> [14, 15, 16] </ref>. Commercial compilation systems, such as KSR's PRESTO runtime library [5], often use fixed size blocking. Finally, it is much easier to combine fixed block size self-scheduling algorithms with data locality than decreasing block size schemes.
Reference: [15] <author> R. Butler, J. Boyle, T. Disz, B. Glickfeld, E. Lusk, R. Overbeek, J. Patterson, and R. Stevens, </author> <title> Portable Programs for Parallel Processors. </title> <address> New York: </address> <publisher> Holt, Rinehart and Winston, Inc, </publisher> <year> 1987. </year>
Reference-contexts: Fixed block size self-scheduling offers a number of other advantages. It is easier to implement and easier to analyze theoretically. It has already been implemented in runtime systems on a number of multiprocessors <ref> [14, 15, 16] </ref>. Commercial compilation systems, such as KSR's PRESTO runtime library [5], often use fixed size blocking. Finally, it is much easier to combine fixed block size self-scheduling algorithms with data locality than decreasing block size schemes.
Reference: [16] <author> S. F. Hummel and E. Schonberg, </author> <title> "Low-Overhead Scheduling of Nested Parallelism," </title> <journal> IBM Journal of Research and Development, </journal> <pages> pp. 743-65, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Fixed block size self-scheduling offers a number of other advantages. It is easier to implement and easier to analyze theoretically. It has already been implemented in runtime systems on a number of multiprocessors <ref> [14, 15, 16] </ref>. Commercial compilation systems, such as KSR's PRESTO runtime library [5], often use fixed size blocking. Finally, it is much easier to combine fixed block size self-scheduling algorithms with data locality than decreasing block size schemes.
Reference: [17] <author> S. Madala and J. B. Sinclair, </author> <title> "Performance of Synchronous Parallel Algorithms with Regular Structures," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 105-116, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Analytically tractable expressions for K opt cannot be derived from Equations 3 and 4. 3.2 Madala and Sinclair A similar approach to estimating the running time was presented by Madala and Sinclair <ref> [17] </ref>. Instead of the characteristic maximum, they used a distribution-independent bound, E [maxfX i g] + p1 p 2p1 , to estimate the duration of the load imbalance [18].
Reference: [18] <author> H. David, </author> <title> Order Statistics. </title> <address> New York: </address> <publisher> John Wiley, </publisher> <year> 1981. </year>
Reference-contexts: Instead of the characteristic maximum, they used a distribution-independent bound, E [maxfX i g] + p1 p 2p1 , to estimate the duration of the load imbalance <ref> [18] </ref>. This expression has the advantage of simplicity but is less accurate an 5 estimate of load imbalance for large values of p. <p> Madala and Sinclair's worst performance occurs when the contention is high. This is because the bound they used to estimate load imbalance gives the best approximation when the underlying distribution is a high, narrow exponential <ref> [18] </ref>. As the contention increases, the distribution becomes lower and flatter, resembling a high, narrow exponential less and less. Kruskal and Weiss' third estimate (Equation 4) does quite badly with estimates inaccurate by as much as 37%.
Reference: [19] <author> T. Montaut, </author> <title> "Ordonnancement dynamique d'un programme decompose en sous-t^aches independantes. analyse des performances," </title> <type> Master's thesis, </type> <institution> Institut de Recherche en Informatique et Systemes Aleatoires, </institution> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: ~ K h + E p [F flK ] , (8) as well as an upper bound E [T ] E [T i ] + p b (N 1)=Kc + p h + E p [F flK ] . (9) A complete development of these equations can be found in <ref> [19] </ref>. The first term on the right hand side is the expected processor starting time, where T i is the time processor i starts working.
Reference: [20] <author> M. Dayde, </author> <year> 1991. </year> <title> Private communication. </title>
Reference-contexts: After repeated failures, the length of the delay is increased using a random exponential backoff strategy. Cache coherency has not been implemented for the data cache on the TC2000. It is the programmer's responsibility to implement cache coherency in software. Preliminary experiments <ref> [20] </ref> suggest that for this reason, use of the data cache is not profitable except large matrices or for read-only applications. A number of software packages for parallel programming support are available on the TC2000. The Uniform System (US) supports memory interleaving and self-scheduling.
Reference: [21] <author> F. Bodin, D. Windheiser, W. Jalby, D. Ataputta, M. Lee, and D. Gannon, </author> <title> "Performance Evaluation and Prediction for Parallel Algorithms on BBN GP1000," </title> <booktitle> in Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pp. 403-13, </pages> <year> 1990. </year>
Reference-contexts: Varying the ratio of global to local memory accesses allows us to study tasks with varying degrees of global network activity. Earlier studies <ref> [21, 22] </ref> suggest that local memory accesses are insensitive to network activity. Hence, local memory operations can be treated in the same way as computational instructions. Two types of memory storage strategy were used, interleaved and hotspot. These storage schemes are two extreme ends of a range.
Reference: [22] <author> L. Kervella, </author> <title> "Etude experimentale de la duree des t^aches paralleles avec un ordonnancement dynamique," </title> <type> Master's thesis, </type> <institution> Institut de Recherche en Informatique et Systemes Aleatoires, </institution> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: Varying the ratio of global to local memory accesses allows us to study tasks with varying degrees of global network activity. Earlier studies <ref> [21, 22] </ref> suggest that local memory accesses are insensitive to network activity. Hence, local memory operations can be treated in the same way as computational instructions. Two types of memory storage strategy were used, interleaved and hotspot. These storage schemes are two extreme ends of a range.
Reference: [23] <author> L. S. et K.O Bowman, </author> <title> Maximum Likelihood Estimation in Small Samples. </title> <publisher> Charles Griffin and Company, </publisher> <year> 1977. </year>
Reference-contexts: To compute the fourth term of Equation 8, we determined the probability distribution 9 of the task duration by fitting the histograms presented in the previous section to classical distributions using the maximum likelihood method as described in <ref> [23] </ref>. We tried fitting several distributions to the data for each experiment. The best results were obtained using the exponential distribution to model data from interleaving experiments. Hotspot data were best modeled using a discrete distribution with two peaks.
Reference: [24] <author> V. Sarkar, </author> <title> "Determining Average Program Execution Times and their Variance," </title> <booktitle> in Proceedings of SIG-PLAN 1989 Conference on Programming Language Design and Implementation, </booktitle> <volume> vol. 34, </volume> <pages> pp. 723-732, </pages> <year> 1969. </year>
Reference-contexts: However, given static analysis of the task structure combined with a lookup table of machine dependent information, it is possible to predict the mean and variance of the task duration distribution (see <ref> [24, 25] </ref> for examples.) A new lookup table must be generated experimentally for each architecture, but constructing the table would be a one-time cost. The work of Gallivan et al. [26] on predicting memory latency using load/store kernels suggests how such a table might be constructed.
Reference: [25] <author> T. Fahringer and H. Zima, </author> <title> "A Static Parameter-Based Performance Tool for Parallel Programs," </title> <booktitle> in Proceedings of the 1993 International Conference on Supercomputing, </booktitle> <address> (Tokyo, Japan), </address> <pages> pp. 207-219, </pages> <institution> Association for Computing Machinery, </institution> <year> 1993. </year>
Reference-contexts: However, given static analysis of the task structure combined with a lookup table of machine dependent information, it is possible to predict the mean and variance of the task duration distribution (see <ref> [24, 25] </ref> for examples.) A new lookup table must be generated experimentally for each architecture, but constructing the table would be a one-time cost. The work of Gallivan et al. [26] on predicting memory latency using load/store kernels suggests how such a table might be constructed.

References-found: 23

