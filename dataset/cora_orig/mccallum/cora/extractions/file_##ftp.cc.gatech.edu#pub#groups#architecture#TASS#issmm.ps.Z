URL: file://ftp.cc.gatech.edu/pub/groups/architecture/TASS/issmm.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/Architecture/TASS/tass.html
Root-URL: 
Title: Experimental Evaluation of Algorithmic Performance on Two Shared Memory Multiprocessors  
Author: Anand Sivasubramaniam Gautam Shah Joonwon Lee Umakishore Ramachandran H. Venkateswaran 
Date: 13-24, April 1991.  
Note: In Proceedings of the First International Symposium on Shared Memory Multiprocessing, pages  
Address: Atlanta, GA 30332-0280. USA.  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: The results of experimenting with three parallel algorithms on the Sequent Symmetry architecture and the BBN Butterfly architecture are reported. The main objective of this study is to understand the impediments to the efficient implementation of parallel algorithms, developed for theoretical models of parallel computation, on realistic parallel architectures. Scheduling, task granularity, and synchronization are the issues that are explored in implementing these algorithms on the two architectures. In the case of BBN Butterfly, which is a distributed shared memory architecture,data distribution in the distributed memories is also studied. The key findings are that synchronization is not a significant cost for the algorithms we studied on the two architectures; the bus is not a bottleneck for the configuration of the Sequent machine that we experimented with; and a fairly simple minded data distribution may be as good as any other on the BBN Butterfly. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.V.Aho, J.E.Hopcroft, J.D.Ullman. </author> <title> The Design and Analysis of Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference: [2] <institution> The Uniform System Approach to Programming the Butterfly Parallel Processor. BBN Advanced Computers Inc., Massachusetts. </institution> <year> 1986. </year>
Reference-contexts: Both machines provide support for parallel programming such as the parallel programming library on the Sequent [11] and the Uniform System <ref> [2] </ref> on the Butterfly. In the description that follows, we use the term task to mean a unit of work and the term process to mean a virtual processor.
Reference: [3] <author> D.Chen, H.Su, P.Yew. </author> <title> The Impact of Synchronization and Granularity on Parallel Systems. </title> <booktitle> International Symposium on Computer Architecture. </booktitle> <year> 1990. </year>
Reference-contexts: Anderson [10] reports results of an experimental and analytical study of parallel merge sort. In this study, implementation of this algorithm on the Sequent is used to verify the speedup with different number of processors with respect to the analytical model. Yew et al. <ref> [3] </ref>, analyze specific parallel programs to identify the appropriate grain size of parallelism that exists in these programs. Further they present a simulation study to measure the impact of synchronization overhead on the execution of these programs.
Reference: [4] <author> R.M.Karp, V.Ramachandran. </author> <title> A Survey of Parallel Algorithms for Shared-Memory Machines. </title> <type> Technical Report UCB-CSD-88-408. </type> <institution> UC Berkeley. </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: Our work is more general in that we experiment with algorithms that represent classes of problems and study synchronization, scheduling and task granularity issues in implementing these algorithms. 4 Observed Results 4.1 List Ranking A parallel algorithm for the list ranking problem is discussed in <ref> [14, 4] </ref>. Figure 1 shows a pseudo-code for this algorithm. The algorithm is data dependent. The randomness of access of the list elements does not favor data partitioning. Therefore, it is not known a priori the best way to partition the data and assign it to the processors.
Reference: [5] <author> H.T.Kung. </author> <title> The Structure of Parallel Algorithms. </title> <booktitle> Advances In Computers. </booktitle> <volume> Vol. 19, </volume> <year> 1980. </year>
Reference: [6] <author> R.E.Ladner, M.J.Fischer. </author> <title> Parallel Prefix Computation. </title> <journal> Journal of Association of Computing Machinery. </journal> <volume> Vol 27, No. 4, </volume> <month> October </month> <year> 1980. </year>
Reference-contexts: This observation reiterates the fact that the bus on the Sequent is a much more shared resource than the switch on the Butterfly. 4.2 Parallel Prefix An algorithm for the parallel prefix problem is discussed in <ref> [6] </ref>, and Figure 10 gives the pseudo code for this algorithm. The algorithm is data oblivious. Each phase of the parallel part can be performed only after all processors in the previous phase have completed their task.
Reference: [7] <author> J.Lee, U.Ramachandran. </author> <title> Synchronization with Multiprocessor Caches. </title> <booktitle> International Symposium on Computer Architecture. </booktitle> <year> 1990. </year>
Reference-contexts: Extensive contention for the globally shared queue is the reason for this behavior. The bus does become a bottleneck in this case and hence this poor performance is quite understandable. This experimental result corroborates the simulation result reported in <ref> [7] </ref>, wherein they show that lock contention leads to poor performance in bus-based shared memory multiprocessors. It is also the reason why the 8 processor performance is much poorer than the 4 or 2 processor case for extremely fine grain data granularity.
Reference: [8] <author> C.Lin, L.Snyder. </author> <title> A Comparison of Programming Models for Shared Memory Multiprocessors. </title> <booktitle> International Conference on Parallel Processing. </booktitle> <year> 1990. </year>
Reference-contexts: Yew et al. [3], analyze specific parallel programs to identify the appropriate grain size of parallelism that exists in these programs. Further they present a simulation study to measure the impact of synchronization overhead on the execution of these programs. Lin and Snyder <ref> [8] </ref> compare message passing and shared memory paradigms for implementing specific parallel algorithms on shared memory multiprocessors.
Reference: [9] <author> S.T.Leutenegger, M.K.Vernon. </author> <title> The Performance of Multiprogrammed Multiprocessor Scheduling Algorithms. </title> <booktitle> Conference on Measurement and Modeling of Computer Systems. </booktitle> <year> 1990. </year>
Reference-contexts: Each of these data distributions generates differing network traffic. Such a study is expected to give us a feel for the effect of the switch contention and remote memory accesses on the performance. The corresponding effect for the Sequent 1 Recently, several researchers <ref> [9, 13, 15] </ref> investigate the relative merits of dynamic and static scheduling policies at the operating system and application level for multiprocessors.
Reference: [10] <author> R.J.Anderson. </author> <title> An Experimental Study of Parallel Merge Sort. </title> <note> Preliminary Version, </note> <institution> 0.1. University of Washington. </institution> <year> 1990. </year>
Reference-contexts: Hence the speedup using n processors refers to the ratio of the completion time of the parallel algorithm on 1 processor to that on n processors. 3 Related Work To our knowledge, there are very few experimental studies that investigate the impact of architectural features on algorithmic performance. Anderson <ref> [10] </ref> reports results of an experimental and analytical study of parallel merge sort. In this study, implementation of this algorithm on the Sequent is used to verify the speedup with different number of processors with respect to the analytical model.
Reference: [11] <institution> Sequent Guide to Parallel Programming. Sequent Computer Systems Inc., Oregon. </institution> <year> 1987. </year>
Reference-contexts: Both machines provide support for parallel programming such as the parallel programming library on the Sequent <ref> [11] </ref> and the Uniform System [2] on the Butterfly. In the description that follows, we use the term task to mean a unit of work and the term process to mean a virtual processor.
Reference: [12] <author> T.E.Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions On Parallel And Distributed Systems. </journal> <month> January, </month> <year> 1990. </year>
Reference-contexts: As the computation granularity is increased further, the effect of contention becomes less significant which explains the observed result. The contention effect is quite similar to what is explained as quiesce 2 time in <ref> [12] </ref>.
Reference: [13] <author> A.Tucker, A.Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared Memory Multiprocessors. </title> <booktitle> 12th ACM Symposium on Operating System Principles. </booktitle> <year> 1989. </year>
Reference-contexts: Each of these data distributions generates differing network traffic. Such a study is expected to give us a feel for the effect of the switch contention and remote memory accesses on the performance. The corresponding effect for the Sequent 1 Recently, several researchers <ref> [9, 13, 15] </ref> investigate the relative merits of dynamic and static scheduling policies at the operating system and application level for multiprocessors.
Reference: [14] <author> J.C.Wyllie. </author> <title> The Complexity of Parallel Computations. </title> <type> PhD Dissertation. </type> <institution> Computer Science Dept., Cornell Univ., Ithaca, NY,1981. </institution>
Reference-contexts: Our work is more general in that we experiment with algorithms that represent classes of problems and study synchronization, scheduling and task granularity issues in implementing these algorithms. 4 Observed Results 4.1 List Ranking A parallel algorithm for the list ranking problem is discussed in <ref> [14, 4] </ref>. Figure 1 shows a pseudo-code for this algorithm. The algorithm is data dependent. The randomness of access of the list elements does not favor data partitioning. Therefore, it is not known a priori the best way to partition the data and assign it to the processors.

References-found: 14

