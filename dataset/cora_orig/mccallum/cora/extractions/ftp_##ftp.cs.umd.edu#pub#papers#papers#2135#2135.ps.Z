URL: ftp://ftp.cs.umd.edu/pub/papers/papers/2135/2135.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Communication and Matrix Computations on Large Message Passing Systems  
Author: G. W. Stewart 
Note: 20742. This work was supported in part by Air Force Office of Sponsored Research under grant AFOSR-82-0078  
Address: College Park, MD  
Affiliation: Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland,  
Date: October 1988  Revised Jan. 1990  
Pubnum: UMIACS-TR-88-81  CS-TR-2135  
Abstract: This paper is concerned with the consequences for matrix computations of having a rather large number of general purpose processors, say ten or twenty thousand, connected in a network in such a way that a processor can communicate only with its immediate neighbors. Certain communication tasks associated with most matrix algorithms are defined and formulas developed for the time required to perform them under several communication regimes. The results are compared with the times for a nominal n 3 floating point operations. The results suggest that it is possible to use a large number of processors to solve matrix problems at a relatively fine granularity, provided fine grain communication is available. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Annoratone, E. Arnould, T. Gross, H. T. Kung, O. Menzicioglu, and J. A. </author> <title> Webb (1987). "The warp Computer: Architecture, Implementation, and Performance." </title> <journal> IEEE Transaction on Computers, </journal> <volume> C-36, </volume> <pages> 1523-1538. </pages>
Reference-contexts: This simple streaming, as we will call it, amounts to packet streaming with a negligible startup time and a packet size of one. Simple streaming requires special hardware for its implementation. The warp <ref> [1] </ref> and the transputer, which provide hardware for synchronizing the delivery of small messages, can perform simple streaming. Moreover, a class of architectures in which neighboring processors can access each other's memories has been proposed by the author and his colleagues [14].
Reference: [2] <author> C. Bischof and C. Van Loan (1987). </author> <title> "The W Y Representation for Products of Householder Transformations." </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8, </volume> <month> s2-s13. </month>
Reference-contexts: In describing the communication tasks we have restricted ourselves to conventional matrix algorithms. For our purposes, the most important alternative is block algorithms, which are currently under investigation in connection with hierarchical shared memories <ref> [2, 4] </ref>. These certainly deserve serious consideration, but at present block algorithms for the full range of matrix computations do not exist.
Reference: [3] <author> S. C. Bruell and G. </author> <title> Balbo (1980). Computational Algorithms for Closed Queueing Networks. </title> <publisher> North Holand, </publisher> <address> New York. </address>
Reference-contexts: Communication and Matrix Computations 3 e.g., matrices corresponding to multi-dimensional meshes. The matrices arising from queuing networks are of this kind <ref> [3] </ref>. A third, perhaps less cogent answer is that dense matrix problems have become an important benchmark for commercial systems; those that do not perform well on them are at a disadvantage in the marketplace.
Reference: [4] <author> J. J. Dongarra, J. Du Croz, I. Duff, and S. </author> <title> Hammarling (1987). "A Proposal for a Set of Level 3 Basic Linear Algebra Subprograms." </title> <journal> SIGNUM Newsletter, </journal> <volume> 22, </volume> <pages> 2-14. </pages>
Reference-contexts: In describing the communication tasks we have restricted ourselves to conventional matrix algorithms. For our purposes, the most important alternative is block algorithms, which are currently under investigation in connection with hierarchical shared memories <ref> [2, 4] </ref>. These certainly deserve serious consideration, but at present block algorithms for the full range of matrix computations do not exist.
Reference: [5] <author> S. C. Eisenstat, M. T. Heath, C. S. Henkel, and C. H. </author> <title> Romine (1988). "Modified Cyclic Algorithms for Solving Triangular Systems on Distributed-Memory Multiprocessors." </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9, </volume> <pages> 589-600. </pages>
Reference-contexts: The combination of packet streaming with either simple reduction or wavefront reduction is quite inefficient unless n is large or p is small. This is confirmed by what people have observed experimentally on message passing systems (e.g., see <ref> [7, 6, 5] </ref>). For wavefront reduction the generation step is almost entirely responsible for the large communication time; for when it is removed (gr/pr/ps) the bold line moves very near the p-axis. The combination of simple reduction and simple streaming is almost as good.
Reference: [6] <author> G. C. Fox, S. W. Otto, and A. J. G. </author> <title> Hey (1987). "Matrix Algorithms on a Hypercube I: Matrix Multiplication." </title> <journal> Parallel Computing, </journal> <volume> 4, </volume> <month> 17-31. </month> <title> 18 Communication and Matrix Computations </title>
Reference-contexts: In all message passing systems currently available, the overhead for this is large; the best require about 50-100 sec to pass a message between processes (e.g., see <ref> [6] </ref>). Since in parallel algorithms for matrix computations data must be passed between programs, we will assume that any communication has a startup time , which is about 100 sec. Specifically we will assume that it requires time + kt (2:2) to transmit a message of length k between programs. <p> The combination of packet streaming with either simple reduction or wavefront reduction is quite inefficient unless n is large or p is small. This is confirmed by what people have observed experimentally on message passing systems (e.g., see <ref> [7, 6, 5] </ref>). For wavefront reduction the generation step is almost entirely responsible for the large communication time; for when it is removed (gr/pr/ps) the bold line moves very near the p-axis. The combination of simple reduction and simple streaming is almost as good.
Reference: [7] <author> A. George, M. T. Heath, and J. </author> <title> Liu (1986). "Parallel Cholesky Factorization on a Shared-Memory Multiprocessor." </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 77, </volume> <pages> 165-187. </pages>
Reference-contexts: The combination of packet streaming with either simple reduction or wavefront reduction is quite inefficient unless n is large or p is small. This is confirmed by what people have observed experimentally on message passing systems (e.g., see <ref> [7, 6, 5] </ref>). For wavefront reduction the generation step is almost entirely responsible for the large communication time; for when it is removed (gr/pr/ps) the bold line moves very near the p-axis. The combination of simple reduction and simple streaming is almost as good.
Reference: [8] <author> G. H. Golub and C. F. Van Loan (1983). </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland. </address>
Reference-contexts: Let us examine these two steps in greater detail. A typical example of a generation step is the computation of multipliers in Gaussian elimination with partial pivoting (for a description of Gaussian elimination and related reductions see <ref> [8] </ref>). Here the maximum element of the current Communication and Matrix Computations 5 u 1 u 3 u 5 - - pivot column must be determined. This is then distributed to to processors responsible for the pivot column, who use it to compute the multipliers.
Reference: [9] <author> J. L. Gustafson, G. R. Montry, and R. E. </author> <title> Benner (1988). "Development of Parallel Methods for a 1024-Processor Hypercube." </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9, </volume> <pages> 609-638. </pages>
Reference-contexts: It is true that the ratio of computation to communication decreases with the problem size, so that if the problem is large enough we can attain high efficiency (for an example of this phenomena in another context, see <ref> [9] </ref>). But will a problem that is large enough to be solved efficiently be too large to be solved in a reasonable time? In this paper we shall investigate this question empirically by what amounts to an elaborate back-of-the-envelope calculation.
Reference: [10] <author> N. J. </author> <title> Higham (1989). "Exploiting Fast Matrix Multiplication within the Level 4 BLAS." </title> <type> Technical Report 89-984, </type> <institution> Department of Computer Science, Cornell University. </institution> <note> To appear in ACM Tran. Math. Soft.. </note>
Reference-contexts: Another alternative is "fast" matrix algorithms that run in time less that O (n 3 ) (for a survey see [15]). Unfortunately, the numerical properties of these algorithms is so little understood, that at present they are not serious contenders (however, see <ref> [10] </ref>). We have also understated the amount of broadcasting involved in typical algorithms. For example, in Gaussian elimination, the pivot row must also be broadcast. But this represents factors of two or three in an application where we are concerned with orders of magnitude.
Reference: [11] <author> S. Lennart Johnsson and Ching-Tien Ho (1986). </author> <title> "Spanning Graphs for Optimum Broadcasting and Personalized Communication in Hypercubes." </title> <institution> Research Report YALE/DCS/RR-500, Department of Computer Science, Yale University. </institution>
Reference-contexts: When the processors form a hypercube, we embed a grid in the hypercube in such a way that each row and each column lies on a sub-hypercube <ref> [12, 11] </ref>. We then assign the matrix to the embedded grid as described above. This insures that the distance between two processors in any row or column is not greater than log 2 p=2. 2.3.
Reference: [12] <author> S. Lennart Johnsson and Ching-Tien Ho (1987). </author> <title> "Matrix Multiplication on Boolean Cubes Using Generic Communication Primitives." </title> <institution> Research Report YALE/DCS/RR-530, Department of Computer Science, Yale University. </institution>
Reference-contexts: When the processors form a hypercube, we embed a grid in the hypercube in such a way that each row and each column lies on a sub-hypercube <ref> [12, 11] </ref>. We then assign the matrix to the embedded grid as described above. This insures that the distance between two processors in any row or column is not greater than log 2 p=2. 2.3.
Reference: [13] <author> Cleve Moler (1982). </author> <title> "matlab Users' Guide." </title> <type> Technical Report CS81-1 (Revised), </type> <institution> Department of Computer Science, University of New Mexico. </institution>
Reference-contexts: With simple streaming, we can implement virtually all. The plots show that there is not much to decide between a grid and a hypercube, at least if the latter has simultaneous fan-out. The ratios for the hypercube 2 Jack Dongarra (personal communication) has used matlab <ref> [13] </ref> to estimate the order constants for a number of matrix algorithms. Communication and Matrix Computations 17 are better, but not markedly so. However, this is in part due to the grid-like nature of dense matrix computations, and these plots should not be used to argue for grids against hypercubes.
Reference: [14] <author> D. P. O'Leary, Roger Pierson, G. W. Stewart, and Mark Wieser (1986). </author> <title> "The Maryland crab: A Module for Building Parallel Computers." </title> <type> Technical Report TR-1660, </type> <institution> Department of Computer Science, University of Maryland. </institution>
Reference-contexts: The warp [1] and the transputer, which provide hardware for synchronizing the delivery of small messages, can perform simple streaming. Moreover, a class of architectures in which neighboring processors can access each other's memories has been proposed by the author and his colleagues <ref> [14] </ref>. Here simple streaming can be done under program control with copy loops synchronized by shared variables. 2.5. Discussion Since we cannot compute everything, we have had to be selective in making assumptions. In this subsection, we will discuss some of the alternatives.
Reference: [15] <author> V. </author> <title> Pan (1984). How to Multiply Matrices Faster. </title> <publisher> Springer, </publisher> <address> New York. </address>
Reference-contexts: Another alternative is "fast" matrix algorithms that run in time less that O (n 3 ) (for a survey see <ref> [15] </ref>). Unfortunately, the numerical properties of these algorithms is so little understood, that at present they are not serious contenders (however, see [10]). We have also understated the amount of broadcasting involved in typical algorithms. For example, in Gaussian elimination, the pivot row must also be broadcast.
Reference: [16] <author> G. W. </author> <title> Stewart (1987). "A Parallel Implementation of the QR Algorithm." </title> <journal> Parallel Computing, </journal> <volume> 5, </volume> <pages> 187-196. </pages>
Reference-contexts: In fact simple streaming is a special case of a more general mode of communication and computation in which items are actually manipulated as they pass from processor to processor. This permits the efficient implementation on mimd systems of algorithms that are essentially systolic (for an example see <ref> [16] </ref>). It should be remarked that this kind of communication is not incompatible with other forms of data routing; on the contrary they should be regarded as supplementary.
Reference: [17] <author> J. E. </author> <title> Van Ness (1986). "Examples of Eigenvalue/vector Use in Electric Power System Problems." </title> <editor> In J. Cullum and R. A. Willoughby, editors, </editor> <booktitle> Large Scale Eigenvalue Problems, </booktitle> <pages> pages 181-192, </pages> <address> Amsterdam. </address> <month> Elsiveir. </month> <title> Communication and Matrix Computations 19 </title>
Reference-contexts: The answer is twofold. In the first place in some applications it is necessary to compute all the eigenvalues of a matrix <ref> [17] </ref>, and the only known way to do this in general is to use dense matrix techniques.
References-found: 17

