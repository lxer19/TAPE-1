URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/92/tr1117.ps.Z
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/tech_reports/tech_reports.html
Root-URL: 
Title: Hardware Support for Synchronization in the Scalable Coherent Interface (SCI)  
Author: Nagi M. Aboulenein Stein Gjessing James R. Goodman Philip J. Woest 
Date: November 3, 1992  
Address: Madison, Wisconsin 53706  Oslo, Norway  Madison, Wisconsin 53706  Evanston, Illinois 60208-3118  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  Department of Informantics University of Oslo  Computer Sciences Department University of Wisconsin-Madison  Department of EECS Northwestern University  
Abstract: The exploitation of the inherent parallelism in applications depends critically on the efficiency of the synchronization and data exchange primitives provided by the hardware. This paper discusses and analyses such primitives as they are implemented in a pending IEEE standard 1596 for communication in a shared memory multiprocessor, the Scalable Coherent Interface (SCI). The SCI synchronization primitives are based on QOLB (originally called QOSB), a hardware primitive previously presented at ASPLOS-III, that shows much promise for reducing and/or eliminating the latencies associated with synchronizing on and accessing shared data. Introducing finer-grained programs in the absence of such latency reduction will likely have little or no benefit. In particular, we discuss how QOLB fits the underlying linked-list cache coherence protocol of SCI. We also present and analyze two important scenarios showing that the QOLB primitives in SCI greatly reduce data communication latencies. These scenarios include critical sections, and the special case of pairwise-sharing. In addition, a number of other issues affecting correctness and performance are discussed, including cache line rollout, graceful initialization, process migration, and forward progress. 
Abstract-found: 1
Intro-found: 1
Reference: [AB86] <author> J. Archibald and J.-L. Baer. </author> <title> "Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model". </title> <journal> Transactions on Computer Systems, </journal> <pages> pages 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: This situation is called "pairwise sharing" . In shared-memory multiprocessors the one-writer/many-readers paradigm of sharing may be enforced by a cache coherence protocol. Archibald and Baer describe and evaluate a number of protocols with this property <ref> [AB86] </ref>. A limitation with coherence protocols is that they only guarantee consistency for a single writable entity, and only for the duration of a single write operation. Most parallel applications require exclusive access to sets of related variables during a number of consecutive read and write operations.
Reference: [And90] <author> T. E. Anderson. </author> <title> "The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: One mechanism that shows promise for reducing and/or eliminating all three of these latencies is the QOLB primitive [GVW89]. In this paper we explore hardware support for locks, critical sections and data exchange using QOLB. Even though software solutions exist for this type of synchronization <ref> [MCS91, And90] </ref>, one of the main points of this paper is that there is a significant benefit in having such hardware support for reducing memory latency. Specifically we discuss the implementation of QOLB in a shared-memory multiprocessor using the Scalable Coherent Interface (SCI). <p> Two software algorithms inspired by QOLB have been developed to minimize network contention for a lock. Each of these implements queues as software-maintained data structures. Anderson <ref> [And90] </ref> presents a scheme to implement a queue as a circular array. Mellor-Crummey and Scott [MCS91], on the other hand, chose to implement a queue as a linked list. Both algorithms succeed in reducing traffic across the interconnect to a constant number of traversals per lock access. <p> Process migration likely cannot be controlled by the user; however, it is usually infrequent. If forward progress must be guaranteed, a program can still use the QOLB/UnQOLB primitives, monitoring its behavior appropriately, and switch to less efficient, software synchronization techniques <ref> [And90, MCS91] </ref> to guarantee progress. Since such behavior is expected to happen relatively rarely, this solution will not impair performance in any appreciable way. <p> In the absence of contention, a simple spin-lock employing the Test&Set primitive is sufficient to guarantee mutual exclusion. However, if a number of processes are competing for entry to a critical section then providing for queues in software may be more appropriate. The latter methods include algorithms by Anderson <ref> [And90] </ref> and by Mellor-Crummey and Scott [MCS91]. For all of these algorithms access to a critical section is controlled by a lock. In this section memory latency and interconnect bandwidth usage are compared for the SCI implementations of QOLB, Mellor-Crummy and Scott (MCS), and Anderson locks. <p> For the case of pairwise sharing, where there is no contention for the lock, Test&Set spin-locks are also included. The main results are sum 16 marized in table 1 at the end of the section. The reader is referred elsewhere for the details of the software queueing algorithms <ref> [And90, MCS91] </ref>. 6.1 Critical Sections A critical section consists of setting a lock, acquiring shared data, performing some computation, and releasing the lock. Any differences between synchronization mechanisms may show up in all but the computation portion. <p> As a first step to investigate this hypothesis we have in this paper analyzed two of the most important concurrent programming scenarios. We have compared the SCI-QOLB implementation to two software schemes <ref> [And90, MCS91] </ref>. The performance of the software schemes is dependent on the coherence protocol of the hardware used, and for our comparison we have assumed they use the base SCI cache coherence protocol.
Reference: [GLL + 90] <author> K. Gharacharloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> "Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors". </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 63-79. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: This delay can be reduced to four by recognizing that P1 need not wait for the unsetting of A lock to complete before attempting to acquire B lock. Thus a relaxed consistency model such as release consistency <ref> [GLL + 90] </ref>, can limit the waiting time to only four network traversals. Consider now how QOLB can reduce the waiting time. Synchronization time can be mostly or entirely overlapped with computation.
Reference: [GVW89] <author> J. R. Goodman, M. K. Vernon, , and P. J. Woest. </author> <title> "A Set of Efficient Synchronization Primitives for a Large-Scale Shared-Memory Multiprocessor". </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III). ACM, </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: As 2 the granularity of sharing becomes finer, these delays may dominate the time to complete a task. One mechanism that shows promise for reducing and/or eliminating all three of these latencies is the QOLB primitive <ref> [GVW89] </ref>. In this paper we explore hardware support for locks, critical sections and data exchange using QOLB. <p> Due to the similarity between QOLB and the SCI implementation of cache coherence, it is natural to extend that implementation to include QOLB, and such an extension is provided as an option to the base SCI protocol. This paper also discusses and analyzes this extension. Previous work <ref> [GVW89] </ref> has described the use of QOLB primarily for eliminating contention over the interconnect. <p> Unfortunately, while Test&Test&Set reduces spinning over the interconnect, contention for a lock may result in unfair allocation and large amounts of interconnect traffic if the lock is held only 3 momentarily. 2.1 QOLB and Hardware Queues The QOLB primitive <ref> [GVW89] </ref> was designed to provide efficient hardware support for critical sections by allowing processes to build distributed hardware queues of waiters for cache lines. 1 A line is the memory entity across which coherency is maintained. <p> This was described in the discussion of cache line rollout. The implementation of the different QOLB operations is given below. We assume that processes are not migrated. This implementation defines QOLB slightly differently from previous work <ref> [GVW89, WG91] </ref>. QOLB (line). A QOLB operation is implemented as a special read operation on the specified line. The operation returns a boolean value: success or failure (respectively 0 and 1). This value is returned at once after a lookup in the cache.
Reference: [IEE91] <author> IEEE Microprocessors Standards Committee. </author> <title> "Scalable Coherent Interface: Logical, Physical and Cache Coherence Specifications". </title> <institution> P1596 Working Group of the Microprocessors Standards Committee, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Specifically we discuss the implementation of QOLB in a shared-memory multiprocessor using the Scalable Coherent Interface (SCI). SCI is a pending standard, designated as IEEE Standards Working Group P1596 <ref> [IEE91] </ref>. SCI is designed to provide an efficient, cache-coherent, shared-memory model to a large number of processing nodes. <p> A low-level logical layer defines arbitration and flow control such that messages are transported reliably and within a predefined time from a source node to a destination node. SCI defines a chained-directory-based, cache-coherence protocol <ref> [IEE91] </ref>. Memory that can be coherently cached is divided into lines that are 64 bytes long. For each line there is a distributed directory that defines the set of nodes whose cache contains a copy of the line. <p> Section 4 summarizes the semantics of the QOLB primitives. A protocol description and implementation of QOLB in SCI can be found in the proposed SCI standards document <ref> [IEE91] </ref>. 3.1 Including QOLB Functionality in the SCI Protocol The general SCI cache coherence protocol defines a sharing list of nodes holding readable copies of a line.
Reference: [JLG + 90] <author> D. V. James, A. T. Laundrie, S. Gjessing, , and G. S. Sohi. </author> <title> "Scalable Coherent Interface". </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 74-77, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The present work discusses QOLB's ability to reduce memory latency by (1) making synchronization common operations more efficient through the elimination of most traversals of the interconnect and (2) by allowing shared data to be prefetched, especially in how this relates to cooperating processes. Similarly, previous discussions of SCI <ref> [JLG + 90] </ref> do not focus on the implementation issues of QOLB or various performance enhancements, such as pairwise sharing. The remainder of the paper is organized as follows. Section 2 presents work related to critical sections, software queues for locks and QOLB. <p> A comparison of QOLB and the software queue techniques in the context of executing critical sections is presented in section 6. 2.2 Scalable Coherent Interface (SCI) SCI is a pending IEEE standard that defines the physical and logical interfaces between modules (called nodes) in a shared-memory multiprocessor <ref> [JLG + 90] </ref>. An SCI multiprocessor can contain up to 64K nodes, each node containing one or more processing elements (with cache), a memory module, a DMA adapter, or a combination of these. Communication between nodes is based upon the sending of messages of two types, requests and responses.
Reference: [Jor83] <author> H. F. Jordan. </author> <title> "Performance Measurements on HEP a Pipelined MIMD Computer". </title> <booktitle> In Proceedings of the 10th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 207-212. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1983. </year>
Reference-contexts: Many shared-memory multiprocessors use locks to enforce access to critical sections. Special atomic synchronization primitives such as "Test&Set" and "Unset" can be provided to acquire and release locks. In addition to critical sections, there are numerous other paradigms for synchronizing processes and sharing data. In barrier synchronization <ref> [Jor83] </ref> a number of processes may wish to guarantee that all have reached a specific point in their execution before any can proceed. As a second example, processes may wish to perform enqueue and dequeue operations in parallel on a queue whose entries represent separate units of work.
Reference: [MCS91] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> "Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors". </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 269-278. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: One mechanism that shows promise for reducing and/or eliminating all three of these latencies is the QOLB primitive [GVW89]. In this paper we explore hardware support for locks, critical sections and data exchange using QOLB. Even though software solutions exist for this type of synchronization <ref> [MCS91, And90] </ref>, one of the main points of this paper is that there is a significant benefit in having such hardware support for reducing memory latency. Specifically we discuss the implementation of QOLB in a shared-memory multiprocessor using the Scalable Coherent Interface (SCI). <p> Two software algorithms inspired by QOLB have been developed to minimize network contention for a lock. Each of these implements queues as software-maintained data structures. Anderson [And90] presents a scheme to implement a queue as a circular array. Mellor-Crummey and Scott <ref> [MCS91] </ref>, on the other hand, chose to implement a queue as a linked list. Both algorithms succeed in reducing traffic across the interconnect to a constant number of traversals per lock access. However, neither allows a lock, or the data associated with a critical section, to be easily prefetched. <p> Process migration likely cannot be controlled by the user; however, it is usually infrequent. If forward progress must be guaranteed, a program can still use the QOLB/UnQOLB primitives, monitoring its behavior appropriately, and switch to less efficient, software synchronization techniques <ref> [And90, MCS91] </ref> to guarantee progress. Since such behavior is expected to happen relatively rarely, this solution will not impair performance in any appreciable way. <p> However, if a number of processes are competing for entry to a critical section then providing for queues in software may be more appropriate. The latter methods include algorithms by Anderson [And90] and by Mellor-Crummey and Scott <ref> [MCS91] </ref>. For all of these algorithms access to a critical section is controlled by a lock. In this section memory latency and interconnect bandwidth usage are compared for the SCI implementations of QOLB, Mellor-Crummy and Scott (MCS), and Anderson locks. <p> For the case of pairwise sharing, where there is no contention for the lock, Test&Set spin-locks are also included. The main results are sum 16 marized in table 1 at the end of the section. The reader is referred elsewhere for the details of the software queueing algorithms <ref> [And90, MCS91] </ref>. 6.1 Critical Sections A critical section consists of setting a lock, acquiring shared data, performing some computation, and releasing the lock. Any differences between synchronization mechanisms may show up in all but the computation portion. <p> As a first step to investigate this hypothesis we have in this paper analyzed two of the most important concurrent programming scenarios. We have compared the SCI-QOLB implementation to two software schemes <ref> [And90, MCS91] </ref>. The performance of the software schemes is dependent on the coherence protocol of the hardware used, and for our comparison we have assumed they use the base SCI cache coherence protocol.
Reference: [MG91] <author> T. Mowry and A. Gupta. </author> <title> "Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors". </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: This limit can be ameliorated to a large extent by prefetching data, at least for networks that provide sufficient bandwidth and programs that are able to request data in advance <ref> [MG91] </ref>. But latency in accessing shared data cannot be entirely hidden data written by another processor cannot be fetched before it is generated. Thus, multiprocessors need to provide efficient mechanisms for the acquisition of shared data.
Reference: [RS84] <author> L. Rudolph and Z. Segall. </author> <title> "Dynamic Decentralized Cache Schemes for MIMD Parallel Processors". </title> <booktitle> In Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1984. </year>
Reference-contexts: Additional latencies in the critical path of an algorithm and unproductive use of interconnect bandwidth, are the result of naive synchronization mechanisms for critical sections. In multiprocessors, the Test&Set operation may give better performance if implemented instead as a Test&Test&Set operation <ref> [RS84] </ref>. This primitive allows processes to spin wait on a locally cached copy of a lock. When the lock is unset all shared copies are invalidated, after which exactly one process will succeed in setting the lock.
Reference: [Sto90] <author> H. S. Stone. </author> <title> "High-Performance Computer Architecture". </title> <publisher> Addison-Wesley, </publisher> <address> 2nd edition, </address> <year> 1990. </year> <pages> Page 362. 21 </pages>
Reference-contexts: As the execution of the program becomes more fine grained i.e. the number of instructions between points of communication is reduced), the sensitivity to access delays on shared data increases. Stone <ref> [Sto90] </ref> introduces the performance measure of MSYPS: Millions of SYnchronizations Per Second. If the amount of processing between synchronizations is comparable to the time to synchronize, then dividing the computation up further (making the granularity finer) will not produce commensurate speedup.
Reference: [WG91] <author> P. J. Woest and J. R. Goodman. </author> <title> "An Analysis of QOLB Synchronization and Prefetching in Shared-Memory Multiprocessors". </title> <booktitle> In Proceedings of the International Symposium on Shared-Memory Multiprocessors, </booktitle> <month> April </month> <year> 1991. </year> <note> Also as Technical Report 1005, </note> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: This was described in the discussion of cache line rollout. The implementation of the different QOLB operations is given below. We assume that processes are not migrated. This implementation defines QOLB slightly differently from previous work <ref> [GVW89, WG91] </ref>. QOLB (line). A QOLB operation is implemented as a special read operation on the specified line. The operation returns a boolean value: success or failure (respectively 0 and 1). This value is returned at once after a lookup in the cache. <p> If prefetching is possible, then QOLB's performance for idle locks improves by a much greater margin. The only latency involved is that for issuing the first QOLB, a second level cache access. These results are very similar to those from a simulation study of a different architecture <ref> [WG91] </ref>. Table 1 also shows results for situations involving pairwise sharing. In this case, the SCI-QOLB implementation produces a sizable increase in performance compared to all of the other algorithms. This benefit is, in all but one case, somewhat smaller for those algorithms that use remote word operations. <p> The results presented in this paper agree well with those of an earlier study <ref> [WG91] </ref> in which the performance benefits of QOLB were quantified for a bus-based architecture. The results demonstrate that there is much to be gained by providing hardware support for critical sections.
Reference: [YTL87] <author> P. C. Yew, N. F. Tzeng, and D. H. Lawrie. </author> <title> "Distributing Hot-Spot Addressing in Large-Scale Multiprocessors". </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 388-395, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Although numerous scenarios exist, it is arguable that only a very few of these would warrant special hardware support. Even for barrier synchronization, which is a primary candidate for hardware support, efficient software solutions exist <ref> [YTL87] </ref>. In this paper, we restrict our attention to efficient support for critical sections. The accessing of a critical section protected, for example, by a lock, results in at least three distinct latencies. First, a process must wait for the critical section to become free (i.e. the lock is unset).
References-found: 13

