URL: ftp://das-ftp.harvard.edu/techreports/tr-24-94.ps.gz
Refering-URL: http://www.deas.harvard.edu/csecse/research/dpf/pic-gather-scatter.html
Root-URL: 
Title: Implementing O(N) N-body algorithms efficiently in data parallel languages (High Performance Fortran)  
Author: Yu Hu S. Lennart Johnsson 
Note: Submitted to the Journal of Scientific Programming.  
Address: Cambridge, Massachusetts  
Affiliation: Parallel Computing Research Group Center for Research in Computing Technology Harvard University  
Date: September 1994  
Pubnum: TR-24-94  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Christopher R. Anderson. </author> <title> An implementation of the fast multipole method without multipoles. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 13(4) </volume> <pages> 923-947, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The optimization techniques for programming hierarchical methods in CMF (HPF) are presented in Sections 5 - 9. Section 10 reports the performance results of our implementation. Section 11 discusses the load-balancing issues and Section 12 summarizes the paper. 2 Hierarchical N -body Methods Hierarchical methods <ref> [1, 4, 11, 9] </ref> for the N -body problem exploit the linearity of the potential field by partitioning the field into two parts, total = nearfield + farfield ; (1) 3 where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles. <p> Anderson's method up to about 4,500 particles, the Barnes-Hut algorithm up to about 6,000 particles, and the Greengard-Rokhlin method for up to about 9,000 particles in three dimensions and with an accuracy of an error decay rate of four in the multipole methods. 2.1 Anderson's multipole method without multipoles Anderson <ref> [1] </ref> used Poisson's formula for representing solutions of Laplace's equation. Let g (x; y; z) denote potential values on a sphere of radius a and denote by the harmonic function external to the sphere with these boundary values. <p> The potential value at ~x is (equation (14) of <ref> [1] </ref>) (~x) = 4 S 2 n=0 a ) n+1 P n (~s i ~x p ) g (a~s)ds; (2) where the integration is carried out over S 2 , the surface of the unit sphere, and P n is the nth Legendre function. <p> Given a numer&lt;ical formula for integrating functions on the surface of the sphere with K integration points ~s i and weights w i , the following formula (equation (15) of <ref> [1] </ref>) is used to approximate the potential at ~x: (~x) i=1 n=0 a ) n+1 P n (~s i ~x p ) g (a~s i )w i : (3) This approximation is called an outer-sphere approximation. <p> Note that in this approximation the series is truncated and the integral is evaluated with a finite number of terms. The approximation used to represent potentials inside a given region of radius a is (equation (16) of <ref> [1] </ref>) (~x) i=1 n=0 r ) n+1 P n (~s i ~x p ) g (a~s i )w i (4) and is called an inner-sphere approximation. The outer-sphere and the inner-sphere approximations define the computational elements in Anderson's hierarchical method.
Reference: [2] <author> J. M. Anderson and M. S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Efficient memory management is the most challenging issue in high performance computing. Techniques for the automatic determination of data distributions with balanced load and efficient communication have been the focus of parallel compiler research in the last several years (for example, see <ref> [2, 20, 34] </ref>). However, no general technique that balances load and generates efficient communication has emerged so far. <p> On the Connection Machine systems CM-5/5E [29], the communication time for the straightforward use of CMF for interactive field computations is more than one order of magnitude higher than the communication time for the technique we use. In Anderson's version of the multipole method <ref> [2] </ref>, all translation operators can be represented by matrices acting on vectors. Moreover, the translation matrices are scale invariant and only depend upon the relative locations of source and destination. <p> Though each of the eight children of a parent requires 875 matrices, the siblings share many matrices. The interactive-field boxes of the eight siblings have offsets in the range [5 + i; 4 + i] fi [5 + j; 4 + j] fi [5 + k; 4 + k]n <ref> [2; 2] </ref> fi [2; 2]fi [2; 2]; i; j; k 2 f0; 1g, respectively. For illustration, see Figure 11. Each offset corresponds to a different translation matrix. <p> The interactive-field boxes of the eight siblings have offsets in the range [5 + i; 4 + i] fi [5 + j; 4 + j] fi [5 + k; 4 + k]n <ref> [2; 2] </ref> fi [2; 2]fi [2; 2]; i; j; k 2 f0; 1g, respectively. For illustration, see Figure 11. Each offset corresponds to a different translation matrix. <p> The interactive-field boxes of the eight siblings have offsets in the range [5 + i; 4 + i] fi [5 + j; 4 + j] fi [5 + k; 4 + k]n <ref> [2; 2] </ref> fi [2; 2]fi [2; 2]; i; j; k 2 f0; 1g, respectively. For illustration, see Figure 11. Each offset corresponds to a different translation matrix. <p> Each offset corresponds to a different translation matrix. The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range [5; 5] fi [5; 5] fi [5; 5]n <ref> [2; 2] </ref> fi [2; 2] fi [2; 2]. For ease of indexing, we generate the translation matrices also for the 125 subdomains excluded from the interactive-field, or a total of 11 fi 11 fi 11 = 1331 matrices. <p> Each offset corresponds to a different translation matrix. The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range [5; 5] fi [5; 5] fi [5; 5]n <ref> [2; 2] </ref> fi [2; 2] fi [2; 2]. For ease of indexing, we generate the translation matrices also for the 125 subdomains excluded from the interactive-field, or a total of 11 fi 11 fi 11 = 1331 matrices. <p> The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range [5; 5] fi [5; 5] fi [5; 5]n <ref> [2; 2] </ref> fi [2; 2] fi [2; 2]. For ease of indexing, we generate the translation matrices also for the 125 subdomains excluded from the interactive-field, or a total of 11 fi 11 fi 11 = 1331 matrices.
Reference: [3] <author> James H. Applegate, Michael R. Douglas, Yekta Gursel, Peter Hunter, Charles L. Seitz, and Gerald J. Sussman. </author> <title> A digital orrery. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-34(9):822-831, </volume> <month> September </month> <year> 1985. </year>
Reference-contexts: Finally, the two contributions to box 0 will be combined with interactions among particles in box 0. Exploiting symmetry saves almost a factor of two in both communication and computation. The idea of exploiting symmetry is similar to the idea used for the linear orrery by Applegate et. al. <ref> [3] </ref>. Here, linear ordering is imposed on the 124 neighbor boxes in 3-D, which contain ordered particles. 15 6.3 Box-box interactions Excessive data movement can easily happen in programs written in high-level languages, such as HPF, which provide a global address space.
Reference: [4] <author> Josh Barnes and Piet Hut. </author> <title> A hierarchical o(n log n) force calculation algorithm. </title> <journal> Nature, </journal> <volume> 324 </volume> <pages> 446-449, </pages> <year> 1986. </year>
Reference-contexts: The optimization techniques for programming hierarchical methods in CMF (HPF) are presented in Sections 5 - 9. Section 10 reports the performance results of our implementation. Section 11 discusses the load-balancing issues and Section 12 summarizes the paper. 2 Hierarchical N -body Methods Hierarchical methods <ref> [1, 4, 11, 9] </ref> for the N -body problem exploit the linearity of the potential field by partitioning the field into two parts, total = nearfield + farfield ; (1) 3 where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles.
Reference: [5] <author> Jack J. Dongarra, Jeremy Du Croz, Iain Duff, and Sven Hammarling. </author> <title> A Set of Level 3 Basic Linear Algebra Subprograms. </title> <type> Technical Report Reprint No. 1, </type> <institution> Argonne National Laboratories, Mathematics and Computer Science Division, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: Since there are no other computations in the hierarchy, the entire hierarchical part takes the form of a collection of matrix-matrix multiplications, which are implemented 20 efficiently on most computers as part of the BLAS (Basic Linear Algebra Subroutines) <ref> [5, 6, 18] </ref>. The Connection Machine Scientific Software Library [31], CMSSL, supports both single-instance and multiple-instance BLAS. <p> For illustration, see Figure 11. Each offset corresponds to a different translation matrix. The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range <ref> [5; 5] </ref> fi [5; 5] fi [5; 5]n [2; 2] fi [2; 2] fi [2; 2]. For ease of indexing, we generate the translation matrices also for the 125 subdomains excluded from the interactive-field, or a total of 11 fi 11 fi 11 = 1331 matrices. <p> For illustration, see Figure 11. Each offset corresponds to a different translation matrix. The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range <ref> [5; 5] </ref> fi [5; 5] fi [5; 5]n [2; 2] fi [2; 2] fi [2; 2]. For ease of indexing, we generate the translation matrices also for the 125 subdomains excluded from the interactive-field, or a total of 11 fi 11 fi 11 = 1331 matrices. <p> For illustration, see Figure 11. Each offset corresponds to a different translation matrix. The union of the interactive-fields of the eight siblings has 11 fi 11 fi 11 5 fi 5 fi 5 = 1206 boxes with 1206 offsets in the range <ref> [5; 5] </ref> fi [5; 5] fi [5; 5]n [2; 2] fi [2; 2] fi [2; 2]. For ease of indexing, we generate the translation matrices also for the 125 subdomains excluded from the interactive-field, or a total of 11 fi 11 fi 11 = 1331 matrices.
Reference: [6] <author> Jack J. Dongarra, Jeremy Du Croz, Sven Hammarling, and Richard J. Hanson. </author> <title> An Extended Set of Fortran Basic Linear Algebra Subprograms. </title> <type> Technical Report Technical Memorandum 41, </type> <institution> Argonne National Laboratories, Mathematics and Computer Science Division, </institution> <month> November </month> <year> 1986. </year>
Reference-contexts: Since there are no other computations in the hierarchy, the entire hierarchical part takes the form of a collection of matrix-matrix multiplications, which are implemented 20 efficiently on most computers as part of the BLAS (Basic Linear Algebra Subroutines) <ref> [5, 6, 18] </ref>. The Connection Machine Scientific Software Library [31], CMSSL, supports both single-instance and multiple-instance BLAS.
Reference: [7] <author> W. D. Elliott and J. A. </author> <title> Board. Fast fourier transform accelerated fast multipole algorithm. </title> <type> Technical Report 94-001, </type> <institution> Dept. of Electrical Engineering, Duke Univ., </institution> <year> 1994. </year>
Reference-contexts: Zhao and Johnsson developed a data-parallel implementation of Zhao's method on the CM-2, and achieved an efficiency of 12% for expansions in Cartesian coordinates, resulting in more costly multipole expansion calculations. Leathrum and Board [19, 16] and Elliott and Board <ref> [7] </ref> achieved efficiencies in the range 14% - 20% in implementing Greengard-Rokhlin's method (see [10]) on the KSR-1. Schmidt and Lee [26] vectorized this method for the Cray YMP and achieved an efficiency of 39% on a single processor.
Reference: [8] <author> High Performance Fortran Forum. </author> <title> High performance fortran; language specification, version 1.0. </title> <journal> Scientific Programming, </journal> <volume> 2(1 - 2):1-170, 1993. 35 </volume>
Reference-contexts: Several of the expressions can also be used in High Performance Fortran (HPF) <ref> [8] </ref>, but no HPF compiler was available when this work started. The techniques we discuss largely result in high performance through careful management of memory references through the use of data distribution directives, array sectioning and array aliasing.
Reference: [9] <author> Leslie Greengard and William D. Gropp. </author> <title> A parallel version of the fast multipole method. </title> <booktitle> In Parallel Processing for Scientific Computing, </booktitle> <pages> pages 213-222. </pages> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: Hence, the matrices are the same for all levels of the hierarchy, and all source destination pairs with the same relative locations use the same matrices at any level in the hierarchy. Thus, all matrices can be precomputed. The translation operators in Greengard-Rokhlin's <ref> [11, 9] </ref> and Zhao's [35] fast multipole methods can also be viewed as matrix-vector multiplications [23]. We discuss the arithmetic optimization in Section 7. <p> The optimization techniques for programming hierarchical methods in CMF (HPF) are presented in Sections 5 - 9. Section 10 reports the performance results of our implementation. Section 11 discusses the load-balancing issues and Section 12 summarizes the paper. 2 Hierarchical N -body Methods Hierarchical methods <ref> [1, 4, 11, 9] </ref> for the N -body problem exploit the linearity of the potential field by partitioning the field into two parts, total = nearfield + farfield ; (1) 3 where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles.
Reference: [10] <author> Leslie Greengard and V. Rokhlin. </author> <title> On the efficient implementation of the fast multipole method. </title> <type> Technical Report YALEU/DCS/RR-602, </type> <institution> Dept. of Computer Science, Yale Univ., </institution> <address> New Haven, CT, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: Leathrum and Board [19, 16] and Elliott and Board [7] achieved efficiencies in the range 14% - 20% in implementing Greengard-Rokhlin's method (see <ref> [10] </ref>) on the KSR-1. Schmidt and Lee [26] vectorized this method for the Cray YMP and achieved an efficiency of 39% on a single processor.
Reference: [11] <author> Leslie Greengard and Vladimir Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> Journal of Computational Physics, </journal> <volume> 73 </volume> <pages> 325-348, </pages> <year> 1987. </year>
Reference-contexts: Hence, the matrices are the same for all levels of the hierarchy, and all source destination pairs with the same relative locations use the same matrices at any level in the hierarchy. Thus, all matrices can be precomputed. The translation operators in Greengard-Rokhlin's <ref> [11, 9] </ref> and Zhao's [35] fast multipole methods can also be viewed as matrix-vector multiplications [23]. We discuss the arithmetic optimization in Section 7. <p> The optimization techniques for programming hierarchical methods in CMF (HPF) are presented in Sections 5 - 9. Section 10 reports the performance results of our implementation. Section 11 discusses the load-balancing issues and Section 12 summarizes the paper. 2 Hierarchical N -body Methods Hierarchical methods <ref> [1, 4, 11, 9] </ref> for the N -body problem exploit the linearity of the potential field by partitioning the field into two parts, total = nearfield + farfield ; (1) 3 where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles.
Reference: [12] <author> Leslie Greengard and Vladimir Rokhlin. </author> <title> Rapid evaluation of potential fields in three dimensions. </title> <type> Technical Report YALEU/DCS/RR-515, </type> <institution> Dept. of Computer Science, Yale Univ., </institution> <address> New Haven, CT, </address> <month> February </month> <year> 1987. </year>
Reference-contexts: Subdomains that are not further decomposed are leaves. In two dimensions, the near-field contains those subdomains that share a boundary point with the considered subdomain. In three dimensions, Greengard-Rokhlin's formulation <ref> [12] </ref> defines the near-field to contain nearest neighbor subdomains which share a boundary point with the considered subdomain and second nearest neighbor subdomains which share a boundary point with the nearest neighbor subdomains.
Reference: [13] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1990. </year>
Reference-contexts: The supernode idea must be modified somewhat for Anderson's method, but the same reduction in computational complexity can be achieved [14]. For gravitational and Coulombic fields division and square roots represent a significant fraction of the arithmetic 30 I Native always 1 II Hennessy & Patterson <ref> [13] </ref> ADD,SUB,MULT - 1 DIV,SQRT - 4 III CM-5E/VU normalized ADD,SUB,MULT - 1 DIV - 5 SQRT - 8 Table 4: Weights for floating-point operations in our three methods for FLOP counts. time. We report floating-point rates for three different weights of these operations as specified in Table 4.
Reference: [14] <author> Yu Hu and S. Lennart Johnsson. </author> <title> A data parallel implementation of hierarchical N -body methods. </title> <type> Technical Report TR-26-94, </type> <institution> Harvard University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: We first give a summary of the timings breakdown in computing the potential field for 100 million uniformly distributed particles on a 256 node CM-5E, then demonstrate the scalability of the implementation. A more detailed analysis of the effectiveness of the techniques is given in <ref> [14] </ref>. In considering the execution times it should be mentioned that our implementation uses the idea of supernodes. Zhao [35] made the observation that of the 875 boxes in the interactive-field, in many cases all eight siblings of a parent are included in the interactive-field. <p> By converting the far-field of the parent box instead of the far-fields of all eight siblings the number of far-field to local-field conversions are reduced to 189 from 875. The supernode idea must be modified somewhat for Anderson's method, but the same reduction in computational complexity can be achieved <ref> [14] </ref>.
Reference: [15] <author> S. Lennart Johnsson and Ching-Tien Ho. </author> <title> Spanning graphs for optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Trans. Computers, </journal> <volume> 38(9) </volume> <pages> 1249-1268, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Each group compute the entire collection of matrices, followed by spreads within groups when a matrix is needed. The replication may also be performed as an all-to-all broadcast <ref> [15] </ref>. The load-balance with this amount of redundant computation is the same as with no redundancy, but the communication cost may be reduced.
Reference: [16] <author> J. A. Board Jr., Z. S. Hakura, W. D. Elliott, D. C. Gray, W. J. Blanke, and J.F. Leathrum Jr. </author> <title> Scalable implementations of multipole-accelerated algorithms for molecular dynamics. </title> <booktitle> In Proc. Scalable High Performance Computing Conference SHPCC94, </booktitle> <year> 1994. </year>
Reference-contexts: Zhao and Johnsson developed a data-parallel implementation of Zhao's method on the CM-2, and achieved an efficiency of 12% for expansions in Cartesian coordinates, resulting in more costly multipole expansion calculations. Leathrum and Board <ref> [19, 16] </ref> and Elliott and Board [7] achieved efficiencies in the range 14% - 20% in implementing Greengard-Rokhlin's method (see [10]) on the KSR-1. Schmidt and Lee [26] vectorized this method for the Cray YMP and achieved an efficiency of 39% on a single processor.
Reference: [17] <author> J. Katzenelson. </author> <title> Computational structure of the N-body problem. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 4 </volume> <pages> 787-815, </pages> <year> 1989. </year>
Reference-contexts: Let l i represent the contribution to the potential field in subdomain i at level l due to particles in the far-field of subdomain i, i.e., the local-field potential in subdomain i at level l. Then, the computational structure is described in the recursive formulation by Katzenelson <ref> [17] </ref>: Algorithm: (A generic hierarchical method) 1. Compute h i for all boxes i at the leaf level h. 2. Upward-pass: for l = h 1; h 2; :::; 2, compute l X i2fchildren (n)g T 1 ( l+1 3.
Reference: [18] <author> C.L. Lawson, R.J. Hanson, D.R. Kincaid, and F.T. Krogh. </author> <title> Basic Linear Algebra Subprograms for Fortran Usage. </title> <journal> ACM TOMS, </journal> <volume> 5(3) </volume> <pages> 308-323, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: Since there are no other computations in the hierarchy, the entire hierarchical part takes the form of a collection of matrix-matrix multiplications, which are implemented 20 efficiently on most computers as part of the BLAS (Basic Linear Algebra Subroutines) <ref> [5, 6, 18] </ref>. The Connection Machine Scientific Software Library [31], CMSSL, supports both single-instance and multiple-instance BLAS.
Reference: [19] <author> James F. Leathrum. </author> <title> The parallel fast multipole method in three dimensions. </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <year> 1992. </year>
Reference-contexts: Zhao and Johnsson developed a data-parallel implementation of Zhao's method on the CM-2, and achieved an efficiency of 12% for expansions in Cartesian coordinates, resulting in more costly multipole expansion calculations. Leathrum and Board <ref> [19, 16] </ref> and Elliott and Board [7] achieved efficiencies in the range 14% - 20% in implementing Greengard-Rokhlin's method (see [10]) on the KSR-1. Schmidt and Lee [26] vectorized this method for the Cray YMP and achieved an efficiency of 39% on a single processor.
Reference: [20] <author> J. Li and M. Chen. </author> <title> Generating explicit communication from shared-memory program references. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 865-876. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: Efficient memory management is the most challenging issue in high performance computing. Techniques for the automatic determination of data distributions with balanced load and efficient communication have been the focus of parallel compiler research in the last several years (for example, see <ref> [2, 20, 34] </ref>). However, no general technique that balances load and generates efficient communication has emerged so far.
Reference: [21] <author> Pangfeng Liu. </author> <title> The parallel implementation of N-body algorithms. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1994. </year>
Reference-contexts: Barnes-Hut O (N log 2 N ) has been implemented using the message passing programming paradigm by Salmon et. al. [25, 32, 33] on the Intel Touchstone Delta and by Liu <ref> [21] </ref> on the CM-5. Salmon et. al. achieved efficiencies in the range 24% - 28%, while Liu using assembly language for time critical kernels achieved 30% efficiency.
Reference: [22] <author> P. S. Lomdahl, P. Tamayo, N. Gronbech-Jensen, and D. M. Beazley. </author> <title> 50 GFlops molecular dynamics on the Connection Machine 5. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 520-527. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: This timing corresponds to about 1 micro-second per particle update. To our knowledge, both the number of particles and the running time are the best reported to date. Furthermore, the running time is within a factor 3 of that reported by using the best short range MD code <ref> [22] </ref> on the CM-5.
Reference: [23] <author> K. Nabors and J. White. Fastcap: </author> <title> A multipole accelerated 3-d capacitance extraction program. </title> <type> Technical report, </type> <institution> MIT Dept. of Electrical Engineering and Computer Science, </institution> <year> 1991. </year>
Reference-contexts: Thus, all matrices can be precomputed. The translation operators in Greengard-Rokhlin's [11, 9] and Zhao's [35] fast multipole methods can also be viewed as matrix-vector multiplications <ref> [23] </ref>. We discuss the arithmetic optimization in Section 7.
Reference: [24] <author> L. S. Nyland, J. F. Prins, and J. H. Reif. </author> <title> A data-parallel implementation of the adaptive fast multipole algorithm. </title> <booktitle> In Proceedings of the DAGS '93 Symposium. Dartmouth Institute for Advanced Graduated Studies in Parallel Computation, </booktitle> <address> Hanover, NH, </address> <year> 1993. </year>
Reference-contexts: Singh et. al. [28, 27] have implemented both O (N log 2 N ) and O (N ) methods on the Stanford DASH machine, but no measures of the achieved efficiency is available. Nyland et. al. <ref> [24] </ref> discussed how to express the three-dimensional adaptive version of the Greengard-Rokhlin method in a data-parallel subset of the Proteus language, which is still under implementation on parallel machines. The efficiencies of the various implementations are summarized in Table 1.
Reference: [25] <author> John K. Salmon. </author> <title> Parallel heirarchical n-body methods. </title> <type> Technical Report CRPC-90-14, </type> <institution> California Institute of Technology, </institution> <year> 1990. </year> <month> 36 </month>
Reference-contexts: Barnes-Hut O (N log 2 N ) has been implemented using the message passing programming paradigm by Salmon et. al. <ref> [25, 32, 33] </ref> on the Intel Touchstone Delta and by Liu [21] on the CM-5. Salmon et. al. achieved efficiencies in the range 24% - 28%, while Liu using assembly language for time critical kernels achieved 30% efficiency.
Reference: [26] <author> K. E. Schmidt and M. A. Lee. </author> <title> Implementing the fast multipole method in three dimensions. </title> <journal> J. Stat. Phy., </journal> <volume> 63(5/6), </volume> <year> 1991. </year>
Reference-contexts: Leathrum and Board [19, 16] and Elliott and Board [7] achieved efficiencies in the range 14% - 20% in implementing Greengard-Rokhlin's method (see [10]) on the KSR-1. Schmidt and Lee <ref> [26] </ref> vectorized this method for the Cray YMP and achieved an efficiency of 39% on a single processor.
Reference: [27] <author> J.P. Singh, Chris Holt, J.L. Hennessey, and Anoop. Gupta. </author> <title> A parallel adaptive fast multipole method. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 54 - 65. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Schmidt and Lee [26] vectorized this method for the Cray YMP and achieved an efficiency of 39% on a single processor. Singh et. al. <ref> [28, 27] </ref> have implemented both O (N log 2 N ) and O (N ) methods on the Stanford DASH machine, but no measures of the achieved efficiency is available.
Reference: [28] <author> J.P. Singh, Chris Holt, Takashi Ttsuka, Anoop. Gupta, and J.L. Hennessey. </author> <title> Load balancing and data locality in hierarchical N-body methods. </title> <type> Technical Report CSL-TR-92-505, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: Schmidt and Lee [26] vectorized this method for the Cray YMP and achieved an efficiency of 39% on a single processor. Singh et. al. <ref> [28, 27] </ref> have implemented both O (N log 2 N ) and O (N ) methods on the Stanford DASH machine, but no measures of the achieved efficiency is available.
Reference: [29] <institution> Thinking Machines Corp. </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year>
Reference-contexts: This issue is of particular importance in gathering boxes for the interactive-field computations. In Section 6 we show how to use array sectioning and array aliasing to implement an effective gathering of nonlocal interactive-field boxes. On the Connection Machine systems CM-5/5E <ref> [29] </ref>, the communication time for the straightforward use of CMF for interactive field computations is more than one order of magnitude higher than the communication time for the technique we use. In Anderson's version of the multipole method [2], all translation operators can be represented by matrices acting on vectors.
Reference: [30] <institution> Thinking Machines Corp. </institution> <note> CM Fortran Reference Manual, Version 2.1, </note> <year> 1993. </year>
Reference-contexts: 1 Introduction The contributions of this paper are techniques for achieving high efficiency in implementing O (N ) N -body algorithms on Massively Parallel Processors (MPPs) and Connection Machine Fortran <ref> [30] </ref> examples of how to express the techniques in high-level languages with an array syntax. Several of the expressions can also be used in High Performance Fortran (HPF) [8], but no HPF compiler was available when this work started. <p> A set of extended intrinsic functions, including mapping inquiry intrinsic subroutines that allow a program to know the exact mapping of an array at run-time. Since no HPF compiler was available when this work was initiated, we used the Connection Machine Fortran (CMF) language <ref> [30] </ref> for our implementation. HPF is heavily influenced by CMF. HPF supports data-parallel programming with a global address space. Programs can be written without any knowledge of the architecture of the memory system. The consequence is that excess data movement often results.
Reference: [31] <institution> Thinking Machines Corp. CMSSL for CM Fortran, </institution> <note> Version 3.1, </note> <year> 1993. </year>
Reference-contexts: translation operations as matrix-vector multiplications and aggregating these operations into multiple-instance matrix-matrix multiplications allow many of the translation operations to be performed at an efficiency of about 80% of peak, or at a rate of 127 Mflop/s per node of a CM-5E using the Connection Machine Scientific Software Library, CMSSL <ref> [31] </ref>. Recognizing and aggregating BLAS operations and using library functions improve the performance of the computations significantly on most architectures, even uniprocessor architectures. For parent-child interactions, eight translation matrices are required in the upward traversal as well as the downward traversal of the hierarchy of grids. <p> Since there are no other computations in the hierarchy, the entire hierarchical part takes the form of a collection of matrix-matrix multiplications, which are implemented 20 efficiently on most computers as part of the BLAS (Basic Linear Algebra Subroutines) [5, 6, 18]. The Connection Machine Scientific Software Library <ref> [31] </ref>, CMSSL, supports both single-instance and multiple-instance BLAS.
Reference: [32] <author> M. Warren and J. Salmon. </author> <title> Astrophysical N-body simulations using hierarchical tree data structures. </title> <booktitle> In Supercomputing '92, </booktitle> <pages> pages 570 - 576. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: Barnes-Hut O (N log 2 N ) has been implemented using the message passing programming paradigm by Salmon et. al. <ref> [25, 32, 33] </ref> on the Intel Touchstone Delta and by Liu [21] on the CM-5. Salmon et. al. achieved efficiencies in the range 24% - 28%, while Liu using assembly language for time critical kernels achieved 30% efficiency.
Reference: [33] <author> M. Warren and J. Salmon. </author> <title> A parallel hashed oct-tree N-body algorithm. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 12 - 21. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Barnes-Hut O (N log 2 N ) has been implemented using the message passing programming paradigm by Salmon et. al. <ref> [25, 32, 33] </ref> on the Intel Touchstone Delta and by Liu [21] on the CM-5. Salmon et. al. achieved efficiencies in the range 24% - 28%, while Liu using assembly language for time critical kernels achieved 30% efficiency.
Reference: [34] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Efficient memory management is the most challenging issue in high performance computing. Techniques for the automatic determination of data distributions with balanced load and efficient communication have been the focus of parallel compiler research in the last several years (for example, see <ref> [2, 20, 34] </ref>). However, no general technique that balances load and generates efficient communication has emerged so far.
Reference: [35] <author> Feng Zhao. </author> <title> An O(N) algorithm for three-dimensional N-body simulations. </title> <type> Technical Report AI Memo 995, </type> <institution> MIT, Artificial Intelligence Laboratory, </institution> <month> October </month> <year> 1987. </year> <month> 37 </month>
Reference-contexts: Hence, the matrices are the same for all levels of the hierarchy, and all source destination pairs with the same relative locations use the same matrices at any level in the hierarchy. Thus, all matrices can be precomputed. The translation operators in Greengard-Rokhlin's [11, 9] and Zhao's <ref> [35] </ref> fast multipole methods can also be viewed as matrix-vector multiplications [23]. We discuss the arithmetic optimization in Section 7. <p> In our implementation of interactive-field computations (which does not exploit the parallelism among the boxes in the interactive-field) each target box needs to fetch the potential vectors of its 875 neighbor boxes (if supernodes <ref> [35] </ref> are not used). 6.3.1 Interactive-field box-box communication The simplest way to express in CMF the fetching of neighbor potential vectors for a target box uses individual CSHIFTs, one for each neighbor, as shown in Figure 10 (a). <p> A more detailed analysis of the effectiveness of the techniques is given in [14]. In considering the execution times it should be mentioned that our implementation uses the idea of supernodes. Zhao <ref> [35] </ref> made the observation that of the 875 boxes in the interactive-field, in many cases all eight siblings of a parent are included in the interactive-field.
References-found: 35

