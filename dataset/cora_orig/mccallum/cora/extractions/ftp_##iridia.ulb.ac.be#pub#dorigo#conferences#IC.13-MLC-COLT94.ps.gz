URL: ftp://iridia.ulb.ac.be/pub/dorigo/conferences/IC.13-MLC-COLT94.ps.gz
Refering-URL: http://iridia.ulb.ac.be/dorigo/pub_x_subj.html
Root-URL: 
Email: mdorigo@ulb.ac.be  colombet@elet.polimi.it  
Title: The Role of the Trainer in Reinforcement Learning  
Author: Marco Dorigo Marco Colombetti 
Address: Avenue Franklin Roosevelt 50 CP 194/6 1050 Bruxelles, Belgium  Piazza Leonardo da Vinci, 32 20133 Milano, Italy  
Affiliation: IRIDIA Universit Libre de Bruxelles  Progetto di intelligenza artificiale e robotica Dipartimento di elettronica e informazione Politecnico di Milano  
Abstract: In this paper we propose a threestage incremental approach to the development of autonomous agents. We discuss some issues about the characteristics which differentiate reinforcement programs (RPs), and define the trainer as a particular kind of RP. We present a set of results obtained running experiments with a trainer which provides guidance to the AutonoMouse, our mousesized autonomous robot. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A.G., Sutton, R.S., and Watkins, C.J.C.H. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In M. Gabriel and J.W. Moore (Eds.), </editor> <title> Learning and computational neuroscience. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Among the most studied and best known algorithms used by researchers in trying to solve RL problems there are Q-learning (Watkins, 1989), the adaptive heuristic critic <ref> (Barto, Sutton and Watkins, 1990) </ref>, and the learning classifier system (Booker, Goldberg, Holland, 1989), which was used as the learning paradigm in the experiments of this paper.
Reference: <author> Booker, L., Goldberg, D.E., and Holland, J.H. </author> <year> (1989). </year> <title> Classifier Systems and Genetic Algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 1-3, 235282. </pages>
Reference-contexts: Among the most studied and best known algorithms used by researchers in trying to solve RL problems there are Q-learning (Watkins, 1989), the adaptive heuristic critic (Barto, Sutton and Watkins, 1990), and the learning classifier system <ref> (Booker, Goldberg, Holland, 1989) </ref>, which was used as the learning paradigm in the experiments of this paper.
Reference: <author> Caironi, P.V.C., and Dorigo, M. </author> <year> (1994). </year> <title> Training Q-Agents. </title> <type> Tech. Rep. </type> <institution> IRIDIA/94-14, Universit Libre de Bruxelles, Belgium. </institution>
Reference: <author> Dorigo, M. </author> <year> (1992). </year> <title> Alecsys and the AutonoMouse: Learning to Control a Real Robot by Distributed Classifier Systems. </title> <type> Technical Report No.92-011, </type> <institution> Politecnico di Milano, Italy. </institution>
Reference-contexts: This is a major point in machine learning research, since simulated environments can be but an approximation of real ones. To study the effect that using real sensors and motors has on the learning process, and its relation to the training procedure, we introduced two different training policies <ref> (Dorigo, 1992) </ref>, which we call reward-the-intention policy, and reward-the-result policy. The Reward-the-Intention Policy We say that a trainer uses the reward-the-intention policy if, in order to decide about the reinforcement to give, it uses observations from an ideal environment which stays between ALECSYS and the AutonoMouse.
Reference: <author> Dorigo, M. </author> <year> (1993). </year> <title> Genetic and NonGenetic Operators in Alecsys. </title> <journal> Evolutionary Computation Journal, </journal> <volume> 1, 2, 151164, </volume> <publisher> MIT Press. </publisher>
Reference: <author> Dorigo, M., and Colombetti, M. </author> <year> (1994). </year> <title> Robot Shaping: Developing Autonomous Agents through Learning. </title> <journal> Artificial Intelligence, </journal> <note> to appear. </note>
Reference: <author> Dorigo, M., and Schnepf, U. </author> <year> (1993). </year> <title> Genetics-based Machine Learning and Behaviour Based Robotics: A New Synthesis. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23, 1, </volume> <pages> 141154. </pages>
Reference: <author> Lin, L-J. </author> <year> (1993). </year> <title> Hierarchical learning of robot skills by reinforcement. </title> <booktitle> Proceedings of 1993 IEEE International Conference on Neural Networks, IEEE, </booktitle> <pages> 181186. </pages>
Reference: <author> Mahadevan, S., and Connell, J. </author> <year> (1992). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55, 2, </volume> <pages> 311365. </pages>
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, 34, 257 277. </volume>
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning with delayed rewards. </title> <publisher> Ph. </publisher> <address> D. </address> <institution> dissertation, Psychology Department, University of Cambridge, </institution> <address> England. </address>
Reference-contexts: Among the most studied and best known algorithms used by researchers in trying to solve RL problems there are Q-learning <ref> (Watkins, 1989) </ref>, the adaptive heuristic critic (Barto, Sutton and Watkins, 1990), and the learning classifier system (Booker, Goldberg, Holland, 1989), which was used as the learning paradigm in the experiments of this paper.
Reference: <author> Whitehead,S.D. </author> <year> (1991a). </year> <title> A study of cooperative mechanisms for faster reinforcement learning. </title> <institution> TR 3 6 5 , Computer Science Dept., University of Rochester. </institution>
Reference: <author> Whitehead,S.D. </author> <year> (1991b). </year> <title> A complexity analysis of cooperative mechansims in reinforcement learning. </title> <booktitle> Proceeding of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> 607613. </pages>
References-found: 13

