URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-92-30.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: VMPP: A Virtual Machine for Parallel Processing  
Abstract: Edmond C. Loyot, Jr. Technical Report No. CS-92-30 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. B. Steel, UNCOL: </author> <title> The Myth and the Fact, Annu. </title> <journal> Rev. Autom. Program., </journal> <volume> Vol. 2, </volume> <year> 1960. </year>
Reference-contexts: This type of approach was first outlined by Steel in 1960 <ref> [1] </ref>. 3.2. Characteristics of a Good Solution In order to provide a good solution to the problems mentioned above, a virtual machine must have the following characteristics: 1) expressibility, 2) implementability, and 3) efficiency. Expressibility refers to a languages ability to express various programming constructs. <p> A composite node is used since there are really several separate operations being performed. An atomic computation node represents a single computational operation. For example, the statement A <ref> [1] </ref> = A [1] + 1 in the FORTRAN segment of Figure 2 (a) can be represented by an atomic computation node as is shown in Figure 2 (c). Memory nodes may also be either composite or atomic. <p> A composite node is used since there are really several separate operations being performed. An atomic computation node represents a single computational operation. For example, the statement A <ref> [1] </ref> = A [1] + 1 in the FORTRAN segment of Figure 2 (a) can be represented by an atomic computation node as is shown in Figure 2 (c). Memory nodes may also be either composite or atomic. A composite memory node represents a segment of memory that consists of several subsegments. <p> DO I = 1 TO 3 (a) A [I] = A [I] + 1 A [2] (b) A [2] = A [2] + 1 A <ref> [1] </ref> A [3] 8 An atomic memory node consists of a single memory segment. For example, a single oating point memory segment would be represented by an atomic memory node as in Figure 2 (c). A composite node in a VMPP program graph can be expanded.
Reference: [2] <author> Jack B. Dennis, </author> <title> First Version of a Data Flow Procedure Language, </title> <type> Technical Report TR-673, </type> <institution> Massachusetts Institute of Technology, Cambridge Massachusetts, </institution> <month> May </month> <year> 1975. </year>
Reference-contexts: VMPP Design VMPP consists of a graph-based virtual machine language and a data-driven execution model. VMPP program graphs consist of nodes, arcs, and tokens. The basic design is motivated by the dataow model of Dennis <ref> [2] </ref>. VMPP program graphs contain two kinds of nodes: computation nodes, and memory nodes. Computation nodes represent some sequence of computations. A computation node can be connected to other computation nodes by directed arcs which represent data dependencies. <p> For example, a one dimensional FORTRAN array of reals could be represented by a composite memory node since the array is actually a group of several individual memory elements (see Figures 2 (a, b)). DO I = 1 TO 3 (a) A [I] = A [I] + 1 A <ref> [2] </ref> (b) A [2] = A [2] + 1 A [1] A [3] 8 An atomic memory node consists of a single memory segment. For example, a single oating point memory segment would be represented by an atomic memory node as in Figure 2 (c). <p> DO I = 1 TO 3 (a) A [I] = A [I] + 1 A <ref> [2] </ref> (b) A [2] = A [2] + 1 A [1] A [3] 8 An atomic memory node consists of a single memory segment. For example, a single oating point memory segment would be represented by an atomic memory node as in Figure 2 (c). <p> DO I = 1 TO 3 (a) A [I] = A [I] + 1 A <ref> [2] </ref> (b) A [2] = A [2] + 1 A [1] A [3] 8 An atomic memory node consists of a single memory segment. For example, a single oating point memory segment would be represented by an atomic memory node as in Figure 2 (c). A composite node in a VMPP program graph can be expanded. <p> The back-end calculates the communication-to-computation ratio for each computation node by first measuring the amount of computation done by the node, then 10 NR = PR + 1 DO J = PR, 4 1 D [][] 3 4 6 D <ref> [2] </ref> (a) (b) (e) M = D [I][PR] / D [PR][PR] DO I = 2, 3 DO J = 1, 4 (M * D [1][J]) M = D [I][2] / D [2][2] D [I][J] = D [I][J] = 3 = M = D [2][1] / D [1][1] D [2][J] = D
Reference: [3] <author> Micah Beck, Richard Johnson, and Keshav Pingali, </author> <title> From Control Flow to Dataow, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 12, No. 2, </volume> <month> June </month> <year> 1991. </year>
Reference-contexts: DO I = 1 TO 3 (a) A [I] = A [I] + 1 A [2] (b) A [2] = A [2] + 1 A [1] A <ref> [3] </ref> 8 An atomic memory node consists of a single memory segment. For example, a single oating point memory segment would be represented by an atomic memory node as in Figure 2 (c). A composite node in a VMPP program graph can be expanded. <p> node with all of its input tokens present can be executed in parallel, 2) synchronization is implicit in the firing rules for the nodes, 3) data-dependence analysis and techniques for removing false dependencies are well understood [11], and 4) techniques for translating procedural languages into dataow graphs have been developed <ref> [3] </ref>. Because of these reasons, traditional dataow seemed like a good starting point for our research. Unfortunately, the traditional dataow model has several shortcomings as an intermediate language for parallel processing. First, traditional dataow is a deterministic model. This presents a problem since several of our source languages are non-deterministic.
Reference: [4] <author> Paul A. Suhler, Jit Biswas, Kim M. Korner, and James C. Browne, TDFL: </author> <title> A Task-Level Dataow Language, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 9, No. 2, </volume> <month> June </month> <year> 1990. </year>
Reference: [5] <author> Arvind and Rishiyur S. Nikhil, </author> <title> Executing a Program on the MIT Tagged-Token Dataow Architecture, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 39, No. 3, </volume> <month> March </month> <year> 1990. </year>
Reference: [6] <author> W. Daniel Hillis and Guy L. Steele, Jr., </author> <title> Data Parallel Algorithms, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 26, No. 12, </volume> <pages> pp. 1170 - 1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: In order to make our arguments convincing, the languages we have selected represent a diverse group of parallel programming paradigms. The paradigms represented are the data-parallel model <ref> [6] </ref>, the object-oriented model and the functional model [7]. We have chosen these models because they are dissimilar and each has a following in todays parallel programming community. The languages chosen and the techniques for translating their key constructs are detailed below. 5.1.1.
Reference: [7] <author> Paul Hudak, </author> <title> Conception, Evolution, and Application of Functional Programming, </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 21, No. 3, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: In order to make our arguments convincing, the languages we have selected represent a diverse group of parallel programming paradigms. The paradigms represented are the data-parallel model [6], the object-oriented model and the functional model <ref> [7] </ref>. We have chosen these models because they are dissimilar and each has a following in todays parallel programming community. The languages chosen and the techniques for translating their key constructs are detailed below. 5.1.1. <p> Since there is no implicit state, all functions and expressions can be evaluated in parallel, subject only to the satisfaction of the data dependencies between them. The most common functional languages used in parallel processing are the dataow languages. Dataow languages are essentially a subset of the functional languages <ref> [7] </ref>. We have chosen SISAL as our example functional language. SISAL SISAL is a dataow/functional language, designed to support single assignment functional programming, particularly on parallel processors.
Reference: [8] <author> Geoffrey Fox, Seema Hiranandani, Ken Kennedy, Charles Koelbel, Uli Kremer, Chau-Wen Tseng, and Min-You Wu, </author> <title> FORTRAN D Language Specification, </title> <type> Technical Report TR90-141, </type> <institution> Department of Computer Science, Rice University, Houston Texas, </institution> <year> 1990. </year>
Reference-contexts: Data-Parallel Translations In the data-parallel model, parallelism is obtained by performing the same set of operations on many data elements simultaneously. A language can support data parallelism either implicitly or explicitly. FORTRAN D <ref> [8] </ref> is an example of an implicit data-parallel language. In FORTRAN D the programmer specifies how the data, in the form of arrays, is to be distributed. Inner loop iterations that act on the data are executed in parallel. <p> VMPP has the advantage that it is a general solution and also provides a better solution in most cases. 6.1. Single Language Solutions to the Portability Problem There are currently several parallel languages designed to provide portability across parallel architectures <ref> [8, 14] </ref>. The approach taken by these languages is to provide a single parallel programming language along with compilers for a variety of parallel architectures. The language is usually designed to be relatively architecture independent. The single language approach suffers from two problems.
Reference: [9] <author> Vasanth Balasundaram, Ken Kennedy, </author> <title> A Technique for Summarizing Data Access and Its Use in Parallelism Enhancing Transformations, </title> <booktitle> Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland OR, </address> <month> June 21-23, </month> <year> 1989, </year> <journal> also listed as SIGPLAN Notices Vol. </journal> <volume> 24, No. 7, </volume> <month> July </month> <year> 1989. </year>
Reference-contexts: Each row-column of this combined data structure could then be placed on a separate processor using the DISTRIBUTE statement. Parallelism is obtained by executing the inner loops that act on these distributed data structures in parallel. FORTRAN D actually contains two levels of parallelism, loop-level parallelism and task-level parallelism <ref> [9] </ref>. Loop-level parallelism is the fine-grained data parallelism discussed above. Task-level parallelism is a coarse-grained functional parallelism based on executing non-loop-related blocks of FORTRAN D code in parallel. This type of parallelism is currently ignored by the proposed FORTRAN D implementation.
Reference: [10] <author> David Callahan and Ken Kennedy, </author> <title> Compiling Programs for Distributed-Memory Computers, </title> <journal> The Journal of Supercomputing, </journal> <volume> 2, </volume> <pages> pp. 151-169, </pages> <year> 1988. </year>
Reference-contexts: However, if this information is necessary it can also be passed to the back-ends by appropriate node annotations. All the work done by the designers of FORTRAN D on data-dependence analysis, loop optimizations and removal of false dependencies <ref> [10] </ref> can be used in a VMPP front-end for FORTRAN D. PC++ PC++ is an explicitly data-parallel programming language in which the programmer specifies a homogenous collection of data elements which are grouped together and can be referenced by a single name.
Reference: [11] <author> Constantine D. Polychronopolus, </author> <title> Parallel Programming and Compilers, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston/Dordrecht/London, </address> <year> 1988. </year>
Reference-contexts: This model has several advantages: 1) parallelism is easy to detect: any node with all of its input tokens present can be executed in parallel, 2) synchronization is implicit in the firing rules for the nodes, 3) data-dependence analysis and techniques for removing false dependencies are well understood <ref> [11] </ref>, and 4) techniques for translating procedural languages into dataow graphs have been developed [3]. Because of these reasons, traditional dataow seemed like a good starting point for our research. Unfortunately, the traditional dataow model has several shortcomings as an intermediate language for parallel processing.
Reference: [12] <author> Bjarne Stroustrup, </author> <title> The C++ Programming Language, Second Edition, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: In PC++, the programmer specifies which operations can be executed in parallel and which data elements can take part in a parallel operation. Like FORTRAN D the programmer must provide data distribution information to the compiler. Because PC++ is based on C++ <ref> [12] </ref>, it falls under the object-oriented paradigm as well as the data-parallel paradigm. However, since the explicit data-parallel model of PC++ distinguishes it from most other parallel object-oriented languages, we have chosen to present it in the data-parallel section.
Reference: [13] <author> Jenq Kuen Lee, and Dennis Gannon, </author> <title> Object Oriented Parallel Programming: Experiments and Results, </title> <booktitle> Proceedings of Supercomputing 91, </booktitle> <address> Albuquerque, NM, </address> <month> November, </month> <year> 1991. </year>
Reference-contexts: Inner loop iterations that act on the data are executed in parallel. The data parallelism is implicit because the programmer simply writes sequential FORTRAN code and need not be concerned with any special instructions for indicating parallelism. PC++ <ref> [13] </ref> is a language that supports explicit data parallelism. In PC++, the programmer specifies which operations can be executed in parallel and which data elements can take part in a parallel operation. Like FORTRAN D the programmer must provide data distribution information to the compiler.
Reference: [14] <author> Andrew S. Grimshaw, </author> <title> An Introduction to Parallel Object-Oriented Programming with Mentat, </title> <institution> Computer Science Report No. TR-91-07, Department of Computer Science, University of Virginia, </institution> <address> Charlottesville, VA, </address> <month> April 4, </month> <year> 1991. </year>
Reference-contexts: For example, a matrix object may contain storage for each row. In this case, each row would become a memory node. All the rows together make up the composite memory node for the entire object. Mentat Mentat <ref> [14] </ref> is a parallel object-oriented programming system that uses the distributed memory approach for objects. It is based on the object-oriented sequential programming language C++. The goal of the Mentat system is to provide the programmer with efficient, easy to use parallelism in an object-oriented programming language. <p> VMPP has the advantage that it is a general solution and also provides a better solution in most cases. 6.1. Single Language Solutions to the Portability Problem There are currently several parallel languages designed to provide portability across parallel architectures <ref> [8, 14] </ref>. The approach taken by these languages is to provide a single parallel programming language along with compilers for a variety of parallel architectures. The language is usually designed to be relatively architecture independent. The single language approach suffers from two problems.
Reference: [15] <author> Brian N. Bershad, Edward D Lazowska, and Henry M. Levy, </author> <title> PRESTO: A System for Object-Oriented Parallel Programming, </title> <journal> Software - Practice and Experience, </journal> <volume> Vol. 18, No. 8, </volume> <pages> pp. 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The resulting dataow information can easily be expressed in the VMPP program graphs. PRESTO PRESTO <ref> [15] </ref> is another parallel object-oriented programming system. Like Mentat, PRESTO is based on C++, but unlike Mentat, PRESTO uses the shared memory approach for objects. In the PRESTO system, parallelism is achieved through user-managed objects called threads. Threads are PRESTOs basic unit of execution.
Reference: [16] <author> C. A. R. Hoare, </author> <title> Monitors: An Operating System Concept, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 17, No. 10, </volume> <month> October </month> <year> 1974. </year>
Reference-contexts: This action corresponds to a call of the Unlock member function. This scheme, illustrated in Figure 6, insures that only one critical section node will execute at a time, providing the mutual exclusion semantics of the Lock class. Since monitors can be implemented using binary semaphores <ref> [16] </ref>, PRESTOs monitors can be translated into VMPP program graphs by representing the monitors using PRESTO Locks and then translating those Locks as discussed above. 5.1.4.
Reference: [17] <author> James McGraw, Stephan Skedzielewski, Stephan Allan, Rod Oldenhoeft, John Glauert, Chris Kirkhan, Bill Noyce, and Robert Thomas, </author> <title> SISAL: Streams and Iterations in a Single Assignment Language, Language Reference Manual, </title> <type> Version 1.2, </type> <institution> Lawrence Livermore National Laboratory Manual M-146 (Rev. 1), Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA, </address> <month> March </month> <year> 1985. </year> <month> 29 </month>
Reference: [18] <author> Robert Baron, Richard F. Rashid, Ellen Siegal, Avadis Tevanian, and Michael Young, MACH-1: </author> <title> An Operating System Environment for Large-Scale Multiprocessor Applications, </title> <journal> IEEE Software, </journal> <month> July </month> <year> 1985. </year>
Reference-contexts: DMMP Translation We have selected the Paragon as our example DMMP architecture. The Paragon has a mesh interconnection network. Processors in the Paragon communicate via message passing. Each processor runs OSF1, a version of Mach which supports processes, tasks, threads, ports etc. <ref> [18] </ref>. VMPP program graphs are translated into code suitable for execution on the Paragon in the following way. First, an appropriate granularity is selected. Once the VMPP program graph has the appropriate granularity, code is generated for each node in the graph. This code will be C with message passing.
Reference: [19] <author> Anita Osterhaug, </author> <title> editor, Guide to Parallel Programming on Sequent Computer Systems, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Memory nodes are scheduled so as to produce the minimum possible communication. Scheduling can be static or dynamic. A scheduling strategy will be developed as part of the research effort. 21 5.2.3. SMMP Translation Another popular parallel architecture is the shared memory multiprocessor. We have chosen the Sequent <ref> [19] </ref> as our example SMMP system. The Sequent is a bus-based multiprocessor that supports shared memory. Processors communicate and synchronize via the shared memory. The process for translating VMPP programs into code for the Sequent is very similar to the DMMP translation process described above.
Reference: [20] <author> Gregory M. Popadopoulos, </author> <title> Implementation of a General Purpose Dataow Multiprocessor, </title> <type> Technical Report TR432, </type> <institution> MIT Lab for Computer Science, </institution> <address> Cambridge, MA, Septem-ber, </address> <year> 1988. </year>
Reference-contexts: At this time, it is unclear how this can be accomplished. This is a matter for further investigation. 5.2.5. Dataow Translation Several dataow architectures are currently being constructed at various universities <ref> [20, 21] </ref>. Although considerably less developed than the architectures already discussed, dataow architectures are of interest to a significant number of researchers in parallel computing. Our example dataow architecture is the Monsoon [20]. <p> Dataow Translation Several dataow architectures are currently being constructed at various universities [20, 21]. Although considerably less developed than the architectures already discussed, dataow architectures are of interest to a significant number of researchers in parallel computing. Our example dataow architecture is the Monsoon <ref> [20] </ref>. The Monsoon machine consists of a number of processing elements with a small amount of local memory, connected to memory elements by a multistage packet switching network. This system contains some special hardware for supporting dataow graph execution.
Reference: [21] <author> A. P. Wim Bohm and John R. Gurd, </author> <title> Iterative Instructions in the Manchester Dataow Computer, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 1, No. 2, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: At this time, it is unclear how this can be accomplished. This is a matter for further investigation. 5.2.5. Dataow Translation Several dataow architectures are currently being constructed at various universities <ref> [20, 21] </ref>. Although considerably less developed than the architectures already discussed, dataow architectures are of interest to a significant number of researchers in parallel computing. Our example dataow architecture is the Monsoon [20].
Reference: [22] <author> Charles E. Leiserson et al, </author> <title> The Network Architecture of the Connection Machine CM-5, </title> <booktitle> Symposium on Parallel and Distributed Algorithms 92, </booktitle> <address> San Diego CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Hybrid Translation Some parallel architectures are really a hybrid of two or more other architectures. Our example hybrid architecture is the CM-5 <ref> [22] </ref>. The CM-5 is a hybrid of the SPMD and DMMP parallel architectures. The CM-5 consists of a group of processors, each with its own private memory. These processors can communicate via message passing. The CM-5 also contains special hardware and software support for SPMD execution of the processors.
Reference: [23] <author> Ralph Duncan, </author> <title> A Survey of Parallel Computer Architectures, </title> <booktitle> IEEE Computer, </booktitle> <month> February, </month> <year> 1990. </year>
Reference: [24] <author> Eran Grabber, </author> <title> VMMP: A Practical Tool for the Development of Portable and Efficient Programs for Multiprocessors, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 1, No. 3, </volume> <month> July </month> <year> 1990. </year>
Reference-contexts: VMPP does not suffer from this problem. 6.2. VMMP Despite the unfortunate similarity in names, VMMP and VMPP are different projects. VMMP is the Virtual Machine for Multi-Processors <ref> [24] </ref>. It provides a virtual machine suitable for large and medium-grain parallel computation. VMMP runs on shared and distributed memory multiprocessors. The VMMP approach is to provide a coherent set of services for parallel pro 24 gramming. The services provide two parallel programming models: tree computations and crowd computations.
Reference: [25] <author> G. A. Geist and V. S. Sunderman, </author> <title> Networked Based Concurrent Computing on the PVM System, </title> <type> Technical Report ORNL/TM-11760, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee, </institution> <month> June, </month> <year> 1991. </year>
Reference-contexts: The code will have to be rewritten to run efficiently on an architecture that supports a drastically different level of granularity. This is not a problem for VMPP, because of its intermediate languages support for multiple granularity. 6.3. PVM PVM is the Parallel Virtual Machine <ref> [25] </ref>. The PVM approach is similar to the VMMP approach. PVM provides a virtual machine suitable for large-grain and medium-grain parallel computation. The PVM programmer is provided with a library of services for parallel processing, which can be used from the usual sequential programming languages like C and FORTRAN.
Reference: [26] <author> Michael L. Scott, Thomas J. LeBlanc, and Brian D. Marsh, </author> <booktitle> Multi-Model Parallel Programming in Psyche, Proceedings of the Second ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming (PPOPP), SIGPLAN Notices Vol. </booktitle> <volume> 25, No. 3, </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: Multi-model Programming in Psyche Multi-model programming is the ability to use more than one programming model in a single program or across a group of programs running on a single machine or operating system. The Psyche operating system <ref> [26] </ref> is designed to support multi-model MIMD programming on shared-memory multiprocessors. Psyche provides a low-level operating system interface that is exible enough to support a variety of MIMD parallel processing models, particularly the shared-memory and message-passing models.
Reference: [27] <author> Robert A. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein, </author> <title> The Program Dependence Web: A Representation Supporting Control-, Data-, and Demand-Driven Interpretation of Imperative Languages </title>
References-found: 27

