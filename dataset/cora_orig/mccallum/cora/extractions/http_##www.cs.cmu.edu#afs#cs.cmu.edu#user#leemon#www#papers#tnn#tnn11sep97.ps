URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/tnn/tnn11sep97.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/index.html
Root-URL: 
Email: Email: scott.weaver@uc.edu  
Phone: 3  
Title: An Analytical Framework for Local Feedforward Networks  
Author: Scott Weaver ; Leemon Baird Marios Polycarpou 
Note: 2 Wright-Patterson Air Force Base WL/AACF 2241 Avionics  
Address: 45221-0030  WPAFB, Ohio 45433-7318  5000 Forbes Avenue  Pittsburgh, Pennsylvania 15213-3891  
Affiliation: 1 Department of Electrical and Computer Engineering University of Cincinnati Cincinnati, Ohio  Circle  Computer Science Department  Carnegie Mellon University  
Abstract: Interference in neural networks occurs when learning in one area of the input space causes unlearning in another area. Networks that are less susceptible to interference are referred to as spatially local networks. To understand these properties, a theoretical framework, consisting of a measure of interference and a measure of network localization, is developed. These measures incorporate not only the network weights and architecture but also the learning algorithm. Using this framework to analyze sigmoidal, multi-layer perceptron (MLP) networks that employ the back-propagation learning algorithm, we address a familiar misconception that single-hidden-layer sigmoidal networks are inherently non-local by demonstrating that given a sufficiently large number of adjustable weights, single-hidden-layer sigmoidal MLPs can be made arbitrarily local while retaining the ability to approximate any continuous function on a compact domain. fl Partially supported under Task 2312 R1 by the United States Air Force Office of Scientific Research.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Sofge and D. White, </author> <title> "Applied learning: optimal control for manufacturing," in Handbook of Intelligent Control Neural, Fuzzy, and Adaptive Approaches (D. </title> <editor> White and D. Sofge, eds.), </editor> <address> (New York, NY), </address> <pages> pp. 259-281, </pages> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year> <month> 15 </month>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of locality <ref> [1, 2, 3, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the locality of a learning system. <p> The interference problem has been uncovered in various forms by researchers in many areas <ref> [5, 1] </ref>. For example, consider a dynamical system after it settles into a desired trajectory (where only a small portion of the input space is reached). Suppose that without noise, a network function approximator learns the system dynamics, reduces the approximation error, and then ceases learning. <p> remains active and continually memorizes the system dynamics along the trajectory (because the error never goes to zero) even though there is no need to do so. "Global Network Collapse" results, as the other areas of the input space (those areas not on the trajectory) gradually unlearn due to interference <ref> [1] </ref>. Another variant of the interference problem is in the classification literature; "Catastrophic Interference" occurs when the training of a new pattern causes the unlearning of originally trained patterns [5]. <p> where w = [a 1 a 8 b 1 b 8 c 1 c 8 ] T , and a i = 1, b i = 0:8, c i = i=8, for i 2 f1; 8g: The centers are assumed to be equally spaced along the input domain X = <ref> [0; 1] </ref>. Assuming the input x is chosen from a uniform distribution over X , the measure of localization for this network can be numerically computed by equation (6) as L f;w;H;X = 1:49. <p> With an appropriate choice of w k and N k , we show the following two statements: 1. I f k ;w k ;H (x; x 0 ) is bounded for all k 2 N; x; x 0 2 <ref> [0; 1] </ref> n . 2. The limit of I f k ;w k ;H (x; x 0 ) as k approaches infinity is zero almost everywhere. <p> by 1 + n, we see interference has an upper bound that is not a function of x or k, that is, I f k ;w k ;H (x; x 0 ) ^xkz k (x; w k )k 2 B 1 = B (37) for all x; x 0 2 <ref> [0; 1] </ref> n . In similar fashion one can show interference is bounded below by B. Now we show that lim k!1 I f k ;w k ;H (x; x 0 ) = 0 almost everywhere. <p> At this point we have met the conditions of the Lebesgue Dominated Convergence Theorem : I f k ;w k ;H (x; x 0 ) is a sequence of integrable functions on <ref> [0; 1] </ref> n fi [0; 1] n . Because there exists a bound B &gt; 0 such that jI f k ;w k ;H (x; x 0 )j B for all k 2 N; x; x 0 2 [0; 1] n and (41) is an integrable function, we see lim E <p> At this point we have met the conditions of the Lebesgue Dominated Convergence Theorem : I f k ;w k ;H (x; x 0 ) is a sequence of integrable functions on <ref> [0; 1] </ref> n fi [0; 1] n . Because there exists a bound B &gt; 0 such that jI f k ;w k ;H (x; x 0 )j B for all k 2 N; x; x 0 2 [0; 1] n and (41) is an integrable function, we see lim E [I f k ;w <p> ;H (x; x 0 ) is a sequence of integrable functions on <ref> [0; 1] </ref> n fi [0; 1] n . Because there exists a bound B &gt; 0 such that jI f k ;w k ;H (x; x 0 )j B for all k 2 N; x; x 0 2 [0; 1] n and (41) is an integrable function, we see lim E [I f k ;w k ;H (x; x 0 ) 2 ] = E [ lim I f k ;w k ;H (x; x 0 ) 2 ] = 0 (42) for X = [0; 1] n . <p> x 0 2 <ref> [0; 1] </ref> n and (41) is an integrable function, we see lim E [I f k ;w k ;H (x; x 0 ) 2 ] = E [ lim I f k ;w k ;H (x; x 0 ) 2 ] = 0 (42) for X = [0; 1] n . Equation (42) implies that there exists a k such that E [I f k ;w k ;H (x; x 0 ) 2 ] &lt; * for arbitrary * &gt; 0 and hence L f;w;H;X can be made arbitrarily large.
Reference: [2] <author> J. H. Friedman, </author> <title> "Local learning based on recursive covering." </title> <journal> submitted to The Annals of Statistics, </journal> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of locality <ref> [1, 2, 3, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the locality of a learning system.
Reference: [3] <author> J. Sjoberg, Q. Zhang, L. Ljung, A. Benveniste, B. Deylon, P.-Y. Glorennec, H. Hjalmarsson, and A. Ju-ditsky, </author> <title> "Nonlinear black-box modeling in system identification: A unified overview," </title> <journal> Automatica, </journal> <volume> vol. 31, </volume> <pages> pp. 1691-1724, </pages> <year> 1995. </year>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of locality <ref> [1, 2, 3, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the locality of a learning system. <p> The functions shown in Figure 3 (b) and 3 (c), however, vanish rapidly at infinity and exhibit local properties (see Sjoberg et al. <ref> [3] </ref>). The network architecture and weights help determine the interference that occurs and hence play an important role in determining network localization.
Reference: [4] <author> W. Baker and J. Farrell, </author> <title> "An introduction to connectionist learning control systems," in Handbook of Intelligent Control Neural, Fuzzy, and Adaptive Approaches (D. </title> <editor> White and D. Sofge, eds.), </editor> <address> (New York, NY), </address> <pages> pp. 35-63, </pages> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of locality <ref> [1, 2, 3, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the locality of a learning system. <p> Therefore, the definition of interference given by (4) becomes I f;w;H (x; x 0 ) = &gt; &lt; ~(x)~(x 0 ) 0 otherwise (12) which is independent of the weights. 1 Baker and Farrell <ref> [4] </ref> use the term "coverage" for this condition. 8 1 partial of f wrt a x 1 partial of f wrt b x 1 partial of f wrt c x P N i=1 a i (1 + e c i b i x ) 1 with respect to an amplitude a
Reference: [5] <author> R. </author> <title> French, "Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference," </title> <booktitle> in Proceedings of the 16th Annual Cognitive Science Society Conference, </booktitle> <volume> vol. 5, </volume> <pages> pp. 207-220, </pages> <year> 1994. </year>
Reference-contexts: The interference problem has been uncovered in various forms by researchers in many areas <ref> [5, 1] </ref>. For example, consider a dynamical system after it settles into a desired trajectory (where only a small portion of the input space is reached). Suppose that without noise, a network function approximator learns the system dynamics, reduces the approximation error, and then ceases learning. <p> Another variant of the interference problem is in the classification literature; "Catastrophic Interference" occurs when the training of a new pattern causes the unlearning of originally trained patterns <ref> [5] </ref>. These and other interference problems may appear different when embedded in their particular applications but the root of these problems is the same; learning tends to interfere with previous learning elsewhere in the input space.
Reference: [6] <author> A. G. Barto, </author> <title> "Connectionist learning for control," in Neural Networks for Control (I. </title> <editor> W. Thomas Miller, R. S. Sutton, and P. J. </editor> <title> Werbos, </title> <type> eds.), </type> <institution> (Massachusetts Institute of Technology, </institution> <address> MA), </address> <pages> pp. 5-58, </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Although local networks, in general, lessen the problem of interference, there are trade-offs to consider (see Barto <ref> [6] </ref> for a nice summary). For example, look-up tables can be thought of as the most local of approximation structures because there is a one-to-one relationship between a point in the input space and an adjustable parameter.
Reference: [7] <author> R. E. Bellman, </author> <title> Adaptive control processes: A guided tour. </title> <publisher> Princeton University, </publisher> <address> NJ: </address> <publisher> Princeton University Press, </publisher> <year> 1961. </year>
Reference-contexts: However, look-up tables are obviously inappropriate when the dimension of the problem grows large because the curse of dimensionality <ref> [7] </ref> causes memory requirements to become prohibitive; furthermore, look-up tables provide no generalization of untrained points. Finding the correct balance between avoiding interference problems, reducing memory requirements, and enhancing generalization, that is, finding a balance 2 between local versus non-local networks, is a key problem in network learning.
Reference: [8] <author> K.-I. Funahashi, </author> <title> "On the approximate realization of continuous mappings by neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 183-192, </pages> <year> 1989. </year>
Reference-contexts: This leads us to a new question, "Can we find a single-hidden-layer MLP network that is arbitrarily local and approximates any smooth function arbitrarily closely in a compact set?" The following theorem combines the universal-localization theorem of Section 3 and well-known, universal-approximation results of single-hidden-layer sigmoidal MLPs. (See, for example <ref> [8, 9, 10] </ref>). Theorem 2 Let X be a compact subset of R n , H = er w h (x; w), and g fl (x) be a real valued continuous function on X . <p> This proof is a combination of the proof of Theorem 1 and a universal-approximation theorem in <ref> [8] </ref>, which says that given a real-valued continuous function g fl (x) on X , then for arbitrary * &gt; 0 there exist a number of nodes N , and weights w, such that g (x; w) = P N c i j=1 b ij x j ) 1 satisfies max
Reference: [9] <author> G. Cybenko, </author> <title> "Approximation by superpositions of a sigmoidal function," Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 303-314, </pages> <year> 1989. </year>
Reference-contexts: This leads us to a new question, "Can we find a single-hidden-layer MLP network that is arbitrarily local and approximates any smooth function arbitrarily closely in a compact set?" The following theorem combines the universal-localization theorem of Section 3 and well-known, universal-approximation results of single-hidden-layer sigmoidal MLPs. (See, for example <ref> [8, 9, 10] </ref>). Theorem 2 Let X be a compact subset of R n , H = er w h (x; w), and g fl (x) be a real valued continuous function on X .
Reference: [10] <author> K. Hornik, M. Stinchcombe, and H. White, </author> <title> "Multilayer feedforward networks are universal approxima-tors," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 359-366, </pages> <year> 1989. </year>
Reference-contexts: This leads us to a new question, "Can we find a single-hidden-layer MLP network that is arbitrarily local and approximates any smooth function arbitrarily closely in a compact set?" The following theorem combines the universal-localization theorem of Section 3 and well-known, universal-approximation results of single-hidden-layer sigmoidal MLPs. (See, for example <ref> [8, 9, 10] </ref>). Theorem 2 Let X be a compact subset of R n , H = er w h (x; w), and g fl (x) be a real valued continuous function on X .
Reference: [11] <author> R. Bartle, </author> <title> The Elements of Real Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1976. </year> <month> 16 </month>
Reference-contexts: To show kz k ()k is bounded from above we use the ratio test (Bartle <ref> [11] </ref> p. 296) and see that lim g (i + 1) = lim e 2 e i + 1 4 therefore P 1 i=0 g (i) is convergent and less then some value B 2 and therefore kz k ()k 2 &lt; 2B 2 .
References-found: 11

