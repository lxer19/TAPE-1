URL: ftp://axon.cs.byu.edu/pub/papers/wilson.aie96.gibl.ps
Refering-URL: http://synapse.cs.byu.edu/~randy/misc/pubs.html
Root-URL: 
Email: e-mail: randy@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: Instance-Based Learning with Genetically Derived Attribute Weights  
Author: D. Randall Wilson, Tony R. Martinez 
Keyword: Key words: instance-based learning, genetic algorithms, instance weights, generalization  
Address: Provo, UT 84602, U.S.A.  
Affiliation: Computer Science Department, Brigham Young University,  
Note: Proceedings of the International Conference on Artificial Intelligence, Expert Systems and Neural Networks (AIE96), pp. 11-14, 1996.  
Abstract: This paper presents an inductive learning system called the Genetic Instance-Based Learning (GIBL) system. This system combines instance-based learning approaches with evolutionary computation in order to achieve high accuracy in the presence of irrelevant or redundant attributes. Evolutionary computation is used to find a set of attribute weights that yields a high estimate of classification accuracy. Results of experiments on 16 data sets are shown, and are compared with a non-weighted version of the instance-based learning system. The results indicate that the generalization accuracy of GIBL is somewhat higher than that of the non-weighted system on regular data, and is significantly higher on data with irrelevant or redundant attributes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cover, T. M., and P. E. Hart, </author> <year> (1967). </year> <title> Nearest Neighbor Pattern Classification, </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory , vol. </journal> <volume> 13, no. 1, </volume> <month> January </month> <year> 1967, </year> <pages> pp. 21-27. </pages>
Reference: [2] <author> Dasarathy, Belur V., </author> <year> (1991). </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, </title> <publisher> Los Alamitos, </publisher> <address> CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [3] <author> Aha, David W., Dennis Kibler, Marc K. Albert, </author> <year> (1991). </year> <title> Instance-Based Learning Algorithms, </title> <journal> Machine Learning, </journal> <volume> vol. 6, </volume> <pages> pp. 37-66. </pages>
Reference: [4] <author> Aha, David W., </author> <year> (1992). </year> <title> Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms, </title> <journal> International Journal of Man-Machine Studies , vol. </journal> <volume> 36, </volume> <pages> pp. 267 - 287. </pages>
Reference: [5] <author> Cost, Scott, and Steven Salzberg, </author> <year> (1993). </year> <title> A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features, </title> <journal> Machine Learning, </journal> <volume> vol. 10, </volume> <pages> pp. 57-78. </pages>
Reference: [6] <author> Domingos, Pedro, </author> <year> (1995). </year> <title> Rule Induction and Instance-Based Learning: A Unified Approach, </title> <booktitle> to appear in The 1995 International Joint Conference on Artificial Intelligence (IJCAI-95). </booktitle>
Reference: [7] <author> Stanfill, C., and D. Waltz, </author> <year> (1986). </year> <title> Toward memory-based reasoning, </title> <journal> Communications of the ACM, </journal> <volume> vol. 29, </volume> <month> December </month> <year> 1986, </year> <pages> pp. 1213-1228. </pages>
Reference: [8] <author> Giraud-Carrier, Christophe, and Tony Martinez, </author> <year> (1995). </year> <title> An Efficient Metric for Heterogeneous Inductive Learning Applications in the Attribute-Value Language, </title> <booktitle> Intelligent Systems, </booktitle> <pages> pp. 341-350. </pages>
Reference-contexts: Using a linear distance measurement on such values makes little sense in this case, so a function is needed that handles nominal inputs. There are many applications that have both linear and nominal attributes and thus require a heterogeneous distance function. GIBL uses a function similar to that in <ref> [8] </ref>.
Reference: [9] <author> Wilson, D. Randall, </author> <year> (1994). </year> <title> Prototype Styles of Generalization, </title> <type> Masters Thesis, </type> <institution> Brigham Young University. </institution>
Reference-contexts: Redundant attributes often have a correlation with the output class and thus cannot always be detected by some techniques that can identify irrelevant attributes <ref> [9] </ref>. However, one way to deal with irrelevant attributes and redundant attributes is to find weight settings for each attribute that seems to improve classification. Wettschereck, Aha and Takao [9] provide an excellent review of many attribute weighting schemes (as well as other kinds of weighting schemes). <p> have a correlation with the output class and thus cannot always be detected by some techniques that can identify irrelevant attributes <ref> [9] </ref>. However, one way to deal with irrelevant attributes and redundant attributes is to find weight settings for each attribute that seems to improve classification. Wettschereck, Aha and Takao [9] provide an excellent review of many attribute weighting schemes (as well as other kinds of weighting schemes). One of their conclusions is that systems which use performance feedback to decide on the values of weights tend to achieve higher accuracy than those that do not.
Reference: [10] <author> Wettschereck, Dietrich, David W. Aha, and Takao Mohri, </author> <year> (1995). </year> <title> A Review and Comparative Evaluation of Feature Weighting Methods for Lazy Learning Algorithms, </title> <type> Technical Report AIC-95-012, </type> <institution> Washington, D.C.: Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence. </note>
Reference: [11] <author> Spears, William M., Kenneth A. De Jong, Thomas Bck, David B. Fogel, and Hugo de Garis, </author> <year> (1993). </year> <title> An Overview of Evolutionary Computation, </title> <booktitle> Proceedings of the European Conference on Machine Learning, </booktitle> <volume> vol. 667, </volume> <pages> pp. 442-459. </pages>
Reference-contexts: Even if only a few (e.g., 10) different values are allowed for consideration for each weight, the size of the weight space increases exponentially with the number of attributes (e.g., 10 m if 10 values are used). Evolutionary Algorithms <ref> [11] </ref> provide heuristics which can aid in searching an intractable space. A population of individuals is initialized to random places in the search space, and each individual is evaluated and given a fitness score.
Reference: [12] <author> Papadimitriou, Christos H., and Jon Louis Bentley, </author> <year> (1980). </year> <title> A Worst-Case Analysis of Nearest Neighbor Searching by Projection, </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> vol. 85, </volume> <booktitle> Automata Languages and Programming, </booktitle> <pages> pp. 470-482. </pages>
Reference-contexts: The results presented in this paper make use of all available instances, but the GIBL system allows the user to specify what proportion of the available instances to use in the evaluation function. The GIBL system also uses a technique called projection <ref> [12] </ref> to reduce the number of distance calculations that must be performed before the nearest neighbor can be found. 4.
Reference: [13] <author> Murphy, P. M., and D. W. Aha, </author> <year> (1993). </year> <title> UCI Repository of Machine Learning Databases. </title> <address> Irvine, CA: </address> <institution> University of California Irvine, Department of Information and Computer Science. </institution>
Reference-contexts: The GIBL algorithm was implemented and tested on 16 databases from the Machine Learning Database Repository at the University of California Irvine <ref> [13] </ref>. Each test consisted of ten trials, each using one of ten partitions of the data randomly selected from the data sets, i.e., 10-fold cross-validation.
References-found: 13

