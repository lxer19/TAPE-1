URL: http://www.cs.cornell.edu/Info/People/coleman/PAPERS/CS97_1642.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/coleman/papers.html
Root-URL: 
Title: Trust Region and Affine Scaling Interior Point Method for Nonconvex Minimization with Linear Inequality Constraints  
Author: Thomas F. Coleman and Yuying Li 
Keyword: Key words. trust region, interior point method, Dikin-affine scaling, Newton step  
Note: A  
Abstract: Technical Report CORNELLCS: TR97-1642 Abstract. A trust region and affine scaling interior point method (TRAM) is proposed for a general nonlinear minimization with linear inequality constraints [8]. In the proposed approach, a Newton step is derived from the complementarity conditions. Based on this Newton step, a trust region subproblem is formed, and the original objective function is monotonically decreased. Explicit sufficient decrease conditions are proposed for satisfying complementarity, dual feasibility and second order optimality. The objective of this paper is to establish global and local convergence properties of the proposed trust region and affine scaling interior point method. It is shown that the proposed decrease conditions are sufficient for achieving complementarity, dual feasibility and second order optimality respectively. It is also established that a trust region solution is asymptotically in the interior of the proposed trust region subproblem and a damped trust region step can achieve quadratic convergence. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. K. Adler, M. Resende, and G. Veiga, </author> <title> An implementation of karmarkar's algorithm for linear programming, </title> <booktitle> Mathematical Programming, </booktitle> <month> 44 </month> <year> (1989). </year>
Reference-contexts: For convex programming problems, interior point methods have proven to be an efficient approach; see [17] for a comprehensive bibliography on these methods. Using these methods, a small number of iterations is typically required to solve large problems <ref> [1, 4, 26] </ref>. The quest for similar successful interior point algorithms for nonconvex programming problems has become increasingly important [13, 7, 9, 10, 18, 15, 3]. <p> The majority of interior point methods, e.g., a path following (see, e.g.,[16]) or a potential function reduction method (see, e.g., [24]), do not have this monotonicity property. Despite lack of polynomial convergence properties, an affine scaling method, e.g., <ref> [12, 1, 4, 26] </ref>, is the only interior point method which approaches a solution by monotonically decreasing the original objective function. For various structured problems, affine scaling Newton methods have been proposed [6, 5, 19, 7, 20, 18, 2, 10].
Reference: [2] <author> M. A. Branch, T. F. Coleman, and Y. Li, </author> <title> A subspace, interior and conjugate gradient method for large-scale bound-constrained minimization, </title> <type> Tech. Report TR95-1525, </type> <institution> Computer Science Department, Cornell University, </institution> <year> 1995. </year>
Reference-contexts: Despite lack of polynomial convergence properties, an affine scaling method, e.g., [12, 1, 4, 26], is the only interior point method which approaches a solution by monotonically decreasing the original objective function. For various structured problems, affine scaling Newton methods have been proposed <ref> [6, 5, 19, 7, 20, 18, 2, 10] </ref>. Using these methods, a sequence of "interior" points fx k g, with the objective function values monotonically decreasing, are generated to converge quadratically to a solution.
Reference: [3] <author> R. H. Byrd, J. C. Gilbert, and J. Nocedal, </author> <title> A trust region method based on interior point techniques for nonlinear programming, </title> <type> Tech. Report OTC96/02, </type> <institution> Optimization Technology Center, Northwestern University, </institution> <year> 1996. </year>
Reference-contexts: Using these methods, a small number of iterations is typically required to solve large problems [1, 4, 26]. The quest for similar successful interior point algorithms for nonconvex programming problems has become increasingly important <ref> [13, 7, 9, 10, 18, 15, 3] </ref>. Negative curvature in a nonconvex programming problem implies that there can be many local minimizers: a computational method is typically able to compute one of them. Assume that an initial feasible point x 0 is available.
Reference: [4] <author> Y.-C. Chen, D. J. Houck, J.-M. Liu, M. Meketon, R. J. V. L. Slutsman, and P. Wang, </author> <title> The AR&T KORBX System, </title> <journal> AT&T Tecnnical Journal, </journal> <year> (1989). </year>
Reference-contexts: For convex programming problems, interior point methods have proven to be an efficient approach; see [17] for a comprehensive bibliography on these methods. Using these methods, a small number of iterations is typically required to solve large problems <ref> [1, 4, 26] </ref>. The quest for similar successful interior point algorithms for nonconvex programming problems has become increasingly important [13, 7, 9, 10, 18, 15, 3]. <p> The majority of interior point methods, e.g., a path following (see, e.g.,[16]) or a potential function reduction method (see, e.g., [24]), do not have this monotonicity property. Despite lack of polynomial convergence properties, an affine scaling method, e.g., <ref> [12, 1, 4, 26] </ref>, is the only interior point method which approaches a solution by monotonically decreasing the original objective function. For various structured problems, affine scaling Newton methods have been proposed [6, 5, 19, 7, 20, 18, 2, 10].

Reference: [9] <author> T. F. Coleman and J. Liu, </author> <title> An interior Newton method for quadratic programming, </title> <type> Tech. Report TR93-1388, </type> <institution> Computer Science Department, Cornell University, </institution> <year> 1993. </year>
Reference-contexts: Using these methods, a small number of iterations is typically required to solve large problems [1, 4, 26]. The quest for similar successful interior point algorithms for nonconvex programming problems has become increasingly important <ref> [13, 7, 9, 10, 18, 15, 3] </ref>. Negative curvature in a nonconvex programming problem implies that there can be many local minimizers: a computational method is typically able to compute one of them. Assume that an initial feasible point x 0 is available.
Reference: [10] <author> J. E. Dennis, M. H. Jr., and L. N. Vicente, </author> <title> Trust-region interior-point sqp algorithms for a class of nonlinear programming problems, </title> <type> Tech. Report TR94-45, </type> <institution> Department of Computational and Applied Mathematics, Rice University, </institution> <year> 1994. </year>
Reference-contexts: Using these methods, a small number of iterations is typically required to solve large problems [1, 4, 26]. The quest for similar successful interior point algorithms for nonconvex programming problems has become increasingly important <ref> [13, 7, 9, 10, 18, 15, 3] </ref>. Negative curvature in a nonconvex programming problem implies that there can be many local minimizers: a computational method is typically able to compute one of them. Assume that an initial feasible point x 0 is available. <p> Despite lack of polynomial convergence properties, an affine scaling method, e.g., [12, 1, 4, 26], is the only interior point method which approaches a solution by monotonically decreasing the original objective function. For various structured problems, affine scaling Newton methods have been proposed <ref> [6, 5, 19, 7, 20, 18, 2, 10] </ref>. Using these methods, a sequence of "interior" points fx k g, with the objective function values monotonically decreasing, are generated to converge quadratically to a solution.
Reference: [11] <author> J. E. Dennis and J. J. Mor e, </author> <title> Quasi-Newton methods, motivation and theory, </title> <journal> SIAM Review, </journal> <volume> 19 (1977), </volume> <pages> pp. 46-89. </pages>
Reference-contexts: Since the first order necessary conditions are satisfied at (x fl ; fl ) with strict complementarity, using Lemma 4.3, kdiag ( p p Thus kJ (x k ; k ) T ~ J T p Applying Theorem 3.4 in <ref> [11] </ref>, f (x k ; p k )g converges quadratically to (x fl ; fl ). Theorem 4.5. Assume that (AS.0) holds and f (x) : F ! &lt; is twice continuously differentiable on F .
Reference: [12] <author> I. Dikin, </author> <title> Iterative solution of problems of linear and quadratic programming, </title> <journal> Doklady Akademiia Nauk SSSR, </journal> <volume> 174 (1967), </volume> <pages> pp. 747-748. </pages>
Reference-contexts: The majority of interior point methods, e.g., a path following (see, e.g.,[16]) or a potential function reduction method (see, e.g., [24]), do not have this monotonicity property. Despite lack of polynomial convergence properties, an affine scaling method, e.g., <ref> [12, 1, 4, 26] </ref>, is the only interior point method which approaches a solution by monotonically decreasing the original objective function. For various structured problems, affine scaling Newton methods have been proposed [6, 5, 19, 7, 20, 18, 2, 10]. <p> The diagonal scaling D 2 k in the 2-norm trust region bound constraint serves a similar purpose as Dikin affine scaling <ref> [12] </ref> for a linear programming problem. The classical Dikin affine scaling uses D 1 k rather than D 1 k .
Reference: [13] <author> A. S. El-Bakry, R. A. Tapia, T. Tsuchiya, and T. Zhang, </author> <title> On the formulation and theory of the Newton interior-point method for nonlinear programming, </title> <year> (1992). </year>
Reference-contexts: Using these methods, a small number of iterations is typically required to solve large problems [1, 4, 26]. The quest for similar successful interior point algorithms for nonconvex programming problems has become increasingly important <ref> [13, 7, 9, 10, 18, 15, 3] </ref>. Negative curvature in a nonconvex programming problem implies that there can be many local minimizers: a computational method is typically able to compute one of them. Assume that an initial feasible point x 0 is available.
Reference: [14] <author> R. Fletcher, </author> <title> Practical Methods of Optimization: Volume 2, Constrained Optimization, </title> <publisher> John Wiley and Sons, </publisher> <year> 1981. </year>
Reference-contexts: In [8], a trust region subproblem is derived from the Newton step for the complementarity conditions of (1.1). The complementarity conditions of (1.1), e.g., <ref> [14] </ref>, can 2 be expressed as an (m + n)-by-(m + n) system of nonlinear equations diag (Ax b) = 0 and A T rf = 0:(2.3) Consider the solution (p N k ; k+1 k ) to the following system, " C k A D k p N k+1 k
Reference: [15] <author> A. Forsgren and P. E. Gill, </author> <title> Primal-dual interior methods for nonconvex nonlinear programming, </title> <type> Tech. Report NA 96-3, </type> <institution> Department of Mathematics, University of California, </institution> <address> San Diego, </address> <year> 1996. </year>
Reference-contexts: Using these methods, a small number of iterations is typically required to solve large problems [1, 4, 26]. The quest for similar successful interior point algorithms for nonconvex programming problems has become increasingly important <ref> [13, 7, 9, 10, 18, 15, 3] </ref>. Negative curvature in a nonconvex programming problem implies that there can be many local minimizers: a computational method is typically able to compute one of them. Assume that an initial feasible point x 0 is available.
Reference: [16] <author> C. C. Gonzaga, </author> <title> Path following methods for linear programming, </title> <journal> SIAM Review, </journal> <volume> 34 (1992), </volume> <pages> pp. 167-227. </pages>
Reference: [17] <author> R. Kranich, </author> <title> Interior point methods for mathematical programming: A bibliograph, </title> <type> tech. report, </type> <note> Available through NETLIB: send e-mail to netlib@research.att.com containing the message "send intbib.bib from bib. </note>
Reference-contexts: This cost often greatly surpasses that of linear algebra work. An efficient algorithm for a nonlinear programming problem should require as few evaluations as possible. For convex programming problems, interior point methods have proven to be an efficient approach; see <ref> [17] </ref> for a comprehensive bibliography on these methods. Using these methods, a small number of iterations is typically required to solve large problems [1, 4, 26]. The quest for similar successful interior point algorithms for nonconvex programming problems has become increasingly important [13, 7, 9, 10, 18, 15, 3].
Reference: [18] <author> Y. Li, </author> <title> A Newton acceleration of the Weiszfeld algorithm for minimizing the sum of Euclidean distances, </title> <note> Computational Optimization and Applications, to appear. </note>
Reference-contexts: Using these methods, a small number of iterations is typically required to solve large problems [1, 4, 26]. The quest for similar successful interior point algorithms for nonconvex programming problems has become increasingly important <ref> [13, 7, 9, 10, 18, 15, 3] </ref>. Negative curvature in a nonconvex programming problem implies that there can be many local minimizers: a computational method is typically able to compute one of them. Assume that an initial feasible point x 0 is available. <p> Despite lack of polynomial convergence properties, an affine scaling method, e.g., [12, 1, 4, 26], is the only interior point method which approaches a solution by monotonically decreasing the original objective function. For various structured problems, affine scaling Newton methods have been proposed <ref> [6, 5, 19, 7, 20, 18, 2, 10] </ref>. Using these methods, a sequence of "interior" points fx k g, with the objective function values monotonically decreasing, are generated to converge quadratically to a solution.
Reference: [19] <author> Y. Li, </author> <title> A globally convergent method for l p problems, </title> <journal> SIAM Journal on Optimization, </journal> <year> (1993), </year> <pages> pp. 609-629. </pages>
Reference-contexts: Despite lack of polynomial convergence properties, an affine scaling method, e.g., [12, 1, 4, 26], is the only interior point method which approaches a solution by monotonically decreasing the original objective function. For various structured problems, affine scaling Newton methods have been proposed <ref> [6, 5, 19, 7, 20, 18, 2, 10] </ref>. Using these methods, a sequence of "interior" points fx k g, with the objective function values monotonically decreasing, are generated to converge quadratically to a solution.
Reference: [20] <author> Y. Li, </author> <title> A trust region and affine scaling method for nonlinearly constrained minimization, </title> <type> Tech. Report TR 94-1463, </type> <institution> Computer Science Department, Cornell University, </institution> <year> 1994, </year> <note> (Submitted to SIAM J. Optimization). </note>
Reference-contexts: Despite lack of polynomial convergence properties, an affine scaling method, e.g., [12, 1, 4, 26], is the only interior point method which approaches a solution by monotonically decreasing the original objective function. For various structured problems, affine scaling Newton methods have been proposed <ref> [6, 5, 19, 7, 20, 18, 2, 10] </ref>. Using these methods, a sequence of "interior" points fx k g, with the objective function values monotonically decreasing, are generated to converge quadratically to a solution.
Reference: [21] <author> R. Monteriro, T. Tsuchiya, and Y. Wang, </author> <title> A simplified global convergence proof of the affine scaling algorithm, </title> <journal> Annals of Operation Research, </journal> <year> (1993). </year>
Reference-contexts: Stepsize choice is important for computational and theoretical convergence be haviors of an affine scaling method for linear programming <ref> [21, 25] </ref>. One would like to take as large a step as possible but this may bring the iterates to the boundary prematurely. Nonlinearity of the problem (1.1) can both alleviate and exacerbate this problem.
Reference: [22] <author> J. J. Mor e, </author> <title> Recent developments in algorithms and software for trust region methods, in Mathematical Programming: The State of the Art, </title> <editor> M. G. A. Bachem and B. Korte, eds., </editor> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1983. </year>
Reference-contexts: Hence ( fl ) i &gt; 0 for any a T i x fl b i = 0. The proof is completed. Lemma 3.3 relates decrease required by (AS.1) and (AS.2) with complementarity and dual feasibility. Its proof is similar to Lemma (4.8) in <ref> [22] </ref>. Lemma 3.3. Assume that fx k g is generated from TRAM, (AS.0) is satisfied, and f (x) : F ! &lt; is continuously differentiable and fB k g is bounded.
Reference: [23] <author> J. J. Mor e and D. Sorensen, </author> <title> Computing a trust region step, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 4 (1983), </volume> <pages> pp. 553-572. </pages>
Reference-contexts: Hence lim k!1 rf T k ~g k = 0. Thus dual feasibility is satisfied at every limit point satisfying strict complementarity. The proof is completed. Before we examine second order convergence, several technical lemmas are re quired. First, we quote Lemma (4.10) in <ref> [23] </ref> below. Lemma 3.8. Let x fl be an isolated limit point of a sequence fx k g in &lt; n .
Reference: [24] <author> M. Todd, </author> <title> Potential-reduction methods in mathematical programming, </title> <type> Tech. Report TR1112, </type> <institution> School of Operations Research and Industrial Engineering, Cornell University, </institution> <year> 1995. </year> <month> 25 </month>
Reference-contexts: Assume that an initial feasible point x 0 is available. An algorithm for which the original objective function is monotonically decreased is desirable in a general nonlinear minimization context. The majority of interior point methods, e.g., a path following (see, e.g.,[16]) or a potential function reduction method (see, e.g., <ref> [24] </ref>), do not have this monotonicity property. Despite lack of polynomial convergence properties, an affine scaling method, e.g., [12, 1, 4, 26], is the only interior point method which approaches a solution by monotonically decreasing the original objective function.
Reference: [25] <author> T. Tsuchiya and R. Monteriro, </author> <title> Superlinear convergence of the affine scaling algorithm, </title> <booktitle> Math--ematical Programming, </booktitle> <year> (1996). </year>
Reference-contexts: Stepsize choice is important for computational and theoretical convergence be haviors of an affine scaling method for linear programming <ref> [21, 25] </ref>. One would like to take as large a step as possible but this may bring the iterates to the boundary prematurely. Nonlinearity of the problem (1.1) can both alleviate and exacerbate this problem.
Reference: [26] <author> R. J. Vanderbei, M. S. Meketon, and B. A. Freedman, </author> <title> A modification of Karmarkar's linear programming algorithm, </title> <journal> Algorithmica, </journal> <volume> 1 (1986), </volume> <pages> pp. 395-407. 26 </pages>
Reference-contexts: For convex programming problems, interior point methods have proven to be an efficient approach; see [17] for a comprehensive bibliography on these methods. Using these methods, a small number of iterations is typically required to solve large problems <ref> [1, 4, 26] </ref>. The quest for similar successful interior point algorithms for nonconvex programming problems has become increasingly important [13, 7, 9, 10, 18, 15, 3]. <p> The majority of interior point methods, e.g., a path following (see, e.g.,[16]) or a potential function reduction method (see, e.g., [24]), do not have this monotonicity property. Despite lack of polynomial convergence properties, an affine scaling method, e.g., <ref> [12, 1, 4, 26] </ref>, is the only interior point method which approaches a solution by monotonically decreasing the original objective function. For various structured problems, affine scaling Newton methods have been proposed [6, 5, 19, 7, 20, 18, 2, 10].
References-found: 22

