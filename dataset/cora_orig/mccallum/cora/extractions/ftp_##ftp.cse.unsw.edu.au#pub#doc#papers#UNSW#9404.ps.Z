URL: ftp://ftp.cse.unsw.edu.au/pub/doc/papers/UNSW/9404.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/research/tr.html
Root-URL: http://www.cse.unsw.edu.au
Title: Intrinsic Complexity of Language Identification  
Author: Sanjay Jain and Arun Sharma 
Affiliation: SCHOOL OF COMPUTER SCIENCE AND ENGINEERING THE UNIVERSITY OF NEW SOUTH WALES  
Note: On the  
Abstract: SCS&E Report 9404 March, 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Finding patterns common to a set of strings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 21 </volume> <pages> 46-62, </pages> <year> 1980. </year>
Reference-contexts: We show that this class is not complete and is in fact equivalent to COINIT under the strong reduction. The second class is the collection of pattern languages introduced by Angluin <ref> [1] </ref>. Pattern languages have been studied extensively in the computational learning theory literature since their introduction as a nontrivial class of languages that could be learned in the limit from only positive data. <p> It is easy to see that fi and witness COINIT TxtEx strong WIEHAGEN. Corollary 1 COINIT j TxtEx strong WIEHAGEN. We next consider the class, PATTERN , of pattern languages introduced by Angluin <ref> [1] </ref>. Suppose V is a set of variables and C is a nonempty finite set of constants. Any w 2 (V [ C) + is called a pattern. <p> First note that for patterns w 1 and w 2 , if L (w 1 ) L (w 2 ) then length of w 1 is at least as large as that of w 2 . Also for patterns of the same length relation is decidable <ref> [1] </ref>. Thus we can form the indexing as required using the following method. We consider only canonical patterns [1]. <p> Also for patterns of the same length relation is decidable <ref> [1] </ref>. Thus we can form the indexing as required using the following method. We consider only canonical patterns [1]. <p> The intrinsic complexity of several classes were considered. It was shown that the self referential class of Wiehagen [15] in which the least element of every language is a grammar for the language and the class of pattern languages introduced by Angluin <ref> [1] </ref> are equivalent in the strong sense. A number of complete classes were presented for both the reductions. It was also shown that the weak and strong reductions are distinct. The results presented were for the widely studied identification in the limit criterion.
Reference: [2] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference-contexts: Let code denote a 1-1 onto mapping from strings in C fl to N . The language associated with the pattern w is defined as L (w) = fcode (f (w)) j f 2 PatMapg. Then, PATTERN = fL (w) j w is a patterng. Angluin <ref> [2] </ref> showed that PATTERN 2 TxtEx. However, we show that PATTERN is not TxtEx weak -complete. Theorem 5 FIN 6 TxtEx weak PATTERN . The above theorem follows directly from Theorem 2, since for any string x, there are only finitely many patterns w such that x 2 L (w).
Reference: [3] <author> M. Blum. </author> <title> A machine independent theory of the complexity of recursive functions. </title> <journal> Journal of the ACM, </journal> <volume> 14 </volume> <pages> 322-336, </pages> <year> 1967. </year>
Reference-contexts: The letter, p, in some contexts, with or without decorations, ranges over programs; in other contexts p ranges over total functions with its range being construed as programs. By we denote an arbitrary fixed Blum complexity measure <ref> [3, 10] </ref> for the '-system. By W i we denote domain (' i ). W i is, then, the r.e. set/language ( N ) accepted (or equivalently, generated) by the '-program i. We also say that i is a grammar for W i . <p> Hence, we adopt the machinery defined for sequences and texts over to finite sequences of grammars and infinite sequences of grammars. Hence, if G = g 0 ; g 1 ; g 2 ; g 3 ; : : :, then G <ref> [3] </ref> denotes the sequence g 0 ; g 1 ; g 2 , G (3) is g 3 , last (G [3]) is g 2 , and prev (G [3]) is the sequence g 0 ; g 1 . Let I be an identification criterion. <p> Hence, if G = g 0 ; g 1 ; g 2 ; g 3 ; : : :, then G <ref> [3] </ref> denotes the sequence g 0 ; g 1 ; g 2 , G (3) is g 3 , last (G [3]) is g 2 , and prev (G [3]) is the sequence g 0 ; g 1 . Let I be an identification criterion. <p> Hence, if G = g 0 ; g 1 ; g 2 ; g 3 ; : : :, then G <ref> [3] </ref> denotes the sequence g 0 ; g 1 ; g 2 , G (3) is g 3 , last (G [3]) is g 2 , and prev (G [3]) is the sequence g 0 ; g 1 . Let I be an identification criterion. We say that an infinite sequence of grammars G is I-admissible for text T just in case G is an infinite sequence of grammars witnessing I-identification of text T .
Reference: [4] <author> J. </author> <title> Case. Periodicity in generations of automata. </title> <journal> Mathematical Systems Theory, </journal> <volume> 8 </volume> <pages> 15-32, </pages> <year> 1974. </year>
Reference-contexts: It is easy to see that fi and witness WIEHAGEN TxtEx strong COINIT. Theorem 4 COINIT TxtEx strong WIEHAGEN. Proof. By operator recursion theorem <ref> [4] </ref> there exists a recursive 1-1 increasing function p such that for all i, W p (i) = fx j x p (i)g. Let fi be such that fi (L) = fx p (i) j i 2 Lg. Note that such a fi can be easily constructed.
Reference: [5] <author> J. </author> <title> Case. The power of vacillation. </title> <editor> In D. Haussler and L. Pitt, editors, </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 133-142. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year> <note> Expanded in [6]. </note>
Reference-contexts: We discuss our results in the context of the identification in the limit paradigm [9]. The analysis can easily be applied to other paradigms like finite identification, behaviorally correct identification [13, 7], and vacillatory identification <ref> [13, 5] </ref> and will be presented in the full paper. We next introduce some technical notions about language learning in order to facilitate our discussion. Informally, a text for a language L is just an infinite sequence of elements, with possible repetitions, of all and only the elements of L. <p> Other criteria of success are finite identification, behaviorally correct identification [13, 7], and vacillatory identification <ref> [13, 5] </ref>. In the present extended abstract, we only discuss results about TxtEx-identification; results relating to the remaining criteria will be presented in the full paper. 3 Weak and Strong Reductions Before we present our reductions we introduce some technical machinery.
Reference: [6] <author> J. </author> <title> Case. The power of vacillation in language learning. </title> <type> Technical Report 93-08, </type> <institution> University of Delaware, </institution> <year> 1992. </year> <note> Expands on [5]; journal article under review. </note>
Reference: [7] <author> J. Case and C. Lynes. </author> <title> Machine inductive inference and language identification. </title> <editor> In M. Nielsen and E. M. Schmidt, editors, </editor> <booktitle> Proceedings of the 9th International Colloquium on Automata, Languages and Programming, </booktitle> <volume> volume 140, </volume> <pages> pages 107-115. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1982. </year>
Reference-contexts: We discuss our results in the context of the identification in the limit paradigm [9]. The analysis can easily be applied to other paradigms like finite identification, behaviorally correct identification <ref> [13, 7] </ref>, and vacillatory identification [13, 5] and will be presented in the full paper. We next introduce some technical notions about language learning in order to facilitate our discussion. <p> Other criteria of success are finite identification, behaviorally correct identification <ref> [13, 7] </ref>, and vacillatory identification [13, 5]. In the present extended abstract, we only discuss results about TxtEx-identification; results relating to the remaining criteria will be presented in the full paper. 3 Weak and Strong Reductions Before we present our reductions we introduce some technical machinery.
Reference: [8] <author> R Freivalds, E. Kinber, and C. H. Smith. </author> <title> On the intrinsic complexity of learning. </title> <note> unpublished. </note>
Reference-contexts: The present paper describes a model which provides an insight into why certain classes are more easily learned than others. Our model adopts a similar study in the context of learning functions by Freivalds, Kinber, and Smith <ref> [8] </ref>. The main idea of the approach is to introduce reductions between collections of languages. If a collection L 1 can be reduced to a collection L 2 , then the learnability of L 1 is no more difficult than that of L 2 . <p> The model described in the present paper captures this gradation in difficulty of various identifiable collections of languages. Following Freivalds, Kinber, and Smith <ref> [8] </ref>, we refer to such a notion of difficulty as "intrinsic complexity." We next present an informal description of reductions that are central to our analysis 1 of the intrinsic complexity of language learning. We discuss our results in the context of the identification in the limit paradigm [9]. <p> If we further require that fi is such that it transforms all texts for a language into texts for some unique language then we have a stronger notion of reduction. In the context of function learning <ref> [8] </ref>, these two notions of reductions are the same. However, surprisingly, in the context of language identification this stronger notion of reduction turns out to be different from its weaker counterpart as we are able to show that FIN is not complete with respect to the stronger reduction. <p> It is felt that the reductions studied in the present paper lay a foundation on which feasibility issues in language identification can be studied. Acknowledgements Our study has clearly been influenced by the work on intrinsic complexity of computable functions by Freivalds, Kinber, and Smith <ref> [8] </ref>. We would like to thank Efim Kinber for helpful discussion and for encouraging us to undertake the present study. 13
Reference: [9] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: We discuss our results in the context of the identification in the limit paradigm <ref> [9] </ref>. The analysis can easily be applied to other paradigms like finite identification, behaviorally correct identification [13, 7], and vacillatory identification [13, 5] and will be presented in the full paper. We next introduce some technical notions about language learning in order to facilitate our discussion. <p> A machine is said to TxtEx-identify a language just in case it TxtEx-identifies each text for the language. TxtEx-identification is essentially identification in the limit paradigm introduced by Gold <ref> [9] </ref>. It is also useful to call a sequence of grammars, g 0 ; g 1 ; g 2 ; : : :, TxtEx-admissible for a text T just in case the sequence of grammars converges to a single correct grammar for the language represented by text T . <p> Thus, M (T [n]) is interpreted as the grammar (index for an accepting program) conjectured by learning machine M on initial sequence T [n]. There are several criteria for a learning machine to be successful on a language. The one defined below was introduced by Gold <ref> [9] </ref> and is also known in the literature as "identification in the limit." Definition 4 [9] (a) M TxtEx-identifies a text T just in case (9i j W i = content (T )) ( 1 i]. (b) M TxtEx-identifies an r.e. language L (written: L 2 TxtEx (M)) just in case <p> There are several criteria for a learning machine to be successful on a language. The one defined below was introduced by Gold <ref> [9] </ref> and is also known in the literature as "identification in the limit." Definition 4 [9] (a) M TxtEx-identifies a text T just in case (9i j W i = content (T )) ( 1 i]. (b) M TxtEx-identifies an r.e. language L (written: L 2 TxtEx (M)) just in case M TxtEx-identifies each text for L. (c) TxtEx = fL E j (9M)[L TxtEx (M)]g. <p> It is easy to see that fi and witness that CONTON n TxtEx strong COSINGLE. Since CONTON n COFIN, we trivially have CONTON n TxtEx strong COFIN (note however that COFIN 62 TxtEx <ref> [9] </ref>). 10 Theorem 9 (a) COSINGLE is TxtEx weak -complete. (b) COFIN is TxtEx weak -hard. (c) For all n 2 N + , CONTON n is TxtEx weak -complete. Proof. We prove part (a). Other parts follow as corollaries. Suppose L TxtEx (M). <p> Since for all j, L j L TxtEx , it follows that L TxtEx is TxtEx strong -complete for TxtEx. 12 5 Language Identification from Informants The concepts of weak and strong reduction can be adopted to language identification from informants. Informally, informants, first introduced by Gold <ref> [9] </ref>, are texts which contain both positive and negative data. Thus if I L is an informant for L, then content (I L ) = fhx; 0i j x 62 Lg [ fhx; 1i j x 2 Lg. <p> Thus if I L is an informant for L, then content (I L ) = fhx; 0i j x 62 Lg [ fhx; 1i j x 2 Lg. Identification in the limit from informants is referred to as InfEx-identification (we refer the reader to <ref> [9] </ref> for details). The definition of weak and strong reduction can be adopted to language identification from informants in a strainghforward way by replacing texts by informants in the Definitions 5 and 7.
Reference: [10] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory Languages and Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1979. </year>
Reference-contexts: The letter, p, in some contexts, with or without decorations, ranges over programs; in other contexts p ranges over total functions with its range being construed as programs. By we denote an arbitrary fixed Blum complexity measure <ref> [3, 10] </ref> for the '-system. By W i we denote domain (' i ). W i is, then, the r.e. set/language ( N ) accepted (or equivalently, generated) by the '-program i. We also say that i is a grammar for W i .
Reference: [11] <author> M. Machtey and P. Young. </author> <title> An Introduction to the General Theory of Algorithms. </title> <publisher> North Holland, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: We define 1 (hx; yi) = x and 2 (hx; yi) = y. h; i can be extended to n-tuples in a natural way. By ' we denote a fixed acceptable programming system for the partial computable functions: N ! N <ref> [14, 11] </ref>. By ' i we denote the partial computable function computed by program i in the '-system. The letter, p, in some contexts, with or without decorations, ranges over programs; in other contexts p ranges over total functions with its range being construed as programs.
Reference: [12] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: We finally present a collection of languages that is complete with respect to strong reduction. Suppose M 0 ; M 1 ; : : : is an enumeration of the learning machines such that, (8L 2 TxtEx)(9i)[L TxtEx (M i )] (there exists such an enumeration, see for example <ref> [12] </ref>). For j 2 N and L 2 E, let S L = fhx; ji j x 2 Lg. Then, let L TxtEx = fS j N ^ L 2 TxtEx (M j )g. Theorem 15 L TxtEx is TxtEx strong complete for TxtEx. Proof.
Reference: [13] <author> D. Osherson and S. Weinstein. </author> <title> Criteria of language learning. </title> <journal> Information and Control, </journal> <volume> 52 </volume> <pages> 123-138, </pages> <year> 1982. </year>
Reference-contexts: We discuss our results in the context of the identification in the limit paradigm [9]. The analysis can easily be applied to other paradigms like finite identification, behaviorally correct identification <ref> [13, 7] </ref>, and vacillatory identification [13, 5] and will be presented in the full paper. We next introduce some technical notions about language learning in order to facilitate our discussion. <p> We discuss our results in the context of the identification in the limit paradigm [9]. The analysis can easily be applied to other paradigms like finite identification, behaviorally correct identification [13, 7], and vacillatory identification <ref> [13, 5] </ref> and will be presented in the full paper. We next introduce some technical notions about language learning in order to facilitate our discussion. Informally, a text for a language L is just an infinite sequence of elements, with possible repetitions, of all and only the elements of L. <p> Other criteria of success are finite identification, behaviorally correct identification <ref> [13, 7] </ref>, and vacillatory identification [13, 5]. In the present extended abstract, we only discuss results about TxtEx-identification; results relating to the remaining criteria will be presented in the full paper. 3 Weak and Strong Reductions Before we present our reductions we introduce some technical machinery. <p> Other criteria of success are finite identification, behaviorally correct identification [13, 7], and vacillatory identification <ref> [13, 5] </ref>. In the present extended abstract, we only discuss results about TxtEx-identification; results relating to the remaining criteria will be presented in the full paper. 3 Weak and Strong Reductions Before we present our reductions we introduce some technical machinery.
Reference: [14] <author> H. Rogers. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <address> New York, 1967. </address> <publisher> Reprinted, MIT Press 1987. </publisher>
Reference-contexts: In Section 3, we introduce our reducibilities. Results are presented in Section 4. Finally, in Section 5, we look at the intrinsic complexity of language identification from both positive and negative data. 2 Notation and Preliminaries Any unexplained recursion theoretic notation is from <ref> [14] </ref>. The symbol N denotes the set of natural numbers, f0; 1; 2; 3; : : :g. Unless otherwise specified, e; g; i; j; k; l; m; n; q; r; s; t; w; x; y, with or without decorations 1 , range over N . <p> Symbols A and S, with or without decorations, range over sets. S, with or without decorations, ranges over finite sets. D 0 ; D 1 ; : : : ; denotes a canonical recursive indexing of all the finite sets <ref> [14] </ref>. We assume that if D i D j then i j (indexing defined in [14] satisfies this property). Cardinality of a set S is denoted by card (S). <p> S, with or without decorations, ranges over finite sets. D 0 ; D 1 ; : : : ; denotes a canonical recursive indexing of all the finite sets <ref> [14] </ref>. We assume that if D i D j then i j (indexing defined in [14] satisfies this property). Cardinality of a set S is denoted by card (S). <p> Symbol R denotes the set of all total computable functions. A pair hi; ji stands for an arbitrary, computable, one-to-one encoding of all pairs of natural numbers onto N <ref> [14] </ref>. We define 1 (hx; yi) = x and 2 (hx; yi) = y. h; i can be extended to n-tuples in a natural way. By ' we denote a fixed acceptable programming system for the partial computable functions: N ! N [14, 11]. <p> We define 1 (hx; yi) = x and 2 (hx; yi) = y. h; i can be extended to n-tuples in a natural way. By ' we denote a fixed acceptable programming system for the partial computable functions: N ! N <ref> [14, 11] </ref>. By ' i we denote the partial computable function computed by program i in the '-system. The letter, p, in some contexts, with or without decorations, ranges over programs; in other contexts p ranges over total functions with its range being construed as programs. <p> It is easy to verify that fi and witness SINGLE TxtEx strong COINIT. Now suppose by way of contradiction that COINIT TxtEx weak SINGLE as witnessed by fi and . By Smullyan's double recursion theorem <ref> [14] </ref>, there exist e 1 &lt; e 2 such that W e 1 = fx j x e 1 g and W e 2 = fx j x e 2 g.
Reference: [15] <author> R. Wiehagen. </author> <title> Identification of formal languages. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 53 </volume> <pages> 571-579, </pages> <year> 1977. </year>
Reference-contexts: We give an example of complete class with respect to the strong reduction. We now discuss two interesting collections that are shown not to be complete with respect to either reduction. The first one is a class of languages introduced by Wiehagen <ref> [15] </ref> which contains all those languages L such that the minimum element in L is a grammar for L. This self-referential class, which can be TxtEx-identified, is a very interesting class as it contains a finite variant of every recursively enumerable language. <p> But then fi and do not witness that FIN TxtEx weak L. Our next example is a collection of languages first introduced by Wiehagen <ref> [15] </ref>. We define, WIEHAGEN = fL 2 E j L = W min (L) g. WIEHAGEN is an interesting class because it can be shown that it contains a finite variant of every recursively enumerable language. It is easy to verify that WIEHAGEN 2 TxtEx. <p> The intrinsic complexity of several classes were considered. It was shown that the self referential class of Wiehagen <ref> [15] </ref> in which the least element of every language is a grammar for the language and the class of pattern languages introduced by Angluin [1] are equivalent in the strong sense. A number of complete classes were presented for both the reductions.
References-found: 15

