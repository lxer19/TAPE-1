URL: ftp://ftp.fwi.uva.nl/pub/computer-systems/aut-sys/reports/wiering.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Email: e-mail: wiering@fwi.uva.nl  
Title: TD Learning of Game Evaluation Functions with Hierarchical Neural Architectures  
Author: Marco A. Wiering 
Note: 1  
Date: April 7, 1995  
Address: Amsterdam  
Affiliation: Department of Computer Systems Faculty of Mathematics and Computer Science University of  
Abstract-found: 0
Intro-found: 1
Reference: [Aarts89] <author> E.H.L. Aarts & Jan Korst. </author> <title> Simulated Annealing and Boltzmann Machines. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1989. </year>
Reference-contexts: When we are often trapped in local minima we should use other minimization procedures e.g. simulated annealing <ref> [Aarts89] </ref>, but these are often significantly slower than conventional back-propagation. 3.2.4 Representing Discontinuous Functions When learning functions which might contain many discontinuities, high accuracy can only be obtained when we use an architecture which can represent these discontinuities. 3.3.
Reference: [Anthony91] <author> M. Anthony. </author> <title> Uniform Convergence and Learnability. </title> <type> PhD thesis, </type> <institution> University of London, </institution> <year> 1991. </year>
Reference-contexts: When we want to learn a complex task, we can expect that the input space for this task is very large and we must use a representation with a large expressive power. The expressive power or VC dimension of a neural network depends mostly on the number of weights <ref> [Anthony91] </ref>, and when we have enough examples and hidden units in the network we are able to learn most functions until a specified level of precision [Vysniausk93]. This means that the architecture contains one or more 'good' states.
Reference: [Berliner77] <author> H. Berliner. </author> <title> Experiences in evaluation with BKG a program that plays backgammon. </title> <booktitle> Proceedings of IJCAI, </booktitle> <pages> (428-433), </pages> <year> 1977. </year>
Reference-contexts: Every time two dice are thrown to determine the possible moves. When equal dice are thrown, the player can play four times the number of eyes on a dice. In the example white plays a blocking move so that black's two pieces have problems going to black's home-table. BKG <ref> [Berliner77] </ref> was the first backgammon program. It was constructed during a human knowledge engineering period of four years after which all important features of the game were used in the reasoning process of the expert system.
Reference: [Boyan92] <author> J. Boyan. </author> <title> Modular Neural Networks for Learning Context-Dependent Game Strategies. </title> <type> Thesis report B.S. </type> , <institution> University of Chicago, </institution> <year> 1992. </year>
Reference-contexts: Expert systems for game-playing require an evaluation function which returns the expected payoff from a position given future optimal play of both sides. Some research studies learning game evaluation functions with neural networks <ref> [Tesauro92, Boyan92, Schraudol94] </ref>. Games provide domains where the evaluation function can differ drastically for similar positions (tic-tac-toe). When learning discontinuous functions, single neural networks tend to generalize over the discontinuities which results in small regions where the local error is very high. <p> In Chapter 5 we will conclude the research and describe the work which has to be done in the future. Chapter 2 Game Playing 2.1 Introduction Games define domains which are easy to represent and evaluate, while expert-level play may require sophisticated abilities of planning, pattern recognition and memory <ref> [Boyan92] </ref>. Computer game algorithms mostly use a position evaluator function which returns the expected payoff for a given position. To decide on a move in a given situation means comparing all possible positions resulting from the current admissible moves or comparing all positions which result from sequences of multiple moves. <p> Tesauro's work was extended by <ref> [Boyan92] </ref> who used a priori knowledge to decompose the input space into subspaces for which independent expert networks were trained. Boyan used a Meta-Pi network [Hampshire89] to combine the evaluations of the trained experts so that a smoother evaluation function would be obtained. <p> This was shown on a speech recognition task where experts were trained on data from different speakers [Hampshire89] and on learning to play backgammon with multiple experts <ref> [Boyan92] </ref>. However, the Meta-Pi network can also be used when the experts are initialized randomly which makes the method more powerful, because we do not have to know an a priori input space division. <p> Using Q-learning would require encoding the move in the state vector which might decrease generalization abilities, because the moves which resulted in a specific position might differ. When we compare the obtained results with <ref> [Boyan92] </ref>, who obtained a maximal match-equity of 0.474 after playing more training games, they are impressive.
Reference: [Cybenko89] <author> G. Cybenko. </author> <title> Approximation by Superpositions of a Sigmoidal Function. </title> <journal> Math. Control Signals Systems, </journal> <volume> 2, </volume> <pages> (303-314), </pages> <year> 1989. </year>
Reference-contexts: Nowadays machine learning techniques are used to trade off human time for computer time. Learning means relating inputs with outputs by the use of many examples. The use of neural networks provides a way to approximate any Borel measurable function <ref> [Cybenko89] </ref>. However, when learning the required knowledge for a real world application with a single neural network, the learning process takes a long time. Another problem with training single networks is that they can easily get caught in a local minimum. <p> They use feed-forward neural networks and back-propagation to learn the division, which makes it possible to learn game-strategies without a priori knowledge. Chapter 3 Function Approximation with Multiple Networks 3.1 Introduction Neural networks can approximate any Borel measurable function until a specified level of precision <ref> [Cybenko89] </ref>. However, the problem is to find good parameters for the networks, which has to be done by learning on examples. Published successes in connectionist learning have been empirical results for very small networks, typically much less than 100 nodes. <p> One hidden layer with enough contain processing units and is not counted as a 'processing' layer. units and non-linear activation functions is sufficient to approximate any Borel measurable function <ref> [Cybenko89] </ref> so all constructed networks in this paper will have at most one hidden layer. 3.2.1 Forward Propagation The first step in using a neural network is to encode the problem in an input to output vector mapping and to choose the number of hidden units.
Reference: [Dayan92] <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8, </booktitle> <pages> (341-362), </pages> <year> 1992. </year>
Reference-contexts: This supervised method results in a loss of precision, because every move would be held equally responsible for the obtained result, which is almost never true. Especially when we want to learn to play games with stochastic elements, the learning process might become very slow. Temporal difference (TD) learning <ref> [Sutton88, Tesauro92, Dayan92, Dayan94] </ref> provides an efficient method to receive learning examples with a higher accuracy, because the evaluation of a given position is adjusted by using the differences between its evaluation and the evaluations of successive positions.
Reference: [Dayan94] <author> P. Dayan & T.J. Sejnowski. </author> <title> TD() Converges with Probability 1. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> (295-301), </pages> <year> 1994. </year>
Reference-contexts: This supervised method results in a loss of precision, because every move would be held equally responsible for the obtained result, which is almost never true. Especially when we want to learn to play games with stochastic elements, the learning process might become very slow. Temporal difference (TD) learning <ref> [Sutton88, Tesauro92, Dayan92, Dayan94] </ref> provides an efficient method to receive learning examples with a higher accuracy, because the evaluation of a given position is adjusted by using the differences between its evaluation and the evaluations of successive positions. <p> Recently Dayan has proved that the TD algorithms converge with probability 1 when a linear representation of the input is used (e.g. lookup tables) <ref> [Dayan94] </ref>. A description of TD learning and an example of its use is given in Appendix A. There are two advantages to learn the game of backgammon by the TD methods over the game of tic-tac-toe. <p> Both are able to learn a control policy which maximizes an agent's performance level, provided that a linear representation of the input (lookup tables) is used, all actions are repeatedly sampled in all states and a proper scheduling of the learning rates is made <ref> [Watkins92, Dayan94] </ref>. The formalisms are formally described in Appendix A. Here we will concentrate on using both formalisms to learn game evaluation functions. AHC-learning learns an evaluation function for states. <p> Therefore the use of lookup tables is no viable paradigm when the state space is very large. However, Dayan has proved that lookup tables trained with temporal difference learning will find a global minimum <ref> [Dayan94] </ref>. Lookup tables are similar to the previous architectures with high neuron sensitivities for the hidden units, in the sense that most parameters are only adjusted in a specific part of the input space. <p> That is why TD learning works better. When TD learning is applied, the generalization error gradually decreases if more games are being played. This is not surprising, because Dayan has proved that TD learning always converges to at least a local minimum <ref> [Dayan94] </ref>. We have also studied using a mixture of supervised and TD learning. This method works better than either of these methods alone. So when only a small learning set is available, this may computationally be more effective than constructing a large learning set with dynamic programming.
Reference: [Esposito93] <author> F. Esposito, D. Malerba & G. Semeraro. </author> <title> Decision Tree Pruning as a Search in the State Space. </title> <editor> In P. B. Brazdil (ed.), </editor> <booktitle> Proceedings of the 1993 European Conference on Machine Learning, </booktitle> <pages> (166-184), </pages> <address> Vienna, </address> <year> 1993. </year>
Reference-contexts: When the state space becomes much larger, it takes more time to find the best state. This has consequences for learning : we have to have an architecture which contains a 'good' solution, but we do not want to have too many superfluous states. Some researchers use pruning <ref> [Esposito93] </ref>, and others make it possible for the architecture to grow [Schaffer92, Gruau92, Simon92]. These methods can be used to find an architecture which contains at least one 'good' state, but a minimum of superfluous states.
Reference: [Fox91] <author> D. Fox, V. Heinze, K. Moller, S. Thrun & G. Veenker. </author> <title> Learning by error-driven decomposition. </title> <editor> In T.Kohonen, K. Mkisara, O. Simular & J. Kangas (eds.), </editor> <booktitle> Proceedings of the 1991 International Conference on Artificial Neural Networks, </booktitle> <volume> (207 - 212), </volume> <publisher> Amsterdam, North-Holland, </publisher> <year> 1991. </year>
Reference-contexts: Nowadays, some researchers are focussing on modular architectures which consist of some small specialistic or expert networks which co-operate to learn the desired function <ref> [Jordan92, Nowlan91, Hampshire89, Fox91, Hashem93] </ref>. For many tasks, this results in more understandable systems which are easier to train, because each expert network learns a specific sub-function and experts can be analyzed and refined independently.
Reference: [Gruau92] <author> F. Gruau. </author> <title> Genetic synthesis of boolean neural networks with a cell rewriting developmental process. </title> <editor> In L.D. Whitley & J.D. Schaffer (eds.), </editor> <booktitle> International Workshop on Combinations of Genetic Algorithms and Neural Networks, </booktitle> <pages> (55-72), </pages> <address> Baltimore, MD: </address> <publisher> IEEE, </publisher> <month> June </month> <year> 1992. </year> <note> 70 BIBLIOGRAPHY 71 </note>
Reference-contexts: This has consequences for learning : we have to have an architecture which contains a 'good' solution, but we do not want to have too many superfluous states. Some researchers use pruning [Esposito93], and others make it possible for the architecture to grow <ref> [Schaffer92, Gruau92, Simon92] </ref>. These methods can be used to find an architecture which contains at least one 'good' state, but a minimum of superfluous states.
Reference: [Hakala94] <author> J. Hakala, C. Koslowski & R. </author> <title> Eckmiller. 'Partition of Unity' RBF Networks are Universal Function Approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> (459-462), </pages> <year> 1994. </year>
Reference-contexts: This posterior probability depends on the error both experts make when presented with the learning example B. Gating Networks The Meta-Pi gating network uses non-linear activation functions for the output units which are strict positive e.g. RBFs <ref> [Hakala94] </ref>, or sigmoids can be used. The gates of the top-level gating network are given by g i = P ; s i &gt; 0 (3:11) where the activation of each output unit s i is computed by equation 3.1.
Reference: [Hampshire89] <author> J.B. Hampshire & A. Waibel. </author> <title> The Meta-Pi network: Building distributed knowledge representations for robust pattern recognition. </title> <type> Tech. Report CMU-CS-89-166, </type> <institution> Carnegie Mellon University, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: My master's thesis describes a study on three divide-and-conquer paradigms : the use of symbolic rules to divide the input space in fixed subspaces, and the hierarchical mixtures of experts [Jacobs91, Nowlan91, Jordan92, Jordan93] and the Meta-Pi network <ref> [Hampshire89] </ref> which use a hierarchy of gating networks to learn to divide the input space in subspaces for which expert networks are trained. These methodologies could be combined with temporal difference learning to learn context specific game-strategies without a priori knowledge. <p> Tesauro's work was extended by [Boyan92] who used a priori knowledge to decompose the input space into subspaces for which independent expert networks were trained. Boyan used a Meta-Pi network <ref> [Hampshire89] </ref> to combine the evaluations of the trained experts so that a smoother evaluation function would be obtained. Results showed that the use of multiple experts outperformed a single network, although his task-decomposition was very simple and did not use human strategies. <p> Nowadays, some researchers are focussing on modular architectures which consist of some small specialistic or expert networks which co-operate to learn the desired function <ref> [Jordan92, Nowlan91, Hampshire89, Fox91, Hashem93] </ref>. For many tasks, this results in more understandable systems which are easier to train, because each expert network learns a specific sub-function and experts can be analyzed and refined independently. <p> We will proceed by summarizing some of the work done in this field, starting with the hierarchical mixtures of experts (HME) methodology [Nowlan91, Jacobs91, Jordan92, Jordan93] in section 3.3.1. This description is followed by the second methodology by Hampshire and Waibel <ref> [Hampshire89] </ref> in section 3.3.2, which is based upon the Meta-Pi network. The use of a selection threshold to make the propagation of the architectures faster is described in section 3.3.3. The differences between the two architectures will be depicted in section 3.3.4. <p> The Meta-Pi network architecture as presented by <ref> [Hampshire89] </ref> has the same structure as given in figure 3.5, but instead of competing networks they use co-operating networks. <p> When the division of the input space is known and experts can be trained independently, the use of the Meta-Pi feed-forward network improves generalization abilities. This was shown on a speech recognition task where experts were trained on data from different speakers <ref> [Hampshire89] </ref> and on learning to play backgammon with multiple experts [Boyan92]. However, the Meta-Pi network can also be used when the experts are initialized randomly which makes the method more powerful, because we do not have to know an a priori input space division.
Reference: [Hashem93] <author> S. Hashem. </author> <title> Optimal Linear Combinations of Neural Networks. </title> <type> PhD thesis, Tech. Report SMS 94-4. </type> <institution> Purdue University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Nowadays, some researchers are focussing on modular architectures which consist of some small specialistic or expert networks which co-operate to learn the desired function <ref> [Jordan92, Nowlan91, Hampshire89, Fox91, Hashem93] </ref>. For many tasks, this results in more understandable systems which are easier to train, because each expert network learns a specific sub-function and experts can be analyzed and refined independently.
Reference: [Jacobs91] <author> R.A. Jacobs, M.I. Jordan, S.J. Nowlan & G.E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: My master's thesis describes a study on three divide-and-conquer paradigms : the use of symbolic rules to divide the input space in fixed subspaces, and the hierarchical mixtures of experts <ref> [Jacobs91, Nowlan91, Jordan92, Jordan93] </ref> and the Meta-Pi network [Hampshire89] which use a hierarchy of gating networks to learn to divide the input space in subspaces for which expert networks are trained. These methodologies could be combined with temporal difference learning to learn context specific game-strategies without a priori knowledge. <p> All gating and expert networks in the architecture receive the same input, although this is not a necessity. We will proceed by summarizing some of the work done in this field, starting with the hierarchical mixtures of experts (HME) methodology <ref> [Nowlan91, Jacobs91, Jordan92, Jordan93] </ref> in section 3.3.1. This description is followed by the second methodology by Hampshire and Waibel [Hampshire89] in section 3.3.2, which is based upon the Meta-Pi network. The use of a selection threshold to make the propagation of the architectures faster is described in section 3.3.3. <p> The differences between the two architectures will be depicted in section 3.3.4. A third architecture which uses knowledge bases containing fixed symbolic rules as gating networks will be described in section 3.3.4. 3.3.1 Hierarchical Mixtures of Experts <ref> [Jacobs91, Jordan92, Nowlan91] </ref> developed a modular gating architecture which can learn to divide the input space in subspaces. Their architecture consists of a number of expert feed-forward networks which receive the same input patterns and compete with each other to produce the desired output vectors.
Reference: [Jordan92] <author> M.I. Jordan & R.A. Jacobs. </author> <title> Hierarchies of adaptive experts. </title> <editor> In J. Moody, S. Hanson & R. Lippmann (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 4, </volume> <pages> (985-993), </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: My master's thesis describes a study on three divide-and-conquer paradigms : the use of symbolic rules to divide the input space in fixed subspaces, and the hierarchical mixtures of experts <ref> [Jacobs91, Nowlan91, Jordan92, Jordan93] </ref> and the Meta-Pi network [Hampshire89] which use a hierarchy of gating networks to learn to divide the input space in subspaces for which expert networks are trained. These methodologies could be combined with temporal difference learning to learn context specific game-strategies without a priori knowledge. <p> Nowadays, some researchers are focussing on modular architectures which consist of some small specialistic or expert networks which co-operate to learn the desired function <ref> [Jordan92, Nowlan91, Hampshire89, Fox91, Hashem93] </ref>. For many tasks, this results in more understandable systems which are easier to train, because each expert network learns a specific sub-function and experts can be analyzed and refined independently. <p> All gating and expert networks in the architecture receive the same input, although this is not a necessity. We will proceed by summarizing some of the work done in this field, starting with the hierarchical mixtures of experts (HME) methodology <ref> [Nowlan91, Jacobs91, Jordan92, Jordan93] </ref> in section 3.3.1. This description is followed by the second methodology by Hampshire and Waibel [Hampshire89] in section 3.3.2, which is based upon the Meta-Pi network. The use of a selection threshold to make the propagation of the architectures faster is described in section 3.3.3. <p> The differences between the two architectures will be depicted in section 3.3.4. A third architecture which uses knowledge bases containing fixed symbolic rules as gating networks will be described in section 3.3.4. 3.3.1 Hierarchical Mixtures of Experts <ref> [Jacobs91, Jordan92, Nowlan91] </ref> developed a modular gating architecture which can learn to divide the input space in subspaces. Their architecture consists of a number of expert feed-forward networks which receive the same input patterns and compete with each other to produce the desired output vectors.
Reference: [Jordan93] <author> M.I. Jordan & R.A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. Submitted to Neural Computation, </title> <type> Tech. Rep. 9301, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: My master's thesis describes a study on three divide-and-conquer paradigms : the use of symbolic rules to divide the input space in fixed subspaces, and the hierarchical mixtures of experts <ref> [Jacobs91, Nowlan91, Jordan92, Jordan93] </ref> and the Meta-Pi network [Hampshire89] which use a hierarchy of gating networks to learn to divide the input space in subspaces for which expert networks are trained. These methodologies could be combined with temporal difference learning to learn context specific game-strategies without a priori knowledge. <p> All gating and expert networks in the architecture receive the same input, although this is not a necessity. We will proceed by summarizing some of the work done in this field, starting with the hierarchical mixtures of experts (HME) methodology <ref> [Nowlan91, Jacobs91, Jordan92, Jordan93] </ref> in section 3.3.1. This description is followed by the second methodology by Hampshire and Waibel [Hampshire89] in section 3.3.2, which is based upon the Meta-Pi network. The use of a selection threshold to make the propagation of the architectures faster is described in section 3.3.3.
Reference: [Judd90] <author> J.S. Judd. </author> <title> Neural Network Design and the Complexity of Learning. </title> <publisher> The MIT press, </publisher> <address> Cambridge, </address> <year> 1990. </year>
Reference-contexts: Published successes in connectionist learning have been empirical results for very small networks, typically much less than 100 nodes. To fully exploit the expressive power of networks on complex real world tasks, they need to get larger and the amount of time to load the training data grows prohibitively <ref> [Judd90] </ref>. Nowadays, some researchers are focussing on modular architectures which consist of some small specialistic or expert networks which co-operate to learn the desired function [Jordan92, Nowlan91, Hampshire89, Fox91, Hashem93].
Reference: [Krose92] <author> B.J.A. Krose & P. van den Smagt. </author> <title> An Introduction to Neural Networks. </title> <month> July </month> <year> 1992. </year>
Reference-contexts: FUNCTION APPROXIMATION WITH MULTIPLE NETWORKS continuous function. 3.2 Multi-layer Feed-forward Networks Neural networks consist of many very simple processing units which communicate by sending numeric signals to each other over a large number of weighted connections <ref> [Krose92] </ref>. The knowledge is stored in the connections and processing units, which are adapted by a learning rule to minimize the error of the output units on the training data. A feed-forward network has a layered structure.
Reference: [Lin93] <author> L.J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, Tech. report CMU-CS-93-103, </type> <institution> Carnegie Mellon University, Pitts-burgh, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: He used a complex algorithm to select parameter adjustments based on the difference between the evaluations of successive positions occurring in a played game to learn the game of checkers. Reinforcement learning is to construct a policy that maximizes future rewards and minimizes future punishments <ref> [Lin93] </ref>. We have to construct a learning procedure which effectively handles the temporal credit assignment problem. The temporal credit assign 2.3. BACKGAMMON 5 ment problem is to assign credit or blame to the experienced situations and actions, which makes it possible to create learning examples. <p> Another way to let the architecture know which move is being looked at, is to use a representation in which for each possible move a different expert network is selected, which only has to evaluate the current position <ref> [Lin93] </ref>. We will use this approach to learn tic-tac-toe by Q-learning. The problem with this is that for a game like backgammon, the set of possible moves is very large ( 1,000), so that the amount of required parameters becomes too large. <p> Maybe this is why in some studies the results of Q-learning with one expert for one action outperformed the use of AHC-learning with one single network <ref> [Lin93] </ref>. The results of using the one expert for one move and the best HME architecture are not significantly different, although the HME architecture is slower. Tic-Tac-Toe Experiment 4 : AHC-learning vs. Q-learning This experiment compares AHC-learning to Q-learning. <p> In our experiment, the supervised learning set is very accurate, but some researchers have tried to use action replay so that examples acquired by TD learning can be reused <ref> [Lin93] </ref>. When these examples are known to be accurate, it might be a good idea to store these examples so that a supervised learning set is created. 4.3. THE ENDGAME OF BACKGAMMON 63 training times are used. <p> So when only a small learning set is available, this may computationally be more effective than constructing a large learning set with dynamic programming. Another method which might be advantageous is to store examples acquired by TD learning, so that these examples can be reused <ref> [Lin93] </ref>. TD learning of the game evaluation function for the endgame of backgammon is reasonable efficient. When a large amount of training games are played, the architecture 66 CHAPTER 4. TD LEARNING WITH MULTIPLE NETWORKS can reach a high level of precision. <p> However, it is very important to have a very precise approximation of the game evaluation function for the endgame, because this approximation may be used to return reinforcement for learning stages before the endgame. By using this kind of hierarchical learning <ref> [Lin93] </ref>, there is no need to play the game any further, so that the learning process will become faster. Chapter 5 Conclusion 5.1 Discussion We have studied learning game evaluation functions with modular neural network architectures. Game evaluation functions usually contain many discontinuities, which makes them difficult to learn.
Reference: [Nadi91] <author> F. Nadi. </author> <title> Topological Design of Modular Neural Networks. </title> <editor> In T. Koho-nen, K. Makisara, O. Simular & J. Kangas (eds.), </editor> <booktitle> Proceedings of the 1991 International Conference on Artificial Neural Networks, </booktitle> <volume> (213 - 218), </volume> <publisher> Ams-terdam, North-Holland, </publisher> <year> 1991. </year>
Reference-contexts: Each layer consists of units which receive their input from units from a layer directly below and send their output to units in a layer directly above the unit (figure 3.1). We consider feed-forward networks with full connections between successive layers which minimizes human engineering time, however some researchers <ref> [Nadi91, Tresp93] </ref> are constructing constrained networks to bias and speed up the learning process. These specialized network topologies will almost always outperform the simple fully connected networks and are more easily to understand.
Reference: [Nowlan91] <author> S.J. Nowlan. </author> <title> Soft Competitive Adaption: Neural Network Learning Algorithms based on Fitting Statistical Mixtures. </title> <type> PhD thesis, Tech. report CMU-CS-91-126, </type> <institution> Carnegie Mellon University, Pittsburgh, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: My master's thesis describes a study on three divide-and-conquer paradigms : the use of symbolic rules to divide the input space in fixed subspaces, and the hierarchical mixtures of experts <ref> [Jacobs91, Nowlan91, Jordan92, Jordan93] </ref> and the Meta-Pi network [Hampshire89] which use a hierarchy of gating networks to learn to divide the input space in subspaces for which expert networks are trained. These methodologies could be combined with temporal difference learning to learn context specific game-strategies without a priori knowledge. <p> Nowadays, some researchers are focussing on modular architectures which consist of some small specialistic or expert networks which co-operate to learn the desired function <ref> [Jordan92, Nowlan91, Hampshire89, Fox91, Hashem93] </ref>. For many tasks, this results in more understandable systems which are easier to train, because each expert network learns a specific sub-function and experts can be analyzed and refined independently. <p> All gating and expert networks in the architecture receive the same input, although this is not a necessity. We will proceed by summarizing some of the work done in this field, starting with the hierarchical mixtures of experts (HME) methodology <ref> [Nowlan91, Jacobs91, Jordan92, Jordan93] </ref> in section 3.3.1. This description is followed by the second methodology by Hampshire and Waibel [Hampshire89] in section 3.3.2, which is based upon the Meta-Pi network. The use of a selection threshold to make the propagation of the architectures faster is described in section 3.3.3. <p> The differences between the two architectures will be depicted in section 3.3.4. A third architecture which uses knowledge bases containing fixed symbolic rules as gating networks will be described in section 3.3.4. 3.3.1 Hierarchical Mixtures of Experts <ref> [Jacobs91, Jordan92, Nowlan91] </ref> developed a modular gating architecture which can learn to divide the input space in subspaces. Their architecture consists of a number of expert feed-forward networks which receive the same input patterns and compete with each other to produce the desired output vectors. <p> a priori probabilities of selecting the i th cluster and (i; j) th expert to produce the output vector P (y = y i ) = g i P (y i = y ij ) = g ij This is the original way of interpreting (but not using) the methodology <ref> [Nowlan91] </ref>. The propagate node acts like a multiple input, single output stochastic switch. This approach makes sense for learning games, because we must avoid making the same repetition of moves and this provides us a way to explore novel states.
Reference: [Rumelhart86] <author> D.E. Rumelhart, G.E. Hinton & R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart & J.L. McClelland (eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> Volume 1, Chapter 8, </volume> <publisher> The MIT press, </publisher> <year> 1986. </year> <note> 72 BIBLIOGRAPHY </note>
Reference-contexts: When the desired output for an example is known, we can adapt all weights so that the next time the error on this example will be smaller. For this we can use the back-propagation <ref> [Rumelhart86] </ref> algorithm. 12 CHAPTER 3.
Reference: [Samuel59] <author> A. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3, </volume> <pages> (210-229), </pages> <year> 1959. </year>
Reference-contexts: Reinforcement learning is attractive, because we only need to design the game-rules and a reinforcement learning module, which takes much less human effort than constructing a whole expert system to play the game. Samuel <ref> [Samuel59, Samuel67] </ref> was the first to construct a reinforcement learning system. He used a complex algorithm to select parameter adjustments based on the difference between the evaluations of successive positions occurring in a played game to learn the game of checkers.
Reference: [Samuel67] <author> A. Samuel. </author> <title> Some studies in machine learning using the game of checkers: </title> <journal> II recent progress . IBM Journal of Research and Development, </journal> <volume> 11, </volume> <pages> (601-617), </pages> <year> 1967. </year>
Reference-contexts: Reinforcement learning is attractive, because we only need to design the game-rules and a reinforcement learning module, which takes much less human effort than constructing a whole expert system to play the game. Samuel <ref> [Samuel59, Samuel67] </ref> was the first to construct a reinforcement learning system. He used a complex algorithm to select parameter adjustments based on the difference between the evaluations of successive positions occurring in a played game to learn the game of checkers.
Reference: [Schaffer92] <editor> J.D. Schaffer, D. Whitley & L.J. Eshelman. </editor> <title> Combinations of Genetic Algorithms and Neural Networks: A Survey of the State of the Art. </title> <editor> In L.D. Whitley & J.D. Schaffer (eds.), </editor> <booktitle> International Workshop on Combinations of Genetic Algorithms and Neural Networks, </booktitle> <pages> (1-37), </pages> <address> Baltimore, MD: </address> <publisher> IEEE, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: This has consequences for learning : we have to have an architecture which contains a 'good' solution, but we do not want to have too many superfluous states. Some researchers use pruning [Esposito93], and others make it possible for the architecture to grow <ref> [Schaffer92, Gruau92, Simon92] </ref>. These methods can be used to find an architecture which contains at least one 'good' state, but a minimum of superfluous states.
Reference: [Schraudol94] <author> N.N. Schraudolph, P. Dayan & T.J. Sejnowski. </author> <title> Temporal difference learning of Position evaluation in the Game of Go. </title> <editor> In J.D. Cowan, G. Tesauro & J. Alspector (eds.), </editor> <booktitle> Advances in Neural Information Processing, </booktitle> <volume> 6, </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Fransisco, </address> <year> 1994. </year>
Reference-contexts: Expert systems for game-playing require an evaluation function which returns the expected payoff from a position given future optimal play of both sides. Some research studies learning game evaluation functions with neural networks <ref> [Tesauro92, Boyan92, Schraudol94] </ref>. Games provide domains where the evaluation function can differ drastically for similar positions (tic-tac-toe). When learning discontinuous functions, single neural networks tend to generalize over the discontinuities which results in small regions where the local error is very high.
Reference: [Simon92] <author> N. Simon, H. Corporaal & E. </author> <title> Kerckhoffs. Variations on the Cascade-Correlation learning architecture for fast convergence in robot control. </title> <journal> Neuro Nimes, </journal> <volume> 5, </volume> <pages> (455-464), </pages> <year> 1992. </year>
Reference-contexts: This has consequences for learning : we have to have an architecture which contains a 'good' solution, but we do not want to have too many superfluous states. Some researchers use pruning [Esposito93], and others make it possible for the architecture to grow <ref> [Schaffer92, Gruau92, Simon92] </ref>. These methods can be used to find an architecture which contains at least one 'good' state, but a minimum of superfluous states.
Reference: [Sperduti92] <author> A. Sperduti. </author> <title> Speed Up Learning and Network Optimization With Extended Back Propagation. </title> <type> Tech. report TR-10/92, </type> <institution> University of Pisa, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: In this manner hidden units only change their incoming weights on a specific part of the input, for other inputs weight changes will be zero. When we make the activation functions very steep, the basis functions are made local. <ref> [Sperduti92] </ref> has found a learning rule which learns neuron sensitivities (see Appendix B), so that the learning process becomes faster and units have different learning rates. <p> The Meta-Pi methodology can profit from the selection threshold to increase the competition between the experts and to minimize the co-operation. Experiments with Extended Back-propagation Finally some experiments have been performed to evaluate the learning rule extended back-propagation (BP+) <ref> [Sperduti92] </ref>, see Appendix B for a description. We have seen that the settings of the neuron sensitivities are important for learning the discontinuous function. BP+ is able to change neuron sensitivities so that learning rates are adapted 34 CHAPTER 3. FUNCTION APPROXIMATION WITH MULTIPLE NETWORKS Architecture hidden u.
Reference: [Sutton88] <author> R. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> (9-44), </pages> <year> 1988. </year>
Reference-contexts: This supervised method results in a loss of precision, because every move would be held equally responsible for the obtained result, which is almost never true. Especially when we want to learn to play games with stochastic elements, the learning process might become very slow. Temporal difference (TD) learning <ref> [Sutton88, Tesauro92, Dayan92, Dayan94] </ref> provides an efficient method to receive learning examples with a higher accuracy, because the evaluation of a given position is adjusted by using the differences between its evaluation and the evaluations of successive positions. <p> From a played game and reinforcement we can generate examples by the temporal difference (TD) methods (Appendix A). Two RL-formalisms have been developed : AHC-learning <ref> [Sutton88] </ref> and Q-learning [Watkins92]. Both are able to learn a control policy which maximizes an agent's performance level, provided that a linear representation of the input (lookup tables) is used, all actions are repeatedly sampled in all states and a proper scheduling of the learning rates is made [Watkins92, Dayan94].
Reference: [Tesauro89] <author> G. Tesauro. </author> <title> Neurogammon : A neural network backgammon learning program. </title> <editor> In D.N.L. Levy & D.F. Beal, (eds.), </editor> <booktitle> Heuristic Programming in Artificial Intelligence : The first Computer Olympiad, </booktitle> <address> Chichester, England, </address> <publisher> Ellis Horwood lim, </publisher> <year> 1989. </year>
Reference-contexts: BKG [Berliner77] was the first backgammon program. It was constructed during a human knowledge engineering period of four years after which all important features of the game were used in the reasoning process of the expert system. Many years later Tesauro <ref> [Tesauro89] </ref> presented his Neurogammon program which was trained on a massive data set of expert preferences of best moves in backgammon positions, but he realized that it would be a better idea to enable a network to see millions of positions and learn from the outcome of its own play.
Reference: [Tesauro92] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8(3/4), </volume> <pages> (257-277), </pages> <publisher> Kluwer Academic Publishers, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: Expert systems for game-playing require an evaluation function which returns the expected payoff from a position given future optimal play of both sides. Some research studies learning game evaluation functions with neural networks <ref> [Tesauro92, Boyan92, Schraudol94] </ref>. Games provide domains where the evaluation function can differ drastically for similar positions (tic-tac-toe). When learning discontinuous functions, single neural networks tend to generalize over the discontinuities which results in small regions where the local error is very high. <p> This supervised method results in a loss of precision, because every move would be held equally responsible for the obtained result, which is almost never true. Especially when we want to learn to play games with stochastic elements, the learning process might become very slow. Temporal difference (TD) learning <ref> [Sutton88, Tesauro92, Dayan92, Dayan94] </ref> provides an efficient method to receive learning examples with a higher accuracy, because the evaluation of a given position is adjusted by using the differences between its evaluation and the evaluations of successive positions. <p> He adapted the TD method to implement TD-Gammon <ref> [Tesauro92] </ref> and trained it first on the simplest case : disengaged bear-off (endgame) and next he trained the network to learn the whole game.
Reference: [Thrun92] <author> S.B. Thrun. </author> <title> Efficient Exploration in Reinforcement Learning. </title> <type> Tech. Report CMU-CS-92-102, </type> <institution> Carnegie Mellon University, Pittsburgh, </institution> <year> 1992. </year>
Reference-contexts: Therefore some kind of exploration is needed. A natural trade-off between exploitation for maximizing the agent's performance level and exploration is to choose moves randomly to a probability distribution determined by the evaluation of the resulting positions <ref> [Thrun92] </ref>. The probability P t (m i ) of selecting a move m i when looking at position x t is computed by the Boltzmann distribution P t (m i ) = P m k 2moves (x t ) e V ( ~y k )=T (4:2) 4.1.
Reference: [Tresp93] <author> V. Tresp, J. Hollatz & S. Ahmad. </author> <title> Network structuring and training using rule-based knowledge. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 5, </volume> <pages> (871-878), </pages> <year> 1993. </year> <note> BIBLIOGRAPHY 73 </note>
Reference-contexts: Each layer consists of units which receive their input from units from a layer directly below and send their output to units in a layer directly above the unit (figure 3.1). We consider feed-forward networks with full connections between successive layers which minimizes human engineering time, however some researchers <ref> [Nadi91, Tresp93] </ref> are constructing constrained networks to bias and speed up the learning process. These specialized network topologies will almost always outperform the simple fully connected networks and are more easily to understand.
Reference: [Vysniausk93] <author> V. Vy~sniauskas, F.C.A. Groen & B.J.A. Krose. </author> <title> The optimal number of learning samples and hidden units in function approximation with a feedforward network. </title> <type> Tech. Report CS-93-15, </type> <institution> University of Amsterdam, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: The expressive power or VC dimension of a neural network depends mostly on the number of weights [Anthony91], and when we have enough examples and hidden units in the network we are able to learn most functions until a specified level of precision <ref> [Vysniausk93] </ref>. This means that the architecture contains one or more 'good' states. When we want to analyze a neural network, we must understand its state space.
Reference: [Watkins92] <author> C.J.C.H. Watkins & P. </author> <title> Dayan. </title> <journal> Q-Learning, Machine Learning, </journal> <volume> 8, </volume> <pages> (279-292), </pages> <year> 1992. </year>
Reference-contexts: From a played game and reinforcement we can generate examples by the temporal difference (TD) methods (Appendix A). Two RL-formalisms have been developed : AHC-learning [Sutton88] and Q-learning <ref> [Watkins92] </ref>. Both are able to learn a control policy which maximizes an agent's performance level, provided that a linear representation of the input (lookup tables) is used, all actions are repeatedly sampled in all states and a proper scheduling of the learning rates is made [Watkins92, Dayan94]. <p> Both are able to learn a control policy which maximizes an agent's performance level, provided that a linear representation of the input (lookup tables) is used, all actions are repeatedly sampled in all states and a proper scheduling of the learning rates is made <ref> [Watkins92, Dayan94] </ref>. The formalisms are formally described in Appendix A. Here we will concentrate on using both formalisms to learn game evaluation functions. AHC-learning learns an evaluation function for states.
Reference: [Whitehead92] <author> S.D. Whitehead. </author> <title> Reinforcement Learning for the Adaptive Control of Perception and Action. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Playing a game is a Markov decision process (see Appendix A), because all states, actions, rewards, and transition probabilities between states are known. The Markov property states that all transitions and rewards depend only upon the current state and the current action <ref> [Whitehead92] </ref>. If we have an evaluation function for positions in the game, we can generate all possible moves, use the evaluation function to compare them and select the move which results in a position with the highest evaluation. <p> Several methods exist to create board positions and to calculate evaluations with which we can improve our evaluation function. One way to create examples fx t ; E (rjx t )g for improving the evaluation function is by the use of dynamic programming <ref> [Whitehead92] </ref> in which we compute examples for all possible positions. In dynamic programming, we start with an arbitrary control policy which is iteratively improved by changing it so that for all possible positions from the endgame until the start of the game, the move which maximizes the merits is chosen.
References-found: 36

