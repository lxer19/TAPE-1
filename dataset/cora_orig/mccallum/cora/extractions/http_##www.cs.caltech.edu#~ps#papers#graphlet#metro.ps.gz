URL: http://www.cs.caltech.edu/~ps/papers/graphlet/metro.ps.gz
Refering-URL: http://www.cs.caltech.edu/~ps/papers/graphlet/
Root-URL: http://www.cs.caltech.edu
Title: Metro: measuring error on simplified surfaces  
Author: Paolo Cignoni, Claudio Rocchini Roberto Scopigno 
Keyword: surface simplification, approximation error, scan conversion, ray casting.  
Date: December 21, 1995  
Affiliation: Istituto per l'Elaborazione dell'Informazione Consiglio Nazionale delle Ricerche  CNUCE Consiglio Nazionale delle Ricerche  
Abstract: This paper presents a new tool, Metro, designed to cover a lack of most of the simplification methods proposed in literature. Metro allows to compare the difference between surfaces (eg. a triangulated mesh and its decimated representation), adopting an approximated approach. It returns both numerical results and error magnitude visualization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Eck, T. De Rose, T. Duchamp, H. Hoppe, M. Lounsbery, and W. Stuetzle. </author> <title> Multiresolution of arbitrary meshes. </title> <booktitle> In Comp. Graph. Proc., Annual Conf. Series (Siggraph '95), </booktitle> <publisher> ACM Press, </publisher> <pages> pages 173-181, </pages> <month> Aug. 6-12 </month> <year> 1995. </year>
Reference-contexts: on maximal curvature locations; the original vertices are then iteratively removed; a re-tiled mesh, built on the new vertices only, is returned [8]; * multiresolution re-tiling: the approach uses re-meshing, re-sampling and wavelet parametrization to build a multiresolution representation of the surface, from which any approximated representation can be extracted <ref> [1] </ref>. A general comparison of the above approaches is not easy, because the criteria to drive the simplification process are highly differentiated and we lack a common way of measuring error. Moreover, most of these approaches return any measure of the approximation error introduced while simplifying the mesh.
Reference: [2] <author> A.S. Glassner. </author> <title> An Introduction to Ray Tracing. </title> <publisher> Academic Press, </publisher> <year> 1989. </year>
Reference-contexts: To implement Metro we adopted the first approach. The incremental solution adopted by canonical triangle scan conversion algorithms assures sufficient efficiency. Distance evaluation has been implemented reusing existing ray tracing functions: facet-ray intersection, intersection sorting <ref> [2] </ref>. Obviously, each ray is traced in the two directions (outwards and inwards the pivot surface). To guarantee sufficient efficiency, the use of the classical ray casting optimization based on object-space decomposition is mandatory.
Reference: [3] <author> OpenInventor Architecture Group. </author> <title> Inventor Mentor: OpenInventor Reference Manual. </title> <publisher> Addison Wesley, </publisher> <year> 1994. </year>
Reference-contexts: The input data format accepted in input are either the Inventor <ref> [3] </ref> format or a row indexed representation (a list of vertex coordinates, and a list of triangular facets, defined by the three indices to the vertex list). <p> The minimum dihedral angle used by Metro in the detection of feature edges is one of the parameters selectable by the user. A visual representation of error is also supported: the pivot mesh is rendered (using the OpenInventor library <ref> [3] </ref>) adopting: * Gouraud shaded rendering, with facets color intensity proportional to the error (Figure 3); * iconical representation (Figure 4); 3D vectors are plotted on the mesh vertices, with their direction and length proportional to the estimated error.
Reference: [4] <author> P. Hinker and C. Hansen. </author> <title> Geometric optimization. </title> <booktitle> In IEEE Visualization '93 Proc., </booktitle> <pages> pages 189-195, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: In the many proposals published recently on the subject, we briefly mention some general and valuable solutions: * coplanar facets merging: coplanar or nearly coplanar data are searched, merged in larger polygons and then re-triangulated into fewer simple facets than those originally required <ref> [4, 6] </ref>; * mesh decimation: based on multiple filtering passes, this approach analyses locally the geometry and topology of the mesh and removes vertices that pass a minimal distance or curvature angle criterion [7]; * mesh optimization: an energy function is evaluated over the mesh, and is minimized either by removing/moving <p> Other approaches let the user define the maximal error that can be introduced in a single simplification step, but return any global error estimate or bound <ref> [7, 4] </ref>. To be more drastic, the field of surface simplification lacks of a formal and universally acknowledged definition of error. Another problem is that some methods best suit smooth surfaces simplification [8], while others [7, 5] guarantee recovering of feature elements (sharp or boundary edges, corner vertices).
Reference: [5] <author> H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. </author> <title> Mesh optimization. </title> <booktitle> In Proceedings of SIGGRAPH '93( Anaheim, </booktitle> <address> CA, </address> <month> August 1-6). </month> <booktitle> In Computer Graphics Proceedings, Annual Conference series, ACM SIGGRAPH, </booktitle> <pages> pages 19-26, </pages> <year> 1993. </year>
Reference-contexts: based on multiple filtering passes, this approach analyses locally the geometry and topology of the mesh and removes vertices that pass a minimal distance or curvature angle criterion [7]; * mesh optimization: an energy function is evaluated over the mesh, and is minimized either by removing/moving vertices or collapsing/swapping edges <ref> [5] </ref>; * re-tiling: a new set of vertices are inserted at random on the original surface mesh, and then moved on the surface to be displaced on maximal curvature locations; the original vertices are then iteratively removed; a re-tiled mesh, built on the new vertices only, is returned [8]; * multiresolution <p> Moreover, most of these approaches return any measure of the approximation error introduced while simplifying the mesh. For example, given the complexity reduction factor set by the user, some methods assure on the "optimality" of the simplified mesh, but they give any measure on the error introduced <ref> [8, 5] </ref>. Other approaches let the user define the maximal error that can be introduced in a single simplification step, but return any global error estimate or bound [7, 4]. To be more drastic, the field of surface simplification lacks of a formal and universally acknowledged definition of error. <p> To be more drastic, the field of surface simplification lacks of a formal and universally acknowledged definition of error. Another problem is that some methods best suit smooth surfaces simplification [8], while others <ref> [7, 5] </ref> guarantee recovering of feature elements (sharp or boundary edges, corner vertices).
Reference: [6] <author> C. Montani, R. Scateni, and R. Scopigno. </author> <title> Discretized Marching Cubes. In R.D. Bergeron and A.E. </title> <publisher> Kaufman, </publisher> <editor> editors, </editor> <booktitle> Visualization '94 Proceedings, </booktitle> <pages> pages 281-287. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: In the many proposals published recently on the subject, we briefly mention some general and valuable solutions: * coplanar facets merging: coplanar or nearly coplanar data are searched, merged in larger polygons and then re-triangulated into fewer simple facets than those originally required <ref> [4, 6] </ref>; * mesh decimation: based on multiple filtering passes, this approach analyses locally the geometry and topology of the mesh and removes vertices that pass a minimal distance or curvature angle criterion [7]; * mesh optimization: an energy function is evaluated over the mesh, and is minimized either by removing/moving
Reference: [7] <author> W.J. Schroeder, J.A. Zarge, and W. Lorensen. </author> <title> Decimation of triangle mesh. </title> <journal> ACM Computer Graphics, </journal> <volume> 26(2) </volume> <pages> 65-70, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: are searched, merged in larger polygons and then re-triangulated into fewer simple facets than those originally required [4, 6]; * mesh decimation: based on multiple filtering passes, this approach analyses locally the geometry and topology of the mesh and removes vertices that pass a minimal distance or curvature angle criterion <ref> [7] </ref>; * mesh optimization: an energy function is evaluated over the mesh, and is minimized either by removing/moving vertices or collapsing/swapping edges [5]; * re-tiling: a new set of vertices are inserted at random on the original surface mesh, and then moved on the surface to be displaced on maximal curvature <p> Other approaches let the user define the maximal error that can be introduced in a single simplification step, but return any global error estimate or bound <ref> [7, 4] </ref>. To be more drastic, the field of surface simplification lacks of a formal and universally acknowledged definition of error. Another problem is that some methods best suit smooth surfaces simplification [8], while others [7, 5] guarantee recovering of feature elements (sharp or boundary edges, corner vertices). <p> To be more drastic, the field of surface simplification lacks of a formal and universally acknowledged definition of error. Another problem is that some methods best suit smooth surfaces simplification [8], while others <ref> [7, 5] </ref> guarantee recovering of feature elements (sharp or boundary edges, corner vertices).
Reference: [8] <author> G. Turk. </author> <title> Re-Tiling Polygonal Surfaces. </title> <journal> ACM Computer Graphics, </journal> <volume> 26(2) </volume> <pages> 55-64, </pages> <month> July </month> <year> 1992. </year> <month> 11 </month>
Reference-contexts: collapsing/swapping edges [5]; * re-tiling: a new set of vertices are inserted at random on the original surface mesh, and then moved on the surface to be displaced on maximal curvature locations; the original vertices are then iteratively removed; a re-tiled mesh, built on the new vertices only, is returned <ref> [8] </ref>; * multiresolution re-tiling: the approach uses re-meshing, re-sampling and wavelet parametrization to build a multiresolution representation of the surface, from which any approximated representation can be extracted [1]. <p> Moreover, most of these approaches return any measure of the approximation error introduced while simplifying the mesh. For example, given the complexity reduction factor set by the user, some methods assure on the "optimality" of the simplified mesh, but they give any measure on the error introduced <ref> [8, 5] </ref>. Other approaches let the user define the maximal error that can be introduced in a single simplification step, but return any global error estimate or bound [7, 4]. To be more drastic, the field of surface simplification lacks of a formal and universally acknowledged definition of error. <p> To be more drastic, the field of surface simplification lacks of a formal and universally acknowledged definition of error. Another problem is that some methods best suit smooth surfaces simplification <ref> [8] </ref>, while others [7, 5] guarantee recovering of feature elements (sharp or boundary edges, corner vertices).
References-found: 8

