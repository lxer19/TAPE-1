URL: http://www.ics.uci.edu/~pazzani/Publications/mlw91-id2of3.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: pmurphy@ics.uci.edu  pazzani@ics.uci.edu  
Title: ID2-of-3: Constructive Induction of M of-N Concepts for Discriminators in Decision Trees  
Author: Patrick M. Murphy Michael J. Pazzani 
Address: Irvine, CA 92717  Irvine, CA 92717  
Affiliation: Dept. of Info. Computer Science University of California,  Dept. of Info. Computer Science University of California,  
Abstract: We discuss an approach to constructing composite features during the induction of decision trees. The composite features correspond to m-of-n concepts. There are three goals of this research. First, we explore a family of greedy methods for building m-of-n concepts (one of which, GS, is described in this paper). Second, we show how these concepts can be formed as internal nodes of decision trees, serving as a bias to the learner. Finally, we evaluate the method on several artificially generated and naturally occurring data sets to determine the effects of this bias.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barsalou, L. </author> <year> (1985). </year> <title> Ideals, central tendency, and frequency of instantiation as determinates of graded structure in categories. </title> <journal> Journal of Experimental Psychology: Learning, Memory and Cognition, </journal> <volume> 11, </volume> <pages> 629-654. </pages>
Reference: <author> Fisher, D. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 139-172. </pages>
Reference-contexts: Since decision trees also have this property, the intent is not to make decision trees more expressive, but rather to bias decision trees to make poly-thetic <ref> (Fisher, 1987) </ref> discriminations. Furthermore, since each node of an m-of -n decision tree has more representation power than that of an ID3 decision tree node, m-of-n decision trees have the potential of being much smaller than ID3's trees when concepts conform to combinations of m-of -n decisions. Table 2.
Reference: <author> Hampson, S. E. & Volper, D. J. </author> <year> (1986). </year> <title> Linear Function Neurons: Structure and Training. </title> <journal> Biological Cybernetics, </journal> <volume> 53, </volume> <pages> 203-217. </pages>
Reference-contexts: Since there are O (d3 d ) concepts in the hypothesis space of Boolean valued features <ref> (Hampson & Volper, 1986) </ref>, each operator application, in such a space, discards O (3 d =d) possible hypotheses. Search depth is no greater than kd.
Reference: <author> Kingsland, L. C. </author> <title> III (1985). The evaluation of medical expert systems: Experience with the AI/RHEUM knowledge-based consultant in rheumatology. </title> <booktitle> Proceedings of the Ninth Annual Symposium on Computer Applications in Medical Care. </booktitle> <address> Washington, DC: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Second, there is some evidence that this bias helps in the acquisition of naturally occurring concepts (Spackman, 1988). For example, a successful medical expert system makes use of "criteria tables" that are essentially m-of -n concepts <ref> (Kingsland, 1985) </ref>. Our motivation is somewhat similar to that of Utgoff (1988) in developing perceptron trees. In particular, the terms constructed to serve as tests at nodes in the decision tree serve as a representational bias for the learner.
Reference: <author> Murphy, P. M & Pazzani, M. J. </author> <year> (1991). </year> <title> ID2-of-3: Constructive induction of M-of-N concepts for discriminators in decision trees (Technical Report 91-37). </title> <institution> Irvine: University of California, Department of Information and Computer Science. </institution>
Reference-contexts: The chosen feature-value, F , becomes the 1-of-(F ) initial hypothesis that operators are then applied to. For a description and comparison of other approaches for constructing m-of -n hypotheses see <ref> (Murphy & Pazzani, 1991) </ref>. Table 1.
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 71-100. </pages>
Reference: <author> Pitt, L. & Valiant, L. G. </author> <year> (1988). </year> <title> Computational limitations on learning from examples. </title> <journal> JACM, </journal> <volume> 35, </volume> <pages> 965-984. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: 1 INTRODUCTION In this paper, we discuss an approach to constructing composite features during the induction of decision trees <ref> (Quinlan, 1986) </ref>. Pagallo and Haussler (1990) describe the FRINGE algorithm that analyzes a decision tree and creates new features that are conjunctions of existing features. <p> Therefore, the algorithm for creating new m-of -n features must not rely on such a test. Instead, we merely determine whether a given m-of-n concept provides gain according to an information-based evaluation function <ref> (Quinlan, 1986) </ref>. Furthermore, the feature construction algorithm makes use of operators to find a set of m-of -n concepts related to a given m-of -n concept and performs hill-climbing search to find the m-of -n concept in this set whose gain is greatest. <p> Adding a feature-value to n may include adding another value to a feature already present in n. One way to specialize an m-of-n hypothesis is to increase m. Decision trees constructed by ID-2-of -3 are like those built by ID3 <ref> (Quinlan, 1986) </ref> except that they contain m-of -n hypotheses as discriminators at internal nodes. 3 CONSTRUCTING M of-N CONCEPTS An m-of-n concept construction algorithm, GS, is described in this section that applies operators during hill-climbing search to successively refine partial m-of -n hypotheses (Table 1).
Reference: <author> Smith, E. E., & Medin, D. L. </author> <year> (1981). </year> <title> Categories and concepts. </title> <address> Cambridge, MA: </address> <publisher> Harvard University Press. </publisher>
Reference: <author> Spackman, K. </author> <year> (1988). </year> <title> Learning categorical decision criteria in biomedical domains. </title> <booktitle> Proceedings of the Fifth International Workshop on Machine Learning (pp. </booktitle> <pages> 36-46). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We are interested in creating composite m-of -n features for several reasons. First, m-of-n concepts more closely resemble "fuzzy categories" with graded structure (Barsalou, 1985; Smith & Medin, 1981). Second, there is some evidence that this bias helps in the acquisition of naturally occurring concepts <ref> (Spackman, 1988) </ref>. For example, a successful medical expert system makes use of "criteria tables" that are essentially m-of -n concepts (Kingsland, 1985). Our motivation is somewhat similar to that of Utgoff (1988) in developing perceptron trees.
Reference: <author> Utgoff, P. </author> <year> (1988). </year> <title> Perceptron trees: A case study in hybrid concept representations. </title> <booktitle> Proceedings of the Seventh National Conference on Artificial Intelligence (pp. </booktitle> <pages> 601-606). </pages> <address> Boston, MA: </address> <publisher> Morgan Kaufmann. </publisher> <editor> Appeared in: </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91). </booktitle>
Reference-contexts: Since the space of m-of-n concepts is a superset of the space of single attribute discriminations, decision trees constructed in this manner are complete <ref> (Utgoff, 1988) </ref> in that they can represent any subset of the instance space. Since decision trees also have this property, the intent is not to make decision trees more expressive, but rather to bias decision trees to make poly-thetic (Fisher, 1987) discriminations.
References-found: 11

