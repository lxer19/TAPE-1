URL: http://www.cs.columbia.edu/~sal/hpapers/kdd98-scale-cost.ps.gz
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: pkc@cs.fit.edu  sal@cs.columbia.edu  
Title: Toward Scalable Learning with Non-uniform Class and Cost Distributions: A Case Study in Credit Card
Author: Philip K. Chan Salvatore J. Stolfo 
Keyword: skewed class distributions, non-uniform error cost, large amounts of data, credit card fraud detection.  
Note: This work was partially funded by grants from DARPA (F30602-96-1-0311), NSF (IRI-96-32225 CDA 96-25374), NYSSTF (423115-445).  
Date: March 16, 1998  
Address: Melbourne, FL 32901  New York, NY 10027  
Affiliation: Computer Science Florida Institute of Technology  Department of Computer Science Columbia University  
Abstract: Very large databases with skewed class distributions and non-uniform cost per error are not uncommon in real-world data mining tasks. One such task is credit card fraud detection: the number of fraudulent transactions is small compared to legitimate ones, the amount of financial loss for each fraudulent transaction depends on the transaction amount and other factors, and millions of transactions occur each day. We devised a multi-classifier meta-learning approach to address these three issues. Our empirical results indicate that the approach can significantly reduce loss due to illegitimate transactions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: Four learning algorithms (C4.5, CART, RIPPER, and BAYES) were used in our experiments. We obtained C4.5 [18] and CART <ref> [1] </ref> as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. RIPPER [10] is a rule learning algorithm and was obtained from W. Cohen.
Reference: [2] <author> W. Buntine and R. Caruana. </author> <title> Introduction to IND and Recursive Partitioning. </title> <institution> NASA Ames Research Center, </institution> <year> 1991. </year>
Reference-contexts: Four learning algorithms (C4.5, CART, RIPPER, and BAYES) were used in our experiments. We obtained C4.5 [18] and CART [1] as part of the IND package <ref> [2] </ref> from NASA Ames Research Center; both algorithms compute decision trees. RIPPER [10] is a rule learning algorithm and was obtained from W. Cohen. BAYES is a naive Bayesian learning algorithm that is based on computing conditional probabilities using the Bayes Rule as described in [9].
Reference: [3] <author> C. Cardie and N. Howe. </author> <title> Improving minority class prediction using case-specific feature weights. </title> <booktitle> In Proc. 14th Intl. Conf. Mach. Learning, </booktitle> <pages> pages 57-65, </pages> <year> 1997. </year>
Reference-contexts: Kubat and Matwin [14] acknowledged the performance degradation effects of skewed class distributions and investigated techniques for removing unnecessary instances from the majority class. Instances that are in the borderline region, noisy, or redundant are candidates for removal. Cardie and Howie <ref> [3] </ref> stated that skewed class distributions are "the norm for learning problems in natural language processing (NLP)." In a case-based learning framework, they studied techniques to extract relevant features from previously built decision trees and customize local feature weights for each case retrieval.
Reference: [4] <author> J. Catlett. </author> <title> Megainduction: A test flight. </title> <booktitle> In Proc. Eighth Intl. Work. Machine Learning, </booktitle> <pages> pages 596-599, </pages> <year> 1991. </year>
Reference-contexts: Until recently, researchers in machine learning have been focused on small data sets. Efficiently learning from large amounts of data has been gaining attention due to the fast growing field of data mining, where data are abundant. Sampling (e.g., <ref> [4] </ref>) and parallelism (e.g., [13, 17]) are the two main directions in scalable learning. Much of the parallelism work focuses on parallelizing a particular algorithm on a particular parallel architecture. That is, a new algorithm or architecture requires substantial amount of parallel programming work.
Reference: [5] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: We devised a multi-classifier meta-learning approach to address these three issues. Our approach is based on creating data subsets with the appropriate class distribution, applying learning algorithms to the subsets independently and in parallel, and integrating to optimize cost performance of the classifiers by learning (meta-learning <ref> [5] </ref>) from their classification behavior. That is, our method utilizes all available training examples and does not change the underlying learning algorithms. It also handles non-uniform cost per error and is cost-sensitive during the learning process. <p> For massive amounts of data, substantial improvement in speed can be achieved for super-linear-time learning algorithms. The generated classifiers are combined by learning (meta-learning) from their classifica tion behavior. Several meta-learning strategies are described in <ref> [5] </ref>. To simplify our dis cussion, we only describe the class-combiner (or stacking [20]) strategy. In this strategy a meta-level training set is composed by using the (base) classifiers' predictions on a validation set as attribute values and the actual classification as the class label.
Reference: [6] <author> P. Chan and S. Stolfo. </author> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning, </booktitle> <pages> pages 90-98, </pages> <year> 1995. </year>
Reference-contexts: This training set is then used to train a meta-classifier. For integrating subsets, class-combiner can be more effective than the voting-based techniques <ref> [6] </ref>. 4.1 Experiments and Results To evaluate our multi-classifier meta-learning approach to skewed class distributions, we used transactions from the first 8 months (10/95 - 5/96) for training, the ninth month (6/96) for validating, and the twelfth month (9/96) for testing (the two-month gap is chosen according to the amount of
Reference: [7] <author> P. Chan and S. Stolfo. </author> <title> Sharing learned models among remote database partitions by local meta-learning. </title> <booktitle> In Proc. Second Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 2-7, </pages> <year> 1996. </year>
Reference-contexts: Although banks do not share credit card data for fear of losing valuable customers to competitors or violating the customers' privacy, a bank can import "black-box" classifiers from other banks to improve its local performance <ref> [7] </ref>. 9
Reference: [8] <author> P. Chan and S. Stolfo. </author> <title> The effects of training class distributions on performance using cost models. </title> <booktitle> Submitted to 15th Intl. Conf. Machine Learning, </booktitle> <year> 1998. </year>
Reference-contexts: We describe the credit card fraud detection task in Section 2. Section 3 examines the 1 effects of training class distributions on the performance. Section 4 discusses our multi--classifier meta-learning approach. For completeness, part of the exposition in this article also appears in a companion paper <ref> [8] </ref>. Section 5 summaries our results and directions. 2 Credit Card Fraud Detection When banks lose money because of credit card fraud, card holders partially (possibly entirely) pay for the loss through higher interest rates, higher membership fees, and reduced benefits. <p> That is, the bank can tolerate more false-alarms (a higher false-positive rate) and aim for fewer misses (a lower false-negative rate), which can be achieved by a larger percentage of fraudulent transactions (positive's) <ref> [8] </ref>.
Reference: [9] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-285, </pages> <year> 1989. </year>
Reference-contexts: RIPPER [10] is a rule learning algorithm and was obtained from W. Cohen. BAYES is a naive Bayesian learning algorithm that is based on computing conditional probabilities using the Bayes Rule as described in <ref> [9] </ref>. The results are plotted in Figure 1. Each data point is an average of 10 classifiers, each of which is generated from a separate month. Each curve represents a different amount of overhead. Fraud is the minority class. As expected, the larger overhead leads to higher cost.
Reference: [10] <author> W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proc. 12th Intl. Conf. Machine Learning, </booktitle> <pages> pages 115-123, </pages> <year> 1995. </year>
Reference-contexts: Four learning algorithms (C4.5, CART, RIPPER, and BAYES) were used in our experiments. We obtained C4.5 [18] and CART [1] as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. RIPPER <ref> [10] </ref> is a rule learning algorithm and was obtained from W. Cohen. BAYES is a naive Bayesian learning algorithm that is based on computing conditional probabilities using the Bayes Rule as described in [9]. The results are plotted in Figure 1.
Reference: [11] <author> T. Fawcett. </author> <title> Learning with skewed class distributions-summary of responses. </title> <journal> Machine Learning List, </journal> <volume> Vol. 8, No. 20, </volume> <month> Dec </month> <year> 1996. </year>
Reference-contexts: A similar task is cellular phone fraud detection [12]. Each of these three issues has not been widely studied in the machine learning research community. In Dec. 96 Fawcett <ref> [11] </ref> summarized the responses to his inquiry on learning with skewed class distributions. The number of responses was amazingly few given skewed distributions are not rare in the real world.
Reference: [12] <author> T. Fawcett and F. Provost. </author> <title> Combining data mining and machine learning for effective user profiling. </title> <booktitle> In Proc. 2nd Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 8-13, </pages> <year> 1996. </year>
Reference-contexts: A similar task is cellular phone fraud detection <ref> [12] </ref>. Each of these three issues has not been widely studied in the machine learning research community. In Dec. 96 Fawcett [11] summarized the responses to his inquiry on learning with skewed class distributions. <p> Another line of cost-sensitive research tries to reduce the cost in using a classifier. For instance, some sensing devices are costlier in the robotics domain [19] and some medical tests are more expensive in the medical diagnosis domain. Fawcett and Provost <ref> [12] </ref> considered non-uniform cost per error in their cellular phone fraud detection task. However, they only considered non-uniform cost in evaluating different algorithms|the learning process was not cost-sensitive. Until recently, researchers in machine learning have been focused on small data sets.
Reference: [13] <author> E. Han, G. Karypis, and V. Kumar. </author> <title> Scalable parallel data mining for association rules. </title> <booktitle> In Proc ACM-SIGMOD-97, </booktitle> <year> 1997. </year>
Reference-contexts: Until recently, researchers in machine learning have been focused on small data sets. Efficiently learning from large amounts of data has been gaining attention due to the fast growing field of data mining, where data are abundant. Sampling (e.g., [4]) and parallelism (e.g., <ref> [13, 17] </ref>) are the two main directions in scalable learning. Much of the parallelism work focuses on parallelizing a particular algorithm on a particular parallel architecture. That is, a new algorithm or architecture requires substantial amount of parallel programming work.
Reference: [14] <author> M. Kubat and S. Matwin. </author> <title> Addressing the curse of imbalanaced training sets: One sided selection. </title> <booktitle> In Proc. 14th Intl. Conf. Machine Learning, </booktitle> <pages> pages 179-186, </pages> <year> 1997. </year>
Reference-contexts: In Dec. 96 Fawcett [11] summarized the responses to his inquiry on learning with skewed class distributions. The number of responses was amazingly few given skewed distributions are not rare in the real world. Kubat and Matwin <ref> [14] </ref> acknowledged the performance degradation effects of skewed class distributions and investigated techniques for removing unnecessary instances from the majority class. Instances that are in the borderline region, noisy, or redundant are candidates for removal.
Reference: [15] <author> M. Pazzani, C. Merz, T. Hume P. Murphy, K. Ali, and C. Brunk. </author> <title> Reducing misclassification costs. </title> <booktitle> In Proc. 11th Intl. Conf. Machine Learning, </booktitle> <pages> pages 217-225, </pages> <year> 1994. </year>
Reference-contexts: Error rate (or accuracy) is commonly used in evaluating learning algorithms; cost-sensitive learning has not been widely investigated. Assuming the errors can be grouped into a few types and each type incurs the same cost, some studies (for example, <ref> [15] </ref>) proposed algorithms that aim to reduce the total cost. Another line of cost-sensitive research tries to reduce the cost in using a classifier. For instance, some sensing devices are costlier in the robotics domain [19] and some medical tests are more expensive in the medical diagnosis domain.
Reference: [16] <author> A. Prodromidis, S. Stolfo, and P. Chan. </author> <title> Pruning classifiers in a distributed meta-learning system. </title> <booktitle> Submitted to 4th Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <year> 1998. </year>
Reference-contexts: Also, more classifiers are generated when the data set is larger or additional learning algorithms are incorporated. Metrics for analyzing an ensemble of classifiers (e.g., diversity, correlated error, and coverage) can be used in pruning unnecessary classifiers <ref> [16] </ref>. More importantly, since thieves also learn and fraud patterns evolve over time, some classifiers are more relevant than others at a particular time. Therefore, an adaptive classifier selection method is essential.
Reference: [17] <author> F. Provost and J. Aronis. </author> <title> Scaling up inductive learning with massive parallelism. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 33-46, </pages> <year> 1996. </year>
Reference-contexts: Until recently, researchers in machine learning have been focused on small data sets. Efficiently learning from large amounts of data has been gaining attention due to the fast growing field of data mining, where data are abundant. Sampling (e.g., [4]) and parallelism (e.g., <ref> [13, 17] </ref>) are the two main directions in scalable learning. Much of the parallelism work focuses on parallelizing a particular algorithm on a particular parallel architecture. That is, a new algorithm or architecture requires substantial amount of parallel programming work.
Reference: [18] <author> J. R. Quinlan. C4.5: </author> <title> programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: In order to vary the fraud distribution from 10% to 90% for each month, we limit the size of the training sets to 6,400 transactions, which are sampled randomly without replacement. Four learning algorithms (C4.5, CART, RIPPER, and BAYES) were used in our experiments. We obtained C4.5 <ref> [18] </ref> and CART [1] as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. RIPPER [10] is a rule learning algorithm and was obtained from W. Cohen.
Reference: [19] <author> M. Tan. </author> <title> Cost-sensitive learning of classification knowledge and its applications in robotics. </title> <booktitle> Machine Learning, </booktitle> <address> 13:7, </address> <year> 1993. </year>
Reference-contexts: Another line of cost-sensitive research tries to reduce the cost in using a classifier. For instance, some sensing devices are costlier in the robotics domain <ref> [19] </ref> and some medical tests are more expensive in the medical diagnosis domain. Fawcett and Provost [12] considered non-uniform cost per error in their cellular phone fraud detection task. However, they only considered non-uniform cost in evaluating different algorithms|the learning process was not cost-sensitive.
Reference: [20] <author> D. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259, </pages> <year> 1992. </year> <month> 10 </month>
Reference-contexts: For massive amounts of data, substantial improvement in speed can be achieved for super-linear-time learning algorithms. The generated classifiers are combined by learning (meta-learning) from their classifica tion behavior. Several meta-learning strategies are described in [5]. To simplify our dis cussion, we only describe the class-combiner (or stacking <ref> [20] </ref>) strategy. In this strategy a meta-level training set is composed by using the (base) classifiers' predictions on a validation set as attribute values and the actual classification as the class label. This training set is then used to train a meta-classifier.
References-found: 20

