URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-95-46.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Evaluation of Dynamic Access Ordering Hardware  
Author: S.A. McKee, C.W. Oliver, Wm.A. Wulf, K.L. Wright, J.H. Aylor 
Web: MIP-9307626.  
Note: This work was supported in part by a grant from Intel Supercomputer Division and by NSF grants MIP-9114110 and  
Abstract: Computer Science Report No. CS-95-46 October 30, 1995 
Abstract-found: 1
Intro-found: 1
Reference: [Ale93] <author> M.J. Alexander, M.W. Bailey, B.R. Childers, J.W. Davidson, and S. Jinturkar, </author> <title> Memory Bandwidth Optimizations for Wide-Bus Machines, </title> <booktitle> Proc. 26th Hawaii International Conference on Systems Sciences (HICSS-26), </booktitle> <month> January </month> <year> 1993, </year> <pages> pages 466-475. </pages> <note> (incorrectly published under M.A. Alexander, et al.) </note>
Reference-contexts: In contrast, our approach attempts to exploit the existing memory bandwidth as much as possible, without increasing bandwidth requirements. It is often possible to take advantage of memory component features by reordering memory accesses at compile time. For instance, the compiler optimizations of Alexander, et al., for wide-bus machines <ref> [Ale93] </ref> have the side-effect of exploiting DRAM features like fast-page mode. Moyer unrolls loops and groups accesses to each stream, so that the cost of each DRAM page-miss can be amortized over several references to the same page [Moy93].
Reference: [Ben91] <author> M.E. Benitez and J. W. Davidson, </author> <title> Code Generation for Streaming: An Access/ Execute Mechanism, </title> <booktitle> Proc. Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <month> April </month> <year> 1991, </year> <pages> pages 132-141. </pages>
Reference-contexts: Even though this is a reduced-functionality prototype system, we have observed performance improvements by over 200% over normal caching. 2. The Stream Memory Controller We describe our approach based on the simplified architecture of Figure 1. In this system, the compiler must detect the presence of streams (as in <ref> [Ben91] </ref>) and arrange to transmit information about them (i.e., base address, stride, length, data size, and whether the stream is being read or written) to the hardware at run-time.
Reference: [Bro95] <author> J. Brooks, </author> <title> Single PE Optimization Techniques for the Cray T3D System, </title> <booktitle> Proc. 1st European T3D Workshop, </booktitle> <month> September, </month> <year> 1995. </year>
Reference-contexts: As Jeff Brooks explains, the rates you see in [a T3D] application depend highly on how the memory is referenced <ref> [Bro95] </ref>. This variance in performance occurs because the T3Ds DRAMs can perform some access sequences faster than others.
Reference: [Cas93] <author> Epoch Users Manual 3.1, </author> <title> Cascade Design Automation, </title> <year> 1993. </year>
Reference: [Cra95] <institution> Cray T3D Massively Parallel Processing System, Cray Research, Inc., </institution> <note> http:// www.cray.com/PUBLIC/product-info/mpp/CRAY_T3D.html, 1995. </note>
Reference-contexts: 1. Introduction As has become painfully obvious, processor speeds are increasing much faster than memory speeds. To illustrate the current problem, consider the multiprocessor Cray T3D <ref> [Cra95] </ref>. The peak Dynamic Random Access Memory (DRAM) read bandwidth for each 150MHz DEC Alpha processor [DEC92] of this machine is 320 Mbytes/sec, or about one 64-bit word per four clock cycles.
Reference: [Dec92] <institution> Digital Technical Journal, Digital Equipment Corporation, </institution> <note> 4(4), Special Issue, 1992, http://www.digital.com/info/DTJ/axp-toc.html. </note>
Reference-contexts: 1. Introduction As has become painfully obvious, processor speeds are increasing much faster than memory speeds. To illustrate the current problem, consider the multiprocessor Cray T3D [Cra95]. The peak Dynamic Random Access Memory (DRAM) read bandwidth for each 150MHz DEC Alpha processor <ref> [DEC92] </ref> of this machine is 320 Mbytes/sec, or about one 64-bit word per four clock cycles. Unfortunately, the actual bandwidth may be as low as 28 Mbytes/sec in other words, the processors can perform up to 42 instructions in the time it takes to read a single DRAM location.
Reference: [Don90] <author> J. Dongarra, J. DuCroz, I. Duff, and S. Hammerling, </author> <title> A set of Level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Further details of the design, implementation, and testing of the SMC ASIC and daughter board can be found elsewhere [McG94,Lan95]. 4. Performance copy, and scale are from the BLAS (Basic Linear Algebra Subroutines) <ref> [Don90] </ref>, and tridiag is tridiagonal gaussian elimination, the fifth Livermore Loop [McM86]. Vaxpy denotes a vector axpy operation that occurs in matrix-vector multiplication by diagonals: a vector a times a vector x plus a vector y. For our purposes, the actual computation in these loops is unimportant.
Reference: [Goo85] <author> J.R. Goodman, J. Hsieh, K. Liou, A.R. Pleszkun, P.B. Schechter, and H.C. Young, </author> <title> PIPE: A VLSI Decoupled Architecture, </title> <booktitle> Proc. 12th International Symposium n Computer Architecture (ISCA), </booktitle> <month> June </month> <year> 1985, </year> <pages> pages 20-27. </pages>
Reference: [IEE92] <author> Memory Catches Up, </author> <title> Special Report, </title> <journal> IEEE Spectrum, </journal> <volume> 29(10) </volume> <month> 34-53 October </month> <year> 1992. </year> <title> [Int91] i860 XP Microprocessor Data Book, </title> <publisher> Intel Corporation, </publisher> <year> 1991. </year>
Reference-contexts: Although the terminology is similar, DRAM pages should not be confused with virtual memory pages. Evaluation of Dynamic Access Ordering Hardware 2 With an advertised bandwidth of 500 Mbyte/s, Rambus is another interesting new memory technology <ref> [IEE92] </ref>. These bus-based systems are capable of delivering a byte of data every 2 ns for a block of information up to 256 bytes long. Like page-mode DRAMs, Rambus devices use banks of sense amplifier latches to cache data on chip. <p> The order of requests strongly affects the performance of other common devices that offer speed-optimizing features (nibble-mode, static column mode, or a small amount of SRAM cache on chip) or exhibit novel organizations (Ramlink and the new synchronous DRAM designs) <ref> [IEE92] </ref>. For interleaved memory systems, the order of requests is important on another level, as well: accesses to different banks can be performed in parallel, and thus happen faster than successive accesses to the same bank.
Reference: [Jou90] <author> N.P. Jouppi, </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> Proc. 17th International Symposium on Computer Architecture (ISCA), </booktitle> <month> May </month> <year> 1990, </year> <pages> pages 364-373. </pages> <institution> Also Digital Equipment Corporation, Western Research Lab, </institution> <note> Technical Note TN-14, </note> <month> March </month> <year> 1990. </year>
Reference-contexts: More complex stream buffers have been evaluated in other contexts. Jouppi performed simulation studies of stream buffers used to prefetch successive cache lines on a cache miss <ref> [Jou90] </ref>, and Palacharla and Kessler investigate the use of a set of stream buffers as a replacement for secondary cache [Pal94]. Although the latter scheme generally increased the cache hit rates of the benchmarks they simulated, these improvements were achieved at the expense of increased main memory bandwidth requirements.
Reference: [Lan95] <author> T.C. Landon, R.H. Klenke, J.H. Aylor, M.H. Salinas, and S.A. McKee, </author> <title> An Approach for Optimizing Synthesized High-Speed ASICs, </title> <booktitle> Proc. of the IEEE International ASIC Conference (ASIC95), </booktitle> <address> Austin, TX, </address> <month> September </month> <year> 1995, </year> <month> pages 245-248. </month> <title> Evaluation of Dynamic Access Ordering Hardware 22 </title>
Reference: [Lee93] <author> K. Lee, </author> <title> The NAS860 Library Users Manual, </title> <type> NAS Technical Report RND-93-003, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: subroutines to mimic Cray instructions on the Intel i860XR include another purely Evaluation of Dynamic Access Ordering Hardware 16 compile-time approach: he treats the cache as a pseudo vector register by reading vector elements in blocks (using non-caching load instructions) and then writing them to a pre-allocated portion of cache <ref> [Lee93] </ref>. Meadows describes a similar scheme for the PGI i860 compiler [Mea92], and Loshin and Budge give a general description of the technique [Los92].
Reference: [Los92] <author> D. Loshin, and D. Budge, </author> <title> Breaking the Memory Bottleneck, Parts 1 & 2, </title> <booktitle> Supercomputing Review, </booktitle> <address> Jan./Feb. </address> <year> 1992. </year>
Reference-contexts: Meadows describes a similar scheme for the PGI i860 compiler [Mea92], and Loshin and Budge give a general description of the technique <ref> [Los92] </ref>. A subset of these authors measured the time to load a single vector via Moyers and Lees schemes on a node of an iPSC/860, observing performance improvements between about 40% to 450% over normal caching, depending on the stride of the vector [McK95a].
Reference: [McG94] <editor> S.W. McGee, R.H. Klenke, J.H. Aylor, and A.J. Schwab, </editor> <title> Design of a Processor Bus Interface ASIC for the Stream Memory Controller, </title> <booktitle> Proc. of the IEEE International ASIC Conference (ASIC94), </booktitle> <address> Rochester, NY, </address> <month> September </month> <year> 1994, </year> <pages> pages 462-465. </pages>
Reference: [McK95a] <author> S.A. McKee, </author> <title> Wm.A. Wulf, Access Ordering and Memory-Conscious Cache Utilization, </title> <booktitle> Proc. First International Symposium on High Performance Computer Architecture, </booktitle> <address> Raleigh, NC, </address> <month> January </month> <year> 1995, </year> <pages> pages 253-262. </pages>
Reference-contexts: A subset of these authors measured the time to load a single vector via Moyers and Lees schemes on a node of an iPSC/860, observing performance improvements between about 40% to 450% over normal caching, depending on the stride of the vector <ref> [McK95a] </ref>. Another option is to augment the purely compile-time approach with a hardware assist. Palacharla and Kessler [Pal95] investigate code restructuring techniques to exploit fast-page mode DRAMs via a hardware read-ahead mechanism on the Cray T3D.
Reference: [McK95b] <author> S.A. McKee, </author> <title> Maximizing Memory Bandwidth for Streamed Computations, </title> <type> Ph.D. Dissertation, </type> <institution> University of Virginia, Department of Computer Science, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: More intelligent schemes are required to achieve good performance on computations involving streams with strides that do not hit all memory banks, and on multiprocessor systems in general <ref> [McK95b] </ref>. Further details of the design, implementation, and testing of the SMC ASIC and daughter board can be found elsewhere [McG94,Lan95]. 4. Performance copy, and scale are from the BLAS (Basic Linear Algebra Subroutines) [Don90], and tridiag is tridiagonal gaussian elimination, the fifth Livermore Loop [McM86]. <p> The dashed lines labeled attainable bandwidth indicate performance limits due to SMC startup costs, unavoidable page misses, or the cost of moving data between the SMC and CPU chips (see <ref> [McK95b] </ref> for derivations of performance bounds), and the solid lines indicate the performance of our access ordering hardware. <p> When we take each kernels inherent bandwidth limits into account, these SMC performances represent between 89% and 98% of the attainable bandwidth for vectors over 128 elements. Our simulation studies indicate that SMCs with deeper FIFOs can exploit nearly the full system bandwidth for long-vector computations <ref> [McK95b] </ref>. Evaluation of Dynamic Access Ordering Hardware 15 5. Related Work The notion that performance of memory-intensive applications can be improved by reordering memory requests has been demonstrated before. <p> Long-vector computations benefit from very deep FIFOs, whereas computations on shorter streams require shallower FIFOs. We have derived algorithms that compilers can use to calculate an appropriate FIFO depth for a particular computation on a given system <ref> [McK95b] </ref>. The need for adjustable FIFO depth comes from the start-up cost associated with using the SMC.
Reference: [McK95c] <author> S.A. McKee, D.A.B. Weikle, K.L. Wright, C.W. Oliver, </author> <title> T.C. Landon, A.P. Voss, Wm.A. Wulf, J.H. Aylor, Avoiding Irreproducible Results: Modeling the Stream Memory Controller, </title> <institution> University of Virginia, Department of Computer Science, </institution> <type> Technical Report CS-95-46, </type> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: We employed five different models (two analytic models, one functional simulator, one gate-level hardware simulator, and one petri-net based system model) designed for different purposes by three different teams of people in two different academic departments <ref> [McK95c] </ref>. The common starting point was a description of the hardware interface and high-level (functional) behavior. These models helped us to refine and validate each stage of our design, but several of their results were unexpected. First, FIFO depth must be tailored to the parameters of a particular computation.
Reference: [McM86] <author> F.H. McMahon, </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, UCRL-53745, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: By memory mapping the control registers and FIFO heads, we avoid having to modify the processors instruction set. Figure 2 illustrates the SMC programming model for tridiagonal elimination, one of the Livermore Loops <ref> [McM86] </ref>. 3. Experimental Implementation In order to demonstrate the viability of dynamic access ordering, we have developed an experimental Stream Memory Controller system. This proof-of-concept version is implemented as a single, semi-custom VLSI integrated circuit interfaced to an Intel i860 host processor [Int91]. <p> Further details of the design, implementation, and testing of the SMC ASIC and daughter board can be found elsewhere [McG94,Lan95]. 4. Performance copy, and scale are from the BLAS (Basic Linear Algebra Subroutines) [Don90], and tridiag is tridiagonal gaussian elimination, the fifth Livermore Loop <ref> [McM86] </ref>. Vaxpy denotes a vector axpy operation that occurs in matrix-vector multiplication by diagonals: a vector a times a vector x plus a vector y. For our purposes, the actual computation in these loops is unimportant.
Reference: [Mea92] <author> L. Meadows, S. Nakamoto, and V. Schuster, </author> <title> A Vectorizing Software Pipelining Compiler for LIW and Superscalar Architectures, </title> <booktitle> Proc. RISC92, </booktitle> <pages> pages 331-343. </pages>
Reference-contexts: Meadows describes a similar scheme for the PGI i860 compiler <ref> [Mea92] </ref>, and Loshin and Budge give a general description of the technique [Los92].
Reference: [Men93] <author> System-1076, </author> <title> Quicksim II Users Manual, </title> <institution> Mentor Graphics Corporation, </institution> <year> 1993. </year>
Reference: [Moy93] <author> S.A. Moyer, </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. Dissertation, </type> <institution> University of Virginia, Department of Computer Science Technical Report CS-93-18, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Moyer unrolls loops and groups accesses to each stream, so that the cost of each DRAM page-miss can be amortized over several references to the same page <ref> [Moy93] </ref>.
Reference: [Pal94] <author> S. Palacharla and R.E. Kessler, </author> <title> Evaluating Stream Buffers as a Secondary Cache Replacement, </title> <booktitle> Proc. 21st International Symposium on Computer Architecture (ISCA), </booktitle> <month> May </month> <year> 1994, </year> <pages> pages 24-33. </pages>
Reference-contexts: More complex stream buffers have been evaluated in other contexts. Jouppi performed simulation studies of stream buffers used to prefetch successive cache lines on a cache miss [Jou90], and Palacharla and Kessler investigate the use of a set of stream buffers as a replacement for secondary cache <ref> [Pal94] </ref>. Although the latter scheme generally increased the cache hit rates of the benchmarks they simulated, these improvements were achieved at the expense of increased main memory bandwidth requirements. In contrast, our approach attempts to exploit the existing memory bandwidth as much as possible, without increasing bandwidth requirements.
Reference: [Pal95] <author> S. Palacharla and R.E. Kessler, </author> <title> Code Restructuring to Exploit Page Mode and Read-Ahead Features of the Cray T3D, Cray Research Internal Report, </title> <month> February </month> <year> 1995. </year>
Reference-contexts: Another option is to augment the purely compile-time approach with a hardware assist. Palacharla and Kessler <ref> [Pal95] </ref> investigate code restructuring techniques to exploit fast-page mode DRAMs via a hardware read-ahead mechanism on the Cray T3D. <p> Palacharla and Kessler measure a performance improvement of up to 75% in two, three, and four-stream examples on a Cray T3D <ref> [Pal95] </ref>, and Brooks demonstrates a factor of 13 improvement (from 3.5 Mops to 51.6 Mops) in T3D performance after applying access ordering to a 3x3 matrix multiplication routine used in Quantum Chromo Dynamics (QCD) codes [Bro94].
Reference: [Qui91] <author> R. Quinnell, </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991. </year> <title> Evaluation of Dynamic Access Ordering Hardware 23 </title>
Reference: [Smi87] <author> J.E. Smith, G.E. Dermer, </author> <title> B.D. Vanderwarn, S.D. Klinger, C.M. Roszewski, D.L. Fowler, and D.R. Scidmore, The ZS-1 Central Processor, </title> <booktitle> Proc. 2nd International Conference on Architectural Support for Programming Languages and Systems (ASPLOS-II), </booktitle> <month> Oct. </month> <year> 1987, </year> <pages> pages 199-204. </pages>
Reference: [Wul92] <author> Wm.A. Wulf, </author> <title> Evaluation of the WM Architecture, </title> <booktitle> Proc. 19th International Symposium on Computer Architecture (ISCA), </booktitle> <month> May </month> <year> 1992, </year> <month> pages 382-390. </month> <title> Evaluation of Dynamic Access Ordering Hardware 24 </title>
Reference-contexts: Our confidence that the SMC could be implemented efficiently was based on the fact that similar designs have been built. For instance, the organization of the SBU is almost identical to the stream units of the WM architecture <ref> [Wul92] </ref>, and the SMC may be thought of as a special case of a decoupled access-execute architecture [Goo85,Smi87]. More complex stream buffers have been evaluated in other contexts.
References-found: 26

