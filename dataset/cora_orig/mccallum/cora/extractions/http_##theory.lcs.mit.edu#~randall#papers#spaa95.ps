URL: http://theory.lcs.mit.edu/~randall/papers/spaa95.ps
Refering-URL: http://theory.lcs.mit.edu/~randall/
Root-URL: 
Email: cel@mit.edu  randall@theory.lcs.mit.edu  
Title: Parallel Algorithms for the Circuit Value Update Problem  
Author: Charles E. Leiserson Keith H. Randall 
Address: 545 Technology Square Cambridge, MA 02139  
Affiliation: MIT Laboratory for Computer Science  
Abstract: The circuit value update problem is the problem of updating values in a representation of a combinational circuit when some of the inputs are changed. We assume for simplicity that each combinational element has bounded fan-in and fan-out and can be evaluated in constant time. This problem is easily solved on an ordinary serial computer in O(W + D) time, where W is the number of elements in the altered subcircuit and D is the subcircuit's embedded depth (its depth measured in the original circuit). In this paper, we show how to solve the circuit value update problem efficiently on a P -processor parallel computer. We give a straightforward synchronous, parallel algorithm that runs in O(W=P + D lg P ) expected time. Our main contribution, however, is an optimistic, asynchronous, parallel algorithm that runs in O(W=P + D + lg W + lg P ) expected time, where W and D are the size and embedded depth, respectively, of the volatile subcircuit, the subcircuit of elements that have inputs which either change or glitch as a result of the update. To our knowledge, our analysis provides the first analytical bounds on the running time of an optimistic algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <year> 1974. </year>
Reference-contexts: The time bounds we have proved have the form O (W=P + D) with some additional logarithmic terms, where W and D are the size and embedded depth, respectively, of either the altered or volatile subcircuit. These bounds are reminscent of Brent's provably good bounds <ref> [1] </ref> for off-line scheduling of dags, except that in Brent's work, D is the true depth of the subcircuit. In our analysis, we rely heavily on the embedded depths of circuit elements as priority queue keys, however.
Reference: [2] <author> K. M. Chandy and J. Misra. </author> <title> Asynchronous distributed simulation via a sequence of parallel computations. </title> <journal> Communications of the ACM, </journal> <volume> 24(11) </volume> <pages> 198-206, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: 1 Introduction Many applicationsincluding event-driven logic simulation <ref> [2, 3, 11] </ref>, spreadsheet calculation, and constraint propagation in artificial intelligence programs [13]maintain a directed acyclic graph (dag) in which each vertex contains a value and each edge represents a data dependency between two values. <p> Moreover, for glitch-free updatesas necessarily occur for monotone circuits, for examplethis optimistic algorithm outperforms the synchronous one. Other researchers have investigated similar concerns. Chan-dy and Misra <ref> [2] </ref> present an asynchronous, parallel algorithm for performing event-driven simulation of a combinational or sequential circuit. Although they attempt to minimize the amount of recalculation, they do not perform sparse updating as do our algorithms. Jefferson [7] proposes an optimistic strategy called time-warping for parallel event-driven simulation.
Reference: [3] <author> K. Choi, S. Y. Hwang, and T. Blank. </author> <title> Incremental-in-time algorithm for digital simulation. </title> <booktitle> In Proc. 25th Design Automation Conference, </booktitle> <pages> pages 501-505, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Many applicationsincluding event-driven logic simulation <ref> [2, 3, 11] </ref>, spreadsheet calculation, and constraint propagation in artificial intelligence programs [13]maintain a directed acyclic graph (dag) in which each vertex contains a value and each edge represents a data dependency between two values. <p> Jefferson [7] proposes an optimistic strategy called time-warping for parallel event-driven simulation. Time-warping evaluates elements optimistically and uses an-timessages to rollback computations that evaluate prematurely. Matsumoto and Taki [11] present a concrete implementation of Jefferson's ideas, and Choi, Hwang, and Blank <ref> [3] </ref> present a time-warping strategy for performing incremental updates on a circuit. Time-warping is apparently a practical strategy for performing sparse updating, but to date, its effectiveness has not been demonstrated theoretically. <p> We are currently studying the problem when preemption is allowed. We are optimistic that our proof techniques can be applied to obtain bounds on other optimistic algorithms, such as simulation by time-warping <ref> [7, 11, 3] </ref>. For example, we are studying the problem of extending our results, which currently are restricted to combinational circuits, to optimistic simulation of sequential circuits. 7 Acknowledgments Thanks to Chris Joerg of MIT for acquainting us with related work and pointing out a bug in our time bounds.
Reference: [4] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill, MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Thus, the total time is O (W + D). We can also use other priority queue implementations to obtain different tradeoffs. For instance, using a binary heap <ref> [4, Chapter 7] </ref> yields a running time of O (W lg W ), and using Johnson's O (lg lg D) queue [8] produces a running time of O (W lg lg D). 3 A Synchronous, Parallel Algorithm This section describes a synchronous, parallel algorithm for the circuit value update problem on
Reference: [5] <author> Rajiv Gupta. </author> <title> The fuzzy barrier: A mechanism for high speed synchronization of processors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 54-63, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Although the optimistic execution of v causes extra work, all elements eventually attain their final values. The global synchronization for termination detection in line 3 of the optimistic algorithm should be implemented using a split-phase or fuzzy barrier <ref> [5, 6, 10] </ref> so that work can continue while the barrier is being evaluated. This implementation imposes an O (1)-time cost per barrier per processor, and a barrier latency of O (lg P ).
Reference: [6] <author> Rajiv Gupta and Charles R. Hill. </author> <title> A scalable implementation of barrier synchronization using an adaptive combining tree. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(3) </volume> <pages> 161-180, </pages> <month> June 89. </month>
Reference-contexts: Although the optimistic execution of v causes extra work, all elements eventually attain their final values. The global synchronization for termination detection in line 3 of the optimistic algorithm should be implemented using a split-phase or fuzzy barrier <ref> [5, 6, 10] </ref> so that work can continue while the barrier is being evaluated. This implementation imposes an O (1)-time cost per barrier per processor, and a barrier latency of O (lg P ).
Reference: [7] <author> David R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions of Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Other researchers have investigated similar concerns. Chan-dy and Misra [2] present an asynchronous, parallel algorithm for performing event-driven simulation of a combinational or sequential circuit. Although they attempt to minimize the amount of recalculation, they do not perform sparse updating as do our algorithms. Jefferson <ref> [7] </ref> proposes an optimistic strategy called time-warping for parallel event-driven simulation. Time-warping evaluates elements optimistically and uses an-timessages to rollback computations that evaluate prematurely. <p> The bound on the running time of this algorithm depends on the size of the volatile subcircuit, as opposed to the size of altered subcircuit. This fact is an inevitable consequence of using an asynchronous, event-driven algorithm <ref> [7] </ref>, because if a glitch can occur in a circuit, it can occur in the simulation as well. If the altered subcircuit is glitch-free, however, the optimistic algorithm is asymptotically faster than the synchronous one. Some aspects of the optimistic algorithm are similar to the synchronous algorithm. <p> We are currently studying the problem when preemption is allowed. We are optimistic that our proof techniques can be applied to obtain bounds on other optimistic algorithms, such as simulation by time-warping <ref> [7, 11, 3] </ref>. For example, we are studying the problem of extending our results, which currently are restricted to combinational circuits, to optimistic simulation of sequential circuits. 7 Acknowledgments Thanks to Chris Joerg of MIT for acquainting us with related work and pointing out a bug in our time bounds.
Reference: [8] <author> Donald B. Johnson. </author> <title> A priority queue in which initialization and queue operations take O(log log D) time. </title> <journal> Mathematical Systems Theory, </journal> <volume> 15(4) </volume> <pages> 295-309, </pages> <month> December </month> <year> 1982. </year>
Reference-contexts: Thus, the total time is O (W + D). We can also use other priority queue implementations to obtain different tradeoffs. For instance, using a binary heap [4, Chapter 7] yields a running time of O (W lg W ), and using Johnson's O (lg lg D) queue <ref> [8] </ref> produces a running time of O (W lg lg D). 3 A Synchronous, Parallel Algorithm This section describes a synchronous, parallel algorithm for the circuit value update problem on a circuit G = (V; E).
Reference: [9] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees and Hypercubes. </title> <publisher> Morgan Kaufman, </publisher> <year> 1992. </year>
Reference-contexts: preprocessing phase for the optimistic algorithm computes the depth of each element and distributes the elements randomly across the processors exactly as in the preprocessing phase for the synchronous algorithm. 1 A tighter analysis gives O (lg P= lg (P lg P=W d )) for W d P lg P=2 <ref> [9, Theorem 3.24] </ref>. The optimistic algorithm also maintains local priority queues Q [p] on each processor p, as in the synchronous algorithm. These priority queues are once again implemented as arrays of linked lists indexed by depth and hold elements that need to be reevaluated.
Reference: [10] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Dou-glas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Although the optimistic execution of v causes extra work, all elements eventually attain their final values. The global synchronization for termination detection in line 3 of the optimistic algorithm should be implemented using a split-phase or fuzzy barrier <ref> [5, 6, 10] </ref> so that work can continue while the barrier is being evaluated. This implementation imposes an O (1)-time cost per barrier per processor, and a barrier latency of O (lg P ).
Reference: [11] <author> Yukinori Matsumoto and Kazuo Taki. </author> <title> Parallel logic simulation on a distributed memory machine. </title> <booktitle> In Proceedings of the European Conference on Design Automation, </booktitle> <pages> pages 76-80, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Many applicationsincluding event-driven logic simulation <ref> [2, 3, 11] </ref>, spreadsheet calculation, and constraint propagation in artificial intelligence programs [13]maintain a directed acyclic graph (dag) in which each vertex contains a value and each edge represents a data dependency between two values. <p> Although they attempt to minimize the amount of recalculation, they do not perform sparse updating as do our algorithms. Jefferson [7] proposes an optimistic strategy called time-warping for parallel event-driven simulation. Time-warping evaluates elements optimistically and uses an-timessages to rollback computations that evaluate prematurely. Matsumoto and Taki <ref> [11] </ref> present a concrete implementation of Jefferson's ideas, and Choi, Hwang, and Blank [3] present a time-warping strategy for performing incremental updates on a circuit. Time-warping is apparently a practical strategy for performing sparse updating, but to date, its effectiveness has not been demonstrated theoretically. <p> We are currently studying the problem when preemption is allowed. We are optimistic that our proof techniques can be applied to obtain bounds on other optimistic algorithms, such as simulation by time-warping <ref> [7, 11, 3] </ref>. For example, we are studying the problem of extending our results, which currently are restricted to combinational circuits, to optimistic simulation of sequential circuits. 7 Acknowledgments Thanks to Chris Joerg of MIT for acquainting us with related work and pointing out a bug in our time bounds.
Reference: [12] <author> Abhiram G. Ranade. </author> <title> How to emulate shared memory. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 42(3) </volume> <pages> 307-326, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: We prove this bound by using a delay-sequence argument <ref> [12] </ref>. We show that when the running time of an execution is large, there must be a long delay sequence in that execution. We then show that with high probability there are no long delay sequences.
Reference: [13] <author> Patrick Henry Winston. </author> <booktitle> Artificial Intelligence. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
References-found: 13

