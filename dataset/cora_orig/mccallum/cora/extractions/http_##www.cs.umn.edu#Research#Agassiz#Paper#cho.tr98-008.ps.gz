URL: http://www.cs.umn.edu/Research/Agassiz/Paper/cho.tr98-008.ps.gz
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Title: High-Level Information An Approach for Integrating Front-End and Back-End Compilers  
Author: Sangyeun Cho, Jenn-Yuan Tsai Yonghong Song Bixia Zheng, Stephen J. Schwinn Xin Wang, Qing Zhao, Zhiyuan Li David J. Lilja and Pen-Chung Yew 
Keyword: data dependence, data flow analysis, program optimization, parallelizing compiler, information compression  
Web: http://www.cs.umn.edu/Research/Agassiz  
Address: Minneapolis, MN 55455 Urbana, IL 61801 West Lafayette, IN 47907 Minneapolis, MN 55455  
Affiliation: Dept. of Comp. Sci. and Eng. Dept. of Comp. Sci. Dept. of Comp. Sci Dept. of Elec. and Comp. Eng. Univ. of Minnesota Univ. of Illinois Purdue University Univ. of Minnesota  
Abstract: Existing compilers for automatically parallelizing application programs are often divided into parallelizing front-end compilers and optimizing back-end compilers. The front-end compilers perform high-level program analysis to identify dependences among relatively coarse-grained program units, such as subroutines and loop iterations, and perform transformations on these large units. The back-end compilers, on the other hand, perform analyses and optimizations on relatively finer-grained operations, such as machine instructions. Due to a lack of high-level information, however, the back-end compilers miss potential optimizations, and the scope of the optimizations is limited to only short-range local transformations. In this paper we propose a new universal High-Level Information (HLI) format to effectively integrate front-end and back-end compilers by passing front-end information to the back-end compiler. Importing this information into an existing back-end leverages the state-of-the-art analysis and transformation capabilities of existing front-end compilers to allow the back-end greater optimization potential than it has when relying on only locally-extracted information. A version of the HLI has been implemented in the SUIF parallelizing compiler and the GCC back-end compiler. Experimental results with the SPEC benchmarks show that HLI can provide GCC with substantially more accurate data dependence information than it can obtain on its own. Our results show that the number of dependence edges in GCC can be reduced by an average of 48% for the integer benchmark programs and an average of 54% for the floating-point benchmark programs studied, which provides greater flexibility to GCC's code scheduling pass. Even with the scheduling optimization limited to basic blocks, the use of HLI produces moderate speedups compared to using only GCC's dependence tests, when the optimized programs are executed on MIPS R4600 and R10000 processors. 0 A short version of this paper appears in the 1998 International Conference on Parallel Processing, Minneapolis, MN, August 1998. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic Translation of FORTRAN Programs to Vector Form, </title> <journal> in ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4): 491 542, </volume> <month> October </month> <year> 1987. </year>
Reference-contexts: Alternatively, a parallelizing compiler may insert a compiler directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [15], PFC <ref> [1] </ref>, Parafrase-2 [21], Polaris [3], Panorama [11], and PTRAN [22], and commercial Fortran compilers, such as KAP [13, 14] and VAST [28], have taken such a source-to-source approach.
Reference: [2] <author> E. Ayguade, C. Barrado, J. Labarta, D. Lopez, S. Moreno, D. Padua, and M. Valero. </author> <title> A Uniform Internal Representation for High-Level and Instruction-Level Transformations, </title> <type> Technical Report 1434, </type> <institution> CSRD, Univ. of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: It also maintains a low-level intermediate representation that is close to the machine code. As another example, the Polaris parallelizing compiler has recently incorporated a low-level representation to enable low-level compiler techniques <ref> [2] </ref>. Nonetheless, results showing how high-level analysis benefits the low-level analysis and optimizations are largely unavailable today.
Reference: [3] <author> W. Blume, R. Eigenmann, J. Hoeflinger, D. Padua, P. Petersen, L. Rauchwerger, and P. Tu. </author> <title> Parallel Programming with Polaris, </title> <booktitle> in IEEE Computer, </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: Alternatively, a parallelizing compiler may insert a compiler directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [15], PFC [1], Parafrase-2 [21], Polaris <ref> [3] </ref>, Panorama [11], and PTRAN [22], and commercial Fortran compilers, such as KAP [13, 14] and VAST [28], have taken such a source-to-source approach.
Reference: [4] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving Register Allocation for Subscripted Variables, </title> <booktitle> in Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 53 65, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: One should note that source-to-source program transformations based on high-level program analysis may indirectly improve uniprocessor performance. For example, array data dependence analysis can enable scalariza-tion of array references, which provides uniprocessor compilers with more opportunities for enhancing register allocation <ref> [4] </ref>. Numerous studies exist in the literature that discuss source-level program transformations to improve the memory performance on uniprocessors (see [18], for example). There have been continued efforts to incorporate uniprocessor parameters and knowledge about low-level code generation strategies into the high-level decisions about program transformations.
Reference: [5] <author> S. Cho and Y. Song. </author> <title> The HLI Implementor's Guide (v0.1), Agassiz Project Internal Document, </title> <note> http://www.cs.umn.edu/Research/Agassiz/memo.html, September 1997. </note>
Reference-contexts: To provide a common interface across different back-ends, HLI stored in the data structures can be retrieved only by using a set of query functions. There are five basic query functions (listed in Appendix A.1) which can be used to construct more complex query functions <ref> [5] </ref>. There are another set of utility functions that simplify the implementation of the user-accessible query functions and maintenance functions (see Section 3.2.3) by hiding the low-level details of the target compiler.
Reference: [6] <author> F. C. Chow. </author> <title> A Portable Machine-Independent Global Optimizer Design and Measurements, </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: Over the past years, both machine independent and machine specific compiler techniques have been developed to enhance the performance of uniprocessors. The former includes well-known techniques such as common subexpression elimination, dead-code removal and constant folding <ref> [19, 6] </ref>. Recent important machine-specific techniques primarily aim at improving instruction-level parallelism and register usage. Trace scheduling and software pipelining are among the most prominent instruction scheduling techniques for efficiently utilizing multiple functional units within a single processor.
Reference: [7] <author> J. C. Dehnert and R. A. Towle. </author> <title> Compiling for the Cydra 5, </title> <journal> in The Journal of Supercomputing, </journal> <volume> 7(1/2): 181 227, </volume> <year> 1993. </year>
Reference-contexts: Furthermore, the scope of the optimizations the back-end can perform, such as improving instruction issuing rates through architecture-aware code scheduling <ref> [7, 10, 12, 17, 20] </ref>, is limited to only short-range, local transformations. Another consequence of this split is that it is not uncommon for transformations performed in the front-end to be ignored, or even undone, in the back-end. <p> Trace scheduling and software pipelining are among the most prominent instruction scheduling techniques for efficiently utilizing multiple functional units within a single processor. These techniques have been implemented in research prototypes such as ELI [10, 20] and IMPACT [12], as well as production compilers, such as the Cydra-5 <ref> [7] </ref> and Multiflow compilers [17]. Current compiler techniques for uniprocessor performance enhancement, both machine independent and machine specific, primarily rely on dataflow analysis for symbolic registers or simple scalars that are not aliased.
Reference: [8] <author> M. Emami, R. Ghiya and L. J. Hendren. </author> <title> Context-Sensitive Interprocedural Points-to Analysis in the Presence of Function Pointers, </title> <booktitle> in Proceedings of the SIGPLAN `94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 242 256, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: With the increased demand for instruction-level parallelism, the importance of incorporating high-level analysis into uniprocessor compilers has been generally recognized. Recent work on pointer and structure analysis aims at accurate recognition of aliases due to pointer dereferences and due to pointer arguments passed to called routines <ref> [8, 30] </ref>. Experimental results in this area have been limited to reporting the accuracy of recognizing aliases. Compared with these studies, this paper presents new data showing how high-level array and pointer analysis can improve dataflow analysis in GCC, a common uniprocessor compiler.
Reference: [9] <author> Christopher Fraser and David Hanson. </author> <title> A Retargetable C Compiler: Design and Implementation, </title> <publisher> Ben-jamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, CA, </address> <year> 1995. </year>
Reference-contexts: Advanced data dependence analysis and data flow analysis regarding array references and pointer dereferences are generally not available to current uniprocessor compilers. The publically available GCC [25] and LCC <ref> [9] </ref> compilers exemplify the situation both compilers maintain low-level intermediate representations of the input programs, keeping no high-level program constructs for array data dependence analysis and pointer-structure analysis. With the increased demand for instruction-level parallelism, the importance of incorporating high-level analysis into uniprocessor compilers has been generally recognized.
Reference: [10] <author> J. R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: Furthermore, the scope of the optimizations the back-end can perform, such as improving instruction issuing rates through architecture-aware code scheduling <ref> [7, 10, 12, 17, 20] </ref>, is limited to only short-range, local transformations. Another consequence of this split is that it is not uncommon for transformations performed in the front-end to be ignored, or even undone, in the back-end. <p> Recent important machine-specific techniques primarily aim at improving instruction-level parallelism and register usage. Trace scheduling and software pipelining are among the most prominent instruction scheduling techniques for efficiently utilizing multiple functional units within a single processor. These techniques have been implemented in research prototypes such as ELI <ref> [10, 20] </ref> and IMPACT [12], as well as production compilers, such as the Cydra-5 [7] and Multiflow compilers [17]. Current compiler techniques for uniprocessor performance enhancement, both machine independent and machine specific, primarily rely on dataflow analysis for symbolic registers or simple scalars that are not aliased.
Reference: [11] <author> J. Gu, Z. Li, and G. Lee. </author> <title> Experience with Efficient Array Data Flow Analysis for Array Privatization, </title> <booktitle> in Proceedings of the 6th ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: The back-end, however, needs machine details to perform its finer-grained optimizations. The penalty for this split, though, is that the back-end misses potential optimization opportunities due to its lack of high-level information. For example, optimizations involving loops, such as scalar promotion or array privatization <ref> [11] </ref>, are difficult to perform in the back-end since complex loop structures can be difficult to identify with the limited information available in the back-end, especially for nested loops. <p> Alternatively, a parallelizing compiler may insert a compiler directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [15], PFC [1], Parafrase-2 [21], Polaris [3], Panorama <ref> [11] </ref>, and PTRAN [22], and commercial Fortran compilers, such as KAP [13, 14] and VAST [28], have taken such a source-to-source approach.
Reference: [12] <author> W. W. Hwu et al. </author> <title> The Superblock: An Effective Technique for VLIW and Superscalar Compilation, </title> <journal> in The Journal of Supercomputing, </journal> <volume> 7(1/2): 229 248, </volume> <year> 1993. </year>
Reference-contexts: Furthermore, the scope of the optimizations the back-end can perform, such as improving instruction issuing rates through architecture-aware code scheduling <ref> [7, 10, 12, 17, 20] </ref>, is limited to only short-range, local transformations. Another consequence of this split is that it is not uncommon for transformations performed in the front-end to be ignored, or even undone, in the back-end. <p> Trace scheduling and software pipelining are among the most prominent instruction scheduling techniques for efficiently utilizing multiple functional units within a single processor. These techniques have been implemented in research prototypes such as ELI [10, 20] and IMPACT <ref> [12] </ref>, as well as production compilers, such as the Cydra-5 [7] and Multiflow compilers [17]. Current compiler techniques for uniprocessor performance enhancement, both machine independent and machine specific, primarily rely on dataflow analysis for symbolic registers or simple scalars that are not aliased.
Reference: [13] <institution> KAP User's Guide, </institution> <type> Technical Report (Document No. 8811002), </type> <institution> Kuck & Associates, Inc. </institution> <month> 17 </month>
Reference-contexts: Alternatively, a parallelizing compiler may insert a compiler directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [15], PFC [1], Parafrase-2 [21], Polaris [3], Panorama [11], and PTRAN [22], and commercial Fortran compilers, such as KAP <ref> [13, 14] </ref> and VAST [28], have taken such a source-to-source approach. Computer vendors generally provide their own compilers to take a source program, which has been paral-lelized by programmers or by a parallelizing compiler, and generate multithreaded machine code, i.e., machine code embedded with thread library calls.
Reference: [14] <institution> KAP for IBM FORTRAN, </institution> <note> User's Guide Version 3.3, Technical Report (Document No. 8811002), </note> <institution> Kuck & Associates, Inc. </institution>
Reference-contexts: Alternatively, a parallelizing compiler may insert a compiler directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [15], PFC [1], Parafrase-2 [21], Polaris [3], Panorama [11], and PTRAN [22], and commercial Fortran compilers, such as KAP <ref> [13, 14] </ref> and VAST [28], have taken such a source-to-source approach. Computer vendors generally provide their own compilers to take a source program, which has been paral-lelized by programmers or by a parallelizing compiler, and generate multithreaded machine code, i.e., machine code embedded with thread library calls.
Reference: [15] <author> D. J. Kuck et al. </author> <title> The Structure of an Advanced Vectorizer for Pipelined Processors, </title> <booktitle> in Proceedings of the 4th International Computer Software and Application Conference, </booktitle> <pages> pp. 709 715, </pages> <month> October </month> <year> 1980. </year>
Reference-contexts: Alternatively, a parallelizing compiler may insert a compiler directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase <ref> [15] </ref>, PFC [1], Parafrase-2 [21], Polaris [3], Panorama [11], and PTRAN [22], and commercial Fortran compilers, such as KAP [13, 14] and VAST [28], have taken such a source-to-source approach.
Reference: [16] <author> M. Lam. </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines, </title> <booktitle> in Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: Accurate data dependence information will allow aggressive scheduling of a memory reference across other memory references, for example. Additionally, loop-carried data dependence information is indispensable to implement a cyclic scheduling algorithm such as software pipelining <ref> [16] </ref>. In loop invariant code removal, a memory reference can only be moved out of a loop when it is guaranteed that no other remaining memory reference in the loop can possibly alias 8 the memory reference.
Reference: [17] <author> P. G. Lowney et al. </author> <title> The Multiflow Trace Scheduling Compiler, </title> <journal> in The Journal of Supercomputing, </journal> <volume> 7(1/2): 51 142, </volume> <year> 1993. </year>
Reference-contexts: Furthermore, the scope of the optimizations the back-end can perform, such as improving instruction issuing rates through architecture-aware code scheduling <ref> [7, 10, 12, 17, 20] </ref>, is limited to only short-range, local transformations. Another consequence of this split is that it is not uncommon for transformations performed in the front-end to be ignored, or even undone, in the back-end. <p> These techniques have been implemented in research prototypes such as ELI [10, 20] and IMPACT [12], as well as production compilers, such as the Cydra-5 [7] and Multiflow compilers <ref> [17] </ref>. Current compiler techniques for uniprocessor performance enhancement, both machine independent and machine specific, primarily rely on dataflow analysis for symbolic registers or simple scalars that are not aliased.
Reference: [18] <author> K. S. McKinley, S. Carr, and C.-W. Tseng. </author> <title> Improving Data Locality with Loop Transformations, </title> <journal> in ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 18, </volume> <pages> pp. 423 453, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: For example, array data dependence analysis can enable scalariza-tion of array references, which provides uniprocessor compilers with more opportunities for enhancing register allocation [4]. Numerous studies exist in the literature that discuss source-level program transformations to improve the memory performance on uniprocessors (see <ref> [18] </ref>, for example). There have been continued efforts to incorporate uniprocessor parameters and knowledge about low-level code generation strategies into the high-level decisions about program transformations. The ASTI optimizer for the IBM XL Fortran compilers [23] is a good example.
Reference: [19] <author> S. S. Muchnick. </author> <title> Advanced Compiler Design and Implementation, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1997. </year>
Reference-contexts: Over the past years, both machine independent and machine specific compiler techniques have been developed to enhance the performance of uniprocessors. The former includes well-known techniques such as common subexpression elimination, dead-code removal and constant folding <ref> [19, 6] </ref>. Recent important machine-specific techniques primarily aim at improving instruction-level parallelism and register usage. Trace scheduling and software pipelining are among the most prominent instruction scheduling techniques for efficiently utilizing multiple functional units within a single processor.
Reference: [20] <author> A. Nicolau. </author> <title> Parallelism, Memory Anti-Aliasing, and Correctness for Trace Scheduling Compilers, </title> <type> Ph.D. Thesis, </type> <institution> Yale University, </institution> <year> 1984. </year>
Reference-contexts: Furthermore, the scope of the optimizations the back-end can perform, such as improving instruction issuing rates through architecture-aware code scheduling <ref> [7, 10, 12, 17, 20] </ref>, is limited to only short-range, local transformations. Another consequence of this split is that it is not uncommon for transformations performed in the front-end to be ignored, or even undone, in the back-end. <p> Recent important machine-specific techniques primarily aim at improving instruction-level parallelism and register usage. Trace scheduling and software pipelining are among the most prominent instruction scheduling techniques for efficiently utilizing multiple functional units within a single processor. These techniques have been implemented in research prototypes such as ELI <ref> [10, 20] </ref> and IMPACT [12], as well as production compilers, such as the Cydra-5 [7] and Multiflow compilers [17]. Current compiler techniques for uniprocessor performance enhancement, both machine independent and machine specific, primarily rely on dataflow analysis for symbolic registers or simple scalars that are not aliased.
Reference: [21] <author> C. D. Polychronopoulos, M. B. Girkar, M. R. Haghighat, C. L. Lee, B. Leung, and D. A. Schouten. </author> <title> Parafrase-2: An Environment for Parallelizing, Partitioning, Synchronizing and Scheduling Programs on Multiprocessors, </title> <booktitle> in Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: Alternatively, a parallelizing compiler may insert a compiler directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [15], PFC [1], Parafrase-2 <ref> [21] </ref>, Polaris [3], Panorama [11], and PTRAN [22], and commercial Fortran compilers, such as KAP [13, 14] and VAST [28], have taken such a source-to-source approach.
Reference: [22] <author> V. Sarkar. </author> <title> The PTRAN Parallel Programming System, Parallel Functional Programming Languages and Compilers, </title> <editor> B. Szymanski, Ed., </editor> <publisher> ACM Press, </publisher> <pages> pp. 309 391, </pages> <year> 1991. </year>
Reference-contexts: Alternatively, a parallelizing compiler may insert a compiler directive before a sequential loop to indicate that the loop can be executed in parallel. Several research parallelizing Fortran compilers, including Parafrase [15], PFC [1], Parafrase-2 [21], Polaris [3], Panorama [11], and PTRAN <ref> [22] </ref>, and commercial Fortran compilers, such as KAP [13, 14] and VAST [28], have taken such a source-to-source approach.
Reference: [23] <author> V. Sarkar. </author> <title> Automatic Selection of High-Order Transformations in the IBM XL FORTRAN Compilers, </title> <journal> in IBM Journal of Research and Development, </journal> <volume> 41(3): 233 264, </volume> <month> May </month> <year> 1997. </year>
Reference-contexts: There have been continued efforts to incorporate uniprocessor parameters and knowledge about low-level code generation strategies into the high-level decisions about program transformations. The ASTI optimizer for the IBM XL Fortran compilers <ref> [23] </ref> is a good example. Nonetheless, the register allocator and instruction scheduler of the uniprocessor compiler still lacks direct information about data dependences concerning complex memory references. 15 New efforts on integrating parallelizing compilers with uniprocessor compilers also have emerged recently.
Reference: [24] <author> S. J. Schwinn. </author> <title> The HLI Interface Specification for Back-End Compilers (v0.1), Agassiz Project Internal Document, </title> <note> http://www.cs.umn.edu/Research/Agassiz/memo.html, September 1997. </note>
Reference-contexts: This section discusses some of the implementation details. Note, however, that the HLI format is platform-independent, and many of the implemented functions are portable to other compilers <ref> [24] </ref>. Figure 3 shows an overview of our HLI implementation in the SUIF compiler and GCC. 3.1 Front-end implementation The HLI generation in the front-end contains two major phases memory access item generation (ITEMGEN) and HLI table construction (TBLCONST).
Reference: [25] <author> R. M. Stallman. </author> <title> Using and Porting GNU CC (for GCC version 2.7), Free Software Foundation, </title> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Section 3 then describes our implementation of this HLI into the SUIF front-end parallelizing compiler [29] and the GCC back-end optimizing compiler <ref> [25] </ref>. Experiments with the SPEC benchmark programs [26] and a GNU utility are presented in Section 4 showing how the use of the high-level information provided by SUIF improves the dependence information available to GCC. <p> the front-end can pass interprocedural data-flow information to the back-end to enable the back-end to move instructions around a function call, for instance. 3 Implementation Issues A version of the HLI described in the previous section has been implemented in the SUIF parallelizing compiler [29] and the GCC back-end compiler <ref> [25] </ref>. This section discusses some of the implementation details. Note, however, that the HLI format is platform-independent, and many of the implemented functions are portable to other compilers [24]. <p> To guarantee that the mapping between the generated memory access items and the GCC RTL instructions is correct, we observe the following RTL generation rules: 2 RTL (Register Transfer Language) is an intermediate representation used by GCC that resembles Lisp lists <ref> [25] </ref>. An RTL chain is the linked list of low-level instructions in the RTL format. 6 1. For a given assignment statement, first generate the memory accesses for the address subexpression on the left hand side. Then generate the memory accesses for the right hand side expression tree. <p> Advanced data dependence analysis and data flow analysis regarding array references and pointer dereferences are generally not available to current uniprocessor compilers. The publically available GCC <ref> [25] </ref> and LCC [9] compilers exemplify the situation both compilers maintain low-level intermediate representations of the input programs, keeping no high-level program constructs for array data dependence analysis and pointer-structure analysis.
Reference: [26] <institution> The Standard Performance Evaluation Corporation, </institution> <note> http://www.specbench.org. </note>
Reference-contexts: Section 3 then describes our implementation of this HLI into the SUIF front-end parallelizing compiler [29] and the GCC back-end optimizing compiler [25]. Experiments with the SPEC benchmark programs <ref> [26] </ref> and a GNU utility are presented in Section 4 showing how the use of the high-level information provided by SUIF improves the dependence information available to GCC. <p> 077.mdljsp2 CFP92 4865 109 23 101.tomcatv CFP95 780 17 22 102.swim CFP95 1124 76 69 103.su2cor CFP95 6759 239 36 107.mgrid CFP95 1725 35 21 141.apsi CFP95 21921 442 21 mean 27 Table 1: Benchmark program characteristics. 4 Benchmark Results Test programs selected from the SPEC92 and SPEC95 benchmark suites <ref> [26] </ref> and the GNU utilities are used in experiments to determine how effective using the HLI is in improving the optimization potential in GCC's code scheduling pass.
Reference: [27] <author> J.-Y. Tsai. </author> <title> High-Level Information Format for Integrating Front-End and Back-End Compilers (v0.2), Agassiz Project Internal Document, </title> <address> http://www.cs.umn.edu/Research/Agassiz/memo.html, March 1997. </address>
Reference-contexts: Related work is discussed in Section 5, with our results and conclusions summarized in Section 6. 1 2 High-Level Information Definition A High-Level Information (HLI) file for a program includes information that is important for back-end optimizations, but is only available or computable in the front-end <ref> [27] </ref>. As shown in Figure 1, an HLI file contains a number of HLI entries. Each HLI entry corresponds to a program unit in the source file and carries the high-level information corresponding to that program unit.
Reference: [28] <institution> VAST-2 for XL FORTRAN, </institution> <note> User's Guide, Edition 1.2, Technical Report (Document No. </note> <institution> VA061), Pacific-Sierra Research Corporation, </institution> <year> 1994. </year>
Reference-contexts: Several research parallelizing Fortran compilers, including Parafrase [15], PFC [1], Parafrase-2 [21], Polaris [3], Panorama [11], and PTRAN [22], and commercial Fortran compilers, such as KAP [13, 14] and VAST <ref> [28] </ref>, have taken such a source-to-source approach. Computer vendors generally provide their own compilers to take a source program, which has been paral-lelized by programmers or by a parallelizing compiler, and generate multithreaded machine code, i.e., machine code embedded with thread library calls.
Reference: [29] <author> R. P. Wilson et al. </author> <title> SUIF: An Infrastructure for Research on Parallelizing and Optimizing Compilers, </title> <journal> in ACM SIGPLAN Notices, </journal> <volume> 29 (12): 31 37, </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: Section 3 then describes our implementation of this HLI into the SUIF front-end parallelizing compiler <ref> [29] </ref> and the GCC back-end optimizing compiler [25]. Experiments with the SPEC benchmark programs [26] and a GNU utility are presented in Section 4 showing how the use of the high-level information provided by SUIF improves the dependence information available to GCC. <p> With the function call REF/MOD table, the front-end can pass interprocedural data-flow information to the back-end to enable the back-end to move instructions around a function call, for instance. 3 Implementation Issues A version of the HLI described in the previous section has been implemented in the SUIF parallelizing compiler <ref> [29] </ref> and the GCC back-end compiler [25]. This section discusses some of the implementation details. Note, however, that the HLI format is platform-independent, and many of the implemented functions are portable to other compilers [24]. <p> These compilers usually spend their primary effort on enhancing the efficiency of the machine code for individual processors. Multithreaded programs may also be written in a relatively high-level language such as C. The SUIF tool <ref> [29] </ref>, for example, provides an option to transform a Fortran program or a C program into a multithreaded C program. Once the thread assignment to individual processors has been determined, parallelizing compilers have little control over the execution of the code by each processor. <p> Nonetheless, the register allocator and instruction scheduler of the uniprocessor compiler still lacks direct information about data dependences concerning complex memory references. 15 New efforts on integrating parallelizing compilers with uniprocessor compilers also have emerged recently. The SUIF tool <ref> [29] </ref>, for instance, maintains a high-level intermediate representation that is close to the source program to support high-level analysis and transformations. It also maintains a low-level intermediate representation that is close to the machine code.
Reference: [30] <author> R. P. Wilson and M. S. Lam. </author> <title> Efficient Context-Sensitive Pointer Analysis for C Programs, </title> <booktitle> in Proceedings of the SIGPLAN `95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 1 12, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: With the increased demand for instruction-level parallelism, the importance of incorporating high-level analysis into uniprocessor compilers has been generally recognized. Recent work on pointer and structure analysis aims at accurate recognition of aliases due to pointer dereferences and due to pointer arguments passed to called routines <ref> [8, 30] </ref>. Experimental results in this area have been limited to reporting the accuracy of recognizing aliases. Compared with these studies, this paper presents new data showing how high-level array and pointer analysis can improve dataflow analysis in GCC, a common uniprocessor compiler.
References-found: 30

