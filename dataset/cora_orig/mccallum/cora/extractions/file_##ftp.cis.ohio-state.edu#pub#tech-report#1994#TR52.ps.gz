URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1994/TR52.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Title: Multi-Phase Redistribution: A Communication-Efficient Approach to Array Redistribution  
Author: S. D. Kaushik C.-H. Huang J. Ramanujam and P. Sadayappan 
Keyword: Array Redistribution, Communication Scheduling, Distributed-Memory Computers, Data Distribution, Data Communication, High Performance Fortran, Node Contention.  
Note: J. Ramanujam is supported in part by an NSF Young Investigator Award CCR-9457768, an NSF grant CCR-9210422, and by LEQSF.  
Address: Columbus, OH 43210  LA 70803  
Affiliation: 1 Department of Computer and Information Science The Ohio State University  Department of Electrical and Computer Engineering Louisiana State University Baton Rouge,  
Pubnum: Technical Report OSU-CISRC-9/94-52  
Email: (fkaushik,chh,sadayg@cis.ohio-state.edu)  (jxr@gate.ee.lsu.edu)  
Phone: 2  
Abstract: Distributed-memory implementations of several scientific applications require array redistribution. Array redistribution is used in languages such as High Performance Fortran to dynamically change the distribution of arrays across processors. Performing array redistribution incurs two overheads an indexing overhead for determining the set of processors to communicate with and the array elements to be communicated, and a communication overhead for performing the necessary irregular all-to-many personalized communication. In this paper efficient runtime methods for performing array redistribution are presented. To reduce the indexing overhead, precise closed forms for enumerating the processors to communicate with and the array elements to be communicated are developed for two special cases of array redistribution involving block-cyclically distributed arrays. The general array redistribution problem for block-cyclically distributed arrays can be expressed in terms of these special cases. Using the developed closed forms, a distributed algorithm for scheduling the irregular communication for redistribution is developed. The generated schedule eliminates node contention and incurs the least communication overhead. The scheduling algorithm has an asymptotically lower scheduling overhead than techniques presented in the literature. Based on the developed closed forms, a cost model for estimating the communication and the indexing overhead for array redistribution is developed. Using this model, a multi-phase approach for reducing the communication cost for array redistribution is presented. The key idea is to perform the redistribution as a sequence of redistributions such that the total cost of the sequence is less than that of direct redistribution. Algorithms for determining the sequence of intermediate data distributions which minimizes the total redistribution time are developed. Extensions of the developed closed forms and algorithms to perform array redistribution of multi-dimensional arrays are presented. Experimental results on the Cray T3D and IBM-SP2 demonstrate the validity of the developed cost models and the efficacy of the multi-phase approach for array redistribution. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bogart. </author> <title> Introductory Combinatorics. </title> <publisher> Harcourt Brace Jovanovich Inc., </publisher> <address> Florida, </address> <note> second edition, </note> <year> 1990. </year>
Reference-contexts: From Theorem 3.1, it follows that each row and each column of COM M has exactly k = min (P; Y ) non-zero entries. From the Birkhoff-Von Neumann Theorem <ref> [1] </ref>, we have 16 cessors. If M is an m fi m matrix of nonnegative integers such that all the elements of each row and column add up to k then M is a sum of k permutation matrices. <p> A graph theoretic framework based on expressing the communication as a bipartite graph and determining complete matchings of this graph <ref> [1, 6] </ref> can be used to determine the permutations. However, these schemes require additional computation to construct the communication matrix COM M , and would incur the added expense of determining k complete matchings.
Reference: [2] <author> S. Bokhari. </author> <title> Multiphase complete exchange on a circuit switched hypercube. </title> <booktitle> In Proc. of Intl. Conf. on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 525-538, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: A processor communicates with several other processors sending distinct array elements to each processor. Several methods for performing the complete exchangea special case of the all-to-many communication in which each processor communicates with every other processor have been presented in the literature <ref> [2, 3, 10, 14, 24, 25, 26] </ref>. A multi-phase approach for complete exchange on hypercube-connected multicomputers was presented in [2]. <p> Several methods for performing the complete exchangea special case of the all-to-many communication in which each processor communicates with every other processor have been presented in the literature [2, 3, 10, 14, 24, 25, 26]. A multi-phase approach for complete exchange on hypercube-connected multicomputers was presented in <ref> [2] </ref>.
Reference: [3] <author> S. Bokhari and H. Berryman. </author> <title> Complete exchange on a circuit switched mesh. </title> <booktitle> In Proc. of Scalable High Performance Computing Conference, </booktitle> <pages> pages 300-306, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: A processor communicates with several other processors sending distinct array elements to each processor. Several methods for performing the complete exchangea special case of the all-to-many communication in which each processor communicates with every other processor have been presented in the literature <ref> [2, 3, 10, 14, 24, 25, 26] </ref>. A multi-phase approach for complete exchange on hypercube-connected multicomputers was presented in [2]. <p> The use of the combining strategy reduces the number of message startups required for the complete exchange but incurs an additional data transmission cost as a data element has to be packed, transmitted and unpacked multiple times. Similar combining or store-and-forward schemes for two-dimensional meshes were presented in <ref> [3, 10, 25, 26] </ref>. The issue of scheduling the irregular all-to-many personalized communication for array redistribution has received relatively little attention. Static and run-time scheduling techniques to reduce node and link contention while performing general irregular all-to-many personalized communication were presented in [21, 23].
Reference: [4] <author> B. M. Chapman, P. Mehrotra, and H. P. Zima. </author> <title> Programming in vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1), </volume> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: 1 Introduction The difficulty of programming distributed-memory machines has been recognized as a major obstacle to their wide-spread acceptance. To address this problem, languages such as High Performance Fortran (HPF) [7, 18], Fortran D [8], and Vienna Fortran <ref> [5, 4] </ref> have been defined to be portable parallel programming platforms for such machines. These languages provide the programmer with a single address space and allow annotations to specify the mapping of arrays among the processors of the distributed-memory machine.
Reference: [5] <author> B. M. Chapman, P. Mehrotra, and H. P. Zima. </author> <title> Vienna Fortran a Fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Language, Compilers and Runtime Environments for Distributed Memory Machines, </booktitle> <pages> pages 39-62. </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction The difficulty of programming distributed-memory machines has been recognized as a major obstacle to their wide-spread acceptance. To address this problem, languages such as High Performance Fortran (HPF) [7, 18], Fortran D [8], and Vienna Fortran <ref> [5, 4] </ref> have been defined to be portable parallel programming platforms for such machines. These languages provide the programmer with a single address space and allow annotations to specify the mapping of arrays among the processors of the distributed-memory machine.
Reference: [6] <author> N. Deo. </author> <title> Graph Theory with Applications to Engineering and Computer Science. </title> <publisher> Prentice Hall of India, </publisher> <year> 1987. </year>
Reference-contexts: A graph theoretic framework based on expressing the communication as a bipartite graph and determining complete matchings of this graph <ref> [1, 6] </ref> can be used to determine the permutations. However, these schemes require additional computation to construct the communication matrix COM M , and would incur the added expense of determining k complete matchings.
Reference: [7] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction The difficulty of programming distributed-memory machines has been recognized as a major obstacle to their wide-spread acceptance. To address this problem, languages such as High Performance Fortran (HPF) <ref> [7, 18] </ref>, Fortran D [8], and Vienna Fortran [5, 4] have been defined to be portable parallel programming platforms for such machines. These languages provide the programmer with a single address space and allow annotations to specify the mapping of arrays among the processors of the distributed-memory machine.
Reference: [8] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C.-W. Tseng, and M. Wu. </author> <title> Fortran-D language specification. </title> <type> Technical Report TR-91-170, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The difficulty of programming distributed-memory machines has been recognized as a major obstacle to their wide-spread acceptance. To address this problem, languages such as High Performance Fortran (HPF) [7, 18], Fortran D <ref> [8] </ref>, and Vienna Fortran [5, 4] have been defined to be portable parallel programming platforms for such machines. These languages provide the programmer with a single address space and allow annotations to specify the mapping of arrays among the processors of the distributed-memory machine.
Reference: [9] <author> E. Grosswald. </author> <title> Topics from the Theory of Numbers. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1984. </year>
Reference-contexts: The total number of tuples is given by the total number of additive partitions of p, a (p) = 1 4 3p p p p <ref> [9] </ref> which is exponential. The optimization problem is solved in (log Y ) time as follows. A min-tuple of size l (as constructed below) is proved to have the the lowest value for Eq. 6.2 among all possible tuples of size l (Refer Appendix A for the proof).
Reference: [10] <author> S. Gupta, S. Hawkinson, and B. Baxter. </author> <title> A binary interleaved algorithm for complete exchange on a mesh architecture. </title> <type> Technical Report Intel Technical Report, </type> <institution> Intel Corporation, </institution> <year> 1993. </year>
Reference-contexts: A processor communicates with several other processors sending distinct array elements to each processor. Several methods for performing the complete exchangea special case of the all-to-many communication in which each processor communicates with every other processor have been presented in the literature <ref> [2, 3, 10, 14, 24, 25, 26] </ref>. A multi-phase approach for complete exchange on hypercube-connected multicomputers was presented in [2]. <p> The use of the combining strategy reduces the number of message startups required for the complete exchange but incurs an additional data transmission cost as a data element has to be packed, transmitted and unpacked multiple times. Similar combining or store-and-forward schemes for two-dimensional meshes were presented in <ref> [3, 10, 25, 26] </ref>. The issue of scheduling the irregular all-to-many personalized communication for array redistribution has received relatively little attention. Static and run-time scheduling techniques to reduce node and link contention while performing general irregular all-to-many personalized communication were presented in [21, 23].
Reference: [11] <author> S. K. S. Gupta, S. D. Kaushik, C.-H. Huang, J. R. Johnson, R. W. Johnson, and P. Sadayappan. </author> <title> A methodology for the generation of data distributions to optimize communication. </title> <booktitle> In Proc. of IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 436-441, </pages> <month> Dec. </month> <year> 1992. </year> <month> 28 </month>
Reference-contexts: We provide a brief overview of the tensor product representation for regular data distributions. The reader is referred to <ref> [11] </ref> for details. Let A be an m fi n matrix and B a p fi q matrix. The tensor product of A and B is the block matrix obtained by replacing each element a i;j of matrix A by a i;j B.
Reference: [12] <author> S. K. S. Gupta, S. D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for effi-cient execution on distributed-memory machines. </title> <type> Technical Report OSU-CISRC-4/94-TR19, </type> <institution> Department of Computer and Information Science, The Ohio State University., </institution> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The index of element A (k) in A loc is its local index. In this paper, we focus on identity alignments, i.e., A (k) is aligned with template cell D (k). Techniques developed in <ref> [12] </ref> can be used for the array redistribution in the general case. For identity alignments, the relationships between the global index, the local index and the processor index for regular data distributions of a one-dimensional array are shown in Table 1.
Reference: [13] <author> S. K. S. Gupta, S. D. Kaushik, S. Mufti, S. Sharma, C.-H. Huang, and P. Sadayappan. </author> <title> On the generation of efficient data communication for distributed-memory machines. </title> <booktitle> In Proc. of Intl. Computing Symposium, </booktitle> <pages> pages 504-513, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: A closed form characterization of these sets would reduce the indexing overhead for packing data into messages on the sending processor and unpacking data at the receiving processor. Such a characterization in terms of simple regular sections is possible for array redistributions restricted to block and cyclically distributed arrays <ref> [13, 17] </ref>. However, in general for the block-cyclic distribution, such a closed form characterization is not possible. Several methods for performing the indexing for array redistribution have been presented in the literature. <p> Several methods for performing the indexing for array redistribution have been presented in the literature. Closed form expressions for determining the processor and data index sets for redistributing block and cyclically distributed arrays were developed in <ref> [13, 17] </ref>. A virtual processor approach for performing redistributions involving block-cyclically distributed arrays was proposed in [13]. The virtual processor approach requires additional memory and computation at runtime to efficiently enumerate the processor and data index sets. <p> Closed form expressions for determining the processor and data index sets for redistributing block and cyclically distributed arrays were developed in [13, 17]. A virtual processor approach for performing redistributions involving block-cyclically distributed arrays was proposed in <ref> [13] </ref>. The virtual processor approach requires additional memory and computation at runtime to efficiently enumerate the processor and data index sets. <p> An array redistribution algorithm which can handle arbitrary source and target processor sets and block-cyclic distributions was presented in [20]. 1 Similar to the virtual processor approach presented in <ref> [13] </ref>, this approach, in general, incurs an additional run--time overhead for solving linear diophantine equations. In this paper, we develop closed form expressions for the processor and data index sets for some special cases of array redistribution involving block-cyclically distributed arrays. <p> Also if p 2 P Send (p), then the message send and receive can be replaced by a local memory copy operation. For array redistributions restricted to BLOCK and CYCLIC distributions, the processor and data index sets can be expressed as simple regular sections <ref> [13, 17] </ref>. However, in general, for block-cyclic distributions the index sets cannot be expressed as regular sections. In this paper, we develop closed form expressions for special cases of array redistribution involving block-cyclic arrays. <p> The index sets for the multi-dimensional array redistribution are developed using those developed for the one-dimensional arrays. * Shape Modifying Redistribution : In general, when either the data or processor array is reshaped during redistribution, techniques developed in <ref> [13] </ref> are required to efficiently enumerate the index sets. However, for particular source and target distributions it is possible to develop closed form expressions for the index sets. Techniques for these are addressed in [16]. 3.2.1 Shape Preserving Array Redistribution We develop the index sets for the shape preserving redistribution. <p> In general, for arbitrary source and target block-cyclic distributions, techniques developed in <ref> [13] </ref> will be required for efficient enumeration of the index sets. However, if the source and the target distribution bases for the multi-dimensional arrays are compatible, a closed form characterization of the index sets is possible.
Reference: [14] <author> S. L. Johnsson and C. T. Ho. </author> <title> Optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> C-38(9):1249-1268, </volume> <month> Sep. </month> <year> 1989. </year>
Reference-contexts: A processor communicates with several other processors sending distinct array elements to each processor. Several methods for performing the complete exchangea special case of the all-to-many communication in which each processor communicates with every other processor have been presented in the literature <ref> [2, 3, 10, 14, 24, 25, 26] </ref>. A multi-phase approach for complete exchange on hypercube-connected multicomputers was presented in [2].
Reference: [15] <author> S. D. Kaushik, C.-H. Huang, R. W. Johnson, and P. Sadayappan. </author> <title> An approach to communication efficient data redistribution. </title> <booktitle> In Proc. of Intl. Conf. on Supercomputing, </booktitle> <pages> pages 364-373, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Techniques for scheduling the communication which utilize the fact that the all-to-many communication arises from special forms of array redistribution, were developed in <ref> [15] </ref>. A communication cost model for redistribution and a multi-phase approach to reduce the communication cost were also developed. <p> In this paper, we extend the multi-phase approach for array redistribution which was developed in <ref> [15] </ref>. The underlying idea of the multi-phase approach is to perform array redistribution as a sequence of redistributions so that the total cost of the sequence is lower than the cost of the direct redistribution. Techniques developed in [15] are applicable when the source and the target block-cyclic distributions satisfy certain <p> we extend the multi-phase approach for array redistribution which was developed in <ref> [15] </ref>. The underlying idea of the multi-phase approach is to perform array redistribution as a sequence of redistributions so that the total cost of the sequence is lower than the cost of the direct redistribution. Techniques developed in [15] are applicable when the source and the target block-cyclic distributions satisfy certain conditions. Recently, redistribution routines for the special cases when either the block size of the source block-cyclic distribution is a multiple of the block size of the target block-cyclic distribution or vice versa, were presented in [28]. <p> Recently, redistribution routines for the special cases when either the block size of the source block-cyclic distribution is a multiple of the block size of the target block-cyclic distribution or vice versa, were presented in [28]. Based on the ideas presented therein, we extend the multi-phase approach presented in <ref> [15] </ref> to handle arbitrary source and target block-cyclic distributions. The primary contributions of this paper are as follows.
Reference: [16] <author> S. D. Kaushik, C.-H. Huang, J. Ramanujam, and P. Sadayappan. </author> <title> Multiphase array redistribution: Modeling and Evaluation. </title> <type> Technical Report OSU-CISRC-9/94-TR52, </type> <institution> Department of Computer and Information Science, The Ohio State University., </institution> <month> Sep. </month> <year> 1994. </year>
Reference-contexts: However, for particular source and target distributions it is possible to develop closed form expressions for the index sets. Techniques for these are addressed in <ref> [16] </ref>. 3.2.1 Shape Preserving Array Redistribution We develop the index sets for the shape preserving redistribution.
Reference: [17] <author> C. Koelbel. </author> <title> Compile-time generation of communication for scientific programs. </title> <booktitle> In Proc. of Supercomputing '91, </booktitle> <pages> pages 101-110, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: A closed form characterization of these sets would reduce the indexing overhead for packing data into messages on the sending processor and unpacking data at the receiving processor. Such a characterization in terms of simple regular sections is possible for array redistributions restricted to block and cyclically distributed arrays <ref> [13, 17] </ref>. However, in general for the block-cyclic distribution, such a closed form characterization is not possible. Several methods for performing the indexing for array redistribution have been presented in the literature. <p> Several methods for performing the indexing for array redistribution have been presented in the literature. Closed form expressions for determining the processor and data index sets for redistributing block and cyclically distributed arrays were developed in <ref> [13, 17] </ref>. A virtual processor approach for performing redistributions involving block-cyclically distributed arrays was proposed in [13]. The virtual processor approach requires additional memory and computation at runtime to efficiently enumerate the processor and data index sets. <p> Also if p 2 P Send (p), then the message send and receive can be replaced by a local memory copy operation. For array redistributions restricted to BLOCK and CYCLIC distributions, the processor and data index sets can be expressed as simple regular sections <ref> [13, 17] </ref>. However, in general, for block-cyclic distributions the index sets cannot be expressed as regular sections. In this paper, we develop closed form expressions for special cases of array redistribution involving block-cyclic arrays.
Reference: [18] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, and M. Zosel. </author> <title> High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction The difficulty of programming distributed-memory machines has been recognized as a major obstacle to their wide-spread acceptance. To address this problem, languages such as High Performance Fortran (HPF) <ref> [7, 18] </ref>, Fortran D [8], and Vienna Fortran [5, 4] have been defined to be portable parallel programming platforms for such machines. These languages provide the programmer with a single address space and allow annotations to specify the mapping of arrays among the processors of the distributed-memory machine.
Reference: [19] <author> L. Ni and P. K. McKinley. </author> <title> A survey of wormhole routing techniques in direct networks. </title> <journal> IEEE Computer, </journal> <volume> 3 </volume> <pages> 62-76, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: For worm-hole routed and circuit-switched networks, the cost of transmitting a large message is nearly independent of the distance between the source and target nodes <ref> [19] </ref>. Node contention arises as a processor can typically receive a single message at a time. Link contention arises when two or more message paths need to share the same interconnection network edge. Node contention can be avoided by scheduling the all-to-many personalized communication as a sequence of permutations.
Reference: [20] <author> S. Ramaswamy and P. Banerjee. </author> <title> Automatic generation of efficient array redistribution routines for distributed-memory multicomputers. </title> <type> Technical Report UILA-ENG-94-2213, </type> <institution> CRHC-94-09, CRHPC, Coordinated Science Laboratory, University of Illinios, </institution> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: For the general redistribution problem involving block-cyclically distributed arrays, an approach which incurs a significant indexing overhead per array element was proposed [28]. An array redistribution algorithm which can handle arbitrary source and target processor sets and block-cyclic distributions was presented in <ref> [20] </ref>. 1 Similar to the virtual processor approach presented in [13], this approach, in general, incurs an additional run--time overhead for solving linear diophantine equations.
Reference: [21] <author> S. Ranka and J.-C. Wang. </author> <title> Static and runtime scheduling of unstructured communication. </title> <journal> Intl. Journal of Computing Systems in Engineering, </journal> <year> 1993. </year>
Reference-contexts: The issue of scheduling the irregular all-to-many personalized communication for array redistribution has received relatively little attention. Static and run-time scheduling techniques to reduce node and link contention while performing general irregular all-to-many personalized communication were presented in <ref> [21, 23] </ref>. In the runtime techniques presented therein, each processor constructs a global communication matrix and replicates the computation for scheduling the communication using this matrix. <p> This closed form characterization facilitates the development of a communication and indexing cost model for array redistribution and a distributed scheduling algorithm which eliminates node contention for the all-to-many personalized communication. The algorithm has an asymptotically lower communication and scheduling overhead than techniques presented in <ref> [21, 23] </ref>. Using the developed cost model, a 2 Table 1: Index mapping functions for regular data distributions.
Reference: [22] <author> S. Ranka, J.-C. Wang, and M. Kumar. </author> <title> Personalized communication avoiding node contention on distributed memory systems. </title> <booktitle> In Proc. of Intl. Conf. on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 241-243, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Node contention can be avoided by scheduling the all-to-many personalized communication as a sequence of permutations. For several current architectures with adaptive/randomized wormhole routing, the impact of node contention on the communication cost is more significant than that of link contention <ref> [22] </ref>. For such architectures, it is reasonable to model the cost of all-to-many personalized communication by factoring the communication into a sequence of permutations and determining the cost of each permutation. <p> As the computation within the loop is fixed, the complexity of the algorithm is (min (P; Y )). The scheduling techniques for all-to-many personalized communication presented in <ref> [22] </ref> address a more general problemscheduling general irregular all-to-many personalized communicationand hence require the 18 construction of the global communication matrix COM M at runtime. Each processor then replicates the com-putation for the partitioning of COM M into permutations. The scheduling overhead is O (P Y log 2 Y ).
Reference: [23] <author> S. Ranka, J.-C. Wang, and M. Kumar. </author> <title> Irregular personalized communication on distributed memory systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1995. To appear. </note>
Reference-contexts: The issue of scheduling the irregular all-to-many personalized communication for array redistribution has received relatively little attention. Static and run-time scheduling techniques to reduce node and link contention while performing general irregular all-to-many personalized communication were presented in <ref> [21, 23] </ref>. In the runtime techniques presented therein, each processor constructs a global communication matrix and replicates the computation for scheduling the communication using this matrix. <p> This closed form characterization facilitates the development of a communication and indexing cost model for array redistribution and a distributed scheduling algorithm which eliminates node contention for the all-to-many personalized communication. The algorithm has an asymptotically lower communication and scheduling overhead than techniques presented in <ref> [21, 23] </ref>. Using the developed cost model, a 2 Table 1: Index mapping functions for regular data distributions.
Reference: [24] <author> D. Scott. </author> <title> Efficient all-to-all communication patterns in hypercube and mesh topologies. </title> <booktitle> In Proc. of Distributed Memory Computing Conference, </booktitle> <pages> pages 398-403, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: A processor communicates with several other processors sending distinct array elements to each processor. Several methods for performing the complete exchangea special case of the all-to-many communication in which each processor communicates with every other processor have been presented in the literature <ref> [2, 3, 10, 14, 24, 25, 26] </ref>. A multi-phase approach for complete exchange on hypercube-connected multicomputers was presented in [2].
Reference: [25] <author> N. S. Sundar, D. N. Jayasimha, D.K. Panda, and P. Sadayappan. </author> <title> Complete exchange in 2D meshes. </title> <booktitle> In Proc. of Scalable High Performance Computing Conference, </booktitle> <pages> pages 406-413, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: A processor communicates with several other processors sending distinct array elements to each processor. Several methods for performing the complete exchangea special case of the all-to-many communication in which each processor communicates with every other processor have been presented in the literature <ref> [2, 3, 10, 14, 24, 25, 26] </ref>. A multi-phase approach for complete exchange on hypercube-connected multicomputers was presented in [2]. <p> The use of the combining strategy reduces the number of message startups required for the complete exchange but incurs an additional data transmission cost as a data element has to be packed, transmitted and unpacked multiple times. Similar combining or store-and-forward schemes for two-dimensional meshes were presented in <ref> [3, 10, 25, 26] </ref>. The issue of scheduling the irregular all-to-many personalized communication for array redistribution has received relatively little attention. Static and run-time scheduling techniques to reduce node and link contention while performing general irregular all-to-many personalized communication were presented in [21, 23].
Reference: [26] <author> R. Thakur and A. Choudhary. </author> <title> All-to-all communication on meshes with wormhole routing. </title> <booktitle> In Proc. of Intl. Parallel Processing Symposium, </booktitle> <pages> pages 525-538, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: A processor communicates with several other processors sending distinct array elements to each processor. Several methods for performing the complete exchangea special case of the all-to-many communication in which each processor communicates with every other processor have been presented in the literature <ref> [2, 3, 10, 14, 24, 25, 26] </ref>. A multi-phase approach for complete exchange on hypercube-connected multicomputers was presented in [2]. <p> The use of the combining strategy reduces the number of message startups required for the complete exchange but incurs an additional data transmission cost as a data element has to be packed, transmitted and unpacked multiple times. Similar combining or store-and-forward schemes for two-dimensional meshes were presented in <ref> [3, 10, 25, 26] </ref>. The issue of scheduling the irregular all-to-many personalized communication for array redistribution has received relatively little attention. Static and run-time scheduling techniques to reduce node and link contention while performing general irregular all-to-many personalized communication were presented in [21, 23].
Reference: [27] <author> R. Thakur and A. Choudhary. </author> <title> Efficient algorithms for array redistribution. </title> <type> Technical Report SCCS-601, </type> <institution> Northeast Parallel Architectures Center, </institution> <month> June </month> <year> 1994. </year> <month> Revised Oct. </month> <year> 1994. </year> <month> 29 </month>
Reference-contexts: The block size of the intermediate block-cyclic distribution is chosen as either a common multiple m or a common divisor d of s and t. The choice of the divisor/multiple should be such that the total communication cost is reduced. A similar approach was concurrently proposed in <ref> [27] </ref>. Using the cost metrics developed for the special cases, the communication cost for the two-stage CYCLIC (s) ! CYCLIC (m) ! CYCLIC (t) distribution is s t s + N t t s + N s + 1 P t e .
Reference: [28] <author> R. Thakur, A. Choudhary, and G. Fox. </author> <title> Runtime array redistribution in HPF programs. </title> <booktitle> In Proc. of Scalable High Performance Computing Conference, </booktitle> <pages> pages 309-316, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The virtual processor approach requires additional memory and computation at runtime to efficiently enumerate the processor and data index sets. Efficient runtime strategies for performing redistribution for special cases involving block-cyclically distributed arrays were presented in <ref> [28] </ref>; these strategies are applicable to array redistributions in which the block size of the source block-cyclic distribution is a multiple of the block size of the target block-cyclic distribution and vice versa. <p> For the general redistribution problem involving block-cyclically distributed arrays, an approach which incurs a significant indexing overhead per array element was proposed <ref> [28] </ref>. An array redistribution algorithm which can handle arbitrary source and target processor sets and block-cyclic distributions was presented in [20]. 1 Similar to the virtual processor approach presented in [13], this approach, in general, incurs an additional run--time overhead for solving linear diophantine equations. <p> The solution to the general array redistribution problem for block-cyclically distributed arrays can be expressed in terms of these special cases. The developed closed forms provide a precise representation for the index sets scanned using the algorithms presented in <ref> [28] </ref>. Performing array redistribution also incurs a communication overhead arising from the all-to-many personalized communication among the processors. A processor communicates with several other processors sending distinct array elements to each processor. <p> Recently, redistribution routines for the special cases when either the block size of the source block-cyclic distribution is a multiple of the block size of the target block-cyclic distribution or vice versa, were presented in <ref> [28] </ref>. Based on the ideas presented therein, we extend the multi-phase approach presented in [15] to handle arbitrary source and target block-cyclic distributions. The primary contributions of this paper are as follows.
Reference: [29] <author> C. Van Loan. </author> <title> Computational Frameworks for the Fast Fourier Transform. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: The block-cyclic distribution is the most general regular data distribution supported in these languages. Array redistribution is used to dynamically change the distribution of an array from a specified source to a specified target distribution. Efficient distributed-memory implementations of several scientific applications including the multi-dimensional fast Fourier transform <ref> [29] </ref> and the Alternate Direction Implicit (ADI) method for solving two-dimensional diffusion equations require array redistribution. Array redistribution is also required when the distributed arrays passed as arguments to runtime libraries or subroutines have distributions different from those of the formal parameters.
Reference: [30] <author> A. Wakatani and M. Wolfe. </author> <title> Effectiveness of message strip-mining for regular and irregular communica tion. </title> <booktitle> In Intl. Conference on Parallel and Distributed Computing Systems, </booktitle> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: In the context of reducing communication cost for array redistribution, Strip-Mining Redistribution, a method in which portions of the array are redistributed in sequence, as opposed to redistributing the entire array in one step to obtain good communication-computation overlap, was presented in <ref> [31, 30] </ref>. In this paper, we extend the multi-phase approach for array redistribution which was developed in [15].
Reference: [31] <author> A. Wakatani and M. Wolfe. </author> <title> A new approach to array redistribution: Strip Mining Redistribution. </title> <booktitle> In Proc. of Parallel Architectures and Languages Europe (PARLE), </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: In the context of reducing communication cost for array redistribution, Strip-Mining Redistribution, a method in which portions of the array are redistributed in sequence, as opposed to redistributing the entire array in one step to obtain good communication-computation overlap, was presented in <ref> [31, 30] </ref>. In this paper, we extend the multi-phase approach for array redistribution which was developed in [15].
References-found: 31

