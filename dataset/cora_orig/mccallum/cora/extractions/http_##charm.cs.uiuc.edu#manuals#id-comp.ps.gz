URL: http://charm.cs.uiuc.edu/manuals/id-comp.ps.gz
Refering-URL: http://charm.cs.uiuc.edu/manuals/
Root-URL: http://www.cs.uiuc.edu
Title: An Introduction to the Id Compiler Computation Structures Group  
Author: Boon S. Ang, Alejandro Caro, Stephen Glim and Andrew Shaw 
Note: This research was supported in part by Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract no. N00014-89-J-1988. Stephen Glim is supported by a fellowship from the National Science Foundation.  
Date: May 7,1991  
Pubnum: Memo 328  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <address> Reading, Massachusetts, USA, </address> <year> 1986. </year>
Reference-contexts: The two modules are similar: they read the parser tables generated by PAGEN and parse the input stream. The only function of these modules is to produce parse-trees from Id source. Strategy: The parser is a LALR (1) parser. More information about parsing can be found in <ref> [1] </ref>. The chapter on PAGEN describes how to change the grammar of the language. Both file-parser and stream-parser are just interpreters for the parser tables generated by PAGEN. <p> This analysis is done using the attribute grammar which is described in more detail in [10]. More information about attribute grammars can be found in <ref> [1] </ref> and in the previous chapter. Strategy: To compute def-use information, a synthesized pt-node attribute is defined for the definition point of each variable use, and an inherited attribute is defined for the use point of each variable definition.
Reference: [2] <author> Lennart Augustsson. </author> <title> A compiler for lazy ml. </title> <booktitle> In Proc. 1984 ACM Conf. on Lisp and Functional Programming, </booktitle> <address> Austin, Texas, </address> <pages> pages 218-227. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1984. </year>
Reference-contexts: Extra arguments are added to the previously internal function definition to pass along variables that the internal function references from its lexical scope. Strategy: The algorithm for lambda lifting is taken from <ref> [2] </ref>. Author: Ken Traub, October 1986 Caveats: Lambda lifting is implemented to support nested function definitions. Scheme and Lisp use environments in order to support nested function definitions. It is not clear that lambda lifting is more efficient than environments.
Reference: [3] <author> L. Damas and R. Milner. </author> <title> Principle Type Schemes for Functional Programs. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-212, </pages> <year> 1982. </year>
Reference-contexts: The type-inference system is based upon the Hindley/Milner type-inference system used in ML, and has been extended to work with separate compilation. A clear exposition of the Hindley/Milner type-inference algorithm can be found in <ref> [3] </ref>. Strategy: Author: Shail Aditya, July 1988 Caveats: The type-inference module has the same limitations that the Hindley-Milner system has. It has exponential complexity, and limits the polymorphism of let-block assigned variables. In addition, in order to resolve the type of certain functions, sometimes separate compilation will not work appropriately.
Reference: [4] <author> Gupta, Shail Aditya. </author> <title> An Incremental Type Inference System for the Programming Language Id. </title> <type> Technical Report MIT/LCS/TR-488, </type> <institution> Laboratory for Computer Science, 545 Technology Square, MIT, </institution> <address> Cambridge, MA 02139, </address> <month> November </month> <year> 1990. </year> <note> First published as the author's Master's thesis. </note>
Reference-contexts: The default is t. Description: The type-inference system determines the type of each variable and function. A more detailed description of the type-inference system can be found in Shail Aditya's Master's thesis <ref> [4] </ref>. The type-inference system is based upon the Hindley/Milner type-inference system used in ML, and has been extended to work with separate compilation. A clear exposition of the Hindley/Milner type-inference algorithm can be found in [3].
Reference: [5] <author> Jamey Hicks. </author> <title> Id compiler back end for ets and monsoon. </title> <type> Technical report, </type> <institution> Laboratory For Computer Science, </institution> <year> 1990. </year>
Reference-contexts: However, manpower constaints have forced us to restrict ourselves to only the implementation independent front and middle ends. The reader interested in a description of a back end for an ETS dataflow machine is referred to Jamey Hick's memo <ref> [5] </ref> The reader is advised to have a copy of the DFCS documentation [10] handy while reading since we make frequent references to tools and structures described in it. A working knowledge of Common Lisp [8] is essential to understand the compiler's code. <p> Current wisdom dictates that graphs are the most appropriate data structure for middle end transformations. The Id compiler has the fortune to use very similar graph structures for both the middle and the ETS back end (described in <ref> [5] </ref>), both program graphs (in the middle end) and ets machine graphs (in the back end) are implemented on top of the same low level structure provided by DFCS.
Reference: [6] <author> Rishiyur S. Nikhil. </author> <title> Id version 90.0: Reference manual. </title> <type> Technical report, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> July </month> <year> 1990. </year>
Reference: [7] <editor> Simon Peyton-Jones. </editor> <booktitle> The Implementation of Functional Programming Languages. </booktitle> <publisher> Prentice Hall, </publisher> <year> 1987. </year>
Reference-contexts: The algorithm for this is described in the file: /jj/nikhil/haskell/pattern-matching.text Unfortunately, this file has not been published perhaps it should be turned into a CSG memo. Other pattern matching algorithms can be found in Simon Peyton Jones's book <ref> [7] </ref>. 57 Strategy: Uses with-instruction-array to traverse every instruction in the graph, hence it doesn't do fetch elimination across encapsulator boundaries 2 . Fetch elimination is defined for i-structures, tuples, sums and cons.
Reference: [8] <author> Guy L. Steele Jr. </author> <title> Common Lisp, The Language. </title> <publisher> Digital Press, </publisher> <year> 1990. </year> <title> The Common Lisp Definition. </title>
Reference-contexts: A working knowledge of Common Lisp <ref> [8] </ref> is essential to understand the compiler's code. Also, Ken Traub's master's thesis [9] describing an initial version of the compiler might be read in parallel with this document. <p> In the following sections, we will first describe the tools used in building the lexer and the parser, give some examples, and proceed with the description of the rest of the modules in the Id compiler. 3.1 PAGEN PAGEN is a lexer and LALR parser generator for Common Lisp <ref> [8] </ref>. Given a definition of a grammar, PAGEN generates the tables used by the table driven lexer and parser. In addition, it also generates the defgrammar and defproduction code for the input grammar.
Reference: [9] <author> Kenneth R. Traub. </author> <title> A Compiler for the MIT Tagged-Token Dataflow Architecture. </title> <type> Technical Report LCS TR-370, </type> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cambridge, MA 02139, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: A working knowledge of Common Lisp [8] is essential to understand the compiler's code. Also, Ken Traub's master's thesis <ref> [9] </ref> describing an initial version of the compiler might be read in parallel with this document. We'd like to thank the compiler hacker's of CSG for giving presentations at the workshop and explaining things in general, including, Ken Traub, Jamey Hicks, Shail Adityah Gupta, Paul Barth and Andy Shaw.
Reference: [10] <author> Kenneth R. Traub, James Hicks, and Shail Aditya. </author> <title> A dataflow compiler substrate. </title> <type> Technical Report CSG Memo 261-1, </type> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cambridge, MA 02139, </address> <month> January </month> <year> 1991. </year> <note> Revised by Jamey Hicks and Shail Aditya. 71 </note>
Reference-contexts: The reader interested in a description of a back end for an ETS dataflow machine is referred to Jamey Hick's memo [5] The reader is advised to have a copy of the DFCS documentation <ref> [10] </ref> handy while reading since we make frequent references to tools and structures described in it. A working knowledge of Common Lisp [8] is essential to understand the compiler's code. <p> The reason why there are many Id compilers is that Id compilers are built on top of the Dataflow Compiler Substrate (DFCS) <ref> [10] </ref>. DFCS is a collection of data structures and abstractions that provide the general functionality for writing compilers for a dataflow language. <p> In the framework provided by DFCS, a compiler is composed of a collection of modules and of a top-level procedure that "supervises the passing of control from module to module" <ref> [10] </ref>. 2 Thus, a compiler in the DFCS framework is really a very flexible software system. One can add modules to a compiler to implement new functionality or one can eliminate modules that become superfluous. <p> Alternative references are provided for readers interested in more detail. 2 Since the top level structure of an Id compiler is defined by using the facilities provided by DFCS, this chapter will parallel much of the discussion found in Chapter 2 of the DFCS reference document <ref> [10] </ref>. However, this chapter takes a top-down approach, while the DFCS document is organized in a bottom-up fashion. <p> This section presents the DFCS macros used to define an Id compiler. It also presents a sample definition of a compiler as well as examples on how to modify the definition to add functionality. For a more detailed description of defcompiler, refer to <ref> [10] </ref>. defcompiler [Macro]name family &clauses [(:wrapper-macro macro-name)] [(:message-hook hook-name)] [(:options foptiong+)] [(:lambda-list fargumentgfl)] [(:option-default option default)] &body modules This macro defines a compiler name belonging to the compiler family family. The compiler is composed of the given sequence of modules modules. <p> The :before-function and :after-function clauses provide hooks to setup and cleanup functions which are called before and after the invocation of the module. The :levels-marked clause provides some rather subtle functionality which is beyond the scope of this document. Interested readers are referred to <ref> [10] </ref>. The :options clause specifies the compiler options used by this module. These options should have been defined using the defcompiler-option macro, which will be described in Section 2.4. <p> In this case, the compiler needs to collect the output of the earlier module into a data structure at the higher level so that it can be passed on the later module. Readers interested in more details should consult the DFCS reference document <ref> [10] </ref>. 2.3.3 Some Compiler Modules A module can be classified as a member of one of four different classes of modules, according to inputs and outputs. The first type of module is the generator module. <p> exsym 67 5 A control region is simply a region of the program, such as a procedure body, loop iteration, or conditional expression, for which the compiler detects termination. 6 The term exsym comes from EXternal SYMbol. 7 The material in this section is also covered in Chapter 5 of <ref> [10] </ref>. 16 (define-toplevel-pragma :include (name expression place) (:processing-module pre-scope-analysis-desugaring) (declare (ignore name)) (if (null expression) (message :warning place "INCLUDE pragma requires a string expression.") (grammarcase expression (:string (let ((file (parse-library-filename (unslashify (ptnode-value expression))))) (let ((exsym-table (file-exsym-table file t))) (when exsym-table (pushnew exsym-table (cdr *exsym-search-path*)))))) (otherwise (message :warning place "INCLUDE pragma requires <p> The functions of the other modules in front-end are described in the next chapter. The lexical token and parse tree data structures are discussed in great detail in Chapter 3 of DFCS: Dataflow Compiler Substrate Manual <ref> [10] </ref>. Note that parse trees are slightly different from syntax trees. They represent the same information in a more compact form by suppressing syntax tree nodes that do not contain any additional information. <p> They represent the same information in a more compact form by suppressing syntax tree nodes that do not contain any additional information. The differences and motivations for doing this are again described in Chapter 3 of DFCS: Dataflow Compiler Substrate Manual <ref> [10] </ref>. <p> In addition, it also generates the defgrammar and defproduction code for the input grammar. For more details about defgrammar and defproduction, please refer to Chapter 4 of DFCS: Dataflow Compiler Substrate Manual <ref> [10] </ref>. <p> The ".pagen-output" file will include an in-package statement naming the package specified by the package-statement in the PAGEN file. Using the Parser The parser should be in a package that uses the Lisp and DFCS package. See the DFCS: Dataflow Compiler Substrate Manual <ref> [10] </ref> for support manipulating parse trees. Some examples are given in the next section. 3.2 Working with parse tree data structures Selectors, constructors and mutators for working with parse tree data structures are defined in DFCS and documented in DFCS: Dataflow Compiler Substrate Manual [10]. <p> the DFCS: Dataflow Compiler Substrate Manual <ref> [10] </ref> for support manipulating parse trees. Some examples are given in the next section. 3.2 Working with parse tree data structures Selectors, constructors and mutators for working with parse tree data structures are defined in DFCS and documented in DFCS: Dataflow Compiler Substrate Manual [10]. We will not repeat the definitions here, but will instead give some examples which illustrate use. 3.2.1 Examples Example 1 In the first example, we make use of the DFCS functions: block-place, ptnode-tag, ptnode-child, ptnode-value, ptnode-children, ptnode-place, ptnode-line, parse-tree-root. <p> The attribute of a ptnode in a parse tree is, in this case, calculated from the attributes of its parent, hence the name inherited. The tools for working with attribute grammars are defined in DFCS and documented in Chapter 4 of DFCS: Dataflow Compiler Substrate Manual <ref> [10] </ref>. Again as in previous sections, we will only give examples of their use. The reader should refer to the DFCS document for details. In particular, the reader should look at the 3 different storage methods for attributes, :ephemeral, :memoized, and :permanent, which are discussed in detail there. <p> More detailed documentation about the parse-tree data structure can be found in <ref> [10] </ref>. The front end of the Id compiler is the group of modules which operates on the parse-tree representation. <p> This analysis is done using the attribute grammar which is described in more detail in <ref> [10] </ref>. More information about attribute grammars can be found in [1] and in the previous chapter. Strategy: To compute def-use information, a synthesized pt-node attribute is defined for the definition point of each variable use, and an inherited attribute is defined for the use point of each variable definition.
References-found: 10

