URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P400.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Title: STABILITY OF LINEAR EQUATIONS SOLVERS IN INTERIOR-POINT METHODS  
Author: STEPHEN J. WRIGHT 
Keyword: Key words. primal-dual interior-point methods, error analysis, stability  
Date: 1994 012  
Note: SIAM J. MAT. ANAL. APPLS c 1994 Society for Industrial and Applied Mathematics Vol. 0, No. 0, pp. 000-000, Month  AMS(MOS) subject classifications. 65G05, 65F05, 90C33  
Abstract: Primal-dual interior-point methods for linear complementarity and linear programming problems solve a linear system of equations to obtain a modified Newton step at each iteration. These linear systems become increasingly ill-conditioned in the later stages of the algorithm, but the computed steps are often sufficiently accurate to be useful. We use error analysis techniques tailored to the special structure of these linear systems to explain this observation and examine how theoretically superlinear convergence of a path-following algorithm is affected by the roundoff errors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Fourer and S. Mehrotra, </author> <title> Solving symmetric indefinite systems in an interior-point method for linear programming, </title> <journal> Mathematical Programming, </journal> <volume> 62 (1993), </volume> <pages> pp. 15-39. </pages>
Reference-contexts: ; x 2 ; : : : ; x n ); Y = diag (y 1 ; y 2 ; : : : ; y n ); e = (1; 1; : : : ; 1) T ; = x T y=n; r = y M x q; oe 2 <ref> [0; 1] </ref>; fl This work was based on research supported by the Office of Scientific Computing, U.S. Depart ment of Energy, under Contract W-31-109-Eng-38. y Mathematics and Computer Science Division, Argonne National Laboratory, 9700 South Cass Avenue, Argonne, IL 60439. 1 2 STEPHEN J. <p> Other codes, notably those of Fourer and Mehrotra <ref> [1] </ref> and Vanderbei [13], handle the formulation (7). Analysis of algorithms for these formulations are discussed in another preprint [16]. In this paper, we focus on the system arising from general monotone LCP (6), and analyze the behavior of Gaussian elimination with pivoting applied to this system. <p> k = diag (y k ): We define the algorithmic framework as follows: Algorithm PFI given fl 0 2 [fl min ; fl max ] and (x 0 ; y 0 ) 2 N (fl 0 ) for k = 0; 1; 2; : : : choose oe k 2 <ref> [0; 1] </ref> and find (u k ; v k ) that satisfies Y k X k u k X k Y k e + oe k k e ;(11) choose fl k+1 2 [fl min ; fl max ] and ff k &gt; 0 such that (x k+1 ; y k+1 <p> Moreover, explicit membership of the neighborhood (10) is usually not enforced. (A more common strategy, for which there is no supporting theory, is to find the largest value of LINEAR EQUATIONS IN INTERIOR-POINT METHODS 5 ff in <ref> [0; 1] </ref> that keeps (x k ; y k ) + ff (u k ; v k ) in the nonnegative orthant and then choose ff k to be a fixed fraction of this length.) The predictor-corrector strategy of Mehrotra [9], used also in the codes of Lustig, Marsten, and Shanno <p> Hence y k i + ff^v k for all ff 2 <ref> [0; 1] </ref> and all k sufficiently large. <p> Proof. From (38), we have (x + ff^u) T (y + ff^v) = (1 ff) + ff [oe + O ( + u)] :(39) Therefore (34b) will hold for all ff 2 <ref> [0; 1] </ref> (with (u; v) replaced by (^u; ^v) and fi k = 0), provided that the term in square brackets in (39) is nonnegative. But nonnegativity is guaranteed for u o oe and sufficiently small, so ff k = 1 satisfies this inequality. <p> Finally, we show that ^(ff) = (x + ff^u) T (y + ff^v)=n is decreasing on the interval ff 2 <ref> [0; 1] </ref>. <p> We have shown that all ff 2 <ref> [0; 1] </ref> satisfy the conditions (34) with (u; v) replaced by (^u; ^v). Moreover, the function in (33) is decreasing over this interval. We conclude that ff k = 1 is the step chosen by the line search procedure, giving the result. <p> k O (1), so the inequality (34b) is satisfied when 1 ff ff ( + u)j k (1 ff)(1 fi k ); that is, when ff 1 + (3) fi k = 1 + (3) fl t k :(46) Providing we can show that ^ k (ff) is decreasing on <ref> [0; 1] </ref>, we have from (45) and (46) that the result holds for j k defined by j k = max j k ; (1) (2) (fl 1 1)(fl max fl min ) : However, ^ 0 so ^ k (ff) is certainly decreasing on [0; 1], and the result is <p> k (ff) is decreasing on <ref> [0; 1] </ref>, we have from (45) and (46) that the result holds for j k defined by j k = max j k ; (1) (2) (fl 1 1)(fl max fl min ) : However, ^ 0 so ^ k (ff) is certainly decreasing on [0; 1], and the result is proved. This result accurately indicates the behavior of fast steps on later iterations of the algorithm. <p> Our test problems are of two types. (i) The matrix M has the form M = ADA T , where A is n fi n r dense with all elements drawn from a uniform distribution on <ref> [1; 1] </ref>, while D is diagonal with diagonal elements of the form 10 o , where o is drawn from a uniform distribution on [0; 1]. <p> M has the form M = ADA T , where A is n fi n r dense with all elements drawn from a uniform distribution on [1; 1], while D is diagonal with diagonal elements of the form 10 o , where o is drawn from a uniform distribution on <ref> [0; 1] </ref>. <p> M is not an artificial feature; in certain applications of (1), including (3), M is structurally rank-deficient.) The solutions x fl and y fl are chosen so that the even-numbered components of x fl and the odd-numbered components of y fl are zero, while the remaining components are uniform on <ref> [0; 1] </ref>. (ii) The matrix M has the form (3a), where the matrix A is dense with elements of the form o 1 10 o 2 , where o 1 and o 2 are drawn from uniform distributions on [ 1 2 ] and [0; 1], respectively. <p> the remaining components are uniform on <ref> [0; 1] </ref>. (ii) The matrix M has the form (3a), where the matrix A is dense with elements of the form o 1 10 o 2 , where o 1 and o 2 are drawn from uniform distributions on [ 1 2 ] and [0; 1], respectively. <p> This requirement is also necessary for nonsingularity of M BB .) The nonzero components of z fl , fl and the complementary vector pair in (3) are all drawn from <ref> [0; 1] </ref>.
Reference: [2] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, Baltimore, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference-contexts: statement that when x and y are any two floating point numbers, op denotes +; ; fi; =, and fl (z) denotes the floating point representation of any real number z, we have fl (x op y) = (x op y)(1 + ffi); jffij u:(9) (See Golub and Van Loan <ref> [2, Section 2.4.2] </ref>.) We assume throughout that u is small enough that O (u) o 1, where O () is the order notation defined above. 2. Assumptions and Basic Results.
Reference: [3] <author> N. J. Higham, </author> <title> Accuracy and Stability of Numerical Algorithms (provisional title), </title> <note> 1994. In preparation. </note>
Reference-contexts: Then the computed solution ^z solves the perturbed system (A + H)^z = b, where jP Hj * m (2 + * m )j ^ Ljj ^ U j;(16) Proof. Follows immediately from Theorem 6.4 of Higham <ref> [3] </ref>. During Gaussian elimination, the size of the largest element in each column of the remaining submatrix may grow as multiples of the pivot rows are added to later rows in the matrix.
Reference: [4] <author> J. Ji, F. A. Potra, and S. Huang, </author> <title> A predictor-corrector method for linear complementarity problems with polynomial complexity and superlinear convergence, </title> <type> Technical Report 18, </type> <institution> Department of Mathematics, University of Iowa, Iowa City, Iowa, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Predictor-corrector methods (see, for example, Ye and Anstreicher [18], Ji, Potra, and Huang <ref> [4] </ref>, Potra [12]) take steps with either oe = 0 or oe = 1. <p> When the initial point is feasible (r 0 = 0), predictor-corrector algorithms such as that of Ji, Potra, and Huang <ref> [4] </ref> are special cases of Algorithm PFI. This framework also includes the infeasible-interior-point algorithms of Zhang [19] and Wright [15, 14]. These algorithms choose fl k+1 and oe k so that a step ff k of nontrivial length can always be taken without violating the required conditions.
Reference: [5] <author> M. Kojima, Y. Kurita, and S. Mizuno, </author> <title> Large-step interior point algorithms for linear complementarity problems, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3 (1993), </volume> <pages> pp. 398-412. </pages>
Reference-contexts: using generally positive values of oe in (4). (As we see later, the algorithm of [14] allows oe = 0 on some iterates in an attempt to attain the rapid local convergence associated with Newton's method.) Potential-reduction methods (see, for example, Kojima, Mizuno, and Yoshise [6], Kojima, Kurita, and Mizuno <ref> [5] </ref>) also determine search directions by solving systems like (4), but they refer to a logarithmic potential function to decide how far to move along the computed direction.
Reference: [6] <author> M. Kojima, S. Mizuno, and A. Yoshise, </author> <title> An O( p nL) iteration potential reduction algorithm for linear complementarity problems, </title> <journal> Mathematical Programming, </journal> <volume> 50 (1991), </volume> <pages> pp. 331-342. </pages>
Reference-contexts: Wright [14]) generate steps by using generally positive values of oe in (4). (As we see later, the algorithm of [14] allows oe = 0 on some iterates in an attempt to attain the rapid local convergence associated with Newton's method.) Potential-reduction methods (see, for example, Kojima, Mizuno, and Yoshise <ref> [6] </ref>, Kojima, Kurita, and Mizuno [5]) also determine search directions by solving systems like (4), but they refer to a logarithmic potential function to decide how far to move along the computed direction.
Reference: [7] <author> I. J. Lustig, R. E. Marsten, and D. F. Shanno, </author> <title> Computational experience with a primal-dual interior point method for linear programming, Linear Algebra and Its Applications, </title> <booktitle> 152 (1991), </booktitle> <pages> pp. </pages> <month> 191-222. </month> <title> [8] , Computational experience with a globally convergent primal-dual predictor-corrector algorithm for linear programming, </title> <type> Technical Report SOR 92-10, </type> <institution> Program in Statistics and Operations Research, Princeton University, Princeton, N. J., </institution> <year> 1992. </year>
Reference-contexts: )u = (b Az + oefl 1 e) AY 1 z Z (A T c + oeZ 1 e):(8) Some interior-point codes for linear programming use the formulation (8), with modifications for handling dense columns in A and for dealing with non-standard linear programming formulations (see Lustig, Marsten, and Shanno <ref> [7, 8] </ref> and Xu, Hung, and Ye [17]). Other codes, notably those of Fourer and Mehrotra [1] and Vanderbei [13], handle the formulation (7). Analysis of algorithms for these formulations are discussed in another preprint [16]. <p> Assumptions and Basic Results. In the remainder of the paper, we focus on path-following interior-point methods. "Infeasible" variants of these methods are among the most widely used practical algorithms for linear programming and LCPs <ref> [7, 8] </ref> and have also been the subject of extensive theoretical investigations, which 4 STEPHEN J. WRIGHT have shown that they can have strong convergence properties under weak assumptions [19, 15, 14].
Reference: [9] <author> S. Mehrotra, </author> <title> On the implementation of a primal-dual interior point method, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 (1992), </volume> <pages> pp. 575-601. </pages>
Reference-contexts: value of LINEAR EQUATIONS IN INTERIOR-POINT METHODS 5 ff in [0; 1] that keeps (x k ; y k ) + ff (u k ; v k ) in the nonnegative orthant and then choose ff k to be a fixed fraction of this length.) The predictor-corrector strategy of Mehrotra <ref> [9] </ref>, used also in the codes of Lustig, Marsten, and Shanno [8], Vanderbei [13], and Xu, Hung, and Ye [17], adds extra terms to the lower part of the right-hand side on "corrector" iterations.
Reference: [10] <author> R. D. C. Monteiro and I. Adler, </author> <title> Interior path-following primal-dual algorithms. part II: Convex quadratic programming, </title> <journal> Mathematical Programming, </journal> <volume> 44 (1989), </volume> <pages> pp. 43-66. </pages>
Reference-contexts: Affine-scaling steps (u; v) are simply Newton steps for the system of nonlinear equations F (x; y) = M x + q y 0 :(5) Path-following methods (see, for example, Monteiro and Adler <ref> [10] </ref>, Zhang [19], Wright [14]) generate steps by using generally positive values of oe in (4). (As we see later, the algorithm of [14] allows oe = 0 on some iterates in an attempt to attain the rapid local convergence associated with Newton's method.) Potential-reduction methods (see, for example, Kojima, Mizuno,
Reference: [11] <author> D. B. </author> <title> Poncele on, Barrier methods for large-scale quadratic programming, </title> <type> PhD thesis, </type> <institution> Stan-ford University, </institution> <year> 1990. </year>
Reference-contexts: Section 2 presents the assumptions and a fundamental result from error analysis. Linear systems that arise in logarithmic barrier methods for constrained optimization methods were analyzed by Ponceleon <ref> [11] </ref>. The Newton equations for each logarithmic subproblem are similar to (6a) in that the large elements occur only on the diagonal.
Reference: [12] <author> F. A. Potra, </author> <title> An o(nl) infeasible-interior-point algorithm for LCP with quadratic convergence, </title> <note> Report on Computational Mathematics 50, </note> <institution> Department of Mathematics, University of Iowa, Iowa City, Iowa, </institution> <month> January </month> <year> 1994. </year> <note> 22 STEPHEN J. WRIGHT </note>
Reference-contexts: Predictor-corrector methods (see, for example, Ye and Anstreicher [18], Ji, Potra, and Huang [4], Potra <ref> [12] </ref>) take steps with either oe = 0 or oe = 1.
Reference: [13] <author> R. J. Vanderbei, </author> <title> LOQO User's Manual, </title> <type> Technical Report SOR 92-5, </type> <institution> Program in Statistics and Operations Research, Princeton University, Princeton, N. J., </institution> <year> 1992. </year>
Reference-contexts: Other codes, notably those of Fourer and Mehrotra [1] and Vanderbei <ref> [13] </ref>, handle the formulation (7). Analysis of algorithms for these formulations are discussed in another preprint [16]. In this paper, we focus on the system arising from general monotone LCP (6), and analyze the behavior of Gaussian elimination with pivoting applied to this system. <p> keeps (x k ; y k ) + ff (u k ; v k ) in the nonnegative orthant and then choose ff k to be a fixed fraction of this length.) The predictor-corrector strategy of Mehrotra [9], used also in the codes of Lustig, Marsten, and Shanno [8], Vanderbei <ref> [13] </ref>, and Xu, Hung, and Ye [17], adds extra terms to the lower part of the right-hand side on "corrector" iterations.
Reference: [14] <author> S. J. Wright, </author> <title> A path-following interior-point algorithm for linear and quadratic optimization problems, </title> <type> Preprint MCS-P401-1293, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> December </month> <year> 1993. </year> <title> [15] , An infeasible-interior-point algorithm for linear complementarity problems, </title> <journal> Mathematical Programming, </journal> <volume> 67 (1994), </volume> <pages> pp. </pages> <month> 29-52. </month> <title> [16] , Stability of linear algebra computations in interior-point methods for linear programming, </title> <type> Preprint MCS-P446-0694, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Affine-scaling steps (u; v) are simply Newton steps for the system of nonlinear equations F (x; y) = M x + q y 0 :(5) Path-following methods (see, for example, Monteiro and Adler [10], Zhang [19], Wright <ref> [14] </ref>) generate steps by using generally positive values of oe in (4). (As we see later, the algorithm of [14] allows oe = 0 on some iterates in an attempt to attain the rapid local convergence associated with Newton's method.) Potential-reduction methods (see, for example, Kojima, Mizuno, and Yoshise [6], Kojima, <p> Newton steps for the system of nonlinear equations F (x; y) = M x + q y 0 :(5) Path-following methods (see, for example, Monteiro and Adler [10], Zhang [19], Wright <ref> [14] </ref>) generate steps by using generally positive values of oe in (4). (As we see later, the algorithm of [14] allows oe = 0 on some iterates in an attempt to attain the rapid local convergence associated with Newton's method.) Potential-reduction methods (see, for example, Kojima, Mizuno, and Yoshise [6], Kojima, Kurita, and Mizuno [5]) also determine search directions by solving systems like (4), but they refer to a logarithmic <p> WRIGHT have shown that they can have strong convergence properties under weak assumptions <ref> [19, 15, 14] </ref>. The path-following infeasible-interior-point framework stated below also includes the class of predictor-corrector methods, for appropriate choices of the initial point and parameters. <p> When the initial point is feasible (r 0 = 0), predictor-corrector algorithms such as that of Ji, Potra, and Huang [4] are special cases of Algorithm PFI. This framework also includes the infeasible-interior-point algorithms of Zhang [19] and Wright <ref> [15, 14] </ref>. These algorithms choose fl k+1 and oe k so that a step ff k of nontrivial length can always be taken without violating the required conditions. In practical implementations of interior-point methods, the framework of Algorithm PFI is usually modified slightly. <p> The other results in the section also hold for complete pivoting, with minor modifications to the proofs. 4. Effect of Roundoff Error on Local Convergence. We now consider the algorithm in <ref> [14] </ref>, which can be described as follows. <p> We turn now to fast step, for which oe k = 0, fi k = fl t k , and fl k+1 = fl min +fl t k (fl max fl min ). The (exact) analysis in Wright <ref> [14] </ref> shows that fast steps are eventually always taken by the algorithm.
Reference: [17] <author> X. Xu, P. Hung, and Y. Ye, </author> <title> A simplified homogeneous and self-dual linear programming algorithm and its implementation. </title> <type> Manuscript, </type> <month> September </month> <year> 1993. </year>
Reference-contexts: e) AY 1 z Z (A T c + oeZ 1 e):(8) Some interior-point codes for linear programming use the formulation (8), with modifications for handling dense columns in A and for dealing with non-standard linear programming formulations (see Lustig, Marsten, and Shanno [7, 8] and Xu, Hung, and Ye <ref> [17] </ref>). Other codes, notably those of Fourer and Mehrotra [1] and Vanderbei [13], handle the formulation (7). Analysis of algorithms for these formulations are discussed in another preprint [16]. <p> ) + ff (u k ; v k ) in the nonnegative orthant and then choose ff k to be a fixed fraction of this length.) The predictor-corrector strategy of Mehrotra [9], used also in the codes of Lustig, Marsten, and Shanno [8], Vanderbei [13], and Xu, Hung, and Ye <ref> [17] </ref>, adds extra terms to the lower part of the right-hand side on "corrector" iterations.
Reference: [18] <author> Y. Ye and K. Anstreicher, </author> <title> On quadratic and O( p nL) convergence of a predictor-corrector algorithm for LCP, Mathematical Programming, Series A, </title> <booktitle> 62 (1993), </booktitle> <pages> pp. 537-551. </pages>
Reference-contexts: Predictor-corrector methods (see, for example, Ye and Anstreicher <ref> [18] </ref>, Ji, Potra, and Huang [4], Potra [12]) take steps with either oe = 0 or oe = 1.
Reference: [19] <author> Y. Zhang, </author> <title> On the convergence of a class of infeasible-interior-point methods for the horizontal linear complementarity problem, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4 (1994), </volume> <pages> pp. 208-227. </pages>
Reference-contexts: Affine-scaling steps (u; v) are simply Newton steps for the system of nonlinear equations F (x; y) = M x + q y 0 :(5) Path-following methods (see, for example, Monteiro and Adler [10], Zhang <ref> [19] </ref>, Wright [14]) generate steps by using generally positive values of oe in (4). (As we see later, the algorithm of [14] allows oe = 0 on some iterates in an attempt to attain the rapid local convergence associated with Newton's method.) Potential-reduction methods (see, for example, Kojima, Mizuno, and Yoshise <p> WRIGHT have shown that they can have strong convergence properties under weak assumptions <ref> [19, 15, 14] </ref>. The path-following infeasible-interior-point framework stated below also includes the class of predictor-corrector methods, for appropriate choices of the initial point and parameters. <p> When the initial point is feasible (r 0 = 0), predictor-corrector algorithms such as that of Ji, Potra, and Huang [4] are special cases of Algorithm PFI. This framework also includes the infeasible-interior-point algorithms of Zhang <ref> [19] </ref> and Wright [15, 14]. These algorithms choose fl k+1 and oe k so that a step ff k of nontrivial length can always be taken without violating the required conditions. In practical implementations of interior-point methods, the framework of Algorithm PFI is usually modified slightly.
References-found: 16

