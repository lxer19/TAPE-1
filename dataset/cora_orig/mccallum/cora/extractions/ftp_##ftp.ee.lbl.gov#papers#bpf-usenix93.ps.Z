URL: ftp://ftp.ee.lbl.gov/papers/bpf-usenix93.ps.Z
Refering-URL: http://www.cs.washington.edu/education/courses/552/CurrentQtr/
Root-URL: 
Email: mccanne@ee.lbl.gov, van@ee.lbl.gov  
Title: The BSD Packet Filter: A New Architecture for User-level Packet Capture  
Author: Steven McCanne and Van Jacobson 
Date: December 19, 1992  
Address: One Cyclotron Road Berkeley, CA 94720  
Affiliation: Lawrence Berkeley Laboratory  
Abstract: Many versions of Unix provide facilities for user-level packet capture, making possible the use of general purpose workstations for network monitoring. Because network monitors run as user-level processes, packets must be copied across the kernel/user-space protection boundary. This copying can be minimized by deploying a kernel agent called a packet filter, which discards unwanted packets as early as possible. The original Unix packet filter was designed around a stack-based filter evaluator that performs sub-optimally on current RISC CPUs. The BSD Packet Filter (BPF) uses a new, register-based filter evaluator that is up to 20 times faster than the original design. BPF also uses a straightforward buffering strategy that makes its overall performance up to 100 times faster than Sun's NIT running on the same hardware. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Braden, R. T. </author> <title> A pseudo-machine for packet monitoring and statistics. </title> <booktitle> In Proceedings of SIGCOMM '88 (Stanford, </booktitle> <address> CA, </address> <month> Aug. </month> <year> 1988), </year> <note> ACM. </note>
Reference-contexts: Historically there have been two approaches to the filter abstraction: a boolean expression tree (used by CSPF) and a directed acyclic control flow graph or CFG (first used by NNStat <ref> [1] </ref> and used by BPF). For example, Figure 4 illustrates the two models with a filter that recognizes either IP or ARP packets on an Ethernet. In the tree model each node represents a boolean operation while the leaves represent test predicates on packet fields. The edges represent operator-operand relationships. <p> Even after leveraging off the experience and pseudo-machine models of CSPF and NNStat <ref> [1] </ref>, the BPF model underwent several generations (and several years) of design and test. We believe the current model offers sufficient generality with no sacrifice in performance. Its evolution was guided by the following design constraints: 1. It must be protocol independent. <p> Each of the Sun NIT-based applications (etherfind, traffic, and rarpd) now has a BPF analog.) Finally, recent versions of NNStat <ref> [1] </ref> and nfswatch can be configured to run over BPF (in addition to running over NIT). 5 Conclusion BPF has proven to be an efficient, extensible, and portable interface for network monitoring.
Reference: [2] <institution> Digital Equipment Corporation. packetfilter(4), </institution> <note> Ultrix V4.1 Manual. </note>
Reference-contexts: Department of Energy under Contract No. DE-AC03-76SF00098. SunOS, the Ultrix Packet Filter <ref> [2] </ref> in DEC's Ultrix and Snoop in SGI's IRIX. These kernel facilities derive from pioneering work done at CMU and Stanford to adapt the Xerox Alto `packet filter' to a Unix kernel [8]. When completed in 1980, the CMU/Stanford Packet Filter, CSPF, provided a much needed and widely used facility.
Reference: [3] <author> Griswold, R. E., and Griswold, M. T. </author> <title> The Icon Programming Language. </title> <publisher> Prentice Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983. </year>
Reference-contexts: A common administrative nuisance is the use of a single IP address by more than one physical host, which arpwatch dutifully detects and reports. A very different application of BPF has been its incorporation into a variant of the Icon Programming Language <ref> [3] </ref>. Two new data types, a packet and a packet generator have been built into the Icon interpreter. Packets appear as first class record objects, allowing convenient "dot operator" access to packet headers.
Reference: [4] <author> Jacobson, V., Leres, C., and McCanne, S. </author> <title> The Tcpdump Manual Page. </title> <institution> Lawrence Berkeley Laboratory, Berkeley, </institution> <address> CA, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: While the CSPF model has shortcomings, it offers a novel generalization of packet filtering: The idea of putting a pseudo-machine language interpreter in the kernel provides a 7 This graph reordering is, however, a non-trivial problem. Our BPF compiler (part of tcpdump <ref> [4] </ref>) contains a fairly sophisticated optimizer to reorder and minimize CFG filters. This optimizer is the subject of a future paper. nice abstraction for describing and implementing the filtering mechanism. <p> The most widely used is tcpdump <ref> [4] </ref>, a network monitoring and data acquisition tool. Tcpdump performs three primary tasks: filter translation, packet acquisition, and packet display. Of interest here is the filter translation mechanism. A filter is specified with a user-friendly, high level description language.
Reference: [5] <author> Leres, C. </author> <title> The Arpwatch Manual Page. </title> <institution> Lawrence Berkeley Laboratory, Berkeley, </institution> <address> CA, </address> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: Of interest here is the filter translation mechanism. A filter is specified with a user-friendly, high level description language. Tcpdump has a built in compiler (and optimizer) which translates the high level filters into BPF programs. Of course, this translation process is transparent to the user. Arpwatch <ref> [5] </ref> is a passive monitoring program that tracks Ethernet to IP address mappings. It notifies the system administrator, via email, when new mappings are established or abnormal behavior is noted.
Reference: [6] <author> McCanne, S. </author> <title> The BPF Manual Page. </title> <institution> Lawrence Berkeley Laboratory, Berkeley, </institution> <address> CA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Table 1 shows the entire BPF instruction set. We have adopted this "assembler syntax" as a means of illustrating BPF filters and for debugging output. The actual encodings are defined with C macros, the details of which we omit here (see <ref> [6] </ref> for full details). The column labelled addr modes lists the addressing modes allowed for each instruction listed in the opcode column.
Reference: [7] <author> Mogul, J. C. </author> <title> Efficient use of workstations for passive monitoring of local area networks. </title> <booktitle> In Proceedings of SIGCOMM '90 (Philadelphia, </booktitle> <address> PA, </address> <month> Sept. </month> <year> 1990), </year> <note> ACM. </note>
Reference: [8] <author> Mogul, J. C., Rashid, R. F., and Accetta, M. J. </author> <title> The packet filter: An efficient mechanism for user-level network code. </title> <booktitle> In Proceedings of 11th Symposium on Operating Systems Principles (Austin, </booktitle> <address> TX, </address> <month> Nov. </month> <year> 1987), </year> <booktitle> ACM, </booktitle> <pages> pp. 39-51. </pages>
Reference-contexts: Department of Energy under Contract No. DE-AC03-76SF00098. SunOS, the Ultrix Packet Filter [2] in DEC's Ultrix and Snoop in SGI's IRIX. These kernel facilities derive from pioneering work done at CMU and Stanford to adapt the Xerox Alto `packet filter' to a Unix kernel <ref> [8] </ref>. When completed in 1980, the CMU/Stanford Packet Filter, CSPF, provided a much needed and widely used facility.
Reference: [9] <author> Rice, S. P. </author> <title> iprof source code, </title> <month> May </month> <year> 1991. </year> <institution> Brown University. </institution>
Reference-contexts: jeq #ETHERPROTO IP, L1, L5 L1: ldb [23] jeq #IPPROTO TCP, L2, L5 L2: ldh [20] jset #0x1fff, L5, L3 L3: ldx 4*([14]&0xf) jeq #N, L4, L5 L4: ret #TRUE L5: ret #0 3.6 Filter Performance Measurements We profiled the BPF and CSPF filtering models outside the kernel using iprof <ref> [9] </ref>, an instruction count profiler. To fully compare the two models, an indirection operator was added to CSPF so it could parse IP headers. The change was minor and did not adversely affect the original filtering performance.
Reference: [10] <author> Sun Microsystems Inc. </author> <title> NIT(4P); SunOS 4.1.1 Reference Manual. </title> <address> Mountain View, CA, </address> <month> Oct. </month> <year> 1990. </year> <title> Part Number: </title> <publisher> 800-5480-10. </publisher>
Reference-contexts: To allow such tools to be constructed, a kernel must contain some facility that gives user-level programs access to raw, unprocessed network traffic.[7] Most of today's workstation operating systems contain such a facility, e.g., NIT <ref> [10] </ref> in fl This is a preprint of a paper to be presented at the 1993 Winter USENIX conference, January 25-29, 1993, San Diego, CA. y This work was supported by the Director, Office of Energy Research, Scientific Computing Staff, of the U.S. Department of Energy under Contract No. DE-AC03-76SF00098. <p> Thus, if the packet is not accepted, only those bytes that were needed by the filtering process are referenced by the host. In contrast, SunOS's STREAMS NIT <ref> [10] </ref> copies the packets before filtering and as a result suffers a performance degradation. The STREAMS packet filter module (nit pf (4M)) sits on top of the packet capture module (nit if (4M)).
References-found: 10

