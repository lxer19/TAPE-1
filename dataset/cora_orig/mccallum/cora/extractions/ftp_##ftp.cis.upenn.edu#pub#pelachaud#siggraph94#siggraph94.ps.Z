URL: ftp://ftp.cis.upenn.edu/pub/pelachaud/siggraph94/siggraph94.ps.Z
Refering-URL: http://www.cis.upenn.edu/~hms/publications.html
Root-URL: 
Title: ANIMATED CONVERSATION: Rule-based Generation of Facial Expression, Gesture Spoken Intonation for Multiple Conversational Agents  
Author: Justine Cassell Catherine Pelachaud Norman Badler Mark Steedman Brett Achorn Tripp Becket Brett Douville Scott Prevost Matthew Stone 
Affiliation: Department of Computer Information Science, University of Pennsylvania  
Abstract: We describe an implemented system which automatically generates and animates conversations between multiple human-like agents with appropriate and synchronized speech, intonation, facial expressions, and hand gestures. Conversations are created by a dialogue planner that produces the text as well as the intonation of the utterances. The speaker/listener relationship, the text, and the intonation in turn drive facial expressions, lip motions, eye gaze, head motion, and arm gesture generators. Coordinated arm, wrist, and hand motions are invoked to create semantically meaningful gestures. Throughout, we will use examples from an actual synthesized, fully animated conversation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Argyle and M. Cook. </author> <title> Gaze and Mutual gaze. </title> <publisher> Cambridge University Press, </publisher> <year> 1976. </year>
Reference-contexts: In this system, facial expressions connected to intonation are automatically generated, while other kinds of expressions (emblems, for example) are specified by hand [29]. 4.7 Symbolic Gaze Specification Gaze can be classified into four primary categories depending on its role in the conversation <ref> [1] </ref>, [12]. In the following, we give rules of action and the functions for each of these four categories (see All right. &lt;pause&gt; You can write the check. planning : corresponds to the first phase of a turn when the speaker organizes her thoughts. <p> On the other hand, during the execution phase, the speaker knows what she is going to say and looks more at the listener. For a short turn (duration less than 1.5 sec.), the speaker and the listener establish eye contact (mutual gaze) <ref> [1] </ref> (short-turn). comment : accompanies and comments speech, by occurring in parallel with accent and emphasis. Accented or emphasized items are punctuated by head nods; the speaker looks toward the listener (accent). The speaker also gazes at the listener more when she asks a question.
Reference: [2] <author> N. I. Badler, B. A. Barsky, and D. Zeltzer, </author> <title> editors. Making Them Move: Mechanics, Control, and Animation of Articulated Figures. </title> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: But this does not imply that the end of manual or synthesized animation is near. Instead it raises the challenge of providing a sophisticated toolkit for human character animation that does not require the presence nor skill of a live actor <ref> [2] </ref>, thus freeing the skilled animator for more challenging tasks. In this paper we present our system for automatically animating conversations between multiple human-like agents with appropriate and synchronized speech, intonation, facial expressions, and hand gestures.
Reference: [3] <author> N. I. Badler, C. Phillips and B. L. Webber. </author> <title> Simulating Humans: Computer Graphics, Animation, and Control. </title> <publisher> Oxford University Press, </publisher> <month> June </month> <year> 1993. </year>
Reference: [4] <author> Welton M. Becket. </author> <title> The jack lisp api. </title> <type> Technical Report MS-CIS-94-01/Graphics Lab 59, </type> <institution> University of Pennsylvania, </institution> <year> 1994. </year>
Reference: [5] <author> Tom Calvert. </author> <title> Composition of realistic animation sequences for multiple human figures. </title> <editor> In Norman I. Badler, Brian A. Barsky, and David Zeltzer, editors, </editor> <title> Making Them Move: Mechanics, </title> <journal> Control, and Animation of Articulated Figures, </journal> <pages> pages 35-50. </pages> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Dynamics of arm gestures in ASL have been studied by Loomis et al [25]. Chen et al [10] constructed a virtual human that can shake hands with an interactive participant. Lee et al [23] automatically generate lifting gestures by considering strength and comfort measures. Moravec and Calvert <ref> [5] </ref> constructed a system that portrays the gestural interaction between two agents as they pass and greet one another.
Reference: [6] <author> J. Cappella. </author> <type> personal communication, </type> <year> 1993. </year>
Reference-contexts: For each of these states, Speaker and Listener can gaze at each other or not. This gives us 12 possibilities, or 24 per dyad. We can then compute the probability of being in each of these states <ref> [6] </ref>. Most of the nodes of the Pat-Net can be characterized by a certain set of states. For example the occurrence of a within-turn signal as we defined it corresponds to the action: person1 looks at the person2 while having the floor and pausing.
Reference: [7] <author> Justine Cassell, Mark Steedman, Norm Badler, Catherine Pelachaud, Matthew Stone, Brett Douville, Scott Prevost and Brett Achorn. </author> <title> Modeling the interaction between speech and gesture. </title> <booktitle> Proceedings of the Cognitive Science Society Annual Conference, </booktitle> <year> 1994. </year>
Reference-contexts: this dialogue is automatically generated on the basis of a database of facts describing the way the world works, a list of the goals of the two agents, and the set of beliefs of those two agents about the world, including the beliefs of the agents about one another [30], <ref> [7] </ref>. In this instance the two agents have goals that change over the course of the dialogue (Gilbert comes to have the goal of helping George get $50; George comes to have the goal of writing a check). <p> If a representational gesture is called for, the system accesses a dictionary of gestures (motion prototypes) that associates semantic representations with possible gestures that might represent them 2 (for further details, see <ref> [7] </ref>). In Figure 2, we see examples of how symbolic gestures are generated from discourse content. 1. Do you have a BLANK CHECK? * In the first frame, an iconic gesture (representing a rectangular check) is generated from the first mention (new to hearer) of the entity `blank check'. 2.
Reference: [8] <author> Justine Cassell and David McNeill. </author> <title> Gesture and the poetics of prose. </title> <journal> Poetics Today, </journal> <volume> 12 </volume> <pages> 375-404, </pages> <year> 1992. </year>
Reference: [9] <author> Justine Cassell, David McNeill, and Karl-Erik McCullough. Kids, </author> <title> don't try this at home: Experimental mismatches of speech and gesture. </title> <booktitle> presented at the International Communication Association annual meeting, </booktitle> <year> 1993. </year>
Reference-contexts: Finally, the importance of the interdependence of speech and gesture is shown by the fact that speakers rely on information conveyed in gesture sometimes even to the exclusion of information conveyed by accompanying speech as they try to comprehend a story <ref> [9] </ref>. Nevertheless, hand gestures and gaze behavior have been virtually absent from attempts to animate semi-autonomous agents in communicative contexts. 2.4 Synchrony of Gesture, Facial Movements, and Speech Facial expression, eye gaze and hand gestures do not do their communicative work only within single utterances, but also have inter-speaker effects.
Reference: [10] <author> D. T. Chen, S. D. Pieper, S. K. Singh, J. M. Rosen, and D. Zeltzer. </author> <title> The virtual sailor: An implementation of interactive human body modeling. </title> <booktitle> In Proc. 1993 Virtual Reality Annual International Symposium, </booktitle> <address> Seattle, WA, </address> <month> September </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: Lee and Kunii [22] built a system that includes handshapes and simple pre-stored facial expressions for American Sign Language (ASL) synthesis. Dynamics of arm gestures in ASL have been studied by Loomis et al [25]. Chen et al <ref> [10] </ref> constructed a virtual human that can shake hands with an interactive participant. Lee et al [23] automatically generate lifting gestures by considering strength and comfort measures. Moravec and Calvert [5] constructed a system that portrays the gestural interaction between two agents as they pass and greet one another.
Reference: [11] <author> M.M. Cohen and D.W. Massaro. </author> <title> Modeling coarticulation in synthetic visual speech. </title> <editor> In N.M. Thalmann and D.Thalmann, editors, </editor> <booktitle> Models and Techniques in Computer Animation, </booktitle> <pages> pages 139-156. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: For each AU the user can select starting and ending points of action, the intensity of action, the start and end tensions and the interpolation method to compute the in-between frames. An alternative approach is proposed by <ref> [11] </ref> with good results. Building a user-interface, [37] propose a categorization of facial expressions depending on their communicative meaning. For each of the facial functions a list of facial displays is performed (for example, remembering corresponds to eyebrow action, eye closure and one side of mouth pull back).
Reference: [12] <author> G. Collier. </author> <title> Emotional expression. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1985. </year>
Reference-contexts: In this system, facial expressions connected to intonation are automatically generated, while other kinds of expressions (emblems, for example) are specified by hand [29]. 4.7 Symbolic Gaze Specification Gaze can be classified into four primary categories depending on its role in the conversation [1], <ref> [12] </ref>. In the following, we give rules of action and the functions for each of these four categories (see All right. &lt;pause&gt; You can write the check. planning : corresponds to the first phase of a turn when the speaker organizes her thoughts.
Reference: [13] <author> W.S. Condon and W.D. </author> <title> Osgton.Speech and body motion synchrony of the speaker-hearer. </title> <editor> In D.H. Hortonand J.J. Jenkins, editors, </editor> <address> The perceptionof Language,pages 150-184. </address> <publisher> Academic Press, </publisher> <year> 1971. </year>
Reference-contexts: They do not occur randomly but rather are synchronized to one's own speech, or to the speech of others <ref> [13] </ref>, [20]. Eye gaze is also an important feature of non-verbal communicative behaviors. <p> Synchrony occurs at all levels of speech: the phonemic segment, word, phrase or long utterance. Different facial motions are characteristic of these different groups <ref> [13] </ref>, [20]. Some of them are more adapted to the phoneme level, like an eye blink, while others act at the word level, like a frown.
Reference: [14] <author> S. Duncan. </author> <title> Some signals and rules for taking speaking turns in conversations. In Weitz, editor, Nonverbal Communication. </title> <publisher> Oxford University Press, </publisher> <year> 1974. </year>
Reference-contexts: If the speaker doesn't emit a within-turn signal by gazing at the listener, the listener can still emit a back-channel which in turn may be followed by a continuation signal by the speaker. But the probability of action of the listener varies with the action of the speaker <ref> [14] </ref>; in particular, it decreases if no signal has occurred from the speaker. In this way the listener reacts to the behavior of the speaker. 4.8 Gaze Generator Each of the dialogic functions appears as a sub-network in the PaT-Net.
Reference: [15] <author> P. Ekman. </author> <title> Movements with precise meanings. </title> <journal> The Journal of Communication, </journal> <volume> 26, </volume> <year> 1976. </year>
Reference-contexts: In this section we discuss facial expression, and turn to gaze in the next section. P. Ekman and his colleagues characterize the set of semantic and syntactic facial expressions depending on their meaning <ref> [15] </ref>. Many facial functions exist (such as manipulators that correspond to biological needs of the face (wetting the lips); emblems and emotional emblems that are facial expressions replacing a word, an emotion) but only some are directly linked to the intonation of the voice.
Reference: [16] <author> P. Ekman. </author> <title> About brows: emotional and conversational signals. </title> <editor> In M. von Cranach, K. Foppa, W. Lepenies, and D. Ploog, editors, </editor> <title> Human ethology: claims and limits of a new disipline: </title> <booktitle> contributions to the Colloquium, </booktitle> <pages> pages 169-248. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England; New-York, </address> <year> 1979. </year>
Reference-contexts: Facial expressions can replace sequences of words (she was dressed [wrinkle nose, stick out tongue]) as well as accompany them <ref> [16] </ref>, and they can serve to help disambiguate what is being said when the acoustic signal is degraded. They do not occur randomly but rather are synchronized to one's own speech, or to the speech of others [13], [20]. Eye gaze is also an important feature of non-verbal communicative behaviors.
Reference: [17] <author> P. Ekman and W. Friesen. </author> <title> Facial Action Coding System. </title> <publisher> Consulting Psychologists Press, Inc., </publisher> <year> 1978. </year>
Reference-contexts: Most of the systems use FACS (Facial Action Coding System) as a notational system <ref> [17] </ref>. This system is based on anatomical studies, and describes any visible facial movements. An action unit AU, the basic element of this system, describes the action produced by one or a group of related muscles. The multi-layer approach [19] allows independent control at each level of the system.
Reference: [18] <author> Jean-Paul Gourret, Nadia Magnenat-Thalmann,and Daniel Thalmann. </author> <title> Simulation of object and human skin deformations in a grasping task. </title> <journal> Computer Graphics, </journal> <volume> 23(3) </volume> <pages> 21-30, </pages> <year> 1989. </year>
Reference-contexts: Animators frequently use key parameter techniques to create arm and hand motions. Rijpkema and Girard [33] created handshapes automatically based on the object being gripped. The Thalmanns <ref> [18, 26] </ref> improved on the hand model to include much better skin models and deformations of the finger tips and the gripped object. Lee and Kunii [22] built a system that includes handshapes and simple pre-stored facial expressions for American Sign Language (ASL) synthesis.
Reference: [19] <author> P. Kalra, A. Mangili, N. Magnenat-Thalmann, and D. </author> <title> Thalmann. </title> <editor> Smile: A multilayeredfacial animationsystem. In T.L. Kunii, editor, </editor> <booktitle> Modeling in Computer Graphics. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: This system is based on anatomical studies, and describes any visible facial movements. An action unit AU, the basic element of this system, describes the action produced by one or a group of related muscles. The multi-layer approach <ref> [19] </ref> allows independent control at each level of the system. At the lowest level (geometric level), geometry of the face can be modified using free form deformation techniques. At the highest level, facial animation can be computed from an input utterance. In M.
Reference: [20] <author> A. Kendon. </author> <title> Movement coordination in social interaction: some examples described. In Weitz, editor, Nonverbal Communication. </title> <publisher> Oxford University Press, </publisher> <year> 1974. </year>
Reference-contexts: They do not occur randomly but rather are synchronized to one's own speech, or to the speech of others [13], <ref> [20] </ref>. Eye gaze is also an important feature of non-verbal communicative behaviors. <p> Synchrony occurs at all levels of speech: the phonemic segment, word, phrase or long utterance. Different facial motions are characteristic of these different groups [13], <ref> [20] </ref>. Some of them are more adapted to the phoneme level, like an eye blink, while others act at the word level, like a frown.
Reference: [21] <author> Adam Kendon. </author> <title> Gesticulation and speech: Two aspects of the process of utterance. </title> <editor> In M.R.Key, editor, </editor> <booktitle> The Relation between Verbal and Nonverbal Communication, </booktitle> <pages> pages 207-227. </pages> <publisher> Mouton, </publisher> <year> 1980. </year>
Reference-contexts: Rather, gesture and speech are so intimately connected that one cannot say which one is dependent on the other. Both can be claimed to arise from a single internal encoding process ([8], <ref> [21] </ref>, [27]). 2.1 Example In this section of the paper we present a fragment of dialogue (the complete dialogue has been synthesized and animated), in which intonation, gesture, head and lip movements, and their inter-synchronization were automatically generated. <p> Note also, however, that following <ref> [21] </ref> we are led to believe that gestures may be more standardized in form than previously thought. the phoneme level. the last beat in the intonational phrase, with the end of relaxation to occur around the end of the intonational phrase.
Reference: [22] <author> Jintae Lee and Tosiyasu L. Kunii. </author> <title> Visual translation: From native language to sign language. </title> <booktitle> In Workshop on Visual Languages, </booktitle> <address> Seattle, WA, 1993. </address> <publisher> IEEE. </publisher>
Reference-contexts: Rijpkema and Girard [33] created handshapes automatically based on the object being gripped. The Thalmanns [18, 26] improved on the hand model to include much better skin models and deformations of the finger tips and the gripped object. Lee and Kunii <ref> [22] </ref> built a system that includes handshapes and simple pre-stored facial expressions for American Sign Language (ASL) synthesis. Dynamics of arm gestures in ASL have been studied by Loomis et al [25]. Chen et al [10] constructed a virtual human that can shake hands with an interactive participant.
Reference: [23] <author> Philip Lee, Susanna Wei, Jianmin Zhao, and Norman I. Badler. </author> <title> Strength guided motion. </title> <journal> Computer Graphics, </journal> <volume> 24(4) </volume> <pages> 253-262, </pages> <year> 1990. </year>
Reference-contexts: Dynamics of arm gestures in ASL have been studied by Loomis et al [25]. Chen et al [10] constructed a virtual human that can shake hands with an interactive participant. Lee et al <ref> [23] </ref> automatically generate lifting gestures by considering strength and comfort measures. Moravec and Calvert [5] constructed a system that portrays the gestural interaction between two agents as they pass and greet one another.
Reference: [24] <author> Mark Liberman and A. L. Buchsbaum. </author> <title> Structure and usage of current Bell Labs text to speech programs. </title> <type> Technical Memorandum TM 11225-850731-11, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1985. </year>
Reference: [25] <author> Jeffrey Loomis, Howard Poizner, Ursula Bellugi, Alynn Blakemore, and John Hollerbach. </author> <title> Computer graphic modeling of American Sign Language. </title> <journal> Computer Graphics, </journal> <volume> 17(3) </volume> <pages> 105-114, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Lee and Kunii [22] built a system that includes handshapes and simple pre-stored facial expressions for American Sign Language (ASL) synthesis. Dynamics of arm gestures in ASL have been studied by Loomis et al <ref> [25] </ref>. Chen et al [10] constructed a virtual human that can shake hands with an interactive participant. Lee et al [23] automatically generate lifting gestures by considering strength and comfort measures.
Reference: [26] <author> Nadia Magnenat-Thalmann and Daniel Thalmann. </author> <title> Human body deformations using joint-dependent local operators and finite-element theory. </title> <editor> In Norman I. Badler, Brian A. Barsky, and David Zeltzer, editors, </editor> <title> Making Them Move: Mechanics, Control, and Animation of Articulated Figures, </title> <address> pages 243-262.Morgan-Kaufmann, San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Animators frequently use key parameter techniques to create arm and hand motions. Rijpkema and Girard [33] created handshapes automatically based on the object being gripped. The Thalmanns <ref> [18, 26] </ref> improved on the hand model to include much better skin models and deformations of the finger tips and the gripped object. Lee and Kunii [22] built a system that includes handshapes and simple pre-stored facial expressions for American Sign Language (ASL) synthesis.
Reference: [27] <author> David McNeill. </author> <title> Hand and Mind: What Gestures Reveal about Thought.University of Chicago, </title> <year> 1992. </year>
Reference-contexts: Rather, gesture and speech are so intimately connected that one cannot say which one is dependent on the other. Both can be claimed to arise from a single internal encoding process ([8], [21], <ref> [27] </ref>). 2.1 Example In this section of the paper we present a fragment of dialogue (the complete dialogue has been synthesized and animated), in which intonation, gesture, head and lip movements, and their inter-synchronization were automatically generated. <p> In some discourse contexts about three-quarters of all clauses are accompanied by gestures of one kind or another; of these, about 40% are iconic, 40% are beats, and the remaining 20% are divided between deictic and metaphoric gestures <ref> [27] </ref>. And surprisingly, although the proportion of different gestures may change, all of these types of gestures, and spontaneous gesturing in general, are found in discourses by speakers of most languages. There is also a semantic and pragmatic relationship between the two media. <p> complex speech, it is the gesture which appears first (<ref> [27] </ref>). At the most local level, individual gestures and words are synchronized in time so that the `stroke' (most energetic part of the gesture) occurs either with or just before the phonologically most prominent syllable of the accompanying speech segment ([21], [27]). At the most global level, we find that the hands of the speaker come to rest at the end of a speaking turn, before the next speaker begins her turn. <p> Due to the structure of the conversation, where the speakers alternate turns, we assume similar alternation in gesturing. (Gesturing by listeners is almost non-existent <ref> [27] </ref>.) For the purposes of gesture generation, phoneme information is ignored; however, utterance barriers must be interpreted both to provide an envelope for the timing of a particular gesture or sequence of gestures and to determine which speaker is gesturing.
Reference: [28] <author> M. Patel. </author> <title> Making FACES. </title> <type> PhD thesis, </type> <institution> School of Mathematical Sciences, University of Bath, Bath, AVON, UK, </institution> <year> 1991. </year>
Reference-contexts: The multi-layer approach [19] allows independent control at each level of the system. At the lowest level (geometric level), geometry of the face can be modified using free form deformation techniques. At the highest level, facial animation can be computed from an input utterance. In M. Patel's model <ref> [28] </ref> facial animation can also be done at different levels of representation. It can be done either at the muscle level, the AU level or the script level.
Reference: [29] <author> C. Pelachaud, N.I. Badler, and M. Steedman. </author> <title> Linguistic issues in facial animation. </title> <editor> In N. Magnenat-Thalmann and D. Thalmann, editors, </editor> <booktitle> Computer Animation '91, </booktitle> <pages> pages 15-30. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: In this system, facial expressions connected to intonation are automatically generated, while other kinds of expressions (emblems, for example) are specified by hand <ref> [29] </ref>. 4.7 Symbolic Gaze Specification Gaze can be classified into four primary categories depending on its role in the conversation [1], [12]. <p> We use FACS to denote facial expressions. Each is represented by two parameters: its time of occurrence and its type. Our algorithm <ref> [29] </ref> embodies rules as described in Section 4.6 to automatically generate facial expressions, following the principle of synchrony. The program scans the input utterance and computes the different facial expressions corresponding to these functional groups. The computation of the lip shape is made in three passes and incorporates coarticulation effects. <p> The gaze direction is sustained by calling for the agent to look at a pre-defined point in the environment until a change is made by another action. For facial expressions, the program outputs the list of AUs that characterize each phonemic element and pause <ref> [29] </ref>. After scanning all the input utterances, all the actions to be performed are specified. Animation files are output.
Reference: [30] <editor> Richard Power. </editor> <booktitle> The organisation of purposeful dialogues. Linguistics, </booktitle> <year> 1977. </year>
Reference-contexts: of this dialogue is automatically generated on the basis of a database of facts describing the way the world works, a list of the goals of the two agents, and the set of beliefs of those two agents about the world, including the beliefs of the agents about one another <ref> [30] </ref>, [7]. In this instance the two agents have goals that change over the course of the dialogue (Gilbert comes to have the goal of helping George get $50; George comes to have the goal of writing a check).
Reference: [31] <author> Scott Prevost and Mark Steedman.Generating contextually appropriate intonation. </author> <booktitle> In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics, </booktitle> <pages> pages 332-340, </pages> <address> Utrecht, </address> <year> 1993. </year>
Reference-contexts: Text is generated and pitch accents and phrasal melodies are placed on generated text as outlined in [36] and <ref> [31] </ref>. This text is converted automatically to a form suitable for input to the AT&T Bell Laboratories TTS synthesizer ([24]).
Reference: [32] <author> Ellen F. Prince. </author> <title> The ZPG letter: Subjects, definiteness and information status. </title> <editor> In S. Thompson and W. Mann, editors, </editor> <booktitle> Discourse description: diverse analyses of a fund raising text, </booktitle> <pages> pages 295-325. </pages> <publisher> John Benjamins B.V., </publisher> <year> 1992. </year>
Reference-contexts: Further, references to entities are classified according to discourse status as either new to discourse and hearer (indefinites), new to discourse but not to hearer (definites on first mention), or old (all others) <ref> [32] </ref>. According to the following rules, these annotations, together with the earlier ones, determine which concepts will have an associated gesture.
Reference: [33] <author> Hans Rijpkema and Michael Girard. </author> <title> Computer animation of hands and grasping. </title> <journal> Computer Graphics, </journal> <volume> 25(4) </volume> <pages> 339-348, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Animators frequently use key parameter techniques to create arm and hand motions. Rijpkema and Girard <ref> [33] </ref> created handshapes automatically based on the object being gripped. The Thalmanns [18, 26] improved on the hand model to include much better skin models and deformations of the finger tips and the gripped object.
Reference: [34] <author> Barbara Robertson. </author> <title> Easy motion. </title> <journal> Computer Graphics World, </journal> <volume> 16(12) </volume> <pages> 33-38, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The emergence of low-cost, real-time motion sensing devices has led to renewed interest in active motion capture since 3D position and orientation trajectories may be acquired directly rather than from tedious image rotoscoping <ref> [34] </ref>. Both facial and gestural motions are efficiently tracked from a suitably harnessed actor. But this does not imply that the end of manual or synthesized animation is near.
Reference: [35] <author> Klaus R. Scherer. </author> <title> The functions of nonverbal signs in conversation. </title> <editor> In H. Giles R. St. Clair, editor, </editor> <booktitle> The Social and Physhological Contexts of Language, </booktitle> <pages> pages 225-243. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1980. </year>
Reference-contexts: When he says the phrase withdraw fifty dollars, Gilbert withdraws his hand towards his chest. 2.2 Communicative Significance of the Face Movements of the head and facial expressions can be characterized by their placement with respect to the linguistic utterance and their significance in transmitting information <ref> [35] </ref>. The set of facial movement clusters contains: * syntactic functions accompany the flow of speech and are synchronized at the verbal level.
Reference: [36] <author> Mark Steedman. </author> <title> Structure and intonation. </title> <booktitle> Language, </booktitle> <volume> 67 </volume> <pages> 260-296, </pages> <year> 1991. </year>
Reference-contexts: Text is generated and pitch accents and phrasal melodies are placed on generated text as outlined in <ref> [36] </ref> and [31]. This text is converted automatically to a form suitable for input to the AT&T Bell Laboratories TTS synthesizer ([24]).
Reference: [37] <author> Akikazu Takeuchi and Katashi Nagao. </author> <title> Communicative facial displays as a new conversational modality. </title> <booktitle> In ACM/IFIP INTERCHI'93, </booktitle> <address> Amsterdam, </address> <year> 1993. </year>
Reference-contexts: For each AU the user can select starting and ending points of action, the intensity of action, the start and end tensions and the interpolation method to compute the in-between frames. An alternative approach is proposed by [11] with good results. Building a user-interface, <ref> [37] </ref> propose a categorization of facial expressions depending on their communicative meaning. For each of the facial functions a list of facial displays is performed (for example, remembering corresponds to eyebrow action, eye closure and one side of mouth pull back). A user talks to the 3D synthetic actor.
Reference: [38] <author> K. Tuite. </author> <title> The production of gesture. </title> <journal> Semiotica, </journal> <volume> 93(1/2), </volume> <year> 1993. </year> <note> 6 Research Acknowledgments This research is partially supported by NSF Grants IRI90-18513, </note> <institution> IRI91-17110, CISE Grant CDA88-22719, NSF graduate fellow ships, NSF VPW GER-9350179; ARO Grant DAAL03-89-C-0031 including participation by the U.S. Army Research Laboratory (Ab erdeen); U.S. Air Force DEPTH contract through Hughes Missile Systems F33615-91-C-000; DMSO through the University of Iowa; National Defense Science and Engineering Graduate Fellowship in Computer Science DAAL03-92-G-0342; and NSF Instrumentation and Laboratory Improvement Program Grant USE-9152503. </institution>
References-found: 38

