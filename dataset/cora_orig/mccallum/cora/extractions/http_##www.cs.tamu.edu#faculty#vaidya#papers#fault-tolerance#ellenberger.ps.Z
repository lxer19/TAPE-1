URL: http://www.cs.tamu.edu/faculty/vaidya/papers/fault-tolerance/ellenberger.ps.Z
Refering-URL: http://www.cs.tamu.edu/faculty/vaidya/Vaidya-ftc.html
Root-URL: http://www.cs.tamu.edu
Title: TRANSPARENT PROCESS ROLLBACK RECOVERY: SOME NEW TECHNIQUES AND A PORTABLE IMPLEMENTATION  
Author: ERNEST LLOYD ELLENBERGER 
Degree: A Thesis by  in partial fulfillment of the requirements for the degree of MASTER OF SCIENCE  
Date: August 1995  
Affiliation: of Graduate Studies of Texas A&M University  
Note: Submitted to the Office  Major Subject: Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Lorenzo Alvisi, Bruce Hoppe, and Keith Marzullo. </author> <title> Nonblocking and orphan-free message logging protocols. </title> <booktitle> In Digest of Papers: The 23rd International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 145-154, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Recently, Alvisi et al. have given a definition of a class of message logging protocols, called family-based protocols, that are neither optimistic nor pessimistic but are are non-blocking and do not roll back non-failed processes. Family-based message logging (which is also called optimal logging) <ref> [1, 2] </ref> logs the contents of each message in the volatile storage of the sender and logs the processing order in the volatile storage of the next process that receives a message from the receiver (the receiver's processing order does not affect any other processes until it has sent a message <p> The measurements are of the impact of the Manetho protocol on the execution times of four different benchmark application programs (these same programs will be used in this thesis and are described in Chapter VI). 9 II.D Family-Based Message Logging Family-based logging (FBL) protocols <ref> [1, 2] </ref> never block the sender of a message and never introduce orphan processes during recovery. <p> In particular, a deterministic process is always stable if it participates in an orphan-free (e.g. pessimistic [4, 20] or optimal <ref> [9, 7, 1, 2] </ref>) message logging protocol. Reducing the set of state intervals that must be made stable will, in general, reduce the number of processes that must be communicated with to commit a state interval, and the reduced communication will yield improved performance. <p> IV.F.3 The Contents of SympInfo i for Family-Based Logging In family-based logging <ref> [1, 2] </ref>, the information necessary to recover a process p is: 1. the data contents of every message that has been sent to p 2. the determinant #m of every message m that p has delivered 4 When a failure is detected, the above information must be replicated immediately (either to <p> the master. #include &lt;msgpass.h&gt; main (int argc, char *argv []) - int i, j, NumSlaves; Message msg; /* Message is a generic message type defined in msgpass.h */ MsgPass_MasterInit (&argc, &argv); NumSlaves = 4; /* the full pathname should be used by the application */ argv [0] = ``/user/erniee/mp.solaris/gauss/slave''; argv <ref> [1] </ref> = NULL; /* start processes 1, 2, 3, and 4 */ for (i=0; i &lt; NumSlaves; i++) Exec (argv, ``''); /* establish a connection between every pair of slave processes */ for (i=1; i &lt;= NumSlaves; i++) for (j=i+1; j &lt;= NumSlaves; j++) ConnectProcesses (i, j); MsgPass_MasterInitComplete (); msg.hdr.msgType =
Reference: [2] <author> Lorenzo Alvisi and Keith Marzullo. </author> <title> Optimal message logging protocols. </title> <type> Technical Report TR94-1457, </type> <institution> Cornell University Department of Computer Science, </institution> <month> September </month> <year> 1994. </year> <title> Some sections are to appear in ICDCS-15 with the title Message Logging: Pessimistic, Optimistic, Causal and Optimal, and others have been submitted to FTCS-25 with the title Trade-Offs in Implementing Optimal Message Logging Protocols. </title>
Reference-contexts: Recently, Alvisi et al. have given a definition of a class of message logging protocols, called family-based protocols, that are neither optimistic nor pessimistic but are are non-blocking and do not roll back non-failed processes. Family-based message logging (which is also called optimal logging) <ref> [1, 2] </ref> logs the contents of each message in the volatile storage of the sender and logs the processing order in the volatile storage of the next process that receives a message from the receiver (the receiver's processing order does not affect any other processes until it has sent a message <p> Larger values of f require more information be appended onto application messages, but these protocols do not introduce any additional non-application messages. The Manetho protocol is a family-based protocol with f = n <ref> [2] </ref>. Many existing recovery techniques combine both checkpointing and message logging. During failure-free operation, the instantaneous state of each process can be saved periodically in a check 7 point, and messages can be logged between checkpoints. <p> The measurements are of the impact of the Manetho protocol on the execution times of four different benchmark application programs (these same programs will be used in this thesis and are described in Chapter VI). 9 II.D Family-Based Message Logging Family-based logging (FBL) protocols <ref> [1, 2] </ref> never block the sender of a message and never introduce orphan processes during recovery. <p> If a process overestimates jm:logj then it will unnecessarily log #m. A process must never underestimate jm:logj. Alvisi and Marzullo <ref> [2] </ref> describe three classes of estimation protocols that differ in the amount of additional information that is piggybacked on messages, and they show that the accuracy of a process's estimate of jm:logj can be increased by increasing the amount of piggybacked information. <p> Since f = 1, #m 1 need not be piggybacked on any subsequent messages p 2 sends. The acknowledgments of m 2 , m 3 , and m 4 have not yet arrived and are not shown in the figure. Alvisi <ref> [2] </ref> extends FBL to f &gt; 1 overlapping failures by requiring a process to piggyback a determinant #m on every message until the process learns that at least f processes (instead of just one process, as is done in the case of f = 1) have received (and hence logged) #m. <p> Alvisi's FBL protocols for f &gt; 1 use a mechanism 11 that maintains "weak dependency vectors", which are updated from the determinants piggybacked on each received message, to determine when it is safe to stop piggybacking a given determinant on outgoing messages. The FBL protocol presented by Alvisi <ref> [2] </ref> uses a data structure called a DetLog (determinant log) at each process to record the determinants logged at the process, and a SendLog at each process to record the message data of each message sent by the process. 12 CHAPTER III Recovery and Efficient Output Commit in the Presence of <p> In particular, a deterministic process is always stable if it participates in an orphan-free (e.g. pessimistic [4, 20] or optimal <ref> [9, 7, 1, 2] </ref>) message logging protocol. Reducing the set of state intervals that must be made stable will, in general, reduce the number of processes that must be communicated with to commit a state interval, and the reduced communication will yield improved performance. <p> When a failure is detected, the replication phase is initiated. 2 In family-based logging, the information necessary to recover the failed process is likely to be distributed across the volatile memories of several processes at which it is sympathetically logged (in the terminology of Alvisi <ref> [2] </ref>) on behalf of the failed process. If any of that information is lost, recovery will be impossible. <p> IV.F.3 The Contents of SympInfo i for Family-Based Logging In family-based logging <ref> [1, 2] </ref>, the information necessary to recover a process p is: 1. the data contents of every message that has been sent to p 2. the determinant #m of every message m that p has delivered 4 When a failure is detected, the above information must be replicated immediately (either to <p> This information is necessary for the operation of the family-based logging recovery algorithm <ref> [2] </ref>. IV.G A Simple One-Round Sim-FS Protocol for the Detection Phase This section describes a protocol that can be used for both general failure detection and as the failure detection phase of reactive replication. <p> The determinant log DetLog contains every determinant logged at the local process, and the send log SendLog contains the message data of every message that has been sent by the local process. These logs are maintained by the family-based logging protocol <ref> [2] </ref>. Determine-SympInfo must be executed at each process k that has sympathetically logged information on behalf of j. The broadcast-based protocol described here simply executes it at every process in the system. <p> The term "logging site" is 5 The logging site concept is described in more detail in Section V.C.3.6 42 due to Alvisi and Marzullo <ref> [2] </ref>. <p> Unix process checkpointing is done with the libckpt library [19], which has been extended with minor modifications that allow multiple checkpoints of different instances of a single program to be saved and stored simultaneously. * The family-based message logging protocol ff suggested by Alvisi and Marzullo <ref> [2] </ref>, which is able to tolerate multiple overlapping failures. In addition, a version of ff that uses logging sites [2] to maintain a single volatile log for each processor in shared memory (to reduce the amount of piggybacked information) has also been implemented. 45 The actual recovery portions of the protocols <p> minor modifications that allow multiple checkpoints of different instances of a single program to be saved and stored simultaneously. * The family-based message logging protocol ff suggested by Alvisi and Marzullo <ref> [2] </ref>, which is able to tolerate multiple overlapping failures. In addition, a version of ff that uses logging sites [2] to maintain a single volatile log for each processor in shared memory (to reduce the amount of piggybacked information) has also been implemented. 45 The actual recovery portions of the protocols have not been implemented; failure-free performance can be meaningfully measured without them, and they are left as future work. <p> equivalent to the ProcessorId type * IntSet a set of integer values (used for keeping track of sets of ProcessId's, ProcessorId's, or LoggingSiteId's) V.C.3 Family-Based Message Logging Implementation This section describes the portion of the recovery system that implements the family-based message logging protocol ff suggested by Alvisi and Marzullo <ref> [2] </ref>. They give a precise pseudocode version of ff , so the main implementation issues are the choices of data structures to represent their abstract set data structures (e.g. set of determinants, set of messages, information piggybacked on a message, and set of ProcessId's). <p> V.C.3.6 Logging Sites The logging site implementation takes advantage of an idea suggested independently by Alvisi and Marzullo <ref> [2] </ref> and Vaidya [30] to reduce message logging overhead when multiple processes share the same processor. <p> V.C.3.7 Implementation of Logging Sites Alvisi and Marzullo <ref> [2] </ref> suggest a similar approach that can be used with any family-based logging protocol, although they do not consider checkpointing. <p> involves replacing most references to Processid's with references to LoggingSiteIds, and changing the HandleSend function so that the determinant #m is not piggybacked on a message sent to a process q when q is known (by the sender) to be on a logging site at which #m is already logged <ref> [2] </ref>. Specifically, Alvisi and Marzullo introduce the function L (p) to denote the logging site associated with process p (this function is constant since processes are assumed to not move between logging sites) and the function P (l) to denote the set of processes associated with logging site l. <p> The ease with which an application originally written for a distributed operating system was converted to use the message passing library shows that the implementation is reasonably complete and usable by application programmers. The recovery system implementation also includes the first known implementation of the logging site technique <ref> [2, 30] </ref> for maintaining message logs in shared memory. Performance measurements of the implementation confirm previous claims that the overhead of checkpointing can be low [8].
Reference: [3] <author> B. Bhargava and S. R. Lian. </author> <title> Independent checkpointing and concurrent rollback recovery for distributed systems an optimistic approach. </title> <booktitle> In Proceedings of the 7th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 3-12, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Checkpoints older than the most recent can be discarded. The overhead of coordination and the nearly simultaneous occurrence of all checkpoints, which is likely to result in contention for the network and stable storage, are the primary drawbacks of coordinated checkpointing. * Independent checkpointing <ref> [3, 12, 14, 22, 29, 32, 33] </ref> allows the processes to take checkpoints independently, whenever they choose.
Reference: [4] <author> A. Borg, W. Blau, W. Graetsch, F. Herrmann, and W. Oberle. </author> <title> Fault tolerance under UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 1-24, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: It follows that such a computation can be recorded by logging the messages it exchanges to a storage medium that survives failures. Pessimistic message logging protocols <ref> [4, 20] </ref> wait (and block the application from proceeding on the local processor) until the message logging operation finishes successfully. The log must contain any message that the system is to process, so pessimistic protocols generally use either an atomic deliver and log or send and log operation. <p> The idea behind the technique presented here is that only the nondeterministic state intervals upon which is dependent must be stable for to be committed if all deterministic processes are always stable. In particular, a deterministic process is always stable if it participates in an orphan-free (e.g. pessimistic <ref> [4, 20] </ref> or optimal [9, 7, 1, 2]) message logging protocol. Reducing the set of state intervals that must be made stable will, in general, reduce the number of processes that must be communicated with to commit a state interval, and the reduced communication will yield improved performance.
Reference: [5] <author> K. M. Chandy and L. Lamport. </author> <title> Distributed snapshots: Determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: This thesis deals with checkpointing and message logging, which are both backward-error-recovery techniques. In distributed systems, the recovered global state must be consistent. A consistent global state is one that could occur during a failure-free run of the application program <ref> [5] </ref>. If the receipt of a message m by some process is recorded in the global state, then the sending of m must also be recorded in the state for the state to be consistent. <p> II.B Checkpointing in Distributed Systems Numerous checkpointing and recovery protocols have been developed for recovering a system to a consistent state. There are two distinct approaches: * Coordinated checkpointing <ref> [5, 27, 17, 18, 7] </ref> requires all processes to synchronize their checkpoints so that the state contained in the union of all the checkpoints (a global checkpoint) is a consistent global state.
Reference: [6] <author> D.R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Instructions for running applications and configuring the system are given in Appendix B. As described in the next chapter, an actual application program that originally ran under Manetho [7] on the V distributed operating system <ref> [6] </ref> has been successfully ported, with no significant changes, to use the message passing library and recovery system. 75 CHAPTER VI Performance Evaluation VI.A Introduction The performance of the recovery system implementation is evaluated here by comparing the execution times of benchmark application programs executing with the recovery system to the <p> 0.10 _sigprocmask [14] 7.9 3.04 0.30 388 0.77 0.77 _poll [19] 5.3 3.24 0.20 15 13.33 13.33 _open [22] 2.6 3.34 0.10 16 6.25 6.25 _close [26] 2.6 3.44 0.10 10 10.00 10.00 _ioctl [27] 2.6 3.54 0.10 6 16.67 16.67 _stat [29] 2.4 3.63 0.09 199 0.45 11.66 ReceiveMsg <ref> [6] </ref> 2.1 3.71 0.08 128 0.63 0.63 Compute [35] 0.5 3.73 0.02 786 0.03 0.03 realloc [48] 0.3 3.74 0.01 2112 0.00 0.00 __fabs [62] 0.3 3.75 0.01 1417 0.01 0.01 _free_unlocked [52] 0.3 3.76 0.01 262 0.04 3.89 SendMsg [11] 0.3 3.77 0.01 133 0.08 0.08 strlen [60] 0.3 3.78 <p> 0.20 3270 0.06 0.06 _sigprocmask [18] 6.5 2.40 0.20 798 0.25 0.25 _read [19] 6.5 2.60 0.20 389 0.51 0.51 _poll [20] 5.2 2.76 0.16 199 0.80 7.74 ReceiveMsg [8] 3.3 2.86 0.10 _getmsg [29] 2.3 2.93 0.07 128 0.55 0.55 Compute [30] 0.7 2.95 0.02 782 0.03 2.14 writen <ref> [6] </ref> 0.7 2.97 0.02 781 0.03 0.40 readn [12] 0.7 2.99 0.02 331 0.06 0.06 memcpy [39] 0.3 3.00 0.01 1635 0.01 0.01 _sigaddset [49] 0.3 3.01 0.01 1088 0.01 0.02 malloc &lt;cycle 1&gt; [41] 0.3 3.02 0.01 786 0.01 0.01 realloc [48] 0.3 3.03 0.01 425 0.02 0.04 search_pending_queue [44] <p> 1.27 _read [11] 7.4 3.97 0.43 4666 0.09 0.09 _sigprocmask [18] 5.1 4.27 0.30 388 0.77 0.77 _poll [21] 5.1 4.57 0.30 11 27.27 27.27 _ioctl [23] 5.0 4.86 0.29 195 1.49 1.53 Update_Logged_Dets [24] 3.2 5.05 0.19 195 0.97 1.07 Log_New_Dets [27] 2.7 5.21 0.16 199 0.80 19.15 ReceiveMsg <ref> [6] </ref> 1.7 5.31 0.10 16 6.25 6.25 _close [37] 1.6 5.41 0.10 _mcount (481) 1.4 5.49 0.08 128 0.63 0.63 Compute [42] 1.0 5.55 0.06 79228 0.00 0.00 IntSet_Cardinality [47] 0.9 5.60 0.05 193 0.26 0.32 FBL_HandleSend [46] 0.7 5.64 0.04 189 0.21 0.77 FBL_HandleAck [32] 0.5 5.67 0.03 36776 0.00 <p> 0.41 4670 0.09 0.09 _sigprocmask [17] 6.2 3.64 0.32 195 1.62 1.63 Update_Logged_Dets [19] 5.0 3.89 0.26 195 1.31 1.42 Log_New_Dets [20] 3.9 4.09 0.20 389 0.51 0.51 _poll [24] 3.9 4.29 0.20 11 18.18 18.18 _ioctl [25] 3.1 4.45 0.16 _mcount (479) 2.2 4.56 0.11 199 0.55 15.77 ReceiveMsg <ref> [6] </ref> 2.0 4.66 0.10 16 6.25 6.25 _close [36] 2.0 4.76 0.10 15 6.67 6.67 _open [37] 1.4 4.83 0.07 80700 0.00 0.00 IntSet_Cardinality [40] 1.2 4.89 0.06 128 0.47 0.47 Compute [45] 1.0 4.94 0.05 193 0.26 0.34 FBL_HandleSend [41] 0.4 4.96 0.02 18339 0.00 0.00 fflush [55] 0.4 4.98 <p> 0.80 1185 0.68 0.68 _read [13] 11.7 2.92 0.52 4654 0.11 0.11 _sigprocmask [15] 9.0 3.32 0.40 20 20.00 20.00 _open [16] 6.8 3.62 0.30 389 0.77 0.77 _poll [20] 4.1 3.80 0.18 _mcount (492) 3.2 3.94 0.14 128 1.09 1.09 Compute [28] 2.7 4.06 0.12 200 0.60 12.45 NewReceiveMsg <ref> [6] </ref> 1.4 4.12 0.06 60217 0.00 0.00 IntSet_Cardinality [35] 1.4 4.18 0.06 193 0.31 0.37 FBL_HandleSend [33] 0.9 4.22 0.04 2028 0.02 0.02 realloc [40] 0.7 4.25 0.03 189 0.16 0.58 FBL_HandleAck [31] 0.5 4.27 0.02 1162 0.02 1.59 writen [7] 0.5 4.29 0.02 195 0.10 0.18 Log_New_Dets [42] 0.5 4.31
Reference: [7] <author> Elmootazbellah N. Elnozahy. Manetho: </author> <title> Fault Tolerance in Distributed Systems Using Rollback-Recovery and Process Replication. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, Texas, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: II.B Checkpointing in Distributed Systems Numerous checkpointing and recovery protocols have been developed for recovering a system to a consistent state. There are two distinct approaches: * Coordinated checkpointing <ref> [5, 27, 17, 18, 7] </ref> requires all processes to synchronize their checkpoints so that the state contained in the union of all the checkpoints (a global checkpoint) is a consistent global state. <p> Strom, Bacon, and Yemini suggest an extension of sender-based logging to tolerate n simultaneous faults [28, 12]. The Manetho message logging protocol <ref> [9, 7] </ref> extends the idea of sender-based message logging by appending to each message an antecedence graph that represents all the events (including message receipt and nondeterministic events) that "happened before" the message. The antecedence graph contains sufficient information recover from an arbitrary number of simultaneous failures. <p> In particular, a deterministic process is always stable if it participates in an orphan-free (e.g. pessimistic [4, 20] or optimal <ref> [9, 7, 1, 2] </ref>) message logging protocol. Reducing the set of state intervals that must be made stable will, in general, reduce the number of processes that must be communicated with to commit a state interval, and the reduced communication will yield improved performance. <p> V.A.1 Recovery Protocols The following protocols have been implemented: * A coordinated checkpointing protocol similar to the Manetho coordinated checkpointing protocol <ref> [7] </ref>. The Manetho protocol has been extended to checkpoint a given set of processes to allow coordinated checkpoints that include only the processes on a single logging site. Only the processes on the logging site of the process that initiates a coordinated checkpoint are included in that coordinated checkpoint. <p> (S) isMember = FALSE; for (l=0; l &lt; MAX_CARDINALITY; l++) if (GETBIT (S, l) && LogSite_L_array [p] == l) - isMember = TRUE; break; - Figure V.C.7: Implementation of P [ (S) V.C.4 Coordinated Checkpointing Implementation The coordinated checkpointing protocol used in the implementation is based on that of Manetho <ref> [7] </ref>, with modifications to allow coordinated checkpointing to include only the processes on the same processor. Different processors can choose to checkpoint independently of each other, so checkpointing need not involve inter-processor communication. The system will still be recoverable if message logging is done for inter-processor messages. <p> Cross-checkpoint ("lost") messages can be detected by comparing the CCN tagged on each message to the receiver process's CCN and logged at the receiver (i.e. treated as input messages, as 74 in Manetho <ref> [7] </ref>). V.D Summary The successful incorporation of multiple different recovery protocols (coordinated checkpointing and family-based message logging) into the recovery system has shown that the system is indeed extensible. An application programmer can select any combination of recovery protocols by setting configuration options when the application is compiled. <p> The application is not required to call any recovery system functions, so the recovery system is transparent to the application programmer. Instructions for running applications and configuring the system are given in Appendix B. As described in the next chapter, an actual application program that originally ran under Manetho <ref> [7] </ref> on the V distributed operating system [6] has been successfully ported, with no significant changes, to use the message passing library and recovery system. 75 CHAPTER VI Performance Evaluation VI.A Introduction The performance of the recovery system implementation is evaluated here by comparing the execution times of benchmark application programs <p> All measurements are of failure-free execution. The benchmark application used here, called gauss <ref> [10, 7] </ref>, has been converted from Manetho to use the message passing library. This application has a master-slave structure, in which a single master process initializes the slaves and reports the total running time upon their completion. gauss performs Gaussian elimination with partial pivoting. <p> Forked and incremental checkpointing, which are provided by libckpt [19], would also improve performance, The overhead of the family-based message logging implementation is around twelve percent for the application tested, which is higher than average overhead of one to four percent of the Manetho protocol <ref> [7, 10] </ref>. This implementation has not been tuned for performance, and can likely be improved. The functions that deal with sets of integers (especially IntSet Cardinality) can be improved. <p> Problem size=128, and there are four processes, all on one processor. 84 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 52.1 1.60 1.60 804 1.99 1.99 _write <ref> [7] </ref> 13.0 2.00 0.40 15 26.67 26.67 _open [11] 6.5 2.20 0.20 3270 0.06 0.06 _sigprocmask [18] 6.5 2.40 0.20 798 0.25 0.25 _read [19] 6.5 2.60 0.20 389 0.51 0.51 _poll [20] 5.2 2.76 0.16 199 0.80 7.74 ReceiveMsg [8] 3.3 2.86 0.10 _getmsg [29] 2.3 2.93 0.07 128 0.55 <p> [72] 0.2 5.75 0.01 3371 0.00 0.00 _free_unlocked [65] 0.2 5.76 0.01 2651 0.00 0.01 malloc &lt;cycle 2&gt; [54] 0.2 5.77 0.01 2333 0.00 0.00 UnblockSig [77] 0.2 5.78 0.01 2333 0.00 0.00 _sigaddset [78] 0.2 5.79 0.01 2217 0.00 0.01 realloc [61] 0.2 5.80 0.01 1168 0.01 1.90 writen <ref> [7] </ref> 0.2 5.81 0.01 389 0.03 0.03 Update_Eval_Help [75] [remaining functions contribute little to execution time] TABLE VII Profiling results for FBL (no logging sites). <p> [70] 0.2 5.02 0.01 4055 0.00 0.00 _mutex_unlock_stub [75] 0.2 5.03 0.01 4021 0.00 0.00 IntSet_SetEmpty [71] 0.2 5.04 0.01 2652 0.00 0.01 malloc &lt;cycle 2&gt; [58] 0.2 5.05 0.01 2195 0.00 0.00 realloc [68] 0.2 5.06 0.01 1945 0.01 0.01 GetNthLargestElement [72] 0.2 5.07 0.01 1168 0.01 1.78 writen <ref> [7] </ref> 0.2 5.08 0.01 389 0.03 0.54 _select [22] 0.2 5.09 0.01 195 0.05 0.05 Update_D [74] 0.2 5.10 0.01 190 0.05 0.58 FBL_HandleAck [33] [remaining functions contribute little to execution time] TABLE VIII Profiling results for FBL (no logging sites). <p> 1.09 Compute [28] 2.7 4.06 0.12 200 0.60 12.45 NewReceiveMsg [6] 1.4 4.12 0.06 60217 0.00 0.00 IntSet_Cardinality [35] 1.4 4.18 0.06 193 0.31 0.37 FBL_HandleSend [33] 0.9 4.22 0.04 2028 0.02 0.02 realloc [40] 0.7 4.25 0.03 189 0.16 0.58 FBL_HandleAck [31] 0.5 4.27 0.02 1162 0.02 1.59 writen <ref> [7] </ref> 0.5 4.29 0.02 195 0.10 0.18 Log_New_Dets [42] 0.5 4.31 0.02 195 0.10 0.11 Update_Logged_Dets [46] 0.2 4.32 0.01 36417 0.00 0.00 _thr_main_stub [63] 0.2 4.33 0.01 17968 0.00 0.00 _fflush_u [60] 0.2 4.34 0.01 2985 0.00 0.01 _free_unlocked [44] 0.2 4.35 0.01 2202 0.00 0.02 malloc &lt;cycle 1&gt; [39]
Reference: [8] <author> Elmootazbellah N. Elnozahy, David B. Johnson, and Willy Zwaenepoel. </author> <title> The performance of consistent checkpointing. </title> <booktitle> In Proceedings of the 11th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 39-47, </pages> <month> October </month> <year> 1992. </year> <month> 99 </month>
Reference-contexts: The performance implications of larger values of f should be investigated. 83 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 36.9 1.40 1.40 812 1.72 1.72 _write <ref> [8] </ref> 26.4 2.40 1.00 796 1.26 1.26 _read [12] 9.0 2.74 0.34 3254 0.10 0.10 _sigprocmask [14] 7.9 3.04 0.30 388 0.77 0.77 _poll [19] 5.3 3.24 0.20 15 13.33 13.33 _open [22] 2.6 3.34 0.10 16 6.25 6.25 _close [26] 2.6 3.44 0.10 10 10.00 10.00 _ioctl [27] 2.6 3.54 <p> ms/call name ============================================================================= 52.1 1.60 1.60 804 1.99 1.99 _write [7] 13.0 2.00 0.40 15 26.67 26.67 _open [11] 6.5 2.20 0.20 3270 0.06 0.06 _sigprocmask [18] 6.5 2.40 0.20 798 0.25 0.25 _read [19] 6.5 2.60 0.20 389 0.51 0.51 _poll [20] 5.2 2.76 0.16 199 0.80 7.74 ReceiveMsg <ref> [8] </ref> 3.3 2.86 0.10 _getmsg [29] 2.3 2.93 0.07 128 0.55 0.55 Compute [30] 0.7 2.95 0.02 782 0.03 2.14 writen [6] 0.7 2.97 0.02 781 0.03 0.40 readn [12] 0.7 2.99 0.02 331 0.06 0.06 memcpy [39] 0.3 3.00 0.01 1635 0.01 0.01 _sigaddset [49] 0.3 3.01 0.01 1088 0.01 <p> Problem size=128, and there are four processes, two per processor. 85 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 34.9 2.04 2.04 1203 1.70 1.70 _write <ref> [8] </ref> 25.6 3.54 1.50 1182 1.27 1.27 _read [11] 7.4 3.97 0.43 4666 0.09 0.09 _sigprocmask [18] 5.1 4.27 0.30 388 0.77 0.77 _poll [21] 5.1 4.57 0.30 11 27.27 27.27 _ioctl [23] 5.0 4.86 0.29 195 1.49 1.53 Update_Logged_Dets [24] 3.2 5.05 0.19 195 0.97 1.07 Log_New_Dets [27] 2.7 5.21 <p> Problem size=128, and there are four processes, all on one processor. 86 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 37.5 1.91 1.91 1203 1.59 1.59 _write <ref> [8] </ref> 19.6 2.91 1.00 1184 0.84 0.84 _read [12] 8.0 3.32 0.41 4670 0.09 0.09 _sigprocmask [17] 6.2 3.64 0.32 195 1.62 1.63 Update_Logged_Dets [19] 5.0 3.89 0.26 195 1.31 1.42 Log_New_Dets [20] 3.9 4.09 0.20 389 0.51 0.51 _poll [24] 3.9 4.29 0.20 11 18.18 18.18 _ioctl [25] 3.1 4.45 <p> Problem size=128, and there are four processes, all on one processor. 87 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 36.0 1.60 1.60 1186 1.35 1.35 _write <ref> [8] </ref> 18.0 2.40 0.80 1185 0.68 0.68 _read [13] 11.7 2.92 0.52 4654 0.11 0.11 _sigprocmask [15] 9.0 3.32 0.40 20 20.00 20.00 _open [16] 6.8 3.62 0.30 389 0.77 0.77 _poll [20] 4.1 3.80 0.18 _mcount (492) 3.2 3.94 0.14 128 1.09 1.09 Compute [28] 2.7 4.06 0.12 200 0.60 <p> The recovery system implementation also includes the first known implementation of the logging site technique [2, 30] for maintaining message logs in shared memory. Performance measurements of the implementation confirm previous claims that the overhead of checkpointing can be low <ref> [8] </ref>. As has been observed by others, checkpointing performs well when checkpoints are written to local disks, but can perform abysmally when several large checkpoints are written to a shared disk simultaneously [31]. The overhead of the family-based message logging implementation is slightly greater than expected, but still reasonable.
Reference: [9] <author> Elmootazbellah N. Elnozahy and Willy Zwaenepoel. Manetho: </author> <title> Transparent rollback-recovery with low overhead, limited rollback, and fast output commit. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(5), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Strom, Bacon, and Yemini suggest an extension of sender-based logging to tolerate n simultaneous faults [28, 12]. The Manetho message logging protocol <ref> [9, 7] </ref> extends the idea of sender-based message logging by appending to each message an antecedence graph that represents all the events (including message receipt and nondeterministic events) that "happened before" the message. The antecedence graph contains sufficient information recover from an arbitrary number of simultaneous failures. <p> In particular, a deterministic process is always stable if it participates in an orphan-free (e.g. pessimistic [4, 20] or optimal <ref> [9, 7, 1, 2] </ref>) message logging protocol. Reducing the set of state intervals that must be made stable will, in general, reduce the number of processes that must be communicated with to commit a state interval, and the reduced communication will yield improved performance. <p> [44] 0.2 4.35 0.01 2202 0.00 0.02 malloc &lt;cycle 1&gt; [39] 0.2 4.36 0.01 1093 0.01 0.01 memcpy [61] 0.2 4.37 0.01 968 0.01 0.01 IntSet_SetEmpty [62] 0.2 4.38 0.01 389 0.03 0.82 Select [18] 0.2 4.39 0.01 389 0.03 0.80 _select [19] 0.2 4.40 0.01 262 0.04 4.09 SendMsg <ref> [9] </ref> 0.2 4.41 0.01 261 0.04 0.06 AllocateNewMsg [53] 0.2 4.42 0.01 190 0.05 3.23 WritePiggybackData [14] 0.2 4.43 0.01 128 0.08 6.83 ForwardMsg [12] [remaining functions contribute little to execution time] TABLE IX Profiling results for FBL with logging sites.
Reference: [10] <author> Elmootazbellah N. Elnozahy and Willy Zwaenepoel. </author> <title> On the use and implementation of message logging. </title> <booktitle> In Digest of Papers: The 24th International Symposium on Fault-Tolerant Computing, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Garbage collection is thus necessary to limit the number of stored checkpoints. An obvious conclusion is that the failure-free 5 overhead of independent checkpointing should be less than that of coordinated checkpointing, but measurements of implementations of both techniques in the Manetho system <ref> [10] </ref> indicate that the two techniques perform similarly. II.C Message Logging The messages exchanged in a distributed computation entirely define the computation if the computation is assumed to be deterministic. <p> The benefit of optimism is an expected reduction in failure-free overhead because of the elimination of waiting for logging operations, at the expense of a more complicated and slower recovery protocol which may roll back non-failed processes. Optimistic message logging can be done either by the sender <ref> [28, 13, 10] </ref>, the receiver [29, 14, 26, 16], or both (the latter is generally not done). <p> II.C.1 Performance In some recent measurements of implementations of message logging protocols in the Manetho system <ref> [10] </ref>, coordinated checkpointing without message logging was found to perform better in terms of failure-free overhead than all of the message logging protocols measured. Furthermore, coordinated checkpointing with message logging always performed better than independent checkpointing with message logging. <p> However, the time to commit output can be reduced by a message logging protocol. The output commit latency of the Manetho message logging protocol is considerably less 8 than that of a pure coordinated-checkpointing protocol for the implementations measured <ref> [10] </ref>. From these measurements it seems that the complexity of message logging is only outweighed by performance benefits if output commit latency is an important aspect of performance. The performance measurements of the Manetho system should not be taken as the last word on the performance of these techniques, however. <p> Message logging protocols generally exhibit better output commit performance than protocols that use checkpointing alone, however <ref> [10] </ref>. This chapter presents a recovery technique that uses message logging for deterministic state intervals and checkpointing for nondeterministic state intervals of the same process and explicitly tracks dependencies on nondeterministic processes. <p> All measurements are of failure-free execution. The benchmark application used here, called gauss <ref> [10, 7] </ref>, has been converted from Manetho to use the message passing library. This application has a master-slave structure, in which a single master process initializes the slaves and reports the total running time upon their completion. gauss performs Gaussian elimination with partial pivoting. <p> Forked and incremental checkpointing, which are provided by libckpt [19], would also improve performance, The overhead of the family-based message logging implementation is around twelve percent for the application tested, which is higher than average overhead of one to four percent of the Manetho protocol <ref> [7, 10] </ref>. This implementation has not been tuned for performance, and can likely be improved. The functions that deal with sets of integers (especially IntSet Cardinality) can be improved. <p> 0.10 _mcount (481) 1.4 5.49 0.08 128 0.63 0.63 Compute [42] 1.0 5.55 0.06 79228 0.00 0.00 IntSet_Cardinality [47] 0.9 5.60 0.05 193 0.26 0.32 FBL_HandleSend [46] 0.7 5.64 0.04 189 0.21 0.77 FBL_HandleAck [32] 0.5 5.67 0.03 36776 0.00 0.00 _thr_main_stub [56] 0.5 5.70 0.03 1165 0.03 1.49 readn <ref> [10] </ref> 0.2 5.71 0.01 18156 0.00 0.00 _fflush_u [69] 0.2 5.72 0.01 4872 0.00 0.00 IntSet_Add [70] 0.2 5.73 0.01 4101 0.00 0.00 IntSet_SetEmpty [71] 0.2 5.74 0.01 3890 0.00 0.00 GetIndexOfArrMax [72] 0.2 5.75 0.01 3371 0.00 0.00 _free_unlocked [65] 0.2 5.76 0.01 2651 0.00 0.01 malloc &lt;cycle 2&gt; [54]
Reference: [11] <author> Yennun Huang and Yi-Min Wang. </author> <title> Why optimistic message logging has not been used in telecommunications systems. </title> <note> Submitted to FTCS-25. </note>
Reference-contexts: Family-based protocols require some communication to retrieve logged information from other processes during recovery, so pessimistic protocols are the best choice for applications that require fast recovery <ref> [11] </ref>. In general, no single protocol is optimal for all possible applications. Family-based protocols can tolerate f overlapping (i.e. concurrent) process failures, where f is an integer between one and the number of processes in the system (n). <p> 16.67 _stat [29] 2.4 3.63 0.09 199 0.45 11.66 ReceiveMsg [6] 2.1 3.71 0.08 128 0.63 0.63 Compute [35] 0.5 3.73 0.02 786 0.03 0.03 realloc [48] 0.3 3.74 0.01 2112 0.00 0.00 __fabs [62] 0.3 3.75 0.01 1417 0.01 0.01 _free_unlocked [52] 0.3 3.76 0.01 262 0.04 3.89 SendMsg <ref> [11] </ref> 0.3 3.77 0.01 133 0.08 0.08 strlen [60] 0.3 3.78 0.01 18 0.56 0.56 _sigfillset [63] 0.1 3.79 0.01 _mcount (443) 0.1 3.79 0.01 _moncontrol [69] 0.0 3.79 0.00 2173 0.00 0.00 _mutex_lock_stub [191] 0.0 3.79 0.00 2108 0.00 0.00 _mutex_unlock_stub [192] 0.0 3.79 0.00 1629 0.00 0.00 _sigemptyset [193] <p> Problem size=128, and there are four processes, all on one processor. 84 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 52.1 1.60 1.60 804 1.99 1.99 _write [7] 13.0 2.00 0.40 15 26.67 26.67 _open <ref> [11] </ref> 6.5 2.20 0.20 3270 0.06 0.06 _sigprocmask [18] 6.5 2.40 0.20 798 0.25 0.25 _read [19] 6.5 2.60 0.20 389 0.51 0.51 _poll [20] 5.2 2.76 0.16 199 0.80 7.74 ReceiveMsg [8] 3.3 2.86 0.10 _getmsg [29] 2.3 2.93 0.07 128 0.55 0.55 Compute [30] 0.7 2.95 0.02 782 0.03 <p> Problem size=128, and there are four processes, two per processor. 85 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 34.9 2.04 2.04 1203 1.70 1.70 _write [8] 25.6 3.54 1.50 1182 1.27 1.27 _read <ref> [11] </ref> 7.4 3.97 0.43 4666 0.09 0.09 _sigprocmask [18] 5.1 4.27 0.30 388 0.77 0.77 _poll [21] 5.1 4.57 0.30 11 27.27 27.27 _ioctl [23] 5.0 4.86 0.29 195 1.49 1.53 Update_Logged_Dets [24] 3.2 5.05 0.19 195 0.97 1.07 Log_New_Dets [27] 2.7 5.21 0.16 199 0.80 19.15 ReceiveMsg [6] 1.7 5.31
Reference: [12] <author> D. B. Johnson. </author> <title> Distributed System Fault Tolerance Using Message Logging and Checkpointing. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Checkpoints older than the most recent can be discarded. The overhead of coordination and the nearly simultaneous occurrence of all checkpoints, which is likely to result in contention for the network and stable storage, are the primary drawbacks of coordinated checkpointing. * Independent checkpointing <ref> [3, 12, 14, 22, 29, 32, 33] </ref> allows the processes to take checkpoints independently, whenever they choose. <p> Optimistic message logging can be done either by the sender [28, 13, 10], the receiver [29, 14, 26, 16], or both (the latter is generally not done). The Sender-based message logging protocol <ref> [13, 12] </ref> requires that each message be logged in the volatile storage of its sender, and the sender must wait until an acknowledgment has been received from the receiver before sending another message. This protocol is pessimistic because of this synchronization. <p> When rollback occurs, the process is restored from its checkpoint, and the messages it had received after the checkpoint was taken are replayed to it by their senders. Strom, Bacon, and Yemini suggest an extension of sender-based logging to tolerate n simultaneous faults <ref> [28, 12] </ref>. The Manetho message logging protocol [9, 7] extends the idea of sender-based message logging by appending to each message an antecedence graph that represents all the events (including message receipt and nondeterministic events) that "happened before" the message. <p> A state interval of process i is called committable if and only if it will never be rolled back, which is true if and only if there exists some recoverable system state in which process i is in some state interval ff <ref> [12, 15] </ref>. Before a message can be sent to the outside world, the state interval from which it is being sent must be committable. <p> Using the definition of a stable state interval given in Equation III.1 with Johnson's system history lattice method [14], it can be shown that: 1. A unique most recent recoverable system state always exists. 2. A recovery algorithm such as Johnson's batch state recovery algorithm <ref> [12] </ref> or FINDREC algorithm [12, 14] will find the most recent recoverable system state. Also note that these recovery algorithms can be modified to restrict their search to nondeterministic dependencies in the same way that Johnson's Commit algorithm is restricted to communicate with only nondeterministic processes in Section III.F. 3. <p> Using the definition of a stable state interval given in Equation III.1 with Johnson's system history lattice method [14], it can be shown that: 1. A unique most recent recoverable system state always exists. 2. A recovery algorithm such as Johnson's batch state recovery algorithm [12] or FINDREC algorithm <ref> [12, 14] </ref> will find the most recent recoverable system state. Also note that these recovery algorithms can be modified to restrict their search to nondeterministic dependencies in the same way that Johnson's Commit algorithm is restricted to communicate with only nondeterministic processes in Section III.F. 3. <p> The performance implications of larger values of f should be investigated. 83 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 36.9 1.40 1.40 812 1.72 1.72 _write [8] 26.4 2.40 1.00 796 1.26 1.26 _read <ref> [12] </ref> 9.0 2.74 0.34 3254 0.10 0.10 _sigprocmask [14] 7.9 3.04 0.30 388 0.77 0.77 _poll [19] 5.3 3.24 0.20 15 13.33 13.33 _open [22] 2.6 3.34 0.10 16 6.25 6.25 _close [26] 2.6 3.44 0.10 10 10.00 10.00 _ioctl [27] 2.6 3.54 0.10 6 16.67 16.67 _stat [29] 2.4 3.63 <p> 0.20 798 0.25 0.25 _read [19] 6.5 2.60 0.20 389 0.51 0.51 _poll [20] 5.2 2.76 0.16 199 0.80 7.74 ReceiveMsg [8] 3.3 2.86 0.10 _getmsg [29] 2.3 2.93 0.07 128 0.55 0.55 Compute [30] 0.7 2.95 0.02 782 0.03 2.14 writen [6] 0.7 2.97 0.02 781 0.03 0.40 readn <ref> [12] </ref> 0.7 2.99 0.02 331 0.06 0.06 memcpy [39] 0.3 3.00 0.01 1635 0.01 0.01 _sigaddset [49] 0.3 3.01 0.01 1088 0.01 0.02 malloc &lt;cycle 1&gt; [41] 0.3 3.02 0.01 786 0.01 0.01 realloc [48] 0.3 3.03 0.01 425 0.02 0.04 search_pending_queue [44] 0.3 3.04 0.01 58 0.17 0.28 fread [43] <p> Problem size=128, and there are four processes, all on one processor. 86 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 37.5 1.91 1.91 1203 1.59 1.59 _write [8] 19.6 2.91 1.00 1184 0.84 0.84 _read <ref> [12] </ref> 8.0 3.32 0.41 4670 0.09 0.09 _sigprocmask [17] 6.2 3.64 0.32 195 1.62 1.63 Update_Logged_Dets [19] 5.0 3.89 0.26 195 1.31 1.42 Log_New_Dets [20] 3.9 4.09 0.20 389 0.51 0.51 _poll [24] 3.9 4.29 0.20 11 18.18 18.18 _ioctl [25] 3.1 4.45 0.16 _mcount (479) 2.2 4.56 0.11 199 0.55 <p> 0.01 IntSet_SetEmpty [62] 0.2 4.38 0.01 389 0.03 0.82 Select [18] 0.2 4.39 0.01 389 0.03 0.80 _select [19] 0.2 4.40 0.01 262 0.04 4.09 SendMsg [9] 0.2 4.41 0.01 261 0.04 0.06 AllocateNewMsg [53] 0.2 4.42 0.01 190 0.05 3.23 WritePiggybackData [14] 0.2 4.43 0.01 128 0.08 6.83 ForwardMsg <ref> [12] </ref> [remaining functions contribute little to execution time] TABLE IX Profiling results for FBL with logging sites.
Reference: [13] <author> D. B. Johnson and W. Zwaenopoel. </author> <title> Sender-based message logging. </title> <booktitle> In Digest of Papers: The 17th International Symposium on Fault-Tolerant Computing, </booktitle> <month> June </month> <year> 1987. </year>
Reference-contexts: The benefit of optimism is an expected reduction in failure-free overhead because of the elimination of waiting for logging operations, at the expense of a more complicated and slower recovery protocol which may roll back non-failed processes. Optimistic message logging can be done either by the sender <ref> [28, 13, 10] </ref>, the receiver [29, 14, 26, 16], or both (the latter is generally not done). <p> Optimistic message logging can be done either by the sender [28, 13, 10], the receiver [29, 14, 26, 16], or both (the latter is generally not done). The Sender-based message logging protocol <ref> [13, 12] </ref> requires that each message be logged in the volatile storage of its sender, and the sender must wait until an acknowledgment has been received from the receiver before sending another message. This protocol is pessimistic because of this synchronization. <p> Problem size=128, and there are four processes, all on one processor. 87 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 36.0 1.60 1.60 1186 1.35 1.35 _write [8] 18.0 2.40 0.80 1185 0.68 0.68 _read <ref> [13] </ref> 11.7 2.92 0.52 4654 0.11 0.11 _sigprocmask [15] 9.0 3.32 0.40 20 20.00 20.00 _open [16] 6.8 3.62 0.30 389 0.77 0.77 _poll [20] 4.1 3.80 0.18 _mcount (492) 3.2 3.94 0.14 128 1.09 1.09 Compute [28] 2.7 4.06 0.12 200 0.60 12.45 NewReceiveMsg [6] 1.4 4.12 0.06 60217 0.00
Reference: [14] <author> D. B. Johnson and W. Zwaenopoel. </author> <title> Recovery in distributed systems using optimistic message logging and checkpointing. </title> <journal> Journal of Algorithms, </journal> <volume> 11(3) </volume> <pages> 462-491, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Checkpoints older than the most recent can be discarded. The overhead of coordination and the nearly simultaneous occurrence of all checkpoints, which is likely to result in contention for the network and stable storage, are the primary drawbacks of coordinated checkpointing. * Independent checkpointing <ref> [3, 12, 14, 22, 29, 32, 33] </ref> allows the processes to take checkpoints independently, whenever they choose. <p> Optimistic message logging can be done either by the sender [28, 13, 10], the receiver <ref> [29, 14, 26, 16] </ref>, or both (the latter is generally not done). The Sender-based message logging protocol [13, 12] requires that each message be logged in the volatile storage of its sender, and the sender must wait until an acknowledgment has been received from the receiver before sending another message. <p> Using the definition of a stable state interval given in Equation III.1 with Johnson's system history lattice method <ref> [14] </ref>, it can be shown that: 1. A unique most recent recoverable system state always exists. 2. A recovery algorithm such as Johnson's batch state recovery algorithm [12] or FINDREC algorithm [12, 14] will find the most recent recoverable system state. <p> Using the definition of a stable state interval given in Equation III.1 with Johnson's system history lattice method [14], it can be shown that: 1. A unique most recent recoverable system state always exists. 2. A recovery algorithm such as Johnson's batch state recovery algorithm [12] or FINDREC algorithm <ref> [12, 14] </ref> will find the most recent recoverable system state. Also note that these recovery algorithms can be modified to restrict their search to nondeterministic dependencies in the same way that Johnson's Commit algorithm is restricted to communicate with only nondeterministic processes in Section III.F. 3. <p> The performance implications of larger values of f should be investigated. 83 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 36.9 1.40 1.40 812 1.72 1.72 _write [8] 26.4 2.40 1.00 796 1.26 1.26 _read [12] 9.0 2.74 0.34 3254 0.10 0.10 _sigprocmask <ref> [14] </ref> 7.9 3.04 0.30 388 0.77 0.77 _poll [19] 5.3 3.24 0.20 15 13.33 13.33 _open [22] 2.6 3.34 0.10 16 6.25 6.25 _close [26] 2.6 3.44 0.10 10 10.00 10.00 _ioctl [27] 2.6 3.54 0.10 6 16.67 16.67 _stat [29] 2.4 3.63 0.09 199 0.45 11.66 ReceiveMsg [6] 2.1 3.71 <p> 0.01 memcpy [61] 0.2 4.37 0.01 968 0.01 0.01 IntSet_SetEmpty [62] 0.2 4.38 0.01 389 0.03 0.82 Select [18] 0.2 4.39 0.01 389 0.03 0.80 _select [19] 0.2 4.40 0.01 262 0.04 4.09 SendMsg [9] 0.2 4.41 0.01 261 0.04 0.06 AllocateNewMsg [53] 0.2 4.42 0.01 190 0.05 3.23 WritePiggybackData <ref> [14] </ref> 0.2 4.43 0.01 128 0.08 6.83 ForwardMsg [12] [remaining functions contribute little to execution time] TABLE IX Profiling results for FBL with logging sites.
Reference: [15] <author> David B. Johnson. </author> <title> Efficient transparent optimistic rollback recovery for distributed application programs. </title> <booktitle> In Proceedings of the 12th Symposium on Reliable Distribted Systems, </booktitle> <month> October </month> <year> 1993. </year> <note> Also available as Computer Science Technical Report CMU-CS-93-127, </note> <institution> Carnegie Mellon University. </institution>
Reference-contexts: An output commit algorithm that uses that observation to reduce communication and checkpointing overhead is presented, and a method for recovering the system state after a failure is described. III.B Definitions This chapter uses Johnson's <ref> [15] </ref> definitions of nondeterministic execution and stable and committable state intervals. A state interval is called stable if and only if its process can be restored to some state in the interval, either from the information available on stable storage or available from 13 other processes. <p> A state interval of process i is called committable if and only if it will never be rolled back, which is true if and only if there exists some recoverable system state in which process i is in some state interval ff <ref> [12, 15] </ref>. Before a message can be sent to the outside world, the state interval from which it is being sent must be committable. <p> Before a message can be sent to the outside world, the state interval from which it is being sent must be committable. A process state interval is nondeterministic if it cannot necessarily be reproduced by restarting the process in the state it had at the beginning of the interval <ref> [15] </ref>. During nondeterministic execution, a process creates a new state interval before sending a message. A nondeterministic state interval can only be made stable by a checkpoint. <p> The output commit algorithm is described in Section III.F, and the recovery algorithm is described in Section III.H III.C Motivation A given state interval can be committed by ensuring that all (remote) process state intervals upon which causally depends are stable. Johnson's Commit algorithm <ref> [15] </ref> communicates with the minimum number of other processes necessary to commit a given state interval, where the set of processes that must be stable (and hence must be communicated with) is determined by the dependencies of the process being committed. <p> Hence direct dependencies alone are not sufficient to track the most recent causal ancestor, and the ND-direct definition is introduced. A commit algorithm that uses only direct dependencies was suggested by Johnson <ref> [15] </ref>. The basic idea is as follows: A commit algorithm can use ND-direct dependencies to commit a state interval by first informing the remote processes that have state intervals upon which is ND-directly dependent that they must make those state intervals stable. <p> Both ND-direct and ND-transitive dependencies allow the commit operation to avoid communication with the deterministic processes upon which the state interval being committed is dependent. Existing commit algorithms <ref> [15] </ref> do not distinguish between deterministic and nondeterministic processes when deciding which processes to communicate with. Hence the method suggested here will have lower communication overhead than existing commit algorithms. The rest of this chapter is organized as follows. <p> of a consistent checkpoint-ing algorithm, in terms of communication and number of checkpoints, can be reduced when only 1 Johnson claims that up to N rounds may be executed, where N is the number of processes in the system, but on the average the number of rounds will be small <ref> [15] </ref>. 18 nondeterministic processes need be checkpointed. The technique suggested here allows nondeterministic processes to send messages without check-pointing, which means that a process that delivers a message from a nondeterministic process is susceptible to being orphaned by the failure of the sender until the sender takes a subsequent checkpoint. <p> The remainder of this section describes the differences between a version of the Commit algorithm that uses ND-direct dependencies and a version that uses ND-transitive dependencies. III.F.1 Commit Algorithm Behavior When Only ND-Direct Dependencies are Main tained Johnson's Commit algorithm <ref> [15] </ref> can be used with no modifications to the algorithm itself. The only change is in way dependencies are tracked (which are used to maintain the DV vector in Johnson's algorithm). <p> III.F.2 Commit Algorithm Behavior When Only ND-Transitive Dependencies are Maintained In this case, Johnson's algorithm would have access to full transitive dependencies. The algorithm will still function correctly: it starts with more accurate (i.e. full transitive) dependency information, and does not need to collect that information in multiple rounds <ref> [15] </ref>. <p> Theorem 2 is similar to a theorem introduced by Johnson <ref> [15] </ref>. The difference between the two stems from the different environment assumed here. <p> Problem size=128, and there are four processes, all on one processor. 87 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 36.0 1.60 1.60 1186 1.35 1.35 _write [8] 18.0 2.40 0.80 1185 0.68 0.68 _read [13] 11.7 2.92 0.52 4654 0.11 0.11 _sigprocmask <ref> [15] </ref> 9.0 3.32 0.40 20 20.00 20.00 _open [16] 6.8 3.62 0.30 389 0.77 0.77 _poll [20] 4.1 3.80 0.18 _mcount (492) 3.2 3.94 0.14 128 1.09 1.09 Compute [28] 2.7 4.06 0.12 200 0.60 12.45 NewReceiveMsg [6] 1.4 4.12 0.06 60217 0.00 0.00 IntSet_Cardinality [35] 1.4 4.18 0.06 193 0.31
Reference: [16] <author> T. Juang and S. Venkatesan. </author> <title> Crash recovery with little overhead. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 454-461, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Optimistic message logging can be done either by the sender [28, 13, 10], the receiver <ref> [29, 14, 26, 16] </ref>, or both (the latter is generally not done). The Sender-based message logging protocol [13, 12] requires that each message be logged in the volatile storage of its sender, and the sender must wait until an acknowledgment has been received from the receiver before sending another message. <p> processes, all on one processor. 87 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 36.0 1.60 1.60 1186 1.35 1.35 _write [8] 18.0 2.40 0.80 1185 0.68 0.68 _read [13] 11.7 2.92 0.52 4654 0.11 0.11 _sigprocmask [15] 9.0 3.32 0.40 20 20.00 20.00 _open <ref> [16] </ref> 6.8 3.62 0.30 389 0.77 0.77 _poll [20] 4.1 3.80 0.18 _mcount (492) 3.2 3.94 0.14 128 1.09 1.09 Compute [28] 2.7 4.06 0.12 200 0.60 12.45 NewReceiveMsg [6] 1.4 4.12 0.06 60217 0.00 0.00 IntSet_Cardinality [35] 1.4 4.18 0.06 193 0.31 0.37 FBL_HandleSend [33] 0.9 4.22 0.04 2028 0.02
Reference: [17] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and roll-back recovery for distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: II.B Checkpointing in Distributed Systems Numerous checkpointing and recovery protocols have been developed for recovering a system to a consistent state. There are two distinct approaches: * Coordinated checkpointing <ref> [5, 27, 17, 18, 7] </ref> requires all processes to synchronize their checkpoints so that the state contained in the union of all the checkpoints (a global checkpoint) is a consistent global state. <p> Unlike Johnson's recovery algorithms, this algorithm has not been shown to always recover the maximum recoverable state. This algorithm is similar to many well known algorithms, such as that of Koo and Toueg <ref> [17] </ref>. <p> Problem size=128, and there are four processes, all on one processor. 86 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 37.5 1.91 1.91 1203 1.59 1.59 _write [8] 19.6 2.91 1.00 1184 0.84 0.84 _read [12] 8.0 3.32 0.41 4670 0.09 0.09 _sigprocmask <ref> [17] </ref> 6.2 3.64 0.32 195 1.62 1.63 Update_Logged_Dets [19] 5.0 3.89 0.26 195 1.31 1.42 Log_New_Dets [20] 3.9 4.09 0.20 389 0.51 0.51 _poll [24] 3.9 4.29 0.20 11 18.18 18.18 _ioctl [25] 3.1 4.45 0.16 _mcount (479) 2.2 4.56 0.11 199 0.55 15.77 ReceiveMsg [6] 2.0 4.66 0.10 16 6.25
Reference: [18] <author> Kai Li, Jeffrey F. Naughton, and James S. Plank. </author> <title> Checkpointing multicomputer applications. </title> <booktitle> In Proceedings of the 10th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 2-11, </pages> <month> September 100 </month>
Reference-contexts: II.B Checkpointing in Distributed Systems Numerous checkpointing and recovery protocols have been developed for recovering a system to a consistent state. There are two distinct approaches: * Coordinated checkpointing <ref> [5, 27, 17, 18, 7] </ref> requires all processes to synchronize their checkpoints so that the state contained in the union of all the checkpoints (a global checkpoint) is a consistent global state. <p> Problem size=128, and there are four processes, all on one processor. 84 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 52.1 1.60 1.60 804 1.99 1.99 _write [7] 13.0 2.00 0.40 15 26.67 26.67 _open [11] 6.5 2.20 0.20 3270 0.06 0.06 _sigprocmask <ref> [18] </ref> 6.5 2.40 0.20 798 0.25 0.25 _read [19] 6.5 2.60 0.20 389 0.51 0.51 _poll [20] 5.2 2.76 0.16 199 0.80 7.74 ReceiveMsg [8] 3.3 2.86 0.10 _getmsg [29] 2.3 2.93 0.07 128 0.55 0.55 Compute [30] 0.7 2.95 0.02 782 0.03 2.14 writen [6] 0.7 2.97 0.02 781 0.03 <p> Problem size=128, and there are four processes, two per processor. 85 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 34.9 2.04 2.04 1203 1.70 1.70 _write [8] 25.6 3.54 1.50 1182 1.27 1.27 _read [11] 7.4 3.97 0.43 4666 0.09 0.09 _sigprocmask <ref> [18] </ref> 5.1 4.27 0.30 388 0.77 0.77 _poll [21] 5.1 4.57 0.30 11 27.27 27.27 _ioctl [23] 5.0 4.86 0.29 195 1.49 1.53 Update_Logged_Dets [24] 3.2 5.05 0.19 195 0.97 1.07 Log_New_Dets [27] 2.7 5.21 0.16 199 0.80 19.15 ReceiveMsg [6] 1.7 5.31 0.10 16 6.25 6.25 _close [37] 1.6 5.41 <p> [63] 0.2 4.33 0.01 17968 0.00 0.00 _fflush_u [60] 0.2 4.34 0.01 2985 0.00 0.01 _free_unlocked [44] 0.2 4.35 0.01 2202 0.00 0.02 malloc &lt;cycle 1&gt; [39] 0.2 4.36 0.01 1093 0.01 0.01 memcpy [61] 0.2 4.37 0.01 968 0.01 0.01 IntSet_SetEmpty [62] 0.2 4.38 0.01 389 0.03 0.82 Select <ref> [18] </ref> 0.2 4.39 0.01 389 0.03 0.80 _select [19] 0.2 4.40 0.01 262 0.04 4.09 SendMsg [9] 0.2 4.41 0.01 261 0.04 0.06 AllocateNewMsg [53] 0.2 4.42 0.01 190 0.05 3.23 WritePiggybackData [14] 0.2 4.43 0.01 128 0.08 6.83 ForwardMsg [12] [remaining functions contribute little to execution time] TABLE IX Profiling
Reference: [19] <author> James S. Plank, Micah Beck, Gerry Kingsley, and Kai Li. Libckpt: </author> <title> Transparent checkpointing under Unix. </title> <booktitle> In Proceedings of the Winter Usenix Conference, </booktitle> <year> 1995. </year>
Reference-contexts: A consistent checkpoint of all processes in the system can be taken by executing the coordinated checkpointing protocol where the set of processes to be checkpointed contains only the set of site coordinators. Unix process checkpointing is done with the libckpt library <ref> [19] </ref>, which has been extended with minor modifications that allow multiple checkpoints of different instances of a single program to be saved and stored simultaneously. * The family-based message logging protocol ff suggested by Alvisi and Marzullo [2], which is able to tolerate multiple overlapping failures. <p> Checkpointing overhead can be reduced by decreasing the checkpointing rate, which is artificially high here because of the short application run time. Forked and incremental checkpointing, which are provided by libckpt <ref> [19] </ref>, would also improve performance, The overhead of the family-based message logging implementation is around twelve percent for the application tested, which is higher than average overhead of one to four percent of the Manetho protocol [7, 10]. <p> of f should be investigated. 83 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 36.9 1.40 1.40 812 1.72 1.72 _write [8] 26.4 2.40 1.00 796 1.26 1.26 _read [12] 9.0 2.74 0.34 3254 0.10 0.10 _sigprocmask [14] 7.9 3.04 0.30 388 0.77 0.77 _poll <ref> [19] </ref> 5.3 3.24 0.20 15 13.33 13.33 _open [22] 2.6 3.34 0.10 16 6.25 6.25 _close [26] 2.6 3.44 0.10 10 10.00 10.00 _ioctl [27] 2.6 3.54 0.10 6 16.67 16.67 _stat [29] 2.4 3.63 0.09 199 0.45 11.66 ReceiveMsg [6] 2.1 3.71 0.08 128 0.63 0.63 Compute [35] 0.5 3.73 <p> processes, all on one processor. 84 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 52.1 1.60 1.60 804 1.99 1.99 _write [7] 13.0 2.00 0.40 15 26.67 26.67 _open [11] 6.5 2.20 0.20 3270 0.06 0.06 _sigprocmask [18] 6.5 2.40 0.20 798 0.25 0.25 _read <ref> [19] </ref> 6.5 2.60 0.20 389 0.51 0.51 _poll [20] 5.2 2.76 0.16 199 0.80 7.74 ReceiveMsg [8] 3.3 2.86 0.10 _getmsg [29] 2.3 2.93 0.07 128 0.55 0.55 Compute [30] 0.7 2.95 0.02 782 0.03 2.14 writen [6] 0.7 2.97 0.02 781 0.03 0.40 readn [12] 0.7 2.99 0.02 331 0.06 <p> processes, all on one processor. 86 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 37.5 1.91 1.91 1203 1.59 1.59 _write [8] 19.6 2.91 1.00 1184 0.84 0.84 _read [12] 8.0 3.32 0.41 4670 0.09 0.09 _sigprocmask [17] 6.2 3.64 0.32 195 1.62 1.63 Update_Logged_Dets <ref> [19] </ref> 5.0 3.89 0.26 195 1.31 1.42 Log_New_Dets [20] 3.9 4.09 0.20 389 0.51 0.51 _poll [24] 3.9 4.29 0.20 11 18.18 18.18 _ioctl [25] 3.1 4.45 0.16 _mcount (479) 2.2 4.56 0.11 199 0.55 15.77 ReceiveMsg [6] 2.0 4.66 0.10 16 6.25 6.25 _close [36] 2.0 4.76 0.10 15 6.67 <p> [60] 0.2 4.34 0.01 2985 0.00 0.01 _free_unlocked [44] 0.2 4.35 0.01 2202 0.00 0.02 malloc &lt;cycle 1&gt; [39] 0.2 4.36 0.01 1093 0.01 0.01 memcpy [61] 0.2 4.37 0.01 968 0.01 0.01 IntSet_SetEmpty [62] 0.2 4.38 0.01 389 0.03 0.82 Select [18] 0.2 4.39 0.01 389 0.03 0.80 _select <ref> [19] </ref> 0.2 4.40 0.01 262 0.04 4.09 SendMsg [9] 0.2 4.41 0.01 261 0.04 0.06 AllocateNewMsg [53] 0.2 4.42 0.01 190 0.05 3.23 WritePiggybackData [14] 0.2 4.43 0.01 128 0.08 6.83 ForwardMsg [12] [remaining functions contribute little to execution time] TABLE IX Profiling results for FBL with logging sites.
Reference: [20] <author> M. L. Powell and D. L. Presotto. </author> <title> Publishing: A reliable broadcast communication mechansim. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 100-109, </pages> <year> 1983. </year>
Reference-contexts: It follows that such a computation can be recorded by logging the messages it exchanges to a storage medium that survives failures. Pessimistic message logging protocols <ref> [4, 20] </ref> wait (and block the application from proceeding on the local processor) until the message logging operation finishes successfully. The log must contain any message that the system is to process, so pessimistic protocols generally use either an atomic deliver and log or send and log operation. <p> The idea behind the technique presented here is that only the nondeterministic state intervals upon which is dependent must be stable for to be committed if all deterministic processes are always stable. In particular, a deterministic process is always stable if it participates in an orphan-free (e.g. pessimistic <ref> [4, 20] </ref> or optimal [9, 7, 1, 2]) message logging protocol. Reducing the set of state intervals that must be made stable will, in general, reduce the number of processes that must be communicated with to commit a state interval, and the reduced communication will yield improved performance. <p> self self total time seconds seconds calls ms/call ms/call name ============================================================================= 52.1 1.60 1.60 804 1.99 1.99 _write [7] 13.0 2.00 0.40 15 26.67 26.67 _open [11] 6.5 2.20 0.20 3270 0.06 0.06 _sigprocmask [18] 6.5 2.40 0.20 798 0.25 0.25 _read [19] 6.5 2.60 0.20 389 0.51 0.51 _poll <ref> [20] </ref> 5.2 2.76 0.16 199 0.80 7.74 ReceiveMsg [8] 3.3 2.86 0.10 _getmsg [29] 2.3 2.93 0.07 128 0.55 0.55 Compute [30] 0.7 2.95 0.02 782 0.03 2.14 writen [6] 0.7 2.97 0.02 781 0.03 0.40 readn [12] 0.7 2.99 0.02 331 0.06 0.06 memcpy [39] 0.3 3.00 0.01 1635 0.01 <p> self self total time seconds seconds calls ms/call ms/call name ============================================================================= 37.5 1.91 1.91 1203 1.59 1.59 _write [8] 19.6 2.91 1.00 1184 0.84 0.84 _read [12] 8.0 3.32 0.41 4670 0.09 0.09 _sigprocmask [17] 6.2 3.64 0.32 195 1.62 1.63 Update_Logged_Dets [19] 5.0 3.89 0.26 195 1.31 1.42 Log_New_Dets <ref> [20] </ref> 3.9 4.09 0.20 389 0.51 0.51 _poll [24] 3.9 4.29 0.20 11 18.18 18.18 _ioctl [25] 3.1 4.45 0.16 _mcount (479) 2.2 4.56 0.11 199 0.55 15.77 ReceiveMsg [6] 2.0 4.66 0.10 16 6.25 6.25 _close [36] 2.0 4.76 0.10 15 6.67 6.67 _open [37] 1.4 4.83 0.07 80700 0.00 <p> self self total time seconds seconds calls ms/call ms/call name ============================================================================= 36.0 1.60 1.60 1186 1.35 1.35 _write [8] 18.0 2.40 0.80 1185 0.68 0.68 _read [13] 11.7 2.92 0.52 4654 0.11 0.11 _sigprocmask [15] 9.0 3.32 0.40 20 20.00 20.00 _open [16] 6.8 3.62 0.30 389 0.77 0.77 _poll <ref> [20] </ref> 4.1 3.80 0.18 _mcount (492) 3.2 3.94 0.14 128 1.09 1.09 Compute [28] 2.7 4.06 0.12 200 0.60 12.45 NewReceiveMsg [6] 1.4 4.12 0.06 60217 0.00 0.00 IntSet_Cardinality [35] 1.4 4.18 0.06 193 0.31 0.37 FBL_HandleSend [33] 0.9 4.22 0.04 2028 0.02 0.02 realloc [40] 0.7 4.25 0.03 189 0.16
Reference: [21] <author> B. Randell. </author> <title> System structure for software fault tolerance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(2):220-232, </volume> <month> June </month> <year> 1975. </year>
Reference-contexts: The recovery protocol must attempt to construct a consistent global state from the checkpoints that are available, so it may be necessary to roll a process back past multiple checkpoints (the occurrence of such is called the domino effect <ref> [21] </ref> because rollbacks may propagate in a chain reaction). Since independent methods do not guarantee that the most recent checkpoints of all processes form a consistent global state, they must maintain more than one previous checkpoint for each process to increase the probability that a consistent global checkpoint will exist. <p> four processes, two per processor. 85 % cumulative self self total time seconds seconds calls ms/call ms/call name ============================================================================= 34.9 2.04 2.04 1203 1.70 1.70 _write [8] 25.6 3.54 1.50 1182 1.27 1.27 _read [11] 7.4 3.97 0.43 4666 0.09 0.09 _sigprocmask [18] 5.1 4.27 0.30 388 0.77 0.77 _poll <ref> [21] </ref> 5.1 4.57 0.30 11 27.27 27.27 _ioctl [23] 5.0 4.86 0.29 195 1.49 1.53 Update_Logged_Dets [24] 3.2 5.05 0.19 195 0.97 1.07 Log_New_Dets [27] 2.7 5.21 0.16 199 0.80 19.15 ReceiveMsg [6] 1.7 5.31 0.10 16 6.25 6.25 _close [37] 1.6 5.41 0.10 _mcount (481) 1.4 5.49 0.08 128 0.63
Reference: [22] <author> D. L. Russell. </author> <title> State restoration in systems of communicating processes. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6(2):183-194, </volume> <month> March </month> <year> 1980. </year>
Reference-contexts: Checkpoints older than the most recent can be discarded. The overhead of coordination and the nearly simultaneous occurrence of all checkpoints, which is likely to result in contention for the network and stable storage, are the primary drawbacks of coordinated checkpointing. * Independent checkpointing <ref> [3, 12, 14, 22, 29, 32, 33] </ref> allows the processes to take checkpoints independently, whenever they choose. <p> self self total time seconds seconds calls ms/call ms/call name ============================================================================= 36.9 1.40 1.40 812 1.72 1.72 _write [8] 26.4 2.40 1.00 796 1.26 1.26 _read [12] 9.0 2.74 0.34 3254 0.10 0.10 _sigprocmask [14] 7.9 3.04 0.30 388 0.77 0.77 _poll [19] 5.3 3.24 0.20 15 13.33 13.33 _open <ref> [22] </ref> 2.6 3.34 0.10 16 6.25 6.25 _close [26] 2.6 3.44 0.10 10 10.00 10.00 _ioctl [27] 2.6 3.54 0.10 6 16.67 16.67 _stat [29] 2.4 3.63 0.09 199 0.45 11.66 ReceiveMsg [6] 2.1 3.71 0.08 128 0.63 0.63 Compute [35] 0.5 3.73 0.02 786 0.03 0.03 realloc [48] 0.3 3.74 <p> [75] 0.2 5.03 0.01 4021 0.00 0.00 IntSet_SetEmpty [71] 0.2 5.04 0.01 2652 0.00 0.01 malloc &lt;cycle 2&gt; [58] 0.2 5.05 0.01 2195 0.00 0.00 realloc [68] 0.2 5.06 0.01 1945 0.01 0.01 GetNthLargestElement [72] 0.2 5.07 0.01 1168 0.01 1.78 writen [7] 0.2 5.08 0.01 389 0.03 0.54 _select <ref> [22] </ref> 0.2 5.09 0.01 195 0.05 0.05 Update_D [74] 0.2 5.10 0.01 190 0.05 0.58 FBL_HandleAck [33] [remaining functions contribute little to execution time] TABLE VIII Profiling results for FBL (no logging sites).
Reference: [23] <author> Laura S. Sabel and Keith Marzullo. </author> <title> Simulating fail-stop in asynchronous distributed systems. </title> <type> Technical Report TR94-1413, </type> <institution> Cornell University Department of Computer Science, </institution> <month> June </month> <year> 1994. </year> <booktitle> Versions appear in Proceedings of the 13th Annual Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1994, </year> <booktitle> and Proceedings of the 13th Symposium on Reliable Distributed Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: IV.C The Relationship between Failure Detection and Reactive Replication Replication occurs when a failure is detected. This section addresses the problem of failure detection in an asynchronous system. Background on the fail-stop model and a method due to Sabel and Marzullo <ref> [23] </ref> for implementing it in asynchronous systems is given in Appendix A. The semantics of the fail-stop model require that every process in the system eventually learns of a failure. <p> Thus a protocol that implements simulated fail-stop is likely to send messages in a pattern that is directly usable by a reactive failure detection protocol. In particular, a "simulated fail stop" (Sim-FS) protocol suggested by Sabel and Marzullo <ref> [23] </ref> requires that a process broadcast 36 a message when a suspected failure is detected. <p> The primary drawback of such a reactive technique is the need for a failure to be detected very soon after it occurs. Fail-stop failure detection can be implemented in an asynchronous environment <ref> [23] </ref>, but in practice the latency between failure and detection must be small. Since the simulated fail-stop failure detection protocol is based on communication timeout, the reactive method described here is only practical in asynchronous distributed systems when sufficiently small timeout values are used to implement failure detection. <p> IV.G A Simple One-Round Sim-FS Protocol for the Detection Phase This section describes a protocol that can be used for both general failure detection and as the failure detection phase of reactive replication. The following one-round protocol from Sabel and Marzullo <ref> [23] </ref> implements a version of asynchronous simulated fail stop (properties sFS2a-d, as described in Appendix A) that is indistinguishable from fail-stop. It is assumed that a failure suspection mechanism exists (e.g. timeout), and that no more than t failures are suspected in any run. <p> [60] 0.3 3.78 0.01 18 0.56 0.56 _sigfillset [63] 0.1 3.79 0.01 _mcount (443) 0.1 3.79 0.01 _moncontrol [69] 0.0 3.79 0.00 2173 0.00 0.00 _mutex_lock_stub [191] 0.0 3.79 0.00 2108 0.00 0.00 _mutex_unlock_stub [192] 0.0 3.79 0.00 1629 0.00 0.00 _sigemptyset [193] 0.0 3.79 0.00 1627 0.00 0.10 BlockSig <ref> [23] </ref> 0.0 3.79 0.00 1627 0.00 0.00 UnblockSig [85] 0.0 3.79 0.00 1627 0.00 0.00 _sigaddset [194] 0.0 3.79 0.00 1627 0.00 0.00 _waitid [195] 0.0 3.79 0.00 1087 0.00 0.02 malloc &lt;cycle 2&gt; [50] 0.0 3.79 0.00 794 0.00 0.01 free [58] [remaining functions contribute little to execution time] TABLE <p> self self total time seconds seconds calls ms/call ms/call name ============================================================================= 34.9 2.04 2.04 1203 1.70 1.70 _write [8] 25.6 3.54 1.50 1182 1.27 1.27 _read [11] 7.4 3.97 0.43 4666 0.09 0.09 _sigprocmask [18] 5.1 4.27 0.30 388 0.77 0.77 _poll [21] 5.1 4.57 0.30 11 27.27 27.27 _ioctl <ref> [23] </ref> 5.0 4.86 0.29 195 1.49 1.53 Update_Logged_Dets [24] 3.2 5.05 0.19 195 0.97 1.07 Log_New_Dets [27] 2.7 5.21 0.16 199 0.80 19.15 ReceiveMsg [6] 1.7 5.31 0.10 16 6.25 6.25 _close [37] 1.6 5.41 0.10 _mcount (481) 1.4 5.49 0.08 128 0.63 0.63 Compute [42] 1.0 5.55 0.06 79228 0.00 <p> Some additional application-provided information, such as an estimate of the frequency and duration of future intervals of nondeterminism, would be useful in choosing the best time at which to checkpoint. 91 APPENDIX A The Simulated Fail-Stop Protocol (Sim-FS) A.A Introduction This appendix summarizes the work of Sabel and Marzullo <ref> [23] </ref> relevant to simulating the fail-stop model in asynchronous systems. The Sim-FS protocol can be used with the reactive replication technique, as described in Section IV.G. This appendix is present to provide some background information on the Sim-FS protocol. <p> Specifically, when the quorum set is of fixed and equal size for each failure detection, the size of each quorum set must be strictly greater than n (t1) t . Furthermore, if that minimum quorum size is used in a one-round protocol, then n t 2 must hold <ref> [23] </ref>. 94 APPENDIX B Running the System This appendix contains some information about using the implementation in practice. B.A Operation A program that uses the message passing library must initialize itself as a master and start slaves by calling Exec.
Reference: [24] <author> R. D. Schlichting and F. B. Schneider. </author> <title> Fail-stop processors: An approach to designing fault-tolerant computing systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(3) </volume> <pages> 222-234, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: Multiple processes may reside on a single processor. Processes on the same processor communicate through message passing. The processes execute at arbitrary speeds relative to each other and do not have synchronized clocks. The processes fail according to the fail-stop model <ref> [24] </ref>. FIFO channels are assumed, but the order of receipt of messages at a single process from two other processes is random (that random order does not make a process itself nondeterministic, however). <p> ms/call name ============================================================================= 34.9 2.04 2.04 1203 1.70 1.70 _write [8] 25.6 3.54 1.50 1182 1.27 1.27 _read [11] 7.4 3.97 0.43 4666 0.09 0.09 _sigprocmask [18] 5.1 4.27 0.30 388 0.77 0.77 _poll [21] 5.1 4.57 0.30 11 27.27 27.27 _ioctl [23] 5.0 4.86 0.29 195 1.49 1.53 Update_Logged_Dets <ref> [24] </ref> 3.2 5.05 0.19 195 0.97 1.07 Log_New_Dets [27] 2.7 5.21 0.16 199 0.80 19.15 ReceiveMsg [6] 1.7 5.31 0.10 16 6.25 6.25 _close [37] 1.6 5.41 0.10 _mcount (481) 1.4 5.49 0.08 128 0.63 0.63 Compute [42] 1.0 5.55 0.06 79228 0.00 0.00 IntSet_Cardinality [47] 0.9 5.60 0.05 193 0.26 <p> ms/call name ============================================================================= 37.5 1.91 1.91 1203 1.59 1.59 _write [8] 19.6 2.91 1.00 1184 0.84 0.84 _read [12] 8.0 3.32 0.41 4670 0.09 0.09 _sigprocmask [17] 6.2 3.64 0.32 195 1.62 1.63 Update_Logged_Dets [19] 5.0 3.89 0.26 195 1.31 1.42 Log_New_Dets [20] 3.9 4.09 0.20 389 0.51 0.51 _poll <ref> [24] </ref> 3.9 4.29 0.20 11 18.18 18.18 _ioctl [25] 3.1 4.45 0.16 _mcount (479) 2.2 4.56 0.11 199 0.55 15.77 ReceiveMsg [6] 2.0 4.66 0.10 16 6.25 6.25 _close [36] 2.0 4.76 0.10 15 6.67 6.67 _open [37] 1.4 4.83 0.07 80700 0.00 0.00 IntSet_Cardinality [40] 1.2 4.89 0.06 128 0.47
Reference: [25] <author> Fred B. Schneider. </author> <title> Byzantine generals in action: Implementing fail-stop processors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(2) </volume> <pages> 145-154, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: 1.59 _write [8] 19.6 2.91 1.00 1184 0.84 0.84 _read [12] 8.0 3.32 0.41 4670 0.09 0.09 _sigprocmask [17] 6.2 3.64 0.32 195 1.62 1.63 Update_Logged_Dets [19] 5.0 3.89 0.26 195 1.31 1.42 Log_New_Dets [20] 3.9 4.09 0.20 389 0.51 0.51 _poll [24] 3.9 4.29 0.20 11 18.18 18.18 _ioctl <ref> [25] </ref> 3.1 4.45 0.16 _mcount (479) 2.2 4.56 0.11 199 0.55 15.77 ReceiveMsg [6] 2.0 4.66 0.10 16 6.25 6.25 _close [36] 2.0 4.76 0.10 15 6.67 6.67 _open [37] 1.4 4.83 0.07 80700 0.00 0.00 IntSet_Cardinality [40] 1.2 4.89 0.06 128 0.47 0.47 Compute [45] 1.0 4.94 0.05 193 0.26 <p> The Sim-FS protocol can be used with the reactive replication technique, as described in Section IV.G. This appendix is present to provide some background information on the Sim-FS protocol. The fail-stop model <ref> [25] </ref> requires that two conditions be satisfied in any run of the system: FS1 The failure of a process is eventually detected by all processes that do not crash FS2 No false failures are detected Sabel and Marzullo show that in an asynchronous system with crash failures, it is impossible to
Reference: [26] <author> A. P. Sistla and J. L. Welch. </author> <title> Efficient distributed recovery using message logging. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 223-238, </pages> <month> August </month> <year> 1989. </year> <month> 101 </month>
Reference-contexts: Optimistic message logging can be done either by the sender [28, 13, 10], the receiver <ref> [29, 14, 26, 16] </ref>, or both (the latter is generally not done). The Sender-based message logging protocol [13, 12] requires that each message be logged in the volatile storage of its sender, and the sender must wait until an acknowledgment has been received from the receiver before sending another message. <p> ms/call name ============================================================================= 36.9 1.40 1.40 812 1.72 1.72 _write [8] 26.4 2.40 1.00 796 1.26 1.26 _read [12] 9.0 2.74 0.34 3254 0.10 0.10 _sigprocmask [14] 7.9 3.04 0.30 388 0.77 0.77 _poll [19] 5.3 3.24 0.20 15 13.33 13.33 _open [22] 2.6 3.34 0.10 16 6.25 6.25 _close <ref> [26] </ref> 2.6 3.44 0.10 10 10.00 10.00 _ioctl [27] 2.6 3.54 0.10 6 16.67 16.67 _stat [29] 2.4 3.63 0.09 199 0.45 11.66 ReceiveMsg [6] 2.1 3.71 0.08 128 0.63 0.63 Compute [35] 0.5 3.73 0.02 786 0.03 0.03 realloc [48] 0.3 3.74 0.01 2112 0.00 0.00 __fabs [62] 0.3 3.75
Reference: [27] <author> Madalene Spezialetti and Phil Kearns. </author> <title> Efficient distributed snapshots. </title> <booktitle> In Proceedings of the 6th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 382-388, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: II.B Checkpointing in Distributed Systems Numerous checkpointing and recovery protocols have been developed for recovering a system to a consistent state. There are two distinct approaches: * Coordinated checkpointing <ref> [5, 27, 17, 18, 7] </ref> requires all processes to synchronize their checkpoints so that the state contained in the union of all the checkpoints (a global checkpoint) is a consistent global state. <p> 1.72 _write [8] 26.4 2.40 1.00 796 1.26 1.26 _read [12] 9.0 2.74 0.34 3254 0.10 0.10 _sigprocmask [14] 7.9 3.04 0.30 388 0.77 0.77 _poll [19] 5.3 3.24 0.20 15 13.33 13.33 _open [22] 2.6 3.34 0.10 16 6.25 6.25 _close [26] 2.6 3.44 0.10 10 10.00 10.00 _ioctl <ref> [27] </ref> 2.6 3.54 0.10 6 16.67 16.67 _stat [29] 2.4 3.63 0.09 199 0.45 11.66 ReceiveMsg [6] 2.1 3.71 0.08 128 0.63 0.63 Compute [35] 0.5 3.73 0.02 786 0.03 0.03 realloc [48] 0.3 3.74 0.01 2112 0.00 0.00 __fabs [62] 0.3 3.75 0.01 1417 0.01 0.01 _free_unlocked [52] 0.3 3.76 <p> 1.70 _write [8] 25.6 3.54 1.50 1182 1.27 1.27 _read [11] 7.4 3.97 0.43 4666 0.09 0.09 _sigprocmask [18] 5.1 4.27 0.30 388 0.77 0.77 _poll [21] 5.1 4.57 0.30 11 27.27 27.27 _ioctl [23] 5.0 4.86 0.29 195 1.49 1.53 Update_Logged_Dets [24] 3.2 5.05 0.19 195 0.97 1.07 Log_New_Dets <ref> [27] </ref> 2.7 5.21 0.16 199 0.80 19.15 ReceiveMsg [6] 1.7 5.31 0.10 16 6.25 6.25 _close [37] 1.6 5.41 0.10 _mcount (481) 1.4 5.49 0.08 128 0.63 0.63 Compute [42] 1.0 5.55 0.06 79228 0.00 0.00 IntSet_Cardinality [47] 0.9 5.60 0.05 193 0.26 0.32 FBL_HandleSend [46] 0.7 5.64 0.04 189 0.21
Reference: [28] <author> R. E. Strom, D. F. Bacon, and S. A. Yemini. </author> <title> Volatile logging in n-fault-tolerant distributed systems. </title> <booktitle> In Digest of Papers: The 18th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 44-49, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The benefit of optimism is an expected reduction in failure-free overhead because of the elimination of waiting for logging operations, at the expense of a more complicated and slower recovery protocol which may roll back non-failed processes. Optimistic message logging can be done either by the sender <ref> [28, 13, 10] </ref>, the receiver [29, 14, 26, 16], or both (the latter is generally not done). <p> When rollback occurs, the process is restored from its checkpoint, and the messages it had received after the checkpoint was taken are replayed to it by their senders. Strom, Bacon, and Yemini suggest an extension of sender-based logging to tolerate n simultaneous faults <ref> [28, 12] </ref>. The Manetho message logging protocol [9, 7] extends the idea of sender-based message logging by appending to each message an antecedence graph that represents all the events (including message receipt and nondeterministic events) that "happened before" the message. <p> 0.01 2 5.00 31.97 _ttyname_r [31] 0.3 3.07 0.01 _mcount (441) 0.0 3.07 0.00 2175 0.00 0.00 _mutex_lock_stub [184] 0.0 3.07 0.00 2112 0.00 0.00 __fabs [185] 0.0 3.07 0.00 2110 0.00 0.00 _mutex_unlock_stub [186] 0.0 3.07 0.00 1637 0.00 0.00 _sigemptyset [187] 0.0 3.07 0.00 1635 0.00 0.07 BlockSig <ref> [28] </ref> 0.0 3.07 0.00 1635 0.00 0.00 UnblockSig [76] 0.0 3.07 0.00 1635 0.00 0.00 _waitid [188] 0.0 3.07 0.00 1418 0.00 0.00 _free_unlocked [54] [remaining functions contribute little to execution time] TABLE VI Profiling results for a nonrecoverable run (no recovery protocols present). <p> 1.60 1186 1.35 1.35 _write [8] 18.0 2.40 0.80 1185 0.68 0.68 _read [13] 11.7 2.92 0.52 4654 0.11 0.11 _sigprocmask [15] 9.0 3.32 0.40 20 20.00 20.00 _open [16] 6.8 3.62 0.30 389 0.77 0.77 _poll [20] 4.1 3.80 0.18 _mcount (492) 3.2 3.94 0.14 128 1.09 1.09 Compute <ref> [28] </ref> 2.7 4.06 0.12 200 0.60 12.45 NewReceiveMsg [6] 1.4 4.12 0.06 60217 0.00 0.00 IntSet_Cardinality [35] 1.4 4.18 0.06 193 0.31 0.37 FBL_HandleSend [33] 0.9 4.22 0.04 2028 0.02 0.02 realloc [40] 0.7 4.25 0.03 189 0.16 0.58 FBL_HandleAck [31] 0.5 4.27 0.02 1162 0.02 1.59 writen [7] 0.5 4.29
Reference: [29] <author> R. E. Strom and S. A. Yemini. </author> <title> Optimistic recovery in distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Checkpoints older than the most recent can be discarded. The overhead of coordination and the nearly simultaneous occurrence of all checkpoints, which is likely to result in contention for the network and stable storage, are the primary drawbacks of coordinated checkpointing. * Independent checkpointing <ref> [3, 12, 14, 22, 29, 32, 33] </ref> allows the processes to take checkpoints independently, whenever they choose. <p> The log must contain any message that the system is to process, so pessimistic protocols generally use either an atomic deliver and log or send and log operation. Optimistic message logging techniques <ref> [29] </ref> are non-blocking because they log messages asynchronously, without waiting for the logging operation to complete before allowing the local application to continue. <p> Optimistic message logging can be done either by the sender [28, 13, 10], the receiver <ref> [29, 14, 26, 16] </ref>, or both (the latter is generally not done). The Sender-based message logging protocol [13, 12] requires that each message be logged in the volatile storage of its sender, and the sender must wait until an acknowledgment has been received from the receiver before sending another message. <p> III.D Maintaining Dependencies on Nondeterministic Processes The causal dependencies of a process (more precisely, the causal dependencies between state intervals of different processes) can be maintained by direct dependency tracking, which adds a single process number to each message, or transitive dependency tracking <ref> [29] </ref>, which adds a vector of length N to each message. Both methods track dependencies on all processes, with no distinction between deterministic and nondeterministic processes. <p> 1.26 _read [12] 9.0 2.74 0.34 3254 0.10 0.10 _sigprocmask [14] 7.9 3.04 0.30 388 0.77 0.77 _poll [19] 5.3 3.24 0.20 15 13.33 13.33 _open [22] 2.6 3.34 0.10 16 6.25 6.25 _close [26] 2.6 3.44 0.10 10 10.00 10.00 _ioctl [27] 2.6 3.54 0.10 6 16.67 16.67 _stat <ref> [29] </ref> 2.4 3.63 0.09 199 0.45 11.66 ReceiveMsg [6] 2.1 3.71 0.08 128 0.63 0.63 Compute [35] 0.5 3.73 0.02 786 0.03 0.03 realloc [48] 0.3 3.74 0.01 2112 0.00 0.00 __fabs [62] 0.3 3.75 0.01 1417 0.01 0.01 _free_unlocked [52] 0.3 3.76 0.01 262 0.04 3.89 SendMsg [11] 0.3 3.77 <p> 1.60 804 1.99 1.99 _write [7] 13.0 2.00 0.40 15 26.67 26.67 _open [11] 6.5 2.20 0.20 3270 0.06 0.06 _sigprocmask [18] 6.5 2.40 0.20 798 0.25 0.25 _read [19] 6.5 2.60 0.20 389 0.51 0.51 _poll [20] 5.2 2.76 0.16 199 0.80 7.74 ReceiveMsg [8] 3.3 2.86 0.10 _getmsg <ref> [29] </ref> 2.3 2.93 0.07 128 0.55 0.55 Compute [30] 0.7 2.95 0.02 782 0.03 2.14 writen [6] 0.7 2.97 0.02 781 0.03 0.40 readn [12] 0.7 2.99 0.02 331 0.06 0.06 memcpy [39] 0.3 3.00 0.01 1635 0.01 0.01 _sigaddset [49] 0.3 3.01 0.01 1088 0.01 0.02 malloc &lt;cycle 1&gt; [41]
Reference: [30] <author> Nitin H. Vaidya. </author> <title> Some thoughts on distributed recovery (preliminary version). </title> <type> Technical Report 94-044, </type> <institution> Texas A&M University Department of Computer Science, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: V.C.3.6 Logging Sites The logging site implementation takes advantage of an idea suggested independently by Alvisi and Marzullo [2] and Vaidya <ref> [30] </ref> to reduce message logging overhead when multiple processes share the same processor. <p> 0.40 15 26.67 26.67 _open [11] 6.5 2.20 0.20 3270 0.06 0.06 _sigprocmask [18] 6.5 2.40 0.20 798 0.25 0.25 _read [19] 6.5 2.60 0.20 389 0.51 0.51 _poll [20] 5.2 2.76 0.16 199 0.80 7.74 ReceiveMsg [8] 3.3 2.86 0.10 _getmsg [29] 2.3 2.93 0.07 128 0.55 0.55 Compute <ref> [30] </ref> 0.7 2.95 0.02 782 0.03 2.14 writen [6] 0.7 2.97 0.02 781 0.03 0.40 readn [12] 0.7 2.99 0.02 331 0.06 0.06 memcpy [39] 0.3 3.00 0.01 1635 0.01 0.01 _sigaddset [49] 0.3 3.01 0.01 1088 0.01 0.02 malloc &lt;cycle 1&gt; [41] 0.3 3.02 0.01 786 0.01 0.01 realloc [48] <p> The ease with which an application originally written for a distributed operating system was converted to use the message passing library shows that the implementation is reasonably complete and usable by application programmers. The recovery system implementation also includes the first known implementation of the logging site technique <ref> [2, 30] </ref> for maintaining message logs in shared memory. Performance measurements of the implementation confirm previous claims that the overhead of checkpointing can be low [8].
Reference: [31] <author> Nitin H. Vaidya. </author> <title> A case for two-level distributed recovery schemes. </title> <booktitle> In Proceedings of SIG-METRICS/Performance, </booktitle> <year> 1995. </year> <note> Also available by anonymous ftp to ftp.cs.tamu.edu, directory pub/vaidya. </note>
Reference-contexts: Clearly, the second protocol should be executed relatively infrequently so that a good tradeoff between failure-free overhead and rollback distance is obtained <ref> [31] </ref>. For example, global consistent checkpoints could be taken periodically (but rarely), and in the event of a failure that message logging cannot tolerate (i.e. failure of a crucial process), the system would be rolled back to the most recent global checkpoint. <p> [49] 0.3 3.01 0.01 1088 0.01 0.02 malloc &lt;cycle 1&gt; [41] 0.3 3.02 0.01 786 0.01 0.01 realloc [48] 0.3 3.03 0.01 425 0.02 0.04 search_pending_queue [44] 0.3 3.04 0.01 58 0.17 0.28 fread [43] 0.3 3.05 0.01 2 5.00 5.02 _endutxent [47] 0.3 3.06 0.01 2 5.00 31.97 _ttyname_r <ref> [31] </ref> 0.3 3.07 0.01 _mcount (441) 0.0 3.07 0.00 2175 0.00 0.00 _mutex_lock_stub [184] 0.0 3.07 0.00 2112 0.00 0.00 __fabs [185] 0.0 3.07 0.00 2110 0.00 0.00 _mutex_unlock_stub [186] 0.0 3.07 0.00 1637 0.00 0.00 _sigemptyset [187] 0.0 3.07 0.00 1635 0.00 0.07 BlockSig [28] 0.0 3.07 0.00 1635 0.00 <p> 0.18 _mcount (492) 3.2 3.94 0.14 128 1.09 1.09 Compute [28] 2.7 4.06 0.12 200 0.60 12.45 NewReceiveMsg [6] 1.4 4.12 0.06 60217 0.00 0.00 IntSet_Cardinality [35] 1.4 4.18 0.06 193 0.31 0.37 FBL_HandleSend [33] 0.9 4.22 0.04 2028 0.02 0.02 realloc [40] 0.7 4.25 0.03 189 0.16 0.58 FBL_HandleAck <ref> [31] </ref> 0.5 4.27 0.02 1162 0.02 1.59 writen [7] 0.5 4.29 0.02 195 0.10 0.18 Log_New_Dets [42] 0.5 4.31 0.02 195 0.10 0.11 Update_Logged_Dets [46] 0.2 4.32 0.01 36417 0.00 0.00 _thr_main_stub [63] 0.2 4.33 0.01 17968 0.00 0.00 _fflush_u [60] 0.2 4.34 0.01 2985 0.00 0.01 _free_unlocked [44] 0.2 4.35 <p> Performance measurements of the implementation confirm previous claims that the overhead of checkpointing can be low [8]. As has been observed by others, checkpointing performs well when checkpoints are written to local disks, but can perform abysmally when several large checkpoints are written to a shared disk simultaneously <ref> [31] </ref>. The overhead of the family-based message logging implementation is slightly greater than expected, but still reasonable. The logging site technique improves performance slightly when many processors share a processor, but actually increases failure-free run time slightly when few processors share a processor.
Reference: [32] <author> Y.-M. Wang and W. K. Fuchs. </author> <title> Optimistic message logging for independent checkpointing in message-passing systems. </title> <booktitle> In Proceedings of the 11th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 147-154, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Checkpoints older than the most recent can be discarded. The overhead of coordination and the nearly simultaneous occurrence of all checkpoints, which is likely to result in contention for the network and stable storage, are the primary drawbacks of coordinated checkpointing. * Independent checkpointing <ref> [3, 12, 14, 22, 29, 32, 33] </ref> allows the processes to take checkpoints independently, whenever they choose. <p> 0.16 199 0.80 19.15 ReceiveMsg [6] 1.7 5.31 0.10 16 6.25 6.25 _close [37] 1.6 5.41 0.10 _mcount (481) 1.4 5.49 0.08 128 0.63 0.63 Compute [42] 1.0 5.55 0.06 79228 0.00 0.00 IntSet_Cardinality [47] 0.9 5.60 0.05 193 0.26 0.32 FBL_HandleSend [46] 0.7 5.64 0.04 189 0.21 0.77 FBL_HandleAck <ref> [32] </ref> 0.5 5.67 0.03 36776 0.00 0.00 _thr_main_stub [56] 0.5 5.70 0.03 1165 0.03 1.49 readn [10] 0.2 5.71 0.01 18156 0.00 0.00 _fflush_u [69] 0.2 5.72 0.01 4872 0.00 0.00 IntSet_Add [70] 0.2 5.73 0.01 4101 0.00 0.00 IntSet_SetEmpty [71] 0.2 5.74 0.01 3890 0.00 0.00 GetIndexOfArrMax [72] 0.2 5.75
Reference: [33] <author> Jian Xu and Robert H. B. Netzer. </author> <title> Adaptive independent checkpointing for reducing rollback propagation. </title> <booktitle> In Proceedings of the 5th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> December </month> <year> 1993. </year> <note> Also available as Technical Report CS-93-25, </note> <institution> Brown University Department of Computer Science. </institution>
Reference-contexts: Checkpoints older than the most recent can be discarded. The overhead of coordination and the nearly simultaneous occurrence of all checkpoints, which is likely to result in contention for the network and stable storage, are the primary drawbacks of coordinated checkpointing. * Independent checkpointing <ref> [3, 12, 14, 22, 29, 32, 33] </ref> allows the processes to take checkpoints independently, whenever they choose. <p> &lt;cycle 2&gt; [58] 0.2 5.05 0.01 2195 0.00 0.00 realloc [68] 0.2 5.06 0.01 1945 0.01 0.01 GetNthLargestElement [72] 0.2 5.07 0.01 1168 0.01 1.78 writen [7] 0.2 5.08 0.01 389 0.03 0.54 _select [22] 0.2 5.09 0.01 195 0.05 0.05 Update_D [74] 0.2 5.10 0.01 190 0.05 0.58 FBL_HandleAck <ref> [33] </ref> [remaining functions contribute little to execution time] TABLE VIII Profiling results for FBL (no logging sites). <p> 0.40 20 20.00 20.00 _open [16] 6.8 3.62 0.30 389 0.77 0.77 _poll [20] 4.1 3.80 0.18 _mcount (492) 3.2 3.94 0.14 128 1.09 1.09 Compute [28] 2.7 4.06 0.12 200 0.60 12.45 NewReceiveMsg [6] 1.4 4.12 0.06 60217 0.00 0.00 IntSet_Cardinality [35] 1.4 4.18 0.06 193 0.31 0.37 FBL_HandleSend <ref> [33] </ref> 0.9 4.22 0.04 2028 0.02 0.02 realloc [40] 0.7 4.25 0.03 189 0.16 0.58 FBL_HandleAck [31] 0.5 4.27 0.02 1162 0.02 1.59 writen [7] 0.5 4.29 0.02 195 0.10 0.18 Log_New_Dets [42] 0.5 4.31 0.02 195 0.10 0.11 Update_Logged_Dets [46] 0.2 4.32 0.01 36417 0.00 0.00 _thr_main_stub [63] 0.2 4.33
References-found: 33

