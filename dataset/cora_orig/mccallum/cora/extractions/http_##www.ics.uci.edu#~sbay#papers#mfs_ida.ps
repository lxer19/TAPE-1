URL: http://www.ics.uci.edu/~sbay/papers/mfs_ida.ps
Refering-URL: http://www.ics.uci.edu/~sbay/
Root-URL: http://www.ics.uci.edu/~sbay/
Email: sbay@ics.uci.edu  
Title: Nearest Neighbor Classification from Multiple Feature Subsets  
Author: Stephen D. Bay 
Keyword: multiple models, combining classifiers, nearest neighbor, feature se lection, voting.  
Date: February 1, 1999  
Address: Irvine, CA 92697, USA  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: Combining multiple classifiers is an effective technique for improving accuracy. There are many general combining algorithms, such as Bagging, Boosting, or Error Correcting Output Coding, that significantly improve classifiers like decision trees, rule learners, or neural networks. Unfortunately, these combining methods do not improve the nearest neighbor classifier. In this paper, we present MFS, a combining algorithm designed to improve the accuracy of the nearest neighbor (NN) classifier. MFS combines multiple NN classifiers each using only a random subset of features. The experimental results are encouraging: On 25 datasets from the UCI Repository, MFS significantly outperformed several standard NN variants and was competitive with boosted decision trees. In additional experiments, we show that MFS is robust to irrelevant features, and is able to reduce both bias and variance components of error. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agresti, A., </author> <title> Categorical Data Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> 1990. </year>
Reference-contexts: Each point represents a pair of classifiers in the ensemble: the x coordinate is the value of the Kappa statistic (how much the classifiers agree) and the y coordinate is the average error rate of the pair. Formally, using Agresti's notation <ref> [1] </ref>, Kappa () is defined as follows: Let ij be the probability that the two classifiers choose class i and j respectively, then = 1 e where o = P ii is the observed agreement of the two classifiers, and e = P is the expected agreement if the classifiers are <p> The first seven datasets were used in the development of the MFS algorithm. In our experiments, we used 10-fold cross-validation without stratification. For the NN classifiers we scaled the continuous features in the training set of each fold to <ref> [0; 1] </ref> and then applied the same transformation to the test set. There were a few exceptions to this procedure: For Waveform, we used ten trials with a random sample of 300 training cases and 4700 test cases to maintain consistency with reported results [41].
Reference: [2] <author> Aha, D. W., and Bankert, R. L., </author> <title> Feature selection for case-based classification of cloud types: An empirical comparison, </title> <booktitle> In Proceedings of the AAAI-94 Workshop on Case-Based Reasoning, </booktitle> <pages> 106-112, </pages> <year> 1994. </year>
Reference-contexts: We compared these to six other algorithms: nearest neighbor (NN), k nearest neighbor 9 (kNN), nearest neighbor with forward (FSS) and backward (BSS) sequential selec-tion of features <ref> [2] </ref> and to a decision tree algorithm, C5.0, with (C5.0B) and without Boosting. All of the NN classifiers used unweighted Euclidean distance for continuous features and Hamming distance for symbolic features. Missing values were treated as informative [50] and considered to be a specific symbolic value.
Reference: [3] <author> Aha, D. W., and Bankert, R. L., </author> <title> Cloud classification using error-correcting output codes, </title> <booktitle> Artificial Intelligence Applications: Natural Resources, Agriculture, and Environmental Science 11, </booktitle> <volume> 1, </volume> <pages> 13-28, </pages> <year> 1997. </year>
Reference-contexts: Each time a pattern is presented for classification, we select a new random subset of features for each classifier. 1 There are variations on applying ECOC to NN classifiers which do decorrelate errors <ref> [3, 44] </ref>. We discuss these in Section 6. 5 As an example, consider Fisher's iris plant classification problem [24, 23]. <p> We are only aware of Skalak's [47, 48] work on combining NN classifiers with small prototype sets, Alpay-din's [6] work with condensed nearest neighbor (CNN) classifiers [29], and Aha and Bankert's <ref> [3] </ref> initial work on combining NN, feature selection, and ECOC which was later expanded on by Ricci and Aha [44]. Skalak and Alpaydin approach the problem of combining NN classifiers similarly. They drastically reduce the size of each classifier's prototype set to destabilize the NN classifier. <p> Accuracy and efficiency are usually conflicting goals for nearest neighbor classifiers, since accuracy improves by increasing the number of prototypes, while efficiency improves by decreasing the number of prototypes. Ricci and Aha [43, 42] (following <ref> [3] </ref>) applied ECOC to the NN classifier (NN-ECOC). Normally, applying ECOC to NN would not work as the errors in the two-class problems would be perfectly correlated; however, they found that applying feature selection to the two-class problems decorrelated errors if different features were 17 selected.
Reference: [4] <author> Aha, D. W., Kibler, D., and Albert, M. K., </author> <title> Instance-based learning algorithms, </title> <booktitle> Machine Learning 6 , 37-66, </booktitle> <year> 1991. </year>
Reference-contexts: Since its inception by Fix and Hodges [25], researchers have investigated many methods for improving the NN classifier, but most work has concentrated on changing the distance metric or manipulating the patterns in the training set <ref> [4, 19] </ref>. Recently, researchers have begun experimenting with general algorithms for improving classification accuracy by combining multiple versions of a single classifier, also known as a multiple model or ensemble approach. <p> Classification time measures the computational expense in classifying a single pattern. For the NN algorithm, we measure the distance between the test pattern and each of the e examples in the training set (which may be edited to reduce its size <ref> [4, 19] </ref>). Calculating the distance between two patterns is dependent on the number of features f . This gives a total complexity of O (ef ) for NN classification 3 . For the MFS algorithm, we call upon a NN classifier n times, where n is the number of classifiers.
Reference: [5] <author> Ali, K. M., and Pazzani, M. J., </author> <title> Error reduction through learning multiple descriptions, </title> <booktitle> Machine Learning 24 , 173-202, </booktitle> <year> 1996. </year>
Reference-contexts: Ali and Pazzani <ref> [5] </ref> verified these results by showing that uncorrelated errors led to improved ensemble performance. There are many methods for creating an ensemble with the above properties. We will examine two main families of ensemble generation algorithms: sampling and error correcting output codes.
Reference: [6] <author> Alpaydin, E., </author> <title> Voting over multiple condensed nearest neighbors, </title> <journal> Artificial Intelligence Review 11, </journal> <pages> 1-5, 115-132, </pages> <year> 1997. </year>
Reference-contexts: We are only aware of Skalak's [47, 48] work on combining NN classifiers with small prototype sets, Alpay-din's <ref> [6] </ref> work with condensed nearest neighbor (CNN) classifiers [29], and Aha and Bankert's [3] initial work on combining NN, feature selection, and ECOC which was later expanded on by Ricci and Aha [44]. Skalak and Alpaydin approach the problem of combining NN classifiers similarly.
Reference: [7] <author> Bay, S. D., </author> <title> Nearest neighbour classification from multiple data representations, </title> <type> Master's thesis, </type> <institution> University of Waterloo, Department of Systems Design Engineering, </institution> <year> 1997. </year>
Reference-contexts: Classification time for kNN, FSS, BSS are not shown here but were almost identical to the NN times <ref> [7] </ref>. 12 Table 3: Win-Loss-Tie statistics: Each cell lists the number of wins, losses, and ties between the row and column algorithm. Cells in bold indicate differences significant at the 95% confidence level using a two-tailed sign test. <p> We used the same basic procedure in Section 4.1. We added 10, 20, and 30 boolean irrelevant features to each of the datasets and then measured the accuracy of kNN and MFS1. Table 5 shows the results for five domains; additional results can be found in <ref> [7] </ref>. We did not directly compare FSS and BSS, because in initial test runs FSS performed similarly to the case where there are no irrelevant features, and BSS performed very badly being unable to overcome its poor starting position (i.e. many irrelevant features).
Reference: [8] <author> Blake, C., Keogh, E., and Merz, C. J., </author> <title> UCI repository of machine learning databases, </title> <year> 1998. </year>
Reference-contexts: C5.0 was run with the default parameter settings. For C5.0 with Boosting, we used 100 trials. We evaluated the algorithms on twenty-five datasets from the UCI Repository of Machine Learning Databases <ref> [8] </ref>. The datasets were chosen to include a wide range of domains and are summarized in Table 1. The first seven datasets were used in the development of the MFS algorithm. In our experiments, we used 10-fold cross-validation without stratification.
Reference: [9] <author> Breiman, L., </author> <title> Bagging predictors, </title> <booktitle> Machine Learning 24 , 123-140, </booktitle> <year> 1996. </year>
Reference-contexts: The outputs of several classifiers are combined in the hope that the accuracy of the whole is greater than the parts. Unfortunately, many combining methods do not improve the NN classifier at all. For example, neither Bagging nor Error Correcting Output Coding (ECOC) improves the NN classifier <ref> [9, 44] </ref>. In this paper, we present a new method of combining nearest neighbor classifiers with the goal of improving classification accuracy. Our approach uses random features for the individual classifiers. In contrast, other combining algorithms usually manipulate the training patterns (Bagging, Boosting) or the class labels (ECOC). <p> The implicit assumption is that using different training sets will decorrelate the errors enough to improve performance. Many researchers have experimented with sampling from the original training set to generate new datasets for the classifiers to learn on. Some sampling methods include: sampling with replacement <ref> [9] </ref>, sampling without replacement [33], adaptive sampling [26], and mutually exclusive partitioning [13]. Bagging [9], which uses sampling with replacement, is one of the best known methods for generating a set of classifiers. <p> Many researchers have experimented with sampling from the original training set to generate new datasets for the classifiers to learn on. Some sampling methods include: sampling with replacement <ref> [9] </ref>, sampling without replacement [33], adaptive sampling [26], and mutually exclusive partitioning [13]. Bagging [9], which uses sampling with replacement, is one of the best known methods for generating a set of classifiers. Analyzing Bagging provides some insights as to why sampling the training set is not effective with nearest neighbor classifiers. <p> Each of the n datasets has the same number of patterns as the original training set. This sampling method (bootstrap sampling) results in each pattern appearing in a given dataset with approximately 63.2% probability (see <ref> [9] </ref> for a detailed explanation). We then train a classifier on each dataset and combine their outputs using simple voting. Bagging has obtained impressive error reductions with decision trees such as CART [9] and C4.5 [41, 26] on a wide range of datasets. However, when Breiman [9] applied Bagging to the <p> sampling method (bootstrap sampling) results in each pattern appearing in a given dataset with approximately 63.2% probability (see <ref> [9] </ref> for a detailed explanation). We then train a classifier on each dataset and combine their outputs using simple voting. Bagging has obtained impressive error reductions with decision trees such as CART [9] and C4.5 [41, 26] on a wide range of datasets. However, when Breiman [9] applied Bagging to the nearest neighbor classifier he found no difference in performance between the bagged and non-bagged classifiers. <p> approximately 63.2% probability (see <ref> [9] </ref> for a detailed explanation). We then train a classifier on each dataset and combine their outputs using simple voting. Bagging has obtained impressive error reductions with decision trees such as CART [9] and C4.5 [41, 26] on a wide range of datasets. However, when Breiman [9] applied Bagging to the nearest neighbor classifier he found no difference in performance between the bagged and non-bagged classifiers. He makes the following comment on when we can expect Bagging to improve classification: The vital element is the instability of the prediction method. <p> Cherkauer [14] was more successful, and was able to combine neural networks that used different hand selected features to achieve human expert level performance in identifying volcanoes from images. Error correlation is related to Breiman's <ref> [9] </ref> concept of stability in classifiers: One method of generating a diverse ensemble of classifiers is to perturb some aspect of the training inputs for which the classifier is unstable. For example, Bagging [9] perturbs the training patterns available to each classifier in the ensemble. <p> Error correlation is related to Breiman's <ref> [9] </ref> concept of stability in classifiers: One method of generating a diverse ensemble of classifiers is to perturb some aspect of the training inputs for which the classifier is unstable. For example, Bagging [9] perturbs the training patterns available to each classifier in the ensemble. Since decision trees are unstable to the patterns, Bagging generates a diverse and effective ensemble. Nearest neighbor classifiers are stable to the patterns, so Bagging generates poor NN ensembles.
Reference: [10] <author> Breiman, L., </author> <title> Bias, variance, and arcing classifiers, </title> <type> Tech. Rep. 460, </type> <institution> Statistics Department, University of California, Berkeley, </institution> <year> 1996. </year>
Reference-contexts: It allows us to decompose the error into meaningful components and to see how the error components change with variations in the algorithm. Several researchers have used the bias-variance analysis error to show how multiple model approaches work. For example, both Breiman <ref> [10] </ref> and Schapire et al. [45] showed that Bagging improves performance by reducing the variance component of error. Kong and Dietterich [34] showed that ECOC could reduce both bias and variance. The bias variance decomposition of error originated in squared error for regression.
Reference: [11] <author> Breiman, L., </author> <title> Heuristics of instability and stabilizations in model selection, </title> <journal> The Annals of Statistics 24, </journal> <volume> 6, </volume> <pages> 2350-2383, </pages> <year> 1996. </year>
Reference-contexts: The bias variance decomposition of error originated in squared error for regression. For classification, 0-1 loss (misclassification rate) is commonly used, but this does not have a straightforward or unique decomposition. Recently, many authors have proposed similar decompositions <ref> [11, 31, 33, 34, 51] </ref>. We used Kong and Dietterich's definitions [34]. They define bias to be "the error of the ideal voted hypothesis," which is the result we would get from combining an infinite number of classifiers, each trained on an independent set of examples. <p> Also, the variance can be negative. This occurs when the algorithm is usually wrong, but sometimes predicts the correct class. We investigated the bias-variance components of error on three datasets originally used by Breiman <ref> [11] </ref> and later by Schapire et. al [45] to evaluate multiple model approaches. The datasets are two class problems, with the individual classes composed of 20-dimensional gaussians (see Appendix B for exact parameter values). We compared four classifiers: NN, kNN, MFS1 with 1 classifier (1-MFS1), and MFS1 with 100 classifiers.
Reference: [12] <author> Catlett, J., </author> <title> Megainduction: machine learning on very large databases, </title> <type> PhD thesis, </type> <institution> Faculty of Arts, University of Sydney, </institution> <year> 1991. </year>
Reference-contexts: Both these learning times assume that the features are discrete and can become much worse in continuous domains because of the need for repeated sorting operations <ref> [12] </ref>. 3.2.2 Space Complexity Nearest neighbor methods store a set of prototypes in memory. For methods without editing, this means that all the examples in the training set are stored taking up equivalent memory to the size of the data set. This is equivalent to O (ef ).
Reference: [13] <author> Chan, P., and Stolfo, S., </author> <title> A comparative evaluation of voting and meta-learning on partitioned data, </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 90-98, </pages> <year> 1995. </year> <month> 21 </month>
Reference-contexts: Many researchers have experimented with sampling from the original training set to generate new datasets for the classifiers to learn on. Some sampling methods include: sampling with replacement [9], sampling without replacement [33], adaptive sampling [26], and mutually exclusive partitioning <ref> [13] </ref>. Bagging [9], which uses sampling with replacement, is one of the best known methods for generating a set of classifiers. Analyzing Bagging provides some insights as to why sampling the training set is not effective with nearest neighbor classifiers.
Reference: [14] <author> Cherkauer, K. J., </author> <title> Human expert-level performance on a scientific image analysis task by a system using combined artificial neural networks, </title> <booktitle> In Working Notes of the AAAI Workshop on Integrating Multiple Learned Models, </booktitle> <editor> P. Chan, Ed., </editor> <month> 15-21, </month> <year> 1996. </year> <note> Available from http://www.cs.fit.edu/imlm. </note>
Reference-contexts: Unfortunately, the error rates in the individual classifiers increased, and as a result there was little or no improvement in the ensemble. Cherkauer <ref> [14] </ref> was more successful, and was able to combine neural networks that used different hand selected features to achieve human expert level performance in identifying volcanoes from images.
Reference: [15] <author> Clark, P., and Boswell, R., </author> <title> Rule induction with CN2: Some recent improvements, </title> <booktitle> In Machine Learning Proceedings of the Fifth European Conference, </booktitle> <pages> 151-163, </pages> <year> 1991. </year>
Reference-contexts: We have e total patterns and v values so the the total time is O (nef (ev)) = O (ne 2 f v) where v is the number of subset sizes we want to evaluate via cross-validation. This training complexity is comparable to other learning algorithms: For example, CN2 <ref> [16, 15] </ref>, a rule based system which use beam search requires in the worst case O (be 2 f 2 ) where b is the beam size [16].
Reference: [16] <author> Clark, P., and Niblett, T., </author> <title> The CN2 induction algorithm, </title> <booktitle> Machine Learning 3 , 261-283, </booktitle> <year> 1989. </year>
Reference-contexts: We have e total patterns and v values so the the total time is O (nef (ev)) = O (ne 2 f v) where v is the number of subset sizes we want to evaluate via cross-validation. This training complexity is comparable to other learning algorithms: For example, CN2 <ref> [16, 15] </ref>, a rule based system which use beam search requires in the worst case O (be 2 f 2 ) where b is the beam size [16]. <p> This training complexity is comparable to other learning algorithms: For example, CN2 [16, 15], a rule based system which use beam search requires in the worst case O (be 2 f 2 ) where b is the beam size <ref> [16] </ref>. ID3, a decision tree algorithm, requires time proportional to the product of the training set size, number of features, and the size of the induced tree [40].
Reference: [17] <author> Cohen, J., </author> <title> A coefficient of agreement for nominal scales, </title> <booktitle> Educational and Psychological Measurement 20, </booktitle> <volume> 1, </volume> <pages> 37-46, </pages> <year> 1960. </year>
Reference-contexts: MFS attempts to use this instability to generate a diverse set of NN classifiers with uncorrelated errors. As a final agrument for using random feature subsets to generate NN ensembles, we can use Kappa-Error diagrams <ref> [17, 36] </ref> to compare and contrast random selection of patterns and features. A Kappa-error diagram allows us to directly visualize the diversity and accuracy of an ensemble of classifiers.
Reference: [18] <author> Cover, T. M., and Hart, P. E., </author> <title> Nearest neighbor pattern classification, </title> <journal> IEEE Transactions on Information Theory 13, </journal> <volume> 1, </volume> <pages> 21-27, </pages> <year> 1967. </year>
Reference-contexts: Specifically, as the number of training examples approaches infinity the NN classifier is bounded by twice the Bayes error rate <ref> [18] </ref>. The kNN classifier is Bayes optimal with proper choice of k [25]. 3.1 Parameter Selection The MFS algorithm has two parameter values that need to be set: the size of the feature subsets, and the number of classifiers to combine.
Reference: [19] <author> Dasarathy, B. V., </author> <title> Nearest Neighbor, NN Norms: NN Pattern Classification Techniques, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA., </address> <year> 1991. </year>
Reference-contexts: Since its inception by Fix and Hodges [25], researchers have investigated many methods for improving the NN classifier, but most work has concentrated on changing the distance metric or manipulating the patterns in the training set <ref> [4, 19] </ref>. Recently, researchers have begun experimenting with general algorithms for improving classification accuracy by combining multiple versions of a single classifier, also known as a multiple model or ensemble approach. <p> Classification time measures the computational expense in classifying a single pattern. For the NN algorithm, we measure the distance between the test pattern and each of the e examples in the training set (which may be edited to reduce its size <ref> [4, 19] </ref>). Calculating the distance between two patterns is dependent on the number of features f . This gives a total complexity of O (ef ) for NN classification 3 . For the MFS algorithm, we call upon a NN classifier n times, where n is the number of classifiers.
Reference: [20] <author> Dietterich, T. G., </author> <title> Machine learning research: Four current directions, </title> <journal> AI Magazine 18, </journal> <volume> 4, </volume> <pages> 97-136, </pages> <year> 1997. </year>
Reference-contexts: For many problems this amount of memory may not be significant, but Dietterich <ref> [20] </ref> notes that on the Letter Recognition dataset (available from the UCI repository) an ensemble of 200 decision trees obtained 100% accuracy but required 59 megabytes of storage! The entire dataset was only 712 kilobytes. 4 Experiments Classification algorithms are complex programs that interact with data in many unpredictable ways.
Reference: [21] <author> Dietterich, T. G., </author> <title> An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization, </title> <note> submitted to Machine Learning, </note> <year> 1998. </year>
Reference-contexts: The main idea is that the classification algorithm should concentrate on the difficult instances. Boosting can generate more diverse ensembles of decision trees than Bagging does because of its ability to manipulate the input distributions <ref> [21] </ref>.
Reference: [22] <author> Dietterich, T. G., and Bakiri, G., </author> <title> Solving multiclass learning problems via error-correcting output codes, </title> <journal> Journal of Artificial Intelligence Research 2 , 263-286, </journal> <year> 1995. </year>
Reference-contexts: They did not report any accuracy improvements on the NN classifier using all of the training patterns. 2.2 Error Correcting Output Codes Error-correcting output coding (ECOC) proposed by Dietterich and Bakiri <ref> [22] </ref> is a method for creating and combining classifiers for multi-class problems by decomposition into multiple two class problems. In ECOC we assign a codeword or binary string for each class.
Reference: [23] <author> Duda, R. O., and Hart, P. E., </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: We discuss these in Section 6. 5 As an example, consider Fisher's iris plant classification problem <ref> [24, 23] </ref>. In this domain, we try to classify iris plants into their specific species: iris-setosa, iris-virginica, and iris-versicolor, based on the following four features: petal length, petal width, sepal length, and sepal width. With MFS we might use three NN classifiers each using a random subset of features.
Reference: [24] <author> Fisher, R. A., </author> <title> The use of multiple measurements in taxonomic problems, </title> <booktitle> Annual Eugenics 7 , 179-188, </booktitle> <year> 1936. </year>
Reference-contexts: We discuss these in Section 6. 5 As an example, consider Fisher's iris plant classification problem <ref> [24, 23] </ref>. In this domain, we try to classify iris plants into their specific species: iris-setosa, iris-virginica, and iris-versicolor, based on the following four features: petal length, petal width, sepal length, and sepal width. With MFS we might use three NN classifiers each using a random subset of features.
Reference: [25] <author> Fix, E., and Hodges, J. L., </author> <title> Discriminatory analysis: Nonparametric discrimination: Consistency properties, </title> <type> Tech. Rep. </type> <institution> Project 21-49-004, </institution> <type> Report Number 4, </type> <institution> USAF School of Aviation Medicine, Randolf Field, Texas, </institution> <year> 1951. </year>
Reference-contexts: For example, it can learn from a small set of examples, can incrementally add new information at runtime, and can give competitive performance with more modern methods such as decision trees or neural networks. Since its inception by Fix and Hodges <ref> [25] </ref>, researchers have investigated many methods for improving the NN classifier, but most work has concentrated on changing the distance metric or manipulating the patterns in the training set [4, 19]. <p> Specifically, as the number of training examples approaches infinity the NN classifier is bounded by twice the Bayes error rate [18]. The kNN classifier is Bayes optimal with proper choice of k <ref> [25] </ref>. 3.1 Parameter Selection The MFS algorithm has two parameter values that need to be set: the size of the feature subsets, and the number of classifiers to combine. We set the subset size parameter based on cross-validation accuracy estimates on the training set for the entire ensemble.
Reference: [26] <author> Freund, Y., and Schapire, R. E., </author> <title> Experiments with a new boosting algorithm, </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pp. 325-332, </pages> <year> 1996. </year>
Reference-contexts: Many researchers have experimented with sampling from the original training set to generate new datasets for the classifiers to learn on. Some sampling methods include: sampling with replacement [9], sampling without replacement [33], adaptive sampling <ref> [26] </ref>, and mutually exclusive partitioning [13]. Bagging [9], which uses sampling with replacement, is one of the best known methods for generating a set of classifiers. Analyzing Bagging provides some insights as to why sampling the training set is not effective with nearest neighbor classifiers. <p> We then train a classifier on each dataset and combine their outputs using simple voting. Bagging has obtained impressive error reductions with decision trees such as CART [9] and C4.5 <ref> [41, 26] </ref> on a wide range of datasets. However, when Breiman [9] applied Bagging to the nearest neighbor classifier he found no difference in performance between the bagged and non-bagged classifiers. <p> A popular alternative to Bagging is Boosting, which uses adaptive sampling of patterns to generate the ensemble. In Boosting <ref> [26] </ref>, the classifiers in the ensemble are trained serially, with the weights on the training instances set adaptively according to the performance of the previous classifiers. <p> Freund and Schapire <ref> [26] </ref> applied a modified version of Boosting to the NN classifier that worked around these problems by limiting each classifier to a small number of prototypes. However, their goal was not to improve accuracy, but to improve speed while maintaining current performance levels.
Reference: [27] <author> Friedman, J. H., Bentley, J. L., and Finkel, R. A., </author> <title> An algorithm for finding best matches in logarithmic expected time, </title> <journal> ACM Trans. Math. </journal> <volume> Software 3 , 209-226, </volume> <year> 1977. </year>
Reference-contexts: Researchers have investigated a number of other approaches to speed up NN classification for a given set of prototypes, including using other data structures <ref> [27, 49] </ref> and approximate approaches [37]. 8 In addition to classification time, we are also interested in learning or training time. For the MFS algorithm, the only parameter that needs to be learned is an appropriate subset size.
Reference: [28] <author> Hansen, L. K., and Salamon, P., </author> <title> Neural network ensembles, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 12 , 993-1001, </journal> <year> 1990. </year>
Reference-contexts: If the classifiers make identical errors, then these errors will propagate through voting to the whole ensemble. We require accuracy to ensure that in the ensemble vote, too many poor classifiers do not drown out the votes of correct classifiers. Hansen and Salamon <ref> [28] </ref> formulated these diversity and accuracy requirements precisely: They showed that under simple voting, if all models have the same probability of error, the probability of error is less than 50%, and the errors are independent, 2 then the overall error by voting will decrease monotonically with increasing number of models.
Reference: [29] <author> Hart, P. E., </author> <title> The condensed nearest neighbor rule, </title> <journal> IEEE Transactions on Information Theory 14 , 515-516, </journal> <year> 1968. </year>
Reference-contexts: We are only aware of Skalak's [47, 48] work on combining NN classifiers with small prototype sets, Alpay-din's [6] work with condensed nearest neighbor (CNN) classifiers <ref> [29] </ref>, and Aha and Bankert's [3] initial work on combining NN, feature selection, and ECOC which was later expanded on by Ricci and Aha [44]. Skalak and Alpaydin approach the problem of combining NN classifiers similarly.
Reference: [30] <author> Ho, T. K., </author> <title> The random subspace method for constructing decision forests, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 20, </journal> <volume> 8, </volume> <pages> 832-844, </pages> <year> 1998. </year>
Reference-contexts: Regardless of which method has better accuracy, MFS appears to have two main advantages over NN-ECOC: (1) MFS is the simpler algorithm, and (2) MFS is not constrained by ECOC to multiclass problems. As a final note, Ho <ref> [30] </ref> independently proposed using random subsets of features to create ensembles of decision trees.
Reference: [31] <author> James, G., and Hastie, T., </author> <title> Generalizations of the bias/variance decomposition for prediction error, </title> <note> http://stat.stanford.edu/gareth, 1997. </note>
Reference-contexts: The bias variance decomposition of error originated in squared error for regression. For classification, 0-1 loss (misclassification rate) is commonly used, but this does not have a straightforward or unique decomposition. Recently, many authors have proposed similar decompositions <ref> [11, 31, 33, 34, 51] </ref>. We used Kong and Dietterich's definitions [34]. They define bias to be "the error of the ideal voted hypothesis," which is the result we would get from combining an infinite number of classifiers, each trained on an independent set of examples.
Reference: [32] <author> Kohavi, R., and John, G. H., </author> <title> Wrappers for feature subset selection, </title> <booktitle> Artificial Intelligence 97, </booktitle> <pages> 1-2, 273-324, </pages> <year> 1996. </year>
Reference-contexts: For the kNN classifier, the value of k was set using cross-validation performance estimates on the training set. For feature selection, we used cross-validation accuracy on the training set for our objective function (also known as a wrapper approach <ref> [32] </ref>). C5.0 was run with the default parameter settings. For C5.0 with Boosting, we used 100 trials. We evaluated the algorithms on twenty-five datasets from the UCI Repository of Machine Learning Databases [8]. The datasets were chosen to include a wide range of domains and are summarized in Table 1.
Reference: [33] <author> Kohavi, R., and Wolpert, D. H., </author> <title> Bias plus variance decomposition for zero-one loss functions, </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <year> 1996. </year>
Reference-contexts: The implicit assumption is that using different training sets will decorrelate the errors enough to improve performance. Many researchers have experimented with sampling from the original training set to generate new datasets for the classifiers to learn on. Some sampling methods include: sampling with replacement [9], sampling without replacement <ref> [33] </ref>, adaptive sampling [26], and mutually exclusive partitioning [13]. Bagging [9], which uses sampling with replacement, is one of the best known methods for generating a set of classifiers. Analyzing Bagging provides some insights as to why sampling the training set is not effective with nearest neighbor classifiers. <p> The bias variance decomposition of error originated in squared error for regression. For classification, 0-1 loss (misclassification rate) is commonly used, but this does not have a straightforward or unique decomposition. Recently, many authors have proposed similar decompositions <ref> [11, 31, 33, 34, 51] </ref>. We used Kong and Dietterich's definitions [34]. They define bias to be "the error of the ideal voted hypothesis," which is the result we would get from combining an infinite number of classifiers, each trained on an independent set of examples.
Reference: [34] <author> Kong, E. B., and Dietterich, T. G., </author> <title> Error-correcting output coding corrects bias and variance, </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 725-730, </pages> <year> 1996. </year>
Reference-contexts: Dietterich and Bakiri showed that ECOC could improve decision trees and neural networks on several multi-class problems from the UCI repository. In an investigation of why ECOC improves performance for some classifiers, Kong and Dietterich <ref> [34] </ref> conclude that ECOC will not work with classifiers that use local information such as the kNN classifier. Wettschereck and Dietterich [53] verified that ECOC did not improve local learners (generalized radial basis function networks [39]), on the NETtalk problem [46]. <p> Several researchers have used the bias-variance analysis error to show how multiple model approaches work. For example, both Breiman [10] and Schapire et al. [45] showed that Bagging improves performance by reducing the variance component of error. Kong and Dietterich <ref> [34] </ref> showed that ECOC could reduce both bias and variance. The bias variance decomposition of error originated in squared error for regression. For classification, 0-1 loss (misclassification rate) is commonly used, but this does not have a straightforward or unique decomposition. <p> The bias variance decomposition of error originated in squared error for regression. For classification, 0-1 loss (misclassification rate) is commonly used, but this does not have a straightforward or unique decomposition. Recently, many authors have proposed similar decompositions <ref> [11, 31, 33, 34, 51] </ref>. We used Kong and Dietterich's definitions [34]. They define bias to be "the error of the ideal voted hypothesis," which is the result we would get from combining an infinite number of classifiers, each trained on an independent set of examples. <p> The bias variance decomposition of error originated in squared error for regression. For classification, 0-1 loss (misclassification rate) is commonly used, but this does not have a straightforward or unique decomposition. Recently, many authors have proposed similar decompositions [11, 31, 33, 34, 51]. We used Kong and Dietterich's definitions <ref> [34] </ref>. They define bias to be "the error of the ideal voted hypothesis," which is the result we would get from combining an infinite number of classifiers, each trained on an independent set of examples.
Reference: [35] <author> Langley, P., and Iba, W., </author> <title> Average-case analysis of a nearest neighbor algorithm, </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 889-894, </pages> <year> 1993. </year>
Reference-contexts: Since decision trees are unstable to the patterns, Bagging generates a diverse and effective ensemble. Nearest neighbor classifiers are stable to the patterns, so Bagging generates poor NN ensembles. Nearest Neighbor classifiers, however, are extremely sensitive to the features used. For example, Langley and Iba <ref> [35] </ref> found that adding just a few irrelevant features could drastically change the NN classifier's outputs (and reduce accuracy). MFS attempts to use this instability to generate a diverse set of NN classifiers with uncorrelated errors.
Reference: [36] <author> Margineantu, D. D., and Dietterich, T. G., </author> <title> Pruning adaptive boosting, </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning, </booktitle> <year> 1997. </year>
Reference-contexts: MFS attempts to use this instability to generate a diverse set of NN classifiers with uncorrelated errors. As a final agrument for using random feature subsets to generate NN ensembles, we can use Kappa-Error diagrams <ref> [17, 36] </ref> to compare and contrast random selection of patterns and features. A Kappa-error diagram allows us to directly visualize the diversity and accuracy of an ensemble of classifiers.
Reference: [37] <author> Miclet, L., and Dabouz, M., </author> <title> Approximative fast nearest-neighbour recognition, </title> <journal> Pattern Recognition Letters 1 , 277-285, </journal> <year> 1983. </year>
Reference-contexts: Researchers have investigated a number of other approaches to speed up NN classification for a given set of prototypes, including using other data structures [27, 49] and approximate approaches <ref> [37] </ref>. 8 In addition to classification time, we are also interested in learning or training time. For the MFS algorithm, the only parameter that needs to be learned is an appropriate subset size.
Reference: [38] <author> Moore, A. W., </author> <title> Efficient Memory-based Learning for Robot Control, </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1991. </year>
Reference-contexts: Another approach is to take advantage of the reduced dimensionality of the feature set used for the individual NN classifiers by using structures like KD-trees. KD-trees offer fast O (log n) lookup for the nearest neighbor, but only for low dimensional (say jF j &lt; 10) datasets <ref> [38] </ref>. Our results indicate that in many domains we can obtain accurate results by combining NN classifiers using very few features (on average MFS1 used 10.6 features, MFS2 used 8.3).
Reference: [39] <author> Poggio, T., and Girosi, F., </author> <title> A theory of networks for approximation and learning, </title> <type> Tech. Rep. AI Memo 1164, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <address> Cambridge, MA., </address> <year> 1989. </year>
Reference-contexts: In an investigation of why ECOC improves performance for some classifiers, Kong and Dietterich [34] conclude that ECOC will not work with classifiers that use local information such as the kNN classifier. Wettschereck and Dietterich [53] verified that ECOC did not improve local learners (generalized radial basis function networks <ref> [39] </ref>), on the NETtalk problem [46]. ECOC fails to work for NN classifiers because the errors are correlated across the binary learning problems 1 . For example, with the NN classifier we predict the class of the closest pattern.
Reference: [40] <author> Quinlan, J. R., </author> <title> Induction of decision trees, </title> <booktitle> Machine Learning 1 , 81-106, </booktitle> <year> 1986. </year>
Reference-contexts: ID3, a decision tree algorithm, requires time proportional to the product of the training set size, number of features, and the size of the induced tree <ref> [40] </ref>. Both these learning times assume that the features are discrete and can become much worse in continuous domains because of the need for repeated sorting operations [12]. 3.2.2 Space Complexity Nearest neighbor methods store a set of prototypes in memory.
Reference: [41] <author> Quinlan, J. R., Bagging, </author> <title> Boosting, </title> <booktitle> and C4.5, In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 725-730, </pages> <year> 1996. </year>
Reference-contexts: We then train a classifier on each dataset and combine their outputs using simple voting. Bagging has obtained impressive error reductions with decision trees such as CART [9] and C4.5 <ref> [41, 26] </ref> on a wide range of datasets. However, when Breiman [9] applied Bagging to the nearest neighbor classifier he found no difference in performance between the bagged and non-bagged classifiers. <p> There were a few exceptions to this procedure: For Waveform, we used ten trials with a random sample of 300 training cases and 4700 test cases to maintain consistency with reported results <ref> [41] </ref>. For Satimage and Vowel, we used the original division into a training and test set, so the results represent one run of each algorithm. 4.2 Accuracy The error rates of the classifiers are shown in Table 2.
Reference: [42] <author> Ricci, F., and Aha, D. W., </author> <title> Bias, variance, and error correcting output codes for local learners, </title> <type> Tech. Rep. </type> <institution> AIC-97-025, Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence, 1997. 23 </note>
Reference-contexts: Accuracy and efficiency are usually conflicting goals for nearest neighbor classifiers, since accuracy improves by increasing the number of prototypes, while efficiency improves by decreasing the number of prototypes. Ricci and Aha <ref> [43, 42] </ref> (following [3]) applied ECOC to the NN classifier (NN-ECOC). Normally, applying ECOC to NN would not work as the errors in the two-class problems would be perfectly correlated; however, they found that applying feature selection to the two-class problems decorrelated errors if different features were 17 selected.
Reference: [43] <author> Ricci, F., and Aha, D. W., </author> <title> Extending local learners with error-correcting output codes, </title> <type> Tech. Rep. </type> <note> AIC-97-001, Navy Center for Applied Research in Artificial Intelligence, </note> <year> 1997. </year>
Reference-contexts: Accuracy and efficiency are usually conflicting goals for nearest neighbor classifiers, since accuracy improves by increasing the number of prototypes, while efficiency improves by decreasing the number of prototypes. Ricci and Aha <ref> [43, 42] </ref> (following [3]) applied ECOC to the NN classifier (NN-ECOC). Normally, applying ECOC to NN would not work as the errors in the two-class problems would be perfectly correlated; however, they found that applying feature selection to the two-class problems decorrelated errors if different features were 17 selected.
Reference: [44] <author> Ricci, F., and Aha, D. W., </author> <title> Error-correcting output codes for local learners, </title> <booktitle> In Proceedings of the 10th European Conference on Machine Learning, </booktitle> <year> 1998. </year>
Reference-contexts: The outputs of several classifiers are combined in the hope that the accuracy of the whole is greater than the parts. Unfortunately, many combining methods do not improve the NN classifier at all. For example, neither Bagging nor Error Correcting Output Coding (ECOC) improves the NN classifier <ref> [9, 44] </ref>. In this paper, we present a new method of combining nearest neighbor classifiers with the goal of improving classification accuracy. Our approach uses random features for the individual classifiers. In contrast, other combining algorithms usually manipulate the training patterns (Bagging, Boosting) or the class labels (ECOC). <p> Each time a pattern is presented for classification, we select a new random subset of features for each classifier. 1 There are variations on applying ECOC to NN classifiers which do decorrelate errors <ref> [3, 44] </ref>. We discuss these in Section 6. 5 As an example, consider Fisher's iris plant classification problem [24, 23]. <p> We are only aware of Skalak's [47, 48] work on combining NN classifiers with small prototype sets, Alpay-din's [6] work with condensed nearest neighbor (CNN) classifiers [29], and Aha and Bankert's [3] initial work on combining NN, feature selection, and ECOC which was later expanded on by Ricci and Aha <ref> [44] </ref>. Skalak and Alpaydin approach the problem of combining NN classifiers similarly. They drastically reduce the size of each classifier's prototype set to destabilize the NN classifier.
Reference: [45] <author> Schapire, R. E., Freund, Y., Bartlett, P., and Lee, W. S., </author> <title> Boosting the margin: A new explanation for the effectiveness of voting methods, </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <year> 1997. </year>
Reference-contexts: It allows us to decompose the error into meaningful components and to see how the error components change with variations in the algorithm. Several researchers have used the bias-variance analysis error to show how multiple model approaches work. For example, both Breiman [10] and Schapire et al. <ref> [45] </ref> showed that Bagging improves performance by reducing the variance component of error. Kong and Dietterich [34] showed that ECOC could reduce both bias and variance. The bias variance decomposition of error originated in squared error for regression. <p> Also, the variance can be negative. This occurs when the algorithm is usually wrong, but sometimes predicts the correct class. We investigated the bias-variance components of error on three datasets originally used by Breiman [11] and later by Schapire et. al <ref> [45] </ref> to evaluate multiple model approaches. The datasets are two class problems, with the individual classes composed of 20-dimensional gaussians (see Appendix B for exact parameter values). We compared four classifiers: NN, kNN, MFS1 with 1 classifier (1-MFS1), and MFS1 with 100 classifiers.
Reference: [46] <author> Sejnowski, T. J., and Rosenberg, C. R., </author> <title> Parallel networks that learn to pronounce English text, </title> <journal> Complex Systems 1 , 145-168, </journal> <year> 1987. </year>
Reference-contexts: Wettschereck and Dietterich [53] verified that ECOC did not improve local learners (generalized radial basis function networks [39]), on the NETtalk problem <ref> [46] </ref>. ECOC fails to work for NN classifiers because the errors are correlated across the binary learning problems 1 . For example, with the NN classifier we predict the class of the closest pattern.
Reference: [47] <author> Skalak, D. B., </author> <title> Prototype Selection for Composite Nearest Neighbor Classifiers, </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <year> 1996. </year>
Reference-contexts: We are only aware of Skalak's <ref> [47, 48] </ref> work on combining NN classifiers with small prototype sets, Alpay-din's [6] work with condensed nearest neighbor (CNN) classifiers [29], and Aha and Bankert's [3] initial work on combining NN, feature selection, and ECOC which was later expanded on by Ricci and Aha [44].
Reference: [48] <author> Skalak, D. B., </author> <title> The sources of increased accuracy for two proposed boosting algorithms, </title> <booktitle> In AAAI '96 Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms, </booktitle> <year> 1996. </year>
Reference-contexts: We are only aware of Skalak's <ref> [47, 48] </ref> work on combining NN classifiers with small prototype sets, Alpay-din's [6] work with condensed nearest neighbor (CNN) classifiers [29], and Aha and Bankert's [3] initial work on combining NN, feature selection, and ECOC which was later expanded on by Ricci and Aha [44].
Reference: [49] <author> Sproull, R. F., </author> <title> Refinements to nearest-neighbour searching in k-dimensional trees, </title> <type> Algorithmica 6 , 579-589, </type> <year> 1991. </year>
Reference-contexts: Researchers have investigated a number of other approaches to speed up NN classification for a given set of prototypes, including using other data structures <ref> [27, 49] </ref> and approximate approaches [37]. 8 In addition to classification time, we are also interested in learning or training time. For the MFS algorithm, the only parameter that needs to be learned is an appropriate subset size.
Reference: [50] <author> Tabachnick, B. G., and Fidell, L. S., </author> <title> Using Multivariate Statistics, Second Edition, </title> <publisher> Harper & Row, </publisher> <year> 1989. </year>
Reference-contexts: All of the NN classifiers used unweighted Euclidean distance for continuous features and Hamming distance for symbolic features. Missing values were treated as informative <ref> [50] </ref> and considered to be a specific symbolic value. In the case of continuous features, a missing value is considered to have a distance of 1 to all non-missing values. For the kNN classifier, the value of k was set using cross-validation performance estimates on the training set.
Reference: [51] <author> Tibshirani, R., </author> <title> Bias, variance and prediction error for classification rules, </title> <type> Tech. rep., </type> <institution> Department of Statistics, University of Toronto, </institution> <year> 1996. </year>
Reference-contexts: The bias variance decomposition of error originated in squared error for regression. For classification, 0-1 loss (misclassification rate) is commonly used, but this does not have a straightforward or unique decomposition. Recently, many authors have proposed similar decompositions <ref> [11, 31, 33, 34, 51] </ref>. We used Kong and Dietterich's definitions [34]. They define bias to be "the error of the ideal voted hypothesis," which is the result we would get from combining an infinite number of classifiers, each trained on an independent set of examples.
Reference: [52] <author> Tumer, K., and Ghosh, J., </author> <title> Error correlation and error reduction in ensemble classifiers, </title> <booktitle> Connection Science 8 , 385-404, </booktitle> <year> 1996. </year>
Reference-contexts: Selecting different feature subsets is an attempt to force the NN classifiers to make different and hopefully uncorrelated errors. Although there is no guarantee that using different feature sets will decorrelate error, Tumer and Ghosh <ref> [52] </ref> found that with neural networks, selectively removing features could decorrelate errors. Unfortunately, the error rates in the individual classifiers increased, and as a result there was little or no improvement in the ensemble.
Reference: [53] <author> Wettschereck, D., and Dietterich, T. G., </author> <title> Improving the performance of radial basis function networks by learning center locations, </title> <booktitle> In Nerual Information Processing Systems, </booktitle> <editor> J. Moody, S. Hanson, and R. Lippmann, Eds., </editor> <volume> vol. 4. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year> <month> 24 </month>
Reference-contexts: In an investigation of why ECOC improves performance for some classifiers, Kong and Dietterich [34] conclude that ECOC will not work with classifiers that use local information such as the kNN classifier. Wettschereck and Dietterich <ref> [53] </ref> verified that ECOC did not improve local learners (generalized radial basis function networks [39]), on the NETtalk problem [46]. ECOC fails to work for NN classifiers because the errors are correlated across the binary learning problems 1 .
References-found: 53

