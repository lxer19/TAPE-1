URL: ftp://ftp.cl.cam.ac.uk/users/bdp/metagame.ps.Z
Refering-URL: http://forum.swarthmore.edu/~jay/learn-game/projects/metagame.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: bdp@cl.cam.ac.uk  
Title: A New Challenge for Games and Learning  
Author: Barney Pell 
Address: Cambridge, UK  
Affiliation: University of Cambridge  
Note: METAGAME:  
Abstract: An earlier version of this paper appears in: H.J. van den Herik and L.V. Allis, editors, Heuristic Programming in Artificial Intelligence 3 The Third Computer Olympiad. Ellis Horwood, 1992. Abstract In most current approaches to Computer Game-Playing, including those employing some form of machine learning, the game analysis mainly is performed by humans. Thus, we are sidestepping largely the interesting (and difficult) questions. Human analysis also makes it difficult to evaluate the generality and applicability of different approaches. To address these problems, we introduce a new challenge: Metagame. The idea is to write programs which take as input the rules of a set of new games within a pre-specified class, generated by a program which is publicly available. The programs compete against each other in many matches on each new game, and they can then be evaluated based on their overall performance and improvement through experience. This paper discusses the goals, research areas, and general concerns for the idea of Metagame. 
Abstract-found: 1
Intro-found: 1
Reference: [AvdHH91] <author> L.V. Allis, H.J. van den Herik, </author> <title> and I.S. Herschberg. Which games will survive. </title> <editor> In D.N.L. Levy and D.F. Beal, editors, </editor> <booktitle> Heuristic Programming in Artificial Intelligence 2 The Second Computer Olympiad. </booktitle> <publisher> Ellis Horwood, </publisher> <year> 1991. </year>
Reference: [AvdMvdH91] <editor> L.V. Allis, M. van der Meulen, and H.J. van den Herik. Databases in awari. In D.N.L. Levy and D.F. Beal, editors, </editor> <booktitle> Heuristic Programming in Artificial Intelligence 2 The Second Computer Olympiad. </booktitle> <publisher> Ellis Horwood, </publisher> <year> 1991. </year>
Reference: [BCG82] <author> E.R. Berlekamp, J.H. Conway, and R.K. Guy. </author> <title> Winning Ways for your mathematical plays. </title> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference: [CBKF91] <author> Gregg Collins, Lawrence Birnbaum, Bruce Krulwich, and Michael Freed. </author> <title> Plan debugging in an intentional system. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: difficult to evaluate, and even more difficult to use for learning in different games, because the games, representations, learning methods, and amount of knowledge en gineering vary with each researcher. 4 In addition, most learning methods are designed to be improved based on watching or playing against knowledgeable opponents (e.g., <ref> [Tad89, Eps91, CBKF91, LM88, LS91, CFR91] </ref>). Although it is certainly important to understand how a program (or person) could learn from good players, it is equally important to know how those good players became good in the first place.
Reference: [CFR91] <author> James P. Callan, Tom E. Fawcett, and Edwina L. Rissland. </author> <title> Adaptive case-based reasoning. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: difficult to evaluate, and even more difficult to use for learning in different games, because the games, representations, learning methods, and amount of knowledge en gineering vary with each researcher. 4 In addition, most learning methods are designed to be improved based on watching or playing against knowledgeable opponents (e.g., <ref> [Tad89, Eps91, CBKF91, LM88, LS91, CFR91] </ref>). Although it is certainly important to understand how a program (or person) could learn from good players, it is equally important to know how those good players became good in the first place.
Reference: [Don92] <author> Ch. Donninger. </author> <title> The relation of mobility, strategy and the mean dead rabbit in chess. </title> <editor> In H.J. van den Herik and L.V. Allis, editors, </editor> <booktitle> Heuristic Programming in Artificial Intelligence 3 The Third Computer Olympiad. </booktitle> <publisher> Ellis Horwood, </publisher> <year> 1992. </year>
Reference: [Eps91] <author> Susan Epstein. </author> <title> Deep Forks In Strategic Maps. </title> <editor> In D.N.L. Levy and D.F. Beal, editors, </editor> <booktitle> Heuristic Programming in Artificial Intelligence 2 The Second Computer Olympiad. </booktitle> <publisher> Ellis Horwood, </publisher> <year> 1991. </year>
Reference-contexts: difficult to evaluate, and even more difficult to use for learning in different games, because the games, representations, learning methods, and amount of knowledge en gineering vary with each researcher. 4 In addition, most learning methods are designed to be improved based on watching or playing against knowledgeable opponents (e.g., <ref> [Tad89, Eps91, CBKF91, LM88, LS91, CFR91] </ref>). Although it is certainly important to understand how a program (or person) could learn from good players, it is equally important to know how those good players became good in the first place.
Reference: [FD89] <author> Nicholas S. Flann and Thomas G. Dietterich. </author> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 187-226, </pages> <year> 1989. </year>
Reference: [Har87] <author> D. Hartmann. </author> <title> How to Extract Relevant Knowledge from Grand Master Games, part 1. </title> <journal> ICCA-Journal, </journal> <volume> 10(1), </volume> <month> March </month> <year> 1987. </year>
Reference: [Kof68] <author> Elliot B. Koffman. </author> <title> Learning games through pattern recognition. </title> <journal> IEEE Transactions on Systems Science and Cybernetics, </journal> <volume> SSC-4(1):12-16, </volume> <year> 1968. </year>
Reference: [Len83] <author> Douglas B. Lenat. </author> <title> The role of heuristics in learning by discovery: Three case studies. In R.S. </title> <editor> Michalski, J.G. Carbonnel, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> volume 1. Morgan-Kaufman, </volume> <year> 1983. </year>
Reference: [LHM + 91] <author> Robert Levinson, Feng-Hsiung Hsu, T. Anthony Marsland, Jonathan Schaeffer, and David E. Wilkins. </author> <title> Panel: </title> <booktitle> The role of chess in artificial intelligence research. In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference: [LM88] <author> Kai-Fu Lee and Sanjoy Mahajan. </author> <title> A pattern classification approach to evaluation function learning. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 1-25, </pages> <year> 1988. </year>
Reference-contexts: difficult to evaluate, and even more difficult to use for learning in different games, because the games, representations, learning methods, and amount of knowledge en gineering vary with each researcher. 4 In addition, most learning methods are designed to be improved based on watching or playing against knowledgeable opponents (e.g., <ref> [Tad89, Eps91, CBKF91, LM88, LS91, CFR91] </ref>). Although it is certainly important to understand how a program (or person) could learn from good players, it is equally important to know how those good players became good in the first place.
Reference: [LS91] <author> Robert A. Levinson and R. Snyder. </author> <title> Adaptive, pattern-oriented chess. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: difficult to evaluate, and even more difficult to use for learning in different games, because the games, representations, learning methods, and amount of knowledge en gineering vary with each researcher. 4 In addition, most learning methods are designed to be improved based on watching or playing against knowledgeable opponents (e.g., <ref> [Tad89, Eps91, CBKF91, LM88, LS91, CFR91] </ref>). Although it is certainly important to understand how a program (or person) could learn from good players, it is equally important to know how those good players became good in the first place.
Reference: [Pel91] <author> Barney Pell. </author> <title> Exploratory Learning in the Game of GO: Initial Results. </title> <editor> In D.N.L. Levy and D.F. Beal, editors, </editor> <booktitle> Heuristic Programming in Artificial Intelligence 2 The Second Computer Olympiad. </booktitle> <publisher> Ellis Horwood, </publisher> <year> 1991. </year> <note> Also appears as University of Cambridge Computer Laboratory Technical Report No. 275. </note>
Reference: [Pel92] <author> Barney Pell. </author> <title> Metagame in Symmetric, Chess-Like Games. </title> <editor> In H.J. van den Herik and L.V. Allis, editors, </editor> <booktitle> Heuristic Programming in Artificial Intelligence 3 The Third Computer Olympiad. </booktitle> <publisher> Ellis Horwood, </publisher> <year> 1992. </year> <note> Also appears as University of Cambridge Computer Laboratory Technical Report No. 277. </note>
Reference: [Roy90] <author> A. J. Roycroft. </author> <title> Expert Against Oracle. </title> <editor> In D. Michie, editor, </editor> <booktitle> Machine Intelligence 11, </booktitle> <pages> pages 347-373. </pages> <publisher> Ellis Horwood, </publisher> <address> Chich-ester, </address> <year> 1990. </year>
Reference: [Tad89] <author> Prasad Tadepalli. </author> <title> Lazy explanation-based learning: A solution to the intractable theory problem. </title> <booktitle> In Proceedings of the 11th 14 International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 694-700, </pages> <year> 1989. </year>
Reference-contexts: difficult to evaluate, and even more difficult to use for learning in different games, because the games, representations, learning methods, and amount of knowledge en gineering vary with each researcher. 4 In addition, most learning methods are designed to be improved based on watching or playing against knowledgeable opponents (e.g., <ref> [Tad89, Eps91, CBKF91, LM88, LS91, CFR91] </ref>). Although it is certainly important to understand how a program (or person) could learn from good players, it is equally important to know how those good players became good in the first place.
Reference: [Wil82] <author> David E. Wilkins. </author> <title> Using knowledge to control tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 1-51, </pages> <year> 1982. </year> <month> 15 </month>
References-found: 19

