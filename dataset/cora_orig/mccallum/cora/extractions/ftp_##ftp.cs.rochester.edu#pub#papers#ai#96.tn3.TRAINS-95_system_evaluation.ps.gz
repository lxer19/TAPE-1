URL: ftp://ftp.cs.rochester.edu/pub/papers/ai/96.tn3.TRAINS-95_system_evaluation.ps.gz
Refering-URL: http://www.cs.rochester.edu/trs/ai-trs.html
Root-URL: 
Title: TRAINS-95 System Evaluation evaluation explores the robustness of the TRAINS-95 system in the presence of
Author: Teresa Sikorski James F. Allen 
Note: The  This work was supported in part by ONR/ARPA grants N00014-92-J-1512 and N00014-95-1-1088, and NSF grant IRI-9503312.  
Date: July 1996  
Address: Rochester, New York 14627  
Affiliation: The University of Rochester Computer Science Department  
Pubnum: TRAINS Technical Note 96-3  
Abstract: In this paper we describe a recent experiment designed to evaluate the performance of the TRAINS-95 system. The evaluation uses a task-based evaluation methodology appropriate for dialogue systems such as TRAINS-95, where a human and a computer interact and collaborate to solve a given problem. In task-based evaluations, techniques are measured in terms of their affect on task performance measures such as how long it takes to develop a solution using the system, and the quality of the final plan produced. 
Abstract-found: 1
Intro-found: 1
Reference: [Allen et al., 1996] <author> James F. Allen, Brad Miller, Eric Ringger, and Teresa Sikorski, </author> <title> "Robust Understanding in a Dialogue System," </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction TRAINS-95 is the first end-to-end implementation in a long-term effort to develop an intelligent planning assistant that is conversationally proficient in natural language. The initial domain is a train route planner, where a human manager and the system must cooperate to develop and execute plans <ref> [Allen et al., 1996; Ferguson et al., 1996] </ref>. TRAINS-95 provides a real-time multi-modal interface between the human and computer.
Reference: [Boros et al., 1996] <author> M. Boros, W. Eckert, F. Gallwitz, G. Gorz, G. Hanrieder, and H. Nie-mann, </author> <title> "Towards Understanding Spontaneous Speech: Word Accuracy Vs. Concept Accuracy," </title> <type> Technical report, </type> <institution> Universitat Erlangen-Nurnberg, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Recent research indicates a linear relationship between word recognition accuracy and the understanding of utterances <ref> [Boros et al., 1996] </ref>.
Reference: [Cohen and Oviatt, 1994] <author> P. Cohen and S. Oviatt, </author> <title> "The Role of Voice Input for Human-Machine Communication," </title> <booktitle> In Proceedings of the National Academy of Sciences, </booktitle> <year> 1994. </year>
Reference-contexts: The experiment results are consistent with our expectations that as speech recognition technology improves and robust techniques for dialogue systems are developed, human-computer interaction using speech 7 input will become increasingly more efficient in comparison with textual input as is the case with human-human interaction <ref> [Cohen and Oviatt, 1994] </ref>. Note that in Figures 3 and 4, we give the average for Tasks 1-4, as well as the average overall 5 tasks.
Reference: [Ferguson et al., 1996] <author> George Ferguson, James Allen, and Brad Miller, "TRAINS-95: </author> <title> Towards a Mixed-Initiative Planning Assistant," </title> <booktitle> In Proceedings of the Third Conference on Artificial Intelligence Planning Systems, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction TRAINS-95 is the first end-to-end implementation in a long-term effort to develop an intelligent planning assistant that is conversationally proficient in natural language. The initial domain is a train route planner, where a human manager and the system must cooperate to develop and execute plans <ref> [Allen et al., 1996; Ferguson et al., 1996] </ref>. TRAINS-95 provides a real-time multi-modal interface between the human and computer.
Reference: [Hirschman et al., 1993] <author> L. Hirschman, M. Bates, D. Dahl, W. Fisher, J. Garofolo, D. Pallet, K. Hunicke-Smith, P. Price, A. Rudnicky, and E. Tzoukermann, </author> <title> "Multi-Site Data Collection and Evaluation in Spoken Language Understanding," </title> <booktitle> In Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <pages> pages 19-24, </pages> <year> 1993. </year>
Reference-contexts: Furthermore, we wanted to be able to use the evaluation results to guide us in system debugging, and to some extent our future research focus. Standard accuracy models used to evaluate speech recognition and data base query tasks such as ATIS <ref> [Hirschman et al., 1993] </ref> are not appropriate for a dialogue evaluation. There is no right answer to an utterance in a dialogue. Rather, there are many different possible ways to answer, each of them equally valid.
Reference: [Huang et al., 1993] <author> D. Huang, F.Alleva, H.W. Hon, M. Y. Hwang, K. F. Lee, and R. Rosen-feld, </author> <title> "The Sphinx-II Speech Recognition System: An Overview," </title> <booktitle> Computer, Speech and Language, </booktitle> <year> 1993. </year>
Reference-contexts: Hardware and Software Configuration All sixteen sessions were conducted in the URCS Speech Lab using identical hardware configurations. The software components used in the experiment included: * A Sphinx-II speech recognizer developed at CMU <ref> [Huang et al., 1993] </ref>, running on a DEC Alpha * TRAINS-95 version 1.3 including the speech recognition post-processor running on a SPARCstation 2 * TrueTalk, a commercial off-the-shelf speech generator (available from Entropics, Inc.) running on a SparcLX Subjects, working at a Sun SPARCstation, wore a headset with a microphone to
Reference: [Polifroni et al., 1992] <author> J. Polifroni, L. Hirschman, S. Seneff, and V. Zue, </author> <title> "Experiments in Evaluating Interactive Spoken Language Systems," </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 28-33, </pages> <year> 1992. </year>
Reference-contexts: Once the goal state and solution quality criteria are defined in objective terms, the evaluation can be completely automated. This gives the task-based evaluation method a significant advantage over other techniques <ref> [Polifroni et al., 1992; Walter, 1992] </ref> where human evaluators must intervene to examine individual responses and assess their correctness. <p> Note, however that a comparison of two dialogue systems one taking a conservative approach that only answers when it is confident (cf. [Smith and Hipp, 1994]), and a robust system that proceeds based on partial understanding showed that the robust system was significantly more successful in completing the same task <ref> [Polifroni et al., 1992] </ref>. * Random Routes Designers of the TRAINS-95 system suspected that since the initial domain is so simple, there would be very limited dialogue between the human and the computer.
Reference: [Ringger and Allen, 1996] <author> E. Ringger and J. F. Allen, </author> <title> "Error Correction Via A PostProcessor For Continuous Speech Recognition," </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1996. </year>
Reference-contexts: The speech recognition post-processor has been successful in improving the word recognition accuracy rate by an additional 5%, on average, in the TRAINS-95 domain <ref> [Ringger and Allen, 1996] </ref>. and whether their speech recognition output was run through the post-processor before being input to the TRAINS-95 parser.
Reference: [Rudnicky, 1993] <author> A. Rudnicky, </author> <title> "Mode Preferences in a Simple Data Retrieval Task," </title> <booktitle> In Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <pages> pages 364-369, </pages> <year> 1993. </year>
Reference-contexts: However, for each task, the amount of time to develop the plan was significantly lower when speech input was used. Speech input was 28 49% faster. This performance of speech input over keyboard input in our experiment is in contrast with experimental results obtained on some previous systems <ref> [Rudnicky, 1993] </ref>.
Reference: [Shriberg et al., 1992] <author> E. Shriberg, E. Wade, and P. Price, </author> <title> "Human-Machine Problem Solving Using Spoken Language Systems (SLS): Factors Affecting Performance and User Satisfaction," </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 49-54, </pages> <year> 1992. </year>
Reference-contexts: The tutorial encouraged the subject to "speak naturally, as if to another person". While we are aware that human-computer dialogue is significantly different than human-human dialogue, there is evidence that such instructions given to test subjects does significantly reduce unnatural speech syles such as hyperarticulation <ref> [Shriberg et al., 1992] </ref>. Since it was important for the subject to understand how the quality of his solutions would be judged, the tutorial also explained how to calculate the amount of time a route takes to travel.
Reference: [Smith and Hipp, 1994] <author> R. Smith and R. D. </author> <title> Hipp, Spoken Natural Language Dialog Systems: A Practical Approach, </title> <publisher> Oxford University Press, </publisher> <year> 1994. </year>
Reference-contexts: Our experiment demonstates that a robust approach can create a high variance in the effectiveness of an interaction. Note, however that a comparison of two dialogue systems one taking a conservative approach that only answers when it is confident (cf. <ref> [Smith and Hipp, 1994] </ref>), and a robust system that proceeds based on partial understanding showed that the robust system was significantly more successful in completing the same task [Polifroni et al., 1992]. * Random Routes Designers of the TRAINS-95 system suspected that since the initial domain is so simple, there would
Reference: [Walter, 1992] <author> S. Walter, </author> <title> "Neal-Montgomery NLP System Evaluation Methodology," </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 323-326, </pages> <year> 1992. </year> <month> 14 </month>
Reference-contexts: Once the goal state and solution quality criteria are defined in objective terms, the evaluation can be completely automated. This gives the task-based evaluation method a significant advantage over other techniques <ref> [Polifroni et al., 1992; Walter, 1992] </ref> where human evaluators must intervene to examine individual responses and assess their correctness.
References-found: 12

