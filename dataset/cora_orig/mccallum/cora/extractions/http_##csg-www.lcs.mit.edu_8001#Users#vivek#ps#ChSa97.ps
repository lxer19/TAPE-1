URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/ps/ChSa97.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/sark_pub.html
Root-URL: 
Email: Email: chowjh@vnet.ibm.com Email: vivek@lcs.mit.edu  
Phone: Phone: 408-927-1751 Phone: 617-253-6035  
Title: False Sharing Elimination by Selection of Runtime Scheduling Parameters  
Author: Jyh-Herng Chow Vivek Sarkar 
Address: 555 Bailey Avenue 545 Technology Square, NE43-206 San Jose, California 95141 Cambridge, Massachusetts 02139  
Affiliation: IBM Santa Teresa Laboratory MIT Laboratory for Computer Science  
Abstract: False sharing can be a source of significant overhead on shared-memory multiprocessors. Several program restructuring techniques to reduce false sharing have been proposed in past work. In this paper, we propose an approach for elimination of false sharing based solely on selection of runtime schedule parameters for parallel loops. This approach leads to more portable code since only the schedule parameters need to be changed to target different multiprocessors. Also, the guarantee of elimination (rather than reduction) of false sharing in a parallel loop can significantly reduce the bookkeeping overhead in some memory consistency mechanisms. We present some preliminary experimental results for this approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David F. Bacon, Jyh-Herng Chow, Dz ching R. Ju, K. Muthukumar, and Vivek Sarkar. </author> <title> A Compiler Framework for Restructuring Data Declarations to Enhance Cache and TLB Effectiveness. </title> <booktitle> CASCON '94 conference, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Changing data structures : Change the layout of data structures, e.g., by array alignment and padding <ref> [18, 1] </ref>. Array alignment is the insertion of dummy space so as to change the starting address of an array variable. Array padding is an increase in the allocated dimension size of an array variable. <p> The cache line offset for a data element e is denoted by o (e), and the offset of the starting address of an array A is denoted by A fl , so we can write A fl = o (A <ref> [1; 1] </ref>).
Reference: [2] <author> David H. Bailey. </author> <title> Unfavorable Strides in Cache Memory Systems. </title> <journal> Scientific Programming, </journal> <volume> 4 </volume> <pages> 53-58, </pages> <year> 1995. </year> <note> RNR Technical Report RNR-92-015, </note> <institution> NASA Ames Research Center. </institution>
Reference-contexts: Further studies are necessary to understand the interaction between blocking for cache locality and chunking for false sharing elimination, as well as the interaction between array dimension padding for reducing cache set conflicts suggested in <ref> [2] </ref> and the array dimension padding (to a multiple of w, which is a power of two) discussed in this paper.
Reference: [3] <author> Jyh-Herng Chow, Leonard E. Lyon, and Vivek Sarkar. </author> <title> Automatic Parallelization for Symmetric Shared-Memory Multiprocessors. </title> <booktitle> CASCON '96 conference, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: The performance measurements were made using the beta version of the IBM XL Fortran SMP compiler on an IBM RS/6000 model J30 SMP workstation containing four 133MHz PowerPC 604 processors. A description of an early version of the beta compiler can be found in <ref> [3] </ref>. The example program is shown in figure 6, along with runtime measurements obtained by scheduling the DOALL loop with different schedule parameters. The timings in figure 6 are for a single call to subroutine TEST with N 1 = N 2 = 8.
Reference: [4] <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Synchronization, Coherence, and Event Ordering in Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 21(2), </volume> <month> February </month> <year> 1988. </year> <title> Survey and Tutorial Series. </title>
Reference-contexts: In most currently implemented memory consistency mechanisms, the false sharing phenomenon also occurs between a read access and a write access to distinct memory locations in the same cache line. However, this form of read-write false sharing can be avoided. Newer memory consistency mechanisms (for weaker memory consistency models <ref> [4] </ref>) recognize that, in the absence of synchronization, a write in a cache line on one processor need not interfere with a read from the same cache line on another processor. So, we use the term "false sharing" to only refer to write-write interference.
Reference: [5] <author> Susan J. Eggers and Tor E. Jeremiassen. </author> <title> Eliminating False Sharing. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: Second, performance can be degraded due to cache line ownership becoming a serial bottleneck thus increasing the miss penalty by the waiting time for ownership. The increase in memory traffic caused by "ping-ponging" of cache lines due to false sharing has been studied by several researchers, e.g., <ref> [18, 8, 5] </ref>. False sharing can also lead to an anomaly in which increasing the cache line size leads to an increase in the number of cache misses observed in parallel programs, even though the programs may have good spatial locality [6]. <p> The techniques that have been proposed in the past for reducing false sharing fall into one of the following approaches <ref> [18, 8, 5] </ref>: Changing loop structures : Transform program loops, e.g., by blocking, alignment, or peeling, so that iterations in a parallel loop access disjoint cache lines [19, 8, 7]. Changing data structures : Change the layout of data structures, e.g., by array alignment and padding [18, 1]. <p> Array padding is an increase in the allocated dimension size of an array variable. Copying data : Copy the data to be updated by the loop into a temporary data structure that does not exhibit false sharing and is well suited to the data access patterns in the loop <ref> [5, 11, 17] </ref>. After the parallel loop completes execution, the temporary data structure is copied back to the original structure. The copy back may exhibit false sharing, however. Changing schedule parameters : Schedule the loop iterations so that concurrently executed iter ations access disjoint cache lines, e.g., [14].
Reference: [6] <author> Susan J. Eggers and Randy H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In ACM International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 257-270, </pages> <year> 1989. </year>
Reference-contexts: False sharing can also lead to an anomaly in which increasing the cache line size leads to an increase in the number of cache misses observed in parallel programs, even though the programs may have good spatial locality <ref> [6] </ref>. In most currently implemented memory consistency mechanisms, the false sharing phenomenon also occurs between a read access and a write access to distinct memory locations in the same cache line. However, this form of read-write false sharing can be avoided.
Reference: [7] <author> Elana D. Granston. </author> <title> Toward a Compile-Time Methodology for Reducing False Sharing and Communication Traffic in Shared Virtual Memory Systems. </title> <booktitle> In Proc. of Sixth Workshop on Language and Compilers for Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: The techniques that have been proposed in the past for reducing false sharing fall into one of the following approaches [18, 8, 5]: Changing loop structures : Transform program loops, e.g., by blocking, alignment, or peeling, so that iterations in a parallel loop access disjoint cache lines <ref> [19, 8, 7] </ref>. Changing data structures : Change the layout of data structures, e.g., by array alignment and padding [18, 1]. Array alignment is the insertion of dummy space so as to change the starting address of an array variable. <p> The remaining chunks (except possibly the last chunk) have chunksize iterations each. The peel parameter has the same effect as the alignment factor in generalized loop blocking <ref> [7] </ref>. The default value is peel = 0. We assume that the compiler translates a DOALL construct by generating appropriate calls to the runtime li brary. <p> Generalized loop blocking <ref> [7] </ref> is a technique that prevents false sharing by blocking the DOALL loop with a non-integer block size such that each iteration in the blocked loop will cover disjoint cache lines.
Reference: [8] <author> Manish Gupta and David A. Padua. </author> <title> Effects of Program Par-allelization and Stripmining Transformation on Cache Performance in a Multiprocessor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages I.301-I.304, </pages> <year> 1991. </year>
Reference-contexts: Second, performance can be degraded due to cache line ownership becoming a serial bottleneck thus increasing the miss penalty by the waiting time for ownership. The increase in memory traffic caused by "ping-ponging" of cache lines due to false sharing has been studied by several researchers, e.g., <ref> [18, 8, 5] </ref>. False sharing can also lead to an anomaly in which increasing the cache line size leads to an increase in the number of cache misses observed in parallel programs, even though the programs may have good spatial locality [6]. <p> The techniques that have been proposed in the past for reducing false sharing fall into one of the following approaches <ref> [18, 8, 5] </ref>: Changing loop structures : Transform program loops, e.g., by blocking, alignment, or peeling, so that iterations in a parallel loop access disjoint cache lines [19, 8, 7]. Changing data structures : Change the layout of data structures, e.g., by array alignment and padding [18, 1]. <p> The techniques that have been proposed in the past for reducing false sharing fall into one of the following approaches [18, 8, 5]: Changing loop structures : Transform program loops, e.g., by blocking, alignment, or peeling, so that iterations in a parallel loop access disjoint cache lines <ref> [19, 8, 7] </ref>. Changing data structures : Change the layout of data structures, e.g., by array alignment and padding [18, 1]. Array alignment is the insertion of dummy space so as to change the starting address of an array variable.
Reference: [9] <author> Alan H. Karp and Vivek Sarkar. </author> <title> Data Merging for Shared-Memory Multiprocessors. </title> <booktitle> Proceedings of the 26th Hawaii International Conference on System Sciences, Wailea, Hawaii, Volume I (Architecture), </booktitle> <pages> pages 244-256, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: This is certainly true for software-based distributed shared memory schemes <ref> [10, 9] </ref>; if the consistency mechanism knows that false sharing is guaranteed to not occur for a specified cache line (i.e., that the cache line will have a single writer between synchronization points), then the fixed overheads of making a clone/twin copy of the cache line can be avoided.
Reference: [10] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proc. of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This is certainly true for software-based distributed shared memory schemes <ref> [10, 9] </ref>; if the consistency mechanism knows that false sharing is guaranteed to not occur for a specified cache line (i.e., that the cache line will have a single writer between synchronization points), then the fixed overheads of making a clone/twin copy of the cache line can be avoided.
Reference: [11] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In ACM International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <year> 1991. </year>
Reference-contexts: Array padding is an increase in the allocated dimension size of an array variable. Copying data : Copy the data to be updated by the loop into a temporary data structure that does not exhibit false sharing and is well suited to the data access patterns in the loop <ref> [5, 11, 17] </ref>. After the parallel loop completes execution, the temporary data structure is copied back to the original structure. The copy back may exhibit false sharing, however. Changing schedule parameters : Schedule the loop iterations so that concurrently executed iter ations access disjoint cache lines, e.g., [14].
Reference: [12] <author> Bill Nitzberg and Virginia Lo. </author> <title> Distributed Shared Memory: A Survey of Issues and Algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <year> 1991. </year>
Reference-contexts: Additionally, if p is unknown at compile-time, the schedule from [14] will have to be constructed at runtime, with proper synchronizations inserted for each iteration. The false sharing problem also occurs in distributed shared memory machines <ref> [12] </ref> where a shared memory abstraction is implemented above physically distributed memories. On these machines, multiple copies of the same memory page can co-exist to facilitate local accesses. Memory consistency is maintained at the page level.
Reference: [13] <institution> IBM Shared Memory System POWER/4 User's Guide and Technical Reference, </institution> <year> 1993. </year>
Reference-contexts: In addition, solutions for eliminating false sharing are necessary to guarantee correct execution on shared-memory multiprocessors that have no automatic (hardware or software) support for cache consistency (e.g., as in <ref> [13] </ref>). The rest of the paper is organized as follows. Section 2 defines our machine and program model. Section 3 describes how to eliminate false sharing by selecting values for the chunksize and chunkstride loop scheduling parameters.
Reference: [14] <author> Barbara Simons, Vivek Sarkar, Jr. Mauricio Breternitz, and Michael Lai. </author> <title> An Optimal Asynchronous Scheduling Algorithm for Software Cache Consistency. </title> <booktitle> Proc. Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: After the parallel loop completes execution, the temporary data structure is copied back to the original structure. The copy back may exhibit false sharing, however. Changing schedule parameters : Schedule the loop iterations so that concurrently executed iter ations access disjoint cache lines, e.g., <ref> [14] </ref>. The first three approaches rely on extensive program restructuring. The fourth approach is less intrusive - it only involves selecting specific runtime schedule parameters or a specific runtime scheduling algorithm for a given parallel loop and target multiprocessor. <p> Note that if c does not divide w, the blocking factor b may not be an integer and the number of iterations executed in the inner loop is not constant. Another scheduling algorithm for preventing false sharing (different from the chunkstride = 2 approach) can be found in <ref> [14] </ref>. Given a single parallel loop L, iteration interference distance d, and the number of processors p, the algorithm determines a schedule S which orders the iterations such that any two concurrent iterations in S will have pairwise separation of at least d. <p> The interference distance d is selected to ensure elimination of false sharing. The schedule constructed by this algorithm is similar to a schedule with chunkstride=d and chunksize=1. An important difference is that the schedule from <ref> [14] </ref> requires at most two synchronizations per iteration, while a chunkstride=d schedule requires d barrier synchronizations. Additionally, if p is unknown at compile-time, the schedule from [14] will have to be constructed at runtime, with proper synchronizations inserted for each iteration. <p> The schedule constructed by this algorithm is similar to a schedule with chunkstride=d and chunksize=1. An important difference is that the schedule from <ref> [14] </ref> requires at most two synchronizations per iteration, while a chunkstride=d schedule requires d barrier synchronizations. Additionally, if p is unknown at compile-time, the schedule from [14] will have to be constructed at runtime, with proper synchronizations inserted for each iteration. The false sharing problem also occurs in distributed shared memory machines [12] where a shared memory abstraction is implemented above physically distributed memories.
Reference: [15] <author> Per Stenstrom. </author> <title> A Survey of Cache Coherence Schemes for Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 12-24, </pages> <year> 1990. </year>
Reference-contexts: to self-variable false sharing if the last element of A and the first element of B are mapped to the same cache line: REAL*4 A (N),B (N) DOALL I=1,N A (I)=I ENDDO Consider a machine with hardware supported cache coherence (e.g., a snooping bus or a directory-based memory consistency mechanism <ref> [15] </ref>). When a processor writes to a location, a write-invalidate protocol will invalidate all cache line copies on other processors that contain the same location. Thus, other processors will incur a cache miss when they attempt to write to a different location that is mapped to the invalidated cache line.
Reference: [16] <author> Peiyi Tang and Pen-Chung Yew. </author> <title> Processor Self-Scheduling for Multiple-Nested Parallel Loops. </title> <booktitle> Proc. of the 1986 Int'l Conf. on Parallel Processing, </booktitle> <month> February </month> <year> 1986. </year>
Reference-contexts: 1 Introduction False sharing occurs when two processors attempt to concurrently write to distinct memory locations in the same cache line. The false sharing is self-variable if the two memory locations belong to the same (array or structure) variable; otherwise, it is cross-variable. Consider a self-scheduled <ref> [16] </ref> execution of the following parallel (DOALL) loop: REAL*4 A (N) DOALL I=1,N A (I)=I ENDDO Because a cache line can contain more than one element, self-variable false sharing can occur in the above loop even though different processors update different array elements.
Reference: [17] <author> Olivier Temam, Elana Granston, and William Jalby. </author> <title> To Copy or Not to Copy: A Compile-Time Technique for Assessing When Data Copying Should be Used to Eliminate Cache Conflicts. </title> <booktitle> In Proc. Supercomputing '93, </booktitle> <pages> pages 410-419, </pages> <year> 1993. </year>
Reference-contexts: Array padding is an increase in the allocated dimension size of an array variable. Copying data : Copy the data to be updated by the loop into a temporary data structure that does not exhibit false sharing and is well suited to the data access patterns in the loop <ref> [5, 11, 17] </ref>. After the parallel loop completes execution, the temporary data structure is copied back to the original structure. The copy back may exhibit false sharing, however. Changing schedule parameters : Schedule the loop iterations so that concurrently executed iter ations access disjoint cache lines, e.g., [14]. <p> As in <ref> [17] </ref>, we restrict the individual subscript expression for each dimension to either be of the form c 1 I 1 +c 2 I 2 + +c d I d + or the form c 0 I 0 + i.e., we do not allow the index variable of the DOALL loop, I
Reference: [18] <author> Josep Torrellas, Monica S. Lam, and John L. Hennessy. </author> <title> Shared Data Placement Optimizations to Reduce Multiprocessor Cache Miss Rates. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages II.266-II.270, </pages> <year> 1990. </year>
Reference-contexts: Second, performance can be degraded due to cache line ownership becoming a serial bottleneck thus increasing the miss penalty by the waiting time for ownership. The increase in memory traffic caused by "ping-ponging" of cache lines due to false sharing has been studied by several researchers, e.g., <ref> [18, 8, 5] </ref>. False sharing can also lead to an anomaly in which increasing the cache line size leads to an increase in the number of cache misses observed in parallel programs, even though the programs may have good spatial locality [6]. <p> The techniques that have been proposed in the past for reducing false sharing fall into one of the following approaches <ref> [18, 8, 5] </ref>: Changing loop structures : Transform program loops, e.g., by blocking, alignment, or peeling, so that iterations in a parallel loop access disjoint cache lines [19, 8, 7]. Changing data structures : Change the layout of data structures, e.g., by array alignment and padding [18, 1]. <p> Changing data structures : Change the layout of data structures, e.g., by array alignment and padding <ref> [18, 1] </ref>. Array alignment is the insertion of dummy space so as to change the starting address of an array variable. Array padding is an increase in the allocated dimension size of an array variable.
Reference: [19] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Data Locality Optimization Algorithm. </title> <booktitle> Proceedings of the ACM SIGPLAN Symposium on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The techniques that have been proposed in the past for reducing false sharing fall into one of the following approaches [18, 8, 5]: Changing loop structures : Transform program loops, e.g., by blocking, alignment, or peeling, so that iterations in a parallel loop access disjoint cache lines <ref> [19, 8, 7] </ref>. Changing data structures : Change the layout of data structures, e.g., by array alignment and padding [18, 1]. Array alignment is the insertion of dummy space so as to change the starting address of an array variable.
References-found: 19

