URL: http://www.cis.ohio-state.edu/~kchen/ijprai97.ps
Refering-URL: http://www.cis.ohio-state.edu/~kchen/on-line.html
Root-URL: 
Email: E-mail: fchen,lwang,chig@cis.pku.edu.cn  
Title: Methods of Combining Multiple Classifiers with Different Features and Their Applications to Text-Independent Speaker Identification
Author: Ke Chen, Lan Wang and Huisheng Chi 
Date: 417-445.  
Note: International Journal of Pattern Recognition and Artificial Intelligence, 11(3), 1997, pp.  Keywords:  
Address: 100871, China  
Affiliation: National Lab of Machine Perception and Center for Information Science Peking University, Beijing  
Abstract: In practical applications of pattern recognition, there are often different features extracted from raw data which needs recognizing. Methods of combining multiple classifiers with different features are viewed as a general problem in various application areas of pattern recognition. In this paper, a systematic investigation has been made and possible solutions are classified into three frameworks, i.e. linear opinion pools, winner-take-all and evidential reasoning. For combining multiple classifiers with different features, a novel method is presented in the framework of linear opinion pools and a modified training algorithm for associative switch is also proposed in the framework of winner-take-all. In the framework of evidential reasoning, several typical methods are briefly reviewed for use. All aforementioned methods have already been applied to text-independent speaker identification. The simulations show that results yielded by the methods described in this paper are better than not only the individual classifiers' but also ones obtained by combining multiple classifiers with the same feature. It indicates that the use of combining multiple classifiers with different features is an effective way to attack the problem of text-independent speaker identification.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. A. Abidi and R. C. Gonzalez. </author> <booktitle> Data Fusion in Robotics and Machine Intelligence. </booktitle> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1992. </year>
Reference-contexts: The basic idea underlying the combination of multiple classifiers is that a consensus is made somehow based upon the results of multiple classifiers for 1 a classification task using an elaborate combination scheme. So far, there have been extensive studies on the combination of multiple classifiers <ref> [1, 10, 20, 27, 35, 68] </ref>. Among these researches, possible solutions to the combination may be classified into three frameworks, i.e. linear opinion pools, winner-take-all and evidential reasoning.
Reference: 2. <author> C. Agnew. </author> <title> Multiple probability assessments by dependent experts. </title> <journal> J. Am. Stat. Assoc., </journal> <volume> 80:343347, </volume> <year> 1985. </year>
Reference-contexts: In the framework of evidential reasoning, for an input pattern, the output of each individual classifier is regarded as an evidence or an event and the combination scheme makes the final decision based upon a method of evidential reasoning or a principle of voting <ref> [2, 5, 20, 26, 40, 53, 68] </ref>. The so-called different features problem refers to that there are numerous types of features which can be extracted from the same raw data for a classification task. Therefore, several different feature sets are available for a given data set.
Reference: 3. <author> B. S. Atal. </author> <title> Automatic speaker recognition based on pitch contour. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 52(6):16871697, </volume> <year> 1972. </year>
Reference-contexts: In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification [9, 18, 24, 28, 49, 63]. These classifiers include distance classifiers <ref> [3, 4, 25, 33, 42] </ref>, neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59]. <p> With respect to the matrix W (i) in Eq.(37), there are various forms which result in the existence of multiple distance classifiers <ref> [3, 25, 28, 42] </ref>.
Reference: 4. <author> B. S. Atal. </author> <title> Effectiveness of linear prediction characteristics of the speech waves for automatic speaker identification and verification. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 55(6):13041312, </volume> <year> 1974. </year>
Reference-contexts: In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification [9, 18, 24, 28, 49, 63]. These classifiers include distance classifiers <ref> [3, 4, 25, 33, 42] </ref>, neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59]. <p> However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. Actually, the performance of a speaker identification system should be evaluated by testing utterances recorded in different sessions <ref> [4, 22, 28, 29, 43, 44, 52, 59, 60] </ref>. As a result, in the experiments, the training set or Set-1 consists of 10 sentences recorded in the first session to train all individual classifiers. <p> combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition [64] more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice <ref> [4, 18, 21, 22, 24, 28, 44, 49, 52, 56] </ref>. As a result, several features have already been investigated [4, 21, 22, 28, 34, 63]. <p> As a result, several features have already been investigated <ref> [4, 21, 22, 28, 34, 63] </ref>. The main outcome of the many feature selection studies was that features which represent pitch and the speech spectrum were the most effective for speaker identification. However, there is less agreement on which parameterization of the speech spectrum to use for features. <p> However, there is less agreement on which parameterization of the speech spectrum to use for features. Common spectrum representations for speaker identification are linear predictive coefficients and their various transformations (cepstral coefficients and PARCOR coefficients etc.) as well as the cepstrum and its variants such as Mel-scale cepstrum <ref> [4, 52, 56, 64] </ref>. As a result, we select four common features for the experiments, i.e. linear predictive coding coefficients (LPCC), linear predictive coding cepstrum (LPC-CEP), cepstrum (CEPS) and Mel-scale cepstrum (MEL-CEP) [51]. <p> On the other hand, it is generally agreed that the voiced parts of an utterance, especially vowels and nasal, are more effective than the unvoiced parts for text-independent speaker identification <ref> [4, 52, 56, 64] </ref>. In experiments, therefore, only the voiced parts of a sentence are kept regardless of their contents by using a simple energy measuring method. The length of the Hamming analysis window is 64 ms without overlapping.
Reference: 5. <author> R. Battiti and A. M. Colla. </author> <title> Democracy in neural nets: voting schemes for classification. Neural Networks, </title> <address> 7(4):691708, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Recently, the combination of multiple classifiers has been viewed as a new direction for the development of highly reliable pattern recognition systems, in particular, optical character recognition (OCR) systems. Preliminary results indicate that combination of several complementary classifiers leads to classifiers with improved performance <ref> [5, 53, 61, 68, 69] </ref>. There are at least two reasons justifying the necessity of combining multiple classifiers. First, for almost any one of the current pattern recognition application areas, there are a number of classification algorithms available developed from different theories and methodologies. <p> These features are also represented in very diversified forms and it is rather hard to lump them together for one single classifier to make a decision. As a result, multiple classifiers are needed to deal with the different features <ref> [5, 68, 69, 71] </ref>. It also results in a general problem how to combine those classifiers with different features to yield the improved performance. <p> In the framework of evidential reasoning, for an input pattern, the output of each individual classifier is regarded as an evidence or an event and the combination scheme makes the final decision based upon a method of evidential reasoning or a principle of voting <ref> [2, 5, 20, 26, 40, 53, 68] </ref>. The so-called different features problem refers to that there are numerous types of features which can be extracted from the same raw data for a classification task. Therefore, several different feature sets are available for a given data set. <p> Using such techniques, the considerably better classification results have been produced in the field of OCR by combining multiple classifiers with different features <ref> [5, 68, 69] </ref>. <p> = 1 when l j 0 = k and j 0 = arg max i2I D p (i) (x p i ) 0 otherwise 4 Combination Methods Based on Evidential Reasoning The combination methods based upon evidential reasoning have been extensively studied and already applied in the field of OCR <ref> [5, 53, 61, 68, 69] </ref>. The basic idea underlying the methods is that the result of each individ ual classifier is regarded as an evidence or an event and the final decision is made by consulting all combined classifiers with a method of evidential reasoning or evidence integrating.
Reference: 6. <author> Y. Bennani. </author> <title> A modular and hybrid connectionist system for speaker identification. </title> <booktitle> Neural Computation, </booktitle> <address> 7(4):791798, </address> <year> 1995. </year>
Reference-contexts: However, the performance of the systems based upon the composite features was not significantly improved. Furthermore, to a certain extent, the use of composite features results in the curse of dimensionality problem. In particular, the problem becomes quite serious when the techniques of neural computing with time-delay <ref> [6, 8, 9, 13, 62] </ref> are used. On the other hand, several kinds of classifiers have been also applied in speaker identification [9, 18, 24, 28, 49, 63]. <p> All utterances were recorded in a quiet room and sampled at 11.025 kHz sampling frequency in 16 bit precision. Some researchers have used the TIMIT database to evaluate their speaker identification systems and achieved the identifying accuracies close to 100% <ref> [6, 7, 8, 32] </ref>. However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. <p> In the experiments, for each speaker, the codebook consists of 32 vectors and the matrix W (i) (i = 1; ; K) in Eq.(39) is available from Eq.(38). 5.2.3 The Multilayer Perceptron As supervised classifiers, neural networks have recently become popular and have been used for speaker identification <ref> [6, 7, 9, 11, 13, 46, 47, 54] </ref>. Neural networks learn complex mappings between inputs and outputs and are particularly useful when the underlying statistics of the considered task are not well understood.
Reference: 7. <author> Y. Bennani, F. Fogelman, and P. Gallinari. </author> <title> A connectionist approach for speaker identification. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1990. </year> <month> 265268. </month>
Reference-contexts: All utterances were recorded in a quiet room and sampled at 11.025 kHz sampling frequency in 16 bit precision. Some researchers have used the TIMIT database to evaluate their speaker identification systems and achieved the identifying accuracies close to 100% <ref> [6, 7, 8, 32] </ref>. However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. <p> In the experiments, for each speaker, the codebook consists of 32 vectors and the matrix W (i) (i = 1; ; K) in Eq.(39) is available from Eq.(38). 5.2.3 The Multilayer Perceptron As supervised classifiers, neural networks have recently become popular and have been used for speaker identification <ref> [6, 7, 9, 11, 13, 46, 47, 54] </ref>. Neural networks learn complex mappings between inputs and outputs and are particularly useful when the underlying statistics of the considered task are not well understood.
Reference: 8. <author> Y. Bennani and P. Gallinari. </author> <title> On the use of TDNN extracted features information in talker identification. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1991. </year> <month> 385388. </month>
Reference-contexts: However, the performance of the systems based upon the composite features was not significantly improved. Furthermore, to a certain extent, the use of composite features results in the curse of dimensionality problem. In particular, the problem becomes quite serious when the techniques of neural computing with time-delay <ref> [6, 8, 9, 13, 62] </ref> are used. On the other hand, several kinds of classifiers have been also applied in speaker identification [9, 18, 24, 28, 49, 63]. <p> All utterances were recorded in a quiet room and sampled at 11.025 kHz sampling frequency in 16 bit precision. Some researchers have used the TIMIT database to evaluate their speaker identification systems and achieved the identifying accuracies close to 100% <ref> [6, 7, 8, 32] </ref>. However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session.
Reference: 9. <author> Y. Bennani and P. Gallinari. </author> <title> Connectionist approaches for automatic speaker recognition. In Proc. ESCA Workshop on Automatic Speaker Recognition, Identification and Verification, </title> <booktitle> 1994. </booktitle> <pages> 95102. </pages>
Reference-contexts: However, the performance of the systems based upon the composite features was not significantly improved. Furthermore, to a certain extent, the use of composite features results in the curse of dimensionality problem. In particular, the problem becomes quite serious when the techniques of neural computing with time-delay <ref> [6, 8, 9, 13, 62] </ref> are used. On the other hand, several kinds of classifiers have been also applied in speaker identification [9, 18, 24, 28, 49, 63]. <p> In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification <ref> [9, 18, 24, 28, 49, 63] </ref>. These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59]. <p> For the same purpose as the selection of common features, we choose four benchmark classifiers commonly used in speaker identification <ref> [9, 18, 24, 28] </ref>, i.e. distance classifier, vector quantization, multilayer perceptron and Gaussian mixture model. 5.2.1 The Distance Classifier The long term averaging was an early method widely adopted for text-independent speaker identification. the basic idea underlying the methods is the comparison of an average computed on test data to a <p> In the experiments, for each speaker, the codebook consists of 32 vectors and the matrix W (i) (i = 1; ; K) in Eq.(39) is available from Eq.(38). 5.2.3 The Multilayer Perceptron As supervised classifiers, neural networks have recently become popular and have been used for speaker identification <ref> [6, 7, 9, 11, 13, 46, 47, 54] </ref>. Neural networks learn complex mappings between inputs and outputs and are particularly useful when the underlying statistics of the considered task are not well understood.
Reference: 10. <author> S. Chatterjee and S. Chatterjee. </author> <title> On combining expert opinions. </title> <journal> Am. J. Math. Management Sci., </journal> <volume> 7(1):271 295, </volume> <year> 1987. </year>
Reference-contexts: The basic idea underlying the combination of multiple classifiers is that a consensus is made somehow based upon the results of multiple classifiers for 1 a classification task using an elaborate combination scheme. So far, there have been extensive studies on the combination of multiple classifiers <ref> [1, 10, 20, 27, 35, 68] </ref>. Among these researches, possible solutions to the combination may be classified into three frameworks, i.e. linear opinion pools, winner-take-all and evidential reasoning.
Reference: 11. <author> K. Chen, D. Xie, and H. Chi. </author> <title> Speaker identification based on hierarchical mixture of experts. </title> <booktitle> In Proc. World Congress on Neural Networks, </booktitle> <address> Washington D.C., </address> <year> 1995. </year> <note> I493I496. 17 </note>
Reference-contexts: In the experiments, for each speaker, the codebook consists of 32 vectors and the matrix W (i) (i = 1; ; K) in Eq.(39) is available from Eq.(38). 5.2.3 The Multilayer Perceptron As supervised classifiers, neural networks have recently become popular and have been used for speaker identification <ref> [6, 7, 9, 11, 13, 46, 47, 54] </ref>. Neural networks learn complex mappings between inputs and outputs and are particularly useful when the underlying statistics of the considered task are not well understood.
Reference: 12. <author> K. Chen, D. Xie, and H. Chi. </author> <title> A modified HME architecture for text-dependent speaker identification. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 7(5):13091313, </volume> <year> 1996. </year>
Reference: 13. <author> K. Chen, D. Xie, and H. Chi. </author> <title> Speaker identification using time-delay HMEs. </title> <journal> Int. J. Neural Systems, </journal> <volume> 7(1):2943, </volume> <year> 1996. </year>
Reference-contexts: However, the performance of the systems based upon the composite features was not significantly improved. Furthermore, to a certain extent, the use of composite features results in the curse of dimensionality problem. In particular, the problem becomes quite serious when the techniques of neural computing with time-delay <ref> [6, 8, 9, 13, 62] </ref> are used. On the other hand, several kinds of classifiers have been also applied in speaker identification [9, 18, 24, 28, 49, 63]. <p> In the experiments, for each speaker, the codebook consists of 32 vectors and the matrix W (i) (i = 1; ; K) in Eq.(39) is available from Eq.(38). 5.2.3 The Multilayer Perceptron As supervised classifiers, neural networks have recently become popular and have been used for speaker identification <ref> [6, 7, 9, 11, 13, 46, 47, 54] </ref>. Neural networks learn complex mappings between inputs and outputs and are particularly useful when the underlying statistics of the considered task are not well understood.
Reference: 14. <author> K. Chen, D. Xie, and H. Chi. </author> <title> Text-dependent speaker identification based on input/output HMMs: an empirical study. Neural Processing Letters, </title> <address> 3(2):8189, </address> <year> 1996. </year>
Reference: 15. <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Stat. Soc. B, </journal> <volume> 39:138, </volume> <year> 1977. </year>
Reference-contexts: For the log-likelihood, we adopt an EM algorithm <ref> [15] </ref> to estimate all parameters in by introducing a set of indicators as missing data to observed data. <p> In this paper, diagonal co variance matrices are used like the work in [52]. Given a sequence of feature vectors from a person's training speech, maximum likelihood estimates of the model parameters are obtained using the EM algorithm <ref> [15, 52] </ref>.
Reference: 16. <author> J. P. </author> <title> Dickenson. Some statistical results in the combination of forecasts. </title> <type> Oper. </type> <institution> Res. Q., 24:253260, </institution> <year> 1973. </year>
Reference-contexts: In regard to the linear coefficients or weights for combination, there are two kinds of methods for assigning values to linear coefficients or weights [35], i.e. weights as veridical probabilities [36, 45, 65, 67] and minimum error weights <ref> [16, 17, 30, 50] </ref>. In the framework of winner-take-all, a device called associative switch is used in the process of classification to choose the classification result of a specific classifier for a specific input pattern [69]. <p> of linear opinion pools, it is possible to directly use the methods with minimum error weights to combine multiple classifiers with different features since the weights could be achieved by performing regression merely based upon the information of classifiers' errors regardless of types of input (feature vector) to each classifier <ref> [16, 17, 30, 50] </ref>. Unfortunately, it is difficult to use the existing techniques with weights as veridical probabilities [36, 45, 65, 67] for handling the problem since in these methods the achievement of linear coefficients usually depends upon the input (feature vector) to each classifier.
Reference: 17. <author> J. P. </author> <title> Dickenson. Some comments on the combination of forecasts. </title> <type> Oper. </type> <institution> Res. Q., 26:205210, </institution> <year> 1975. </year>
Reference-contexts: In regard to the linear coefficients or weights for combination, there are two kinds of methods for assigning values to linear coefficients or weights [35], i.e. weights as veridical probabilities [36, 45, 65, 67] and minimum error weights <ref> [16, 17, 30, 50] </ref>. In the framework of winner-take-all, a device called associative switch is used in the process of classification to choose the classification result of a specific classifier for a specific input pattern [69]. <p> of linear opinion pools, it is possible to directly use the methods with minimum error weights to combine multiple classifiers with different features since the weights could be achieved by performing regression merely based upon the information of classifiers' errors regardless of types of input (feature vector) to each classifier <ref> [16, 17, 30, 50] </ref>. Unfortunately, it is difficult to use the existing techniques with weights as veridical probabilities [36, 45, 65, 67] for handling the problem since in these methods the achievement of linear coefficients usually depends upon the input (feature vector) to each classifier.
Reference: 18. <author> G. Doddington. </author> <title> Speaker recognition identifying people by their voice. </title> <booktitle> Proceedings of IEEE, </booktitle> <address> 73(11):16511664, </address> <year> 1986. </year>
Reference-contexts: Unfortunately, none of those features is perfect for robustness so that there is less agreement on which parameterization of the speech spectrum to use for features <ref> [18, 24, 28, 52, 49] </ref>. In addition, some researchers intended to lump two or more features together into a composite feature [24, 43, 44, 48]. However, the performance of the systems based upon the composite features was not significantly improved. <p> In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification <ref> [9, 18, 24, 28, 49, 63] </ref>. These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59]. <p> combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition [64] more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice <ref> [4, 18, 21, 22, 24, 28, 44, 49, 52, 56] </ref>. As a result, several features have already been investigated [4, 21, 22, 28, 34, 63]. <p> For the same purpose as the selection of common features, we choose four benchmark classifiers commonly used in speaker identification <ref> [9, 18, 24, 28] </ref>, i.e. distance classifier, vector quantization, multilayer perceptron and Gaussian mixture model. 5.2.1 The Distance Classifier The long term averaging was an early method widely adopted for text-independent speaker identification. the basic idea underlying the methods is the comparison of an average computed on test data to a
Reference: 19. <author> K. R. Farrell, R. J. Mammone, and K. T. Assaleh. </author> <title> Speaker recognition using neural network classifier. </title> <journal> IEEE Trans. Audio, Speech Processing, </journal> <volume> 2(1):194205, </volume> <year> 1994. </year>
Reference: 20. <author> S. </author> <title> French. Group consensus probability distributions: a critical survey. </title> <editor> In J. M. Bernardo, D. V. Lindley M. H. DeGroot, and A. F. M. Smith, editors, </editor> <booktitle> Bayesian Statistics, </booktitle> <volume> volume 2. </volume> <publisher> Elsevier Science Publishers, North-Holland, </publisher> <year> 1985. </year>
Reference-contexts: The basic idea underlying the combination of multiple classifiers is that a consensus is made somehow based upon the results of multiple classifiers for 1 a classification task using an elaborate combination scheme. So far, there have been extensive studies on the combination of multiple classifiers <ref> [1, 10, 20, 27, 35, 68] </ref>. Among these researches, possible solutions to the combination may be classified into three frameworks, i.e. linear opinion pools, winner-take-all and evidential reasoning. <p> In the framework of evidential reasoning, for an input pattern, the output of each individual classifier is regarded as an evidence or an event and the combination scheme makes the final decision based upon a method of evidential reasoning or a principle of voting <ref> [2, 5, 20, 26, 40, 53, 68] </ref>. The so-called different features problem refers to that there are numerous types of features which can be extracted from the same raw data for a classification task. Therefore, several different feature sets are available for a given data set.
Reference: 21. <author> S. Furui. </author> <title> Cepstral analysis technique for automatic speaker verification. </title> <journal> IEEE Trans. Acoust. Speech, Signal Processing, </journal> <volume> 29(2):254272, </volume> <year> 1981. </year>
Reference-contexts: combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition [64] more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice <ref> [4, 18, 21, 22, 24, 28, 44, 49, 52, 56] </ref>. As a result, several features have already been investigated [4, 21, 22, 28, 34, 63]. <p> As a result, several features have already been investigated <ref> [4, 21, 22, 28, 34, 63] </ref>. The main outcome of the many feature selection studies was that features which represent pitch and the speech spectrum were the most effective for speaker identification. However, there is less agreement on which parameterization of the speech spectrum to use for features.
Reference: 22. <author> S. Furui. </author> <title> Comparison of speaker recognition methods using statistical features and dynamic features. </title> <journal> IEEE Trans. Acoust. Speech, Signal Processing, </journal> <volume> 29(3):197200, </volume> <year> 1981. </year>
Reference-contexts: However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. Actually, the performance of a speaker identification system should be evaluated by testing utterances recorded in different sessions <ref> [4, 22, 28, 29, 43, 44, 52, 59, 60] </ref>. As a result, in the experiments, the training set or Set-1 consists of 10 sentences recorded in the first session to train all individual classifiers. <p> combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition [64] more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice <ref> [4, 18, 21, 22, 24, 28, 44, 49, 52, 56] </ref>. As a result, several features have already been investigated [4, 21, 22, 28, 34, 63]. <p> As a result, several features have already been investigated <ref> [4, 21, 22, 28, 34, 63] </ref>. The main outcome of the many feature selection studies was that features which represent pitch and the speech spectrum were the most effective for speaker identification. However, there is less agreement on which parameterization of the speech spectrum to use for features.
Reference: 23. <author> S. Furui. </author> <title> Research on individual features in speech waves and automatic speaker recognition techniques. Speech Communication, </title> <address> 5(2):183197, </address> <year> 1986. </year>
Reference: 24. <author> S. Furui. </author> <title> An overview of speaker recognition technology. In Proc. ESCA Workshop on Automatic Speaker Recognition, Identification and Verification, </title> <booktitle> 1994. </booktitle> <pages> 19. </pages>
Reference-contexts: Unfortunately, none of those features is perfect for robustness so that there is less agreement on which parameterization of the speech spectrum to use for features <ref> [18, 24, 28, 52, 49] </ref>. In addition, some researchers intended to lump two or more features together into a composite feature [24, 43, 44, 48]. However, the performance of the systems based upon the composite features was not significantly improved. <p> Unfortunately, none of those features is perfect for robustness so that there is less agreement on which parameterization of the speech spectrum to use for features [18, 24, 28, 52, 49]. In addition, some researchers intended to lump two or more features together into a composite feature <ref> [24, 43, 44, 48] </ref>. However, the performance of the systems based upon the composite features was not significantly improved. Furthermore, to a certain extent, the use of composite features results in the curse of dimensionality problem. <p> In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification <ref> [9, 18, 24, 28, 49, 63] </ref>. These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59]. <p> combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition [64] more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice <ref> [4, 18, 21, 22, 24, 28, 44, 49, 52, 56] </ref>. As a result, several features have already been investigated [4, 21, 22, 28, 34, 63]. <p> For the same purpose as the selection of common features, we choose four benchmark classifiers commonly used in speaker identification <ref> [9, 18, 24, 28] </ref>, i.e. distance classifier, vector quantization, multilayer perceptron and Gaussian mixture model. 5.2.1 The Distance Classifier The long term averaging was an early method widely adopted for text-independent speaker identification. the basic idea underlying the methods is the comparison of an average computed on test data to a
Reference: 25. <author> S. Furui, F. Itakura, and S. Saito. </author> <title> Talker recognition by longtime averaged speech spectrum. </title> <journal> Electronics and Communications in Japan, </journal> <volume> 55-A(10):5461, </volume> <year> 1972. </year>
Reference-contexts: In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification [9, 18, 24, 28, 49, 63]. These classifiers include distance classifiers <ref> [3, 4, 25, 33, 42] </ref>, neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59]. <p> With respect to the matrix W (i) in Eq.(37), there are various forms which result in the existence of multiple distance classifiers <ref> [3, 25, 28, 42] </ref>.
Reference: 26. <author> A. Gelfand, B. Ballick, and D. Dey. </author> <title> Modeling expert opinion arising as a partial probabilistic specification. </title> <journal> J. Am. Stat. Assoc., </journal> <volume> 90:598604, </volume> <year> 1995. </year>
Reference-contexts: In the framework of evidential reasoning, for an input pattern, the output of each individual classifier is regarded as an evidence or an event and the combination scheme makes the final decision based upon a method of evidential reasoning or a principle of voting <ref> [2, 5, 20, 26, 40, 53, 68] </ref>. The so-called different features problem refers to that there are numerous types of features which can be extracted from the same raw data for a classification task. Therefore, several different feature sets are available for a given data set.
Reference: 27. <author> C. Genest and J. V. Zidek. </author> <title> Combining probability distributions: a critique and an annotated bibliography. </title> <type> Statist. </type> <institution> Sci., 1:114148, </institution> <year> 1986. </year>
Reference-contexts: The basic idea underlying the combination of multiple classifiers is that a consensus is made somehow based upon the results of multiple classifiers for 1 a classification task using an elaborate combination scheme. So far, there have been extensive studies on the combination of multiple classifiers <ref> [1, 10, 20, 27, 35, 68] </ref>. Among these researches, possible solutions to the combination may be classified into three frameworks, i.e. linear opinion pools, winner-take-all and evidential reasoning.
Reference: 28. <author> H. Gish and M. Schmidt. </author> <title> Text-independent speaker identification. </title> <journal> IEEE Signal Processing Magazine, </journal> <pages> pages 1832, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Speaker identification is the process of determining from which of the registered speaker when a given utterance comes. Furthermore, speaker identification systems can be either text-independent or text-dependent. By text-independent, we refer to that the identification procedure should work for any text in either training or testing <ref> [28] </ref>. This is a different problem than text-dependent speaker identification, where the text in both training and testing is the same or is known. Speaker identification is a rather hard task since a speaker's voice always changes in time. <p> Unfortunately, none of those features is perfect for robustness so that there is less agreement on which parameterization of the speech spectrum to use for features <ref> [18, 24, 28, 52, 49] </ref>. In addition, some researchers intended to lump two or more features together into a composite feature [24, 43, 44, 48]. However, the performance of the systems based upon the composite features was not significantly improved. <p> In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification <ref> [9, 18, 24, 28, 49, 63] </ref>. These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59]. <p> These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation <ref> [28, 29, 52, 57, 59] </ref>. Since there are many kinds of features and classifiers, speaker identification becomes a typical task which needs to combine multiple classifiers with different features for robustness. <p> However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. Actually, the performance of a speaker identification system should be evaluated by testing utterances recorded in different sessions <ref> [4, 22, 28, 29, 43, 44, 52, 59, 60] </ref>. As a result, in the experiments, the training set or Set-1 consists of 10 sentences recorded in the first session to train all individual classifiers. <p> combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition [64] more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice <ref> [4, 18, 21, 22, 24, 28, 44, 49, 52, 56] </ref>. As a result, several features have already been investigated [4, 21, 22, 28, 34, 63]. <p> As a result, several features have already been investigated <ref> [4, 21, 22, 28, 34, 63] </ref>. The main outcome of the many feature selection studies was that features which represent pitch and the speech spectrum were the most effective for speaker identification. However, there is less agreement on which parameterization of the speech spectrum to use for features. <p> For the same purpose as the selection of common features, we choose four benchmark classifiers commonly used in speaker identification <ref> [9, 18, 24, 28] </ref>, i.e. distance classifier, vector quantization, multilayer perceptron and Gaussian mixture model. 5.2.1 The Distance Classifier The long term averaging was an early method widely adopted for text-independent speaker identification. the basic idea underlying the methods is the comparison of an average computed on test data to a <p> With respect to the matrix W (i) in Eq.(37), there are various forms which result in the existence of multiple distance classifiers <ref> [3, 25, 28, 42] </ref>.
Reference: 29. <author> H. Gish, M. Schmidt, </author> <title> and A Mieke. A robust segmental method for text-independent speaker identification. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1994. </year> <month> 145148. </month>
Reference-contexts: These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation <ref> [28, 29, 52, 57, 59] </ref>. Since there are many kinds of features and classifiers, speaker identification becomes a typical task which needs to combine multiple classifiers with different features for robustness. <p> However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. Actually, the performance of a speaker identification system should be evaluated by testing utterances recorded in different sessions <ref> [4, 22, 28, 29, 43, 44, 52, 59, 60] </ref>. As a result, in the experiments, the training set or Set-1 consists of 10 sentences recorded in the first session to train all individual classifiers.
Reference: 30. <author> C. W. J. Granger and R. Ramanathan. </author> <title> Improved methods of combining forecasts. </title> <journal> J. Forecasting, </journal> <volume> 3:197 204, </volume> <year> 1984. </year>
Reference-contexts: In regard to the linear coefficients or weights for combination, there are two kinds of methods for assigning values to linear coefficients or weights [35], i.e. weights as veridical probabilities [36, 45, 65, 67] and minimum error weights <ref> [16, 17, 30, 50] </ref>. In the framework of winner-take-all, a device called associative switch is used in the process of classification to choose the classification result of a specific classifier for a specific input pattern [69]. <p> of linear opinion pools, it is possible to directly use the methods with minimum error weights to combine multiple classifiers with different features since the weights could be achieved by performing regression merely based upon the information of classifiers' errors regardless of types of input (feature vector) to each classifier <ref> [16, 17, 30, 50] </ref>. Unfortunately, it is difficult to use the existing techniques with weights as veridical probabilities [36, 45, 65, 67] for handling the problem since in these methods the achievement of linear coefficients usually depends upon the input (feature vector) to each classifier.
Reference: 31. <author> S. Grossberg. </author> <title> Competition, decision and consensus. </title> <journal> J. Mathematical Analysis and Applications, </journal> <volume> 66:470 493, </volume> <year> 1978. </year>
Reference-contexts: scheme always chooses only one classifier among several classifiers to use its result as the final decision for a specific input pattern, the chosen classifier could be viewed as a winner and the style of the combination method is similar to the principle of winner-take-all in the unsupervised learning paradigm <ref> [31, 37] </ref>.
Reference: 32. <author> J. He, L. Liu, and G. Palm. </author> <title> A text-independent speaker identification system based on neural networks. </title> <booktitle> In Proc. Int. conf. Spoken Language Processing, </booktitle> <address> Yokohama, </address> <year> 1994. </year>
Reference-contexts: All utterances were recorded in a quiet room and sampled at 11.025 kHz sampling frequency in 16 bit precision. Some researchers have used the TIMIT database to evaluate their speaker identification systems and achieved the identifying accuracies close to 100% <ref> [6, 7, 8, 32] </ref>. However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. <p> It should be noted that the size of the analysis window is slightly larger than the commonly used sizes (normally 16 ~ 32 ms) since it has been found that the identification performance is degraded with a normal analysis window <ref> [32] </ref>. Whenever the short-time energy of a frame of the sentence is higher than a predefined threshold, spectral features will be calculated.
Reference: 33. <author> A. Higgins, L. Bahler, and J. Porter. </author> <title> Voice identification using nearest-neighbor distance measure. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1993. </year> <month> 375378. </month>
Reference-contexts: In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification [9, 18, 24, 28, 49, 63]. These classifiers include distance classifiers <ref> [3, 4, 25, 33, 42] </ref>, neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59].
Reference: 34. <author> M. Homayounpour and G. Chollet. </author> <title> A comparison of some relevant parametric representations for speaker verification. In Proc. ESCA Workshop on Automatic Speaker Recognition, Identification and Verification, </title> <booktitle> 1994. </booktitle> <pages> 19. </pages>
Reference-contexts: As a result, several features have already been investigated <ref> [4, 21, 22, 28, 34, 63] </ref>. The main outcome of the many feature selection studies was that features which represent pitch and the speech spectrum were the most effective for speaker identification. However, there is less agreement on which parameterization of the speech spectrum to use for features.
Reference: 35. <author> R. A. Jocobs. </author> <title> Methods for combining experts' probability assessments. </title> <journal> Neural Computation, </journal> <volume> 7(5):867 888, </volume> <year> 1995. </year>
Reference-contexts: The basic idea underlying the combination of multiple classifiers is that a consensus is made somehow based upon the results of multiple classifiers for 1 a classification task using an elaborate combination scheme. So far, there have been extensive studies on the combination of multiple classifiers <ref> [1, 10, 20, 27, 35, 68] </ref>. Among these researches, possible solutions to the combination may be classified into three frameworks, i.e. linear opinion pools, winner-take-all and evidential reasoning. <p> In the framework of linear opinion pools, the combination schemes make the final decision through the use of a linear combination of multiple classifiers' results. In regard to the linear coefficients or weights for combination, there are two kinds of methods for assigning values to linear coefficients or weights <ref> [35] </ref>, i.e. weights as veridical probabilities [36, 45, 65, 67] and minimum error weights [16, 17, 30, 50]. In the framework of winner-take-all, a device called associative switch is used in the process of classification to choose the classification result of a specific classifier for a specific input pattern [69]. <p> One is that there is no analysis of value of information from dependent classifiers in the case of different features as input though the topic has been recently discussed in the case of the same feature as the input of dependent classifiers <ref> [35] </ref>. The other is that for a given task an effective method of searching for an optimal correspon-dency on available classifiers and different features will be needed to be developed though the exhaustive way might work as it did in the paper.
Reference: 36. <author> R. A. Jocobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <booktitle> Neural Computation, </booktitle> <address> 3:7987, </address> <year> 1991. </year>
Reference-contexts: In regard to the linear coefficients or weights for combination, there are two kinds of methods for assigning values to linear coefficients or weights [35], i.e. weights as veridical probabilities <ref> [36, 45, 65, 67] </ref> and minimum error weights [16, 17, 30, 50]. In the framework of winner-take-all, a device called associative switch is used in the process of classification to choose the classification result of a specific classifier for a specific input pattern [69]. <p> Unfortunately, it is difficult to use the existing techniques with weights as veridical probabilities <ref> [36, 45, 65, 67] </ref> for handling the problem since in these methods the achievement of linear coefficients usually depends upon the input (feature vector) to each classifier.
Reference: 37. <author> T. Kohonen. </author> <title> Self-organization and associative memory. </title> <publisher> Springer-Verlag, </publisher> <address> Tokyo, </address> <year> 1988. </year>
Reference-contexts: scheme always chooses only one classifier among several classifiers to use its result as the final decision for a specific input pattern, the chosen classifier could be viewed as a winner and the style of the combination method is similar to the principle of winner-take-all in the unsupervised learning paradigm <ref> [31, 37] </ref>.
Reference: 38. <author> K. Li and E. Wrench. </author> <title> An approach to text-independent speaker recognition with short utterances. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1983. </year> <month> 555558. </month>
Reference-contexts: the matrix W (i) has the following form: W (i) = 1 1 T i X (~x t ~ (i) )(~x t ~ (i) ) T ; i = 1; ; K: (38) 5.2.2 Vector Quantization As a non-parametric model, the vector quantization (VQ) classifier was applied to speaker identification <ref> [59, 38] </ref> and has since been the benchmark classifier for text-independent speaker identification systems. Typically, a speaker is modeled by a VQ codebook of 32 ~ 128 vectors derived using the LBG algorithm [39].
Reference: 39. <author> T. Linde, A. Buzo, and R. M. Gray. </author> <title> An algorithm for vector quantization. </title> <journal> IEEE Trans. Communications, </journal> <volume> 28(1):8495, </volume> <year> 1980. </year>
Reference-contexts: Typically, a speaker is modeled by a VQ codebook of 32 ~ 128 vectors derived using the LBG algorithm <ref> [39] </ref>. The clustering and recognition are carried out using the distance measure in Eq.(37) with the matrix W (i) (i = 1; ; K) in Eq.(38).
Reference: 40. <author> D. V. Lindley. </author> <title> Reconciliation of discrete probability distributions. </title> <editor> In J. M. Bernardo, D. V. Lindley M. H. DeGroot, and A. F. M. Smith, editors, </editor> <booktitle> Bayesian Statistics, </booktitle> <volume> volume 2. </volume> <publisher> Elsevier Science Publishers, North-Holland, </publisher> <year> 1985. </year>
Reference-contexts: In the framework of evidential reasoning, for an input pattern, the output of each individual classifier is regarded as an evidence or an event and the combination scheme makes the final decision based upon a method of evidential reasoning or a principle of voting <ref> [2, 5, 20, 26, 40, 53, 68] </ref>. The so-called different features problem refers to that there are numerous types of features which can be extracted from the same raw data for a classification task. Therefore, several different feature sets are available for a given data set.
Reference: 41. <author> E. J. Mandler and J. Schurmann. </author> <title> Combining the classification results of independent classifiers based on the Dempster/Shafer theory of evidence. </title> <journal> Pattern Recognition and Artificial Intelligence, </journal> <volume> 5:381393, </volume> <year> 1988. </year>
Reference-contexts: follows, E (D) = k if bel (k) = max i2 bel (i) T M + 1 otherwise (29) where 0 &lt; T 1 is a predefined threshold. 4.2 A Combination Method Based on Dempster-Shafer Theory The Dempster-Shafer (D-S) theory of evidence [58] has been applied for combining multiple classifiers <ref> [41, 53, 68] </ref>. In the method used in our work, the combination is made in the situation that the recognition, substitution and rejection rates of each individual classifier are merely necessary as prior knowledge [68].
Reference: 42. <author> J. Markel, B. Oshika, and A. Gray Jr. </author> <title> Text-independent speaker recognition from a large linguistically unconstrained time-space data base. </title> <journal> IEEE Trans. Acoust., Speech, Sigal Processing, </journal> <volume> 27:7482, </volume> <year> 1979. </year>
Reference-contexts: In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification [9, 18, 24, 28, 49, 63]. These classifiers include distance classifiers <ref> [3, 4, 25, 33, 42] </ref>, neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59]. <p> mixture model. 5.2.1 The Distance Classifier The long term averaging was an early method widely adopted for text-independent speaker identification. the basic idea underlying the methods is the comparison of an average computed on test data to a collection of stored averages developed for each of the speakers in training <ref> [42] </ref>. As a result, the distance classifiers play a prominent role for classification in the methods. <p> With respect to the matrix W (i) in Eq.(37), there are various forms which result in the existence of multiple distance classifiers <ref> [3, 25, 28, 42] </ref>.
Reference: 43. <author> T. Matsui and S. Furui. </author> <title> A text-dependent speaker recognition method robust against utterance variations. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1991. </year> <month> 377380. </month>
Reference-contexts: Unfortunately, none of those features is perfect for robustness so that there is less agreement on which parameterization of the speech spectrum to use for features [18, 24, 28, 52, 49]. In addition, some researchers intended to lump two or more features together into a composite feature <ref> [24, 43, 44, 48] </ref>. However, the performance of the systems based upon the composite features was not significantly improved. Furthermore, to a certain extent, the use of composite features results in the curse of dimensionality problem. <p> However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. Actually, the performance of a speaker identification system should be evaluated by testing utterances recorded in different sessions <ref> [4, 22, 28, 29, 43, 44, 52, 59, 60] </ref>. As a result, in the experiments, the training set or Set-1 consists of 10 sentences recorded in the first session to train all individual classifiers.
Reference: 44. <author> T. Matsui and S. Furui. </author> <title> Speaker recognition technology. </title> <journal> NTT Review, </journal> <volume> 7(2):4048, </volume> <year> 1995. </year>
Reference-contexts: Unfortunately, none of those features is perfect for robustness so that there is less agreement on which parameterization of the speech spectrum to use for features [18, 24, 28, 52, 49]. In addition, some researchers intended to lump two or more features together into a composite feature <ref> [24, 43, 44, 48] </ref>. However, the performance of the systems based upon the composite features was not significantly improved. Furthermore, to a certain extent, the use of composite features results in the curse of dimensionality problem. <p> It is followed by the results of individual classifiers and a specific combination. As a case study, finally, the results for comparison are also reported. 5.1 Speech Database, Feature Selection and Performance Evaluation 5.1.1 The Database There is no standard database (benchmark) to evaluate speaker identification systems <ref> [44] </ref>, though the DARPA TIMIT database which was originally designed to evaluate automatic speech recognition systems is often borrowed to evaluate speaker identification systems. The database for experiments reported in this paper is a subset of the standard speech database in China. <p> However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. Actually, the performance of a speaker identification system should be evaluated by testing utterances recorded in different sessions <ref> [4, 22, 28, 29, 43, 44, 52, 59, 60] </ref>. As a result, in the experiments, the training set or Set-1 consists of 10 sentences recorded in the first session to train all individual classifiers. <p> combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition [64] more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice <ref> [4, 18, 21, 22, 24, 28, 44, 49, 52, 56] </ref>. As a result, several features have already been investigated [4, 21, 22, 28, 34, 63].
Reference: 45. <author> S. J. Nowlan. </author> <title> Competing experts: An experimental investigation of associative mixture models. </title> <type> Tech. Rep. </type> <institution> CRG-TR-90-5, Department of Computer Science, University of Toronto, </institution> <year> 1990. </year>
Reference-contexts: In regard to the linear coefficients or weights for combination, there are two kinds of methods for assigning values to linear coefficients or weights [35], i.e. weights as veridical probabilities <ref> [36, 45, 65, 67] </ref> and minimum error weights [16, 17, 30, 50]. In the framework of winner-take-all, a device called associative switch is used in the process of classification to choose the classification result of a specific classifier for a specific input pattern [69]. <p> Unfortunately, it is difficult to use the existing techniques with weights as veridical probabilities <ref> [36, 45, 65, 67] </ref> for handling the problem since in these methods the achievement of linear coefficients usually depends upon the input (feature vector) to each classifier.
Reference: 46. <author> J. Oglesby and J. S. Mason. </author> <title> Optimization of neural models for speaker identification. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1990. </year> <month> 261264. </month>
Reference-contexts: In the experiments, for each speaker, the codebook consists of 32 vectors and the matrix W (i) (i = 1; ; K) in Eq.(39) is available from Eq.(38). 5.2.3 The Multilayer Perceptron As supervised classifiers, neural networks have recently become popular and have been used for speaker identification <ref> [6, 7, 9, 11, 13, 46, 47, 54] </ref>. Neural networks learn complex mappings between inputs and outputs and are particularly useful when the underlying statistics of the considered task are not well understood.
Reference: 47. <author> J. Oglesby and J. S. Mason. </author> <title> Radial basis function networks for speaker recognition. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1991. </year> <month> 393396. </month>
Reference-contexts: In the experiments, for each speaker, the codebook consists of 32 vectors and the matrix W (i) (i = 1; ; K) in Eq.(39) is available from Eq.(38). 5.2.3 The Multilayer Perceptron As supervised classifiers, neural networks have recently become popular and have been used for speaker identification <ref> [6, 7, 9, 11, 13, 46, 47, 54] </ref>. Neural networks learn complex mappings between inputs and outputs and are particularly useful when the underlying statistics of the considered task are not well understood.
Reference: 48. <author> J. P. Openshaw, Z. P. Sun, and J. S. Mason. </author> <title> A comparison of composite features under degraded speech in speaker recognition. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1993. </year> <month> II371II374. </month>
Reference-contexts: Unfortunately, none of those features is perfect for robustness so that there is less agreement on which parameterization of the speech spectrum to use for features [18, 24, 28, 52, 49]. In addition, some researchers intended to lump two or more features together into a composite feature <ref> [24, 43, 44, 48] </ref>. However, the performance of the systems based upon the composite features was not significantly improved. Furthermore, to a certain extent, the use of composite features results in the curse of dimensionality problem.
Reference: 49. <author> D. O'Shaughnessy. </author> <title> Speaker recognition. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(4):417, </volume> <year> 1986. </year> <month> 19 </month>
Reference-contexts: Unfortunately, none of those features is perfect for robustness so that there is less agreement on which parameterization of the speech spectrum to use for features <ref> [18, 24, 28, 52, 49] </ref>. In addition, some researchers intended to lump two or more features together into a composite feature [24, 43, 44, 48]. However, the performance of the systems based upon the composite features was not significantly improved. <p> In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification <ref> [9, 18, 24, 28, 49, 63] </ref>. These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59]. <p> combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition [64] more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice <ref> [4, 18, 21, 22, 24, 28, 44, 49, 52, 56] </ref>. As a result, several features have already been investigated [4, 21, 22, 28, 34, 63].
Reference: 50. <author> M. P. Perrone. </author> <title> Improving regression estimation: averaging methods of variance reduction with extensions to general convex measure optimization. </title> <type> Ph.D. thesis, </type> <institution> Department of Physics, Brown University, </institution> <year> 1993. </year>
Reference-contexts: In regard to the linear coefficients or weights for combination, there are two kinds of methods for assigning values to linear coefficients or weights [35], i.e. weights as veridical probabilities [36, 45, 65, 67] and minimum error weights <ref> [16, 17, 30, 50] </ref>. In the framework of winner-take-all, a device called associative switch is used in the process of classification to choose the classification result of a specific classifier for a specific input pattern [69]. <p> of linear opinion pools, it is possible to directly use the methods with minimum error weights to combine multiple classifiers with different features since the weights could be achieved by performing regression merely based upon the information of classifiers' errors regardless of types of input (feature vector) to each classifier <ref> [16, 17, 30, 50] </ref>. Unfortunately, it is difficult to use the existing techniques with weights as veridical probabilities [36, 45, 65, 67] for handling the problem since in these methods the achievement of linear coefficients usually depends upon the input (feature vector) to each classifier. <p> s=1 , the test utterance is identified with speaker k only if k = arg max 1iK O i , where O i = 1 S s=1 O i (~x 0 In the experiments reported in the paper, the three-layered fully connected MLP is used and the 2-fold cross-validation technique <ref> [50] </ref> is employed for finding an appropriate architecture of the MLP for the considered task. As a result, the numbers of neurons in the hidden layer cover from 32 to 36 which depend upon the dimension of chosen feature vectors. <p> The method called BAYES could also achieve the satisfactory results for all cases except the correspondency-3, which is consistent with other applications of the BAYES method <ref> [50, 68] </ref>. 6 Conclusions We have described several methods of combining multiple classifiers with different features and their application to text-independent speaker identification In particular, we classify the state-of-the-art techniques for combining multiple classifiers into three frameworks. The methods in the same framework share the similar principle for combination.
Reference: 51. <author> L. R. Rabiner and B. H. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <address> Englewood Cliffs, </address> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1993. </year>
Reference-contexts: As a result, we select four common features for the experiments, i.e. linear predictive coding coefficients (LPCC), linear predictive coding cepstrum (LPC-CEP), cepstrum (CEPS) and Mel-scale cepstrum (MEL-CEP) <ref> [51] </ref>. On the other hand, it is generally agreed that the voiced parts of an utterance, especially vowels and nasal, are more effective than the unvoiced parts for text-independent speaker identification [4, 52, 56, 64].
Reference: 52. <author> D. A. Reynolds. </author> <title> A Gaussian mixture modeling approach to text-independent speaker identification. </title> <type> Ph.D. Thesis, </type> <institution> Department of Electrical Engineering, Georgia Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Unfortunately, none of those features is perfect for robustness so that there is less agreement on which parameterization of the speech spectrum to use for features <ref> [18, 24, 28, 52, 49] </ref>. In addition, some researchers intended to lump two or more features together into a composite feature [24, 43, 44, 48]. However, the performance of the systems based upon the composite features was not significantly improved. <p> These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation <ref> [28, 29, 52, 57, 59] </ref>. Since there are many kinds of features and classifiers, speaker identification becomes a typical task which needs to combine multiple classifiers with different features for robustness. <p> However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. Actually, the performance of a speaker identification system should be evaluated by testing utterances recorded in different sessions <ref> [4, 22, 28, 29, 43, 44, 52, 59, 60] </ref>. As a result, in the experiments, the training set or Set-1 consists of 10 sentences recorded in the first session to train all individual classifiers. <p> combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition [64] more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice <ref> [4, 18, 21, 22, 24, 28, 44, 49, 52, 56] </ref>. As a result, several features have already been investigated [4, 21, 22, 28, 34, 63]. <p> However, there is less agreement on which parameterization of the speech spectrum to use for features. Common spectrum representations for speaker identification are linear predictive coefficients and their various transformations (cepstral coefficients and PARCOR coefficients etc.) as well as the cepstrum and its variants such as Mel-scale cepstrum <ref> [4, 52, 56, 64] </ref>. As a result, we select four common features for the experiments, i.e. linear predictive coding coefficients (LPCC), linear predictive coding cepstrum (LPC-CEP), cepstrum (CEPS) and Mel-scale cepstrum (MEL-CEP) [51]. <p> On the other hand, it is generally agreed that the voiced parts of an utterance, especially vowels and nasal, are more effective than the unvoiced parts for text-independent speaker identification <ref> [4, 52, 56, 64] </ref>. In experiments, therefore, only the voiced parts of a sentence are kept regardless of their contents by using a simple energy measuring method. The length of the Hamming analysis window is 64 ms without overlapping. <p> For utterances of all 20 speakers, total numbers of feature vectors are 10057 frames in Set-1, 4270 frames in Set-2 and 4604 frames in Set-3, respectively. 5.1.3 Performance Evaluation The evaluation of a speaker identification experiment is conducted in the following manner <ref> [52] </ref>. After feature extraction, the test speech is to produce a sequence of feature vectors denoted as f ~ f 1 ; ; ~ f t g. The sequence of feature vectors is divided into overlapping segments of S feature vectors. <p> speakers in the system. (In the experiments reported in this paper, there are 20 neurons in the output layer.) The backpropagation algorithm is used for training the MLPs [55]. 5.2.4 Gaussian Mixture Model As a parametric model, the Gaussian mixture model (GMM) was more recently applied to text-dependent speaker identification <ref> [52] </ref> and has demonstrated excellent performance for short test utterances. The basic idea underlying the GMM method lies in that the distribution of feature vectors extracted from a person's speech is modeled by a GMM density. <p> Collectively, the parameters of a speaker's density model are denoted as i = fff (i) (i) (i) j g. In this paper, diagonal co variance matrices are used like the work in <ref> [52] </ref>. Given a sequence of feature vectors from a person's training speech, maximum likelihood estimates of the model parameters are obtained using the EM algorithm [15, 52]. <p> In this paper, diagonal co variance matrices are used like the work in [52]. Given a sequence of feature vectors from a person's training speech, maximum likelihood estimates of the model parameters are obtained using the EM algorithm <ref> [15, 52] </ref>.
Reference: 53. <author> G. Rogova. </author> <title> Combining the results of several neural network classifiers. Neural Networks, </title> <address> 7(5):777781, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Recently, the combination of multiple classifiers has been viewed as a new direction for the development of highly reliable pattern recognition systems, in particular, optical character recognition (OCR) systems. Preliminary results indicate that combination of several complementary classifiers leads to classifiers with improved performance <ref> [5, 53, 61, 68, 69] </ref>. There are at least two reasons justifying the necessity of combining multiple classifiers. First, for almost any one of the current pattern recognition application areas, there are a number of classification algorithms available developed from different theories and methodologies. <p> In the framework of evidential reasoning, for an input pattern, the output of each individual classifier is regarded as an evidence or an event and the combination scheme makes the final decision based upon a method of evidential reasoning or a principle of voting <ref> [2, 5, 20, 26, 40, 53, 68] </ref>. The so-called different features problem refers to that there are numerous types of features which can be extracted from the same raw data for a classification task. Therefore, several different feature sets are available for a given data set. <p> = 1 when l j 0 = k and j 0 = arg max i2I D p (i) (x p i ) 0 otherwise 4 Combination Methods Based on Evidential Reasoning The combination methods based upon evidential reasoning have been extensively studied and already applied in the field of OCR <ref> [5, 53, 61, 68, 69] </ref>. The basic idea underlying the methods is that the result of each individ ual classifier is regarded as an evidence or an event and the final decision is made by consulting all combined classifiers with a method of evidential reasoning or evidence integrating. <p> follows, E (D) = k if bel (k) = max i2 bel (i) T M + 1 otherwise (29) where 0 &lt; T 1 is a predefined threshold. 4.2 A Combination Method Based on Dempster-Shafer Theory The Dempster-Shafer (D-S) theory of evidence [58] has been applied for combining multiple classifiers <ref> [41, 53, 68] </ref>. In the method used in our work, the combination is made in the situation that the recognition, substitution and rejection rates of each individual classifier are merely necessary as prior knowledge [68].
Reference: 54. <author> L. Rudasi and S. A. Zahorian. </author> <title> Text-independent talker identification with neural networks. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1991. </year> <month> 389392. </month>
Reference-contexts: In the experiments, for each speaker, the codebook consists of 32 vectors and the matrix W (i) (i = 1; ; K) in Eq.(39) is available from Eq.(38). 5.2.3 The Multilayer Perceptron As supervised classifiers, neural networks have recently become popular and have been used for speaker identification <ref> [6, 7, 9, 11, 13, 46, 47, 54] </ref>. Neural networks learn complex mappings between inputs and outputs and are particularly useful when the underlying statistics of the considered task are not well understood.
Reference: 55. <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing: Explorations in the microstruc-ture of cognition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Once we have the desired output the training procedure is the application of an existing learning algorithm such as backpropagation algorithm <ref> [55] </ref> on the training set S 1 . To design the appropriate desired output of the MLP used as the combination scheme, we first define a criterion for selecting a winner when there are several classifiers to give the correct result for a specific sample. <p> in the input layer is either 20 or 24.) and the number of neurons in the output layer is the population of speakers in the system. (In the experiments reported in this paper, there are 20 neurons in the output layer.) The backpropagation algorithm is used for training the MLPs <ref> [55] </ref>. 5.2.4 Gaussian Mixture Model As a parametric model, the Gaussian mixture model (GMM) was more recently applied to text-dependent speaker identification [52] and has demonstrated excellent performance for short test utterances.
Reference: 56. <author> M. R. Sambur. </author> <title> Selection of acoustic features for speaker identification. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> 23:176182, </volume> <year> 1975. </year>
Reference-contexts: combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition [64] more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice <ref> [4, 18, 21, 22, 24, 28, 44, 49, 52, 56] </ref>. As a result, several features have already been investigated [4, 21, 22, 28, 34, 63]. <p> However, there is less agreement on which parameterization of the speech spectrum to use for features. Common spectrum representations for speaker identification are linear predictive coefficients and their various transformations (cepstral coefficients and PARCOR coefficients etc.) as well as the cepstrum and its variants such as Mel-scale cepstrum <ref> [4, 52, 56, 64] </ref>. As a result, we select four common features for the experiments, i.e. linear predictive coding coefficients (LPCC), linear predictive coding cepstrum (LPC-CEP), cepstrum (CEPS) and Mel-scale cepstrum (MEL-CEP) [51]. <p> On the other hand, it is generally agreed that the voiced parts of an utterance, especially vowels and nasal, are more effective than the unvoiced parts for text-independent speaker identification <ref> [4, 52, 56, 64] </ref>. In experiments, therefore, only the voiced parts of a sentence are kept regardless of their contents by using a simple energy measuring method. The length of the Hamming analysis window is 64 ms without overlapping.
Reference: 57. <author> R. Schwartz, S. Roucos, and M. Berouti. </author> <title> The application of probability density estimation to text-independent speaker identification. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, 1982. </booktitle> <volume> 1649 1652. </volume>
Reference-contexts: These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation <ref> [28, 29, 52, 57, 59] </ref>. Since there are many kinds of features and classifiers, speaker identification becomes a typical task which needs to combine multiple classifiers with different features for robustness.
Reference: 58. <author> G. Shafer. </author> <title> A Mathematical Theory of Evidence. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1976. </year>
Reference-contexts: by combining multiple classifiers with different features as follows, E (D) = k if bel (k) = max i2 bel (i) T M + 1 otherwise (29) where 0 &lt; T 1 is a predefined threshold. 4.2 A Combination Method Based on Dempster-Shafer Theory The Dempster-Shafer (D-S) theory of evidence <ref> [58] </ref> has been applied for combining multiple classifiers [41, 53, 68]. In the method used in our work, the combination is made in the situation that the recognition, substitution and rejection rates of each individual classifier are merely necessary as prior knowledge [68]. <p> For combination, we first collect the evidences into groups with those impacting the same proposition in each group, and then combine the evidences in the each group, respectively. Let us denote m () and bel () as basic probability assignment (BPA) function and belief value in the D-S theory <ref> [58] </ref>, respectively. <p> For each group E j , a combined BPA m E j can be obtained by recursively applying the combination rule in the D-S theory <ref> [58] </ref> to BPA's m j 1 ; ; m j r provided by e j 1 (x p j 1 ); ; e j r (x p j r ) since all evidences e j 1 (x p j 1 ) = l 0 j .
Reference: 59. <author> F. Soong, A. Rosenberg, L. Rabiner, and B. Juang. </author> <title> A vector quantization approach to speaker recognition. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1985. </year> <month> 387390. </month>
Reference-contexts: These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation <ref> [28, 29, 52, 57, 59] </ref>. Since there are many kinds of features and classifiers, speaker identification becomes a typical task which needs to combine multiple classifiers with different features for robustness. <p> However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. Actually, the performance of a speaker identification system should be evaluated by testing utterances recorded in different sessions <ref> [4, 22, 28, 29, 43, 44, 52, 59, 60] </ref>. As a result, in the experiments, the training set or Set-1 consists of 10 sentences recorded in the first session to train all individual classifiers. <p> the matrix W (i) has the following form: W (i) = 1 1 T i X (~x t ~ (i) )(~x t ~ (i) ) T ; i = 1; ; K: (38) 5.2.2 Vector Quantization As a non-parametric model, the vector quantization (VQ) classifier was applied to speaker identification <ref> [59, 38] </ref> and has since been the benchmark classifier for text-independent speaker identification systems. Typically, a speaker is modeled by a VQ codebook of 32 ~ 128 vectors derived using the LBG algorithm [39].
Reference: 60. <author> F. K. Soong and A. E. Rosenberg. </author> <title> On the use of instantaneous and transitional spectral information in speaker recognition. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> 36(6):871879, </volume> <year> 1988. </year>
Reference-contexts: However, it is not sufficient to claim that such systems are robust since there is little variation of speakers' characteristics carried in voices recorded in the same session. Actually, the performance of a speaker identification system should be evaluated by testing utterances recorded in different sessions <ref> [4, 22, 28, 29, 43, 44, 52, 59, 60] </ref>. As a result, in the experiments, the training set or Set-1 consists of 10 sentences recorded in the first session to train all individual classifiers.
Reference: 61. <author> C. Y. Suen, T. A. Nadal, T. A. Mai, R. Legault, and L. Lam. </author> <title> Recognition of totally unconstrained handwritten numerals based on the concept of multiple experts. </title> <editor> In C. Y. Suen, editor, </editor> <title> Forniers in Handwriting Recognition, </title> <institution> Montreal: Concordia University, </institution> <year> 1990. </year> <month> 131143. </month>
Reference-contexts: 1 Introduction Recently, the combination of multiple classifiers has been viewed as a new direction for the development of highly reliable pattern recognition systems, in particular, optical character recognition (OCR) systems. Preliminary results indicate that combination of several complementary classifiers leads to classifiers with improved performance <ref> [5, 53, 61, 68, 69] </ref>. There are at least two reasons justifying the necessity of combining multiple classifiers. First, for almost any one of the current pattern recognition application areas, there are a number of classification algorithms available developed from different theories and methodologies. <p> = 1 when l j 0 = k and j 0 = arg max i2I D p (i) (x p i ) 0 otherwise 4 Combination Methods Based on Evidential Reasoning The combination methods based upon evidential reasoning have been extensively studied and already applied in the field of OCR <ref> [5, 53, 61, 68, 69] </ref>. The basic idea underlying the methods is that the result of each individ ual classifier is regarded as an evidence or an event and the final decision is made by consulting all combined classifiers with a method of evidential reasoning or evidence integrating. <p> If each opinion could be viewed as an evidence, then the process of making a consensus will be regarded as the process of evidential reasoning. There have been several combination methods based upon the different voting principles <ref> [61, 68] </ref>. For a sample D, each classifier e j (j = 1; ; N ) produces a result of classification based upon one kind of feature extracted from the sample D, say x p j (1 p j P ), i.e. e j (x p j ) = i.
Reference: 62. <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> 37:328339, </volume> <year> 1989. </year>
Reference-contexts: However, the performance of the systems based upon the composite features was not significantly improved. Furthermore, to a certain extent, the use of composite features results in the curse of dimensionality problem. In particular, the problem becomes quite serious when the techniques of neural computing with time-delay <ref> [6, 8, 9, 13, 62] </ref> are used. On the other hand, several kinds of classifiers have been also applied in speaker identification [9, 18, 24, 28, 49, 63].
Reference: 63. <author> R. Wohlford, E. Wrench Jr., and B. Lamdel. </author> <title> A comparison of four techniques for automatic speaker recognition. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1980. </year> <month> 908911. </month>
Reference-contexts: In particular, the problem becomes quite serious when the techniques of neural computing with time-delay [6, 8, 9, 13, 62] are used. On the other hand, several kinds of classifiers have been also applied in speaker identification <ref> [9, 18, 24, 28, 49, 63] </ref>. These classifiers include distance classifiers [3, 4, 25, 33, 42], neural network classifiers [6, 7, 8, 11, 12, 13, 14, 19, 32, 46, 47, 54] and classifiers based upon parametric or non-parametric density estimation [28, 29, 52, 57, 59]. <p> As a result, several features have already been investigated <ref> [4, 21, 22, 28, 34, 63] </ref>. The main outcome of the many feature selection studies was that features which represent pitch and the speech spectrum were the most effective for speaker identification. However, there is less agreement on which parameterization of the speech spectrum to use for features.
Reference: 64. <author> J. Wolf. </author> <title> Efficient acoustic parameters for speaker recognition. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 51(6):20442056, </volume> <year> 1972. </year>
Reference-contexts: upon the voting principle and the test set or Set-3 is composed of 5 sentences recorded in the third session for testing both individual classifiers and combination methods described in the paper. 5.1.2 Feature Selection Although Wolf outlined a set of desirable attributes on the chosen features for speaker recognition <ref> [64] </ref> more than 20 years ago, unfortunately, it is highly unlikely to find any set of features which simultaneously has all 11 those attributes in practice [4, 18, 21, 22, 24, 28, 44, 49, 52, 56]. <p> However, there is less agreement on which parameterization of the speech spectrum to use for features. Common spectrum representations for speaker identification are linear predictive coefficients and their various transformations (cepstral coefficients and PARCOR coefficients etc.) as well as the cepstrum and its variants such as Mel-scale cepstrum <ref> [4, 52, 56, 64] </ref>. As a result, we select four common features for the experiments, i.e. linear predictive coding coefficients (LPCC), linear predictive coding cepstrum (LPC-CEP), cepstrum (CEPS) and Mel-scale cepstrum (MEL-CEP) [51]. <p> On the other hand, it is generally agreed that the voiced parts of an utterance, especially vowels and nasal, are more effective than the unvoiced parts for text-independent speaker identification <ref> [4, 52, 56, 64] </ref>. In experiments, therefore, only the voiced parts of a sentence are kept regardless of their contents by using a simple energy measuring method. The length of the Hamming analysis window is 64 ms without overlapping.
Reference: 65. <author> L. Xu and M. I. Jordan. </author> <title> EM learning on a generalized finite mixture model for combining multiple classifiers. </title> <booktitle> In Proc. World Congress on Neural Networks, </booktitle> <address> San Diego, </address> <year> 1993. </year> <month> IV227IV230. </month>
Reference-contexts: In regard to the linear coefficients or weights for combination, there are two kinds of methods for assigning values to linear coefficients or weights [35], i.e. weights as veridical probabilities <ref> [36, 45, 65, 67] </ref> and minimum error weights [16, 17, 30, 50]. In the framework of winner-take-all, a device called associative switch is used in the process of classification to choose the classification result of a specific classifier for a specific input pattern [69]. <p> Unfortunately, it is difficult to use the existing techniques with weights as veridical probabilities <ref> [36, 45, 65, 67] </ref> for handling the problem since in these methods the achievement of linear coefficients usually depends upon the input (feature vector) to each classifier. <p> Recent researches show that the techniques with weights as veridical probabilities can achieve considerably good results in the applications to combination of multiple classifiers with the same feature <ref> [65, 67] </ref>. In this paper, we present a novel linear combination scheme with weights as veridical probabilities to extend the state-of-the-art techniques for combining multiple classifiers with different features. <p> Conclusions are drawn in the final section. 2 A Linear Combination Method for Different Features In this section, we present a novel method of combining multiple classifiers with different features on the basis of the work in <ref> [65, 67] </ref> which can merely combine multiple classifiers with the same feature. <p> Fortunately, these outputs can be transformed into the form in Eq.(1) using a function g (s) <ref> [65] </ref>, that is, p jk (x p j ) = P M k = 1; ; M (2) where g (s) 0. <p> Therefore, the results of combining classifiers with the feature CEPS is not reported here. In addition, it is worth noting that in the circumstance of multiple classifiers with the same feature the LIN-COM-DIF method is degenerated into the method in <ref> [65] </ref>. That is, it is just the case that there is the unique fi 1 and the value fi 1 is always one in Eq.(4). Here, we still call the method LIN-COM-DIF for consistency. Accordingly, the results of correspondency-3, correspondency-4 and correspondency-5 are shown in Table 1517, respectively.
Reference: 66. <author> L. Xu and M. I. Jordan. </author> <title> A modified gating network for the mixtures of experts architecture. </title> <booktitle> In Proc. World Congress on Neural Networks, </booktitle> <address> San Diego, </address> <year> 1994. </year> <month> II405II410. </month>
Reference-contexts: the a posteriori probabilities are obtained as follows, h i (y (t) jx i ) = E [I i jX ]; h ij (y (t) jx i ) = E [I i ; I j jX ]: (12) To simplify the computation in the Maximization step (M-step), a trick in <ref> [66, 67] </ref> is used to rewrite Eq.(4) into the following equivalent form P (y; D) = P (yjD; )P (x i ; ') = j=1 i=1 where P (x i ; ') = P N r=1 ir P (x i ; ' ir ).
Reference: 67. <author> L. Xu, M. I. Jordan, and G. E. Hinton. </author> <title> An alternative model for mixture of experts. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <pages> pages 633 640, </pages> <address> Cambridge MA, 1995. </address> <publisher> MIT Press. </publisher> <pages> 20 </pages>
Reference-contexts: In regard to the linear coefficients or weights for combination, there are two kinds of methods for assigning values to linear coefficients or weights [35], i.e. weights as veridical probabilities <ref> [36, 45, 65, 67] </ref> and minimum error weights [16, 17, 30, 50]. In the framework of winner-take-all, a device called associative switch is used in the process of classification to choose the classification result of a specific classifier for a specific input pattern [69]. <p> Unfortunately, it is difficult to use the existing techniques with weights as veridical probabilities <ref> [36, 45, 65, 67] </ref> for handling the problem since in these methods the achievement of linear coefficients usually depends upon the input (feature vector) to each classifier. <p> Recent researches show that the techniques with weights as veridical probabilities can achieve considerably good results in the applications to combination of multiple classifiers with the same feature <ref> [65, 67] </ref>. In this paper, we present a novel linear combination scheme with weights as veridical probabilities to extend the state-of-the-art techniques for combining multiple classifiers with different features. <p> Conclusions are drawn in the final section. 2 A Linear Combination Method for Different Features In this section, we present a novel method of combining multiple classifiers with different features on the basis of the work in <ref> [65, 67] </ref> which can merely combine multiple classifiers with the same feature. <p> the a posteriori probabilities are obtained as follows, h i (y (t) jx i ) = E [I i jX ]; h ij (y (t) jx i ) = E [I i ; I j jX ]: (12) To simplify the computation in the Maximization step (M-step), a trick in <ref> [66, 67] </ref> is used to rewrite Eq.(4) into the following equivalent form P (y; D) = P (yjD; )P (x i ; ') = j=1 i=1 where P (x i ; ') = P N r=1 ir P (x i ; ' ir ).
Reference: 68. <author> L. Xu, A. Krzyzak, and C. Y. Suen. </author> <title> Methods of combining multiple classifiers and their applications to handwriting recognition. </title> <journal> IEEE Trans. Sys. Man. Cybern., </journal> <volume> 23(3):418435, </volume> <year> 1992. </year>
Reference-contexts: 1 Introduction Recently, the combination of multiple classifiers has been viewed as a new direction for the development of highly reliable pattern recognition systems, in particular, optical character recognition (OCR) systems. Preliminary results indicate that combination of several complementary classifiers leads to classifiers with improved performance <ref> [5, 53, 61, 68, 69] </ref>. There are at least two reasons justifying the necessity of combining multiple classifiers. First, for almost any one of the current pattern recognition application areas, there are a number of classification algorithms available developed from different theories and methodologies. <p> These features are also represented in very diversified forms and it is rather hard to lump them together for one single classifier to make a decision. As a result, multiple classifiers are needed to deal with the different features <ref> [5, 68, 69, 71] </ref>. It also results in a general problem how to combine those classifiers with different features to yield the improved performance. <p> The basic idea underlying the combination of multiple classifiers is that a consensus is made somehow based upon the results of multiple classifiers for 1 a classification task using an elaborate combination scheme. So far, there have been extensive studies on the combination of multiple classifiers <ref> [1, 10, 20, 27, 35, 68] </ref>. Among these researches, possible solutions to the combination may be classified into three frameworks, i.e. linear opinion pools, winner-take-all and evidential reasoning. <p> In the framework of evidential reasoning, for an input pattern, the output of each individual classifier is regarded as an evidence or an event and the combination scheme makes the final decision based upon a method of evidential reasoning or a principle of voting <ref> [2, 5, 20, 26, 40, 53, 68] </ref>. The so-called different features problem refers to that there are numerous types of features which can be extracted from the same raw data for a classification task. Therefore, several different feature sets are available for a given data set. <p> Using such techniques, the considerably better classification results have been produced in the field of OCR by combining multiple classifiers with different features <ref> [5, 68, 69] </ref>. <p> = 1 when l j 0 = k and j 0 = arg max i2I D p (i) (x p i ) 0 otherwise 4 Combination Methods Based on Evidential Reasoning The combination methods based upon evidential reasoning have been extensively studied and already applied in the field of OCR <ref> [5, 53, 61, 68, 69] </ref>. The basic idea underlying the methods is that the result of each individ ual classifier is regarded as an evidence or an event and the final decision is made by consulting all combined classifiers with a method of evidential reasoning or evidence integrating. <p> The methods of evidential reasoning (integrating) are usually based upon voting principle, Bayesian theory and Dempster-Shafer evidence theory. In this section, we briefly review some methods in <ref> [68] </ref> which have been applied to the experiments of text-independent speaker identification reported in this paper. In the sequel, the original representation in [68] will be rewritten for the purpose of combining multiple classifiers with different features. 8 4.1 A Combination Method in Bayesian Formalism In order to combine multiple classifiers <p> In this section, we briefly review some methods in <ref> [68] </ref> which have been applied to the experiments of text-independent speaker identification reported in this paper. In the sequel, the original representation in [68] will be rewritten for the purpose of combining multiple classifiers with different features. 8 4.1 A Combination Method in Bayesian Formalism In order to combine multiple classifiers with different features in Bayesian formalism, the error of each classifier must be taken into consideration. <p> As a result, the error of each classifiers e j may be described by its confusion matrix <ref> [68] </ref>, P T j , as follows, P T j = 6 6 6 n 11 n 12 : : : n 1M n 1 (M+1) (j) (j) (j) (j) . . . . . . . . . . . . . . . . . . . . . <p> An integrated belief value bel () is defined according to Bayesian formula and the conditional probabilities <ref> [68] </ref> as follows, bel (i) = j=1 P (D 2 C i je j (x p j ) = l j ) i=1 j=1 P (D 2 C i je j (x p j ) = l j ) where P M i=1 bel (i) = 1 since D 2 C <p> follows, E (D) = k if bel (k) = max i2 bel (i) T M + 1 otherwise (29) where 0 &lt; T 1 is a predefined threshold. 4.2 A Combination Method Based on Dempster-Shafer Theory The Dempster-Shafer (D-S) theory of evidence [58] has been applied for combining multiple classifiers <ref> [41, 53, 68] </ref>. In the method used in our work, the combination is made in the situation that the recognition, substitution and rejection rates of each individual classifier are merely necessary as prior knowledge [68]. <p> In the method used in our work, the combination is made in the situation that the recognition, substitution and rejection rates of each individual classifier are merely necessary as prior knowledge <ref> [68] </ref>. <p> The combination in Eq.(31) can be calculated with a fast computing method in <ref> [68] </ref>. <p> If each opinion could be viewed as an evidence, then the process of making a consensus will be regarded as the process of evidential reasoning. There have been several combination methods based upon the different voting principles <ref> [61, 68] </ref>. For a sample D, each classifier e j (j = 1; ; N ) produces a result of classification based upon one kind of feature extracted from the sample D, say x p j (1 p j P ), i.e. e j (x p j ) = i. <p> The method called BAYES could also achieve the satisfactory results for all cases except the correspondency-3, which is consistent with other applications of the BAYES method <ref> [50, 68] </ref>. 6 Conclusions We have described several methods of combining multiple classifiers with different features and their application to text-independent speaker identification In particular, we classify the state-of-the-art techniques for combining multiple classifiers into three frameworks. The methods in the same framework share the similar principle for combination.
Reference: 69. <author> L. Xu, A. Krzyzak, and C. Y. Suen. </author> <title> Associative switch for combining multiple classifiers. </title> <journal> Journal of Artificial Neural Networks, </journal> <volume> 1(1):77100, </volume> <year> 1994. </year>
Reference-contexts: 1 Introduction Recently, the combination of multiple classifiers has been viewed as a new direction for the development of highly reliable pattern recognition systems, in particular, optical character recognition (OCR) systems. Preliminary results indicate that combination of several complementary classifiers leads to classifiers with improved performance <ref> [5, 53, 61, 68, 69] </ref>. There are at least two reasons justifying the necessity of combining multiple classifiers. First, for almost any one of the current pattern recognition application areas, there are a number of classification algorithms available developed from different theories and methodologies. <p> These features are also represented in very diversified forms and it is rather hard to lump them together for one single classifier to make a decision. As a result, multiple classifiers are needed to deal with the different features <ref> [5, 68, 69, 71] </ref>. It also results in a general problem how to combine those classifiers with different features to yield the improved performance. <p> In the framework of winner-take-all, a device called associative switch is used in the process of classification to choose the classification result of a specific classifier for a specific input pattern <ref> [69] </ref>. <p> In the framework of winner-take-all, the combination scheme, associative switch, could also combine multiple classifiers with different features by using the mapping or coding of different features as the input of the associative switch instead of different features themselves <ref> [69] </ref>. Using such techniques, the considerably better classification results have been produced in the field of OCR by combining multiple classifiers with different features [5, 68, 69]. <p> Using such techniques, the considerably better classification results have been produced in the field of OCR by combining multiple classifiers with different features <ref> [5, 68, 69] </ref>. <p> In this paper, we present a novel linear combination scheme with weights as veridical probabilities to extend the state-of-the-art techniques for combining multiple classifiers with different features. In addition, we also propose a modified training method for the associative switch <ref> [69] </ref> in the framework of winner-take-all in order to yield better performance. Speaker identification is the process of determining from which of the registered speaker when a given utterance comes. Furthermore, speaker identification systems can be either text-independent or text-dependent. <p> When there is no classifier to classify the sample D correctly, the combination scheme will reject it. To complete the task of choosing a classifier for each sample, a device called associative switch has been proposed <ref> [69] </ref>. <p> For this purpose, we adopt an encoding mechanism to produce a mapping or coding of the sample D , M (D), as the input of the WTA-CM. Like the encoding mechanism in <ref> [69] </ref>, here, M (D) is just the label vector consisting of N labels produced by individual classifiers i.e. M (D) = [l 1 ; ; l N ] for the sample D. <p> = 1 when l j 0 = k and j 0 = arg max i2I D p (i) (x p i ) 0 otherwise 4 Combination Methods Based on Evidential Reasoning The combination methods based upon evidential reasoning have been extensively studied and already applied in the field of OCR <ref> [5, 53, 61, 68, 69] </ref>. The basic idea underlying the methods is that the result of each individ ual classifier is regarded as an evidence or an event and the final decision is made by consulting all combined classifiers with a method of evidential reasoning or evidence integrating. <p> As a result, the best identifying results produced by different combination methods on the optimal correspondency and the corresponding rejection thresholds are shown in Table 6. For the purpose of comparison, we have done the experiments on the optimal correspondency using the original associative switch in <ref> [69] </ref> and the proposed method in section 3. In [69], several methods were 15 proposed for producing the desired output of the MLP used for selecting a winner. In the experiments, we exhaustively used those methods in [69] and only the best identifying result is reported here. <p> For the purpose of comparison, we have done the experiments on the optimal correspondency using the original associative switch in <ref> [69] </ref> and the proposed method in section 3. In [69], several methods were 15 proposed for producing the desired output of the MLP used for selecting a winner. In the experiments, we exhaustively used those methods in [69] and only the best identifying result is reported here. <p> done the experiments on the optimal correspondency using the original associative switch in <ref> [69] </ref> and the proposed method in section 3. In [69], several methods were 15 proposed for producing the desired output of the MLP used for selecting a winner. In the experiments, we exhaustively used those methods in [69] and only the best identifying result is reported here. We call the original associative switch ASSOC-SW and the best identifying results produced by the M-ASSOC-SW and ASSOC-SW are shown in Table 7.
Reference: 70. <author> L. Xu, J. Oglesby, and J. S. Mason. </author> <title> The optimization of perceptually-based features for speaker identification. </title> <booktitle> In Proc. Int. conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1989. 520523. </year>

References-found: 70

