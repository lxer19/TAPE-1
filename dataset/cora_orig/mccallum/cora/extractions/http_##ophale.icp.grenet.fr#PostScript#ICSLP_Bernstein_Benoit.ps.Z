URL: http://ophale.icp.grenet.fr/PostScript/ICSLP_Bernstein_Benoit.ps.Z
Refering-URL: http://ophale.icp.grenet.fr/benoit.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: FOR SPEECH PERCEPTION BY HUMANS OR MACHINES, THREE SENSES ARE BETTER THAN ONE Spoken Language
Author: Lynne E. Bernstein House Christian Benot 
Address: Los Angeles, California  Grenoble, France  
Affiliation: Ear Institute  Institut de la Communication Parle Universit Stendhal  
Abstract: A growing assemblage of researchers has, in recent years, adopted methods and theories that acknowledge and exploit the multisensory nature of speech perception. This paper, which is an introduction to the special session, The Senses of Speech Perception, gives a brief historical review of research concerning the multiple senses of speech perception, discusses major issues , and suggests directions for future research . (Work supported in part by NIH grant DC00695.) 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> McGurk, H., and MacDonald, J. </author> ( <title> 1976 ). Hearing lips and seeing voices: A new illusion, </title> <booktitle> Nature 264, </booktitle> <pages> 746-748. </pages>
Reference-contexts: HISTORICAL BACKGROUND Current interest in multisensory speech perception can be attributed largely to findings reported by McGurk and MacDonald <ref> [1] </ref>. They discovered that when acoustic /b A / was presented in synchrony with a face saying /gA/, subjects reported that they heard /d A/.
Reference: 2. <author> Green, K. P. </author> ( <year> 1996). </year> <title> Studies of the McGurk effect: Implications for theories of speech perception, </title> <booktitle> these Proceedings. </booktitle>
Reference-contexts: This effect of vision on heard speech drew attention because it was unanticipated by then extant theories of speech perception and was even contrary to intuitions. Other related studies followed the initial description of the McGurk illusion [e.g., 2,3], and several different theories were offered [4,5,6]. Green <ref> [2] </ref>, in this session, presents an overview of research on the McGurk illusion. The paper by Sekiyama et al. [3], in this session, shows that the illusion is sensitive to native language, acoustic qua lity, la nguage proficiency, and cultural factors. <p> Possibilities include the s ounds of speech [26], the articulatory gestures [5], the talkers linguistic intentions [4], and the linguistic units of language [27]. Green <ref> [2] </ref>, in this session, points out that the McGurk illusion is frequently interpreted as evidence that the common metric is articulatory, and he reviews other phenomena that support either an articulatory or auditory metric. Integration implies that the perceiving system somehow registers that diverse information belongs to the same event.
Reference: 3. <author> Sekiyama, K., Tokhura, Y., and Umeda, M. </author> ( <title> 1996 ). "A few factors which affect the degree of incorporating lip-read information into speech perception", </title> <booktitle> these Proceedings. </booktitle>
Reference-contexts: Other related studies followed the initial description of the McGurk illusion [e.g., 2,3], and several different theories were offered [4,5,6]. Green [2], in this session, presents an overview of research on the McGurk illusion. The paper by Sekiyama et al. <ref> [3] </ref>, in this session, shows that the illusion is sensitive to native language, acoustic qua lity, la nguage proficiency, and cultural factors. Although highly influential, the McGurk illusion s hould not be mistaken as the seminal research on crossmodal speech effects.
Reference: 4. <author> Liberman, A. M., and Mattingly, I. G. </author> ( <title> 1985 ), "The motor theory of speech perception revised, </title> <type> Cognition 21 , 1-36. </type>
Reference-contexts: His inventory of common metrics for intersensory integration is similar to the inventory of positions taken by theorists in speech perception concerning what are the objects of speech perception. Possibilities include the s ounds of speech [26], the articulatory gestures [5], the talkers linguistic intentions <ref> [4] </ref>, and the linguistic units of language [27]. Green [2], in this session, points out that the McGurk illusion is frequently interpreted as evidence that the common metric is articulatory, and he reviews other phenomena that support either an articulatory or auditory metric.
Reference: 5. <author> Fowler, C. A. </author> ( <title> 1986 ). "An event approach to the study of speech perception from a direct realist perspective," </title> <journal> J. </journal> <volume> Phonet. </volume> <pages> 14 , 3-28. </pages>
Reference-contexts: His inventory of common metrics for intersensory integration is similar to the inventory of positions taken by theorists in speech perception concerning what are the objects of speech perception. Possibilities include the s ounds of speech [26], the articulatory gestures <ref> [5] </ref>, the talkers linguistic intentions [4], and the linguistic units of language [27]. Green [2], in this session, points out that the McGurk illusion is frequently interpreted as evidence that the common metric is articulatory, and he reviews other phenomena that support either an articulatory or auditory metric.
Reference: 6. <author> Massaro, D. W. </author> ( <title> 1987 ). Speech perception by ear and eye: A paradigm for psychological inquiry (Lawrence Erlbaum Associates, </title> <publisher> London). </publisher>
Reference: 7. <author> Sumby, W. H., and Pollack, I. </author> ( <title> 1954 ). Visual contribution to speech intelligibility in noise, </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 26, </volume> <pages> 212-215. </pages>
Reference: 8. <author> Miller, G. A., Heise, G. A., and Lichten, W. </author> <year> (1951). </year> <title> The intelligibility of speech as a function of the context of the test materials, </title> <journal> J. Exp. Psychol. </journal> <volume> 41, </volume> <pages> 329-335. </pages>
Reference: 9. <author> O'Neill, J. J. </author> ( <year> 1954). </year> <title> Contributions of the visual components of oral symbols to speech comprehension, </title> <journal> J. Speech Hear. Disord. </journal> <volume> 19, </volume> <pages> 429-439. </pages>
Reference: 10. <author> Erber, N. P. </author> <year> (1969). </year> <title> Interaction of audition and vision in the recognition of speech stimuli, </title> <journal> J. Speech Hear. </journal> <pages> Res . 14 , 848-857. </pages>
Reference: 11. <author> Benot, C., Mohamadi, T., and Kandel, S. </author> ( <title> 1994 ). "Effects of phonetic context on the audiovisual intelligibility of French, </title> <journal> J. Speech Hear. </journal> <pages> Res . 37 , 1195-1203. </pages>
Reference: 12. <editor> Jeffers, J., and Barley, M. ( 1971 ). Speechreading (Lipreading) (C. C. Thomas, Springfield, </editor> <address> IL). </address>
Reference: 13. <author> Erber, N. P., and McMahan, D. A. </author> ( <title> 1976 ). Effects of sentence context on recognition of words through lipreading by deaf children, </title> <journal> J. Speech Hear. </journal> <pages> Res . 19 , 112-119. </pages>
Reference: 14. <author> Nitchie, E. B. </author> ( <year> 1916). </year> <title> "The use of homophenous words," </title> <journal> Volta Rev. </journal> <pages> 18 , 85-93. </pages>
Reference: 15. <author> Utley, J. </author> ( <year> 1946). </year> <title> A test of lip reading ability, </title> <journal> J. Speech Hear. Disord. </journal> <volume> 11, </volume> <pages> 109-116. </pages>
Reference: 16. <author> Gault, R. H. </author> ( <year> 1924). </year> <title> Progress in experiments on tactual interpretation of oral speech, </title> <journal> Soc. Psychol. </journal> <volume> 14, </volume> <pages> 155-159. </pages>
Reference-contexts: Initial research in this area predated McGurk, in some cases by more than 50 years [12,13,14,15]. Finally, research on sensory supplements or subs titutes for use by individuals with profound hearing loss is the precedent for investigations of speech perception via touch and dates to the early twentieth century <ref> [16] </ref>. Reed [17], in this session, discusses Tadoma, a method used by a small number of deaf-blind people to perceive speech through their hands. Developments in artificial sensory substitution have afforded further evidence that speech can be perceived via touch [18,19,20,21,22].
Reference: 17. <author> Reed, C. M. </author> ( <year> 1996). </year> <title> The implications of the Tadoma method of speechreading for spoken language processing, </title> <booktitle> these Proceedings. </booktitle>
Reference-contexts: Finally, research on sensory supplements or subs titutes for use by individuals with profound hearing loss is the precedent for investigations of speech perception via touch and dates to the early twentieth century [16]. Reed <ref> [17] </ref>, in this session, discusses Tadoma, a method used by a small number of deaf-blind people to perceive speech through their hands. Developments in artificial sensory substitution have afforded further evidence that speech can be perceived via touch [18,19,20,21,22]. <p> Campbell [32], in this session, presents evidence that visual-alone speech perception engages somewhat different visual areas of the brain than does audiovisual speech perception. Her studies with brain-lesioned patients suggest the possibility that the architecture for processing visible speech differs from that for acoustic speech. Reed <ref> [17] </ref>, in this session, reports on studies of experienced deaf-blind users of Tadoma. Reed and colleagues [33] estimated that Tadoma performance with connected speech was equivalent to listeners' performance with sentences in speech-to-noise conditions of 0 dB, that is, approximately 70% words correct in sentences.
Reference: 18. <author> Sherrick, C. E. </author> ( <year> 1984). </year> <title> Basic and applied research on tactile aids for deaf people: Progress and prospects, </title> <journal> J. Acoust. Soc. Am. </journal> <pages> 75 , 1325-1342. </pages>
Reference: 19. <author> Summers, I. R. (Ed.) </author> <year> (1992). </year> <title> Tactile Aids for the Hearing Impaired (London, </title> <publisher> Whurr). </publisher>
Reference: 20. <author> Bernstein, L. E., Demorest, M. E., Coulter, D. C., and O'Connell, M. P. </author> <year> (1991). </year> <title> Lipreading sentences with vibrotactile vocoders: Performance of normal-hearing and hearing-impaired subjects, </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 90, </volume> <pages> 2971-2984. </pages>
Reference: 21. <author> Waldstein, R. S., and Boothroyd, A. </author> ( <year> 1995). </year> <title> Speechreading supplemented by single-channel and multichannel tactile displays of voice f undamental frequency, </title> <journal> J. Speech Hear. Res. </journal> <volume> 38, </volume> <pages> 690-705. </pages>
Reference: 22. <author> Weisenberger, J. M., Broadstone, S. M. and Saunders, F. </author> <year> (1989), </year> <title> Evaluation of two mu ltichannel tactile aids for the hearing impaired, </title> <journal> J. Acoust. Soc. </journal> <volume> Am . 86, </volume> <pages> 1764-1775. </pages>
Reference: 23. <author> Breeuwer, M., and Plomp, R. </author> ( <title> 1986 ). Speechreading supplemented with auditorily presented speech parameters, </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 79, </volume> <pages> 481-499. </pages>
Reference: 24. <author> McGrath, M. and Summerfield, Q. </author> ( <year> 1985). </year> <title> "Intermodal timing relations and audiovisual speech rec ognition by normal hearing adults, </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 77, </volume> <pages> 676-685. </pages>
Reference: 25. <author> Summerfield, Q. </author> ( <year> 1987). </year> <title> "Some preliminaries to a comprehensive account of audiovisual speech perception, in Hearing by eye: The psychology of lipreading, edited by B. </title> <editor> Dodd and R. Campbell, </editor> <publisher> (Lawrence Erlbaum Associates, </publisher> <pages> London) pp. 3-51. </pages>
Reference-contexts: Theoretical explanations are needed for superadditivity, as well as for enhanced speech perception in noise, the McGurk illusion, and visual-tactile speech perception. Each of these i nvolves intersensory information organization and integration. Summerfield <ref> [25] </ref> argued that integration of speech information occurs prior to linguistic categorization, and he suggested alternative common metrics as terms in the integration calculus. <p> Also, lip dynamics can be evaluated with techniques such as optical flow analysis. Successful automatic lipreaders will certainly integrate all these techniques together in a hybrid approach that will weight processors of color, lighting, shape, and movement. Optical-acoustic integration is a major challenge for machine speech processing. Following Summerfield <ref> [25] </ref>, several models have been proposed and evaluated for automatic integration [40,41]. It is possible that speech information transmitted through various channels (e.g., a microphone, a camera, or a "face-glove") to a machine must be recoded into an "amodal" representation.
Reference: 26. <author> Diehl, R., and Kluender, K. </author> ( <year> 1989). </year> <title> On the objects of speech perception, </title> <journal> Ecol. Psychol. </journal> <volume> 1, </volume> <pages> 121-144. </pages>
Reference-contexts: His inventory of common metrics for intersensory integration is similar to the inventory of positions taken by theorists in speech perception concerning what are the objects of speech perception. Possibilities include the s ounds of speech <ref> [26] </ref>, the articulatory gestures [5], the talkers linguistic intentions [4], and the linguistic units of language [27].
Reference: 27. <author> Remez, R. E. </author> ( <year> 1996). </year> <title> Critique: Auditory form and gestural topology in the perception of speech, </title> <journal> J. Acoust. Soc. Am. </journal> <pages> 99 , 1695-1698. </pages>
Reference-contexts: Possibilities include the s ounds of speech [26], the articulatory gestures [5], the talkers linguistic intentions [4], and the linguistic units of language <ref> [27] </ref>. Green [2], in this session, points out that the McGurk illusion is frequently interpreted as evidence that the common metric is articulatory, and he reviews other phenomena that support either an articulatory or auditory metric.
Reference: 28. <author> Remez, R. E. </author> ( <year> 1996). </year> <title> "Perceptual organization of speech in one and several modalities: Common functions, common resources," </title> <booktitle> these Proceedings. </booktitle>
Reference-contexts: Integration implies that the perceiving system somehow registers that diverse information belongs to the same event. Remez <ref> [28] </ref>, in this session, argues that intersensory integration shares characteristics with intrasensory integration. He proposes that the organization of speech information is not achieved by a cognitive decision process nor by similarity principles of Gestalt psychology but rather relies on perception of informational coherency across sensory systems. <p> Reported high correlations between 3D facial marker positions and RMS amplitude of the speech acoustic signals are evidence that speech signals afford characteristics that could induce the intersensory coherency Remez <ref> [28] </ref> predicts. A possible implication of the Vatikiotis-Bateson et al. results is that a common metric is not required for intersensory integration, if temporal alignment is maintained across sources of information. Pisoni et al. [31], in this session, investigated whether audiovisual speech processing enhances and/or diminishes memory capacity.
Reference: 29. <author> Meredith, M. A., & Stein, B. E. </author> ( <title> 1986 ). Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory integration, </title> <journal> J. Neurophysiol. </journal> <volume> 56, </volume> <pages> 640-662. </pages>
Reference: 30. <author> Vatikiotis-Bateson, E., Munhall, K. G., Kasahara, Y., Garcia, F., and Yehia, H. </author> ( <year> 1996). </year> <title> Characterizing audiovisual information during speech, </title> <booktitle> these Proceedings. </booktitle>
Reference-contexts: Acceptance of this proposal implies a search for the perceptual principals [see 29] and/or formal expressions for intersensory coherency. Vatikiotis-Bateson et al. <ref> [30] </ref>, in this session, discuss several quantitative analyses that appear to reflect coherency across data from video recordings of talkers faces, 3D facial marker positions, speech acoustics, and EMG recordings from perioral muscle activity.
Reference: 31. <author> Pisoni, D. B., Saldana, H. M., and Sheffert, S. M. </author> <year> (1996). </year> <title> Multimodal encoding of speech in memory: A first report, </title> <booktitle> these Proceedings. </booktitle>
Reference-contexts: A possible implication of the Vatikiotis-Bateson et al. results is that a common metric is not required for intersensory integration, if temporal alignment is maintained across sources of information. Pisoni et al. <ref> [31] </ref>, in this session, investigated whether audiovisual speech processing enhances and/or diminishes memory capacity. They assume the integration process and ask how it affects higher-level processing. Their results suggest that intersensory integration is achieved at a cost to memory span, although it enhances long-term storage. <p> That several of the individuals studied had little if any auditory experience can be seen as an existence proof for speech perception via vision alone. Results from congenitally deaf and deaf-blind individuals strongly suggest that speech perception is potentially moda lity independent. The paper by Pisoni et al. <ref> [31] </ref> is, however, a caution against a premature leap in this direction, given their findings that modality-specific information is carried forward into long-term memory.
Reference: 32. <author> Campbell, R. </author> ( <year> 1996). </year> <title> Seeing speech in space and time: Psychological and neurological findings, </title> <booktitle> these Proceedings. </booktitle>
Reference-contexts: One source of modality-specific effects is the localization of processing in different brain structures as a function of the site of sensory stimulation. Campbell <ref> [32] </ref>, in this session, presents evidence that visual-alone speech perception engages somewhat different visual areas of the brain than does audiovisual speech perception. Her studies with brain-lesioned patients suggest the possibility that the architecture for processing visible speech differs from that for acoustic speech.
Reference: 33. <author> Reed, C. M., Rabinowitz, W. M., Durlach, N. I., Delhorne, L. A., Braida, L. D., Pemberton, J. C., Mulcahey, B. D., and Washington, D. L. </author> ( <year> 1992). </year> <title> Analytic study of the Tadoma method: Improving performance through the use of supplementary tactual displays, </title> <journal> J. Speech Hear. Res. </journal> <volume> 35, </volume> <pages> 450-465. </pages>
Reference-contexts: Her studies with brain-lesioned patients suggest the possibility that the architecture for processing visible speech differs from that for acoustic speech. Reed [17], in this session, reports on studies of experienced deaf-blind users of Tadoma. Reed and colleagues <ref> [33] </ref> estimated that Tadoma performance with connected speech was equivalent to listeners' performance with sentences in speech-to-noise conditions of 0 dB, that is, approximately 70% words correct in sentences. Their results are seen as an existence proof for speech perception via touch alone.
Reference: 34. <author> Bernstein, L. E., Demorest, M. E., and Tucker, P. E. </author> ( <title> 1996 ). Speech perception without hearing, </title> <note> manuscript in preparation. </note>
Reference-contexts: Their results are seen as an existence proof for speech perception via touch alone. Bernstein, Demorest and Tucker <ref> [34] </ref> have studied highly accurate visual speech perception in adults with profound congenital hearing losses. S imilarly to the expert Tadoma users, subjects accuracy rates for words in sentences hovered near 80% words correct. <p> The paper by Pisoni et al. [31] is, however, a caution against a premature leap in this direction, given their findings that modality-specific information is carried forward into long-term memory. Furthermore, the fact that lipreading proficiency varies more widely in both hearing and deaf populations <ref> [34] </ref> than does auditory speech perception suggests that modality does matter in ways that need to be specified. 5. TECHNOLOGICAL ISSUES It has long been a human dream to create machines able to speak and/or to understand speech.
Reference: 35. <author> Petajan, E. </author> ( <year> 1984). </year> <title> Automatic Lipreading to Enhance Speech Recognition, </title> <institution> PhD thesis (University of Illinois at Urbana). </institution>
Reference: 36. <author> Brooke, M., and Petajan, E. </author> ( <year> 1986). </year> <title> Seeing speech: Investigations into the synthesis and recognition of visible speech movements using automatic image processing and computer graphics, </title> <booktitle> Proc. Int. Conf. Speech I/O, Techniques & Applications (London) pp. </booktitle> <pages> 104-109. </pages>
Reference: 37. <author> Brooke, N. M. </author> ( <year> 1996). </year> <title> Using the visual component in automatic speech recognition, </title> <booktitle> these Proceedings. </booktitle>
Reference-contexts: At the same time, real-time image processing is now accessible on many computers: small lightweight micro-cameras are increasingly common, so that it is now feasible to design automatic lipreaders. As discussed by Brooke <ref> [37] </ref>, there are two main areas of research in the design of audiovisual speech recognizers - extraction of optical characteristics and integration of information both addressing basic questions relevant to the human ability to process speech bimodally.
Reference: 38. <author> Finn, K. E. </author> ( <year> 1986). </year> <title> An Investigation of Visible Lip Information to be Used in Automated Speech Recognition, </title> <type> PhD thesis (Georgetown University, </type> <address> Washington, DC). </address>
Reference: 39. <author> Stork, D., and Hennecke, M., </author> <title> Editors ( 1996). Speechreading by Humans and Machines , NATO-ASI Series F: </title> <journal> Computer and System Sciences, </journal> <volume> Vol. </volume> <publisher> 150 (Springer-Verlag, </publisher> <address> Berlin). </address>
Reference: 40. <author> Robert-Ribes, J., Schwartz, J. L., and Escudier, P. </author> <year> (1995). </year> <title> A comparison of models for fusion of the auditory and visual sensors for speech perception, </title> <journal> Artificial Intelligence Rev. </journal> <volume> 9, </volume> <pages> 323-346. </pages>
Reference-contexts: Following Summerfield [25], several models have been proposed and evaluated for automatic integration [40,41]. It is possible that speech information transmitted through various channels (e.g., a microphone, a camera, or a "face-glove") to a machine must be recoded into an "amodal" representation. Robert-Ribes et al. <ref> [40] </ref> suggested the motor (or articulatory) space as a common metric. A prerequisite to a common metric is to determine whether integration should occur early or late, that is, whether the optical and acoustical flows are processed and decoded separately or appended as a single vector.
Reference: 41. <author> Adjoudani, A., and Benot, C. </author> ( <year> 1995). </year> <title> "Audio-visual speech recogn ition compared across two architectures, </title> <booktitle> Proceedings of the Eurospeech95 Conference, Vol. 2 (Madrid, Spain), </booktitle> <pages> pp. 1563-1566. </pages>
Reference-contexts: A prerequisite to a common metric is to determine whether integration should occur early or late, that is, whether the optical and acoustical flows are processed and decoded separately or appended as a single vector. Adjoudani and Benot <ref> [41] </ref> demonstrated better performance when outputs from the optical and acoustical decoders were first processed independently and then weighted depending on their estimated reliability.
References-found: 41

