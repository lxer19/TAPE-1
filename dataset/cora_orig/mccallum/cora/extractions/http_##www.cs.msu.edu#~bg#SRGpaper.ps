URL: http://www.cs.msu.edu/~bg/SRGpaper.ps
Refering-URL: http://www.cs.msu.edu/~bg/
Root-URL: http://www.cs.msu.edu
Email: fbg, wulfekuh, punchg@cps.msu.edu  
Title: Discovering Concepts in Raw Text: Building Semantic Relationship Graphs  
Author: Boris Gelfand Marilyn Wulfekuhler William Punch Wells Hall, 
Web: http://isl.cps.msu.edu/GA  
Address: E. Lansing MI 48824  
Affiliation: Michigan State University,  
Note: Genetic Algorithms Research and Applications Group, the GARAGe A714  
Abstract: We describe a system for extracting concepts from unstructured text. We do this by clustering document words and then assembling a structure which relates these words semantically. The clustering process identifies words which co-occur across a set of documents and creates groups of words which suggest a semantic context common across the document set. This context is formalized by identifying semantic relationships between the cluster words using a lexical database to build a Semantic Relationship Graph (SRG). This SRG is a directed graph which conveys a robust representation of the sub- and super-class relationships between the correct word senses. We show how this process can be applied to a user-selected set of HTML documents; the SRGs can subsequently aid in searching the World Wide Web for documents which are similar. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> O. Etzioni, </author> <title> "The world-wide web: Quagmire or gold mine?," </title> <journal> Communications of the ACM, </journal> <volume> vol. 39, </volume> <pages> pp. 65-68, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Mining textual information presents challenges over data mining of relational or transaction databases because there are no predefined fields or features and no standard formats. However, mining information from unstructured textual resources such as the World Wide Web has great potential payoff and has received much recent attention <ref> [1] </ref>. One approach for effectively mining relevant information from raw text is based on finding common "themes" or "concepts" in a set of documents.
Reference: [2] <author> C. Fox, </author> <title> "Lexical analysis and stoplists," in Information Retrieval Data Structures and Algorithms (W. </title> <editor> B. Frakes and R. Baeza-Yates, </editor> <booktitle> Eds.), </booktitle> <pages> pp. 102-130, </pages> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: We then parse each document for words, where a word is defined as a contiguous string of letters. We ignore HTML tags, digits, punctuation, and common English words (such as the, of, and, etc) from a pre-defined stop list. Our stop list is the one given by Fox <ref> [2] </ref>. Note that we do not stem the words.
Reference: [3] <author> M. Wulfekuhler and W. Punch, </author> <title> "Finding salient features for personal web page categories," </title> <booktitle> in Sixth International World Wide Web Conference, </booktitle> <address> (Santa Clara, CA), </address> <month> April </month> <year> 1997. </year>
Reference-contexts: In our previous work reported in <ref> [3] </ref>, the value M i;j represented the number of times word j occurred in document i. Using occurrence counts as values without normalization has the effect of lending more emphasis to longer documents when computing distance between vectors. However, a pure normalization for document length loses important contextual information. <p> Using occurrence counts as values without normalization has the effect of lending more emphasis to longer documents when computing distance between vectors. However, a pure normalization for document length loses important contextual information. A full discussion of normalization issues can be found in <ref> [3] </ref>. Instead of the actual count of word occurrences, we currently use a modified ordinal number ranking system within a single document where 0 values and ties are allowed.
Reference: [4] <author> J. A. Hartigan and M. A. Wong, </author> <title> "A k-means clustering algorithm," </title> <journal> Applied Statistics, </journal> <volume> vol. 28, </volume> <pages> pp. 100-108, </pages> <year> 1979. </year>
Reference-contexts: We have found that this mitigates the problem of long documents getting more emphasis, yet still retains the important relationships between occurrences of words which make unusual contexts stand out from common usages. To group the features, we used Hartigan's K-means partitional clustering algorithm <ref> [4] </ref> as implemented by S-PLUS 1 , where K points are chosen randomly as the means of K groups in the n dimensional space. Each of the d column vectors is then assigned to the group whose mean is closest in Euclidean distance.
Reference: [5] <author> G. Miller, </author> <title> "Wordnet: A lexical database for english," </title> <journal> Communications of the ACM, </journal> <pages> pp. 39-41, </pages> <month> Nov </month> <year> 1995. </year>
Reference-contexts: The knowledge base that we have selected to use is WordNet 3 , a lexical database consisting of cross-referenced sub- and super-class relationships for most common nouns and verbs <ref> [5] </ref>. Sub- and super-class relationships are a natural way of relating concepts, and provide for much greater flexibility than a dictionary or synonym-based thesaurus. WordNet also provides different senses of a word.
Reference: [6] <author> Lycos, </author> <note> http://www.lycos.com. </note>
Reference-contexts: Current web searching is based on traditional information retrieval techniques and is typically based on boolean operations of keywords. Major web search services such as Lycos <ref> [6] </ref>, Alta Vista [7], WebCrawler [8], and Open Text [9] employ software robots (also called spiders) which traverse the web and create an index based on the full text of the documents they find.
Reference: [7] <institution> Alta Vista, </institution> <note> http://altavista.digital.com. </note>
Reference-contexts: Current web searching is based on traditional information retrieval techniques and is typically based on boolean operations of keywords. Major web search services such as Lycos [6], Alta Vista <ref> [7] </ref>, WebCrawler [8], and Open Text [9] employ software robots (also called spiders) which traverse the web and create an index based on the full text of the documents they find.
Reference: [8] <author> WebCrawler, </author> <note> http://www.webcrawler.com. </note>
Reference-contexts: Current web searching is based on traditional information retrieval techniques and is typically based on boolean operations of keywords. Major web search services such as Lycos [6], Alta Vista [7], WebCrawler <ref> [8] </ref>, and Open Text [9] employ software robots (also called spiders) which traverse the web and create an index based on the full text of the documents they find.
Reference: [9] <institution> Open Text, </institution> <note> http://index.opentext.net. 10 </note>
Reference-contexts: Current web searching is based on traditional information retrieval techniques and is typically based on boolean operations of keywords. Major web search services such as Lycos [6], Alta Vista [7], WebCrawler [8], and Open Text <ref> [9] </ref> employ software robots (also called spiders) which traverse the web and create an index based on the full text of the documents they find.
References-found: 9

