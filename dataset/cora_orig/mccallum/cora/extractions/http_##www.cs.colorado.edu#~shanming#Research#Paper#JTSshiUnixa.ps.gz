URL: http://www.cs.colorado.edu/~shanming/Research/Paper/JTSshiUnixa.ps.gz
Refering-URL: http://www.cs.colorado.edu/~shanming/Research/paper.html
Root-URL: http://www.cs.colorado.edu
Email: shi shanming@jpmorgan.com  aweigend@stern.nyu.edu  
Phone: Tel: 212-648-1760  
Title: Density Forecasting Using Hidden Markov Experts  
Author: Shanming Shi J. P. Morgan Andreas S. Weigend Leonard N. Stern 
Web: www.cs.colorado.edu/~shanming  www.stern.nyu.edu/~aweigend  
Address: 60 Wall Street, New York, NY 10260  New York University 44 West Fourth Street, MEC 9-74 New York, NY 10012, USA  
Affiliation: Co. Inc.  Department of Information Systems  School of Business  
Abstract-found: 0
Intro-found: 1
Reference: <author> Bates, J. M. and Granger, C. W. J. </author> <year> (1969). </year> <title> "The combination of forecasts", </title> <journal> Operations Research Quarterly 20: </journal> <pages> 451-468. </pages>
Reference-contexts: Furthermore, 1 The key idea gated experts share with hidden Markov experts can be clarified in comparison to the standard idea of combining predictive models obtained on different information sets. These individual models usually weigh all their training points equally <ref> (Bates and Granger 1969, Granger 1989) </ref>. 3 we also need to compare hidden Markov experts with models that do not assume an underlying hidden Markov process but otherwise differ as little as possible. The natural comparison is between hidden Markov experts and gated experts.
Reference: <author> Baum, L. E. </author> <year> (1972). </year> <title> "An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov process", </title> <booktitle> Inequalities 3: </booktitle> <pages> 1-8. </pages>
Reference: <author> Baum, L. E. and Eagon, J. A. </author> <year> (1963). </year> <title> "An inequality with applications to statistical prediction for functions of Markov processes and to a model for ecology", </title> <journal> Bull. Amer. Math. Soc. </journal> <volume> 73: </volume> <pages> 360-363. </pages> <note> 17 Baum, </note> <author> L. E., Petrie, T., Soules, G. and Weiss, N. </author> <year> (1970). </year> <title> "A maximization technique ocurring in the statistical analysis of probabilistic functions of Markov chains", </title> <journal> Annals of Mathematical Statistics 41: </journal> <pages> 164-171. </pages>
Reference-contexts: This is expressed as a matrix of transition probabilities between the hidden states. We do not know these transition probabilities either; they also have to be estimated from the data. Fortunately, the statistically solid framework of hidden Markov models <ref> (Baum and Eagon 1963) </ref> provides algorithms to estimate the unknown quantities.
Reference: <author> Chatfield, C. </author> <year> (1993). </year> <title> "Calculating interval forecasts", </title> <journal> Journal of Business and Economics Statistics 11: </journal> <pages> 121-135. </pages>
Reference-contexts: Moreover, in this paper, we use the model in conditional density forecasting instead of point forecasting (Shi and Weigend 1997). Density forecasting is an essential tool in risk management. However, while point forecasting is more common in forecasting literature, a few studies discussed interval forecasts <ref> (Chatfield 1993, Christoffersen 1997) </ref> and probability forecasts (Clemen and Winkler 1995, Murphy and Winkler 1992). Diebold et al. (1998) suggest several reasons for the negligence: the difficulty of distribution assumption, the difficulty in evaluation, and the lack of demands from practice.
Reference: <author> Christoffersen, P. </author> <year> (1997). </year> <title> "Evaluating interval forecasts", </title> <journal> International Economic Review, Forthcoming. </journal>
Reference: <author> Clemen, R.T., A. M. and Winkler, R. </author> <year> (1995). </year> <title> "Screening probability forecasts: Contrasts between choosing and combining", </title> <journal> International Journal of Forecasting 11: </journal> <pages> 133-146. </pages>
Reference-contexts: Density forecasting is an essential tool in risk management. However, while point forecasting is more common in forecasting literature, a few studies discussed interval forecasts (Chatfield 1993, Christoffersen 1997) and probability forecasts <ref> (Clemen and Winkler 1995, Murphy and Winkler 1992) </ref>. Diebold et al. (1998) suggest several reasons for the negligence: the difficulty of distribution assumption, the difficulty in evaluation, and the lack of demands from practice. Diebold et al. (1998) describe a simple way to directly evaluate the density of forecasts.
Reference: <author> Dempster, A. P., Laird, N. M. and Rubin, D. B. </author> <year> (1977). </year> <title> "Maximum likelihood from incomplete data via the EM algorithm", </title> <journal> Journal of Royal Statistical Society B 39: </journal> <pages> 1-38. </pages>
Reference: <author> Diebold, F. X., Gunther, T. A. and Tay, A. S. </author> <year> (1998). </year> <title> "Evaluating density forecasts", </title> <journal> Review of economics and statistics, Forthcoming. </journal>
Reference: <author> Durland, J. M. and McCurdy, T. H. </author> <year> (1994). </year> <title> "Duration-dependent transitions in a Markov model of u.s. gnp growth", </title> <journal> Journal of Business and Economic Statistics 12: </journal> <pages> 279-288. </pages>
Reference: <author> Engel, C. and Hamilton, J. D. </author> <year> (1990). </year> <title> "Long swings in the dollar: Are they in the data and do markets know it?", </title> <journal> The American Economic Review 80: </journal> <pages> 689-713. </pages>
Reference: <author> Ferguson, J. D. </author> <year> (1980). </year> <title> "Hidden Markov analysis: an introduction", </title> <editor> in J. D. Ferguson (ed.), </editor> <title> The Symposium on the Applications of Hidden Markov Models to Text and Speech, </title> <publisher> Princeton, </publisher> <address> NJ, </address> <pages> pp. 143-179. </pages>
Reference-contexts: From an analysis point of view, an observed time series shows the evidence about the hidden path. Therefore, in a hidden Markov model the output probabilities impose a veil <ref> (Ferguson 1980) </ref> between the states and the observer of the time series. The task for Markov modeling is to lift the veil. A Hidden Markov model is called "hidden" because these states cannot be seen directly from the observed data.
Reference: <author> Fraser, A. M. and Dimitriadis, A. </author> <year> (1994). </year> <title> "Forecasting probability densities by using hidden Markov models", </title> <editor> in A. S. Weigend and N. A. Gershenfeld (eds), </editor> <title> Time Series Prediction: Forecasting the Future and Understanding the Past, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <pages> pp. 265-282. </pages>
Reference-contexts: Hidden Markov models have been widely used in the field of speech recognition (Huang, Ariki and Jack 1990). 4 The concept of the transition among states can also be used in modeling the time dependency of regime switching <ref> (Fraser and Dimitriadis 1994) </ref>. Poritz (1982) has first shown that linear prediction analysis can be combined with hidden Markov models. Hamilton (1990) introduced the Markov switches in the context of a vector autoregression. <p> Emission parameters: In the original work, Baum et al. (1970) only estimated the unconditional density of the observations. In this research, parametrical emission models are assumed. For each individual emission model, maximizing Eq. 10 is the same as maximizing the following <ref> (Fraser and Dimitriadis 1994) </ref> G = t=1 j=1 j log P (y t jx t ; s t = j; j ) (11) where j represents the parameters of the emission model of state j. Equation 11 can be viewed as a cost function for the emission model.
Reference: <author> Granger, C. W. J. </author> <year> (1989). </year> <title> "Combining forecasts-Twenty years later", </title> <journal> Journal of Forecasting 8: </journal> <pages> 167-173. </pages>
Reference: <author> Gray, S. F. </author> <year> (1996). </year> <title> "Modeling the conditional distribution of interest rates as a regime-switching process", </title> <journal> Journal of Financial Economics 42: </journal> <pages> 27-62. </pages>
Reference: <author> Hamilton, J. D. </author> <year> (1990). </year> <title> "Analysis of time series subject to changes in regime ", Journal of Econometrics 45: </title> <type> 39-70. </type>
Reference: <author> Hamilton, J. D. and Susmel, R. </author> <year> (1994). </year> <title> "Autoregressive conditional heteroskedasticity and changes in regime", </title> <journal> Journal of Econometrics 64: </journal> <pages> 307-333. </pages>
Reference: <author> Harvey, A. C. </author> <year> (1991). </year> <title> Forecasting, Structural Time Series Models and the Kalman Filter, </title> <address> Cambridge, U.K. </address>
Reference-contexts: Both the states and the observed process can be either discrete or continuous. In speech recognition, the states and the observations are both discrete. In state space models, both states and observations are continuous <ref> (Harvey 1991, Timmer and Weigend 1997) </ref>. The discussed Hidden Markov experts use discrete states (corresponding to the regimes) and continuous observations (corresponding to the time series).
Reference: <author> Huang, X. D., Ariki, Y. and Jack, M. A. </author> <year> (1990). </year> <title> Hidden Markov Models for Speech Recognition, </title> <publisher> Edinburgh University Press, Edinburgh. </publisher>
Reference-contexts: Baum and Eagon (1963) solved this problem for hidden Markov models with discrete observation densities. Baum, Petrie, Soules and Weiss (1970) extend the algorithm to many of the classical distributions. Hidden Markov models have been widely used in the field of speech recognition <ref> (Huang, Ariki and Jack 1990) </ref>. 4 The concept of the transition among states can also be used in modeling the time dependency of regime switching (Fraser and Dimitriadis 1994). Poritz (1982) has first shown that linear prediction analysis can be combined with hidden Markov models.
Reference: <author> Jocobs, R. A., Jordan, M. I., Nowlan, S. J. and Hinton, G. E. </author> <year> (1991). </year> <title> "Adaptive mixtures of local experts", </title> <booktitle> Neural Computation 3: </booktitle> <pages> 79-87. </pages>
Reference-contexts: Previous work introduced a model class called mixture of experts <ref> (Jocobs, Jordan, Nowlan and Hinton 1991) </ref>, gated experts (Weigend, Mangeas and Srivastava 1995), or society of experts (Rumel-hart, Durbin, Golden and Chauvin 1996). These architectures consist of several so-called experts. The outputs of the experts are nonlinear functions of their inputs and can be interpreted as conditional means (supervised learning).
Reference: <author> Juang, B. H. </author> <year> (1984). </year> <title> "On hidden Markov model and dynamic time warping for speech recognition-a unified view", </title> <journal> AT&T BLTJ 63: </journal> <pages> 1213-1243. </pages>
Reference: <author> Lahiri, K. and Wang, J. G. </author> <year> (1994). </year> <title> "Predicting cyclical turning points with leading index in a Markov switching model", </title> <journal> Journal of Forecasting 13: </journal> <pages> 245-263. </pages> <note> 18 Liporace, </note> <author> L. A. </author> <year> (1982). </year> <title> "Maximum likelihood estimation for multivariate observations of Markov sources", </title> <journal> IEEE Trans. Inform. Theory IT-28: </journal> <pages> 729-734. </pages>
Reference: <author> Murphy, A. H. and Winkler, R. L. </author> <year> (1992). </year> <title> "Diagnostic verification of probability forecasts", </title> <journal> International Journal of Forecasting 7: </journal> <pages> 435-455. </pages>
Reference: <author> Poritz, A. B. </author> <year> (1982). </year> <title> "Linear predictive hidden Markov models and the speech signal", </title> <booktitle> Proc. </booktitle> <address> ICASSP'82, Paris, France, </address> <pages> pp. 1291-1294. </pages>
Reference: <author> Rumelhart, D. E., Durbin, R., Golden, R. and Chauvin, Y. </author> <year> (1996). </year> <title> "Backpropagation: The basic theory", </title> <editor> in P. Smolensky, M. C. Mozer and D. E. Rumelhart (eds), </editor> <booktitle> Mathematical Perspectives on Neural Networks, </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <pages> pp. 533-566. </pages>
Reference: <author> Shi, S. and Weigend, A. S. </author> <year> (1997). </year> <title> "Taking time seriously: Hidden Markov experts applied to financial engineering", </title> <booktitle> Proceedings of the 1997 IEEE/IAFE Conference on Computational Intelligence for Financial Engineering(CIFEr'97), </booktitle> <address> Piscataway, </address> <publisher> NJ:IEEE Service Center, </publisher> <pages> pp. 244-252. </pages>
Reference-contexts: Some constraints need to be imposed. We propose the model class of hidden Markov experts <ref> (Shi and Weigend 1997) </ref>. This class strikes a balance between time-ignoring regression models and fully recurrent architectures. Hidden Markov experts do take time into account explicitly, yet avoid the difficulties of fully recurrent architectures by imposing stringent constraints on the way time enters 1 . <p> By using forward-backward algorithms, hidden Markov experts are more efficient in computation than the Markov switching models. By introducing nonlinear experts, hidden Markov experts can be more flexible in modeling dynamics. Moreover, in this paper, we use the model in conditional density forecasting instead of point forecasting <ref> (Shi and Weigend 1997) </ref>. Density forecasting is an essential tool in risk management. However, while point forecasting is more common in forecasting literature, a few studies discussed interval forecasts (Chatfield 1993, Christoffersen 1997) and probability forecasts (Clemen and Winkler 1995, Murphy and Winkler 1992).
Reference: <author> Timmer, J. and Weigend, A. S. </author> <year> (1997). </year> <title> "Modeling volatility using state space models", </title> <journal> International Journal of Neural Systems. </journal>

References-found: 26

