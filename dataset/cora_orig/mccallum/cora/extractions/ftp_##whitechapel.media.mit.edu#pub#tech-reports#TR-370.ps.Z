URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-370.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: Email: firfan|sbasu|trevor|sandyg@media.mit.edu  
Title: Modeling, Tracking and Interactive Animation of Faces and Heads using Input from Video A similar
Author: Irfan Essa, Sumit Basu, Trevor Darrell, Alex Pentland 
Keyword: Facial Modeling, Facial Animation, Interactive Animation, Expressions and Gestures, Computer Vision.  
Address: Cambridge MA 02139, U.S.A.  
Affiliation: The Media Laboratory, Massachusetts Institute of Technology  
Pubnum: Perceptual Computing Section,  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 370 Appears: Proceedings of Computer Animation '96 Conference, Geneva, Switzerland, June 1996 Abstract We describe tools that use measurements from video for the extraction of facial modeling and animation parameters, head tracking, and real-time interactive facial animation. These tools share common goals but rely on varying details of physical and geometric modeling and in their input measurement system. Accurate facial modeling involves fine details of geometry and muscle coarticulation. By coupling pixel-by-pixel measurements of surface motion to a physically-based face model and a muscle control model, we have been able to obtain detailed spatio-temporal records of both the displacement of each point on the facial surface and the muscle control required to produce the observed facial motion. We will discuss the importance of this visually extracted representation in terms or realistic facial motion synthesis. Additionally, by coupling sparse, fast visual measurements with our physically-based model via an interpolation process, we have produced a real-time interactive facial animation/mimicking system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Azarbayejani, B. Horowitz, and A. Pentland. </author> <title> Recursive estimation of structure and motion using the relative orientation constraint. </title> <booktitle> In Proceedings of the Computer Vision and Pattern Recognition Conference, </booktitle> <year> 1993. </year>
Reference-contexts: Extrac 1 mechanics parameters of the skin using FEM. tion of head orientation is extremely important for human-machine interaction and for synthesis of a virtual actor with realistic head and facial motions. Azarbeyajani and Pent-land <ref> [1] </ref> present a recursive estimation method based on tracking of small facial features like the corners of the eyes or mouth. However its use of feature tracking limited its applicability to sequences in which the same points were visible over the entire image sequence.
Reference: [2] <author> Sumit Basu, Irfan Essa, and Alex Pentland. </author> <title> Motion regularization for model-based head tracking. </title> <type> Technical Report 362, </type> <institution> MIT Media Laboratory, Perceptual Computing Section, </institution> <month> January </month> <year> 1996. </year> <note> Available as MIT Media Lab Perceptual Computing Techreport # 362 from http://www-white.media.mit.edu/vismod/. </note>
Reference-contexts: This is much in the style of Horowitz and Pentland [23]. The model's 3-D location and rotation is then modified by these parameters, and used as the starting point for interpreting the next frame, and so on (see Basu, Essa and Pentland <ref> [2] </ref> for further details). Our experiments (shown below) demonstrate that this method can provide 8 (a) (b) (c) eyebrows., (d) tracking a smile and (e) a surprise expression. very robust tracking over hundreds of image frames for a very wide range of head motions.
Reference: [3] <author> M. J. Black and Y. Yacoob. </author> <title> Tracking and recognizing rigid and non-rigid facial motions using local parametric model of image motion. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 374-381. </pages> <publisher> IEEE Computer Society, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: However its use of feature tracking limited its applicability to sequences in which the same points were visible over the entire image sequence. Black and Yacoob <ref> [3] </ref> have developed a method that uses a eight parameter 2-D model for head tracking. Being inherently 2-D, this method does not allow estimation of 3-D parameters.
Reference: [4] <author> T. Darrell and A. Pentland. </author> <title> Space-time gestures. </title> <booktitle> In Proceedings of the Computer Vision and Pattern Recognition Conference, </booktitle> <year> 1993. </year> <note> Available as MIT Media Lab Perceptual Computing Techreport # 197 from http://www-white.media.mit.edu/vismod/. </note>
Reference-contexts: performed using piecewise linear interpolation implemented using a Radial Basis Function (RBF) network [28]. (We have also implemented a Gaussian RBF and obtained equivalent results.) This specific implementation with details on learning and interpolation techniques and the appearance-based method for both faces and hands is explored in much detail in <ref> [10, 4] </ref>. The RBF training process associates the set of view scores with the facial state, e.g., the motor control parameters for the corresponding expression. If we train views using the entire face as a template, the appearance of the entire face helps determine the facial state.
Reference: [5] <author> P. Ekman. </author> <title> Facial expression of emotion: An old controversy and new findings. </title> <journal> Philosophical Transactions: Biological Sciences (Series B), </journal> <volume> 335(1273) </volume> <pages> 63-69, </pages> <year> 1992. </year>
Reference-contexts: For instance, it has been observed that a major Showing the estimation and correction loop (a), the dynamics loop (b), and the feedback loop (c) (from [11]). difference between real smiles and forced or fake smiles is motion near the corner of the eye <ref> [5] </ref>. We have been able to observe and quantify the relative timing and amplitude of this near-eye motion using our system. Interactive Animation: Ideally, we would like to use the above method for interactive animation. However, the above method extracts fine-grained information and is hence far from interactive-time.
Reference: [6] <author> P. Ekman and W. V. Friesen. </author> <title> Facial Action Coding System. </title> <publisher> Consulting Psychologists Press Inc., </publisher> <address> 577 College Avenue, Palo Alto, California 94306, </address> <year> 1978. </year>
Reference-contexts: However, there is very little detailed information on the spatial and temporal patterning of human facial muscles. This lack of information about muscle coactivation has forced computer graphics researchers to either fall back on qualitative models such as FACS (Facial Action Coding System, designed by psychologists <ref> [6] </ref> to describe and evaluate facial movements), or invent their own coactivation models [39, 29]. Consequently, today's best facial modeling employs very sophisticated geometric and physical models, but only primitive models of muscle control.
Reference: [7] <author> P. Ekman, T. Huang, T. Sejnowski, and J. Hager (Editors). </author> <title> Final Report to NSF of the Planning Workshop on Facial Expression Understanding. </title> <type> Technical report, </type> <institution> National Science Foundation, Human Interaction Lab., </institution> <address> UCSF, CA 94143, </address> <year> 1993. </year>
Reference-contexts: In contrast, current systems typically use simple linear ramps to approxi mate the actuation profile. Other limitations of FACS include the inability to describe fine eye and lip motions, and the inability to describe the coarticulation effects found most commonly in speech <ref> [7, 22] </ref>. Although the muscle-based models used in computer graphics have alleviated some of these problems [33], they are still too simple to accurately describe real facial motion.
Reference: [8] <author> A. Emmett. </author> <title> Digital portfolio: Tony de peltrie. </title> <journal> Computer Graphics World, </journal> <volume> 8(10) </volume> <pages> 72-77, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Consequently, today's best facial modeling employs very sophisticated geometric and physical models, but only primitive models of muscle control. Lack of a good control model is also the limiting factor in production of extended facial animations. The best animations are still produced by artists who carefully craft key-frames <ref> [17, 8, 21] </ref>, a time-consuming and laborious process. Even though the key-frame process does not require an explicit control model, it is likely that such a model would help by providing the artist with the right animation control knobs.
Reference: [9] <author> I. Essa. </author> <title> Analysis, Interpretation, and Synthesis of Facial Expressions. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, MIT Media Laboratory, </institution> <address> Cambridge, MA 02139, USA, </address> <year> 1994. </year> <note> Available as MIT Media Lab Perceptual Computing Techreport # 303 from http://www-white.media.mit.edu/vismod/. </note>
Reference-contexts: Using these interpolation and strain-displacement matrices, the mass, stiffness and damping matrix for each element were computed and then assembled into a matrix for the whole mesh <ref> [9] </ref>. Material properties of real human skin were used in this computation. These material properties were obtained from Pieper's Facial Surgery Simulator [26] and from other studies on mechanical properties of human bodies (e.g., [37]). <p> This motion is projected onto the mesh and produces deformation of the skin. The control-feedback loop (see Figure 2) estimates the muscle control needed to produce the observed temporal and spatial patterning. Mathematical details of the model and estimation framework are described in <ref> [9] </ref>. Limitations of Existing Representations The goal of this work is to develop a tool for more accurately modeling facial motion. The current state-of-the-art for facial description (either FACS itself or muscle-control versions of FACS) have two major weaknesses: * The action units are purely local spatial patterns.
Reference: [10] <author> I. Essa, T. Darrell, and A. Pentland. </author> <title> Tracking facial motion. </title> <booktitle> In Proceedings of the Workshop on Motion of Non-rigid and Articulated Objects, </booktitle> <pages> pages 36-42. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year> <note> Available as MIT Media Lab Perceptual Computing Techreport # 272 from http://www-white.media.mit.edu/vismod/. </note>
Reference-contexts: performed using piecewise linear interpolation implemented using a Radial Basis Function (RBF) network [28]. (We have also implemented a Gaussian RBF and obtained equivalent results.) This specific implementation with details on learning and interpolation techniques and the appearance-based method for both faces and hands is explored in much detail in <ref> [10, 4] </ref>. The RBF training process associates the set of view scores with the facial state, e.g., the motor control parameters for the corresponding expression. If we train views using the entire face as a template, the appearance of the entire face helps determine the facial state.
Reference: [11] <author> I. Essa and A. Pentland. </author> <title> A vision system for observing and extracting facial action parameters. </title> <booktitle> In Proceedings of the Computer Vision and Pattern Recognition Conference, </booktitle> <pages> pages 76-83. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year> <note> Available as MIT Media Lab Perceptual Computing Techreport # 247 from http://www-white.media.mit.edu/vismod/. </note>
Reference-contexts: For instance, it has been observed that a major Showing the estimation and correction loop (a), the dynamics loop (b), and the feedback loop (c) (from <ref> [11] </ref>). difference between real smiles and forced or fake smiles is motion near the corner of the eye [5]. We have been able to observe and quantify the relative timing and amplitude of this near-eye motion using our system. <p> However, such measurements are usually noisy, and such noise can produce a chaotic physical response. Consequently an estimation and control framework needs to be incorporated into the system to obtain stable and well-proportioned results <ref> [11] </ref>. To begin analysis of a facial motion, the geometric mesh needs to be initialized and accurately fit to the face in the 3 NEUTRAL RAISE EYEBROW SMILE (a) (b) FACS-BASED (c) VIDEO-BASED (d) FACS-BASED (e) VIDEO-BASED and (e) visual measurements for raise eyebrow and smile expressions. image.
Reference: [12] <author> S. Glenn. </author> <title> VActor animation system. </title> <booktitle> In ACM SIGGRAPH Visual Proceedings, </booktitle> <pages> page 223, </pages> <institution> SimGraphics Engineering Corporation, </institution> <year> 1993. </year>
Reference-contexts: The difficulty of facial animation has sparked the interest of the research community in performance-driven animation: driving a computer animation by simply animating your own face (or an actor's face). The VACTOR system <ref> [12] </ref>, for instance, uses a physical system for measuring movement of the face. A system using infra-red cameras to track markers on a persons face has been reportedly used for several animations in movies.
Reference: [13] <author> B. K. P. Horn and B. G. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: We discuss them briefly here: Facial Modeling and Analysis: Modeling facial motion requires detailed measurement across the entire facial surface. Consequently, our facial modeling tool uses pixel-by-pixel measurements of surface motion (optical flow <ref> [13] </ref>) as input measurements. These dense motion measurements are then coupled to a physically-based face model and to a muscle control model.
Reference: [14] <author> M. Kass, A. Witkin, and D. Terzopoulos. Snakes: </author> <title> Active contour models. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1(4) </volume> <pages> 321-331, </pages> <year> 1987. </year>
Reference-contexts: Williams [41] and Litwinowitcz [16] placed marks on peoples faces, so that they could track the 3-D displacement of the facial surface from video. Terzopoulos and Waters [34] used snakes <ref> [14, 32] </ref> to track makeup-highlighted facial features, and then used the displacements of these features to passively deform (i.e., there was no interaction between the model and the measurements and the measurements just drive the model) a physically-based face model.
Reference: [15] <author> Y. Lee, D. Terzopoulos, and K. Waters. </author> <title> Realistic modeling for facial animation. </title> <booktitle> In ACM SIGGRAPH Conference Proceedings, </booktitle> <year> 1995. </year>
Reference-contexts: The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement. In facial modeling this complexity can be partially addressed by the use of sophisticated physical models of skin and muscle <ref> [33, 40, 26, 15] </ref>. However, there is very little detailed information on the spatial and temporal patterning of human facial muscles. <p> Lee, Ter-zopoulos and Waters <ref> [15] </ref> have recently shown that the can generate very detailed 3-D facial models for animation. Saulnier et al. [30] suggest a template-based method for tracking and animation.
Reference: [16] <author> P. Litwinowitcz and L. Williams. </author> <title> Animating images with drawings. </title> <booktitle> ACM SIGGRAPH Conference Proceedings, </booktitle> <pages> pages 409-412, </pages> <year> 1994. </year> <booktitle> Annual Conference Series. </booktitle>
Reference-contexts: The VACTOR system [12], for instance, uses a physical system for measuring movement of the face. A system using infra-red cameras to track markers on a persons face has been reportedly used for several animations in movies. Williams [41] and Litwinowitcz <ref> [16] </ref> placed marks on peoples faces, so that they could track the 3-D displacement of the facial surface from video.
Reference: [17] <author> N. Magnenat-Thalmann, E. Primeau, and D. Thalmann. </author> <title> Abstract muscle action procedures for face animation. </title> <journal> The Visual Computer, </journal> <volume> 3 </volume> <pages> 290-297, </pages> <year> 1988. </year> <title> 10 Frame 33 Frame 100 Frame 121 Frame 180 Frame 256 (a) Original Image Sequence [300 frames]. (b) Tracking using 3-D ellipsoidal model. Frame 12 Frame 56 Frame 75 Frame 111 Frame 131 (a) Original Image Sequence [300 frames]. (b) Tracking using 3-D ellipsoidal model. ff fi fl for tracking. The plots show the comparison for the six parameters between modeled (dotted line) and our analysis (solid line). Angles are in Radians and positions in Pixels. </title> <type> 11 </type>
Reference-contexts: Originally, researchers focused on simply being able to accurately model facial motion [20, 27, 39, 22]. As the tools for facial modeling have improved, other researchers have begun to develop methods for producing extended facial animation sequences <ref> [17, 26, 41, 33] </ref>. The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement. In facial modeling this complexity can be partially addressed by the use of sophisticated physical models of skin and muscle [33, 40, 26, 15]. <p> Consequently, today's best facial modeling employs very sophisticated geometric and physical models, but only primitive models of muscle control. Lack of a good control model is also the limiting factor in production of extended facial animations. The best animations are still produced by artists who carefully craft key-frames <ref> [17, 8, 21] </ref>, a time-consuming and laborious process. Even though the key-frame process does not require an explicit control model, it is likely that such a model would help by providing the artist with the right animation control knobs.
Reference: [18] <author> B. Moghaddam and A. Pentland. </author> <title> Face recognition using view-based and modular eigenspaces. In Automatic Systems for the Identification and Inspection of Humans, </title> <booktitle> volume 2277. SPIE, </booktitle> <year> 1994. </year>
Reference-contexts: For this we need to locate a face and the facial features in the image. To automate this process we are using the View-based and Modular Eigenspace methods of Pentland and Moghaddam <ref> [18, 25] </ref>. Using this method we can automatically extract the positions of the eyes, nose and lips in an image as shown in face image to match the canonical face mesh (Figure 3 (b) and (c)).
Reference: [19] <author> B. Moghaddam and A. Pentland. </author> <title> Probabistic visual learning for object detection. </title> <booktitle> In Proceedings of the International Conference on Computer Vision. IEEE Computer Society, </booktitle> <year> 1995. </year> <note> Available as MIT Media Lab Perceptual Computing Techreport # 326 from http://www-white.media.mit.edu/vismod/. </note>
Reference-contexts: This model-based method does not require the same features on the face to be visible over the entire length of the sequence and is stable over 2 (a) (b) described by Pentland et al. <ref> [19, 24] </ref>, using a canonical model of a face. (a) Mesh (b) Muscles and nodes (dots). extended sequences, including those with large and rapid head motions.
Reference: [20] <author> F. Parke. </author> <title> Parameterized modeling for facial animation. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 2(9) </volume> <pages> 61-68, </pages> <year> 1982. </year>
Reference-contexts: 1 Introduction The communicative power of the face makes facial modeling and animation one of the most important topics in computer graphics. Originally, researchers focused on simply being able to accurately model facial motion <ref> [20, 27, 39, 22] </ref>. As the tools for facial modeling have improved, other researchers have begun to develop methods for producing extended facial animation sequences [17, 26, 41, 33]. The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement.
Reference: [21] <author> F. I. Parke. </author> <title> Techniques of facial animation. </title> <editor> In Nadia Mag-nenat Thalmann and Daniel Thalmann, editors, </editor> <title> New Trends in Animation and Visualization, </title> <booktitle> chapter 16, </booktitle> <pages> pages 229-241. </pages> <publisher> John Wiley and Sons, </publisher> <year> 1991. </year>
Reference-contexts: Consequently, today's best facial modeling employs very sophisticated geometric and physical models, but only primitive models of muscle control. Lack of a good control model is also the limiting factor in production of extended facial animations. The best animations are still produced by artists who carefully craft key-frames <ref> [17, 8, 21] </ref>, a time-consuming and laborious process. Even though the key-frame process does not require an explicit control model, it is likely that such a model would help by providing the artist with the right animation control knobs.
Reference: [22] <author> C. Pelachaud, N. Badler, and M. Viaud. </author> <title> Final Report to NSF of the Standards for Facial Animation Workshop. </title> <type> Technical report, </type> <institution> National Science Foundation, University of Pennsyl-vania, </institution> <address> Philadelphia, PA 19104-6389, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction The communicative power of the face makes facial modeling and animation one of the most important topics in computer graphics. Originally, researchers focused on simply being able to accurately model facial motion <ref> [20, 27, 39, 22] </ref>. As the tools for facial modeling have improved, other researchers have begun to develop methods for producing extended facial animation sequences [17, 26, 41, 33]. The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement. <p> In contrast, current systems typically use simple linear ramps to approxi mate the actuation profile. Other limitations of FACS include the inability to describe fine eye and lip motions, and the inability to describe the coarticulation effects found most commonly in speech <ref> [7, 22] </ref>. Although the muscle-based models used in computer graphics have alleviated some of these problems [33], they are still too simple to accurately describe real facial motion.
Reference: [23] <author> A. Pentland and B. Horowitz. </author> <title> Recovery of nonrigid motion and structure. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 13(7) </volume> <pages> 730-742, </pages> <month> July 1r991. </month>
Reference-contexts: The unconstrained optical flow is first computed for the entire sequence, and the rigid motion of the 3-D head model that best accounts for the observed flow is interpreted as the motion of the head. This is much in the style of Horowitz and Pentland <ref> [23] </ref>. The model's 3-D location and rotation is then modified by these parameters, and used as the starting point for interpreting the next frame, and so on (see Basu, Essa and Pentland [2] for further details).
Reference: [24] <author> A. Pentland, B. Moghaddam, and T. Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Computer Vision and Pattern Recognition Conference, </booktitle> <pages> pages 84-91. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year> <note> Available as MIT Media Lab Perceptual Computing Techreport # 245 from http://www-white.media.mit.edu/vismod/. </note>
Reference-contexts: This model-based method does not require the same features on the face to be visible over the entire length of the sequence and is stable over 2 (a) (b) described by Pentland et al. <ref> [19, 24] </ref>, using a canonical model of a face. (a) Mesh (b) Muscles and nodes (dots). extended sequences, including those with large and rapid head motions.
Reference: [25] <author> A. Pentland, B. Moghaddam, and T. Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Computer Vision and Pattern Recognition Conference, </booktitle> <pages> pages 84-91. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: For this we need to locate a face and the facial features in the image. To automate this process we are using the View-based and Modular Eigenspace methods of Pentland and Moghaddam <ref> [18, 25] </ref>. Using this method we can automatically extract the positions of the eyes, nose and lips in an image as shown in face image to match the canonical face mesh (Figure 3 (b) and (c)).
Reference: [26] <author> S. Pieper, J. Rosen, and D. Zeltzer. </author> <title> Interactive graphics for plastic surgery: A task level analysis and implementation. Computer Graphics, Special Issue: </title> <booktitle> ACM Siggraph, 1992 Symposium on Interactive 3D Graphics, </booktitle> <pages> pages 127-134, </pages> <year> 1992. </year>
Reference-contexts: Originally, researchers focused on simply being able to accurately model facial motion [20, 27, 39, 22]. As the tools for facial modeling have improved, other researchers have begun to develop methods for producing extended facial animation sequences <ref> [17, 26, 41, 33] </ref>. The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement. In facial modeling this complexity can be partially addressed by the use of sophisticated physical models of skin and muscle [33, 40, 26, 15]. <p> The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement. In facial modeling this complexity can be partially addressed by the use of sophisticated physical models of skin and muscle <ref> [33, 40, 26, 15] </ref>. However, there is very little detailed information on the spatial and temporal patterning of human facial muscles. <p> Using these interpolation and strain-displacement matrices, the mass, stiffness and damping matrix for each element were computed and then assembled into a matrix for the whole mesh [9]. Material properties of real human skin were used in this computation. These material properties were obtained from Pieper's Facial Surgery Simulator <ref> [26] </ref> and from other studies on mechanical properties of human bodies (e.g., [37]). Physically-based skin model muscles were attached to this using the method of Waters and Terzopoulos [39, 33] and using muscle data from Pieper [26]. <p> These material properties were obtained from Pieper's Facial Surgery Simulator <ref> [26] </ref> and from other studies on mechanical properties of human bodies (e.g., [37]). Physically-based skin model muscles were attached to this using the method of Waters and Terzopoulos [39, 33] and using muscle data from Pieper [26]. This provides an anatomical muscle model of the face that deforms on actuation of muscles. We believe that this is an extremely detailed model for facial animation. However, this model is unable to represent wrinkles as Viaud et al. [35] models.
Reference: [27] <author> S. M. Platt and N. I. Badler. </author> <title> Animating facial expression. </title> <booktitle> ACM SIGGRAPH Conference Proceedings, </booktitle> <volume> 15(3) </volume> <pages> 245-252, </pages> <year> 1981. </year>
Reference-contexts: 1 Introduction The communicative power of the face makes facial modeling and animation one of the most important topics in computer graphics. Originally, researchers focused on simply being able to accurately model facial motion <ref> [20, 27, 39, 22] </ref>. As the tools for facial modeling have improved, other researchers have begun to develop methods for producing extended facial animation sequences [17, 26, 41, 33]. The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement.
Reference: [28] <author> T. Poggio and F. Girosi. </author> <title> A theory of networks for approximation and learning. </title> <type> Technical Report A.I. Memo No. 1140, </type> <institution> Artificial Intelligence Lab, MIT, </institution> <address> Cambridge, MA, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: If there is no match between the image and the existing expressions, an interpolated motor actuation is generated based on a weighted combination of expressions. The mapping from vision scores to motor controls is performed using piecewise linear interpolation implemented using a Radial Basis Function (RBF) network <ref> [28] </ref>. (We have also implemented a Gaussian RBF and obtained equivalent results.) This specific implementation with details on learning and interpolation techniques and the appearance-based method for both faces and hands is explored in much detail in [10, 4].
Reference: [29] <author> M. Rydfalk. CANDIDE: </author> <title> A Parameterized Face. </title> <type> PhD thesis, </type> <institution> Linkoping University, Department of Electrical Engineering, </institution> <month> Oct </month> <year> 1987. </year>
Reference-contexts: This lack of information about muscle coactivation has forced computer graphics researchers to either fall back on qualitative models such as FACS (Facial Action Coding System, designed by psychologists [6] to describe and evaluate facial movements), or invent their own coactivation models <ref> [39, 29] </ref>. Consequently, today's best facial modeling employs very sophisticated geometric and physical models, but only primitive models of muscle control. Lack of a good control model is also the limiting factor in production of extended facial animations. <p> The last column shows the generated expressions of smile using FACS and our representation. For our standard FACS implementation, we are using the CANDIDE model, which is a computer graphics model for implementing FACS motions <ref> [29] </ref>. To illustrate that the resulting parameters for facial expressions are more spatially detailed than FACS, comparisons of the expressions of raising eyebrow and smile produced by standard FACS-like muscle activations and our visually extracted muscle activations are shown in Figure 9.
Reference: [30] <author> A. Saulnier, M. L. Viaud, and D. Geldreich. </author> <title> Real-time facial analysis and synthesis chain. </title> <booktitle> In International Workshop on Automatic Face and Gesture Recogntion, </booktitle> <pages> pages 86-91, </pages> <address> Zurich, Switzerland, </address> <year> 1995. </year> <editor> Editor, M. </editor> <publisher> Bichsel. </publisher>
Reference-contexts: Lee, Ter-zopoulos and Waters [15] have recently shown that the can generate very detailed 3-D facial models for animation. Saulnier et al. <ref> [30] </ref> suggest a template-based method for tracking and animation. Such methods have the limitations of requiring initial training or initialization, are limited in the range of face and head motions they can track, and are insensitive to very fine motions.
Reference: [31] <author> E. P. Simoncelli. </author> <title> Distributed Representation and Analysis of Visual Motion. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: After the initial registering of the model to the image as shown in Figure 4, Pixel-by-pixel motion estimates (optical flow) are computed using methods of Simoncelli <ref> [31] </ref> and Wang [38]. The model on the face image tracks the motion of the head and the face correctly as long as there is not an excessive amount of head movement during an expression. Motion vectors for some of these expressions are shown in Figure 6.
Reference: [32] <author> D. Terzopoulos and R. Szeliski. </author> <title> Tracking with kalman snakes. </title> <editor> In A. Blake and A. Yuille, editors, </editor> <booktitle> Active Vision, </booktitle> <pages> pages 3-20. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Williams [41] and Litwinowitcz [16] placed marks on peoples faces, so that they could track the 3-D displacement of the facial surface from video. Terzopoulos and Waters [34] used snakes <ref> [14, 32] </ref> to track makeup-highlighted facial features, and then used the displacements of these features to passively deform (i.e., there was no interaction between the model and the measurements and the measurements just drive the model) a physically-based face model.
Reference: [33] <author> D. Terzopoulos and K. Waters. </author> <title> Physically-based facial modeling, analysis, and animation. </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> 1 </volume> <pages> 73-80, </pages> <year> 1990. </year>
Reference-contexts: Originally, researchers focused on simply being able to accurately model facial motion [20, 27, 39, 22]. As the tools for facial modeling have improved, other researchers have begun to develop methods for producing extended facial animation sequences <ref> [17, 26, 41, 33] </ref>. The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement. In facial modeling this complexity can be partially addressed by the use of sophisticated physical models of skin and muscle [33, 40, 26, 15]. <p> The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement. In facial modeling this complexity can be partially addressed by the use of sophisticated physical models of skin and muscle <ref> [33, 40, 26, 15] </ref>. However, there is very little detailed information on the spatial and temporal patterning of human facial muscles. <p> Material properties of real human skin were used in this computation. These material properties were obtained from Pieper's Facial Surgery Simulator [26] and from other studies on mechanical properties of human bodies (e.g., [37]). Physically-based skin model muscles were attached to this using the method of Waters and Terzopoulos <ref> [39, 33] </ref> and using muscle data from Pieper [26]. This provides an anatomical muscle model of the face that deforms on actuation of muscles. We believe that this is an extremely detailed model for facial animation. However, this model is unable to represent wrinkles as Viaud et al. [35] models. <p> Other limitations of FACS include the inability to describe fine eye and lip motions, and the inability to describe the coarticulation effects found most commonly in speech [7, 22]. Although the muscle-based models used in computer graphics have alleviated some of these problems <ref> [33] </ref>, they are still too simple to accurately describe real facial motion. Consequently, we have focused our efforts on characterizing the functional form of the actuation profile, and 4 (a) Raising Eyebrows (b) Happiness/Smile Expression Magnitude of Control Point Deformation AU 2 From Video.
Reference: [34] <author> D. Terzopoulus and K. Waters. </author> <title> Analysis and synthesis of facial image sequences using physical and anatomical models. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 15(6) </volume> <pages> 569-579, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: A system using infra-red cameras to track markers on a persons face has been reportedly used for several animations in movies. Williams [41] and Litwinowitcz [16] placed marks on peoples faces, so that they could track the 3-D displacement of the facial surface from video. Terzopoulos and Waters <ref> [34] </ref> used snakes [14, 32] to track makeup-highlighted facial features, and then used the displacements of these features to passively deform (i.e., there was no interaction between the model and the measurements and the measurements just drive the model) a physically-based face model.
Reference: [35] <author> M. Viaud and H. Yahia. </author> <title> Facial animation with muscle and wrinkle simulation. </title> <booktitle> In IMAGECON 1993: Second International Conference on Image Communication, </booktitle> <pages> pages 117-121, </pages> <year> 1993. </year>
Reference-contexts: This provides an anatomical muscle model of the face that deforms on actuation of muscles. We believe that this is an extremely detailed model for facial animation. However, this model is unable to represent wrinkles as Viaud et al. <ref> [35] </ref> models. Our facial modeling system functions by using optical motion measurements to drive the physical face model. However, such measurements are usually noisy, and such noise can produce a chaotic physical response.
Reference: [36] <institution> Viewpoint Data Labs. 625 S. State, </institution> <address> Orem, UT 84058, USA, +1-800-DATASET. http://www.viewpoint.com/home.shtml. </address>
Reference-contexts: The ellipsoidal model is superimposed on the image. To demonstrate the accuracy of the system's position and orientation estimates, we have compared the results to a calibrated synthetic sequence. This sequence was generated by animating a synthetic head (model courtesy of Viewpoint Data Labs Inc. <ref> [36] </ref>) using the SGI graphics libraries. The motion parameters used to drive the model were in the same format as those estimated by the system, and were obtained from running the system on a separate image sequence (not shown). <p> Acknowledgments We would like to thank Baback Moghaddam, Eero Si-moncelli, and John Y. Wang for their help. Also thanks to Viewpoint Data Labs Inc. <ref> [36] </ref> for sharing their 3-D models.
Reference: [37] <author> S. A. Wainwright, W. D. Biggs, J. D. Curry, and J. M. Gosline. </author> <title> Mechanical Design in Organisms. </title> <publisher> Princeton University Press, </publisher> <year> 1976. </year>
Reference-contexts: Material properties of real human skin were used in this computation. These material properties were obtained from Pieper's Facial Surgery Simulator [26] and from other studies on mechanical properties of human bodies (e.g., <ref> [37] </ref>). Physically-based skin model muscles were attached to this using the method of Waters and Terzopoulos [39, 33] and using muscle data from Pieper [26]. This provides an anatomical muscle model of the face that deforms on actuation of muscles.
Reference: [38] <author> J. Y. A. Wang and E. Adelson. </author> <title> Layered representation for motion analysis. </title> <booktitle> In Proceedings of the Computer Vision and Pattern Recognition Conference, </booktitle> <year> 1993. </year>
Reference-contexts: After the initial registering of the model to the image as shown in Figure 4, Pixel-by-pixel motion estimates (optical flow) are computed using methods of Simoncelli [31] and Wang <ref> [38] </ref>. The model on the face image tracks the motion of the head and the face correctly as long as there is not an excessive amount of head movement during an expression. Motion vectors for some of these expressions are shown in Figure 6.
Reference: [39] <author> K. Waters. </author> <title> A muscle model for animating three-dimensional facial expression. </title> <booktitle> ACM SIGGRAPH Conference Proceedings, </booktitle> <volume> 21(4) </volume> <pages> 17-23, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction The communicative power of the face makes facial modeling and animation one of the most important topics in computer graphics. Originally, researchers focused on simply being able to accurately model facial motion <ref> [20, 27, 39, 22] </ref>. As the tools for facial modeling have improved, other researchers have begun to develop methods for producing extended facial animation sequences [17, 26, 41, 33]. The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement. <p> This lack of information about muscle coactivation has forced computer graphics researchers to either fall back on qualitative models such as FACS (Facial Action Coding System, designed by psychologists [6] to describe and evaluate facial movements), or invent their own coactivation models <ref> [39, 29] </ref>. Consequently, today's best facial modeling employs very sophisticated geometric and physical models, but only primitive models of muscle control. Lack of a good control model is also the limiting factor in production of extended facial animations. <p> Material properties of real human skin were used in this computation. These material properties were obtained from Pieper's Facial Surgery Simulator [26] and from other studies on mechanical properties of human bodies (e.g., [37]). Physically-based skin model muscles were attached to this using the method of Waters and Terzopoulos <ref> [39, 33] </ref> and using muscle data from Pieper [26]. This provides an anatomical muscle model of the face that deforms on actuation of muscles. We believe that this is an extremely detailed model for facial animation. However, this model is unable to represent wrinkles as Viaud et al. [35] models.
Reference: [40] <author> K. Waters and D. Terzopoulos. </author> <title> Modeling and animating faces using scanned data. </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> 2 </volume> <pages> 123-128, </pages> <year> 1991. </year>
Reference-contexts: The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement. In facial modeling this complexity can be partially addressed by the use of sophisticated physical models of skin and muscle <ref> [33, 40, 26, 15] </ref>. However, there is very little detailed information on the spatial and temporal patterning of human facial muscles.
Reference: [41] <author> L. Williams. </author> <title> Performance-driven facial animation. </title> <booktitle> ACM SIGGRAPH Conference Proceedings, </booktitle> <volume> 24(4) </volume> <pages> 235-242, </pages> <year> 1990. </year> <month> 12 </month>
Reference-contexts: Originally, researchers focused on simply being able to accurately model facial motion [20, 27, 39, 22]. As the tools for facial modeling have improved, other researchers have begun to develop methods for producing extended facial animation sequences <ref> [17, 26, 41, 33] </ref>. The principle difficulty in both facial modeling and animation is the sheer complexity of human facial and head movement. In facial modeling this complexity can be partially addressed by the use of sophisticated physical models of skin and muscle [33, 40, 26, 15]. <p> The VACTOR system [12], for instance, uses a physical system for measuring movement of the face. A system using infra-red cameras to track markers on a persons face has been reportedly used for several animations in movies. Williams <ref> [41] </ref> and Litwinowitcz [16] placed marks on peoples faces, so that they could track the 3-D displacement of the facial surface from video.
References-found: 41

