URL: ftp://ftp.cgl.ucsf.edu/pub/pett/go/ladder/NeuroGo.ps.gz
Refering-URL: http://www.cgl.ucsf.edu/go/Programs/NeuroGo-PS.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: markus.enzenberger@physik.uni-muenchen.de  
Title: The Integration of A Priori Knowledge into a Go Playing Neural Network  
Author: Markus Enzenberger 
Date: September 1996  
Abstract: The best current computer Go programs are hand crafted expert systems. They are using conventional AI technics such as pattern matching, rule based systems and goal oriented selective search. Due to the increasing complexity of managing this kind of knowledge representation by hand, the playing strength of these programs is still far from human master level. This article describes methods for integrating expert Go knowledge into a learning artificial neural network. These methods are implemented in the program NeuroGo. The network learns by playing against itself using temporal difference learning and backpropagation. The expert knowledge that is implemented at present in NeuroGo is simple compared with a conventional computer Go program. Despite of this, NeuroGo is able to achieve a playing strength which is equal to a conventional program playing at a medium level.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Benson, D. </author> <year> (1976). </year> <title> Life in the Game of Go. Reprinted in: Computer Games II, </title> <editor> Levy, D. (Editor), </editor> <publisher> Springer, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Basically it is D. Benson's algorithm <ref> [1] </ref> [3] which can detect a certain class of unconditionally alive strings and territory. The extensions of this algorithm by M. Muller [3] are added. 4 Results In an experiment, the performance of NeuroGo against a conventional Go program on a 9fi9 board was tested.
Reference: [2] <author> Fotland, D. </author> <year> (1993). </year> <title> Knowledge representation in The Many Faces of Go. Manuscript available by Internet ftp://bsdserver.ucsf.edu/Go/comp/mfg.Z. </title>
Reference-contexts: The size of the hidden layer was varied between 3 and 24 neurons per unit. The test opponent was the commercially available program "The Many Faces of Go" <ref> [2] </ref> (Release 2, Revision 8.03). This program was also used by N. Schraudolph et al. [5]. They came up with a network being able to beat this program at a low level (2-3) by using it as a training partner.
Reference: [3] <author> Muller, M. </author> <year> (1995). </year> <title> Computer Go as a Sum of Local Games: An Application of Combinatorial Game Theory. </title> <type> Dissertation, </type> <institution> ETH Zurich. </institution> <note> Available by Internet ftp://ftp.inf.ethz.ch/pub/publications/dissertations/th11006.ps.gz. </note>
Reference-contexts: Basically it is D. Benson's algorithm [1] <ref> [3] </ref> which can detect a certain class of unconditionally alive strings and territory. The extensions of this algorithm by M. Muller [3] are added. 4 Results In an experiment, the performance of NeuroGo against a conventional Go program on a 9fi9 board was tested. <p> Basically it is D. Benson's algorithm [1] <ref> [3] </ref> which can detect a certain class of unconditionally alive strings and territory. The extensions of this algorithm by M. Muller [3] are added. 4 Results In an experiment, the performance of NeuroGo against a conventional Go program on a 9fi9 board was tested. The size of the hidden layer was varied between 3 and 24 neurons per unit.
Reference: [4] <author> Petterson, E. </author> <year> (1994). </year> <title> The Computer Go Ladder. Internet World Wide Web Page http://cgl.ucsf.edu/go/ladder.html. </title>
Reference-contexts: All errors in the evaluation of a string unit are multiplied with its number of stones. This is necessary for learning to predict the overall score accurately. 3 NeuroGo NeuroGo is a program which is based on the described architecture. It is participating in the Computer Go Ladder <ref> [4] </ref>. Because of the limited available computational resources and the need for patterns which are frequently occur-ing, the implemented knowledge is rather simple compared with conventional Go programs.
Reference: [5] <author> Schraudolph, N., Dayan, P., Sejnowski, T., </author> <year> (1994). </year> <title> Temporal Difference Learning of Position Evaluation in the Game of Go. </title> <note> In: Neural Information Processing Systems 6, Morgan Kaufmann 1994. Available by Internet ftp://bsdserver.ucsf.edu/Go/comp/td-go.ps.Z. </note>
Reference-contexts: This approach becomes more difficult to handle, 1 the bigger the programs become. Side effects of newly integrated knowledge and unforeseen interaction of rules are a problem. Therefore a hybrid approach which combines heuristic knowledge with machine learning is appealing. Schraudolph, Dayan and Sejnowski <ref> [5] </ref> have shown that a neural network can learn to beat a commercial Go program at a low level if special care is given to reflect the translational and colour symmetries of the pattern recognition task in the architecture of the network. <p> The size of the hidden layer was varied between 3 and 24 neurons per unit. The test opponent was the commercially available program "The Many Faces of Go" [2] (Release 2, Revision 8.03). This program was also used by N. Schraudolph et al. <ref> [5] </ref>. They came up with a network being able to beat this program at a low level (2-3) by using it as a training partner. To avoid over specialization to a single opponent, NeuroGo was trained only by playing games against itself.
Reference: [6] <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages> <note> Available by Internet ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/sutton-88.ps.gz. 8 </note>
Reference-contexts: The resulting positions are evaluated by the system. The move with the highest value is picked then. After the game has ended, the position is scored and played backwards. The training target is determined by the temporal difference algorithm TD (0) <ref> [6] </ref>: it is given by the output of the system for the position one move later or the terminal position if no moves follow. The weight changes are calculated by the backpropagation rule.
References-found: 6

