URL: ftp://ftp.cs.city.ac.uk/papers/93/sarc93-3.ps.Z
Refering-URL: http://www.cs.umd.edu/~keleher/bib/dsmbiblio/node5.html
Root-URL: 
Title: Experiences with Distributed Shared Memory  
Address: Northampton Square London EC1V 0HB  
Affiliation: Systems Architecture Research Centre Department of Computer Science City University  
Abstract: A major problem with programming systems such as distributed memory multicom-puters or networks of workstations has been the necessity for explicit, time-consuming and expensive message passing. Distributed shared memory enables such systems to appear to have a common memory though they may not physically share it. The Systems Architecture Research Centre at City University has worked on implementations of these kinds of systems and is experienced in the kinds of benefits they can offer. These are outlined for applications to fault tolerance. A DSM server implementation and its use are described. A method for using hardware to assist distributed shared memory is also described. Finally, a new architecture the Centre is studying is detailed.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Osmon, T. Stiemerling, A. Valsamidis, A. Whitcroft, Wilkinson.T., and N. Williams, </author> <title> "The Topsy project: a position paper," </title> <booktitle> in Parle '92, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: 1 Introduction The Systems Architecture Research Centre at City University (SARC) has been working at both the hardware and software levels on distributed memory multiprocessors for several years. Specifically, members of SARC have designed and implemented the Topsy multicom-puter and its associated Unix-like operating system, Meshix <ref> [1] </ref>. The motivation behind this approach lay in the Centre's belief that this was the best way of achieving true scalability in a multiprocessor system.
Reference: [2] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam, </author> <title> "The Stanford DASH multiprocessor," </title> <journal> IEEE Computer, </journal> <volume> vol. 25, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: The original implementation of Topsy provided only distributed memory but the demands for shared memory made by applications like Ingres prompted the implementation of a distributed shared memory (DSM) server. In addition, recent work on DASH at Stanford University <ref> [2] </ref> and comments by Gordon Bell [3] have reinforced the Centre's movement in this direction. The benefits of shared memory systems are becoming more apparent, especially after some work done by the Centre on fault-tolerance, large single address space systems and novel operating systems. <p> document concludes with plans for future work. 1 References are given at the end to more detailed treatment of the work | these papers are available on request. 2 An Overview of Distributed Shared Memory Distributed shared memory architectures have been developed in a number of laboratories, in the US <ref> [2] </ref> and Europe [4], out of attempts to build scalable parallel machines in the face of two constraints: * saturation of buses and physical shared memory * long latencies of rotating storage The first of these constrains physically shared memory machines to a maximum of about 20 processors.
Reference: [3] <author> G. Bell, </author> <title> "Ultracomputers: A Teraflop Before Its Time," </title> <journal> Communications of the ACM, </journal> <volume> vol. 35, </volume> <pages> pp. 27-47, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The original implementation of Topsy provided only distributed memory but the demands for shared memory made by applications like Ingres prompted the implementation of a distributed shared memory (DSM) server. In addition, recent work on DASH at Stanford University [2] and comments by Gordon Bell <ref> [3] </ref> have reinforced the Centre's movement in this direction. The benefits of shared memory systems are becoming more apparent, especially after some work done by the Centre on fault-tolerance, large single address space systems and novel operating systems.
Reference: [4] <author> E. Hagersten, </author> <title> Toward Scalable Cache Only Memory Architectures. </title> <type> PhD thesis, </type> <institution> Swedish Institute of Computer Science, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: plans for future work. 1 References are given at the end to more detailed treatment of the work | these papers are available on request. 2 An Overview of Distributed Shared Memory Distributed shared memory architectures have been developed in a number of laboratories, in the US [2] and Europe <ref> [4] </ref>, out of attempts to build scalable parallel machines in the face of two constraints: * saturation of buses and physical shared memory * long latencies of rotating storage The first of these constrains physically shared memory machines to a maximum of about 20 processors.
Reference: [5] <author> T. Stiemerling, T. Wilkinson, and A. Saulsbury, </author> <title> "Implementing DVSM on the TOPSY multicomputer," in Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS III), </title> <month> March </month> <year> 1992. </year>
Reference-contexts: generally provide higher performance, and finer-grain sharing of the DSM, for example of individual words or cache-lines. 3 The Meshix DSM Server SARC's first venture into the area of DSM was to develop an external pager for Meshix; this is currently in use and supports System V Unix-style shared memory <ref> [5] </ref>. The external pager is implemented as a user-level server which has the advantage that the server can be changed without having to re-compile the kernel. The scheme used places a user-level DSM server process on every node.
Reference: [6] <author> K. Li and P. Hudak, </author> <title> "Memory coherence in shared virtual memory systems," </title> <journal> ACM TOCS, </journal> <volume> vol. 7, </volume> <pages> pp. 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The scheme used places a user-level DSM server process on every node. This server handles the DSM requests made by processes on its local node, and communicates with the servers on the other nodes to satisfy these requests. The DSM servers use a dynamic distributed manager algorithm <ref> [6] </ref> to maintain strong coherence of the DSM. Processes which make use of the DSM access it as if it were part of their normal data space.
Reference: [7] <author> B. D. Fleisch and G. J. Popek, </author> <title> "Mirage: A Coherent Distributed Shared Memory Design," </title> <journal> ACM Operating System Review, </journal> <volume> vol. 23, no. 5, </volume> <pages> pp. 211-222, </pages> <year> 1989. </year>
Reference-contexts: This has led to the investigation of extensions to the DSM server to improve performance. A version of the server now exists which uses a delta scheme <ref> [7] </ref>. This scheme simply requires that a node retains ownership of the page for a minimum time before yielding the ownership to another node. This allows the node to perform some useful work before it is "stolen".
Reference: [8] <author> T. Wilkinson, T. Stiemerling, P. Osmon, A. Saulsbury, and P. Kelly, "Angel: </author> <title> A Proposed Multiprocessor Operating System Kernel (Extended Abstract)," </title> <booktitle> in European Workshop on Parallel Computing, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: Thus Angel was born; it was designed around a single address space maintained by DSM techniques <ref> [8] </ref>. There are three very important features in the design of Angel: * minimalism. A micro-kernel is provided which supplies only the most basic functionality | any required services or additional functionality can be built above it. * separation.
Reference: [9] <author> T. Wilkinson et al., </author> <title> "Compiling for a 64-Bit Single Address Space Architecture," </title> <type> Tech. Rep. </type> <institution> TCU/SARC/1993/1, SARC, City University Computer Science Department, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Even though a single address space system such as Angel is markedly different from a UNIX system private address space system, our work has shown that with compiler support most applications can still be run with no alterations and a very small performance penalty <ref> [9] </ref>. 4.1 The Structure of Angel Angel's address space is divided into objects, each of which is at least one page in size with no nesting of objects possible. Each object has an associated protection descriptor which specifies the access conditions for that object.
Reference: [10] <author> A. Whitcroft and P. Osmon, </author> <title> "The CBIC: Architectural Support for Message Passing or Shared Memory?," </title> <booktitle> in U.K. Performance Engineering Workshop, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: To identify the most effective and useful hardware to develop, the project evaluated the performance of current message-passing systems. As a basis for this, a detailed study of the messaging system used by Meshix was undertaken, and the results published <ref> [10] </ref>. These results showed that a message passing system has inherent costs that limit its performance, regardless of the presence of hardware support or improved bandwidth.
Reference: [11] <author> P. Osmon, </author> <title> "Memory Device." </title> <booktitle> International Patent Application PCT/GB92/01070, </booktitle> <year> 1992. </year>
Reference-contexts: SARC has also exploited a different approach to HDSM which involves the construction of a new distributed physical memory layer with moderate latency providing high bandwidth 8 optical communications. This work has led to patenting a wafer scale device containing bulk memory and high bandwidth communications switching <ref> [11] </ref>. Memory tiles on a wafer are interconnected, with optical links interconnecting wafers within the same computer and also between different computers.
Reference: [12] <author> T. Wilkinson, </author> <title> "Implementing Fault Tolerance in a 64-bit Distributed Operating System," </title> <type> Tech. Rep., </type> <institution> City University, </institution> <year> 1993. </year>
Reference-contexts: In these contexts, survival can be taken to include avoiding any data loss, in addition to correctly continuing execution without compromising security. Recent work done by Tim Wilkinson at SARC uses the information available to a DSM system to support checkpointing of individual processes with high efficiency <ref> [12] </ref>. The idea is to use the history of data exchanges supporting DSM to isolate the data which has to be checkpointed, and those nodes that will be affected if it is lost.
Reference: [13] <author> J. Singh, W.-D. Weber, and A. Gupta, </author> <title> "SPLASH: Stanford Parallel Applications for Shared-Memory," </title> <type> Tech. Rep., </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1992. </year>
Reference-contexts: Although the thesis describes only single node failures, the ideas can be extended to multiple nodes. Simulation work with standard shared-memory benchmarks (the Stanford SPLASH suite <ref> [13] </ref>) has already shown that this works very well indeed when replicated data is stored in RAM. Importantly, the checkpointing algorithm is highly parallel so its behaviour scales well with larger configurations.
References-found: 13

