URL: http://www.cs.berkeley.edu/~culler/papers/isca97.ps
Refering-URL: http://www.cs.berkeley.edu/~culler/papers/
Root-URL: 
Title: Effects of Communication Latency, Overhead, and Bandwidth in a Cluster Architecture  
Author: Richard P. Martin, Amin M. Vahdat, David E. Culler and Thomas E. Anderson 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Abstract: This work provides a systematic study of the impact of communication performance on parallel applications in a high performance network of workstations. We develop an experimental system in which the communication latency, overhead, and bandwidth can be independently varied to observe the effects on a wide range of applications. Our results indicate that current efforts to improve cluster communication performance to that of tightly integrated parallel machines results in significantly improved application performance. We show that applications demonstrate strong sensitivity to overhead, slowing down by a factor of 60 on 32 processors when overhead is increased from 3 to 103 s. Applications in this study are also sensitive to per-message bandwidth, but are surprisingly tolerant of increased latency and lower per-byte bandwidth. Finally, most applications demonstrate a highly linear dependence to both overhead and per-message bandwidth, indicating that further improvements in communication performance will continue to improve application performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Binchini, D. Chaiken, K. Johnson, D. Kranz, J. Ku biatowicz, B. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pp. 2-13, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller <ref> [1, 20, 32] </ref>, to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported
Reference: [2] <author> A. Alexandrov, M. Ionescu, K. Schauser, and C. Scheiman. LogGP: </author> <title> Incorporating Long Messages into the LogP model One step closer towards a realistic model for parallel computation. </title> <booktitle> In 7th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: This four-parameter characterization of communication performance is based on the LogP model <ref> [2, 14] </ref>, the framework for our systematic investigation of the communication design space. By adjusting these parameters, we can observe changes in the execution time of applications on a spectrum of systems ranging from the current high-performance cluster to conventional LAN based clusters. <p> Because many machines have separate mechanisms for long messages (e.g. DMA), it is useful to extend the model with an additional gap parameter, G, which specifies the time-per-byte, or the reciprocal of the bulk transfer bandwidth <ref> [2] </ref>. In our machine, G is determined by the DMA rate to or from the network interface, rather than the network link bandwidth. <p> While the programming model does not provide automatic replication with cache coherence, a number of the applications perform application-specific software caching. The language has been ported to many platforms <ref> [2, 34, 43, 44] </ref>. The sources for the applications, compiler, and communication layer can be obtained from a publically available site 1 . 3.2 Technique The key experimental innovation is to modify the communication layer so that it can emulate a system with arbitrary overhead, gap, or latency. <p> Phase 2 of the algorithm consists of entirely local disk operations. Unlike the other applications, NOW-sort performs a large amount of I/O, so can overlap communication overhead with disk accesses. * Radb: This version of the radix sort <ref> [2] </ref> was restructured to use bulk messages. After the the global histogram phase, all keys are sent to their destination processor in one bulk mes Program Avg. Msg./ Max Msg./ Msg./ Msg. Barrier Percent Percent Bulk Small Proc Proc Proc/ms Interval (s) Interval (ms) Bulk Msg. Reads Msg. KB/s Msg.
Reference: [3] <author> T.E. Anderson, D.E. Culler, D.A. Patterson, </author> <title> and the NOW Team. A Case for NOW (Networks of Workstations). </title> <journal> IEEE Micro, </journal> <volume> vol. 15, </volume> <pages> pp. 54-64, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Dash), greater opportunity for specialization (e.g. Tempest [38]), or a cleaner communication interface (e.g., T3E vs. T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters closer to that of the more tightly integrated parallel machines <ref> [3, 12, 21, 35, 43] </ref>. Moving forward from these research alternatives, a crucial question to answer is how much do the improvements in communication performance actually improve application performance. The goal of this work is to provide a systematic study of the impact of communication performance on parallel applications.
Reference: [4] <author> A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, D. E. Culler, J M. Hellerstein, and D. A. Patterson. </author> <title> High-Performance Sorting on Networks of Workstations. </title> <booktitle> In Proceedings of 1997 ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Finally, a global phase successivelymerges components between neighboring processors. The communication to computation ratio is determined by the size of the graph. * NOW-sort: The version of NOW-sort used in this study sorts records from disk-to-disk in two passes <ref> [4] </ref>. The sort is highly tuned, setting a the MinuteSort world record in 1997. The sorting algorithm contains two phases. In the first phase, each processor reads the records from disk and sends them to the final destination processor. <p> Surprisingly, the NOW-sort is also insensitive to reduced bandwidth. This version of the NOW-sort uses two disks per node. Each disk can deliver 5.5 MB/s of bandwidth <ref> [4] </ref>, and during the communication phase a single disk is used for reading and the other for writing. As Figure 8 shows, NOW-sort is disk limited.
Reference: [5] <author> R. Arpaci, D.E. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller <ref> [5, 10, 29, 41] </ref> or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, <p> [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support <ref> [5, 26, 29, 37] </ref>, supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 9401156), Sun Microsystems, California MICRO, Hewlett Packard, Intel, Mi-crosoft, and Mitsubishi. Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship.
Reference: [6] <author> E. Barton, J. Crownie, and M. McLaren. </author> <title> Message Passing on the Meiko CS-2. </title> <booktitle> In Parallel Computing, </booktitle> <volume> vol. 20, </volume> <pages> pp. 497-507, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors <ref> [6, 8, 37] </ref>, providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 9401156), Sun Microsystems, California MICRO, Hewlett Packard, Intel, Mi-crosoft, and Mitsubishi.
Reference: [7] <author> M.A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E.W. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus <ref> [7, 31] </ref>, providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 9401156), Sun Microsystems, California MICRO, Hewlett Packard, Intel, Mi-crosoft, and <p> Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship. The authors can be contacted at frmartin, vahdat, culler, teag@cs.berkeley.edu. reflective memory operations <ref> [7, 21] </ref>, and providing lean communication software layers [35, 43, 44]. Recently, we have seen a shift to designs that accept a reduction in communication performance to obtain greater generality (e.g., Flash vs. Dash), greater opportunity for specialization (e.g. Tempest [38]), or a cleaner communication interface (e.g., T3E vs. T3D).
Reference: [8] <author> N.J. Boden, D. Cohen, R.E. Felderman, A.E. Kulawik, C.L. Seitz, J.N. Seizovic, and W. </author> <title> Su. </title> <journal> MyrinetA Gigabet-per-Second Local-Area Network. IEEE Micro, </journal> <volume> vol. 15, </volume> <pages> pp. 29-38, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors <ref> [6, 8, 37] </ref>, providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 9401156), Sun Microsystems, California MICRO, Hewlett Packard, Intel, Mi-crosoft, and Mitsubishi. <p> Each has a single Myricom M2F network interface card on the SBUS, containing 128 KB SRAM card memory and a 37.5 MHz LANai processor <ref> [8] </ref>. This processor plays a key role in allowing us to independently vary LogGP parameters. The machines are interconnected with ten 8-port Myrinet switches (model M2F, 160 MB/s per port).
Reference: [9] <author> S. Borkar. </author> <title> Supporting Systolic and Memory Communication in iWarp. </title> <booktitle> In The 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 70-81, </pages> <address> Seattle, WA, USA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor <ref> [9, 11, 12, 17, 22, 23, 36, 40] </ref>, integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481,
Reference: [10] <author> E. D. Brooks III, B. C. Gorda, K. H. Warren, </author> <title> and T.S. Welcome. BBN TC2000 Architecture and Programming Models. </title>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller <ref> [5, 10, 29, 41] </ref> or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5,
Reference: [11] <author> J.B. Carter, A. Davis, R. Kuramkote, C. Kuo, L.B. Stoller, and M. Swanson. </author> <title> Avalanche: A Communication and Memory Architecture for Scalable Parallel Computing. </title> <type> Technical Report UUCS-95-022, </type> <institution> University of Utah, </institution> <year> 1995. </year>
Reference-contexts: These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor <ref> [9, 11, 12, 17, 22, 23, 36, 40] </ref>, integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, <p> The results suggest that there is considerable additional gain to be obtained by further reducing the communication overheads in clusters. Ongoing architectural efforts which integrate the network interface closer to the processor, either by placing the network interface onto the memory bus <ref> [11, 12, 38] </ref>, into the memory controller [29], or by minimizing the number of stores and loads required to launch a communication event will continue to improve application performance. Acknowledgments We would like to thank Bob Horst for motivating this study.
Reference: [12] <author> D. Chiou, B.S. Ang, Arvind, M.J. Beckerle, G.A. Boughton, R. Greiner, J.E. Hicks, and J.C. Hoe. StarT-NG: </author> <title> Delivering Seamless Parallel Computing. </title> <booktitle> In EURO-PAR'95 Conference, </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor <ref> [9, 11, 12, 17, 22, 23, 36, 40] </ref>, integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, <p> Dash), greater opportunity for specialization (e.g. Tempest [38]), or a cleaner communication interface (e.g., T3E vs. T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters closer to that of the more tightly integrated parallel machines <ref> [3, 12, 21, 35, 43] </ref>. Moving forward from these research alternatives, a crucial question to answer is how much do the improvements in communication performance actually improve application performance. The goal of this work is to provide a systematic study of the impact of communication performance on parallel applications. <p> The results suggest that there is considerable additional gain to be obtained by further reducing the communication overheads in clusters. Ongoing architectural efforts which integrate the network interface closer to the processor, either by placing the network interface onto the memory bus <ref> [11, 12, 38] </ref>, into the memory controller [29], or by minimizing the number of stores and loads required to launch a communication event will continue to improve application performance. Acknowledgments We would like to thank Bob Horst for motivating this study.
Reference: [13] <author> D.E. Culler, A.C. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pp. 262-273, </pages> <year> 1993. </year>
Reference-contexts: This processor plays a key role in allowing us to independently vary LogGP parameters. The machines are interconnected with ten 8-port Myrinet switches (model M2F, 160 MB/s per port). All but two of the applications are written in an SPMD model using Split-C <ref> [13] </ref>, a parallel extension of the C programming language that provides a global address space on distributed memory machines. Split-C (version 961015) is based on GCC (ver-sion 2.6.3) and Generic Active Messages (version 961015), which is the base communication layer throughout the study. <p> The dark line off the diagonal reflects the global histogram phase, where the ranks are accumulated across processors in a kind of pipelined cyclic shift. The grey background is the global distribution phase. Overall, the communication is frequent, write-based and balanced. * EM3D: EM3D <ref> [13] </ref> is the kernel of an application that mod els propagation of electromagnetic waves through objects in three dimensions. It first spreads an irregular bipartite graph over all processors.
Reference: [14] <author> D.E. Culler, R.M. Karp, D.A. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 262-273, </pages> <year> 1993. </year>
Reference-contexts: This four-parameter characterization of communication performance is based on the LogP model <ref> [2, 14] </ref>, the framework for our systematic investigation of the communication design space. By adjusting these parameters, we can observe changes in the execution time of applications on a spectrum of systems ranging from the current high-performance cluster to conventional LAN based clusters. <p> However, it is also important that the communication cost model not be too deeply wedded to a specific machine implementation. The LogP model <ref> [14] </ref> provides such a middle ground by characterizing the performance of the key resources, but not their structure.
Reference: [15] <author> D.E. Culler, L.T. Liu, R.P. Martin, and C.O. Yoshikawa. </author> <title> Assessing Fast Network Interfaces. </title> <booktitle> In IEEE Micro, </booktitle> <volume> vol. 16, </volume> <pages> pp. 35-43, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Two popular parallel computers, the Intel Paragon and the Meiko CS-2 are included for comparison. The LogGP characteristics for our cluster, the Berkeley NOW, are summarized in Table 1. For reference, we also provide measured LogGP characteristics for two tightly integrated parallel processors, the Intel Paragon and Meiko CS-2 <ref> [15] </ref>. 3 Methodology In this section we describe the empirical methodology of our study. The experimental apparatus consists of commercially available hardware and system software, augmented with publicly available research software that has been modified to conduct the experiment. <p> Such a calibration can be obtained by running a set of Active Message micro-benchmarks, described in <ref> [15] </ref>.
Reference: [16] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural Requirements of Parallel Scientific Applications with Explicit Communication. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pp. 2-13, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In principle, the wind tunnel provides enough power to systematically determine application sensitivity to the LogGP parameters within a given protocol, although the small local memory of the underlying CM5 may limit the study to small data sets. Cypher <ref> [16] </ref> described the characteristics of a set of substantial message passing applications also showing that application behavior varies widely. The applications were developed in the context of fairly heavy weight message passing libraries and are more heavily biased to bulk transfers.
Reference: [17] <author> W. J. Dally, J. S. Keen, and M. D. Noakes. </author> <title> The J-Machine Architecture and Evaluation. </title> <booktitle> In COMPCON, </booktitle> <pages> pp. 183-188, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor <ref> [9, 11, 12, 17, 22, 23, 36, 40] </ref>, integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481,
Reference: [18] <author> D.L. Dill, A. Drexler, A.J. Hu, and C.H Yang. </author> <title> Protocol Verification as a Hardware Design Aid. </title> <booktitle> In International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <year> 1992. </year>
Reference-contexts: The dark spots in Figure 4f indicate the presence of hot objects which are visible from multiple points in the scene. * Parallel Mur': In this parallel version of a popular protocol verification tool <ref> [18, 42] </ref>, the exponential space of all reachable protocol states are explored to catch protocol bugs. Each processor maintains a work queue of unexplored states. A hash function maps states to owning processors. When a new state is discovered, it is sent to the proper processor.
Reference: [19] <author> A.C. Dusseau, D.E. Culler, K.E. Schauser, and R.P. Martin. </author> <title> Fast Paral lel Sorting Under LogP: Experience with the CM-5. </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 7, </volume> <pages> pp. 791-805, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Each application is discussed briefly below. * Radix Sort: sorts a large collection of 32-bit keys spread over the processors, and is thoroughly analyzed in <ref> [19] </ref>. It progresses as two iterations of three phases. First, each processor determines the local rank for one digit of its keys. Second, the global rank of each key is calculated from local histograms. Finally, each processor uses the global histogram to distribute the keys to the proper location. <p> The sensitivity to overhead for Radix on 32 processors is over double that of 16 processors. When overhead rises to 100s, the slowdown differential between 16 and 32 processors is a factor of three. The global histogram phase contains a serialization proportional to the radix and number of processors <ref> [19] </ref>. In the unmodified case, the phase accounts for 20% of the overall execution time on 32 processors. When the overhead is set to 100s, this phase accounts for 60% of the overall execution time.
Reference: [20] <author> S. Frank, H. Burkhard II, and J. Rthnie. </author> <title> The KSR 1: Bridging the Gap Between Shared Memory and MPPs. </title> <booktitle> In COMPCON, </booktitle> <pages> pp. 285-294, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller <ref> [1, 20, 32] </ref>, to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported
Reference: [21] <author> R. B. Gillett. </author> <title> Memory Channel Network for PCI. </title> <booktitle> In IEEE Micro, </booktitle> <volume> vol. 16, </volume> <pages> pp. 12-18, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship. The authors can be contacted at frmartin, vahdat, culler, teag@cs.berkeley.edu. reflective memory operations <ref> [7, 21] </ref>, and providing lean communication software layers [35, 43, 44]. Recently, we have seen a shift to designs that accept a reduction in communication performance to obtain greater generality (e.g., Flash vs. Dash), greater opportunity for specialization (e.g. Tempest [38]), or a cleaner communication interface (e.g., T3E vs. T3D). <p> Dash), greater opportunity for specialization (e.g. Tempest [38]), or a cleaner communication interface (e.g., T3E vs. T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters closer to that of the more tightly integrated parallel machines <ref> [3, 12, 21, 35, 43] </ref>. Moving forward from these research alternatives, a crucial question to answer is how much do the improvements in communication performance actually improve application performance. The goal of this work is to provide a systematic study of the impact of communication performance on parallel applications.
Reference: [22] <author> V. G. Grafe and J. E. Hoch. </author> <title> The Epsilon-2 Hybrid Dataflow Architec ture. </title> <booktitle> In COMPCON, </booktitle> <pages> pp. 88-93, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor <ref> [9, 11, 12, 17, 22, 23, 36, 40] </ref>, integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481,
Reference: [23] <author> J. R. Gurd, C. C. Kerkham, and I. Watson. </author> <title> The Manchester Prototype Dataflow Computer. </title> <journal> In Communications of the ACM, </journal> <volume> vol. 28, </volume> <pages> pp. 34-52, </pages> <month> Jan. </month> <year> 1985. </year>
Reference-contexts: These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor <ref> [9, 11, 12, 17, 22, 23, 36, 40] </ref>, integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481,
Reference: [24] <author> M. Heinrich, J. Kuskin, D. Ofelt, J. Heinlein, J. Baxter, J. P. Singh, R. Simoni, K. Gharachorloo, D .Nakahira, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 274-285, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: These issues have driven a number of efforts to develop powerful simulators [38, 39], as well as to develop flexible hardware prototypes <ref> [24] </ref>. The drawback of a real system is that it is most suited to investigate design points that are slower than the base hardware. Thus, to perform the study we must use a prototype communication layer and network hardware with better performance than what is generally available.
Reference: [25] <author> C. Holt, M. Heinrich, J. P. Singh, E. Rothberg, and J. Hennessy. </author> <title> The Effects of Latency, Occupancy, and Bandwidth in Distributed Shared Memory Multiprocessors. </title> <type> Technical Report CSL-TR-95-660, </type> <institution> Stan-ford University, </institution> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: This result suggests that in some cases, rather than making a significant investment to double a machine's processing capacity, the investment may be better directed toward improving the performance of the communication system. 6 Related Work The Flash team recently conducted a study with very similar goals under simulation <ref> [25] </ref>. This study focuses on understanding the performance requirements for a communication controller for a cache-coherent distributed memory machine.
Reference: [26] <author> R. W. Horst. TNet: </author> <title> A Reliable System Area Network. </title> <journal> IEEE Micro, </journal> <volume> vol. 15, </volume> <pages> pp. 37-45, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support <ref> [5, 26, 29, 37] </ref>, supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 9401156), Sun Microsystems, California MICRO, Hewlett Packard, Intel, Mi-crosoft, and Mitsubishi. Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship.
Reference: [27] <author> J. Kay and J. Pasquale. </author> <title> The Importance of Non-Data-Touching Over heads in TCP/IP. </title> <booktitle> In Proceedings of the 1993 SIGCOMM, </booktitle> <pages> pp. 259-268, </pages> <address> San Francisco, CA, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: The extreme left portion of the x-axis represents runs on our cluster. As overhead is increased, the system becomes similar to a switched LAN implementation. Currently, 100 s of overhead with latency and gap values similar to our network is approximately characteristic of TCP/IP protocol stacks <ref> [27, 28, 43] </ref>. At this extreme, applications slow down from 2x to over 50x. Clearly, efforts to reduce cluster communication overhead have been successful. Further, all but one of our applications demonstrate a linear dependence to overhead, suggesting that further reduction in overhead will continue to yield improved performance.
Reference: [28] <author> K. Keeton, D. A. Patterson, and T. E. Anderson. </author> <title> LogP Quantified: The Case for Low-Overhead Local Area Networks. In Hot Interconnects III, </title> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The extreme left portion of the x-axis represents runs on our cluster. As overhead is increased, the system becomes similar to a switched LAN implementation. Currently, 100 s of overhead with latency and gap values similar to our network is approximately characteristic of TCP/IP protocol stacks <ref> [27, 28, 43] </ref>. At this extreme, applications slow down from 2x to over 50x. Clearly, efforts to reduce cluster communication overhead have been successful. Further, all but one of our applications demonstrate a linear dependence to overhead, suggesting that further reduction in overhead will continue to yield improved performance.
Reference: [29] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Ghara chorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pp. 302-313, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller <ref> [5, 10, 29, 41] </ref> or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, <p> [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support <ref> [5, 26, 29, 37] </ref>, supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 9401156), Sun Microsystems, California MICRO, Hewlett Packard, Intel, Mi-crosoft, and Mitsubishi. Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship. <p> The results suggest that there is considerable additional gain to be obtained by further reducing the communication overheads in clusters. Ongoing architectural efforts which integrate the network interface closer to the processor, either by placing the network interface onto the memory bus [11, 12, 38], into the memory controller <ref> [29] </ref>, or by minimizing the number of stores and loads required to launch a communication event will continue to improve application performance. Acknowledgments We would like to thank Bob Horst for motivating this study.
Reference: [30] <author> A. R. Lebeck and D. A. Wood. </author> <title> Dynamic Self-Invalidation: Reduc ing Coherence Overhead in Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The Flash hardware prototype should provide an ideal testbed to perform a systematic in situ study like the one reported here in the DSM context. The Wind Tunnel team has explored a number of cooperative shared memory design points relative to the Tempest interface through simulation and prototyping <ref> [30, 38] </ref>, focused primarily on protocols. In principle, the wind tunnel provides enough power to systematically determine application sensitivity to the LogGP parameters within a given protocol, although the small local memory of the underlying CM5 may limit the study to small data sets.
Reference: [31] <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Ganmukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D.S. Wells, M. C. Wond, S. Yang, and R. Zak. </author> <title> The Network Architecture of the CM-5. </title> <booktitle> In Symposium on Parallel and Distributed Algorithms, </booktitle> <pages> pp. 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus <ref> [7, 31] </ref>, providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 9401156), Sun Microsystems, California MICRO, Hewlett Packard, Intel, Mi-crosoft, and
Reference: [32] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH Prototype: Implementationand Performance. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller <ref> [1, 20, 32] </ref>, to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported
Reference: [33] <author> S. Lumetta, A. Krishnamurthy, and D. E. Culler. </author> <title> Towards Modeling the Performance of a Fast Connected Components Algorithm on Parallel Machines. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: On reception of a state description, a processor first checks if the state has been reached before. If the state is new, the processor adds it to the work queue to be validated against an assertion list. * Connected Components: First, a graph is spread across all processors <ref> [33] </ref>. Each processor then performs a connected components on its local subgraph to collapse portions of its components into representative nodes. Next, the graph is globally adjusted to point remote edges (crossing processor boundaries) at the respective representative nodes. Finally, a global phase successivelymerges components between neighboring processors.
Reference: [34] <author> R. P. Martin. HPAM: </author> <title> An Active Message Layer for a Network of Workstations. </title> <booktitle> In Proceedings of the 2nd Hot Interconnects Conference, </booktitle> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: While the programming model does not provide automatic replication with cache coherence, a number of the applications perform application-specific software caching. The language has been ported to many platforms <ref> [2, 34, 43, 44] </ref>. The sources for the applications, compiler, and communication layer can be obtained from a publically available site 1 . 3.2 Technique The key experimental innovation is to modify the communication layer so that it can emulate a system with arbitrary overhead, gap, or latency.
Reference: [35] <author> S. Pakin, M. Lauria, and A. Chien. </author> <title> High Performance Messaging on Workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship. The authors can be contacted at frmartin, vahdat, culler, teag@cs.berkeley.edu. reflective memory operations [7, 21], and providing lean communication software layers <ref> [35, 43, 44] </ref>. Recently, we have seen a shift to designs that accept a reduction in communication performance to obtain greater generality (e.g., Flash vs. Dash), greater opportunity for specialization (e.g. Tempest [38]), or a cleaner communication interface (e.g., T3E vs. T3D). <p> Dash), greater opportunity for specialization (e.g. Tempest [38]), or a cleaner communication interface (e.g., T3E vs. T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters closer to that of the more tightly integrated parallel machines <ref> [3, 12, 21, 35, 43] </ref>. Moving forward from these research alternatives, a crucial question to answer is how much do the improvements in communication performance actually improve application performance. The goal of this work is to provide a systematic study of the impact of communication performance on parallel applications.
Reference: [36] <author> G. M. Papadopoulos and D. E. Culler. Monsoon: </author> <title> An Explicit Token Store Architecture. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 82-91, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor <ref> [9, 11, 12, 17, 22, 23, 36, 40] </ref>, integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481,
Reference: [37] <author> P. Pierce and G. Regnier. </author> <title> The Paragon Implementation of the NX Message Passing Interface. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pp. 184-190, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors <ref> [6, 8, 37] </ref>, providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 9401156), Sun Microsystems, California MICRO, Hewlett Packard, Intel, Mi-crosoft, and Mitsubishi. <p> [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support <ref> [5, 26, 29, 37] </ref>, supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 9401156), Sun Microsystems, California MICRO, Hewlett Packard, Intel, Mi-crosoft, and Mitsubishi. Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship.
Reference: [38] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pp. 325-336, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Recently, we have seen a shift to designs that accept a reduction in communication performance to obtain greater generality (e.g., Flash vs. Dash), greater opportunity for specialization (e.g. Tempest <ref> [38] </ref>), or a cleaner communication interface (e.g., T3E vs. T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters closer to that of the more tightly integrated parallel machines [3, 12, 21, 35, 43]. <p> These issues have driven a number of efforts to develop powerful simulators <ref> [38, 39] </ref>, as well as to develop flexible hardware prototypes [24]. The drawback of a real system is that it is most suited to investigate design points that are slower than the base hardware. <p> The Flash hardware prototype should provide an ideal testbed to perform a systematic in situ study like the one reported here in the DSM context. The Wind Tunnel team has explored a number of cooperative shared memory design points relative to the Tempest interface through simulation and prototyping <ref> [30, 38] </ref>, focused primarily on protocols. In principle, the wind tunnel provides enough power to systematically determine application sensitivity to the LogGP parameters within a given protocol, although the small local memory of the underlying CM5 may limit the study to small data sets. <p> The results suggest that there is considerable additional gain to be obtained by further reducing the communication overheads in clusters. Ongoing architectural efforts which integrate the network interface closer to the processor, either by placing the network interface onto the memory bus <ref> [11, 12, 38] </ref>, into the memory controller [29], or by minimizing the number of stores and loads required to launch a communication event will continue to improve application performance. Acknowledgments We would like to thank Bob Horst for motivating this study.
Reference: [39] <author> M. Rosenblum, S. A. Herrod, E. Witchel, and A .Gupta. </author> <title> Complete Computer Simulation: The SimOS Approach. </title> <booktitle> In IEEE Parallel and Distributed Technology, </booktitle> <month> Fall </month> <year> 1995. </year>
Reference-contexts: These issues have driven a number of efforts to develop powerful simulators <ref> [38, 39] </ref>, as well as to develop flexible hardware prototypes [24]. The drawback of a real system is that it is most suited to investigate design points that are slower than the base hardware.
Reference: [40] <author> S. Sakai, Y. Yamaguchi, K. Hiraki, Y. Kodama, and T. Yuba. </author> <title> An Ar chitecture of a Dataflow Single Chip Processor. </title> <booktitle> In Proc. of the 16th Annual Int. Symp. on Comp. Arch., </booktitle> <pages> pp. 46-53, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller [5, 10, 29, 41] or the cache controller [1, 20, 32], to incorporating messaging deep into the processor <ref> [9, 11, 12, 17, 22, 23, 36, 40] </ref>, integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5, 26, 29, 37], supporting This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481,
Reference: [41] <author> S. L. Scott. </author> <title> Synchronizationand Communication in the T3E Multipro cessor. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for ProgrammingLanguages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Many research efforts in parallel computer architecture have focused on improving various aspects of communication performance. These investigations cover a vast spectrum of alternatives, ranging from integrating message transactions into the memory controller <ref> [5, 10, 29, 41] </ref> or the cache controller [1, 20, 32], to incorporating messaging deep into the processor [9, 11, 12, 17, 22, 23, 36, 40], integrating the network interface on the memory bus [7, 31], providing dedicated message processors [6, 8, 37], providing various kinds of bulk transfer support [5,
Reference: [42] <author> U. Stern and D. L. Dill. </author> <title> Parallelizing the Murphi Verifier. </title> <note> Submitted for publication. </note>
Reference-contexts: The dark spots in Figure 4f indicate the presence of hot objects which are visible from multiple points in the scene. * Parallel Mur': In this parallel version of a popular protocol verification tool <ref> [18, 42] </ref>, the exponential space of all reachable protocol states are explored to catch protocol bugs. Each processor maintains a work queue of unexplored states. A hash function maps states to owning processors. When a new state is discovered, it is sent to the proper processor.
Reference: [43] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A User Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the Fifteenth SOSP, </booktitle> <pages> pp. 40-53, </pages> <address> Copper Mountain, CO, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship. The authors can be contacted at frmartin, vahdat, culler, teag@cs.berkeley.edu. reflective memory operations [7, 21], and providing lean communication software layers <ref> [35, 43, 44] </ref>. Recently, we have seen a shift to designs that accept a reduction in communication performance to obtain greater generality (e.g., Flash vs. Dash), greater opportunity for specialization (e.g. Tempest [38]), or a cleaner communication interface (e.g., T3E vs. T3D). <p> Dash), greater opportunity for specialization (e.g. Tempest [38]), or a cleaner communication interface (e.g., T3E vs. T3D). At the same time, a number of investigations are focusing on bringing the communication performance of clusters closer to that of the more tightly integrated parallel machines <ref> [3, 12, 21, 35, 43] </ref>. Moving forward from these research alternatives, a crucial question to answer is how much do the improvements in communication performance actually improve application performance. The goal of this work is to provide a systematic study of the impact of communication performance on parallel applications. <p> While the programming model does not provide automatic replication with cache coherence, a number of the applications perform application-specific software caching. The language has been ported to many platforms <ref> [2, 34, 43, 44] </ref>. The sources for the applications, compiler, and communication layer can be obtained from a publically available site 1 . 3.2 Technique The key experimental innovation is to modify the communication layer so that it can emulate a system with arbitrary overhead, gap, or latency. <p> The extreme left portion of the x-axis represents runs on our cluster. As overhead is increased, the system becomes similar to a switched LAN implementation. Currently, 100 s of overhead with latency and gap values similar to our network is approximately characteristic of TCP/IP protocol stacks <ref> [27, 28, 43] </ref>. At this extreme, applications slow down from 2x to over 50x. Clearly, efforts to reduce cluster communication overhead have been successful. Further, all but one of our applications demonstrate a linear dependence to overhead, suggesting that further reduction in overhead will continue to yield improved performance.
Reference: [44] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Ac tive Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Anderson was also supported by a National Science Foundation Presidential Faculty Fellowship. The authors can be contacted at frmartin, vahdat, culler, teag@cs.berkeley.edu. reflective memory operations [7, 21], and providing lean communication software layers <ref> [35, 43, 44] </ref>. Recently, we have seen a shift to designs that accept a reduction in communication performance to obtain greater generality (e.g., Flash vs. Dash), greater opportunity for specialization (e.g. Tempest [38]), or a cleaner communication interface (e.g., T3E vs. T3D). <p> While the programming model does not provide automatic replication with cache coherence, a number of the applications perform application-specific software caching. The language has been ported to many platforms <ref> [2, 34, 43, 44] </ref>. The sources for the applications, compiler, and communication layer can be obtained from a publically available site 1 . 3.2 Technique The key experimental innovation is to modify the communication layer so that it can emulate a system with arbitrary overhead, gap, or latency.
Reference: [45] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pp. 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Also, we are able to run applications on realistic input sizes, so we escape the difficulties of attempting to size the machine parameters down to levels appropriate for the small problems feasible on a simulator and then extrapolating to the real case <ref> [45] </ref>. These issues have driven a number of efforts to develop powerful simulators [38, 39], as well as to develop flexible hardware prototypes [24]. The drawback of a real system is that it is most suited to investigate design points that are slower than the base hardware. <p> For our input of 16 million keys Sample sort spends 85% of the time in the two communication phases. * Barnes: Our implementation of this hierarchical N-Body force calculation is similar to the version in the SPLASH benchmark suite <ref> [45] </ref>. However, the main data structure, a spatial oct-tree, is replicated in software rather than hardware. Each timestep consists of two phases, a tree construction phase and an interaction phase among the simulated bodies. Updates of the oct-tree are synchronized through blocking locks.
References-found: 45

