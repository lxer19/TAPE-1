URL: http://drl.cs.uiuc.edu/pubs/tjs96.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/tjs96.html
Root-URL: http://www.cs.uiuc.edu
Title: Multidimensional Array I/O in Panda 1.0  
Author: KENT E. SEAMONS MARIANNE WINSLETT 
Keyword: Parallel I/O, collective I/O, parallel file systems, multidimensional arrays, application program interfaces.  
Address: Gulf Tower, Pittsburgh, PA 15219 USA  Urbana, IL 61801 USA  
Affiliation: Transarc Corporation, The  Department of Computer Science, University of Illinois,  
Note: The Journal of Supercomputing,  c 1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: seamons@transarc.com  winslett@cs.uiuc.edu  
Date: 10, 1-22 (1996)  Received June 1995.  
Abstract: Large multidimensional arrays are a common data type in high-performance scientific applications. Without special techniques for handling input and output, I/O can easily become a large fraction of execution time for applications using these arrays, especially on parallel platforms. Our research seeks to provide scientific programmers with simpler and more abstract interfaces for accessing persistent multidimensional arrays, and to produce advanced I/O libraries supporting more efficient layout alternatives for these arrays on disk and in main memory. We have created the Panda (Persistence AND Arrays) I/O library as a result of developing interfaces and libraries for applications in computational fluid dynamics in the areas of checkpoint, restart, and time-step output data. In the applications we have studied, we find that a simple, abstract interface can be used to insulate programmers from physical storage implementation details, while providing improved I/O performance at the same time. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. L. Bell and G. S. Patterson, Jr. </author> <title> Data organization in large numerical computations. </title> <journal> The Journal of Supercomputing, </journal> <volume> 1(1) </volume> <pages> 105-136, </pages> <year> 1987. </year>
Reference-contexts: 1. Introduction Multidimensional arrays are a fundamental data type in scientific computing and are used extensively in Grand Challenge applications <ref> [1] </ref>. Often these arrays are persistent; that is, they outlive the invocation of the program that created them. Portability and performance with respect to input and output (I/O) of arrays pose significant challenges to applications that access large persistent arrays.
Reference: 2. <author> R. Bordawekar, A. Choudhary, K. Kennedy, C. Koelbel, and M. Paleczny. </author> <title> A model and compilation strategy for out-of-core data parallel programs. </title> <type> Technical Report CRPC-TR94507-S, </type> <note> Center for Research on Parallel Computation, </note> <month> December </month> <year> 1994. </year>
Reference-contexts: After much discussion, I/O directives were not included in the first version of HPF. Vienna Fortran has compiler extensions to support I/O [5] and experiments were conducted writing distributed array chunks directly to a file rather than in traditional order [4]. Out-of-core compilation strategies for I/O are being developed <ref> [26, 2] </ref>. Run-time primitives to support a two-phase access strategy for parallel I/O of arrays are described in a study by Bordawekar et al. [3].
Reference: 3. <author> R. Bordawekar, J. M. del Rosario, and A. Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <address> Portland, Ore., </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Under this strategy, array chunks in memory are stored directly as array chunks on disk and each compute node can issue one write request to the parallel file system for each array chunk it contains. This is a twist on the two-phase I/O strategy <ref> [3] </ref>, in which array data in memory are reorganized to a conforming distribution and then written to disk. Panda avoids the need to reorganize by using the current in-memory array distribution as the conforming distribution on disk. <p> Out-of-core compilation strategies for I/O are being developed [26, 2]. Run-time primitives to support a two-phase access strategy for parallel I/O of arrays are described in a study by Bordawekar et al. <ref> [3] </ref>. In this approach, for read operations the compute nodes cooperate to bring all the data into memory in a way that minimizes the total number of disk accesses by having the data layout in memory conform to the data layout on disk.
Reference: 4. <author> P. Brezany, M. Gernt, P. Mehrotra, and H. Zima. </author> <title> Concurrent file operations in a High Performance Fortran. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 230-237, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: After much discussion, I/O directives were not included in the first version of HPF. Vienna Fortran has compiler extensions to support I/O [5] and experiments were conducted writing distributed array chunks directly to a file rather than in traditional order <ref> [4] </ref>. Out-of-core compilation strategies for I/O are being developed [26, 2]. Run-time primitives to support a two-phase access strategy for parallel I/O of arrays are described in a study by Bordawekar et al. [3].
Reference: 5. <author> P. Brezany, T. A. Mueck, and E. Schikuta. </author> <title> Language, compiler and parallel database support for I/O intensive applications. </title> <booktitle> In Proceedings of the High Performance Computing and Networking 1995 Europe. </booktitle> <publisher> Springer-Verlag, </publisher> <month> May </month> <year> 1995. </year>
Reference-contexts: After much discussion, I/O directives were not included in the first version of HPF. Vienna Fortran has compiler extensions to support I/O <ref> [5] </ref> and experiments were conducted writing distributed array chunks directly to a file rather than in traditional order [4]. Out-of-core compilation strategies for I/O are being developed [26, 2].
Reference: 6. <author> P. Corbett, D. Feitelson, Y. Hsu, J.-P. Prost, M. Snir, S. Fineberg, B. Nitzberg, B. Traversat, and P. Wong. </author> <title> MPI-IO: A parallel file I/O interface for MPI. </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year> <note> Version 0.3. </note>
Reference-contexts: Several large projects (e.g., [7, 14, 22, 8]) from the HPCC community are devoted to the development of new file systems better suited to parallel I/O. A standard interface for parallel I/O has been proposed by Corbett et al. <ref> [6] </ref>. Panda supports a higher level interface than that recommended in these efforts. 7.
Reference: 7. <author> P. F. Corbett and D. G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference-contexts: Version 2.0 of Panda incorporates a logical-level version of disk-directed I/O, called server-directed I/O, which we have found to do a very good job of avoiding the buffering errors that plagued our runs with CFS. Several large projects (e.g., <ref> [7, 14, 22, 8] </ref>) from the HPCC community are devoted to the development of new file systems better suited to parallel I/O. A standard interface for parallel I/O has been proposed by Corbett et al. [6]. Panda supports a higher level interface than that recommended in these efforts. 7.
Reference: 8. <author> J. M. del Rosario, M. Harry, and A. Choudhary. </author> <title> The design of VIP-FS: A virtual, parallel file system for high performance parallel and distributed computing. </title> <type> Technical Report SCCS-628, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Version 2.0 of Panda incorporates a logical-level version of disk-directed I/O, called server-directed I/O, which we have found to do a very good job of avoiding the buffering errors that plagued our runs with CFS. Several large projects (e.g., <ref> [7, 14, 22, 8] </ref>) from the HPCC community are devoted to the development of new file systems better suited to parallel I/O. A standard interface for parallel I/O has been proposed by Corbett et al. [6]. Panda supports a higher level interface than that recommended in these efforts. 7.
Reference: 9. <author> D. DeWitt, N. Kabra, J. Luo, J. M. Patel, and J.-B. Yu. </author> <title> Client-server Paradise. </title> <booktitle> In Proceedings of the 20th VLDB Conference, </booktitle> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: Panda 1.0 was optimized for write-centered operations such as checkpoint and time-step output. Performance for restart was not critical and Panda 1.0 contained no special optimizations for reading arrays. The NAS Intel iPSC/860 <ref> (decommissioned in the fall of 1994) </ref> is a parallel machine with 128 compute nodes and 10 additional I/O nodes. Each I/O node has approximately a 1-Mbyte buffer for files and a single attached disk with a peak transfer rate of 1 Mbyte/s. <p> A few commercial DBMSs support multidimensional arrays, including Interbase [15], Orion [37], and Stratum [29]. The POSTGRES DBMS has been enhanced to support multidimensional arrays and chunked schemas [28]. DeWitt et al. <ref> [9] </ref> describe a sequential client-server version of Paradise, a database system designed for GIS applications. A parallel version is planned for the future. Paradise supports two-dimensional raster images and divides the images into tiles to be stored on disk. Tiles in Paradise are comparable to two-dimensional array chunks in Panda.
Reference: 10. <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Tiles in Paradise are comparable to two-dimensional array chunks in Panda. Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [10, 11, 13, 38] </ref> the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data distribution across the processors that is appropriate for the computation.
Reference: 11. <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR 90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Tiles in Paradise are comparable to two-dimensional array chunks in Panda. Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [10, 11, 13, 38] </ref> the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data distribution across the processors that is appropriate for the computation.
Reference: 12. <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <address> Portland, Ore., </address> <month> November </month> <year> 1993. </year>
Reference-contexts: An example of an astrophysics application with similar I/O needs is described in work by R. Thakur et al. [34]. An example of an I/O library motivated by the same I/O operations is described in work by N. Galbreath et al. <ref> [12] </ref>. Two application groups at the National Center for Supercomputing Applications at the University of Illinois also have similar I/O needs. First, the Astronomy and Astrophysics Group has cosmology applications that perform large computations on the CM-5 at NCSA. <p> During a second phase the compute nodes permute the data in memory to the required memory layout. The reverse strategy holds for writes. The two-phase access strategy could be integrated into Panda. Galbreath et al. <ref> [12] </ref> report on experiences with parallel applications at Argonne National Laboratory, emphasizing the value of high-level abstractions. MULTIDIMENSIONAL ARRAY I/O IN PANDA 1.0 19 Disk-directed I/O for collective I/O operations has been proposed [16].
Reference: 13. <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8): </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: Tiles in Paradise are comparable to two-dimensional array chunks in Panda. Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [10, 11, 13, 38] </ref> the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data distribution across the processors that is appropriate for the computation.
Reference: 14. <author> J. V. Huber, Jr., C. L. Elford, D. A. Reed, A. A. Chien, and D. S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1995. </year> <note> 22 K. </note> <author> E. SEAMONS AND M. </author> <note> WINSLETT </note>
Reference-contexts: Version 2.0 of Panda incorporates a logical-level version of disk-directed I/O, called server-directed I/O, which we have found to do a very good job of avoiding the buffering errors that plagued our runs with CFS. Several large projects (e.g., <ref> [7, 14, 22, 8] </ref>) from the HPCC community are devoted to the development of new file systems better suited to parallel I/O. A standard interface for parallel I/O has been proposed by Corbett et al. [6]. Panda supports a higher level interface than that recommended in these efforts. 7.
Reference: 15. <author> Interbase Software Corporation. </author> <title> Interbase Data Definition Guide. </title> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: Persistent multidimensional arrays are generally stored in a file using traditional array order, whether using file format systems [24, 35], database management systems <ref> [15, 29, 37] </ref>, or hand-coded persistence. 2 K. E. SEAMONS AND M. WINSLETT Traditional array order in a file is satisfactory for some applications. <p> HDF is planning to incorporate chunked physical schemas and compression, guided by results from Panda. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [21]. A few commercial DBMSs support multidimensional arrays, including Interbase <ref> [15] </ref>, Orion [37], and Stratum [29]. The POSTGRES DBMS has been enhanced to support multidimensional arrays and chunked schemas [28]. DeWitt et al. [9] describe a sequential client-server version of Paradise, a database system designed for GIS applications. A parallel version is planned for the future.
Reference: 16. <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year> <month> Revised November 8, </month> <year> 1994. </year>
Reference-contexts: A common approach to writing out the contents of an array on distributed-memory parallel machines, known as the direct method [33] or traditional caching <ref> [16] </ref>, is for the processors to open a shared file and directly write their array elements to the proper location in the file so that the array elements are stored in traditional array order. <p> This approach performs poorly in many cases due to excess disk seeks and write-back errors <ref> [33, 16] </ref>. The parallel file system is barraged with many small, strided write requests and has no means to optimize for the global I/O operation that is taking place, namely, writing out an entire array, because the file system interface is at a low level of abstraction. <p> The two-phase access strategy could be integrated into Panda. Galbreath et al. [12] report on experiences with parallel applications at Argonne National Laboratory, emphasizing the value of high-level abstractions. MULTIDIMENSIONAL ARRAY I/O IN PANDA 1.0 19 Disk-directed I/O for collective I/O operations has been proposed <ref> [16] </ref>. Under this approach compute nodes tell the I/O nodes about an upcoming collective I/O operation and, based on this semantic information, the I/O nodes direct the flow of data during the operation so they can form large contiguous disk I/O requests rather than many smaller requests. <p> This allows on-the-fly reorganization of data from one physical schema to another during I/O and promises to be faster than an in-memory sort prior to I/O. This simulation by D. Kotz <ref> [16] </ref> promises good performance. More recently, simulations have shown disk-directed I/O to have advantages over traditional caching in both an out-of-core LU decomposition problem [17] and on irregular distributions of data [18].
Reference: 17. <author> D. Kotz. </author> <title> Disk-directed I/O for an out-of-core computation. </title> <type> Technical Report PCS-TR95-251, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: This simulation by D. Kotz [16] promises good performance. More recently, simulations have shown disk-directed I/O to have advantages over traditional caching in both an out-of-core LU decomposition problem <ref> [17] </ref> and on irregular distributions of data [18]. Version 2.0 of Panda incorporates a logical-level version of disk-directed I/O, called server-directed I/O, which we have found to do a very good job of avoiding the buffering errors that plagued our runs with CFS.
Reference: 18. <author> D. Kotz. </author> <title> Expanding the potential for disk-directed I/O. </title> <type> Technical Report PCS-TR95-254, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: This simulation by D. Kotz [16] promises good performance. More recently, simulations have shown disk-directed I/O to have advantages over traditional caching in both an out-of-core LU decomposition problem [17] and on irregular distributions of data <ref> [18] </ref>. Version 2.0 of Panda incorporates a logical-level version of disk-directed I/O, called server-directed I/O, which we have found to do a very good job of avoiding the buffering errors that plagued our runs with CFS.
Reference: 19. <author> D. A. Lane. </author> <title> Visualization of time-dependent flow fields. </title> <booktitle> In Proceedings of IEEE Visualization '93, </booktitle> <pages> pages 32-38, </pages> <address> San Jose, Calif., </address> <month> October </month> <year> 1993. </year>
Reference-contexts: The algorithmic details of one flow solver are given in the work of Ryan and Weeratunga [27]. The output data consist almost exclusively of multidimensional arrays [36] and are typically analyzed by scientific visualization tools that perform operations such as particle tracing <ref> [20, 19] </ref>. The application programs at NAS are one example of a significant number of scientific applications that perform conceptually simple, high-level array I/O operations. For instance, flow solvers typically require checkpoint and restart capabilities. A simulation involving several arrays may run for many hours.
Reference: 20. <author> D. A. Lane. </author> <title> Ufat | A particle tracer for time-dependent flow fields. </title> <booktitle> In Proceedings of IEEE Visualization '94, </booktitle> <pages> pages 257-264, </pages> <address> Washington, D.C., </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The algorithmic details of one flow solver are given in the work of Ryan and Weeratunga [27]. The output data consist almost exclusively of multidimensional arrays [36] and are typically analyzed by scientific visualization tools that perform operations such as particle tracing <ref> [20, 19] </ref>. The application programs at NAS are one example of a significant number of scientific applications that perform conceptually simple, high-level array I/O operations. For instance, flow solvers typically require checkpoint and restart capabilities. A simulation involving several arrays may run for many hours.
Reference: 21. <author> D. Maier and B. Vance. </author> <title> A call to order. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Principles of Database Systems, </booktitle> <pages> pages 1-16, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Neither package includes any specialized techniques for optimizing array I/O. HDF is planning to incorporate chunked physical schemas and compression, guided by results from Panda. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk <ref> [21] </ref>. A few commercial DBMSs support multidimensional arrays, including Interbase [15], Orion [37], and Stratum [29]. The POSTGRES DBMS has been enhanced to support multidimensional arrays and chunked schemas [28]. DeWitt et al. [9] describe a sequential client-server version of Paradise, a database system designed for GIS applications.
Reference: 22. <author> S. A. Moyer and V. S. Sunderam. </author> <title> PIOUS: A scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Version 2.0 of Panda incorporates a logical-level version of disk-directed I/O, called server-directed I/O, which we have found to do a very good job of avoiding the buffering errors that plagued our runs with CFS. Several large projects (e.g., <ref> [7, 14, 22, 8] </ref>) from the HPCC community are devoted to the development of new file systems better suited to parallel I/O. A standard interface for parallel I/O has been proposed by Corbett et al. [6]. Panda supports a higher level interface than that recommended in these efforts. 7.
Reference: 23. <author> H. </author> <title> Nag. </title> <type> Personal communication. </type> <institution> Intel Corporation, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Experiments show that even for a fixed amount of data, CFS gives better performance if a compute node issues a single write request for its data than if the node divides the data into several write requests. This finding was confirmed by independent testing conducted at Intel <ref> [23] </ref>. CFS prefetches seven disk blocks on every read [23], including a read made in order to write a subpart of the fetched block. <p> This finding was confirmed by independent testing conducted at Intel <ref> [23] </ref>. CFS prefetches seven disk blocks on every read [23], including a read made in order to write a subpart of the fetched block.
Reference: 24. <institution> National Center for Supercomputing Applications. </institution> <note> NCSA HDF Reference Manual, Version 3.3. </note> <institution> University of Illinois, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Persistent multidimensional arrays are generally stored in a file using traditional array order, whether using file format systems <ref> [24, 35] </ref>, database management systems [15, 29, 37], or hand-coded persistence. 2 K. E. SEAMONS AND M. WINSLETT Traditional array order in a file is satisfactory for some applications. <p> Related Work This research is related to previous work in file format systems, databases, compilers, and parallel I/O. Until fairly recently, scientists had only flat files for storing data. Now several file format systems in widespread use support storing multidimensional array data, such as Hierarchical Data Format (HDF) <ref> [24] </ref> and NetCDF [35]. Both HDF and NetCDF store scientific data in self-describing, machine-independent files and organize fixed-size arrays using traditional array ordering on disk. Neither package includes any specialized techniques for optimizing array I/O. HDF is planning to incorporate chunked physical schemas and compression, guided by results from Panda.
Reference: 25. <author> B. Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: E. SEAMONS AND M. WINSLETT and a parallel file system. 5. Performance This section describes the performance of Panda 1.0 on the Intel iPSC/860 at NASA Ames Research Center <ref> [25] </ref>, concentrating on write operations. Panda 1.0 was optimized for write-centered operations such as checkpoint and time-step output. Performance for restart was not critical and Panda 1.0 contained no special optimizations for reading arrays. <p> As a basis for judging Panda's performance, note that the peak theoretical I/O throughput for the NAS iPSC/860's file system, Concurrent File System (CFS), is 10 Mbytes/s, due to I/O node-to-disk hardware limitations, and the practical maximum that can be expected for writes is 7-8 Mbytes/s <ref> [25] </ref>. Network contention is not a factor in I/O performance on the NAS iPSC/860, and the critical resource is I/O node buffer space [25]. The shortage of I/O node buffer space creates a bottleneck in the system. <p> system, Concurrent File System (CFS), is 10 Mbytes/s, due to I/O node-to-disk hardware limitations, and the practical maximum that can be expected for writes is 7-8 Mbytes/s <ref> [25] </ref>. Network contention is not a factor in I/O performance on the NAS iPSC/860, and the critical resource is I/O node buffer space [25]. The shortage of I/O node buffer space creates a bottleneck in the system. CFS breaks up each compute node's write request into 4-Kbyte packets and routes them one by one to the appropriate I/O node. <p> The overhead of a global synchronization is minimal on the iPSC/860 <ref> [25] </ref>. Total run time is that reported by the last compute node to finish, and the aggregate throughput is computed as total run time divided by the total number of array bytes passed to Panda's I/O routines. <p> The iPSC/860 is a busy machine, shared with other users; thus all throughputs reported are the best of five or more runs. The performance study conducted by B. Nitzberg <ref> [25] </ref> reports that timings varied as much as 20% between runs when the machine was dedicated to one user. We also found CFS to be very erratic and difficult to analyze. Preallocation is slow, so in all experiments except Figure 5, preallocation is performed before timing begins. <p> Note that for all schemas in Figure 5, throughput first rises, then declines as the number of compute nodes increases. Throughput rises until reaching 16 or 32 compute nodes, because a lesser number of compute nodes is insufficient to keep the I/O subsystem busy <ref> [25] </ref>. Throughput declines thereafter because the NAS iPSC/860's I/O system is not scalable, due to the shortage of I/O node buffer space, as described earlier. <p> that greater throughput could be obtained by restricting the number of compute nodes allowed to issue I/O requests simultaneously, so that longer contiguous writes would be possible once the buffer was filled. (In a truly scalable I/O system, such restrictions would bring no benefit.) These are known as grouped writes <ref> [25] </ref>. We conducted experiments to write twenty 10-Mbyte or twenty 20-Mbyte arrays to compare interleaved and non-interleaved schemas. These data sets, which total 200 Mbytes and 400 Mbytes respectively, only fit on 64 or 128 compute nodes on the NAS iPSC/860. <p> E. SEAMONS AND M. WINSLETT schemas improves as array sizes are increased, and eventually approaches the performance of interleaved schemas as array sizes increase. This pattern is consistent with the findings in work by B. Nitzberg <ref> [25] </ref>. comparing interleaved and non-interleaved schemas. We also conducted experiments writing two 8-Mbyte arrays comparing interleaved and non-interleaved schemas, as well as grouped writes using groups of 16 compute nodes.
Reference: 26. <author> M. Paleczny, K. Kennedy, and C. Koelbel. </author> <title> Compiler support for out-of-core arrays on data parallel machines. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 110-118, </pages> <address> McLean, Va., </address> <month> February </month> <year> 1995. </year>
Reference-contexts: After much discussion, I/O directives were not included in the first version of HPF. Vienna Fortran has compiler extensions to support I/O [5] and experiments were conducted writing distributed array chunks directly to a file rather than in traditional order [4]. Out-of-core compilation strategies for I/O are being developed <ref> [26, 2] </ref>. Run-time primitives to support a two-phase access strategy for parallel I/O of arrays are described in a study by Bordawekar et al. [3].
Reference: 27. <author> J. S. Ryan and S. K. Weeratunga. </author> <title> Parallel computation of 3-D Navier-Stokes flowfields for supersonic vehicles. </title> <booktitle> In 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <address> Reno, Nev., </address> <year> 1993. </year> <note> AIAA Paper 93-0064. </note>
Reference-contexts: A "flow solver," usually a Fortran program that runs on a parallel computer system, takes the grid data and generates output data by solving systems of equations that govern fluid flow. The algorithmic details of one flow solver are given in the work of Ryan and Weeratunga <ref> [27] </ref>. The output data consist almost exclusively of multidimensional arrays [36] and are typically analyzed by scientific visualization tools that perform operations such as particle tracing [20, 19].
Reference: 28. <author> S. Sarawagi and M. Stonebraker. </author> <title> Efficient organization of large multidimensional arrays. </title> <booktitle> In Proceedings of the 10th International Conference on Data Engineering, </booktitle> <pages> pages 328-336, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [21]. A few commercial DBMSs support multidimensional arrays, including Interbase [15], Orion [37], and Stratum [29]. The POSTGRES DBMS has been enhanced to support multidimensional arrays and chunked schemas <ref> [28] </ref>. DeWitt et al. [9] describe a sequential client-server version of Paradise, a database system designed for GIS applications. A parallel version is planned for the future. Paradise supports two-dimensional raster images and divides the images into tiles to be stored on disk.
Reference: 29. <institution> Scientific and Engineering Software. </institution> <note> Stratum Technical Reference Manual. Austin, Tex., </note> <month> October </month> <year> 1989. </year>
Reference-contexts: Persistent multidimensional arrays are generally stored in a file using traditional array order, whether using file format systems [24, 35], database management systems <ref> [15, 29, 37] </ref>, or hand-coded persistence. 2 K. E. SEAMONS AND M. WINSLETT Traditional array order in a file is satisfactory for some applications. <p> HDF is planning to incorporate chunked physical schemas and compression, guided by results from Panda. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [21]. A few commercial DBMSs support multidimensional arrays, including Interbase [15], Orion [37], and Stratum <ref> [29] </ref>. The POSTGRES DBMS has been enhanced to support multidimensional arrays and chunked schemas [28]. DeWitt et al. [9] describe a sequential client-server version of Paradise, a database system designed for GIS applications. A parallel version is planned for the future.
Reference: 30. <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <address> Washington, D.C., </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Our research seeks to provide simpler and more abstract interfaces for the I/O of scientific array data and to produce advanced I/O libraries supporting more efficient layout alternatives for multidimensional arrays on disk and in main memory <ref> [30, 31, 32] </ref>. Panda (Persistence AND Arrays) is an I/O library we have developed that embodies our ideas. This paper describes the approach taken by version 1.0 of Panda. The I/O bottleneck problem is a serious impediment to many scientific computing applications that process persistent arrays.
Reference: 31. <author> K. E. Seamons and M. Winslett. </author> <title> Physical schemas for large multidimensional arrays in scientific computing applications. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 218-227, </pages> <address> Charlottesville, Va., </address> <month> September </month> <year> 1994. </year>
Reference-contexts: Our research seeks to provide simpler and more abstract interfaces for the I/O of scientific array data and to produce advanced I/O libraries supporting more efficient layout alternatives for multidimensional arrays on disk and in main memory <ref> [30, 31, 32] </ref>. Panda (Persistence AND Arrays) is an I/O library we have developed that embodies our ideas. This paper describes the approach taken by version 1.0 of Panda. The I/O bottleneck problem is a serious impediment to many scientific computing applications that process persistent arrays. <p> Additionally, high-level interfaces offer freedom and flexibility within the I/O library for optimizations in creating an efficient implementation on a given architecture. Panda provides interfaces for checkpoints, restarts, and time-step output saves, as described in Section 1. As described in previous work <ref> [31] </ref>, Panda supports these MULTIDIMENSIONAL ARRAY I/O IN PANDA 1.0 5 three operations through a high-level application programming interface (API) whereby programmers first say what arrays they want to have included in all checkpoints and all time-step output saves, and then periodically request that the checkpoint or time-step output save be
Reference: 32. <author> K. E. Seamons and M. Winslett. </author> <title> A data management approach for handling large compressed arrays in high performance computing. </title> <booktitle> In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 119-128, </pages> <address> McLean, Va., </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Our research seeks to provide simpler and more abstract interfaces for the I/O of scientific array data and to produce advanced I/O libraries supporting more efficient layout alternatives for multidimensional arrays on disk and in main memory <ref> [30, 31, 32] </ref>. Panda (Persistence AND Arrays) is an I/O library we have developed that embodies our ideas. This paper describes the approach taken by version 1.0 of Panda. The I/O bottleneck problem is a serious impediment to many scientific computing applications that process persistent arrays.
Reference: 33. <author> R. Thakur and A. Choudhary. </author> <title> Accessing sections of out-of-core arrays using an extended two-phase method. </title> <type> Technical Report SCCS-685, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: A common approach to writing out the contents of an array on distributed-memory parallel machines, known as the direct method <ref> [33] </ref> or traditional caching [16], is for the processors to open a shared file and directly write their array elements to the proper location in the file so that the array elements are stored in traditional array order. <p> This approach performs poorly in many cases due to excess disk seeks and write-back errors <ref> [33, 16] </ref>. The parallel file system is barraged with many small, strided write requests and has no means to optimize for the global I/O operation that is taking place, namely, writing out an entire array, because the file system interface is at a low level of abstraction.
Reference: 34. <author> R. Thakur, E. Lusk, and W. Gropp. </author> <title> I/O characterization of a portable astrophysics application on the IBM SP and Intel Paragon. </title> <type> Technical Report MCS-P534-0895, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1995. </year> <type> Preprint. </type>
Reference-contexts: These high-level operations are common in other domains besides computational fluid dynamics. An example of an astrophysics application with similar I/O needs is described in work by R. Thakur et al. <ref> [34] </ref>. An example of an I/O library motivated by the same I/O operations is described in work by N. Galbreath et al. [12]. Two application groups at the National Center for Supercomputing Applications at the University of Illinois also have similar I/O needs.
Reference: 35. <institution> University Corporation for Atmospheric Research, Unidata Program Center. </institution> <note> NetCDF User's Guide, Version 2.0. </note> <month> October </month> <year> 1992. </year>
Reference-contexts: Persistent multidimensional arrays are generally stored in a file using traditional array order, whether using file format systems <ref> [24, 35] </ref>, database management systems [15, 29, 37], or hand-coded persistence. 2 K. E. SEAMONS AND M. WINSLETT Traditional array order in a file is satisfactory for some applications. <p> Until fairly recently, scientists had only flat files for storing data. Now several file format systems in widespread use support storing multidimensional array data, such as Hierarchical Data Format (HDF) [24] and NetCDF <ref> [35] </ref>. Both HDF and NetCDF store scientific data in self-describing, machine-independent files and organize fixed-size arrays using traditional array ordering on disk. Neither package includes any specialized techniques for optimizing array I/O. HDF is planning to incorporate chunked physical schemas and compression, guided by results from Panda.
Reference: 36. <author> P. Walatka, P. Buning, L. Pierce, and P. Elson. </author> <title> PLOT3D User's Manual, </title> <type> NASA Technical Memorandum 101067. </type> <institution> NASA Ames Research Center, Unidata Program Center, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: The algorithmic details of one flow solver are given in the work of Ryan and Weeratunga [27]. The output data consist almost exclusively of multidimensional arrays <ref> [36] </ref> and are typically analyzed by scientific visualization tools that perform operations such as particle tracing [20, 19]. The application programs at NAS are one example of a significant number of scientific applications that perform conceptually simple, high-level array I/O operations.
Reference: 37. <author> XIDAK Inc. </author> <title> Overview of Orion, </title> <type> Version 2 Release 3(4). </type> <institution> Palo Alto, Calif., </institution> <year> 1991. </year>
Reference-contexts: Persistent multidimensional arrays are generally stored in a file using traditional array order, whether using file format systems [24, 35], database management systems <ref> [15, 29, 37] </ref>, or hand-coded persistence. 2 K. E. SEAMONS AND M. WINSLETT Traditional array order in a file is satisfactory for some applications. <p> HDF is planning to incorporate chunked physical schemas and compression, guided by results from Panda. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [21]. A few commercial DBMSs support multidimensional arrays, including Interbase [15], Orion <ref> [37] </ref>, and Stratum [29]. The POSTGRES DBMS has been enhanced to support multidimensional arrays and chunked schemas [28]. DeWitt et al. [9] describe a sequential client-server version of Paradise, a database system designed for GIS applications. A parallel version is planned for the future.
Reference: 38. <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran | A language specification. </title> <type> Technical Report ICASE Interim Report 21, </type> <institution> MS 132c, ICASE, </institution> <year> 1992. </year>
Reference-contexts: Tiles in Paradise are comparable to two-dimensional array chunks in Panda. Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [10, 11, 13, 38] </ref> the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data distribution across the processors that is appropriate for the computation.
References-found: 38

