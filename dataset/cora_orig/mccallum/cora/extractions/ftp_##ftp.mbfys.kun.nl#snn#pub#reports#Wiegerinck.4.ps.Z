URL: ftp://ftp.mbfys.kun.nl/snn/pub/reports/Wiegerinck.4.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00041.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: How Dependencies between Successive Examples Affect On-Line Learning 1  
Author: Wim Wiegerinck and Tom Heskes 
Address: Geert Grooteplein 21, 6525 EZ Nijmegen, The Netherlands.  
Affiliation: Laboratory. Department of Medical Physics and Biophysics, University of Nijmegen,  
Note: RWCP 2 Novel Functions SNN 3  
Abstract: We study the dynamics of on-line learning for a large class of neural networks and learning rules, including backpropagation for multilayer perceptrons. In this paper, we focus on the case where successive examples are dependent, and we analyze how these dependencies affect the learning process. We define the representation error and the prediction error. The representation error measures how well the environment is represented by the network after learning. The prediction error is the average error which a continually learning network makes on the next example. In the neighborhood of a local minimum of the error surface, we calculate these errors. We find that the more predictable the example presentation, the higher the representation error, i.e. the less accurate the asymptotic representation of the whole environment. Furthermore we study the learning process in the presence of a plateau. Plateaus are flat spots on the error surface, which can severely slow down the learning process. In particular, they are notorious in applications with multilayer perceptrons. Our results, which are confirmed by simulations of a multilayer perceptron learning a chaotic time series using backpropagation, explain how dependencies between examples can help the learning process to escape from a plateau. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S. </author> <year> (1967). </year> <title> A theory of adaptive pattern classifiers. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 16 </volume> <pages> 299-307. </pages>
Reference: <author> Barnard, E. </author> <year> (1992). </year> <title> Optimization for training neural nets. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3 </volume> <pages> 232-240. </pages>
Reference: <author> Benviste, A., Metivier, M., and Priouret, P. </author> <year> (1987). </year> <title> Adaptive algorithms and stochastic approximations. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Many papers on this subject, have been restricted to independent presentation of examples, i.e. the probability p (~x; n) to present an example ~x at iteration step n is given by a probability distribution (~x), independent of its predecessor. Dependencies between successive examples have been studied in <ref> (Benviste et al., 1987) </ref>[and references herein] and recently in (Kuan and White, 1993; Wiegerinck and Heskes, 1994). <p> This approach, based on (van Kampen, 1992), assumes that the distribution of weights, with initial form P (w; 0) = ffi (w w (0)), remains sharply peaked as n increases. We follow the heuristic treatment in <ref> (Benviste et al., 1987) </ref> and average the learning rule over a "mesoscopic" time scale (Hansen et al., 1993) which is much larger than the typical time scale of the example dynamics yet much smaller than the time scale on which the weights can change significantly. <p> In fact, as long as the three previously mentioned time scales remain separated, the theory may also include weight-dependent transition probabilities t (~xj~x 0 ; w; t) <ref> (Benviste et al., 1987) </ref>. The vector ~x does not neccessarily represent an example. It may have components describing other fast variables. For instance, fast variables have been utilized to study learning with momentum (Wiegerinck et al., 1994), where the adaptation rule does not satisfy (1).
Reference: <author> Brunak, S., Engelbrecht, J., and Knudsen, S. </author> <year> (1990). </year> <title> Cleaning up gene databases. Nature, </title> <publisher> 343:123. </publisher>
Reference: <author> Cachin, C. </author> <year> (1994). </year> <title> Pedagogical pattern selection strategies. </title> <booktitle> Neural Networks, </booktitle> <volume> 7 </volume> <pages> 175-181. </pages>
Reference: <author> Chauvin, Y. </author> <year> (1990). </year> <title> Generalization performance of overtrained back-propagation networks. </title> <editor> In Almeida, L. and Wellekens, C., editors, </editor> <booktitle> Lecture notes in Computer Science (vol. </booktitle> <volume> 412), </volume> <pages> pages 46-55, </pages> <address> Berlin. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In practical cases one often has access to limited number of training data. In such a case at the global minimum the network model might overfit the data and this training optimum may therefore not be optimal for generalization purposes <ref> (Chauvin, 1990) </ref>. Actually, (Hochreiter and Schmidhuber, 1995) presents an algorithm that searches for flat spots to achieve a better generalization.
Reference: <author> Finnoff, W. </author> <year> (1994). </year> <title> Diffusion approximations for the constant learning rate backpropagation algorithm and resistance to local minima. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 285-295. </pages>
Reference-contexts: However, even a local theory can be useful to understand some aspects of global properties <ref> (Finnoff, 1994) </ref>. Our study of learning on plateaus is an example of a local analysis of on-line learning which accounts for huge, non-local effects (section 5). In section 3 we heuristically derived the first terms in a hierarchy of deterministic differential equations approximating the stochastic learning process.
Reference: <author> Hansen, L., Pathria, R., and Salamon, P. </author> <year> (1993). </year> <title> Stochastic dynamics of supervised learning. </title> <journal> Journal of Physics A, </journal> <volume> 26 </volume> <pages> 63-71. </pages>
Reference-contexts: We follow the heuristic treatment in (Benviste et al., 1987) and average the learning rule over a "mesoscopic" time scale <ref> (Hansen et al., 1993) </ref> which is much larger than the typical time scale of the example dynamics yet much smaller than the time scale on which the weights can change significantly.
Reference: <author> Haykin, S. </author> <year> (1994). </year> <title> Neural Networks, A Comprehensive Foundation. </title> <publisher> MacMillan, </publisher> <address> Hamilton, Ontario. </address>
Reference-contexts: As a more realistic example consider a problem with a fixed training set of P examples. A commonly used incremental learning strategy presents in each epoch of P learning steps each example only once <ref> (Haykin, 1994) </ref>. In other words, the patterns are aranged in a randomly ordered sequence [~x (1); : : : ; ~x (P )]. It is obvious that this sequence-based or cyclic learning introduces dependencies between the examples. Moreover, the subsequent examples are negatively correlated.
Reference: <author> Hertz, J., Krogh, A., and Palmer, R. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City. </address> <publisher> 14 W. </publisher> <editor> Wiegerinck and T. Heskes Heskes, T. </editor> <year> (1994). </year> <title> On Fokker-Planck approximations of on-line learning processes. </title> <journal> Journal of Physics A, </journal> <volume> 27 </volume> <pages> 5145-5160. </pages>
Reference: <author> Heskes, T. and Kappen, B. </author> <year> (1991). </year> <title> Learning processes in neural networks. </title> <journal> Physical Review A, </journal> <volume> 44 </volume> <pages> 2718-2726. </pages>
Reference: <author> Heskes, T. and Kappen, B. </author> <year> (1992). </year> <title> Learning-parameter adjustment in neural networks. </title> <journal> Physical Review A, </journal> <volume> 45 </volume> <pages> 8885-8893. </pages>
Reference: <author> Heskes, T. and Wiegerinck, W. </author> <year> (1995). </year> <title> A theoretical comparison of batch-mode, on-line, cyclic, and almost cyclic learning. </title> <journal> IEEE Transactions on Neural Networks, </journal> <note> Accepted. </note>
Reference-contexts: Wiegerinck and T. Heskes cyclic learning than in randomized learning. Indeed, it can be shown analytically that the leading term of the fluctuations completely vanishes in cyclic learning <ref> (Heskes and Wiegerinck, 1995) </ref>. As a consequence randomized learning has a much larger chance to escape from a plateau than cyclic learning. In conclusion, we recommend natural learning (with positive correlations) if the problem at hand suffers from a plateau.
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1995). </year> <title> Simplifying neural nets by discovering flat minima. </title> <editor> In Tessauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In practical cases one often has access to limited number of training data. In such a case at the global minimum the network model might overfit the data and this training optimum may therefore not be optimal for generalization purposes (Chauvin, 1990). Actually, <ref> (Hochreiter and Schmidhuber, 1995) </ref> presents an algorithm that searches for flat spots to achieve a better generalization.
Reference: <author> Hondou, T. and Sawada, Y. </author> <year> (1994). </year> <title> Analysis of learning processes of chaotic time series by neural networks. </title> <journal> Progress of Theoretical Physics, </journal> <volume> 91 </volume> <pages> 397-402. </pages>
Reference-contexts: In both cases we initialize with the same small weights, * &lt; v fl ; w fiff &lt; *. Small random weights are often recommended to prevent early saturation of the weights (Lee et al., 1991). As reported earlier <ref> (Hondou and Sawada, 1994) </ref>, simulations show a dramatic difference between the two learning strategies in their performance learning the tent map (cf. fig 1 and fig 2). To understand this difference, we will study the weight dynamics by local linearizations.
Reference: <author> Hoptroff, R. </author> <year> (1993). </year> <title> The principles and practice of time series forecasting and business modelling using neural nets. </title> <journal> Neural Computing and Applications, </journal> <volume> 1 </volume> <pages> 59-66. </pages>
Reference: <author> Hush, D., Horne, B., and Salas, J. </author> <year> (1992). </year> <title> Error surfaces for multilayer perceptrons. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 22 </volume> <pages> 1152-1161. </pages>
Reference-contexts: In section 5 we use the results of section 3 to study the effect of dependencies when the learning process is stuck on a so-called plateau in the error surface. Plateaus are frequently present in the error surface of multilayer perceptrons <ref> (Hush et al., 1992) </ref>. Using the results in this section the remarkable difference between randomized learning and natural learning, which has been mentioned in the previous paragraph, is explained. The last section gives a brief summary and discussion. 2 W. Wiegerinck and T. <p> In this section, however, we will consider so-called "plateaus". Plateaus are flat spots on the global-error surface. They are often the cause of the extreme long learning times and/or the bad convergence results in multilayer perceptron applications with the backpropagation algorithm <ref> (Hush et al., 1992) </ref>. On a plateau, the gradient of E is negligible and H has some positive eigenvalues but also some zero eigenvalues. Plateaus can be viewed as indifferent equilibrium points of the ODE dynamics.
Reference: <author> Kohonen, T. </author> <year> (1982). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69. </pages>
Reference-contexts: 1986), where the examples ~x (n) are combinations of input vectors (x 1 (n); : : : ; x k (n)) and desired output vectors (y 1 (n); : : : ; y l (n)), as well as in unsupervised learning such as Kohonen's self-organizing rule for topological feature maps <ref> (Kohonen, 1982) </ref>, where ~x (n) stands for the input vector (x 1 (n); : : : ; x k (n)).
Reference: <author> Kuan, C. and White, H. </author> <year> (1993). </year> <title> Artificial neural networks: an econometric perspective. Econometric Reviews. </title> <publisher> in press. </publisher>
Reference: <author> Lapedes, A. and Farber, R. </author> <year> (1988). </year> <title> How neural networks work. </title> <editor> In Anderson, D., editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <pages> pages 442-456, </pages> <address> New York. </address> <publisher> American Institute of Physics. </publisher>
Reference: <author> Lee, Y., Oh, S., and Kim, M. </author> <year> (1991). </year> <title> The effect of initial weights on premature saturation in backpropagation learning. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages 765-770. </pages> <publisher> IEEE. </publisher>
Reference-contexts: In both cases we initialize with the same small weights, * &lt; v fl ; w fiff &lt; *. Small random weights are often recommended to prevent early saturation of the weights <ref> (Lee et al., 1991) </ref>. As reported earlier (Hondou and Sawada, 1994), simulations show a dramatic difference between the two learning strategies in their performance learning the tent map (cf. fig 1 and fig 2). To understand this difference, we will study the weight dynamics by local linearizations.
Reference: <author> Leen, T. and Moody, J. </author> <year> (1992). </year> <title> Weight space probability densities in stochastic learning: I. Dynamics and equilibria. </title> <editor> In Hanson, S., Cowan, J., and Giles, L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 451-458, </pages> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ludik, J. and Cloete, I. </author> <year> (1994). </year> <title> Incremental increased complexity training. </title> <editor> In Verleysen, M., editor, </editor> <booktitle> Proceedings of the European Symposium on Artificial Neural Networks '94, </booktitle> <pages> pages 161-165, </pages> <address> Brussels. </address> <publisher> D facto publications. </publisher>
Reference: <author> Mpitsos, G. and Burton, M. </author> <year> (1992). </year> <title> Convergence and divergence in neural networks: processing of chaos and biological analogy. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 605-625. </pages>
Reference: <author> Munro, P. </author> <year> (1992). </year> <title> Repeat until bored: A pattern selection strategy. </title> <editor> In Moody, J., Hanson, S., and Lippman, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 1001-1008, </pages> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Orr, G. and Leen, T. </author> <year> (1992). </year> <title> Weight space probability densities in stochastic learning: II. Transients and basin hopping times. </title> <editor> In Hanson, S., Cowan, J., and Giles, L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 507-514, </pages> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Penney, W., Coolen, A., and Sherrington, D. </author> <year> (1993). </year> <title> Coupled dynamics of fast spins and slow interactions in neural networks and spin systems. </title> <journal> Journal of Physics A, </journal> <volume> 26 </volume> <pages> 3681-3695. </pages>
Reference-contexts: Other obvious candidates for fast variables in neural network theory may be rapidly changing neuron states in recurrent networks. Thus, our framework may be applied to the analysis of the joint dynamics of neurons and weights <ref> (Penney et al., 1993) </ref>. In conclusion, the techniques for the local approximation of stochastic processes with separate time scales prove to be powerful tools for the analysis of on-line learning in neural networks. Acknowledgments We thank the referees for their useful suggestions.
Reference: <author> Radons, G. </author> <year> (1993). </year> <title> On stochastic dynamics of supervised learning. </title> <journal> Journal of Physics A, </journal> <volume> 26 </volume> <pages> 3455-3461. </pages>
Reference: <author> Ritter, H. and Schulten, K. </author> <year> (1988). </year> <title> Convergence properties of Kohonen's topology conserving maps: fluctuations, stability, and dimension selection. </title> <journal> Biological Cybernetics, </journal> <volume> 60 </volume> <month> 59-71. </month> <title> How Dependencies between Successive Examples Affect On-Line Learning 15 Rumelhart, </title> <editor> D., Hinton, G., and Williams, R. </editor> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> 323:533-536. 
Reference: <author> Schuster, H. </author> <year> (1989). </year> <title> Deterministic Chaos. </title> <type> VCH, </type> <note> Weinheim, second revised edition. </note> <author> van Kampen, N. </author> <year> (1992). </year> <title> Stochastic Processes in Physics and Chemistry. </title> <publisher> North-Holland, Amsterdam. </publisher>
Reference-contexts: We consider the tent map y (x) = 2 (1=2 jx 1=2j); 0 x 1 which we view as a dynamical system producing a chaotic time series x (n +1) = y (x (n)) <ref> (Schuster, 1989) </ref>. <p> Simulation performed with a single network. Learning parameter = 0:1. Weights initialization: * = 10 4 . Data points are plotted every 10 4 iterations. from the interval [0; 1] <ref> (Schuster, 1989) </ref>, the corresponding output y (x) is computed, and the pair fx; y (x)g is presented to the network. In both cases we initialize with the same small weights, * &lt; v fl ; w fiff &lt; *.
Reference: <author> Weigend, A. and Gershenfeld, N., </author> <title> editors (1993). Predicting the Future and Understanding the Past: A Comparison of Approaches. </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> Weigend, A., Huberman, B., and Rumelhart, D. </author> <year> (1990). </year> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209. </pages>
Reference: <author> Werbos, P. </author> <year> (1974). </year> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University. </institution>
Reference: <author> White, H. </author> <year> (1989). </year> <title> Some asymptotic results for learning in single hidden-layer feedforward network models. </title> <journal> Jour. Amer. Stat. Ass., </journal> <volume> 84 </volume> <pages> 1003-1013. </pages>
Reference: <author> Wiegerinck, W. and Heskes, T. </author> <year> (1994). </year> <title> On-line learning with time-correlated patterns. </title> <journal> Europhysics Letters, </journal> <volume> 28 </volume> <pages> 451-455. </pages>
Reference-contexts: The limitation to first-order Markov processes is not as severe as it might seem at first sight, since stationary Markov processes of any finite order k can be incorporated in the formalism by redefining the vectors ~x to include the last k examples <ref> (Wiegerinck and Heskes, 1994) </ref>. The Markov process is assumed to have a unique asymptotic or stationary distribution (~x), i.e., we assume that we can take limits like lim 1 N1 X (~x (n)) = d~x (~x)(~x) in which (~x) is some function of the patterns. <p> However, in <ref> (Wiegerinck and Heskes, 1994) </ref> it is shown that the evolution equation of P (w; n) can be expanded systematically in the small learning parameter . <p> These fluctuations are typically of order p . (Their "square" 2 (t) is of order ). In (Benviste et al., 1987; Kuan and White, 1993) a Wiener process is rigorously derived to describe these fluctuations. In <ref> (Wiegerinck and Heskes, 1994) </ref> a Fokker-Planck equation which describes these fluctuations is derived. In the next section we will study how these fluctuations affects some asymptotic error measures. The last equation (22.c) decribes a bias u between the mean w and the ODE approximation w ODE . <p> The vector ~x does not neccessarily represent an example. It may have components describing other fast variables. For instance, fast variables have been utilized to study learning with momentum <ref> (Wiegerinck et al., 1994) </ref>, where the adaptation rule does not satisfy (1). Other obvious candidates for fast variables in neural network theory may be rapidly changing neuron states in recurrent networks.
Reference: <author> Wiegerinck, W., Komoda, A., and Heskes, T. </author> <year> (1994). </year> <title> Stochastic dynamics of learning with momentum in neural networks. </title> <journal> Journal of Physics A, </journal> <volume> 27 </volume> <pages> 4425-4437. </pages>
Reference-contexts: The limitation to first-order Markov processes is not as severe as it might seem at first sight, since stationary Markov processes of any finite order k can be incorporated in the formalism by redefining the vectors ~x to include the last k examples <ref> (Wiegerinck and Heskes, 1994) </ref>. The Markov process is assumed to have a unique asymptotic or stationary distribution (~x), i.e., we assume that we can take limits like lim 1 N1 X (~x (n)) = d~x (~x)(~x) in which (~x) is some function of the patterns. <p> However, in <ref> (Wiegerinck and Heskes, 1994) </ref> it is shown that the evolution equation of P (w; n) can be expanded systematically in the small learning parameter . <p> These fluctuations are typically of order p . (Their "square" 2 (t) is of order ). In (Benviste et al., 1987; Kuan and White, 1993) a Wiener process is rigorously derived to describe these fluctuations. In <ref> (Wiegerinck and Heskes, 1994) </ref> a Fokker-Planck equation which describes these fluctuations is derived. In the next section we will study how these fluctuations affects some asymptotic error measures. The last equation (22.c) decribes a bias u between the mean w and the ODE approximation w ODE . <p> The vector ~x does not neccessarily represent an example. It may have components describing other fast variables. For instance, fast variables have been utilized to study learning with momentum <ref> (Wiegerinck et al., 1994) </ref>, where the adaptation rule does not satisfy (1). Other obvious candidates for fast variables in neural network theory may be rapidly changing neuron states in recurrent networks.
Reference: <author> Wong, F. </author> <year> (1991). </year> <title> Time series forecasting using backpropagation networks. </title> <journal> Neurocomputing, </journal> <pages> pages 147-159. </pages>
References-found: 37

