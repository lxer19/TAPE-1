URL: http://www.eecs.uic.edu/~kasif/new3.ps
Refering-URL: http://www.eecs.uic.edu/~kasif/learn-research.html
Root-URL: 
Email: kasif@cs.jhu.edu  salzberg@cs.jhu.edu  waltz@research.nj.nec.com  rachlin@cs.jhu.edu  aha@aic.nrl.navy.mil  
Title: A Probabilistic Framework for Memory-Based Reasoning  
Author: Simon Kasif Steven Salzberg David Waltz John Rachlin David Aha 
Abstract: In this paper, we propose a probabilistic framework for Memory-Based Reasoning (MBR). The framework allows us to clarify the technical merits and limitations of several recently published MBR methods and to design new variants. The proposed computational framework consists of three components: a specification language to define an adaptive notion of relevant context for a query; mechanisms for retrieving this context; and local learning procedures that are used to induce the desired action from this context. Based on the framework we derive several analytical and empirical results that shed light on MBR algorithms. We introduce the notion of an MBR transform, and discuss its utility for learning algorithms. We also provide several perspectives on memory-based reasoning from a multi-disciplinary point of view.
Abstract-found: 1
Intro-found: 1
Reference: [AKA91] <author> D. Aha, D. Kibler, and M. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1), </volume> <year> 1991. </year>
Reference-contexts: A good historical collection of nearest-neighbor algorithms is Dasarathy [Das91], which contains references going back to the 1950s. The use of local models for function estimation and smoothing in an MBR framework is described in Atkeson et al. [AMS95], who also include a review of the literature. See also <ref> [AKA91] </ref> for a variety of results on nearest-neighbor learning algorithms. Kernel density estimation and Partzen windows are broadly defined areas of non-parametric statistics that rely on memories to perform classification, to learn functions, and to estimate probability distributions [DH73]. These techniques are widely used in statistics and pattern recognition.
Reference: [AMS95] <author> Christopher Atkeson, Andrew Moore, and Stefan Schaal. </author> <title> Locally weighted learning. </title> <note> submitted for publication, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: A good historical collection of nearest-neighbor algorithms is Dasarathy [Das91], which contains references going back to the 1950s. The use of local models for function estimation and smoothing in an MBR framework is described in Atkeson et al. <ref> [AMS95] </ref>, who also include a review of the literature. See also [AKA91] for a variety of results on nearest-neighbor learning algorithms. Kernel density estimation and Partzen windows are broadly defined areas of non-parametric statistics that rely on memories to perform classification, to learn functions, and to estimate probability distributions [DH73].
Reference: [Aok65] <author> M. Aoki. </author> <title> On some convergence questions in bayesian optimization problems. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> 10 </volume> <pages> 180-182, </pages> <year> 1965. </year>
Reference-contexts: This classifier has been evaluated in recent machine learning papers (e.g., [CN89]) and many variations on the Bayesian approach have been considered in a wide range of domains. The classic work on Bayesian classifiers goes back many years, <ref> [DH73, Aok65, Hug68, Cha71, CH69, AW68] </ref>. Some recent results on naive Bayes classifiers in the machine learning community can be found in [PS90, LI92]. It is important to note that Pebls computes the same statistics as the naive Bayes classifier.
Reference: [AW68] <author> R. Albrecht and W. Werner. </author> <title> Error analysis of a statistical decision method. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 10 </volume> <pages> 34-38, </pages> <year> 1968. </year>
Reference-contexts: This classifier has been evaluated in recent machine learning papers (e.g., [CN89]) and many variations on the Bayesian approach have been considered in a wide range of domains. The classic work on Bayesian classifiers goes back many years, <ref> [DH73, Aok65, Hug68, Cha71, CH69, AW68] </ref>. Some recent results on naive Bayes classifiers in the machine learning community can be found in [PS90, LI92]. It is important to note that Pebls computes the same statistics as the naive Bayes classifier.
Reference: [Ben75] <author> J. L. Bentley. </author> <title> Multidimensional binary search trees used for associative searching. </title> <journal> Commun. ACM, </journal> <volume> 18(9) </volume> <pages> 509-517, </pages> <year> 1975. </year> <month> 21 </month>
Reference-contexts: There are numerous data structures that have been developed to search the database of instances efficiently when a query is presented, mostly based on the k-d tree framework <ref> [Ben75] </ref>. These techniques allow one to find relevant instances in logarithmic expected time (logarithmic in the size of the transformed database). Note that it may be difficult to define an efficient data structure on the original space since the attributes may be symbolic.
Reference: [BH95] <author> J. S. Breese and D. Heckerman. </author> <title> Decision theoretic case-based reasoning. </title> <booktitle> In Proc. AI and Statistics, </booktitle> <pages> pages 56-63, </pages> <year> 1995. </year>
Reference-contexts: This paradigm of performing inferences from data is often broadly referred to as memory-based reasoning (MBR). Memory-based reasoning has been used successfully in a number of domains such as classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology [YL93, ZMW92, CS93], robotics [MAS95], diagnosis <ref> [BH95] </ref>, computer vision [Ede94], and many other pattern recognition and machine learning applications. Given this popularity, it is important to define a unified framework for this work which will standardize the components of some MBR systems, unify programming notation, and provide the basis for theoretical studies.
Reference: [BV92] <author> L. Bottou and V. Vapnik. </author> <title> Local learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 888-900, </pages> <year> 1992. </year>
Reference-contexts: Finally, in a collection of papers, Vapnik and his group introduce a theoretical notion of local learning which corresponds to learning a local function at a particular point in response to a query <ref> [BV92, VB93] </ref>. 4 Value Difference Metrics To introduce the MBR framework, we will first define and give an example of an adaptive distance function. We begin by considering a traditional classification problem.
Reference: [CH69] <author> B. Chandrasekan and T. Harley. </author> <title> Comments on the mean accuracy of statistical pattern recognizers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 15 </volume> <pages> 421-423, </pages> <year> 1969. </year>
Reference-contexts: This classifier has been evaluated in recent machine learning papers (e.g., [CN89]) and many variations on the Bayesian approach have been considered in a wide range of domains. The classic work on Bayesian classifiers goes back many years, <ref> [DH73, Aok65, Hug68, Cha71, CH69, AW68] </ref>. Some recent results on naive Bayes classifiers in the machine learning community can be found in [PS90, LI92]. It is important to note that Pebls computes the same statistics as the naive Bayes classifier.
Reference: [Cha71] <author> B. Chandrasekan. </author> <title> Independence of measurements and the mean recognition accuracy. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 17 </volume> <pages> 452-456, </pages> <year> 1971. </year>
Reference-contexts: This classifier has been evaluated in recent machine learning papers (e.g., [CN89]) and many variations on the Bayesian approach have been considered in a wide range of domains. The classic work on Bayesian classifiers goes back many years, <ref> [DH73, Aok65, Hug68, Cha71, CH69, AW68] </ref>. Some recent results on naive Bayes classifiers in the machine learning community can be found in [PS90, LI92]. It is important to note that Pebls computes the same statistics as the naive Bayes classifier.
Reference: [CMSW92] <author> R. Creecy, B. Masand, S. Smith, and D. L. Waltz. </author> <title> Trading MIPS and memory for knowledge engineering. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 48-64, </pages> <year> 1992. </year>
Reference-contexts: This paradigm of performing inferences from data is often broadly referred to as memory-based reasoning (MBR). Memory-based reasoning has been used successfully in a number of domains such as classification of news articles [MLW92], census data <ref> [CMSW92] </ref>, software agents [MK93], computational biology [YL93, ZMW92, CS93], robotics [MAS95], diagnosis [BH95], computer vision [Ede94], and many other pattern recognition and machine learning applications.
Reference: [CN89] <author> P. E. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-284, </pages> <year> 1989. </year>
Reference-contexts: A new point is classified into C i if P (C i ) j P (x j jC i ) is maximal. This classifier has been evaluated in recent machine learning papers (e.g., <ref> [CN89] </ref>) and many variations on the Bayesian approach have been considered in a wide range of domains. The classic work on Bayesian classifiers goes back many years, [DH73, Aok65, Hug68, Cha71, CH69, AW68].
Reference: [CS93] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10(1) </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: This paradigm of performing inferences from data is often broadly referred to as memory-based reasoning (MBR). Memory-based reasoning has been used successfully in a number of domains such as classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology <ref> [YL93, ZMW92, CS93] </ref>, robotics [MAS95], diagnosis [BH95], computer vision [Ede94], and many other pattern recognition and machine learning applications. <p> This component can be easily automated. The MBR paradigm is more general than the above probabilistic framework. However, this overview is broad enough to capture many existing algorithms (e.g., <ref> [SW86, CS93, MK93, ZMW92, MAS95] </ref>). <p> It suggests that the distance between two examples should be related to the effect each feature has on the action taken (in this case classification). Value-difference metrics (VDM) were introduced by Stanfill and Waltz [SW86]. MVDM is a modified variant of VDM, introduced by Cost and Salzberg <ref> [CS93] </ref> and incorporated in the MBR system Pebls. Note that the VDM and MVDM operate on databases of examples with discrete attributes, (symbolic attributes or discretized real-valued features), i.e., each database entry contains a set of discrete values and possible other information such as a class label. Cost and Salzberg [CS93] <p> <ref> [CS93] </ref> and incorporated in the MBR system Pebls. Note that the VDM and MVDM operate on databases of examples with discrete attributes, (symbolic attributes or discretized real-valued features), i.e., each database entry contains a set of discrete values and possible other information such as a class label. Cost and Salzberg [CS93] demonstrated that Pebls using MVDM performs well relative to several other learning algorithms on a number of practical problems (see also [ZMW92]). 5 The MBR Transform In this section we present a probabilistic framework for MBR. <p> That is, we can learn to perform more accurately on a more general class of problems then the original probabilistic model would allow us to do. In particular, we report on an experimental and analytical comparison between a particular MBR system called Pebls <ref> [CS93] </ref> and a widely-used probabilistic method known as the naive Bayesian classifier, which is specified by the simple probabilistic network in Figure 2. We would like to emphasize that we are not advocating specific learned metrics and their induced MBR transforms.
Reference: [Das91] <author> Belur V. Dasarathy, </author> <title> editor. Nearest neighbor (NN) norms: NN pattern classification techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: The earliest algorithms that might properly be classed as MBR methods date back to work in the 1960s on the application of local regression to a set of nearest neighbors, a technique known as kernel regression [Nad64, Roy66]. A good historical collection of nearest-neighbor algorithms is Dasarathy <ref> [Das91] </ref>, which contains references going back to the 1950s. The use of local models for function estimation and smoothing in an MBR framework is described in Atkeson et al. [AMS95], who also include a review of the literature. See also [AKA91] for a variety of results on nearest-neighbor learning algorithms.
Reference: [DH73] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: See also [AKA91] for a variety of results on nearest-neighbor learning algorithms. Kernel density estimation and Partzen windows are broadly defined areas of non-parametric statistics that rely on memories to perform classification, to learn functions, and to estimate probability distributions <ref> [DH73] </ref>. These techniques are widely used in statistics and pattern recognition. The use of the term memory-based reasoning in a broad context was introduced by Stanfill and Waltz [SW86], who also introduced the Value Difference Metric (VDM) to define similarity when 3 using symbolic-valued features. <p> This classifier has been evaluated in recent machine learning papers (e.g., [CN89]) and many variations on the Bayesian approach have been considered in a wide range of domains. The classic work on Bayesian classifiers goes back many years, <ref> [DH73, Aok65, Hug68, Cha71, CH69, AW68] </ref>. Some recent results on naive Bayes classifiers in the machine learning community can be found in [PS90, LI92]. It is important to note that Pebls computes the same statistics as the naive Bayes classifier.
Reference: [Ede94] <author> S. Edelman. </author> <title> Representation, similarity and the chorus of prototypes. Minds and Machines, </title> <year> 1994. </year>
Reference-contexts: Memory-based reasoning has been used successfully in a number of domains such as classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology [YL93, ZMW92, CS93], robotics [MAS95], diagnosis [BH95], computer vision <ref> [Ede94] </ref>, and many other pattern recognition and machine learning applications. Given this popularity, it is important to define a unified framework for this work which will standardize the components of some MBR systems, unify programming notation, and provide the basis for theoretical studies. <p> In computer vision, some forms of memory-based reasoning have been a popular theme in applications such as character recognition and face recognition. For example, there is the notion of a chorus of prototypes where an object is defined again by a vector of distances to other objects <ref> [Ede94] </ref>. That is, each object is mapped into a probability distribution and the distance between two objects is determined by computing a standard distance (geodesic) in a space of probability distributions. A different but obviously related notion is the idea of radial basis functions defined over clusters of instances.
Reference: [Fri94] <author> J. H. Friedman. </author> <title> Flexible metric nearest neighbor classification. </title> <type> Technical report, </type> <institution> Stanford University, Statistics Dept., </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Some very recent work in statistics generalizes traditional nearest neighbor learning with adaptive neighborhood techniques. There the issue is finding the correct (relevant) neighborhood to a given query in a classification task <ref> [HT94, Fri94] </ref>.
Reference: [GS94] <author> Z. Gilboa and D. Schmeidler. </author> <title> Case-based decision theory. </title> <type> Technical report, </type> <institution> Nortwestern University, </institution> <year> 1994. </year>
Reference-contexts: In this domain, the notion of retrieving a relevant context or a document is related to our work. Recent ground breaking work in economics introduced the notion of case-based decision theory <ref> [GS94] </ref>, where the utility of an action in a particular state is computed by using a kernel density estimation technique over a set of stored memories. Some very recent work in statistics generalizes traditional nearest neighbor learning with adaptive neighborhood techniques.
Reference: [Hol93] <author> R. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference-contexts: 90.6 80.5 WI-Breast 96.7 96.7 95.6 Mushroom 99.7 100.0 100.0 While the performance on Pebls on these benchmarks is quite good, the nature of the concept class is not perfectly understood for these data sets (and, in fact, some of these datasets are known to be quite easy to classify <ref> [Hol93] </ref>). Therefore, one cannot make any strong conclusions from these results. We primarily wanted to demonstrate that MVDM-based systems can be effective for some standard benchmarks. In [SC92, ZMW92] MVDM is used to deliver a relatively high accuracy on protein secondary structure prediction.
Reference: [HT94] <author> T. Hastie and R. Tibshirani. </author> <title> Discriminant adaptive nearest neighbor classification. </title> <type> Technical report, </type> <institution> Stanford University, Statistics Dept., </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Some very recent work in statistics generalizes traditional nearest neighbor learning with adaptive neighborhood techniques. There the issue is finding the correct (relevant) neighborhood to a given query in a classification task <ref> [HT94, Fri94] </ref>.
Reference: [Hug68] <author> G. Hughes. </author> <title> On the mean accuracy of statistical pattern recognizers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 14 </volume> <pages> 55-63, </pages> <year> 1968. </year>
Reference-contexts: This classifier has been evaluated in recent machine learning papers (e.g., [CN89]) and many variations on the Bayesian approach have been considered in a wide range of domains. The classic work on Bayesian classifiers goes back many years, <ref> [DH73, Aok65, Hug68, Cha71, CH69, AW68] </ref>. Some recent results on naive Bayes classifiers in the machine learning community can be found in [PS90, LI92]. It is important to note that Pebls computes the same statistics as the naive Bayes classifier.
Reference: [JTT92] <author> D. Jones, W. Taylor, and J. Thornton. </author> <title> The rapid generation of mutation data matrices from protein sequences. </title> <journal> CABIOS, </journal> <volume> 8(3) </volume> <pages> 275-282, </pages> <year> 1992. </year>
Reference-contexts: than to BEAR? If our training set consists of protein sequences, we face the similar problem of determining functional similarity based on sequence similarity; i.e., does the protein's structure change if one replaces amino acid G by amino acid H? (Indeed biologists have produced mutation matrices that capture this notion <ref> [JTT92] </ref>). The key technical question is how to learn a "good" distance metric from data (and optionally partial models provided by domain experts).
Reference: [KM78] <author> J. B. Kruskal and Wish M. </author> <title> Multidimensional Scaling. </title> <publisher> Sage Publications, </publisher> <year> 1978. </year>
Reference-contexts: This issue raises a large number of questions which have been addressed in statistical research on relevance and similarity, and that resulted in methods such as multidimensional scaling, singular value decomposion, principal components analysis, and factor analysis <ref> [KM78, She80] </ref> and vector quantization. Since the notions of relevance and similarity are somewhat ill-defined without a specific task in mind, there is substantial literature on axiomatic definitions of relevance and similarity [Tve77].
Reference: [Lau95] <author> S. L. Lauritzen. </author> <title> The em algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis, </journal> <pages> pages 191-201, </pages> <year> 1995. </year>
Reference-contexts: Our framework captures this and other natural distributions such as statistical mixtures [TSM85], probabilistic hierarchies and complex probabilistic models [Pea88], and models that include hidden variables <ref> [Lau95] </ref>.) The model itself will vary depending on the goals of the system; in fact, the same database can be used with different models to produce different MBR systems that solve different problems. In each case the model will induce a different transformation.
Reference: [LI92] <author> P. Langley and W. Iba. </author> <title> An analysis of Bayesian classifiers. </title> <booktitle> In Proc. Tenth Natl. Conf. on Artificial Intelligence, </booktitle> <pages> pages 223-228, </pages> <address> Menlo Park, CA, 1992. </address> <publisher> AAAI Press. </publisher> <pages> 22 </pages>
Reference-contexts: The classic work on Bayesian classifiers goes back many years, [DH73, Aok65, Hug68, Cha71, CH69, AW68]. Some recent results on naive Bayes classifiers in the machine learning community can be found in <ref> [PS90, LI92] </ref>. It is important to note that Pebls computes the same statistics as the naive Bayes classifier. Thus, during training the running time of both methods is the same. However, while Bayes summarizes these statistics in simple rules, Pebls uses them as part of an MBR classifier.
Reference: [MAS95] <author> A. Moore, C. Atkeson, and S. Schaal. </author> <title> Memory-based learning for control. </title> <note> Artificial Intelligence Review (to appear), </note> <year> 1995. </year>
Reference-contexts: This paradigm of performing inferences from data is often broadly referred to as memory-based reasoning (MBR). Memory-based reasoning has been used successfully in a number of domains such as classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology [YL93, ZMW92, CS93], robotics <ref> [MAS95] </ref>, diagnosis [BH95], computer vision [Ede94], and many other pattern recognition and machine learning applications. Given this popularity, it is important to define a unified framework for this work which will standardize the components of some MBR systems, unify programming notation, and provide the basis for theoretical studies. <p> This component can be easily automated. The MBR paradigm is more general than the above probabilistic framework. However, this overview is broad enough to capture many existing algorithms (e.g., <ref> [SW86, CS93, MK93, ZMW92, MAS95] </ref>).
Reference: [MK93] <author> P. Maes and R. Kozierok. </author> <title> Learning interface agents. </title> <booktitle> In Proc. of the Eleventh National Conf. on Artificial Intelligence, </booktitle> <pages> pages 459-465, </pages> <address> Washington, D.C., 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This paradigm of performing inferences from data is often broadly referred to as memory-based reasoning (MBR). Memory-based reasoning has been used successfully in a number of domains such as classification of news articles [MLW92], census data [CMSW92], software agents <ref> [MK93] </ref>, computational biology [YL93, ZMW92, CS93], robotics [MAS95], diagnosis [BH95], computer vision [Ede94], and many other pattern recognition and machine learning applications. <p> This component can be easily automated. The MBR paradigm is more general than the above probabilistic framework. However, this overview is broad enough to capture many existing algorithms (e.g., <ref> [SW86, CS93, MK93, ZMW92, MAS95] </ref>).
Reference: [MLW92] <author> B. Masand, G. Linoff, and D. L. Waltz. </author> <title> Classifying news stories using memory based reasoning. </title> <booktitle> In Proceedings of the SIGIR, </booktitle> <pages> pages 59-65, </pages> <year> 1992. </year>
Reference-contexts: This paradigm of performing inferences from data is often broadly referred to as memory-based reasoning (MBR). Memory-based reasoning has been used successfully in a number of domains such as classification of news articles <ref> [MLW92] </ref>, census data [CMSW92], software agents [MK93], computational biology [YL93, ZMW92, CS93], robotics [MAS95], diagnosis [BH95], computer vision [Ede94], and many other pattern recognition and machine learning applications.
Reference: [Mur95] <author> P.M. Murphy. </author> <title> UCI repository of machine learning databases a machine-readable data repository. </title> <institution> Maintained at the Department of Information and Computer Science, University of California, Irvine. </institution> <note> Anonymous FTP from ics.uci.edu in the directory pub/machine-learning-databases, </note> <year> 1995. </year>
Reference-contexts: For completeness, we provide comparisons to nearest-neighbor (NN) using the overlap metric (which counts the number of feature value mismatches between two examples). To get an initial sense of relative performance, we selected eight datasets from the University of California at Irvine's repository of machine learning databases <ref> [Mur95] </ref>. For the experiments below, all methods treated the feature values as symbols, even if the values were numbers. In other words, if a feature had values 1, 2, and 3, we would treat those exactly the same as if they were A, B, and C.
Reference: [Nad64] <author> E.A. Nadaraya. </author> <title> On estimating regression. </title> <journal> Theory of Probability and Its Applications, </journal> <volume> 9 </volume> <pages> 141-142, </pages> <year> 1964. </year>
Reference-contexts: The earliest algorithms that might properly be classed as MBR methods date back to work in the 1960s on the application of local regression to a set of nearest neighbors, a technique known as kernel regression <ref> [Nad64, Roy66] </ref>. A good historical collection of nearest-neighbor algorithms is Dasarathy [Das91], which contains references going back to the 1950s. The use of local models for function estimation and smoothing in an MBR framework is described in Atkeson et al. [AMS95], who also include a review of the literature.
Reference: [Omo87] <author> S. Omohundro. </author> <title> Efficient algorithms with neural network behavior. </title> <journal> Complex Systems, </journal> <pages> pages 273-347, </pages> <year> 1987. </year>
Reference-contexts: The VDM is an adaptive distance metric that adjusts itself to a database of examples, and can then be used for retrieval (see Section 4). Tree-based methods for partitioning data into regions (e.g., <ref> [Omo89, Omo87] </ref>) such as k-d trees or decision trees [Qui93] also can be used to define a relevant local neighborhood. Thus, instead of seeing a decision tree as a classification device in the MBR context, a decision tree defines a static partitioning of space into regions.
Reference: [Omo89] <author> S. Omohundro. </author> <title> Five balltree construction algorithms. </title> <type> Technical Report 89-063, </type> <institution> International Computer Science Institute, Berkeley, California, </institution> <year> 1989. </year>
Reference-contexts: The VDM is an adaptive distance metric that adjusts itself to a database of examples, and can then be used for retrieval (see Section 4). Tree-based methods for partitioning data into regions (e.g., <ref> [Omo89, Omo87] </ref>) such as k-d trees or decision trees [Qui93] also can be used to define a relevant local neighborhood. Thus, instead of seeing a decision tree as a classification device in the MBR context, a decision tree defines a static partitioning of space into regions.
Reference: [Pea88] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: This specification includes a set of random variables associated with different features in the data. The user also specifies a set of probabilistic assumptions such as priors and independence assumptions. Such knowledge can be expressed using Bayes networks (see Pearl <ref> [Pea88] </ref>), which provide graphical support for specifying complex probability distributions. The difficulty of this step is obviously related to the complexity of the model that user wants to impose on the data. In most cases, this can be done by using a library of models or using domain knowledge. 3. <p> Each edge of the tree is therefore associated with a matrix of conditional probabilities of the variable X i given the class Y . in probabilistic networks, and thus can be obtained directly from the network <ref> [Pea88] </ref>. As mentioned in the introduction, this paper is not addressing the automatic derivation of the transformation from the model. We primarily want to motivate the practical usefulness of probabilistic MBR methods, analyze their performance and suggest the possibility automating the transform. <p> ; x 4 ; x 5 ; x 6 ) = p (x 1 jx 0 )p (x 2 jx 0 )p (x 3 jx 1 )p (x 4 jx 1 )p (x 5 jx 2 )p (x 6 jx 2 )p (x 0 ) The reader is referred to <ref> [Pea88] </ref> for a detailed information on the computational and statistical advantages of this factorization. Let us now assume we have a large database of tuples of the form (x 0 ; x 1 ; x 2 ; x 3 ; x 4 ; x 5 ; x 6 ). <p> Given the evidence E we can easily compute the probability distribution of x 0 given E, using the efficient inference algorithm for trees outlined in <ref> [Pea88] </ref>. The algorithm will perform optimally (minimizing Bayes risk) if the model is correct. However, in practice the model may not be correct. Thus the model will not provide adequate performance on this prediction task even if infinite amount of data is available. <p> 3 ; x 4 ; x 5 ; x 6 ) to a tuple (x 0 ); (x 1 ); (x 2 ); (x 3 ); (x 4 ); (x 5 ); (x 6 ) where the vector is defined as (x i ) = p (Ejx i ) (see <ref> [Pea88] </ref> for additional information). Note that attributes x 1 and x 2 are in fact getting "filled", although no specific direct evidence for these variables has been provided. Instead, we assume these attributes assume the value p (Ejx 1 ) and p (Ejx 2 ) respectively. <p> We also note that terms such as p (Ejx 2 ) can be factored into simpler expressions because of the independence assumptions made in the structure of the tree network <ref> [Pea88] </ref>. We now can match the transformed query to each item in the database as specified above for MVDM. <p> Our framework captures this and other natural distributions such as statistical mixtures [TSM85], probabilistic hierarchies and complex probabilistic models <ref> [Pea88] </ref>, and models that include hidden variables [Lau95].) The model itself will vary depending on the goals of the system; in fact, the same database can be used with different models to produce different MBR systems that solve different problems. In each case the model will induce a different transformation.
Reference: [PS90] <author> M. Pazzani and W. Sarrett. </author> <title> Average case analysis of conjunctive learning algorithms. </title> <booktitle> In Proc. of the Workshop on Machine Learning, </booktitle> <pages> pages 339-347, </pages> <year> 1990. </year>
Reference-contexts: The classic work on Bayesian classifiers goes back many years, [DH73, Aok65, Hug68, Cha71, CH69, AW68]. Some recent results on naive Bayes classifiers in the machine learning community can be found in <ref> [PS90, LI92] </ref>. It is important to note that Pebls computes the same statistics as the naive Bayes classifier. Thus, during training the running time of both methods is the same. However, while Bayes summarizes these statistics in simple rules, Pebls uses them as part of an MBR classifier.
Reference: [PTL93] <author> F. Pereira, N. Tishby, and L. Lee. </author> <title> Distributional clustering of English words. </title> <booktitle> In Proc. of the 30-th Meeting of the Assoc. for Computational Linguistics, </booktitle> <pages> pages 183-190, </pages> <year> 1993. </year>
Reference-contexts: In natural language processing we find some similar notions. For example, one can define the "semantics" of a verb as a probability distribution over the set of nouns that follow it <ref> [PTL93] </ref>. Thus, each verb is mapped into a point in a high-dimensional real-valued space. Then the similarity of two verbs can be computed by computing the distance between the two probability distributions (such as the Kullback-Leibler divergence).
Reference: [Qui93] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The VDM is an adaptive distance metric that adjusts itself to a database of examples, and can then be used for retrieval (see Section 4). Tree-based methods for partitioning data into regions (e.g., [Omo89, Omo87]) such as k-d trees or decision trees <ref> [Qui93] </ref> also can be used to define a relevant local neighborhood. Thus, instead of seeing a decision tree as a classification device in the MBR context, a decision tree defines a static partitioning of space into regions.
Reference: [RKSA94] <author> J. Rachlin, S. Kasif, S. Salzberg, and D. Aha. </author> <title> Towards a better understanding of memory-based and bayesian classifiers. </title> <booktitle> In Proc. of the Eleventh Internatl. Conf. on Machine Learning, </booktitle> <pages> pages 242-250, </pages> <address> New Brunswick, NJ, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: In Section 9.1 we consider a number of basic functional distributions for 2-dimensional data on a finite grid. We show relative learning curves for Pebls and Bayes. Experiments on additional artificial data (generated by a Markov process) are described in Rachlin et al. <ref> [RKSA94] </ref>, which also reports on artificially generated data in higher dimensional spaces. 9.1 Two-dimensional feature space Our first set of tests considered 10,000 points on an evenly spaced 100 fi 100 grid on the unit square. For simplicity, the examples have just two class labels, A and B.
Reference: [Roy66] <author> R.M. Royall. </author> <title> A Class of Nonparametric Estimators of a Smooth Regression Function. </title> <type> PhD thesis, </type> <institution> Department of Statistics, Stanford University, </institution> <year> 1966. </year>
Reference-contexts: The earliest algorithms that might properly be classed as MBR methods date back to work in the 1960s on the application of local regression to a set of nearest neighbors, a technique known as kernel regression <ref> [Nad64, Roy66] </ref>. A good historical collection of nearest-neighbor algorithms is Dasarathy [Das91], which contains references going back to the 1950s. The use of local models for function estimation and smoothing in an MBR framework is described in Atkeson et al. [AMS95], who also include a review of the literature.
Reference: [SC92] <author> S. Salzberg and S. </author> <title> Cost. Predicting protein secondary structure with a nearest-neighbor algorithm. </title> <journal> Journal of Molecular Biology, </journal> <volume> 227 </volume> <pages> 371-374, </pages> <year> 1992. </year>
Reference-contexts: Therefore, one cannot make any strong conclusions from these results. We primarily wanted to demonstrate that MVDM-based systems can be effective for some standard benchmarks. In <ref> [SC92, ZMW92] </ref> MVDM is used to deliver a relatively high accuracy on protein secondary structure prediction. That is, it matches or exceeds the accuracy of carefully tuned neural networks on a relatively sparse training data which is rather surprising.
Reference: [SDLC93] <author> D.J. Spiegelhalter, A.P. Dawid, S.L. Lauritzen, and R.G. Cowell. </author> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8(3) </volume> <pages> 219-283, </pages> <year> 1993. </year>
Reference-contexts: The framework introduced above is aimed at the goal of developing a set of automated procedures that use both standard ("off the shelf") and novel components to build flexible MBR systems. We can use a variety of existing software systems to specify probabilistic graphical models <ref> [SDLC93] </ref>. Such packages often provide effective procedures to learn the parameters of the model. The system then needs to code the transformation on the space induced by the model. In many cases this 2 transformation is straightforward (see below). <p> The transformation of values into probabilities defined here will be referred to as the MBR transform. The MBR transform has several desirable theoretical properties, as discussed in later sections. 6 Towards Probabilistic Reasoning Using MBR The approach outline above generalizes to more complex graphical probabilistic models such as in <ref> [SDLC93] </ref> which may include hidden variables and therefore perform constructive induction.
Reference: [She80] <author> R. N. Sheppard. </author> <title> Multidimensional scaling, tree fitting and clustering. </title> <booktitle> Science, </booktitle> <pages> pages 1317-1323, </pages> <year> 1980. </year>
Reference-contexts: This issue raises a large number of questions which have been addressed in statistical research on relevance and similarity, and that resulted in methods such as multidimensional scaling, singular value decomposion, principal components analysis, and factor analysis <ref> [KM78, She80] </ref> and vector quantization. Since the notions of relevance and similarity are somewhat ill-defined without a specific task in mind, there is substantial literature on axiomatic definitions of relevance and similarity [Tve77].
Reference: [SW86] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <year> 1986. </year> <month> 23 </month>
Reference-contexts: This formalism is motivated by the method originally proposed by Stanfill and 1 Computer Science Dept., Johns Hopkins U., Baltimore, MD 2 NEC Research Institute, Princeton, NJ 08540 3 Navy Center for Applied Research in AI, Naval Research Laboratory, Washington DC 20375 1 Waltz <ref> [SW86] </ref>. The framework clarifies the technical merits and limitations of the original approach and the papers the followed it. <p> The first and the last components are common to all MBR systems that we are familiar with. Steps 2-5 (i.e. the use of graphical probabilistic models to define relevant context for answering queries) are unique to our proposed framework and generalize the original proposal by <ref> [SW86] </ref>. The framework introduced above is aimed at the goal of developing a set of automated procedures that use both standard ("off the shelf") and novel components to build flexible MBR systems. We can use a variety of existing software systems to specify probabilistic graphical models [SDLC93]. <p> This component can be easily automated. The MBR paradigm is more general than the above probabilistic framework. However, this overview is broad enough to capture many existing algorithms (e.g., <ref> [SW86, CS93, MK93, ZMW92, MAS95] </ref>). <p> This component can be easily automated. The MBR paradigm is more general than the above probabilistic framework. However, this overview is broad enough to capture many existing algorithms (e.g., [SW86, CS93, MK93, ZMW92, MAS95]). In addition, this framework clarifies and analyzes the advantages of the technique originally proposed in <ref> [SW86] </ref>, and suggests new variants that do not exist in the literature. 3 Summary of Related Research Inducing distance metrics and judging relevance using complex probabilistic models and data is a very basic research topic. <p> These techniques are widely used in statistics and pattern recognition. The use of the term memory-based reasoning in a broad context was introduced by Stanfill and Waltz <ref> [SW86] </ref>, who also introduced the Value Difference Metric (VDM) to define similarity when 3 using symbolic-valued features. The VDM is an adaptive distance metric that adjusts itself to a database of examples, and can then be used for retrieval (see Section 4). <p> It suggests that the distance between two examples should be related to the effect each feature has on the action taken (in this case classification). Value-difference metrics (VDM) were introduced by Stanfill and Waltz <ref> [SW86] </ref>. MVDM is a modified variant of VDM, introduced by Cost and Salzberg [CS93] and incorporated in the MBR system Pebls.
Reference: [TC91] <author> H. Turtle and W. Bruce Croft. </author> <title> Evaluation of an inference network-based retrieval model. </title> <journal> ACM Transactions on Information Systems, </journal> <year> 1991. </year>
Reference-contexts: In information retrieval, we find a recent use of Bayes networks to specify the notion of relevance of a document to a topic or a query <ref> [TC91] </ref>. In this domain, the notion of retrieving a relevant context or a document is related to our work.
Reference: [TSM85] <author> D. M. Titterington, A. Smith, and U. E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: The model implicit in this transformation is a probabilistic model of a set of points normally distributed around a small number of centroids, which can be easily specified by a Bayes network. Our framework captures this and other natural distributions such as statistical mixtures <ref> [TSM85] </ref>, probabilistic hierarchies and complex probabilistic models [Pea88], and models that include hidden variables [Lau95].) The model itself will vary depending on the goals of the system; in fact, the same database can be used with different models to produce different MBR systems that solve different problems.
Reference: [Tve77] <author> A. Tversky. </author> <title> Features of similarity. </title> <journal> Psychological Review, </journal> <volume> 84 </volume> <pages> 324-357, </pages> <year> 1977. </year>
Reference-contexts: Since the notions of relevance and similarity are somewhat ill-defined without a specific task in mind, there is substantial literature on axiomatic definitions of relevance and similarity <ref> [Tve77] </ref>. The issues addressed in this paper (e.g, probabilistic models of relevance or similarity) are quite broad and have been addressed in psychology, statistics, information retrieval, AI, pattern recognition, computer vision and natural language processing. Thus, it is impossible to enumerate all possible references to potentially relevant papers.
Reference: [Val84] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: In effect, the value of x will be ignored, exactly as it should be. Stronger results than this can be derived. For example, we can show good performance, in the PAC-learnability sense <ref> [Val84] </ref>, after seeing a small number of examples. Let e be the probability of error of the classifier. Then it is not difficult to show that after seeing C=*(ln 1=ffi) examples (where C is some constant), P (e &gt; *) &lt; 1 ffi.
Reference: [VB93] <author> V. Vapnik and L. Bottou. </author> <title> Local learning algorithms for pattern recognition and dependency estimation. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 893-909, </pages> <year> 1993. </year>
Reference-contexts: Finally, in a collection of papers, Vapnik and his group introduce a theoretical notion of local learning which corresponds to learning a local function at a particular point in response to a query <ref> [BV92, VB93] </ref>. 4 Value Difference Metrics To introduce the MBR framework, we will first define and give an example of an adaptive distance function. We begin by considering a traditional classification problem.
Reference: [YL93] <author> T.-M. Yi and E. Lander. </author> <title> Protein secondary structure prediction using nearest-neighbor methods. </title> <journal> Journal of Molecular Biology, </journal> <volume> 232 </volume> <pages> 1117-1129, </pages> <year> 1993. </year>
Reference-contexts: This paradigm of performing inferences from data is often broadly referred to as memory-based reasoning (MBR). Memory-based reasoning has been used successfully in a number of domains such as classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology <ref> [YL93, ZMW92, CS93] </ref>, robotics [MAS95], diagnosis [BH95], computer vision [Ede94], and many other pattern recognition and machine learning applications.
Reference: [ZMW92] <author> Xiru Zhang, Jill P. Mesirov, and David L. Waltz. </author> <title> A hybrid system for protein secondary structure prediction. </title> <journal> Journal of Molecular Biology, </journal> <volume> 225 </volume> <pages> 1049-1063, </pages> <year> 1992. </year> <month> 24 </month>
Reference-contexts: This paradigm of performing inferences from data is often broadly referred to as memory-based reasoning (MBR). Memory-based reasoning has been used successfully in a number of domains such as classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology <ref> [YL93, ZMW92, CS93] </ref>, robotics [MAS95], diagnosis [BH95], computer vision [Ede94], and many other pattern recognition and machine learning applications. <p> This component can be easily automated. The MBR paradigm is more general than the above probabilistic framework. However, this overview is broad enough to capture many existing algorithms (e.g., <ref> [SW86, CS93, MK93, ZMW92, MAS95] </ref>). <p> Cost and Salzberg [CS93] demonstrated that Pebls using MVDM performs well relative to several other learning algorithms on a number of practical problems (see also <ref> [ZMW92] </ref>). 5 The MBR Transform In this section we present a probabilistic framework for MBR. We show a case study where the definition of a distance metric such as MVDM follows naturally from a simple probabilistic model. We start with a simple example of a binary classification problem. <p> Therefore, one cannot make any strong conclusions from these results. We primarily wanted to demonstrate that MVDM-based systems can be effective for some standard benchmarks. In <ref> [SC92, ZMW92] </ref> MVDM is used to deliver a relatively high accuracy on protein secondary structure prediction. That is, it matches or exceeds the accuracy of carefully tuned neural networks on a relatively sparse training data which is rather surprising.
References-found: 48

