URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P596.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts96.htm
Root-URL: http://www.mcs.anl.gov
Email: foster@mcs.anl.gov  carl@compbio.caltech.edu  snir@watson.ibm.com  
Title: Generalized Communicators in the Message Passing Interface  
Author: Ian Foster Carl Kesselman Marc Snir T. J. 
Address: Argonne, IL 60439, U.S.A.  Pasadena, CA 91125, U.S.A.  P.O. Box 218, Yorktown Heights, NY 10598, U.S.A.  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  Beckman Institute California Institute of Technology  Watson Research Center IBM  
Abstract: We propose extensions to the Message Passing Interface (MPI) that generalize the MPI communicator concept to allow multiple communication endpoints per process, dynamic creation of endpoints, and the transfer of endpoints between processes. The generalized communicator construct can be used to express a wide range of interesting communication structures, including collective communication operations involving multiple threads per process, communications between dynamically created threads, and object-oriented applications in which communications are directed to specific objects. Furthermore, this enriched functionality can be provided in a manner that preserves backward compatibility with MPI. We describe the proposed extensions, illustrate their use with examples, and discuss implementation issues. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Foster, J. Geisler, and S. Tuecke. </author> <title> MPI on the I-WAY: A wide-area, multimethod implementation of the Message Passing Interface. </title> <booktitle> In Proceedings of the 1996 MPI Developers Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: A distinguished value might be used to detect termination */ while (!done) MPI_Recv (msg, 1, MPI_INT, 0, 99, receiver, status); MPI_Comm_free (receiver); - sender_side (MPI_Comm comm, int nbr) - MPI_Comm sender; MPI_Comm_name rnames <ref> [1] </ref>; int msg, rslots [1]; /* Create a new communicator object */ MPI_Comm_create_local (sender); /* Receive LCO name from other process, add to send list */ MPI_Recv (rnames, 1, MPI_CNAME, nbr, 99, comm, status); rslots [0] = 0; MPI_Add_send_slots (sender, 1, rnames, rslots); /* Send messages to other process, on newly <p> A distinguished value might be used to detect termination */ while (!done) MPI_Recv (msg, 1, MPI_INT, 0, 99, receiver, status); MPI_Comm_free (receiver); - sender_side (MPI_Comm comm, int nbr) - MPI_Comm sender; MPI_Comm_name rnames <ref> [1] </ref>; int msg, rslots [1]; /* Create a new communicator object */ MPI_Comm_create_local (sender); /* Receive LCO name from other process, add to send list */ MPI_Recv (rnames, 1, MPI_CNAME, nbr, 99, comm, status); rslots [0] = 0; MPI_Add_send_slots (sender, 1, rnames, rslots); /* Send messages to other process, on newly created channel */ for <p> In this environment, a send slot can be represented as a global pointer to a remote queue corresponding to a receive slot, and a send operation can be implemented as a remote enqueue operation. This technique has been used to construct an implementation of ordinary MPI <ref> [1] </ref>. 6. Conclusions We have presented extensions to the MPI communicator that permit the representation of more general and flexible communication structures. These extensions are backwards compatible with MPI, meaning that any existing MPI program will execute correctly in a system that supports the new constructs.
Reference: [2] <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> The Nexus approach to integrating multithreading and communication. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1996. To appear. </note>
Reference-contexts: However, one can imagine optimiza tions that recognize sets of LCOs representing MPI communicator or intercommunicator structures, and revert to the more compact representation in this case. An alternative implementation approach would use a communication library such as Nexus <ref> [2] </ref> that provides global pointer and single-sided communication operations. In this environment, a send slot can be represented as a global pointer to a remote queue corresponding to a receive slot, and a send operation can be implemented as a remote enqueue operation.
Reference: [3] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A high-performance, portable implementation of the MPI message passing interface standard. </title> <type> Technical Report ANL/MCS-TM-213, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1996. </year>
Reference-contexts: Implementation Issues The modifications to an MPI implementation required to support our proposed MPI extensions are inevitably focused within the MPI communicator construct. Hence, we introduce this discussion of implementation issues by describing how communicators are represented within one widely used MPI implementation, MPICH <ref> [3] </ref>. The two principal components of an MPI communicator as represented in MPICH are a process group and a context. The process group is represented as an ordered set of process identifiers, stored as an integer array. A process's rank in a group refers to its index in this array.
Reference: [4] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: 1. Introduction One of the most important features of the Message Passing Interface (MPI) <ref> [4, 7] </ref> is the communicator, which allows the programmer to define unique communication spaces within which a set of processes can communicate without fear of interference. Communicators are created by collective calls that create a local instance of a communicator object in each of a set of processes.
Reference: [5] <author> M. Haines, P. Mehrotra, and D. Cronk. Ropes: </author> <title> Support for collective operations among distributed threads. </title> <type> Technical Report 95-36, </type> <institution> Institute for Computer Application in Science and Engineering, </institution> <year> 1995. </year>
Reference-contexts: This feature makes it possible to perform collective operations involving multiple threads <ref> [5] </ref>, where the number of threads may be greater than the number of processes, a situation that can arise on shared-memory multiprocessors or in programs that create one thread per application "task." structures that can be specified using the port construct.
Reference: [6] <author> A. Skjellum, N. Doss, K. Viswanathan, A. Chowdappa, and P. </author> <title> Bangalore. Extending the message passing interface. </title> <booktitle> In Proc. 1994 Scalable Parallel Libraries Conf. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Furthermore, the port becomes a first-class object and can be sent to other processes via MPI messages. Other extensions to the MPI communicator have been proposed. For example, Skjellum et al. <ref> [6] </ref> propose mechanisms that allow for a richer set of collective operations over communicators, as well as extensions that support multithreaded execution. The extensions presented here are orthogonal to these proposals.
Reference: [7] <author> M. Snir, S. W. Otto, S. Huss-Lederman, D. W. Walker, and J. Dongarra. </author> <title> MPI: The Complete Reference. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1996. </year> <month> 8 </month>
Reference-contexts: 1. Introduction One of the most important features of the Message Passing Interface (MPI) <ref> [4, 7] </ref> is the communicator, which allows the programmer to define unique communication spaces within which a set of processes can communicate without fear of interference. Communicators are created by collective calls that create a local instance of a communicator object in each of a set of processes.
References-found: 7

