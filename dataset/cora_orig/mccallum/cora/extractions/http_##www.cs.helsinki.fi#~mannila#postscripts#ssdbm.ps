URL: http://www.cs.helsinki.fi/~mannila/postscripts/ssdbm.ps
Refering-URL: http://www.cs.helsinki.fi/research/pmdm/datamining/publications.html
Root-URL: 
Email: E-mail: Heikki.Mannila@cs.helsinki.fi  
Title: Data mining: machine learning, statistics, and databases  
Author: Heikki Mannila 
Web: URL: http://www.cs.helsinki.fi/~mannila/  
Address: FIN-00014 Helsinki, Finland  
Affiliation: Department of Computer Science University of Helsinki,  
Abstract: Knowledge discovery in databases and data mining aim at semiautomatic tools for the analysis of large data sets. We give an overview of the area and present some of the research issues, especially from the database angle. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of ACM SIG-MOD Conference on Management of Data (SIG-MOD'93), </booktitle> <pages> pages 207 - 216, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A relation r = ft 1 ; : : : ; t n g over the schema R is a matrix with columns R and n rows, each row being a vector of length p of 0's and 1's. An association rule <ref> [1] </ref> about r is an expression of the form X ) B, where X R and B 2 R n X. <p> Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 12, 14, 28] </ref>. A typical approach [1, 2] is to use that fact that all subsets of a frequent set are also frequent. First find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs. <p> Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways [1, 2, 12, 14, 28]. A typical approach <ref> [1, 2] </ref> is to use that fact that all subsets of a frequent set are also frequent. First find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs.
Reference: [2] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I. Verkamo. </author> <title> Fast discovery of association rules. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307 - 328. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 12, 14, 28] </ref>. A typical approach [1, 2] is to use that fact that all subsets of a frequent set are also frequent. First find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs. <p> Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways [1, 2, 12, 14, 28]. A typical approach <ref> [1, 2] </ref> is to use that fact that all subsets of a frequent set are also frequent. First find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs. <p> This method has the advantage that rows that do not contribute to any frequent set will not be inspected more than once. For comparisons of the two approaches, see <ref> [2, 14, 28] </ref>. The algorithms described above work quite nicely on large input relations.
Reference: [3] <author> P. A. Boncz, W. Quak, and M. L. Kersten. </author> <title> Monet and its geographical extensions: a novel approach to high-performance GIS processing. </title> <booktitle> In EDBT'96, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: A very interesting experiment in this direction is the work on the Monet database server developed at CWI in the Netherlands by Martin Ker-sten and others <ref> [15, 3] </ref>. The Monet system is based on the vertical partitioning of the relations: a relation with k attributes is decomposed into k relations, each with two attributes: the OID and one of the original attributes.
Reference: [4] <author> L. De Raedt and M. Bruynooghe. </author> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93), </booktitle> <pages> pages 1058 - 1053, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: task is to find the set P I (d; P) = fp 2 P j p occurs sufficiently often in d and p is interestingg: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [4, 5, 18, 19, 20] </ref>; some theoretical results can be found in [24], and a suggested logical formalism in [17]. For association rules, the pattern class is the set of all rules of the form X ) B, and a rule is interesting if its confidence is sufficiently high.
Reference: [5] <author> L. De Raedt and S. Dzeroski. </author> <title> First-order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70:375 - 392, </volume> <year> 1994. </year>
Reference-contexts: task is to find the set P I (d; P) = fp 2 P j p occurs sufficiently often in d and p is interestingg: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [4, 5, 18, 19, 20] </ref>; some theoretical results can be found in [24], and a suggested logical formalism in [17]. For association rules, the pattern class is the set of all rules of the form X ) B, and a rule is interesting if its confidence is sufficiently high.
Reference: [6] <author> J. Elder IV and D. </author> <title> Pregibon. A statistical perspective on knowledge discovery in databases. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 83 - 113. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: A more fashionable term is exploratory data analysis (EDA) [29], which stressed the supremacy of data as guiding the analysis process. KDD and EDA have very similar aims and methods. According to the interesting statistical perspective on KDD by Elder and Pregibon <ref> [6] </ref>, the focus of statis tics has gradually moved from model estimation to model selection. Instead of looking for the parameter values that make a model fit the data well, also the model structure is part of the search process.
Reference: [7] <author> U. M. Fayyad, S. G. Djorgovski, and N. Weir. </author> <title> Automating the analysis and cataloging of sky surveys. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 471 - 494. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Efficient support for such iteration is one important development topic in KDD. Prominent applications of KDD include health care data, financial applications, and scientific data [26, 19]. One of the more spectacular applications is the SKICAT system <ref> [7] </ref>, which operates on 3 terabytes of image data, producing a classification of approximately 2 billion sky objects into a few classes. The task is obviously impossible to do manually.
Reference: [8] <author> U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From data mining to knowledge discovery: An overview. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, pages 1 -34. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Discovering knowledge from data should therefore be seen as a process containing several steps: 1. understanding the domain; 2. preparing the data set; 3. discovering patterns (data mining), 4. postprocessing of discovered patterns, and 5. putting the results into use. (See <ref> [8] </ref> for a slightly different process model and ex cellent discussion.) Understanding the domain of the data is naturally a prerequisite for extracting anything useful: the user of a KDD system has to have some sort of understanding about the application area before any valuable information can be obtained. <p> The pattern discovery phase in KDD is the step where the interesting and frequently occurring patterns are discovered from the data. In this paper we follow the terminology introduced in <ref> [8] </ref>: data mining refers to the pattern discovery part of knowledge discovery. Elsewhere, especially in industry, data mining is often used as a synonym for KDD.
Reference: [9] <editor> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: There is a suspicion that there might be nuggets of useful information hiding in the masses of unanalyzed or underanalyzed data, and therefore semiautomatic methods for locating interesting information from data would be useful. There are several successful applications of data mining. See <ref> [9] </ref> for a recent overview of the area. This paper gives a short discussion of some of the database-oriented research issues in knowledge discovery. We start in Section 2 by briefly discussing the KDD process, basic data mining techniques, and listing some prominent applications.
Reference: [10] <editor> Z. Galil and E. Ukkonen, editors. </editor> <booktitle> 6th Annual Symposium on Combinatorial Pattern Matching (CPM 95) : Espoo, </booktitle> <address> Finland, </address> <month> July </month> <year> 1995, </year> <booktitle> volume 937 of Lecture Notes in Computer Science, </booktitle> <address> Berlin, 1995. </address> <publisher> Springer. </publisher>
Reference-contexts: The view of data mining as locating frequently occurring and interesting patterns from data suggests that data mining can benefit from the extensive research done in the area of combinatorial pattern matching; see, e.g., <ref> [10] </ref>. 7 Towards higher-level data mining Currently, data mining research and development consists mainly of isolated applications.
Reference: [11] <author> J. Gray, A. Bosworth, A. Layman, and H. Pira--hesh. </author> <title> Data Cube: A relational aggregation operator generalizing group-by, </title> <booktitle> cross-tab, and subtotals. In 12th International Conference on Data Engineering (ICDE'96), </booktitle> <pages> pages 152 - 159, </pages> <address> New Orleans, Louisiana, </address> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Another, less obvious example is given by the collection of frequent sets of a 0-1 valued relation [23]: the collection of frequent sets can be used to give approximate answers to arbitrary boolean queries about the data, even though the frequent sets represent only conjunctive concepts. The data cube <ref> [11] </ref> can also be viewed as a condensed representation for a class of queries. Similarly, in computational geometry the notion of an "-approximation [27] is closely related. Developing condensed representations for various classes of patterns seems a promising way of improving the effectiveness of data mining algorithms.
Reference: [12] <author> J. Han and Y. Fu. </author> <title> Discovery of multiple-level association rules from large databases. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 420 - 431, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 12, 14, 28] </ref>. A typical approach [1, 2] is to use that fact that all subsets of a frequent set are also frequent. First find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs.
Reference: [13] <author> K. Hatonen, M. Klemettinen, H. Mannila, P. Ronkainen, and H. Toivonen. </author> <title> Knowledge discovery from telecommunication network alarm databases. </title> <booktitle> In 12th International Conference on Data Engineering (ICDE '96), </booktitle> <pages> pages 115 - 122, </pages> <address> New Orleans, Louisiana, </address> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: For example, a telecommunications network alarm database is used to collect all the notifications about abnormal situations in the network. The number of event types is around 200, and there are 1000-10000 alarms per day <ref> [13] </ref>. As a first step in analyzing such data, one can try to find which event types occur frequently close together. Denoting by E the set of all event types, an episode ' is a partially ordered set of elements from E.
Reference: [14] <author> M. Holsheimer, M. Kersten, H. Mannila, and H. Toivonen. </author> <title> A perspective on databases and data mining. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD'95), </booktitle> <pages> pages 150 - 155, </pages> <address> Mon-treal, Canada, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 12, 14, 28] </ref>. A typical approach [1, 2] is to use that fact that all subsets of a frequent set are also frequent. First find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs. <p> This method has the advantage that rows that do not contribute to any frequent set will not be inspected more than once. For comparisons of the two approaches, see <ref> [2, 14, 28] </ref>. The algorithms described above work quite nicely on large input relations. <p> The system is built on the extensive use of main memory, has an extensible set of basic operations, and supports shared-memory parallelism. Experiments with Monet on data mining applications have produced quite good results <ref> [15, 14] </ref>. 8 Condensed representations We remarked in Section 2 that KDD is an iterative process.
Reference: [15] <author> M. Holsheimer, M. Kersten, and A. Siebes. </author> <title> Data surveyor: Searching the nuggets in parallel. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 447 - 467. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: [ fp 2 C j p is sufficiently frequent in dg; C := new candidates generated from F ; od; output interesting patterns from F ; In addition to discovering association rules or fre-quent episodes, istantiations of this algorithm include finding keys of relations [21], hill-climbing searches for best descriptions <ref> [15, 19] </ref>, etc. In hill-climbing, the set C will contain only the neighbors of the current "most interesting" pattern. For other strategies, a possible problem with this algorithm is that the test for interestingness is applied only at the end. <p> A very interesting experiment in this direction is the work on the Monet database server developed at CWI in the Netherlands by Martin Ker-sten and others <ref> [15, 3] </ref>. The Monet system is based on the vertical partitioning of the relations: a relation with k attributes is decomposed into k relations, each with two attributes: the OID and one of the original attributes. <p> The system is built on the extensive use of main memory, has an extensible set of basic operations, and supports shared-memory parallelism. Experiments with Monet on data mining applications have produced quite good results <ref> [15, 14] </ref>. 8 Condensed representations We remarked in Section 2 that KDD is an iterative process.
Reference: [16] <author> T. Imielinski. </author> <title> A database view on data mining. </title> <booktitle> Invited talk at the KDD'95 conference. </booktitle>
Reference-contexts: Among others, Imielinski <ref> [16] </ref> has eloquently argued that data mining is today at the same state as data management was in the 1960's: then all data management applications were ad hoc; only the advent of the relational model and powerful query languages made it possible to develop applications fast.
Reference: [17] <author> M. Jaeger, H. Mannila, and E. Weydert. </author> <title> Data mining as selective theory extraction in probabilistic logic. </title> <booktitle> In SIGMOD'96 Data Mining Workshop, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: in d and p is interestingg: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning [4, 5, 18, 19, 20]; some theoretical results can be found in [24], and a suggested logical formalism in <ref> [17] </ref>. For association rules, the pattern class is the set of all rules of the form X ) B, and a rule is interesting if its confidence is sufficiently high. For finding episodes, the patterns are the episodes and there need not be any interestingness criterion.
Reference: [18] <author> J.-U. Kietz and S. Wrobel. </author> <title> Controlling the complexity of learning in logic through syntactic and task-oriented models. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 335 - 359. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: task is to find the set P I (d; P) = fp 2 P j p occurs sufficiently often in d and p is interestingg: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [4, 5, 18, 19, 20] </ref>; some theoretical results can be found in [24], and a suggested logical formalism in [17]. For association rules, the pattern class is the set of all rules of the form X ) B, and a rule is interesting if its confidence is sufficiently high.
Reference: [19] <author> W. Kloesgen. </author> <title> Efficient discovery of interesting statements in databases. </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 4(1):53 - 69, </volume> <year> 1995. </year>
Reference-contexts: Efficient support for such iteration is one important development topic in KDD. Prominent applications of KDD include health care data, financial applications, and scientific data <ref> [26, 19] </ref>. One of the more spectacular applications is the SKICAT system [7], which operates on 3 terabytes of image data, producing a classification of approximately 2 billion sky objects into a few classes. The task is obviously impossible to do manually. <p> task is to find the set P I (d; P) = fp 2 P j p occurs sufficiently often in d and p is interestingg: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [4, 5, 18, 19, 20] </ref>; some theoretical results can be found in [24], and a suggested logical formalism in [17]. For association rules, the pattern class is the set of all rules of the form X ) B, and a rule is interesting if its confidence is sufficiently high. <p> [ fp 2 C j p is sufficiently frequent in dg; C := new candidates generated from F ; od; output interesting patterns from F ; In addition to discovering association rules or fre-quent episodes, istantiations of this algorithm include finding keys of relations [21], hill-climbing searches for best descriptions <ref> [15, 19] </ref>, etc. In hill-climbing, the set C will contain only the neighbors of the current "most interesting" pattern. For other strategies, a possible problem with this algorithm is that the test for interestingness is applied only at the end.
Reference: [20] <author> H. Mannila and K.-J. Raiha. </author> <title> Design by example: An application of Armstrong relations. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33(2):126 - 141, </volume> <year> 1986. </year>
Reference-contexts: task is to find the set P I (d; P) = fp 2 P j p occurs sufficiently often in d and p is interestingg: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [4, 5, 18, 19, 20] </ref>; some theoretical results can be found in [24], and a suggested logical formalism in [17]. For association rules, the pattern class is the set of all rules of the form X ) B, and a rule is interesting if its confidence is sufficiently high.
Reference: [21] <author> H. Mannila and K.-J. Raiha. </author> <title> Design of Relational Databases. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Wokingham, UK, </address> <year> 1992. </year>
Reference-contexts: p in d; F := F [ fp 2 C j p is sufficiently frequent in dg; C := new candidates generated from F ; od; output interesting patterns from F ; In addition to discovering association rules or fre-quent episodes, istantiations of this algorithm include finding keys of relations <ref> [21] </ref>, hill-climbing searches for best descriptions [15, 19], etc. In hill-climbing, the set C will contain only the neighbors of the current "most interesting" pattern. For other strategies, a possible problem with this algorithm is that the test for interestingness is applied only at the end.
Reference: [22] <author> H. Mannila and H. Toivonen. </author> <title> Discovering generalized episodes using minimal occurrences. </title> <type> Technical Report C-1996-12, </type> <institution> University of Helsinki, Department of Computer Science, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: The algorithm can be further improved by using incremental recognition of episodes; see [25] for details, and <ref> [22] </ref> for extensions.
Reference: [23] <author> H. Mannila and H. Toivonen. </author> <title> Multiple uses of frequent sets and condensed representations. </title> <type> Technical Report C-1996-13, </type> <institution> University of Helsinki, Department of Computer Science, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: Another, less obvious example is given by the collection of frequent sets of a 0-1 valued relation <ref> [23] </ref>: the collection of frequent sets can be used to give approximate answers to arbitrary boolean queries about the data, even though the frequent sets represent only conjunctive concepts. The data cube [11] can also be viewed as a condensed representation for a class of queries.
Reference: [24] <author> H. Mannila and H. Toivonen. </author> <title> On an algorithm for finding all interesting sentences. </title> <booktitle> In Cybernetics and Systems Research '96, </booktitle> <address> Vienna, Austria, </address> <month> Apr. </month> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: based on one nice but simple observation 1 This is actually not quite true: the running time is O (N F 0 ), where F 0 is the sum of the sizes of the frequent sets and the sets in the candidate collection C during the operation of the algorithm <ref> [24] </ref>. (subsets of frequent sets must be frequent), and use straightforward implementation techniques. <p> 2 P j p occurs sufficiently often in d and p is interestingg: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning [4, 5, 18, 19, 20]; some theoretical results can be found in <ref> [24] </ref>, and a suggested logical formalism in [17]. For association rules, the pattern class is the set of all rules of the form X ) B, and a rule is interesting if its confidence is sufficiently high.
Reference: [25] <author> H. Mannila, H. Toivonen, and A. I. Verkamo. </author> <title> Discovering frequent episodes in sequences. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining (KDD'95), </booktitle> <pages> pages 210 - 215, </pages> <address> Montreal, Canada, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The algorithm can be further improved by using incremental recognition of episodes; see <ref> [25] </ref> for details, and [22] for extensions.
Reference: [26] <author> C. J. Matheus, G. Piatetsky-Shapiro, and D. Mc-Neill. </author> <title> Selecting and reporting what is interesting. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 495 - 515. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Efficient support for such iteration is one important development topic in KDD. Prominent applications of KDD include health care data, financial applications, and scientific data <ref> [26, 19] </ref>. One of the more spectacular applications is the SKICAT system [7], which operates on 3 terabytes of image data, producing a classification of approximately 2 billion sky objects into a few classes. The task is obviously impossible to do manually.
Reference: [27] <author> K. Mulmuley. </author> <title> Computational Geometry: An Introduction Through Randomized Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: The data cube [11] can also be viewed as a condensed representation for a class of queries. Similarly, in computational geometry the notion of an "-approximation <ref> [27] </ref> is closely related. Developing condensed representations for various classes of patterns seems a promising way of improving the effectiveness of data mining algorithms.
Reference: [28] <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 432 - 444, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways <ref> [1, 2, 12, 14, 28] </ref>. A typical approach [1, 2] is to use that fact that all subsets of a frequent set are also frequent. First find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs. <p> This method has the advantage that rows that do not contribute to any frequent set will not be inspected more than once. For comparisons of the two approaches, see <ref> [2, 14, 28] </ref>. The algorithms described above work quite nicely on large input relations.
Reference: [29] <author> J. W. Tukey. </author> <title> Exploratory Data Analysis. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1977. </year>
Reference-contexts: In statistics the term data mining has been used for a long time, often in slightly derogatory fashion, as referring to data analysis without clearly formulated hypotheses. A more fashionable term is exploratory data analysis (EDA) <ref> [29] </ref>, which stressed the supremacy of data as guiding the analysis process. KDD and EDA have very similar aims and methods. According to the interesting statistical perspective on KDD by Elder and Pregibon [6], the focus of statis tics has gradually moved from model estimation to model selection.
References-found: 29

