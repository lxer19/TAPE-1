URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-299.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: SCHEDULING AND CODE GENERATION FOR PARALLEL ARCHITECTURES  Written under the direction of  
Author: BY TAO YANG 
Degree: A dissertation submitted to the Graduate School|New Brunswick  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Professor Apostolos Gerasoulis and approved by  
Date: May, 1993  
Note: Graduate Program in Computer Science  
Address: New Jersey  Brunswick, New Jersey  
Affiliation: Rutgers, The State University of  New  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> T. Adam, K.M. Chandy and J. R. Dickson, </author> <title> A comparison of list schedules for parallel processing Systems, </title> <journal> CACM, </journal> <month> 17:12 </month> <year> (1974), </year> <pages> 685-690. </pages>
Reference-contexts: ENDWHILE question is what choices of priority lists will consistently give schedules that are "close" to the optimum schedule. Experimentally, Adam, Chandy and Dickson <ref> [1] </ref> answered the above question by conducting an extensive empirical performance study of CP algorithm with other four priority list scheduling algorithms. <p> Also Hu [53] shows that CP is optimal for scheduling tree DAGs of equal task sizes on p processors. Previous research has emphasized the worst case analysis in list scheduling. The experimental results in <ref> [1] </ref>, however, raise an interesting question. Are there any un derlying properties that characterize the near optimality of certain list scheduling algorithms such as CP ? Furthermore the proof of optimality of CP for equal weight DAGs raises another interesting question. <p> derived from the following expression: P T i = max (T others ; clock + max n x 2F LIST L (x)) where T others = 8 &gt; : max n j 2W LIST fST (j) + L (j)g W LIST 6= ; 0 otherwise: All heuristic algorithms studied in <ref> [1] </ref> can be considered in this framework. We assume non-backtracking to avoid the high complexity, then the values of P T 0 ; P T 1 ; , 81 P T v are monotonically increasing. <p> If ffi = 0 then ffi-lopt heuristic is called local optimum. The important question is, of course, if local optimality implies near-optimality in the sense of Adam, Chandy and Dickson <ref> [1] </ref>. While it is difficult to prove such result theoretically, we do know that the CP heuristic is near optimal [1]. Therefore it would be of interest to see if CP is local optimum. We expect local optimum heuristics to perform better on average than heuristics with large nonzero ffi. <p> The important question is, of course, if local optimality implies near-optimality in the sense of Adam, Chandy and Dickson <ref> [1] </ref>. While it is difficult to prove such result theoretically, we do know that the CP heuristic is near optimal [1]. Therefore it would be of interest to see if CP is local optimum. We expect local optimum heuristics to perform better on average than heuristics with large nonzero ffi. We present an example to demonstrate this point. <p> We expect local optimum heuristics to perform better on average than heuristics with large nonzero ffi. We present an example to demonstrate this point. In Figure 5.5, we show results of two list scheduling algorithms studied in <ref> [1] </ref>: the CP and HLFNET (highest levels first with no estimated computation times). The priority list for CP is fn 3 ; n 1 ; n 2 ; n 4 ; n 5 g since L (3) = 6 L (1) = 4 : : : L (5) = 1. <p> Corollary 5.1 CP scheduling is local optimum for DAGs with equal weights. Local optimality, of course, does not imply optimality but we expect a near optimum performance of heuristics that possess this property. Adam, Chandy and Dickson <ref> [1] </ref> have provided the experimental evidence for such a performance for a local optimum heuristic. <p> For fine grain DAGs: (a) Ready list scheduling is superior to free list scheduling in general. (b) The variation of the performance for heuristics that use ready list scheduling and critical path information is within 15% on average. This result is similar to Adam, Chandy and Dickson <ref> [1] </ref> where they have observed a 10% variation between CP and random priority list when communication is zero. <p> The degree of parallelism for this case is m = 14. The root of 146 A [0,2,4,6] A [1,3,5,7] A [0] A [4] A [2] A [6] A <ref> [1] </ref> A [5] A [3] A [7] SPLIT MERGE the MERGE tree which combine large vectors has large arithmetic computation while while the leaf of tree has a small operation.
Reference: [2] <author> I. Ahmad, A. Choudhary, G. Fox, K. Parasuram, R. Ponnusamy, S. Ranka, and R. Thakur, </author> <title> Implementation and scalability of Fortran 90D intrinsic functions on distributed memory machines, </title> <type> NPAC Tech Report, </type> <institution> Syracuse University, </institution> <year> 1993. </year>
Reference-contexts: The reason is that the task partitioning does not provide sufficient parallelism and also does not distribute arithmetic load evenly among tasks. The degree of parallelism for this case is m = 14. The root of 146 A [0,2,4,6] A [1,3,5,7] A [0] A [4] A <ref> [2] </ref> A [6] A [1] A [5] A [3] A [7] SPLIT MERGE the MERGE tree which combine large vectors has large arithmetic computation while while the leaf of tree has a small operation. <p> We have also proposed other communication optimizations and integrated them in one code generation scheme. Fortran D project at Rice [51] and Fortran 90D at Syracuse <ref> [2] </ref> have emphasized compiling and code generation system for high performance Fortran. The mapping scheme is data driven, i.e. let a user define the data mapping and let the system determine the mapping of computation based on the owner compute rule.
Reference: [3] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante, </author> <title> An overview of the PTRAN analysis system for multiprocessing, </title> <journal> J. of Parallel and Dist Computing, </journal> <month> Oct 88. </month>
Reference-contexts: There are three generally distinct ways in addressing the programming difficulties for parallel architectures: The first approach considers the problem of automatic parallelization and scheduling from sequential programs. The emphasis has been in the development of compilers or software tools that will assist in programming parallel architectures <ref> [3, 80] </ref>. One of the obstacles in the development of parallelizing compilers is the automatic identification of embedded parallelism in a sequential program. This is because the precise dependence analysis problem is not tractable in a polynomial time. Significant progress has been made towards finding approximate solutions [83, 95]. <p> The degree of parallelism for this case is m = 14. The root of 146 A [0,2,4,6] A [1,3,5,7] A [0] A [4] A [2] A [6] A [1] A [5] A <ref> [3] </ref> A [7] SPLIT MERGE the MERGE tree which combine large vectors has large arithmetic computation while while the leaf of tree has a small operation. <p> Automatic derivation of DAG parallelism is important for PYRROS. The general dependence analysis is NP-complete but for some special cases there exist polynomial algorithms [83, 95]. And systems such as ParaS-cope [58], PAT [7] and PTRAN <ref> [3] </ref> are helpful for users to uncover the parallelism. Cytron, Hind and Hsieh [28] presented an approach for DAG generation. Balasundaram and Kennedy [8] described a method to seek coarse grain parallelism by summarizing data access.
Reference: [4] <author> F. L. Alvarado, </author> <title> The sparse matrix manipulation system users manual, </title> <institution> University of Wisconsin, </institution> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: The reason is that the task partitioning does not provide sufficient parallelism and also does not distribute arithmetic load evenly among tasks. The degree of parallelism for this case is m = 14. The root of 146 A [0,2,4,6] A [1,3,5,7] A [0] A <ref> [4] </ref> A [2] A [6] A [1] A [5] A [3] A [7] SPLIT MERGE the MERGE tree which combine large vectors has large arithmetic computation while while the leaf of tree has a small operation. <p> Bhattacharjya [12] has developed a system that generates the sparse graphs using the GJ algorithm. It starts from a dense GJ task graph and deletes those tasks that perform operations on zeros. The test data come from Alvarado's sparse matrix manipulation system <ref> [4] </ref>. Figure 7.16 shows the dense graph that solves 8 fi 8 matrix. and a sparse DAG after deleting useless tasks.
Reference: [5] <author> M. A. Al-Mouhamed, </author> <title> Lower Bound on the Number of Processors and Time for Scheduling Precedence Graphs with Communication Costs, </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> vol. 16, no. 12, </volume> <pages> pp. 1390-1401, </pages> <year> 1990. </year>
Reference-contexts: Chretienne [20] has proposed an optimal algorithm for a fork and join, which zeroes multiple edges. The complexity of his algorithm is O (m log B) where B = minf P m i=1 t i ; fi 1 + t 1 g + t x . Al-Mouhamed <ref> [5] </ref> has also used the idea of zeroing multiple incoming edges of a task to compute a lower bound for scheduling a DAG, using an O (m 2 ) algorithm, but no feasible schedules that reach the bound are produced by this algorithm. <p> The degree of parallelism for this case is m = 14. The root of 146 A [0,2,4,6] A [1,3,5,7] A [0] A [4] A [2] A [6] A [1] A <ref> [5] </ref> A [3] A [7] SPLIT MERGE the MERGE tree which combine large vectors has large arithmetic computation while while the leaf of tree has a small operation.
Reference: [6] <author> F.D. Anger, J. Hwang, and Y. Chow, </author> <title> Scheduling with sufficient loosely coupled processors, </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> vol. 9, </volume> <pages> pp. 87-92, </pages> <year> 1990. </year>
Reference-contexts: Another is that none of these general algorithms determines the optimum for special DAGs such as join, fork and coarse grain tree. Even though polynomial time algorithms for special types of DAGs exist in the literature, Chretienne [21], Anger, Hwang and Chow <ref> [6] </ref>, these algorithms are not general. As far as we know no scheduling algorithm exists that works well for arbitrary graphs, finds optimal schedules for special DAGs and also has a low complexity. <p> Chretienne [21] developed an O (v) optimal algorithm for this tree. Anger, Hwang and Chow <ref> [6] </ref> proposed an O (v) optimal algorithm for tree DAGs with the condition that all communication edge weights are smaller than the task node execution weights, which 64 are special cases of coarse grain tree DAGs. These two algorithms are only specific to this kind of trees. <p> The reason is that the task partitioning does not provide sufficient parallelism and also does not distribute arithmetic load evenly among tasks. The degree of parallelism for this case is m = 14. The root of 146 A [0,2,4,6] A [1,3,5,7] A [0] A [4] A [2] A <ref> [6] </ref> A [1] A [5] A [3] A [7] SPLIT MERGE the MERGE tree which combine large vectors has large arithmetic computation while while the leaf of tree has a small operation.
Reference: [7] <author> B. Appelbe, C. McDowell, and K. Smith, Start/Pat: </author> <title> A parallel programming toolkit, </title> <booktitle> IEEE Software 6, </booktitle> <month> 4 (July </month> <year> 1989), </year> <pages> pp. 29-38. </pages>
Reference-contexts: The degree of parallelism for this case is m = 14. The root of 146 A [0,2,4,6] A [1,3,5,7] A [0] A [4] A [2] A [6] A [1] A [5] A [3] A <ref> [7] </ref> SPLIT MERGE the MERGE tree which combine large vectors has large arithmetic computation while while the leaf of tree has a small operation. <p> Automatic derivation of DAG parallelism is important for PYRROS. The general dependence analysis is NP-complete but for some special cases there exist polynomial algorithms [83, 95]. And systems such as ParaS-cope [58], PAT <ref> [7] </ref> and PTRAN [3] are helpful for users to uncover the parallelism. Cytron, Hind and Hsieh [28] presented an approach for DAG generation. Balasundaram and Kennedy [8] described a method to seek coarse grain parallelism by summarizing data access.
Reference: [8] <author> V. Balasundaram and K. Kennedy, </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations, </title> <booktitle> Proc. of 1989 ACM SIGPLAN, </booktitle> <pages> pp. 41-53. </pages>
Reference-contexts: And systems such as ParaS-cope [58], PAT [7] and PTRAN [3] are helpful for users to uncover the parallelism. Cytron, Hind and Hsieh [28] presented an approach for DAG generation. Balasundaram and Kennedy <ref> [8] </ref> described a method to seek coarse grain parallelism by summarizing data access. Irigoin and Triolet [55], Wolf and Lam [96] addressed a program partitioning method for loop parallelism.
Reference: [9] <author> A. Bakre and A. Gerasoulis, </author> <title> Numerical applications in LINDA on message-passing architectures, </title> <type> Report, </type> <year> 1992. </year>
Reference-contexts: Systems such as HeNCE/PVM [10] have been shown very useful in synthesizing program modules into a large software package. Scheduling techniques can be further used to improve the utilization of computer resources. Clustering techniques have been also shown useful for LINDA programming in <ref> [9] </ref>. * Incorporating dependence analysis and program partitioning. 156 We have seen that coarse grain parallelism is most successful for current message passing architectures. Automatic derivation of DAG parallelism is important for PYRROS. The general dependence analysis is NP-complete but for some special cases there exist polynomial algorithms [83, 95].
Reference: [10] <author> A. Beguelin, J.J. Dongarra, G.A. Geist, R. Manchek, </author> <title> V.S. Sunderam, Graphical development tools for network-based concurrent supercomputing, </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <year> 1991, </year> <month> pp.435-444. </month>
Reference-contexts: TaskGra-pher uses task duplication heuristics. Work by Chung and Ranka [19] also uses task duplication. The complexity of their algorithms is over O (v 2 ). HeNCE/PVM by Beguelin et. al <ref> [10] </ref> is a software system for exploiting large grain parallelism over heterogeneous computer networks. A graphic interface is provided for users to define program modules and their parallelism. The system integrates those computation modules and executes them over a network. <p> Thus for irregular problems, special scheduling and code generation systems can be developed for automating the optimization process. Other applications of PYRROS algorithms could be for coarse grain parallelism on a network of computers. Systems such as HeNCE/PVM <ref> [10] </ref> have been shown very useful in synthesizing program modules into a large software package. Scheduling techniques can be further used to improve the utilization of computer resources.
Reference: [11] <author> F. Berman, </author> <title> Experience with an automatic solution to the mapping problem. </title> <editor> In L. Jamieson, D. Gannon an R. Douglass, Eds., </editor> <title> The Characteristics of Parallel Algorithms (MIT press, </title> <year> 1987) </year> <month> 307-334. </month>
Reference-contexts: The SUPERB project by Zima et. al. [106] uses the similar techniques. Some recent research on data mapping by Gupta and Baner-jee [49] has been automating data mapping process such that better load balancing can be achieved. PREP-P by Berman <ref> [11] </ref> is a software system that automatically maps undirected communication graphs onto CHiP machine architectures. OREGAMI/LaRCS by Lo et. al. [68] software tool is used for the mapping of task graphs onto processors. Both systems use graph embedding algorithms to assign a set of tasks into processors.
Reference: [12] <author> P. Bhattacharjya, </author> <title> Gauss-Jordan elimination for sparse matrix on message-passing architecture, </title> <type> Report, </type> <year> 1991. </year>
Reference-contexts: We have performed some tests on PYRROS performance for some sparse task graphs. Bhattacharjya <ref> [12] </ref> has developed a system that generates the sparse graphs using the GJ algorithm. It starts from a dense GJ task graph and deletes those tasks that perform operations on zeros. The test data come from Alvarado's sparse matrix manipulation system [4].
Reference: [13] <author> S. Bokhari, </author> <title> On the mapping problem, </title> <journal> IEEE Trans. Comput. </journal> <volume> C-30 3(1981), </volume> <pages> 207-214. 175 </pages>
Reference-contexts: Determining an optimal physical mapping is hard because this problem can be reduced to graph isomorphism problem. Currently we use a heuristic algorithm due to Bokhari <ref> [13] </ref>. This algorithm starts from an initial assignment, then performs a series of pairwise interchanges so that the F (CC) is reduced monotonically. The complexity of this algorithm is O (p 3 ).
Reference: [14] <author> D. Callahan, and K. Kennedy, </author> <title> Compiling Programs for Distributed-memory Multiprocessors, </title> <journal> Journal of Supercomputing, </journal> <volume> Vol. 2, </volume> <year> 1988, </year> <pages> pp. 151-169. </pages>
Reference-contexts: In [17], it is suggested that semi-automatic tools might be more appropriate for parallelism detection. The second approach allows a user to specify parallelism information as an input and 2 then a compiler system generates efficient code <ref> [14, 16, 34, 86, 94, 106] </ref>. For example, KALI [61] and FORTRAN D [51] both provide parallel constructs in their language. Even if all parallelism is available, program scheduling and communication optimization are crucial to ensure good performance of automatically produced code. <p> 4 4 4 4 2 S 2 S 2 S 3 S 3 S 1 1 1 4 4 4 4 (a) Program task graph. (b) PT=12 1 1 the distribution of data items then the system will assign program statements to processors based on so called "owner computes rule" <ref> [14, 86] </ref>: a processor executes a computation unit if this unit modifies the data that the processor owns. Assume that a user distributes array x and y according to the element indices, i.e. processor i owns x (i) and y (i).
Reference: [15] <author> D. Cann, </author> <title> Retire Fortran? A Debate Rekindled, </title> <booktitle> Proc. of Supercomputing 91, IEEE, </booktitle> <pages> pp 264-272, </pages> <year> 1991. </year>
Reference: [16] <author> M. Chen, Y.I Choo and J. Li, </author> <title> Compiling Parallel Programs by Optimizing Performance, </title> <journal> J. of Supercomputing, </journal> <volume> 2, </volume> <month> 171-207 </month> <year> (1988). </year>
Reference-contexts: In [17], it is suggested that semi-automatic tools might be more appropriate for parallelism detection. The second approach allows a user to specify parallelism information as an input and 2 then a compiler system generates efficient code <ref> [14, 16, 34, 86, 94, 106] </ref>. For example, KALI [61] and FORTRAN D [51] both provide parallel constructs in their language. Even if all parallelism is available, program scheduling and communication optimization are crucial to ensure good performance of automatically produced code. <p> The initial overhead of such run-time compilation is usually high but this cost is amortized over all iterations. PYRROS algorithms could be useful at the first iteration. The CRYSTAL project by Chen, Choo and Li <ref> [16] </ref> has been targeted at developing a parallelizing compiler system for their functional language. Li and Chen [67] have recognized the importance of effective communication and have developed a communication routine selection system in CRYSTAL to generate code for message passing architectures.
Reference: [17] <author> D. Y. Cheng and D. M. Pase, </author> <title> An Evaluation of Automatic and Interactive Parallel Programming Tools, </title> <booktitle> Proc. of IEEE Supercomputing 91, </booktitle> <pages> pp. 412-423. </pages>
Reference-contexts: Significant progress has been made towards finding approximate solutions [83, 95]. However, the false dependencies in approximated solutions could have a negative impact on performance. This was recently demonstrated by a study on the performance of automatic parallelization compilers <ref> [17] </ref>, where it was found that "automatic tools produce insufficient performance improvement" due to false dependencies. In [17], it is suggested that semi-automatic tools might be more appropriate for parallelism detection. <p> However, the false dependencies in approximated solutions could have a negative impact on performance. This was recently demonstrated by a study on the performance of automatic parallelization compilers <ref> [17] </ref>, where it was found that "automatic tools produce insufficient performance improvement" due to false dependencies. In [17], it is suggested that semi-automatic tools might be more appropriate for parallelism detection. The second approach allows a user to specify parallelism information as an input and 2 then a compiler system generates efficient code [14, 16, 34, 86, 94, 106].
Reference: [18] <author> H. Cheong and A. V. Veidenbaum, </author> <title> Stale data detection and coherence enforcement using flow analysis, </title> <booktitle> Proc. of Intel. Conf. on Parallel Processing, </booktitle> <year> 1988, </year> <pages> pp. 138-145. </pages>
Reference-contexts: We have developed a scheme for correct execution of parallel tasks for distributed memory architectures. We have also presented the properties of a DAG as a condition to ensure this correctness. Data coherence is related to cache coherence. Cheong and Veidenbaum <ref> [18] </ref> described data flow analysis techniques to detect stale data copies (a data copy in a processor cache does not have the up-to-date value) for shared memory architectures. In our case, a dead copy could be stale but also could be because no tasks access this copy.
Reference: [19] <author> Y. C. Chung and S. Ranka, </author> <title> An optimization approach for static scheduling of directed-acyclic graphs on distributed memory multiprocessors, </title> <type> Report, </type> <institution> Syracuse Univ, </institution> <year> 1992. </year>
Reference-contexts: Precedence is not considered in PREP-P and OREGAMI/LaRCS. HYPERTOOL by Wu and Gajski [98] and TaskGrapher by El-Rewini and Lewis [33] 155 have the same goals as PYRROS and use the same model of computation. TaskGra-pher uses task duplication heuristics. Work by Chung and Ranka <ref> [19] </ref> also uses task duplication. The complexity of their algorithms is over O (v 2 ). HeNCE/PVM by Beguelin et. al [10] is a software system for exploiting large grain parallelism over heterogeneous computer networks. A graphic interface is provided for users to define program modules and their parallelism.
Reference: [20] <author> Ph. Chretienne, </author> <title> Task Scheduling over Distributed Memory Machines, </title> <booktitle> Proc. of Inter. Workshop on Parallel and Distributed Algorithms, </booktitle> <publisher> (North Holland, Ed.), </publisher> <year> 1989. </year>
Reference-contexts: The NP-completeness of clustering for parallel time minimization has been shown by Sarkar [90], Chretienne <ref> [20] </ref> and Papadimitriou and Yannakakis [78]. <p> As a consequence, the length of DS or SubDS going through this node could be reduced even more substantially. To achieve such a greedy goal, a minimization procedure that zeros multiple incoming edges of the selected free node is needed to be introduced in the DSC algorithm. Chretienne <ref> [20] </ref> has proposed an optimal algorithm for a fork and join, which zeroes multiple edges. The complexity of his algorithm is O (m log B) where B = minf P m i=1 t i ; fi 1 + t 1 g + t x . <p> For a join, the DSC uses the minimization procedure to minimize the tlevel value of the root and the solution is symmetrical to the optimal result for a fork. 2 Chretienne <ref> [20] </ref> describes a special algorithm for scheduling a fork with a complexity of O (m log B) where and B = minf P m i=1 t i ; fi 1 + t 1 g + t x . <p> Since this scheduling problem is NP-complete for a general fine grain tree and for a DAG which is a concatenation of a fork and a join (series parallel DAG) <ref> [20, 22] </ref>, the analysis result shows that DSC not only has a low complexity but also attains an optimality degree as much as a general polynomial algorithm could achieve. In [105], we have also compared DSC with the MD algorithm [98] and the ETF algorithm [54].
Reference: [21] <author> Ph. Chretienne, </author> <title> A Polynomial Algorithm to Optimially Schedule Tasks over an ideal Distributed System under Tree-like Presedence Constraints, </title> <journal> European Journal of Operational Research, </journal> <note> 2:43 (1989), pp225-230. </note>
Reference-contexts: The NP-completeness of clustering for parallel time minimization has been shown by Sarkar [90], Chretienne [20] and Papadimitriou and Yannakakis [78]. For clustering special classes of DAGs, Chretienne <ref> [21] </ref> shows that the problems of scheduling a join, fork DAG or coarse grain tree DAG are solvable in a polynomial time, but the complexity jumps to NP-complete for scheduling fine-grain tree DAGs and a DAG structure by concatenating a fork and a join together. <p> Another is that none of these general algorithms determines the optimum for special DAGs such as join, fork and coarse grain tree. Even though polynomial time algorithms for special types of DAGs exist in the literature, Chretienne <ref> [21] </ref>, Anger, Hwang and Chow [6], these algorithms are not general. As far as we know no scheduling algorithm exists that works well for arbitrary graphs, finds optimal schedules for special DAGs and also has a low complexity. <p> Chretienne <ref> [21] </ref> developed an O (v) optimal algorithm for this tree. Anger, Hwang and Chow [6] proposed an O (v) optimal algorithm for tree DAGs with the condition that all communication edge weights are smaller than the task node execution weights, which 64 are special cases of coarse grain tree DAGs.
Reference: [22] <author> Ph. Chretienne, </author> <title> Complexity of Tree Scheduling with Interprocessor Communication Delays, </title> <type> Tech. Report, M.A.S.I. </type> <institution> 90.5, Universite Pierre et Marie Curie, </institution> <year> 1990. </year>
Reference-contexts: An out-tree is a directed tree in which the root has incoming degree zero and other nodes have the incoming degree one. Scheduling in/out trees is still NP-hard in general as shown by Chretienne <ref> [22] </ref> and DSC will not give the optimal solution. However, DSC will yield optimal solutions for coarse grain trees and a class of fine grain trees. Coarse grain trees Theorem 4.7 DSC gives an optimal solution for a coarse grain in-tree. <p> Since this scheduling problem is NP-complete for a general fine grain tree and for a DAG which is a concatenation of a fork and a join (series parallel DAG) <ref> [20, 22] </ref>, the analysis result shows that DSC not only has a low complexity but also attains an optimality degree as much as a general polynomial algorithm could achieve. In [105], we have also compared DSC with the MD algorithm [98] and the ETF algorithm [54].
Reference: [23] <author> E.G. Coffman and R.L. Graham, </author> <title> Optimal scheduling for two-processors systems, </title> <journal> Acta Informatica, </journal> <volume> 3 (1972), </volume> <pages> 200-213. </pages>
Reference-contexts: Their conclusion is that the CP heuristic is superior to others since it is near-optimal (i.e. within 5 percent of the optimal in 90 percent of random cases.) Theoretically, Coffman and Graham <ref> [23] </ref> show that CP is optimal for scheduling DAGs of equal task sizes on two processors. Also Hu [53] shows that CP is optimal for scheduling tree DAGs of equal task sizes on p processors. Previous research has emphasized the worst case analysis in list scheduling. <p> Furthermore for a tree DAG with equal weights CP scheduling is optimum on arbitrary number of processors Hu [53] and for any DAG with equal weights CP is optimum on two processors Coffman and Graham <ref> [23] </ref>. 5.4 Execution Ordering with Nonzero Communication In this section, we extend the previous analysis to the task execution problem: 84 * Given p clusters of unordered tasks mapped onto p physical processors. Determine a schedule for these clusters. processors.
Reference: [24] <author> J. Y. Colin and Ph. Chretienne, </author> <title> C.P.M. Scheduling with Small Communication Delays and Task Duplication, </title> <type> Report, M.A.S.I. </type> <institution> 90.1, Universite Pierre et Marie Curie, </institution> <year> 1990. </year>
Reference: [25] <author> J. W. Cooley and J. W. Tukey, </author> <title> An algorithm for the machine calculation of complex Fourier series, </title> <journal> Mathematics of Computation, </journal> <volume> Vol. 19, No. 90, </volume> <year> 1965, </year> <pages> pp. 297-301. </pages>
Reference-contexts: The next example is the classical FFT transform of an array of complex numbers A with length n = 2 m into a array B: B j = k=1 n (k1)j : The FFT algorithm by Cooley and Tukey <ref> [25] </ref> can be considered as recursively computing the even and odd elements of A and then combining them together as shown in trees: the MERGE tree and SPLIT tree.
Reference: [26] <author> M. Cosnard, M. Marrakchi, Y. Robert, and D. Trystram, </author> <title> Parallel Gaussian Elimination on an MIMD Computer, </title> <journal> Parallel Computing, </journal> <volume> vol. 6, </volume> <pages> pp. 275-296, </pages> <year> 1988. </year>
Reference-contexts: The task T j k defined in Figure 3.7 uses column k to modify column j of the matrix A. The matrix is partitioned into columns consistent with the data accessing pattern of tasks. The new dependence graph is given in Figure 3.8 <ref> [26, 69] </ref>. Task T k+1 k is a broadcasting node, sending the same column k + 1 to all T j k+1 , j = k + 2 : n + 1. This new DAG has degree of parallelism equal to n. <p> In the literature, the following natural clustering has been used for this GJ DAG which assumes that each cluster contains all tasks that modify the same column <ref> [26, 39, 77, 85, 87] </ref> 2 . j In [42], we prove the following result: Theorem 3.3 If g (GJ) 1, the natural linear clustering is optimal for both clique and ring architectures. Table 3.2 gives the granularity values of the GJ DAG for different architectures. <p> One nonlinear clustering for the GJ DAG is to wrap n linear clusters around a ring of p processors with p &lt; n <ref> [26, 39, 77] </ref>. Table 3.3 lists the result of linear clustering when n = 128 and p = 128 on nCUBE-II and also the results of wrap nonlinear clustering when p = 8; 16; 64. PT is the parallel time in milliseconds. <p> O ((v + e) log v) for DSC. 4.6.2 Choleski Decomposition DAG In the second example we use a well known numerical computing DAG, the Choleski decomposition (CD) DAG given in Cosnard et al. <ref> [26] </ref>. In Figure 4.12 we show the PT improvement ratio (P T Improv) of Kim and Browne's and MCP over DSC. The matrix is of dimension 250 fi 250 and v = 31000 nodes and e = 62000 edges. The x-axis is the inverse granularity ratio 1=g of the graph.
Reference: [27] <author> R. Cytron and J. Ferrante, </author> <title> What's in a name? The Value of Renaming for Parallelism Detection and Storage Allocation, </title> <booktitle> Proc. of ICPP 87, </booktitle> <pages> pp. 19-27. </pages>
Reference-contexts: The above theorems show that a DAG derived from a sequential program based on a full data dependence analysis can easily satisfy DA. Any DAG that does not satisfy DA2, DA3, or DA4 can be transformed to one that satisfies them by renaming data. Cytron et al. <ref> [27] </ref> discussed a method for renaming to remove anti and output dependencies. A special approach is to associate a data item with its producer name such that no same 105 data ID with different data content will be used in this DAG.
Reference: [28] <author> R. Cytron, M. Hind, and W. Hsieh, </author> <title> Automatic generation of DAG parallelism, </title> <booktitle> Proc. of SIGPLAN Conf. on Programming Language Design and Implementations, </booktitle> <year> 1989, </year> <pages> pp. 54-68. </pages>
Reference-contexts: Automatic derivation of DAG parallelism is important for PYRROS. The general dependence analysis is NP-complete but for some special cases there exist polynomial algorithms [83, 95]. And systems such as ParaS-cope [58], PAT [7] and PTRAN [3] are helpful for users to uncover the parallelism. Cytron, Hind and Hsieh <ref> [28] </ref> presented an approach for DAG generation. Balasundaram and Kennedy [8] described a method to seek coarse grain parallelism by summarizing data access. Irigoin and Triolet [55], Wolf and Lam [96] addressed a program partitioning method for loop parallelism.
Reference: [29] <author> M. Cosnard, B. Tourancheau, and G. Villard, </author> <title> Gaussian Elimination on Message Passing Architecture, </title> <booktitle> Lecture Notes in Computer Science 297, </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <year> 1987, </year> <pages> pp. 611-628. 176 </pages>
Reference: [30] <author> J.J. Dongarra, . and D. C. Sorensen, </author> <title> SCHEDULE: Tools for Developing and Analyzing Parallel Fortran Programs, in The Characteristics of Parallel Algorithms, D.B. </title> <editor> Gannon, L.H. Jamieson and R.J. Douglass (Eds), </editor> <publisher> MIT Press, </publisher> <year> 1987, </year> <month> pp363-394. </month>
Reference-contexts: We will also describe some future work. SCHEDULER by Dongarra and Sorensen <ref> [30] </ref> uses centralized dynamic scheduling. This approach to scheduling and code generation is appropriate for shared memory architectures with few processors. For message passing architectures centralized scheduling will not perform well because of high control overhead in maintaining a global task queue. <p> For many problems, task parallelism cannot be derived at compile time. Dynamic scheduling has been very successful for shared memory architectures to handle such problems <ref> [30, 80] </ref>. For distributed memory architectures, it is a challenging problem to balance run-time scheduling overhead and gain performance. If a problem is iterative, then the inspector and executor approach [61, 89] can be used. Our scheduling algorithms could be adapted to such a run-time optimization scheme.
Reference: [31] <author> J.J. Dongarra, I. Duff, D.C. Sorensen, and H.A. van der Vorst, </author> <title> Solving Linear Systems on Vector and Shared Memory Computers, </title> <publisher> SIAM press,1991. </publisher>
Reference-contexts: A popular approach is to use a submatrix data partition known as the BLAS-3 block partition <ref> [31] </ref>. The matrix of n fi n is divided into N fi N submatrices and each submatrix has size of r fi r where N = n=r. Each task T j k in the block GJ DAG is operating on a block of columns composed of N submatrices.
Reference: [32] <author> T.H. Dunigan, </author> <title> Performance of the INTEL iPSC/860 and nCUBE 6400 hypercube, </title> <institution> ORNL/TM-11790, Oak Ridge National Lab., TN, </institution> <year> 1991. </year>
Reference-contexts: The communication delay between two non neighboring processors p i and p j is usually estimated using the linear model: Dist (p i ; p j )(ff + N fi). For a machine with a wormhole mechanism, this expression overestimates the actual delay. Dunigan <ref> [32] </ref> gives tables for estimating such delays on INTEL iPSC/86 and nCUBE-II based on benchmarking. However with communication contention in a network, his formula may underestimate the actual delay. <p> An estimation of the granularity for this DAG is g (G) !=(ff+fi) where ! is the time for one addition or subtraction, plus one multiplication. In Table 3.1, we list the parameters of several architectures for single-precision arithmetic and the value of !=(ff + fi), see <ref> [32] </ref>. <p> In the current hypercube machine such as nCUBE-2, the wormhole transmission of a message can be used so that the distance of two processors ( H (p 1 ; p 2 )) does not play a significant role in the communication latency if network communication load is not heavy <ref> [32] </ref>. For wormhole communication between p 1 and p 2 , a wormhole path is created between them and this path uses the channels between the intermediate routing processors.
Reference: [33] <author> H. El-Rewini and T.G. Lewis, </author> <title> Scheduling parallel program tasks onto arbitrary target machines, </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 9(1990), </volume> <pages> 138-153. </pages>
Reference-contexts: We use the macro-dataflow task model of execution. A task receives all input in parallel before starting execution, executes to completion without interruption, and immediately sends the output to all successor tasks in parallel, El-Rewini and Lewis <ref> [33] </ref>, Sarkar [90], Wu and Gajski [98]. * Scheduling In this stage, program tasks and associated data items are mapped onto processors. It is called processor assignment. Within each processor, tasks are executed one by one following some order. <p> Both systems use graph embedding algorithms to assign a set of tasks into processors. Precedence is not considered in PREP-P and OREGAMI/LaRCS. HYPERTOOL by Wu and Gajski [98] and TaskGrapher by El-Rewini and Lewis <ref> [33] </ref> 155 have the same goals as PYRROS and use the same model of computation. TaskGra-pher uses task duplication heuristics. Work by Chung and Ranka [19] also uses task duplication. The complexity of their algorithms is over O (v 2 ).
Reference: [34] <author> J. Feo, D. Cann, and R. Oldehoeft, </author> <title> A report on the Sisal language project, </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 10(1990), </volume> <pages> pp. 349-365. </pages>
Reference-contexts: In [17], it is suggested that semi-automatic tools might be more appropriate for parallelism detection. The second approach allows a user to specify parallelism information as an input and 2 then a compiler system generates efficient code <ref> [14, 16, 34, 86, 94, 106] </ref>. For example, KALI [61] and FORTRAN D [51] both provide parallel constructs in their language. Even if all parallelism is available, program scheduling and communication optimization are crucial to ensure good performance of automatically produced code. <p> We have used that idea and developed more efficient scheduling algorithms. Also we have developed a code generation method that integrates scheduling result for message passing architectures. Currently Feo, Cann and Oldehoeft <ref> [34] </ref> have been working on a new compiler system for SISAL. 154 The KALI parallel programming language by Koelbel and Mehrotra [61] has addressed code generation for DOALL parallelism. It does not address program scheduling. The DAG parallelism in PYRROS is more general.
Reference: [35] <author> J. Ferrante, K. Ottenstein, and J. Warren, </author> <title> The program dependence graph and its use in optimization, </title> <journal> ACM Trans. on Programming languages and Systems, </journal> <volume> Vol. 9, No. 3, </volume> <year> 1987, </year> <pages> pp. 319-349. </pages>
Reference-contexts: Each basic unit of compu tation is called a task. Then the dependence relation between tasks constitutes their communication edges. There are three basic types of data dependence: true, anti and output. Data dependency analysis technique can be found in Wolfe and Banerjee [95]. Control dependence <ref> [35] </ref> is not considered in this thesis. Tasks and their dependence constitute a direct acyclic dataflow graph (DAG). We can assign weights to nodes and edges. A node weight is the time for executing this computation unit.
Reference: [36] <author> M. R. Garey and D.S. Johnson, </author> <title> Computers and Intractability, a Guide to the Theory of NP-completeness, W.H. </title> <publisher> Freeman and Company: </publisher> <address> New York, </address> <year> 1979. </year>
Reference: [37] <author> G.A. Geist and M.T. Heath, </author> <title> Matrix Factorization on a Hypercube Multiprocessor, in Hypercube Multiprocessors, </title> <address> Philadelphia:SIAM, </address> <year> 1986, </year> <pages> pp. 161-180. </pages>
Reference-contexts: This method is simple, with complexity O (v log v) and has been shown to work well in practice, e.g. Saad [87], Geist and Heath <ref> [37] </ref>, Ortega [77], Gerasoulis and Nelken [39]. 1. Compute the arithmetic load LM j for each cluster. Determine the average load A = P u 2. Sort the clusters in an increasing order of their loads. 3. Assign virtual processors to those clusters with LM j A. 4. <p> The virtual processor for cluster M j where 1 j r is defined as following with wrap mapping: V P (j) = (j 1) mod q: The formula for reflection mapping can be found in Geist and Heath <ref> [37] </ref>. The main difference of our approach and the classical load balancing method in [38] is that in our case only the clusters with loads less than the average are merged. Clusters with higher than average load are mapped directly to separate processors.
Reference: [38] <author> A. George, , M.T. Heath, and J. Liu, </author> <title> Parallel Cholesky factorization on a shared memory processor, </title> <journal> Lin. Algebra Appl., </journal> <volume> 77(1986), </volume> <pages> pp. 165-187. </pages>
Reference-contexts: Merge u clusters into p clusters. 2. Map the p clusters into p physical processors so that high communication clusters are close to each other in the network. 5.1.1 Cluster merging We use a variation to work profiling method, George et. al. <ref> [38] </ref> for cluster merging. This method is simple, with complexity O (v log v) and has been shown to work well in practice, e.g. Saad [87], Geist and Heath [37], Ortega [77], Gerasoulis and Nelken [39]. 1. Compute the arithmetic load LM j for each cluster. <p> The main difference of our approach and the classical load balancing method in <ref> [38] </ref> is that in our case only the clusters with loads less than the average are merged. Clusters with higher than average load are mapped directly to separate processors. In this way, the load is better balanced.
Reference: [39] <author> A. Gerasoulis, I. Nelken. </author> <title> Static scheduling for linear algebra DAGs. </title> <booktitle> Proc. of 4th Conf. on Hypercubes, Monterey, </booktitle> <volume> Vol. 1, </volume> <year> 1989, </year> <pages> 671-674. </pages>
Reference-contexts: At the second step, clusters are mapped onto physical processors. Kim and Browne [59, 60] have experimented with the clustered two-step method and the unclustered one-step scheduling method and they found that clustered scheduling results in substantial improvements in performance. Gerasoulis and Nelken <ref> [39, 74] </ref> have used this two-step method in parallel linear algebra computation on nCUBE and CYBER machines. <p> In the literature, the following natural clustering has been used for this GJ DAG which assumes that each cluster contains all tasks that modify the same column <ref> [26, 39, 77, 85, 87] </ref> 2 . j In [42], we prove the following result: Theorem 3.3 If g (GJ) 1, the natural linear clustering is optimal for both clique and ring architectures. Table 3.2 gives the granularity values of the GJ DAG for different architectures. <p> One nonlinear clustering for the GJ DAG is to wrap n linear clusters around a ring of p processors with p &lt; n <ref> [26, 39, 77] </ref>. Table 3.3 lists the result of linear clustering when n = 128 and p = 128 on nCUBE-II and also the results of wrap nonlinear clustering when p = 8; 16; 64. PT is the parallel time in milliseconds. <p> This method is simple, with complexity O (v log v) and has been shown to work well in practice, e.g. Saad [87], Geist and Heath [37], Ortega [77], Gerasoulis and Nelken <ref> [39] </ref>. 1. Compute the arithmetic load LM j for each cluster. Determine the average load A = P u 2. Sort the clusters in an increasing order of their loads. 3. Assign virtual processors to those clusters with LM j A. 4.
Reference: [40] <author> A. Gerasoulis , S. Venugopal, and T. Yang, </author> <title> Clustering task graphs for message passing architectures, </title> <booktitle> 1990 Proc. of ACM Inter. Conf. on Supercomputing, Ams-terdam, </booktitle> <pages> pp. 447-456, </pages> <year> 1990. </year>
Reference: [41] <author> A. Gerasoulis and S. Venugopal, </author> <title> Linear clustering of linear algebra task graphs for local memory systems, </title> <type> Report, </type> <year> 1990. </year>
Reference-contexts: This bound is the special case of the following theorem described in Gerasoulis and Yang [42] and the proof is based on Gerasoulis and Venugopal <ref> [41] </ref>.
Reference: [42] <author> A. Gerasoulis and T. Yang, </author> <title> On the granularity and clustering of directed acyclic task graphs, </title> <note> To appear in IEEE Trans. on Parallel and Distributed Systems., </note> <year> 1993. </year>
Reference-contexts: Corollary 3.1 Optimal linear clustering is NP-complete. Even though linear clustering is still very difficult, for coarse grain DAGs any linear clustering algorithm guarantees performance within a factor of two of the optimum. This bound is the special case of the following theorem described in Gerasoulis and Yang <ref> [42] </ref> and the proof is based on Gerasoulis and Venugopal [41]. <p> In the literature, the following natural clustering has been used for this GJ DAG which assumes that each cluster contains all tasks that modify the same column [26, 39, 77, 85, 87] 2 . j In <ref> [42] </ref>, we prove the following result: Theorem 3.3 If g (GJ) 1, the natural linear clustering is optimal for both clique and ring architectures. Table 3.2 gives the granularity values of the GJ DAG for different architectures. <p> ENDWHILE 4.4.2 The minimization procedure for zeroing multiple incoming edges To reduce tlevel (n x ) in DSC a minimization procedure that zeros multiple incoming edges of n x is needed. An optimal algorithm for a join DAG has been described in <ref> [42] </ref> and an optimal solution is shown in Figure 4.5 (c). The basic procedure is to first sort 51 1.
Reference: [43] <author> A. Gerasoulis and T. Yang, </author> <title> Static scheduling of parallel programs for message passing architectures.Lecture Notes in Computer Science, No. </title> <booktitle> 634, Parallel Processing: CONPAR 92 - VAPP V, </booktitle> <address> Springer-Varlag, </address> <year> 1992, </year> <pages> pp. 601-612. </pages>
Reference: [44] <author> A. Gerasoulis and T. Yang, </author> <title> A comparison of clustering heuristics for scheduling DAGs on multiprocessors, </title> <journal> J. of Distributed and Parallel Computing, special issue on scheduling and load balancing, </journal> <volume> Vol. 16, No. 4, </volume> <pages> pp. </pages> <month> 276-291 (Dec. </month> <year> 1992). </year>
Reference-contexts: As a matter of fact most of the existing algorithms can be characterized by using such a framework <ref> [44] </ref>. The initial step assumes that each node is mapped in a unit cluster. In other steps the algorithm tries to improve the previous clustering by merging appropriate clusters. A merging operation is performed by zeroing an edge cost connecting two clusters 1 . <p> Kim and Browne's produces linear cluster-ings. Wu and Gajski's MCP algorithm is similar to DSC-I except that the priority of a free node is chosen as blevel instead of tlevel + blevel. A comparison of these algorithms is given in Gerasoulis and Yang <ref> [44] </ref>.
Reference: [45] <author> A. Gerasoulis and T. Yang, </author> <title> Scheduling program task graphs on MIMD architectures, To appear as a book chapter in Algorithm Derivation and Program Transformation, </title> <editor> R. Paige, J. Reif, and R. Wachter (Eds.), </editor> <publisher> Kluwer Publisher, </publisher> <year> 1992. </year> <month> 177 </month>
Reference: [46] <author> M. Girkar and C. Polychronopoulos, </author> <title> Partitioning programs for parallel execution, </title> <booktitle> Proc. of the 1988 ACM Inter. Conf. on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July 4-8, </month> <year> 1988. </year>
Reference-contexts: Without task duplication, many heuristic scheduling algorithms for arbitrary DAGs have been proposed in the literature, e.g. Kim and Browne [60], Sarkar [90], Wu and Gajski [98], Girkar and Polychronopoulos <ref> [46] </ref>. One difficulty with most existing algorithms for general DAGs is their high complexity. Another is that none of these general algorithms determines the optimum for special DAGs such as join, fork and coarse grain tree.
Reference: [47] <author> M. Girkar and C. Polychronopoulos, </author> <title> Automatic extraction of functinal parallelism from ordinary programs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 3, No. 2, </volume> <year> 1992, </year> <pages> pp. 166-178. </pages>
Reference-contexts: PARAFRASE-2 [81] by Polychronopoulos et. al., is a parallelizing compiler system that performs dependence analysis, program partitioning and dynamic scheduling on shared memory machines. Recently they have proposed hierarchical task graph (HTG) for modeling parallel computation encapsulating both data and control dependence <ref> [47] </ref>. This representation provides a basis for exploiting both loop parallelism and unstructured functional parallelism. For message passing architectures, hierarchical granularity control is necessary to attain the performance and automatic scheduling for HTGs. This is still an open problem.
Reference: [48] <author> R. L. Graham, </author> <title> Bounds for certain multiprocessing anomalies, </title> <journal> Bell System Tech. J. </journal> , <volume> 45 (1966), </volume> <pages> 1563-1581. </pages>
Reference-contexts: However, how to incorporate such strategies to obtain the optimal or near optimal solution is a challenging problem. 4.1 Complexity Issue and Previous Approaches The complexity of this clustering problem has been found much more difficult than that of the classical scheduling problem where the communication cost is ignored, Graham <ref> [48] </ref> and Lenstra and Kan [66]. The NP-completeness of clustering for parallel time minimization has been shown by Sarkar [90], Chretienne [20] and Papadimitriou and Yannakakis [78]. <p> A generic procedure of list scheduling is in Figure 5.4. What is interesting about list scheduling is that the performance is within 50% of the optimum independently of the priority list as shown by Graham <ref> [48] </ref>.
Reference: [49] <author> M. Gupta and P. Banerjee, </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on Multicomputers, </title> <journal> IEEE. Trans. on Parallel and Distributed Systems, </journal> <volume> Vol 3, No. 2, </volume> <month> Mar. </month> <year> 1992, </year> <pages> pp. 179-193. </pages>
Reference-contexts: Our mapping scheme is computation driven, i.e., the assignment of computation first and data mapping is based on computation mapping. The SUPERB project by Zima et. al. [106] uses the similar techniques. Some recent research on data mapping by Gupta and Baner-jee <ref> [49] </ref> has been automating data mapping process such that better load balancing can be achieved. PREP-P by Berman [11] is a software system that automatically maps undirected communication graphs onto CHiP machine architectures.
Reference: [50] <author> M.S. Hecht, </author> <title> Flow Analysis of Computer Programs, </title> <publisher> North-Holland, </publisher> <address> New York, </address> <year> 1977. </year>
Reference: [51] <author> S. Hiranandani, K. Kennedy, and C.W. Tseng, </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines, </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <pages> pp. 86-100. </pages>
Reference-contexts: The second approach allows a user to specify parallelism information as an input and 2 then a compiler system generates efficient code [14, 16, 34, 86, 94, 106]. For example, KALI [61] and FORTRAN D <ref> [51] </ref> both provide parallel constructs in their language. Even if all parallelism is available, program scheduling and communication optimization are crucial to ensure good performance of automatically produced code. <p> The parallel time is computed assuming that processors are fully connected. * Solution 1: nonoptimal A nonoptimal solution is depicted in Figure 2.2 (b). The parallel time (PT) of executing this task graph is equal to 12. This processor assignment can be produced by Fortran D <ref> [51] </ref>. <p> We have incorporated the idea of aggregate communication idea used in [67]. One difference is that we use asynchronous instead of synchronous communication. We have also developed techniques for eliminating redundant communication and maintaining data coherence, and have incorporated them in 132 one execution scheme. Hiranandani, Kennedy and Tseng <ref> [51] </ref> and Rogers and Pingali [86] have described several communication optimizations for Fortran D and Id Nouveau. Message vector ization combines several small messages into a big message to reduce startup overhead. <p> Li and Chen [67] have recognized the importance of effective communication and have developed a communication routine selection system in CRYSTAL to generate code for message passing architectures. We have also proposed other communication optimizations and integrated them in one code generation scheme. Fortran D project at Rice <ref> [51] </ref> and Fortran 90D at Syracuse [2] have emphasized compiling and code generation system for high performance Fortran. The mapping scheme is data driven, i.e. let a user define the data mapping and let the system determine the mapping of computation based on the owner compute rule.
Reference: [52] <author> J.A. Hoogeveen, S.L. van de Velde, and B. Veltman, </author> <title> Complexity of scheduling multiprocessor tasks with prespecified processor allocations, </title> <publisher> CWI, </publisher> <address> Report BS-R9211 June 1992, Netherlands. </address>
Reference-contexts: At each step, it examines one edge and zeros this edge if the parallel time does not increase. Sarkar's algorithm requires the computation of the parallel time at each step and this problem is NP-hard in stong sense <ref> [52] </ref>. <p> Then determine an execution ordering of the tasks within each processor so that the parallel time is minimized. The general task ordering problem above is NP-hard in stong sense <ref> [52] </ref>. Thus the main question is how to devise heuristics with good performance and low computational 77 complexity. This problem is closely related to the classical list scheduling. <p> i RCP max (D; S z;y S z;w jn z 2 F LIST (u) fn y ; n w g) max (0; 0) = 0. 2 5.5 Theoretical and Experimental Comparisons 5.5.1 Optimality on fork and join DAGs Finding the optimum in the general case is NP-hard in stong sense <ref> [52] </ref>, even for chains of tasks. However, for a fork and join show in Figure 5.8. We show that they are solvable in a polynomial time by RCP and RCP fl . Theorem 5.4 Given a processor assignment for a fork DAG, RCP and RCP fl find an optimal schedule.
Reference: [53] <author> T.C. Hu, </author> <title> Parallel sequencing and assembly lines problems, </title> <journal> Operations Research, </journal> <volume> 9 (1961), </volume> <pages> 841-848. </pages>
Reference-contexts: Also Hu <ref> [53] </ref> shows that CP is optimal for scheduling tree DAGs of equal task sizes on p processors. Previous research has emphasized the worst case analysis in list scheduling. The experimental results in [1], however, raise an interesting question. <p> Adam, Chandy and Dickson [1] have provided the experimental evidence for such a performance for a local optimum heuristic. Furthermore for a tree DAG with equal weights CP scheduling is optimum on arbitrary number of processors Hu <ref> [53] </ref> and for any DAG with equal weights CP is optimum on two processors Coffman and Graham [23]. 5.4 Execution Ordering with Nonzero Communication In this section, we extend the previous analysis to the task execution problem: 84 * Given p clusters of unordered tasks mapped onto p physical processors.
Reference: [54] <author> J. J. Hwang, Y. C. Chow, F. D. Anger, and C. Y. Lee, </author> <title> Scheduling precedence graphs in systems with interprocessor communication times, </title> <journal> SIAM J. Comput., </journal> <pages> pp. 244-257, </pages> <year> 1989. </year>
Reference-contexts: In [105], we have also compared DSC with the MD algorithm [98] and the ETF algorithm <ref> [54] </ref>. The idea of identifying the important tasks (DS nodes) in DSC is the same as MD. However, the way to find DS nodes is different. <p> ETF <ref> [54] </ref> is a scheduling algorithm for a bounded number (p) of processors with arbitary network topologies with a complexity of (pv 2 ). ETF performs v scheduling step and selects a task with the eariest starting time at each step for scheduling.
Reference: [55] <author> F. Irigoin and R. Triolet, </author> <title> Supernode partitioning, </title> <booktitle> Proc. of ACM SIGPLAN Sym-posi. on Principles of Programming Languages, </booktitle> <year> 1988, </year> <pages> pp. 319-329. </pages>
Reference-contexts: And systems such as ParaS-cope [58], PAT [7] and PTRAN [3] are helpful for users to uncover the parallelism. Cytron, Hind and Hsieh [28] presented an approach for DAG generation. Balasundaram and Kennedy [8] described a method to seek coarse grain parallelism by summarizing data access. Irigoin and Triolet <ref> [55] </ref>, Wolf and Lam [96] addressed a program partitioning method for loop parallelism. We may use an interactive system that incorporate those methods and facilitate users to derive DAG computation. * General parallel computation DAG computation still has its limitation.
Reference: [56] <author> N. Karmarkar, </author> <title> A new parallel architecture for sparse matrix computation based on finite project geometries, </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <pages> pp. 358-369. </pages>
Reference-contexts: The solution is then derived by an iterative method such as Newton-Raphson which iterates over the same dataflow graph since the topology of the iteration matrix remains the same but the data change at each step, Karmarkar <ref> [56] </ref>. We have performed some tests on PYRROS performance for some sparse task graphs. Bhattacharjya [12] has developed a system that generates the sparse graphs using the GJ algorithm. It starts from a dense GJ task graph and deletes those tasks that perform operations on zeros.
Reference: [57] <author> H. Kasahara and S. Narita. </author> <title> Practical multi-processor scheduling algorithms for efficient parallel processing, </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-33 (1984), </volume> <pages> pp. 1023-1029. </pages>
Reference-contexts: When two tasks have the same priority we break the tie by using the most immediate successors first (MISF) principle <ref> [57] </ref>. If the numbers of their successors are equal, we break the tie randomly. Function head (L) returns the first node in the sorted list L, which is the task with the highest priority.
Reference: [58] <author> K. Kennedy, K.S. NcKinley, and C. Tseng, </author> <title> Analysis and transformation in the ParaScope Editor, </title> <booktitle> In Proc. of 1991 ACM Inter. Conf. on Supercomputing, </booktitle> <address> Ger-many, </address> <year> 1991. </year>
Reference-contexts: Automatic derivation of DAG parallelism is important for PYRROS. The general dependence analysis is NP-complete but for some special cases there exist polynomial algorithms [83, 95]. And systems such as ParaS-cope <ref> [58] </ref>, PAT [7] and PTRAN [3] are helpful for users to uncover the parallelism. Cytron, Hind and Hsieh [28] presented an approach for DAG generation. Balasundaram and Kennedy [8] described a method to seek coarse grain parallelism by summarizing data access.
Reference: [59] <author> S. J. Kim, </author> <title> A general approach to multiprocessor scheduling, </title> <institution> TR-88-04, DCS, Univ. of Texas at Austin, </institution> <year> 1988. </year>
Reference-contexts: At the first step a task graph is clustered assuming that there is a sufficient number of fully connected processors. When two tasks are assigned in the same cluster, they are executed in the same processor. At the second step, clusters are mapped onto physical processors. Kim and Browne <ref> [59, 60] </ref> have experimented with the clustered two-step method and the unclustered one-step scheduling method and they found that clustered scheduling results in substantial improvements in performance. Gerasoulis and Nelken [39, 74] have used this two-step method in parallel linear algebra computation on nCUBE and CYBER machines.
Reference: [60] <author> S.J. Kim and J.C Browne, </author> <title> A general approach to mapping of parallel computation upon multiprocessor architectures, </title> <booktitle> Proc. of Int'l Conf. on Parallel Processing, </booktitle> <volume> vol 3, </volume> <pages> pp. 1-8, </pages> <year> 1988. </year> <month> 178 </month>
Reference-contexts: At the first step a task graph is clustered assuming that there is a sufficient number of fully connected processors. When two tasks are assigned in the same cluster, they are executed in the same processor. At the second step, clusters are mapped onto physical processors. Kim and Browne <ref> [59, 60] </ref> have experimented with the clustered two-step method and the unclustered one-step scheduling method and they found that clustered scheduling results in substantial improvements in performance. Gerasoulis and Nelken [39, 74] have used this two-step method in parallel linear algebra computation on nCUBE and CYBER machines. <p> Without task duplication, many heuristic scheduling algorithms for arbitrary DAGs have been proposed in the literature, e.g. Kim and Browne <ref> [60] </ref>, Sarkar [90], Wu and Gajski [98], Girkar and Polychronopoulos [46]. One difficulty with most existing algorithms for general DAGs is their high complexity. Another is that none of these general algorithms determines the optimum for special DAGs such as join, fork and coarse grain tree. <p> there exist a larger class of fine grain trees which are tractable in polynomial time, say for example the weights are not uniform in the above trees. 4.6 A Comparison with Other Algorithms and Experimental Results There are other algorithms for general DAGs developed by Sarkar [90], Kim and Browne <ref> [60] </ref>, and Wu and Gajski [98]. Kim and Browne's produces linear cluster-ings. Wu and Gajski's MCP algorithm is similar to DSC-I except that the priority of a free node is chosen as blevel instead of tlevel + blevel. A comparison of these algorithms is given in Gerasoulis and Yang [44].
Reference: [61] <author> C. Koelbel, and P. Mehrotra, </author> <title> Compiling global name-space parallel loops for distributed exection, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 2, No. 4 , 1991, </volume> <pages> pp. 440-451. </pages>
Reference-contexts: In [17], it is suggested that semi-automatic tools might be more appropriate for parallelism detection. The second approach allows a user to specify parallelism information as an input and 2 then a compiler system generates efficient code [14, 16, 34, 86, 94, 106]. For example, KALI <ref> [61] </ref> and FORTRAN D [51] both provide parallel constructs in their language. Even if all parallelism is available, program scheduling and communication optimization are crucial to ensure good performance of automatically produced code. <p> This code ensures a correct program execution in a distributed memory machine for arbitrary DAG parallelism with a shared memory programming style. Several other groups have also developed compiler code generation techniques for message-passing architectures. We discuss some related work. Koelbel and Mehrotra <ref> [61] </ref> have proposed a code generation method for compiling DOALL loop parallelism (no dependencies across loops) which allows program segments to access data items using a global name space. Our work is more general in the sense that we address general DAG parallelism and also allow global data naming. <p> Also we have developed a code generation method that integrates scheduling result for message passing architectures. Currently Feo, Cann and Oldehoeft [34] have been working on a new compiler system for SISAL. 154 The KALI parallel programming language by Koelbel and Mehrotra <ref> [61] </ref> has addressed code generation for DOALL parallelism. It does not address program scheduling. The DAG parallelism in PYRROS is more general. KALI system has been also focusing on run-time optimization with a similar idea as in the PARTI system by Saltz et.al [89]. <p> Dynamic scheduling has been very successful for shared memory architectures to handle such problems [30, 80]. For distributed memory architectures, it is a challenging problem to balance run-time scheduling overhead and gain performance. If a problem is iterative, then the inspector and executor approach <ref> [61, 89] </ref> can be used. Our scheduling algorithms could be adapted to such a run-time optimization scheme. In general, a combination between static scheduling and dynamic scheduling is needed to assure both performance and flexibility on message passing distributed machines.
Reference: [62] <author> B. Kruatrachue B. and T. Lewis, </author> <title> Grain Size Determination for Parallel Processing, </title> <journal> IEEE Software, </journal> <pages> pp. 23-32, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: This algorithm has a complexity of O (v 3 (v log v+e)) where v is the number of tasks and e is the number of edges. Kruatrachue and Lewis <ref> [62] </ref> have also given an O (v 4 ) algorithm for a general DAG based on task duplication. One difficulty with allowing task duplication is that it increases the space complexity when executing parallel programs on a real machine and as a result the algorithms might not be practical. <p> If there is, it tries to find some tasks to fill that gap. The algorithm is time consuming and its complexity exceeds O (v 2 ). This idea is similar to the one used in the insertion heuristic algorithm ISH by Kruatrachue and Lewis <ref> [62] </ref>. We have used a program to generate random DAGs. The parameters that can be varied are the number of tasks v, the probability of having a precedence edge between two nodes, the task execution time and communication weights.
Reference: [63] <author> S.Y. Kung, </author> <title> VLSI Array Processors, </title> <publisher> New Jersey:Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Kung <ref> [63] </ref>, pp. 170. Each 1 A program optimization used in ID Nouveau [86] is message vectorization which combines smaller messages into a larger message. 29 task performs a simple arithmetic operation and communicates with each other in one floating point number.
Reference: [64] <author> P. Ladkin and B. Simons, </author> <title> Compile-time analysis of communicating processes, </title> <booktitle> Proc. of 6th ACM Inter. Conf. on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July, </month> <year> 1992, </year> <pages> pp. 248-259. </pages>
Reference-contexts: A common error in programming message passing distributed memory machines is deadlock, e.g. Masticola and Ryder [71], Ladkin and Simons <ref> [64] </ref>. Because we allow shared memory programming style in a DAG, another difficulty arises when different tasks at different times modify the same data items. Thus we need to make sure each task receives the correct copies of data items.
Reference: [65] <author> Y. Lan, A. H. Esfahanian, and L. M. Ni, </author> <title> Multicast in hypercube multiprocessors, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8, </volume> <month> 30-41 </month> <year> (1990). </year>
Reference-contexts: From this experiment we conclude that the average effective ratio of the elimination algorithm is high and has a substantial improvement over algorithm A. Lan, Esfahanian and Ni <ref> [65] </ref> have also proposed a multicasting algorithm for hypercube, our algorithm uses subgraph elimination that could reduce more unnecessary 131 forwarding processors comparing with their algorithm.
Reference: [66] <author> J. K. Lenstra and A.H.G. Rinnooy Kan, </author> <title> Complexity of Scheduling under Precedence Constraints, Operation Research, </title> <month> 26:1 </month> <year> (1978). </year>
Reference-contexts: strategies to obtain the optimal or near optimal solution is a challenging problem. 4.1 Complexity Issue and Previous Approaches The complexity of this clustering problem has been found much more difficult than that of the classical scheduling problem where the communication cost is ignored, Graham [48] and Lenstra and Kan <ref> [66] </ref>. The NP-completeness of clustering for parallel time minimization has been shown by Sarkar [90], Chretienne [20] and Papadimitriou and Yannakakis [78].
Reference: [67] <author> J. Li and M. Chen, </author> <title> Compiling Communication-Efficient Programs for Massively Parallel Machines, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 2, No. 3 , 1991, </volume> <pages> pp. 361-376. </pages>
Reference-contexts: Our work is more general in the sense that we address general DAG parallelism and also allow global data naming. Li and Chen <ref> [67] </ref> in their CRYSTAL project have proposed the code optimization using aggregate communications. We have incorporated the idea of aggregate communication idea used in [67]. One difference is that we use asynchronous instead of synchronous communication. <p> Our work is more general in the sense that we address general DAG parallelism and also allow global data naming. Li and Chen <ref> [67] </ref> in their CRYSTAL project have proposed the code optimization using aggregate communications. We have incorporated the idea of aggregate communication idea used in [67]. One difference is that we use asynchronous instead of synchronous communication. We have also developed techniques for eliminating redundant communication and maintaining data coherence, and have incorporated them in 132 one execution scheme. <p> PYRROS algorithms could be useful at the first iteration. The CRYSTAL project by Chen, Choo and Li [16] has been targeted at developing a parallelizing compiler system for their functional language. Li and Chen <ref> [67] </ref> have recognized the importance of effective communication and have developed a communication routine selection system in CRYSTAL to generate code for message passing architectures. We have also proposed other communication optimizations and integrated them in one code generation scheme.
Reference: [68] <author> V. M. Lo, S. Rajopadhye, S. Gupta, D. Keldsen, M. Mohamed, J. A. Telle, OREGAMI: </author> <title> software tools for mapping parallel computations to parallel architectures, </title> <booktitle> Proc. Int'l. Conf. on Parallel Processing, 1990, </booktitle> <volume> Vol. </volume> <pages> II 88-92. </pages>
Reference-contexts: This algorithm starts from an initial assignment, then performs a series of pairwise interchanges so that the F (CC) is reduced monotonically. The complexity of this algorithm is O (p 3 ). However we plan to implement different algorithms in future which have been studied extensively by Lo et.al <ref> [68] </ref>. 5.2 Task Execution Ordering: An Introduction Now we consider the following problem: * Assume that a processor assignment for each task is given. <p> Some recent research on data mapping by Gupta and Baner-jee [49] has been automating data mapping process such that better load balancing can be achieved. PREP-P by Berman [11] is a software system that automatically maps undirected communication graphs onto CHiP machine architectures. OREGAMI/LaRCS by Lo et. al. <ref> [68] </ref> software tool is used for the mapping of task graphs onto processors. Both systems use graph embedding algorithms to assign a set of tasks into processors. Precedence is not considered in PREP-P and OREGAMI/LaRCS.
Reference: [69] <author> R.E. Lord, J.S. Kowalik, and S. P. Kumar, </author> <title> Solving linear algebraic equations on an MIMD computer, </title> <journal> Journal of the ACM, </journal> <volume> vol. 30, </volume> <pages> pp. 103-117, </pages> <year> 1983. </year>
Reference-contexts: The task T j k defined in Figure 3.7 uses column k to modify column j of the matrix A. The matrix is partitioned into columns consistent with the data accessing pattern of tasks. The new dependence graph is given in Figure 3.8 <ref> [26, 69] </ref>. Task T k+1 k is a broadcasting node, sending the same column k + 1 to all T j k+1 , j = k + 2 : n + 1. This new DAG has degree of parallelism equal to n.
Reference: [70] <author> P. K. McKinley, H. Xu, A. H. Esfahanian, and L. M. Ni, </author> <title> Unicast-based multi-cast communication in wormhole-routed networks, </title> <booktitle> Proc. of 1992 Inter. Conf. on Parallel Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 10-19. </pages>
Reference-contexts: Lan, Esfahanian and Ni [65] have also proposed a multicasting algorithm for hypercube, our algorithm uses subgraph elimination that could reduce more unnecessary 131 forwarding processors comparing with their algorithm. McKinley et. al <ref> [70] </ref> have proposed a new contention-free algorithm, called the U-cube tree algorithm that takes the advantage of wormhole routing so that no forwarding overhead is involved.
Reference: [71] <author> S. Masticola and B. G. Ryder, </author> <title> Static infinite wait anomaly detection in polynomial time, </title> <booktitle> Proc. of Inter. Conf. on Parallel Processing, </booktitle> <year> 1990, </year> <pages> pp II78-II87. </pages>
Reference-contexts: A common error in programming message passing distributed memory machines is deadlock, e.g. Masticola and Ryder <ref> [71] </ref>, Ladkin and Simons [64]. Because we allow shared memory programming style in a DAG, another difficulty arises when different tasks at different times modify the same data items. Thus we need to make sure each task receives the correct copies of data items.
Reference: [72] <author> C. McGreary C. and H. Gill H, </author> <title> Automatic determination of grain size for efficient parallel processing, </title> <journal> Communications of ACM, </journal> <volume> vol. 32, </volume> <pages> pp. 1073-1078, </pages> <month> Sept., </month> <year> 1989. </year>
Reference: [73] <author> C. Moler, </author> <title> Matrix Computation on Distributed Memory Multiprocessors, In Hypercube Multiprocessors 1986, </title> <publisher> SIAM, </publisher> <pages> pp. 181-195. </pages>
Reference-contexts: The resulting clustering using this program is the natural linear clustering. The clusters are mapped to the processors using wrap mapping along the gray code ring of the hypercube, Moler <ref> [73] </ref> and Saad [87]. T j k uses block column k to modify block column j.
Reference: [74] <author> I. H. Nelken, </author> <title> Parallelization for MIMD multiprocessors with applications to linear algebra algorithms, </title> <type> Ph.D Thesis, </type> <institution> LCSR-TR-134, Rutgers University, </institution> <year> 1989. </year>
Reference-contexts: At the second step, clusters are mapped onto physical processors. Kim and Browne [59, 60] have experimented with the clustered two-step method and the unclustered one-step scheduling method and they found that clustered scheduling results in substantial improvements in performance. Gerasoulis and Nelken <ref> [39, 74] </ref> have used this two-step method in parallel linear algebra computation on nCUBE and CYBER machines. <p> Exploiting fine grain statement-level parallelism will result in poor performance. To increase the granularity of a program DAG, we should use coarse grain procedure-level or loop-level partitioning. We consider the example of kji Gauss-Jordan (GJ) algorithm with no pivoting <ref> [74] </ref>. for k = 1 : n a k;j = a k;j =a k;k a i;j = a i;j a i;k fl a k;j endfor endfor endfor 3.3.1 Statement level fine grain parallelism If we take each assignment statement as a task, we get a three dimensional dependence graph with the
Reference: [75] <author> P. Newton and J. Browne, </author> <title> The CODE 2.0 graphical parallel programming language, </title> <booktitle> Proc. of 6th ACM Inter. Conf. on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July, </month> <year> 1992, </year> <pages> pp. 167-177. </pages>
Reference-contexts: A graphic interface is provided for users to define program modules and their parallelism. The system integrates those computation modules and executes them over a network. CODE 2.0 by Newton and Browne <ref> [75] </ref> has a similar approach for coarse grain parallel computation.
Reference: [76] <author> T. F. Ngai, </author> <title> Runtime resource management in concurrent systems, </title> <type> Tech. Report, </type> <institution> CSL-TR-92-504, Stanford Univ., </institution> <month> Jan. </month> <year> 1992. </year> <month> 179 </month>
Reference-contexts: Our scheduling algorithms could be adapted to such a run-time optimization scheme. In general, a combination between static scheduling and dynamic scheduling is needed to assure both performance and flexibility on message passing distributed machines. For example, Ngai <ref> [76] </ref> used clustering techniques for run-time resource management. Also one could use static symbolic clustering but use dynamic scheduling within each processor at run-time. For loop parallelism, there is a regularity in task dependence. The size of an iteration space depends on some variables.
Reference: [77] <author> J.M. Ortega, </author> <title> Introduction to Parallel and Vector Solution of Linear Systems, </title> <address> New York:Plenum, </address> <year> 1988. </year>
Reference-contexts: The third is a manual approach in the sense that it uses existing sequential languages with an extension of synchronization and communication library functions to distribute data and programs and schedule their execution, Ortega <ref> [77] </ref>. This approach has been the most successful with respect to the utilization of message passing architectures in the literature. <p> Most of these problems are NP-complete or NP-hard. Our goal is to design low complexity algorithms without compromising performance. A very successful approach to scheduling is a two-step method, e.g. Sarkar [90], Ortega <ref> [77] </ref>. At the first step a task graph is clustered assuming that there is a sufficient number of fully connected processors. When two tasks are assigned in the same cluster, they are executed in the same processor. At the second step, clusters are mapped onto physical processors. <p> In the literature, the following natural clustering has been used for this GJ DAG which assumes that each cluster contains all tasks that modify the same column <ref> [26, 39, 77, 85, 87] </ref> 2 . j In [42], we prove the following result: Theorem 3.3 If g (GJ) 1, the natural linear clustering is optimal for both clique and ring architectures. Table 3.2 gives the granularity values of the GJ DAG for different architectures. <p> One nonlinear clustering for the GJ DAG is to wrap n linear clusters around a ring of p processors with p &lt; n <ref> [26, 39, 77] </ref>. Table 3.3 lists the result of linear clustering when n = 128 and p = 128 on nCUBE-II and also the results of wrap nonlinear clustering when p = 8; 16; 64. PT is the parallel time in milliseconds. <p> This method is simple, with complexity O (v log v) and has been shown to work well in practice, e.g. Saad [87], Geist and Heath [37], Ortega <ref> [77] </ref>, Gerasoulis and Nelken [39]. 1. Compute the arithmetic load LM j for each cluster. Determine the average load A = P u 2. Sort the clusters in an increasing order of their loads. 3. Assign virtual processors to those clusters with LM j A. 4. <p> To compare the performance of PYRROS with other commonly used schedules for this task graph, we have written a program based on "owners compute rule" shown in Figure 7.1, Ortega <ref> [77] </ref>. The resulting clustering using this program is the natural linear clustering. The clusters are mapped to the processors using wrap mapping along the gray code ring of the hypercube, Moler [73] and Saad [87]. T j k uses block column k to modify block column j. <p> The resulting clustering using this program is the natural linear clustering. The clusters are mapped to the processors using wrap mapping along the gray code ring of the hypercube <ref> [77, 87] </ref>. T j k is in cluster j and is mapped to processor p j = j mod p. In Table 7.3, we show the performance of the automatically generated code by PYRROS and the hand-written program.
Reference: [78] <author> C. Papadimitriou and M. Yannakakis, </author> <title> Towards on an architecture-independent analysis of parallel algorithms, </title> <journal> SIAM J. Comput., </journal> <volume> vol. 19, </volume> <pages> pp. 322-328, </pages> <year> 1990. </year>
Reference-contexts: The NP-completeness of clustering for parallel time minimization has been shown by Sarkar [90], Chretienne [20] and Papadimitriou and Yannakakis <ref> [78] </ref>. <p> Basically there have been two approaches in the literature addressing this scheduling problem. The first approach considers heuristics for arbitrary DAGs and the second studies optimal algorithms for special classes of DAGs. When task duplication is allowed, Papadimitriou and Yannakakis <ref> [78] </ref> have proposed an approximate algorithm for 36 a DAG with equal task weights and equal edge weights, which guarantees a performance within 50% of the optimum.
Reference: [79] <author> Picouleau, C., </author> <title> Two new NP-Complete scheduling problems with communication delays and unlimited number of processors, </title> <type> M.A.S.I, </type> <institution> Universite Pierre et Marie Curie Tour 45-46 B314, 4, place Jussieu, </institution> <year> 1991. </year>
Reference-contexts: This theorem shows that the solution for a coarse grain DAG is equivalent to that of finding an optimal linear clustering. Picouleau <ref> [79] </ref> has shown that the scheduling problem for coarse grain DAGs is NP-complete, therefore optimal linear clustering is NP-complete. Corollary 3.1 Optimal linear clustering is NP-complete.
Reference: [80] <author> C. D. Polychronopoulos, </author> <title> Parallel Programming and Compilers, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: There are three generally distinct ways in addressing the programming difficulties for parallel architectures: The first approach considers the problem of automatic parallelization and scheduling from sequential programs. The emphasis has been in the development of compilers or software tools that will assist in programming parallel architectures <ref> [3, 80] </ref>. One of the obstacles in the development of parallelizing compilers is the automatic identification of embedded parallelism in a sequential program. This is because the precise dependence analysis problem is not tractable in a polynomial time. Significant progress has been made towards finding approximate solutions [83, 95]. <p> For many problems, task parallelism cannot be derived at compile time. Dynamic scheduling has been very successful for shared memory architectures to handle such problems <ref> [30, 80] </ref>. For distributed memory architectures, it is a challenging problem to balance run-time scheduling overhead and gain performance. If a problem is iterative, then the inspector and executor approach [61, 89] can be used. Our scheduling algorithms could be adapted to such a run-time optimization scheme.
Reference: [81] <author> C. D. Polychronopoulos, M. Girkar, M. Haghighat,C. Lee, B. Leung, and D. Schouten, </author> <title> The Structure of Parafrase-2: An advanced parallelizing compiler for C and Fortran, in Languages and Compilers for Parallel Computing, </title> <editor> D. Gelernter, A. Nicolau and D. Padua (Eds.), </editor> <year> 1990. </year>
Reference-contexts: SCHEDULER by Dongarra and Sorensen [30] uses centralized dynamic scheduling. This approach to scheduling and code generation is appropriate for shared memory architectures with few processors. For message passing architectures centralized scheduling will not perform well because of high control overhead in maintaining a global task queue. PARAFRASE-2 <ref> [81] </ref> by Polychronopoulos et. al., is a parallelizing compiler system that performs dependence analysis, program partitioning and dynamic scheduling on shared memory machines. Recently they have proposed hierarchical task graph (HTG) for modeling parallel computation encapsulating both data and control dependence [47].
Reference: [82] <author> R. Pozo, </author> <title> Performance modeling of sparse matrix methods for distributed memory architectures, </title> <booktitle> in Lecture Notes in Computer Science, </booktitle> <volume> No. 634, </volume> <booktitle> Parallel Processing: CONPAR 92 - VAPP V, </booktitle> <address> Springer-Varlag, </address> <year> 1992, </year> <pages> pp. 677-688. </pages>
Reference-contexts: Pozo <ref> [82] </ref> investigated the irregularity of sparse matrix graphs and showed that classical methods for shared memory machines do not work properly for message-passing architectures. It is mandatory to have a proper partitioning and automatic scheduling to gain scalable speedup.
Reference: [83] <author> W. Pugh, </author> <title> The Omega test: a fast and practical integer programming algorithm for dependence analysis, </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <address> NM, </address> <year> 1991, </year> <pages> pp. 4-13. </pages>
Reference-contexts: One of the obstacles in the development of parallelizing compilers is the automatic identification of embedded parallelism in a sequential program. This is because the precise dependence analysis problem is not tractable in a polynomial time. Significant progress has been made towards finding approximate solutions <ref> [83, 95] </ref>. However, the false dependencies in approximated solutions could have a negative impact on performance. This was recently demonstrated by a study on the performance of automatic parallelization compilers [17], where it was found that "automatic tools produce insufficient performance improvement" due to false dependencies. <p> Automatic derivation of DAG parallelism is important for PYRROS. The general dependence analysis is NP-complete but for some special cases there exist polynomial algorithms <ref> [83, 95] </ref>. And systems such as ParaS-cope [58], PAT [7] and PTRAN [3] are helpful for users to uncover the parallelism. Cytron, Hind and Hsieh [28] presented an approach for DAG generation. Balasundaram and Kennedy [8] described a method to seek coarse grain parallelism by summarizing data access.
Reference: [84] <author> P. </author> <title> Quinton, Mapping recurrences on parallel architectures, </title> <booktitle> Proc. of Int. Conf on Supercomputing ICS 88, </booktitle> <editor> L.P. Kartashev and S.I. Kartashev Eds., </editor> <year> 1988, </year> <pages> III 1-8. </pages>
Reference-contexts: Also one could use static symbolic clustering but use dynamic scheduling within each processor at run-time. For loop parallelism, there is a regularity in task dependence. The size of an iteration space depends on some variables. Symbolic scheduling could provide solutions independent of the problem size, Quinton <ref> [84] </ref>. A DAG static scheduling algorithm can be used in evaluating various solutions of symbolic schedules. 157 On the other hand, control dependence needs to be considered for general computation. Girkar and Polychronopoulos [47]'s hierarchical task graph can be used for exploiting more general parallelism from a program.
Reference: [85] <author> Y. Robert, B. Tourancheu, and G. Villard, </author> <title> Data allocation strategies for the Gauss and Jordan algorithms on a ring of processors, </title> <journal> Information Processing Letters, </journal> <volume> vol. 31, pp.21-29, </volume> <year> 1989. </year>
Reference-contexts: In the literature, the following natural clustering has been used for this GJ DAG which assumes that each cluster contains all tasks that modify the same column <ref> [26, 39, 77, 85, 87] </ref> 2 . j In [42], we prove the following result: Theorem 3.3 If g (GJ) 1, the natural linear clustering is optimal for both clique and ring architectures. Table 3.2 gives the granularity values of the GJ DAG for different architectures.
Reference: [86] <author> A. Rogers and K. Pingali, </author> <title> Process decomposition through locality of reference, </title> <booktitle> Proc. of SIGPLAN 1989, </booktitle> <pages> pp. 69-80. </pages>
Reference-contexts: In [17], it is suggested that semi-automatic tools might be more appropriate for parallelism detection. The second approach allows a user to specify parallelism information as an input and 2 then a compiler system generates efficient code <ref> [14, 16, 34, 86, 94, 106] </ref>. For example, KALI [61] and FORTRAN D [51] both provide parallel constructs in their language. Even if all parallelism is available, program scheduling and communication optimization are crucial to ensure good performance of automatically produced code. <p> 4 4 4 4 2 S 2 S 2 S 3 S 3 S 1 1 1 4 4 4 4 (a) Program task graph. (b) PT=12 1 1 the distribution of data items then the system will assign program statements to processors based on so called "owner computes rule" <ref> [14, 86] </ref>: a processor executes a computation unit if this unit modifies the data that the processor owns. Assume that a user distributes array x and y according to the element indices, i.e. processor i owns x (i) and y (i). <p> Kung [63], pp. 170. Each 1 A program optimization used in ID Nouveau <ref> [86] </ref> is message vectorization which combines smaller messages into a larger message. 29 task performs a simple arithmetic operation and communicates with each other in one floating point number. <p> One difference is that we use asynchronous instead of synchronous communication. We have also developed techniques for eliminating redundant communication and maintaining data coherence, and have incorporated them in 132 one execution scheme. Hiranandani, Kennedy and Tseng [51] and Rogers and Pingali <ref> [86] </ref> have described several communication optimizations for Fortran D and Id Nouveau. Message vector ization combines several small messages into a big message to reduce startup overhead. However, it is hard to judge when vectorization should be used or not since message pipelining could also improve performance.
Reference: [87] <author> Y. Saad, </author> <title> Gaussian elimination on hypercubes, in Parallel Algorithms and Architectures, </title> <editor> Cosnard, M. et al. Eds., </editor> <publisher> Elsevier Science Publishers, North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: In the literature, the following natural clustering has been used for this GJ DAG which assumes that each cluster contains all tasks that modify the same column <ref> [26, 39, 77, 85, 87] </ref> 2 . j In [42], we prove the following result: Theorem 3.3 If g (GJ) 1, the natural linear clustering is optimal for both clique and ring architectures. Table 3.2 gives the granularity values of the GJ DAG for different architectures. <p> Namely, one must use a partitioning on the iPSC/860 about 4 times coarser than that used on nCUBE-II. the block GJ algorithm with n = 128. It lists the parallel time in milliseconds for two types of broadcasting algorithms <ref> [87] </ref>: (a) the gray-code ring broadcasting (b) the hypercube spanning tree. We vary the granularity by varying the size of the submatrix partitioning r. <p> We vary the granularity by varying the size of the submatrix partitioning r. The parallel time for ring broadcasting is slightly smaller than that of tree broadcasting when log p 6, but is much smaller for larger log p = 7. This result is consistent with Saad <ref> [87] </ref>. When r = 1, GJ has the worst performance for p = 128 processors because the granularity is too fine and the parallelism implemented by linear clustering is not useful. When r 2, the granularity increases, the GJ DAG becomes coarse grain and linear clustering performs well. <p> This method is simple, with complexity O (v log v) and has been shown to work well in practice, e.g. Saad <ref> [87] </ref>, Geist and Heath [37], Ortega [77], Gerasoulis and Nelken [39]. 1. Compute the arithmetic load LM j for each cluster. Determine the average load A = P u 2. Sort the clusters in an increasing order of their loads. 3. <p> The resulting clustering using this program is the natural linear clustering. The clusters are mapped to the processors using wrap mapping along the gray code ring of the hypercube, Moler [73] and Saad <ref> [87] </ref>. T j k uses block column k to modify block column j. <p> The resulting clustering using this program is the natural linear clustering. The clusters are mapped to the processors using wrap mapping along the gray code ring of the hypercube <ref> [77, 87] </ref>. T j k is in cluster j and is mapped to processor p j = j mod p. In Table 7.3, we show the performance of the automatically generated code by PYRROS and the hand-written program.
Reference: [88] <author> Y. Saad and M. H. Schultz, </author> <title> Data communication in hypercubes, </title> <institution> DCS/RR-428, Research Report, Yale University, </institution> <year> 1985. </year>
Reference-contexts: In this section we present an algorithm for implementing multicasting on a hypercube. For one-to-all broadcasting such that every processor in a machine needs this message, a minimum spanning tree algorithm on a hypercube has been developed by Saad and 120 Schultz <ref> [88] </ref>. However as we will see from the discussion in the next subsection this algorithm is not suitable for multicasting where not every processor needs to receive a message. We will present an efficient algorithm for multicasting. <p> Namely, a message packet is sent from a source to a node and this node forwards this packet to another node. Finally the packet reaches the destination. We have assumed that communication between nodes is reliable and error-free. Saad and Schultz <ref> [88] </ref> have developed a spanning tree algorithm for broadcasting 121 in a hypercube. A node in a tree forwards a message to its child nodes by toggling successive bits in a direction from the least-significant to most significant. We call this tree as left-toggling spanning tree, L tree for short.
Reference: [89] <author> Saltz, J., Crowley, K., Mirchandaney, R. and Berryman,H., </author> <title> Run-time scheduling and execution of loops on message passing machines, </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> Vol. 8, </volume> <year> 1990, </year> <pages> pp. 303-312. </pages>
Reference-contexts: It does not address program scheduling. The DAG parallelism in PYRROS is more general. KALI system has been also focusing on run-time optimization with a similar idea as in the PARTI system by Saltz et.al <ref> [89] </ref>. When a problem is iterative in nature, its program dependence graph can be derived during the first iteration at run-time, and then data access information can be used to optimize communication for nonlocal data access in future iterations. <p> Dynamic scheduling has been very successful for shared memory architectures to handle such problems [30, 80]. For distributed memory architectures, it is a challenging problem to balance run-time scheduling overhead and gain performance. If a problem is iterative, then the inspector and executor approach <ref> [61, 89] </ref> can be used. Our scheduling algorithms could be adapted to such a run-time optimization scheme. In general, a combination between static scheduling and dynamic scheduling is needed to assure both performance and flexibility on message passing distributed machines.
Reference: [90] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: The main optimization issues are balancing computation among processors, reducing inter-processor communication and overlapping communication with computation. Most of these problems are NP-complete or NP-hard. Our goal is to design low complexity algorithms without compromising performance. A very successful approach to scheduling is a two-step method, e.g. Sarkar <ref> [90] </ref>, Ortega [77]. At the first step a task graph is clustered assuming that there is a sufficient number of fully connected processors. When two tasks are assigned in the same cluster, they are executed in the same processor. At the second step, clusters are mapped onto physical processors. <p> We use the macro-dataflow task model of execution. A task receives all input in parallel before starting execution, executes to completion without interruption, and immediately sends the output to all successor tasks in parallel, El-Rewini and Lewis [33], Sarkar <ref> [90] </ref>, Wu and Gajski [98]. * Scheduling In this stage, program tasks and associated data items are mapped onto processors. It is called processor assignment. Within each processor, tasks are executed one by one following some order. <p> The NP-completeness of clustering for parallel time minimization has been shown by Sarkar <ref> [90] </ref>, Chretienne [20] and Papadimitriou and Yannakakis [78]. <p> Without task duplication, many heuristic scheduling algorithms for arbitrary DAGs have been proposed in the literature, e.g. Kim and Browne [60], Sarkar <ref> [90] </ref>, Wu and Gajski [98], Girkar and Polychronopoulos [46]. One difficulty with most existing algorithms for general DAGs is their high complexity. Another is that none of these general algorithms determines the optimum for special DAGs such as join, fork and coarse grain tree. <p> The initial step assumes that each node is mapped in a unit cluster. In other steps the algorithm tries to improve the previous clustering by merging appropriate clusters. A merging operation is performed by zeroing an edge cost connecting two clusters 1 . Sarkar's algorithm We consider Sarkar's algorithm <ref> [90] </ref>, pp. 123-131, as an example of an edge-zeroing clustering refinement algorithm. This algorithm first sorts the e edges of the DAG in a decreasing order of edge weights and then performs e clustering steps by examining edges from left to right in the sorted list. <p> open question remains if there exist a larger class of fine grain trees which are tractable in polynomial time, say for example the weights are not uniform in the above trees. 4.6 A Comparison with Other Algorithms and Experimental Results There are other algorithms for general DAGs developed by Sarkar <ref> [90] </ref>, Kim and Browne [60], and Wu and Gajski [98]. Kim and Browne's produces linear cluster-ings. Wu and Gajski's MCP algorithm is similar to DSC-I except that the priority of a free node is chosen as blevel instead of tlevel + blevel. <p> For message passing architectures, hierarchical granularity control is necessary to attain the performance and automatic scheduling for HTGs. This is still an open problem. SISAL is a functional parallel language that allows users to explicitly express pro gram parallelism. Sarkar <ref> [90] </ref> has developed an initial version of a compiler system that uses two step scheduling method for shared memory machines. We have used that idea and developed more efficient scheduling algorithms. Also we have developed a code generation method that integrates scheduling result for message passing architectures.
Reference: [91] <author> V. Sarkar, </author> <title> Determining average program execution times and their variance, </title> <booktitle> Proc. of 1989 SIGPLAN, ACM, </booktitle> <pages> pp. 298-312. </pages>
Reference-contexts: Its edge weight is the shortest time for transferring this data 10 item between two processors if n i and n j are in separate processors. The compiler algorithms for deriving the average weight values are discussed in Sarkar <ref> [91] </ref>. We use the macro-dataflow task model of execution.
Reference: [92] <author> D. Shasha and M. Snir, </author> <title> Efficient and correct execution of parallel programs that share memory, </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> Vol 10, No. 2, </volume> <year> 1988, </year> <pages> pp. 282-312. 180 </pages>
Reference-contexts: Currently we have not incorporated the automatic vectorization. We expect that a DAG is derived based on coarse grain parallelism and messages between tasks have large sizes. Our work bears resemblance to that for shared memory machines. Shasha and Snir <ref> [92] </ref> described the conditions for ensuring correct execution of parallel programs that share memory. We have developed a scheme for correct execution of parallel tasks for distributed memory architectures. We have also presented the properties of a DAG as a condition to ensure this correctness.
Reference: [93] <author> H. Stone, </author> <title> High-Performance Computer Architectures, </title> <address> Reading, Mass.: </address> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: Basically there are two distinct strategies for scheduling: parallelizing tasks or sequentializing tasks. The trade-off point between parallelization and sequentialization is closely related to the granularity value: the ratio between the task computation and communication. If communication cost is too high, parallelization is not encouraged. Stone <ref> [93] </ref> has analyzed the granularity issues by examining a set of tasks in which every task computes R units of time and communicates with all other tasks at an overhead of C.
Reference: [94] <author> P. Tseng, </author> <title> Compiling programs for a linear systolic array, </title> <booktitle> Proc. of SIGPLAN 1990, </booktitle> <pages> pp 311-321. </pages>
Reference-contexts: In [17], it is suggested that semi-automatic tools might be more appropriate for parallelism detection. The second approach allows a user to specify parallelism information as an input and 2 then a compiler system generates efficient code <ref> [14, 16, 34, 86, 94, 106] </ref>. For example, KALI [61] and FORTRAN D [51] both provide parallel constructs in their language. Even if all parallelism is available, program scheduling and communication optimization are crucial to ensure good performance of automatically produced code.
Reference: [95] <author> M. Wolfe and U. Banerjee, </author> <title> Data dependence and its application to parallel processing, </title> <journal> International Journal of Parallel Programming, </journal> <volume> Vol. 16, No. 2, </volume> <year> 1987, </year> <pages> pp. 137-178. </pages>
Reference-contexts: One of the obstacles in the development of parallelizing compilers is the automatic identification of embedded parallelism in a sequential program. This is because the precise dependence analysis problem is not tractable in a polynomial time. Significant progress has been made towards finding approximate solutions <ref> [83, 95] </ref>. However, the false dependencies in approximated solutions could have a negative impact on performance. This was recently demonstrated by a study on the performance of automatic parallelization compilers [17], where it was found that "automatic tools produce insufficient performance improvement" due to false dependencies. <p> Each basic unit of compu tation is called a task. Then the dependence relation between tasks constitutes their communication edges. There are three basic types of data dependence: true, anti and output. Data dependency analysis technique can be found in Wolfe and Banerjee <ref> [95] </ref>. Control dependence [35] is not considered in this thesis. Tasks and their dependence constitute a direct acyclic dataflow graph (DAG). We can assign weights to nodes and edges. A node weight is the time for executing this computation unit. <p> We now study the properties of a DAG. Since data accessing of tasks in a DAG must be synchronized through data dependence edges and there are three types of data dependencies between tasks (true, anti, and output) <ref> [95] </ref>, we assume that a DAG satisfies the following conditions. 103 DA1: A task receives distinct data items. No data items with the same ID are received from different tasks. DA2: True and anti data dependence. <p> Automatic derivation of DAG parallelism is important for PYRROS. The general dependence analysis is NP-complete but for some special cases there exist polynomial algorithms <ref> [83, 95] </ref>. And systems such as ParaS-cope [58], PAT [7] and PTRAN [3] are helpful for users to uncover the parallelism. Cytron, Hind and Hsieh [28] presented an approach for DAG generation. Balasundaram and Kennedy [8] described a method to seek coarse grain parallelism by summarizing data access.
Reference: [96] <author> M. E. Wolf and M. S. Lam, </author> <title> A loop transformation theory and an algorithm to maximize parallelism, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> Oct, </month> <year> 1991, </year> <pages> pp. 452-471. </pages>
Reference-contexts: We partition the nfin mesh into N fiN mesh blocks. Each block is a rfir submatrix where n = N fir. The blocked algorithm with respect to loop i and j is shown in Figure 7.19. This partitioning is also known as tiling <ref> [96] </ref>. The dependence graph is shown in Figure 7.20 (a) and (b). We schedule this PDE DAG and repeatedly execute the same schedule with respect to loop k in Figure 7.18. Each processor executes assigned tasks one by one in one iteration. <p> Cytron, Hind and Hsieh [28] presented an approach for DAG generation. Balasundaram and Kennedy [8] described a method to seek coarse grain parallelism by summarizing data access. Irigoin and Triolet [55], Wolf and Lam <ref> [96] </ref> addressed a program partitioning method for loop parallelism. We may use an interactive system that incorporate those methods and facilitate users to derive DAG computation. * General parallel computation DAG computation still has its limitation. For many problems, task parallelism cannot be derived at compile time.
Reference: [97] <author> R. Wolski and J. Feo, </author> <title> Program Parititoning for NUMA Multiprocessor Computer Systems, </title> <type> Tech. Report, </type> <institution> Lawrence Livermore Nat. Lab., </institution> <year> 1992. </year>
Reference: [98] <author> M. Y. Wu and D. Gajski, Hypertool: </author> <title> A programming aid for message-passing systems, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. 3, pp.330-343, </volume> <year> 1990. </year>
Reference-contexts: We use the macro-dataflow task model of execution. A task receives all input in parallel before starting execution, executes to completion without interruption, and immediately sends the output to all successor tasks in parallel, El-Rewini and Lewis [33], Sarkar [90], Wu and Gajski <ref> [98] </ref>. * Scheduling In this stage, program tasks and associated data items are mapped onto processors. It is called processor assignment. Within each processor, tasks are executed one by one following some order. Message receiving and sending is conducted before and after a task execution if inter-processor communication is necessary. <p> Without task duplication, many heuristic scheduling algorithms for arbitrary DAGs have been proposed in the literature, e.g. Kim and Browne [60], Sarkar [90], Wu and Gajski <ref> [98] </ref>, Girkar and Polychronopoulos [46]. One difficulty with most existing algorithms for general DAGs is their high complexity. Another is that none of these general algorithms determines the optimum for special DAGs such as join, fork and coarse grain tree. <p> of fine grain trees which are tractable in polynomial time, say for example the weights are not uniform in the above trees. 4.6 A Comparison with Other Algorithms and Experimental Results There are other algorithms for general DAGs developed by Sarkar [90], Kim and Browne [60], and Wu and Gajski <ref> [98] </ref>. Kim and Browne's produces linear cluster-ings. Wu and Gajski's MCP algorithm is similar to DSC-I except that the priority of a free node is chosen as blevel instead of tlevel + blevel. A comparison of these algorithms is given in Gerasoulis and Yang [44]. <p> In [105], we have also compared DSC with the MD algorithm <ref> [98] </ref> and the ETF algorithm [54]. The idea of identifying the important tasks (DS nodes) in DSC is the same as MD. However, the way to find DS nodes is different. <p> In a recent personal communication [99], the authors of MD made some corrections for the Fact 1 description in <ref> [98] </ref>, pp. 336. <p> OREGAMI/LaRCS by Lo et. al. [68] software tool is used for the mapping of task graphs onto processors. Both systems use graph embedding algorithms to assign a set of tasks into processors. Precedence is not considered in PREP-P and OREGAMI/LaRCS. HYPERTOOL by Wu and Gajski <ref> [98] </ref> and TaskGrapher by El-Rewini and Lewis [33] 155 have the same goals as PYRROS and use the same model of computation. TaskGra-pher uses task duplication heuristics. Work by Chung and Ranka [19] also uses task duplication. The complexity of their algorithms is over O (v 2 ).
Reference: [99] <author> M. Y. Wu, </author> <type> Personal Communications, </type> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Next we describe our experiments and compare with other algorithms in scheduling 6 Fact 1 in MD tries to guarantee the non-increasing of the length of the current critical path but it does not necessarily make the length shorter. In a recent personal communication <ref> [99] </ref>, the authors of MD made some corrections for the Fact 1 description in [98], pp. 336.
Reference: [100] <author> J. Yang, L. Bic and A. Nicolau, </author> <title> A mapping strategy for MIMD computers, </title> <booktitle> Proc. of 1991 Inter. Conf. on Parallel Processing, </booktitle> <volume> vol I, </volume> <pages> pp. 102-109. </pages>
Reference: [101] <author> T. Yang and A. Gerasoulis, </author> <title> A fast static scheduling algorithm for DAGs on an unbounded number of processors, </title> <booktitle> Proc. of Supercomputing '91, IEEE, </booktitle> <address> Albuquerque, NM, </address> <year> 1991, </year> <pages> pp. 633-642. </pages>
Reference: [102] <author> T. Yang and A. Gerasoulis. </author> <title> A parallel programming tool for scheduling on distributed memory multiprocessors. </title> <booktitle> Proc. of Scalable High Performance Computing Conference, IEEE, </booktitle> <address> Williamsburg, VA., </address> <month> April, </month> <year> 1992, </year> <pages> pp. 350-357. </pages>
Reference: [103] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static task scheduling and code generation for message-passing multiprocessors, </title> <booktitle> Proc. of 6th ACM Inter. Conf. on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July, </month> <year> 1992, </year> <pages> pp. 428-437. </pages>
Reference: [104] <author> T. Yang and A. Gerasoulis, </author> <title> List scheduling with and without communication delay, </title> <note> To appear in Parallel Computing, </note> <year> 1993. </year>
Reference: [105] <author> T. Yang and A. Gerasoulis, </author> <title> DSC: Scheduling parallel tasks on an unbounded number of processors, </title> <note> Submitted for publication. </note>
Reference-contexts: In <ref> [105] </ref>, we have also compared DSC with the MD algorithm [98] and the ETF algorithm [54]. The idea of identifying the important tasks (DS nodes) in DSC is the same as MD. However, the way to find DS nodes is different. <p> The task prioritization in ETF differs from DSC since a task with the eariest starting time may not be in a DS. Another difference is that ETF does not use the minimization procedure. A detailed comparison and experiment results will be in <ref> [105] </ref>. Next we describe our experiments and compare with other algorithms in scheduling 6 Fact 1 in MD tries to guarantee the non-increasing of the length of the current critical path but it does not necessarily make the length shorter. <p> In a recent personal communication [99], the authors of MD made some corrections for the Fact 1 description in [98], pp. 336. See <ref> [105] </ref> for the detail. 68 DAGs other than the primitive structures. 4.6.1 Random DAGs #cases #tasks #tasks #edges C/R PT Impro PT Impro Time Min-Max Ave Ave Min-Max Min-Max Ave Faster G1 22 44-98 70.1 311.3 0.83-5.6 4.6%-33.5% 17.8% 92.5 G3 31 250-540 329.0 3430 1.68-8.7 21%-37.8% 25.8% 1854.5 Table 4.3:
Reference: [106] <author> H. Zima, H. J. Bast, and M. Gerndt, </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization, </title> <journal> Parallel Computing, </journal> <volume> Vol 6., </volume> <year> 1988, </year> <pages> pp. 1-18. 181 </pages>
Reference-contexts: In [17], it is suggested that semi-automatic tools might be more appropriate for parallelism detection. The second approach allows a user to specify parallelism information as an input and 2 then a compiler system generates efficient code <ref> [14, 16, 34, 86, 94, 106] </ref>. For example, KALI [61] and FORTRAN D [51] both provide parallel constructs in their language. Even if all parallelism is available, program scheduling and communication optimization are crucial to ensure good performance of automatically produced code. <p> Our mapping scheme is computation driven, i.e., the assignment of computation first and data mapping is based on computation mapping. The SUPERB project by Zima et. al. <ref> [106] </ref> uses the similar techniques. Some recent research on data mapping by Gupta and Baner-jee [49] has been automating data mapping process such that better load balancing can be achieved. PREP-P by Berman [11] is a software system that automatically maps undirected communication graphs onto CHiP machine architectures.
References-found: 106

