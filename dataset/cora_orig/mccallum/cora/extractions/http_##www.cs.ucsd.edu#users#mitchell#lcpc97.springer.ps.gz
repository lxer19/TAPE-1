URL: http://www.cs.ucsd.edu/users/mitchell/lcpc97.springer.ps.gz
Refering-URL: http://www.cs.ucsd.edu/users/mitchell/papers.html
Root-URL: http://www.cs.ucsd.edu
Title: Quantifying the Multi-Level Nature of Tiling Interactions  
Author: Nicholas Mitchell, Larry Carter, Jeanne Ferrante, and Karin Hogstedt 
Address: La Jolla CA 92093-0114  
Affiliation: Computer Science and Engineering Department, UCSD,  
Abstract: Optimizations, including tiling, often target a single level of memory or parallelism, such as cache. These optimizations usually operate on a level-by-level basis, guided by a cost function parameterized by features of that single level. The benefit of optimizations guided by these one-level cost functions decreases as architectures tend towards a hierarchy of memory and of parallelism. We have identified three common architectural scenarios where a single tiling choice could be improved by using information from multiple levels in concert. For the first two scenarios, we derive multi-level cost functions which guide the optimal choice of tile size and shape, and quantify the improvement gained. We give both analysis and simulation results to support our points. For the third scenario, we summarize our findings. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Automatic partitioning of parallel loops and data arrays for distributed shared memory multiprocessors. </title> <booktitle> In Int. Conf. on Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: Quantifying performance: We employ counting arguments similar to <ref> [21, 13, 11, 1, 25] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. <p> Quantifying performance: We employ counting arguments similar to [21, 13, 11, 1, 25] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as <ref> [21, 11, 1] </ref> give methods for choosing tile size in a nested loop; [21] uses a "fits-in" constraint based only on memory capacity (not block size), [11]'s "fits-in" constraint does not fully utilize block size information, and [1] limits block size to one. <p> Single-level tile characteristics: Works such as [21, 11, 1] give methods for choosing tile size in a nested loop; [21] uses a "fits-in" constraint based only on memory capacity (not block size), [11]'s "fits-in" constraint does not fully utilize block size information, and <ref> [1] </ref> limits block size to one. In contrast to these and other approaches to tiling size selection, our multi-level approach uses the block size at each level in a multi-level cost function. Single-level unification: Unimodular transformations can guide loop transformations for locality [27] and parallelism [28].
Reference: 2. <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In PPoPP, </booktitle> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality [27, 5, 7, 6] and exploit parallelism <ref> [2, 28, 12, 18, 22] </ref>. For instance, a one-level cost function for a tiling might reflect only whether the tile fits in cache (perhaps by considering cache size, line size, and cache associativity [11]), but not the effect of the tiling on instruction level parallelism.
Reference: 3. <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In LCPC, </booktitle> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 4. <author> S. Carr. </author> <title> Combining optimizations for cache and instruction-level parallelism. </title> <booktitle> In PACT, </booktitle> <year> 1996. </year>
Reference-contexts: Bold elements highlight the important features in each architecture. sequence of transformations to parallelize a given program [15]. None of these works seeks to unify guidance for multiple levels of the memory hierarchy. Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert <ref> [4] </ref>. Loop fusion and distribution affect both parallelism and locality [20]. These two works do not directly address tiling or the multilevel nature of the interactions. Rather than use multi-level cost functions, [29] performs a pruned search on the space of possible combinations of minimizations of one-level cost functions.
Reference: 5. <author> S. Carr and K. Kennedy. </author> <title> Compiler blockability of numerical algorithms. </title> <editor> J. </editor> <booktitle> of Supercomputing, </booktitle> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality <ref> [27, 5, 7, 6] </ref> and exploit parallelism [2, 28, 12, 18, 22].
Reference: 6. <author> S. Carr and K. Kennedy. </author> <title> Improving the ratio of memory operations to floatingpoint operations in loops. </title> <journal> TOPLAS, </journal> <volume> 16(6), </volume> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality <ref> [27, 5, 7, 6] </ref> and exploit parallelism [2, 28, 12, 18, 22].
Reference: 7. <author> S. Carr, K. S. McKinley, and C. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In ASPLOS, </booktitle> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality <ref> [27, 5, 7, 6] </ref> and exploit parallelism [2, 28, 12, 18, 22].
Reference: 8. <author> L. Carter, J. Ferrante, and S. F. Hummel. </author> <title> Efficient parallelism via hierarchical tiling. </title> <booktitle> In Parallel Processing for Scientific Computing, </booktitle> <month> Feb. </month> <year> 1995. </year>
Reference: 9. <author> L. Carter, J. Ferrante, and S. F. Hummel. </author> <title> Hierarchical tiling for improved superscalar perfomance. </title> <booktitle> In IPPS, </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference: 10. <author> L. Carter, J. Ferrante, S. F. Hummel, B. Alpern, and K. S. Gatlin. </author> <title> Hierarchical tiling: A methodology for high performance. </title> <type> Technical Report CS96-508, UCSD, </type> <institution> Department of Computer Science and Engineering, </institution> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: The opportunity to execute iterations from independent planes has been lost. Instruction Level Parallelism only: What tiling choice maximizes instruction level parallelism? Here we use a similarly simple function for ILP execution time, given as a table of values; an example for the IBM Power2 with values taken from <ref> [10] </ref> is given in Tab. 3 (b). The third column gives the execution time in cycles per iteration per point of the original ISG, I LP T ime (p), for p = 1 to 3 planes.
Reference: 11. <author> S. Coleman and K. S. McKinley. </author> <title> Tile size selection using cache organization and data layout. </title> <booktitle> In PLDI, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Such one-level cost functions commonly increase locality [27, 5, 7, 6] and exploit parallelism [2, 28, 12, 18, 22]. For instance, a one-level cost function for a tiling might reflect only whether the tile fits in cache (perhaps by considering cache size, line size, and cache associativity <ref> [11] </ref>), but not the effect of the tiling on instruction level parallelism. However, recent trends towards greater architectural complexity have increased the amount of information available to an optimizing compiler. Many machines now have multiple levels of memory and of parallelism, typically arranged hierarchically. <p> Researchers have developed a number solutions to this information expansion problem. Many have simply ignored the multi-level information, instead relying on one-level cost functions <ref> [27, 11, 25] </ref>. Others rephrase program optimization as a search problem and invent heuristics to prune the search [15, 29]. <p> Quantifying performance: We employ counting arguments similar to <ref> [21, 13, 11, 1, 25] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. <p> Quantifying performance: We employ counting arguments similar to [21, 13, 11, 1, 25] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as <ref> [21, 11, 1] </ref> give methods for choosing tile size in a nested loop; [21] uses a "fits-in" constraint based only on memory capacity (not block size), [11]'s "fits-in" constraint does not fully utilize block size information, and [1] limits block size to one.
Reference: 12. <author> P. Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part I, one-dimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(5), </volume> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality [27, 5, 7, 6] and exploit parallelism <ref> [2, 28, 12, 18, 22] </ref>. For instance, a one-level cost function for a tiling might reflect only whether the tile fits in cache (perhaps by considering cache size, line size, and cache associativity [11]), but not the effect of the tiling on instruction level parallelism.
Reference: 13. <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <booktitle> In LCPC, </booktitle> <year> 1991. </year>
Reference-contexts: Quantifying performance: We employ counting arguments similar to <ref> [21, 13, 11, 1, 25] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy.
Reference: 14. <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 5(5), </volume> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 15. <author> D. Gannon and K. Wang. </author> <title> Applying AI Techniques to Program Optimization for Parallel Computers, chapter 12. </title> <publisher> McGraw Hill Co., </publisher> <year> 1989. </year>
Reference-contexts: Researchers have developed a number solutions to this information expansion problem. Many have simply ignored the multi-level information, instead relying on one-level cost functions [27, 11, 25]. Others rephrase program optimization as a search problem and invent heuristics to prune the search <ref> [15, 29] </ref>. We suggest a different solution which first formulates the system to be optimized by quantifying both the effects of tiling choices and the interactions between such choices in a single formula, and then proceeds to minimize this formula. <p> Three architectural scenarios with deleterious tiling interactions. Higher modules represent larger, slower memory units; lower modules are smaller and faster caches or processors. Bold elements highlight the important features in each architecture. sequence of transformations to parallelize a given program <ref> [15] </ref>. None of these works seeks to unify guidance for multiple levels of the memory hierarchy. Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert [4]. Loop fusion and distribution affect both parallelism and locality [20].
Reference: 16. <author> K. Hogstedt, L. Carter, and J. Ferrante. </author> <title> Calculating the idle time of a tiling. </title> <booktitle> In POPL, </booktitle> <year> 1997. </year>
Reference-contexts: This section only briefly summarizes tnat work. As in previous sections, we analyzed an example code using single-level cost functions as well as the in-concert strategy. Our previous work <ref> [16] </ref> has shown that the relationship between the shape of the iteration space and the shape of the tile determine synchronization overhead and data dependence stalls (idle time) for each level of parallelism. <p> A tiling for multiple levels in concert (a hierarchical tiling) has the freedom to define the shape and size of the iteration space at all levels but the first. As tiles execute atomically, the tiles at one level become the iteration space at the next. The results of <ref> [16] </ref> can be used by the in-concert strategy to balance the tradeoffs between the multiple levels, potentially performing better than the (one-level) tiling strategies used today. 4 Conclusions and Future work Simple cost functions guide simple situations. <p> Prior work concentrating on this scenario should optimize well. We further conjecture that, in the presence of two or more of the architectural features presented here, independent 9 The full paper is available at http://www-ucsd.edu/users/carter. Idle time tradoffs for multilevel tiling were also illustrated by example in <ref> [16] </ref>. optimizations for the features (even using multi-level cost functions) is not the best strategy. We are currently developing an interaction-aware guidance system in a compiler for hierarchical tiling.
Reference: 17. <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In POPL, </booktitle> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 18. <author> W. Kelly and W. Pugh. </author> <title> A unifying framework for iteration reordering transfor-mations. </title> <booktitle> In Int. Conf. on Alg. and Arch. for Parallel Processing, </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality [27, 5, 7, 6] and exploit parallelism <ref> [2, 28, 12, 18, 22] </ref>. For instance, a one-level cost function for a tiling might reflect only whether the tile fits in cache (perhaps by considering cache size, line size, and cache associativity [11]), but not the effect of the tiling on instruction level parallelism.
Reference: 19. <author> K. Kennedy and K. S. McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Int. Conf. on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 20. <author> K. Kennedy and K. S. McKinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <booktitle> In LCPC, </booktitle> <year> 1993. </year>
Reference-contexts: None of these works seeks to unify guidance for multiple levels of the memory hierarchy. Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert [4]. Loop fusion and distribution affect both parallelism and locality <ref> [20] </ref>. These two works do not directly address tiling or the multilevel nature of the interactions. Rather than use multi-level cost functions, [29] performs a pruned search on the space of possible combinations of minimizations of one-level cost functions.
Reference: 21. <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In ASPLOS, </booktitle> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] </ref> can improve both data locality and parallel execution time. <p> Quantifying performance: We employ counting arguments similar to <ref> [21, 13, 11, 1, 25] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. <p> Quantifying performance: We employ counting arguments similar to [21, 13, 11, 1, 25] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as <ref> [21, 11, 1] </ref> give methods for choosing tile size in a nested loop; [21] uses a "fits-in" constraint based only on memory capacity (not block size), [11]'s "fits-in" constraint does not fully utilize block size information, and [1] limits block size to one. <p> No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as [21, 11, 1] give methods for choosing tile size in a nested loop; <ref> [21] </ref> uses a "fits-in" constraint based only on memory capacity (not block size), [11]'s "fits-in" constraint does not fully utilize block size information, and [1] limits block size to one.
Reference: 22. <author> D. Lavery and W. Hwu. </author> <title> Unrolling-based optimizations for modulo scheduling. </title> <booktitle> In MICRO-28, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality [27, 5, 7, 6] and exploit parallelism <ref> [2, 28, 12, 18, 22] </ref>. For instance, a one-level cost function for a tiling might reflect only whether the tile fits in cache (perhaps by considering cache size, line size, and cache associativity [11]), but not the effect of the tiling on instruction level parallelism.
Reference: 23. <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: to quantify the difference in results of using one-level and versus multi-level cost functions. 2 Background Given a loop nest, the Iteration Space Graph (ISG) [24] is a directed acyclic graph whose nodes represent the initial values and computations in the loop body, and whose edges represent data 1 dependences <ref> [23] </ref>. The loop nest gives an order for executing the nodes of the ISG. Tiling [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] can improve both data locality and parallel execution time.
Reference: 24. <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling multidimensional iteration spaces for nonshared memory machines. </title> <booktitle> In Supercomputing, </booktitle> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: We use these codes in the different scenarios to quantify the difference in results of using one-level and versus multi-level cost functions. 2 Background Given a loop nest, the Iteration Space Graph (ISG) <ref> [24] </ref> is a directed acyclic graph whose nodes represent the initial values and computations in the loop body, and whose edges represent data 1 dependences [23]. The loop nest gives an order for executing the nodes of the ISG. <p> The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 25. <author> V. Sarkar, G. R. Gao, and S. Han. </author> <title> Locality analysis for distributed shared-memory multiprocessors. </title> <booktitle> In LCPC, </booktitle> <year> 1996. </year>
Reference-contexts: Researchers have developed a number solutions to this information expansion problem. Many have simply ignored the multi-level information, instead relying on one-level cost functions <ref> [27, 11, 25] </ref>. Others rephrase program optimization as a search problem and invent heuristics to prune the search [15, 29]. <p> Quantifying performance: We employ counting arguments similar to <ref> [21, 13, 11, 1, 25] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy.
Reference: 26. <author> V. Sarkar and R. Thekkath. </author> <title> A general framework for iteration-reordering loop transformations (Technical Summary). </title> <booktitle> In PLDI, </booktitle> <year> 1992. </year>
Reference-contexts: Single-level unification: Unimodular transformations can guide loop transformations for locality [27] and parallelism [28]. These works unify only improvement-enabling transformations such as skewing, interchange, and reversal, and do not consider locality and parallelism in concert. The work in <ref> [26] </ref> incorporates a larger set of transformations and unifies the transformation legality checks. AI search techniques on a decision tree of possible optimizations may find a good architectural scenario instance superscalar TLB clustered SMP section 3.1 3.2 3.3 (a) (b) (c) Fig. 1. Three architectural scenarios with deleterious tiling interactions. <p> Our aim is to provide a system whereby a compiler (or human) may consider (quantitatively) the combined effect of memory hierarchy optimizations. In this sense, our work is similar to the "first-class" transformation representation of unimodular matrices and <ref> [26] </ref>. 3 Multi-level cost functions yield better performance or parallelism. The first, (a), represents any architecture where multiple children share a single memory, such as a cache shared by multiple processors.
Reference: 27. <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In PLDI, </booktitle> <year> 1991. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality <ref> [27, 5, 7, 6] </ref> and exploit parallelism [2, 28, 12, 18, 22]. <p> Researchers have developed a number solutions to this information expansion problem. Many have simply ignored the multi-level information, instead relying on one-level cost functions <ref> [27, 11, 25] </ref>. Others rephrase program optimization as a search problem and invent heuristics to prune the search [15, 29]. <p> Whether or not the minimization is closed-form, when tiling for a hierarchy of memory and parallelism this technique derives a multi-level cost function. In this paper, we present evidence to support two claims about cost functions - One-level cost functions may not globally optimize. We show that tiling <ref> [30, 27] </ref> for a single level using a one-level cost function leads to a globally suboptimal choice. In Sec. 3.1, we show for an example code that an optimal choice for cache leaves little instruction level parallelism, and an optimal choice for instruction level parallelism can cause poor cache usage. <p> The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] </ref> can improve both data locality and parallel execution time. <p> In contrast to these and other approaches to tiling size selection, our multi-level approach uses the block size at each level in a multi-level cost function. Single-level unification: Unimodular transformations can guide loop transformations for locality <ref> [27] </ref> and parallelism [28]. These works unify only improvement-enabling transformations such as skewing, interchange, and reversal, and do not consider locality and parallelism in concert. The work in [26] incorporates a larger set of transformations and unifies the transformation legality checks.
Reference: 28. <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> Trans. on Parallel and Distributed Systems, </journal> <volume> 2(4), </volume> <year> 1991. </year>
Reference-contexts: 1 Introduction If computers had only a single level of memory or parallelism, relatively simple cost functions could successfully guide optimization decisions. Such one-level cost functions commonly increase locality [27, 5, 7, 6] and exploit parallelism <ref> [2, 28, 12, 18, 22] </ref>. For instance, a one-level cost function for a tiling might reflect only whether the tile fits in cache (perhaps by considering cache size, line size, and cache associativity [11]), but not the effect of the tiling on instruction level parallelism. <p> The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] </ref> can improve both data locality and parallel execution time. <p> In contrast to these and other approaches to tiling size selection, our multi-level approach uses the block size at each level in a multi-level cost function. Single-level unification: Unimodular transformations can guide loop transformations for locality [27] and parallelism <ref> [28] </ref>. These works unify only improvement-enabling transformations such as skewing, interchange, and reversal, and do not consider locality and parallelism in concert. The work in [26] incorporates a larger set of transformations and unifies the transformation legality checks.
Reference: 29. <author> M. E. Wolf, D. Maydan, and D. Chen. </author> <title> Combining loop transformations considering caches and scheduling. </title> <booktitle> In MICRO-29, </booktitle> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: Researchers have developed a number solutions to this information expansion problem. Many have simply ignored the multi-level information, instead relying on one-level cost functions [27, 11, 25]. Others rephrase program optimization as a search problem and invent heuristics to prune the search <ref> [15, 29] </ref>. We suggest a different solution which first formulates the system to be optimized by quantifying both the effects of tiling choices and the interactions between such choices in a single formula, and then proceeds to minimize this formula. <p> Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert [4]. Loop fusion and distribution affect both parallelism and locality [20]. These two works do not directly address tiling or the multilevel nature of the interactions. Rather than use multi-level cost functions, <ref> [29] </ref> performs a pruned search on the space of possible combinations of minimizations of one-level cost functions. It is not clear whether this method of extending to multiple levels is equivalent to a multi-level cost function. <p> In general, such a table can be calculated for compile-time use from the computation structure of the body of the loop, given the details of the underlying processor. In fact, scheduling optimizations such as <ref> [29] </ref> typically construct such tables. As the number of planes increases, the rate of execution time improvement decreases and can even become negative. A compiler that only considers ILP would choose the minimum number of planes, in this case p = 3, which realizes the minimum execution time. <p> In this simple case, using the cache-specific cost function or the ILP-specific cost function did not yield the best solution, which is found by minimizing the sum of the two cost level-specific functions. Others <ref> [29] </ref> have proposed a different solution to the ILP-cache trade-off. They recognize the interdependence of optimization choices and use a search procedure to take into account many of these interactions. However, they prune the search procedure by making decisions for ILP before tiling decisions for cache locality. <p> Factors such as the associativity of the TLB and the alignment of the matrix can influence whether A remains resident; the determination of these factors is beyond the scope of this paper. Instead, like <ref> [29] </ref>, we use the estimated effective cache size. In particular, the rest of this section assumes fully associative cache 7 . In this case we assume that the A submatrix will reside in the TLB with the assumption B t (H; W ) :75C t .
Reference: 30. <author> M. J. Wolfe. </author> <title> Iteration space tiling for memory hierarchies. </title> <booktitle> In Parallel Processing for Scientific Computing, </booktitle> <year> 1987. </year>
Reference-contexts: Whether or not the minimization is closed-form, when tiling for a hierarchy of memory and parallelism this technique derives a multi-level cost function. In this paper, we present evidence to support two claims about cost functions - One-level cost functions may not globally optimize. We show that tiling <ref> [30, 27] </ref> for a single level using a one-level cost function leads to a globally suboptimal choice. In Sec. 3.1, we show for an example code that an optimal choice for cache leaves little instruction level parallelism, and an optimal choice for instruction level parallelism can cause poor cache usage. <p> The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] </ref> can improve both data locality and parallel execution time.
Reference: 31. <author> M. J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Supercomputing, </booktitle> <year> 1989. </year>
Reference-contexts: The loop nest gives an order for executing the nodes of the ISG. Tiling <ref> [14, 17, 30, 31, 21, 24, 27, 28, 3, 19] </ref> can improve both data locality and parallel execution time.
References-found: 31

