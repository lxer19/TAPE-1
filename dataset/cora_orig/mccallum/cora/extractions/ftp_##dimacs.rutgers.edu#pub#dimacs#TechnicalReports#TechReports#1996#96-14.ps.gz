URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1996/96-14.ps.gz
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1996.html
Root-URL: http://www.cs.rutgers.edu
Email: mthorup@diku.dk,  
Title: sorting in O(n log log n) time and linear space using addition, shift, and bit-wise
Author: by Mikkel Thorup 
Web: http://www.diku.dk/~mthorup  
Address: Universitetsparken 1, DK-2100 Copenhagen East, Denmark  
Affiliation: Department of Computer Science, University of Copenhagen  
Note: Randomized  DIMACS is a cooperative project of Rutgers University, Princeton University, AT&T Bell Laboratories and Bellcore. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Abstract: DIMACS Technical Report 96-14 July 1996 
Abstract-found: 1
Intro-found: 1
Reference: [AH92] <author> S. Albers and T. Hagerup, </author> <title> Improved parallel integer sorting without concurrent writing, </title> <booktitle> in Proceedings of the 3rd ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 463-472, </pages> <year> 1992. </year>
Reference-contexts: We use x # d as a shorthand for x " (d mod jxj). Note that our shifts are cyclic. As an exercise in our notation, we implement a standard routine that we will use fre quently. Similar implementations may be found in, for example, <ref> [AH92] </ref>. Algorithm A: Mask (x; k). Returns y where yhii k equals 1 k if xhii k 6= 0 k ; 0 k otherwise. A.1. x 1 = (x ^ (0 1 k1 ) jxj=k ) + (0 1 k1 ) jxj=k . <p> We are starting with a sequence X of n keys, each taking up one word. Our aim is to shorten the keys, so that they can be sorted in linear time (sorting implies that we group duplicates); either by packed sorting <ref> [AH92, AHNR95] </ref>, or by radix sort with character of length log n, depending on the relationship between n and w. - 4 - First, we have a clustering phase where we compute a short signature of each key so that with high probability, if to keys get the same signature, they <p> so, essentially, we can always assume that we have enough keys to fill the words. 4 Long words In this section, we prove Theorem 3 under the following assumption: log w = !(log log n) () w = (log n) !(1) (1) As pointed out in [AHNR95], the results from <ref> [AH92] </ref> imply Lemma 5 On an Practical we can sort n keys of length O (w=(log n log log n)) in time O (n) and space O (n). Hence, in this section, we goal is to reduce the length of our keys to O (w=(log n log log n)).
Reference: [AHNR95] <author> A. Andersson, T. Hagerup, S. Nilsson, and R. Raman. </author> <title> Sorting in linear time? In Proc. </title> <booktitle> 27th ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 427-436, </pages> <year> 1995. </year>
Reference-contexts: The above algorithm only makes shifts by powers of two, and it only needs O (log n) random words. Our time bound matches that of the current fastest sorting algorithm by Andersson, Hagerup, Raman, and Nilsson <ref> [AHNR95] </ref>. Their algorithm has two variants: one is deterministic uses space 2 "w , where w is the word length and " is a positive constant. Thus the space is unbounded in terms of n. The other variant is randomized and uses linear space like ours, but it uses multiplication. <p> However, that algorithm is based on some powerful nonstandard AC 0 operations. As stated in Theorem 1, in this paper we reduce the running time to O (n log log n) and use standard operations only. - 2 - Techniques Previous randomized linear space O (n log log n) <ref> [AHNR95] </ref> makes extensive use of multiplication based hashing. More precisely, they use hashing to identify duplicates in a list of keys. <p> The space consumption is that of identifying the doublicates. If we use universal hashing [CW79] as in <ref> [AHNR95] </ref>, the identification of duplicates is done in linear time on the unit cost RAM: simply scan the keys one by one, adding each key to the list of previous identical keys, if any. <p> We are starting with a sequence X of n keys, each taking up one word. Our aim is to shorten the keys, so that they can be sorted in linear time (sorting implies that we group duplicates); either by packed sorting <ref> [AH92, AHNR95] </ref>, or by radix sort with character of length log n, depending on the relationship between n and w. - 4 - First, we have a clustering phase where we compute a short signature of each key so that with high probability, if to keys get the same signature, they <p> n) fi (1) , so, essentially, we can always assume that we have enough keys to fill the words. 4 Long words In this section, we prove Theorem 3 under the following assumption: log w = !(log log n) () w = (log n) !(1) (1) As pointed out in <ref> [AHNR95] </ref>, the results from [AH92] imply Lemma 5 On an Practical we can sort n keys of length O (w=(log n log log n)) in time O (n) and space O (n).
Reference: [AMRT96] <author> A. Andersson, P.B. Miltersen, S. Riis, and M. </author> <title> Thorup. Static Dictionaries on AC 0 RAMs: Query time fi( p log n= log log n) is necessary and sufficient (see http://www.diku.dk/~mthorup). FOCS'96, </title> <note> to appear. </note>
Reference-contexts: The above mentioned deterministic algorithm of Andersson et.al. solves the problem, but uses space unbounded in terms of n. Since then Raman has found a deterministic Practical RAM algorithm running in time O (n p linear space. Also, in <ref> [AMRT96] </ref> is presented a randomized linear space algorithm with running time O (n (log log n) 2 ). However, that algorithm is based on some powerful nonstandard AC 0 operations. <p> If we use universal hashing [CW79] as in [AHNR95], the identification of duplicates is done in linear time on the unit cost RAM: simply scan the keys one by one, adding each key to the list of previous identical keys, if any. However, if follows from <ref> [AMRT96] </ref>, that if we restrict ourselves to AC 0 -operations, any such "on-line" approach for finding duplicates, is going to take time (n q log n= log log n). <p> In fact, the lower-bound holds more generally for any instruction set if we say that the cost of an operation is the minimal depth of a polynomial sized circuit computing it. On the other hand, in <ref> [AMRT96] </ref> it is shown that the real "off-line" duplicates problem can be solved in time O (n log log n) using some power-full non-standard AC 0 -operations. Using Lemma 2, this leads to an O (n (log log n) 2 ) AC 0 sorting algorithm. <p> The above type of reasoning is straightforward, and in the rest of the paper we will often be less explicit about it. 3 The general approach We will borrow the general approach from <ref> [AMRT96] </ref>. We are starting with a sequence X of n keys, each taking up one word. <p> In contrast it follows from <ref> [AMRT96] </ref>, that if we want to hash integers one at the time, the cost is ( q k= log k) on the more general AC 0 RAM, even if we allow pre-processed tables of size 2 k O (1) .
Reference: [BH89] <author> P. Beame and J. H-astad. </author> <title> Optimal bounds on the decision problems on the CRCW PRAM, </title> <editor> J. </editor> <booktitle> ACM 36 (1989), </booktitle> <pages> 643-670. </pages>
Reference-contexts: The other variant is randomized and uses linear space like ours, but it uses multiplication. Multiplication is considered expensive from several perspectives. From a theoretical view-point, it is known that the minimal depth of a polynomial sized circuit implementing multiplication is fi (log w= log log w) <ref> [BH89] </ref>, which is unbounded in n. In contrast, all the Practical RAM operations are in AC 0 , that is, they are all implementable by polynomial sized constant depth circuits. From a practical view-point, we may circumvent the theoretical lower-bound by pipelining several multiplications at the time.
Reference: [BMM96] <author> A. Brodnik, P.B. Miltersen, and I. Munro. </author> <title> Word level parallelism some upper and lower bounds, </title> <type> Manuscript, </type> <year> 1996. </year>
Reference-contexts: This completes the proof of Theorem 1. 6 Batched universal hashing In this section, we briefly describe some other results that can be achieved with similar techniques. In <ref> [BMM96] </ref> it is argued that Shonhage-Strassen multiplication can be simulated on the Practical RAM to run in time O ((log w)(log log w) 2 ).
Reference: [CW79] <author> J.L. Carter and M.N. Wegman. </author> <title> Universal classes of hash functions, </title> <journal> J. Comp. Syst. Sci. </journal> <volume> 18 (1979), </volume> <pages> 143-154. </pages>
Reference-contexts: The space consumption is that of identifying the doublicates. If we use universal hashing <ref> [CW79] </ref> as in [AHNR95], the identification of duplicates is done in linear time on the unit cost RAM: simply scan the keys one by one, adding each key to the list of previous identical keys, if any.
Reference: [Die96] <author> M. Dietzfelbinger. </author> <title> Universal classes of hash functions, </title> <booktitle> In Proc. STACS'96, </booktitle> <volume> LNCS 1046, </volume> <pages> pages 569-580, </pages> <year> 1996. </year>
Reference-contexts: In fact, they get that - 11 - if a word consists of consecutive pairs of k-bit integers, we can multiply all pairs in time O (log k)(log log k) 2 ). Combining this with the techniques from above and the multiplicative strongly universal hashing from <ref> [Die96] </ref>, one can implement batched strongly universal hashing of n w, w-bit integers into k-bit integers in time O (n log k (log log k) 2 ), that is, at cost O (log k (log log k) 2 ) per hashed integer.
Reference: [FW93] <author> M.L. Fredman and D.E. Willard. </author> <title> Surpassing the information theoretic bound with fusion trees. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 47 </volume> <pages> 424-436, </pages> <year> 1993. </year>
Reference-contexts: Hence Practical RAMs may be used for future sorting co-processors with large word length. The problem of doing o (n log n) sorting with AC 0 operations was posed by Fredman and Willard <ref> [FW93] </ref> when they presented the first o (n log n) sorting algorithms. Both their deterministic O (n log n= log log n) algorithm and their randomized O ( p log n) algorithm were heavily based on multiplication.
Reference: [Mil96] <author> P.B. Miltersen. </author> <title> Lower bounds for static dictionaries on RAMs with bit operations but no multiplication. </title> <journal> ICALP'96, </journal> <note> to appear. - 12 </note> - 
Reference-contexts: 1 Introduction In this paper we consider sorting on a very simple RAM where the only word-operations are addition, shift, and bit-wise boolean operations. Besides these word-operations, we have direct and indirect addressing, jumps, and conditional statements. Such a RAM has been referred to as a Practical RAM <ref> [Mil96] </ref>. In this paper we show Theorem 1 On a Practical RAM, there is a randomized algorithm sorting n words in O (n log log n) time and linear space. The above algorithm only makes shifts by powers of two, and it only needs O (log n) random words.
Reference: [Pap94] <author> C.H. Papadimitriou. </author> <title> Computational Complexity. </title> <publisher> Addison Wesley, </publisher> <year> 1994. </year>
Reference-contexts: First choose a random permutation of the pieces in a key. Divide (x) into q = r=(3 log r) segments. In each segment, we expect 3 log r non-empty pieces. Using Chernoff bounds (see e.g. <ref> [Pap94] </ref>), it follows that the probability that a segment has more than 6 log r non-empty pieces is &lt; 1=r. Actually our distribution is not quite Binomial distribution. It would be if each non-empty piece was independently assigned a random segment.
Reference: [PS76] <author> Pratt and Stockmeyer. </author> <title> A Characterization of the Power of Vector Machines. </title> <journal> JCSS, </journal> <volume> 12 </volume> <pages> 198-221, </pages> <year> 1976. </year>
Reference-contexts: Thus, swapping the quadrants is done in time O (q). Thus T (q) 2T (q=2) + q, so T (q) = O (q log q). A similar matrix transposition is used in <ref> [PS76] </ref> for unbounded word-lengths. Lemma 13 We can calculate Sign H (x) in O (1 + b=w log b) amortized cost per key. Proof: Consider words ~x 1 ; : : : ; ~x b , each consisting of w=b keys.
Reference: [Ram96] <author> R. Raman. </author> <title> Fast Algorithms for Shortest Paths and Sorting. </title> <journal> ESA'96, </journal> <note> to appear. </note>
Reference: [Tho96] <author> M. </author> <title> Thorup. On RAM priority queues. </title> <booktitle> In Proceedings of the 7th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 59-67, </pages> <year> 1996. </year>
Reference-contexts: In this paper, we show Theorem 3 We can identify the duplicates in a list of n (w=q)-bit keys in O (n+n log log n=q) expected time by a randomized Practical RAM algorithm using linear space. Together with Lemma 2, this immediately implies Theorem 1. In <ref> [Tho96] </ref> is given a general reduction from amortized monotone priority queues to sorting. Here a monotone priority queue is a priority queue where the minimum is nondecreasing. This restriction does not affect most gready algorithms.
References-found: 13

