URL: ftp://ftp.cs.rochester.edu/pub/u/kthanasi/95.JPDC.SW-Coherence.ps.gz
Refering-URL: http://www.cs.rochester.edu/research/cashmere/97-12_DEC_report/bib.html
Root-URL: 
Email: fkthanasi,scottg@cs.rochester.edu  
Title: High Performance Software Coherence for Current and Future Architectures  
Author: Leonidas I. Kontothanassis and Michael L. Scott 
Note: This work was supported in part by NSF InstitutionalInfrastructure grant no. CDA-8822724 and ONR research grant no. N00014-92-J-1801 (in conjunction with the DARPA Research in Information Science and TechnologyHigh Performance Computing, Software Science and Technology program, ARPA Order no. 8930).  
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Bianchini, D. Chaiken, K. Johnson, D. Kranz, J. Kubiatowicz, B. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the Twenty-Second International Symposium on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year> <month> 31 </month>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others [2, 12]. Coherence is substantially harder to achieve on large-scale multiprocessors <ref> [1, 15, 19, 23] </ref>; it increases both the cost of the machine and the time and intellectual effort required to bring it to market. Given the speed of advances in microprocessor technology, long development times generally lead to machines with out-of-date processors. <p> Interrupt handling - 40, 140, or 500 cycles between interrupt occurrence and start of execution of the interrupt handler. These values represent the expected cost of an interrupt for a very fast interrupt mechanism (e.g. Sparcle <ref> [1] </ref>, a normal one, and a particularly slow one. TLB Management - 24, 48, or 120 cycles for tlb service fault. These values represent the expected cost of a tlb fill when done in fast hardware, somewhat slower hardware, or entirely in software.
Reference: [2] <author> J. Archibald and J. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others <ref> [2, 12] </ref>. Coherence is substantially harder to achieve on large-scale multiprocessors [1, 15, 19, 23]; it increases both the cost of the machine and the time and intellectual effort required to bring it to market.
Reference: [3] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: We simulated 40000 particles for 10 steps in our studies. Water is a molecular dynamics simulation computing inter- and intra-molecule forces for a set of water molecules. We used 256 molecules and 3 times steps. Finally appbt is from the NASA parallel benchmarks suite <ref> [3] </ref>. It computes an approximation to Navier-Stokes equations. It was translated to shared memory from the original message-based form by Doug Burger and Sanjay Mehta at the University of Wisconsin.
Reference: [4] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Machines in this non-cache-coherent, non-uniform memory access (NCC-NUMA) class include the Cray Research T3D and the Princeton Shrimp <ref> [4] </ref>. In comparison to hardware-coherent machines, NCC-NUMAs can more easily be built from commodity parts, with only a small incremental cost per processor for large systems, and can follow improvements in microprocessors and other hardware technologies closely. <p> Moreover, experience with machines such as the IBM RP3, the BBN Butterfly series, and the current Cray Research T3D suggests that a memory-mapped interface to the network (without coherence) is not much more expensive than a message-passing interface. Memory-mapped interfaces for ATM networks are likely to be available soon <ref> [4] </ref>; we see our work as ideally suited to machines equipped with such an interface. We are currently pursuing software protocol optimizations that should improve performance for important classes of programs.
Reference: [5] <author> W. J. Bolosky, M. L. Scott, R. P. Fitzgerald, R. J. Fowler, and A. L. Cox. </author> <title> NUMA Policies and Their Relation to Memory Architecture. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 212-221, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Hardware coherence may also be affected, due to the placement of pages in memory 22 modules, but this is a secondary effect we have chosen to ignore in our study. Previous studies on the impact of page size on the performance of Software DSM systems <ref> [5] </ref> indicate that the smaller pages can provide significant performance improvements. The main reason for this result is the reduction in false sharing achieved by smaller coherence units. <p> Our use of uncached references to reduce the overhead of coherence management can also be found in systems for NUMA memory management <ref> [5, 10, 22] </ref>. Designed for machines without caches, these systems migrate and replicate pages in the manner of distributed shared memory systems, but also make on-line decisions between page movement and uncached reference.
Reference: [6] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Unfortunately, the current state of the art in software coherence for message-passing machines provides performance nowhere close to that of hardware cache coherence. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations <ref> [6, 18, 35] </ref>. First, because they are based on messages, DSM systems must interrupt the execution of remote processors in order to perform any time-critical inter-processor operations. <p> Third, in order to maximize concurrency in the face of false sharing in page-size blocks, the fastest DSM systems permit multiple writable copies of a page, forcing them to compute diffs with older versions in order to merge the changes <ref> [6, 18] </ref>. <p> As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin <ref> [6] </ref>, Treadmarks [18], and the work of Petersen and Li [26], we allow more than one processor to write a page concurrently, and we use a variant of release consistency [23] to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) <p> Delayed write notices were shown to improve performance in the Munin distributed shared memory system <ref> [6] </ref>, which runs on networks of workstations and communicates solely via messages. Though the relative costs of operations are quite different, experiments indicate (see section 4) that delayed transitions are generally beneficial in our environment as well. <p> We have also examined architectural alternatives and program-structuring issues that were not addressed by Petersen and Li. Our work resembles Munin <ref> [6] </ref> and lazy release consistency [17] in its use of delayed write notices, but we take advantage of the globally accessible physical address space for cache fills and for access to the coherent map and the local weak lists.
Reference: [7] <author> Y. Chen and A. Veidenbaum. </author> <title> An Effective Write Policy for Software Coherence Schemes. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: We ran our experiments under three different assumptions: write-through caches where each individual write is immediately sent to memory, write-back caches with per-word hardware dirty bits in the cache, and write-through caches with a write-merge buffer <ref> [7] </ref> that hangs onto recently-written lines and coalesces any writes that are directed to the same line. The write-merge buffer also requires per-word dirty bits to make sure that falsely shared lines are merged correctly. <p> Specifically, we compare the performance obtained with a write-through cache, a write-back cache, and a write-through cache with a buffer for merging writes <ref> [7] </ref>. The policy is applied on only shared data. Private data uses by default a write-back policy. <p> With a large amount of write traffic we may have simply replaced waiting for the write-back with waiting for missing 21 acknowledgments. Write-through caches with a write-merge buffer <ref> [7] </ref> employ a small fully associative buffer between the cache and the interconnection network. The buffer merges writes to the same cache line, and allocates a new entry for a write to a non-resident cache line.
Reference: [8] <author> H. Cheong and A. V. Veidenbaum. </author> <title> Compiler-Directed Cache Management in Multiprocessors. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 39-47, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Both these systems use their extra hardware to overlap coherence processing and computation (possibly at the expense of extra coherence traffic) in order to avoid a higher waiting penalty at synchronization operations. Coherence for distributed memory with per-processor caches can also be maintained entirely by a compiler <ref> [8] </ref>. Under this approach the compiler inserts the appropriate cache flush and invalidation instructions in the code, to enforce data consistency.
Reference: [9] <author> M. Cierniak and W. Li. </author> <title> Unifying data and control transformations for distributed shared-memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995 </year> <month> (to appear). </month>
Reference-contexts: Current trends, however, are reducing the importance of each of these advantages. Relaxed consistency models mitigate the impact of false sharing by limiting spurious coherence operations to synchronization points, and programmers and compilers are becoming better at avoiding false sharing as well <ref> [9, 14] </ref>. While any operation implemented in hardware is likely to be faster than equivalent software, particularly complex operations cannot always be implemented in hardware at reasonable cost.
Reference: [10] <author> A. L. Cox and R. J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year> <month> 32 </month>
Reference-contexts: A page leaves the weak state and becomes uncached when no processor has a mapping to the page anymore. The state of a page is a property of the system as a whole, not (as in most protocols) the viewpoint of a single processor. Borrowing terminology from PLATINUM <ref> [10] </ref>, the distributed data structure consisting of this information stored at home nodes is called the coherent map. <p> Our use of uncached references to reduce the overhead of coherence management can also be found in systems for NUMA memory management <ref> [5, 10, 22] </ref>. Designed for machines without caches, these systems migrate and replicate pages in the manner of distributed shared memory systems, but also make on-line decisions between page movement and uncached reference.
Reference: [11] <author> A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: a Case Study. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Moreover, the combination of efficient software coherence with fast (e.g., ATM) networks would make parallel programming on networks of workstations a practical reality <ref> [11] </ref>. Unfortunately, the current state of the art in software coherence for message-passing machines provides performance nowhere close to that of hardware cache coherence. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations [6, 18, 35].
Reference: [12] <author> S. J. Eggers and R. H. Katz. </author> <title> Evaluation of the Performance of Four Snooping Cache Coherency Protocols. </title> <booktitle> In Proceedings of the Sixteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 2-15, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others <ref> [2, 12] </ref>. Coherence is substantially harder to achieve on large-scale multiprocessors [1, 15, 19, 23]; it increases both the cost of the machine and the time and intellectual effort required to bring it to market.
Reference: [13] <author> M. J. Feeley, J. S. Chase, V. R. Narasayya, and H. M. Levy. </author> <title> Log-Based Distributed Shared Memory. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Alteratively, coherence can be maintained in object-oriented system by tracking method calls, or by identifying the specific data structures protected by particular synchronization operations <ref> [13, 30, 35] </ref>. Such an approach can make it substantially easier for the compiler to implement consistency, but only for restricted programming models. 29 7 Conclusions We have shown that supporting a shared memory programming model while maintaining high performance does not necessarily require expensive hardware.
Reference: [14] <author> E. Granston. </author> <title> Toward a Compile-Time Methodology for Reducing False Sharing and Communication Traffic in Shared Virtual Memory System. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Lecture Notes in Computer Science, </booktitle> <pages> pages 273-289. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Current trends, however, are reducing the importance of each of these advantages. Relaxed consistency models mitigate the impact of false sharing by limiting spurious coherence operations to synchronization points, and programmers and compilers are becoming better at avoiding false sharing as well <ref> [9, 14] </ref>. While any operation implemented in hardware is likely to be faster than equivalent software, particularly complex operations cannot always be implemented in hardware at reasonable cost.
Reference: [15] <author> D. B. Gustavson. </author> <title> The Scalable Coherent Interface and Related Standards Projects. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 10-22, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others [2, 12]. Coherence is substantially harder to achieve on large-scale multiprocessors <ref> [1, 15, 19, 23] </ref>; it increases both the cost of the machine and the time and intellectual effort required to bring it to market. Given the speed of advances in microprocessor technology, long development times generally lead to machines with out-of-date processors.
Reference: [16] <author> M. D. Hill and J. R. Larus. </author> <title> Cache Considerations for Multiprocessor Programmers. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 97-102, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Furthermore we show that the flexibility of software coherence can allow for optimizations that may be too hard to implement in a hardware-coherent system and that can further improve performance. Our program modifications are also beneficial for hardware-coherent systems ; several are advocated in the literature <ref> [16] </ref>. <p> In cases of contention the lock protecting the coherent map entry is unavailable: it is owned by the processor (s) attempting to map the page for access. Data structure alignment and padding are well-known methods of reducing false sharing <ref> [16] </ref>. Since coherence blocks in software coherent systems are large (4K bytes in our case), it is unreasonable to require padding of data structures to that size. However we can often pad data structures to subpage boundaries so that a collection of them will fit exactly in a page.
Reference: [17] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: We have also examined architectural alternatives and program-structuring issues that were not addressed by Petersen and Li. Our work resembles Munin [6] and lazy release consistency <ref> [17] </ref> in its use of delayed write notices, but we take advantage of the globally accessible physical address space for cache fills and for access to the coherent map and the local weak lists.
Reference: [18] <author> P. Keleher, A. L. Cox, S. Dwarkadas, and W. Zwaenepoel. ParaNet: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the USENIX Winter '94 Technical Conference, </booktitle> <address> San Francisco, CA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Unfortunately, the current state of the art in software coherence for message-passing machines provides performance nowhere close to that of hardware cache coherence. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations <ref> [6, 18, 35] </ref>. First, because they are based on messages, DSM systems must interrupt the execution of remote processors in order to perform any time-critical inter-processor operations. <p> Third, in order to maximize concurrency in the face of false sharing in page-size blocks, the fastest DSM systems permit multiple writable copies of a page, forcing them to compute diffs with older versions in order to merge the changes <ref> [6, 18] </ref>. <p> As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin [6], Treadmarks <ref> [18] </ref>, and the work of Petersen and Li [26], we allow more than one processor to write a page concurrently, and we use a variant of release consistency [23] to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) As in
Reference: [19] <author> Kendall Square Research. </author> <title> KSR1 Principles of Operation. </title> <address> Waltham MA, </address> <year> 1992. </year>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others [2, 12]. Coherence is substantially harder to achieve on large-scale multiprocessors <ref> [1, 15, 19, 23] </ref>; it increases both the cost of the machine and the time and intellectual effort required to bring it to market. Given the speed of advances in microprocessor technology, long development times generally lead to machines with out-of-date processors.
Reference: [20] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Distributed Shared Memory for New Generation Networks. </title> <type> TR 578, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> March </month> <year> 1995. </year> <month> 33 </month>
Reference-contexts: In comparison to hardware-coherent machines, NCC-NUMAs can more easily be built from commodity parts, with only a small incremental cost per processor for large systems, and can follow improvements in microprocessors and other hardware technologies closely. In another paper <ref> [20] </ref>, we show that NCC-NUMAs provide performance advantages over DSM systems ranging from 50% to as much as an order of magnitude. On the downside, our NCC-NUMA protocols require the ability to control a processor's cache explicitly, a capability provided by many but not all current microprocessors.
Reference: [21] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Our experiments document the effectiveness of software coherence for a wide range of hardware parameters. Based on current trends, we predict that software coherence can provide an even 1 The advent of protocol processors <ref> [21, 28] </ref> can make it possible to combine the flexibility of software with the speed and parallelism provided by hardware. 3 more cost-effective alternative to hardware-based cache coherence on future generations of machines. <p> The best performance, clearly, will be obtained by systems that combine the speed and concurrency of existing hardware coherence mechanisms with the flexibility of software coherence. This goal may be achieved by a new generation of machines with programmable network controllers <ref> [21, 28] </ref>. It is not 30 yet clear whether the additional performance of such machines will justify their design time and cost. Our suspicion, based on our results, is that less elaborate hardware will be more cost effective.
Reference: [22] <author> R. P. LaRowe Jr. and C. S. Ellis. </author> <title> Experimental Comparison of Memory Management Policies for NUMA Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Our use of uncached references to reduce the overhead of coherence management can also be found in systems for NUMA memory management <ref> [5, 10, 22] </ref>. Designed for machines without caches, these systems migrate and replicate pages in the manner of distributed shared memory systems, but also make on-line decisions between page movement and uncached reference.
Reference: [23] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: By far the most challenging part of the problem is maintaining cache coherence. Coherence is easy to achieve on small, bus-based machines, where every processor can see the memory traffic of the others [2, 12]. Coherence is substantially harder to achieve on large-scale multiprocessors <ref> [1, 15, 19, 23] </ref>; it increases both the cost of the machine and the time and intellectual effort required to bring it to market. Given the speed of advances in microprocessor technology, long development times generally lead to machines with out-of-date processors. <p> As in Munin [6], Treadmarks [18], and the work of Petersen and Li [26], we allow more than one processor to write a page concurrently, and we use a variant of release consistency <ref> [23] </ref> to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) As in the work of Petersen and Li, we exploit the global physical address space to move data at the granularity of cache lines: instead of copying pages we map <p> In fact, early experiments we have conducted with on-line NUMA policies and relaxed consistency have failed badly in their attempt to determine when to use uncached references. On the hardware side our work bears a resemblance to the Stanford Dash project <ref> [23] </ref> in the use of a relaxed consistency model, and to the Georgia Tech Beehive project [32] in the use of relaxed consistency and per-word dirty bits for successful merging of inconsistent cache lines.
Reference: [24] <author> M. Marchetti, L. Kontothanassis, R. Bianchini, and M. L. Scott. </author> <title> Using Simple Page Placement Policies to Reduce the Cost of Cache Fills in Coherent Shared-Memory Systems. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <address> Santa Barbata, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Designed for machines without caches, these systems migrate and replicate pages in the manner of distributed shared memory systems, but also make on-line decisions between page movement and uncached reference. We have experimented with dynamic page movement in conjunction with software coherence on NCC-NUMA machines <ref> [24] </ref>, and have found that while appropriate placement of a unique page copy reduces the average cache fill cost appreciably, replication of pages provides no significant benefit in the presence of hardware caches. Moreover, we have found that relaxed consistency greatly reduces the opportunities for profitable uncached data access.
Reference: [25] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: If lock operations are properly designed we can also hide the latency for the data transfers behind the latency for the lock operations themselves. If we employ a distributed queue-based lock <ref> [25] </ref>, a read of the coherent map entry can be initiated immediately after starting the fetch-and-store operation that retrieves the lock's tail pointer. If the fetch-and-store returns nil (indicating that the lock was free), then the data will arrive right away.
Reference: [26] <author> K. Petersen and K. Li. </author> <title> Cache Coherence for Shared Memory Multiprocessors Based on Virtual Memory Support. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <address> Newport Beach, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: We then describe our experimental methodology and application suite in section 3 and present results in section 4. We compare our protocol to a variety of existing alternatives, including release-consistent hardware, straightforward sequentially-consistent software, and a coherence scheme for small-scale NCC-NUMAs due to Petersen and Li <ref> [26] </ref>. We show that certain simple program modifications can improve the performance of software coherence substantially. <p> As in most software coherence systems, we use virtual memory protection bits to enforce consistency at the granularity of pages. As in Munin [6], Treadmarks [18], and the work of Petersen and Li <ref> [26] </ref>, we allow more than one processor to write a page concurrently, and we use a variant of release consistency [23] to limit coherence operations to synchronization points. (Between these points, processors can continue to use stale data in their caches.) As in the work of Petersen and Li, we exploit <p> We are also hopeful that smarter compilers will be able to make many of the changes automatically. 4.2 Software coherence protocol alternatives This section compares our software protocol (presented in section 2) to the protocol devised by Petersen and Li <ref> [26] </ref> (modified to distribute the centralized weak list among the memories of the machine), and to a sequentially consistent page-based cache coherence protocol. <p> We must use the write-protect bits to generate page faults. rel.centr.nodel: Same as rel.distr.nodel, except that write notices are propagated by inserting weak pages in a global list which is traversed on acquires. This is the protocol of Petersen and Li <ref> [26] </ref>, with the exception that while the weak list is conceptually centralized, its entries are distributed physically among the nodes of the machine. rel.centr.del: Same as rel.distr.del, except that write notices are propagated by inserting weak pages in a global list which is traversed on acquires. seq: A sequentially consistent software <p> Moving to relaxed consistency, however, and to an architecture that uses pages for the unit of coherence but cache lines for the data fetch unit, reverses the decision in favor of large pages <ref> [26] </ref>. Relaxed consistency mitigates the impact of false sharing, and the larger page size reduces the length of the weak list that needs to be traversed on an acquire operation. <p> The program suffers more from the cost of initiating uncached references than from a lack of bandwidth: its performance improves only moderately as bandwidth increases. 6 Related Work Our work is most closely related to that of Petersen and Li <ref> [26] </ref>: we both use the notion of weak pages, and purge caches on acquire operations.
Reference: [28] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level Shared-Memory. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Our experiments document the effectiveness of software coherence for a wide range of hardware parameters. Based on current trends, we predict that software coherence can provide an even 1 The advent of protocol processors <ref> [21, 28] </ref> can make it possible to combine the flexibility of software with the speed and parallelism provided by hardware. 3 more cost-effective alternative to hardware-based cache coherence on future generations of machines. <p> The best performance, clearly, will be obtained by systems that combine the speed and concurrency of existing hardware coherence mechanisms with the flexibility of software coherence. This goal may be achieved by a new generation of machines with programmable network controllers <ref> [21, 28] </ref>. It is not 30 yet clear whether the additional performance of such machines will justify their design time and cost. Our suspicion, based on our results, is that less elaborate hardware will be more cost effective.
Reference: [29] <author> E. Rothberg, J. P. Singh, and A. Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> In Proceedings of the Twentieth International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: While there is no universal agreement on the appropriate cache size for simulations with a given data set size, recent work confirms that the relative sizes of per-processor caches and working sets are a crucial factor in performance <ref> [29] </ref>. All the results in previous sections were obtained with caches sufficiently large to hold the working set, in order to separate the performance of the coherence protocols from the effect of conflict/capacity misses.
Reference: [30] <author> H. S. Sandhu. </author> <title> Algorithms for Dynamic Software Cache Coherence. </title> <journal> In Journal of Parallel and Distributed Computing, </journal> <note> October 1995 (to appear). 34 </note>
Reference-contexts: Alteratively, coherence can be maintained in object-oriented system by tracking method calls, or by identifying the specific data structures protected by particular synchronization operations <ref> [13, 30, 35] </ref>. Such an approach can make it substantially easier for the compiler to implement consistency, but only for restricted programming models. 29 7 Conclusions We have shown that supporting a shared memory programming model while maintaining high performance does not necessarily require expensive hardware.
Reference: [31] <author> I. Schoinas, B. Falsafi, A. R. Lebeck, S. K. Reinhardt, J. R. Larus, and D. A. Wood. </author> <title> Fine--grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Hardware protocols also have the advantage of smaller block sizes, and therefore less false sharing, but improvements in program structuring techniques, and the use of relaxed consistency, are eroding this advantage too. Recent work also suggests <ref> [31, 35] </ref> that software-coherent systems may be able to enforce consistency on small blocks efficiently by using binary editing techniques to embed coherence operations in the program text.
Reference: [32] <author> G. Shah and U. Ramachandran. </author> <title> Towards Exploiting the Architectural Features of Beehive. </title> <institution> GIT-CC-91/51, College of Computing, Georgia Institute of Technology, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: On the hardware side our work bears a resemblance to the Stanford Dash project [23] in the use of a relaxed consistency model, and to the Georgia Tech Beehive project <ref> [32] </ref> in the use of relaxed consistency and per-word dirty bits for successful merging of inconsistent cache lines. Both these systems use their extra hardware to overlap coherence processing and computation (possibly at the expense of extra coherence traffic) in order to avoid a higher waiting penalty at synchronization operations.
Reference: [33] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Sor computes the steady state temperature of a metal sheet using a banded parallelization of red-black successive overrelaxation on a 640 fi 640 grid. Fft computes a one-dimensional FFT on a 65536-element array of complex numbers. Mp3d and water are part of the SPLASH suite <ref> [33] </ref>. Mp3d is a wind-tunnel airflow simulation. We simulated 40000 particles for 10 steps in our studies. Water is a molecular dynamics simulation computing inter- and intra-molecule forces for a set of water molecules. We used 256 molecules and 3 times steps.
Reference: [34] <author> J. E. Veenstra and R. J. Fowler. MINT: </author> <title> A Front End for Efficient Simulation of Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Second International Workshop on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS 94), </booktitle> <pages> pages 201-207, </pages> <address> Durham, NC, </address> <month> January - February </month> <year> 1994. </year>
Reference-contexts: Our simulator consists of two parts: a front end, Mint <ref> [34] </ref>, that simulates the execution of the processors, and a back end that simulates the memory system.

References-found: 33

