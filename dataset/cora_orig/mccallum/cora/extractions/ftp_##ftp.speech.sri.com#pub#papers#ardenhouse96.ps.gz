URL: ftp://ftp.speech.sri.com/pub/papers/ardenhouse96.ps.gz
Refering-URL: http://www.speech.sri.com/people/sankar/publications.html
Root-URL: 
Title: NOISE-RESISTANT FEATURE EXTRACTION AND MODEL TRAINING FOR ROBUST SPEECH RECOGNITION  
Author: A. Sankar A. Stolcke T. Chung L. Neumeyer M. Weintraub H. Franco F. Beaufays 
Address: Menlo Park, CA  
Affiliation: Speech Technology And Research Laboratory SRI International  
Abstract: In this paper we report on our recent work on noise-robust feature extraction and model training to alleviate the mismatch caused by different microphones and ambient room noise in the context of the 1995 DARPA-sponsored H3 benchmark test, which used the unlimited-vocabulary North American Business News (NABN) database. We present a novel noise-robust feature extraction algorithm that is a combination of our previously developed minimum mean square error (MMSE) log-energy estimation algorithm and the probabilistic optimum filtering (POF) algorithm. We also studied an approach based on training the automatic speech recognition (ASR) system with previously collected noisy speech. While both the above approaches gave significant improvements, it was found that combining them gave the best results. We also report on a new part-of-speech (POS) language model that makes it possible to train robust POS language models that incorporate longer contexts than is possible with word-based language models. Preliminary results using this approach were encouraging. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Erell and M. Weintraub, </author> <title> "Filterbank-Energy Estimation Using Mixture and Markov Models for Recognition of Noisy Speech," </title> <journal> IEEE-TSAP, </journal> <volume> vol. 1, </volume> <pages> pp. 68-76, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: We present a novel noise-robust feature extraction algorithm that is a combination of our previously developed minimum mean square error (MMSE) log-energy estimation algorithm <ref> [1] </ref> and the probabilistic optimum filtering (POF) algorithm [2]. Our ASR system is based on hidden Markov models (HMM) which were trained using the SI-284 Wall Street Journal (WSJ) Sennheiser clean speech database. <p> NOISE-ROBUST FEATURE EXTRACTION In order to reduce the mismatch caused by ambient room noise, we experimented with methods based on the MMSE log-energy estimation method <ref> [1] </ref> and the POF algorithm [2], which were previously developed at SRI. We then developed a new method that combined these two approaches. 3.1. <p> The estimates L y of the log-energy of the noisy speech are distorted because of the noise. The MMSE approach estimates the clean log-energy L x given the observed log energy L y by computing the conditional expected value of the clean log-energy given the noisy log-energy <ref> [1] </ref>. Thus ^ L x = E [L x jL y ] (2) Once the MMSE estimates of the log-energy are computed, we apply the inverse discrete cosine transform to get the Mel cepstrum. 3.2.
Reference: [2] <author> L. Neumeyer and M. Weintraub, </author> <title> "Probabilistic Optimum Filtering for Robust Speech Recognition," </title> <booktitle> in ICASSP, </booktitle> <pages> pp. </pages> <address> I-417-I-420, </address> <year> 1994. </year>
Reference-contexts: We present a novel noise-robust feature extraction algorithm that is a combination of our previously developed minimum mean square error (MMSE) log-energy estimation algorithm [1] and the probabilistic optimum filtering (POF) algorithm <ref> [2] </ref>. Our ASR system is based on hidden Markov models (HMM) which were trained using the SI-284 Wall Street Journal (WSJ) Sennheiser clean speech database. <p> NOISE-ROBUST FEATURE EXTRACTION In order to reduce the mismatch caused by ambient room noise, we experimented with methods based on the MMSE log-energy estimation method [1] and the POF algorithm <ref> [2] </ref>, which were previously developed at SRI. We then developed a new method that combined these two approaches. 3.1. MMSE log-energy estimation Suppose the observed speech y is generated by passing the sum of the original speech x and colored noise n through a microphone channel. <p> Instead, it is assumed that the original (clean) speech cepstra x c can be recovered from the noisy speech cepstra y c by applying a set of linear transformations to the noisy speech cepstra. Each transformation in the set is tied to a separate acoustic region <ref> [2] </ref>. In order to train the POF filters, it is necessary to use stereo pairs of cepstrum vectors from noisy and clean speech. Ideally, we would like to use stereo pairs where the noisy speech matches that observed in the test environment.
Reference: [3] <author> V. Digalakis, D. Rtischev, and L. Neumeyer, </author> <title> "Speaker adaptation using constrained reestimation of Gaussian mixtures," </title> <journal> IEEE-TSAP, </journal> <volume> vol. 3, no. 5, </volume> <pages> pp. 357-366, </pages> <year> 1995. </year>
Reference-contexts: Acoustic adaptation techniques can also be used to bridge the mismatch between the training and testing acoustic environments, and the differences between individual test speakers and the training population. In previous work, maximum-likelihood and Bayesian adaptation techniques have been developed <ref> [3, 4, 5, 6] </ref>, and have given significant improvements in performance for both nonnative speakers [3] and noisy speech [7]. More recently, we have developed new feature and model-space adaptation techniques and applied them to both nonnative and native speakers [8]. <p> In previous work, maximum-likelihood and Bayesian adaptation techniques have been developed [3, 4, 5, 6], and have given significant improvements in performance for both nonnative speakers <ref> [3] </ref> and noisy speech [7]. More recently, we have developed new feature and model-space adaptation techniques and applied them to both nonnative and native speakers [8]. We have also recently developed a novel part-of-speech (POS) tag-based language model. <p> ACOUSTIC ADAPTATION Acoustic adaptation can also be used to reduce the mismatch between training and testing acoustic environments. In previous work, maximum-likelihood (ML) and Bayesian adaptation algorithms have been developed <ref> [3, 11, 4, 5, 6] </ref>, and applied to nonnative speech recognition [3] and noisy speech recognition [7]. In particular, maximum-likelihood adaptation has been applied in both the feature-space and model-space [4, 8]. <p> ACOUSTIC ADAPTATION Acoustic adaptation can also be used to reduce the mismatch between training and testing acoustic environments. In previous work, maximum-likelihood (ML) and Bayesian adaptation algorithms have been developed [3, 11, 4, 5, 6], and applied to nonnative speech recognition <ref> [3] </ref> and noisy speech recognition [7]. In particular, maximum-likelihood adaptation has been applied in both the feature-space and model-space [4, 8]. In feature-space schemes, the test speech features are transformed to match the trained models, whereas in model-space schemes, the model parameters are transformed to match the test features. <p> In feature-space schemes, the test speech features are transformed to match the trained models, whereas in model-space schemes, the model parameters are transformed to match the test features. In our previous ML adaptation algorithms, we used an affine transformation for each feature component <ref> [3, 4] </ref>. This transform results in a corresponding transformation of the HMM mean and variance vectors. In order to approximate complex functions, a separate transform is used for different Gaussian clusters [3]. The mapping was applied separately to each feature component in order to make the problem mathematically tractable. <p> In our previous ML adaptation algorithms, we used an affine transformation for each feature component [3, 4]. This transform results in a corresponding transformation of the HMM mean and variance vectors. In order to approximate complex functions, a separate transform is used for different Gaussian clusters <ref> [3] </ref>. The mapping was applied separately to each feature component in order to make the problem mathematically tractable. However, if only the mean vectors are transformed and the variances left untransformed, then a full-matrix affine transformation can be easily estimated [12].
Reference: [4] <author> A. Sankar and C.-H. Lee, </author> <title> "A Maximum-Likelihood Approach to Stochastic Matching for Robust Speech Recognition," </title> <address> IEEE-TSAP, </address> <month> May </month> <year> 1996, </year> <note> to appear. </note>
Reference-contexts: Acoustic adaptation techniques can also be used to bridge the mismatch between the training and testing acoustic environments, and the differences between individual test speakers and the training population. In previous work, maximum-likelihood and Bayesian adaptation techniques have been developed <ref> [3, 4, 5, 6] </ref>, and have given significant improvements in performance for both nonnative speakers [3] and noisy speech [7]. More recently, we have developed new feature and model-space adaptation techniques and applied them to both nonnative and native speakers [8]. <p> ACOUSTIC ADAPTATION Acoustic adaptation can also be used to reduce the mismatch between training and testing acoustic environments. In previous work, maximum-likelihood (ML) and Bayesian adaptation algorithms have been developed <ref> [3, 11, 4, 5, 6] </ref>, and applied to nonnative speech recognition [3] and noisy speech recognition [7]. In particular, maximum-likelihood adaptation has been applied in both the feature-space and model-space [4, 8]. <p> In previous work, maximum-likelihood (ML) and Bayesian adaptation algorithms have been developed [3, 11, 4, 5, 6], and applied to nonnative speech recognition [3] and noisy speech recognition [7]. In particular, maximum-likelihood adaptation has been applied in both the feature-space and model-space <ref> [4, 8] </ref>. In feature-space schemes, the test speech features are transformed to match the trained models, whereas in model-space schemes, the model parameters are transformed to match the test features. In our previous ML adaptation algorithms, we used an affine transformation for each feature component [3, 4]. <p> In feature-space schemes, the test speech features are transformed to match the trained models, whereas in model-space schemes, the model parameters are transformed to match the test features. In our previous ML adaptation algorithms, we used an affine transformation for each feature component <ref> [3, 4] </ref>. This transform results in a corresponding transformation of the HMM mean and variance vectors. In order to approximate complex functions, a separate transform is used for different Gaussian clusters [3]. The mapping was applied separately to each feature component in order to make the problem mathematically tractable. <p> This approach was found to give better results than a component-wise transformation for speaker adaptation because of the modeling of the dependencies between feature components [8]. We have also developed adaptation methods that use transformations of the HMM variances as described in <ref> [4, 11, 8] </ref>. In one feature-space approach, we assume that the test features are obtained by adding a random bias term to the original speech features [4, 11]. The bias is modeled as a Gaussian random variable with mean b and variance 2 b . <p> We have also developed adaptation methods that use transformations of the HMM variances as described in [4, 11, 8]. In one feature-space approach, we assume that the test features are obtained by adding a random bias term to the original speech features <ref> [4, 11] </ref>. The bias is modeled as a Gaussian random variable with mean b and variance 2 b . The speech means and variances are now transformed by adding the mean and variance of the random bias to the HMM means and variances. <p> The speech means and variances are now transformed by adding the mean and variance of the random bias to the HMM means and variances. In a second model-space approach, we have developed a technique for scaling the HMM variances <ref> [4, 8] </ref>. In this case, each component of the variance vector (in a diagonal covariance matrix) is scaled according to 2 x ; (3) where ff is a scale factor to be estimated. <p> In this case, each component of the variance vector (in a diagonal covariance matrix) is scaled according to 2 x ; (3) where ff is a scale factor to be estimated. We have previously reported improvements by using variance transformations for both channel mismatches <ref> [11, 4] </ref> and speaker adaptation [8]. Because of lack of time, we did not use this approach for the H3 evaluations. However, the variance scaling transformation of Equation 3 was used to advantage in the H3 evaluations by other researchers [13].
Reference: [5] <author> J. Gauvain and C.-H. Lee, </author> <title> "Maximum a posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains," </title> <journal> IEEE-TSAP, </journal> <volume> vol. 2, </volume> <pages> pp. 291-298, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Acoustic adaptation techniques can also be used to bridge the mismatch between the training and testing acoustic environments, and the differences between individual test speakers and the training population. In previous work, maximum-likelihood and Bayesian adaptation techniques have been developed <ref> [3, 4, 5, 6] </ref>, and have given significant improvements in performance for both nonnative speakers [3] and noisy speech [7]. More recently, we have developed new feature and model-space adaptation techniques and applied them to both nonnative and native speakers [8]. <p> ACOUSTIC ADAPTATION Acoustic adaptation can also be used to reduce the mismatch between training and testing acoustic environments. In previous work, maximum-likelihood (ML) and Bayesian adaptation algorithms have been developed <ref> [3, 11, 4, 5, 6] </ref>, and applied to nonnative speech recognition [3] and noisy speech recognition [7]. In particular, maximum-likelihood adaptation has been applied in both the feature-space and model-space [4, 8].
Reference: [6] <author> V. Digalakis, D. Rtischev, and L. Neumeyer, </author> <title> "Speaker adaptation using combined transformation and bayesian methods," </title> <note> 1994, accepted for publication in IEEE-TSAP. </note>
Reference-contexts: Acoustic adaptation techniques can also be used to bridge the mismatch between the training and testing acoustic environments, and the differences between individual test speakers and the training population. In previous work, maximum-likelihood and Bayesian adaptation techniques have been developed <ref> [3, 4, 5, 6] </ref>, and have given significant improvements in performance for both nonnative speakers [3] and noisy speech [7]. More recently, we have developed new feature and model-space adaptation techniques and applied them to both nonnative and native speakers [8]. <p> ACOUSTIC ADAPTATION Acoustic adaptation can also be used to reduce the mismatch between training and testing acoustic environments. In previous work, maximum-likelihood (ML) and Bayesian adaptation algorithms have been developed <ref> [3, 11, 4, 5, 6] </ref>, and applied to nonnative speech recognition [3] and noisy speech recognition [7]. In particular, maximum-likelihood adaptation has been applied in both the feature-space and model-space [4, 8].
Reference: [7] <author> L. Neumeyer and M. Weintraub, </author> <title> "Robust Speech Recognition in Noise using Mapping and Adaptation Techniques," </title> <booktitle> in ICASSP, </booktitle> <pages> pp. 141-144, </pages> <year> 1995. </year>
Reference-contexts: In previous work, maximum-likelihood and Bayesian adaptation techniques have been developed [3, 4, 5, 6], and have given significant improvements in performance for both nonnative speakers [3] and noisy speech <ref> [7] </ref>. More recently, we have developed new feature and model-space adaptation techniques and applied them to both nonnative and native speakers [8]. We have also recently developed a novel part-of-speech (POS) tag-based language model. <p> We trained POF filters at various SNRs from -5 dB to 34 dB at 3 dB intervals. During testing, we computed the SNR for a sentence and used the POF filter with the closest SNR. This approach is similar to our previous work <ref> [7] </ref>. However, in [7] the noise used to train the POF filters was matched with the test environment noise. 3.3. MMSE/POF combined approach The above two approaches can be easily combined as follows. First, the noisy speech is processed using the MMSE log-energy estimation algorithm. <p> We trained POF filters at various SNRs from -5 dB to 34 dB at 3 dB intervals. During testing, we computed the SNR for a sentence and used the POF filter with the closest SNR. This approach is similar to our previous work <ref> [7] </ref>. However, in [7] the noise used to train the POF filters was matched with the test environment noise. 3.3. MMSE/POF combined approach The above two approaches can be easily combined as follows. First, the noisy speech is processed using the MMSE log-energy estimation algorithm. <p> ACOUSTIC ADAPTATION Acoustic adaptation can also be used to reduce the mismatch between training and testing acoustic environments. In previous work, maximum-likelihood (ML) and Bayesian adaptation algorithms have been developed [3, 11, 4, 5, 6], and applied to nonnative speech recognition [3] and noisy speech recognition <ref> [7] </ref>. In particular, maximum-likelihood adaptation has been applied in both the feature-space and model-space [4, 8]. In feature-space schemes, the test speech features are transformed to match the trained models, whereas in model-space schemes, the model parameters are transformed to match the test features.
Reference: [8] <author> L. Neumeyer, A. Sankar, and V. Digalakis, </author> <title> "A Comparative Study of Speaker Adaptation Techniques," </title> <booktitle> in EUROSPEECH, </booktitle> <pages> pp. 1127-1130, </pages> <year> 1995. </year>
Reference-contexts: More recently, we have developed new feature and model-space adaptation techniques and applied them to both nonnative and native speakers <ref> [8] </ref>. We have also recently developed a novel part-of-speech (POS) tag-based language model. The idea behind this approach is to tag each word by its part-of-speech and then train a statistical language model using these tags as terminal symbols. <p> In previous work, maximum-likelihood (ML) and Bayesian adaptation algorithms have been developed [3, 11, 4, 5, 6], and applied to nonnative speech recognition [3] and noisy speech recognition [7]. In particular, maximum-likelihood adaptation has been applied in both the feature-space and model-space <ref> [4, 8] </ref>. In feature-space schemes, the test speech features are transformed to match the trained models, whereas in model-space schemes, the model parameters are transformed to match the test features. In our previous ML adaptation algorithms, we used an affine transformation for each feature component [3, 4]. <p> However, if only the mean vectors are transformed and the variances left untransformed, then a full-matrix affine transformation can be easily estimated [12]. This approach was found to give better results than a component-wise transformation for speaker adaptation because of the modeling of the dependencies between feature components <ref> [8] </ref>. We have also developed adaptation methods that use transformations of the HMM variances as described in [4, 11, 8]. In one feature-space approach, we assume that the test features are obtained by adding a random bias term to the original speech features [4, 11]. <p> This approach was found to give better results than a component-wise transformation for speaker adaptation because of the modeling of the dependencies between feature components [8]. We have also developed adaptation methods that use transformations of the HMM variances as described in <ref> [4, 11, 8] </ref>. In one feature-space approach, we assume that the test features are obtained by adding a random bias term to the original speech features [4, 11]. The bias is modeled as a Gaussian random variable with mean b and variance 2 b . <p> The speech means and variances are now transformed by adding the mean and variance of the random bias to the HMM means and variances. In a second model-space approach, we have developed a technique for scaling the HMM variances <ref> [4, 8] </ref>. In this case, each component of the variance vector (in a diagonal covariance matrix) is scaled according to 2 x ; (3) where ff is a scale factor to be estimated. <p> In this case, each component of the variance vector (in a diagonal covariance matrix) is scaled according to 2 x ; (3) where ff is a scale factor to be estimated. We have previously reported improvements by using variance transformations for both channel mismatches [11, 4] and speaker adaptation <ref> [8] </ref>. Because of lack of time, we did not use this approach for the H3 evaluations. However, the variance scaling transformation of Equation 3 was used to advantage in the H3 evaluations by other researchers [13]. <p> However, the variance scaling transformation of Equation 3 was used to advantage in the H3 evaluations by other researchers [13]. We note that the different adaptation techniques described above can be applied in sequence as in <ref> [8] </ref>. In the H3 test, the speaker session boundaries are assumed to be known, and this information could be used by adaptation algorithms. We derived initial hypotheses for each session by generating N-best lists using the baseline acoustic models and bigram language models. <p> We derived initial hypotheses for each session by generating N-best lists using the baseline acoustic models and bigram language models. These lists were rescored using trigram language models to generate the hypotheses that were used for acoustic adaptation. The method we used was a block-diagonal <ref> [8] </ref> matrix affine transformation [12] of the HMM mean vectors. The adapted models were then used to acoustically rescore the N-best lists.
Reference: [9] <author> V. Digalakis, P. Monaco, and H. Murveit, "Genones: </author> <title> Generalized mixture tying in continuous hidden markov model-based speech recognizers," </title> <note> 1994, accepted for publication in IEEE-TSAP. </note>
Reference-contexts: For the H3-C0 test data, we used the same utterances recorded through the Sennheiser channel. The acoustic models we used were continuous-density, genonic HMMs <ref> [9] </ref>. Separate HMMs were trained for males and females using the SI-284 Sennheiser WSJ data. These genonic HMMs have about 1,800 Gaussian mixtures, each with 32 components. For development, we generated a statistical language model using the 60,000 most frequent words in the 1994 NABN text corpus.
Reference: [10] <author> H. Murveit, J. Butzberger, V. Digalakis, and M. Weintraub, </author> <title> "Large-Vocabulary Dictation Using SRI's DECIPHER(TM) Speech Recognition System: </title> <booktitle> Progressive-Search Techniques," in ICASSP, </booktitle> <pages> pp. </pages> <address> II-319-II-322, </address> <year> 1993. </year>
Reference-contexts: In order to facilitate quick experimentation during development, we generated word lattices for the H3-C0 Sennheiser test speech with the genonic HMMs described above using a forward-backward search, a bigram language model, and the word-life algorithm described in <ref> [10] </ref>. These lattices were then used for experimenting with different algorithms for the noisy H3-P0 data. While this gives overly optimistic results for the H3-P0 data, these results can still be used to qualitatively compare the performance of different algorithms. 3.
Reference: [11] <author> A. Sankar and C.-H. Lee, </author> <title> "Stochastic Matching for Robust Speech Recognition," </title> <journal> IEEE Sig. Proc. Letts., </journal> <volume> vol. 1, </volume> <pages> pp. 124-125, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: ACOUSTIC ADAPTATION Acoustic adaptation can also be used to reduce the mismatch between training and testing acoustic environments. In previous work, maximum-likelihood (ML) and Bayesian adaptation algorithms have been developed <ref> [3, 11, 4, 5, 6] </ref>, and applied to nonnative speech recognition [3] and noisy speech recognition [7]. In particular, maximum-likelihood adaptation has been applied in both the feature-space and model-space [4, 8]. <p> This approach was found to give better results than a component-wise transformation for speaker adaptation because of the modeling of the dependencies between feature components [8]. We have also developed adaptation methods that use transformations of the HMM variances as described in <ref> [4, 11, 8] </ref>. In one feature-space approach, we assume that the test features are obtained by adding a random bias term to the original speech features [4, 11]. The bias is modeled as a Gaussian random variable with mean b and variance 2 b . <p> We have also developed adaptation methods that use transformations of the HMM variances as described in [4, 11, 8]. In one feature-space approach, we assume that the test features are obtained by adding a random bias term to the original speech features <ref> [4, 11] </ref>. The bias is modeled as a Gaussian random variable with mean b and variance 2 b . The speech means and variances are now transformed by adding the mean and variance of the random bias to the HMM means and variances. <p> In this case, each component of the variance vector (in a diagonal covariance matrix) is scaled according to 2 x ; (3) where ff is a scale factor to be estimated. We have previously reported improvements by using variance transformations for both channel mismatches <ref> [11, 4] </ref> and speaker adaptation [8]. Because of lack of time, we did not use this approach for the H3 evaluations. However, the variance scaling transformation of Equation 3 was used to advantage in the H3 evaluations by other researchers [13].
Reference: [12] <author> C. J. Legetter and P. C. Woodland, </author> <title> "Flexible Speaker Adaptation using Maximum Likelihood Linear Regression," </title> <booktitle> in Proc. ARPA-SLS Workshop, </booktitle> <pages> pp. 110-115, </pages> <year> 1995. </year>
Reference-contexts: The mapping was applied separately to each feature component in order to make the problem mathematically tractable. However, if only the mean vectors are transformed and the variances left untransformed, then a full-matrix affine transformation can be easily estimated <ref> [12] </ref>. This approach was found to give better results than a component-wise transformation for speaker adaptation because of the modeling of the dependencies between feature components [8]. We have also developed adaptation methods that use transformations of the HMM variances as described in [4, 11, 8]. <p> We derived initial hypotheses for each session by generating N-best lists using the baseline acoustic models and bigram language models. These lists were rescored using trigram language models to generate the hypotheses that were used for acoustic adaptation. The method we used was a block-diagonal [8] matrix affine transformation <ref> [12] </ref> of the HMM mean vectors. The adapted models were then used to acoustically rescore the N-best lists. We also used adapted crossword models in a similar acoustic rescoring pass, the only difference being that for the crossword models we also used an unseen triphone modeling scheme [14].
Reference: [13] <author> P. C. Woodland, M. J. F. Gales, D. Pye, and V. Valtchev, </author> <title> "The HTK Large Vocabulary Recognition System for the 1995 ARPA H3 Task," </title> <booktitle> elsewhere in these proceedings. </booktitle>
Reference-contexts: Because of lack of time, we did not use this approach for the H3 evaluations. However, the variance scaling transformation of Equation 3 was used to advantage in the H3 evaluations by other researchers <ref> [13] </ref>. We note that the different adaptation techniques described above can be applied in sequence as in [8]. In the H3 test, the speaker session boundaries are assumed to be known, and this information could be used by adaptation algorithms. <p> EXPERIMENTS WITH MODEL TRAINING After the conclusion of the benchmarks, we studied a method for improving the recognition performance for noisy speech based on training the HMMs using the alternate microphone SI-284 database. Such an approach was used to advantage by some sites during the evaluations (for example see <ref> [13] </ref>). We would expect this approach to work well if there is a match between the noise during training and testing. However, if there is a significant departure from the training conditions, then the performance will not be good.
Reference: [14] <author> V. Digalakis, M. Weintraub, A. Sankar, H. Franco, L. Neumeyer, and H. Murveit, </author> <title> "Continuous Speech Dictation on ARPA's North American Business News Domain," </title> <booktitle> in Proc. ARPA-SLS Workshop, </booktitle> <pages> pp. 88-93, </pages> <year> 1995. </year>
Reference-contexts: The adapted models were then used to acoustically rescore the N-best lists. We also used adapted crossword models in a similar acoustic rescoring pass, the only difference being that for the crossword models we also used an unseen triphone modeling scheme <ref> [14] </ref>. In order to adapt the crossword models, we used a context-independent (CI) phone loop as the reference for each sentence. This procedure was used mainly because of the lack of time.
Reference: [15] <author> A. Sankar, L. Neumeyer, and M. Weintraub, </author> <title> "An Experimental Study of Acoustic Adaptation Algorithms," </title> <booktitle> in ICASSP, </booktitle> <year> 1996. </year>
Reference-contexts: This procedure was used mainly because of the lack of time. However we have previously found that at operating error rates of about 20%, the performance of the CI phone loop approach is comparable to that of an approach using the first-pass hypotheses for adaptation <ref> [15] </ref>. An advantage of the CI-phone loop approach is that we do not have to do a recognition pass through a large network in order to generate hypotheses before adaptation. 5.
Reference: [16] <author> K. W. Church, </author> <title> "A stochastic parts program and noun phrase parser for unrestricted text," </title> <booktitle> in Second Conference on App. Nat. Lang. Proc., </booktitle> <pages> pp. 136-143, </pages> <year> 1988. </year>
Reference-contexts: For POS tagging we use the well-established probabilistic paradigm that models word sequences as outputs from a hidden Markov process whose states are POS N-grams <ref> [16] </ref>.
Reference: [17] <author> T. R. Niesler and P. C. Woodland, </author> <title> "A variable-length category-based N-gram language model," </title> <booktitle> in ICASSP, </booktitle> <year> 1996. </year>
Reference-contexts: POS-based models typically do not improve on word N-gram models in these tasks since they do not capture local lexical co-occurrence statistics as well (although they can offer good size/performance trade-offs <ref> [17] </ref>). The three models used here (one word-based, two POS-based) represent complementary aspects of language and should be combined using a principled technique such as maximum entropy [18].
Reference: [18] <author> R. Rosenfeld, </author> <title> Adaptive Statistical Language Modeling: </title>
Reference-contexts: The three models used here (one word-based, two POS-based) represent complementary aspects of language and should be combined using a principled technique such as maximum entropy <ref> [18] </ref>. We chose the linear weighting approach as a suboptimal compromise that was less computationally expensive and easy to integrate into our system. 6.
References-found: 18

