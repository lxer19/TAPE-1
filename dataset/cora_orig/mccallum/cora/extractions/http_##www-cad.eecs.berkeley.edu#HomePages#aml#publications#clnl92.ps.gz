URL: http://www-cad.eecs.berkeley.edu/HomePages/aml/publications/clnl92.ps.gz
Refering-URL: http://www-cad.eecs.berkeley.edu/HomePages/aml/publications/index.html
Root-URL: http://www.cs.berkeley.edu
Title: What Can Boolean Networks Learn  
Author: Arlindo L. Oliveira Alberto Sangiovanni-Vincentelli 
Date: June 26, 1992  
Address: Berkeley, Berkeley CA 94720  
Affiliation: Dept. of EECS, UC  
Abstract: We study the generalization abilities of networks that are composed of boolean nodes, i.e., nodes that implement only basic boolean functions: and, or and not. The majority of the network learning algorithms proposed so far generate networks where each node implements a threshold function and are inappropriate for the generation of boolean networks from training set data. We propose an algorithm that, given a training set, generates a boolean network of small complexity that is compatible with the training set. The algorithm, inspired in techniques used in the logic synthesis community for the design of VLSI circuits, generates both the connectivity pattern and the architecture of the network. Furthermore, the resulting network can be implemented in silicon in a straightforward way. Experimental results obtained in a set of problems from the machine learning literature show that the generalization performed by boolean networks synthesized with this algorithm compares favorably with the generalization obtained by alternative learning algorithms. Some of these results and examples of the layout of networks obtained using the algorithm are presented and discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Haussler A. Blumer, A. Ehrenfeucht and M. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: For example, the expression derived in <ref> [1] </ref>, m &gt; 1 * (ln (r) + ln ( 1 ffi )) and the best bounds presented in [7] for r lead to a minimum number of 1360 examples to PAC-learn the Xor8 concept with * = ffi = 0:1. Table 1 lists the results obtained.
Reference: [2] <author> K. A. Bartlett, D. G. Bostick, G. D. Hachtel, R. M. Jacoby, M. R. Lightner, P. H. Mo-ceyunas, C. R. Morrison, and D. Ravenscroft. BOLD: </author> <title> A multi-level logic optimization system. </title> <booktitle> In IEEE International Conference on Computer-Aided Design, </booktitle> <year> 1987. </year>
Reference-contexts: Multi-level synthesis algorithms <ref> [6, 2] </ref> are even less efficient and much worst at using the extra degrees of freedom allowed by missing input combinations. 2 Definitions Let B = f0; 1g. A completely specified Boolean function, f , is a mapping from B n ! B.
Reference: [3] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. </author> <title> Classifying learnable geometric concepts with the vapnik-chervonenkis dimension. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 273-282, </pages> <note> Salem, 1986. ACM. </note>
Reference-contexts: More specifically, assume the concept to be learned corresponds to a boolean function in some set of functions, S N , and that S 1 ; S 2 :::S N1 can be defined obeying S 1 S 2 :::S N1 S N . Theoretical results <ref> [3] </ref> have shown that if the learner outputs one function f T that is compatible with the training set, then the generalization accuracy is a decreasing function of the VC-dimension of S k where k is the smallest integer such that f T 2 S k 2 .
Reference: [4] <author> R. Brayton, G. Hachtel, C. McMullen, and A. Sangiovanni-Vincentelli. </author> <title> Logic Minimization Algorithms for VLSI Synthesis. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1984. </year>
Reference-contexts: Regrettably, this is not the case. Classical logic synthesis algorithms can be divided in two types: synthesis of two-level networks and synthesis of multi-level networks. Many algorithms for the synthesis of minimal two-level networks (i.e., and-or networks) have been proposed <ref> [4, 10] </ref> and they could, in principle, be used to generate minimal networks given the data in the training set. <p> The two-level minimization required for this step obtains a DNF expression for f with few cubes and is performed by the algorithm described in [12], although any two-level minimizer like ESPRESSO <ref> [4] </ref> could, in principle, be used. <p> This problem is NP-complete [9] and we need to solve an instance of high dimension. Therefore, an exact solution is hopelessly costly, in most cases. Several algorithms have been proposed for obtaining a good solution for this problem. For example, the covering algorithm used in ESPRESSO <ref> [4] </ref> is able to find an approximate solution for fairly large sized problems in reasonable time. However, one of the steps of this algorithm is to find a maximal independent set of rows and use this set to compute a lower bound in the size of V .
Reference: [5] <author> R. K. Brayton and Curt McMullen. </author> <title> The decomposition and factorization of boolean expressions. </title> <booktitle> In Proceedings of International Symposium in Circuits and Systems, </booktitle> <pages> pages 49-54. </pages> <address> (Rome), </address> <year> 1982. </year>
Reference-contexts: The maximal cube divisors are given by the conjunction of the literals in the columns that correspond to maximal rectangles, i.e., rectangles that are not properly contained in any other rectangle. This concept is closely related with the concept of a co-kernel, introduced in <ref> [5] </ref>, Define f = (f OF F ; f ON ; f DC ) and let f = (f ON ; f OF F ; f DC ) be the function we want to reexpress as g (h (::)).. The cubes that we select as candidates are: 1.
Reference: [6] <author> R. K. Brayton, R. L. Rudell, A. L. Sangiovanni-Vincentelli, and A. R. R. Wang. </author> <title> MIS: A multiple-level logic optimization system. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <volume> CAD-6(6):1062-1081, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: Multi-level synthesis algorithms <ref> [6, 2] </ref> are even less efficient and much worst at using the extra degrees of freedom allowed by missing input combinations. 2 Definitions Let B = f0; 1g. A completely specified Boolean function, f , is a mapping from B n ! B.
Reference: [7] <author> Paul E. Dunne. </author> <title> The Complexity of Boolean Networks. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1988. </year>
Reference-contexts: For example, the expression derived in [1], m &gt; 1 * (ln (r) + ln ( 1 ffi )) and the best bounds presented in <ref> [7] </ref> for r lead to a minimum number of 1360 examples to PAC-learn the Xor8 concept with * = ffi = 0:1. Table 1 lists the results obtained.
Reference: [8] <author> S.E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 524-532, </pages> <address> San Mateo, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Discrete multi-valued attributes were encoded using a one-hot encoding. The performance of the algorithm (MIFES) was compared with the performance of two alternative machine learning algorithms: a reimplementation of ID3 [14] and the cascade-correlation algorithm <ref> [8] </ref>. For each problem, a set of examples was created and 5 independent runs were performed.
Reference: [9] <author> M.R. Garey and D.S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1979. </year> <month> 10 </month>
Reference-contexts: Element M ij of M is 1 if the jth cube covers the ith element of O. We want to select a set of columns of minimal cardinality such that every row has at least a 1 in one of the selected columns. This problem is NP-complete <ref> [9] </ref> and we need to solve an instance of high dimension. Therefore, an exact solution is hopelessly costly, in most cases. Several algorithms have been proposed for obtaining a good solution for this problem.
Reference: [10] <author> S. Hong, R. Cain, and D. Ostapko. </author> <title> Mini: A heuristic approach for logic minimization. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 18(5), </volume> <year> 1974. </year>
Reference-contexts: Regrettably, this is not the case. Classical logic synthesis algorithms can be divided in two types: synthesis of two-level networks and synthesis of multi-level networks. Many algorithms for the synthesis of minimal two-level networks (i.e., and-or networks) have been proposed <ref> [4, 10] </ref> and they could, in principle, be used to generate minimal networks given the data in the training set.
Reference: [11] <author> P. M. Murphy and D. W. Aha. </author> <title> Repository of Machine Learning Databases Machine readable data repository. </title> <institution> University of California, Irvine. </institution>
Reference-contexts: Xor8 is the exclusive or of 8 inputs. The next three concepts accept a compact multi-level representation (as do the previous two) and are listed in the appendix. Finally, the last three have been selected from the UCI machine learning database <ref> [11] </ref> and they satisfy the following criterion: two class classification with discrete valued input attributes. Discrete multi-valued attributes were encoded using a one-hot encoding.
Reference: [12] <author> A. L. Oliveira and A. Sangiovanni-Vincentelli. </author> <title> Lsat an algorithm for the synthesis of two level threshold gate networks. </title> <booktitle> In Proceedings of ICCAD-91, </booktitle> <pages> pages 130-133. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Cubes that are maximal cube divisors of E f where E f is an expression for a function compatible with f obtained by two-level minimization. The two-level minimization required for this step obtains a DNF expression for f with few cubes and is performed by the algorithm described in <ref> [12] </ref>, although any two-level minimizer like ESPRESSO [4] could, in principle, be used. However, the algorithm described in [12] was developed specifically for two-level minimization in situations where the function can be described by two sets of minterms and is much more efficient than general purpose two-level minimizers. 3.3 Solving the <p> The two-level minimization required for this step obtains a DNF expression for f with few cubes and is performed by the algorithm described in <ref> [12] </ref>, although any two-level minimizer like ESPRESSO [4] could, in principle, be used. However, the algorithm described in [12] was developed specifically for two-level minimization in situations where the function can be described by two sets of minterms and is much more efficient than general purpose two-level minimizers. 3.3 Solving the set covering problem Selecting a set V W of minimal cardinality that covers O is a difficult problem.
Reference: [13] <author> A. L. Oliveira and A. Sangiovanni-Vincentelli. </author> <title> Constructive induction using a non-greedy strategy for feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference in Machine Learning, </booktitle> <address> San Mateo, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: on y i or y i . y i is a fanout node of y j if there exists a directed edge e ji and a fanin node if there is a directed edge e ij . 3 The synthesis algorithm The algorithm presented here uses some ideas presented in <ref> [13] </ref> but uses a very different approach for hidden node selection. The approach proposed incrementally selects a set of hidden unit functions such that the implementation of a function compatible with f over the new variables defined by these functions is simpler.
Reference: [14] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Discrete multi-valued attributes were encoded using a one-hot encoding. The performance of the algorithm (MIFES) was compared with the performance of two alternative machine learning algorithms: a reimplementation of ID3 <ref> [14] </ref> and the cascade-correlation algorithm [8]. For each problem, a set of examples was created and 5 independent runs were performed.
Reference: [15] <author> J. Rissanen. </author> <title> Stochastic complexity and mideling. </title> <journal> Annals of Statistics, </journal> <volume> 14 </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: A similar argument for the preference for networks with few literals could be made using the Minimal Description Length Principle (MDLP) of Rissanen <ref> [15] </ref>. Given that the objective is to generate a boolean network with as few literals as possible, it would seem that general purpose logic synthesis techniques could be used to synthesize a compact network given the data in the training set. Regrettably, this is not the case.
Reference: [16] <author> J. Wogulis W. Iba and P. Langley. </author> <title> Trading off simplicity and coverage in incremental concept learning. </title> <booktitle> In Proceedings of the Fifth International Conference in Machine Learning, </booktitle> <pages> pages 73-79, </pages> <address> San Mateo, 1992. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 11 </pages>
Reference-contexts: The same training data and validation procedure was used for each of the methods. The size of the training sets used to learn each of the concepts was fixed either according to a simple 7 empirical rule or set equal to values previously used in other experiments <ref> [16] </ref>.
References-found: 16

