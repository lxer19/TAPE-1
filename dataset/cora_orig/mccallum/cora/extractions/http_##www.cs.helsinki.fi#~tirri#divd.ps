URL: http://www.cs.helsinki.fi/~tirri/divd.ps
Refering-URL: http://www.cs.Helsinki.FI/research/cosco/Projects/NONE/
Root-URL: 
Phone: 26,  
Title: Bayesian Finite Mixtures for Nonlinear Modeling of Educational data  
Author: Henry Tirri and Tomi Silander 
Address: P.O.Box  FIN-00014 University of Helsinki, Finland Kirsi Tirri P.O. Box 38,Department  Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  University of  
Date: March 1997)  
Note: A paper presented at the 1997 Annual Meeting of the American Educational Research Association (Chicago, IL, USA,  of Teacher Education FIN-00014  
Abstract: In this paper we discuss a Bayesian approach for finding latent classes in the data. In our approach we use finite mixture models to describe the underlying structure in the data, and demonstrate that the possibility to use full joint probability models raises interesting new prospects for exploratory data analysis. The concepts and methods discussed are illustrated with a case study using a data set from a recent educational study. The Bayesian classification approach described has been implemented, and presents an appealing addition to the standard toolbox for exploratory data analysis of educational data.
Abstract-found: 1
Intro-found: 1
Reference: <author> Basilevsky, A. </author> <year> (1994). </year> <title> Statistical Factor Analysis and Related Methods. Theory and Applications. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: It will be also useful to talk about a set of models, which we will call a model family M. Examples of model families include the set of linear functions <ref> (Basilevsky, 1994) </ref>, or the set of graphical structures describing independence assumptions (Heckerman et al., 1995). For the classification problem, a model fi simply means a description of the classes in terms of the joint probability distribution of X 1 ; : : : ; X m .
Reference: <author> Bernardo, J. and Smith, A. </author> <year> (1994). </year> <title> Bayesian theory. </title> <publisher> John Wiley. </publisher>
Reference-contexts: Conversely, models with little structure do not predict the given data or new data well. The real question is to find an appropriate balance between these two aspects. Bayesian theory <ref> (Bernardo and Smith, 1994) </ref> (together with its information theoretic interpretation (Rissanen, 1989; Wallace and Freeman, 1987)) explicitly trades model complexity, as determined by prior probabilities, against the 4 fit to the data. This trade-off is in fact a direct consequence of Bayes' theorem discussed below. <p> Searching for the most probable model means finding the model fi that maximizes the probability p (fi i jD), which is called the posterior probability. The prior, likelihood and posterior are connected via the Bayes' theorem (see e.g., <ref> (Bernardo and Smith, 1994) </ref>: p (fi j D) = p (D) Taking the negative logarithm of this expression turns the products into sums, and gives us log p (fijD) = log p (Djfi) log p (fi) + constant: (2) Since we are only interested in the relative probability of the different <p> Since our estimation of the network parameters will be Bayesian <ref> (Bernardo and Smith, 1994) </ref> we need to fix the prior distributions for the parameters. <p> Modeling the full joint distribution gives us an extremely powerful exploratory tool. Explorations can be done in the setting, where we study the variable predictive distributions <ref> (Bernardo and Smith, 1994) </ref> of a new (actual or imaginary) data vector. Here we only want to briefly address some interesting question types that can be answered by such a tool: * Variable distributions for a given explaining variable assignment.
Reference: <author> Chatfield, C. a. A. C. </author> <year> (1980). </year> <title> Introduction to Multivariate Analysis. </title> <publisher> Chapman and Hall, </publisher> <address> New York. </address>
Reference-contexts: This is partly due to the controversial nature of the latent variable approaches as practiced in the applied end of the spectrum, exploratory factor analysis being a prime example of the continuing debates on the validity and arbitrariness of the method (see e.g., the discussion in <ref> (Chatfield, 1980) </ref>). On the other hand recent years have seen an impressive growth of interest in building complex latent variable models of natural phenomena and manmade systems.
Reference: <author> Cheeseman, P. and Stutz, J. </author> <year> (1996). </year> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In Fayyad, U., Piatetsky-Shapiro, G., Smyth, P., and Uthurusamy, R., editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 6. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park. </address>
Reference-contexts: An interested reader can find more formal treatment of the general ideas discussed above in the seminal works by Rissanen (Rissanen, 1987; Rissanen, 1989) and Wallace et al. (Wallace and Boulton, 1968; Wallace and Freeman, 1987); the Bayesian classification is addressed in <ref> (Cheeseman and Stutz, 1996) </ref>. 4 Model family: finite mixtures Like any other Bayesian inference, Bayesian classification is always relative to a model family M.
Reference: <author> Cover, T. and Thomas, J. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY. </address>
Reference-contexts: Now the connection between 6 Bayesian probability theory and the coding approach becomes clear: from in-formation theory we know that log p ( ~ d i ) is the theoretically optimal minimum message length to encode a particular data vector ~ d i <ref> (Cover and Thomas, 1991) </ref>. The minimum message length in (2) is the sum of two terms. The first term is the information to describe the model fi, which is greater for more complex, and thus less probable, models. <p> Bayesian finite mixture approach the corresponding notion would be the Kullback-Leibler distance of the unconditional and conditional marginal likelihood of X i , i.e., D KL (p (X i jY = k; fi); p (X i jfi)); where D KL (p; q) is the relative entropy between p and q <ref> (Cover and Thomas, 1991) </ref>.
Reference: <author> Everitt, B. and Hand, D. </author> <year> (1981). </year> <title> Finite Mixture Distributions. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Gilks, W. R., Richardson, S., and J., S. D. </author> <year> (1996). </year> <title> Markov chain Monte Carlo in practice. </title> <publisher> Chapman & Hall, </publisher> <address> London, GB. </address>
Reference-contexts: In particular, the developments in building latent variable models expressed with graphical structures such as Bayesian networks (Heckerman, 1996; Lauritzen, 1996) and in Bayesian analysis using Markov Chain Monte Carlo methods <ref> (Gilks et al., 1996) </ref> have completely changed the level of complexity that can be addressed in modeling of data. There is no reason to doubt that Bayesian latent variable approaches with nonlinear models will have a profound impact on modeling of social phenomena also.
Reference: <author> Heckerman, D. </author> <year> (1996). </year> <title> A tutorial on learning with bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, Advanced Technology Division, One Microsoft Way, </institution> <address> Redmond, WA 98052. </address>
Reference: <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <year> (1995). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243. </pages>
Reference-contexts: It will be also useful to talk about a set of models, which we will call a model family M. Examples of model families include the set of linear functions (Basilevsky, 1994), or the set of graphical structures describing independence assumptions <ref> (Heckerman et al., 1995) </ref>. For the classification problem, a model fi simply means a description of the classes in terms of the joint probability distribution of X 1 ; : : : ; X m .
Reference: <author> Klecka, W. </author> <year> (1981). </year> <title> Discriminant analysis. </title> <publisher> Sage Publications, </publisher> <address> Beverly Hills, CA. </address>
Reference: <author> Kontkanen, P., Myllymaki, P., and Tirri, H. </author> <year> (1996a). </year> <title> Comparing Bayesian model class selection criteria by discrete finite mixtures. </title> <editor> In Dowe, D., Korb, K., and Oliver, J., editors, </editor> <booktitle> Information, Statistics and Induction in Science, </booktitle> <pages> pages 364-374, </pages> <booktitle> Proceedings of the ISIS'96 Conference, </booktitle> <address> Mel-bourne, Australia. </address> <publisher> World Scientific, Singapore. </publisher>
Reference: <author> Kontkanen, P., Myllymaki, P., and Tirri, H. </author> <year> (1996b). </year> <title> Predictive data mining with finite mixtures. </title> <editor> In Simoudis, E., Han, J., and Fayyad, U., editors, </editor> <booktitle> Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 176-182, </pages> <address> Portland, Oregon. </address>
Reference: <author> Kontkanen, P., Myllymaki, P., and Tirri, H. </author> <year> (1997). </year> <title> Experimenting with the Cheeseman-Stutz evidence approximation for predictive modeling and data mining. </title> <booktitle> In Proceedings of Tenth International FLAIRS Conference (to appear), </booktitle> <address> Daytona Beach, Florida. </address>
Reference: <author> Lauritzen, S. </author> <year> (1996). </year> <title> Graphical Models. </title> <publisher> Oxford University Press. </publisher>
Reference: <author> Niemi, H. and Tirri, K. </author> <year> (1996). </year> <title> Effectiveness of teacher education. New challenges and approaches to evaluation. </title> <type> Technical Report A 6/1996, </type> <institution> Department of Teacher Education in Tampere University. </institution>
Reference-contexts: The objective of the project was to evaluate the effectiveness of Finnish teacher education at various levels from individual to international teacher education policy. A more detailed description of the framework and research conducted in the project is discussed in <ref> (Niemi and Tirri, 1996) </ref>. The data adopted to this study was gathered to investigate how well the Finnish teacher education had been able to achieve the goals set to it. The goals were selected from school-law, programs of teacher education and other documents describing teachers' work at school.
Reference: <author> Niemi, H. and Tirri, K. </author> <year> (1997). </year> <title> Readiness for teaching profession evaluated by teachers and teacher educators. </title> <publisher> In Press. </publisher>
Reference-contexts: The evaluation instrument consisted of 41 behavior statements (and information about the teacher education department), and used a Likert scale from 1 to 5 for the assertions. The results of this evaluation study are reported in the forthcoming study <ref> (Niemi and Tirri, 1997) </ref>. The data sample used for our comparison is derived from the teachers' data in the study described above. This data consist of ratings of 204 Finnish teachers. <p> We will now proceed and illustrate the Bayesian approach described with a case study using the Effectiveness data set. The standard factor analysis results for this same data set are reported in <ref> (Niemi and Tirri, 1997) </ref>. General explorative analysis As described in Section 5 the methods estimating the domain joint probability distribution can be used in exploring much more complex dependency patterns than simple covariances.
Reference: <author> Rissanen, J. </author> <year> (1987). </year> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 49(3) </volume> <pages> 223-239 and 252-265. </pages>
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <address> New Jersey. </address>
Reference: <author> Titterington, D., Smith, A., and Makov, U. </author> <year> (1985). </year> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: For the classification problema very natural model family is the set of discrete finite mixtures ((Everitt and Hand, 1981), <ref> (Titterington et al., 1985) </ref>), where the joint domain probability distribution is approximated as a weighted sum of mixture distributions. <p> Most commonly used component functions in the literature are the univariate normal distributions (see e.g., <ref> (Titterington et al., 1985) </ref>). In educational domains the variables are usually discrete, thus we can drop the assumption of the form of the distribution.
Reference: <author> Wallace, C. and Boulton, D. </author> <year> (1968). </year> <title> An information measure for classifiation. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194. </pages>
Reference: <author> Wallace, C. and Freeman, P. </author> <year> (1987). </year> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 49(3) </volume> <pages> 240-265. 18 </pages>
References-found: 21

