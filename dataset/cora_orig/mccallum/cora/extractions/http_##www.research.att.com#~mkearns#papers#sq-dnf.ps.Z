URL: http://www.research.att.com/~mkearns/papers/sq-dnf.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Title: Weakly Learning DNF and Characterizing Statistical Query Learning Using Fourier Analysis  
Author: Avrim Blum Merrick Furst Jeffrey Jackson Michael Kearns Yishay Mansour Steven Rudich 
Address: Room 2A-423, 600 Mountain Avenue, P.O. Box 636, Murray Hill, NJ 07974.  
Note: Contact author. Address: AT&T Bell  Electronic mail: mkearns@research.att.com This research was supported in part by The Israel Science Foundation administered by The Israel Academy of Science and Humanities and by a grant of the Israeli Ministry of Science and Technology.  
Affiliation: Carnegie Mellon University  Carnegie Mellon University  Carnegie Mellon University  AT&T Bell Laboratories  Tel-Aviv University  Carnegie Mellon University  Laboratories,  
Abstract: We present new results, both positive and negative, on the well-studied problem of learning disjunctive normal form (DNF) expressions. We first prove that an algorithm due to Kushilevitz and Mansour [16] can be used to weakly learn DNF using membership queries in polynomial time, with respect to the uniform distribution on the inputs. This is the first positive result for learning unrestricted DNF expressions in polynomial time in any nontrivial formal model of learning. It provides a sharp contrast with the results of Kharitonov [15], who proved that AC 0 is not efficiently learnable in the same model (given certain plausible cryptographic assumptions). We also present efficient learning algorithms in various models for the read-k and SAT-k subclasses of DNF. For our negative results, we turn our attention to the recently introduced statistical query model of learning [11]. This model is a restricted version of the popular Probably Approximately Correct (PAC) model [23], and practically every class known to be efficiently learnable in the PAC model is in fact learnable in the statistical query model [11]. Here we give a general characterization of the complexity of statistical query learning in terms of the number of uncorrelated functions in the concept class. This is a distribution-dependent quantity yielding upper and lower bounds on the number of statistical queries required for learning on any input distribution. As a corollary, we obtain that DNF expressions and decision trees are not even weakly learnable with fl This research is sponsored in part by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. Support also is sponsored by the National Science Foundation under Grant No. CC-9119319. Blum also supported in part by NSF National Young Investigator grant CCR-9357793. Views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of Wright Laboratory or the United States Government, or NSF. respect to the uniform input distribution in polynomial time in the statistical query model. This result is information-theoretic and therefore does not rely on any unproven assumptions. It demonstrates that no simple modification of the existing algorithms in the computational learning theory literature for learning various restricted forms of DNF and decision trees from passive random examples (and also several algorithms proposed in the experimental machine learning communities, such as the ID3 algorithm for decision trees [22] and its variants) will solve the general problem. The unifying tool for all of our results is the Fourier analysis of a finite class of boolean functions on the hypercube. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Howard Aizenstein, Lisa Hellerstein, and Leonard Pitt. </author> <title> Read-thrice DNF is hard to learn with membership and equivalence queries. </title> <booktitle> In Proceedings of the 33rd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 523-532, </pages> <year> 1992. </year>
Reference-contexts: Due to the dearth of positive results for unrestricted DNF expressions, various restricted DNF classes have attracted considerable attention in the literature <ref> [4, 2, 10, 3, 1, 5, 17, 8] </ref>. Here we extend some of these results. <p> We formalize this as follows: the learning algorithm is given access to a statistics oracle. A query to this oracle is a pair (g; t ), where g is a function g : f0; 1g n fi f+1; 1g ! f+1; 1g, and t 2 <ref> [0; 1] </ref> is a real number called the tolerance of the query.
Reference: [2] <author> Howard Aizenstein and Leonard Pitt. </author> <title> Exact learning of read-twice DNF formulas. </title> <booktitle> In Proceedings of the 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 170-179, </pages> <year> 1991. </year>
Reference-contexts: Due to the dearth of positive results for unrestricted DNF expressions, various restricted DNF classes have attracted considerable attention in the literature <ref> [4, 2, 10, 3, 1, 5, 17, 8] </ref>. Here we extend some of these results. <p> Here we extend some of these results. In particular, it is known that the class of read-k DNF (that is, DNF ex-pressions in which every variable appears at most k times) is learnable in polynomial time in the distribution-independent PAC model using membership queries for k 2 <ref> [2, 10] </ref>, but is as hard to learn in this same model as unrestricted DNF for k 3 [10].
Reference: [3] <author> Howard Aizenstein and Leonard Pitt. </author> <title> Exact learning of read-k disjoint DNF and not-so-disjoint DNF. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 71-76, </pages> <year> 1992. </year>
Reference-contexts: Due to the dearth of positive results for unrestricted DNF expressions, various restricted DNF classes have attracted considerable attention in the literature <ref> [4, 2, 10, 3, 1, 5, 17, 8] </ref>. Here we extend some of these results. <p> Aizenstein and Pitt <ref> [3] </ref> have shown that read-k, SAT` DNF (that is, DNF expressions which are both read-k and such that at most ` terms are satisfied by any input) can be efficiently learned in the distribution-independent PAC model using membership queries.
Reference: [4] <author> Dana Angluin, Michael Frazier, and Leonard Pitt. </author> <title> Learning conjunctions of Horn clauses. </title> <booktitle> In Proceedings of the 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 186-192, </pages> <year> 1990. </year>
Reference-contexts: Due to the dearth of positive results for unrestricted DNF expressions, various restricted DNF classes have attracted considerable attention in the literature <ref> [4, 2, 10, 3, 1, 5, 17, 8] </ref>. Here we extend some of these results.
Reference: [5] <author> Avrim Blum and Steven Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-389, </pages> <year> 1992. </year>
Reference-contexts: Due to the dearth of positive results for unrestricted DNF expressions, various restricted DNF classes have attracted considerable attention in the literature <ref> [4, 2, 10, 3, 1, 5, 17, 8] </ref>. Here we extend some of these results.
Reference: [6] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haus-sler, and Manfred Warumth. </author> <title> Learnability and the Vapnik-Chervonenkis Dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Note that unlike the well-known Vapnik-Chervonenkis (VC) dimension, which is a distribution-independent quantity and is known to characterize the number of random examples required to learn in the distribution-independent PAC model <ref> [6] </ref>, the statistical query dimension is a distribution-dependent quantity. It is possible to prove a one-sided polynomial relationship between the two quantities: namely, if F is a class of VC dimension d, then there exists a distribution D such that SQ-DIM (F ; D) = (d) 4 .
Reference: [7] <author> Bruck, J. </author> <title> Harmonic Analysis of Polynomial Threshold Functions. </title> <journal> SIAM Journal of Discrete Mathematics, </journal> <volume> 5, </volume> <pages> pp. 168-177, </pages> <year> 1990. </year>
Reference-contexts: We show that c P T 1 is weakly learnable with respect to uniform using queries. c P T 1 is a rather general class containing many functions, such as majority, that are not approximable by AC 0 circuits. Our weak learnability proof builds on the work of Bruck <ref> [7] </ref>. Theorem 11 c P T 1 is weakly learnable using membership queries with respect to the uniform distribution.
Reference: [8] <author> Nader H. Bshouty. </author> <title> Exact learning via the monotone theory. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 302-311, </pages> <year> 1993. </year>
Reference-contexts: Due to the dearth of positive results for unrestricted DNF expressions, various restricted DNF classes have attracted considerable attention in the literature <ref> [4, 2, 10, 3, 1, 5, 17, 8] </ref>. Here we extend some of these results.
Reference: [9] <author> Merrick L. Furst, Jeffrey C. Jackson, and Sean W. Smith. </author> <title> Improved learning of AC 0 functions. </title> <booktitle> In Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 317-325, </pages> <year> 1991. </year>
Reference-contexts: In order to prove the theorem, we will need to use an extension of the Fourier theory to an arbitrary distribution; this extension has been examined in the computational learning theory literature before by Furst, Jackson and Smith <ref> [9] </ref>. Thus let D be an arbitrary probability distri bution over f0; 1g n .
Reference: [10] <author> Thomas R. Hancock. </author> <title> Learning 2DNF formulas and k decision trees. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 199-209, </pages> <year> 1991. </year>
Reference-contexts: Due to the dearth of positive results for unrestricted DNF expressions, various restricted DNF classes have attracted considerable attention in the literature <ref> [4, 2, 10, 3, 1, 5, 17, 8] </ref>. Here we extend some of these results. <p> Here we extend some of these results. In particular, it is known that the class of read-k DNF (that is, DNF ex-pressions in which every variable appears at most k times) is learnable in polynomial time in the distribution-independent PAC model using membership queries for k 2 <ref> [2, 10] </ref>, but is as hard to learn in this same model as unrestricted DNF for k 3 [10]. <p> of read-k DNF (that is, DNF ex-pressions in which every variable appears at most k times) is learnable in polynomial time in the distribution-independent PAC model using membership queries for k 2 [2, 10], but is as hard to learn in this same model as unrestricted DNF for k 3 <ref> [10] </ref>. Aizenstein and Pitt [3] have shown that read-k, SAT` DNF (that is, DNF expressions which are both read-k and such that at most ` terms are satisfied by any input) can be efficiently learned in the distribution-independent PAC model using membership queries.
Reference: [11] <author> Michael J. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <year> 1993. </year>
Reference-contexts: We show that c P T 1 is weakly learnable with respect to uniform using membership queries. In the second part of the paper, we examine the DNF learning problem in the recently introduced statistical query model of learning <ref> [11] </ref>. This is a restricted version of the PAC model in which the learning algorithm does not actually receive labeled examples of the unknown target function drawn with respect to the input distribution. <p> An important feature of this model is that any class efficiently learnable from statistical queries (for all distributions or special distributions, respectively) is efficiently learnable in the PAC model with an arbitrarily high rate of classification noise (for all distributions or special distributions, respectively) <ref> [11] </ref>. Furthermore, it has been demonstrated [11] that practically every class known to be efficiently learnable in the PAC model (either for all distributions or special distributions) is also efficiently learnable in the statistical query model (and thus is efficiently PAC learnable with classification noise). <p> An important feature of this model is that any class efficiently learnable from statistical queries (for all distributions or special distributions, respectively) is efficiently learnable in the PAC model with an arbitrarily high rate of classification noise (for all distributions or special distributions, respectively) <ref> [11] </ref>. Furthermore, it has been demonstrated [11] that practically every class known to be efficiently learnable in the PAC model (either for all distributions or special distributions) is also efficiently learnable in the statistical query model (and thus is efficiently PAC learnable with classification noise). <p> In other words, PAC model algorithms almost always learn by estimating probabilities. (A notable exception to this is the class of parity functions, which is known to be efficiently learnable in the PAC model but is not efficiently learnable in the statistical query model <ref> [11] </ref>.) Consequently, any light we can shed on the problem of learning DNF expressions in the statistical query model provides insight into the still unresolved problem of learning DNF in the basic PAC model. <p> n, s, and 1=ffi, then F is weakly learnable using membership queries with respect to the uniform distribution, or alternatively, is efficiently 1=2 1=p (n; s)-approximated. 2.2 The Statistical Query Learning Model Unlike the membership query models of learning we have defined so far, in the statistical query learning model <ref> [11] </ref> the learner is not explicitly allowed to see labeled examples 1 Unless subscripted by a distribution D, all probabilities and expectations are taken with respect to the uniform distribution on f0; 1g n . (~x; f (~x)) of the target concept, but instead may only esti-mate probabilities involving labeled examples. <p> Thus, A must run in polynomial time, and is allowed to make only efficiently computable queries with inverse polynomial tolerance. The motivation for this notion of efficiency is that every class learnable from statistical queries is efficiently learnable in the PAC model by a straightforward simulation argument <ref> [11] </ref>, and thus the statistical query model can be regarded as a natural restriction on the type of computations a PAC model algorithm can perform. The general motivation for the statistical query model can be found in the paper of Kearns [11]. <p> in the PAC model by a straightforward simulation argument <ref> [11] </ref>, and thus the statistical query model can be regarded as a natural restriction on the type of computations a PAC model algorithm can perform. The general motivation for the statistical query model can be found in the paper of Kearns [11]. <p> and statistical query dimensions, implies that for any class of VC dimension d, there is a distribution on which d 1=3 =2 statistical queries of toler ance 1=d 1=3 must be made for PAC learning (this bound is incomparable to a similar lower bound given in the paper of Kearns <ref> [11] </ref>). However, as we have already noted, dramatically stronger lower bounds for statistical query learning may hold, even for natural input distributions. For instance, as promised, we have the following corollary. 4 To see this, let D be uniform on a shattered set S of size 2 blog dc d.
Reference: [12] <author> Michael Kearns, Ming Li, Lenny Pitt and Leslie G. Valiant. </author> <title> On the learnability of Boolean formulae. </title> <booktitle> In Proceedings of the 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 285-295, </pages> <year> 1987. </year>
Reference-contexts: Indeed, in the distribution-independent PAC model there are representation-dependent hardness results (that is, hardness results that assume certain syntactic restrictions on the learning algorithm's hypothesis) for learning even some rather restricted forms of DNF <ref> [21, 12] </ref>. These hardness results left unresolved the status of learning DNF formulas in the absence of hypothesis restrictions, or with respect to the uniform distribution, or using membership queries.
Reference: [13] <author> Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <booktitle> In Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 341-352, </pages> <year> 1992. </year>
Reference-contexts: Now considering the distribution over all inputs ~x as well as h's random choices, we get Pr [h 6= f ] 2 2 (Lemma 3) A similar but slightly weaker randomized approximation method was given by Kearns, Schapire, and Sellie <ref> [13] </ref>. Putting the results of this section together, we have the fol lowing.
Reference: [14] <author> Roni Khardon. </author> <title> On using the Fourier transform to learn disjoint DNF. </title> <type> Unpublished manuscript, </type> <month> Septem-ber </month> <year> 1993. </year>
Reference-contexts: (Theorem 9) By restricting the size of terms in the SAT-k DNF's considered and using exact reconstruction and derandomization techniques similar to those of Kushilevitz and Mansour [16], we can extend the above to a deterministic, distribution-independent learning result (this generalizes a similar result for SAT-1 (disjoint) DNF by Khardon <ref> [14] </ref>).
Reference: [15] <author> Michael Kharitonov. </author> <title> Cryptographic hardness of distribution-specific learning. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 372-381, </pages> <year> 1993. </year>
Reference-contexts: Thus, we give the first positive result for learning unrestricted DNF in a nontrivial model of learning. This result provides a sharp contrast between DNF formulas and the more general class of AC 0 circuits, which Kharitonov <ref> [15] </ref> proved is not learnable in this same model under certain cryptographic assumptions. Due to the dearth of positive results for unrestricted DNF expressions, various restricted DNF classes have attracted considerable attention in the literature [4, 2, 10, 3, 1, 5, 17, 8]. Here we extend some of these results.
Reference: [16] <author> Eyal Kushilevitz and Yishay Mansour. </author> <title> Learning decision trees using the Fourier spectrum. </title> <journal> SIAM Journal on Computing 22(6) </journal> <pages> 1331-1348, </pages> <month> December </month> <year> 1993. </year> <booktitle> (Also in Proceedings of the Twenty Third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 455-464, </pages> <year> 1991.) </year>
Reference-contexts: These hardness results left unresolved the status of learning DNF formulas in the absence of hypothesis restrictions, or with respect to the uniform distribution, or using membership queries. We prove that an algorithm due to Kushilevitz and Man-sour <ref> [16] </ref> can be used to weakly learn DNF formulas with respect to the uniform distribution using membership queries. Thus, we give the first positive result for learning unrestricted DNF in a nontrivial model of learning. <p> The same statement also applies to several well-studied algorithms proposed in the experimental machine learning community, including the ID3 algorithm for learning decision trees [22] and its variants. All of our results rely heavily on the Fourier representation of functions on the hypercube <ref> [18, 16, 20] </ref>, demonstrating once again the utility of spectral analysis tools in computational learning theory. 2 Definitions and Notation 2.1 Learning on the Uniform Distribution Using Member ship Queries A concept is a boolean function on an input space X (which in this paper will always be f0; 1g n <p> A t-sparse function is a function that has at most t non zero Fourier coefficients. The support of a function f is the set fA j ^ f (A) 6= 0g. 3 Preliminaries Our positive learnability results rely heavily on an algorithm of Kushilevitz and Mansour <ref> [16] </ref> (the KM algorithm) which finds, with high probability, close approximations to all of the large Fourier coefficients of a function f . The KM algorithm is allowed to make membership queries for f . <p> The KM algorithm is allowed to make membership queries for f . Kushile-vitz and Mansour have shown that given such approximate coefficients one can strongly learn some important concept classes such as decision trees <ref> [16] </ref>. However, while the KM algorithm is a key element of our learning scheme, we need to extend their approach somewhat to handle the case where the large Fourier coefficients give us only a weak approximation to the target function. <p> Noting that g = 2P 0 1 completes the proof. (Theorem 9) By restricting the size of terms in the SAT-k DNF's considered and using exact reconstruction and derandomization techniques similar to those of Kushilevitz and Mansour <ref> [16] </ref>, we can extend the above to a deterministic, distribution-independent learning result (this generalizes a similar result for SAT-1 (disjoint) DNF by Khardon [14]).
Reference: [17] <author> Eyal Kushilevitz and Dan Roth. </author> <title> On learning visual concepts and DNF formulae. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 317-326, </pages> <year> 1993. </year>
Reference-contexts: Due to the dearth of positive results for unrestricted DNF expressions, various restricted DNF classes have attracted considerable attention in the literature <ref> [4, 2, 10, 3, 1, 5, 17, 8] </ref>. Here we extend some of these results.
Reference: [18] <author> Nathan Linial, Yishay Mansour, and Noam Nisan. </author> <title> Constant depth circuits, Fourier transform, </title> <journal> and learnabil-ity. Journal of the ACM 40(3) </journal> <pages> 607-620, </pages> <month> July </month> <year> 1993. </year> <booktitle> (Also in 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 574-579, </pages> <year> 1989.) </year>
Reference-contexts: The same statement also applies to several well-studied algorithms proposed in the experimental machine learning community, including the ID3 algorithm for learning decision trees [22] and its variants. All of our results rely heavily on the Fourier representation of functions on the hypercube <ref> [18, 16, 20] </ref>, demonstrating once again the utility of spectral analysis tools in computational learning theory. 2 Definitions and Notation 2.1 Learning on the Uniform Distribution Using Member ship Queries A concept is a boolean function on an input space X (which in this paper will always be f0; 1g n <p> The algorithm runs in time polynomial in n, s, and log (1=ffi). 4 Positive Results 4.1 Weakly Learning DNF Linial, Mansour, and Nisan <ref> [18] </ref> showed that AC 0 , the class of constant-depth circuits, is learnable in superpolynomial but subexponential time with respect to the uniform distribution by proving that for every AC 0 function f almost all of the "large" Fourier coefficients of f are coefficients of parities of "few" variables.
Reference: [19] <author> Richard Lipton. </author> <type> Personal communication. </type>
Reference-contexts: T 1 represent the class of functions computable as the majority of a polynomial (in n) number of parities; equivalently, this is the class of functions that are the sign of a polynomially-sparse function having 3 A similar result has also been shown by Lipton using a somewhat different analysis <ref> [19] </ref>. polynomially-bounded integer Fourier coefficients. We show that c P T 1 is weakly learnable with respect to uniform using queries. c P T 1 is a rather general class containing many functions, such as majority, that are not approximable by AC 0 circuits.
Reference: [20] <author> Yishay Mansour. </author> <title> An O(n log log n ) learning algorithm for DNF under the uniform distribution. </title> <booktitle> In Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 53-61, </pages> <year> 1992. </year>
Reference-contexts: The same statement also applies to several well-studied algorithms proposed in the experimental machine learning community, including the ID3 algorithm for learning decision trees [22] and its variants. All of our results rely heavily on the Fourier representation of functions on the hypercube <ref> [18, 16, 20] </ref>, demonstrating once again the utility of spectral analysis tools in computational learning theory. 2 Definitions and Notation 2.1 Learning on the Uniform Distribution Using Member ship Queries A concept is a boolean function on an input space X (which in this paper will always be f0; 1g n
Reference: [21] <author> Lenny Pitt and Leslie G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the ACM, </journal> <volume> 35(4): </volume> <pages> 965-984, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Indeed, in the distribution-independent PAC model there are representation-dependent hardness results (that is, hardness results that assume certain syntactic restrictions on the learning algorithm's hypothesis) for learning even some rather restricted forms of DNF <ref> [21, 12] </ref>. These hardness results left unresolved the status of learning DNF formulas in the absence of hypothesis restrictions, or with respect to the uniform distribution, or using membership queries.
Reference: [22] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106. </pages>
Reference-contexts: The same statement also applies to several well-studied algorithms proposed in the experimental machine learning community, including the ID3 algorithm for learning decision trees <ref> [22] </ref> and its variants.
Reference: [23] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: 1 Introduction and History We present new results, both positive and negative, on the well-studied problem of learning DNF expressions. The problem of efficiently learning DNF in any nontrivial formal model of learning has been of central interest to the computational learning theory community since the seminal paper of Valiant <ref> [23] </ref> introducing the popular Probably Approximately Correct (PAC) learning model.
References-found: 23

