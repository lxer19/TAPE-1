URL: http://www.bell-labs.com/user/seung/papers/up-prop.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00428.html
Root-URL: 
Email: fjhoh|seungg@bell-labs.com  
Title: Learning Generative Models with the Up-Propagation Algorithm  
Author: Jong-Hoon Oh and H. Sebastian Seung 
Address: Murray Hill, NJ 07974  
Affiliation: Bell Labs, Lucent Technologies  
Abstract: Up-propagation is an algorithm for inverting and learning neural network generative models. Sensory input is processed by inverting a model that generates patterns from hidden variables using top-down connections. The inversion process is iterative, utilizing a negative feedback loop that depends on an error signal propagated by bottom-up connections. The error signal is also used to learn the generative model from examples. The algorithm is benchmarked against principal component analysis in In his doctrine of unconscious inference, Helmholtz argued that perceptions are formed by the interaction of bottom-up sensory data with top-down expectations. According to one interpretation of this doctrine, perception is a procedure of sequential hypothesis testing. We propose a new algorithm, called up-propagation, that realizes this interpretation in layered neural networks. It uses top-down connections to generate hypotheses, and bottom-up connections to revise them. It is important to understand the difference between up-propagation and its ancestor, the backpropagation algorithm[1]. Backpropagation is a learning algorithm for recognition models. As shown in Figure 1a, bottom-up connections recognize patterns, while top-down connections propagate an error signal that is used to learn the recognition model. In contrast, up-propagation is an algorithm for inverting and learning generative models, as shown in Figure 1b. Top-down connections generate patterns from a set of hidden variables. Sensory input is processed by inverting the generative model, recovering hidden variables that could have generated the sensory data. This operation is called either pattern recognition or pattern analysis, depending on the meaning of the hidden variables. Inversion of the generative model is done iteratively, through a negative feedback loop driven by an error signal from the bottom-up connections. The error signal is also used for learning the connections experiments on images of handwritten digits.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323 </volume> <pages> 533-536, </pages> <year> 1986. </year>
Reference-contexts: Up-propagation with a one-step generative model was applied to the USPS database [4], which consists of example images of handwritten digits. Each of the 7291 training and 2007 testing images was normalized to a 16 fi 16 grid with pixel intensities in the range <ref> [0; 1] </ref>. A separate model was trained for each digit class. The nonlinearity f was the logistic function. Batch optimization of (10) was done by PCA with n principal components is shown for comparison. <p> In a truly probabilistic model, like a belief network, every layer of the generation process adds variability. In conclusion, we briefly compare up-propagation to other algorithms and architectures. 1. In backpropagation <ref> [1] </ref>, only the recognition model is explicit. Iterative gradient descent methods can be used to invert the recognition model, though this implicit generative model generally appears to be inaccurate [7, 8]. 2. Up-propagation has an explicit generative model, and recognition is done by inverting the generative model.
Reference: [2] <author> D. D. Lee and H. S. Seung. </author> <title> Unsupervised learning by convex and conic coding. </title> <journal> Adv. Neural Info. Proc. Syst., </journal> <volume> 9 </volume> <pages> 515-521, </pages> <year> 1997. </year>
Reference-contexts: Up-propagation has an explicit generative model, and recognition is done by inverting the generative model. The accuracy of this implicit recognition model has not yet been tested empirically. Iterative inversion of generative models has also been proposed for linear networks <ref> [2, 9] </ref> and probabilistic belief networks [10]. 3. In the autoencoder [11] and the Helmholtz machine [12], there are separate models of recognition and generation, both explicit. Recognition uses only bottom-up connections, and generation uses only top-down connections. Neither process is iterative.
Reference: [3] <author> G. E. Hinton, P. Dayan, and M. Revow. </author> <title> Modeling the manifolds of images of hand-written digits. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 8 </volume> <pages> 65-74, </pages> <year> 1997. </year>
Reference-contexts: Continuous variability is clearly not confined to visual images, but is present in many other domains. Many existing techniques for modeling pattern manifolds, such as PCA or PCA mixtures <ref> [3] </ref>, depend on linear or locally linear approximations to the manifold. Up-prop constructs a globally parametrized, nonlinear manifold. 3 ONE-STEP GENERATION The simplest generative model of the form (1) has just one step (L = 1).
Reference: [4] <author> Y. LeCun et al. </author> <title> Learning algorithms for classification: a comparison on handwritten digit recognition. </title> <editor> In J.-H. Oh, C. Kwon, and S. Cho, editors, </editor> <booktitle> Neural networks: the statistical mechanics perspective, </booktitle> <pages> pages 261-276, </pages> <address> Singapore, 1995. </address> <publisher> World Scientific. </publisher>
Reference-contexts: Up-propagation with a one-step generative model was applied to the USPS database <ref> [4] </ref>, which consists of example images of handwritten digits. Each of the 7291 training and 2007 testing images was normalized to a 16 fi 16 grid with pixel intensities in the range [0; 1]. A separate model was trained for each digit class. The nonlinearity f was the logistic function.
Reference: [5] <author> D. P. Bertsekas. </author> <title> Nonlinear programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: The nonlinearity f was the logistic function. Batch optimization of (10) was done by PCA with n principal components is shown for comparison. The up-prop network performs better on both the training set and test set. gradient descent with adaptive stepsize control by the Armijo rule <ref> [5] </ref>. In most cases, the stepsize varied between 10 1 and 10 3 , and the optimization usually converged within 10 3 epochs.
Reference: [6] <author> K. Kang, J.-H. Oh, C. Kwon, and Y. Park. </author> <title> Generalization in a two-layer neural network. </title> <journal> Phys. Rev., </journal> <volume> E48:4805-4809, </volume> <year> 1993. </year>
Reference-contexts: While up-prop for one-step generation converged within several hundred epochs, up-prop for two-step generation often needed several thousand epochs or more to converge. We often found long plateaus in the learning curves, which may be due to the permutation symmetry of the network architecture <ref> [6] </ref>. 5 DISCUSSION To summarize the experiments discussed above, we constructed separate generative models, one for each digit class. Relative to PCA, each generative model was superior at encoding digits from its corresponding class. This enhanced generative ability was due to the use of nonlinearity.
Reference: [7] <author> J. Kindermann and A. Linden. </author> <title> Inversion of neural networks by gradient descent. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 277-286, </pages> <year> 1990. </year>
Reference-contexts: In conclusion, we briefly compare up-propagation to other algorithms and architectures. 1. In backpropagation [1], only the recognition model is explicit. Iterative gradient descent methods can be used to invert the recognition model, though this implicit generative model generally appears to be inaccurate <ref> [7, 8] </ref>. 2. Up-propagation has an explicit generative model, and recognition is done by inverting the generative model. The accuracy of this implicit recognition model has not yet been tested empirically.
Reference: [8] <author> Y. Lee. </author> <title> Handwritten digit recognition using K nearest-neighbor, radial-basis function, and backpropagation neural networks. </title> <journal> Neural Comput., </journal> <volume> 3 </volume> <pages> 441-449, </pages> <year> 1991. </year>
Reference-contexts: In conclusion, we briefly compare up-propagation to other algorithms and architectures. 1. In backpropagation [1], only the recognition model is explicit. Iterative gradient descent methods can be used to invert the recognition model, though this implicit generative model generally appears to be inaccurate <ref> [7, 8] </ref>. 2. Up-propagation has an explicit generative model, and recognition is done by inverting the generative model. The accuracy of this implicit recognition model has not yet been tested empirically.
Reference: [9] <author> R. P. N. Rao and D. H. Ballard. </author> <title> Dynamic model of visual recognition predicts neural response properties in the visual cortex. </title> <journal> Neural Comput., </journal> <volume> 9 </volume> <pages> 721-63, </pages> <year> 1997. </year>
Reference-contexts: Up-propagation has an explicit generative model, and recognition is done by inverting the generative model. The accuracy of this implicit recognition model has not yet been tested empirically. Iterative inversion of generative models has also been proposed for linear networks <ref> [2, 9] </ref> and probabilistic belief networks [10]. 3. In the autoencoder [11] and the Helmholtz machine [12], there are separate models of recognition and generation, both explicit. Recognition uses only bottom-up connections, and generation uses only top-down connections. Neither process is iterative.
Reference: [10] <author> L. K. Saul, T. Jaakkola, and M. I. Jordan. </author> <title> Mean field theory for sigmoid belief networks. </title> <journal> J. Artif. Intell. Res., </journal> <volume> 4 </volume> <pages> 61-76, </pages> <year> 1996. </year>
Reference-contexts: Up-propagation has an explicit generative model, and recognition is done by inverting the generative model. The accuracy of this implicit recognition model has not yet been tested empirically. Iterative inversion of generative models has also been proposed for linear networks [2, 9] and probabilistic belief networks <ref> [10] </ref>. 3. In the autoencoder [11] and the Helmholtz machine [12], there are separate models of recognition and generation, both explicit. Recognition uses only bottom-up connections, and generation uses only top-down connections. Neither process is iterative. Both processes can operate completely inde <br>- pendently; they only interact during learning. 4.
Reference: [11] <author> G. W. Cottrell, P. Munro, and D. Zipser. </author> <title> Image compression by back propagation: an example of extensional programming. </title> <editor> In N. E. Sharkey, editor, </editor> <title> Models of cognition: a review of cognitive science. </title> <publisher> Ablex, </publisher> <address> Norwood, NJ, </address> <year> 1989. </year>
Reference-contexts: The accuracy of this implicit recognition model has not yet been tested empirically. Iterative inversion of generative models has also been proposed for linear networks [2, 9] and probabilistic belief networks [10]. 3. In the autoencoder <ref> [11] </ref> and the Helmholtz machine [12], there are separate models of recognition and generation, both explicit. Recognition uses only bottom-up connections, and generation uses only top-down connections. Neither process is iterative. Both processes can operate completely inde <br>- pendently; they only interact during learning. 4.
Reference: [12] <author> G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. </author> <title> The "wake-sleep" algorithm for unsupervised neural networks. </title> <journal> Science, </journal> <volume> 268 </volume> <pages> 1158-1161, </pages> <year> 1995. </year>
Reference-contexts: The accuracy of this implicit recognition model has not yet been tested empirically. Iterative inversion of generative models has also been proposed for linear networks [2, 9] and probabilistic belief networks [10]. 3. In the autoencoder [11] and the Helmholtz machine <ref> [12] </ref>, there are separate models of recognition and generation, both explicit. Recognition uses only bottom-up connections, and generation uses only top-down connections. Neither process is iterative. Both processes can operate completely inde <br>- pendently; they only interact during learning. 4.
Reference: [13] <author> H. S. Seung. </author> <title> Pattern analysis and synthesis in attractor neural networks. </title> <editor> In K.-Y. M. Wong, I. King, and D.-Y. Yeung, editors, </editor> <booktitle> Theoretical Aspects of Neural Computation: </booktitle>
Reference-contexts: Recognition uses only bottom-up connections, and generation uses only top-down connections. Neither process is iterative. Both processes can operate completely inde <br>- pendently; they only interact during learning. 4. In attractor neural networks <ref> [13, 14] </ref> and the Boltzmann machine [15], recognition and generation are performed by the same recurrent network. Each process is iterative, and each utilizes both bottom-up and top-down connections. Computation in these networks is chiefly based on positive, rather than negative feedback.
References-found: 13

