URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/ICML98_SPS.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Title: Intra-Option Learning about Temporally Abstract Actions  
Author: Richard S. Sutton, Doina Precup Satinder Singh 
Address: Amherst, MA 01003-4610  Boulder, CO 80309-0430  
Affiliation: Department of Computer Science University of Massachusetts  Department of Computer Science University of Colorado  
Abstract: Several researchers have proposed modeling temporally abstract actions in reinforcement learning by the combination of a policy and a termination condition, which we refer to as an option. Value functions over options and models of options can be learned using methods designed for semi-Markov decision processes (SMDPs). However, these methods all require an option to be executed to termination. In this paper we explore methods that learn about an option from small fragments of experience consistent with that option, even if the option itself is not executed. We call these methods intra-option learning methods because they learn from experience within an option. Intra-option methods are sometimes much more efficient than SMDP methods because they can use off-policy temporal-difference mechanisms to learn simultaneously about all the options consistent with an experience, not just the few that were actually executed. In this paper we present intra-option learning methods for learning value functions over options and for learning multi-step models of the consequences of options. We present computational examples in which these new methods learn much faster than SMDP methods and learn effectively when SMDP methods cannot learn at all. We also sketch Keywords: Reinforcement Learning, Temporal Abstraction, Hierarchical Learning Semi-Markov Decision Processes, Model Learning E-mail: rich@cs.umass.edu Phone: 978-897-6174 Multiple submission statement: This paper contains results that are also being prepared to form part of a larger paper to be submitted for journal publication. a convergence proof for intra-option value learning.
Abstract-found: 1
Intro-found: 1
Reference: [Bradtke and Duff, 1995] <author> Bradtke, S. J. and Duff, M. O. </author> <year> (1995). </year> <title> Reinforcement learning methods for continuous-time markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 393400, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: [Dayan, 1993] <author> Dayan, P. </author> <year> (1993). </year> <title> Improving generalization for temporal difference learning: </title> <booktitle> The successor representation. Neural Computation, </booktitle> <address> 5:613624. </address>
Reference: [Dayan and Hinton, 1993] <author> Dayan, P. and Hinton, G. E. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 271278, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Dietterich, 1997] <author> Dietterich, T. G. </author> <year> (1997). </year> <title> Hierarchical reinforcement learning with the maxq value function decomposition. </title> <type> Technical report, </type> <institution> Department of Computer Science, Oregon State University. </institution>
Reference: [Huber and Grupen, 1997] <author> Huber, M. and Grupen, R. A. </author> <year> (1997). </year> <title> A feedback control structure for on-line learning tasks. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <address> 22:303315. </address>
Reference: [Jaakkola et al., 1994] <author> Jaakkola, T., Jordan, M., and Singh, S. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. Neural Computation, </title> <publisher> 6(6):11851201. </publisher>
Reference: [Kaelbling, 1993] <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning ICML'93, </booktitle> <pages> pages 167173, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Mahadevan and Connell, 1992] <author> Mahadevan, S. and Connell, J. </author> <year> (1992). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, 55(2-3):311365. </journal>
Reference: [Mahadevan et al., 1997] <author> Mahadevan, S., Marchalleck, N., Das, T. K., and Gosavi, A. </author> <year> (1997). </year> <title> Self-improving factory simulation using continuou-time average-reward reinforcement learning. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning (ICML'97), </booktitle> <pages> pages 202210. </pages>
Reference: [McGovern et al., 1997] <author> McGovern, E. A., Sutton, R. S., and Fagg, A. H. </author> <year> (1997). </year> <title> Roles of macro-actions in accelerating reinforcement learning. </title> <booktitle> In Grace Hopper Celebration of Women in Computing, </booktitle> <pages> pages 1317. </pages>
Reference: [Moore and Atkeson, 1993] <author> Moore, A. W. and Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. Machine Learning, </title> <publisher> 13:103130. </publisher>
Reference: [Parr, 1998] <author> Parr, R. </author> <year> (1998). </year> <title> Hierarchical control and learning for markov decision pro-cesses. </title> <journal> Personnal Communication. </journal>
Reference: [Parr and Russell, 1998] <author> Parr, R. and Russell, S. </author> <year> (1998). </year> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> In Advances in Neural Information Processing Systems, volume 10, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: [Precup and Sutton, 1998] <author> Precup, D. and Sutton, R. S. </author> <year> (1998). </year> <title> Multi-time models for temporally abstract planning. </title> <booktitle> In Advances in Neural Infomation Processing Systems, volume 10, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: [Precup et al., 1998] <author> Precup, D., Sutton, R. S., and Singh, S. </author> <year> (1998). </year> <title> Theoretical results on reinforcement learning with temporally abstract options. </title> <booktitle> In Proceedings of the Tenth European Conference on Machine Learning (ECML'98). </booktitle> <publisher> Springer Verlag. In press. </publisher>
Reference: [Singh, 1992] <author> Singh, S. P. </author> <year> (1992). </year> <title> Scaling reinforcement learning by learning variable temporal resolution models. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning ICML'92, pages 202207, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Sutton, 1990] <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning ICML'90, </booktitle> <pages> pages 216224, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Sutton, 1995] <author> Sutton, R. S. </author> <year> (1995). </year> <title> TD models: Modeling the world as a mixture of time scales. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning ICML'95, </booktitle> <pages> pages 531539, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Sutton and Barto, 1998] <author> Sutton, R. S. and Barto, A. G. </author> <year> (1998). </year> <title> Reinforcement Learning. An Introduction. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <month> 17 </month>
References-found: 19

