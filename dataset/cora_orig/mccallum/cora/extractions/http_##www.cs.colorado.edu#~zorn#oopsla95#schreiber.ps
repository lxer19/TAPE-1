URL: http://www.cs.colorado.edu/~zorn/oopsla95/schreiber.ps
Refering-URL: http://www.cs.colorado.edu/~zorn/oopsla95/papers.html
Root-URL: http://www.cs.colorado.edu
Abstract-found: 0
Intro-found: 1
Reference: [Abra94] <author> K. Abramowicz, J. Alt, H. Schreiber and M. Wallrath. </author> <title> Evaluierung objektorientierter Datenbanksysteme - Datenmodelle, Funktiona-litt, Performance. </title> <address> FZI-Publication 3/94. For-schungszentrum Informatik, Haid-und-Neu-Strae 10-14, 76131 Karlsruhe, Germany, </address> <note> August 1994 (in German). </note>
Reference-contexts: The benchmark was applied to the selection of an ODBMS for a ship-building CAD system, comparing Objectivity, ObjectStore, Obst, Ontos and Versant. Although it taught much about the mentioned systems and led to a satisfactory selection, the most important message of this study <ref> [Abra94] </ref> has been that the performance ranking depends heavily on the applied test scenario and the used scaling factors. This aspect will therefore not be further investigated in this paper. Another result has been that trying to solve the portability problem by employing inheritance was not reasonable.
Reference: [Ande90] <author> T. L. Anderson, A. J. Berre, M. Mallison, H. H. Porter and B. Schneider. </author> <title> The HyperModel Benchmark. </title> <booktitle> In Proc. International Conference on Extending Database Technology, </booktitle> <address> Venice, Italy, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: The basic operations are described below: simple-read: Search all objects of an OID collection and read their time stamp date. For direct accesses, this operation is similar to the Lookup of OO1 [Catt92] or the nameOIDLookup of the HyperModel benchmark <ref> [Ande90] </ref>. For traversing access, simple-read is a generalization of closure1NAttSum and seqScan of the HyperModel benchmark. simple-write (touch): Search all objects of an OID collection and update their time stamp date. For direct accesses, this operation is similar to Traversal T3 of OO7 [Care93]. <p> Test Scenario As mentioned earlier, the scaling factors significantly inu-ence the results of the benchmark. To show the importance of structure preservation and multi-user tests, an arbitrary scenario will be used. For sake of simplicity, the Hyper-Model <ref> [Ande90] </ref> is taken as a reference point. This scenario is represented by the JUSTITIA parameters of Table 1. For an algorithm to derive benchmark parameters from application characteristics, the reader is referred to [Schr95]. 4.
Reference: [Care93] <author> M. J. Carey, D. J. DeWitt and J. F. Naughton. </author> <title> The OO7 Benchmark. </title> <booktitle> In Proc. ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pp. 1221, </pages> <address> Washington, DC, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: For traversing access, simple-read is a generalization of closure1NAttSum and seqScan of the HyperModel benchmark. simple-write (touch): Search all objects of an OID collection and update their time stamp date. For direct accesses, this operation is similar to Traversal T3 of OO7 <ref> [Care93] </ref>. However, indexing of the date attribute is not prescribed.
Reference: [Catt92] <author> R. G. G. Cattell and J. Skeen. </author> <title> Object Operations Benchmark. </title> <journal> ACM Transactions on Data base Systems, </journal> <volume> 17(1):131, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: The basic operations are described below: simple-read: Search all objects of an OID collection and read their time stamp date. For direct accesses, this operation is similar to the Lookup of OO1 <ref> [Catt92] </ref> or the nameOIDLookup of the HyperModel benchmark [Ande90]. For traversing access, simple-read is a generalization of closure1NAttSum and seqScan of the HyperModel benchmark. simple-write (touch): Search all objects of an OID collection and update their time stamp date.
Reference: [Chau95] <author> A. B. Chaudhri. </author> <title> An Annotated Bibliography of Benchmarks for Object Databases. </title> <journal> ACM SIG MOD Record, </journal> <volume> 24(1):18, </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: These criteria could be further classified into those with technical (relevance) and economical nature (simplicity, portability and scalability). There have already been many approaches, which all have their strengths and weaknesses in one or the other of the above areas. A brief survey of known attempts is given in <ref> [Chau95] </ref>. However, a benchmark wont be used if not all of the above criteria are equally fulfilled. The following deficiencies have been observed in known approaches: Snapshot approach: The structure preservation ability of an ODBMS was not measured. Instead, test procedures were always measured on initially built databases.
Reference: [Edel92] <author> D. R. Edelson. </author> <title> Smart Pointers: Theyre Smart, But Theyre Not Pointers. </title> <type> Technical Report UCSC-CRL-92-27, </type> <institution> Computer Engineering & Information Sciences, University of California, </institution> <address> Santa Cruz, CA 95064 USA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: This aspect will therefore not be further investigated in this paper. Another result has been that trying to solve the portability problem by employing inheritance was not reasonable. This approach led to cumbersome constructs, to circumvent ODBMS specific problems of inheritance and reference implementation (see also <ref> [Edel92] </ref>). As a result, the benchmark had to be redesigned to ease porting it onto other ODBMS platforms. The new version has been implemented on top of two commercial ODBMS 1 . Due to legal constraints they will be called P1 and P2 in the following.
Reference: [Gray93] <author> J. Gray (Ed.). </author> <title> The Benchmark Handbook for Database and Transaction Processing Systems. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> 2nd edi tion, </address> <year> 1993. </year>
Reference-contexts: The best way to prove the usability of an ODBMS is to implement and measure critical parts of an application on top of it. Unfortunately, this approach is only rarely applicable. Therefore, people tend to use synthetical benchmarks for the performance measurement. The fundamental book of Jim Gray <ref> [Gray93] </ref> states the key criteria for any benchmark: it should be relevant, simple, portable and scalable. These criteria could be further classified into those with technical (relevance) and economical nature (simplicity, portability and scalability).
Reference: [Litt61] <author> J. D. C. Little. </author> <title> A Proof for the Queueing Formula . Operations Research, </title> <address> 3(9):383387, </address> <month> May </month> <year> 1961. </year>
Reference-contexts: The most important aspect for this type of measurement is the elaboration of an ODBMSs saturation, meaning the point at which an ODBMSs throughput does not increase when adding additional clients, although there are no synchronization conicts. The system throughput can easily be computed by applying Littles law <ref> [Litt61] </ref> to the measured 2.5 7.5 12.5 0 2 4 6 8 10 12 P1 cold P1 warm P2 cold P2 warm response time [s] blow-up factor 20 60 100 response time [s] blow-up factor P1 cold P1 warm P2 cold P2 warm response times.
Reference: [Schr94] <author> H. Schreiber. JUSTITIA: </author> <title> A Generic Benchmark for the OODBMS Selection. </title> <booktitle> In 4th Int. Conf. on Data and Knowledge Systems for Manufacturing and Engineering, </booktitle> <pages> pp. 324331, </pages> <address> Shatin, Hong Kong, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Neglected non-DBMS loads: External loads, which could have a significant effect on performance, due to differences in ODBMS architectures, were not covered. The JUSTITIA benchmark aimed at curing these deficiencies. The specification and justification can be found in <ref> [Schr94] </ref>. It has been based on two central design decisions. Firstly, the implementation effort had to be reduced through extensive reusability and portability. Secondly, the saved time had to be reinvested into enhancements of relevance and scalability. <p> Both are scalable to allow the mapping of application characteristics onto the benchmark. This section will give a brief overview of both. For details and justification readers are referred to <ref> [Schr94] </ref> and [Schr95]. 2.1 The JUSTITIA Class Structure There are three different categories of objects in any application, namely static objects, which do not change their size during runtime, simple dynamic objects, containing data fields of dynamic size, and complex dynamic objects, built up of other objects. <p> The benchmark software will then generate the database and run the test procedures without any need of further implementations. Mapping of the benchmark on different ODBMS can be done through implementing a defined protocol layer. In contrast to the inheritance approach of <ref> [Schr94] </ref>, the mapping is realized through access functions in the redesigned version of the benchmark [Schr95]. In order to control locking conicts when running the benchmark in a multi-user setup, the database can be built in different sections, which are structurally identical.
Reference: [Schr95] <author> H. Schreiber. </author> <title> Ein Benchmark-System zur Lei-stungsmessung objektorientierter Datenbank-systeme fr ingenieurwissenschaftliche Anwen-dungen. </title> <type> Dissertation, </type> <institution> Universitt Karlsruhe, Germany, </institution> <note> October 1995 (to appear in German) L lW= </note>
Reference-contexts: Both are scalable to allow the mapping of application characteristics onto the benchmark. This section will give a brief overview of both. For details and justification readers are referred to [Schr94] and <ref> [Schr95] </ref>. 2.1 The JUSTITIA Class Structure There are three different categories of objects in any application, namely static objects, which do not change their size during runtime, simple dynamic objects, containing data fields of dynamic size, and complex dynamic objects, built up of other objects. <p> Mapping of the benchmark on different ODBMS can be done through implementing a defined protocol layer. In contrast to the inheritance approach of [Schr94], the mapping is realized through access functions in the redesigned version of the benchmark <ref> [Schr95] </ref>. In order to control locking conicts when running the benchmark in a multi-user setup, the database can be built in different sections, which are structurally identical. <p> For sake of simplicity, the Hyper-Model [Ande90] is taken as a reference point. This scenario is represented by the JUSTITIA parameters of Table 1. For an algorithm to derive benchmark parameters from application characteristics, the reader is referred to <ref> [Schr95] </ref>. 4. Structure Preservation Ability Some ODBMS reorganize their databases on-line or chose sophisticated techniques to assign single objects optimally on pages. Although this additional work may be well invested and may prove useful in some applications, it was not tested in any ODBMS benchmark.
References-found: 10

