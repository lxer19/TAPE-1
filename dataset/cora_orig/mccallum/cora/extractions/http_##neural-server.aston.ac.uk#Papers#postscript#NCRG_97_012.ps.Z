URL: http://neural-server.aston.ac.uk/Papers/postscript/NCRG_97_012.ps.Z
Refering-URL: http://www.cs.toronto.edu/~carl/gp.html
Root-URL: 
Title: PREDICTION WITH GAUSSIAN PROCESSES: FROM LINEAR REGRESSION TO LINEAR PREDICTION AND BEYOND To appear in
Phone: Tel: +44 (0)121 333 4631  
Author: C. K. I. WILLIAMS ed. M. I. Jordan, 
Note: Kluwer Academic Press, 1998  
Date: October 27, 1997  
Web: http://www.ncrg.aston.ac.uk/  
Address: Birmingham B4 7ET United Kingdom  
Affiliation: Neural Computing Research Group Dept of Computer Science Applied Mathematics Aston University  
Pubnum: Technical Report NCRG/97/012  
Abstract: The main aim of this paper is to provide a tutorial on regression with Gaussian processes. We start from Bayesian linear regression, and show how by a change of viewpoint one can see this method as a Gaussian process predictor based on priors over functions, rather than on priors over parameters. This leads in to a more general discussion of Gaussian processes in section 4. Section 5 deals with further issues, including hierarchical modelling and the setting of the parameters that control the Gaussian process, the covariance functions for neural network models and the use of Gaussian processes in classification problems. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aizerman, M. A., E. M. Braverman, and L. I. </author> <month> Rozoner </month> <year> (1964). </year> <title> Theoretical foundations of the potential function method in pattern recognition learning. </title> <booktitle> Automation and Remote Control 25, </booktitle> <pages> 821-837. </pages>
Reference-contexts: fact that an input vector can be expanded into an infinite-dimensional space ( 1 (x); 2 (x); : : : ) but that the necessary computations can be carried out efficiently due to Mercer's theorem has been used in some other contexts, for example in the method of potential functions <ref> (due to Aizerman, Braverman and Rozoner, 1964) </ref> and in support vector machines (Vapnik, 1995).
Reference: <author> Barber, D. and C. K. I. </author> <title> Williams (1997). Gaussian Processes for Bayesian Classification via Hybrid Monte Carlo. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference: <author> Box, G. E. P. and G. C. </author> <month> Tiao </month> <year> (1973). </year> <title> Bayesian Inference in Statistical Analysis. </title> <address> Reading, Mass.: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Bridle, J. </author> <year> (1990). </year> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. </title> <editor> In F. Fougelman-Soulie and J. Herault (Eds.), </editor> <booktitle> NATO ASI series on systems and computer science. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: An early reference to this approach is the work of Silverman (1978). For the classification problem with more than two classes, a simple extension of this idea using the "softmax" function <ref> (Bridle, 1990) </ref> gives the predicted probability for class k as (kjx) = P : (38) For the rest of this section we shall concentrate on the two-class problem; extension of the methods to the multi-class case is relatively straightforward.
Reference: <author> Cressie, N. A. C. </author> <year> (1993). </year> <title> Statistics for Spatial Data. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Duane, S., A. D. Kennedy, B. J. Pendleton, and D. </author> <month> Roweth </month> <year> (1987). </year> <title> Hybrid Monte Carlo. </title> <journal> Physics Letters B 195, </journal> <pages> 216-222. </pages>
Reference: <author> Gelman, A., J. B. Carlin, H. S. Stern, and D. B. </author> <title> Rubin (1995). Bayesian Data Analysis. </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: These work by constructing a Markov chain whose equilibrium distribution is the desired distribution P (jD); the integral in equation 33 is then approximated using samples from the Markov chain. Two standard methods for constructing MCMC methods are the Gibbs sampler and Metropolis-Hastings algorithms <ref> (see, e.g., Gelman et al, 1995) </ref>.
Reference: <author> Gibbs, M. and D. J. C. </author> <title> MacKay (1997a). Efficient Implementation of Gaussian Processes. </title> <type> Draft manuscript, </type> <note> available from http://wol.ra.phy.cam.ac.uk/mackay/homepage.html. </note>
Reference: <author> Gibbs, M. and D. J. C. </author> <title> MacKay (1997b). Variational Gaussian Process Classifiers. </title> <type> Draft manuscript, </type> <note> available via http://wol.ra.phy.cam.ac.uk/mackay/homepage.html. </note>
Reference: <author> Girard, D. </author> <year> (1989). </year> <title> A fast "Monte Carlo cross-validation" procedure for large least squares problems with noisy data. </title> <journal> Numer. Math. </journal> <volume> 56, </volume> <pages> 1-23. </pages>
Reference: <author> Girosi, F., M. Jones, and T. </author> <title> Poggio (1995). Regularization Theory and Neural Networks Architectures. </title> <booktitle> Neural Computation 7(2), </booktitle> <pages> 219-269. </pages>
Reference: <author> Goldberg, P. W., C. K. I. Williams, and C. M. </author> <title> Bishop (1997). Regression with Input-dependent Noise: A Gaussian Process Treatment. </title> <note> Accepted to NIPS*97. </note>
Reference-contexts: can also be used in a hierarchical regression model where it is assumed that the noise process has a variance that depends on x, and that this noise-field N (x) is drawn from a prior generated from an independent Gaussian process Z (x) by N (x) = exp Z (x) <ref> (see Goldberg et al, 1997) </ref>. 6 Discussion In this paper I have shown how to move from simple Bayesian linear regression to regression with Gaussian processes, and have discussed some of the issues in using Gaussian process prediction for the kinds of problem that neural networks have also been used on.
Reference: <author> Green, P. J. and B. W. </author> <title> Silverman (1994). Nonparametric regression and generalized linear models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> Handcock, M. S. and M. L. </author> <title> Stein (1993). A Bayesian Analysis of Kriging. </title> <type> Technometrics 35(4), </type> <pages> 403-410. </pages>
Reference: <author> Hastie, T. </author> <year> (1996). </year> <title> Pseudosplines. </title> <journal> Journal of the Royal Statistical Society B 58, </journal> <pages> 379-396. </pages>
Reference: <author> Hastie, T. J. and R. J. </author> <month> Tibshirani </month> <year> (1990). </year> <title> Generalized Additive Models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: to linear prediction : : : As we have already seen in section 3.3, if the prior is a general Gaussian process and we assume a Gaussian noise model, then the predicted y-value is just some linear combination of the t-values; the method is said to be a linear smoother <ref> (Hastie and Tibshirani, 1990) </ref> or a linear predictor. In section 3 we have seen how linear regression can be seen from a function-space viewpoint.
Reference: <author> Hornik, K. </author> <year> (1993). </year> <title> Some new results on neural network approximation. </title> <booktitle> Neural Networks 6(8), </booktitle> <pages> 1069-1072. </pages>
Reference: <author> Hutchinson, M. </author> <year> (1989). </year> <title> A stochastic estimator for the trace of the influence matrix for Laplacian smoothing splines. </title> <booktitle> Communications in statistics:Simulation and computation 18, </booktitle> <pages> 1059-1076. </pages>
Reference: <author> Journel, A. G. and C. J. </author> <month> Huijbregts </month> <year> (1978). </year> <title> Mining Geostatistics. </title> <publisher> Academic Press. </publisher>
Reference: <author> Kimeldorf, G. and G. </author> <title> Wahba (1970). A correspondence between Bayesian estimation of stochastic processes and smoothing by splines. </title> <journal> Annals of Mathematical Statistics 41, </journal> <pages> 495-502. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1992). </year> <title> A Practical Bayesian Framework for Backpropagation Networks. </title> <booktitle> Neural Computation 4(3), </booktitle> <pages> 448-472. </pages>
Reference-contexts: Given an observed dataset, a posterior distribution over the weights and hyperparameters (rather than just a point estimate) will be induced. However, for neural network models this posterior cannot usually be obtained analytically; computational methods used include approximations <ref> (MacKay, 1992) </ref> or the evaluation of integrals using Monte Carlo methods (Neal, 1996). In the Bayesian approach to neural networks, a prior on the weights of a network induces a prior over functions.
Reference: <author> MacKay, D. J. C. </author> <year> (1993). </year> <title> Bayesian Methods for Backpropagation Networks. </title> <editor> In J. L. van Hem-men, E. Domany, and K. Schulten (Eds.), </editor> <title> Models of Neural Networks II. Springer. PREDICTION WITH GAUSSIAN PROCESSES: FROM LINEAR REGRESSION TO LINEAR PREDICTION AND BEYOND 18 Mardia, </title> <editor> K. V. and R. J. Marshall (1984). </editor> <title> Maximum likelihood estimation for models of residual covariance in spatial regression. </title> <journal> Biometrika 71(1), </journal> <pages> 135-146. </pages>
Reference: <author> Neal, R. M. </author> <year> (1997). </year> <title> Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification. </title> <type> Draft manuscript, </type> <note> available from http://www.cs.toronto.edu/~radford/. </note>
Reference: <author> Neal, R. M. </author> <year> (1996). </year> <title> Bayesian Learning for Neural Networks. </title> <address> New York: Springer. </address> <booktitle> Lecture Notes in Statistics 118. </booktitle>
Reference-contexts: Given an observed dataset, a posterior distribution over the weights and hyperparameters (rather than just a point estimate) will be induced. However, for neural network models this posterior cannot usually be obtained analytically; computational methods used include approximations (MacKay, 1992) or the evaluation of integrals using Monte Carlo methods <ref> (Neal, 1996) </ref>. In the Bayesian approach to neural networks, a prior on the weights of a network induces a prior over functions. An alternative method of putting a prior over functions is to use a Gaussian process (GP) prior over functions. <p> Gaussian process priors have the advantage over neural networks that at least the lowest level of a Bayesian hierarchical model can be treated analytically. Recent work <ref> (Williams and Rasmussen, 1996, inspired by observations in Neal, 1996) </ref> has extended the use of these priors to higher dimensional problems that have been traditionally tackled with other techniques such as neural networks, decision trees etc and has shown that good results can be obtained. <p> from the test point when making predictions, thereby producing a PREDICTION WITH GAUSSIAN PROCESSES: FROM LINEAR REGRESSION TO LINEAR PREDICTION AND BEYOND 13 smaller matrix to be inverted. 5.4 The covariance function of neural networks My own interest in using Gaussian processes for regression was sparked by Radford Neal's observation <ref> (Neal, 1996) </ref>, that under a Bayesian treatment, the functions produced by a neural network with certain kinds of prior distribution over its weights will tend to a Gaussian process prior over functions as the number of hidden units in the network tends to infinity. <p> Let b and the v's have independent zero-mean distributions of variance 2 b and v respectively, and let the weights u j for each hidden unit be independently and identically distributed. Denoting all weights by w, we obtain <ref> (following Neal, 1996) </ref> E w [f (x)] = 0 (35) b + j v E u [h j (x; u)h j (x 0 ; u)] (36) b + H 2 where equation 37 follows because all of the hidden units are identically distributed.
Reference: <author> O'Hagan, A. </author> <year> (1978). </year> <title> Curve Fitting and Optimal Design for Prediction (with discussion). </title> <journal> Journal of the Royal Statistical Society B 40(1), </journal> <pages> 1-42. </pages>
Reference: <author> O'Sullivan, F., B. S. Yandell, and W. J. </author> <month> Raynor </month> <year> (1986). </year> <title> Automatic Smoothing of Regression Functions in Generalized Linear Models. </title> <journal> Journal of the American Statistical Association 81, </journal> <pages> 96-103. </pages>
Reference: <author> Poggio, T. and F. </author> <title> Girosi (1990). Networks for approximation and learning. </title> <booktitle> Proceedings of IEEE 78, </booktitle> <pages> 1481-1497. </pages>
Reference: <author> Press, W. H., S. A. Teukolsky, W. T. Vetterling, and B. P. </author> <title> Flannery (1992). Numerical Recipes in C (second ed.). </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: The equivalence of the two expressions for the variance can be proved by using the following matrix identity <ref> (the Woodbury formula, Press et al, 1992, section 2.7) </ref> (X + Y Z) 1 = X 1 X 1 Y (I + ZX 1 Y ) 1 ZX 1 (26) w + fi T ) 1 to obtain A 1 = w w T ( 2 which may be substituted into
Reference: <author> Rasmussen, C. E. </author> <year> (1996). </year> <title> Evaluation of Gaussian Processes and Other Methods for Non-linear Regression. </title> <type> Ph. D. thesis, </type> <institution> Dept. of Computer Science, University of Toronto. </institution> <note> Available from http://www.cs.utoronto.ca/~carl/. </note>
Reference-contexts: Gaussian process priors have the advantage over neural networks that at least the lowest level of a Bayesian hierarchical model can be treated analytically. Recent work <ref> (Williams and Rasmussen, 1996, inspired by observations in Neal, 1996) </ref> has extended the use of these priors to higher dimensional problems that have been traditionally tackled with other techniques such as neural networks, decision trees etc and has shown that good results can be obtained.
Reference: <author> Ripley, B. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks. </title> <address> Cambridge, UK: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Sacks, J., W. J. Welch, T. J. Mitchell, and H. P. </author> <title> Wynn (1989). Design and Analysis of Computer Experiments. </title> <booktitle> Statistical Science 4(4), </booktitle> <pages> 409-435. </pages>
Reference-contexts: Essentially splines correspond to Gaussian processes with a particular choice of covariance function 3 . Gaussian process prediction was also suggested by O'Hagan (1978), and is widely used in the analysis of computer experiments <ref> (e.g Sacks et al, 1989) </ref>, although in this application it is assumed that the observations are noise-free. A connection to neural networks was made by Poggio and Girosi (1990) and Girosi, Jones and Poggio (1995) with their work on Regularization Networks.
Reference: <author> Sampson, P. D. and P. </author> <month> Guttorp </month> <year> (1992). </year> <title> Nonparametric estimation of nonstationary covariance structure. </title> <journal> Journal of the American Statistical Association 87, </journal> <pages> 108-119. </pages>
Reference: <author> Silverman, B. W. </author> <year> (1978). </year> <title> Density Ratios, Empirical Likelihood and Cot Death. </title> <journal> Applied Statistics 27(1), </journal> <pages> 26-33. </pages>
Reference: <author> Silverman, B. W. </author> <year> (1985). </year> <title> Some aspects of the spline smoothing approach to non-parametric regression curve fitting (with discussion). </title> <journal> J. Roy. Stat. Soc. </journal> <volume> B 47(1), </volume> <pages> 1-52. </pages>
Reference: <author> Skilling, J. </author> <year> (1993). </year> <title> Bayesian numerical analysis. </title> <editor> In W. T. Grandy, Jr. and P. Milonni (Eds.), </editor> <title> Physics and Probability. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <address> New York: </address> <publisher> Springer Verlag. </publisher> <editor> von Mises, R. </editor> <year> (1964). </year> <title> Mathematical Theory of Probability and Statistics. </title> <publisher> Academic Press. </publisher>
Reference-contexts: ( 1 (x); 2 (x); : : : ) but that the necessary computations can be carried out efficiently due to Mercer's theorem has been used in some other contexts, for example in the method of potential functions (due to Aizerman, Braverman and Rozoner, 1964) and in support vector machines <ref> (Vapnik, 1995) </ref>. In support vector regression the prior over functions is as described above, but instead of using a squared error loss function (which corresponds to Gaussian noise), a modified version of the l 1 error metric jt i y i j is used, called the *-insensitive loss function. <p> Finding the maximum a posteriori (or MAP) y-values for the training points and test point can now be achieved using quadratic programming <ref> (see Vapnik, 1995 for details) </ref>. 3 Technically splines require generalized covariance functions (see Cressie x5.4), and they have a power-law spectral density S (!) / ! fi with fi &gt; 0.
Reference: <author> Wahba, G. </author> <year> (1990). </year> <title> Spline Models for Observational Data. </title> <booktitle> Society for Industrial and Applied Mathematics. CBMS-NSF Regional Conference series in applied mathematics. </booktitle>
Reference: <author> Whittle, P. </author> <year> (1963). </year> <title> Prediction and regulation by linear least-square methods. </title> <publisher> English Universities Press. </publisher>
Reference: <author> Williams, C. K. I. </author> <year> (1997a). </year> <title> Computation with infinite neural networks. </title> <note> Submitted to Neural Computation. </note>
Reference: <author> Williams, C. K. I. </author> <year> (1997b). </year> <title> Computing with infinite networks. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Williams, C. K. I. and C. E. </author> <title> Rasmussen (1996). Gaussian processes for regression. </title> <editor> In D. </editor> <publisher> S. </publisher>
Reference-contexts: Gaussian process priors have the advantage over neural networks that at least the lowest level of a Bayesian hierarchical model can be treated analytically. Recent work <ref> (Williams and Rasmussen, 1996, inspired by observations in Neal, 1996) </ref> has extended the use of these priors to higher dimensional problems that have been traditionally tackled with other techniques such as neural networks, decision trees etc and has shown that good results can be obtained.
Reference: <editor> Touretzky, M. C. Mozer, and M. E. Hasselmo (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pp. 514-520. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Wong, E. </author> <year> (1971). </year> <title> Stochastic Processes in Information and Dynamical Systems. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Wahba has been influential in promoting the use of spline 1 In fact Bochner's theorem <ref> (see, e.g. Wong, 1971) </ref> states that the positive definite functions C (h) which are continuous at 0 and satisfy C (0) = 1 are exactly the characteristic functions. 2 For stationary covariance functions the spectral density is the Fourier transform of the covariance function. <p> The eigenfunctions are orthogonal and can be chosen to be normalized so that R R i (x) j (x) dx = ffi ij where ffi ij is the Kronecker delta. Mercer's theorem <ref> (see, e.g. Wong, 1971) </ref> states that the covariance function can be expressed as C (x; x 0 ) = i=1 This decomposition is just the infinite-dimensional analogue of the diagonalization of a real symmetric matrix.
Reference: <author> Zhu, H. and R. </author> <title> Rohwer (1996). Bayesian Regression Filters and the Issue of Priors. Neural Computing and Applications 4, 130-142. PREDICTION WITH GAUSSIAN PROCESSES: FROM LINEAR REGRESSION TO LINEAR PREDICTION AND BEYOND 19 Zhu, </title> <editor> H., C. K. I. Williams, R. J. Rohwer, and M. </editor> <month> Morciniec </month> <year> (1997). </year> <title> Gaussian Regression and Optimal Finite Dimensional Linear Models. </title> <type> Technical Report NCRG/97/011, </type> <institution> Aston University, UK. </institution> <note> Available from http://www.ncrg.aston.ac.uk/Papers/ </note> .
References-found: 45

