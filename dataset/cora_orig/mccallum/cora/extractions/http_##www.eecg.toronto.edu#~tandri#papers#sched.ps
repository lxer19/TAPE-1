URL: http://www.eecg.toronto.edu/~tandri/papers/sched.ps
Refering-URL: http://www.cs.toronto.edu/~tandri/
Root-URL: 
Email: Email: hui@csri.toronto.edu  
Phone: Phone: (416)  
Title: Locality and Loop Scheduling on NUMA Multiprocessors  
Author: Hui Li, Sudarsan Tandri, Michael Stumm, and Kenneth C. Sevcik 
Keyword: Key Words: Locality, Loop Scheduling, NUMA Multiprocessors, Data Partitioning, Locality-based Dynamic Scheduling.  
Address: Toronto ON M5S 1A4 CANADA  978-6986  
Affiliation: Computer Science Research Institute University of Toronto  
Abstract: An important issue in the parallel execution of loops is how to partition and schedule the loops onto the available processors. While most existing dynamic scheduling algorithms manage load imbalances well, they fail to take locality into account and therefore perform poorly on parallel systems with non-uniform memory access times. In this paper, we propose a new loop scheduling algorithm, Locality-based Dynamic Scheduling (LDS), that exploits locality, and dynamically balances the load. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gordon Bell. </author> <title> Ultracomputers: A teraflop before its time. </title> <journal> CACM, </journal> <volume> 35(8) </volume> <pages> 27-47, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Section 3 describes data locality and why it is an important factor that cannot be neglected in loop scheduling algorithms. In particular, we argue that data locality is important even in systems with hardware-based cache coherence and in cache-only-memory architectures (COMA), such as the KSR <ref> [1] </ref>. The locality based dynamic scheduling (LDS) algorithm we propose in this paper is described in Section 4, and compared against the affinity scheduling algorithm developed at the University of Rochester, the only other loop scheduling algorithm we are aware of that also takes memory access locality into consideration.
Reference: [2] <author> William J. Bolosky, Michael L.Scott, Robert P. Fitzgerald, Robert J. Fowler, and Alan L. Cox. </author> <title> NUMA policies and their relation to memory architecture. </title> <booktitle> In ASPLOS-IV Proc-cedings, </booktitle> <pages> pages 212-221, </pages> <month> April </month> <year> 1991. </year> <month> 19 </month>
Reference-contexts: these 6 Architecture Cache Local Memory Remote Memory Hector 1 10 24 DASH 1 22 61 RP3 1 10 15 Table 2: Latency for memory read operation in processor clocks systems having most of the accessed data local to the accessing processor can be a major factor in improving performance <ref> [2, 3, 12] </ref>. In parallelizing a loop, it is important to consider the partitioning of both the data space and the loop iteration space, and how both are mapped onto the processors. For good performance, it is essential that the loop partitions and scheduling match the data partitions.
Reference: [3] <author> Stephen Curran and Michael Stumm. </author> <title> A comparison of basic CPU scheduling algorithms for multiprocessor Unix. </title> <journal> Computing Systems, </journal> <volume> 3(4), </volume> <month> Fall </month> <year> 1990. </year>
Reference-contexts: these 6 Architecture Cache Local Memory Remote Memory Hector 1 10 24 DASH 1 22 61 RP3 1 10 15 Table 2: Latency for memory read operation in processor clocks systems having most of the accessed data local to the accessing processor can be a major factor in improving performance <ref> [2, 3, 12] </ref>. In parallelizing a loop, it is important to consider the partitioning of both the data space and the loop iteration space, and how both are mapped onto the processors. For good performance, it is essential that the loop partitions and scheduling match the data partitions.
Reference: [4] <author> Benjamin Gamsa. </author> <title> Region-oriented main memory management in shared-memory NUMA multiprocessors. </title> <type> Master's thesis, </type> <institution> Depatrment of Computer Science, University of Toronto, Toronto, CANADA, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Hector provides a single global physical address space; each memory module contains one portion of the global memory. Access time to memory is a function of memory hierarchy. Cache coherence is maintained by software at cache line granularity, based on a region-oriented memory management scheme <ref> [4] </ref>. Figures 5-8 show the response times of the four applications listed above when run with the different scheduling policies. 14 15 In matrix multiplication, all processes must access all of the matrix B.
Reference: [5] <author> Susan F. Hummel, Edith Schonberg, and Lawrence E. Flynn. </author> <title> Factoring: A method for scheduling parallel loops. </title> <journal> CACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: For example, self scheduling [6] partitions the loop into fixed size chunks, which are conceptually organized in a single system-wide queue, and each processor obtains a new chunk from the queue when it has completed its previous chunk. More recent proposals, such as guided self scheduling (GSS)[11], factoring <ref> [5] </ref>, and trapezoid [14], vary the size of the chunks; they start with large chunks in order to reduce the overhead in accessing the central queue and then progressively use smaller chunks in order to maintain good load balance. These scheduling algorithms are described in detail in Section 2. <p> When the execution time of the iterations differ, it is possible that an early subtask could be so large that it does not complete by the time all other subtasks have completed <ref> [5] </ref>; this load imbalance problem is addressed with the factoring algorithm. <p> The GSS, factoring and trapezoid algorithms are described in Section 2. LDS is described in Section 4. to the number of remaining iterations at the beginning of these allocations <ref> [5] </ref>. Hence P consecutive subtasks will be of the same size, before the granularity is decreased. If the variance of the amount of computation performed by each iteration is large, then factoring performs better than GSS [5]. <p> Section 4. to the number of remaining iterations at the beginning of these allocations <ref> [5] </ref>. Hence P consecutive subtasks will be of the same size, before the granularity is decreased. If the variance of the amount of computation performed by each iteration is large, then factoring performs better than GSS [5]. The Trapezoid method also assigns a decreasing number of iterations to subtasks and thus is a variation of GSS. In this case, however, the subtask size decreases linearly instead of exponentially [14]. <p> For this reason, and because the load in this computation is well balanced, all of the scheduling algorithms perform equally well, as shown in Figure 5. Our results for matrix multiplication differ from those of a similar experiment performed on the RP3 by Hummel et al. <ref> [5] </ref>. In our case, static partitioning, namely cyclic-D, performs marginally better than the other scheduling algorithms. Hummel's results indicate that static partitioning performs far worse than the dynamic schemes.
Reference: [6] <author> Clyde P. Kruskal and Alan Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> SE-11(10):1001-1016, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: Dynamic scheduling algorithms, on the other hand, assign the loop partitions at run time, depending on the speed and progress of the processors. For example, self scheduling <ref> [6] </ref> partitions the loop into fixed size chunks, which are conceptually organized in a single system-wide queue, and each processor obtains a new chunk from the queue when it has completed its previous chunk. <p> Self scheduling, guided self scheduling, factoring, and the trapezoid method are examples of dynamic scheduling algorithms. Self scheduling partitions the loops into subtasks containing one or more iterations <ref> [6] </ref>. Each processor then continuously allocates and executes one subtask at a time until no subtasks are left for processing. If the number of iterations per subtask is fixed and greater than one, then this scheduling strategy is generally referred to as fixed-size chunking [6]. <p> subtasks containing one or more iterations <ref> [6] </ref>. Each processor then continuously allocates and executes one subtask at a time until no subtasks are left for processing. If the number of iterations per subtask is fixed and greater than one, then this scheduling strategy is generally referred to as fixed-size chunking [6]. With fixed-size chunking it can be difficult to choose the correct granularity. Small granularity increases the overhead of accessing the data structure controlling the subtasks that still need to be executed. Larger granularity can lead to load imbalances when the last set of subtasks is being executed.
Reference: [7] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: However, many of the more modern, especially scalable, shared memory multiprocessors have non-uniform memory access (NUMA) cost; i.e. the cost of accessing memory increases with the distance between the accessing processor and the target memory. Examples of multiprocessors with non-uniform memory access costs include DASH <ref> [7] </ref>, Hector [15], BBN [13], RP3 [8], and Cedar [9]. In these systems, data locality is obviously important for good application performance, and for this reason it is important that loop scheduling algo 2 rithms also take data locality into account.
Reference: [8] <author> Jack G. Lipovski and Miroslaw Malek. </author> <title> Parallel Computing: Theory and Comparisons, </title>
Reference-contexts: Examples of multiprocessors with non-uniform memory access costs include DASH [7], Hector [15], BBN [13], RP3 <ref> [8] </ref>, and Cedar [9]. In these systems, data locality is obviously important for good application performance, and for this reason it is important that loop scheduling algo 2 rithms also take data locality into account.
References-found: 8

