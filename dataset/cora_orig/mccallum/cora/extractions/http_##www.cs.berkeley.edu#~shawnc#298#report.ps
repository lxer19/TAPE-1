URL: http://www.cs.berkeley.edu/~shawnc/298/report.ps
Refering-URL: http://www.cs.berkeley.edu/~shawnc/298/
Root-URL: http://www.cs.berkeley.edu
Email: &lt;shawnc@cs.berkeley.edu&gt;  
Title: A Survey on Reinforcement Learning in Global Optimization  
Author: Shuangyu Shawn Chang 
Date: 27 March 1998  
Abstract: This report surveys recent development on the global combinatorial optimization using reinforcement learning methods. It introduces the general background of combinatorial optimization problems and reinforcement learning techniques, describes observations and previous works in this area, and focuses on Boyan and Moore's recent work, the STAGE algorithm with the assistant of reinforcement learning.
Abstract-found: 1
Intro-found: 1
Reference: [Bertsekas and Tsitsiklis 96] <author> D. Bertsekas and J. Tsitsiklis. </author> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1996. </year>
Reference-contexts: However, for state space of very large dimension, the value function has to be generalized with a value function approximator, which takes a vector of encoded state features and produces an approximation of the state value. This technique is referred to as Neural-Dynamic Programming <ref> [Bertsekas and Tsitsiklis 96] </ref>, or Value Function Approximation [Boyan 96]. Popular methods for learning value function approximator include Q-learning, temporal difference learning (TD), etc. These methods are based on the Bellman equations, and sample state space trajectories, obtained either from training samples or simulations.
Reference: [Boese et al. 94] <author> Boese, K. D.; Kahng, A. B.; and Muddu, S. </author> <year> 1994. </year> <title> A new adaptive multi-start technique for combinatorial global optimizations. </title> <journal> Operations Research Letters 16 </journal> <pages> 101-113. </pages>
Reference-contexts: Previous works by Boese, Kahng, and Muddu <ref> [Boese et al. 94] </ref>, have showed that in some combinatorial global optimization domains, there exist some apparent global geometric structure in the optimization cost surfaces.
Reference: [Boyan and Moore 95] <author> Boyan, J. A., and A. W. Moore, </author> <title> "Generalization in Reinforcement Learning: Safely Approximating the Value Function." </title> <editor> In Tesauro, G., D. S. Touret-zky, and T. K. Leen (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: These methods are based on the Bellman equations, and sample state space trajectories, obtained either from training samples or simulations. In these methods, value function approximators are usually updated all over the state space. Boyan and Moore had presented two algorithms, Grow-Support <ref> [Boyan and Moore 95] </ref> and ROUT [Boyan and Moore 96] in the large acyclic domain, as variants to existing Dijkstra's and DAG-SP algorithms. The Grow-Support algorithm uses roll-outs, rather than one-step backups, to assign training values and to keep inaccurate states out of the training set.
Reference: [Boyan 96] <author> Boyan, J. A. </author> <title> "Learning Evaluation Functions." </title> <type> Ph.D. Thesis Proposal, CMU, </type> <month> June </month> <year> 1996. </year>
Reference-contexts: Learning a good evaluation function becomes essential in combinatorial optimization, and two families of learning algorithms are considered: meta-optimization fl EE298-14 Neural Dynamic Programming Class Project, Spring 1998, Prof. John N. Tsit siklis 1 and reinforcement learning <ref> [Boyan 96] </ref>. Meta-optimization is to assume a fixed parametric form for the evaluation function and optimize it directly. <p> This technique is referred to as Neural-Dynamic Programming [Bertsekas and Tsitsiklis 96], or Value Function Approximation <ref> [Boyan 96] </ref>. Popular methods for learning value function approximator include Q-learning, temporal difference learning (TD), etc. These methods are based on the Bellman equations, and sample state space trajectories, obtained either from training samples or simulations. In these methods, value function approximators are usually updated all over the state space.
Reference: [Boyan and Moore 96] <author> Boyan, J. A. and A. W. Moore. </author> <title> "Learning Evaluation Functions for Large Acyclic Domains." </title> <editor> In L. Saitta (ed.), </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: These methods are based on the Bellman equations, and sample state space trajectories, obtained either from training samples or simulations. In these methods, value function approximators are usually updated all over the state space. Boyan and Moore had presented two algorithms, Grow-Support [Boyan and Moore 95] and ROUT <ref> [Boyan and Moore 96] </ref> in the large acyclic domain, as variants to existing Dijkstra's and DAG-SP algorithms. The Grow-Support algorithm uses roll-outs, rather than one-step backups, to assign training values and to keep inaccurate states out of the training set.
Reference: [Boyan and Moore 97] <author> Boyan, J. A. and A. W. Moore. </author> <title> "Using Prediction to Improve Combinatorial Optimization Search." </title> <booktitle> Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <year> 1997. </year>
Reference-contexts: value functions using much less data than TD () for several problems in the large acyclic domain. 3 Learning Predictive Evaluation Function for Global Optimization This section describes STAGE, a more recent algorithm specifically for global optimization problems using reinforcement learning techniques, by Boyan and Moore [Boyan and Moore 98] <ref> [Boyan and Moore 97] </ref>. 3.1 Observations and Previous Works In the NASA space shuttle payload planning problem, Zhang and Dietterich [Zhang and Ditterich 95b] demonstrated successes in applying TD () to train a neural network to learn a heuristic evaluation function over states. <p> When a local minimum for both the original objective function and the new 4 evaluation function is reached, STAGE has to reset the search to a random start-ing point. A possible alternative and extension is to define more levels of evaluation functions, and apply STAGE recursively to itself <ref> [Boyan and Moore 97] </ref>. This idea needs further exploration. 3.3 Applications Boyan and Moore [Boyan and Moore 98] reported a variety of experimental results of STAGE, and comparisons with other traditional search strategies, such as variants of hill-climbing, simulated annealing, WALKSAT, etc.
Reference: [Boyan and Moore 98] <author> Boyan, J. A. and A. W. Moore. </author> <title> "Learning Evaluation Functions for Global Optimization and Boolean Satisfiability." </title> <booktitle> Fifteenth National Conference on Artificial Intelligence (AAAI), 1998 (to appear). </booktitle> <pages> 6 </pages>
Reference-contexts: In addition to providing a good measure for the utilities of a state (directly related to the objective function), an evaluation function should also give some hint on which states predictably lead to good states using some local search algorithm <ref> [Boyan and Moore 98] </ref>. Learning a good evaluation function becomes essential in combinatorial optimization, and two families of learning algorithms are considered: meta-optimization fl EE298-14 Neural Dynamic Programming Class Project, Spring 1998, Prof. John N. Tsit siklis 1 and reinforcement learning [Boyan 96]. <p> that ROUT learned excellent value functions using much less data than TD () for several problems in the large acyclic domain. 3 Learning Predictive Evaluation Function for Global Optimization This section describes STAGE, a more recent algorithm specifically for global optimization problems using reinforcement learning techniques, by Boyan and Moore <ref> [Boyan and Moore 98] </ref> [Boyan and Moore 97]. 3.1 Observations and Previous Works In the NASA space shuttle payload planning problem, Zhang and Dietterich [Zhang and Ditterich 95b] demonstrated successes in applying TD () to train a neural network to learn a heuristic evaluation function over states. <p> The smoothing may remove some local minima. However, in STAGE, Boyan and Moore <ref> [Boyan and Moore 98] </ref> rebutted the hypothesis that the success of the STAGE depends on the smoothing effect on the objective function with empirical evidence. This argument remains to be further investigated. <p> To guarantee convergence, STAGE requires the search algorithm to be proper (terminates with probability one) and behave as a Markov chain (although one of the experiments <ref> [Boyan and Moore 98] </ref> on boolean formula satisfiability showed that learning converges despite the process is not Markovian). <p> A possible alternative and extension is to define more levels of evaluation functions, and apply STAGE recursively to itself [Boyan and Moore 97]. This idea needs further exploration. 3.3 Applications Boyan and Moore <ref> [Boyan and Moore 98] </ref> reported a variety of experimental results of STAGE, and comparisons with other traditional search strategies, such as variants of hill-climbing, simulated annealing, WALKSAT, etc. Successful applications of STAGE include bin-packing, VLSI channel routing, Bayesian network structure learning, radiotherapy treatment planning, cartogram design, and boolean formula satisfiability.
Reference: [McAllester et al. 97] <author> McAllester, D.; Kautz, H.; and Selman, B. </author> <year> 1997. </year> <title> Evidence for invariants in local search. </title> <booktitle> In Proceedings of AAAI-97. </booktitle>
Reference-contexts: However, they did not use machine learning method in finding new starting state, but used a hand-built domain-specific routine for this task. This idea of using extra features in the evaluation functions to guide local search can be extended to include the recent works by McAllester, Selman, and Kautz <ref> [McAllester et al. 97] </ref>. They found two useful invariants, noise level invariant, and ratio invariant of objective function's mean to its variance, across many local search strategies in some optimization problems, such as boolean formula satisfiability.
Reference: [Moore and Schneider 96] <author> A. W. Moore and J. Schneider, </author> <title> Memory-based Stochastic Optimization, </title> <address> NIPS-95, </address> <year> 1995. </year>
Reference-contexts: Meta-optimization is to assume a fixed parametric form for the evaluation function and optimize it directly. This approach does not take advantage of the dynamic programming principles, and will not be the focus of this survey; nevertheless, it should be noted that recent advances in local optimization <ref> [Moore and Schneider 96] </ref> may help make them superior to reinforcement learning in practice in some domains.
Reference: [Zhang and Ditterich 95a] <author> W. Zhang and T. Ditterich. </author> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: An one-step lookahead search procedure is carried out based on the learned evaluation function. They also provided possible explanations <ref> [Zhang and Ditterich 95a] </ref> why TD () with value function approximation is suitable for certain problems such as job-shop scheduling and Tesauro's backgammon system.
Reference: [Zhang and Ditterich 96] <author> Zhang, W., Dietterich, T. G., </author> <year> (1996). </year> <title> High-Performance Job-Shop Scheduling With A Time-Delay TD() Network. </title> <editor> D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo (Eds.) </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 8, </volume> <pages> 1024-1030. </pages>
References-found: 11

