URL: ftp://ftp.cag.lcs.mit.edu/pub/raw/documents/st-scheduling.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/~barua/papers/index.html
Root-URL: 
Email: fwalt,barua,chinnama,jbabbg@lcs.mit.edu fvivek,samang@lcs.mit.edu  
Title: Space-Time Scheduling of Instruction-Level Parallelism on a Raw Machine  
Author: Walter Lee, Rajeev Barua, Devabhaktuni Srikrishna, Jonathan Babb, Vivek Sarkar, Saman Amarasinghe, 
Web: http://cag-www.lcs.mit.edu/raw  
Address: Cambridge, MA 02139, U.S.A.  
Affiliation: M.I.T. Laboratory for Computer Science  
Abstract: Advances in VLSI technology will enable chips with over a billion transistors within the next decade. Unfortunately, the centralized-resource architectures of modern microprocessors are ill-suited to exploit such advances. Achieving a high level of parallelism at a reasonable clock speed requires distributing the processor resources a trend already visible in the dual-register-file architecture of the Alpha 21264. A Raw microprocessor takes an extreme position in this space by distributing all its resources such as instruction streams, register files, memory ports, and ALUs over a pipelined two-dimensional interconnect, and exposing them fully to the compiler. Compilation for instruction-level parallelism (ILP) on such distributed-resource machines requires both spatial instruction scheduling and traditional temporal instruction scheduling. This paper describes the techniques used by the Raw compiler to handle these issues. Preliminary results from a SUIF-based compiler for sequential programs written in C and Fortran indicate that the Raw approach to exploiting ILP can achieve speedups scalable with the number of processors for applications with such parallelism. The Raw architecture attempts to provide performance that is at least comparable to that provided by scaling an existing architecture, but that can achieve orders of magnitude improvement in performance for applications with a large amount of parallelism. This paper offers some positive results in this direction. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amarasingle, J. Anderson, C. Wilson, S. Liao, B. Murphy, R. French, and M. Lam. </author> <title> Multiprocessors from a Software Perspective. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 5261, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The size of the datasets in these benchmarks is intentionally made to be small to feature the low communication overhead of Raw. Traditional multiprocessors, with their high overheads, would be unable to attain speedup for such datasets <ref> [1] </ref>. Most of the speedup attained can be attributed to the exploitation of ILP, but unrolling plays a beneficiary role as well. Unrolling speeds up a program by reducing its loop overhead and exposing scalar optimizations across loop iterations.
Reference: [2] <author> D. August, W. mei Hwu, and S. Mahlke. </author> <title> A Framework for Balancing Control Flow and Predication. </title> <booktitle> In Proceedings of the 30th International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: We intend to conduct a comprehensive study of this issue in the future. It is worthwhile to compare predicated execution to control localization. In terms of flexibility, predicated execution requires fine-grained if-conversion <ref> [2] </ref>, while control localization can support coarse-grained if-conversion. In terms of execution cost, predicated execution incurs the cost of executing all paths; an instruction which is nullified still occupies a unit of execution resources. <p> Further, an LC-VLIW machine assumes a single VLIW instruction stream, while Raw is multisequential. Many ILP-enhancing techniques have been developed to increase the amount of parallelism available within a basic block. These techniques include control speculation [9], data speculation [16], trace/superblock scheduling [7] [13], and predicated execution <ref> [2] </ref>. Several characteristics of Raw affect the application of these techniques. In Raw, a global branch is more costly relative to other instructions because it requires an explicit broadcast followed by local branches. This cost is reflected in the side exits of traces.
Reference: [3] <author> J. Babb, M. Frank, V. Lee, E. Waingold, R. Barua, M. Taylor, J. Kim, S. Devabhaktuni, and A. Agarwal. </author> <title> The raw benchmark suite: Computation structures for general purpose computing. </title> <booktitle> In IEEE Symposium on Field-Programmable Custom Computing Machines, </booktitle> <address> Napa Valley, CA, </address> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: Speedup compares the run-time of the RAWCC-compiled code versus the run-time of the code generated by the Machsuif MIPS compiler. The benchmarks we select include programs from the Raw benchmark suite <ref> [3] </ref>, program kernels from the nasa7 benchmark of Spec92, tomcatv of Spec92, and the kernel basic block which accounts for 50% of the run-time in fpppp of Spec92.
Reference: [4] <author> J. Babb, R. Tessier, M. Dahl, S. Hanono, D. Hoki, and A. Agarwal. </author> <title> Logic emulation with virtual wires. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> 16(6):609626, </volume> <month> June </month> <year> 1997. </year>
Reference-contexts: The Raw task partitioning and scheduling problem is similar, with tasks defined to be individual instructions. The communication scheduling problem has been studied in the context of FPGA routing. For example, a technique called VirtualWires <ref> [4] </ref> alleviates the pin limitation of FPGAs by multiplexing each pin to communicate more than one values. Communication events are scheduled on pins over time.
Reference: [5] <author> A. Capitanio, N. Dutt, and A. Nicolau. </author> <title> Partitioned Register Files for VLIWs: A Preliminary Analysis of Tradeoffs. </title> <booktitle> In Proceedings of the 25th International Symposium on Microarchitecture, </booktitle> <year> 1992. </year> <month> 19 </month>
Reference-contexts: Compilation for some other kinds of clustered VLIW architectures have also been considered in past work. For example, a brief description of compilation for an LC-VLIW (Limited Connectivity VLIW) architecture can be found in <ref> [5] </ref>. This approach includes partitioning of instructions across clusters as well as insertion of explicit inter-cluster register-to-register data movement instructions. However, there are significant differences between the LC-VLIW machine model and a Raw machine.
Reference: [6] <author> J. R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <type> In Ph.D Thesis, </type> <institution> Yale University, </institution> <year> 1985. </year>
Reference-contexts: For array references which are affine functions of loop indices, we have developed a technique which uses unrolling to satisfy the static reference property. Our technique is a more fully developed version of the technique used by the Bulldog compiler to perform memory-bank disambiguation <ref> [6] </ref>. A description of this theory is beyond the scope of this paper. This paper focuses on results which can be attained when the Raw compiler succeeds in identifying static references. We do not address the issues pertaining to dynamic references in this paper. <p> The Bulldog compiler <ref> [6] </ref> targets a VLIW machine consisting of clusters of functional units, register files, and memory connected via a partial crossbar. Bulldog adopts a two-step approach for space-time scheduling in which instructions are first mapped spatially and then temporally.
Reference: [7] <author> J. A. Fisher. </author> <title> Trace Scheduling: A Technique for Global Microcode Compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 7(C-30):478490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Further, an LC-VLIW machine assumes a single VLIW instruction stream, while Raw is multisequential. Many ILP-enhancing techniques have been developed to increase the amount of parallelism available within a basic block. These techniques include control speculation [9], data speculation [16], trace/superblock scheduling <ref> [7] </ref> [13], and predicated execution [2]. Several characteristics of Raw affect the application of these techniques. In Raw, a global branch is more costly relative to other instructions because it requires an explicit broadcast followed by local branches. This cost is reflected in the side exits of traces.
Reference: [8] <author> L. Gwennap. </author> <title> Digital 21264 Sets New Standard. </title> <type> Microprocessor Report, </type> <pages> pages 1116, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: An early visible effect of the scalability problem in commercial architectures is apparent in the clustered organization of the Multiflow computer [12]. More recently, the Alpha 21264 <ref> [8] </ref> duplicates its register file to provide the requisite number of ports at a reasonable clock speed. A cluster is formed by organizing half of the functional units and half of the cache ports around each register file. Cross-cluster communication incurs an extra cycle of latency.
Reference: [9] <author> V. Kathail, M. Schlansker, and B. R. Rau. </author> <title> HPL PlayDoh Architecture Specification: </title> <note> Version 1.0. In HP Laboratories Technical Report 93-80, </note> <month> Feb </month> <year> 1994. </year>
Reference-contexts: Further, an LC-VLIW machine assumes a single VLIW instruction stream, while Raw is multisequential. Many ILP-enhancing techniques have been developed to increase the amount of parallelism available within a basic block. These techniques include control speculation <ref> [9] </ref>, data speculation [16], trace/superblock scheduling [7] [13], and predicated execution [2]. Several characteristics of Raw affect the application of these techniques. In Raw, a global branch is more costly relative to other instructions because it requires an explicit broadcast followed by local branches.
Reference: [10] <author> M. S. Lam. </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines. </title> <booktitle> In Proc. Int'l Conf. on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 318328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: On the other hand, control localization obviates the need for predicated execution on Raw: it permits local execution of more general control flow constructs, and it does not require support from the ISA. Control localization is similar to the technique of hierarchical reduction <ref> [10] </ref>. They both share the 18 basic idea of collapsing control constructs into a single abstract node. They differ in application and in details. First, hierarchical reduction is used for software pipelining on VLIWs; control localization is used for general instruction scheduling on the Raw machine.
Reference: [11] <author> M. S. Lam and R. P. Wilson. </author> <title> Limits of Control Flow on Parallelism. </title> <booktitle> In Proceedings of 19th Annual Interna tional Symposium on Computer Architecture, </booktitle> <pages> pages 4657, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In addition to its scalability and simplicity, the Raw machine is an attractive NURA machine for several reasons: * Multisequentiality: Multisequentiality, the presence of multiple flows of control, is useful for four reasons. First, it significantly enhances the potential amount of parallelism a machine can exploit <ref> [11] </ref>. Second, it enables control localization, a technique we introduce in Section 4.1 to allow ILP to be scheduled across branches.
Reference: [12] <author> P. G. Lowney and et al. </author> <title> The Multiflow Trace Scheduling Compiler. </title> <booktitle> In Journal of Supercomputing, </booktitle> <pages> pages 51142, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: An early visible effect of the scalability problem in commercial architectures is apparent in the clustered organization of the Multiflow computer <ref> [12] </ref>. More recently, the Alpha 21264 [8] duplicates its register file to provide the requisite number of ports at a reasonable clock speed. A cluster is formed by organizing half of the functional units and half of the cache ports around each register file. <p> A processor can be composed from replicated processing units whose pipelines are coupled together at the register level so that they can exploit ILP cooperatively. The VLIW Multiflow TRACE machine is a machine which adopts such a solution <ref> [12] </ref>. On the other hand, its main motivation for this organization is to provide enough register ports. Communication between clusters are performed via global busses, which in modern and future-generation technology would severely degrade the clock speed of the machine. <p> There are two major distinctions between a VLIW machine and Raw machine. First, they differ in resource organization. VLIW machines of various degrees of scalability have been proposed, ranging from completely centralized machines to machines with distributed functional units, register files, and 5 memory <ref> [12] </ref>. The Raw machine, on the other hand, is the first ILP microprocessor that provides a scalable, two-dimensional interconnect between clusters of resources. <p> Second, the spatial mapping performed by BUG is driven by a greedy depth-first traversal that maps all instructions in a connected subgraph with a common root before processing the next subgraph. As observed in <ref> [12] </ref>, such a greedy algorithm is often inappropriate for parallel computations such as those obtained by unrolling parallel loops. In contrast, instruction partitioning and placement in RAWCC uses a global priority function that can intermingle instructions from different connected components of the dag.
Reference: [13] <author> W. mei Hwu, S. Mahlke, W. Chen, P. Chang, N. Warter, R. Bringmann, R. Ouellette, R. Hank, T. Kiyohara, G. Haab, J. Holm, and D. Lavery. </author> <title> The Superblock: An Effective Technique for VLIW and Superscalar Compilation. </title> <journal> The Journal of Supercomputing, </journal> <volume> 7(1), </volume> <month> Jan </month> <year> 1993. </year>
Reference-contexts: Further, an LC-VLIW machine assumes a single VLIW instruction stream, while Raw is multisequential. Many ILP-enhancing techniques have been developed to increase the amount of parallelism available within a basic block. These techniques include control speculation [9], data speculation [16], trace/superblock scheduling [7] <ref> [13] </ref>, and predicated execution [2]. Several characteristics of Raw affect the application of these techniques. In Raw, a global branch is more costly relative to other instructions because it requires an explicit broadcast followed by local branches. This cost is reflected in the side exits of traces.
Reference: [14] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> Pitman, London and The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year> <booktitle> In the series, Research Monographs in Parallel and Distributed Computing. </booktitle>
Reference: [15] <author> M. D. Smith. </author> <title> Extending suif for machine-dependent optimizations. </title> <booktitle> In Proceedings of the First SUIF Com piler Workshop, </booktitle> <pages> pages 1425, </pages> <address> Stanford, CA, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: The second phase, the basic block orchestrater, performs the space-time scheduling of ILP on each basic block. It will be discussed in detail below. The final phase generates code for the processors and the switches. It uses the MIPS back-end developed in Machine SUIF <ref> [15] </ref>, with a few modifications to handle the communication instructions and communication registers. 3.3 Basic block orchestrater The basic block orchestrater exploits the ILP within a basic block by distributing the parallelism within the basic block across the tiles. <p> Table 1 gives some basic characteristics of the benchmarks. Speedup We compare results of the Raw compiler with the results of a MIPS compiler provided by Machsuif <ref> [15] </ref> targeted for an R2000. Table 2 shows the speedups attained by the benchmarks for Raw machines of various sizes. Note that these speedups do not measure the advantage Raw is attaining over modern architectures due to a faster clock.
Reference: [16] <author> G. Sohi, S. Breach, and T. Vijaykumar. </author> <title> Multiscalar Processors. </title> <booktitle> In Proceedings of the 22nd Annual Interna tional Symposium on Computer Architecture, </booktitle> <pages> pages 414425, </pages> <year> 1995. </year>
Reference-contexts: Communication within a cluster occurs at normal speed, while communication across clusters takes an additional cycle. This example suggests an evolutionary path that resolves the scalability problem: impose a hierarchy on the organization of hardware resources <ref> [16] </ref>. A processor can be composed from replicated processing units whose pipelines are coupled together at the register level so that they can exploit ILP cooperatively. The VLIW Multiflow TRACE machine is a machine which adopts such a solution [12]. <p> Further, an LC-VLIW machine assumes a single VLIW instruction stream, while Raw is multisequential. Many ILP-enhancing techniques have been developed to increase the amount of parallelism available within a basic block. These techniques include control speculation [9], data speculation <ref> [16] </ref>, trace/superblock scheduling [7] [13], and predicated execution [2]. Several characteristics of Raw affect the application of these techniques. In Raw, a global branch is more costly relative to other instructions because it requires an explicit broadcast followed by local branches.
Reference: [17] <author> E. Waingold, M. Taylor, V. Sarkar, W. Lee, V. Lee, J. Kim, M. Frank, P. Finch, S. Devabhaktuni, R. Barua, J. Babb, S. Amarasinghe, and A. Agarwal. </author> <title> Baring It All To Software: Raw Machines. </title> <booktitle> Computer, </booktitle> <pages> pages 8693, </pages> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: In particular, register or memory access by a functional unit will have a gradation of access time. This fundamental change in processor model will necessitate a corresponding change in compiler technology. Instruction scheduling becomes a spatial problem as well as a temporal problem. The Raw machine <ref> [17] </ref> is a scalable microprocessor architecture with non-uniform register access latencies (NURA). As such, its compilation problem is similar to that which will be encountered by extrapolations of existing architectures. <p> Instruction scheduling becomes a spatial problem as well as a temporal problem. This extra dimension requires compilation techniques beyond that which are used to exploit ILP on modern machines. Raw architecture The Raw machine <ref> [17] </ref> is a NURA architecture motivated by the need to design simple and highly scalable processors. As depicted in Figure 1, a Raw machine comprises a simple, replicated tile, each with its own instruction stream, and a programmable, tightly integrated interconnect between tiles.
Reference: [18] <author> R. Wilson and et al. </author> <title> SUIF: A Parallelizing and Optimizing Research Compiler. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(12):31 37, </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: We do not address the issues pertaining to dynamic references in this paper. However, we observe in Section 5 that decoupled instruction streams allow the Raw machine to tolerate timing variations due to events such as dynamic memory accesses. 3.2 RAWCC RAWCC, the Raw compiler, is implemented using SUIF <ref> [18] </ref>, the Stanford University Intermediate Format. It compiles both C and Fortran programs. The Raw compiler consists of three phases.
Reference: [19] <author> T. Yang and A. Gerasoulis. </author> <title> DSC: Scheduling parallel tasks on an unbounded number of processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(9):951967, </volume> <year> 1994. </year>
Reference-contexts: Subsequent phases guarantee that instructions with no mapping constraints in the same cluster will be mapped to the same tile. The clustering technique approximates communication cost by assuming an idealized fully-connected switch with uniform latency. RAWCC employs a greedy, critical path based technique called Dominant Sequent Clustering <ref> [19] </ref>. Initially, each instruction node belongs to a unit cluster. Communication between clusters is assigned a uniform cost. The algorithm visits instruction nodes in topological order. At each step, it selects from the list of candidates the instruction on the longest execution path.
References-found: 19

