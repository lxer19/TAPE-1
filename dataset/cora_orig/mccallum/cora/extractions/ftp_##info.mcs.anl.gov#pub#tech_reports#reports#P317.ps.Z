URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P317.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts92.htm
Root-URL: http://www.mcs.anl.gov
Title: ADIFOR: A FORTRAN SYSTEM FOR PORTABLE AUTOMATIC DIFFERENTIATION  
Author: Christian Bischof Andreas Griewank 
Note: Preprint MCS-P317-0792  
Address: Argonne, Illinois 60439  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: Automatic differentiation provides the foundation for sensitivity analysis and subsequent design optimization of complex systems by reliably computing derivatives of large computer codes, with the potential of doing it many times faster compared to current approaches. This paper describes the ADIFOR (Automatic DIfferentiation of FORtran) system, a translator that augments Fortran programs with statements for the computation of derivatives. ADIFOR accepts arbitrary Fortran 77 code defining the computation of a function and writes portable Fortran 77 code for the computation of its derivatives. Our goal is to free the computational scientist from worrying about the accurate and efficient computation of derivatives, even for complicated "functions", thereby enabling him to concentrate on the more important issues of system modeling and algorithm design. This paper gives an overview of the principles underlying the ADIFOR system, and comments on the power of automatic differentiation for computing derivatives of implicitly-defined functions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Christian Bischof, Alan Carle, George Corliss, and Andreas Griewank. ADIFOR: </author> <title> automatic differentiation in a source translator environment. </title> <note> ADI-FOR Working Note #5, </note> <institution> MCS-P288-0192, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year> <note> Accepted for publication in Proceedings of International Symposium on Symbolic and Algebraic Computation. </note>
Reference-contexts: Recently, however, process towards a general-purpose automatic differentiation tool competitive with divided differences has been made with the development of ADIFOR (Automatic Differentiation in Fortran) <ref> [2, 5, 3, 1] </ref>. ADIFOR provides automatic differentiation for programs written in Fortran 77.
Reference: [2] <author> Christian Bischof, Alan Carle, George Corliss, An-dreas Griewank, and Paul Hovland. Adifor: </author> <title> Generating derivative codes from Fortran programs. </title> <note> ADIFOR Working Note #1, </note> <institution> MCS-P263-0991, Mathematics and Computer Science Division, Ar-gonne National Laboratory, </institution> <year> 1991. </year> <note> To appear in Scientific Programming. </note>
Reference-contexts: in computing w, but in reverse order: if s = f (t), then tbar += sbar * (df / dt) if s = f (t,u), then tbar += sbar * (df /dt) ubar += sbar * (df /du) Using this simple recipe (see [15, 23]), and applying straightforward optimizations (see <ref> [2] </ref> for a more detailed description), we generate the so-called adjoint code shown in Figure 2 for computing w and its derivatives. <p> Recently, however, process towards a general-purpose automatic differentiation tool competitive with divided differences has been made with the development of ADIFOR (Automatic Differentiation in Fortran) <ref> [2, 5, 3, 1] </ref>. ADIFOR provides automatic differentiation for programs written in Fortran 77.
Reference: [3] <author> Christian Bischof, George Corliss, and Andreas Griewank. </author> <title> ADIFOR exception handling. </title> <note> ADI-FOR Working Note #3, </note> <institution> MCS-TM-159, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Recently, however, process towards a general-purpose automatic differentiation tool competitive with divided differences has been made with the development of ADIFOR (Automatic Differentiation in Fortran) <ref> [2, 5, 3, 1] </ref>. ADIFOR provides automatic differentiation for programs written in Fortran 77.
Reference: [4] <author> Christian Bischof, George Corliss, and Andreas Griewank. </author> <title> Computing second- and higher-order derivatives through univariate Taylor series. </title> <note> ADI-FOR Working Note #6, </note> <institution> MCS-P296-0392, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference: [5] <author> Christian Bischof and Paul Hovland. </author> <title> Using ADI-FOR to compute dense and sparse Jacobians. </title> <note> AD-IFOR Working Note #2, </note> <institution> MCS-TM-158, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Recently, however, process towards a general-purpose automatic differentiation tool competitive with divided differences has been made with the development of ADIFOR (Automatic Differentiation in Fortran) <ref> [2, 5, 3, 1] </ref>. ADIFOR provides automatic differentiation for programs written in Fortran 77.
Reference: [6] <author> Christian H. Bischof, Alan Carle, George Corliss, Andreas Griewank, Paul Hovland, and Moe El-Khadiri. </author> <title> Getting started with adifor. </title> <note> ADIFOR Working Note #9, </note> <institution> ANL-MCS-TM-164, Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Another aspect that we have to address is that of input and output. ADIFOR does not yet properly handle file input and output (although there are ways to get around this <ref> [6] </ref>). The reason is that we cannot trace data dependencies through I/O statements. However, in many MDO projects the codes modeling the various disciplines have been developed and are maintained by distinct groups on different platform.
Reference: [7] <author> D. Callahan, K. Cooper, R. T. Hood, Ken Kennedy, and Linda M. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(4), </volume> <month> De-cember </month> <year> 1988. </year>
Reference: [8] <author> Alan Carle, Keith D. Cooper, Robert T. Hood, Ken Kennedy, Linda Torczon, and Scott K. Warren. </author> <title> A practical environment for scientific programming. </title> <journal> IEEE Computer, </journal> 20(11) 75-89, November 1987. 
Reference: [9] <author> Bruce Christianson. </author> <title> Reverse accumulation and accurate rounding error estimates for taylor series coefficients. </title> <journal> Optimization Methods and Software, </journal> <volume> 1(1) </volume> <pages> 81-94, </pages> <year> 1992. </year>
Reference-contexts: Applying automatic differentiation to the generic iteration of Figure 8, we obtain the iteration shown in dx . While it is natural to do so, this currently has to be done by hand. Gilbert [14] and Christianson <ref> [9] </ref> show that this iteration produces meaningful results for iterations such as Newton's method.
Reference: [10] <author> Bruce D. Christianson. </author> <title> Automatic Hessians by reverse accumulation. </title> <type> Technical Report NOC TR228, </type> <institution> The Numerical Optimisation Center, Hat-field Polytechnic, Hatfield, U.K., </institution> <month> April </month> <year> 1990. </year>
Reference: [11] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Software for estimating sparse Jacobian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10:329 - 345, </volume> <year> 1984. </year>
Reference-contexts: So if S is the identity, ADIFOR computes the full Jacobian, and if S is just a vector, ADIFOR computes the product of the Jacobian by a vector. "Compressed" versions of sparse Jacobians can be computed by exploiting the same graph coloring techniques <ref> [12, 11] </ref> that are used for divided difference approximations of sparse Jacobians. The idea is best understood with an example.
Reference: [12] <author> T. F. Coleman and J. J. </author> <title> More. Estimation of sparse Jacobian matrices and graph coloring problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20:187 - 209, </volume> <year> 1984. </year>
Reference-contexts: So if S is the identity, ADIFOR computes the full Jacobian, and if S is just a vector, ADIFOR computes the product of the Jacobian by a vector. "Compressed" versions of sparse Jacobians can be computed by exploiting the same graph coloring techniques <ref> [12, 11] </ref> that are used for divided difference approximations of sparse Jacobians. The idea is best understood with an example.
Reference: [13] <author> H.M. Elbanna and L.A. Carlson. </author> <title> Determination of aerodynamic sensitivity coefficients in the transonic and supersonic regimes. </title> <booktitle> In Proceedings of the 27th AIAA Aerospace Sciences Meeting, </booktitle> <institution> AIAA Paper 89-0532. American Institute of Aeronautics and Astronautics, </institution> <year> 1989. </year>
Reference-contexts: For example, Figure 11 shows the convergence behavior of the lift coefficient, which is implicitly defined as a function of the maximal airfoil thickness, the mach number, the camber, maximal camber position, and the angle of attack in Elbanna and Carlson's transonic code <ref> [13] </ref> for M = 1:2; ff = 1 with an NACA 1406 airfoil.
Reference: [14] <author> Jean-Charles Gilbert. </author> <title> Automatic differentiation and iterative processes. </title> <journal> Optimization Methods and Software, </journal> <volume> 1(1) </volume> <pages> 13-22, </pages> <year> 1992. </year>
Reference-contexts: Applying automatic differentiation to the generic iteration of Figure 8, we obtain the iteration shown in dx . While it is natural to do so, this currently has to be done by hand. Gilbert <ref> [14] </ref> and Christianson [9] show that this iteration produces meaningful results for iterations such as Newton's method.
Reference: [15] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <editor> In M. Iri and K. Tanabe, editors, </editor> <booktitle> Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83 - 108. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: simple rule to the binary statements executed in computing w, but in reverse order: if s = f (t), then tbar += sbar * (df / dt) if s = f (t,u), then tbar += sbar * (df /dt) ubar += sbar * (df /du) Using this simple recipe (see <ref> [15, 23] </ref>), and applying straightforward optimizations (see [2] for a more detailed description), we generate the so-called adjoint code shown in Figure 2 for computing w and its derivatives.
Reference: [16] <author> Andreas Griewank. </author> <title> Automatic evaluation of first-and higher-derivative vectors. </title> <editor> In R. Seydel, F. W. Schneider, T. Kupper, and H. Troger, editors, </editor> <booktitle> Proceedings of the Conference at Wurzburg, </booktitle> <month> Aug. </month> <year> 1990, </year> <title> Bifurcation and Chaos: Analysis, Algorithms, </title> <journal> Applications, </journal> <volume> volume 97, </volume> <pages> pages 135 - 148. </pages> <publisher> Birkhauser Verlag, </publisher> <address> Basel, Switzerland, </address> <year> 1991. </year>
Reference: [17] <author> Andreas Griewank and George F. Corliss, </author> <title> editors. Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Other applications arise in real-time simulation and control of road vehicles, robots, and macromolecules, parame-ter identification problems in geophysical systems and accelerator beam tracing for the design of optical instruments and particle accelerators <ref> [17] </ref>. There are four approaches to computing derivatives: By Hand: This is error-prone, and applicable only in simple cases.
Reference: [18] <author> Andreas Griewank and Shawn Reese. </author> <title> On the calculation of Jacobian matrices by the Markowitz rule. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 126 - 135. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: The derivatives computed by automatic differentiation are guaranteed to be reliable, unlike those computed by divided difference approximations. Griewank and Reese <ref> [18] </ref> have shown that in the presence of round-off the derivative objects computed by automatic differentiation are the exact result of a nonlinear system whose elementary partial derivatives have been perturbed by factors of at most (1 + ") 2 , where " is the relative machine precision.
Reference: [19] <author> T. L. Holst, M. D. Salas, and R. W. Claus. </author> <title> The NASA computational aerosciences program toward Teraflop computing. </title> <booktitle> In Proceedings of the 30th Aerospace Sciences Meeting, </booktitle> <pages> pages AIAA Paper 92-0558. </pages> <institution> American Institute of Aeronautics and Astronautics, </institution> <year> 1992. </year>
Reference-contexts: For example, a major thrust of the Computational Aerosciences project of NASA's High Performance Computing and Communications Program is multidisciplinary design and optimization of a high speed civil transport <ref> [19] </ref>. A related activity is the HiSAIR (High Speed Airframe Integration Research) project which also has the optimization of a high speed civil transport as a focus and encompasses a greater breadth of technical disciplines influencing the design of such a vehicle.
Reference: [20] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In Andreas Griewank and George Corliss, editors, </editor> <booktitle> Proceedings of the Workshop on Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </booktitle> <address> Philadelphia, </address> <year> 1991. </year> <note> SIAM. To appear. </note>
Reference-contexts: Since then there have been various implementations of auto matic differentiation, and a recent survey can be found in <ref> [20] </ref>. However, for the most part, they were conceived by the need for accurate first- and higher-order derivatives in a certain application.
Reference: [21] <author> V. M. Korivi, A. C. Taylor, P. A. Newman, G. W. Hou, and H. E. Jones. </author> <title> An incremental strategy for calculating consistent discrete CFD sensitivity derivatives. </title> <type> NASA Technical Memorandum 104207, </type> <institution> NASA Langley Research Center, </institution> <month> Febru-ary </month> <year> 1992. </year>
Reference-contexts: Moreover, by judiciously deactivating certain variables one can implement various semi-analytical schemes for sensitivity analysis that have been developed and tested by aeronautical engineers (see, e.g. <ref> [22, 21, 24] </ref>. Again, derivative convergence is not a priori guaranteed, but it can be tested construc tively with little extra effort. 5 Future Work Our goal is to further decrease the complexity of computing derivatives, both in terms of man-hours and cpu-seconds.
Reference: [22] <author> P. A. Newman, G. J.-W. Hou, H. E. Jones, A. C. Taylor, and V. M. Korivi. </author> <title> Observations on computational methodologies for use in large-scale, gradient-based, multidisciplinary design incorporating advanced CFD codes. </title> <type> NASA Technical Memorandum 104206, </type> <institution> NASA Langley Research Center, </institution> <year> 1992. </year>
Reference-contexts: Moreover, by judiciously deactivating certain variables one can implement various semi-analytical schemes for sensitivity analysis that have been developed and tested by aeronautical engineers (see, e.g. <ref> [22, 21, 24] </ref>. Again, derivative convergence is not a priori guaranteed, but it can be tested construc tively with little extra effort. 5 Future Work Our goal is to further decrease the complexity of computing derivatives, both in terms of man-hours and cpu-seconds.
Reference: [23] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Assume that rt contains the derivatives of t with respect to the independent variables x, rt = @ t @ t ! We can propagate those derivatives by using elementary differentiation arithmetic based on the chain rule (see for example <ref> [23] </ref> for more details). <p> simple rule to the binary statements executed in computing w, but in reverse order: if s = f (t), then tbar += sbar * (df / dt) if s = f (t,u), then tbar += sbar * (df /dt) ubar += sbar * (df /du) Using this simple recipe (see <ref> [15, 23] </ref>), and applying straightforward optimizations (see [2] for a more detailed description), we generate the so-called adjoint code shown in Figure 2 for computing w and its derivatives.
Reference: [24] <author> G. R. Shubin. </author> <title> Obtaining "cheap" optimization gradients from computational aerodynamics codes. </title> <institution> Applied Mathematics and Statistics Technical Report AMS-TR-164, Boeing Computer Services, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Moreover, by judiciously deactivating certain variables one can implement various semi-analytical schemes for sensitivity analysis that have been developed and tested by aeronautical engineers (see, e.g. <ref> [22, 21, 24] </ref>. Again, derivative convergence is not a priori guaranteed, but it can be tested construc tively with little extra effort. 5 Future Work Our goal is to further decrease the complexity of computing derivatives, both in terms of man-hours and cpu-seconds.
Reference: [25] <author> G. R. Shubin, A. B. Stephens, H. M. Glaz, A. B. Wardlaw, and L. B. Hackerman. </author> <title> Steady shock tracking, Newton's method, and the supersonic blunt body problem. </title> <journal> SIAM J. on Sci. and Stat. Computing, </journal> <volume> 3(2):127 - 144, </volume> <month> June </month> <year> 1982. </year>
Reference-contexts: The resulting decrease in complexity compared to a straightforward forward mode implementation usually is substantial. For example, the code for the blunt body shock tracking problem by Shubin <ref> [25] </ref> needs to compute the 190 fi 190 Jacobian of a "function" described by 1400 lines of Fortran code. When we execute this code for a particular set of input values, we execute a total of 1840 assignment statements that were augmented with derivative computations.
References-found: 25

