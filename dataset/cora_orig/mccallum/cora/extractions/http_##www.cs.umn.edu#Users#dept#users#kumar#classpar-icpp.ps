URL: http://www.cs.umn.edu/Users/dept/users/kumar/classpar-icpp.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: vsingh@hitachi.com fhan,kumarg@cs.umn.edu  
Title: Parallel Formulations of Decision-Tree Classification Algorithms  
Author: A. Srivastava E. Han V. Kumar V. Singh Anurag Srivastava and Vineet Singh 
Keyword: Data mining, parallel processing, classification, scalability, decision trees.  
Note: A significant part of this work was done while  were at IBM TJ Watson Research Center. This work was supported by NSF grant ASC-9634719, Army Research Office contract DA/DAAH04-95-1-0538, Cray Research Inc. Fellowship, and IBM partnership award, the content of which does not necessarily reflect the policy of the government, and no official endorsement should be inferred. Access to computing facilities was provided by AHPCRC, Minnesota Supercomputer Institute, Cray Research Inc., and NSF grant CDA-9414015.  
Address: anurags@hitachi.com University of Minnesota  
Affiliation: Information Technology Lab Dept. of Computer Science Information Technology Lab Hitachi America, Ltd. Army HPC Research Center Hitachi America, Ltd.  
Abstract: Classification decision tree algorithms are used extensively for data mining in many domains such as retail target marketing, fraud detection, etc. Highly parallel algorithms for constructing classification decision trees are desirable for dealing with large data sets in reasonable amount of time. Algorithms for building classification decision trees have a natural concurrency, but are difficult to parallelize due to the inherent dynamic nature of the computation. In this paper, we present parallel formulations of classification decision tree learning algorithm based on induction. We describe two basic parallel formulations. One is based on Synchronous Tree Construction Approach and the other is based on Partitioned Tree Construction Approach. We discuss the advantages and disadvantages of using these methods and propose a hybrid method that employs the good features of these methods. We also provide the analysis of the cost of computation and communication of the proposed hybrid method. Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 
Abstract-found: 1
Intro-found: 1
Reference: [AIS93] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: We also provide the analysis of the cost of computation and communication of the proposed hybrid method. Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4.5 [Qui93], CDP <ref> [AIS93] </ref>, SLIQ [MAR96], and SPRINT [SAM96] use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g.
Reference: [ARS97] <author> K. Alsabti, S. Ranka, and V. Singh. </author> <title> A one-pass algorithm for accurately estimating quantiles for disk-resident data. </title> <booktitle> In Proc. of the 23rd VLDB Conference, </booktitle> <year> 1997. </year>
Reference-contexts: In this case, the parallel formulations as presented in the previous subsections are directly applicable without any modification. Another approach towards discretization is to discretize at every node in the tree. There are two examples of this approach. The first example can be found in [ARS98] where quantiles <ref> [ARS97] </ref> are used to discretize continuous attributes. The second example of this approach to discretize at each node is SPEC [SSHK97] where a clustering technique is used.
Reference: [ARS98] <author> K. Alsabti, S. Ranka, and V. Singh. </author> <title> CLOUDS: Classification for large or out-of-core datasets. </title> <note> http://www.cise.ufl.edu/ranka/dm.html, 1998. </note>
Reference-contexts: In this case, the parallel formulations as presented in the previous subsections are directly applicable without any modification. Another approach towards discretization is to discretize at every node in the tree. There are two examples of this approach. The first example can be found in <ref> [ARS98] </ref> where quantiles [ARS97] are used to discretize continuous attributes. The second example of this approach to discretize at each node is SPEC [SSHK97] where a clustering technique is used.
Reference: [BFOS84] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, Monterrey, </publisher> <address> CA, </address> <year> 1984. </year>
Reference-contexts: Table 3 shows the class distribution information of data attribute H umi di t y. Once the class distribution information of all the attributes are gathered, each attribute is evaluated in terms of either entropy [Qui93] or Gini Index <ref> [BFOS84] </ref>. The best attribute is selected as a test for the node expansion. The C4.5 algorithm generates a classification-decision tree for the given training data set by recursively partitioning the data. The decision tree is grown using depth-first strategy.
Reference: [Cat91] <author> J. Catlett. </author> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> PhD thesis. </type> <institution> University of Sydney, </institution> <year> 1991. </year>
Reference-contexts: One way to reduce the computational complexity of building a decision tree classifier using large training datasets is to use only a small sample of the training data. Such methods do not yield the same classification accuracy as a decision tree classifier that uses the entire data set <ref> [WC88, Cat91, CS93a, CS93b] </ref>. In order to get reasonable accuracy in a reasonable amount of time, parallel algorithms may be required. Classification decision tree construction algorithms have natural concurrency, as once a node is generated, all of its children in the classification tree can be generated concurrently.
Reference: [CS93a] <author> Philip K. Chan and Salvatore J. Stolfo. </author> <title> Experiments on multistrategy learning by metalearning. </title> <booktitle> In Proc. Second Intl. Conference on Info. and Knowledge Mgmt., </booktitle> <pages> pages 314-323, </pages> <year> 1993. </year>
Reference-contexts: One way to reduce the computational complexity of building a decision tree classifier using large training datasets is to use only a small sample of the training data. Such methods do not yield the same classification accuracy as a decision tree classifier that uses the entire data set <ref> [WC88, Cat91, CS93a, CS93b] </ref>. In order to get reasonable accuracy in a reasonable amount of time, parallel algorithms may be required. Classification decision tree construction algorithms have natural concurrency, as once a node is generated, all of its children in the classification tree can be generated concurrently.
Reference: [CS93b] <author> Philip K. Chan and Salvatore J. Stolfo. </author> <title> Metalearning for multistrategy learning and parallel learning. </title> <booktitle> In Proc. Second Intl. Conference on Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: One way to reduce the computational complexity of building a decision tree classifier using large training datasets is to use only a small sample of the training data. Such methods do not yield the same classification accuracy as a decision tree classifier that uses the entire data set <ref> [WC88, Cat91, CS93a, CS93b] </ref>. In order to get reasonable accuracy in a reasonable amount of time, parallel algorithms may be required. Classification decision tree construction algorithms have natural concurrency, as once a node is generated, all of its children in the classification tree can be generated concurrently.
Reference: [DMT94] <editor> D.J. Spiegelhalter D. Michie and C.C. Taylor. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks [Lip87], genetic algorithms [Gol89], and decision trees [Qui93] have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy <ref> [DMT94] </ref> and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [Qui93], and SLIQ [MAR96] are based on the ID3 classification decision tree algorithm [Qui93]. In the data mining domain, the data to be processed tends to be very large.
Reference: [Gol89] <author> D. E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimizations and Machine Learning. </title> <address> morgan-kaufman, </address> <year> 1989. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks [Lip87], genetic algorithms <ref> [Gol89] </ref>, and decision trees [Qui93] have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [DMT94] and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [Qui93], and SLIQ [MAR96] are based on the ID3 classification decision tree algorithm [Qui93].
Reference: [Hon97] <author> S.J. Hong. </author> <title> Use of contextual information for feature ranking and discretization. </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> 9(5) </volume> <pages> 718-730, </pages> <month> September/October </month> <year> 1997. </year>
Reference-contexts: Once again, communication in these formulations [SAM96, JKK98] can be reduced using the hybrid scheme of Section 3.3. Another completely different way of handling continuous attributes is to discretize them once as a preprocessing 11 Table 4: Symbols used in the analysis. step <ref> [Hon97] </ref>. In this case, the parallel formulations as presented in the previous subsections are directly applicable without any modification. Another approach towards discretization is to discretize at every node in the tree. There are two examples of this approach.
Reference: [JKK98] <author> M.V. Joshi, G. Karypis, and V. Kumar. ScalParC: </author> <title> A new scalable and efficient parallel classification alg orithm for mining large datasets. </title> <booktitle> In Proc. of the International Parallel Processing Symposium, </booktitle> <year> 1998. </year> <month> 22 </month>
Reference-contexts: A more efficient way of handling continuous attributes without incurring the high cost of repeated sorting is to use the pre-sorting technique used in algorithms SLIQ [MAR96], SPRINT [SAM96], and ScalParC <ref> [JKK98] </ref>. These algorithms require only one pre-sorting step, but need to construct a hash table at each level of the classification tree. In the parallel formulations of these algorithms, the content of this hash table needs to be available globally, requiring communication among processors. <p> These algorithms require only one pre-sorting step, but need to construct a hash table at each level of the classification tree. In the parallel formulations of these algorithms, the content of this hash table needs to be available globally, requiring communication among processors. Existing parallel formulations of these schemes <ref> [SAM96, JKK98] </ref> perform communication that is similar in nature to that of our synchronous tree construction approach discussed in Section 3.1. Once again, communication in these formulations [SAM96, JKK98] can be reduced using the hybrid scheme of Section 3.3. <p> Existing parallel formulations of these schemes <ref> [SAM96, JKK98] </ref> perform communication that is similar in nature to that of our synchronous tree construction approach discussed in Section 3.1. Once again, communication in these formulations [SAM96, JKK98] can be reduced using the hybrid scheme of Section 3.3. Another completely different way of handling continuous attributes is to discretize them once as a preprocessing 11 Table 4: Symbols used in the analysis. step [Hon97].
Reference: [KGGK94] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Algorithm Design and Analysis. </title> <publisher> Benjamin Cummings/ Addison Wesley, </publisher> <address> Redwod City, </address> <year> 1994. </year>
Reference-contexts: Depth-First or Breadth-First), and call that node as the current node. At the beginning, root node is selected as the current node. 2. For each data attribute, collect class distribution information of the local data at the current node. 3. Exchange the local class distribution information using global reduction <ref> [KGGK94] </ref> among processors. 4. Simultaneously compute the entropy gains of each attribute at each processor and select the best attribute for child node expansion. 5. <p> Here we give a detailed analysis for the case when only discrete attributes are present. The analysis for the case with continuous attributes can be found in [SSHK97]. The detailed study of the communication patterns used in this analysis can be found in <ref> [KGGK94] </ref>. Table 4 describes the symbols used in this section. 12 4.1 Assumptions * The processors are connected in a hypercube topology. Complexity measures for other topologies can be easily derived by using the communication complexity expressions for other topologies given in [KGGK94]. * The expression for communication and computation are <p> used in this analysis can be found in <ref> [KGGK94] </ref>. Table 4 describes the symbols used in this section. 12 4.1 Assumptions * The processors are connected in a hypercube topology. Complexity measures for other topologies can be easily derived by using the communication complexity expressions for other topologies given in [KGGK94]. * The expression for communication and computation are written for a full binary tree with 2 L leaves at depth L. <p> Refer to <ref> [KGGK94] </ref> section 3.7 for details. 14 in the splitting phase [KK94]. So splitting is done when: X .Communication Cost/ Moving Cost C Load Balancing This criterion for splitting ensures that the communication cost for this scheme will be within twice the communication cost for an optimal scheme [KK94]. <p> splitting the idle partition is included as a part of the busy partition and the computation proceeds as described above. 4.3 Scalability Analysis Isoefficiency metric has been found to be a very useful metric of scalability for a large number of problems on a large class of commercial parallel computers <ref> [KGGK94] </ref>. It is defined as follows. Let P be the number of processors and W the problem size (in total time taken for the best sequential algorithm). If W needs to grow as f E .P/ to maintain an efficiency E, then f E .
Reference: [KK94] <author> George Karypis and Vipin Kumar. </author> <title> Unstructured tree search on simd parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 379-391, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Refer to [KGGK94] section 3.7 for details. 14 in the splitting phase <ref> [KK94] </ref>. So splitting is done when: X .Communication Cost/ Moving Cost C Load Balancing This criterion for splitting ensures that the communication cost for this scheme will be within twice the communication cost for an optimal scheme [KK94]. The splitting is recursive and is applied as many times as required. <p> Refer to [KGGK94] section 3.7 for details. 14 in the splitting phase <ref> [KK94] </ref>. So splitting is done when: X .Communication Cost/ Moving Cost C Load Balancing This criterion for splitting ensures that the communication cost for this scheme will be within twice the communication cost for an optimal scheme [KK94]. The splitting is recursive and is applied as many times as required. Once splitting is done, the above computations are applied to each partition. When a partition of processors starts to idle, then it sends a request to a busy partition about its idle state.
Reference: [Lip87] <author> R. Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 4(22), </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks <ref> [Lip87] </ref>, genetic algorithms [Gol89], and decision trees [Qui93] have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [DMT94] and they are relatively inexpensive to compute.
Reference: [MAR96] <author> M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology, </booktitle> <address> Avignon, France, </address> <year> 1996. </year>
Reference-contexts: Several classification models like neural networks [Lip87], genetic algorithms [Gol89], and decision trees [Qui93] have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [DMT94] and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [Qui93], and SLIQ <ref> [MAR96] </ref> are based on the ID3 classification decision tree algorithm [Qui93]. In the data mining domain, the data to be processed tends to be very large. Hence, it is highly desirable to design computationally efficient as well as scalable algorithms. <p> We also provide the analysis of the cost of computation and communication of the proposed hybrid method. Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4.5 [Qui93], CDP [AIS93], SLIQ <ref> [MAR96] </ref>, and SPRINT [SAM96] use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> This process is repeated for each continuous attribute. 4 Recently proposed classification algorithms SLIQ <ref> [MAR96] </ref> and SPRINT [SAM96] avoid costly sorting at each node by pre-sorting continuous attributes once in the beginning. In SPRINT, each continuous attribute is maintained in a sorted attribute list. In this list, each entry contains a value of the attribute and its corresponding record id. <p> Hence even in this case, it will be useful to use a scheme similar to the hybrid approach discussed in Section 3.3. A more efficient way of handling continuous attributes without incurring the high cost of repeated sorting is to use the pre-sorting technique used in algorithms SLIQ <ref> [MAR96] </ref>, SPRINT [SAM96], and ScalParC [JKK98]. These algorithms require only one pre-sorting step, but need to construct a hash table at each level of the classification tree. In the parallel formulations of these algorithms, the content of this hash table needs to be available globally, requiring communication among processors. <p> We use binary splitting at each decision tree node and grow the tree in breadth first manner. For generating large datasets, we have used the widely used synthetic dataset proposed in the SLIQ paper <ref> [MAR96] </ref> for all our experiments. Ten classification functions were also proposed in [MAR96] for these datasets. We have used the function 2 dataset for our algorithms. In this dataset, there are two class labels and each record consists of 9 attributes having 3 categoric and 6 continuous attributes. <p> We use binary splitting at each decision tree node and grow the tree in breadth first manner. For generating large datasets, we have used the widely used synthetic dataset proposed in the SLIQ paper <ref> [MAR96] </ref> for all our experiments. Ten classification functions were also proposed in [MAR96] for these datasets. We have used the function 2 dataset for our algorithms. In this dataset, there are two class labels and each record consists of 9 attributes having 3 categoric and 6 continuous attributes.
Reference: [Qui93] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks [Lip87], genetic algorithms [Gol89], and decision trees <ref> [Qui93] </ref> have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [DMT94] and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [Qui93], and SLIQ [MAR96] are based on the ID3 classification decision tree algorithm [Qui93]. <p> Several classification models like neural networks [Lip87], genetic algorithms [Gol89], and decision trees <ref> [Qui93] </ref> have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [DMT94] and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [Qui93], and SLIQ [MAR96] are based on the ID3 classification decision tree algorithm [Qui93]. In the data mining domain, the data to be processed tends to be very large. Hence, it is highly desirable to design computationally efficient as well as scalable algorithms. <p> genetic algorithms [Gol89], and decision trees <ref> [Qui93] </ref> have been proposed. Decision trees are probably the most popular since they obtain reasonable accuracy [DMT94] and they are relatively inexpensive to compute. Most current classification algorithms such as C4.5 [Qui93], and SLIQ [MAR96] are based on the ID3 classification decision tree algorithm [Qui93]. In the data mining domain, the data to be processed tends to be very large. Hence, it is highly desirable to design computationally efficient as well as scalable algorithms. <p> We also provide the analysis of the cost of computation and communication of the proposed hybrid method. Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4.5 <ref> [Qui93] </ref>, CDP [AIS93], SLIQ [MAR96], and SPRINT [SAM96] use Hunt's method [Qui93] as the basic algorithm. <p> Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4.5 <ref> [Qui93] </ref>, CDP [AIS93], SLIQ [MAR96], and SPRINT [SAM96] use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> Table 2 shows the class distribution information of data attribute Outlook at the root of the decision tree shown in Figure 1. For a continuous attribute, binary tests involving all the distinct values 2 Table 1: A small training data set <ref> [Qui93] </ref> 3 Table 2: Class Distribution Information of Attribute Outlook Table 3: Class Distribution Information of Attribute H umi di t y of the attribute are considered. Table 3 shows the class distribution information of data attribute H umi di t y. <p> Table 3 shows the class distribution information of data attribute H umi di t y. Once the class distribution information of all the attributes are gathered, each attribute is evaluated in terms of either entropy <ref> [Qui93] </ref> or Gini Index [BFOS84]. The best attribute is selected as a test for the node expansion. The C4.5 algorithm generates a classification-decision tree for the given training data set by recursively partitioning the data. The decision tree is grown using depth-first strategy.
Reference: [SAM96] <author> J. Shafer, R. Agrawal, and M. Mehta. SPRINT: </author> <title> A scalable parallel classifier for data mining. </title> <booktitle> In Proc. of the 22nd VLDB Conference, </booktitle> <year> 1996. </year>
Reference-contexts: Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4.5 [Qui93], CDP [AIS93], SLIQ [MAR96], and SPRINT <ref> [SAM96] </ref> use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> This process is repeated for each continuous attribute. 4 Recently proposed classification algorithms SLIQ [MAR96] and SPRINT <ref> [SAM96] </ref> avoid costly sorting at each node by pre-sorting continuous attributes once in the beginning. In SPRINT, each continuous attribute is maintained in a sorted attribute list. In this list, each entry contains a value of the attribute and its corresponding record id. <p> A more efficient way of handling continuous attributes without incurring the high cost of repeated sorting is to use the pre-sorting technique used in algorithms SLIQ [MAR96], SPRINT <ref> [SAM96] </ref>, and ScalParC [JKK98]. These algorithms require only one pre-sorting step, but need to construct a hash table at each level of the classification tree. In the parallel formulations of these algorithms, the content of this hash table needs to be available globally, requiring communication among processors. <p> These algorithms require only one pre-sorting step, but need to construct a hash table at each level of the classification tree. In the parallel formulations of these algorithms, the content of this hash table needs to be available globally, requiring communication among processors. Existing parallel formulations of these schemes <ref> [SAM96, JKK98] </ref> perform communication that is similar in nature to that of our synchronous tree construction approach discussed in Section 3.1. Once again, communication in these formulations [SAM96, JKK98] can be reduced using the hybrid scheme of Section 3.3. <p> Existing parallel formulations of these schemes <ref> [SAM96, JKK98] </ref> perform communication that is similar in nature to that of our synchronous tree construction approach discussed in Section 3.1. Once again, communication in these formulations [SAM96, JKK98] can be reduced using the hybrid scheme of Section 3.3. Another completely different way of handling continuous attributes is to discretize them once as a preprocessing 11 Table 4: Symbols used in the analysis. step [Hon97]. <p> We have used the function 2 dataset for our algorithms. In this dataset, there are two class labels and each record consists of 9 attributes having 3 categoric and 6 continuous attributes. The same dataset was also used by the SPRINT algorithm <ref> [SAM96] </ref> for evaluating its performance. Experiments were done on an IBM SP2. The results for comparing speedup of the three parallel formulations are reported for parallel runs on 1, 2, 4, 8, and 16 processors. More experiments for the hybrid approach are reported for up to 128 processors.
Reference: [SAR95] <author> R. Shankar, K. Alsabti, and S. Ranka. </author> <title> Many-to-many communication with bounded traffic. </title> <booktitle> In Frontiers '95, the fifth symposium on advances in massively parallel computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Also, the cost for load balancing assumes that there is no network congestion. This is a reasonable assumption for networks that are bandwidth-rich as is the case with most commercial systems. Without assuming anything about network congestion, load balancing phase can be done using transportation primitive <ref> [SAR95] </ref> in time 2 fl N P fl t w time provided N P O.P 2 / Splitting is done when the accumulated cost of communication becomes equal to the cost of moving records around 1 If the message size is large, by routing message in parts, this communication step can
Reference: [SSHK97] <author> Anurag Srivastava, Vineet Singh, Eui-Hong Han, and Vipin Kumar. </author> <title> An efficient, scalable, parallel classifier for data mining. </title> <type> Technical Report TR-97-010,http://www.cs.umn.edu/kumar, </type> <institution> Department of Computer Science, University of Minnesota, M inneapolis, </institution> <year> 1997. </year>
Reference-contexts: There are two examples of this approach. The first example can be found in [ARS98] where quantiles [ARS97] are used to discretize continuous attributes. The second example of this approach to discretize at each node is SPEC <ref> [SSHK97] </ref> where a clustering technique is used. SPEC has been shown to be very efficient in terms of runtime and has also been shown to perform essentially identical to several other widely used tree classifiers in terms of classification accuracy [SSHK97]. <p> of this approach to discretize at each node is SPEC <ref> [SSHK97] </ref> where a clustering technique is used. SPEC has been shown to be very efficient in terms of runtime and has also been shown to perform essentially identical to several other widely used tree classifiers in terms of classification accuracy [SSHK97]. Parallelization of the discretization at every node of the tree is similar in nature to the parallelization of the computation of entropy gain for discrete attributes, because both of these methods of discretization require some global communication among all the processors that are responsible for a node. <p> In particular, parallel formulations of the clustering step in SPEC is essentially identical to the parallel formulations for the discrete case discussed in the previous subsections <ref> [SSHK97] </ref>. 4 Analysis of the Hybrid Algorithm In this section, we provide the analysis of the hybrid algorithm proposed in Section 3.3. Here we give a detailed analysis for the case when only discrete attributes are present. The analysis for the case with continuous attributes can be found in [SSHK97]. <p> subsections <ref> [SSHK97] </ref>. 4 Analysis of the Hybrid Algorithm In this section, we provide the analysis of the hybrid algorithm proposed in Section 3.3. Here we give a detailed analysis for the case when only discrete attributes are present. The analysis for the case with continuous attributes can be found in [SSHK97]. The detailed study of the communication patterns used in this analysis can be found in [KGGK94]. Table 4 describes the symbols used in this section. 12 4.1 Assumptions * The processors are connected in a hypercube topology. <p> Each processor can send or receive a maximum of N P training data items. Assuming no congestion in the interconnection network, cost for load balancing is: Cost for load balancing phase 2 fl N fl t w (4) A detailed derivation of Equation 4 above is given in <ref> [SSHK97] </ref>. Also, the cost for load balancing assumes that there is no network congestion. This is a reasonable assumption for networks that are bandwidth-rich as is the case with most commercial systems. <p> For these experiments, we used the original data set with continuous attributes and used a clustering technique to discretize continuous attributes at each decision tree node <ref> [SSHK97] </ref>. Note that the parallel formulation gives almost identical performance as the serial algorithm in terms of accuracy and classification tree size [SSHK97]. The results in Figure 8 show the speedup of the hybrid approach. <p> For these experiments, we used the original data set with continuous attributes and used a clustering technique to discretize continuous attributes at each decision tree node <ref> [SSHK97] </ref>. Note that the parallel formulation gives almost identical performance as the serial algorithm in terms of accuracy and classification tree size [SSHK97]. The results in Figure 8 show the speedup of the hybrid approach. The results confirm that the hybrid approach is indeed very effective. 19 To study the scaleup behavior, we kept the dataset size at each processor constant at 50,000 examples and increased the number of processors.
Reference: [WC88] <author> J. Wirth and J. Catlett. </author> <title> Experiments on the costs and benefits of windowing in ID3. </title> <booktitle> In 5th Int'l Conference on Machine learning, </booktitle> <year> 1988. </year> <month> 23 </month>
Reference-contexts: One way to reduce the computational complexity of building a decision tree classifier using large training datasets is to use only a small sample of the training data. Such methods do not yield the same classification accuracy as a decision tree classifier that uses the entire data set <ref> [WC88, Cat91, CS93a, CS93b] </ref>. In order to get reasonable accuracy in a reasonable amount of time, parallel algorithms may be required. Classification decision tree construction algorithms have natural concurrency, as once a node is generated, all of its children in the classification tree can be generated concurrently.
References-found: 20

