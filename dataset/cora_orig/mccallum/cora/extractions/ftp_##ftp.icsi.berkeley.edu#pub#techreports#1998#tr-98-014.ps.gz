URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1998/tr-98-014.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1998.html
Root-URL: http://www.icsi.berkeley.edu
Title: Incorporating Information From Syllable-length Time Scales into Automatic Speech Recognition  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Su-Lin Wu 
Date: May 1998  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Abstract: Incorporating the concept of the syllable into speech recognition may improve recognition accuracy through the integration of information over syllable-length time spans. Evidence from psychoacoustics and phonology suggests that humans use the syllable as a basic perceptual unit. Nonetheless, the explicit use of such long-time-span units is comparatively unusual in automatic speech recognition systems for English. The work described in this thesis explored the utility of information collected over syllable-related time-scales. The first approach involved integrating syllable segmentation information into the speech recognition process. The addition of acoustically-based syllable onset estimates [184] resulted in a 10% relative reduction in word-error rate. The second approach began with developing four speech recognition systems based on long-time-span features and units, including modulation spectrogram features [80]. Error analysis suggested the strategy of combining, which led to the implementation of methods that merged the outputs of syllable-based recognition systems with the phone-oriented baseline system at the frame level, the syllable level and the whole-utterance level. These combined systems exhibited relative improvements of 20-40% compared to the baseline system for clean and reverberant speech test cases. fl This report is a revised version of the author's thesis, which was submitted to the Department of Electrical Engineering and Computer Science on May 20, 1998 in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the University of California, Berkeley. This work was supervised by Professor Nelson Morgan. The thesis committee also included Professors Steven Greenberg, John Wawrzynek and Charles Stone. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Takayuki Arai and Steven Greenberg. </author> <title> The temporal properties of spoken Japanese are similar to those of English. </title> <booktitle> In Eurospeech, </booktitle> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year> <pages> ESCA. </pages>
Reference-contexts: Arai and Greenberg showed a similar pattern for Japanese <ref> [1] </ref>. Kingsbury and Morgan speculated that the performance improvement with reverberation provided by the modulation spectrogram features could be attributed to a more robust representation of syllabic segments leading to fewer deletions [106].
Reference: [2] <author> Aristotle. </author> <title> Categories. </title> <note> http://classics.mit.edu, 350 B.C.E. Translated by E. M. Edghill. </note>
Reference: [3] <author> W. C. Athas and C. L. Seitz. </author> <title> Multicomputers: Message-passing concurrent computers. </title> <journal> IEEE Computer, </journal> <volume> 21(8) </volume> <pages> 9-24, </pages> <month> August </month> <year> 1988. </year>
Reference: [4] <author> Steve Austin, Richard Schwartz, and Paul Placeway. </author> <title> The forward-backward search algorithm. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 697-700, </pages> <address> Toronto, </address> <month> May </month> <year> 1991. </year> <note> IEEE. </note>
Reference: [5] <author> Carlos Avendano, Sangita Tibrewala, and Hynek Hermansky. </author> <title> Multiresolution channel normalization for ASR in reverberant environments. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 3, </volume> <pages> pages 1107-1110, </pages> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year> <pages> ESCA. </pages>
Reference-contexts: The modulation spectrogram appears to more stably represent speech by reducing the presence of parts of the speech signal that are not important in determining phonetic identity. 1 Chapter 3 also discusses reverberation. 2 Avendano, Tibrewala and Hermansky discuss another approach to improving recognition accuracy for reverberant speech <ref> [5] </ref>. 79 The primary hypothesis behind the modulation spectrogram is that phonetic information is encoded in the speech signal as relatively slow changes in the spectral structure of speech. 3 Such a hypothesis matches the timing properties of the articulators and auditory cortical neuron activity [80].
Reference: [6] <author> R. H. Baayen, R. Piepenbrock, and H. van Rijn. </author> <title> The CELEX lexical database. </title> <publisher> cdrom, </publisher> <year> 1993. </year>
Reference: [7] <author> L. Bahl, P. Cohen, A. Cole, F. Jelinek, B. Lewis, and R. Mercer. </author> <title> Further results on the recognition of a continuously read natural corpus. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 3, </volume> <pages> pages 872-875, </pages> <address> Denver, Colorado, </address> <month> April </month> <year> 1980. </year> <note> IEEE. </note>
Reference: [8] <author> L. Bahl, P. de Souza, P. Gopalakrishnan, D. Nahamoo, and M. Picheny. </author> <title> A fast match for continuous speech recognition using allophonic models. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 17-20, </pages> <address> San Francisco, California, </address> <month> March </month> <year> 1992. </year> <note> IEEE. </note>
Reference: [9] <author> L. Bahl, P. Gopalakrishan, D. Kanevsky, and D. Nahamoo. </author> <title> Matrix fast match: A fast method for identifying a short list of candidate words for decoding. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 345-348, </pages> <address> Glasgow, Scotland, </address> <month> May </month> <year> 1989. </year> <note> IEEE. </note>
Reference: [10] <author> L. Bahl, F. Jelinek, and R. Mercer. </author> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(2):179-190, </volume> <month> March </month> <year> 1983. </year>
Reference: [11] <author> James K. Baker. </author> <title> The DRAGON system- An overview. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-23(1):24-29, </volume> <month> February </month> <year> 1975. </year> <month> 144 </month>
Reference: [12] <author> L. E. Baum and J. A. Eagon. </author> <title> An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology. </title> <journal> Bulletin of the American Mathematical Society, </journal> <volume> 73(3) </volume> <pages> 360-363, </pages> <month> May </month> <year> 1967. </year>
Reference: [13] <author> Jon Atli Benediktsson, Johannes R. Sveinsson, Okan K. Ersoy, and Phillip H. Swain. </author> <title> Parallel consensual neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 8(1) </volume> <pages> 54-64, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: Combining the outputs of multiple neural networks is an open research topic that this thesis does not fully address. Included among the many possible techniques is the neural network boosting algorithm AdaBoost [57] 1 and parallel consensual neural networks <ref> [13] </ref>. Because ASR includes a crucial decoding step subsequent to the pattern classification stage, there is an added level of complexity when considering the combination of neural network outputs. Combining methods for multiple recognition streams are discussed more generally in Section 3.5.
Reference: [14] <author> Christopher M. Bishop. </author> <title> Neural Networks for Pattern Recognition, </title> <booktitle> chapter 6.6.1, </booktitle> <pages> pages 226-228. </pages> <publisher> Clarendon Press, </publisher> <address> New York, New York, </address> <year> 1995. </year> <title> Interpretation of hidden units. </title>
Reference-contexts: From early trials, indicated in Table 5.3, this appeared to be too few for this amount of data. The role of the hidden layer in a neural network is to effect a nonlinear transformation on input data towards maximizing a discrimination measure <ref> [14] </ref>. This can be informally thought of as carving the input space with hyperplanes. The number of hidden units is related to the granularity of the pieces of the input space compartmentalized by these hyperplanes. The larger the number of hidden units, the finer the granularity.
Reference: [15] <author> Jean-Marc Boite. </author> <title> Speech training and recognition unified tool, </title> <note> 1998. More information can be found at http://tcts.fpms.ac.be/speech/strut.html. </note>
Reference-contexts: Many small groups develop their systems with the help of HTK (Hidden Markov Model Tool Kit) [212], the CSLU Speech Toolkit [22], or STRUT (Speech Training and Recognition Unified Tool) <ref> [15] </ref>, which provide many of the processing elements needed. ASR has also grown into viable commercial products. As the market for speech recognition applications enlarges, consumers will begin to drive the industry.
Reference: [16] <author> Antonio Bonafonte, Rafael Estany, and Eugenio Vives. </author> <title> Study of subword units for Spanish speech recognition. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 3, </volume> <pages> pages 1607-1610, </pages> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year> <pages> ESCA. </pages>
Reference: [17] <author> Herve Bourlard. </author> <title> Towards increasing speech recognition error rates. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 2, </volume> <pages> pages 883-893, </pages> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year> <pages> ESCA. </pages>
Reference-contexts: Because state-of-the-art performance is considered critical, research directions have suffered from a certain degree of inertia. Innovative new systems are usually couched in small pilot studies which suffer in comparison to larger, more mature systems. Attempting radical departures from the established mode has become increasingly difficult <ref> [17] </ref>. There are initiatives aimed at combating this trend: some research organizations attempt to contribute by attaching their work to existing, state-of-the-art systems. Boston University, for example, entered the 1997 ARPA Switchboard Evaluation in collaboration with an industrial partner [149].
Reference: [18] <author> Herve Bourlard, Bart D'hoore, and Jean-Marc Boite. </author> <title> Optimizing recognition and rejection performance in wordspotting systems. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 373-376, </pages> <address> Adelaide, South Australia, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference: [19] <author> Herve Bourlard and Stephane Dupont. </author> <title> A new ASR approach based on independent processing and recombination of partial frequency bands. </title> <booktitle> In ICSLP, </booktitle> <volume> volume 1, </volume> <pages> pages 426-429, </pages> <address> Philadephia, Pennsylvania, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Between combination points the recognition behavior of the systems can diverge and desynchronize. This combination method uses the HMM-recombination algorithm of Bourlard and Dupont <ref> [19] </ref>. The background of this method is discussed in Section 3.5. To use standard decoders without modifying them, Bourlard and Dupont combined HMM models in a many-to-many mapping before the model was input into the decoder.
Reference: [20] <author> Herve Bourlard and Nelson Morgan. </author> <title> Connectionist Speech Recognition- A Hybrid Approach. </title> <publisher> Kluwer Academic Press, </publisher> <year> 1994. </year>
Reference: [21] <author> Thomas Bub, Wolfgang Wahlster, and Alex Waibel. </author> <title> Verbmobil: The combination of deep and shallow processing for spontaneous speech translation. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 71-74, </pages> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: Interfacing among parts in collaborative work is a difficult engineering issue by itself, as shown by the efforts of the Verbmobil engineers. The Verbmobil project in Germany involved, at one point, 29 separate sites with 150 researchers and engineers <ref> [21] </ref>. The integration of the efforts was a large and time-consuming task, aside from the speech recognition aspects. The syllable can play a part in smoothing many kinds of interactions because of its function as a basic unit.
Reference: [22] <author> CSLU speech toolkit, </author> <year> 1998. </year> <note> More information can be found at http://cse.ogi.edu/CSLU/toolkit/toolkit.html. </note>
Reference-contexts: Small research groups are currently encountering significant difficulties in developing their own recognition systems that are competitive with the offerings of larger, established players. Many small groups develop their systems with the help of HTK (Hidden Markov Model Tool Kit) [212], the CSLU Speech Toolkit <ref> [22] </ref>, or STRUT (Speech Training and Recognition Unified Tool) [15], which provide many of the processing elements needed. ASR has also grown into viable commercial products. As the market for speech recognition applications enlarges, consumers will begin to drive the industry.
Reference: [23] <author> Lin Lawrance Chase. </author> <title> Blame assignment for errors made by large vocabulary speech recognizers. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 3, </volume> <pages> pages 1563-1566, </pages> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year> <pages> ESCA. </pages>
Reference-contexts: In most cases, however, the best systems selected were the ones without additional forced alignment, so the "ground truth" labels are adequate for evaluation. 9 Lin Chase suggests some solutions to these issues <ref> [23, 24] </ref>. 89 Frame-level Syllable level Word-level Utterance level Number of Tokens 230,000 5703 4673 1206 Table 5.5: Number of recognition tokens at each level for the Numbers development test set.
Reference: [24] <author> Lin Lawrance Chase. </author> <title> Error-Responsive Feedback Mechanisms for Speech Recognizers. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, The Robotics Institute, Pittsburgh, Penn-sylvania, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: As discussed by Chase <ref> [24] </ref>, what the "right" answer is for the output of a recognizer or a stage of a recognizer can depend on many factors and can vary across levels of analysis. For each of the analyses discussed below, the notion of the "truth" for the relevant scoring method is discussed. <p> In most cases, however, the best systems selected were the ones without additional forced alignment, so the "ground truth" labels are adequate for evaluation. 9 Lin Chase suggests some solutions to these issues <ref> [23, 24] </ref>. 89 Frame-level Syllable level Word-level Utterance level Number of Tokens 230,000 5703 4673 1206 Table 5.5: Number of recognition tokens at each level for the Numbers development test set.
Reference: [25] <author> Colin Cherry and Roger Wiley. </author> <title> Speech communication in very noisy environments. </title> <booktitle> Nature, </booktitle> <address> 214:1164, </address> <month> June </month> <year> 1967. </year> <month> 145 </month>
Reference: [26] <author> Kenneth W. Church. </author> <title> Phonological parsing and lexical retrieval. </title> <editor> In Uli H. Frauenfelder and Lorraine Komisarjevsky Tyler, editors, </editor> <title> Spoken Word Recognition, </title> <journal> Cognition Special Issues, </journal> <volume> chapter 3, </volume> <pages> pages 53-69. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference: [27] <author> Kenneth W. Church and William A. Gale. </author> <title> A comparison of the enhanced good-turing and deleted estimation methods for estimating probabilities of english bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> (5):19-54, 1991. 
Reference: [28] <author> John Clark and Colin Yallop. </author> <title> Phonetics and Phonology, </title> <booktitle> chapter 5, </booktitle> <pages> pages 124-127, 287. </pages> <publisher> Basil Blackwell, Ltd., </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference: [29] <author> George N. Clements and Samuel Jay Keyser. </author> <title> CV Phonology: A Generative Theory of the Syllable. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1983. </year>
Reference: [30] <author> R. A. Cole, M. Noel, T. Lander, and T. Durham. </author> <title> New telephone speech corpora at CSLU. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 1, </volume> <pages> pages 821-824, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: The trials described below did not incorporate any advance information from the test utterances. The subset of the Numbers corpus used for these experiments was phonetically transcribed at OGI <ref> [30] </ref>. Dan Ellis' (at ICSI) adaptation [47] of Bill Fisher's (NIST) syllabification program tsylb2 [51] automatically generated syllable boundaries for the training data from the phonological interpretations of the phonetic transcriptions. The neural network training procedure in Section 4.1.2 used these onsets.
Reference: [31] <author> Ron Cole, Lynette Hirschman, Les Atlas, Mary Beckman, Alan Biermann, Marcia Bush, Mark Clements, Jordan Cohen, Oscar Garcia, Brian Hanson, Hynek Herman-sky, Steve Levinson, Kathy McKeown, Nelson Morgan, David G. Novick, Mari Osten-dorf, Sharon Oviatt, Patti Price, Harvey Silverman, Judy Spitz, Alex Waibel, Clifford Weinstein, Steve Zahorian, and Victor Zue. </author> <title> The challenge of spoken language systems: Research directions for the nineties. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 3(1) </volume> <pages> 1-21, </pages> <month> January </month> <year> 1995. </year>
Reference: [32] <author> Richard Comerford, John Makhoul, and Richard Schwartz. </author> <title> The voice of the computer is heard in the land and it listens too! IEEE Spectrum, </title> <booktitle> 34(12) </booktitle> <pages> 39-47, </pages> <month> December </month> <year> 1997. </year>
Reference-contexts: Eric Fosler-Lussier's two-level decoder [54] may map neatly onto a multiple processor machine, since the probabilities of different words (or syllables) are computed independently. As mentioned in Chapter 3, some recent advances in speech recognition technology have been attributed to general improvements in hardware performance <ref> [32] </ref>. If this is the case, using parallel and concurrent machines should be highly advantageous to speech recognition research. 7.5 Reflections on the Future of ASR Research The field of automatic speech recognition is entering a new stage of maturation.
Reference: [33] <author> G. Cook and T. Robinson. </author> <title> Transcribing Broadcast News with the 1997 Abbot system. </title> <booktitle> In ICASSP, </booktitle> <address> Seattle, Washington, </address> <month> April </month> <year> 1998. </year> <note> IEEE. </note>
Reference-contexts: Using a very similar methodology to detect syllable onsets and the aforementioned scheme for incorporating onsets into their system, Cook and Robin-son found their error rate improved from 31.5% to 28.8%, a 8.6% relative reduction in word error rate <ref> [33] </ref>. 4.6 Summary Detecting syllable boundaries and nuclei has the potential to improve recognition accuracy by helping to accurately segment speech signals. Estimates of syllable onsets were used as constraints in a special-purpose decoder that explicitly represented the syllable as an intermediate stage between phones and words. <p> A method for incorporating onsets without the use of a special decoder was later developed and shared with colleagues who applied these ideas to Broadcast News, a large vocabulary corpus, and achieved a similar improvement in accuracy <ref> [33] </ref>. These experiments indicated the potential of using syllable-based information at other levels. Investigating this involved the development of a focus experimental system with syllable-based, long-time-span elements at the levels of feature analysis, neural network output and recognition unit. <p> Scaling up to larger tasks will entail addressing the following issues: Dynamic Syllabification for Syllables Onsets The extensibility of incorporating syllable-onsets is indicated by the work of colleagues with a 65,000-word vocabulary task <ref> [33] </ref>. Nonetheless, there are unanswered questions relating to the application of syllable-onsets to larger vocabularies. The pilot studies using onsets derived from knowledge of the correct answers (the "cheating" experiments) indicated a larger potential for improvement than has been realized.
Reference: [34] <author> G. D. Cook, D. J. Kershaw, J. D. M. Christie, and A. J. Robinson. </author> <title> Transcription of Broadcast Television and Radio News: the 1996 Abbot system. </title> <booktitle> In DARPA Speech Recognition Workshop, Westfields Internatinal Conference Center, </booktitle> <address> Chantilly, Virginia, </address> <month> February </month> <year> 1997. </year> <pages> DARPA. </pages>
Reference-contexts: The HMM-recombination work in Section 6.4 used a similar, but more elaborate paradigm. We shared this strategy with Cook and Robinson, who incorporated syllable boundary information into an experimental version of their ABBOT recognition system <ref> [34] </ref> for the DARPA Hub-4 Broadcast News task [71]. Their system included a trigram language model and a 65,000-word vocabulary. <p> However, Cook and Robinson were able to use the same syllable onset scheme to a similar, positive effect in a large vocabulary task, thus showing that the benefits are consistent and the method is scalable <ref> [34] </ref>. The ambiguity of syllable 131 boundaries did not directly affect the recognition performance of the systems with syllable--based elements at the signal processing, neural network context window and recognition unit level since these systems did not enforce rigorous temporal boundaries.
Reference: [35] <institution> Coordinated by National Institute of Standards and Technology. Conversational Speech Recognition Workshop DARPA Hub-5e Evaluation, Baltimore, Maryland, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: Complete chips are most often left to large corporations to fully develop. In ASR, academic recognition systems can still compete with industrial systems. University systems, such as Cambridge University's HTK and connectionist groups, and Carnegie Mellon University's group, are still among the front runners in organized evaluations <ref> [35] </ref>. Because state-of-the-art performance is considered critical, research directions have suffered from a certain degree of inertia. Innovative new systems are usually couched in small pilot studies which suffer in comparison to larger, more mature systems. Attempting radical departures from the established mode has become increasingly difficult [17].
Reference: [36] <author> M. Cravero, R. Pieraccini, and F. Raineri. </author> <title> Definition and evaluation of phonetic units for speech recognition by hidden Markov models. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 3, </volume> <pages> pages 2235-2238, </pages> <address> Tokyo, Japan, </address> <month> April </month> <year> 1986. </year> <note> IEEE. </note>
Reference: [37] <author> Anne Cutler, Sally Butterfield, and John N. Williams. </author> <title> The perceptual integrity of syllabic onsets. </title> <journal> Journal of Memory and Language, </journal> <volume> 26 </volume> <pages> 406-418, </pages> <year> 1987. </year>
Reference: [38] <author> Anne Cutler, Jacques Mehler, Dennis Norris, and Juan Segui. </author> <title> The syllable's differing role in the segmentation of French and English. </title> <journal> Journal of Memory and Language, </journal> <volume> 25 </volume> <pages> 385-400, </pages> <year> 1986. </year>
Reference: [39] <author> Walter Daeleman and Antal van den Bosch. </author> <title> Generalization performance of back-propagation learning on a syllabification task. </title> <editor> In M.F.J. Drossaers and A Nijholt, editors, </editor> <booktitle> Proceedings of TWLT3: Connectionism and Natural Language Processing, </booktitle> <pages> pages 27-37, </pages> <institution> University of Twente, </institution> <year> 1992. </year>
Reference: [40] <author> S. B. Davis and P. Mermelstein. </author> <title> Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 28(4) </volume> <pages> 357-366, </pages> <month> August </month> <year> 1980. </year>
Reference: [41] <author> Renato De Mori and Michael Galler. </author> <title> The use of syllable phonotactics for word hy-pothesization. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 2, </volume> <pages> pages 877-880, </pages> <address> Atlanta, Georgia, </address> <month> May </month> <year> 1996. </year> <note> IEEE. </note>
Reference: [42] <author> Renato De Mori and Giovanna Giordano. </author> <title> A parser for segmenting continuous speech into pseudo-syllabic nuclei. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 3, </volume> <pages> pages 876-879, </pages> <address> Denver, Colorado, </address> <month> April </month> <year> 1980. </year> <note> IEEE. </note>
Reference: [43] <author> John R. Deller, Jr., John G. Proakis, and John H. L. Hansen. </author> <title> Discrete-Time Processing of Speech Signals. </title> <publisher> Macmillan Publishing Company, </publisher> <address> New York, </address> <year> 1993. </year>
Reference: [44] <author> N. Rex Dixon and Harvey F. Silverman. </author> <title> The 1976 modular acoustic processor. </title> <journal> IEEE Transactions of Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-25(5):367-379, </volume> <month> Octo-ber </month> <year> 1977. </year>
Reference: [45] <author> Stephane Dupont, Herve Bourlard, and Christophe Ris. </author> <title> Using multiple time scales in a multi-stream speech recognition system. </title> <booktitle> In Eurospeech, </booktitle> <pages> pages 3-6, </pages> <address> Rhodes, Greece, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: Combining at the syllable level allowed the decoder to use information from each of the streams in a more desynchronized manner. In the version of the HMM recombination strategy <ref> [45] </ref> implemented at ICSI, 4 the decoding could use a phone probability from one stream and a half-syllable (from the syllabary) probability from the second stream subject only to the constraint that the two streams have common syllable beginning and end points.
Reference: [46] <author> Harold T. Edwards. </author> <title> Applied Phonetics: The Sounds of American English. Singular Publishing Group, </title> <publisher> Inc., </publisher> <address> San Diego, California, </address> <year> 1992. </year>
Reference: [47] <author> Dan Ellis. syllify. </author> <booktitle> Inhouse software at ICSI, 1996. Tcl/TK interface for Fisher's tsylb2 program. </booktitle>
Reference-contexts: The trials described below did not incorporate any advance information from the test utterances. The subset of the Numbers corpus used for these experiments was phonetically transcribed at OGI [30]. Dan Ellis' (at ICSI) adaptation <ref> [47] </ref> of Bill Fisher's (NIST) syllabification program tsylb2 [51] automatically generated syllable boundaries for the training data from the phonological interpretations of the phonetic transcriptions. The neural network training procedure in Section 4.1.2 used these onsets.
Reference: [48] <author> Lee D. Erman and Victor R. Lesser. </author> <title> The Hearsay-II speech understanding system: A tutorial. </title> <editor> In W. A. Lea, editor, </editor> <booktitle> Trends in Speech Recognition, chapter 16, </booktitle> <pages> pages 361-381. </pages> <institution> Speech Science Publications, Apple Valley, MN, </institution> <year> 1980. </year> <note> Reprinted in (Waibel and Lee, </note> <year> 1990). </year>
Reference: [49] <author> Kevin R. Farrell, Ravi P. Ramachandran, and Richard J. Mammone. </author> <title> An analysis of data fusion methods for speaker verification. </title> <booktitle> In ICASSP, </booktitle> <address> Seattle, Washington, </address> <month> April </month> <year> 1998. </year> <note> IEEE. </note>
Reference-contexts: By using similar procedures, trends that appear across recognition stages can be made more apparent. At some stages, additional evaluation was helpful, as discussed in the later sections of this chapter. The work of Farrell, Ramachandran and Mannone on combining systems in the context of speaker verification <ref> [49] </ref> inspired the error analysis method described in this chapter. In their paper, Farrell et al. evaluated four commonly-used models for the speaker verification task and three ways to combine the four scores.
Reference: [50] <author> Michael Finke and Alex Waibel. </author> <title> Flexible transcription alignment. </title> <booktitle> In ASRU, </booktitle> <pages> pages 34-40, </pages> <address> Santa Barbara, CA, </address> <month> December </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: Such "re-syllabification" phenomena are not easily accommodated within the syllabic representational framework used in the decoder in a generalizable fashion. One possible solution, which increases the complexity of the decoding considerably, is to use multiword clustering, as described in <ref> [50] </ref>. By modeling multiple words in sequence together, alternate syllable segmentations can be modeled. Re-syllabification, however, can happen in a large number of word combinations, so the multiword set may become very large.
Reference: [51] <author> Bill Fisher. tsylb2. </author> <title> Source code available through ftp from NIST, </title> <year> 1995. </year>
Reference-contexts: The trials described below did not incorporate any advance information from the test utterances. The subset of the Numbers corpus used for these experiments was phonetically transcribed at OGI [30]. Dan Ellis' (at ICSI) adaptation [47] of Bill Fisher's (NIST) syllabification program tsylb2 <ref> [51] </ref> automatically generated syllable boundaries for the training data from the phonological interpretations of the phonetic transcriptions. The neural network training procedure in Section 4.1.2 used these onsets. These syllable boundaries do not necessarily respect word boundaries, unlike the syllabifications used in the pilot experiments. <p> In this thesis the set of syllables includes only those occurring in the Numbers corpus, a tiny fraction of the syllables occurring in the English language. Fisher's automatic syllabifier tsylb <ref> [51] </ref> (via a Tcl/Tk interface created by Dan Ellis (at ICSI)) partitioned the pronunciations in the lexicon. The process uses pronunciations defined in terms of the ICSI56 phone set, to produce a set of corresponding syllables.
Reference: [52] <author> Eric Fosler. </author> <title> Automatic learning of a model for word pronunciations: Status report. In Conversational Speech Recognition Workshop: DARPA Hub-5E Evaluation. </title> <publisher> NIST, </publisher> <month> May 13-15 </month> <year> 1997. </year> <month> 147 </month>
Reference: [53] <author> Eric Fosler. </author> <title> Evidence for syntactic and semantic repair effects in auditory processing. Linguistics 220 Class Project, </title> <month> May </month> <year> 1997. </year>
Reference: [54] <author> Eric Fosler-Lussier. </author> <title> two-level decoder. </title> <booktitle> Inhouse software at ICSI, </booktitle> <year> 1998. </year> <title> An implementation of the two-level decoding algorithm with the capability to combine intermediate scores. </title>
Reference-contexts: A method that avoids the creation of massive HMMs would ease the computational resource problem. Eric Fosler-Lussier (of ICSI) has been working on a two-level Viterbi decoder <ref> [54] </ref>. The two-level decoding algorithm [171, 159] was historically set aside in favor of the more efficient single-pass Viterbi decoding algorithm. The two-level approach, however, is more amenable to combination schemes. The likelihood for each word is calculated for every possible time alignment. <p> Although the work in this thesis does not explore parallel computing further, some of the conclusions of this work are applicable to concurrent processing. Namely, combining information from multiple streams is an obviously concurrent operation. Eric Fosler-Lussier's two-level decoder <ref> [54] </ref> may map neatly onto a multiple processor machine, since the probabilities of different words (or syllables) are computed independently. As mentioned in Chapter 3, some recent advances in speech recognition technology have been attributed to general improvements in hardware performance [32].
Reference: [55] <author> Uli H. Frauenfelder. </author> <title> The interface between acoustic-phonetic and lexical processing. </title> <editor> In M. E. H. Schouten, editor, </editor> <title> The Auditory Processing of Speech- From Sounds to Words, </title> <booktitle> number 10 in Speech Research, </booktitle> <pages> pages 325-338. </pages> <publisher> Mouton de Gruyter, </publisher> <address> New York, </address> <year> 1992. </year>
Reference: [56] <author> Norman R. French, Charles W. Carter, Jr., and Walter Koenig, Jr. </author> <title> The words and sounds of telephone conversations. </title> <journal> The Bell System Technical Journal, </journal> <volume> IX:290-325, </volume> <month> April </month> <year> 1930. </year>
Reference: [57] <author> Yoav Freund and Robert E. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <year> 1996. </year>
Reference-contexts: The dual to recognizer selection is "recognizer fusion," the merging of the outputs of multiple systems. Combining the outputs of multiple neural networks is an open research topic that this thesis does not fully address. Included among the many possible techniques is the neural network boosting algorithm AdaBoost <ref> [57] </ref> 1 and parallel consensual neural networks [13]. Because ASR includes a crucial decoding step subsequent to the pattern classification stage, there is an added level of complexity when considering the combination of neural network outputs. Combining methods for multiple recognition streams are discussed more generally in Section 3.5.
Reference: [58] <author> Jurgen Fritsch. ACID/HNN: </author> <title> A framework for heirarchical connectionist acoustic modeling. </title> <editor> In Sadaoki Furui, B.-H. Juang, and Wu Chou, editors, </editor> <booktitle> Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <pages> pages 164-171, </pages> <address> Santa Barbara, California, </address> <month> December </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: For example, instead of one massive neural network, using several smaller networks, perhaps arranged in a hierarchy of graduated generality, can perhaps be used to manage the complexity. Fritsch suggested and implemented this strategy for context-dependent acoustic models <ref> [58] </ref>. The Syllable-based Speech Processing Team of the 1997 LVCSR Workshop [67] developed a means of dealing with the problem of augmenting a syllabary for a large vocabulary: they used syllable models for the more frequently occurring words and handled the remainder with standard phoneme-based models.
Reference: [59] <author> Osamu Fujimura. </author> <title> Syllable as a unit of speech recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-23(1):82-87, </volume> <month> February </month> <year> 1975. </year>
Reference: [60] <author> Osamu Fujimura. </author> <title> Syllables as concatenated demisyllables and affixes. </title> <journal> Journal of the Acoustical Society of America, 59(Suppl. </journal> <volume> 1):S55, </volume> <month> Spring </month> <year> 1976. </year>
Reference-contexts: Each syllable was represented with 2 distinct states. 4 The syllables were divided in the middle of each syllable's nucleus. The halves are referred to in this thesis as "half-syllables." These units are not called "demisyllables" as defined by Fujimura <ref> [60, 61] </ref>, though conceptually they are similar, in order to avoid the additional context and meaning carried by the term "demisyllable" in the research literature. Typically, demisyllables are formed from syllables divided just after the initial CV transition, not in the middle of the nucleus as with these half-syllables.
Reference: [61] <author> Osamu Fujimura. </author> <title> Demisyllables as sets of features: comments on Clements's paper. </title> <editor> In John Kingston and Mary E. Beckman, editors, </editor> <title> Between the grammar and physics of speech, number 1 in Papers in laboratory phonology, </title> <booktitle> chapter 18, </booktitle> <pages> pages 334-340. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1990. </year>
Reference-contexts: Each syllable was represented with 2 distinct states. 4 The syllables were divided in the middle of each syllable's nucleus. The halves are referred to in this thesis as "half-syllables." These units are not called "demisyllables" as defined by Fujimura <ref> [60, 61] </ref>, though conceptually they are similar, in order to avoid the additional context and meaning carried by the term "demisyllable" in the research literature. Typically, demisyllables are formed from syllables divided just after the initial CV transition, not in the middle of the nucleus as with these half-syllables.
Reference: [62] <author> Osamu Fujimura. </author> <title> Syllable timing computation in the C/D model. </title> <booktitle> In ICSLP, </booktitle> <pages> pages 519-522, </pages> <address> Yokohama, Japan, </address> <month> September </month> <year> 1994. </year>
Reference: [63] <author> Osamu Fujimura. </author> <title> Prosodic organization of speech based on syllables: The C/D model. </title> <booktitle> In Proceedings of the XIIIth International Congress of Phonetic Sciences, </booktitle> <volume> volume 3, </volume> <pages> pages 10-17, </pages> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1995. </year>
Reference: [64] <author> Sadaoki Furui. </author> <title> On the role of spectral transition for speech perception. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 80(4) </volume> <pages> 1016-1025, </pages> <month> October </month> <year> 1986. </year>
Reference: [65] <author> Sadaoki Furui and Chin-Hui Lee. </author> <title> Robust speech recognition- An overview. </title> <booktitle> In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <pages> page 93, </pages> <address> Snowbird, Utah, </address> <month> December </month> <year> 1995. </year> <journal> IEEE. </journal> <volume> 148 </volume>
Reference: [66] <author> M. J. F. Gales and S. Young. </author> <title> An improved approach to the hidden Markov model de-composition of speech and noise. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 233-236, </pages> <address> San Francisco, California, </address> <month> March </month> <year> 1992. </year> <note> IEEE. </note>
Reference: [67] <author> Aravind Ganapathiraju, Vaibhava Goel, Joseph Picone, Andres Corrada, George Doddington, Katrin Kirchhoff, Mark Ordowski, and Barbara Wheatley. </author> <title> Syllable-a promising recognition unit for LVCSR. </title> <booktitle> In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <address> Santa Barbara, California, Decem-ber 1997. </address> <publisher> IEEE. </publisher>
Reference-contexts: For example, instead of one massive neural network, using several smaller networks, perhaps arranged in a hierarchy of graduated generality, can perhaps be used to manage the complexity. Fritsch suggested and implemented this strategy for context-dependent acoustic models [58]. The Syllable-based Speech Processing Team of the 1997 LVCSR Workshop <ref> [67] </ref> developed a means of dealing with the problem of augmenting a syllabary for a large vocabulary: they used syllable models for the more frequently occurring words and handled the remainder with standard phoneme-based models. The team reported some success with this method.
Reference: [68] <author> Jean-Luc Gauvain. </author> <title> A syllable-based isolated word recognition experiment. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 57-60, </pages> <address> Tokyo, Japan, </address> <month> April </month> <year> 1986. </year> <note> IEEE. </note>
Reference: [69] <author> Dan Gildea and Eric Fosler-Lussier. </author> <title> Numbers lexicon. Inhouse lexicon specification for the Numbers corpus., </title> <year> 1996. </year>
Reference-contexts: Dan Gildea and Eric Fosler-Lussier (both at ICSI) created the new lexicon using the phonetic transcription data from the training set of the Numbers corpus. The resulting lexicon included 32 words (and their range of 178 possible pronunciations), comprising 118 different syllables <ref> [69] </ref>. This lexicon included approximately 90% of the pronunciation variations in the corpus, as reflected in the hand-transcriptions.
Reference: [70] <author> John J. Godfrey, Edward C. Holliman, and Jane McDaniel. </author> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 517-520, </pages> <address> San Francisco, California, </address> <month> March </month> <year> 1992. </year> <note> IEEE. </note>
Reference: [71] <author> David Graff. </author> <title> The 1996 Broadcast News Speech and Language-Model Corpus. </title> <booktitle> In DARPA Speech Recognition Workshop, Westfields International Conference Center, </booktitle> <address> Chantilly, Virginia, </address> <month> February </month> <year> 1997. </year> <pages> DARPA. </pages>
Reference-contexts: The HMM-recombination work in Section 6.4 used a similar, but more elaborate paradigm. We shared this strategy with Cook and Robinson, who incorporated syllable boundary information into an experimental version of their ABBOT recognition system [34] for the DARPA Hub-4 Broadcast News task <ref> [71] </ref>. Their system included a trigram language model and a 65,000-word vocabulary.
Reference: [72] <author> P. D. Green, L. A. Boucher, N. R. Kew, and A. J. H. Simons. </author> <title> The SYLK project final report. By private communication with P. </title> <editor> Green, </editor> <year> 1993. </year>
Reference: [73] <author> P. D. Green, N. R. Kew, and D. A. Miller. </author> <title> Speech representations in the SYLK recognition project. </title> <editor> In M. P. Cooke, S. W. Beet, and M. D. Crawford, editors, </editor> <booktitle> Visual Representation of Speech Signals, chapter 26, </booktitle> <pages> pages 265-272. </pages> <publisher> John Wiley, </publisher> <year> 1993. </year>
Reference: [74] <author> Steven Greenberg. </author> <title> Understanding speech understanding: Towards a unified theory of speech perception. </title> <booktitle> In Workshop on the Auditory Basis of Speech Perception, </booktitle> <pages> pages 1-8, </pages> <address> Keele, United Kingdom, </address> <month> July </month> <year> 1996. </year> <pages> ESCA. </pages>
Reference-contexts: Greenberg suggests that human speech recognition relies on temporal dynamics in coarse spectral patterns <ref> [74] </ref>. For efficient communication, human beings rely on the use of multiple, redundant, coarse patterns to obtain the robustness to noise and other nonlinguistic sources of variability [75]. The human brain may employ a rather sparse representation that exhibits most of the temporal dynamics of the speech signal.
Reference: [75] <author> Steven Greenberg. </author> <title> On the origins of speech intelligibility. </title> <booktitle> In Proceedings of the ESCA Workshop on Robust Speech Recognition for Unknown Communication Channels, </booktitle> <pages> pages 23-32, </pages> <address> Pont-a-Mousson, France, </address> <month> April </month> <year> 1997. </year> <pages> ESCA. </pages>
Reference-contexts: Greenberg suggests that human speech recognition relies on temporal dynamics in coarse spectral patterns [74]. For efficient communication, human beings rely on the use of multiple, redundant, coarse patterns to obtain the robustness to noise and other nonlinguistic sources of variability <ref> [75] </ref>. The human brain may employ a rather sparse representation that exhibits most of the temporal dynamics of the speech signal. Todd incorporates the temporal, or rhythmic nature of auditory processing via what he refers to as dynamic spatio-temporal receptive fields [189, 190].
Reference: [76] <author> Steven Greenberg. </author> <title> The Switchboard transcription project. In Frederick Jelinek, editor, 1996 Large Vocabulary Continuous Speech Recognition Summer Research Workshop Technical Reports, </title> <booktitle> number 24 in Research Notes, </booktitle> <address> Baltimore, Maryland, </address> <month> April </month> <year> 1997. </year> <title> Center for Language and Speech Processing, </title> <publisher> Johns Hopkins University. </publisher>
Reference-contexts: Moreover, they do not generally perceive small variations in syllables in conversational speech, if the word is correctly understood. For example, it was found by transcribers in the Switchboard Transcription Project <ref> [76] </ref> that the word "problem" was often pronounced in a reduced manner, "proem," though the word was initially perceived, before careful examination of the spectrogram, as the fully expressed, two-syllable word.
Reference: [77] <author> Steven Greenberg. </author> <type> Personal communication., </type> <year> 1998. </year>
Reference: [78] <author> Steven Greenberg. </author> <title> Speaking in shorthand- a syllable-centric perspective for understanding pronunciation variation. In Proceedings of the ESCA Workshop on Modeling Pronunciation Variation for Automatic Speech Recognition, </title> <address> Kekrade, Netherlands, </address> <month> May </month> <year> 1998. </year> <note> ESCA. 149 </note>
Reference-contexts: The decoding process can be thought of attaching the syllable onset hints and the most likely phonetic realizations as features to hypothesized syllabic intervals. This interpretive model, based on the framework of the syllable, is consistent with the discussion of the human speech perceptual process in <ref> [78] </ref>. This model will be discussed again in the context of the work with combining recognition systems at the syllable level in Section 6.4. The syllable interval interpretation provides an instructive connective fabric between these experiments and those of Chapters 5 and 6. <p> The 2-state framework minimally reflects the heterogeneous structure of the syllable; syllable onsets are generally preserved and but syllable codas are often deleted <ref> [78] </ref>. The boundary between the 2 states is initialized to be the nucleus, which usually maintains its vocalic nature through transformations. <p> The syllable deletion effect in spoken speech can skew the accuracy scoring, but complete syllable deletion is very infrequent; usually the syllable onset, at the very least, is preserved <ref> [78] </ref>. In these experiments, syllable error rates varied moderately from word error rates, but, in general, word error rate is a good predictor of syllable error rate and vice versa. <p> While whole syllables can occasionally be deleted, as discussed previously, the rate of complete deletion is very small compared to the deletion rate of phones <ref> [78] </ref>. 110 Variant (paired with Baseline) Baseline Only Correct Variant Only Correct Identical-Incorrect (count) Different-Incorrect RASTA + phones, 17 frames 23.8% 25.2% 42.7% (212) 8.3% RASTA + half-syllables, 17 frames 40.6% 26.6% 23.3% (148) 9.6% modulation spectrogram + phones, 17 frames 43.9% 26.9% 20.5% (138) 8.8% modulation spectrogram + half-syllables, 17 <p> The phone hypotheses from the baseline, and the half-syllable hy 115 potheses from the focus system become features of the underlying syllable-length interval. Greenberg uses such a model to explain pronunciation variation <ref> [78] </ref>. The resulting improvement in word-error rate is probably due to the successful combination of the complementary aspects of the two recognition systems.
Reference: [79] <author> Steven Greenberg, Joy Hollenback, and Dan Ellis. </author> <title> Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus. </title> <booktitle> In ICSLP, </booktitle> <volume> volume Supplement, </volume> <pages> pages S24-S27, </pages> <address> Philadelphia, Pennsylvania, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: E. R. PLP 6.4% 37.6% RASTA 6.4% 26.0% modulation spectrogram 8.5% 27.3% RASTA combined with PLP 5.7% 26.9% RASTA combined with modulation spectrogram 5.5% 20.1% Table 5.1: Partial list of performance results (word error rate) from original experiments with modulation spectrogram features [107]. distribution in the modulation spectrum <ref> [79] </ref> for conversational American English. Arai and Greenberg showed a similar pattern for Japanese [1]. Kingsbury and Morgan speculated that the performance improvement with reverberation provided by the modulation spectrogram features could be attributed to a more robust representation of syllabic segments leading to fewer deletions [106].
Reference: [80] <author> Steven Greenberg and Brian E. D. Kingsbury. </author> <title> The modulation spectrogram: In pursuit of an invariant representation of speech. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 3, </volume> <pages> pages 1647-1650, </pages> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: This chapter describes the development of the baseline and the experimental, syllable 78 based systems in detail. This exposition begins with a brief review of the background of the modulation spectrogram features <ref> [80, 107, 105] </ref>. Next, Section 5.2 describes the syllable-based training and recognition targets and lexicon. <p> Two of the experimental systems developed, including the focus system, used modulation spectrogram features to incorporated syllable-timing at the feature extraction level. The modulation spectrogram features supplanted the RASTA-PLP features used in the baseline recognition system. This section summarizes the work of Greenberg, Kingsbury and Morgan <ref> [80, 105, 107] </ref> on the modulation spectrogram features, as used for the work described in this thesis. Green-berg began looking towards the modulation spectrum as a means for explaining the effects of many sources of acoustic variation in speech, such as speaker differences and adverse environmental conditions. <p> recognition accuracy for reverberant speech [5]. 79 The primary hypothesis behind the modulation spectrogram is that phonetic information is encoded in the speech signal as relatively slow changes in the spectral structure of speech. 3 Such a hypothesis matches the timing properties of the articulators and auditory cortical neuron activity <ref> [80] </ref>. The modulation spectrogram represents the speech signal as a distribution of slow modulations, from 0 to 8 Hz with a peak at 4 Hz, across time and frequency. The 4-Hz sensitivity corresponds roughly to syllabic frequencies. <p> The modulation spectrogram incorporates a simple automatic gain control and emphasizes spectro-temporal peaks. The specific signal processing details required to produce the modulation spectrogram features are described in <ref> [80, 105, 107] </ref> and illustrated in Figure 5.1. These steps improve the relative stability of these features in comparison to the conventional spectrogram. The signal processing results in 15 features plus 15 delta features for a total of 30 features per frame. <p> Their systems had somewhat different parameters, but used the same Numbers task for evaluation. In the experimental trials reported by Greenberg and Kingsbury <ref> [80] </ref>, they found that the modulation spectrogram features performed slightly worse than a more conventional front-end method, PLP, for clean speech, but better for reverberant speech, by a statistically significant margin, as shown in Table 5.1. <p> Greenberg and Kingsbury first demonstrated the value of this method for a phone-based recognizer employing modulation spectrogram features combined at the frame level with a RASTA-PLP system <ref> [80] </ref>. The combining experiments summarized below replicate the original findings, and extend the strategy to larger granularity combinations with the focus system (such as at the syllable and whole-utterance level). Combining two recognition systems increases the total number of parameters involved.
Reference: [81] <author> Fran~cois Grosjean and James Paul Gee. </author> <title> Prosodic structure and spoken word recognition. </title> <editor> In Uli H. Frauenfelder and Lorraine Komisarjevsky Tyler, editors, </editor> <title> Spoken Word Recognition, </title> <journal> Cognition Special Issue, </journal> <pages> pages 135-155. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year>
Reference: [82] <author> Michael Hammond. </author> <title> Syllable parsing in English and French. </title> <note> Available through http://aruba.ccit.arizona.edu/ hammond, </note> <month> May </month> <year> 1995. </year>
Reference: [83] <author> Alfred Hauenstein. </author> <title> The syllable re-revisited. </title> <type> Technical Report TR-96-035, ICSI, </type> <month> August </month> <year> 1996. </year>
Reference: [84] <author> Alfred Hauenstein. </author> <title> Using syllables in a hybrid HMM-ANN recognition system. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 3, </volume> <pages> pages 1203-1206, </pages> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year> <pages> ESCA. </pages>
Reference: [85] <author> Hynek Hermansky. </author> <title> Perceptual linear predictive (PLP) analysis of speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 87(4) </volume> <pages> 1738-1752, </pages> <month> April </month> <year> 1990. </year>
Reference: [86] <author> Hynek Hermansky and Nelson Morgan. </author> <title> RASTA processing of speech. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2(4) </volume> <pages> 578-589, </pages> <month> October </month> <year> 1994. </year>
Reference: [87] <author> Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari. </author> <title> Decision combination in multiple classifier systems. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(1) </volume> <pages> 66-75, </pages> <month> January </month> <year> 1994. </year>
Reference: [88] <author> Mike Hochberg. y0. </author> <title> Software package, WERNICKE distribution., </title> <month> August </month> <year> 1993. </year> <title> Viterbi decoder in use at ICSI. </title>
Reference-contexts: speech performance, not data collected with the reverberant sets. 5.3.1 Experimental Procedure The baseline system, the focus system and each supplemental experimental system variant were very similar and used the following elements: * A 400 hidden-unit, fully-connected, single hidden-layer neural network, for frame-level probability estimation. * A Viterbi decoder, y0 <ref> [88] </ref>. <p> The y0 decoder <ref> [88] </ref> (no modification necessary), used these probabilities as input and produced words and 106 sentences, as illustrated in Figure 6.1. The only change made to the decoder parameters was that the language model scaling factor 3 was doubled. <p> The background of this method is discussed in Section 3.5. To use standard decoders without modifying them, Bourlard and Dupont combined HMM models in a many-to-many mapping before the model was input into the decoder. As mentioned previously, the HMM-recombination scheme was reimplemented at ICSI for the y0 <ref> [88] </ref> decoder. As illustrated for a simple example in Figure 6.2, the HMM-recombination scheme expands two parallel HMMs (an atypical form) into a single HMM with a conventional description. <p> This scheme was implemented using a sequence of three different decoders, y0 <ref> [88] </ref> for its forced alignment capability, noway [164, 163, 165] for its lattice generation function, and lat-tice2nbest [166] for its lattice decoding ability. 6 A number of interfacing scripts glued the programs together, enabling state desynchronization to occur over the entire utterance.
Reference: [89] <author> Zhihong Hu, Johan Schalkwyk, Etienne Barnard, and Ronald Cole. </author> <title> Speech recognition using syllable-like units. </title> <booktitle> In ICSLP, </booktitle> <volume> volume 2, </volume> <pages> pages 1117-1120, </pages> <address> Philadephia, Pennsylvania, </address> <month> October </month> <year> 1996. </year>
Reference: [90] <author> X. D. Huang and M. A. Jack. </author> <title> Semi-continuous hidden Markov models for speech signals. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 3(3) </volume> <pages> 239-252, </pages> <month> July </month> <year> 1989. </year> <note> Reprinted in (Waibel and Lee, </note> <year> 1990). </year>
Reference: [91] <author> Melvyn J. Hunt, Matthew Lennig, and Paul Mermelstein. </author> <title> Use of dynamic programming in a syllable-based continuous speech recognition system. </title> <editor> In David Sankoff and Joseph B. Kruskal, editors, </editor> <title> Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison, </title> <booktitle> chapter 5, </booktitle> <pages> pages 163-188. </pages> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <address> Reading Massachusetts, </address> <year> 1983. </year> <month> 150 </month>
Reference: [92] <author> M.J. Hunt, M. Lennig, and P. Mermelstein. </author> <title> Experiments in syllable-based recognition of continuous speech. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 3, </volume> <pages> pages 880-883, </pages> <address> Denver, Colorado, </address> <month> April </month> <year> 1980. </year> <note> IEEE. </note>
Reference: [93] <author> F. Jelinek. </author> <title> Fast sequential decoding algorithm using a stack. </title> <journal> IBM J. Res. Develop., </journal> <volume> 13 </volume> <pages> 675-685, </pages> <month> November </month> <year> 1969. </year>
Reference: [94] <author> F. Jelinek. </author> <title> Self-organized language modeling for speech recognition. </title> <editor> In Alex Waibel and Kai-Fu Lee, editors, </editor> <booktitle> Readings in Speech Recognition, chapter 8, </booktitle> <pages> pages 450-506. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference: [95] <author> F. Jelinek, L. R. Bahl, and R. L. Mercer. </author> <title> Design of a linguistic statistical decoder for the recognition of continuous speech. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 21 </volume> <pages> 250-256, </pages> <month> May </month> <year> 1975. </year>
Reference: [96] <author> James J. Jenkins, Winifred Strange, and Salvatore Miranda. </author> <title> Vowel identification in mixed-speaker silent-center syllables. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 95(2) </volume> <pages> 1030-1043, </pages> <month> February </month> <year> 1994. </year>
Reference: [97] <author> John T. Jensen. </author> <title> English Phonology, </title> <booktitle> volume 99 of Series IV- Current Issues in Linguistic Theory, chapter 3, </booktitle> <pages> pages 47-76. </pages> <publisher> John Benjamins Publishing Company, </publisher> <address> Philadel-phia, </address> <year> 1993. </year>
Reference: [98] <author> M. Jones and P.C. Woodland. </author> <title> Modelling syllable characteristics to improve a large vocabulary continuous speech recogniser. </title> <booktitle> In ICSLP, </booktitle> <volume> volume 4, </volume> <pages> pages 2171-2174, </pages> <address> Yokohama, Japan, </address> <month> September </month> <year> 1994. </year>
Reference: [99] <author> Rhys James Jones, Simon Downey, and John S. Mason. </author> <title> Continuous speech recognition using syllables. </title> <booktitle> In EuroSpeech, </booktitle> <volume> volume 3, </volume> <pages> pages 1171-1174, </pages> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year> <pages> ESCA. </pages>
Reference: [100] <author> Dan Jurafsky and Nikki Mirghafori. </author> <title> ICSI speech recognition system. Inhouse document at ICSI, </title> <year> 1995. </year>
Reference: [101] <author> Daniel Kahn. </author> <title> Syllable-based Generalizations in English Phonology. </title> <booktitle> Outstanding Dissertations in Linguistics. </booktitle> <publisher> Garland Publishing, </publisher> <address> New York, </address> <year> 1980. </year>
Reference: [102] <author> P. Kenny, R. Hollan, V. Gupta, M Lennig, P Mermelstein, and D. O'Shaughnessy. </author> <title> A fl -admissible heuristics for rapid lexical access. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 689-692, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year> <note> IEEE. </note>
Reference: [103] <author> Michael Kenstowicz and Charles Kisseberth. </author> <title> Generative Phonology. </title> <publisher> Harcourt Brace Jovanovich, </publisher> <address> Orlando, </address> <year> 1979. </year>
Reference: [104] <author> Brian E. D. </author> <title> Kingsbury. </title> <type> Personal communication, </type> <month> March </month> <year> 1998. </year> <title> Modulation spectrogram processing diagram. </title>
Reference: [105] <author> Brian E. D. Kingsbury. </author> <title> Perceptually-inspired signal processing strategies for robust speech recognition in reverberant environments. </title> <type> PhD thesis, </type> <institution> UC Berkeley, </institution> <year> 1998. </year> <note> To be published. 151 </note>
Reference-contexts: This chapter describes the development of the baseline and the experimental, syllable 78 based systems in detail. This exposition begins with a brief review of the background of the modulation spectrogram features <ref> [80, 107, 105] </ref>. Next, Section 5.2 describes the syllable-based training and recognition targets and lexicon. <p> Two of the experimental systems developed, including the focus system, used modulation spectrogram features to incorporated syllable-timing at the feature extraction level. The modulation spectrogram features supplanted the RASTA-PLP features used in the baseline recognition system. This section summarizes the work of Greenberg, Kingsbury and Morgan <ref> [80, 105, 107] </ref> on the modulation spectrogram features, as used for the work described in this thesis. Green-berg began looking towards the modulation spectrum as a means for explaining the effects of many sources of acoustic variation in speech, such as speaker differences and adverse environmental conditions. <p> The modulation spectrogram incorporates a simple automatic gain control and emphasizes spectro-temporal peaks. The specific signal processing details required to produce the modulation spectrogram features are described in <ref> [80, 105, 107] </ref> and illustrated in Figure 5.1. These steps improve the relative stability of these features in comparison to the conventional spectrogram. The signal processing results in 15 features plus 15 delta features for a total of 30 features per frame. <p> The same frame-level combination approach is analyzed more thoroughly later, in Chapter 6. Kingsbury et al. has continued to develop the modulation spectrogram beyond the 81 version used for the work in this thesis; the details about his continuing work can be found in his Ph.D. thesis <ref> [105] </ref>. It was necessary to choose a version of the modulation spectrogram features for this thesis work, however, and it was not practical to continuously update the work with revised features. <p> Experimental methods and time constraints limited the individual refinement of the systems. Further optimization possibilities include: Using Improved Features A revised version of the modulation spectrogram features is under development by Kings-bury and Greenberg <ref> [105] </ref>. The latest version of the features outperforms the older version used in this thesis, particularly for the reverberant test case. Using the refined version of the modulation spectrogram features may help to reduce the absolute error rate for the combined systems.
Reference: [106] <author> Brian E. D. Kingsbury and Nelson Morgan. </author> <title> Recognizing reverberant speech with RASTA-PLP. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 2, </volume> <pages> pages 1259-1262, </pages> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: Human subjects, when asked to transcribe the words in moderately reverberant speech, achieve a performance level that is vastly better than the best automatic speech recognizer, as illustrated with Numbers in Section 3.2 and in <ref> [106] </ref>. Kingsbury implemented and refined the original modulation spectrogram proposal and focused his attention on improving the accuracy of speech recognition systems in the case of reverberant speech 2 and in the presence of additive noise. <p> Arai and Greenberg showed a similar pattern for Japanese [1]. Kingsbury and Morgan speculated that the performance improvement with reverberation provided by the modulation spectrogram features could be attributed to a more robust representation of syllabic segments leading to fewer deletions <ref> [106] </ref>. Modulation spectrogram features seem to emphasize the high-energy portions of the speech signal usually associated with syllabic nuclei.
Reference: [107] <author> Brian E. D. Kingsbury, Nelson Morgan, and Steven Greenberg. </author> <title> Robust speech recognition using the modulation spectrogram. Speech Communication, </title> <note> 1998. In press. </note>
Reference-contexts: This chapter describes the development of the baseline and the experimental, syllable 78 based systems in detail. This exposition begins with a brief review of the background of the modulation spectrogram features <ref> [80, 107, 105] </ref>. Next, Section 5.2 describes the syllable-based training and recognition targets and lexicon. <p> Two of the experimental systems developed, including the focus system, used modulation spectrogram features to incorporated syllable-timing at the feature extraction level. The modulation spectrogram features supplanted the RASTA-PLP features used in the baseline recognition system. This section summarizes the work of Greenberg, Kingsbury and Morgan <ref> [80, 105, 107] </ref> on the modulation spectrogram features, as used for the work described in this thesis. Green-berg began looking towards the modulation spectrum as a means for explaining the effects of many sources of acoustic variation in speech, such as speaker differences and adverse environmental conditions. <p> The modulation spectrogram incorporates a simple automatic gain control and emphasizes spectro-temporal peaks. The specific signal processing details required to produce the modulation spectrogram features are described in <ref> [80, 105, 107] </ref> and illustrated in Figure 5.1. These steps improve the relative stability of these features in comparison to the conventional spectrogram. The signal processing results in 15 features plus 15 delta features for a total of 30 features per frame. <p> E. R. Reverb W. E. R. PLP 6.4% 37.6% RASTA 6.4% 26.0% modulation spectrogram 8.5% 27.3% RASTA combined with PLP 5.7% 26.9% RASTA combined with modulation spectrogram 5.5% 20.1% Table 5.1: Partial list of performance results (word error rate) from original experiments with modulation spectrogram features <ref> [107] </ref>. distribution in the modulation spectrum [79] for conversational American English. Arai and Greenberg showed a similar pattern for Japanese [1]. <p> Modulation spectrogram features seem to emphasize the high-energy portions of the speech signal usually associated with syllabic nuclei. Kingsbury et al. pointed out again in <ref> [107] </ref> that most of the energy that appears in displays of the modulation spectrogram falls between onsets, and the observed energy appears to correspond roughly with syllabic nuclei. <p> The performance of a recognition system based on modulation spectrogram features is compared to the performance of systems based on other feature analysis methods in Table 5.1, reproduced from <ref> [107] </ref>. Kingsbury et al. produced these experiments with an ANN/HMM hybrid system similar to the paradigm used for this thesis work (i.e., both are based on the ICSI system). Their systems had somewhat different parameters, but used the same Numbers task for evaluation. <p> size) 189,344 5.9% modulation spectrogram (original) 99,056 8.5% modulation spectrogram (doubled in size) 198,112 8.2% Table 5.4: Performance results (word error rates) showing the effect of doubling the number of parameters by increasing the number of hidden units, from 488 to 976 (RASTA-PLP) and from 328 to 656 (modulation spectrogram) <ref> [107] </ref>. from a hidden layer size of 400 to 1000 units. Increased complexity in the mapping provided by the hidden layer of the neural network produced some benefit. This is not unreasonable in view of the added variation introduced by the artificial reverberation. <p> The word error rate scores for reverberant speech are provided in Table 5.3 for comparison purposes with other experimental systems with similar numbers of parameters. Kingsbury et al. report similar effects when the number of parameters in their systems were doubled on the same task <ref> [107] </ref>. Although the systems in the paper by Kingsbury et al. use the same ICSI methodology, their systems had 9 frames of neural network input context instead of 17 frames. <p> The results of Kingsbury et al. with doubling the number of hidden units in the neural network from 488 to 976 (with RASTA-PLP features) and from 87 328 to 656 (with modulation spectrogram features) are shown in Table 5.4, reprinted from <ref> [107] </ref>. The performance results with double the number of hidden units are not statistically different from the original system. As has been observed in the past by others, the technique of merely adding more parameters eventually produces diminishing returns and requires more complex training algorithms. <p> The relationships between the columns of Table 5.8 reflect those observed with the clean speech case. As was found by Kingsbury et al. <ref> [107] </ref>, the modulation spectrogram system with phoneme-based recognition units performed almost as well as a comparable RASTA-PLP system, also with phoneme-based recognition units. Both resulted in a word error rate of around 26%. Using a larger input window to the neural network longer acoustic context) always seemed to help. <p> 5.8% 9 frames **baseline** phones, 17 frames reverb 28.0% 25.8% 17.7% Table 6.8: Performance results (word error rate) scores of each system independently and after combining, at the frame level on clean and reverberant versions of the Numbers eval uation test set. 108 findings are consistent with those previously reported <ref> [107] </ref>. They are also consistent with the suggestions of the frame level analyses and comparisons.
Reference: [108] <author> Katrin Kirchhoff. </author> <title> Phonologically structured HMMs for speech recognition. </title> <booktitle> In Second Meeting of the ACL SIG in Computational Phonology, </booktitle> <pages> pages 45-49, </pages> <address> Santa Cruz, California, </address> <month> June </month> <year> 1996. </year> <booktitle> ACL. </booktitle>
Reference: [109] <author> Katrin Kirchhoff. </author> <title> Syllable-level desynchronisation of phonetic features for speech recognition. </title> <booktitle> In ICSLP, </booktitle> <volume> volume 4, </volume> <pages> pages 2274-2276, </pages> <address> Philadephia, Pennsylvania, </address> <month> Octo-ber </month> <year> 1996. </year>
Reference: [110] <author> Katrin Kirchhoff. </author> <title> Statistical analysis of the VERBMOBIL corpus. </title> <note> Unpublished Memo, </note> <month> April </month> <year> 1997. </year>
Reference: [111] <author> Dennis H. Klatt. </author> <title> Review of the ARPA speech understanding project. </title> <journal> The Journal of the Acoustical Society of America, </journal> <volume> 62(6) </volume> <pages> 1324-1366, </pages> <month> December </month> <year> 1977. </year> <note> Reprinted in (Waibel and Lee, </note> <year> 1990). </year>
Reference: [112] <author> H. Klemm, F. Class, and U. Kilian. </author> <title> Word- and phrase spotting with syllable-based garbage modelling. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 3, </volume> <pages> pages 2157-2160, </pages> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year> <pages> ESCA. </pages>
Reference: [113] <author> Peter Ladefoged. </author> <title> A Course in Phonetics. </title> <publisher> Harcourt Brace Jovanovich, </publisher> <address> New York, third edition, </address> <year> 1993. </year>
Reference: [114] <author> Wayne A. Lea, Mark F. Medress, and Toby E. Skinner. </author> <title> A prosodically guided speech understanding strategy. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-23(1):30-38, </volume> <month> February </month> <year> 1975. </year>
Reference: [115] <author> Kai-Fu Lee, Hsiao-Wuen Hon, and Raj Reddy. </author> <title> A overview of the SPHINX speech recognition system. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 38(1) </volume> <pages> 35-45, </pages> <month> January </month> <year> 1990. </year>
Reference: [116] <author> Lin-Shan Lee. </author> <title> Voice dictation of Mandarin Chinese. </title> <journal> IEEE Signal Processing Magazine, </journal> <pages> pages 63-100, </pages> <month> July </month> <year> 1997. </year>
Reference: [117] <author> Victor R. Lesser, Richard D. Fennell, Lee D. Erman, and D. Raj Reddy. </author> <title> Organization of the Hearsay II speech understanding system. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-23(1):11-23, </volume> <month> February </month> <year> 1975. </year>
Reference: [118] <author> Franklin Mark Liang. </author> <title> Word Hy-phen-a-tion By Com-pu-ter. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1983. </year>
Reference: [119] <author> Sung-Chien Lin, Lee-Feng Chien, Keh-Jiann Chen, and Lin-Shan Lee. </author> <title> A syllable-based very-large-vocabulary voice retrieval system for Chinese databases with textual attributes. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 1, </volume> <pages> pages 203-206, </pages> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year> <note> ESCA. 152 </note>
Reference: [120] <author> Richard Lippmann. </author> <title> Speech perception by humans and machines. </title> <booktitle> In Workshop on the Auditory Basis of Speech Perception, </booktitle> <pages> pages 309-316, </pages> <address> Keele, United Kingdom, </address> <month> July </month> <year> 1996. </year> <pages> ESCA. </pages>
Reference: [121] <author> E. Lleida, J.B. Mari~no, J. Salavedra, and A. Bonafonte. </author> <title> Syllabic fillers for Spanish HMM keyword spotting. </title> <booktitle> In ICSLP, </booktitle> <volume> volume 1, </volume> <pages> pages 5-8, </pages> <address> Banff, Alberta, Canada, </address> <month> October </month> <year> 1992. </year>
Reference: [122] <author> B. T. Lowerre and D. R. Reddy. </author> <title> The HARPY speech understanding system. </title> <editor> In W. A. Lea, editor, </editor> <title> Trends in Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1980. </year>
Reference: [123] <author> Dominic W. Massaro. </author> <title> Preperceptual auditory images. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 85(3) </volume> <pages> 411-417, </pages> <year> 1970. </year>
Reference: [124] <author> Dominic W. Massaro. </author> <title> Preperceptual images, processing time and perceptual units in auditory perception. </title> <journal> Psychological Review, </journal> <volume> 79(2) </volume> <pages> 124-145, </pages> <year> 1972. </year>
Reference: [125] <author> Dominic W. Massaro. </author> <title> Perceptual units in speech recognition. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 102(2) </volume> <pages> 199-208, </pages> <year> 1974. </year>
Reference: [126] <author> Shoichi Matsunaga, Takeshi Matsumura, and Harald Singer. </author> <title> Continuous speech recognition using non-uniform unit based acoustic and language models. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 3, </volume> <pages> pages 1619-1622, </pages> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year> <pages> ESCA. </pages>
Reference: [127] <author> Jacques Mehler, Jean Yves Dommergues, and Uli Frauenfelder. </author> <title> The syllable's role in speech segmentation. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 20 </volume> <pages> 298-305, </pages> <year> 1981. </year>
Reference: [128] <author> William S. Meisel. </author> <title> State of the art: </title> <booktitle> Applications. In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <pages> pages 29-44, </pages> <address> Snowbird, Utah, </address> <month> December </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: ASR has also grown into viable commercial products. As the market for speech recognition applications enlarges, consumers will begin to drive the industry. The product features desired by users, rather than basic science interests, will dictate the research agenda of many organizations <ref> [128] </ref>. For example, the commercial viability of using speech recognition for information retrieval has focused interest, and funding, on the aspects ASR appropriate for this task. Customers will more strongly influence the direction of research than will academia. A similar evolutionary process occurred in the field of microprocessor design.
Reference: [129] <author> Paul Mermelstein. </author> <title> Automatic segmentation of speech into syllabic units. </title> <journal> J. Acoust. Soc. Am, </journal> <volume> 58(4) </volume> <pages> 880-883, </pages> <month> October </month> <year> 1975. </year>
Reference: [130] <author> Joanne L. Miller and Peter D. Eimas. </author> <title> Observations on speech perception, its development, and the search for a mechanism. </title> <editor> In Judith C. Goodman and Howard C. Nusbaum, editors, </editor> <title> The Development of Speech Perception:The Transition from Speech Sounds to Spoken Words, </title> <booktitle> chapter 2, </booktitle> <pages> pages 37-55. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1994. </year>
Reference: [131] <author> N. Morgan, H. Hermansky, and H.G. Hirsch. </author> <title> Recognition of speech in additive and convolutional noise based on RASTA spectral processing. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 83-86, </pages> <address> Minneapolis, Minnesota, </address> <month> April </month> <year> 1993. </year> <note> IEEE. </note>
Reference: [132] <author> Nelson Morgan. </author> <title> Room Acoustics Simulation with Discrete-Time Hardware. </title> <type> PhD thesis, </type> <institution> UC Berkeley, </institution> <year> 1980. </year> <month> 153 </month>
Reference: [133] <author> Nelson Morgan and Herve Bourlard. </author> <title> Continuous speech recognition. </title> <journal> IEEE Signal Processing Magazine, </journal> <volume> 12(3) </volume> <pages> 25-42, </pages> <month> May </month> <year> 1995. </year>
Reference: [134] <author> Nelson Morgan, Herve Bourlard, Steve Greenberg, and Hynek Hermansky. </author> <title> Stochastic perceptual auditory-event-based models for speech recognition. </title> <booktitle> In ICSLP, </booktitle> <pages> pages 1943-1946, </pages> <address> Yokohama, Japan, </address> <month> September </month> <year> 1994. </year>
Reference: [135] <author> Nelson Morgan and Hynek Hermansky. </author> <title> RASTA extensions: Robustness to additive and convolutional noise. </title> <booktitle> In Proceedings of the Workshop on Speech Processing in Adverse Conditions, </booktitle> <address> Cannes, France, </address> <month> November </month> <year> 1992. </year>
Reference: [136] <author> Nelson Morgan, Su-Lin Wu, and Herve Bourlard. </author> <title> Digit recognition with stochastic perceptual speech models. </title> <booktitle> In Eurospeech, </booktitle> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year>
Reference: [137] <author> Hy Murveit, John Butzberger, Vassilios Digalakis, and Mitch Weintraub. </author> <title> Large-vocabulary dictation using SRI's Decipher T M speech recognition system: Progressive search techniques. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 2, </volume> <pages> pages 319-322, </pages> <address> Minneapolis, Minnesota, </address> <month> April </month> <year> 1993. </year> <note> IEEE. </note>
Reference: [138] <author> H. Ney. </author> <title> The use of a one-stage dynamic programming algorithm for connected word recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-32(2):263-271, </volume> <month> April </month> <year> 1984. </year>
Reference: [139] <author> H. Ney and X. Aubert. </author> <title> A word graph algorithm for large vocabulary, continuous speech recognition. </title> <booktitle> In ICSLP, </booktitle> <pages> pages 1355-1358, </pages> <address> Yokohama, Japan, </address> <month> September </month> <year> 1994. </year>
Reference: [140] <author> H. Ney, R. Haeb-Umbach, B.-H. Tran, and M. Oerder. </author> <title> Improvements in beam search for 10,000-word continuous speech recognition. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 9-12, </pages> <address> San Francisco, California, </address> <month> March </month> <year> 1992. </year> <note> IEEE. </note>
Reference: [141] <author> NIST. </author> <title> Continuous speech recognition corpus, </title> <month> September </month> <year> 1993. </year> <institution> National Institute of Standards and Technology Speech. </institution>
Reference: [142] <author> NIST. </author> <note> sclite version 1.3. Distributed by NIST, </note> <month> March </month> <year> 1996. </year> <title> Scores speech recognition system output. </title>
Reference-contexts: The word error rate score is typically calculated using dynamic programming to determine the minimum number of insertions, deletions and substitutions required to reconcile a recognized string with a given correct string of words. The algorithm has been standardized; the scores reported here use the sclite <ref> [142] </ref> scoring utility and the ICSI-local wordscore [161] utility. Both are consistent with the NIST standard. The simple dynamic programming method of scoring has the disadvantage of not applying higher-level 88 knowledge or time-alignment information.
Reference: [143] <author> Dennis Norris and Anne Cutler. </author> <title> The relative accessibility of phonemes and syllables. </title> <journal> Perception and Psychophysics, </journal> <volume> 43(6) </volume> <pages> 541-550, </pages> <year> 1988. </year>
Reference: [144] <author> Lynne C. Nygaard and David B. Pisoni. </author> <title> Speech perception: New directions in research and theory. </title> <editor> In Joanne L. Miller and Peter D. Eimas, editors, </editor> <title> Speech, Language and Communication, </title> <journal> volume 11 of Handbook of Perception and Cognition, </journal> <volume> chapter 3, </volume> <pages> pages 63-96. </pages> <publisher> Academic Press, </publisher> <address> San Diego, California, 2 edition, </address> <year> 1995. </year>
Reference: [145] <author> J.J. Odell, V. Valtchev, P. C. Woodland, and S.J. Young. </author> <title> A one pass decoder design for large vocabulary recognition. </title> <booktitle> In Proceedings ARPA Human Language Technology Workshop, </booktitle> <pages> pages 405-410. </pages> <booktitle> ARPA, </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> March </month> <year> 1994. </year>
Reference: [146] <author> Martin Oerder and Hermann Ney. </author> <title> Word graphs: An efficient interface between continuous-speech recognition and language understanding. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 2, </volume> <pages> pages 119-122, </pages> <address> Minneapolis, Minnesota, </address> <month> April </month> <year> 1993. </year> <journal> IEEE. </journal> <volume> 154 </volume>
Reference: [147] <author> Ralph N. Ohde and Donald J. </author> <title> Sharf. Phonetic Analysis of Normal and Abnormal Speech. </title> <publisher> MacMillan Publishing Company, </publisher> <address> New York, </address> <year> 1992. </year>
Reference: [148] <author> Douglas O'Shaughnessy. </author> <title> Speech Communication, </title> <booktitle> chapter 5, </booktitle> <pages> pages 164-203. </pages> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1987. </year>
Reference: [149] <author> Mari Ostendorf, R. Bates, J. Hancock, R. Iyer, A. Kannan, I. Shaik, and M. Siu. </author> <title> The Boston University LVSCR benchmark system. In Conversational Speech Recognition Workshop DARPA Hub-5E Evaluation. </title> <publisher> NIST, </publisher> <month> May </month> <year> 1997. </year>
Reference-contexts: There are initiatives aimed at combating this trend: some research organizations attempt to contribute by attaching their work to existing, state-of-the-art systems. Boston University, for example, entered the 1997 ARPA Switchboard Evaluation in collaboration with an industrial partner <ref> [149] </ref>. Collaborative efforts between research partners will figure more prominently in the future of automatic speech recognition. Joint efforts can be integrated in many different ways.
Reference: [150] <author> Douglas B. Paul. </author> <title> Algorithms for an optimal A fl search and linearizing the search in the stack decoder. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 693-696, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year> <note> IEEE. </note>
Reference: [151] <author> Barbara Peskin, Larry Gillick, Natalie Liberman, Mike Newman, Paul van Mulbregt, and Steven Wegmann. </author> <title> Progress in recognizing conversational telephone speech. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 3, </volume> <pages> pages 1811-1814, </pages> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference: [152] <author> Joe Picone. </author> <title> Public domain speech recognition technology. </title> <type> Personal communication., </type> <year> 1998. </year> <title> Newly established project. </title>
Reference-contexts: SUIF aims towards a modular architecture that is easily extensible and maintainable. Some ASR research groups have already taken steps towards similar frameworks for speech recognition (for example, the public domain speech recognition technology effort headed by Joe Picone <ref> [152] </ref>). 138 Of course, not every research interest can fit into a combination or interface model. Some directions will necessitate the development of complete recognition systems from scratch. Extensive alliances also raise many logistical and political issues.
Reference: [153] <author> D. B. Pisoni, T. D. Carrell, and S. J. Gans. </author> <title> Perception of the duration of rapid spectrum changes in speech and nonspeech signals. </title> <journal> Perception and Psychophysics, </journal> <volume> 34(4) </volume> <pages> 314-322, </pages> <year> 1983. </year>
Reference: [154] <author> B. Plannerer and B. Ruske. </author> <title> Recognition of demisyllable based units using semicontin-uous hidden Markov models. </title> <booktitle> In ICASSP, pages I581-I584, </booktitle> <address> San Francisco, California, </address> <month> March </month> <year> 1992. </year>
Reference: [155] <author> B. Plannerer and B. Ruske. </author> <title> A continuous speech recognition system using phonotactic constraints. </title> <booktitle> In Eurospeech, </booktitle> <pages> pages 859-862, </pages> <address> Berlin, Germany, </address> <month> September </month> <year> 1993. </year>
Reference: [156] <author> Patti Price, William M. Fisher, Jared Bernstein, and David S. Pallett. </author> <title> The DARPA 1000-word Resource Management database for continuous speech recognition. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 651-654, </pages> <address> New York, New York, </address> <month> April </month> <year> 1988. </year> <note> IEEE. </note>
Reference: [157] <author> L. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286, </pages> <month> February </month> <year> 1989. </year>
Reference: [158] <author> L. Rabiner. </author> <title> Applications of speech recognition in the area of telecommunications. </title> <booktitle> In ASRU, </booktitle> <pages> pages 501-510, </pages> <address> Santa Barbara, CA, </address> <month> December </month> <year> 1997. </year> <note> IEEE. </note>
Reference: [159] <author> L. Rabiner and B.-H. Juang. </author> <title> Fundamentals of Speech Recognition, </title> <booktitle> chapter 7.3, </booktitle> <pages> pages 395-400. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1993. </year>
Reference-contexts: A method that avoids the creation of massive HMMs would ease the computational resource problem. Eric Fosler-Lussier (of ICSI) has been working on a two-level Viterbi decoder [54]. The two-level decoding algorithm <ref> [171, 159] </ref> was historically set aside in favor of the more efficient single-pass Viterbi decoding algorithm. The two-level approach, however, is more amenable to combination schemes. The likelihood for each word is calculated for every possible time alignment.
Reference: [160] <author> L. Rabiner, B.-H. Juang, S. Levinson, and M. Sondhi. </author> <title> Recognition of isolated digits using hidden Markov Models with continuous mixture densities. </title> <journal> AT&T Technical Journal, </journal> <volume> 64(6) </volume> <pages> 1211-1234, </pages> <note> July-August 1985. [161] wordscore. Inhouse software at ICSI, </note> <month> February </month> <year> 1997. </year> <title> Scores speech recognition system output. </title> <type> 155 </type>
Reference: [162] <author> W. Reichl and G. Ruske. </author> <title> Syllable segmentation of continuous speech with artificial neural networks. </title> <booktitle> In Eurospeech, </booktitle> <pages> pages 1771-1774, </pages> <address> Berlin, Germany, </address> <month> September </month> <year> 1993. </year>
Reference: [163] <author> Steve Renals and Mike Hochberg. </author> <title> Decoder technology for connectionist large vocabulary speech recognition. </title> <type> Technical Report CUED/F-INFENG/TR.186, </type> <institution> Cambridge University Engineering Department, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Some experiments also used noway <ref> [164, 163, 165] </ref>, a stack decoder using a Viterbi criterion, for its lattice generation capability, and lattice2nbest [166], for its N -best list generation function. * A backoff bigram grammar 7 derived from the training set. * A multiple pronunciation lexicon represented as a set of HMMs, with simple minimum duration <p> This scheme was implemented using a sequence of three different decoders, y0 [88] for its forced alignment capability, noway <ref> [164, 163, 165] </ref> for its lattice generation function, and lat-tice2nbest [166] for its lattice decoding ability. 6 A number of interfacing scripts glued the programs together, enabling state desynchronization to occur over the entire utterance. The decoding sequence used can produce a somewhat different behavior from y0.
Reference: [164] <author> Steve Renals and Mike Hochberg. </author> <title> Efficient search using posterior phone probability estimates. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 596-603, </pages> <address> Detroit, Michigan, </address> <month> May </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: Some experiments also used noway <ref> [164, 163, 165] </ref>, a stack decoder using a Viterbi criterion, for its lattice generation capability, and lattice2nbest [166], for its N -best list generation function. * A backoff bigram grammar 7 derived from the training set. * A multiple pronunciation lexicon represented as a set of HMMs, with simple minimum duration <p> This scheme was implemented using a sequence of three different decoders, y0 [88] for its forced alignment capability, noway <ref> [164, 163, 165] </ref> for its lattice generation function, and lat-tice2nbest [166] for its lattice decoding ability. 6 A number of interfacing scripts glued the programs together, enabling state desynchronization to occur over the entire utterance. The decoding sequence used can produce a somewhat different behavior from y0.
Reference: [165] <author> Steve Renals and Mike Hochberg. </author> <title> Efficient evaluation of the LVCSR search space using the noway decoder. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 149-152, </pages> <address> Atlanta, Georgia, </address> <month> May </month> <year> 1996. </year> <note> IEEE. </note>
Reference-contexts: Some experiments also used noway <ref> [164, 163, 165] </ref>, a stack decoder using a Viterbi criterion, for its lattice generation capability, and lattice2nbest [166], for its N -best list generation function. * A backoff bigram grammar 7 derived from the training set. * A multiple pronunciation lexicon represented as a set of HMMs, with simple minimum duration <p> This scheme was implemented using a sequence of three different decoders, y0 [88] for its forced alignment capability, noway <ref> [164, 163, 165] </ref> for its lattice generation function, and lat-tice2nbest [166] for its lattice decoding ability. 6 A number of interfacing scripts glued the programs together, enabling state desynchronization to occur over the entire utterance. The decoding sequence used can produce a somewhat different behavior from y0.
Reference: [166] <author> Tony Robinson. lattice2nbest. </author> <title> Part of slib package, </title> <year> 1997. </year>
Reference-contexts: Some experiments also used noway [164, 163, 165], a stack decoder using a Viterbi criterion, for its lattice generation capability, and lattice2nbest <ref> [166] </ref>, for its N -best list generation function. * A backoff bigram grammar 7 derived from the training set. * A multiple pronunciation lexicon represented as a set of HMMs, with simple minimum duration modeling. <p> This scheme was implemented using a sequence of three different decoders, y0 [88] for its forced alignment capability, noway [164, 163, 165] for its lattice generation function, and lat-tice2nbest <ref> [166] </ref> for its lattice decoding ability. 6 A number of interfacing scripts glued the programs together, enabling state desynchronization to occur over the entire utterance. The decoding sequence used can produce a somewhat different behavior from y0.
Reference: [167] <author> Aaron E. Rosenberg, Lawrence R. Rabiner, Jay G. Wilpon, and Daniel Kahn. </author> <title> Demisyllable-based isolated word recognition system. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 31(3) </volume> <pages> 713-726, </pages> <month> June </month> <year> 1983. </year>
Reference: [168] <author> Paul Rozin, Susan Poritsky, and Raina Sotsky. </author> <title> American children with reading problems can easily learn to read English represented by Chinese characters. </title> <journal> Science, </journal> <volume> 171(3977) </volume> <pages> 1264-1267, </pages> <month> March </month> <year> 1971. </year>
Reference: [169] <author> G. Ruske, B. Plannerer, and T. Schultz. </author> <title> Stochastic modeling of syllable-based units for continuous speech recognition. </title> <booktitle> In ICSLP, </booktitle> <pages> pages 1503-1506, </pages> <address> Banff, Canada, </address> <month> October </month> <year> 1992. </year>
Reference: [170] <author> Stuart Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach, chapter 3-4. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1994. </year>
Reference: [171] <author> H. Sakoe. </author> <title> Two-level DP-matching- a dynamic programming-based pattern matching algorithm for connected word recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-27(6):588-595, </volume> <month> December </month> <year> 1979. </year>
Reference-contexts: A method that avoids the creation of massive HMMs would ease the computational resource problem. Eric Fosler-Lussier (of ICSI) has been working on a two-level Viterbi decoder [54]. The two-level decoding algorithm <ref> [171, 159] </ref> was historically set aside in favor of the more efficient single-pass Viterbi decoding algorithm. The two-level approach, however, is more amenable to combination schemes. The likelihood for each word is calculated for every possible time alignment.
Reference: [172] <author> H. Sakoe and S. Chiba. </author> <title> Dynamic programming algorithm optimization for spoken word recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 27(1) </volume> <pages> 43-49, </pages> <month> February </month> <year> 1978. </year>
Reference: [173] <author> H. B. Savin and T. G. Bever. </author> <title> The nonperceptual reality of the phoneme. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 9 </volume> <pages> 295-302, </pages> <year> 1970. </year>
Reference: [174] <author> Florien Schiel. </author> <title> Verbmobil concurrent models. </title> <type> Private communication., </type> <year> 1997. </year>
Reference-contexts: This reduced the total number of recognition units over using whole syllables. Each syllable could have been represented with more than 2 states, each with independently derived probability densities, as in the pilot study by Schiel <ref> [174] </ref>. 5 The half-syllable unit appeared to be a reasonable starting point for the investigations in this thesis. The 2-state framework minimally reflects the heterogeneous structure of the syllable; syllable onsets are generally preserved and but syllable codas are often deleted [78].
Reference: [175] <author> R. Schwartz, Y. Chow, S. Roucos, M. Krasner, and J. Makhoul. </author> <title> Improved hidden Markov modeling of phonemes for continuous speech recognition. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 3, </volume> <pages> page 35.6, </pages> <address> San Diego, California, </address> <month> March </month> <year> 1984. </year> <note> IEEE. </note>
Reference: [176] <author> Richard Schwartz and Steve Austin. </author> <title> A comparison of several approximate algorithms. </title> <booktitle> In ICSLP, </booktitle> <volume> volume 1, </volume> <pages> pages 701-704, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year> <journal> IEEE. </journal> <volume> 156 </volume>
Reference: [177] <author> Richard Schwartz and Yen-Lu Chow. </author> <title> The N -best algorithm: An efficient and exact procedure for finding the N most likely sentence hypotheses. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 81-83, </pages> <address> Albuquerque, New Mexico, </address> <month> April </month> <year> 1990. </year> <note> IEEE. </note>
Reference: [178] <author> Richard Schwartz, Jack Klovstad, John Makhoul, and John Sorensen. </author> <title> A preliminary design of a phonetic vocoder based on a diphone model. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 32-35, </pages> <address> Denver, Colorado, </address> <month> April </month> <year> 1980. </year> <note> IEEE. </note>
Reference: [179] <author> Holger Schwenk. </author> <title> Using boosting to improve a hybrid HMM/neural network speech recognizer. Oral presentation at "Machines that Learn" workshop, </title> <address> Snowbird Utah., </address> <month> April </month> <year> 1998. </year>
Reference-contexts: The combined systems described in this chapter did not exceed the parameter count of the 1000-hidden-unit neural network system in Table 5.3. Therefore, the further performance improvements in the combined systems must result from 1 Schwenk has implemented a version of AdaBoost for the Numbers corpus <ref> [179] </ref>. 103 the additional structuring of the parameters. Combining multiple recognition streams exhibited performance advantages over the individual constituent systems at each of the three stages of decoding. Each level, however, (i.e. at the frame, syllable and whole-utterance levels) has separate interpretations and implementation properties.
Reference: [180] <author> J. Segui, U. Frauenfelder, and J. Mehler. </author> <title> Phoneme monitoring, syllable monitoring and lexical access. </title> <journal> British Journal of Psychology, </journal> <volume> 72 </volume> <pages> 471-477, </pages> <year> 1981. </year>
Reference: [181] <author> Juan Segui. </author> <title> The syllable: A basic perceptual unit in speech processing. </title> <editor> In Herman Bouma and Don G. Bouwhius, editors, </editor> <title> Attention and Performance X: </title> <booktitle> Control of Language Processes, Proceedings of the Tenth International Sumposium on Attention and Performance, </booktitle> <pages> pages 165-181, </pages> <address> Hillsdale, New Jersey, 1984. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: [182] <author> Juan Segui, Emmanuel Dupoux, and Jacques Mehler. </author> <title> The role of the syllable in speech segmentation, phoneme identification and lexical access. </title> <editor> In Gerry Altmann, editor, </editor> <booktitle> Cognitive Models of Speech Processing, chapter 12, </booktitle> <pages> pages 263-280. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference: [183] <author> William Shakespeare. </author> <title> The Tempest. Online version. From the Complete Works of William Shakespeare. </title> <address> http://the-tech.mit.edu/Shakespeare/works.html. </address>
Reference: [184] <author> Michael L. Shire. </author> <title> Syllable onset detection from acoustics. </title> <type> Master's thesis, </type> <institution> UC Berke-ley, </institution> <year> 1997. </year>
Reference-contexts: The baseline system used the following elements: 4 Future work that was planned for this decoder was eventually subsumed into other directions which did not require the use of this specially-designed decoder. 5 Stack decoding is also discussed in Section 3.4. 68 decoding <ref> [184] </ref>. * RASTA-PLP8 features, 25-ms frames, calculated every 10 ms. (A total of 18 features per frame.) * Phone-based recognition units. * A 400 hidden unit, fully-connected, single hidden layer neural network with 9 frames (nominally 105 ms) of neural network input context. * A special-purpose decoder with an explicitly represented <p> Acoustically-estimated (non-cheating) onsets, based on acoustic features developed by Shire and Greenberg <ref> [184] </ref>, were incorporated into the speech recognition system resulting in a 10% relative improvement in accuracy with clean speech (OGI Numbers), as shown in Table 7.1.
Reference: [185] <author> Frank K. Soong and Eng-Fong Huang. </author> <title> A tree-trellis based fast search for finding the N best sentence hypotheses in continuous speech recognition. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 705-708, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year> <note> IEEE. </note>
Reference: [186] <author> Switchboard corpus: </author> <title> Recorded telephone conversations. Produced by NIST, </title> <booktitle> sponsored by DARPA, </booktitle> <month> October </month> <year> 1992. </year>
Reference: [187] <author> SUIF compiler system, </author> <year> 1998. </year> <note> More information can be found at http://www-suif.stanford.edu. </note>
Reference-contexts: Modern compilers have also become ponderously large, with many similar performance evaluation issues as speech recognition software. The National Compiler Infrastructure project attempts to address some of these problems: the project uses SUIF (Stanford University Intermediate Format) as a platform for supporting collaboration between compiler researchers <ref> [187] </ref>. SUIF aims towards a modular architecture that is easily extensible and maintainable.
Reference: [188] <author> David L. Thomson. </author> <title> Ten case studies of the effect of field conditions on speech recognition errors. </title> <booktitle> In ASRU, </booktitle> <pages> pages 511-518, </pages> <address> Santa Barbara, CA, </address> <month> December </month> <year> 1997. </year> <note> IEEE. </note>
Reference: [189] <author> Neil Todd. </author> <title> Towards a theory of the principal monaural pathway: Pitch, time and auditory grouping. </title> <editor> In Willam Ainsworth and Steven Greenberg, editors, </editor> <booktitle> Workshop on the Auditory Basis of Speech Perception, </booktitle> <pages> pages 216-221, </pages> <address> Keele University, United Kingdom, </address> <month> July </month> <year> 1996. </year> <note> ESCA. 157 </note>
Reference-contexts: The human brain may employ a rather sparse representation that exhibits most of the temporal dynamics of the speech signal. Todd incorporates the temporal, or rhythmic nature of auditory processing via what he refers to as dynamic spatio-temporal receptive fields <ref> [189, 190] </ref>. Todd and Lee 102 also discuss the combination of simultaneous information into some sort of multimodal, multi-scale sensory representation in the human brain.
Reference: [190] <author> Neil Todd and Christopher Lee. </author> <title> A sensory-motor theory of speech perception: Impli--cations for learning, representation and recognition. </title> <editor> In Steven Greenberg and William Ainsworth, editors, </editor> <title> Listening to Speech: An Auditory Perspective. </title> <publisher> Oxford University Press, </publisher> <year> 1998. </year> <note> To be published. </note>
Reference-contexts: The human brain may employ a rather sparse representation that exhibits most of the temporal dynamics of the speech signal. Todd incorporates the temporal, or rhythmic nature of auditory processing via what he refers to as dynamic spatio-temporal receptive fields <ref> [189, 190] </ref>. Todd and Lee 102 also discuss the combination of simultaneous information into some sort of multimodal, multi-scale sensory representation in the human brain.
Reference: [191] <author> Mikio Tohyama. </author> <title> Response statistics of rooms. </title> <editor> In Malcolm J. Crocker, editor, </editor> <booktitle> Encyclopedia of Acoustics, </booktitle> <volume> volume 2, chapter 77, </volume> <pages> pages 913-923. </pages> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, New York, </address> <year> 1997. </year>
Reference: [192] <author> Rebecca Treiman and Andrea Zukowski. </author> <title> Toward an understanding of English syllabification. </title> <journal> Journal of Memory and Language, </journal> <volume> 29(1) </volume> <pages> 66-85, </pages> <month> February </month> <year> 1990. </year>
Reference: [193] <author> A. P. Varga and R. K. Moore. </author> <title> Hidden Markov model decomposition of speech and noise. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 2, </volume> <pages> pages 845-848, </pages> <address> Albuquerque, New Mexico, </address> <month> April </month> <year> 1990. </year> <note> IEEE. </note>
Reference: [194] <author> A. P. Varga and R. K. Moore. </author> <title> Simultaneous recognition of concurrent speech signals using hidden Markov model decomposition. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 3, </volume> <pages> pages 1175-1178, </pages> <address> Genova, Italy, </address> <month> September </month> <year> 1991. </year> <pages> ESCA. </pages>
Reference: [195] <author> Klara Vicsi and Attila Vig. </author> <title> Text independent neural network/rule based hybrid continuous speech recognition. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 3, </volume> <pages> pages 2201-2204, </pages> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year> <pages> ESCA. </pages>
Reference: [196] <author> Alex Waibel. </author> <title> Prosody and Speech Recognition. </title> <booktitle> Research Notes in Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference: [197] <author> Richard M. Warren. </author> <title> Identification times for phonemic components of graded complexity for spelling of speech. </title> <journal> Perception and Psychoacoustics, </journal> <volume> 9(4) </volume> <pages> 345-349, </pages> <year> 1971. </year>
Reference: [198] <author> Richard M. Warren. </author> <title> Perceptual processing of speech and other perceptual patterns: Some similarities and differences. </title> <editor> In Steven Greenberg and William Ainsworth, editors, </editor> <title> Listening to Speech: An Auditory Perspective. </title> <publisher> Oxford University Press, </publisher> <year> 1998. </year> <note> To appear. </note>
Reference: [199] <author> Richard M. Warren, James A. Bashford, and Daniel A. Gardner. </author> <title> Tweaking the lexicon: Organization of vowel sequences into words. </title> <journal> Perception and Psychophysics, </journal> <volume> 47(5) </volume> <pages> 423-432, </pages> <year> 1990. </year>
Reference: [200] <author> Richard M. Warren, Eric W. Healy, and Magdalene H. Chalikia. </author> <title> The vowel-sequence illusion: Intrasubject stability and intersubject agreement of syllabic forms. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 100(4) </volume> <pages> 2452-2461, </pages> <month> October </month> <year> 1996. </year>
Reference: [201] <author> John Wawrzynek, Krste Asanovic, Brian E. D. Kingsbury, James Beck, David John-son, and Nelson Morgan. Spert-II: </author> <title> A vector microprocessor system. </title> <journal> IEEE Computer, </journal> <volume> 29(3) </volume> <pages> 79-86, </pages> <year> 1996. </year>
Reference: [202] <author> Robert Weide. </author> <note> The Carnegie Mellon Pronouncing Dictionary v0.4. </note> <institution> Carnegie Mellon University, </institution> <year> 1996. </year> <month> 158 </month>
Reference-contexts: The lexicon for this set of experiments included 32 single-pronunciation words, comprising 30 different syllables. These pronunciations were derived from the Carnegie Mellon University (CMU) dictionary <ref> [202] </ref> and syllabified according to standard dictionary principles. In general, the pronunciations reflected orthodox pronunciations of the words.
Reference: [203] <author> Walter Weigel. </author> <title> Continuous speech recognition with vowel-context-independent hidden Markov models for demisyllables. </title> <booktitle> In ICSLP, </booktitle> <volume> volume 2, </volume> <pages> pages 701-704, </pages> <address> Kobe, Japan, </address> <month> November </month> <year> 1990. </year>
Reference: [204] <author> Gethin Williams and Steve Renals. </author> <title> Confidence measures for hybrid HMM/ANN speech recognition. </title> <booktitle> In Eurospeech, </booktitle> <volume> volume 4, </volume> <pages> pages 1955-1958, </pages> <address> Rhodes, Greece, Oc-tober 1997. ESCA. </address>
Reference-contexts: If the different systems both agree on the best answer and that answer is wrong, designing an algorithm that can both detect and correct the error is a much more formidable problem. One possible method is to use local accuracy estimates [206] or measures of confidence <ref> [204] </ref> for both systems. The reliability of such methods, however, can degrade when the recognition system encounters speech that is markedly divergent from the training samples. In these experiments, the differences between systems are distilled down to the proportion of exactly alike errors in the respective recognition outputs.
Reference: [205] <author> P. C. Woodland, C. J. Leggetter, J. J. Odell, V. Valtchev, and S. J. Young. </author> <title> The development of the 1994 HTK large vocabulary speech recognition system. </title> <booktitle> In Spoken Language Systems Technology Workshop, </booktitle> <address> Austin, Texas, </address> <month> January </month> <year> 1995. </year> <pages> ARPA. </pages>
Reference: [206] <author> Kevin Woods, W. Philip Kegelmeyer Jr., and Kevin Bowyer. </author> <title> Combination of multiple classifiers using local accuracy estimates. </title> <journal> IEEE Transactions of Pattern Analysis and Machine Intelligence, </journal> <volume> 19(4) </volume> <pages> 405-410, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: If the different systems both agree on the best answer and that answer is wrong, designing an algorithm that can both detect and correct the error is a much more formidable problem. One possible method is to use local accuracy estimates <ref> [206] </ref> or measures of confidence [204] for both systems. The reliability of such methods, however, can degrade when the recognition system encounters speech that is markedly divergent from the training samples.
Reference: [207] <author> Charles Clayton Wooters. </author> <title> Lexical Modelling in a Speaker Independent Speech Understanding System. </title> <type> PhD thesis, </type> <institution> UC Berkeley, </institution> <month> November </month> <year> 1993. </year> <note> ICSI Technical Report TR-93-068. </note>
Reference-contexts: Words can vary in pronunciation depending on factors such as phonological context and individual speaker characteristics. Multiple-pronunciation models derived from training data more accurately characterize the representative acoustic information for each word than canonical definitions. This results in somewhat higher accuracy, a documented effect in the ICSI system <ref> [207] </ref>. Forced alignment and embedded training techniques were not used to optimize the training labels and lexicon for this experiment. Forced alignment and embedded training typically cause the training labels and lexicon to become closely matched and tuned to the stochastic properties learned by the neural network.
Reference: [208] <author> Su-Lin Wu. </author> <title> Properties of stochastic perceptual auditory-event-based models for automatic speech recognition. </title> <type> Master's thesis, </type> <institution> UC Berkeley, </institution> <month> May </month> <year> 1995. </year> <note> Also appears as ICSI Technical Report TR-95-023. </note>
Reference: [209] <author> Su-Lin Wu, Brian E. D. Kingsbury, Nelson Morgan, and Steven Greenberg. </author> <title> Incorporating information from syllable-length time scales into automatic speech recognition. </title> <booktitle> In ICASSP, </booktitle> <address> Seattle, Washington, </address> <month> April </month> <year> 1998. </year> <note> IEEE. </note>
Reference-contexts: This work was briefly presented at the International Conference on Acoustics, Speech and Signal Processing, 1998 <ref> [209] </ref>. 116 Clean Reverberant Variant (paired with Baseline) Number of Error Sents Percentage of Total Sents Number of Error Sents Percentage of Total Sents RASTA + phones, 17 frames 286 23.7% 863 71.6% RASTA + half-syllables, 17 frames 360 29.9% 920 76.3% modulation spectrogram + phones, 17 frames 378 32.1% 936
Reference: [210] <author> Su-Lin Wu, Michael L. Shire, Steven Greenberg, and Nelson Morgan. </author> <title> Integrating syllable boundary information into speech recognition. </title> <booktitle> In ICASSP, volume 1, </booktitle> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference: [211] <author> Steve Young. </author> <title> Large vocabulary continuous speech recognition: A review. </title> <booktitle> In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <pages> pages 3-28, </pages> <address> Snowbird, Utah, </address> <month> December </month> <year> 1995. </year> <note> IEEE. </note>
Reference: [212] <author> Steve Young, Julian Odell, Dave Ollason, Valtcho Valtchev, and Phil Woodland. </author> <title> The HTK Book for HTK Version 2.1. </title> <address> Cambridge University, </address> <year> 1997. </year> <month> 159 </month>
Reference-contexts: Small research groups are currently encountering significant difficulties in developing their own recognition systems that are competitive with the offerings of larger, established players. Many small groups develop their systems with the help of HTK (Hidden Markov Model Tool Kit) <ref> [212] </ref>, the CSLU Speech Toolkit [22], or STRUT (Speech Training and Recognition Unified Tool) [15], which provide many of the processing elements needed. ASR has also grown into viable commercial products. As the market for speech recognition applications enlarges, consumers will begin to drive the industry.
References-found: 211

