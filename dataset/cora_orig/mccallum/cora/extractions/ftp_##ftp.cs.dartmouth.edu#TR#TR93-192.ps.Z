URL: ftp://ftp.cs.dartmouth.edu/TR/TR93-192.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR93-192/
Root-URL: http://www.cs.dartmouth.edu
Title: OFF-LINE CURSIVE HANDWRITING RECOGNITION USING STYLE PARAMETERS  
Author: Berrin A. Yanikoglu Peter A. Sandon 
Pubnum: Technical Report PCS-TR93-192  
Abstract-found: 0
Intro-found: 1
Reference: [Bur80] <author> D.J. Burr. </author> <title> Designing a handwriting reader. </title> <address> pages 715-722, </address> <year> 1980. </year>
Reference-contexts: The second is the angle of the 10 vector joining the center of gravities of the top and bottom halves of the letter, assuming uniform weight distribution <ref> [Bur80] </ref>. We combine the two methods since there are problems associated with each of them. For example, the edge detection method may estimate the slant of a non-slanted x as 30 degrees due to its diagonal stroke.
Reference: [Bur87] <author> D.J. Burr. </author> <title> Experiments with a connectionist text reader. </title> <booktitle> In IEEE First International Conference on Neural Networks, </booktitle> <volume> volume 4, </volume> <pages> pages 717-724, </pages> <year> 1987. </year>
Reference-contexts: The sample page was written discretely by a writer (W1) who also wrote a training set. The recognition rates were 81% for letters and 93% for words. These results are similar to the best handprinted word recognition rates [KHB89] <ref> [Bur87] </ref>. In the second experiment, we replicated the first experiment, except that the page was written cursively this time. 70% of the words were recognized correctly and 76% were in the top three word choices. All of the words were correctly segmented.
Reference: [DH73] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year> <month> 18 </month>
Reference-contexts: It is based on the algorithm that finds the outline of an object, in <ref> [DH73] </ref>.
Reference: [EFU90] <author> S. Edelman, T. Flash, and S. Ullman. </author> <title> Reading cursive handwriting by alignment of letter prototypes. </title> <journal> International Journal of Computer Vision, </journal> <volume> 5(3) </volume> <pages> 303-331, </pages> <year> 1990. </year>
Reference-contexts: Our word recognition results compare favorably to those of Srihari and Bozinovic [SR87] and to the lexical word recognition performance of the system by Edelman et al <ref> [EFU90] </ref>, although an exact comparison is not possible (see [YS93] for a summary of related work). We are currently training the letter recognition network with an expanded training set that includes some cursive letters and collecting more test data to test the system thoroughly.
Reference: [HP90] <author> J. Hampshire II and B. PearlMutter. </author> <title> Equivalence proofs for multi-layer perceptron classifiers and the bayesian discriminant function. </title> <editor> In D. Touretzky, J. Elman, T. Sejnowski, and G. Hinton, editors, </editor> <booktitle> Connectionist Models, </booktitle> <pages> pages 159-172. </pages> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1990. </year>
Reference-contexts: Hampshire and Pearlmutter, and others have shown that a network with enough connections approximates the Bayesian discriminant function such that the output values approximate the a posteriori probabilities, if trained with sufficient training data and using mean squared error estimation <ref> [HP90] </ref>, [RRK + 90]. 8 Word recognition To recognize the image of a word, we first assume that it is in the lexicon 3 and rate each word to find the one that best matches the image. Two hypotheses for finding the likelihood of words are compared for performance.
Reference: [KHB89] <author> A. Kundu, Y. He, and P. Bahl. </author> <title> Recognition of handwritten word: First and second order hidden markov model based approach. </title> <journal> Pattern Recognition, </journal> <volume> 22 </volume> <pages> 283-297, </pages> <year> 1989. </year>
Reference-contexts: The sample page was written discretely by a writer (W1) who also wrote a training set. The recognition rates were 81% for letters and 93% for words. These results are similar to the best handprinted word recognition rates <ref> [KHB89] </ref> [Bur87]. In the second experiment, we replicated the first experiment, except that the page was written cursively this time. 70% of the words were recognized correctly and 76% were in the top three word choices. All of the words were correctly segmented.
Reference: [Kon82] <author> A.G. Konheim. </author> <title> Cryptography: A Primer. </title> <publisher> John Wiley and Sons, </publisher> <address> NewYork, </address> <year> 1982. </year>
Reference-contexts: A Markov process is a stochastic process such that the probability of going into a state depends only on the current state [RJ86] [KS60] <ref> [Kon82] </ref>. In other words, knowledge of all the previous states does not give any more information than that of the previous state.
Reference: [KS60] <author> J. Kemeny and L. Snell. </author> <title> Finite Markov Chains. </title> <publisher> Cambridge University Press, </publisher> <year> 1960. </year>
Reference-contexts: A Markov process is a stochastic process such that the probability of going into a state depends only on the current state [RJ86] <ref> [KS60] </ref> [Kon82]. In other words, knowledge of all the previous states does not give any more information than that of the previous state.
Reference: [RJ86] <author> L.R. Rabiner and B.H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> AASP magazine, </journal> <volume> 3(1) </volume> <pages> 4-16, </pages> <year> 1986. </year> <month> 19 </month>
Reference-contexts: A Markov process is a stochastic process such that the probability of going into a state depends only on the current state <ref> [RJ86] </ref> [KS60] [Kon82]. In other words, knowledge of all the previous states does not give any more information than that of the previous state.
Reference: [RRK + 90] <author> D. W. Ruck, S. K. Rogers, M. Kabrisky, M. E. Oxley, and B. W. Suter. </author> <title> The mul-tilayer perceptron as an approximation to a Bayes optimal discriminant function. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(4) </volume> <pages> 296-298, </pages> <year> 1990. </year>
Reference-contexts: Hampshire and Pearlmutter, and others have shown that a network with enough connections approximates the Bayesian discriminant function such that the output values approximate the a posteriori probabilities, if trained with sufficient training data and using mean squared error estimation [HP90], <ref> [RRK + 90] </ref>. 8 Word recognition To recognize the image of a word, we first assume that it is in the lexicon 3 and rate each word to find the one that best matches the image. Two hypotheses for finding the likelihood of words are compared for performance.
Reference: [SR87] <author> S. Srihari and R. Bozinovic. </author> <title> A multi-level perception approach to reading cursive script. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 217-255, </pages> <year> 1987. </year>
Reference-contexts: Since the algorithm employs 8-connectedness, the image is first smoothed using a 3x3 Gaussian mask to avoid problems caused by thin, slanted strokes. 3 Finding the reference lines The reference lines of a text line <ref> [SR87] </ref> are the four horizontal lines that mark the top of the ascenders, the top of the main bodies of letters, the baseline and the bottom of the descenders. They will be referred to as l1, l2, l3 and l4, from top to bottom. <p> The segmentation results and the recognition performance on the second test set which was almost pure cursive handwriting shows the success of our segmentation algorithm. Our word recognition results compare favorably to those of Srihari and Bozinovic <ref> [SR87] </ref> and to the lexical word recognition performance of the system by Edelman et al [EFU90], although an exact comparison is not possible (see [YS93] for a summary of related work).
Reference: [TSW90] <author> C.C. Tappert, C.Y. Suen, and T. Wakahara. </author> <title> The state of the art in on-line handwriting recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 787-808, </pages> <year> 1990. </year>
Reference-contexts: On-line devices can capture dynamic characteristics of the handwriting, such as the number and order of strokes, and velocity and pressure changes, but the information must be processed in real time. Off-line approaches have less information available for recognition, but have no real-time constraints <ref> [TSW90] </ref>. The system we are developing is for off-line recognition of cursive, handwritten text. The input to the system is the scanned image of a page of cursive handwriting, written roughly along the rulings.
Reference: [YS93] <author> Berrin A. Yanikoglu and Peter A. Sandon. </author> <title> Off-line cursive handwriting recognition using neural networks. </title> <booktitle> In SPIE Conference on Applications of Artificial Neural Networks, </booktitle> <year> 1993. </year> <month> 20 </month>
Reference-contexts: Our word recognition results compare favorably to those of Srihari and Bozinovic [SR87] and to the lexical word recognition performance of the system by Edelman et al [EFU90], although an exact comparison is not possible (see <ref> [YS93] </ref> for a summary of related work). We are currently training the letter recognition network with an expanded training set that includes some cursive letters and collecting more test data to test the system thoroughly.
References-found: 13

