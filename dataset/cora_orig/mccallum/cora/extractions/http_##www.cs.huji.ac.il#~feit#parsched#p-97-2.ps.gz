URL: http://www.cs.huji.ac.il/~feit/parsched/p-97-2.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched97.html
Root-URL: http://www.cs.huji.ac.il
Title: Using Queue Time Predictions for Processor Allocation  
Author: Allen B. Downey 
Affiliation: University of California at Berkeley San Diego Supercomputer Center  
Abstract: When a moldable job is submitted to a space-sharing parallel computer, it must choose whether to begin execution on a small, available cluster or wait in queue for more processors to become available. To make this decision, it must predict how long it will have to wait for the larger cluster. We propose statistical techniques for predicting these queue times, and develop an allocation strategy that uses these predictions. We present a workload model based on observed workloads at the San Diego Supercomputer Center and the Cornell Theory Center, and use this model to drive simulations of various allocation strategies. We find that prediction-based allocation not only improves the turnaround time of individual jobs; it also improves the utilization of the system as a whole. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Su-Hui Chiang, Rajesh K. Mansharamani, and Mary K. Vernon. </author> <title> Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies. </title> <booktitle> In Proceedings of the ACM Sigmet-rics Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1994. </year>
Reference-contexts: In most cases, this metric is average turnaround time [10] [17] [16] [9] <ref> [1] </ref> [15], although some studies also consider throughput [13]. Rosti, Smirni et al. use power, which is the ratio of throughput to mean response time [14]. <p> Several studies have shown that this strategy performs well for a range of workloads [17] [9] [12] [19] <ref> [1] </ref> [2]. The problem with this strategy is that it forces users to accept decisions that are contrary to their interests. For example, if there is a large job at the head of the queue, it will be forced to run on any available cluster, even a single processor.
Reference: [2] <author> Allen B. Downey. </author> <title> A parallel workload model and its implications for processor allocation. </title> <type> Technical Report CSD-96-922, </type> <institution> University of California at Berkeley, </institution> <year> 1996. </year>
Reference-contexts: In prior work we used a parallel extension of slowdown, which is the ratio of the actual response time of a job to the time it would have taken on a dedicated machine <ref> [2] </ref>. Feitelson and Rudolph use a different formulation of slowdown [7]. There are several common problems with system-centric schedulers: Starvation: For many system-centric schedulers there is an identifiable class of jobs that receives unacceptable service. <p> Several studies have shown that this strategy performs well for a range of workloads [17] [9] [12] [19] [1] <ref> [2] </ref>. The problem with this strategy is that it forces users to accept decisions that are contrary to their interests. For example, if there is a large job at the head of the queue, it will be forced to run on any available cluster, even a single processor. <p> The strategy that users would choose is the one that maximizes time savings per job. Under OPT, the average time savings per job is 13.8 minutes. 6.1 STUB: stubborn self-interest Previously <ref> [2] </ref>, we evaluated a simple strategy, STUB, in which users impose a minimum cluster size on their jobs of f A, where f is some fraction between 0 and 1.
Reference: [3] <author> Allen B. Downey. </author> <title> A model for speedup of parallel programs. </title> <type> Technical Report CSD-97-933, </type> <institution> University of Cal-ifornia at Berkeley, </institution> <year> 1997. </year> <month> 12 </month>
Reference-contexts: Thus, it may not be correct to use a concrete workload from one system to simulate and evaluate another. Our goal is to create an abstract workload that separates the characteristics of the job mix from the effect of the system. Previously <ref> [3] </ref>, we proposed a model of moldable jobs that characterizes each job by three parameters: L, the sequential lifetime of the job, A, the average parallelism, and , which measures the job's variance in parallelism. <p> In previous work we showed that this family of speedup profiles captures, at least approximately, the behavior of a variety of parallel scientific applications on a variety of architectures <ref> [3] </ref>. 2.2 Low variance model, 1 program with low variance in parallelism. The degree of parallelism is A for all but some fraction of the duration (0 1). The remaining time is divided between a sequential component and a high-parallelism component. <p> In previous work, we proposed a way to infer this value from observed speedup curves <ref> [3] </ref>. To test this technique, we collected speedup curves for a variety of scientific applications running on a variety of parallel computers. We found that the parameter , which approximates the coefficient of variance of parallelism, was typically in the range 0-2, with occasional higher values.
Reference: [4] <author> Allen B. Downey. </author> <title> Predicting queue times on space--sharing parallel computers. </title> <booktitle> In 11th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1997. </year> <note> To appear. Also available as University of California technical report number CSD-96-906. </note>
Reference-contexts: For this study, we use a uniform distribution between 0 and 2. 4 Predicting queue times In previous work we presented statistical techniques for predicting the remaining queue time for a job at the head of the queue <ref> [4] </ref>. Since we use these predictions in Section 6.3, we summarize the techniques here. <p> As a result, our queue time predictions are not very accurate. In the real systems we have examined, the information provided by users significantly improves the quality of the predictions <ref> [4] </ref>. We would like to investigate the effect of this improvement on our results. As part of the DOCT project [11] we are in the process of implementing system agents that provide predicted queue times on space-sharing parallel machines.
Reference: [5] <author> Derek L. Eager, John Zahorjan, and Edward L. La-zowska. </author> <title> Speedup versus efficiency in parallel systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(3) </volume> <pages> 408-423, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: When = 0 the curve matches the theoretical upper bound for speedup| bound at first by the "hardware limit" (linear speedup) and then by the "software limit" (the average parallelism A). As approaches infinity, the curve approaches the theoretical lower bound on speedup derived by Eager et al. <ref> [5] </ref>: S min (n) = An=(A + n 1). Of course, for many jobs there will be ranges of n where this model is inapplicable. For example, a job with large memory requirements will run poorly (or not at all) when n is small.
Reference: [6] <author> Dror G. Feitelson and Bill Nitzberg. </author> <title> Job characteristics of a production parallel scientific workload on the NASA Ames iPSC/860. </title> <editor> In D. G. Feitelson and L. Rudolph, editors, </editor> <title> Job Scheduling Strategies for Parallel Processing, </title> <publisher> Springer-Verlag LNCS Vol 949, </publisher> <pages> pages 337-360, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Thus, workloads in current supercomputing environments are made up almost entirely of static jobs. There is some evidence that the fraction of moldable jobs is significant <ref> [6] </ref>, and it is likely to increase as users 1 shift to higher-level programming models. <p> We compare these schedules according to several performance metrics. Our simulations try to capture the daily work cycle that has been observed in several supercomputing environments (the Intel iPSC/860 at NASA Ames and the Paragon at SDSC <ref> [6] </ref> [20]): * In early morning there are few arrivals, utilization is at its lowest, and queue lengths are short. * During the day, the arrival rate increases and jobs accumulate in queue.
Reference: [7] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Evaluation of design choices for gang scheduling using distributed hierarchical control. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 35 </volume> <pages> 18-34, </pages> <year> 1996. </year>
Reference-contexts: In prior work we used a parallel extension of slowdown, which is the ratio of the actual response time of a job to the time it would have taken on a dedicated machine [2]. Feitelson and Rudolph use a different formulation of slowdown <ref> [7] </ref>. There are several common problems with system-centric schedulers: Starvation: For many system-centric schedulers there is an identifiable class of jobs that receives unacceptable service. For example, utilization-maximizing schedulers tend to starve jobs with low parallel efficiency and odd-sized jobs that cause fragmentation. Turnaround-minimizing schedulers tend to starve large jobs.
Reference: [8] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Towards convergence in job schedulers for parallel supercomputers. </title> <editor> In D. G. Feitelson and L. Rudolph, editors, </editor> <title> Job Scheduling Strategies for Parallel Processing, </title> <publisher> Springer-Verlag LNCS Vol 1162, </publisher> <pages> pages 1-26, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: The goal of this paper is to evaluate the usefulness of these predictions for processor allocation. 1.1 Adaptive Jobs Feitelson and Rudolph <ref> [8] </ref> propose the following classification of adaptive parallel jobs: rigid jobs can run only on a fixed cluster size; moldable jobs can be configured to run on a range of cluster sizes, but once they begin execution, they cannot change cluster size.
Reference: [9] <author> Dipak Ghosal, Giuseppe Serazzi, and Satish K. Tripathi. </author> <title> The processor working set and its use in scheduling multiprocessor systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(5) </volume> <pages> 443-453, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In most cases, this metric is average turnaround time [10] [17] [16] <ref> [9] </ref> [1] [15], although some studies also consider throughput [13]. Rosti, Smirni et al. use power, which is the ratio of throughput to mean response time [14]. <p> Our baseline strategy is AVG, which assigns free processors to queued jobs in FIFO order, giving each job no more than A processors, where A is the average parallelism of the job. Several studies have shown that this strategy performs well for a range of workloads [17] <ref> [9] </ref> [12] [19] [1] [2]. The problem with this strategy is that it forces users to accept decisions that are contrary to their interests.
Reference: [10] <author> Shikharesh Majumdar, Derek L. Eager, and Richard B. Bunt. </author> <title> Scheduling in multiprogrammed parallel systems. </title> <booktitle> In Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 104-113, </pages> <year> 1988. </year>
Reference-contexts: This paper addresses scheduling strategies for moldable jobs. 1.2 System-centric scheduling Many simulation and analytic studies have examined the performance of system-centric allocation strategies, that is, strategies designed to maximize an aggregate performance metric without regard for individual jobs. In most cases, this metric is average turnaround time <ref> [10] </ref> [17] [16] [9] [1] [15], although some studies also consider throughput [13]. Rosti, Smirni et al. use power, which is the ratio of throughput to mean response time [14].
Reference: [11] <author> Reagan Moore and Richard Klobuchar. </author> <title> DOCT home page http://www.sdsc.edu/DOCT. San Diego Supercomputer Center, </title> <year> 1996. </year>
Reference-contexts: In the real systems we have examined, the information provided by users significantly improves the quality of the predictions [4]. We would like to investigate the effect of this improvement on our results. As part of the DOCT project <ref> [11] </ref> we are in the process of implementing system agents that provide predicted queue times on space-sharing parallel machines. Users can take advantage of this information to choose what jobs to run, when to run them, and how many processors to allocate for each.
Reference: [12] <author> Vijay K. Naik, Sanjeev K. Setia, and Mark S. Squillante. </author> <title> Performance analysis of job scheduling policies in parallel supercomputing environments. </title> <booktitle> In Supercomputing '93 Conference Proceedings, </booktitle> <pages> pages 824-833, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Our baseline strategy is AVG, which assigns free processors to queued jobs in FIFO order, giving each job no more than A processors, where A is the average parallelism of the job. Several studies have shown that this strategy performs well for a range of workloads [17] [9] <ref> [12] </ref> [19] [1] [2]. The problem with this strategy is that it forces users to accept decisions that are contrary to their interests.
Reference: [13] <author> Eric W. Parsons and Kenneth C. Sevcik. </author> <title> Coordinated allocation of memory and processors in multiprocessors. </title> <booktitle> In Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 57-67, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: In most cases, this metric is average turnaround time [10] [17] [16] [9] [1] [15], although some studies also consider throughput <ref> [13] </ref>. Rosti, Smirni et al. use power, which is the ratio of throughput to mean response time [14].
Reference: [14] <author> Emilia Rosti, Evgenia Smirni, Lawrence W. Dowdy, Giuseppe Serazzi, and Brian M. Carlson. </author> <title> Robust partitioning policies of multiprocessor systems. Performance Evaluation, </title> <address> 19(2-3):141-165, </address> <month> Mar </month> <year> 1994. </year>
Reference-contexts: In most cases, this metric is average turnaround time [10] [17] [16] [9] [1] [15], although some studies also consider throughput [13]. Rosti, Smirni et al. use power, which is the ratio of throughput to mean response time <ref> [14] </ref>. In prior work we used a parallel extension of slowdown, which is the ratio of the actual response time of a job to the time it would have taken on a dedicated machine [2]. Feitelson and Rudolph use a different formulation of slowdown [7].
Reference: [15] <author> Emilia Rosti, Evgenia Smirni, Giuseppe Serazzi, and Lawrence W. Dowdy. </author> <title> Analysis of non-work-conserving processor partitioning policies. </title> <editor> In D. G. Feitelson and L. Rudolph, editors, </editor> <title> Job Scheduling Strategies for Parallel Processing, </title> <publisher> Springer-Verlag LNCS Vol 949, </publisher> <pages> pages 165-181, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: In most cases, this metric is average turnaround time [10] [17] [16] [9] [1] <ref> [15] </ref>, although some studies also consider throughput [13]. Rosti, Smirni et al. use power, which is the ratio of throughput to mean response time [14].
Reference: [16] <author> Sanjeev K. Setia and Satish K. Tripathi. </author> <title> A comparative analysis of static processor partitioning policies for parallel computers. </title> <booktitle> In Proceedings of the Internation-sal Workshop on Modeling and Simulation of Computer and Telecommunications Systems (MASCOTS), </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: In most cases, this metric is average turnaround time [10] [17] <ref> [16] </ref> [9] [1] [15], although some studies also consider throughput [13]. Rosti, Smirni et al. use power, which is the ratio of throughput to mean response time [14].
Reference: [17] <author> Kenneth C. Sevcik. </author> <title> Characterizations of parallelism in applications and their use in scheduling. </title> <journal> Performance Evaluation Review, </journal> <volume> 17(1) </volume> <pages> 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: This paper addresses scheduling strategies for moldable jobs. 1.2 System-centric scheduling Many simulation and analytic studies have examined the performance of system-centric allocation strategies, that is, strategies designed to maximize an aggregate performance metric without regard for individual jobs. In most cases, this metric is average turnaround time [10] <ref> [17] </ref> [16] [9] [1] [15], although some studies also consider throughput [13]. Rosti, Smirni et al. use power, which is the ratio of throughput to mean response time [14]. <p> The remaining time is divided between a sequential component and a high-parallelism component. The average parallelism of this profile is A; the variance is V = (A 1) 2 . 1 The parallelism profile is the distribution of potential paral lelism during the execution of a program <ref> [17] </ref>. 3 a) use to derive our speedup model. <p> Our baseline strategy is AVG, which assigns free processors to queued jobs in FIFO order, giving each job no more than A processors, where A is the average parallelism of the job. Several studies have shown that this strategy performs well for a range of workloads <ref> [17] </ref> [9] [12] [19] [1] [2]. The problem with this strategy is that it forces users to accept decisions that are contrary to their interests.
Reference: [18] <author> Joseph Skovira, Waiman Chan, Honbo Zhou, and David Lifka. </author> <title> The EASY - LoadLeveler API project. </title> <editor> In D. G. Feitelson and L. Rudolph, editors, </editor> <title> Job Scheduling Strategies for Parallel Processing, </title> <publisher> Springer-Verlag LNCS Vol 1162, </publisher> <pages> pages 41-47, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Backfilling is the process of allowing these jobs to run, on the condition that they not delay the large job. The EASY scheduler uses this strategy <ref> [18] </ref> for rigid jobs. In future work we plan to add backfilling to our strategy for moldable jobs. 1.5 Outline Section 2 presents speedup model we use in Section 3 to develop an abstract workload model.
Reference: [19] <author> Evgenia Smirni, Emilia Rosti, Lawrence W. Dowdy, and Giuseppe Serazzi. </author> <title> Evaluation of multiprocessor allocation policies. </title> <type> Technical report, </type> <institution> Vanderbilt University, </institution> <year> 1993. </year>
Reference-contexts: Our baseline strategy is AVG, which assigns free processors to queued jobs in FIFO order, giving each job no more than A processors, where A is the average parallelism of the job. Several studies have shown that this strategy performs well for a range of workloads [17] [9] [12] <ref> [19] </ref> [1] [2]. The problem with this strategy is that it forces users to accept decisions that are contrary to their interests. For example, if there is a large job at the head of the queue, it will be forced to run on any available cluster, even a single processor.
Reference: [20] <author> Kurt Windisch, Virginia Lo, Dror Feitelson, Bill Nitzberg, and Reagan Moore. </author> <title> A comparison of workload traces from two production parallel machines. </title> <booktitle> In 6th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <year> 1996. </year> <month> 13 </month>
Reference-contexts: We compare these schedules according to several performance metrics. Our simulations try to capture the daily work cycle that has been observed in several supercomputing environments (the Intel iPSC/860 at NASA Ames and the Paragon at SDSC [6] <ref> [20] </ref>): * In early morning there are few arrivals, utilization is at its lowest, and queue lengths are short. * During the day, the arrival rate increases and jobs accumulate in queue.
References-found: 20

