URL: http://www.cs.ucl.ac.uk/staff/A.Hunter/ipmu92.ps
Refering-URL: http://www.cs.ucl.ac.uk/staff/A.Hunter/papers.html
Root-URL: http://www.cs.ucl.ac.uk
Email: email: j.cussens@elm.cc.kcl.ac.uk  email: abh@doc.ic.ac.uk  
Phone: 2  
Title: Using Maximum Entropy in a Defeasible Logic with Probabilistic Semantics  
Author: James Cussens and Anthony Hunter 
Affiliation: Department of Computing, Imperial College 180 Queen's  
Address: King's College Strand, London, WC2R 2LS, UK  Gate, London, SW7 2BZ, UK  
Note: 1 Centre for Logic and Probability in IT,  
Abstract: In this paper we make defeasible inferences from conditional probabilities using the Principle of Total Evidence. This gives a logic that is a simple extension of the axiomatization of probabilistic logic as defined by Halpern's AX 1 . For our consequence relation, the reasoning is further justified by an assumption of the typicality of individuals mentioned in the data. For databases which do not determine a unique probability distribution, we select by default the distribution with Maximum Entropy. We situate this logic in the context of preferred models semantics. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Fahiem Bacchus. </author> <title> Representing and Reasoning with Probabilistic Knowledge: A Logical Approach to Probabilities. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference: 2. <author> P. Cheeseman. </author> <title> A method of computing generalized bayesian probability values. </title> <booktitle> In IJCAI'83, </booktitle> <pages> pages 198-202. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference: 3. <author> James Cussens and Anthony Hunter. </author> <title> Using defeasible logic for a window on a probabilistic database: some preliminary notes. </title> <booktitle> In Symbolic and Qualitative Approaches for Uncertainty, </booktitle> <pages> pages 146-152. </pages> <booktitle> Lecture Notes in Computer Science 548, </booktitle> <publisher> Springer, </publisher> <year> 1991. </year>
Reference: 4. <author> Dov Gabbay. </author> <title> Theoretical foundations for non-monotonic reasoning in expert systems. </title> <editor> In K. R. Apt, editor, </editor> <booktitle> Proceedings NATO Advanced Study Institute on Logics and Models of Concurrent Systems, </booktitle> <pages> pages 439-457. </pages> <publisher> Springer, </publisher> <year> 1985. </year>
Reference-contexts: Indeed, it is straightforward to show that if (; ) is satisfiable then [[; ]] is non-empty, since there is no consistent (; ) and ff such that (; ) j ff and (; ) j :ff. In comparison with Gabbay's axiomatization of the consequence relation <ref> [4] </ref>, cut holds if no conditional probability values deducible from lie in the interval ( 2 ; ]. Similarly, cautious monotonicity holds if no conditional probability values deducible from lie in the set ((1 ) 2 ; 1 ]. However, in general, neither cut nor cautious monotonicity hold.
Reference: 5. <author> B. Grosof. </author> <title> Non-monotonicity in probabilistic reasoning. </title> <editor> In K. R. Apt, editor, </editor> <booktitle> Uncertainty in Artificial Intelligence 3, </booktitle> <pages> pages 237-249. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: Viewing the use of Maximum Entropy as capturing a notion of relevance in probabilistic data follows the approach discussed by [11]. We can also view the use of Maximum Entropy as an inheritance principle as discussed by <ref> [5] </ref>.
Reference: 6. <author> Joseph Y. Halpern. </author> <title> An analysis of first-order logics of probability. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 311-350, </pages> <year> 1990. </year>
Reference-contexts: In particular, using probability theory gives us an opportunity to clarify aspects of non-monotonic logics. Here we show how using established principles from probability theory, together with the axioms of probability, can provide a useful semantic foundation for a de-feasible logic. For the logic we extend the approach of <ref> [6] </ref> to allow non-monotonic reasoning from a database of conditional probability statements and ground formulae. We justify such non-monotonic inferences by the Principle of Total Evidence. We use this when we have two or more conditional probabilities all of whose conditions are satisfied. <p> Proof. See <ref> [6] </ref>. ut Lemma 2. Using the definition of preferred entailment, the following equivalence holds: (; ) j ff iff (; ) j= w oe () (ff oe j ( V Proof. Assume (; ) j ff holds, then choose an arbitrary M such that M j= (; ).
Reference: 7. <author> H. Kautz and B. Selman. </author> <title> Hard problems for simple default logics. </title> <journal> Artificial Intelligence, </journal> <volume> 49 </volume> <pages> 243-279, </pages> <year> 1991. </year>
Reference-contexts: This is in contrast to existing non-monotonic logics where there is no analogue to completing the database, but the reasoning is intractable| for example for default logic <ref> [7] </ref>.
Reference: 8. <author> B. Lewis. </author> <title> Approximating probability distributions to reduce storage requirements. </title> <journal> Information and Control, </journal> <volume> 2 </volume> <pages> 214-225, </pages> <year> 1959. </year>
Reference: 9. <author> N. Nilsson. </author> <title> Probabilistic logic. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 71-87, </pages> <year> 1986. </year>
Reference: 10. <author> J. Paris and A. Vencovska. </author> <title> A note on the inevitability of maximum entropy. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 4 </volume> <pages> 183-223, </pages> <year> 1990. </year>
Reference-contexts: In this example the maximum entropy completion was straightforward. Unfortunately, in general this does not seem to be the case. Quoting from <ref> [10] </ref>, `if we accept maximum entropy,: : : , the problem of actually computing weights to any reasonable approximation is NP-hard and thus probably infeasible'.
Reference: 11. <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: In this situation, we can also see that Maximum Entropy is introducing an assumption that new properties are irrelevant to the probability distribution unless otherwise stated. Viewing the use of Maximum Entropy as capturing a notion of relevance in probabilistic data follows the approach discussed by <ref> [11] </ref>. We can also view the use of Maximum Entropy as an inheritance principle as discussed by [5].
Reference: 12. <author> Y. Shoham. </author> <title> Reasoning about Change. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Using preferred models builds on the non-monotonic logics framework initially proposed by <ref> [12] </ref>.
References-found: 12

