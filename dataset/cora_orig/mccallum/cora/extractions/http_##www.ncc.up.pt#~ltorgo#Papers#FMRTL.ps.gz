URL: http://www.ncc.up.pt/~ltorgo/Papers/FMRTL.ps.gz
Refering-URL: http://www.ncc.up.pt/~ltorgo/Papers/list_pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email  
Title: Functional Models for Regression Tree Leaves  
Author: Lus Torgo R. Campo Alegre, Porto PORTUGAL 
Web: ltorgo ncc.up.pt WWW http://www.ncc.up.pt/~ltorgo  
Affiliation: LIACC University of Porto  
Abstract: This paper presents a study about functional models for regression tree leaves. We evaluate experimentally several alternatives to the averages commonly used in regression trees. We have implemented a regression tree learner (HTL) that is able to use several alternative models in the tree leaves. We study the effect on accuracy and the computational cost of these alternatives. The experiments carried out on 11 data sets revealed that it is possible to significantly outperform the naive averages of regression trees. Among the four alternative models that we evaluated, kernel regressors were usually the best in terms of accuracy. Our study also indicates that by integrating regression trees with other regression approaches we are able to overcome the limitations of individual methods both in terms of accuracy as well as in computational efficiency.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. </author> <title> (1990) : A study of instance-based learning algorithms for supervised learning tasks: Mathematical, empirical, and psychological evaluations. </title> <type> PhD Thesis. Tech. Report 90-42. </type> <institution> University of California at Irvine, Department of Information and Computer Science. </institution>
Reference-contexts: Several techniques exist that try to overcome these limitations. Feature weighing can help to reduce the influence of irrelevant features by better tuning the distance function (Wettshereck et al. in press). Sampling techniques, indexing schemes and instance prototypes are some of the methods used with large samples <ref> (Aha, 1990) </ref>. Still, local regression methods have a strong limitation when it comes to getting a better insight of the structure of the domain.
Reference: <author> Aha, D. </author> <title> (1992) : Generalizing from case studies : A case study. </title> <booktitle> In Proceedings of the 9th International Conference on Machine Learning. </booktitle> <editor> Sleeman,D. & Edwards,P. (eds.). </editor> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Atkeson,C.G., Moore,A.W., Schaal,S. </author> <title> (in press) : Locally Weighted Learning. Special issue on lazy learning. </title> <editor> Aha, D. (Ed.). </editor> <booktitle> Artificial Intelligence Review. </booktitle>

Reference: <author> Broadley, C. E. </author> <title> (1995) : Recursive automatic bias selection for classifier construction. Machine Learning , 20 , 63-94. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Experimental comparisons of different learning methods on various real world problems have shown the impossibility of selecting a method that performs better in all domains (Michie et al., 1994). This is sometimes called the selective superiority problem <ref> (Broadley , 1995) </ref>. Several authors have tried to automatically identify the applicability of the different methods (Aha, 1992; Brazdil et al., 1994). Other lines of research have tried to take advantage of the different biases of the methods and obtain combined predictions (Breiman, 1996; Freund & Schapire, 1995; Wolpert, 1992).
Reference: <author> Chatfield, C. </author> <title> (1983) : Statistics for technology (third edition). </title> <publisher> Chapman and Hall, Ltd. </publisher>
Reference-contexts: With respect to the stopping criterion for tree growth we use two thresholds. The first is a minimum number of examples in a node. The default value used in our experiments is 20 examples. The second stopping criterion is a minimum value of a statistic called Coefficient of Variation <ref> (Chatfield, 1983) </ref> that captures the relative spread of a set of values. This coefficient is given by CV y Y where s y is the standard deviation of the Y values. By default HTL stops growing a tree when the value of CV reaches 0.15.
Reference: <author> Cleveland, </author> <title> W.S. (1979) : Robust locally weighted regression and smoothing scatterplots. </title> <journal> Journal of the American Statistical Association, </journal> <pages> 74 , 829-836. </pages>
Reference: <author> Cleveland,W.S., Loader,C.R. </author> <title> (1995) : Smoothing by Local Regression: Principles and Methods (with discussion). </title> <booktitle> In Computational Statistics. Deng,K., Moore,A.W. (1995) : Multiresolution Instance--based Learning. In Proceedings of IJCAI95. </booktitle> <month> Domingos,P. </month> <title> (1996) : Unifying Instance-based and Rule-based Induction. </title> <booktitle> In Machine Learning , 24 -2, </booktitle> <pages> 141-168. </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Draper,N.R., Smith,H. </author> <title> (1981) : Applied Regression Analysis, 2nd edition, </title> <publisher> John Wiley. </publisher>
Reference: <author> Fix, E., Hodges, J.L. </author> <title> (1951) : Discriminatory analysis, nonparametric discrimination consistency properties. </title> <type> Technical Report 4, </type> <institution> Randolph Filed, TX: US Air Force, School of Aviation Medicine. Freund,Y., </institution> <month> Schapire,R.E. </month> <title> (1995) : A decision-theoretic generalization of online learning and an application to boosting. </title> <institution> Technical Report . AT & T Bell Laboratories. </institution>
Reference-contexts: These global parametric approaches have been widely used and give good predictive results when the assumed model correctly fits the data. Modern statistical approaches to regression include k-nearest neighbors <ref> (Fix & Hodges, 1951) </ref>, kernel regression (Watson, 1964; Nadaraya, 1964), local polynomial regression (Stone, 1977; Cleveland, 1979), radial basis functions, neural networks, projection pursuit regression, adaptive splines, and others. ML researchers have always been mainly concerned with classification problems.

Reference: <editor> Bergadano,F. & De Raedt,L. (eds.). </editor> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> 784. </volume> <publisher> Springer-Verlag. </publisher>
Reference: <author> Merz,C.J., Murphy,P.M. </author> <note> (1996) : UCI repository of machine learning databases [ http:// www.ics.uci.edu/ MLReposiroty.html]. </note> <institution> Irvine, CA. University of California, Dept. of Information and Computer Science. Michie,D., Spiegelhalter;D.J. & Taylor,C.C. </institution> <year> (1994): </year> <title> Machine Learning, </title> <booktitle> Neural and Statistical Classification , Ellis Horwood Series in Artificial Intelligence, </booktitle> <year> 1994. </year>
Reference: <author> Nadaraya, </author> <title> E.A. (1964) : On estimating regression. Theory of Probability and its Applications, </title> <booktitle> 9 :141-142. Press,W., Teukolsky,S., Vetterling,W., Flannery,B. (1992) : Numerical Recipes in C, 2nd edition. </booktitle> <publisher> Cambridge University Press. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> (1992): </year> <title> Learning with Continuos Classes. </title> <booktitle> In Proceedings of the 5th Australian Joint Conference on Artificial Intelligence. World Scientific, </booktitle> <year> 1992. </year>
Reference-contexts: Some of the existent approaches to regression trees differ on this later issue. CART (Breiman et al., 1984) assigns a constant to the leaves (the average Y value). RETIS ( Karalic, 1992) and M5 <ref> (Quinlan, 1992) </ref> are able to use linear models of the predictor variables. Weiss & Indurkhyas system uses nearest neighbor models on the right side of propositional rules. In another work, Indurkhya & Weiss (1995) suggest that the same approach could be followed on regression trees. <p> This model is obtained using the already mentioned SVD technique. Predictions of testing instances that fall on this leaf are then obtained by evaluating the linear model. This turns out to be similar to the strategy followed by RETIS (Karalic, 1992) or M5 <ref> (Quinlan, 1992) </ref>. 3.2.2 Local Models Local models are computationally more expensive as they need to be reevaluated for each query. HTL is able to use k-nearest neighbors models and kernel regressors. Although they are usually called non-parametric approaches they strongly depend on several design issues.
Reference: <author> Quinlan,J.R. </author> <title> (1993) : Combining Instance-based and Model-based Learning. </title> <booktitle> Proceedings of the 10th ICML. </booktitle> <publisher> Morgan Kaufmann. </publisher> <address> Robnik-Sikonja,M., </address> <month> Kononenko,I. </month> <title> (1996) : Context-sensitive attribute estimation in regression. </title> <booktitle> Proceedings of the ICML-96 Workshop on Learning in Context-Sensitive Domains. Salzberg,S. (1991) : A nearest hyperrectangle learning method. Machine Learning , 6 -3, </booktitle> <pages> 251-276. </pages> <publisher> Kluwer Academic Publishers. </publisher> <address> Smyth,P., Gray,A., </address> <month> Fayyad,U.M. </month> <title> (1995) : Retrofitting Decision Tree Classifiers using Kernel Density Estimation. </title> <booktitle> Proceedings of the 12th ICML. </booktitle> <editor> Prieditis,A., Russel,S. (Eds.). </editor> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Stone, C.J. </author> <title> (1977) : Consistent nonparametric regression. </title> <journal> The Annals of Statistics, </journal> <pages> 5 , 595-645. </pages> <note> Torgo,L. Gama,J. (1997) : Search-based Class Discretization. To appear in Proceedings of ECML97. Springer-Verlag. </note> <author> Utgoff,P. </author> <title> (1989) : Incremental induction of decision trees. </title> <booktitle> Machine Learning , 4 -2, </booktitle> <pages> 161-186. </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Watson, G.S. </author> <title> (1964) : Smooth Regression Analysis. </title> <journal> Sankhya: The Indian Journal of Statistics , Series A, </journal> <pages> 26 : 359-372. </pages>
Reference: <author> Weiss, S. and Indurkhya, N. </author> <title> (1993) : Rule-base Regression. </title> <booktitle> In Proc. of the 13th IJCAI, </booktitle> <address> p.1072-1078. </address>
Reference: <author> Weiss, S., Indurkhya, N. </author> <title> (1995) : Rule-based Machine Learning Methods for Functional Prediction. </title> <journal> In Journal Of Artificial Intelligence Research (JAIR), </journal> <volume> 3 , p.383-403. </volume> <month> Wettschereck,D. </month> <title> (1994) : A study of distance-based machine learning algorithms. </title> <type> PhD thesis. </type> <institution> Oregon State University. Wettschereck,D., Aha,D.W., </institution> <month> Mohri,T. </month> <title> (in press) : A review and empirical evaluation of feature weighting methods for a class of lazzy learning algorithms. Special issue on lazy learning. </title> <editor> Aha, D. (Ed.). </editor> <booktitle> Artificial Intelligence Review. </booktitle>
Reference-contexts: Moreover, the search space explored by rule learners is larger than the one of trees. This means that rule learning systems may find solutions that tree learners can not but at the cost of computational complexity <ref> (Indurkhya & Weiss, 1995) </ref>. These two later observations indicate that HTL should be able to cope with larger problems than Weiss & Indurkhyas system. Another important difference to our work is on the type of local models used.
Reference: <author> Wolpert,D.H. </author> <title> (1992) : Stacked Generalization. </title> <booktitle> In Neural Networks, </booktitle> <address> 5 , (p.241-259). </address>
References-found: 19

