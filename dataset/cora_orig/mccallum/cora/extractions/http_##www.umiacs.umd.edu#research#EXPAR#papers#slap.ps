URL: http://www.umiacs.umd.edu/research/EXPAR/papers/slap.ps
Refering-URL: http://www.umiacs.umd.edu/research/EXPAR/papers/slap.html
Root-URL: 
Title: Efficient Image Processing Algorithms on the Scan Line Array Processor 1  
Author: David Helman Joseph JaJa 
Address: College Park, MD 20742  College Park, MD 20742  
Affiliation: Department of Electrical Engineering University of Maryland  Institute for Advanced Computer Studies Department of Electrical Engineering Institute for Systems Research University of Maryland  
Abstract: We develop efficient algorithms for low and intermediate level image processing on the scan line array processor, a SIMD machine consisting of a linear array of cells that processes images in a scan line fashion. For low level processing, we present algorithms for block DFT, block DCT, convolution, template matching, shrinking, and expanding which run in real-time. By real-time, we mean that, if the required processing is based on neighborhoods of size m fi m, then the output lines are generated at a rate of O(m) operations per line and a latency of O(m) scan lines, which is the best that can be achieved on this model. We also develop an algorithm for median filtering which runs in almost real-time at a cost of O(m log m) time per scan line and a latency of b m 2 c scan lines. For intermediate level processing, we present optimal algorithms for translation, histogram computation, scaling, and rotation. We also develop efficient algorithms for labelling the connected components and determining the convex hulls of multiple figures which run in O(n log n) and O(n log 2 n) time, respectively. The latter algorithms are significantly simpler and easier to implement than those already reported in the literature for linear arrays. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K.E. Batcher, </author> <title> "Design of a Massively Parallel Processor", </title> <journal> IEEE Transactions on Computers, </journal> <year> (1980), </year> <pages> pp. 836-840. </pages>
Reference-contexts: Consequently, a great deal of effort has been devoted to developing parallel architectures and algorithms for real-time image processing. The simplest category of the proposed architectures is the two-dimensional array, or mesh. Examples of this class include the MPP <ref> [1] </ref>, the CLIPP series [2], the MasPar [3], the DAP [4], and the GAPP [5].
Reference: [2] <author> T.J. Fountain, K.N. Mathews, and M.J.B. Duff, </author> <title> "The CLIP7A Image Processor", </title> <journal> IEEE Trans. on Pattern Anal. Machine Intell., </journal> <year> (1980), </year> <pages> pp. 310-319. </pages>
Reference-contexts: Consequently, a great deal of effort has been devoted to developing parallel architectures and algorithms for real-time image processing. The simplest category of the proposed architectures is the two-dimensional array, or mesh. Examples of this class include the MPP [1], the CLIPP series <ref> [2] </ref>, the MasPar [3], the DAP [4], and the GAPP [5].
Reference: [3] <author> J.R. </author> <title> Nickolls,"The Design of the MasPar MP-1: A Cost Effective Massively Parallel Computer," </title> <booktitle> in IEEE Digest of Papers-Compcon, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> (1990), </year> <pages> pp. 25-28. </pages>
Reference-contexts: Consequently, a great deal of effort has been devoted to developing parallel architectures and algorithms for real-time image processing. The simplest category of the proposed architectures is the two-dimensional array, or mesh. Examples of this class include the MPP [1], the CLIPP series [2], the MasPar <ref> [3] </ref>, the DAP [4], and the GAPP [5].
Reference: [4] <author> S.F. Reddaway, </author> <title> "DAP A Distributed Processor Array", </title> <booktitle> First Ann. Symp. </booktitle> <institution> Comput. Architect., </institution> <year> (1973), </year> <pages> pp. 61-65. </pages>
Reference-contexts: Consequently, a great deal of effort has been devoted to developing parallel architectures and algorithms for real-time image processing. The simplest category of the proposed architectures is the two-dimensional array, or mesh. Examples of this class include the MPP [1], the CLIPP series [2], the MasPar [3], the DAP <ref> [4] </ref>, and the GAPP [5].
Reference: [5] <author> Eugene L. </author> <title> Cloud, "The Geometric Arithmetic Parallel Processor", </title> <booktitle> Proc. of 2nd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <year> (1988), </year> <pages> pp. 373-381. </pages>
Reference-contexts: The simplest category of the proposed architectures is the two-dimensional array, or mesh. Examples of this class include the MPP [1], the CLIPP series [2], the MasPar [3], the DAP [4], and the GAPP <ref> [5] </ref>. The general intent behind these SIMD (Single Instruction Stream, Single Data Stream) machines is that the dimensions of the mesh should match those of the input image, and that the pixels should be assigned to processors so as to maintain the spatial relationships of the image.
Reference: [6] <author> V. Cantoni and S. Levialdi, "PAPIA", </author> <title> in Parallel Computer Vision, </title> <editor> L. Uhr ed., </editor> <publisher> Academic Press, </publisher> <year> (1987), </year> <pages> pp. 3-14. </pages>
Reference-contexts: Thus, this pyramidal architecture is more appropriate for intermediate level tasks which require the global exchange of information. Examples of this pyramidal machine include the PAPIA <ref> [6] </ref> and the GAM [7] systems. Of course, when compared to the mesh, there is an increased cost associated with the pyramid due to its increased complexity.
Reference: [7] <author> D. Schaefer et al., </author> <title> "The GAM Pyramid", in Parallel Computer Vision, </title> <editor> L. Uhr ed., </editor> <publisher> Academic Press, </publisher> <year> (1987), </year> <pages> pp. 15-42. </pages>
Reference-contexts: Thus, this pyramidal architecture is more appropriate for intermediate level tasks which require the global exchange of information. Examples of this pyramidal machine include the PAPIA [6] and the GAM <ref> [7] </ref> systems. Of course, when compared to the mesh, there is an increased cost associated with the pyramid due to its increased complexity.
Reference: [8] <author> C.C. Weems, S.P. Levitan, A.R. Hanson, E.M. Riseman, D.B. Shu, and J.G. Nash, </author> <title> "The Image Understanding Architecture", </title> <journal> Int. J. Computer Vision, </journal> <year> (1989), </year> <pages> pp. 251-282. </pages>
Reference-contexts: The basic idea behind creating these hybrids is to enable the programmer to utilize the most efficient architecture for whatever particular problem is presented. Examples of this class of architectures include the Image Understanding Architecture (IUA) <ref> [8] </ref>, the Associative String Processor (ASP) [9], 1 and NETRA [10]. While this strategy for architecture design probably offers the best hope for achieving optimal performance across the spectrum of processing tasks, it will also be the most expensive. Fortunately, not all applications require all levels of image processing.
Reference: [9] <author> R.M. Lea, </author> <title> "The Asp: A Cost-Effective Parallel Microcomputer", </title> <booktitle> IEEE Micro, </booktitle> <year> (1988), </year> <pages> pp. 10-29. </pages>
Reference-contexts: The basic idea behind creating these hybrids is to enable the programmer to utilize the most efficient architecture for whatever particular problem is presented. Examples of this class of architectures include the Image Understanding Architecture (IUA) [8], the Associative String Processor (ASP) <ref> [9] </ref>, 1 and NETRA [10]. While this strategy for architecture design probably offers the best hope for achieving optimal performance across the spectrum of processing tasks, it will also be the most expensive. Fortunately, not all applications require all levels of image processing.
Reference: [10] <author> M. Sharma, J.H. Patel, and N. Ahuja, </author> <title> "NETRA: An Architecture for a Large Scale Multiprocessor Vision System", </title> <booktitle> in Workshop on Computer Architecture for Pattern Analysis and Image Database Management, </booktitle> <year> (1985), </year> <pages> pp. 92-98. </pages>
Reference-contexts: The basic idea behind creating these hybrids is to enable the programmer to utilize the most efficient architecture for whatever particular problem is presented. Examples of this class of architectures include the Image Understanding Architecture (IUA) [8], the Associative String Processor (ASP) [9], 1 and NETRA <ref> [10] </ref>. While this strategy for architecture design probably offers the best hope for achieving optimal performance across the spectrum of processing tasks, it will also be the most expensive. Fortunately, not all applications require all levels of image processing.
Reference: [11] <author> A.I. Fisher and P.T. Highnam, </author> <title> "Real-Time Image Processing on Scan Line Array Processors", </title> <booktitle> IEEE Computer Society Workshop on Computer Architectures for Pattern Analysis and Image Database Management, </booktitle> <year> (1985), </year> <pages> pp. 484-489. </pages>
Reference-contexts: This architecture has been proposed and in some cases implemented under a variety of names, including the Scan Line Array Processor (SLAP) <ref> [11] </ref> [12], the Princeton Engine [13], and the Sarnoff Engine [14]. From here on, this architecture will be referred to as the SLAP.
Reference: [12] <author> A.I. Fisher, </author> <title> "Scan Line Array Processors for Image Computations", </title> <booktitle> International Conference on Computer Architecture, </booktitle> <year> (1986), </year> <pages> pp. 338-345. </pages>
Reference-contexts: This architecture has been proposed and in some cases implemented under a variety of names, including the Scan Line Array Processor (SLAP) [11] <ref> [12] </ref>, the Princeton Engine [13], and the Sarnoff Engine [14]. From here on, this architecture will be referred to as the SLAP.
Reference: [13] <author> D. Chin et. al., </author> <title> "The Princeton Engine: A Real-Time Video System Simulator", </title> <journal> IEEE Trans. on Consumer Electronics, </journal> <month> (May </month> <year> 1988), </year> <pages> pp. 285-297. </pages>
Reference-contexts: This architecture has been proposed and in some cases implemented under a variety of names, including the Scan Line Array Processor (SLAP) [11] [12], the Princeton Engine <ref> [13] </ref>, and the Sarnoff Engine [14]. From here on, this architecture will be referred to as the SLAP. The basic topology of this SIMD machine is a linear array of processors, in which the number of processors corresponds to the number of pixels in each row of the image.
Reference: [14] <author> S. Knight et al., </author> <title> "The Sarnoff Engine: A Massively Parallel Computer for High Definition System Simulation", </title> <booktitle> Proceedings of ApplicationSpecific Array Processors, </booktitle> <year> (1992), </year> <pages> pp. 342-357. </pages>
Reference-contexts: This architecture has been proposed and in some cases implemented under a variety of names, including the Scan Line Array Processor (SLAP) [11] [12], the Princeton Engine [13], and the Sarnoff Engine <ref> [14] </ref>. From here on, this architecture will be referred to as the SLAP. The basic topology of this SIMD machine is a linear array of processors, in which the number of processors corresponds to the number of pixels in each row of the image.
Reference: [15] <author> G. Baudet and D. Stevenson, </author> <title> "Optimal Sorting Algorithms for Parallel Computers", </title> <journal> IEEE Trans. on Computers, </journal> <year> (1978), </year> <pages> pp. 84-87. </pages>
Reference-contexts: Surprisingly, the design of algorithms which utilize the linear array architecture has received only modest attention to date in the literature. Specifically, algorithms have been proposed for sorting <ref> [15] </ref>, matrix multiplication [16], and the Hough transform [17]. Algorithms have also been proposed to solve a number of other graph [18] and geometric problems [19], but only by assuming that the image data is already partitioned among the processors according to the shu*ed row-major distribution.
Reference: [16] <author> I.V. Ramakrishnan and P.J. Varman, </author> <title> "Modular Matrix Multiplication on a Linear Array", </title> <journal> IEEE Trans. on Computers, </journal> <year> (1984), </year> <pages> pp. 952-958. </pages>
Reference-contexts: Surprisingly, the design of algorithms which utilize the linear array architecture has received only modest attention to date in the literature. Specifically, algorithms have been proposed for sorting [15], matrix multiplication <ref> [16] </ref>, and the Hough transform [17]. Algorithms have also been proposed to solve a number of other graph [18] and geometric problems [19], but only by assuming that the image data is already partitioned among the processors according to the shu*ed row-major distribution.
Reference: [17] <author> A.L. Fisher and P.T. Highnam, </author> <title> "Computing the Hough Transform on a Scan-Line Array Processor", </title> <journal> IEEE Trans. Pattern Anal. Machine Intell.", </journal> <year> (1989), </year> <pages> pp. 262-265. </pages>
Reference-contexts: Surprisingly, the design of algorithms which utilize the linear array architecture has received only modest attention to date in the literature. Specifically, algorithms have been proposed for sorting [15], matrix multiplication [16], and the Hough transform <ref> [17] </ref>. Algorithms have also been proposed to solve a number of other graph [18] and geometric problems [19], but only by assuming that the image data is already partitioned among the processors according to the shu*ed row-major distribution.
Reference: [18] <author> K. Doshi and P. Varman, </author> <title> "Optimal Graph Algorithms on a Fixed-Size Linear Array", </title> <journal> IEEE Transactions on Computers, </journal> <year> (1987), </year> <pages> pp. 460-470. 16 </pages>
Reference-contexts: Surprisingly, the design of algorithms which utilize the linear array architecture has received only modest attention to date in the literature. Specifically, algorithms have been proposed for sorting [15], matrix multiplication [16], and the Hough transform [17]. Algorithms have also been proposed to solve a number of other graph <ref> [18] </ref> and geometric problems [19], but only by assuming that the image data is already partitioned among the processors according to the shu*ed row-major distribution. The advantage of this shu*ed row-major distribution is that it significantly reduces the cost of global communication between the different contiguous regions of the image.
Reference: [19] <author> H.M. Alnuweiri and V.K. </author> <title> Prasanna , "Optimal Geometric Algorithms for Digitized Images on Fixed-Size Linear Arrays and Scan-Line Arrays", </title> <booktitle> Distributed Computing, </booktitle> <year> (1991), </year> <pages> pp. 55-65. </pages>
Reference-contexts: Specifically, algorithms have been proposed for sorting [15], matrix multiplication [16], and the Hough transform [17]. Algorithms have also been proposed to solve a number of other graph [18] and geometric problems <ref> [19] </ref>, but only by assuming that the image data is already partitioned among the processors according to the shu*ed row-major distribution. The advantage of this shu*ed row-major distribution is that it significantly reduces the cost of global communication between the different contiguous regions of the image. <p> The complexities of these last two algorithms compare favorably with those of the existing algorithms, which respectively require O (n) and O (n log n) time but which also assume the shu*ed row-major distribution <ref> [19] </ref>.
Reference: [20] <author> A.K. Jain, </author> <title> Fundamentals of Digital Image Processing, </title> <publisher> Prentice Hall, </publisher> <year> (1989). </year>
Reference-contexts: For low level operations, we develop real-time algorithms for block DFT, block DCT, convolution, template matching, shrinking, and expanding <ref> [20] </ref>. We also develop an algorithm for median filtering which runs in almost real-time at a cost of O (m log m) time per scan line and a latency of b m 2 c scan lines [20]. 3 For intermediate level operations, we develop optimal algorithms for translation, his-togram computation, scaling, <p> develop real-time algorithms for block DFT, block DCT, convolution, template matching, shrinking, and expanding <ref> [20] </ref>. We also develop an algorithm for median filtering which runs in almost real-time at a cost of O (m log m) time per scan line and a latency of b m 2 c scan lines [20]. 3 For intermediate level operations, we develop optimal algorithms for translation, his-togram computation, scaling, and translation [20], although for the sake of brevity, we will omit discussion of the latter two algorithms. <p> develop an algorithm for median filtering which runs in almost real-time at a cost of O (m log m) time per scan line and a latency of b m 2 c scan lines <ref> [20] </ref>. 3 For intermediate level operations, we develop optimal algorithms for translation, his-togram computation, scaling, and translation [20], although for the sake of brevity, we will omit discussion of the latter two algorithms. We also develop algorithms for labelling the connected components and for determining the convex hulls of multiple components which run in O (n log n) time and O (n log 2 n), respectively [20]. <p> translation <ref> [20] </ref>, although for the sake of brevity, we will omit discussion of the latter two algorithms. We also develop algorithms for labelling the connected components and for determining the convex hulls of multiple components which run in O (n log n) time and O (n log 2 n), respectively [20]. The complexities of these last two algorithms compare favorably with those of the existing algorithms, which respectively require O (n) and O (n log n) time but which also assume the shu*ed row-major distribution [19].
Reference: [21] <author> T. Cormen, C. Leiserson, and R. Rivest, </author> <title> Introduction to Algorithms, </title> <publisher> McGraw Hill, </publisher> <year> (1991), </year> <pages> 281-286. </pages>
Reference-contexts: One such data structure is the order-statistic tree <ref> [21] </ref>. It allows us to dynamically (1) delete an element, (2) insert an element, or (3) locate the p th smallest element (for any integer p) in time proportional to the logarithm of the tree size.
Reference: [22] <author> A.V. Oppenheim and R.W. Schafer, </author> <title> Discrete-Time Signal Processing, </title> <publisher> Prentice Hall, </publisher> <year> (1989). </year>
Reference-contexts: A straightforward computation of the convolution would require fi (n 2 m 2 ) operations and therefore would require a minimum of (m 2 ) operations per scan line on the SLAP. Suppose, instead, that we employ the overlap-and-add strategy <ref> [22] </ref>. Specifically, we partition the input 6 image X into non-overlapping m fi m blocks referred to as X 0 and indexed by (a; b).
Reference: [23] <author> D.R. Helman, </author> <title> "Efficient Image Processing Algorithms on the Scan Line Array Processor", M.S. </title> <type> Thesis, </type> <institution> University of Maryland, College Park, Maryland, </institution> <year> (1993). </year> <month> 17 </month>
Reference-contexts: For a more detailed presentation of this algorithm, see <ref> [23] </ref>.
References-found: 23

