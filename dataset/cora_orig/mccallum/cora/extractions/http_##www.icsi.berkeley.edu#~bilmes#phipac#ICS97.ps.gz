URL: http://www.icsi.berkeley.edu/~bilmes/phipac/ICS97.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/~bilmes/phipac/
Root-URL: http://www.icsi.berkeley.edu
Email: fbilmes,krste,cheewhye,demmelg@cs.berkeley.edu  
Title: Optimizing Matrix Multiply using PHiPAC: a Portable, High-Performance, ANSI C Coding Methodology  
Author: Jeff Bilmes Krste Asanovic Chee-Whye Chin Jim Demmel 
Note: The author acknowledges the support of JSEP contract F49620-94-C-0038.  The author acknowledges the support of ONR URI Grant N00014-92-J-1617.  The author acknowledges the support of ARPA contract DAAL03-91-C-0047 (University of Tennessee Subcontract ORA4466.02).  The author acknowledges the support of ARPA contract DAAL03-91-C-0047 (University of Tennessee Subcontract ORA4466.02), ARPA contract DAAH04-95-1-0077 (University of Tennessee Subcontract ORA7453.02), DOE grant DE-FG03-94ER25219, DOE contract W-31-109-Eng-38, NSF grant ASC-9313958, and DOE grant DE-FG03-94ER25206. To appear in the proceedings of the International Conference on Supercomputing, July 1997, Vienna, Austria  
Address: Berkeley CA, 94720  Berkeley CA, 94704  
Affiliation: CS Division, University of California at Berkeley  International Computer Science Institute  CS Division, University of California at Berkeley and the International Computer Science Institute.  CS Division, University of California at Berkeley and the International Computer Science Institute.  CS Division, University of California at Berkeley.  CS Division and Mathematics Dept., University of Califor-nia at Berkeley.  
Abstract: Modern microprocessors can achieve high performance on linear algebra kernels but this currently requires extensive machinespecific hand tuning. We have developed a methodology whereby near-peak performance on a wide range of systems can be achieved automatically for such routines. First, by analyzing current machines and C compilers, we've developed guidelines for writing Portable, High-Performance, ANSI C (PHiPAC, pronounced "fee-pack"). Second, rather than code by hand, we produce parameterized code generators. Third, we write search scripts that find the best parameters for a given system. We report on a BLAS GEMM compatible multilevel cache-blocked matrix multiply generator which produces code that achieves around 90% of peak on the Sparcstation-20/61, IBM RS/6000-590, HP 712/80i, SGI Power Challenge R8k, and SGI Octane R10k, and over 80% of peak on the SGI Indigo R4k. The resulting routines are competitive with vendor-optimized BLAS GEMMs. 
Abstract-found: 1
Intro-found: 1
Reference: [ABB + 92] <author> E. Anderson, Z. Bai, C. Bischof, J. Dem-mel, J. Dongarra, J. Du Croz, A. Green-baum, S. Hammarling, A. McKenney, S. Os-trouchov, and D. Sorensen. </author> <note> LAPACK users' guide, release 1.0. In SIAM, Philadelphia, </note> <year> 1992. </year>
Reference-contexts: We are currently working on a better L1 blocking strategy and accompanying methods for search based on various criteria [LRW91]. The PHiPAC GEMM can be used with Bo K-agstrom's GEMM-based BLAS3 package [BLL93] and LAPACK <ref> [ABB + 92] </ref>. We have also written parameterized generators for matrix-vector and vector-matrix multiply, dot product, AXPY, convolution, and outer-product, and further generators, such as for FFT, are planned. We wish to thank Ed Rothberg of SGI for help obtaining the R8K and R10K performance plots.
Reference: [ACF95] <author> B. Alpern, L. Carter, and J. Ferrante. </author> <title> Space-limited procedures: A methodology for portable high-performance. </title> <booktitle> In International Working Conference on Massively Parallel Programming Models, </booktitle> <year> 1995. </year>
Reference-contexts: Ideally, the routines would be written once in a high-level language and fed to an optimizing compiler for each machine. There is a large literature on relevant compiler techniques, many of which use matrix multiplication as a test case <ref> [WL91, LRW91, MS95, ACF95, CFH95, SMP + 96] </ref> 1 . While these compiler heuristics generate reasonably good code in general, they tend not to generate near-peak code for any one operation. A high-level language's semantics might also obstruct aggressive compiler optimizations. <p> The PHiPAC methodology has three components. First, we have developed a generic model of current C compilers and microprocessors that provides guidelines for producing portable high-performance ANSI C code. Second, rather than hand code particular routines, we write parameterized generators <ref> [ACF95, MS95] </ref> that produce code according to our guidelines. Third, we write scripts that automatically tune code for a particular system by varying the generators' parameters and benchmarking the resulting routines.
Reference: [AGZ94] <author> R. Agarwal, F. Gustavson, and M. Zubair. </author> <title> IBM Engineering and Scientific Subroutine Library, Guide and Reference, 1994. Available through IBM branch offices. </title>
Reference-contexts: 1 Introduction The use of a standard linear algebra library interface, such as BLAS [LHKK79, DCHH88, DCDH90], enables portable application code to obtain high-performance provided that an optimized library (e.g., <ref> [AGZ94, KHM94] </ref>) is available and affordable. Developing an optimized library, however, is a difficult and time-consuming task. <p> Another commonly used method is to code using a high level language but with manual tuning to match the underlying architecture <ref> [AGZ94, KHM94] </ref>. While less tedious than coding in assembler, this approach still requires writing machine specific code which is not performance-portable across a range of systems. Ideally, the routines would be written once in a high-level language and fed to an optimizing compiler for each machine.
Reference: [BAD + ] <author> J. Bilmes, K. Asanovic, J. Demmel, D. Lam, and C.W. Chin. </author> <note> The PHiPAC WWW home page. http://www.icsi. berkeley.edu/~bilmes/phipac. </note>
Reference-contexts: We have created a Web site from which the alpha release is available and on which we plan to list blocking parameters for many systems <ref> [BAD + ] </ref>. We are currently working on a better L1 blocking strategy and accompanying methods for search based on various criteria [LRW91]. The PHiPAC GEMM can be used with Bo K-agstrom's GEMM-based BLAS3 package [BLL93] and LAPACK [ABB + 92].
Reference: [BAD + 96] <author> J. Bilmes, K. Asanovic, J. Demmel, D. Lam, and C.W. Chin. PHiPAC: </author> <title> A portable, high-performance, ANSI C coding methodology and its application to matrix multiply. </title> <note> LA-PACK working note 111, </note> <institution> University of Ten-nessee, </institution> <year> 1996. </year>
Reference-contexts: future work. 2 PHiPAC By analyzing the microarchitectures of a range of machines, such as workstations and microprocessor-based SMP and MPP nodes, and the output of their ANSI C compilers, we derived a set of guidelines that help us attain high performance across a range of machine and compiler combinations <ref> [BAD + 96] </ref>. From our analysis of various ANSI C compilers, we determined we could usually rely on reasonable register allocation, instruction selection, and instruction scheduling. <p> We search the above for 1 K 0 K max 0 where K max 0 = 20 but is adjustable. Empirically, K max 0 &gt; 20 has never shown appreciable benefit. Our initial strategy <ref> [BAD + 96] </ref> benchmarked a set of square matrices that fit in L1 cache. We then chose the L0 parameters that achieved the highest performance. While this approach gave good performance, the searches were time consuming.
Reference: [BLL93] <author> B.K-agstrom, P. Ling, and C. Van Loan. </author> <title> Portable high performance GEMM-based level 3 BLAS. In R.F. </title> <editor> Sincovec et al., editor, </editor> <booktitle> Parallel Processing for Scientific Computing, </booktitle> <pages> pages 339-346, </pages> <address> Philadelphia, 1993. </address> <publisher> SIAM Publications. </publisher>
Reference-contexts: We are currently working on a better L1 blocking strategy and accompanying methods for search based on various criteria [LRW91]. The PHiPAC GEMM can be used with Bo K-agstrom's GEMM-based BLAS3 package <ref> [BLL93] </ref> and LAPACK [ABB + 92]. We have also written parameterized generators for matrix-vector and vector-matrix multiply, dot product, AXPY, convolution, and outer-product, and further generators, such as for FFT, are planned. We wish to thank Ed Rothberg of SGI for help obtaining the R8K and R10K performance plots.
Reference: [BLS91] <author> D. H. Bailey, K. Lee, and H. D. Simon. </author> <title> Using Strassen's algorithm to accelerate the solution of linear systems. </title> <journal> J. Supercomputing, </journal> <volume> 4 </volume> <pages> 97-371, </pages> <year> 1991. </year>
Reference-contexts: Developing an optimized library, however, is a difficult and time-consuming task. Even excluding algorithmic variants such as Strassen's method <ref> [BLS91] </ref> for matrix multiplication, these routines have a large design space with many parameters such as blocking sizes, loop nesting permutations, loop unrolling depths, software pipelining strategies, register allocations, and instruction schedules. Furthermore, these parameters have complicated interactions with the increasingly sophisticated microarchitectures of new microprocessors.
Reference: [CDD + 96] <author> J. Choi, J. Demmel, I. Dhillon, J. Don-garra, S. Ostrouchov, A. Petitet, K. Stan-ley, D. Walker, and R.C. Whaley. ScaLA-PAC: </author> <title> A portable linear algebra library for distributed memory computers design issues and performance. </title> <note> LAPACK working note 95, </note> <institution> University of Tennessee, </institution> <year> 1996. </year>
Reference-contexts: We focus on matrix multiplication in this paper, but we have produced other generators including dot-product, AXPY, and convolution, which have similarly demonstrated portable high performance. We concentrate on producing high quality uniprocessor libraries for microprocessor-based systems because multiprocessor libraries, such as <ref> [CDD + 96] </ref>, can be readily built from uniprocessor libraries. For vector and other architectures, however, our machine model would likely need substantial modification. Section 2 describes our generic C compiler and microprocessor model, and develops the resulting guidelines for writing portable high-performance C code.
Reference: [CFH95] <author> L. Carter, J. Ferrante, and S. Flynn Hum-mel. </author> <title> Hierarchical tiling for improved superscalar performance. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Ideally, the routines would be written once in a high-level language and fed to an optimizing compiler for each machine. There is a large literature on relevant compiler techniques, many of which use matrix multiplication as a test case <ref> [WL91, LRW91, MS95, ACF95, CFH95, SMP + 96] </ref> 1 . While these compiler heuristics generate reasonably good code in general, they tend not to generate near-peak code for any one operation. A high-level language's semantics might also obstruct aggressive compiler optimizations. <p> Within the K-loop is our fully-unrolled 2 fi 2 fi 2 core matrix multiply. The code is not unlike the register code in <ref> [CFH95] </ref>. In our terminology, the leading dimensions LDA, LDB, and LDC are called Astride, Bstride, and Cstride respectively. The four local variables c0 0 through c1 1 hold a complete C destination block.
Reference: [DCDH90] <author> J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The use of a standard linear algebra library interface, such as BLAS <ref> [LHKK79, DCHH88, DCDH90] </ref>, enables portable application code to obtain high-performance provided that an optimized library (e.g., [AGZ94, KHM94]) is available and affordable. Developing an optimized library, however, is a difficult and time-consuming task.
Reference: [DCHH88] <author> J. Dongarra, J. Du Cros, S. Hammarling, and R.J. Hanson. </author> <title> An extended set of FORTRAN basic linear algebra subroutines. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14 </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The use of a standard linear algebra library interface, such as BLAS <ref> [LHKK79, DCHH88, DCDH90] </ref>, enables portable application code to obtain high-performance provided that an optimized library (e.g., [AGZ94, KHM94]) is available and affordable. Developing an optimized library, however, is a difficult and time-consuming task.
Reference: [GL89] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: We create a full BLAS-compatible GEMM, by generating all required matrix multiply variants and linking with our GEMM-compatible interface that includes error checking. mm gen produces a cache-blocked matrix multiply <ref> [GL89, LRW91, MS95] </ref>, restructuring the algorithm for unit stride, and reducing the number of cache misses and unnecessary loads and stores.
Reference: [KHM94] <author> C. Kamath, R. Ho, </author> <title> and D.P. Manley. DXML: A high-performance scientific subroutine library. </title> <journal> Digital Technical Journal, </journal> <volume> 6(3) </volume> <pages> 44-56, </pages> <month> Summer </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The use of a standard linear algebra library interface, such as BLAS [LHKK79, DCHH88, DCDH90], enables portable application code to obtain high-performance provided that an optimized library (e.g., <ref> [AGZ94, KHM94] </ref>) is available and affordable. Developing an optimized library, however, is a difficult and time-consuming task. <p> Another commonly used method is to code using a high level language but with manual tuning to match the underlying architecture <ref> [AGZ94, KHM94] </ref>. While less tedious than coding in assembler, this approach still requires writing machine specific code which is not performance-portable across a range of systems. Ideally, the routines would be written once in a high-level language and fed to an optimizing compiler for each machine.
Reference: [LHKK79] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> Basic linear algebra subprograms for FORTRAN usage. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5 </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction The use of a standard linear algebra library interface, such as BLAS <ref> [LHKK79, DCHH88, DCDH90] </ref>, enables portable application code to obtain high-performance provided that an optimized library (e.g., [AGZ94, KHM94]) is available and affordable. Developing an optimized library, however, is a difficult and time-consuming task.
Reference: [LRW91] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of ASP-LOS IV, </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Ideally, the routines would be written once in a high-level language and fed to an optimizing compiler for each machine. There is a large literature on relevant compiler techniques, many of which use matrix multiplication as a test case <ref> [WL91, LRW91, MS95, ACF95, CFH95, SMP + 96] </ref> 1 . While these compiler heuristics generate reasonably good code in general, they tend not to generate near-peak code for any one operation. A high-level language's semantics might also obstruct aggressive compiler optimizations. <p> We create a full BLAS-compatible GEMM, by generating all required matrix multiply variants and linking with our GEMM-compatible interface that includes error checking. mm gen produces a cache-blocked matrix multiply <ref> [GL89, LRW91, MS95] </ref>, restructuring the algorithm for unit stride, and reducing the number of cache misses and unnecessary loads and stores. <p> Under control of command line parameters, mm gen can produce blocking code for any number of levels of memory hierarchy, including register, L1 cache, TLB, L2 cache, and so on. mm gen's code can also perform copy optimization <ref> [LRW91] </ref>, optionally with a different accumulator precision. The latest version can also generate the innermost loop with various forms of software pipelining. <p> Figure 2 lists the L1 cache blocking core code comprising the 3 nested loops, M, N, and K. mm gen does not vary the loop permutation <ref> [MS95, LRW91] </ref> because the re sulting gains in locality are subsumed by the method described below. The outer M loop in Figure 2 maintains pointers c0 and a0 to rows of register blocks in the A and C matrices. <p> We would like to make the L1 blocks large to increase data reuse but larger L1 blocks increase the probability of cache conflicts <ref> [LRW91] </ref>. Tradeoffs between Mand N- loop overheads, memory access patterns, and TLB structure also affect the best L1 size. We currently perform a relatively simple search of the L1 parameter space. <p> Our measurements exaggerate this effect by including all power-of-2 sized matrices, and by allocating all regions contiguously in memory. For matrix multiply, we can reduce cache conflicts by copying to contiguous memory when pathological strides are encountered <ref> [LRW91] </ref>. Unfortunately, this approach does not help dot product. One drawback of the PHiPAC approach is that we can not control the order compilers schedule independent loads. <p> We have created a Web site from which the alpha release is available and on which we plan to list blocking parameters for many systems [BAD + ]. We are currently working on a better L1 blocking strategy and accompanying methods for search based on various criteria <ref> [LRW91] </ref>. The PHiPAC GEMM can be used with Bo K-agstrom's GEMM-based BLAS3 package [BLL93] and LAPACK [ABB + 92]. We have also written parameterized generators for matrix-vector and vector-matrix multiply, dot product, AXPY, convolution, and outer-product, and further generators, such as for FFT, are planned.
Reference: [MS95] <author> J.D. McCalpin and M. Smotherman. </author> <title> Automatic benchmark generation for cache optimization of matrix algorithms. </title> <editor> In R. Geist and S. Junkins, editors, </editor> <booktitle> Proceedings of the 33rd Annual Southeast Conference, </booktitle> <pages> pages 195-204. </pages> <publisher> ACM, </publisher> <month> March </month> <year> 1995. </year>
Reference-contexts: Ideally, the routines would be written once in a high-level language and fed to an optimizing compiler for each machine. There is a large literature on relevant compiler techniques, many of which use matrix multiplication as a test case <ref> [WL91, LRW91, MS95, ACF95, CFH95, SMP + 96] </ref> 1 . While these compiler heuristics generate reasonably good code in general, they tend not to generate near-peak code for any one operation. A high-level language's semantics might also obstruct aggressive compiler optimizations. <p> The PHiPAC methodology has three components. First, we have developed a generic model of current C compilers and microprocessors that provides guidelines for producing portable high-performance ANSI C code. Second, rather than hand code particular routines, we write parameterized generators <ref> [ACF95, MS95] </ref> that produce code according to our guidelines. Third, we write scripts that automatically tune code for a particular system by varying the generators' parameters and benchmarking the resulting routines. <p> We create a full BLAS-compatible GEMM, by generating all required matrix multiply variants and linking with our GEMM-compatible interface that includes error checking. mm gen produces a cache-blocked matrix multiply <ref> [GL89, LRW91, MS95] </ref>, restructuring the algorithm for unit stride, and reducing the number of cache misses and unnecessary loads and stores. <p> Figure 2 lists the L1 cache blocking core code comprising the 3 nested loops, M, N, and K. mm gen does not vary the loop permutation <ref> [MS95, LRW91] </ref> because the re sulting gains in locality are subsumed by the method described below. The outer M loop in Figure 2 maintains pointers c0 and a0 to rows of register blocks in the A and C matrices.
Reference: [SMP + 96] <author> R. Saavedra, W. Mao, D. Park, J. Chame, and S. Moon. </author> <title> The combined effectiveness of unimodular transformations, tiling, and software prefetching. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <month> April 15-19 </month> <year> 1996. </year>
Reference-contexts: Ideally, the routines would be written once in a high-level language and fed to an optimizing compiler for each machine. There is a large literature on relevant compiler techniques, many of which use matrix multiplication as a test case <ref> [WL91, LRW91, MS95, ACF95, CFH95, SMP + 96] </ref> 1 . While these compiler heuristics generate reasonably good code in general, they tend not to generate near-peak code for any one operation. A high-level language's semantics might also obstruct aggressive compiler optimizations.
Reference: [WL91] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Ideally, the routines would be written once in a high-level language and fed to an optimizing compiler for each machine. There is a large literature on relevant compiler techniques, many of which use matrix multiplication as a test case <ref> [WL91, LRW91, MS95, ACF95, CFH95, SMP + 96] </ref> 1 . While these compiler heuristics generate reasonably good code in general, they tend not to generate near-peak code for any one operation. A high-level language's semantics might also obstruct aggressive compiler optimizations.
Reference: [Wol96] <author> M. Wolfe. </author> <title> High performance compilers for parallel computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year> <month> 8 </month>
Reference-contexts: A high-level language's semantics might also obstruct aggressive compiler optimizations. Moreover, it takes significant time and investment before compiler research appears in production compilers, so these capabilities are often unavailable. While both microarchitectures and compilers will improve over time, we expect it will 1 A longer list appears in <ref> [Wol96] </ref>. 1 be many years before a single version of a library routine can be compiled to give near-peak performance across a wide range of machines. We have developed a methodology, named PHiPAC, for developing Portable High-Performance linear algebra libraries in ANSI C.
References-found: 19

