URL: http://www.ics.uci.edu/~kibler/ml97.ps
Refering-URL: http://www.ics.uci.edu/~kibler/
Root-URL: 
Email: fpdatta, kiblerg@ics.uci.edu  pdatta@ics.uci.edu  
Phone: Phone  
Title: Learning Symbolic Prototypes  
Author: Piew Datta and Dennis Kibler 
Keyword: Inductive learning, classification, instance-based learning, prototype learning, clustering, experimental evaluation.  
Note: Email:  number: (714) 824-2111  
Address: Irvine, CA 92717  
Affiliation: Department of Information and Computer Science University of California  
Abstract: We present an empirical analysis of symbolic prototype learners for synthetic and real domains. The prototypes are learned by modifying the minimum-distance classifier to solve problems with symbolic attributes, attribute weighting, and its inability to learn multiple prototypes for a class. These extensions are implemented in SNMC. In the second half of this paper, we provide empirical analysis, characterizing situations where symbolic prototypes have advantages over traditional methods such as decision trees and instance-based methods. Empirical analysis on real domains show that SNMC increases classification accuracy by 10% over the original minimum-distance classifier and has a higher average generalization accuracy than both C4.5 and PEBLS on 20 domains from the UCI data repository. Finding multiple prototypes for classes results in the same or higher accuracy than learning Submission statement: A portion of this paper overlaps another paper submitted to another conference. If both papers are accepted the overlap will be reduced by referencing the other paper and additional research will be added. The overlap was necessary to explain some of the ideas. a single prototype for classes.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler, D. & Albert M. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine learning, </journal> <volume> volume 6, </volume> <pages> pp 37-66. </pages> <address> Boston, MA: </address> <publisher> Kluwer Publishers. </publisher>
Reference: <author> Auer, P., Holte, R. & Maass, W. </author> <year> (1995). </year> <title> Theory and applications of agnostic PAC-learning with small decision trees. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Brieman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Monterey, Ca. </address> <publisher> Wadsworth & Brooks/Cole Advanced Books & Software. </publisher>
Reference: <author> Cheeseman, P. & Stutz, P. </author> <year> (1996). </year> <title> Bayesian classification (AutoClass): theory and results, in Advances in Knowledge Discovery and Data Mining, </title> <type> U.M. </type>
Reference: <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, R. Uthurusamy (eds.), </editor> <address> Cambridge, MA: </address> <publisher> AAAI/MIT Press, </publisher> <pages> pp. 153-180. </pages>
Reference: <author> Clark, P. and Boswell, R. </author> <year> (1991). </year> <title> Rule Induction with CN2: some recent improvements. </title> <booktitle> European Working Session on Learning. </booktitle>
Reference: <author> Cost, S. and Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> volume 10, </volume> <pages> pp. 57-78. </pages> <address> Boston, MA: </address> <publisher> Kluwer Publishers. </publisher>
Reference-contexts: In section 5 we provide experimental evaluations for SNM and SNMC in synthetic domains and in 20 domains from the UCI Repository. We use synthetic domains to characterize situations where SNMC, C4.5 and PEBLS excel. We also compare SNM and SNMC with C4.5 (Quinlan, 1993), PEBLS <ref> (Cost & Salzberg, 1993) </ref>, and the minimum-distance classifier using generalization accuracy as our comparison metric. Section 6 summarizes our contributions. 2 Symbolic Attributes The distance metric for nearest neighbor and minimum-distance classifier is crucial to their predictive capabilities.
Reference: <author> Datta, P. and Kibler, D. </author> <year> (1995). </year> <title> Learning Prototypical Concept Descriptions. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine 15 Learning. </booktitle> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dougherty, J., Kohavi, R., Sahami, M. </author> <year> (1995). </year> <title> Supervised and Unsupervised Discretization of Continuous Features. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Duda, R., and Hart P. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: The two parameters to this algorithm are the distance metric and k, the number of clusters to create. Euclidean distance is the most commonly applied distance measure, although other distance metrics exist such as city block distance and Mahalanobis distance <ref> (Duda & Hart 1973) </ref>. Finding k, on the other hand, is a more serious open problem.
Reference: <author> Fayyad, U. & Irani, K. </author> <year> (1992). </year> <title> The attribute selection problem in decision tree generation. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence. </booktitle> <address> San Jose, Ca. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The MVDM provides a more precise weighing method than simply stating that an attribute has a particular weight; it weighs each value of the symbolic attributes. The MVDM has some similar characteristics to the information gain metric (Quinlan, 1993) and the ORT metric <ref> (Fayyad & Irani, 1992) </ref>. Lemma 1 If the MVDM for attribute A = 0 (i.e. D (v,w)=0 8 values k of A) then the information gain for A is 0. <p> The ORT metric also compares the class probabilities of the resulting sets. If the class probabilities of the resulting sets are equal then the ORT metric is minimized <ref> (Fayyad & Irani, 1992) </ref> like the MVDM and the information gain metric. Thus far SNM does not weigh real-value attributes. A simple method of weighing real-value attributes is to discretize them and use the MVDM to define the distances between each of their new discretized values.
Reference: <author> Iba, W. & Langley, P. </author> <year> (1992). </year> <title> Induction of one-level decision trees. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning. </booktitle> <address> Ab-erdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> MacQueen, J. </author> <year> (1967). </year> <title> Some methods for classification and analysis of mul-tivariate observations. </title> <booktitle> In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability. </booktitle> <volume> Vol. </volume> <pages> 1. </pages> <institution> University of California Press, Berkeley. </institution>
Reference: <author> Murphy, P. and Aha, D. </author> <year> (1994). </year> <title> UCI repository of machine learning databases [machine readable data repository]. </title> <type> Tech. Rep., </type> <institution> University of California, Irvine. </institution>
Reference-contexts: We realize that the test sets in the 30 runs are not independent. Kohavi (1995) recommends multiple runs of cross validation, but we contend that this methodology suffers from the same independence problem. Table 1 shows the results on 11 domains from the UCI Data Repository <ref> (Murphy & Aha 1994) </ref> containing either a mixture of symbolic and real-value attributes or only real-value attributes. <p> C4.5 is biased towards crisp concepts, since it searches for a few relevant attributes. A prototype representation is more appropriate for cumulative concepts. 5.2 Real World Domains We compared C4.5, PEBLS, the minimum-distance classifier, SNM, and SNMC on 20 domains from the UCI Repository <ref> (Murphy & Aha, 1994) </ref>. Three of these domains are artificial (LED domains and Hayes Roth) and the remaining are real world domains. They have a variety of characteristics including different types of attributes, and a large range in the number of examples, attributes, and classes.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: In section 5 we provide experimental evaluations for SNM and SNMC in synthetic domains and in 20 domains from the UCI Repository. We use synthetic domains to characterize situations where SNMC, C4.5 and PEBLS excel. We also compare SNM and SNMC with C4.5 <ref> (Quinlan, 1993) </ref>, PEBLS (Cost & Salzberg, 1993), and the minimum-distance classifier using generalization accuracy as our comparison metric. Section 6 summarizes our contributions. 2 Symbolic Attributes The distance metric for nearest neighbor and minimum-distance classifier is crucial to their predictive capabilities. <p> The MVDM provides a more precise weighing method than simply stating that an attribute has a particular weight; it weighs each value of the symbolic attributes. The MVDM has some similar characteristics to the information gain metric <ref> (Quinlan, 1993) </ref> and the ORT metric (Fayyad & Irani, 1992). Lemma 1 If the MVDM for attribute A = 0 (i.e. D (v,w)=0 8 values k of A) then the information gain for A is 0.
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Smyth, P. </author> <year> (1996). </year> <title> Clustering using Monte Carlo Cross-Validation. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining. </booktitle> <address> Seattle, Washington. </address>
Reference: <author> Stanfill, C., & Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM. </journal> <volume> vol. 29. no 12. </volume> <pages> pp. 1213-1228. </pages>
Reference: <author> Zhang, J. </author> <year> (1992). </year> <title> Selecting typical instances in instance-based learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 16 </pages>
References-found: 20

