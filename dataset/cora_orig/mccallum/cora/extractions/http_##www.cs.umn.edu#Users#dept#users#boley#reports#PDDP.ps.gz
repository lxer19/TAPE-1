URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/PDDP.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/
Root-URL: http://www.cs.umn.edu
Title: Principal Direction Divisive Partitioning  
Author: Daniel Boley 
Address: 200 Union Street S.E., Rm 4-192 Minneapolis, MN 55455, USA  
Affiliation: Department of Computer Science and Engineering University of Minnesota  
Abstract: We propose a new algorithm capable of partitioning a set of documents or other samples based on an embedding in a high dimensional Euclidean space (i.e. in which every document is a vector of real numbers). The method is unusual in that it is divisive, as opposed to agglomerative, and operates by repeatedly splitting clusters into smaller clusters. The splits are not based on any distance or similarity measure. The documents are assembled in to a matrix which is very sparse. It is this sparsity that permits the algorithm to be very efficient. The performance of the method is illustrated with a set of text documents obtained from the World Wide Web. Some possible extensions are proposed for further investigation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. R. Anderberg. </author> <title> Cluster Analysis for Applications. </title> <publisher> Academic Press, </publisher> <year> 1973. </year>
Reference-contexts: The word "Divisive" comes from the following taxonomy of clustering algorithms in [12, p298] (originally from <ref> [1] </ref>): 1. hierarchical agglomerative clustering 2. hierarchical divisive clustering 3. iterative partitioning 4. density search clustering 2 5. factor analytic clustering 6. clumping 7. graph-theoretic clustering Our algorithm is a hierarchical algorithm, but the overwhelming majority of existing hierarchical algorithms which treat the documents as a point in Euclidean space work
Reference: [2] <author> M. W. Berry, S. T. Dumais, and G. W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: However, the number of features collected using these methods still tends to be very large, and determining which features can be discarded without affecting the quality of clusters is difficult. Latent Semantic Indexing (LSI) <ref> [2] </ref> was proposed as a method for query-based document retrieval in which the noise present in data sets of very high dimensionality is reduced by orthogonal projection.
Reference: [3] <author> P. Cheeseman and J. Stutz. </author> <title> Baysian classification (autoclass): Theory and results. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering [9] and nearest-neighbor clustering [10] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space. AutoClass <ref> [3] </ref> is a method using Bayesian analysis based on the probabilistic mixture modeling [14]. Given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters. There are a number of problems with clustering in a multi-dimensional space using traditional distance- or probability-based methods. <p> If the 12 dimensionality is high, then the calculated mean values may not differ significantly from one cluster to the next, especially if the initial set of clusters are poor. Hence the cluster means may not provide good separation. Similarly, probabilistic methods such as Bayesian classification used in AutoClass <ref> [3] </ref> do not perform well when the size of the feature space is much larger than the size of the sample set and the attributes are not statistically independent, as is typical of document categorization applications on the Web.
Reference: [4] <author> D. Cutting, D. Karger, J. Pedersen, and J. Tukey. Scatter/gather: </author> <title> a cluster-based approach to browsing large document collections. </title> <booktitle> In 15th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'92), </booktitle> <pages> pages 318-329, </pages> <year> 1992. </year>
Reference-contexts: As a hierarchical divisive algorithm, the PDDP algorithm would be appropriate for the scatter/gather task of <ref> [4] </ref>. The scatter/gather task consists of a "scatter" of a large collection of documents into a "small" number of clusters, and a "gather" task in which certain of those resulting clusters are combined. The scatter/gather process is then repeated on the resulting gathered cluster. In [4], the scatter task was carried <p> for the scatter/gather task of <ref> [4] </ref>. The scatter/gather task consists of a "scatter" of a large collection of documents into a "small" number of clusters, and a "gather" task in which certain of those resulting clusters are combined. The scatter/gather process is then repeated on the resulting gathered cluster. In [4], the scatter task was carried out by a modified agglomeration algorithm to which some heuristics were applied to achieve running times faster than the O (m 2 ) expected running time. <p> We remark that this expected running time is linear in the number of documents (m), modulo the number of iterations within the SVD computation, whereas unmodified agglomeration algorithms typically have O (m 2 ) running time <ref> [4] </ref>. 4 Related Work Existing approaches to document clustering are generally based on either probabilistic methods, or distance and similarity measures (see [6]). Distance-based methods such as k-means analysis, hierarchical clustering [9] and nearest-neighbor clustering [10] use a selected set of words (features) appearing in different documents as the dimensions. <p> It is seen that the theme of each cluster can be deduced from the leading words. We remark that if we were to apply the "join" operator <ref> [4] </ref> described in Section 1 with k = 5 and l = 3, then clusters 3 and 4 would be joined. Further consolidation could occur using l = 2 or perhaps with larger values of k. <p> In this section, we discuss several extensions that could be applied to this algorithm, including some preliminary experimental results validating some of them. We remark that we have already mentioned the possible application of the PDDP algorithm to the scatter/gather method of <ref> [4] </ref>. 6.1 Classification of New Documents The simplest extension to the PDDP algorithm of Table 2 is to classify a new set of documents according to the clusters from an original document set by using the original binary PDDP 17 divisive algorithm agglomerative algorithm data 8 16 32 16 32 set <p> One may use the Classification algorithm to assign all the new documents to existing clusters and then use one of the many existing updating algorithms on the set of clusters (see, e.g., [5, p201 or p225] or <ref> [4] </ref>) The drawback of such an approach is that clusters created by another updating method no longer has any connection with the PDDP tree. A possible approach for rapidly constructing a new PDDP tree is outlined as follows.
Reference: [5] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and scene analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year> <month> 24 </month>
Reference-contexts: Let w = M p e=p be the sample mean of the documents d 1 ; : : : ; d p , and the covariance matrix is C = (M p we T )(M p we T ) T . The Karhunen-Loeve transformation <ref> [5] </ref> consists of projecting the columns of M p onto the space spanned by the leading k eigenvectors (those corresponding to the k largest eigenvalues). The result is a representation of the original data in k degrees of freedom instead of the original n. <p> Then further heuristics were applied to obtain the matrices for data sets J2 through J11. These are summarized in Table 3 [8]. We applied two algorithms to this data set, the divisive PDDP algorithm (Table 2), and an agglomerative algorithm <ref> [5] </ref>. The agglomerative algorithm is shown only for comparative purposes, and is briefly summarized as follows. We start with m singleton clusters, one per individual document. We then repeatedly find the two clusters that are "closest together" according to a distance measure [5]. <p> PDDP algorithm (Table 2), and an agglomerative algorithm <ref> [5] </ref>. The agglomerative algorithm is shown only for comparative purposes, and is briefly summarized as follows. We start with m singleton clusters, one per individual document. We then repeatedly find the two clusters that are "closest together" according to a distance measure [5]. Those two clusters are merged into a single cluster, and the process is repeated. The "distance" is defined as the angle between the cluster centroids. This simple agglomerative algorithm was effective for some scalings, but slow. <p> One may use the Classification algorithm to assign all the new documents to existing clusters and then use one of the many existing updating algorithms on the set of clusters (see, e.g., <ref> [5, p201 or p225] </ref> or [4]) The drawback of such an approach is that clusters created by another updating method no longer has any connection with the PDDP tree. A possible approach for rapidly constructing a new PDDP tree is outlined as follows.
Reference: [6] <author> W. B. Frakes and R. Baeza-Yates. </author> <title> Information Retrieval Data Structures and Algo--rithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: linear in the number of documents (m), modulo the number of iterations within the SVD computation, whereas unmodified agglomeration algorithms typically have O (m 2 ) running time [4]. 4 Related Work Existing approaches to document clustering are generally based on either probabilistic methods, or distance and similarity measures (see <ref> [6] </ref>). Distance-based methods such as k-means analysis, hierarchical clustering [9] and nearest-neighbor clustering [10] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space.
Reference: [7] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins Univ. Press, 3rd edition, </publisher> <year> 1996. </year>
Reference-contexts: The diagonal entries of are called singular values and satisfy oe 1 oe 2 oe minfm;ng 0. The columns of UV are the left and right singular vectors, respectively. It is well known <ref> [7] </ref> that the eigendecomposition of C = AA T is related to the SVD of A by where we use 2 as a shorthand for the n fi n diagonal matrix diagfoe 2 1 ; oe 2 minfm;ng ; 0; : : : ; 0g (extended with zeros if m &lt; <p> When C has large dimensionality this can be a significant expense. Standard off-the-shelf methods typically compute all the eigenvalues and eigenvectors. Even if special methods that compute only selected eigenvectors are used, many methods still require the computation of all the eigenvalues <ref> [7] </ref>. In addition, the covariance matrix is actually the product of a matrix and its transpose. This is exactly the situation where it is well known that better accuracy can be had by using the Singular Value Decomposition (SVD) [7]. <p> used, many methods still require the computation of all the eigenvalues <ref> [7] </ref>. In addition, the covariance matrix is actually the product of a matrix and its transpose. This is exactly the situation where it is well known that better accuracy can be had by using the Singular Value Decomposition (SVD) [7]. <p> We need only the leading singular vectors u 1 ; v 1 , which we will denote u; v for short. A fast method for computing a partial SVD of a matrix can be constructed using the Lanczos algorithm <ref> [7] </ref>. This algorithm takes advantage of sparsity present in the matrix M p . In our implementation of this algorithm, we implicitly compute the eigenvalues of AA T or A T A, whichever has a smaller dimension. <p> Once the eigenvalue is found, the generated Lanczos vectors are used to compute the approximate corresponding eigenvector. All of these properties mentioned here are based on the theory present in <ref> [7] </ref> and hence is not further discussed here. 3.3 Overall Algorithm We summarize the overall algorithm used to carry out the partitioning. The basic algorithm constructs a binary tree, the PDDP tree. <p> The square of the Frobenius norm of A = (a ij ) is given by kAk 2 X ja ij j 2 ; and equals the Frobenius norm of the covariance matrix C as well as the sum of the eigen values oe 2 i of C <ref> [7] </ref>: F = kCk F = i i : 0. Start with n fi m matrix M of (scaled) document vectors, and a desired number of clusters c max . 1. Initialize Binary Tree with a single Root Node (Table 1). 2.
Reference: [8] <author> S. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore. WebACE: </author> <title> A web agent for document categorization and exploration. </title> <institution> Univ. of Minn. Comp. Sci. </institution> <type> report TR 97-049, </type> <year> 1997. </year>
Reference-contexts: 1 Introduction Unsupervised clustering of documents is a critical component for the exploration of large unstructured document sets. As part of a larger project, the WebACE Project <ref> [11, 8] </ref>, we have developed a novel algorithm for unsupervised partitioning of a large data set which exhibits several useful features. <p> The result was the matrix "J1" with 185 columns corresponding to the 185 documents, and 10536 rows each corresponding to a word. Then further heuristics were applied to obtain the matrices for data sets J2 through J11. These are summarized in Table 3 <ref> [8] </ref>. We applied two algorithms to this data set, the divisive PDDP algorithm (Table 2), and an agglomerative algorithm [5]. The agglomerative algorithm is shown only for comparative purposes, and is briefly summarized as follows. We start with m singleton clusters, one per individual document. <p> This selection was carried out by preprocessing step as part of a hypergraph algorithm which is also capable of clustering. Comparative results of this algorithm have been reported in <ref> [11, 8] </ref>. We illustrate in Table 7 a sample cluster from one of the cases of Table 6. Table 7 shows 16 clusters in which for each cluster we show the hand-assigned labels for documents in that cluster as well as the 5 most common words in the cluster.
Reference: [9] <author> A. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering <ref> [9] </ref> and nearest-neighbor clustering [10] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space.
Reference: [10] <author> S. Lu and K. Fu. </author> <title> A sentence-to-sentence clustering procedure for pattern analysis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8 </volume> <pages> 381-389, </pages> <year> 1978. </year>
Reference-contexts: Distance-based methods such as k-means analysis, hierarchical clustering [9] and nearest-neighbor clustering <ref> [10] </ref> use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space. AutoClass [3] is a method using Bayesian analysis based on the probabilistic mixture modeling [14].
Reference: [11] <author> J. Moore, S. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Web page categorization and feature selection using association rule and principal component clustering. </title> <booktitle> In 7th Workshop on Information Technologies and Systems (WITS '97), </booktitle> <address> Dec. 1997. Atlanta. </address>
Reference-contexts: 1 Introduction Unsupervised clustering of documents is a critical component for the exploration of large unstructured document sets. As part of a larger project, the WebACE Project <ref> [11, 8] </ref>, we have developed a novel algorithm for unsupervised partitioning of a large data set which exhibits several useful features. <p> This selection was carried out by preprocessing step as part of a hypergraph algorithm which is also capable of clustering. Comparative results of this algorithm have been reported in <ref> [11, 8] </ref>. We illustrate in Table 7 a sample cluster from one of the cases of Table 6. Table 7 shows 16 clusters in which for each cluster we show the hand-assigned labels for documents in that cluster as well as the 5 most common words in the cluster.
Reference: [12] <author> M. Nadler and E. P. Smith. </author> <title> Pattern Recognition Engineering. </title> <publisher> Wiley, </publisher> <year> 1993. </year>
Reference-contexts: We use the word "Partitioning" to reflect the fact that we place all the documents in one cluster, so that at every stage the clusters are disjoint and their union equals the entire set of documents. The word "Divisive" comes from the following taxonomy of clustering algorithms in <ref> [12, p298] </ref> (originally from [1]): 1. hierarchical agglomerative clustering 2. hierarchical divisive clustering 3. iterative partitioning 4. density search clustering 2 5. factor analytic clustering 6. clumping 7. graph-theoretic clustering Our algorithm is a hierarchical algorithm, but the overwhelming majority of existing hierarchical algorithms which treat the documents as a point <p> The formula (7) is an example of a linear discriminant function. Linear discriminant functions have been used extensively to partition samples in a test set into two classes. An example is the Fisher linear discriminant function (see e.g. <ref> [12] </ref>), typically used with a training set of samples with known "correct" classifications. As such it is usually used as a tool in "supervised learning," in which a training set with previously known class designations are used. <p> Previous algorithms for unsupervised clustering based on the use of one-dimensional Bayesian analysis or linear discriminants is very limited, at least to the knoweldge of this author. A hint in this direction appears in <ref> [12, p500] </ref>, where the repeated use of a Fisher-style linear discriminant is suggested, resulting in a hierarchical classification. It is even 13 suggested that the Karhunen Loeve transformation might lead to good directions.
Reference: [13] <author> G. Salton and M. J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: We refer to the scaling (3) as "norm scaling." An alternative scaling is the tfidf scaling <ref> [13] </ref>, defined as follows: let ~ d i = max j (TF j ) DF i , then d i = ~ d i j ( ~ d j ) 2 where DF i is the number of different documents in which the i word appears, among all documents in the <p> Some words are more frequent in a document than other words. Simple frequency of the occurrence of words is not adequate, as some documents are larger than others. Furthermore, some words may occur frequently across documents. Techniques such as tfidf <ref> [13] </ref> have been proposed precisely to deal with some of these problems. Secondly, the number of all the words in all the documents can be very large. Distance-based schemes generally require the calculation of the mean of document clusters. <p> Those two clusters are merged into a single cluster, and the process is repeated. The "distance" is defined as the angle between the cluster centroids. This simple agglomerative algorithm was effective for some scalings, but slow. In both cases we used either the tfidf scaling (3.1) <ref> [13] </ref> or the norm scaling (3). For the agglomerative algorithm, the times were independent of the scaling. The tfidf scaling destroys the sparsity, so the performance of the PDDP algorithm is substantially degraded and is not reported. The resulting times are shown in Table 5.
Reference: [14] <author> D. Titterington, A. Smith, and U. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year> <month> 25 </month>
Reference-contexts: Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space. AutoClass [3] is a method using Bayesian analysis based on the probabilistic mixture modeling <ref> [14] </ref>. Given a data set it finds maximum parameter values for a specific probability distribution functions of the clusters. There are a number of problems with clustering in a multi-dimensional space using traditional distance- or probability-based methods. First, it is not trivial to define a distance measure in this space.
References-found: 14

