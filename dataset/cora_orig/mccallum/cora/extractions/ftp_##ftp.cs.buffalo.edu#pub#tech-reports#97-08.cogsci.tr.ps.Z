URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/97-08.cogsci.tr.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Email: fehrlich|rapaportg@cs.buffalo.edu  
Title: A Computational Theory of Vocabulary Expansion connections used for the definition by selecting particular kinds
Author: Karen Ehrlich and William J. Rapaport 
Note: The Project and Its Significance limit the  Evidence for this can be seen in the psychological literature  
Web: http://www.cs.buffalo.edu/~snwiz  
Address: NY 14260  
Affiliation: Department of Computer Science and Center for Cognitive Science State University of New York at Buffalo, Buffalo,  
Abstract: As part of an interdisciplinary project to develop a computational cognitive model of a reader of narrative text, we are developing a computational theory of how natural-language-understanding systems can automatically expand their vocabulary by determining from context the meaning of words that are unknown, misunderstood, or used in a new sense. `Con-text' includes surrounding text, grammatical information, and background knowledge, but no external sources. Our thesis is that the meaning of such a word can be determined from context, can be revised upon further encounters with the word, converges to a dictionary-like definition if enough context has been provided and there have been enough exposures to the word, and eventually settles down to a steady state that is always subject to revision upon further encounters with the word. The system is being implemented in the SNePS knowledge-representation and reasoning system. This document is a slightly modified version (containing the algorithms that appear in Figure 1, below) of that which is to appear in Proceedings of the 19th Annual Conference of the Cognitive Science Society (Stanford University) (Mahwah, NJ: Lawrence Erlbaum Associates). It is Technical Report 97-08 (Buffalo: SUNY Buffalo Department of Computer Science) and Technical Report 97-2 (Buffalo: SUNY Buffalo Center for Cognitive Science). It is also available on-line at &lt;http://www.cs.buffalo.edu/pub/WWW/faculty/ rapaport/Papers/vocab.cogsci.tr.ps&gt;. We are developing a computational theory of how NLU systems (including humans) can automatically expand their vocabulary by determining from context the meaning of words that are unknown to the system, familiar but misunderstood, or used in a new sense (Ehrlich 1995). `Context' includes surrounding text, grammatical information, and background knowledge, but no access to a dictionary (Zadrozny & Jensen 1991) or other external sources of information (including a human). We take the meaning of a word (as understood by a cognitive agent) to be its position in a network of words, propositions, and other concepts (Quillian 1968, 1969). In this (idiolectic) sense, the meaning of a word for a cognitive agent is determined by idiosyncratic experience with it. The contextual meaning described above includes a word's relation to every concept in the agent's mind. Thus, the extreme interpretation of meaning as context defines every word in terms of every other word an agent knows. This is circular and too unwieldy for use. In another sense, the meaning of a word is its dictionary definition, usually containing less information. Thus, we We claim that a meaning for a word can be determined from any context, can be revised and refined upon further encounters with it, and converges to a dictionary-like definition given enough context and exposures to it. Each encounter with it yields a definitiona hypothesis about meaning. Subsequent encounters provide opportunities for unsupervised revision of this hypothesis, with no (human) trainers or error-correction techniques. The hypothesized definitions are not guaranteed to converge to a correct meaning (if such exists) but to one stable with respect to further encounters. Finally, no domain-specific background information is required for developing the definition. The vocabulary-expansion system is part of an interdisciplinary project developing a computational cognitive model of a reader of narrative text (Duchan et al. 1995). To fully model a reader, it is important to model the ability to learn from reading, in particular, to expand one's vocabulary in a natural way while reading, without having to stop to ask someone or to consult a dictionary. A complete lexicon cannot be manually encoded, nor could it contain new words or new meanings (Zernik & Dyer 1987). Text-understanding, message-processing, and information-extraction systems need to be robust in the presence of unknown expressions, especially systems using unconstrained input text and operating independently of human intervention, such as intelligent agents. E.g., a system designed to locate interesting news items from an online information server should not be limited to keyword searchesif the user is interested in news items about dogs, and the filter detects items about brachets (a term not in its lexicon), it should deliver those items as soon as it figures out that a brachet is a kind of dog. Two features of our system mesh nicely with these desiderata, summarized as the advantages of learning over being told: (1) Being told requires human intervention. Our system operates independently of a human teacher or trainer (with 
Abstract-found: 1
Intro-found: 0
Reference: <author> Berwick, R.C. </author> <year> (1983), </year> <title> Learning Word Meanings from Ex amples, </title> <journal> IJCAI-83: </journal> <pages> 459-461. </pages>
Reference: <author> Cravo, M.R., & Martins, J.P. </author> <year> (1993), </year> <title> SNePSwD: A New comer to the SNePS Family, </title> <type> JETAI 5: </type> <pages> 135-148. </pages>
Reference: <editor> Duchan, J.F.; Bruder, G.A.; & Hewitt, L.E. (eds.) </editor> <booktitle> (1995), Deixis in Narrative: A Cognitive Science Perspective (Erl-baum). </booktitle>
Reference: <author> Ehrlich, K. </author> <year> (1995), </year> <title> Automatic Vocabulary Expansion through Narrative Context, </title> <type> TR 95-09 (Buffalo: </type> <institution> SUNY Buffalo Dept. of Computer Science). </institution>
Reference: <author> Elshout-Mohr, M., & van Daalen-Kapteijns, M.M. </author> <year> (1987), </year> <title> Cognitive Processes in Learning Word Meanings, </title> <booktitle> in M. </booktitle>
Reference: <author> G. McKeown & M. E. Curtis (eds.), </author> <title> The Nature of Vocabulary Acquisition (Erlbaum): </title> <type> 53-71. </type>
Reference: <author> Gleitman, L. </author> <year> (1990), </year> <title> The Structural Sources of Verb Mean ings, </title> <journal> Lang. </journal> <volume> Acquisition 1: </volume> <pages> 1-55. </pages>
Reference: <author> Gleitman, L. </author> <year> (1994), </year> <title> A Picture is Worth a Thousand Words But That's the Problem (talk presented at the 1st Int'l. </title> <institution> Summer Inst. in Cog. Sci. (SUNY Buffalo, </institution> <note> July 1994); abstract in Proc. 16th Annual Conf. </note> <institution> Cog. Sci. Soc.: </institution> <month> 965). </month>
Reference: <author> Granger, R.H. </author> <year> (1977), </year> <title> Foul-Up: A Program that Figures Out Meanings of Words from Context, </title> <booktitle> IJCAI-77: </booktitle> <pages> 67-68. </pages>
Reference: <author> Haas, N., & Hendrix, G. </author> <year> (1983), </year> <title> Learning by Being Told: Acquiring Knowledge for Information Management, </title> <editor> in Michalski et al. (eds.), </editor> <booktitle> Machine Learning (Tioga): </booktitle> <pages> 405-428. </pages>
Reference: <author> Hastings, P.M. </author> <year> (1994), </year> <title> Automatic Acquisition of Word Meaning from Context, </title> <type> Ph.D. </type> <institution> diss. (Comp. Sci. & Eng'g., Univ. of Michigan). </institution>
Reference: <author> Hastings, P.M., & Lytinen, S.L. </author> <year> (1994a), </year> <title> The Ups and Downs of Lexical Acquisition, </title> <booktitle> AAAI-94: </booktitle> <pages> 754-759. </pages>
Reference: <author> Hastings, P.M., & Lytinen, S.L. </author> <year> (1994b), </year> <title> Objects, Actions, Nouns, and Verbs, </title> <booktitle> Proc. 16th Annual Conf. Cog. </booktitle> <publisher> Sci. Soc.: </publisher> <pages> 397-402. </pages>
Reference: <author> Johnson-Laird, P.N. </author> <year> (1987), </year> <title> The Mental Representation of the Meanings of Words, in A.I. </title> <editor> Goldman (ed.), </editor> <booktitle> Readings in Philosophy and Cognitive Science (MIT, </booktitle> <year> 1993): </year> <pages> 561-583. </pages>
Reference: <author> Kiersey, D.M. </author> <year> (1982), </year> <title> Word Learning with Hierarchy Guided Inference, </title> <journal> AAAI-82: </journal> <pages> 172-178. </pages>
Reference: <author> Malory, Sir T. (1470), Le Morte Darthur, ed. R.M. Lumiansky (Collier, </author> <year> 1982). </year>
Reference: <author> Martins, J., & Cravo M.R. </author> <year> (1991), </year> <title> How to Change Your Mind, </title> <type> No us 25: </type> <pages> 537-551. </pages>
Reference: <author> Martins, J., & Shapiro, S.C. </author> <year> (1988), </year> <title> A Model for Belief Revision, </title> <journal> Artif. Intell. </journal> <volume> 35: </volume> <pages> 25-79. </pages>
Reference: <author> Quillian, </author> <title> M.R. (1968), Semantic Memory, </title> <editor> in M. Minsky (ed.) </editor> <booktitle> Semantic Information Processing (MIT): </booktitle> <pages> 227-270. </pages>
Reference: <author> Quillian, </author> <title> M.R. (1969), The Teachable Language Compre hender: A Simulation Program and Theory of Language, </title> <journal> Comm. ACM 12: </journal> <pages> 459-476. </pages>
Reference: <author> Rapaport, W.J. </author> <year> (1988), </year> <title> Syntactic Semantics: </title> <booktitle> Foundations of Computational Natural-Language Understanding, in J.H. </booktitle>
Reference: <editor> Fetzer (ed.), </editor> <booktitle> Aspects of Artificial Intelligence (Kluwer): </booktitle> <pages> 81-131. </pages>
Reference: <author> Shapiro, S.C. </author> <year> (1979), </year> <title> The SNePS Semantic Network Pro cessing System, </title> <editor> in N. Findler (ed.), </editor> <booktitle> Associative Networks (Academic): </booktitle> <pages> 179-203. </pages>
Reference: <author> Shapiro, S.C. </author> <year> (1982), </year> <title> Generalized Augmented Transition Network Grammars for Generation from Semantic Networks, </title> <journal> American J. Comp. Ling. </journal> <volume> 8: </volume> <pages> 12-25. </pages>
Reference: <author> Shapiro, S.C. </author> <year> (1989), </year> <title> The CASSIE Projects: An Approach to Natural Language Competence, in J.P. Martins & E.M. </title> <editor> Morgado (eds.), </editor> <booktitle> Proc. 4th Portugese Conf. A.I. (EPIA-89), Lecture Notes in A.I. </booktitle> <publisher> 390 (Springer-Verlag): </publisher> <pages> 362-380. </pages>
Reference: <author> Shapiro, S.C., & Rapaport, W.J. </author> <year> (1991), </year> <title> Models and Minds: Knowledge Representation for Natural-Language Competence, </title> <editor> in R. Cummins & J. Pollock (eds.), </editor> <publisher> Philosophy and AI (MIT): </publisher> <pages> 215-259. </pages>
Reference: <author> Shapiro, S.C., & Rapaport, W.J. </author> <year> (1992), </year> <title> The SNePS Family, </title> <booktitle> Computers and Mathematics with Applications 23: </booktitle> <pages> 243-275. </pages>
Reference: <author> Shapiro, S.C., & Rapaport, W.J. </author> <year> (1995), </year> <title> An Introduction to a Computational Reader of Narrative, </title> <editor> in Duchan et al. </editor> <year> 1995: </year> <pages> 79-105. </pages>
Reference: <author> Siskind, J.M. </author> <year> (1996), </year> <title> A Computational Study of Cross Situational Techniques for Learning Word-to-Meaning Mappings, </title> <journal> Cognition 61: </journal> <pages> 39-91. </pages>
Reference: <author> Srihari, R.K. </author> <year> (1991), </year> <title> PICTION: A System that Uses Captions to Label Human Faces in Newspaper Photographs, </title> <booktitle> AAAI-91: </booktitle> <pages> 80-85. </pages>
Reference: <author> Srihari, R.K. & Rapaport, W.J. </author> <year> (1989), </year> <title> Extracting Visual Information from Text: Using Captions to Label Human Faces in Newspaper Photographs, </title> <booktitle> Proc. 11th Annual Conf. Cog. </booktitle> <publisher> Sci. Soc.: </publisher> <pages> 364-371. </pages>
Reference: <author> Sternberg, R.J. </author> <year> (1987), </year> <title> Most Vocabulary is Learned from Context, in M.G. McKeown & M.E. Curtis (eds.), The Nature of Vocabulary Acquisition (Erlbaum): 89-105. </title> <booktitle> Webster's New Int'l. Dictionary of the English Language (2/e, </booktitle> <address> Unabridged) (Merriam, </address> <year> 1937). </year>
Reference: <author> Zadrozny, W., & Jensen, K. </author> <year> (1991), </year> <title> Semantics of Para graphs, </title> <journal> Comp. Ling. </journal> <volume> 17: </volume> <pages> 171-209. </pages>

References-found: 33

