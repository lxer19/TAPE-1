URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95545.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: e-mail: granston@cs.rice.edu  e-mail: sasha@csrd.uiuc.edu  
Title: Combining Flow and Dependence Analyses to Expose Redundant Array Accesses  
Author: Elana D. Granston Alexander V. Veidenbaum 
Keyword: Key words: redundancy detection, communication analysis, flow analysis of parallel programs, dependence analysis, and data reuse.  
Address: 6100 S. Main Street, Houston, Texas 77005  1308 W. Main Street, Urbana, Illinois 61801  
Affiliation: Center for Research on Parallel Computation Rice University  Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Note: To appear in the International Journal of Parallel Programming  
Abstract: The success of large-scale hierarchical and distributed shared memory systems hinges on our ability to reduce delays resulting from remote accesses to shared data. To facilitate this, we present a compile-time algorithm for analyzing programs with doall-style parallelism to determine when read and write accesses to shared data are redundant (unnecessary). Once identified, redundant remote accesses can be replaced by local accesses or eliminated entirely. This optimization improves program performance in two ways. First, slow memory accesses are replaced by faster ones. Second, the time to perform other remote memory accesses may be reduced as a result of the decreased traffic level. We also show how the information obtained through redundancy analysis can be used for other compiler optimizations such as prefetching and cache management. 
Abstract-found: 1
Intro-found: 1
Reference: [AHD93] <author> Bill Applebe, Charles Hardnett, and Sri Doddapaneni. </author> <title> Program Transformation for Locality Using Affinity Regions. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: This popularity is due in part to the lower overhead of SPMD-style scheduling in the presence of short epoch instances, which are common in practice, and in part to the additional opportunities to exploit locality that can be exposed at compile time <ref> [AL93, AHD93, BGM95] </ref>. With static inter-epoch scheduling, the compiler must choose some technique for mapping iterations to processors. Although there are several methods for assigning loop iterations to processors, advocating specific scheduling policies is not our goal.
Reference: [AL93] <author> Jennifer Anderson and Monica Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: This popularity is due in part to the lower overhead of SPMD-style scheduling in the presence of short epoch instances, which are common in practice, and in part to the additional opportunities to exploit locality that can be exposed at compile time <ref> [AL93, AHD93, BGM95] </ref>. With static inter-epoch scheduling, the compiler must choose some technique for mapping iterations to processors. Although there are several methods for assigning loop iterations to processors, advocating specific scheduling policies is not our goal. <p> Anderson and Lam <ref> [AL93] </ref> have extended the first approach so that they could analyze multiple loop nests and, based on this analysis, apply transformations to increase the number of redundancies (equivalently, to increase locality). However, their framework cannot handle conditionally executed code. Others have concentrated on extending the second approach.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: The concept of downwardly reaching varies subtly from the classical definition of reaching <ref> [ASU86] </ref>, where data reach from definitions (writes) only. For our purposes, data can reach from either a read or write reference, since either can produce a locally available data copy. <p> In a structured program, two types of intervals arise: those that correspond to loops, and those for which there are no backward branches to blocks within the interval [RP86, GS90]. These are termed loop intervals and non-loop intervals, respectively. By definition <ref> [ASU86, RP86] </ref>, each interval has one entry block. For structured programs, the program flow graph can be constructed such that each loop interval has not only one entry block but one exit block as well. <p> For specific details regarding algorithm extensions, see Granston [Gra92]. 5.2 Identifying Redundancy Inducers Earlier in this paper, an algorithm was presented for identifying the redundant references themselves, as well as the references that induce each redundancy. The analog in classical flow analysis is use-definition chains <ref> [ASU86] </ref>. Alternatively, we can compute the analog of definition-use chains, namely the set of redundancy inducers and, for each redundancy inducer, the set of redundancies that it induces. Consider the problem of identifying read redundancy inducers.
Reference: [Bal90] <author> Vasanth Balasundaram. </author> <title> A Mechanism for Keeping Useful Internal Information in Parallel Programming Tools: The Data Access Descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 154-170, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods <ref> [Bal90, HK91, Sch89] </ref> and exact dependence analysis techniques such as [HHL90, Pug92] to meet these requirements. Hence, one of the strengths of the algorithm that will be presented in Section 4.2 is that it is not dependent on any particular method for representing array regions and computing set operations. <p> contains all data conditionally referenced within the loop: CREF w (summary blk) = ] 1llmax COUT w l (exit blk): The union operators in the above equations are inscribed with a plus sign (+) to indicate that these unions ] i min ii max h (i) are implemented by translation <ref> [Bal90] </ref>, whereby the range of the union iteration space i min : i max is substituted for the union iteration variable i in function h.
Reference: [BGM95] <author> Fran~cois Bodin, Elana D. Granston, and Thierry Montaut. </author> <title> Page-level Affinity Scheduling for Eliminating False Sharing. </title> <booktitle> In Fifth Workshop on Compilers for Parallel Computers, Malaga, </booktitle> <address> Spain, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: This popularity is due in part to the lower overhead of SPMD-style scheduling in the presence of short epoch instances, which are common in practice, and in part to the additional opportunities to exploit locality that can be exposed at compile time <ref> [AL93, AHD93, BGM95] </ref>. With static inter-epoch scheduling, the compiler must choose some technique for mapping iterations to processors. Although there are several methods for assigning loop iterations to processors, advocating specific scheduling policies is not our goal.
Reference: [BJEW91] <author> Francois Bodin, William Jalby, Christine Eisenbeis, and Daniel Windheiser. </author> <title> Window-Based Register Allocation. </title> <type> Technical report, </type> <institution> INRIA, </institution> <year> 1991. </year>
Reference-contexts: Although we did not consider the use of registers in the examples presented above, the private copies could just as easily have been register-allocated, assuming that a sufficient number of registers were available <ref> [BJEW91] </ref>. Therefore, basic redundancy analysis is useful both for optimizing cache coherence protocols and for determining when private copies can be safely made.
Reference: [CKM88] <author> Ron Cytron, Steve Karlovsky, and Kevin P. McAuliffe. </author> <title> Automatic Management of Programmable Caches Using Flow Analysis. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 229-238, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The compiler can override this default for sets of shared data by using directives or making private copies of data to be cached, but then it assumes the responsibility of maintaining coherence for these data. Compiler-directed coherence algorithms <ref> [Vei86, CKM88, CV88, DMCK92] </ref> must be conservative.
Reference: [CON93] <institution> CONVEX Computer Corporation, 3000 Waterview Parkway, Richardson, TX 75083-3851. Exemplar Architecture, </institution> <month> November </month> <year> 1993. </year> <title> Order No. </title> <publisher> DHW-014. </publisher>
Reference-contexts: Although research prototypes of such systems have existed for some time [GKLS83, LP92, LLJ + 92, KDCZ94], recently commercial versions such as the Kendall Square Research KSR1 and KSR2 [Ken92], the Cray T3D [Cra93], and the CONVEX Exemplar <ref> [CON93] </ref> have appeared on the market, and plans to build others have been announced. Although there is significant architectural variation between these systems, all of them have some local storage facilities | for example, cache and software-controllable local memory | that are cheaper to access than remote memory.
Reference: [Cra93] <author> Cray Research, Inc. </author> <title> CRAY T3D System Architecture Overview, </title> <year> 1993. </year>
Reference-contexts: 1 Introduction Large-scale, high-performance, hierarchical and distributed shared memory systems are currently gaining in popularity. Although research prototypes of such systems have existed for some time [GKLS83, LP92, LLJ + 92, KDCZ94], recently commercial versions such as the Kendall Square Research KSR1 and KSR2 [Ken92], the Cray T3D <ref> [Cra93] </ref>, and the CONVEX Exemplar [CON93] have appeared on the market, and plans to build others have been announced.
Reference: [CV88] <author> Hoichi Cheong and Alexander V. Veidenbaum. </author> <title> Stale Data Detection and Coherence Enforcement Using Flow Analysis. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 138-145, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The compiler can override this default for sets of shared data by using directives or making private copies of data to be cached, but then it assumes the responsibility of maintaining coherence for these data. Compiler-directed coherence algorithms <ref> [Vei86, CKM88, CV88, DMCK92] </ref> must be conservative. <p> Therefore, the number of unnecessary invalidations that the compiler inserts is inversely proportional to the precision of the information available at compile time. 4 Because the results of redundancy analysis provide more detailed information on stale data (equivalently, non-redundant data) than was previously available <ref> [CV88] </ref>, the compiler can be more sparing regarding the number of invalidation instructions that it inserts. For example, consider Figure 2 (left). Because of our scheduling assumptions, the references to B [i] are redundant. <p> However, their framework cannot handle conditionally executed code. Others have concentrated on extending the second approach. Gross and Steenkiste [GS90] and Rosene [Ros90] were the first to combine flow and dependence analyses to analyze array dependences in sequential programs at a more precise level. Cheong and Veidenbaum <ref> [CV88] </ref>, while still treating arrays at a name-only level, were the first to extend flow analysis to handle programs with doall loops. Their technique handles sequential loops and control flow constructs as well.
Reference: [DMCK92] <author> Ervan Darnell, John M. Mellor-Crummey, and Ken Kennedy. </author> <title> Automatic Software Cache Coherence Through Vectorization. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 129-138, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The compiler can override this default for sets of shared data by using directives or making private copies of data to be cached, but then it assumes the responsibility of maintaining coherence for these data. Compiler-directed coherence algorithms <ref> [Vei86, CKM88, CV88, DMCK92] </ref> must be conservative.
Reference: [FGS94] <author> Jeanne Ferrante, Dirk Grunwald, and Harini Srinivasan. </author> <title> Computing Communication Sets for Control Parallel Programs. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Cheong and Veidenbaum [CV88], while still treating arrays at a name-only level, were the first to extend flow analysis to handle programs with doall loops. Their technique handles sequential loops and control flow constructs as well. Ferrante et al. <ref> [FGS94] </ref> presented a combined flow and dependence analysis technique that could be adapted to detect redundancies. While their framework can handle explicitly parallel programs with cobegin/coend and post/wait constructs as well as conditional code and array subscript expressions, it cannot handle loops.
Reference: [GKLS83] <author> Daniel Gajski, David Kuck, Duncan Lawrie, and Ahmed Sameh. </author> <title> Cedar a Large Scale Multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 524-529, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: 1 Introduction Large-scale, high-performance, hierarchical and distributed shared memory systems are currently gaining in popularity. Although research prototypes of such systems have existed for some time <ref> [GKLS83, LP92, LLJ + 92, KDCZ94] </ref>, recently commercial versions such as the Kendall Square Research KSR1 and KSR2 [Ken92], the Cray T3D [Cra93], and the CONVEX Exemplar [CON93] have appeared on the market, and plans to build others have been announced.
Reference: [GMB] <author> Elana D. Granston, Thierry Montaut, and Fran~cois Bodin. </author> <title> Loop Transformations to Prevent False Sharing. </title> <note> To appear in the International Journal of Parallel Programming. </note>
Reference-contexts: When static inter-epoch scheduling is employed, optimizations such as aligning accesses across several doall loops can increase the number of redundancies that exists <ref> [GMB] </ref>. Note that, even when reuse is detected, insufficient temporal locality or local storage space may prevent us from capitalizing on them. Hence, redundancy analysis is most beneficial when combined with locality-enhancing program transformations.
Reference: [Gra92] <author> Elana D. Granston. </author> <title> Reducing Memory Access Delays in Large-Scale, Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Center for Supercomputing Research and Development, </institution> <type> Technical Report 1257, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Therefore, the equations for computing the sets of data that unconditionally reach across boundary blocks must reflect the fact that data can now reach across boundary blocks from other processors as well. For specific details regarding algorithm extensions, see Granston <ref> [Gra92] </ref>. 5.2 Identifying Redundancy Inducers Earlier in this paper, an algorithm was presented for identifying the redundant references themselves, as well as the references that induce each redundancy. The analog in classical flow analysis is use-definition chains [ASU86].
Reference: [GS90] <author> Thomas Gross and Peter Steenkiste. </author> <title> Structured Dataflow Analysis for Arrays and Its Use in an Optimizing Compiler. </title> <journal> Software Practice& Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Therefore, we use a variant of interval analysis that was devised by Gross and Steenkiste <ref> [GS90] </ref> specifically for combining flow and dependence analyses. Forward flow analysis is used to compute downwardly reaching references; backward flow analysis is used to compute upwardly reaching ones. <p> In a structured program, two types of intervals arise: those that correspond to loops, and those for which there are no backward branches to blocks within the interval <ref> [RP86, GS90] </ref>. These are termed loop intervals and non-loop intervals, respectively. By definition [ASU86, RP86], each interval has one entry block. For structured programs, the program flow graph can be constructed such that each loop interval has not only one entry block but one exit block as well. <p> However, their framework cannot handle conditionally executed code. Others have concentrated on extending the second approach. Gross and Steenkiste <ref> [GS90] </ref> and Rosene [Ros90] were the first to combine flow and dependence analyses to analyze array dependences in sequential programs at a more precise level. Cheong and Veidenbaum [CV88], while still treating arrays at a name-only level, were the first to extend flow analysis to handle programs with doall loops.
Reference: [GSS94] <author> Manish Gupta, Edith Schonberg, and Harini Srinivasan. </author> <title> A Unified Framework for Optimizing Communication. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: In contrast, the combined flow and dependence analysis technique presented in this paper can handle doall loops, sequential loops (including multiple loop nests and non-perfectly nested loops), and conditional code, while considering array subscript expressions. While we targeted loop-level parallelism, Gupta et al. <ref> [GSS94] </ref> and Hanxleden and Kennedy [HK93] concentrated on combining flow and dependence analyses with the goal of optimizing the placement of sends and receives when compiling data-parallel programs.
Reference: [HHL90] <author> Lorenz Huelsbergen, Douglas Hahn, and James Larus. </author> <title> Exact Data Dependence Analysis Using Data Access Descriptors. </title> <type> Technical Report 945, </type> <institution> Computer Science Department, University of Wisconsin-Madison, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods [Bal90, HK91, Sch89] and exact dependence analysis techniques such as <ref> [HHL90, Pug92] </ref> to meet these requirements. Hence, one of the strengths of the algorithm that will be presented in Section 4.2 is that it is not dependent on any particular method for representing array regions and computing set operations.
Reference: [HK91] <author> Paul Havlak and Ken Kennedy. </author> <title> An Implementation of Interprocedural Bounded Regular Section Analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods <ref> [Bal90, HK91, Sch89] </ref> and exact dependence analysis techniques such as [HHL90, Pug92] to meet these requirements. Hence, one of the strengths of the algorithm that will be presented in Section 4.2 is that it is not dependent on any particular method for representing array regions and computing set operations.
Reference: [HK93] <author> Reinhard von Hanxleden and Ken Kennedy. </author> <title> A Code Placement Framework and Its Application to Communication Generation. </title> <type> Technical Report CRPC-TR93337-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: In contrast, the combined flow and dependence analysis technique presented in this paper can handle doall loops, sequential loops (including multiple loop nests and non-perfectly nested loops), and conditional code, while considering array subscript expressions. While we targeted loop-level parallelism, Gupta et al. [GSS94] and Hanxleden and Kennedy <ref> [HK93] </ref> concentrated on combining flow and dependence analyses with the goal of optimizing the placement of sends and receives when compiling data-parallel programs.
Reference: [KDCZ94] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed Shared Memory On Standard Workstations and Operating Systems. </title> <booktitle> In Winter Usenix Conference, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction Large-scale, high-performance, hierarchical and distributed shared memory systems are currently gaining in popularity. Although research prototypes of such systems have existed for some time <ref> [GKLS83, LP92, LLJ + 92, KDCZ94] </ref>, recently commercial versions such as the Kendall Square Research KSR1 and KSR2 [Ken92], the Cray T3D [Cra93], and the CONVEX Exemplar [CON93] have appeared on the market, and plans to build others have been announced.
Reference: [Ken92] <institution> Kendall Square Research Corporation. Kendall Square Research Technical Summary, </institution> <year> 1992. </year> <month> 34 </month>
Reference-contexts: 1 Introduction Large-scale, high-performance, hierarchical and distributed shared memory systems are currently gaining in popularity. Although research prototypes of such systems have existed for some time [GKLS83, LP92, LLJ + 92, KDCZ94], recently commercial versions such as the Kendall Square Research KSR1 and KSR2 <ref> [Ken92] </ref>, the Cray T3D [Cra93], and the CONVEX Exemplar [CON93] have appeared on the market, and plans to build others have been announced.
Reference: [LLJ + 92] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Large-scale, high-performance, hierarchical and distributed shared memory systems are currently gaining in popularity. Although research prototypes of such systems have existed for some time <ref> [GKLS83, LP92, LLJ + 92, KDCZ94] </ref>, recently commercial versions such as the Kendall Square Research KSR1 and KSR2 [Ken92], the Cray T3D [Cra93], and the CONVEX Exemplar [CON93] have appeared on the market, and plans to build others have been announced.
Reference: [LP92] <author> Z. Lajormi and T. Priol. KOAN: </author> <title> A Shared-Memory for the iPSC/2 Hypercube. In CON-PAR/VAPP92, </title> <publisher> LNCS 634. Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Large-scale, high-performance, hierarchical and distributed shared memory systems are currently gaining in popularity. Although research prototypes of such systems have existed for some time <ref> [GKLS83, LP92, LLJ + 92, KDCZ94] </ref>, recently commercial versions such as the Kendall Square Research KSR1 and KSR2 [Ken92], the Cray T3D [Cra93], and the CONVEX Exemplar [CON93] have appeared on the market, and plans to build others have been announced.
Reference: [MR79] <author> E. Morel and C. </author> <title> Renvoise. Global Optimization by Suppression of Partial Redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: The compiler can detect and eliminate redundant references to scalar data in sequential programs relatively easily with the use of well-established techniques for eliminating redundant computations <ref> [MR79, RWZ88] </ref>. However, handling parallel numerical applications adds additional levels of complexity. First, redundant accesses to array data become increasingly important to detect. 1 Second, the partitioning of the computation across processors must be considered. <p> A subset of these data, namely A [1:N:6], conditionally downwardly reaches R 1 from R 0 . Therefore, R 1 is conditionally read redundant. In Figure 4, recall that array 4 The term partially redundant is used differently here than in Morel and Renvoise <ref> [MR79] </ref>. 8 elements B [1:N:2] unconditionally upwardly reach R 0 from the pair of references R 1 and R 2 , because B [1:N:2] are written along both branches of the if statement.
Reference: [Pug92] <author> William Pugh. </author> <title> The Omega Test: A Fast and Practical Integer Programming Algorithm for Dependence Analysis. </title> <journal> Communications of the ACM, </journal> <pages> pages 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods [Bal90, HK91, Sch89] and exact dependence analysis techniques such as <ref> [HHL90, Pug92] </ref> to meet these requirements. Hence, one of the strengths of the algorithm that will be presented in Section 4.2 is that it is not dependent on any particular method for representing array regions and computing set operations.
Reference: [Ros90] <author> Carl M. Rosene. </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <note> Technical Report COMP TR90-112, </note> <month> March </month> <year> 1990. </year>
Reference-contexts: However, their framework cannot handle conditionally executed code. Others have concentrated on extending the second approach. Gross and Steenkiste [GS90] and Rosene <ref> [Ros90] </ref> were the first to combine flow and dependence analyses to analyze array dependences in sequential programs at a more precise level. Cheong and Veidenbaum [CV88], while still treating arrays at a name-only level, were the first to extend flow analysis to handle programs with doall loops.
Reference: [RP86] <author> Barbara G. Ryder and Marvin C. Paull. </author> <title> Elimination Algorithms for Data Flow Analysis. </title> <journal> Computing Surveys, </journal> <volume> 18(3) </volume> <pages> 277-316, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: In a structured program, two types of intervals arise: those that correspond to loops, and those for which there are no backward branches to blocks within the interval <ref> [RP86, GS90] </ref>. These are termed loop intervals and non-loop intervals, respectively. By definition [ASU86, RP86], each interval has one entry block. For structured programs, the program flow graph can be constructed such that each loop interval has not only one entry block but one exit block as well. <p> In a structured program, two types of intervals arise: those that correspond to loops, and those for which there are no backward branches to blocks within the interval [RP86, GS90]. These are termed loop intervals and non-loop intervals, respectively. By definition <ref> [ASU86, RP86] </ref>, each interval has one entry block. For structured programs, the program flow graph can be constructed such that each loop interval has not only one entry block but one exit block as well. <p> Assume that the upper bounds on the nesting depth, the number of array dimensions, and the number of summary shapes permitted per data descriptor are small constants. Then, the total number of basic and summary blocks is O (S) <ref> [RP86] </ref>. Consequently, the number of steps is also bounded by O (S). At worst, the time to perform a single set operation is O (S V). Because there are only a finite number of set operations per step, the time cost per step is also O (S V).
Reference: [RWZ88] <author> Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Global Value Numbers and Redundant Computations. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 12-27. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1988. </year>
Reference-contexts: The compiler can detect and eliminate redundant references to scalar data in sequential programs relatively easily with the use of well-established techniques for eliminating redundant computations <ref> [MR79, RWZ88] </ref>. However, handling parallel numerical applications adds additional levels of complexity. First, redundant accesses to array data become increasingly important to detect. 1 Second, the partitioning of the computation across processors must be considered. <p> In Section 2.2, we demonstrate the importance of computing redundancy information by presenting examples to show how the results of such analysis could be used to reduce delays due to shared data accesses on today's systems. 1 Although Rosen et al. <ref> [RWZ88] </ref> also handle the case of individual array elements when subscript expressions are equivalent, handling just this limited case is insufficient when trying to aggressively optimize parallel applications. 2 doall i=1 to N enddoall doall i=1 to N enddoall fork p i =1 to N barrier if my pid = p
Reference: [Sch89] <author> Dale Schouten. </author> <title> An Overview of Interprocedural Analysis Techniques for High Performance Paralleliz-ing Compilers. </title> <type> Master's thesis, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Last, the shape must be represented as a function of the indices of enclosing loops, so that the reference instance corresponding to a particular iteration can be represented. It is straightforward to adapt existing shape representation methods <ref> [Bal90, HK91, Sch89] </ref> and exact dependence analysis techniques such as [HHL90, Pug92] to meet these requirements. Hence, one of the strengths of the algorithm that will be presented in Section 4.2 is that it is not dependent on any particular method for representing array regions and computing set operations.
Reference: [TGJ93] <author> Olivier Temam, Elana D. Granston, and William Jalby. </author> <title> To Copy or Not to Copy: A Compile-Time Technique for Assessing When Data Copying Should be Used to Eliminate Cache Conflicts. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 410-419, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: For simplicity and increased readability, we omit stripmining from the examples in this paper. 5 of shared data misses due to cache conflicts <ref> [TGJ93] </ref>. Although we did not consider the use of registers in the examples presented above, the private copies could just as easily have been register-allocated, assuming that a sufficient number of registers were available [BJEW91].
Reference: [Vei86] <author> Alexander V. Veidenbaum. </author> <title> A Compiler-assisted Cache Coherence Solution for Multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 1029-1036, </pages> <month> August </month> <year> 1986. </year> <month> 35 </month>
Reference-contexts: The compiler can override this default for sets of shared data by using directives or making private copies of data to be cached, but then it assumes the responsibility of maintaining coherence for these data. Compiler-directed coherence algorithms <ref> [Vei86, CKM88, CV88, DMCK92] </ref> must be conservative.
References-found: 32

