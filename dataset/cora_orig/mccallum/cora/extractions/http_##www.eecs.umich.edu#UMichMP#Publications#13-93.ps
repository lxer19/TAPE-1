URL: http://www.eecs.umich.edu/UMichMP/Publications/13-93.ps
Refering-URL: http://www.eecs.umich.edu/UMichMP/abstracts.html
Root-URL: http://www.cs.umich.edu
Title: A Microarchitectural Performance Evaluation of a 3.2 Gbyte/s Microprocessor Bus  
Author: Tim Stanley, Michael Upton, Patrick Sherhart Trevor Mudge, Richard Brown 
Keyword: I/O Microarchitecture, Bandwidth, Latency, Hardware Description Language, Performance Modeling.  
Address: Ann Arbor, Michigan 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory Department of Electrical Engineering and Computer Science University of Michigan  
Abstract: This paper evaluates the performance of 3.2 Gbyte/s peak bandwidth, low-latency arbitration bus connecting a GaAs superscalar CPU to a GaAs memory management unit. A microarchitectural performance model was written in the Verilog hardware description language. Bus transactions characteristic of the SPECint92 benchmarks and other workloads were generated as input. Sustained bandwidths of 1.68 Gbytes/s were achieved with arbitration costs of less than 0.5 cycles per data transfer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. L. Hennessy and N. P. Jouppi, </author> <title> "Computer technology and architecture: An evolving interaction," </title> <journal> IEEE Computer, </journal> <volume> vol. 24, </volume> <pages> pp. 18-29, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Processor clock speeds are increasing at 50 % per year <ref> [1] </ref>, due to technology improvements and a better understanding of the interaction between technology and architecture. <p> Necessarily, numerous architectural innovations have been developed to minimize the processor and memory hierarchy performance mismatch (see Table 1). Some researchers suggest that a reduction in latency, rather than an increase in peak bandwidth might lead to faster machines <ref> [1] </ref>. We agree and would be pleased to find memory access latencies scaling with our clock speed.
Reference: [2] <author> D. Dobberpuhl, et al., </author> <title> "A 200 MHz 64b dual-issue CMOS microprocessor," </title> <booktitle> in 1992 IEEE International Solid-State Circuits Conference Digest of Technical Papers, </booktitle> <pages> pp. 254-256. </pages> <publisher> IEEE, </publisher> <month> Feb </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Processor clock speeds are increasing at 50 % per year [1], due to technology improvements and a better understanding of the interaction between technology and architecture. Superscalar CMOS microprocessors have reached clock speeds of 200 MHz <ref> [2] </ref>; an ECL microprocessor has achieved speeds of 300 MHz [3]; and GaAs promises to become a viable processor technology at speeds exceeding 200 MHz [4]. fl This work was supported by the Advance Research Projects Agency under DARPA/ARO Contract DAAL03-90-C-0028.
Reference: [3] <author> N. Jouppi, et al., </author> <title> "A 300 MHz 115W 32b bipolar ECL microprocessor with on-chip caches," </title> <booktitle> in 1993 IEEE International Solid-State Circuits Conference Digest of Technical Papers, </booktitle> <pages> pp. 84-85. </pages> <publisher> IEEE, </publisher> <month> Feb </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Processor clock speeds are increasing at 50 % per year [1], due to technology improvements and a better understanding of the interaction between technology and architecture. Superscalar CMOS microprocessors have reached clock speeds of 200 MHz [2]; an ECL microprocessor has achieved speeds of 300 MHz <ref> [3] </ref>; and GaAs promises to become a viable processor technology at speeds exceeding 200 MHz [4]. fl This work was supported by the Advance Research Projects Agency under DARPA/ARO Contract DAAL03-90-C-0028.
Reference: [4] <author> M. Upton, T. Huff, P. Sherhart, P. Barker, R. McVay, T. Stanley, R. Brown, R. Lomax, T. Mudge, and K. Sakallah, </author> <title> "A 160,000-transistor GaAs microprocessor," </title> <booktitle> in 1993 IEEE International Solid-State Circuits Conference Digest of Technical Papers, </booktitle> <pages> pp. 92-93. </pages> <publisher> IEEE, </publisher> <month> Feb </month> <year> 1993. </year>
Reference-contexts: Superscalar CMOS microprocessors have reached clock speeds of 200 MHz [2]; an ECL microprocessor has achieved speeds of 300 MHz [3]; and GaAs promises to become a viable processor technology at speeds exceeding 200 MHz <ref> [4] </ref>. fl This work was supported by the Advance Research Projects Agency under DARPA/ARO Contract DAAL03-90-C-0028. Meanwhile, it has been widely observed that the speed of DRAM technologies is increasing at only about 7 % per year and is not keeping pace with microprocessor performance.
Reference: [5] <author> A. Smith, </author> <title> "Sequential program prefetching in memory hierarchies," </title> <journal> IEEE Computer, </journal> <volume> vol. 11, no. 12, </volume> <pages> pp. 7-21, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: Hardware prefetching <ref> [5] </ref>, stream buffers [6], and software-directed prefetching [7] effectively trade increased bandwidth for reduced latency. Latency Tolerance Non-blocking and lockup-free caches [8] lessen the impact of data dependencies by allowing execution to proceed while a cache miss is serviced.
Reference: [6] <author> N. Jouppi, </author> <title> "Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers," </title> <booktitle> in Proceedings of the 17 th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 364-373, </pages> <address> New York NY (USA), </address> <month> May </month> <year> 1990, </year> <note> IEEE. </note>
Reference-contexts: Hardware prefetching [5], stream buffers <ref> [6] </ref>, and software-directed prefetching [7] effectively trade increased bandwidth for reduced latency. Latency Tolerance Non-blocking and lockup-free caches [8] lessen the impact of data dependencies by allowing execution to proceed while a cache miss is serviced. Dynamic scheduling techniques trade additional latency to achieve better latency tolerance and high throughput.
Reference: [7] <author> D. Callahan, K. Kennedy, and D. Porterfield, </author> <title> "Software prefetching," </title> <booktitle> in Proceedings of the 4 th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 40-52. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: Hardware prefetching [5], stream buffers [6], and software-directed prefetching <ref> [7] </ref> effectively trade increased bandwidth for reduced latency. Latency Tolerance Non-blocking and lockup-free caches [8] lessen the impact of data dependencies by allowing execution to proceed while a cache miss is serviced. Dynamic scheduling techniques trade additional latency to achieve better latency tolerance and high throughput.
Reference: [8] <author> D. Kroft, </author> <title> "Lockup-free instruction fetch/prefetch cache organization," </title> <booktitle> in Proceedings of the 8 th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 81-87, </pages> <year> 1981. </year>
Reference-contexts: Hardware prefetching [5], stream buffers [6], and software-directed prefetching [7] effectively trade increased bandwidth for reduced latency. Latency Tolerance Non-blocking and lockup-free caches <ref> [8] </ref> lessen the impact of data dependencies by allowing execution to proceed while a cache miss is serviced. Dynamic scheduling techniques trade additional latency to achieve better latency tolerance and high throughput. This is an instance where latency is transformed to increased throughput (bandwidth).
Reference: [9] <author> G. Kane and J. Heinrich, </author> <title> MIPS RISC Architecture, </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: Dynamic scheduling techniques trade additional latency to achieve better latency tolerance and high throughput. This is an instance where latency is transformed to increased throughput (bandwidth). Architecturally specified load and branch delay slots <ref> [9] </ref> tolerate latency by filling empty pipeline stages. Compiler loop unrolling and code reordering tolerate latency by exposing instruction and data parallelism. Increased Bandwidth Requirements Multi-issue processors improve system performance by increasing the actual bandwidth built into the system but further exacerbate the impact of latency on overall system performance. <p> The 300+ MHz superscalar CPU chip implements a subset of the the MIPS ISA <ref> [9] </ref>, has a non-blocking D-cache, and provides hardware I- and D-stream prefetch-ing. The 300+ MHz Cache and Memory Management Unit (MMU) implements concurrent I- and D-stream datapaths, off-chip secondary caches, hardware prefetching, memory management support, and an I/O bus interface.
Reference: [10] <author> M. Johnson, </author> <title> Superscalar Microprocessor Design, </title> <publisher> Prentice-Hall Inc., </publisher> <year> 1991. </year>
Reference-contexts: In general, load/store implementations intended to perform more than 1 instruction per cycle are increasingly more sensitive to branch, load, and cache miss latency <ref> [10] </ref>. These architectures demand concurrent I- and D-stream access. Table 1: Effect of Recent Architectural Trends on System Bandwidth and Latency. Recent architectural trends can be grouped into three categories: techniques to reduce average access latency; techniques to tolerate latency; and techniques that use additional system bandwidth to increase throughput.
Reference: [11] <author> N. Kushiyama, et al., </author> <title> "A 500-megabyte/s data-rate 4.5m DRAM," </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 28, no. 4, </volume> <pages> pp. 490-498, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The ideal bus would be limited by the bandwidth of the interconnect medium, and failing this by the signal rise/fall times. We have adopted a signaling scheme similar to that used by Rambus 1 <ref> [11] </ref> that is limited by the signal rise/fall times rather than by clock skew or signal setup/hold time.
References-found: 11

