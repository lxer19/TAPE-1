URL: ftp://ftp.cnl.salk.edu/pub/marni/cg96bart.ps.Z
Refering-URL: http://www.cnl.salk.edu/~marni/
Root-URL: 
Title: Learning Viewpoint Invariant Representations of Faces in an Attractor Network  
Author: Marian Stewart Bartlett ; and Terrence J. Sejnowski ;; 
Web: www: http://www.cnl.salk.edu/ marni  ftp.cnl.salk.edu, pub/marni/cg96bart.ps.Z  
Affiliation: 1 University of California San Diego; 2 Howard Hughes Medical Institute; 3 The Salk Institute;  
Date: July 12-15, 1996.  
Address: San Diego, CA,  
Note: Presented at the 18th Cognitive Science Society Meeting,  anonymous ftp:  
Abstract: In natural visual experience, different views of an object tend to appear in close temporal proximity as an animal manipulates the object or navigates around it. We investigated the ability of an attractor network to acquire view invariant visual representations by associating first neighbors in a pattern sequence. The pattern sequence contains successive views of faces of ten individuals as they change pose. Under the network dynamics developed by Griniasty, Tsodyks & Amit (1993), multiple views of a given subject fall into the same basin of attraction. We use an independent component (ICA) representation of the faces for the input patterns (Bell & Sejnowski, 1995). The ICA representation has advantages over the principal component representation (PCA) for viewpoint-invariant recognition both with and without the attractor network, suggesting that ICA is a better representation than PCA for object recognition. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amit, D. </author> <year> 1995. </year> <title> The Hebbian paradigm reintegrated: Local reverberations as internal representations. </title> <journal> Behav. and Brain Sci. </journal> <volume> 18: </volume> <pages> 617-657. </pages>
Reference-contexts: The sustained activity in attractor networks could support Hebbian learning of temporal associations across much larger time scales than that supported by the open-time of the NMDA channel. Perceptual representations have been related to basins of attraction in activity patterns across an assembly of cells <ref> (Amit, 1995) </ref>. Attractor networks with Hebbian learning mechanisms are capable of acquiring temporal associations between randomly generated patterns (Griniasty, Tsodyks, & Amit, 1993). We investigated the ability of attractor network dynamics to acquire pose invariant representations of faces by forming temporal associations between different views of the faces. A.
Reference: <author> Bartlett, M. Stewart, and Sejnowski, T., </author> <year> (1996). </year> <title> Unsupervised learning of invariant representations of faces through temporal association. </title> <journal> Computational Neuroscience: International Review of Neurobiology Suppl. </journal> <volume> 1 J.M Bower, </volume> <editor> Ed., </editor> <publisher> Academic Press, </publisher> <address> San Diego, CA: 317-322. ftp: ftp.cnl.salk.edu, pub/marni/cns95.ta.ps.Z Beymer, D. </address> <year> 1994. </year> <title> Face recognition under varying pose. </title> <booktitle> In Proceedings of the 1994 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. </booktitle> <address> Los Alamitos, CA: </address> <publisher> IEEE Comput. Soc. Press: </publisher> <pages> 756-61. </pages>
Reference: <author> Bell, A. & Sejnowski, T., </author> <title> (to appear). The independent components of natural scenes are edge filters. </title> <booktitle> Vision Research. </booktitle>
Reference: <author> Bell, A., &Sejnowski, T., </author> <year> 1995. </year> <title> An information Maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Comp. </journal> <volume> 7: </volume> <pages> 1129-1159. </pages> <note> ftp: </note> <author> ftp.cnl.salk.edu, pub/tony/bell.blind.ps.Z Cottrell & Metcalfe, </author> <year> 1991. </year> <title> Face, gender and emotion recognition using holons. </title> <booktitle> In Advances in Neural Information Processing Systems 3, </booktitle> <editor> D. Touretzky, (Ed.), </editor> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA: 564 - 571. </address>
Reference-contexts: The independent components are found through an unsupervised learning algorithm that maximizes the mutual information between the input and the output of a linear transformation <ref> (Bell & Sejnowski, 1995) </ref>. Maximizing the mutual information between the input and the output is equivalent to maximizing the joint entropy of the output units. The weight matrix for this transformation is found by gradient ascent on the joint entropy of the output units.
Reference: <author> Field, D. </author> <year> (1994). </year> <title> What is the goal of sensory encoding? Neural Comp. </title> <booktitle> 6 </booktitle> <pages> 559-601. </pages>
Reference-contexts: ICA also provides a more sparse representation than PCA, which is advantageous for associative memory <ref> (Field, 1994) </ref>. Forty independent component basis vectors derived from the 40 principal components on the left. Basis vectors are the images that maximally activate the independent component filters. For PCA the basis vectors are the same as the components themselves. D.
Reference: <author> Foldiak, P. </author> <year> 1991. </year> <title> Learning invariance from transformation sequences. </title> <journal> Neural Comp. </journal> <volume> 3 </volume> <pages> 194-200. </pages>
Reference: <author> Griniasty, M., Tsodyks, M., & Amit, D. </author> <year> (1993). </year> <title> Conversion of temporal correlations between stimuli to spatial correlations between attractors. </title> <journal> Neural Comp. </journal> <volume> 5 </volume> <pages> 1-17. </pages>
Reference-contexts: Perceptual representations have been related to basins of attraction in activity patterns across an assembly of cells (Amit, 1995). Attractor networks with Hebbian learning mechanisms are capable of acquiring temporal associations between randomly generated patterns <ref> (Griniasty, Tsodyks, & Amit, 1993) </ref>. We investigated the ability of attractor network dynamics to acquire pose invariant representations of faces by forming temporal associations between different views of the faces. A.
Reference: <author> Miyashita, Y. </author> <year> 1988. </year> <title> Neuronal correlate of visual associative long-term memory in the primate temporal cortex. </title> <booktitle> Nature 335(27) </booktitle> <pages> 817-820. </pages>
Reference-contexts: A. Relation to Temporal Lobe Neurons Neurons in the primate anterior inferior temporal lobe are capable of forming temporal associations in their sustained activity patterns. Following prolonged exposure to a sequence of patterns, correlations emerged in the responses to neighboring patterns in the sequence <ref> (Miyashita, 1988) </ref>. delay-match-to-sample task. AIT neurons produced sustained responses to the patterns following removal of the stimulus. Bottom: Autocorrelograms of sustained firing rates to different stimuli plotted as a function of relative position of the stimuli in the training sequence.
Reference: <author> O'Reilly, R. & Johnson, M. </author> <year> 1994. </year> <title> Object recognition and sensitive periods: A computational analysis of visual imprinting. </title> <journal> Neural Comp. </journal> <volume> 6 </volume> <pages> 357-389. </pages>
Reference: <author> O`Toole, Abdi, H., Deffenbacher, K., & Velantin, D. </author> <year> (1993). </year> <title> Low-dimensional representation of faces in higher dimensions of the face space. </title> <journal> Journal of the Optical Society of America A, v10 n3, </journal> <pages> 405-11. </pages>
Reference: <author> Rhodes, P. </author> <year> 1992. </year> <title> The long open time of the NMDA channel facilitates the self-organization of invariant object responses in cortex. </title> <publisher> Soc. </publisher> <address> Neurosci. Abst. 18:740. </address>
Reference-contexts: Temporal association may be an important aspect of invariance learning in the ventral visual stream (Rolls, 1995). A temporal window for Hebbian learning could be provided by the 0.5 second open-time of the NMDA channel <ref> (Rhodes, 1992) </ref>, or by reciprocal connections between cortical regions (O'Reilly, 1994).
Reference: <author> Rolls, E. </author> <year> 1995. </year> <title> Learning Mechanisms in the temporal lobe visual cortex. </title> <journal> Behav. Brain Res. </journal> <volume> 66 </volume> <pages> 177-185. </pages>
Reference-contexts: Capturing the temporal relationships among patterns is a way to automatically associate different views of an object without requiring complex geometrical transformations or three dimensional structural descriptions (Stryker, 1991). Temporal association may be an important aspect of invariance learning in the ventral visual stream <ref> (Rolls, 1995) </ref>. A temporal window for Hebbian learning could be provided by the 0.5 second open-time of the NMDA channel (Rhodes, 1992), or by reciprocal connections between cortical regions (O'Reilly, 1994).
Reference: <author> Stryker, M. </author> <year> 1991. </year> <title> Temporal Associations. </title> <booktitle> Nature 354 </booktitle> <pages> 108-109. </pages>
Reference-contexts: In natural visual experience, different views of an object tend to appear in close temporal proximity. Capturing the temporal relationships among patterns is a way to automatically associate different views of an object without requiring complex geometrical transformations or three dimensional structural descriptions <ref> (Stryker, 1991) </ref>. Temporal association may be an important aspect of invariance learning in the ventral visual stream (Rolls, 1995). A temporal window for Hebbian learning could be provided by the 0.5 second open-time of the NMDA channel (Rhodes, 1992), or by reciprocal connections between cortical regions (O'Reilly, 1994).
Reference: <author> Turk, M., & Pentland, A. </author> <year> 1991. </year> <title> Eigenfaces for Recognition. </title> <journal> J. of Cog. Neurosci. </journal> <volume> 3(1):71 - 86. </volume>
Reference-contexts: A representational basis in which the high order statistics are decorrelated may be more powerful for face recognition than one in which only the second order statistics are decorrelated, as in the Eigenface representation <ref> (Turk & Pentland, 1991) </ref>. ICA also provides a more sparse representation than PCA, which is advantageous for associative memory (Field, 1994). Forty independent component basis vectors derived from the 40 principal components on the left. Basis vectors are the images that maximally activate the independent component filters.
Reference: <author> Wallis, G. & Rolls, E. </author> <year> 1996. </year> <title> A model of invariant object recognition in the visual system. </title> <type> Technical Report, </type> <institution> Oxford University Department of Experimental Psychology. </institution>
Reference: <author> Weinshall, D.& Edelman, S. </author> <year> 1991. </year> <title> A self-organizing multiple view representation of 3D objects. </title> <journal> Bio. Cyber. </journal> <volume> 64(3) </volume> <pages> 209-219. </pages>
References-found: 16

