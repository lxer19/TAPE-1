URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1995/umsi-95-276.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1995/
Root-URL: http://www.cs.umn.edu
Title: Krylov Subspace Methods on Parallel Computers  
Author: Yousef Saad 
Keyword: Key words: Large linear systems; Krylov subspace methods; Iterative methods; Preconditioned Conjugate Gradient; Multicoloring; Incomplete LU preconditioning.  
Affiliation: Minnesota Supercomputer Institute.  
Note: Research supported by the National Science Foundation under grant number NSF/CCR-9214116, by ARPA under grant number NIST 60NANB2D1272, and by the  
Abstract: Krylov subspace methods have enjoyed a growing popularity in computational sciences and engineering in the last decade. They are likely to make further inroads in various applications mainly because of the advances of more sophisticated modeling techniques, particularly three-dimensional ones. Another important reason for the progress of iterative methods, is the inherent difficulty in implementing full-scale direct methods on parallel computers. It is likely that direct solvers will progressively be used as local solvers in conjunction with iterative accelerators. In this overview we discuss some of the issues related to the implementations of parallel iterative solvers, focussing on implementations on distributed parallel environments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. M. Adams and J. Ortega. </author> <title> A multi-color SOR Method for Parallel Computers. </title> <booktitle> In Proceedings of the 1982 International Conference on Pararallel Processing, </booktitle> <pages> pages 53-56, </pages> <year> 1982. </year>
Reference-contexts: One single step of the multicolor SOR iteration can then be efficiently implemented by accessing these p submatrices in succession. One of the most promising approaches in the context of parallel precondi-tioners is to combine multiple-step SOR or SSOR preconditioning with multi-coloring <ref> [40, 12, 1, 31, 34, 32] </ref>. This combines the main advantages of multi-coloring, namely ample degree of parallelism, with the robustness afforded by multiple step techniques. A multiple-step relaxation consists simply of performing s steps of the relaxation scheme instead of one.
Reference: [2] <author> E. C. Anderson and Y. Saad. </author> <title> Solving sparse triangular systems on parallel computers. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1 </volume> <pages> 73-96, </pages> <year> 1989. </year>
Reference-contexts: The preconditioned residual vector M 1 (b Ax i ) obtained at step i is M-orthogonal to all the previous preconditioned residual vectors. So far there has been just a few general principles used to develop parallel preconditioners. The first idea is to exploit `wavefronts' or `level-scheduling' <ref> [2, 38, 45, 52, 48, 49, 20, 44, 4] </ref> for solving the triangular systems that arise in many preconditioners. These are essentially parallel implementations of the forward and backward triangular solves and can always be applied to any ILU-type preconditioner, without changing the preconditioner itself. <p> This can be easily understood by considering the reordered matrix on the right-side of Figure 1.2. 8 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS The idea of wavefronts can easily be extended to general sparse matrices. Here, an inexpensive preprocessing is required to obtain the levels; for details see <ref> [2, 41] </ref>. Performance can be more than adequate on vector processors, by exploiting suitable storage formats and efficient matrix-vector multiplications. For typical problems a gain of a factor of 3 to 5 is easily achievable when compared with a standard implementation. However, this approach has two drawbacks.
Reference: [3] <author> W. E. </author> <title> Arnoldi. The principle of minimized iteration in the solution of the matrix eigenvalue problem. </title> <journal> Quart. Appl. Math., </journal> <volume> 9 </volume> <pages> 17-29, </pages> <year> 1951. </year>
Reference-contexts: The conjugate gradient method is a particular instance of this method when the matrix is symmetric positive definite. Another method in this class is the Full Orthogonalization Method (FOM) [36] which is closely related to Arnoldi's method for solving eigenvalue problems <ref> [3] </ref>. ORTHORES [23] developed by Jea and Young is mathematically equivalent to FOM as is another method derived by Axelsson [5]. By definition, the final residual vector is orthogonal to all of the Krylov subspace. Thus, these methods are sometimes termed orthogonal residual methods.
Reference: [4] <author> C. C. Ashcraft and R. G. Grimes. </author> <title> On vectorizing incomplete factorization and SSOR preconditioners. </title> <journal> SIAM J. on Sci. and Stat. Comput., </journal> <volume> 9 </volume> <pages> 122-151, </pages> <year> 1988. </year>
Reference-contexts: The preconditioned residual vector M 1 (b Ax i ) obtained at step i is M-orthogonal to all the previous preconditioned residual vectors. So far there has been just a few general principles used to develop parallel preconditioners. The first idea is to exploit `wavefronts' or `level-scheduling' <ref> [2, 38, 45, 52, 48, 49, 20, 44, 4] </ref> for solving the triangular systems that arise in many preconditioners. These are essentially parallel implementations of the forward and backward triangular solves and can always be applied to any ILU-type preconditioner, without changing the preconditioner itself.
Reference: [5] <author> O. Axelsson. </author> <title> Conjugate gradient type-methods for unsymmetric and inconsistent systems of linear equations. </title> <journal> Lin. Alg. and its Appl., </journal> <volume> 29 </volume> <pages> 1-16, </pages> <year> 1980. </year>
Reference-contexts: Another method in this class is the Full Orthogonalization Method (FOM) [36] which is closely related to Arnoldi's method for solving eigenvalue problems [3]. ORTHORES [23] developed by Jea and Young is mathematically equivalent to FOM as is another method derived by Axelsson <ref> [5] </ref>. By definition, the final residual vector is orthogonal to all of the Krylov subspace. Thus, these methods are sometimes termed orthogonal residual methods. They have the important property of minimizing the A-norm of the error over x 0 + K m when the matrix A is symmetric positive.
Reference: [6] <author> O. Axelsson. </author> <title> A generalized conjugate gradient, least squares method. </title> <journal> Nu-mer. Math., </journal> <volume> 51 </volume> <pages> 209-227, </pages> <year> 1987. </year>
Reference-contexts: Methods in this category are often termed Minimal Error methods. Quite a few methods have been developed in this class <ref> [6, 23, 14, 43] </ref>. Finally, among oblique projection methods, are the methods based on bi-conjugacy and the Lanczos biorthogonalization method. In these methods, L m = K m (A T ; r 0 ) and K m = K m (A; r 0 ).
Reference: [7] <author> O. Axelsson and P. S. Vassilevski. </author> <title> A block generalized conjugate gradient solver with inner iiterations and variable step preconditioning. </title> <journal> SIAM J. on Mat.rix Anal. and Appl., </journal> <volume> 12, </volume> <year> 1991. </year>
Reference-contexts: A multiple-step relaxation consists simply of performing s steps of the relaxation scheme instead of one. In fact, the number of steps can vary arbitrarily if a "flexible variant" of an accelerator is used. These variants allow a right-preconditioner to be essentially arbitrary; see e.g., <ref> [39, 7, 51] </ref>. In [40] it was shown by means of experiments that on vector computers, one-step relaxation was often far from being the optimal number of steps to use when preconditioning a linear system.
Reference: [8] <author> M. W. Benson and P. O. Frederickson. </author> <title> Iterative solution of large sparse linear systems arising in certain multidimensional approximation problems. </title> <journal> Utilitas Math., </journal> <volume> 22 </volume> <pages> 127-140, </pages> <year> 1982. </year>
Reference-contexts: An alternative is to attempt to obtain a preconditioning matrix M which approximates A 1 directly. For example we may seek a sparse matrix M to minimize the Frobenius norm of I AM . Techniques of this type have received much attention in recent years <ref> [8, 21, 11, 22, 27, 26, 10, 9] </ref>. 14 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS One of the reasons for this interest is parallelism. Another is the common failure of ILU-type preconditioners for indefinite problems.
Reference: [9] <author> E. Chow and Y. Saad. </author> <title> Approximate inverse techniques for block-partitioned matrices. </title> <type> Technical Report UMSI - 95 - 13, </type> <institution> University of Minnesota, Supercomputer Institute, </institution> <address> Minneapolis, MN 55415, </address> <month> Jan. </month> <year> 1995. </year> <note> Submitted. 24 KRYLOV SUBSPACE METHODS 25 </note>
Reference-contexts: An alternative is to attempt to obtain a preconditioning matrix M which approximates A 1 directly. For example we may seek a sparse matrix M to minimize the Frobenius norm of I AM . Techniques of this type have received much attention in recent years <ref> [8, 21, 11, 22, 27, 26, 10, 9] </ref>. 14 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS One of the reasons for this interest is parallelism. Another is the common failure of ILU-type preconditioners for indefinite problems. <p> One of the most promising uses of approximate inverse techniques is to combine them with other, e.g. block, factorizations. A number of techniques based on this viewpoint were developed in [25] and, more recently, in <ref> [9] </ref>. The main idea in [9] is to exploit blockings of the original linear system in the block form, E C x g : (1.10) This blocking may originate from a domain decomposition partitioning for example or may be the original structure of A as in the Navier-Stokes equations for example. <p> One of the most promising uses of approximate inverse techniques is to combine them with other, e.g. block, factorizations. A number of techniques based on this viewpoint were developed in [25] and, more recently, in <ref> [9] </ref>. The main idea in [9] is to exploit blockings of the original linear system in the block form, E C x g : (1.10) This blocking may originate from a domain decomposition partitioning for example or may be the original structure of A as in the Navier-Stokes equations for example.
Reference: [10] <author> E. Chow and Y. Saad. </author> <title> Approximate inverse preconditioners for general sparse matrices. </title> <type> Technical Report UMSI 94-101, </type> <institution> University of Minnesota Supercomputer Institute, </institution> <address> Minneapolis, MN 55415, </address> <month> May </month> <year> 1994. </year> <note> Submitted. </note>
Reference-contexts: An alternative is to attempt to obtain a preconditioning matrix M which approximates A 1 directly. For example we may seek a sparse matrix M to minimize the Frobenius norm of I AM . Techniques of this type have received much attention in recent years <ref> [8, 21, 11, 22, 27, 26, 10, 9] </ref>. 14 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS One of the reasons for this interest is parallelism. Another is the common failure of ILU-type preconditioners for indefinite problems. <p> They will become denser as the iteration number progresses. This was exploited either explicitly or implicitly in several papers; see, e.g., <ref> [10] </ref>. Rough approximations can be obtained inexpensively by performing a very small number of steps starting with the columns of the current M . The initial approximation M is often taken to be the identity matrix.
Reference: [11] <author> J. D. F. Cosgrove, J. C. Daz, and A. Griewank. </author> <title> Approximate inverse preconditioning for sparse linear systems. </title> <journal> Intl. J. Comp. Math., </journal> <volume> 44 </volume> <pages> 91-110, </pages> <year> 1992. </year>
Reference-contexts: An alternative is to attempt to obtain a preconditioning matrix M which approximates A 1 directly. For example we may seek a sparse matrix M to minimize the Frobenius norm of I AM . Techniques of this type have received much attention in recent years <ref> [8, 21, 11, 22, 27, 26, 10, 9] </ref>. 14 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS One of the reasons for this interest is parallelism. Another is the common failure of ILU-type preconditioners for indefinite problems.
Reference: [12] <author> M. A. DeLong and J. M. Ortega. </author> <title> SOR as a preconditioner. </title> <type> Technical Report CS-94-43, </type> <institution> Department of Computer Science, University of Virginia, </institution> <address> Charlottesville, VA, </address> <year> 1994. </year>
Reference-contexts: One single step of the multicolor SOR iteration can then be efficiently implemented by accessing these p submatrices in succession. One of the most promising approaches in the context of parallel precondi-tioners is to combine multiple-step SOR or SSOR preconditioning with multi-coloring <ref> [40, 12, 1, 31, 34, 32] </ref>. This combines the main advantages of multi-coloring, namely ample degree of parallelism, with the robustness afforded by multiple step techniques. A multiple-step relaxation consists simply of performing s steps of the relaxation scheme instead of one. <p> In [40] it was shown by means of experiments that on vector computers, one-step relaxation was often far from being the optimal number of steps to use when preconditioning a linear system. This was confirmed by Delong and Ortega <ref> [12] </ref> who show, in addition, that ! = 1 was also far from being optimal. However, in the context of preconditioning, the usual optimal formulas for obtaining the best ! are no longer valid.
Reference: [13] <author> P. F. Dubois, A. Greenbaum, and G. H. Rodrigue. </author> <title> Approximating the inverse of a matrix for use on iterative algorithms on vectors processors. </title> <journal> Computing, </journal> <volume> 22 </volume> <pages> 257-268, </pages> <year> 1979. </year>
Reference-contexts: We refer to these preconditioners as "data-parallel" only to emphasize the fact that they are characterized by a high degree of fine-grain parallelism. Among the simplest techniques used in this context are polynomial precon-ditioners; see, e.g., <ref> [37, 24, 19, 13] </ref>. Another strategy which has been exploited is that of multicoloring and independent set orderings. These are discussed next.
Reference: [14] <author> H. C. Elman. </author> <title> Iterative Methods for Large Sparse Nonsymmetric Systems of Linear Equations. </title> <type> PhD thesis, </type> <institution> Yale University, Computer Science Dept., </institution> <address> New Haven, CT., </address> <year> 1982. </year>
Reference-contexts: Methods in this category are often termed Minimal Error methods. Quite a few methods have been developed in this class <ref> [6, 23, 14, 43] </ref>. Finally, among oblique projection methods, are the methods based on bi-conjugacy and the Lanczos biorthogonalization method. In these methods, L m = K m (A T ; r 0 ) and K m = K m (A; r 0 ).
Reference: [15] <author> C. Farhat and J. X. Roux. </author> <title> Implicit parallel processing in structural mechanics. </title> <booktitle> Computational Mechanics Advances, </booktitle> <volume> 2(1) </volume> <pages> 1-124, </pages> <year> 1994. </year>
Reference-contexts: Preconditioners for Distributed Sparse Matrices We now turn our attention to preconditioning techniques for distributed sparse matrices. Many of the ideas used in preconditioning distributed sparse matrices are borrowed from Domain Decomposition literature; see for example <ref> [15, 30] </ref>. Among the simplest of these are the use of block preconditionings based on the domains. These are termed Schwarz alternating procedures in the PDE literature. Let R i be a restriction operator which projects a global vector onto its restriction in subdomain i.
Reference: [16] <author> R. Fletcher. </author> <title> Conjugate gradient methods for indefinite systems. </title> <editor> In G. A. Watson, editor, </editor> <booktitle> Proceedings of the Dundee Biennal Conference on Numerical Analysis 1974, </booktitle> <pages> pages 73-89, </pages> <address> New York, 1975. </address> <publisher> University of Dundee,Scotland, Springer Verlag. </publisher>
Reference-contexts: In these methods, L m = K m (A T ; r 0 ) and K m = K m (A; r 0 ). In the nonsymmetric case, the bi-conjugate gradient method (BCG) due to Lanczos [29] and Fletcher <ref> [16] </ref> is a good representative of this class. There are various mathematically equivalent formulations of the biconjugate gradient method some of which are more numerically viable than others.
Reference: [17] <author> R. W. Freund and N. M. Nachtigal. </author> <title> QMR: a quasi-minimal residual method for non-Hermitian linear systems. </title> <journal> Numer. Math., </journal> <volume> 60 </volume> <pages> 315-339, </pages> <year> 1991. </year>
Reference-contexts: In the nonsymmetric case, the bi-conjugate gradient method (BCG) due to Lanczos [29] and Fletcher [16] is a good representative of this class. There are various mathematically equivalent formulations of the biconjugate gradient method some of which are more numerically viable than others. The QMR algorithm <ref> [17] </ref> ignores the non-orthogonality of the Lanczos basis and attempts to minimize the 2-norm of the expression of the residual in this basis. An efficient variation on the BCG algorithm, called CGS (Conjugate gradient squared) and proposed by Sonneveld [46, 33], avoids the use of the transpose of A.
Reference: [18] <author> Roland W. Freund. </author> <title> A Transpose-Free Quasi-Minimal Residual algorithm for non-Hermitian linear systems. </title> <journal> SIAM J. on Sci. Comput., </journal> <volume> 14(2) </volume> <pages> 470-482, </pages> <year> 1993. </year>
Reference-contexts: This gave rise to a whole class of efficient schemes named transpose-free techniques, two notable examples of which are the BiConjugate Gradient Stabilized (BICGSTAB) algorithm of van der Vorst [50], and the Transpose-Free QMR (TFQMR) algorithm of Freund <ref> [18] </ref>. The Krylov subspace methods mentioned above are often referred to as accelerators. More important than the accelerator for the success of a Krylov subspace method is the preconditioning technique used.
Reference: [19] <author> A. Greenbaum and G. H. Rodrigue. </author> <title> The incomplete choleski conjugate gradient for the star (5-point) operator. </title> <type> Technical Report UCID 17574, </type> <institution> Lawrence Livermore National Lab., Livermore, California, </institution> <year> 1977. </year>
Reference-contexts: We refer to these preconditioners as "data-parallel" only to emphasize the fact that they are characterized by a high degree of fine-grain parallelism. Among the simplest techniques used in this context are polynomial precon-ditioners; see, e.g., <ref> [37, 24, 19, 13] </ref>. Another strategy which has been exploited is that of multicoloring and independent set orderings. These are discussed next.
Reference: [20] <author> A. Greenbaum, C. Li, and H. Z. Chao. </author> <title> Parallelizing preconditioned conjugate gradient algorithms. </title> <journal> Computer Physics Communications, </journal> <volume> 53 </volume> <pages> 295-309, </pages> <year> 1989. </year> <title> 26 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS </title>
Reference-contexts: The preconditioned residual vector M 1 (b Ax i ) obtained at step i is M-orthogonal to all the previous preconditioned residual vectors. So far there has been just a few general principles used to develop parallel preconditioners. The first idea is to exploit `wavefronts' or `level-scheduling' <ref> [2, 38, 45, 52, 48, 49, 20, 44, 4] </ref> for solving the triangular systems that arise in many preconditioners. These are essentially parallel implementations of the forward and backward triangular solves and can always be applied to any ILU-type preconditioner, without changing the preconditioner itself.
Reference: [21] <author> M. Grote and H. D. Simon. </author> <title> Parallel preconditioning and approximate inverses on the connection machine. </title> <editor> In R. F. Sincovec, D. E. Keyes, L. R. Petzold, and D. A. Reed, editors, </editor> <booktitle> Parallel Processing for Scientific Computing - vol. </booktitle> <volume> 2, </volume> <pages> pages 519-523. </pages> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: An alternative is to attempt to obtain a preconditioning matrix M which approximates A 1 directly. For example we may seek a sparse matrix M to minimize the Frobenius norm of I AM . Techniques of this type have received much attention in recent years <ref> [8, 21, 11, 22, 27, 26, 10, 9] </ref>. 14 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS One of the reasons for this interest is parallelism. Another is the common failure of ILU-type preconditioners for indefinite problems.
Reference: [22] <author> T. Huckle and M. Grote. </author> <title> A new approach to parallel preconditioning with sparse approximate inverses. </title> <type> Technical Report SCCM-94-03, </type> <institution> Stanford University, Scientific Computing and Computational Mathematics Program, Stanford, California, </institution> <year> 1994. </year>
Reference-contexts: An alternative is to attempt to obtain a preconditioning matrix M which approximates A 1 directly. For example we may seek a sparse matrix M to minimize the Frobenius norm of I AM . Techniques of this type have received much attention in recent years <ref> [8, 21, 11, 22, 27, 26, 10, 9] </ref>. 14 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS One of the reasons for this interest is parallelism. Another is the common failure of ILU-type preconditioners for indefinite problems.
Reference: [23] <author> K. C. Jea and D. M. Young. </author> <title> Generalized conjugate gradient acceleration of nonsymmetrizable iterative methods. </title> <journal> Lin. Alg. and its Appl., </journal> <volume> 34 </volume> <pages> 159-194, </pages> <year> 1980. </year>
Reference-contexts: The conjugate gradient method is a particular instance of this method when the matrix is symmetric positive definite. Another method in this class is the Full Orthogonalization Method (FOM) [36] which is closely related to Arnoldi's method for solving eigenvalue problems [3]. ORTHORES <ref> [23] </ref> developed by Jea and Young is mathematically equivalent to FOM as is another method derived by Axelsson [5]. By definition, the final residual vector is orthogonal to all of the Krylov subspace. Thus, these methods are sometimes termed orthogonal residual methods. <p> Methods in this category are often termed Minimal Error methods. Quite a few methods have been developed in this class <ref> [6, 23, 14, 43] </ref>. Finally, among oblique projection methods, are the methods based on bi-conjugacy and the Lanczos biorthogonalization method. In these methods, L m = K m (A T ; r 0 ) and K m = K m (A; r 0 ).
Reference: [24] <author> O. G. Johnson, C. A. Micchelli, and G. Paul. </author> <title> Polynomial preconditionings for conjugate gradient calculations. </title> <journal> SIAM J. on Numer. Anal., </journal> <volume> 20 </volume> <pages> 362-376, </pages> <year> 1983. </year>
Reference-contexts: We refer to these preconditioners as "data-parallel" only to emphasize the fact that they are characterized by a high degree of fine-grain parallelism. Among the simplest techniques used in this context are polynomial precon-ditioners; see, e.g., <ref> [37, 24, 19, 13] </ref>. Another strategy which has been exploited is that of multicoloring and independent set orderings. These are discussed next.
Reference: [25] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> On a family of two-level precon-ditionings of the incomplete block factorization type. </title> <journal> Soviet Journal of Numerical Analysis and Mathematical Modeling, </journal> <volume> 1 </volume> <pages> 293-320, </pages> <year> 1986. </year>
Reference-contexts: More important is the fact that the preconditioning operation is now purely a matrix-vector product. One of the most promising uses of approximate inverse techniques is to combine them with other, e.g. block, factorizations. A number of techniques based on this viewpoint were developed in <ref> [25] </ref> and, more recently, in [9].
Reference: [26] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> Factorized sparse approximate inverse preconditionings II. solution of 3d fe systems on massively parallel computers. </title> <type> Technical Report EM-RR 3/92, </type> <institution> Elegant Mathematics, Inc., </institution> <address> Bothell, Washington, </address> <year> 1992. </year>
Reference-contexts: An alternative is to attempt to obtain a preconditioning matrix M which approximates A 1 directly. For example we may seek a sparse matrix M to minimize the Frobenius norm of I AM . Techniques of this type have received much attention in recent years <ref> [8, 21, 11, 22, 27, 26, 10, 9] </ref>. 14 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS One of the reasons for this interest is parallelism. Another is the common failure of ILU-type preconditioners for indefinite problems.
Reference: [27] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> Factorized sparse approximate inverse preconditionings I. </title> <journal> theory. SIAM J. on Mat.rix Anal. and Appl., </journal> <volume> 14 </volume> <pages> 45-58, </pages> <year> 1993. </year>
Reference-contexts: An alternative is to attempt to obtain a preconditioning matrix M which approximates A 1 directly. For example we may seek a sparse matrix M to minimize the Frobenius norm of I AM . Techniques of this type have received much attention in recent years <ref> [8, 21, 11, 22, 27, 26, 10, 9] </ref>. 14 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS One of the reasons for this interest is parallelism. Another is the common failure of ILU-type preconditioners for indefinite problems.
Reference: [28] <author> V. Kumar, A. Grama, A. Gupta, and G. Kapyris. </author> <title> Parallel Computing. </title> <publisher> Benjamin Cummings, </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: First, it is typical that there is a substantial number of small level-sets. In the above example, level-sets start with a size of one then increase by one for each next level. These small levels can hamper performance considerably for massively parallel machines (a consequence of Amdahl's law <ref> [28] </ref>). The second difficulty is that the maximum degree of parallelism can be rather small. For rectangular 2-dimensional meshes it is of the order of the smaller of the number of mesh points in the x- and y-directions.
Reference: [29] <author> C. </author> <title> Lanczos. Solution of systems of linear equations by minimized iterations. </title> <journal> J. of Res. of the Nat. Bur. of Stand., </journal> <volume> 49 </volume> <pages> 33-53, </pages> <year> 1952. </year>
Reference-contexts: In these methods, L m = K m (A T ; r 0 ) and K m = K m (A; r 0 ). In the nonsymmetric case, the bi-conjugate gradient method (BCG) due to Lanczos <ref> [29] </ref> and Fletcher [16] is a good representative of this class. There are various mathematically equivalent formulations of the biconjugate gradient method some of which are more numerically viable than others.
Reference: [30] <author> P. LeTallec. </author> <title> Domain decomposition methods in computational mechanics. </title> <booktitle> Computational Mechanics Advances, </booktitle> <volume> 1(2) </volume> <pages> 121-220, </pages> <year> 1994. </year>
Reference-contexts: Preconditioners for Distributed Sparse Matrices We now turn our attention to preconditioning techniques for distributed sparse matrices. Many of the ideas used in preconditioning distributed sparse matrices are borrowed from Domain Decomposition literature; see for example <ref> [15, 30] </ref>. Among the simplest of these are the use of block preconditionings based on the domains. These are termed Schwarz alternating procedures in the PDE literature. Let R i be a restriction operator which projects a global vector onto its restriction in subdomain i.
Reference: [31] <author> J. Ortega. </author> <title> Orderings for conjugate gradient preconditionings. </title> <journal> SIAM J. on Sci. and Stat. Comput., </journal> <volume> 12 </volume> <pages> 565-582, </pages> <year> 1991. </year> <title> KRYLOV SUBSPACE METHODS 27 </title>
Reference-contexts: One single step of the multicolor SOR iteration can then be efficiently implemented by accessing these p submatrices in succession. One of the most promising approaches in the context of parallel precondi-tioners is to combine multiple-step SOR or SSOR preconditioning with multi-coloring <ref> [40, 12, 1, 31, 34, 32] </ref>. This combines the main advantages of multi-coloring, namely ample degree of parallelism, with the robustness afforded by multiple step techniques. A multiple-step relaxation consists simply of performing s steps of the relaxation scheme instead of one.
Reference: [32] <author> J. M. Ortega. </author> <title> Introduction to Parallel and Vector Solution of Linear Systems. </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: One single step of the multicolor SOR iteration can then be efficiently implemented by accessing these p submatrices in succession. One of the most promising approaches in the context of parallel precondi-tioners is to combine multiple-step SOR or SSOR preconditioning with multi-coloring <ref> [40, 12, 1, 31, 34, 32] </ref>. This combines the main advantages of multi-coloring, namely ample degree of parallelism, with the robustness afforded by multiple step techniques. A multiple-step relaxation consists simply of performing s steps of the relaxation scheme instead of one.
Reference: [33] <author> S. J. Polak, C. Den Heijer, W. H. A. Schilders, and P. Markowich. </author> <title> Semiconductor device modelling from the numerical point of view. </title> <journal> Internat. J. Numer. Meth. Eng., </journal> <volume> 24 </volume> <pages> 763-838, </pages> <year> 1987. </year>
Reference-contexts: The QMR algorithm [17] ignores the non-orthogonality of the Lanczos basis and attempts to minimize the 2-norm of the expression of the residual in this basis. An efficient variation on the BCG algorithm, called CGS (Conjugate gradient squared) and proposed by Sonneveld <ref> [46, 33] </ref>, avoids the use of the transpose of A. This gave rise to a whole class of efficient schemes named transpose-free techniques, two notable examples of which are the BiConjugate Gradient Stabilized (BICGSTAB) algorithm of van der Vorst [50], and the Transpose-Free QMR (TFQMR) algorithm of Freund [18].
Reference: [34] <author> E. L Poole and J. M. Ortega. </author> <title> Multicolor ICCG methods for vector computers. </title> <journal> SIAM J. on Numer. Anal., </journal> <volume> 24 </volume> <pages> 1394-1418, </pages> <year> 1987. </year>
Reference-contexts: One single step of the multicolor SOR iteration can then be efficiently implemented by accessing these p submatrices in succession. One of the most promising approaches in the context of parallel precondi-tioners is to combine multiple-step SOR or SSOR preconditioning with multi-coloring <ref> [40, 12, 1, 31, 34, 32] </ref>. This combines the main advantages of multi-coloring, namely ample degree of parallelism, with the robustness afforded by multiple step techniques. A multiple-step relaxation consists simply of performing s steps of the relaxation scheme instead of one.
Reference: [35] <author> G. Radicati di Brozolo and Y. Robert. </author> <title> Parallel conjugate gradient-like algorithms for solving sparse non-symmetric systems on a vector multiprocessor. </title> <journal> Parallel Computing, </journal> <volume> 11 </volume> <pages> 223-239, </pages> <year> 1989. </year>
Reference-contexts: In each domain a direct or iterative solver must be used. In addition, the subdomain partitions may be allowed to overlap. This technique works reasonably well for a small number of overlapping subdomains as was shown in experiments using a purely algebraic form in <ref> [35] </ref>. This can be extended to block Gauss-Seidel or Symmetric Gauss-Seidel techniques in which, likewise, a block is associated with a domain. In the PDE framework this is referred to as a multiplicative Schwarz procedure.
Reference: [36] <author> Y. Saad. </author> <title> Krylov subspace methods for solving large unsymmetric linear systems. </title> <journal> Mathematics of Computation, </journal> <volume> 37 </volume> <pages> 105-126, </pages> <year> 1981. </year>
Reference-contexts: The conjugate gradient method is a particular instance of this method when the matrix is symmetric positive definite. Another method in this class is the Full Orthogonalization Method (FOM) <ref> [36] </ref> which is closely related to Arnoldi's method for solving eigenvalue problems [3]. ORTHORES [23] developed by Jea and Young is mathematically equivalent to FOM as is another method derived by Axelsson [5]. By definition, the final residual vector is orthogonal to all of the Krylov subspace.
Reference: [37] <author> Y. Saad. </author> <title> Practical use of polynomial preconditionings for the conjugate gradient method. </title> <journal> SIAM J. on Sci. and Stat. Comput., </journal> <volume> 6 </volume> <pages> 865-881, </pages> <year> 1985. </year>
Reference-contexts: We refer to these preconditioners as "data-parallel" only to emphasize the fact that they are characterized by a high degree of fine-grain parallelism. Among the simplest techniques used in this context are polynomial precon-ditioners; see, e.g., <ref> [37, 24, 19, 13] </ref>. Another strategy which has been exploited is that of multicoloring and independent set orderings. These are discussed next.
Reference: [38] <author> Y. Saad. </author> <title> Krylov subspace methods on supercomputers. </title> <journal> SIAM J. on Sci. and Stat. Comput., </journal> <volume> 10 </volume> <pages> 1200-1232, </pages> <year> 1989. </year>
Reference-contexts: The preconditioned residual vector M 1 (b Ax i ) obtained at step i is M-orthogonal to all the previous preconditioned residual vectors. So far there has been just a few general principles used to develop parallel preconditioners. The first idea is to exploit `wavefronts' or `level-scheduling' <ref> [2, 38, 45, 52, 48, 49, 20, 44, 4] </ref> for solving the triangular systems that arise in many preconditioners. These are essentially parallel implementations of the forward and backward triangular solves and can always be applied to any ILU-type preconditioner, without changing the preconditioner itself.
Reference: [39] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <journal> SIAM J. on Sci. and Stat. Comput., </journal> <volume> 14 </volume> <pages> 461-469, </pages> <year> 1993. </year>
Reference-contexts: A multiple-step relaxation consists simply of performing s steps of the relaxation scheme instead of one. In fact, the number of steps can vary arbitrarily if a "flexible variant" of an accelerator is used. These variants allow a right-preconditioner to be essentially arbitrary; see e.g., <ref> [39, 7, 51] </ref>. In [40] it was shown by means of experiments that on vector computers, one-step relaxation was often far from being the optimal number of steps to use when preconditioning a linear system.
Reference: [40] <author> Y. Saad. </author> <title> Highly parallel preconditioners for general sparse matrices. </title> <editor> In G. Golub, M. Luskin, and A. Greenbaum, editors, </editor> <title> Recent Advances in Iterative Methods, </title> <journal> IMA Volumes in Mathematics and Its Applications, </journal> <volume> volume 60, </volume> <pages> pages 165-199, </pages> <address> New York, 1994. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: One single step of the multicolor SOR iteration can then be efficiently implemented by accessing these p submatrices in succession. One of the most promising approaches in the context of parallel precondi-tioners is to combine multiple-step SOR or SSOR preconditioning with multi-coloring <ref> [40, 12, 1, 31, 34, 32] </ref>. This combines the main advantages of multi-coloring, namely ample degree of parallelism, with the robustness afforded by multiple step techniques. A multiple-step relaxation consists simply of performing s steps of the relaxation scheme instead of one. <p> A multiple-step relaxation consists simply of performing s steps of the relaxation scheme instead of one. In fact, the number of steps can vary arbitrarily if a "flexible variant" of an accelerator is used. These variants allow a right-preconditioner to be essentially arbitrary; see e.g., [39, 7, 51]. In <ref> [40] </ref> it was shown by means of experiments that on vector computers, one-step relaxation was often far from being the optimal number of steps to use when preconditioning a linear system.
Reference: [41] <author> Y. Saad. </author> <title> Iterative Methods for Sparse Linear Systems. </title> <publisher> PWS publishing, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: This property does not extend to nonsymmetric problems. 4 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS In the second class of methods, L m is taken to be equal to AK m where K m = K m (A; r 0 ). It can be shown, see e.g., <ref> [41] </ref> that in this case the approximate solution x m minimizes the residual norm kb Axk 2 over all candidate vectors in x 0 + K m . Methods in this category are often termed Minimal Error methods. <p> This can be easily understood by considering the reordered matrix on the right-side of Figure 1.2. 8 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS The idea of wavefronts can easily be extended to general sparse matrices. Here, an inexpensive preprocessing is required to obtain the levels; for details see <ref> [2, 41] </ref>. Performance can be more than adequate on vector processors, by exploiting suitable storage formats and efficient matrix-vector multiplications. For typical problems a gain of a factor of 3 to 5 is easily achievable when compared with a standard implementation. However, this approach has two drawbacks. <p> It can also save storage, a more critical issue in some cases. We now discuss the case of a general sparse matrix A. There are a number of inexpensive techniques to color an arbitrary graph, see e.g. <ref> [41] </ref>. The simplest heuristic used is to traverse the graph in any fashion and for each node visited assign the smallest color number allowable. By allowable, we mean a color number that is not already assigned to any of the vertices that are adjacent to the node being visited. <p> The unknowns associated with the diagonal block D constitute the independent set. This is a less restrictive form of multicoloring discussed earlier. Inexpensive algorithms for finding independent set orderings are easy to develop and are similar to multicoloring algorithms <ref> [41] </ref>. When the rows associated with an independent set are eliminated, we obtain a smaller linear system which is again sparse. Then we can find an independent set for this reduced system and repeat the process of reduction. The process can be repeated recursively a few times.
Reference: [42] <author> Y. Saad and A. Malevsky. PSPARSLIB: </author> <title> A portable library of distributed memory sparse iterative solvers. </title> <editor> In V. E. Malyshkin et al., editor, </editor> <booktitle> Proceedings of Parallel Computing Technologies (PaCT-95), 3-rd international conference, </booktitle> <address> St. Petersburg, </address> <month> Sept. </month> <year> 1995, 1995. </year>
Reference-contexts: Also recall that steps 1 and 2 are independent and can overlap. An example of the implementation of this operation as it is in PSPARSLIB <ref> [42] </ref>, is illustrated next: call MSG bdx send (nloc,x,y,nproc,proc,ix,ipr,ptrn,ierr) c do local matrix-vector product for local points call amux (nloc,x,y,aloc,jaloc,ialoc) KRYLOV SUBSPACE METHODS 19 c receive the boundary information call MSG bdx receive (nloc,x,y,nproc,proc,ix,ipr,ptrn,ierr) c do local matrix-vector product for external points nrow = nloc - nbnd + 1 call amux1
Reference: [43] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. on Sci. and Stat. Comput., </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year> <title> 28 1. KRYLOV SUBSPACE METHODS ON PARALLEL COMPUTERS </title>
Reference-contexts: Methods in this category are often termed Minimal Error methods. Quite a few methods have been developed in this class <ref> [6, 23, 14, 43] </ref>. Finally, among oblique projection methods, are the methods based on bi-conjugacy and the Lanczos biorthogonalization method. In these methods, L m = K m (A T ; r 0 ) and K m = K m (A; r 0 ).
Reference: [44] <author> Y. Saad and M. H. Schultz. </author> <title> Parallel implementations of preconditioned conjugate gradient methods. </title> <editor> In W. E. Fitzgibbon, editor, </editor> <title> Mathematical and Computational Methods in Seismic Exploration and Reservoir Modeling, </title> <address> Philadelphia, PA, </address> <year> 1986. </year> <note> SIAM. </note>
Reference-contexts: The preconditioned residual vector M 1 (b Ax i ) obtained at step i is M-orthogonal to all the previous preconditioned residual vectors. So far there has been just a few general principles used to develop parallel preconditioners. The first idea is to exploit `wavefronts' or `level-scheduling' <ref> [2, 38, 45, 52, 48, 49, 20, 44, 4] </ref> for solving the triangular systems that arise in many preconditioners. These are essentially parallel implementations of the forward and backward triangular solves and can always be applied to any ILU-type preconditioner, without changing the preconditioner itself.
Reference: [45] <author> J. H. Saltz. </author> <title> Automated problem scheduling and reduction of synchronization delay effects. </title> <type> Technical Report 87-22, </type> <institution> ICASE, Hampton, VA, </institution> <year> 1987. </year>
Reference-contexts: The preconditioned residual vector M 1 (b Ax i ) obtained at step i is M-orthogonal to all the previous preconditioned residual vectors. So far there has been just a few general principles used to develop parallel preconditioners. The first idea is to exploit `wavefronts' or `level-scheduling' <ref> [2, 38, 45, 52, 48, 49, 20, 44, 4] </ref> for solving the triangular systems that arise in many preconditioners. These are essentially parallel implementations of the forward and backward triangular solves and can always be applied to any ILU-type preconditioner, without changing the preconditioner itself.
Reference: [46] <author> P. Sonneveld. </author> <title> CGS, a fast Lanczos-type solver for nonsymmetric linear systems. </title> <journal> SIAM J. on Sci. and Stat. Comput., </journal> <volume> 10(1) </volume> <pages> 36-52, </pages> <year> 1989. </year>
Reference-contexts: The QMR algorithm [17] ignores the non-orthogonality of the Lanczos basis and attempts to minimize the 2-norm of the expression of the residual in this basis. An efficient variation on the BCG algorithm, called CGS (Conjugate gradient squared) and proposed by Sonneveld <ref> [46, 33] </ref>, avoids the use of the transpose of A. This gave rise to a whole class of efficient schemes named transpose-free techniques, two notable examples of which are the BiConjugate Gradient Stabilized (BICGSTAB) algorithm of van der Vorst [50], and the Transpose-Free QMR (TFQMR) algorithm of Freund [18].
Reference: [47] <author> H. A. van der Vorst. </author> <title> A vectorizable version of some ICCG methods. </title> <journal> SIAM J. on Sci. and Stat. Comput., </journal> <volume> 3 </volume> <pages> 350-356, </pages> <year> 1982. </year>
Reference-contexts: For example, the inverse of a sparse triangular matrix can be approximated using a simple polynomial expansion. Solutions of bidiagonal systems which are required in the forward-backward solutions for structured matrices are approximated similarly <ref> [47] </ref>. At one extreme, the inverse of A can also be approximated by a polynomial in A. Multiplying this polynomial of A by an arbitrary vector can be done using a sequence of matrix-vector products. In "data-parallel preconditioners," a maximum degree of parallelism is sought.
Reference: [48] <author> H. A. van der Vorst. </author> <title> The performance of FORTRAN implementations for preconditioned conjugate gradient methods on vector computers. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 49-58, </pages> <year> 1986. </year>
Reference-contexts: The preconditioned residual vector M 1 (b Ax i ) obtained at step i is M-orthogonal to all the previous preconditioned residual vectors. So far there has been just a few general principles used to develop parallel preconditioners. The first idea is to exploit `wavefronts' or `level-scheduling' <ref> [2, 38, 45, 52, 48, 49, 20, 44, 4] </ref> for solving the triangular systems that arise in many preconditioners. These are essentially parallel implementations of the forward and backward triangular solves and can always be applied to any ILU-type preconditioner, without changing the preconditioner itself.
Reference: [49] <author> H. A. van der Vorst. </author> <title> Large tridiagonal and block tridiagonal linear systems on vector and parallel computers. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 303-311, </pages> <year> 1987. </year>
Reference-contexts: The preconditioned residual vector M 1 (b Ax i ) obtained at step i is M-orthogonal to all the previous preconditioned residual vectors. So far there has been just a few general principles used to develop parallel preconditioners. The first idea is to exploit `wavefronts' or `level-scheduling' <ref> [2, 38, 45, 52, 48, 49, 20, 44, 4] </ref> for solving the triangular systems that arise in many preconditioners. These are essentially parallel implementations of the forward and backward triangular solves and can always be applied to any ILU-type preconditioner, without changing the preconditioner itself.
Reference: [50] <author> H. A. van der Vorst. </author> <title> Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of non-symmetric linear systems. </title> <journal> SIAM J. on Sci. and Stat. Comput., </journal> <volume> 12 </volume> <pages> 631-644, </pages> <year> 1992. </year>
Reference-contexts: This gave rise to a whole class of efficient schemes named transpose-free techniques, two notable examples of which are the BiConjugate Gradient Stabilized (BICGSTAB) algorithm of van der Vorst <ref> [50] </ref>, and the Transpose-Free QMR (TFQMR) algorithm of Freund [18]. The Krylov subspace methods mentioned above are often referred to as accelerators. More important than the accelerator for the success of a Krylov subspace method is the preconditioning technique used.
Reference: [51] <author> H. A. van der Vorst and C. Vuik. GMRESR: </author> <title> a family of nested GMRES methods. Numerical Linear Algebra with Applications, </title> <booktitle> 1 </booktitle> <pages> 369-386, </pages> <year> 1994. </year>
Reference-contexts: A multiple-step relaxation consists simply of performing s steps of the relaxation scheme instead of one. In fact, the number of steps can vary arbitrarily if a "flexible variant" of an accelerator is used. These variants allow a right-preconditioner to be essentially arbitrary; see e.g., <ref> [39, 7, 51] </ref>. In [40] it was shown by means of experiments that on vector computers, one-step relaxation was often far from being the optimal number of steps to use when preconditioning a linear system.
Reference: [52] <author> O. Wing and J. W. Huang. </author> <title> A computation model of parallel solution of linear equations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29:632-638, </volume> <year> 1980. </year>
Reference-contexts: The preconditioned residual vector M 1 (b Ax i ) obtained at step i is M-orthogonal to all the previous preconditioned residual vectors. So far there has been just a few general principles used to develop parallel preconditioners. The first idea is to exploit `wavefronts' or `level-scheduling' <ref> [2, 38, 45, 52, 48, 49, 20, 44, 4] </ref> for solving the triangular systems that arise in many preconditioners. These are essentially parallel implementations of the forward and backward triangular solves and can always be applied to any ILU-type preconditioner, without changing the preconditioner itself.
References-found: 52

