URL: http://www.cs.orst.edu:80/~tadepall/research/papers/theory-of-ebl.ps
Refering-URL: http://www.cs.orst.edu:80/~tadepall/research/publications.html
Root-URL: 
Email: (tadepalli@cs.orst.edu)  
Title: A Formalization of Explanation-Based Macro-operator Learning  
Author: Prasad Tadepalli 
Address: Corvallis, OR 97331-3202  
Affiliation: Department of Computer Science Oregon State University  
Abstract: In spite of the popularity of Explanation-Based Learning (EBL), its theoretical basis is not well-understood. Using a generalization of Probably Approximately Correct (PAC) learning to problem solving domains, this paper formalizes two forms of Explanation-Based Learning of macro-operators and proves the sufficient conditions for their success. These two forms of EBL, called "Macro Caching" and "Serial Parsing," respectively exhibit two distinct sources of power or "bias": the sparseness of the solution space and the decomposability of the problem-space. The analysis shows that exponential speedup can be achieved when either of these biases is suitable for a domain. Somewhat surprisingly, it also shows that computing the preconditions of the macro-operators is not necessary to obtain these speedups. The theoretical results are confirmed by experiments in the domain of Eight Puzzle. Our work suggests that the best way to address the utility problem in EBL is to implement a bias which exploits the problem-space structure of the set of domains that one is interested in learning.
Abstract-found: 1
Intro-found: 1
Reference: [ Angluin, 1988 ] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1988. </year>
Reference-contexts: The testing scheme, described in Table 1, has a slightly better bound on the number of test examples than that of <ref> [ Angluin, 1988 ] </ref> .
Reference: [ Chalasani et al., 1991 ] <author> P. Chalasani, O. Etzioni, and J. Mount. </author> <title> Detecting and Exploiting Decomposability in Update Graphs. </title> <booktitle> In 2nd Int'l Conference on Principles of Knowledge Representation and Reasoning, KR'91. </booktitle>
Reference-contexts: The basic idea here is to learn the macro table column by column, using multiple examples to disambiguate the feature that corresponds to a given column. <ref> [ Chalasani et al., 1991 ] </ref> describes algorithms that detect serial and to tal decomposability by experimentation. One consequence of our formalization is that it blurs the distinction between EBL and empirical learning.
Reference: [ Cohen, 1989 ] <author> W. Cohen. </author> <title> Solution path caching mechanisms which provably improve performance. </title> <type> Technical Report DCS-TR-254, </type> <institution> Rutgers University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: For example, <ref> [ Cohen, 1989 ] </ref> analyzes a "Solution Path Caching" mechanism and shows that organizing the solution sequences of the examples in a tree and restricting the search of the problem solver to this tree improves the performance of the problem solver with a high probability.
Reference: [ Dejong and Mooney, 1986 ] <author> G. Dejong and R. Mooney. </author> <title> Explanation-based learning: A differentiating view. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1986. </year>
Reference-contexts: 1 Introduction Explanation-Based Learning (EBL) treats learning as improving the efficiency of a problem solver <ref> [ Dejong and Mooney, 1986, Mitchell et al., 1986 ] </ref> . The standard EBL systems start with complete and correct, if inefficient, problem solvers. Learning involves taking a set of examples, i.e., problem-solution pairs, as input and producing an efficient problem solver as output.
Reference: [ Etzioni, 1990 ] <author> O. Etzioni. </author> <title> Why prodigy/ebl works. </title> <booktitle> In Proceedings of AAAI-90, </booktitle> <year> 1990. </year>
Reference-contexts: While our result on Macro Caching is consistent with their conclusion, we also show that the structure of the problem-space can be exploited to make learning profitable in other domains. Serial decomposability is just one example of a problem-space structure. <ref> [ Etzioni, 1990 ] </ref> describes another kind of problem-space structure called "non-recursive explanations" which explains the successful performance of PRODIGY in a number of domains. When this structure is present, the size of the explanation of a control heuristic is independent of the solution length. <p> When this structure is present, the size of the explanation of a control heuristic is independent of the solution length. Somewhat surprisingly, Etzioni also found that examples were not the key reason for PRODIGY's successful performance <ref> [ Etzioni, 1990 ] </ref> . Etzioni's program STATIC matches PRODIGY's performance by statically analyzing the problem-space without using any examples. Thus, even though [ Etzioni, 1990 ] explains why PRODIGY works, it fails to explain the role of examples in EBL-type systems in distribution-independent learning. <p> Somewhat surprisingly, Etzioni also found that examples were not the key reason for PRODIGY's successful performance <ref> [ Etzioni, 1990 ] </ref> . Etzioni's program STATIC matches PRODIGY's performance by statically analyzing the problem-space without using any examples. Thus, even though [ Etzioni, 1990 ] explains why PRODIGY works, it fails to explain the role of examples in EBL-type systems in distribution-independent learning. This paper shows that EBL can gain from the problem-space structure as well as examples.
Reference: [ Greiner and Likuski, 1989 ] <author> R. Greiner, and J. Likuski. </author> <title> Incorporating Redundant Learned Rules: A Preliminary Formal Analysis of EBL. </title> <booktitle> In Proceedings of IJCAI-89. </booktitle>
Reference-contexts: Hence, Solution Path Caching (like Macro Caching but unlike Serial Parsing) fails to learn (by our definition) in domains like Eight Puzzle. Greiner and Likuski formalize EBL as adding redundant learned rules to a horn-clause knowledge base to hasten query-processing <ref> [ Greiner and Likuski, 1989 ] </ref> . This model is extended to recursive domain theories in [ Subramanian and Feldman, 1990 ] . They conclude that learning macro-rules in EBL in such domains is not profitable in general unless strong assumptions are made about the problem distribution.
Reference: [ Haussler, 1990 ] <author> D. Haussler. </author> <title> Probably Approximately Correct Learning. </title> <booktitle> In Proceedings of AAAI-90, </booktitle> <address> Boston, MA, </address> <month> August, </month> <year> 1990. </year>
Reference: [ Korf, 1985 ] <author> R. Korf. Macro-operators: </author> <title> A weak method for learning. </title> <journal> Artificial Intelligence, </journal> <volume> 26, </volume> <year> 1985. </year>
Reference-contexts: The theoretical predictions of our analysis are confirmed by an implementation of Serial Parsing in the domain of Eight Puzzle. The main contribution of this paper is a successful integration of EBL work with PAC learning. It also extends Korf's work on macro-operator learning <ref> [ Korf, 1985 ] </ref> to incremental learning by observation. Our work also suggests that the utility problem in EBL can be solved by building learning programs that implement biases which exploit the domain structure. The rest of the paper is organized as follows: Section 2 introduces PAC Problem Solving. <p> it does not work in domains like Rubik's Cube and Eight Puzzle, where each macro can solve at most one problem, and the problem distribution is not sparse. 4 Serial Parsing This section describes how EBL can succeed for arbitrary problem distributions by exploiting a problem-space structure called serial decomposability <ref> [ Korf, 1985 ] </ref> . Here we make the assumption that states are representable as vectors of discrete valued features, hv 1 ; :::; v n i, where the maximum number of values a feature can take is bounded by a polynomial in n. <p> A domain is serially decomposable for a given total ordering on the set of features if the effect of any operator in the domain on a feature value is a function of the values of only that feature and all the features that precede it <ref> [ Korf, 1985 ] </ref> . Rubik's Cube is serially decomposable for any ordering of features (also called "totally decomposable"). In Eight Puzzle, the effect of an operator on any tile depends only on the positions of that tile and the blank in the original state. <p> Korf showed that if a domain is serially decomposable and every state reachable from a solvable state is also solvable, then it has a macro table <ref> [ Korf, 1985 ] </ref> . <p> If a full macro table with appropriately ordered fea tures is given, then it can be used to construct solutions from any initial state without any backtracking search <ref> [ Korf, 1985 ] </ref> . <p> Korf's learning program builds a macro table by exhaustively searching for a correct entry for each cell in the table <ref> [ Korf, 1985 ] </ref> . Thus, Korf's work might be characterized as "learning by doing." Our method, called Serial Parsing, extends his work to incremental learning of macro tables by observation.
Reference: [ Laird et al., 1986 ] <author> J. E. Laird, P.S. Rosenbloom, and A. Newell. </author> <title> Chunking in Soar: The Anatomy of a General Learning Mechanism. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <year> 1986. </year>
Reference-contexts: Serial Parsing is given the order in which subgoals are achieved. In systems like SOAR that successfully learn macros using EBL, the goal ordering is implicitly given by defining the subgoals such that they include one another <ref> [ Laird et al., 1986 ] </ref> . In [ Tadepalli, 1991 ] , a method called Batch Parsing is described which learns the subgoal ordering along with macros for the subgoals.
Reference: [ Minton, 1990 ] <author> S. Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 42, </volume> <year> 1990. </year>
Reference-contexts: Secondly, it is known that EBL systems do not always lead to performance improvement with learning. After learning a large number of rules, they might face what is called the "utility problem," i.e., the problem of complexity of using the learned knowledge <ref> [ Minton, 1990 ] </ref> . Since the utility problem can far outweigh the reduction in search due to learning, a successful formalization should take this into account and characterize the sufficient conditions for a guaranteed performance improvement.
Reference: [ Mitchell et al., 1986 ] <author> T. Mitchell, R. Keller, and S. Kedar-Cabelli. </author> <title> Explanation based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <year> 1986. </year>
Reference-contexts: 1 Introduction Explanation-Based Learning (EBL) treats learning as improving the efficiency of a problem solver <ref> [ Dejong and Mooney, 1986, Mitchell et al., 1986 ] </ref> . The standard EBL systems start with complete and correct, if inefficient, problem solvers. Learning involves taking a set of examples, i.e., problem-solution pairs, as input and producing an efficient problem solver as output. <p> Section 6 discusses the related work, and the final section summarizes the contributions. 2 Probably Approximately Correct Problem Solving In order to formally analyze EBL, we need to precisely define what it means for EBL, or any other speedup learning method, to succeed. The problem definition in <ref> [ Mitchell et al., 1986 ] </ref> is not sufficient for our purposes, because it does not address the issue of learning from multiple examples. This section presents a definition which draws from the earlier formal frameworks presented in [ Natarajan and Tadepalli, 1988 ] . <p> However, this domain theory is too inefficient to use directly to solve problems. The operationality constraint of EBL <ref> [ Mitchell et al., 1986 ] </ref> may be viewed as defining a hypothesis space of potential efficient problem solvers, one of which is the target problem solver.
Reference: [ Natarajan and Tadepalli, 1988 ] <author> B. Natarajan and P. Tadepalli. </author> <title> Two new frameworks for learning. </title> <booktitle> In Proceedings of Machine Learning Conference, </booktitle> <address> Ann Arbor, MI, </address> <year> 1988. </year>
Reference-contexts: The problem definition in [ Mitchell et al., 1986 ] is not sufficient for our purposes, because it does not address the issue of learning from multiple examples. This section presents a definition which draws from the earlier formal frameworks presented in <ref> [ Natarajan and Tadepalli, 1988 ] </ref> . The key difference between purely empirical learning and EBL is that an EBL system is also provided with a "domain theory," which, we assume, is in the form of a set of goals and operators. <p> This framework is very similar to that introduced in <ref> [ Natarajan and Tadepalli, 1988 ] </ref> . The main difference is the idea of the hypothesis space which is not present in [ Natarajan and Tadepalli, 1988 ] . Another difference is that it uses a less powerful SOLVED-PROBLEM oracle than the USEFUL oracle of [ Natarajan and Tadepalli, 1988 ] <p> This framework is very similar to that introduced in <ref> [ Natarajan and Tadepalli, 1988 ] </ref> . The main difference is the idea of the hypothesis space which is not present in [ Natarajan and Tadepalli, 1988 ] . Another difference is that it uses a less powerful SOLVED-PROBLEM oracle than the USEFUL oracle of [ Natarajan and Tadepalli, 1988 ] which is capable of generating optimal solutions for any problem. <p> similar to that introduced in <ref> [ Natarajan and Tadepalli, 1988 ] </ref> . The main difference is the idea of the hypothesis space which is not present in [ Natarajan and Tadepalli, 1988 ] . Another difference is that it uses a less powerful SOLVED-PROBLEM oracle than the USEFUL oracle of [ Natarajan and Tadepalli, 1988 ] which is capable of generating optimal solutions for any problem. We also insist that SOLVED-PROBLEM must solve all problems using the same target problem solver chosen from the hypothesis space.
Reference: [ Natarajan, 1989 ] <author> B. Natarajan. </author> <title> On learning sets and functions. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <year> 1989. </year>
Reference: [ Ratner and Warmuth, 1986 ] <author> D. Ratner, and M. War-muth. </author> <title> Finding a shortest solution for the N X N extension of the 15-PUZZLE is intractable. </title> <booktitle> In Proceedings of AAAI-86, </booktitle> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference-contexts: In fact, this assumption is most likely violated if the teacher gives only optimal solutions, because finding optimal solutions for the N fi N generalization of Eight Puzzle is intractable <ref> [ Ratner and Warmuth, 1986 ] </ref> . Hence the macros learned from optimal solutions do not fit into macro tables.
Reference: [ Subramanian and Feldman, 1990 ] <author> D. Subramanian and R. Feldman. </author> <title> The utility of ebl in recursive domain theories. </title> <booktitle> In Proceedings of AAAI-90, </booktitle> <address> Boston, MA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Greiner and Likuski formalize EBL as adding redundant learned rules to a horn-clause knowledge base to hasten query-processing [ Greiner and Likuski, 1989 ] . This model is extended to recursive domain theories in <ref> [ Subramanian and Feldman, 1990 ] </ref> . They conclude that learning macro-rules in EBL in such domains is not profitable in general unless strong assumptions are made about the problem distribution.
Reference: [ Tadepalli, 1990 ] <author> P. Tadepalli. </author> <title> Tractable learning and planning in games. </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Rutgers University, </institution> <year> 1990. </year>
Reference-contexts: Since, as our later definitions show, our formalization ignores all polynomial factors in sample size, learning time, and problem solving time, our results do not change whether the operators are parameterized or not. A parameterized model was considered and similar results were proved in <ref> [ Tadepalli, 1990 ] </ref> . For simplicity of exposition, this paper considers only non-parameterized operators. A problem solver f for D is a deterministic program that takes as input a problem, hs, gi, and computes its solution sequence, if such exists.
Reference: [ Tadepalli, 1991 ] <author> P. Tadepalli. </author> <title> Learning with Inscrutable Theories. </title> <editor> In Birnbaum, L. and Collins, G., (eds.) </editor> <booktitle> Machine Learning: Proceedings of International Machine Learning Workshop, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan kaufmann. </publisher>
Reference-contexts: Serial Parsing is given the order in which subgoals are achieved. In systems like SOAR that successfully learn macros using EBL, the goal ordering is implicitly given by defining the subgoals such that they include one another [ Laird et al., 1986 ] . In <ref> [ Tadepalli, 1991 ] </ref> , a method called Batch Parsing is described which learns the subgoal ordering along with macros for the subgoals.
Reference: [ Valiant, 1984 ] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 11(27), </volume> <month> August </month> <year> 1984. </year>
Reference-contexts: The examples provide information about the problem distribution and eliminate the need to search for solutions. While there are satisfactory formal models of empirical learning based on variants of Probably Approximately Correct (PAC) learning <ref> [ Valiant, 1984, Haus-sler, 1990 ] </ref> , EBL systems are not theoretically well-understood. There are some practical problems in EBL which can be directly attributed to the lack of adequate theoretical understanding. Firstly, there is no clear definition of "success" for EBL systems.
References-found: 18

