URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1997/UM-CS-1997-005.ps
Refering-URL: http://www.cs.umass.edu/~dprecup/publications.html
Root-URL: 
Email: Net: utgoff@cs.umass.edu  
Phone: Telephone: (413) 545-4843  
Title: Classification Using -Machines And Constructive Function Approximation  
Author: Doina Precup Paul E. Utgoff 
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Technical Report 97-05 January 21, 1997 Abstract The new classification algorithm CLEF combines a version of a linear machine known as a -machine with a non-linear function approximator that constructs its own features. The algorithm finds non-linear decision boundaries by constructing features that are needed to learn the necessary discriminant functions. The CLEF algorithm is proven to separate all consistently labelled training instances, even when they are not linearly separable in the input variables. The algorithm is illustrated on a variety of tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cohen, P. R., & Jensen, D. </author> <year> (1997). </year> <title> Overfitting explained. </title> <booktitle> Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics. </booktitle>
Reference-contexts: This repeated partitioning is problematic because the number of instances present in each block is reduced after each partition. Moreover, the way in which the tests are picked also leads to a problem related to the statistical issue of multiple comparisons <ref> (Cohen & Jensen, 1997) </ref>. Feed forward neural networks have also been used for classification. However, networks are usually difficult to tune, since their behavior depends on several parameters (Mooney, Shavlik, Towell & Gove, 1989).
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> Wiley & Sons. </publisher>
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> Small nets and short paths: Optimising neural computation. </title> <type> Doctoral dissertation, </type> <institution> Center for Cognitive Science, University of Edin-burgh. </institution>
Reference-contexts: If no linear separation can be found given the current feature set, by gradually reducing the size of the corrections, the weights will still settle in a particular range <ref> (Frean, 1990) </ref>. In this case, a new feature will be added, and training will resume with a new machine. In the worst case, the process will continue until all the 2 n features that are possible have been generated.
Reference: <author> Mooney, R., Shavlik, J., Towell, G., & Gove, A. </author> <year> (1989). </year> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 775-780). </pages> <address> Detroit, Michigan: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Feed forward neural networks have also been used for classification. However, networks are usually difficult to tune, since their behavior depends on several parameters <ref> (Mooney, Shavlik, Towell & Gove, 1989) </ref>. Backpropagation, (Rumelhart, Hinton & Williams, 1986) which is the best known algorithm for training feed forward multilayered neural networks, is not guaranteed to converge to a global minimum of the error surface.
Reference: <author> Murphy, P. M., & Aha, D. W. </author> <year> (1994). </year> <title> UCI repository of machine learning databases, </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference-contexts: Will it find a separating -machine in a reasonable amount of time? Will it construct a large number of features, perhaps producing an incomprehensible classifier? In order to answer some of these questions empirically, CLEF and C4.5 were run on twenty classification tasks, mostly from the UCI data repository <ref> (Murphy & Aha, 1994) </ref>. This allows for a comparison in terms of classification accuracy. In addition, several measurements of interrest were taken on CLEF during these runs. All values are computed from a ten-fold stratified cross-validation, with CLEF and C4.5 using identical folds for each task.
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <title> Learning machines. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: A linear machine is a set of R linear discriminant functions g i used collectively to assign an instance to one of R classes <ref> (Nilsson, 1965) </ref>. Let x = (1; x 1 ; ::x n ) be an instance description. Each discriminant function g i (x) has the form w T i x, where w is an (n + 1)-dimensional vector of coefficients (weights). <p> The amount of correction c can be computed using the fractional error correction rule <ref> (Nilsson, 1965) </ref>: c = ff 2x T x If the training instances are linearly separable, this update rule guarantees that the linear machine will converge to a boundary that classifies them correctly (Nilsson, 1965). <p> The amount of correction c can be computed using the fractional error correction rule <ref> (Nilsson, 1965) </ref>: c = ff 2x T x If the training instances are linearly separable, this update rule guarantees that the linear machine will converge to a boundary that classifies them correctly (Nilsson, 1965). For many tasks, linear combinations of the input values are not enough to discriminate the groups of instances belonging to each class. When a nonlinear discriminant is needed, a possible solution is to use a -machine (Nilsson, 1965), which is much like a linear machine. <p> the linear machine will converge to a boundary that classifies them correctly <ref> (Nilsson, 1965) </ref>. For many tasks, linear combinations of the input values are not enough to discriminate the groups of instances belonging to each class. When a nonlinear discriminant is needed, a possible solution is to use a -machine (Nilsson, 1965), which is much like a linear machine. <p> Thus, the initial approximation returns 0 for every instance. To update the approximation, the training procedure revisits the training instances and adjusts the weights of the discriminant functions using the fractional error correction rule <ref> (Nilsson, 1965) </ref>. Only features that matched the instance have their weights adjusted, because features that did not match have value 0. Of course, the approximation with the one most general feature is rarely adequate.
Reference: <author> Quinlan, J. R. </author> <year> (1985). </year> <title> Decision trees and multi-valued attributes. </title> <editor> In Michie (Ed.), </editor> <title> Machine Intelligence. Classification Using -Machines 12 Quinlan, </title> <editor> J. R. </editor> <year> (1993). </year> <title> C4.5: Programs for machine learning. </title> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R.J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart & McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Feed forward neural networks have also been used for classification. However, networks are usually difficult to tune, since their behavior depends on several parameters (Mooney, Shavlik, Towell & Gove, 1989). Backpropagation, <ref> (Rumelhart, Hinton & Williams, 1986) </ref> which is the best known algorithm for training feed forward multilayered neural networks, is not guaranteed to converge to a global minimum of the error surface. This paper presents a new approach to classification, that aims to eliminate the disadvantages of the methods mentioned above.
Reference: <author> Utgoff, P. E., & Precup, D. </author> <year> (1997). </year> <title> Constructive function approximation, </title> <type> (Technical Report 97-04), </type> <institution> Amherst, MA: University of Massachusetts, Department of Computer Science. </institution>
Reference-contexts: One would like to construct such functions f j automatically, based on the training instances. 3 Constructing a -machine for classification An automatic method for constructing a -machine adequate for the task at hand is needed. To this end, we employ the ELF function approximation algorithm <ref> (Utgoff & Precup, 1997) </ref>, which produces an approximation in the form of a -machine. Furthermore, ELF constructs new features as needed, Classification Using -Machines 3 by identifying subsets of instances that share intrinsic properties.
References-found: 9

