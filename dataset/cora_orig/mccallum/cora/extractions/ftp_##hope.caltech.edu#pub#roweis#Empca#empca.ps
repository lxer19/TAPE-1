URL: ftp://hope.caltech.edu/pub/roweis/Empca/empca.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00358.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: EM Algorithms for PCA and SPCA  
Author: Sam Roweis 
Abstract: I present an expectation-maximization (EM) algorithm for principal component analysis (PCA). The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data. It is computationally very efficient in space and time. It also naturally accommodates missing information. I also introduce a new variant of PCA called sensible principal component analysis (SPCA) which defines a proper density model in the data space. Learning for SPCA is also done with an EM algorithm. I report results on synthetic and real data showing that these EM algorithms correctly and efficiently find the leading eigenvectors of the covariance of datasets in a few iterations using up to hundreds of thousands of datapoints in thousands of dimensions.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society series B, </journal> <volume> 39:138, </volume> <year> 1977. </year>
Reference-contexts: Methods such as the snap-shot algorithm [7] do this by assuming that the eigenvectors being searched for are linear combinations of the datapoints; their complexity is O (n 3 ). In this note, I present a version of the expectation-maximization (EM) algorithm <ref> [1] </ref> for learning the principal components of a dataset. The algorithm does not require computing the sample covariance and has a complexity limited by O (knp) operations where k is the number of leading eigenvectors to be learned. <p> As expected, the EM algorithm scales more favourably in cases where k is small and both p and n are large. If k p n (we want all the eigenvectors) then all methods are O (p 3 ). The standard convergence proofs for EM <ref> [1] </ref> apply to this algorithm as well, so we can be sure that it will always reach a local maximum of likelihood.
Reference: [2] <author> B. S. Everitt. </author> <title> An Introducction to Latent Variable Models. </title> <publisher> Chapman and Hill, </publisher> <address> London, </address> <year> 1984. </year>
Reference-contexts: It can be easily derived as the zero noise limit of the standard algorithms (see for example <ref> [3, 2] </ref> and section 4 below) by replacing the usual e-step with the projection above.
Reference: [3] <author> Zoubin Ghahramani and Geoffrey Hinton. </author> <title> The EM algorithm for mixtures of factor analyzers. </title> <type> Technical Report CRG-TR-96-1, </type> <institution> Dept. of Computer Science, University of Toronto, </institution> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: It can be easily derived as the zero noise limit of the standard algorithms (see for example <ref> [3, 2] </ref> and section 4 below) by replacing the usual e-step with the projection above.
Reference: [4] <author> Zoubin Ghahramani and Michael I. Jordan. </author> <title> Supervised learning from incomplete data via an EM approach. </title> <editor> In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 120127. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Most of the methods discussed above cannot accommodate missing values and so incomplete points must either be discarded or completed using a variety of ad-hoc interpolation methods. On the other hand, the EM algorithm for PCA enjoys all the benefits <ref> [4] </ref> of other EM algorithms in terms of estimating the maximum likelihood values for missing information directly at each iteration.
Reference: [5] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, USA, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: If Lanc-zos or Arnoldi methods are used to compute this SVD, the resulting iterations are similar to those of the EM algorithm. Space prohibits detailed discussion of these sophisticated methods, but two excellent general references are <ref> [5, 6] </ref>. The second class of methods are the competitive learning methods for finding the principal subspace such as Sanger's and Oja's rules.
Reference: [6] <author> R. B. Lehoucq, D. C. Sorensen, and C. Yang. </author> <title> Arpack users' guide: Solution of large scale eigenvalue problems with implicitly restarted Arnoldi methods. </title> <note> Technical Report from http://www.caam.rice.edu/software/ARPACK/, Computational and Applied Mathematics, </note> <institution> Rice University, </institution> <month> October </month> <year> 1997. </year>
Reference-contexts: If Lanc-zos or Arnoldi methods are used to compute this SVD, the resulting iterations are similar to those of the EM algorithm. Space prohibits detailed discussion of these sophisticated methods, but two excellent general references are <ref> [5, 6] </ref>. The second class of methods are the competitive learning methods for finding the principal subspace such as Sanger's and Oja's rules.
Reference: [7] <author> L. Sirovich. </author> <title> Turbulence and the dynamics of coherent structures. </title> <journal> Quarterly Applied Mathematics, </journal> <volume> 45(3):561590, </volume> <year> 1987. </year>
Reference-contexts: Fortunately, several techniques exist for efficient matrix diagonalization when only the first few leading eigenvectors and eigenvalues are required (for example the power method [10] which is only O (p 2 )). covariance explicitly. Methods such as the snap-shot algorithm <ref> [7] </ref> do this by assuming that the eigenvectors being searched for are linear combinations of the datapoints; their complexity is O (n 3 ). In this note, I present a version of the expectation-maximization (EM) algorithm [1] for learning the principal components of a dataset.
Reference: [8] <author> Michael Tipping and Christopher Bishop. </author> <title> Mixtures of probabilistic principal component analyzers. </title> <type> Technical Report NCRG/97/003, </type> <institution> Neural Computing Research Group, Aston University, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: The standard convergence proofs for EM [1] apply to this algorithm as well, so we can be sure that it will always reach a local maximum of likelihood. Furthermore, Tipping and Bishop have shown <ref> [8, 9] </ref> that the only stable local extremum is the global maximum at which the true principal subspace is found; so it converges to the correct result. Another possible concern is that the number of iterations required for convergence may scale with p or n.
Reference: [9] <author> Michael Tipping and Christopher Bishop. </author> <title> Probabilistic principal component analysis. </title> <type> Technical Report NCRG/97/010, </type> <institution> Neural Computing Research Group, Aston University, </institution> <month> September </month> <year> 1997. </year>
Reference-contexts: The standard convergence proofs for EM [1] apply to this algorithm as well, so we can be sure that it will always reach a local maximum of likelihood. Furthermore, Tipping and Bishop have shown <ref> [8, 9] </ref> that the only stable local extremum is the global maximum at which the true principal subspace is found; so it converges to the correct result. Another possible concern is that the number of iterations required for convergence may scale with p or n.
Reference: [10] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Claredon Press, Oxford, </publisher> <address> England, </address> <year> 1965. </year>
Reference-contexts: Fortunately, several techniques exist for efficient matrix diagonalization when only the first few leading eigenvectors and eigenvalues are required (for example the power method <ref> [10] </ref> which is only O (p 2 )). covariance explicitly. Methods such as the snap-shot algorithm [7] do this by assuming that the eigenvectors being searched for are linear combinations of the datapoints; their complexity is O (n 3 ).
References-found: 10

