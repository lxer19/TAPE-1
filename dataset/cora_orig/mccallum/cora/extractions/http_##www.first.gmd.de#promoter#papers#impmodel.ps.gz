URL: http://www.first.gmd.de/promoter/papers/impmodel.ps.gz
Refering-URL: http://www.first.gmd.de/promoter/papers/index.html
Root-URL: 
Email: fbi,ens,mk,wilhelmi,samg@first.gmd.de  
Title: An Object-oriented Implementation Model for the PROMOTER Language  
Author: Hua Bi, Peter Enskonatus, Matthias Kessler, Matthias Wilhelmi, Michael Sander 
Keyword: Massively Parallel Programming Model, Data Parallelism, SPMD, Data Topology.  
Note: This work is supported by the Real World Computing Partnership (RWCP), Japan.  
Address: Rudower Chaussee 5, 12489 Berlin, Germany  
Affiliation: RWCP Massively Parallel Systems GMD Laboratory  
Pubnum: Technical Report  
Abstract: The PROMOTER programming language is designated for data parallel appli cations that are to run on massively parallel computers with distributed memory. This paper presents an object-oriented implementation model for the PROMOTER language. An object oriented approach to compile data-parallel programs to message passing programs can reduce design complexity, facilitate reuse of components, and ease specialization of code, result in an efficient and portable implementation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bozkus, Z., Choudary, A., Fox, G., Haupt, T. and Ranka, S.: </author> <title> Compiling HPF for Distributed memory MIMD Computers. </title> <editor> In Lilja, D and Bird, P. (eds): </editor> <booktitle> In Impact of Compilation Technology on Computer Archirecture, </booktitle> <pages> 1-19, </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Efficient implementation of data-parallel languages is especially difficult, considering that these languages attempt to provide a missing link between data-parallel applications and distributed memory architectures in an architecture-independent way. Some work has been done in implementing data-parallel languages <ref> [6, 4, 1, 2, 10] </ref>. However, most of them only discuss regular topologies, regular mapping strategies, or static distributed arrays. In our implementation model we have considered both regular and irregular topologies, both regular and irregular mapping, both static and dynamic distributed objects.
Reference: [2] <author> Brandes, T.: </author> <title> Compiling Data Parallel Programs to Message Passing Programs for Massively Parallel MIMD Systems. </title> <editor> In Giloi W. K., Jaehnichen S., Shriver B.D. (eds): </editor> <booktitle> Proc. MPPM'93 Internat. Working Conference on Massively Parallel Programming Models, </booktitle> <publisher> IEEE-CS Press, </publisher> <year> 1994. </year>
Reference-contexts: Efficient implementation of data-parallel languages is especially difficult, considering that these languages attempt to provide a missing link between data-parallel applications and distributed memory architectures in an architecture-independent way. Some work has been done in implementing data-parallel languages <ref> [6, 4, 1, 2, 10] </ref>. However, most of them only discuss regular topologies, regular mapping strategies, or static distributed arrays. In our implementation model we have considered both regular and irregular topologies, both regular and irregular mapping, both static and dynamic distributed objects.
Reference: [3] <author> Fox, G., Hiranandani, S., Kennedy, K., Koelcel, C., Kremer, U., Tseng, C., and Wu, M.: </author> <title> Fortran D Language Specification. </title> <type> CRPC-TR 90079, </type> <institution> Center for Research on Parallel Computation of Rice University, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: However, programming them by using a message passing library is still difficult, architecture dependent and error-prone. Therefore, data-parallel programming languages such as HPF [7] (High Performance Fortran), Fortran D <ref> [3] </ref>, PC++ [9], MPC++ [8], PROMOTER [5], and many others have been introduced to support machine-independent expression of data parallelism. Efficient implementation of data-parallel languages is especially difficult, considering that these languages attempt to provide a missing link between data-parallel applications and distributed memory architectures in an architecture-independent way.
Reference: [4] <author> Callahan, D. and Kennedy, K.: </author> <title> Compiling Programs for Distributed Memory Multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <pages> 171-207, </pages> <year> 1988. </year>
Reference-contexts: Efficient implementation of data-parallel languages is especially difficult, considering that these languages attempt to provide a missing link between data-parallel applications and distributed memory architectures in an architecture-independent way. Some work has been done in implementing data-parallel languages <ref> [6, 4, 1, 2, 10] </ref>. However, most of them only discuss regular topologies, regular mapping strategies, or static distributed arrays. In our implementation model we have considered both regular and irregular topologies, both regular and irregular mapping, both static and dynamic distributed objects.
Reference: [5] <author> Giloi, W.K., Schramm, A.: </author> <title> PROMOTER, An Application-oriented Programming Model for Massive Parallelism. Proceedings of the Massively Parallel Programming Model Working Con ference, </title> <publisher> IEEE. </publisher> <month> September </month> <year> 1993. </year> <title> 14 PROMOTER: Implementation Model </title>
Reference-contexts: 1 Introduction PROMOTER <ref> [5] </ref> is a language for data parallel applications that are to run on massively parallel computers with distributed memory. PROMOTER allows a problem-oriented description of the spatial structure of the data space and a flexible control of data parallel operations with temporal coordination. <p> However, programming them by using a message passing library is still difficult, architecture dependent and error-prone. Therefore, data-parallel programming languages such as HPF [7] (High Performance Fortran), Fortran D [3], PC++ [9], MPC++ [8], PROMOTER <ref> [5] </ref>, and many others have been introduced to support machine-independent expression of data parallelism. Efficient implementation of data-parallel languages is especially difficult, considering that these languages attempt to provide a missing link between data-parallel applications and distributed memory architectures in an architecture-independent way.
Reference: [6] <author> Hiranandani, S., Kennedy, K. and Tseng, C.W.: </author> <title> Compiler optimization for Fortran D on MIMD Distributed-memory machine. </title> <booktitle> Proc. </booktitle> <address> Supercomputing'91, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Efficient implementation of data-parallel languages is especially difficult, considering that these languages attempt to provide a missing link between data-parallel applications and distributed memory architectures in an architecture-independent way. Some work has been done in implementing data-parallel languages <ref> [6, 4, 1, 2, 10] </ref>. However, most of them only discuss regular topologies, regular mapping strategies, or static distributed arrays. In our implementation model we have considered both regular and irregular topologies, both regular and irregular mapping, both static and dynamic distributed objects.
Reference: [7] <institution> The High Performance Fortran Forum : High Performance Fortran Language Specification, </institution> <note> Version 1.0. </note> <institution> Rice University, Houston Texas, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: PROMOTER: Implementation Model 13 6 Conclusions Massively parallel computers with distributed memory are gaining more and more popularity for their optimal scalability. However, programming them by using a message passing library is still difficult, architecture dependent and error-prone. Therefore, data-parallel programming languages such as HPF <ref> [7] </ref> (High Performance Fortran), Fortran D [3], PC++ [9], MPC++ [8], PROMOTER [5], and many others have been introduced to support machine-independent expression of data parallelism.
Reference: [8] <author> Ishikawa, Y.: </author> <title> The MPC++ Programming Language, V1.0 Specification with Commentary, Doc ument Version 0.1, </title> <type> Technical Report 94014, </type> <institution> Tsukuba Research Center, Real World Computing Partnership, </institution> <address> Japan, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Therefore our implementation model is portable on different plat forms, for example, the parallel operating system PEACE [12] on the parallel distributed-memory machine MANNA (our first implementation), the MPC++ <ref> [8] </ref> on the massively parallel machine RWC-1 (our second implementation) or one of the portable message passing interfaces such as MPI, PVM, and PARMACS. <p> However, programming them by using a message passing library is still difficult, architecture dependent and error-prone. Therefore, data-parallel programming languages such as HPF [7] (High Performance Fortran), Fortran D [3], PC++ [9], MPC++ <ref> [8] </ref>, PROMOTER [5], and many others have been introduced to support machine-independent expression of data parallelism. Efficient implementation of data-parallel languages is especially difficult, considering that these languages attempt to provide a missing link between data-parallel applications and distributed memory architectures in an architecture-independent way.
Reference: [9] <author> Lee, J. K. and Gannon, D.: </author> <title> Object-oriented Parallel Programming: Experiment and results. </title> <booktitle> Proc. of Supercomputing 91, </booktitle> <pages> 273-282, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: However, programming them by using a message passing library is still difficult, architecture dependent and error-prone. Therefore, data-parallel programming languages such as HPF [7] (High Performance Fortran), Fortran D [3], PC++ <ref> [9] </ref>, MPC++ [8], PROMOTER [5], and many others have been introduced to support machine-independent expression of data parallelism. Efficient implementation of data-parallel languages is especially difficult, considering that these languages attempt to provide a missing link between data-parallel applications and distributed memory architectures in an architecture-independent way.
Reference: [10] <author> Hatcher, P., Lapadula, A., Jones, R., Quinn, M. and Anderson, R.: </author> <title> A Production-Quality Cfl Compiler for Hypercube Multicomputers. </title> <booktitle> The 3rd ACM SIGPLAN sym. on PPOPP, </booktitle> <volume> 26 </volume> <pages> 73-82, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Efficient implementation of data-parallel languages is especially difficult, considering that these languages attempt to provide a missing link between data-parallel applications and distributed memory architectures in an architecture-independent way. Some work has been done in implementing data-parallel languages <ref> [6, 4, 1, 2, 10] </ref>. However, most of them only discuss regular topologies, regular mapping strategies, or static distributed arrays. In our implementation model we have considered both regular and irregular topologies, both regular and irregular mapping, both static and dynamic distributed objects.
Reference: [11] <author> Schramm, A.: </author> <title> Concepts of Formal Description of the PROMOTER Language. </title> <type> Technical Report, </type> <institution> GMD-FIRST, </institution> <year> 1994. </year>
Reference: [12] <author> Schroeder-Preiskschat, W.: </author> <title> PEACE A Software Backplane for Parallel Computing, </title> <booktitle> Parallel Computing 20, </booktitle> <pages> 1471-1485, </pages> <year> 1994. </year>
Reference-contexts: Therefore our implementation model is portable on different plat forms, for example, the parallel operating system PEACE <ref> [12] </ref> on the parallel distributed-memory machine MANNA (our first implementation), the MPC++ [8] on the massively parallel machine RWC-1 (our second implementation) or one of the portable message passing interfaces such as MPI, PVM, and PARMACS.
References-found: 12

