URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/ml90-nettalk.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: tgd@cs.orst.edu  hild@irav1.ira.uka.de  haya@cs.orst.edu  
Title: A Comparative Study of ID3 and Backpropagation for English Text-to-Speech Mapping  
Author: Thomas G. Dietterich Hermann Hild s Ghulum Bakiri 
Address: Corvallis, OR 97331-3902  
Affiliation: Department of Computer Science Oregon State University  
Abstract: The performance of the error backpropagation (BP) and ID3 learning algorithms was compared on the task of mapping English text to phonemes and stresses. Under the distributed output code developed by Sejnowski and Rosenberg, it is shown that BP consistently out-performs ID3 on this task by several percentage points. Three hypotheses explaining this difference were explored: (a) ID3 is overfitting the training data, (b) BP is able to share hidden units across several output units and hence can learn the output units better, and (c) BP captures statistical information that ID3 does not. We conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple statistical learning procedure, the performance of BP can be approached but not matched. More complex statistical procedures can improve the performance of both BP and ID3 substantially. A study of the residual errors suggests that there is still substantial room for improvement in learning methods for text-to-speech mapping.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bernstein, J., and Pisoni, D. B., </author> <year> (1980). </year> <title> Unlimited text-to-speech system: description and evaluation of a microprocessor-based device. </title> <booktitle> Proceedings of the Int. conf. Acoust. Speech Signal Process, ICASSP-80, </booktitle> <pages> 576-579. </pages>
Reference: <author> Klatt, D. </author> <year> (1987). </year> <title> Review of text-to-speech conversion for English. </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 82, (3), </volume> <pages> 737-793. </pages>
Reference-contexts: 1 Introduction The task of mapping English text into speech is quite difficult <ref> (see Klatt, 1987) </ref>. One particularly difficult step involves mapping words (i.e., strings of letters) into strings of phonemes and stresses. In this paper, we compare two machine learning algorithms applied to the task of learning this text-to-speech mapping.
Reference: <author> McClelland, J. L., and Rumelhart, D. E. </author> <year> (1988). </year> <title> Explorations in Parallel Distributed Processing, </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Mingers, J. </author> <year> (1989). </year> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4 (2), </volume> <pages> 227-243. </pages>
Reference: <author> Mooney, R., Shavlik, J., Towell, G., and Gove, A. </author> <year> (1989). </year> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> IJCAI-89: Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <pages> 775-80. </pages>
Reference-contexts: The second row shows the effect of applying a 2 test (at the .90 confidence level) to decide whether further growth of the decision tree is statistically justified (Quinlan, 1986a). As other authors have reported <ref> (Mooney et al., 1989) </ref>, this hurts performance in the Nettalk domain. The third row shows the effect of applying Quinlan's technique of reduce-error pruning (Quinlan, 1987). Mingers (1989) provides evidence that this is one of the best pruning techniques.
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess endgames, </title> <editor> in Michalski, R. S., Carbonell, J., and Mitchell, T. M., (eds.), </editor> <booktitle> Machine learning: An ar-tificial intelligence approach, Vol. I, </booktitle> <address> Palo Alto: </address> <publisher> Tioga Press. </publisher> <pages> 463-482. </pages>
Reference-contexts: The version we employed used the information gain criterion to choose which feature to place at the root of each decision tree (and subtree). We did not employ windowing <ref> (Quinlan, 1983) </ref>, CHI-square forward pruning (Quin-lan, 1986a), or any kind of reverse pruning (Quinlan, 1987).
Reference: <author> Quinlan, J. R. </author> <year> (1986a). </year> <title> The effect of noise on concept learning. </title> <editor> In Michalski, R. S., Carbonell, J., and Mitchell, T. M., (eds.), </editor> <booktitle> Machine learning, Vol. II, </booktitle> <address> Palo Alto: </address> <publisher> Tioga Press. </publisher> <pages> 149-166. </pages>
Reference-contexts: The second row shows the effect of applying a 2 test (at the .90 confidence level) to decide whether further growth of the decision tree is statistically justified <ref> (Quinlan, 1986a) </ref>. As other authors have reported (Mooney et al., 1989), this hurts performance in the Nettalk domain. The third row shows the effect of applying Quinlan's technique of reduce-error pruning (Quinlan, 1987). Mingers (1989) provides evidence that this is one of the best pruning techniques.
Reference: <author> Quinlan, J. R. </author> <year> (1986b). </year> <title> Induction of Decision Trees, </title> <booktitle> Machine Learning,1 (1), </booktitle> <pages> 81-106. </pages>
Reference: <author> Quinlan, J. R., </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference-contexts: The version we employed used the information gain criterion to choose which feature to place at the root of each decision tree (and subtree). We did not employ windowing (Quinlan, 1983), CHI-square forward pruning (Quin-lan, 1986a), or any kind of reverse pruning <ref> (Quinlan, 1987) </ref>. <p> As other authors have reported (Mooney et al., 1989), this hurts performance in the Nettalk domain. The third row shows the effect of applying Quinlan's technique of reduce-error pruning <ref> (Quinlan, 1987) </ref>. Mingers (1989) provides evidence that this is one of the best pruning techniques. For this row, a decision tree was built using the 800-word training set, and then pruned using the additional words from the 1000-word training set that do not appear in the 800-word training set.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., and McClel-land, J. L., (eds.) </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol 1. </volume> <pages> 318-362. </pages>
Reference-contexts: Each 29-bit string represents a letter (one bit for each letter, period, comma, and blank), and hence, only one bit is set to 1 in each 29-bit string. This produces a string of 203 bits for each window. The error backpropagation algorithm <ref> (Rumelhart, Hinton & Williams, 1986) </ref> is widely applied to train artificial neural networks. We replicated the network architecture and training procedure employed by Se-jnowski and Rosenberg (1987).
Reference: <author> Sejnowski, T. J., and Rosenberg, C. R. </author> <year> (1987). </year> <title> Parallel networks that learn to pronouce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
References-found: 11

