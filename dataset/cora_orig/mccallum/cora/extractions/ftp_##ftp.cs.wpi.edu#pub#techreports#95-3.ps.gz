URL: ftp://ftp.cs.wpi.edu/pub/techreports/95-3.ps.gz
Refering-URL: http://cs.wpi.edu/Resources/techreports/index.html
Root-URL: 
Email: email: fhachem,staylorg@cs.wpi.edu  
Phone: phone: (508) 831-f5409,5669,5449g Fax: (508) 831-5776  
Title: Approximate Query Answering In Numerical Databases  
Author: Nabil Hachem, Chenye Bao, and Stephen Taylor 
Address: Worcester, MA 01609, USA  
Affiliation: Department of Computer Science Worcester Polytechnic Institute  
Abstract: Scientific databases are usually large, distributed and dynamically changing. We address the problem of efficient processing of queries in scientific databases, especially in very large numerical databases. Previous work has focused on how to store the database and the design of index structures for the efficient access of data. Recently more and more statistical methods have been used in query optimization. Those methods essentially attempt to approximate the distribution of the attribute values in order to estimate the selectivity of query results. We introduce a new methodology that uses regression techniques to approximate the actual attribute values. Through analysis of the data, one derives a set of characteristic functions to form a "regression database," a compressed image of the original database. Based on these functions, approximate answers to queries may be provided within a pre-specified tolerable error, but without the expensive search overhead usually inherent with the use of indexing techniques. We propose a framework to build regression databases. An experimental prototype is implemented to evaluate the technique in terms of realizability, efficiency and practicality. The results demonstrate that our approach is complementary to conventional approaches and to statistical methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Birkes and Y. Dodge, </author> <title> Alternative Methods of Regression, </title> <publisher> John Wiley & Sons, </publisher> <year> 1993. </year>
Reference-contexts: X i can be X 2 0 i , or any other transforms of X i , or combination of any of these transforms. These forms can be chosen into a regression model as the basis fitting function. Additional details can be found in <ref> [1, 2, 6, 14] </ref>. 1 Regression Techniques Domain of Optimality Least Squares when error distribution is exactly normal Least-Absolute-Deviations when error distribution has heavy tails and is effective at controlling bias Huber M-estimate when error distribution has heavy tails Nonparametric when error distribution has heavy tails Bayesian prior information are taken <p> Table 1 classifies some of the commonly used regression techniques and briefly describes under which conditions they are optimal. We discuss the Least Squares regression (LSR) method. Additional details on the other methods is found in <ref> [1, 4] </ref>. Consider a one-dimensional data set with n observations and p independent variables. <p> The LS estimate is the best linear unbiased estimate <ref> [1] </ref>. The disadvantage is that it assumes that the random errors are normally distributed and allows "outlying" data points to influence the final determination of the regression function parameters. Before using regression techniques, we need to choose a set of basis functions that may fit the set of observations.
Reference: [2] <author> S. Chatterjee and B. Price, </author> <title> Regression Analysis by Example, </title> <publisher> John Wiley & Sons, </publisher> <year> 1977. </year>
Reference-contexts: We propose to use regression techniques to approximate the actual attribute values of the data. Regression analysis is a statistical technique for investigating multidimensional/multivariate data. It provides a conceptually simple method for establishing a functional relationships among variables <ref> [2] </ref>. We applied Least Squares Regression, which minimizes the sum of squares of differences between the observed values and the corresponding approximate values, to compute the set of coefficients of the fitting function. <p> X i can be X 2 0 i , or any other transforms of X i , or combination of any of these transforms. These forms can be chosen into a regression model as the basis fitting function. Additional details can be found in <ref> [1, 2, 6, 14] </ref>. 1 Regression Techniques Domain of Optimality Least Squares when error distribution is exactly normal Least-Absolute-Deviations when error distribution has heavy tails and is effective at controlling bias Huber M-estimate when error distribution has heavy tails Nonparametric when error distribution has heavy tails Bayesian prior information are taken
Reference: [3] <author> C. M. Chen and N. Roussopoulos, </author> <title> Adaptive Selective Estimation Using Query Feedback, </title> <booktitle> In Proceeding of ACM-SIGMOD International Conference on Management of Data, </booktitle> <year> 1994, </year> <pages> 1-20. </pages>
Reference-contexts: Our approach can provide accurate approximating functions to any kind of data set since the functions are derived from the characteristics of the actual data. Chen and Roussopoulos proposed a method of approximating the attribute value distribution by a polynomial function using a query feedback mechanism <ref> [3] </ref>. Their approach is useful when queries cover a small range of the database.
Reference: [4] <author> B. Chenye, </author> <title> Approximate Query Answering in Numerical Databases, </title> <type> MS thesis, </type> <institution> WPI CS dept., </institution> <address> Worces-ter, MA, </address> <year> 1995. </year>
Reference-contexts: Table 1 classifies some of the commonly used regression techniques and briefly describes under which conditions they are optimal. We discuss the Least Squares regression (LSR) method. Additional details on the other methods is found in <ref> [1, 4] </ref>. Consider a one-dimensional data set with n observations and p independent variables. <p> Another technique would be to chose the boundary between segments as the value of the newly inserted data point along the split. Relative error and absolute error both have advantages and disadvantages. Which should be chosen depends on the applications <ref> [4] </ref>. 4 Implementation We implemented three different algorithms within the framework. The implementation and evaluation parameters for each algorithm are tabulated in Table 2. The algorithms were tested using a 2-dimensional 1 This estimate is pessimistic, as usually 8 bytes double-precision representations may not be needed.
Reference: [5] <author> W. Cochran, </author> <title> Sampling Techniques, </title> <publisher> John Wiley & Sons, </publisher> <year> 1977. </year>
Reference-contexts: Using all data points in a segment to compute the coefficients may be slow. To speed up the computation, one might just want to use a subset of representative data points to compute the coefficients without losing much accuracy. Sampling techniques have been proposed for that purpose <ref> [5, 10, 11, 20, 21] </ref>. The space required to store the computed matrix depends on the number of data points "n" as well as the number of coefficients of the fitting function.
Reference: [6] <author> N. R. Draper and H. Smith, </author> <title> Applied Regression Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> 1966. </year>
Reference-contexts: The error e is assumed to be a random disturbance with mean 0 and a common variance denoted by 2 . When we say that a model is linear or nonlinear, we are referring to linearity or nonlinearity in the parameters <ref> [6] </ref>. The value of the highest power of a predictor variable in the model is the order of the model. For example, Y = fi 10 + fi 11 X + fi 12 X 2 + e is a second-order (in X) linear (in the fi's) regression model. <p> X i can be X 2 0 i , or any other transforms of X i , or combination of any of these transforms. These forms can be chosen into a regression model as the basis fitting function. Additional details can be found in <ref> [1, 2, 6, 14] </ref>. 1 Regression Techniques Domain of Optimality Least Squares when error distribution is exactly normal Least-Absolute-Deviations when error distribution has heavy tails and is effective at controlling bias Huber M-estimate when error distribution has heavy tails Nonparametric when error distribution has heavy tails Bayesian prior information are taken
Reference: [7] <author> J. Fedorowicz, </author> <title> Database Evaluation Using Multiple Regression Techniques, </title> <booktitle> In Proceedings ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Boston, MA, </address> <year> 1984, </year> <pages> 70-76. </pages>
Reference-contexts: Yet, our approach is achieved at the cost of the accuracy of the query results. Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods <ref> [7, 9, 12, 15, 18, 19, 20, 23, 25, 27] </ref>. We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. <p> We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. Parametric methods have a problem <ref> [7] </ref>: if no known statistical model fits the actual distribution, any attempt to approximate the distribution will be in vain. Sampling methods are rather costly due to run-time disk I/O [9, 12, 15, 20]. Only curve fitting methods are similar to our approach [27].
Reference: [8] <author> G. Graefe, </author> <title> Query Evaluation Techniques for Large Databases, </title> <booktitle> ACM Computing Survey Vol 25 No 2, </booktitle> <month> June </month> <year> 1993, </year> <pages> 73-170. </pages>
Reference-contexts: 1 Introduction Scientific databases are usually large, distributed and dynamically changing. The efficient processing of queries in database systems, and specifically in very large numerical databases is an important research problem <ref> [8, 13, 16, 17, 28] </ref>. When the precise attribute values are not significant in answering a query, using an approximate representation can reduce the storage space for loading the database and the time for searching the large amount of information.
Reference: [9] <author> P. J. Haas and A. N. Swami, </author> <title> Sequential Sampling Procedures for Query Size Estimation, </title> <booktitle> In Proceeding of ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> San Diego, CA, </address> <year> 1992, </year> <pages> 341-350. </pages>
Reference-contexts: Yet, our approach is achieved at the cost of the accuracy of the query results. Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods <ref> [7, 9, 12, 15, 18, 19, 20, 23, 25, 27] </ref>. We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. <p> Parametric methods have a problem [7]: if no known statistical model fits the actual distribution, any attempt to approximate the distribution will be in vain. Sampling methods are rather costly due to run-time disk I/O <ref> [9, 12, 15, 20] </ref>. Only curve fitting methods are similar to our approach [27]. Our approach can provide accurate approximating functions to any kind of data set since the functions are derived from the characteristics of the actual data.
Reference: [10] <author> W. Hou, G. Ozsoyoglu and B.K. Taneja, </author> <title> Statistical Estimators for Relational Algebra Expressions, </title> <booktitle> In Proceeding of ACM SIGMOD, </booktitle> <address> Austin, TX, </address> <year> 1988, </year> <pages> 276-287. </pages>
Reference-contexts: Using all data points in a segment to compute the coefficients may be slow. To speed up the computation, one might just want to use a subset of representative data points to compute the coefficients without losing much accuracy. Sampling techniques have been proposed for that purpose <ref> [5, 10, 11, 20, 21] </ref>. The space required to store the computed matrix depends on the number of data points "n" as well as the number of coefficients of the fitting function.
Reference: [11] <author> W. Hou, G. Ozsoyoglu and B.K. Taneja, </author> <title> Processing Aggregate Relational Queries with Hard Time Constraints, </title> <booktitle> In Proceeding of ACM SIGMOD, </booktitle> <address> Portland, OR, </address> <year> 1989, </year> <pages> 68-77. 18 </pages>
Reference-contexts: Using all data points in a segment to compute the coefficients may be slow. To speed up the computation, one might just want to use a subset of representative data points to compute the coefficients without losing much accuracy. Sampling techniques have been proposed for that purpose <ref> [5, 10, 11, 20, 21] </ref>. The space required to store the computed matrix depends on the number of data points "n" as well as the number of coefficients of the fitting function.
Reference: [12] <author> W. Hou, G. Ozsoyoglu and E. Dogdu, </author> <title> Error-Constrained Count Query Evaluation in Relational Databases, </title> <booktitle> In Proceeding of ACM SIGMOD, </booktitle> <year> 1991, </year> <pages> 278-287. </pages>
Reference-contexts: Yet, our approach is achieved at the cost of the accuracy of the query results. Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods <ref> [7, 9, 12, 15, 18, 19, 20, 23, 25, 27] </ref>. We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. <p> Parametric methods have a problem [7]: if no known statistical model fits the actual distribution, any attempt to approximate the distribution will be in vain. Sampling methods are rather costly due to run-time disk I/O <ref> [9, 12, 15, 20] </ref>. Only curve fitting methods are similar to our approach [27]. Our approach can provide accurate approximating functions to any kind of data set since the functions are derived from the characteristics of the actual data.
Reference: [13] <author> M. Jarke and J. Koch, </author> <title> Query Optimization in Database System, </title> <journal> ACM Computing Surveys, Sept. </journal> <volume> Vol. 16, </volume> <year> 1984, </year> <pages> 111-152. </pages>
Reference-contexts: 1 Introduction Scientific databases are usually large, distributed and dynamically changing. The efficient processing of queries in database systems, and specifically in very large numerical databases is an important research problem <ref> [8, 13, 16, 17, 28] </ref>. When the precise attribute values are not significant in answering a query, using an approximate representation can reduce the storage space for loading the database and the time for searching the large amount of information.
Reference: [14] <author> P. Lancaster and K. Salkauskas, </author> <title> Curve and Surface Fitting, </title> <publisher> Academic Press, </publisher> <year> 1986. </year>
Reference-contexts: X i can be X 2 0 i , or any other transforms of X i , or combination of any of these transforms. These forms can be chosen into a regression model as the basis fitting function. Additional details can be found in <ref> [1, 2, 6, 14] </ref>. 1 Regression Techniques Domain of Optimality Least Squares when error distribution is exactly normal Least-Absolute-Deviations when error distribution has heavy tails and is effective at controlling bias Huber M-estimate when error distribution has heavy tails Nonparametric when error distribution has heavy tails Bayesian prior information are taken <p> If the error population is assumed to be normally distributed, Least Squares Regression may be used. In this work, we focused on two classes of basis functions: polynomial interpolation and Fourier Series. Polynomial Interpolation The most elementary notion of curve fitting is that of interpolation <ref> [14, 26] </ref>. The goal of interpolation is to derive coefficients of a polynomial function that exactly fits a given set of data points. For regression databases, we are interested in the case where there are more conditions to be satisfied than parameters to be adjusted.
Reference: [15] <author> R. J. Lipton, J. F. Naughton and D. A. Schneider, </author> <title> Practical Selectivity Estimation through Adaptive Sampling, </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <address> Atlantic City, NJ, </address> <year> 1990, </year> <pages> 1-12. </pages>
Reference-contexts: Yet, our approach is achieved at the cost of the accuracy of the query results. Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods <ref> [7, 9, 12, 15, 18, 19, 20, 23, 25, 27] </ref>. We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. <p> Parametric methods have a problem [7]: if no known statistical model fits the actual distribution, any attempt to approximate the distribution will be in vain. Sampling methods are rather costly due to run-time disk I/O <ref> [9, 12, 15, 20] </ref>. Only curve fitting methods are similar to our approach [27]. Our approach can provide accurate approximating functions to any kind of data set since the functions are derived from the characteristics of the actual data.
Reference: [16] <author> W.S. </author> <title> Luk and P.A. Black, On Cost Estimation in Processing a Query in a Distributed Database System, </title> <booktitle> In Proceeding of the IEEE 5th International Computer Software and Application Conference, </booktitle> <address> Chicago, IL, </address> <month> Nov. </month> <year> 1981, </year> <pages> 18-22. </pages> <publisher> IEEE, </publisher> <address> New York, </address> <pages> 24-32. </pages>
Reference-contexts: 1 Introduction Scientific databases are usually large, distributed and dynamically changing. The efficient processing of queries in database systems, and specifically in very large numerical databases is an important research problem <ref> [8, 13, 16, 17, 28] </ref>. When the precise attribute values are not significant in answering a query, using an approximate representation can reduce the storage space for loading the database and the time for searching the large amount of information.
Reference: [17] <author> M. V. Mannino, P. Chu and T. Sager, </author> <title> Statistical Profile Estimation in Database Systems, </title> <journal> ACM Computing Surveys, Sept. </journal> <volume> Vol. 20, No. 3, </volume> <year> 1988, </year> <pages> 191-221. </pages>
Reference-contexts: 1 Introduction Scientific databases are usually large, distributed and dynamically changing. The efficient processing of queries in database systems, and specifically in very large numerical databases is an important research problem <ref> [8, 13, 16, 17, 28] </ref>. When the precise attribute values are not significant in answering a query, using an approximate representation can reduce the storage space for loading the database and the time for searching the large amount of information.
Reference: [18] <author> T.H. Merrett and E. Otoo, </author> <title> Distribution Models of Relations, </title> <booktitle> In Proceedings 5th VLDB Conference, </booktitle> <address> Rio De Janero, Brazil, </address> <year> 1979, </year> <pages> 418-425. </pages>
Reference-contexts: Yet, our approach is achieved at the cost of the accuracy of the query results. Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods <ref> [7, 9, 12, 15, 18, 19, 20, 23, 25, 27] </ref>. We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. <p> Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods [7, 9, 12, 15, 18, 19, 20, 23, 25, 27]. We intend to map the actual data directly against regression functions. With histogram methods <ref> [18, 19, 23, 25] </ref>, one needs to store the detailed statistics about the database. Parametric methods have a problem [7]: if no known statistical model fits the actual distribution, any attempt to approximate the distribution will be in vain.
Reference: [19] <author> M. Muralikrishma and D. J. DeWitt, </author> <title> Equi-depth Histograms for Estimating Selectivity Factors for Multi-dimensional Queries, </title> <booktitle> In Proceedings ACM-SIGMOD Conference on management of Data, </booktitle> <address> Chicago, Illinois, </address> <year> 1988, </year> <pages> 28-36. </pages>
Reference-contexts: Yet, our approach is achieved at the cost of the accuracy of the query results. Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods <ref> [7, 9, 12, 15, 18, 19, 20, 23, 25, 27] </ref>. We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. <p> Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods [7, 9, 12, 15, 18, 19, 20, 23, 25, 27]. We intend to map the actual data directly against regression functions. With histogram methods <ref> [18, 19, 23, 25] </ref>, one needs to store the detailed statistics about the database. Parametric methods have a problem [7]: if no known statistical model fits the actual distribution, any attempt to approximate the distribution will be in vain.
Reference: [20] <author> F. Olken and D. Rotem, </author> <title> Simple Random Sampling for Relational Databases, </title> <booktitle> In Proceeding 12th VLDB, </booktitle> <address> Kyoto, </address> <month> August, </month> <year> 1986, </year> <pages> 160-169. </pages>
Reference-contexts: Using all data points in a segment to compute the coefficients may be slow. To speed up the computation, one might just want to use a subset of representative data points to compute the coefficients without losing much accuracy. Sampling techniques have been proposed for that purpose <ref> [5, 10, 11, 20, 21] </ref>. The space required to store the computed matrix depends on the number of data points "n" as well as the number of coefficients of the fitting function. <p> Yet, our approach is achieved at the cost of the accuracy of the query results. Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods <ref> [7, 9, 12, 15, 18, 19, 20, 23, 25, 27] </ref>. We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. <p> Parametric methods have a problem [7]: if no known statistical model fits the actual distribution, any attempt to approximate the distribution will be in vain. Sampling methods are rather costly due to run-time disk I/O <ref> [9, 12, 15, 20] </ref>. Only curve fitting methods are similar to our approach [27]. Our approach can provide accurate approximating functions to any kind of data set since the functions are derived from the characteristics of the actual data.
Reference: [21] <author> F. Olken and D. Rotem, </author> <title> Random Sampling from B+ Trees, </title> <booktitle> In Proceeding 15th VLDB, </booktitle> <address> Amsterdam, </address> <year> 1989, </year> <pages> 269-278. </pages>
Reference-contexts: Using all data points in a segment to compute the coefficients may be slow. To speed up the computation, one might just want to use a subset of representative data points to compute the coefficients without losing much accuracy. Sampling techniques have been proposed for that purpose <ref> [5, 10, 11, 20, 21] </ref>. The space required to store the computed matrix depends on the number of data points "n" as well as the number of coefficients of the fitting function.
Reference: [22] <author> L. M. Olsen and A. Warnock III, GEDEX: </author> <title> Selected Data Sets for Greenhouse Effect Detection Experiment, </title> <booktitle> The Space Agency Forum on The International Space Year, NASA, </booktitle> <year> 1992. </year>
Reference-contexts: 327 93 13 1 1 0 Degree=6 518 318 95 29 16 22 18 Degree=8 518 306 101 34 13 19 22 Table 3: Data points distribution as a function of the error level for the synthetic data set Solar data set The ERB-SOLI data set from NASA's GEDEX CD-ROM <ref> [22] </ref> was used. It consists of 4,794 data points on daily averages of solar irradiance, collected by the NIMBUS-7 satellite between 1978 and 1991. Temperature data set The temperature data set was extracted from the GEDEX CD-ROM as well. <p> We performed various experiments using a synthetic as well as real data sets from the GEDEX database <ref> [22] </ref>. Experimental results indicate that our approach is promising in that it can achieve a 188:1 compression ratio over the original database while at the same time being able to provide answers to queries within a relative tolerable error as low as 0.002.
Reference: [23] <author> G. Piatetsky-Shapiro and C. Connell, </author> <title> Accurate Estimation of the Number of Tuples Satisfying a Condition, </title> <booktitle> In Proceedings ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Boston, MA, </address> <year> 1984, </year> <pages> 256-275. </pages>
Reference-contexts: Yet, our approach is achieved at the cost of the accuracy of the query results. Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods <ref> [7, 9, 12, 15, 18, 19, 20, 23, 25, 27] </ref>. We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. <p> Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods [7, 9, 12, 15, 18, 19, 20, 23, 25, 27]. We intend to map the actual data directly against regression functions. With histogram methods <ref> [18, 19, 23, 25] </ref>, one needs to store the detailed statistics about the database. Parametric methods have a problem [7]: if no known statistical model fits the actual distribution, any attempt to approximate the distribution will be in vain.
Reference: [24] <author> W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery, </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: e 1 . . . 3 7 5 2 6 4 f 2 f n 7 7 : It is well known that A = (X T X) 1 X T F gives the coefficients that minimize the sum of the squares of differences between the observations and the approximations <ref> [24] </ref>.
Reference: [25] <author> P.G. Selinger, M.M. Astrahan, D.D. Chamberlin, R.A. Lorie and T.G Price, </author> <title> Access Path Selection in a Relational Database Management System, </title> <booktitle> In Proceedings ACM-SIGMOD, </booktitle> <address> San Jose, CA, </address> <year> 1979, </year> <pages> 23-34. </pages>
Reference-contexts: Yet, our approach is achieved at the cost of the accuracy of the query results. Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods <ref> [7, 9, 12, 15, 18, 19, 20, 23, 25, 27] </ref>. We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. <p> Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods [7, 9, 12, 15, 18, 19, 20, 23, 25, 27]. We intend to map the actual data directly against regression functions. With histogram methods <ref> [18, 19, 23, 25] </ref>, one needs to store the detailed statistics about the database. Parametric methods have a problem [7]: if no known statistical model fits the actual distribution, any attempt to approximate the distribution will be in vain.
Reference: [26] <author> J.Stoer and R.Bulirsch, </author> <title> Introduction to Numerical Analysis, </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: If the error population is assumed to be normally distributed, Least Squares Regression may be used. In this work, we focused on two classes of basis functions: polynomial interpolation and Fourier Series. Polynomial Interpolation The most elementary notion of curve fitting is that of interpolation <ref> [14, 26] </ref>. The goal of interpolation is to derive coefficients of a polynomial function that exactly fits a given set of data points. For regression databases, we are interested in the case where there are more conditions to be satisfied than parameters to be adjusted.
Reference: [27] <author> W. Sun, Y. Ling, N. Rishe and Y. Deng, </author> <title> An Instant and Accurate Size Estimation Method for Joins and Selection in a Retrieval-Intensive Environment, </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <address> Washington, DC, </address> <year> 1993, </year> <pages> 79-88. </pages>
Reference-contexts: Yet, our approach is achieved at the cost of the accuracy of the query results. Recent work on query optimization essentially attempt to map the distribution of the actual data by statistical methods <ref> [7, 9, 12, 15, 18, 19, 20, 23, 25, 27] </ref>. We intend to map the actual data directly against regression functions. With histogram methods [18, 19, 23, 25], one needs to store the detailed statistics about the database. <p> Parametric methods have a problem [7]: if no known statistical model fits the actual distribution, any attempt to approximate the distribution will be in vain. Sampling methods are rather costly due to run-time disk I/O [9, 12, 15, 20]. Only curve fitting methods are similar to our approach <ref> [27] </ref>. Our approach can provide accurate approximating functions to any kind of data set since the functions are derived from the characteristics of the actual data. Chen and Roussopoulos proposed a method of approximating the attribute value distribution by a polynomial function using a query feedback mechanism [3].
Reference: [28] <author> C. Yu and C. Chang, </author> <title> Distributed Query Processing, </title> <journal> ACM Computing Surveys, </journal> <year> 1984, </year> <pages> 399-433. 19 </pages>
Reference-contexts: 1 Introduction Scientific databases are usually large, distributed and dynamically changing. The efficient processing of queries in database systems, and specifically in very large numerical databases is an important research problem <ref> [8, 13, 16, 17, 28] </ref>. When the precise attribute values are not significant in answering a query, using an approximate representation can reduce the storage space for loading the database and the time for searching the large amount of information.
References-found: 28

