URL: http://vibes.cs.uiuc.edu/Publications/Papers/Cluster.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/publications.htm
Root-URL: http://www.cs.uiuc.edu
Title: REAL-TIME STATISTICAL CLUSTERING FOR EVENT TRACE REDUCTION  
Author: OLEG Y. NICKOLAYEV, PHILIP C. ROTHy AND DANIEL A. REEDz 
Keyword: Key Words. clustering, event tracing, parallel processing  
Abstract: Event tracing provides the detailed data needed to understand the dynamics of interactions among application resource demands and system responses. However, capturing the large volume of dynamic performance data inherent in detailed tracing can perturb program execution and stress secondary storage systems. Moreover, it can overwhelm a user or performance analyst with potentially irrelevant data. Using the Pablo performance environment's support for real-time data analysis, we show that dynamic statistical data clustering can dramatically reduce the volume of captured performance data by identifying and recording event traces only from representative processors. In turn, this makes possible low overhead, interactive visualization and performance tuning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. A. Aydt, SDDF: </author> <title> The Pablo Self-Describing Data Format, </title> <type> tech. rep., </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: This extension processes the events as they are generated, computing the sliding window average metrics needed for statistical clustering and buffering the raw event data. The extension software periodically 11 sends the metric values to a statistical clusterer using the Pablo self-describing data format (SDDF) <ref> [1] </ref>. This clusterer can execute either on another processor of the same system or on a network-attached workstation. Using the sliding window metrics, the clusterer software identifies the current clusters and cluster representatives. The clusterer then returns the identity of the cluster representatives to the processors.
Reference: [2] <author> P. E. Crandall, R. A. Aydt, A. A. Chien, and D. A. Reed, </author> <title> Characterization of a Suite of Input/Output Intensive Applications, </title> <booktitle> in Proceedings of Supercomputing '95, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Because both of these codes 1 Real-time data reduction is the subject of x5. 7 have been studied extensively as part of the Scalable I/O Initiative <ref> [10, 2] </ref>, we have a large set of experimental data as a comparative baseline. 4.1. Synthetic Aperture Radar (SAR). The SAR technique is used for high-resolution ground surface imaging [8]. Unlike conventional radar, which uses only signal amplitude to produce images, SAR uses both signal phase and amplitude. <p> Hartree Quantum Chemistry (HF). The second of our two test applications is a quantum chemistry code that calculates the non-relativistic interactions among atomic nuclei, electrons in the presence of other electrons, and electrons interacting with nuclei <ref> [2] </ref>. The major portion of the Hartree (HF) code is solution of a set of self-consistent field equations. The calculation requires a large volume of precomputed integral data that must be read from secondary storage.
Reference: [3] <author> W. Gropp, E. Lusk, and A. Skjellum, </author> <title> Using MPI: Portable Parallel Programming with the Message Passing Interface, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Program Models and Performance Metric Spaces. Almost all programs designed for large parallel systems are either functional decompositions, single-program, multiple-data (SPMD) (e.g., based on MPI <ref> [3] </ref>), or data parallel (e.g., based on HPF [4]). In the first case, each function executes on a different set of processors (or processes), naturally partitioning the processors into equivalence classes.
Reference: [4] <author> HPFF, </author> <title> High-Performance Fortran Language Specfication, version 1.1, </title> <type> tech. rep., </type> <institution> High Performance Fortran Forum, </institution> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Program Models and Performance Metric Spaces. Almost all programs designed for large parallel systems are either functional decompositions, single-program, multiple-data (SPMD) (e.g., based on MPI [3]), or data parallel (e.g., based on HPF <ref> [4] </ref>). In the first case, each function executes on a different set of processors (or processes), naturally partitioning the processors into equivalence classes.
Reference: [5] <author> A. K. Jain and R. C. Dubes, </author> <title> Algorithms for Clustering Data, </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Consequently, the potential data reduction is enormous (e.g., if there is only one equivalence class, then the event trace volume is dependent only on total execution time, not the number of processors). 3. Statistical Data Clustering. A wide variety of statistical clustering algorithms have been proposed <ref> [5] </ref>, each distinguished by the method used to partition multidimensional data sets as clusters. Hierarchical clustering algorithms determine a hierarchy of clusters within a data set. Conversely, partitional clustering algorithms iteratively modify an initial partition of the data set to identify an optimal clustering. <p> The second class begins with a single cluster containing all data points and iteratively splits clusters whose points excessively dispersed. For additional details, see <ref> [15, 9, 5] </ref>. 4 0 8 16 24 Represen tativ e cessor Execution time In addition to choosing cluster manipulation algorithms, one must specify when cluster membership should be recomputed. Abstractly, clustering often will most accurately track metric space variations when it detects all cluster membership changes.
Reference: [6] <author> A. D. Malony and D. A. Reed, </author> <title> A Hardware-Based Performance Monitor for the Intel iPSC/2 Hypercube, </title> <booktitle> in 1990 ACM International Conference on Supercomputing, Association for Computing Machinery, </booktitle> <month> June </month> <year> 1990, </year> <pages> pp. 213-216. </pages>
Reference-contexts: Although throttling prevents generation of extremely large data volumes, it sacrifices a consistent view of application behavior by periodically substituting counts for the more detailed traces. Others have proposed hardware support for event capture and processing <ref> [6, 14] </ref>. These techniques lower the overhead for event capture by providing data collection and extraction mechanisms that can operate in parallel with an application. Dynamic statistical clustering is complementary to all of these approaches.
Reference: [7] <author> B. P. Miller, M. D. Callaghan, J. M. Cargille, J. K. Hollingsworth, R. B. Irwin, K. L. Karavanic, and T. Newhall, </author> <title> The Paradyn Parallel Performance Measurement Tools, </title> <journal> in IEEE Computer, </journal> <volume> vol. 28, </volume> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Each strikes a different balance between performance data volume, instrumentation perturbation, and implementation complexity. By recording the detailed temporal and spatial patterns of software interactions, timestamped event tracing subsumes the functions of profiling, timing, and counting (i.e., one can generate profiles, counts, and timing summaries from event traces) <ref> [12, 7, 14] </ref>. The flexibility of event tracing is not without price. If the events of interest occur frequently or the size of the instrumented system is large, tracing can quickly generate prodigious volumes of data. <p> In turn, this opens the possibility of low cost, real-time visualization of truly large scale parallel systems. 6. Related Work. Several techniques have been proposed or developed to understand the dynamic behavior of large parallel applications while avoiding perturbations due to excessive instrumentation. Miller's Paradyn system <ref> [7] </ref> supports dynamic instrumentation via object code patching. Via this technique, users or performance tools can dynamically enable or disable instrumentation at selected application sites. Our Pablo instrumentation library supports event throttling [11].
Reference: [8] <author> C. Miller, D. G. Payne, T. Phung, H. Siegel, and R. D. Williams, </author> <title> Parallel Processing of Spaceborne Imaging Radar Data on the Paragon, </title> <booktitle> in Newsletter of the Concurrent Supercomputing Consortium, </booktitle> <month> Mar. </month> <year> 1995, </year> <pages> pp. 7-9. </pages>
Reference-contexts: Synthetic Aperture Radar (SAR). The SAR technique is used for high-resolution ground surface imaging <ref> [8] </ref>. Unlike conventional radar, which uses only signal amplitude to produce images, SAR uses both signal phase and amplitude. This allows a larger aperture to be synthesized, creating higher resolution images.
Reference: [9] <author> O. Nickolayev, </author> <title> Performance Data Reduction Using Dynamic Statistical Clustering, </title> <type> Master's thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: The second class begins with a single cluster containing all data points and iteratively splits clusters whose points excessively dispersed. For additional details, see <ref> [15, 9, 5] </ref>. 4 0 8 16 24 Represen tativ e cessor Execution time In addition to choosing cluster manipulation algorithms, one must specify when cluster membership should be recomputed. Abstractly, clustering often will most accurately track metric space variations when it detects all cluster membership changes. <p> When choosing a representative, the processor with the highest count belonging to the cluster is selected, regardless of its distance to the cluster's centroid. Only when two processors have the same count is the processor closest to the centroid selected. Our experiments <ref> [9] </ref> have shown that this greatly increases trace segment length with little effect on the quality of the recorded data. 4. Post-Mortem Clustering. <p> Below, we present a small subset of these experiments; for complete details, see <ref> [15, 9] </ref>. 8 For the SAR code, we varied the sliding window L from 0.5 to eight seconds, with the window increment I one half this value. The largest window size was chosen because it was roughly equal to the duration of the shortest computational phase.
Reference: [10] <author> J. T. Poole, </author> <title> Scalable I/O Initiative. </title> <institution> California Institute of Technology, </institution> <note> Available at http://www.ccsf.caltech.edu/SIO/, 1996. </note>
Reference-contexts: Because both of these codes 1 Real-time data reduction is the subject of x5. 7 have been studied extensively as part of the Scalable I/O Initiative <ref> [10, 2] </ref>, we have a large set of experimental data as a comparative baseline. 4.1. Synthetic Aperture Radar (SAR). The SAR technique is used for high-resolution ground surface imaging [8]. Unlike conventional radar, which uses only signal amplitude to produce images, SAR uses both signal phase and amplitude.
Reference: [11] <author> D. A. Reed, R. A. Aydt, R. J. Noe, P. C. Roth, K. A. Shields, B. W. Schwartz, and L. F. Tavera, </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment, </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, ed., </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993, </year> <pages> pp. 104-113. </pages>
Reference-contexts: Miller's Paradyn system [7] supports dynamic instrumentation via object code patching. Via this technique, users or performance tools can dynamically enable or disable instrumentation at selected application sites. Our Pablo instrumentation library supports event throttling <ref> [11] </ref>. By monitoring the observed event rate and comparing it to user-specified high and low water marks, the Pablo library can dynamically replace event tracing with less invasive measures like counts.
Reference: [12] <author> D. A. Reed, C. L. Elford, T. Madhyastha, W. H. Scullin, R. A. Aydt, and E. Smirni, </author> <title> I/O, Performance Analysis, and Performance Data Immersion, </title> <booktitle> in Proceedings of MASCOTS '96, </booktitle> <month> Feb. </month> <year> 1996, </year> <pages> pp. 1-12. </pages>
Reference-contexts: Each strikes a different balance between performance data volume, instrumentation perturbation, and implementation complexity. By recording the detailed temporal and spatial patterns of software interactions, timestamped event tracing subsumes the functions of profiling, timing, and counting (i.e., one can generate profiles, counts, and timing summaries from event traces) <ref> [12, 7, 14] </ref>. The flexibility of event tracing is not without price. If the events of interest occur frequently or the size of the instrumented system is large, tracing can quickly generate prodigious volumes of data. <p> During the first phase, 0-25 seconds for our test problem, local variables are initialized. The second phase, 25-40 seconds, reads data from the input files. During the final phase, 40-80 seconds, the actual data processing occurs. input phase. This snapshot is taken from the Avatar performance data immersion system <ref> [12, 13] </ref>, which supports real-time display of dynamic performance data and direct manipulation of application behavior. In Figure 4a, each processor's events are portrayed as timelines along the periphery of a cylinder, with most recent events nearest the viewer and processors numbered beginning clockwise from zero at the top. <p> Finally, we used three trace event types to identify behavioral classes: blocking send begin, nonblocking send begin, and file read. The efficacy of a particular choice depends on the relative contribution of these events to application behavior. The Pablo instrumentation library <ref> [12] </ref>, generates blocking (nonblocking) send begin events whenever a processor begins transmission of a blocking (nonblocking) message. For these two communication events, we chose the message length as the performance metric for clustering. Similarly, a file read event is generated after a processor completes a file read operation. <p> To effectively reduce the total volume of performance data, statistical clustering must operate during program execution, capturing traces only from representative processors. Below, we briefly describe our experiences with an implementation of real-time statistical clustering that builds on the Pablo performance environment's instrumentation library <ref> [12, 13] </ref>. 5.1. Software Design. Figure 10 shows the integration of the Pablo performance instrumentation library with a real-time statistical clustering infrastructure. The standard Pablo instrumentation software captures dynamic performance data via instrumented source code that is linked with the data capture library. <p> Real-time statistical clustering makes possible the automatic selection of key software behaviors for real-time display. Acknowledgments. Matthew Gardner processed large numbers of our trace files, generating the still images from the Avatar <ref> [12] </ref> virtual environment used to prepare this paper. His help in understanding the dynamics of the clustering algorithms was invaluable.
Reference: [13] <author> D. A. Reed, C. L. Elford, T. M. Madhyastha, E. Smirni, and S. E. </author> <title> Lamm, The Next Frontier: Interactive and Closed Loop Performance Steering, </title> <booktitle> in Proceedings of the 1996 ICPP Workshop on Challenges for Parallel Processing, </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: During the first phase, 0-25 seconds for our test problem, local variables are initialized. The second phase, 25-40 seconds, reads data from the input files. During the final phase, 40-80 seconds, the actual data processing occurs. input phase. This snapshot is taken from the Avatar performance data immersion system <ref> [12, 13] </ref>, which supports real-time display of dynamic performance data and direct manipulation of application behavior. In Figure 4a, each processor's events are portrayed as timelines along the periphery of a cylinder, with most recent events nearest the viewer and processors numbered beginning clockwise from zero at the top. <p> To effectively reduce the total volume of performance data, statistical clustering must operate during program execution, capturing traces only from representative processors. Below, we briefly describe our experiences with an implementation of real-time statistical clustering that builds on the Pablo performance environment's instrumentation library <ref> [12, 13] </ref>. 5.1. Software Design. Figure 10 shows the integration of the Pablo performance instrumentation library with a real-time statistical clustering infrastructure. The standard Pablo instrumentation software captures dynamic performance data via instrumented source code that is linked with the data capture library. <p> Moreover, we have shown that the technique can be implemented efficiently for real-time operation. Building on these results, we plan to integrate real-time clustering with a new performance analysis environment that supports immersive representations of software structure and dynamics <ref> [13] </ref>. This environment, called Virtue, will allow software developers to directly manipulate software components and their behavior while immersed in scalable, hierarchical representations of software structure and real-time performance data. Real-time statistical clustering makes possible the automatic selection of key software behaviors for real-time display. Acknowledgments.
Reference: [14] <author> B. Ries, R. Anderson, W. Auld, D. Breazeal, K. Callaghan, E. Richards, and W. Smith, </author> <title> The Paragon Performance Monitoring Environment, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> Nov. </month> <year> 1993, </year> <pages> pp. 850-859. </pages>
Reference-contexts: Each strikes a different balance between performance data volume, instrumentation perturbation, and implementation complexity. By recording the detailed temporal and spatial patterns of software interactions, timestamped event tracing subsumes the functions of profiling, timing, and counting (i.e., one can generate profiles, counts, and timing summaries from event traces) <ref> [12, 7, 14] </ref>. The flexibility of event tracing is not without price. If the events of interest occur frequently or the size of the instrumented system is large, tracing can quickly generate prodigious volumes of data. <p> Although throttling prevents generation of extremely large data volumes, it sacrifices a consistent view of application behavior by periodically substituting counts for the more detailed traces. Others have proposed hardware support for event capture and processing <ref> [6, 14] </ref>. These techniques lower the overhead for event capture by providing data collection and extraction mechanisms that can operate in parallel with an application. Dynamic statistical clustering is complementary to all of these approaches.

References-found: 14

