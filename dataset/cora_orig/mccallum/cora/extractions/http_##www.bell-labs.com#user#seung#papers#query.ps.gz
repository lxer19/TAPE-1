URL: http://www.bell-labs.com/user/seung/papers/query.ps.gz
Refering-URL: http://www.bell-labs.com/user/seung/papers/index.html
Root-URL: 
Email: seung@mars.huji.ac.il  manfred.opper@  haim@galaxy.huji.ac.il  
Title: Query by Committee  
Author: H. S. Seung M. Opper H. Sompolinsky 
Web: physik.uni-giessen.dbp.de  
Address: Jerusalem 91904, Israel  Justus-Liebig-Universitat Giessen D-6300 Giessen, Germany  Jerusalem 91904, Israel  
Affiliation: Racah Institute of Physics and Center for Neural Computation Hebrew University  Institut fur Theoretische Physik  Racah Institute of Physics and Center for Neural Computation Hebrew University  
Abstract: We propose an algorithm called query by committee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law. We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [Bau91] <author> E. Baum. </author> <title> Neural net algorithms that learn in polynomial time from examples and queries. </title> <journal> IEEE Trans. in Neural Networks, </journal> <volume> 2 </volume> <pages> 5-19, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Although query algorithms have been proposed for a variety of learning problems <ref> [Bau91] </ref>, little work has gone into understanding the general principles by which these algorithms should be constructed. In this work, we argue that the Shannon information of a query can be a suitable guide [Fed72].
Reference: [Fed72] <author> V. V. Fedorov. </author> <title> Theory of Optimal Experi ments. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: 1 Introduction Although query algorithms have been proposed for a variety of learning problems [Bau91], little work has gone into understanding the general principles by which these algorithms should be constructed. In this work, we argue that the Shannon information of a query can be a suitable guide <ref> [Fed72] </ref>. We further show that the degree of disagreement among a committee of learners can serve as an estimate of this information value.
Reference: [GD89] <author> E. Gardner and B. Derrida. </author> <title> Three unfinished works on the optimal storage capacity of networks. </title> <journal> J. Phys., </journal> <volume> A22:1983-1994, </volume> <year> 1989. </year>
Reference-contexts: In the following, the query by committee algorithm is first illustrated using a very simple model, the high-low game. We then move on to a more complicated model, perceptron learning of another perceptron <ref> [GD89] </ref>. For both models, the information gain approaches a finite value as the number of queries goes to infinity. This asymptotically finite information gain leads to generalization error that decreases exponentially with the number of queries.
Reference: [GT90] <author> G. Gyorgyi and N. Tishby. </author> <title> Statistical theory of learning a rule. </title> <editor> In W. K. Theumann and R. Koberle, editors, </editor> <booktitle> Neural Networks and Spin Glasses, </booktitle> <pages> pages 3-36, </pages> <address> Singapore, 1990. </address> <publisher> World Scientific. </publisher>
Reference-contexts: The coefficient of P in the resulting exponential is given by loghi rather than hlog i. 4.1 Random inputs When all inputs are chosen at random from the distribution (34), the replica method can be used to calculate the entropy of the posterior distribution <ref> [GT90] </ref>. The calculation is exact in the thermodynamic limit, where P; N ! 1 with ff = P=N constant.
Reference: [HKS91] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of bayesian learning using information theory and the VC dimension. </title> <editor> In M. K. Warmuth and L. G. Valiant, editors, </editor> <booktitle> Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is written as P (Wj~ 1 ; : : : ; ~ P ) = V 1 0; otherwise, (2) where V P is the volume of W P . We consider the Gibbs training algorithm <ref> [HKS91] </ref>, in which the weight vector W is drawn at random from this posterior distribution. A query algorithm specifies a way of choosing another input X P +1 with which to query the teacher.
Reference: [KR90] <author> W. Kinzel and P. Rujan. </author> <title> Improving a net work generalization ability by selecting examples. </title> <journal> Europhys. Lett., </journal> <volume> 13 </volume> <pages> 473-477, </pages> <year> 1990. </year>
Reference-contexts: A practical drawback of this scheme is that the time to find a query diverges like the inverse of the generalization error. In this respect, algorithms which construct queries directly may be superior. For example, the query algorithm proposed by Kinzel and Rujan <ref> [KR90, WR92] </ref> for perceptron learning constructs input vectors that are perpendicular to the student weight vector. In conjunction with the Gibbs training algorithm, this method yields generalization performance only slightly worse than that of the committee algorithms for moderate ff, but much faster query times.
Reference: [MP88] <author> M. L. Minsky and S. Papert. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <note> expanded edition, </note> <year> 1988. </year>
Reference-contexts: We have performed a Monte Carlo simulation of this algorithm. The deterministic perceptron algorithm <ref> [MP88] </ref> was used to obtain a weight vector in the version space. Then zero-temperature Monte Carlo was used to random walk inside the version space. To maintain acceptance rates of approximately 50%, the size of the Monte Carlo step was scaled downward with increasing ff.
Reference: [OH91] <author> M. Opper and D. Haussler. </author> <title> Generalization performance of bayes optimal classification algorithm for learning a perceptron. </title> <journal> Phys. Rev. Lett., </journal> <volume> 66 </volume> <pages> 2677-2680, </pages> <year> 1991. </year>
Reference-contexts: Query the teacher about this input vector. Train the committee again using the new enlarged training set, and repeat. As k ! 1 this algorithm approaches the bisection algorithm. This algorithm is very much in the spirit of <ref> [OH91] </ref>, in which consensus was used to improve generalization performance. Here we use lack of consensus to choose a query, or a principle of maximal disagreement.
Reference: [SST92] <author> H. S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Phys. Rev., </journal> <volume> A45:6056-6091, </volume> <year> 1992. </year>
Reference-contexts: This will enable us to use techniques from statistical mechanics <ref> [SST92] </ref>. After training 2k students on the same training set, the query by committee algorithm selects an input that is classified as positive by half of the committee, and negative by the other half. By maximizing disagreement among the committee, the information gain of the query can be made high. <p> * g = 1 cos 1 q : (40) In the large ff limit, this leads to the inverse power law behavior * g (ff) ff The large ff asymptotics can also be obtained by examining the scaling of the entropy with the generalization error, similar to the arguments in <ref> [SST92] </ref> using the microcanonical high-T limit for a general classification of learning curves.
Reference: [TLS89] <author> N. Tishby, E. Levin, and S. Solla. </author> <title> Consistent inference of probabilities in layered networks: Predictions and generalization. </title> <booktitle> In Proc. Int. Joint Conf. on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 403-409, </pages> <address> Washington, DC, 1989. </address> <publisher> IEEE. </publisher>
Reference-contexts: If the prior distribution P 0 (W) is assumed to be flat, then the posterior distribution is uniform on the version space, and vanishes outside <ref> [TLS89] </ref>. This is written as P (Wj~ 1 ; : : : ; ~ P ) = V 1 0; otherwise, (2) where V P is the volume of W P .
Reference: [WR92] <author> T. L. H. Watkin and A. Rau. </author> <title> Selecting exam ples for perceptrons. </title> <journal> J. Phys., </journal> <volume> A25:113-121, </volume> <year> 1992. </year>
Reference-contexts: A practical drawback of this scheme is that the time to find a query diverges like the inverse of the generalization error. In this respect, algorithms which construct queries directly may be superior. For example, the query algorithm proposed by Kinzel and Rujan <ref> [KR90, WR92] </ref> for perceptron learning constructs input vectors that are perpendicular to the student weight vector. In conjunction with the Gibbs training algorithm, this method yields generalization performance only slightly worse than that of the committee algorithms for moderate ff, but much faster query times.
References-found: 11

