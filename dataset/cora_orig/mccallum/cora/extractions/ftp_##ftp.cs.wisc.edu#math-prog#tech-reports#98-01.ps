URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/98-01.ps
Refering-URL: http://www.cs.wisc.edu/~paulb/papers.html
Root-URL: 
Email: paulb@cs.wisc.edu Fayyad@microsoft.com olvi@cs.wisc.edu  
Title: Data Mining: Overview and Optimization Opportunities  
Author: P. S. Bradley Usama M. Fayyad O. L. Mangasarian 
Date: January 19, 1998  
Web: http://www.cs.wisc.edu/~ paulb http://www.research.microsoft.com/~fayyad http://www.cs.wisc.edu/~ olvi  
Address: Wisconsin Redmond, WA 98052  1210 West Dayton Street 1210 West Dayton Street Madison, WI 53706 Madison, WI 53706  
Affiliation: Computer Sciences Department Microsoft Research Computer Sciences Department University of  University of Wisconsin  
Abstract: This paper is intended to serve as an overview of a rapidly emerging research and applications area. In addition to providing a general overview, motivating the importance of data mining problems within the area of knowledge discovery in databases, our aim is to list some of the pressing research challenges, and outline opportunities for contributions by the optimization research communities. Towards these goals, we include formulations of the basic categories of data mining methods as optimization problems. We also provide examples of successful mathematical programming approaches to some data mining problems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Special issue on data mining. </editor> <booktitle> Communications of The ACM, </booktitle> <volume> vol. 39, no. </volume> <month> 11 </month> <year> 1996. </year>
Reference-contexts: However, the bulk of applications in this area have been achieved using statistical pattern recognition, machine learning, and database approaches. We do not cover those application case studies in this paper. Applications are covered in [36], and in the special issue of Communications of the ACM <ref> [1] </ref> 1 . <p> We assume that these data points are a finite sample from some unknown probability density function which maps a data point in R n to the interval <ref> [0; 1] </ref>. The goal is to compute an estimate of the true probability density function. We will denote our estimate of the true probability density function (PDF) by p (x). Parametric approaches to density estimation fix the functional form of the estimated probability density function (PDF) p (x). <p> This case study appears in [17] and is summarized here. The linear systems used are based upon ideas related to signal processing [47, 99] and more specifically to an example in [2, Equation (8)]. We consider the following true signal g (t) : <ref> [0; 1] </ref> ! R: g (t) = j=1 We assume that the true signal g (t) cannot be sampled precisely, but that the following observed signal can be sampled: ~g (t) = (g (t) + error); sampled at times : t i = i 4 t; 4t = 0:04; i = <p> Going beyond L-norms. Current approaches utilize all dimensions of a problem with equal weight or use feature selection to weight each dimension with a 0 or 1 weight. Allowing a variable weight in the interval <ref> [0; 1] </ref> for example would introduce a scaling of the problem that could enhance separation either by a linear or nonlinear separating approach [67, 68, 9]. 7 Concluding Remarks KDD holds the promise of an enabling technology that could unlock the knowledge lying dormant in huge databases.
Reference: [2] <author> T. J. Abatzoglou, J. M. Mendel, and G. A. Harada. </author> <title> The constrained total least squares technique and its application to harmonic superposition. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39 </volume> <pages> 1070-1087, </pages> <year> 1991. </year>
Reference-contexts: This case study appears in [17] and is summarized here. The linear systems used are based upon ideas related to signal processing [47, 99] and more specifically to an example in <ref> [2, Equation (8)] </ref>.
Reference: [3] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and I.C. Verkamo. </author> <title> Fast discovery of association rules. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in knowledge Discovery and Data Mining, </booktitle> <pages> pages 307 - 328. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: Some of these techniques are beginning to be scaled to operate on large databases. Examples in classification (Section 3.1) include decision trees [81], in summarization (Section 3.3) association rules <ref> [3] </ref>, and in clustering [107] 3.1 Predictive Modeling The goal is to predict some field (s) in a database based on other fields. If the field being predicted is a numeric (continuous) variable (such as a physical measurement of e.g. height) then the prediction problem is a regression problem. <p> This class of methods is distinguished from the above in that rather than predicting a specified field (e.g. classification) or grouping cases together (e.g. clustering) the goal is to find relations between fields. One 6 common method is called association rules <ref> [3] </ref>. Associations are rules that state that certain combinations of values occur with other combinations of values with a certain frequency and certainty. A common application of this is market basket analysis were one would like to summarize which products are bought with what other products.
Reference: [4] <author> M. R. Anderberg. </author> <title> Cluster Analysis for Applications. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year> <month> 27 </month>
Reference-contexts: This property makes the centers computed by the k-Mean algorithm less robust to outliers in the data. We note that k-Mean convergence has been shown in [92] and a discussion relating to the convergence proof is presented in <ref> [4] </ref>. We note here that clustering can also be done using density estimation. Given k clusters, one can associate a probability density function (model) with each of the clusters. The problem then reduces to estimating the parameters of each model.
Reference: [5] <author> B. Bank, J. Guddat, D. Klatte, B. Kummer, and K. </author> <title> Tammer. Nonlinear Parametric Optimization. </title> <address> Akamie-Verlag, Berlin, </address> <year> 1982. </year>
Reference-contexts: Further study of the problem and other approaches constitute important research areas. 5. Effect of data perturbation on derived models. Perturbation of mathematical programs is a widely studied area, for example <ref> [89, 88, 5, 38, 64] </ref>. Applying these results to specific data mining problems where the data is constantly being augmented and revised without having to rebuild the mathematical programming model would be a valuable contribution. 6. Modeling noise.
Reference: [6] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illinois, </address> <year> 1992. </year>
Reference-contexts: 12 A related linear programming formulation of problem (18) is the following: minimize w;fl;y;z k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks [69, 8, 11, 16], decision tree construction <ref> [6, 11, 69, 7] </ref> and calculating nonlinear discriminants by nonlinearly transforming the given data [102, 23, 84]. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate has a continuous output in contrast to <p> Local versus global methods. Because of size considerations we often do not want to model the whole data set. We therefore need to pre-partition the data and model it locally. One way to achieve this is via a decision tree approach <ref> [6, 82] </ref> in which the data is partitioned into local regions and modeled individually. A regression approach to this is given by Regression trees in CART [21]. Another example is to cluster the data first, then model each cluster locally. 9.
Reference: [7] <author> K. P. Bennett and J. A. </author> <title> Blue. A support vector machine approach to decision trees. </title> <institution> Department of Mathematical Sciences Math Report No. 97-100, Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1997. </year> <note> Available at http://www.math.rpi.edu/ bennek/. </note>
Reference-contexts: 12 A related linear programming formulation of problem (18) is the following: minimize w;fl;y;z k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks [69, 8, 11, 16], decision tree construction <ref> [6, 11, 69, 7] </ref> and calculating nonlinear discriminants by nonlinearly transforming the given data [102, 23, 84]. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate has a continuous output in contrast to
Reference: [8] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher>
Reference-contexts: The formulation (5) is equivalent to the following robust linear programming formulation (RLP) proposed in <ref> [8] </ref> and effectively used to solve problems from real-world domains [77]: min m e T z jAw + efl + e 5 y; Bw efl + e 5 z; y = 0; z = 0 : (6) The linear program (6) or, equivalently, the formulation (5) define a separating plane P <p> the generalization error of the classifier [84]. 12 A related linear programming formulation of problem (18) is the following: minimize w;fl;y;z k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks <ref> [69, 8, 11, 16] </ref>, decision tree construction [6, 11, 69, 7] and calculating nonlinear discriminants by nonlinearly transforming the given data [102, 23, 84]. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate
Reference: [9] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: Two principal reasons for choosing the 1-norm in (5) are: (i) Problem (5) is then reducible to a linear program (6) with many important theoretical properties making it an effective computational tool <ref> [9] </ref>. (ii) The 1-norm is less sensitive to outliers such as those occurring when the underlying data distributions have pronounced tails, hence (5) has a similar effect to that of robust regression [53],[50, pp 82-87]. <p> We further note that the solution w = 0 occurs if and only if e T A = k in which case the solution w = 0 is not unique <ref> [9, Theorems 2.5 and 2.6] </ref>. Hence a useful plane P (2) is always generated by the robust linear program (6). The linear programming formulation (6) obtains an approximate separating plane that minimizes a weighted sum of the distances of misclassified points to the approximate separating plane. <p> Visualization and understandability of derived models. Presenting a model visually enhances its utility and increases the chances of its correct usage. Many mathematical programming models possess this property. For example the robust linear separator of 26 <ref> [9] </ref> has a very simple geometric representation as a plane separating most or all the elements of two sets, which has been used to advantage in medical applications [78, 77]. <p> Allowing a variable weight in the interval [0; 1] for example would introduce a scaling of the problem that could enhance separation either by a linear or nonlinear separating approach <ref> [67, 68, 9] </ref>. 7 Concluding Remarks KDD holds the promise of an enabling technology that could unlock the knowledge lying dormant in huge databases.
Reference: [10] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization & Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: We further note that the constraints of (39) are uncoupled in the variables (c; d) and the variable t. Hence the Uncoupled Bilinear Programming Algorithm <ref> [10, Algorithm 2.1] </ref> is applicable. This algorithm alternates between solving a linear program in the variable t and a linear program in the variables (c; d) and terminates finitely at a point satisfying the minimum principle necessary optimality condition for problem (39) [10, Theorem 2.1]. <p> Hence the Uncoupled Bilinear Programming Algorithm [10, Algorithm 2.1] is applicable. This algorithm alternates between solving a linear program in the variable t and a linear program in the variables (c; d) and terminates finitely at a point satisfying the minimum principle necessary optimality condition for problem (39) <ref> [10, Theorem 2.1] </ref>. Due to the simple structure of (39), the two linear programs can be solved explicitly in closed form. This leads to the following algorithm. Algorithm 4.4 k-Median Algorithm.
Reference: [11] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Multicategory separation via linear programming. </title> <journal> Optimization Methods and Software, </journal> <volume> 3 </volume> <pages> 27-39, </pages> <year> 1993. </year>
Reference-contexts: the generalization error of the classifier [84]. 12 A related linear programming formulation of problem (18) is the following: minimize w;fl;y;z k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks <ref> [69, 8, 11, 16] </ref>, decision tree construction [6, 11, 69, 7] and calculating nonlinear discriminants by nonlinearly transforming the given data [102, 23, 84]. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate <p> 12 A related linear programming formulation of problem (18) is the following: minimize w;fl;y;z k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks [69, 8, 11, 16], decision tree construction <ref> [6, 11, 69, 7] </ref> and calculating nonlinear discriminants by nonlinearly transforming the given data [102, 23, 84]. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate has a continuous output in contrast to
Reference: [12] <author> D. P. Bertsekas. </author> <title> Nonlinear Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: Approximate solutions. (a) The problem of identifying constraints which are inactive and will remain inactive as optimization proceeds is an important and interesting practical problem. Computational algorithms towards this goal have appeared in <ref> [24, 42, 106, 12] </ref>. A possible simple approach for this problem is the use of the classical exterior quadratic penalty function [41] and dropping constraints for which the product of the penalty parameter (which is increasing to infinity) times the constraint violation is less than some tolerance.
Reference: [13] <author> C. M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: In general, it should be noted that the problem trading off the simplicity of a model with how well it fits the training data is a well-studied problem. In statistics this is known as the bias-variance tradeoff [44], in Bayesian inference it is known as penalized likelihood <ref> [13, 51] </ref>, and in pattern recognition/machine learning it manifests itself as the minimum message length (MML) [105] problem. <p> The strength of the parametric approach comes from the ability to quickly compute p (x) for any given x. In contrast nonparametric models allow very general forms of the estimate p (x) but suffer in that the number of model variables grows directly with the number of data points <ref> [13] </ref>. We consider a semi-parametric approach to density estimation incorporating advantages of both the parametric and nonparametric models. The semi-parametric approach considered here is the mixture model. <p> Here, our estimate p (x) of true PDF is a linear combination of k "basis functions", where k is a parameter of the model typically much less than the number of data points M . Aside from a slight change of notation, this section follows pages 60-67 of <ref> [13] </ref>. We estimate the PDF by: p (x) = `=1 17 P (`) is the prior probability of the data point having been generated by component ` of the mixture and p (xj`) are the conditional densities.
Reference: [14] <author> S. Bos. </author> <title> A realizable learning task which shows overfitting. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 218-224, </pages> <address> Cambridge, MA, 1996. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Overtraining can also lead to poor generalization even when the complexity of the function class from which ^g is constructed is optimal <ref> [14] </ref>. The key to good generalization is correctly estimating the complexity of the true mapping g while avoiding overtraining. This problem is compounded by the fact that we have only a finite sampling of g, which, in addition, may be corrupted by noise.
Reference: [15] <author> R. Brachman, T. Khabaza, W. Kloesgen, G. Piatetsky-Shapiro, and E. Simoudis. </author> <title> Industrial applications of data mining and knowledge discovery. </title> <journal> Communications of ACM, </journal> <volume> 39(11), </volume> <year> 1996. </year>
Reference-contexts: For examples of success stories in applications in industry see <ref> [15] </ref> and in science analysis see [34]. More detailed case studies are found in [36]. Driving the growth of this field are strong forces (both economic and social) that are a product of the data overload phenomenon.
Reference: [16] <author> P. S. Bradley and O. L. Mangasarian. </author> <title> Parsimonious side propagation. </title> <type> Technical Report 97-11, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> October </month> <year> 1997. </year> <title> ICASSP98: </title> <booktitle> International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Seattle May 12-15, </address> <year> 1998, </year> <note> submitted. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-03.ps.Z. </note>
Reference-contexts: the generalization error of the classifier [84]. 12 A related linear programming formulation of problem (18) is the following: minimize w;fl;y;z k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks <ref> [69, 8, 11, 16] </ref>, decision tree construction [6, 11, 69, 7] and calculating nonlinear discriminants by nonlinearly transforming the given data [102, 23, 84]. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate
Reference: [17] <author> P. S. Bradley, O. L. Mangasarian, and J. B. Rosen. </author> <title> Parsimonious least norm approximation. </title> <type> Technical Report 97-03, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> March </month> <year> 1997. </year> <note> Computational Optimization and Applications, to appear. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-03.ps.Z. </note>
Reference-contexts: We first consider the following Parsimonious Least Norm Approximation (PLNA) which "chooses" the fewest elements of F such that ^g approximates our sampling of the true regression function <ref> [17] </ref>: min (1 )kAw bk 1 + (e T jwj fl ); 2 [0; 1): (26) The value of is chosen to maximize a measure of generalization, estimated by cross-validation [96], for instance. <p> With this approximation (26) becomes the following concave minimization over a polyhedral set: minimize w;y;v subject to y Aw b y; (28) This problem is effectively solved by the successive linearization algorithm <ref> [17, Algorithm 3.1] </ref> which terminates finitely at a point satisfying the minimum principle necessary optimality condition for (28). For a sufficiently large, but finite value of the parameter ff in the negative exponential, a solution of (28) is in fact a solution of (27) [17, Theorem 2.1]. <p> For a sufficiently large, but finite value of the parameter ff in the negative exponential, a solution of (28) is in fact a solution of (27) <ref> [17, Theorem 2.1] </ref>. <p> The Least Least Norm Approximation (LLNA) method minimizes the 1-norm residual plus a 1-norm penalty on the size of the coefficient vector w <ref> [17] </ref> whose solution involves solving a single linear program: min (1 )kAw bk 1 + kwk 1 : (29) The Method-of-Frames De-Noising [27] refers to the minimization of the least square fit error plus a 2-norm penalization term on the coefficient vector: 14 min (1 ) kAw bk 2 kwk 2 <p> This case study appears in <ref> [17] </ref> and is summarized here. The linear systems used are based upon ideas related to signal processing [47, 99] and more specifically to an example in [2, Equation (8)]. <p> This approach, which leads to minimization of a concave function on (often) a polyhedral set [72], has been used very effectively for feature selection [19] and noise suppression <ref> [17] </ref>. Although the polyhedral concave minimization problem is a difficult problem, fast and finite algorithms for finding very useful stationary points make it a very effective tool for data reduction. Further study of the problem and other approaches constitute important research areas. 5. Effect of data perturbation on derived models.
Reference: [18] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Clustering via concave minimization. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems -9-, </booktitle> <pages> pages 368-374, </pages> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-03.ps.Z. </publisher>
Reference-contexts: Hence this is a difficult optimization problem. We propose simplifying this problem slightly by introducing a "selection" variable t i` justified by the following simple lemma <ref> [18, Lemma 3.1] </ref>. Lemma 4.3 Let a 2 R k . <p> We focus our attention on solving (38) with the 1-norm distance for which problem (38) can be written as the following bilinear program <ref> [18] </ref>: minimize c;d;t i=1 `=1 t i` (e T d i` ) subject to d i` x i c ` d i` ; i = 1; : : : ; m; ` = 1; : : : ; k; `=1 t i` = 1; t i` 0; i = 1; :
Reference: [19] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Feature selection via mathematical programming. </title> <journal> INFORMS Journal on Computing, </journal> <note> 1998. To appear. Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-21.ps.Z. </note>
Reference-contexts: We note that the step function can be approximated using Lemma 4.1 with the resulting problem being an LPEC. For a further discussion of this LPEC formulation as well as a formulation that approximates the step function by a sigmoid, see <ref> [19] </ref>. <p> Even though it is difficult to find a global solution to this problem, a fast successive linear approximation (SLA) algorithm <ref> [19, Algorithm 2.1] </ref> terminates at a stationary point which satisfies the minimum principle necessary optimality condition for problem (17) [19, Theorem 2.2] and leads to a sparse w with good generalization properties. 4.3 Predictive Modeling: Support Vector Machines The previous discussion of feature selection naturally leads to a strongly related framework: <p> Even though it is difficult to find a global solution to this problem, a fast successive linear approximation (SLA) algorithm [19, Algorithm 2.1] terminates at a stationary point which satisfies the minimum principle necessary optimality condition for problem (17) <ref> [19, Theorem 2.2] </ref> and leads to a sparse w with good generalization properties. 4.3 Predictive Modeling: Support Vector Machines The previous discussion of feature selection naturally leads to a strongly related framework: the support vector machine (SVM) [102, 91, 84]. <p> This approach, which leads to minimization of a concave function on (often) a polyhedral set [72], has been used very effectively for feature selection <ref> [19] </ref> and noise suppression [17]. Although the polyhedral concave minimization problem is a difficult problem, fast and finite algorithms for finding very useful stationary points make it a very effective tool for data reduction. Further study of the problem and other approaches constitute important research areas. 5.
Reference: [20] <author> E. J. Bredensteiner and K. P. Bennett. </author> <title> Feature minimization within decision trees. </title> <note> Department of Mathematical Sciences Math Report No. 218, </note> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1995. </year> <title> Computational Optimizations and Applications. </title> <note> to appear. </note>
Reference-contexts: Having a minimal number of features often leads to better generalization and simpler models that can be more easily interpreted. This problem is also addressed by statistical [45, 61], machine learning [57, 62] as well as other mathematical programming formulations <ref> [20, 72] </ref>. In general, it should be noted that the problem trading off the simplicity of a model with how well it fits the training data is a well-studied problem.
Reference: [21] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Monterey, CA, </address> <year> 1984. </year> <month> 28 </month>
Reference-contexts: We therefore need to pre-partition the data and model it locally. One way to achieve this is via a decision tree approach [6, 82] in which the data is partitioned into local regions and modeled individually. A regression approach to this is given by Regression trees in CART <ref> [21] </ref>. Another example is to cluster the data first, then model each cluster locally. 9. Modeling rare events (e.g. low probability items like fraud or rare diseases).
Reference: [22] <author> A. Brooke, D. Kendrick, and A. Meeraus. </author> <title> GAMS: A User's Guide. </title> <publisher> The Scientific Press, </publisher> <address> South San Francisco, CA, </address> <year> 1988. </year>
Reference-contexts: A small or zero value of this product is indicative of a small multiplier and hence an inactive constraint. 3. Mathematical programming languages geared for data mining problems. Although there are languages specifically designed for setting up and solving mathematical programs, such as AMPL [43] and GAMS <ref> [22, 55] </ref>, and languages that can be easily adapted for the same purpose, such as MATLAB [80], no specific language has been designed for mathematical programming applications to data mining. Such a language could make such applications more widely accepted by the data mining community. 4. Data reduction techniques.
Reference: [23] <author> C. J. C. Burges. </author> <title> A tutorial on support vector machines for pattern recognition. </title> <type> Technical report, </type> <institution> Bell Laboratories, Lucent Technologies, </institution> <address> Holmdel, NJ 07733-3030, </address> <month> November </month> <year> 1997. </year>
Reference-contexts: The SVM can be seen as a way of determining the relevant parameters in polynomial, neural network, or radial basis function classifiers. While most approaches are based on the idea of minimizing an error in separating the given data (i.e. minimizing training set error), SVMs incorporate structured risk minimization <ref> [102, 23] </ref> which minimizes an upper bound on the generalization error. For a more detailed discussion of SVMs, see [102, 23, 104]. Consider the simple case when the sets A and B are linearly separable. <p> While most approaches are based on the idea of minimizing an error in separating the given data (i.e. minimizing training set error), SVMs incorporate structured risk minimization [102, 23] which minimizes an upper bound on the generalization error. For a more detailed discussion of SVMs, see <ref> [102, 23, 104] </ref>. Consider the simple case when the sets A and B are linearly separable. The idea is to determine, among the infinite number of planes correctly separating A from B, the one which will have smallest generalization error. <p> k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks [69, 8, 11, 16], decision tree construction [6, 11, 69, 7] and calculating nonlinear discriminants by nonlinearly transforming the given data <ref> [102, 23, 84] </ref>. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate has a continuous output in contrast to a discrete output for the classification problem.
Reference: [24] <author> J. V. Burke and J. J. </author> <title> More. On the identification of active constraints. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 25(5) </volume> <pages> 1197-1211, </pages> <year> 1988. </year>
Reference-contexts: Approximate solutions. (a) The problem of identifying constraints which are inactive and will remain inactive as optimization proceeds is an important and interesting practical problem. Computational algorithms towards this goal have appeared in <ref> [24, 42, 106, 12] </ref>. A possible simple approach for this problem is the use of the classical exterior quadratic penalty function [41] and dropping constraints for which the product of the penalty parameter (which is increasing to infinity) times the constraint violation is less than some tolerance.
Reference: [25] <author> C. L. Carter, C. Allen, and D. E. Henson. </author> <title> Relation of tumor size, lymph node status, and survival in 24,740 breast cancer cases. </title> <journal> Cancer, </journal> <volume> 63 </volume> <pages> 181-187, </pages> <year> 1989. </year> <title> SEER: Surveillence, Epidemiology and End Results Program of the National Cancer Institute. </title>
Reference-contexts: These two features were then normalized to have mean = 0 and standard deviation = 1. This dataset then consists of 194 points in R 2 . The k-Median and k-Mean algorithms were also applied to the SEER database <ref> [25] </ref> consisting of the two features of tumor size and nodes positive for 21,960 instances. The k-Median and k-Mean Algorithms were applied to both datasets with k = 3.
Reference: [26] <author> Chunhui Chen and O. L. Mangasarian. </author> <title> Hybrid misclassification minimization. </title> <booktitle> Advances in Computational Mathematics, </booktitle> <volume> 5(2) </volume> <pages> 127-136, </pages> <year> 1996. </year> <note> Advances in Computational Mathematics, to appear. Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-05.ps.Z. </note>
Reference-contexts: These properties and an algorithm for solving (12) are given in [70]. We note that a hybrid algorithm is proposed in <ref> [26] </ref> addressing the misclassification minimization problem (9) which performs the following two steps at each iteration. The first step consists of determining w 2 R n by solving the linear program (6) for a fixed value of fl 2 R n . <p> The new fl is then determined as the one that minimizes the number of points misclassified by the separating plane P , with w determined in the previous step. The Hybrid Misclassification Minimization Algorithm <ref> [26, Algorithm 5] </ref> terminates when the number of misclassifications is not decreased.
Reference: [27] <author> S. S. Chen, D. L. Donoho, and M. A. Saunders. </author> <title> Atomic decomposition by basis pursuit. </title> <type> Technical Report 479, </type> <institution> Department of Statistics, Stanford University, Stanford, </institution> <address> California 94305, </address> <month> February </month> <year> 1999. </year> <note> Available by http://playfair.stanford.EDU/reports/chen s/BasisPursuit.ps.Z. </note>
Reference-contexts: We relax the notion that elements of the function set be linearly independent. Hence the solution to the problem is not unique and from all possible solutions, we wish to find one which performs best, or generalizes, to new data. For instance, the set of functions discussed in <ref> [27] </ref> consist of an "overcomplete waveform dictionary stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, : : : ". Notice that by choice of the function set, linear regression and polynomial regression are easily cast in this framework. <p> The method of frames [30] finds a solution with minimal 2-norm: minimize w subject to Aw = b: (22) For the same, noise-free situation, the Basis Pursuit method <ref> [27] </ref> finds a solution with minimal 1-norm: minimize w subject to Aw = b: (23) Consider a noisy RHS vector b in which case an exact solution to the system Aw = b may not exist. <p> The Least Least Norm Approximation (LLNA) method minimizes the 1-norm residual plus a 1-norm penalty on the size of the coefficient vector w [17] whose solution involves solving a single linear program: min (1 )kAw bk 1 + kwk 1 : (29) The Method-of-Frames De-Noising <ref> [27] </ref> refers to the minimization of the least square fit error plus a 2-norm penalization term on the coefficient vector: 14 min (1 ) kAw bk 2 kwk 2 Similarly, the Basis Pursuit De-Noising [27] refers to a solution of: min (1 ) kAw bk 2 The Support Vector Method has <p> single linear program: min (1 )kAw bk 1 + kwk 1 : (29) The Method-of-Frames De-Noising <ref> [27] </ref> refers to the minimization of the least square fit error plus a 2-norm penalization term on the coefficient vector: 14 min (1 ) kAw bk 2 kwk 2 Similarly, the Basis Pursuit De-Noising [27] refers to a solution of: min (1 ) kAw bk 2 The Support Vector Method has also been extended to the regression problem [32, 101].
Reference: [28] <author> E.F. Codd. </author> <title> Providing olap (on-line analytical processing) to user-analysts: An it mandate. </title> <type> Technical report, </type> <institution> E.F. Codd and Associates, </institution> <year> 1993. </year>
Reference-contexts: Data warehousing is the first step in transforming a database system from a system whose primary purpose is reliable storage to one whose primary use is decision support. A closely related area is called On-Line Analytical Processing (OLAP) named after principles first advocated by Codd <ref> [28] </ref>. The current emphasis of OLAP systems is on supporting query-driven exploration of the data warehouse. Part of this entails precomputing aggregates along data "dimensions" in the multi-dimensional data store.
Reference: [29] <author> F. Cordellier and J. Ch. Fiorot. </author> <title> On the Fermat-Weber problem with convex cost functionals. </title> <journal> Mathematical Programming, </journal> <volume> 14 </volume> <pages> 295-311, </pages> <year> 1978. </year>
Reference-contexts: Stop when c `;j = c `;j+1 ; ` = 1; : : : ; k. If the 2-norm (not 2-norm squared) is used in the objective of (38), the cluster center update subproblem becomes the considerably harder Weber problem <ref> [29, 85] </ref> which locates a center in R n closest in sum of Euclidean distances to a finite set of given points.
Reference: [30] <author> I. Daubechies. </author> <title> Time-frequency localization operators: A geometric phase space approach. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 34 </volume> <pages> 605-612, </pages> <year> 1988. </year>
Reference-contexts: Consider the simplest case where there is no noise in the RHS vector b (i.e. g can be sampled precisely), and the set of function in F gives rise to a matrix A for which exact solutions to Aw = b exist. The method of frames <ref> [30] </ref> finds a solution with minimal 2-norm: minimize w subject to Aw = b: (22) For the same, noise-free situation, the Basis Pursuit method [27] finds a solution with minimal 1-norm: minimize w subject to Aw = b: (23) Consider a noisy RHS vector b in which case an exact solution
Reference: [31] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the em algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The Expectation-Maximization (EM) algorithm <ref> [31] </ref> iteratively minimizes (48) for P (`); ` = 1; : : : ; k for fixed values of ` and ` , then minizizes (48) for ` ; ` ; ` = 1; : : : ; k for fixed P (`).
Reference: [32] <author> H. Drucker, C. J. C Burges, L. Kaugman, A. Smola, and V. Vapnik. </author> <title> Support vector regression machines. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. </publisher>
Reference-contexts: square fit error plus a 2-norm penalization term on the coefficient vector: 14 min (1 ) kAw bk 2 kwk 2 Similarly, the Basis Pursuit De-Noising [27] refers to a solution of: min (1 ) kAw bk 2 The Support Vector Method has also been extended to the regression problem <ref> [32, 101] </ref>.
Reference: [33] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: However, this joint density is rarely known and very difficult to estimate. Hence one has to resort to various techniques for estimating. These techniques include: 1. Density estimation, e.g. kernel density estimators <ref> [33] </ref> or graphical representations of the joint density [51]. 2. Metric-space based methods: define a distance measure on data points and guess the class value based on proximity to data points in the training set. For example, the K-nearest-neighbor method [33]. 3. <p> Density estimation, e.g. kernel density estimators <ref> [33] </ref> or graphical representations of the joint density [51]. 2. Metric-space based methods: define a distance measure on data points and guess the class value based on proximity to data points in the training set. For example, the K-nearest-neighbor method [33]. 3. Projection into decision regions: divide the attribute space into decision regions and associate a prediction with each. For example linear discriminant analysis finds linear separators, while decision tree or rule-based classifiers make a piecewise constant approximation of the decision surface.
Reference: [34] <author> U. Fayyad, D. Haussler, and P. Stolorz. </author> <title> Mining science data. </title> <journal> Communications of ACM, </journal> <volume> 39(11), </volume> <year> 1996. </year>
Reference-contexts: For examples of success stories in applications in industry see [15] and in science analysis see <ref> [34] </ref>. More detailed case studies are found in [36]. Driving the growth of this field are strong forces (both economic and social) that are a product of the data overload phenomenon.
Reference: [35] <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From data mining to knowledge discovery: An overview. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in knowledge Discovery and Data Mining, </booktitle> <pages> pages 1 - 36. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: In this paper, as in <ref> [35] </ref>, we draw a distinction between the latter, which we call KDD, and "data mining". The term data mining has been mostly used by statisticians, data analysts, and the database communities. <p> We have omitted arrows illustrating these potential iterations to keep the figure simple. 2.1 Basic Definitions We adopt the definitions of KDD and Data mining provided in <ref> [35] </ref> as follows: Knowledge Discovery in Databases: is the process of identifying valid, novel, potentially useful, and ultimately understandable structure in data. Here data is a set of facts (cases in a database) and structure refers to either patterns or models.
Reference: [36] <editor> U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: However, the bulk of applications in this area have been achieved using statistical pattern recognition, machine learning, and database approaches. We do not cover those application case studies in this paper. Applications are covered in <ref> [36] </ref>, and in the special issue of Communications of the ACM [1] 1 . <p> For examples of success stories in applications in industry see [15] and in science analysis see [34]. More detailed case studies are found in <ref> [36] </ref>. Driving the growth of this field are strong forces (both economic and social) that are a product of the data overload phenomenon. We view the need to deliver workable solutions to pressing problems as a very healthy pressure on the KDD field.
Reference: [37] <author> U.M. Fayyad, S.G. Djorgovski, and N. Weir. </author> <title> Application of classification and clustering to sky survey cataloging and analysis. </title> <editor> In E. Wegman and S. Azen, editors, </editor> <booktitle> Computing Science and Statistics, </booktitle> <volume> volume 29(2), </volume> <pages> pages 178 - 186, </pages> <address> Fairfax, VA, USA, </address> <year> 1997. </year> <title> Interface Foundation of North America. </title>
Reference-contexts: For example the use of the infinity norm as originally proposed in [67, 68] can be used for capturing such rare events. In general, objects that occurr with a very low frequency (e.g. in fraud detection applications or in detecting quasars in sky surveys in astronomy <ref> [37] </ref>) are likely to be dismissed as insignificant outliers or simply disappear in L2-norm based methods and standard principal component analysis. 10. Going beyond L-norms. Current approaches utilize all dimensions of a problem with equal weight or use feature selection to weight each dimension with a 0 or 1 weight.
Reference: [38] <author> M. C. Ferris and O. L. Mangasarian. </author> <title> Finite perturbation of convex programs. </title> <journal> Applied Mathematics and Optimization, </journal> <volume> 23 </volume> <pages> 263-273, </pages> <year> 1991. </year>
Reference-contexts: Further study of the problem and other approaches constitute important research areas. 5. Effect of data perturbation on derived models. Perturbation of mathematical programs is a widely studied area, for example <ref> [89, 88, 5, 38, 64] </ref>. Applying these results to specific data mining problems where the data is constantly being augmented and revised without having to rebuild the mathematical programming model would be a valuable contribution. 6. Modeling noise.
Reference: [39] <author> M. C. Ferris and O. L. Mangasarian. </author> <title> Parallel constraint distribution. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 487-500, </pages> <year> 1991. </year>
Reference-contexts: Scaling to large sets of constraints and variables. 25 (a) Decomposing constraints and variables into subsets. The parallel constraint distribution approach of <ref> [39] </ref> is one way of decomposing problem constraints among parallel processors or parallel virtual machines (PVM) [46, 65], while the parallel variable distribution of [40] can be similarly applied when dealing with databases with a very large number of attributes. (b) Algorithms for dealing with constraints and variables sequentially.
Reference: [40] <author> M. C. Ferris and O. L. Mangasarian. </author> <title> Parallel variable distribution. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4(4) </volume> <pages> 815-832, </pages> <year> 1994. </year>
Reference-contexts: Scaling to large sets of constraints and variables. 25 (a) Decomposing constraints and variables into subsets. The parallel constraint distribution approach of [39] is one way of decomposing problem constraints among parallel processors or parallel virtual machines (PVM) [46, 65], while the parallel variable distribution of <ref> [40] </ref> can be similarly applied when dealing with databases with a very large number of attributes. (b) Algorithms for dealing with constraints and variables sequentially. Mathematical programming approaches that deal with problem data in a sequential manner are very useful for processing large databases.
Reference: [41] <author> A. V. Fiacco and G. P. McCormick. </author> <title> Nonlinear Programming: Sequential Unconstrained Minimization Techniques. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: Computational algorithms towards this goal have appeared in [24, 42, 106, 12]. A possible simple approach for this problem is the use of the classical exterior quadratic penalty function <ref> [41] </ref> and dropping constraints for which the product of the penalty parameter (which is increasing to infinity) times the constraint violation is less than some tolerance. This product approximates the Lagrange multiplier for the constraint.
Reference: [42] <author> S. D. Flam. </author> <title> On finite convergence and constraint identification of subgradient projection methods. </title> <journal> Mathematical Programming, </journal> <volume> 57 </volume> <pages> 427-437, </pages> <year> 1992. </year>
Reference-contexts: Approximate solutions. (a) The problem of identifying constraints which are inactive and will remain inactive as optimization proceeds is an important and interesting practical problem. Computational algorithms towards this goal have appeared in <ref> [24, 42, 106, 12] </ref>. A possible simple approach for this problem is the use of the classical exterior quadratic penalty function [41] and dropping constraints for which the product of the penalty parameter (which is increasing to infinity) times the constraint violation is less than some tolerance.
Reference: [43] <author> R. Fourer, D. Gay, and B. Kernighan. </author> <title> AMPL. </title> <publisher> The Scientific Press, </publisher> <address> South San Francisco, California, </address> <year> 1993. </year>
Reference-contexts: A small or zero value of this product is indicative of a small multiplier and hence an inactive constraint. 3. Mathematical programming languages geared for data mining problems. Although there are languages specifically designed for setting up and solving mathematical programs, such as AMPL <ref> [43] </ref> and GAMS [22, 55], and languages that can be easily adapted for the same purpose, such as MATLAB [80], no specific language has been designed for mathematical programming applications to data mining. Such a language could make such applications more widely accepted by the data mining community. 4.
Reference: [44] <author> J. Friedman. </author> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery, </title> <type> 1(1), </type> <year> 1997. </year>
Reference-contexts: In general, it should be noted that the problem trading off the simplicity of a model with how well it fits the training data is a well-studied problem. In statistics this is known as the bias-variance tradeoff <ref> [44] </ref>, in Bayesian inference it is known as penalized likelihood [13, 51], and in pattern recognition/machine learning it manifests itself as the minimum message length (MML) [105] problem.
Reference: [45] <author> K. Fukunaga. </author> <title> Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: Having a minimal number of features often leads to better generalization and simpler models that can be more easily interpreted. This problem is also addressed by statistical <ref> [45, 61] </ref>, machine learning [57, 62] as well as other mathematical programming formulations [20, 72]. In general, it should be noted that the problem trading off the simplicity of a model with how well it fits the training data is a well-studied problem.
Reference: [46] <author> A. Geist, A. Beguelin, Dongarra J, W. Jiang, R. Mancheck, and V. Sunderam. </author> <title> PVM Parallel Virtual Machine. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Scaling to large sets of constraints and variables. 25 (a) Decomposing constraints and variables into subsets. The parallel constraint distribution approach of [39] is one way of decomposing problem constraints among parallel processors or parallel virtual machines (PVM) <ref> [46, 65] </ref>, while the parallel variable distribution of [40] can be similarly applied when dealing with databases with a very large number of attributes. (b) Algorithms for dealing with constraints and variables sequentially.
Reference: [47] <author> Arthur A. Giordano. </author> <title> Least Square Estimation With Applications to Digital Signal Processing. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: This case study appears in [17] and is summarized here. The linear systems used are based upon ideas related to signal processing <ref> [47, 99] </ref> and more specifically to an example in [2, Equation (8)].
Reference: [48] <author> C. Glymour, R. Scheines, and P. Spirtes ABD K. Kelly. </author> <title> Discovering Causal Structure. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Density estimation methods in general fall under this category, so do methods for explicit causal modeling (e.g. <ref> [48] </ref> and [51]). 3.5 Change and Deviation Detection These methods account for sequence information, be it time-series or some other ordering (e.g. protein sequencing in genome mapping). The distinguishing feature of this class of methods is that ordering of observations is important and must be accounted for.
Reference: [49] <author> J. Gray, S. Chaudhuri, A. Bosworth, A. Layman, D. Reichart, M. Venkatrao, F. Pellow, and H. Pira-hesh. </author> <title> Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub totals. Data Mining and Knowledge Discovery, </title> <type> 1(1), </type> <year> 1997. </year>
Reference-contexts: Enhance database management systems to support new primitives for the efficient extraction of necessary sufficient statistics as well as more efficient sampling schemes. This includes providing SQL support for new primitives that may be needed (e.g. <ref> [49] </ref>). Sufficient statistics are properties of the data that, from the perspective of the mining algorithm, eliminate the need for the data. Examples of sufficient statistics include histograms, counts, and sometimes data samples. 10. Scale methods to parallel databases with hundreds of tables, thousands of fields, and terabytes of data.
Reference: [50] <author> M. H. Hassoun. </author> <title> Fundamentals of Artificial Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference: [51] <author> D. Heckerman. </author> <title> Bayesian networks for data mining. Data Mining and Knowledge Discovery, </title> <type> 1(1), </type> <year> 1997. </year>
Reference-contexts: However, this joint density is rarely known and very difficult to estimate. Hence one has to resort to various techniques for estimating. These techniques include: 1. Density estimation, e.g. kernel density estimators [33] or graphical representations of the joint density <ref> [51] </ref>. 2. Metric-space based methods: define a distance measure on data points and guess the class value based on proximity to data points in the training set. For example, the K-nearest-neighbor method [33]. 3. <p> Density estimation methods in general fall under this category, so do methods for explicit causal modeling (e.g. [48] and <ref> [51] </ref>). 3.5 Change and Deviation Detection These methods account for sequence information, be it time-series or some other ordering (e.g. protein sequencing in genome mapping). The distinguishing feature of this class of methods is that ordering of observations is important and must be accounted for. <p> In general, it should be noted that the problem trading off the simplicity of a model with how well it fits the training data is a well-studied problem. In statistics this is known as the bias-variance tradeoff [44], in Bayesian inference it is known as penalized likelihood <ref> [13, 51] </ref>, and in pattern recognition/machine learning it manifests itself as the minimum message length (MML) [105] problem.
Reference: [52] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: Other methods for addressing the regression problem, which are also optimization problems, include neural networks <ref> [52] </ref> and smoothing splines [103]. We present mathematical programming formulations that attempt to estimate the true, unknown regression function g by a linear combination of functions from some pre-defined function set. We relax the notion that elements of the function set be linearly independent.
Reference: [53] <author> P. J. Huber. </author> <title> Robust Statistics. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: [54] <author> S. Van Huffel and J. Vandewalle. </author> <title> The Total Least Squares Problem, Computational Aspects and Analysis. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1991. </year>
Reference-contexts: Then the least squares solution to (21) is obtained by solving the following problem <ref> [54] </ref>: min kAw bk 2 : (24) If the elements of b consist of noisy measurements of g and the noise is uncorrelated over samples with zero mean and equal variance, then the estimate computed by solving (24) has the smallest variance in the class of estimation methods satisfying the following <p> and the noise is uncorrelated over samples with zero mean and equal variance, then the estimate computed by solving (24) has the smallest variance in the class of estimation methods satisfying the following two conditions: 1) the estimate is unbiased and, 2) the estimate is a linear function of b <ref> [54] </ref>. We consider the least one norm formulation which may provide an estimate ^g (19) which is more robust to outliers in the case of noise in b.
Reference: [55] <institution> IBM Optimization Subroutine Library. GAMS- The Solver Manuals: OSL. GAMS Development Corporation, </institution> <address> Washington, D.C., </address> <year> 1994. </year>
Reference-contexts: A small or zero value of this product is indicative of a small multiplier and hence an inactive constraint. 3. Mathematical programming languages geared for data mining problems. Although there are languages specifically designed for setting up and solving mathematical programs, such as AMPL [43] and GAMS <ref> [22, 55] </ref>, and languages that can be easily adapted for the same purpose, such as MATLAB [80], no specific language has been designed for mathematical programming applications to data mining. Such a language could make such applications more widely accepted by the data mining community. 4. Data reduction techniques.
Reference: [56] <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Here we are concerned with nonhierarchical clustering approaches in which the number of clusters or groups is fixed a priori. For a description of hierarchical clustering procedures see <ref> [56] </ref>. <p> Stop when c `;j = c `;j+1 ; ` = 1; : : : ; k. If we consider problem (38) with the 2-norm squared, the iterative algorithm is the popular k-Mean approach to clustering <ref> [56] </ref>.
Reference: [57] <author> G. H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Having a minimal number of features often leads to better generalization and simpler models that can be more easily interpreted. This problem is also addressed by statistical [45, 61], machine learning <ref> [57, 62] </ref> as well as other mathematical programming formulations [20, 72]. In general, it should be noted that the problem trading off the simplicity of a model with how well it fits the training data is a well-studied problem.
Reference: [58] <author> E. L. Kaplan and P. Meier. </author> <title> Nonparametric estimation from incomplete observations. </title> <journal> J. Am. Stat. Assoc., </journal> <volume> 53 </volume> <pages> 457-481, </pages> <year> 1958. </year>
Reference-contexts: The k-Median and k-Mean algorithms were also applied to the SEER database [25] consisting of the two features of tumor size and nodes positive for 21,960 instances. The k-Median and k-Mean Algorithms were applied to both datasets with k = 3. Survival curves <ref> [58] </ref> were then constructed for each cluster, representing expected percent of surviving patients as a function of 4 Available from UCI Repository of Machine Learning Databases, University of California, Irvine.
Reference: [59] <author> M. Kearns, Y. Mansour, and A. Y. Ng. </author> <title> An information-theoretic analysis of hard and soft assignment methods for clustering. </title> <booktitle> In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 282 - 293. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: The relationship between the hard assignment method of the k-Mean algorithm (Algorithm 4.5) and the EM Algorithm can be found in <ref> [59] </ref>. In [83] a more general treatment of EM and the underlying optimization problem can be found. 19 5 Case Studies 5.1 Classification Using Linear Programming The formulation (6) has been successfully used to determine a classification function that objectively performs breast cancer diagnosis [77].
Reference: [60] <editor> J. Kettenring and D. Pregibon, editors. </editor> <title> Statistics and Massive Data Sets, Report to the Committee on Applied and Theoretical Statistics, </title> <address> Washington, D.C., </address> <year> 1996. </year> <institution> National Research Council. </institution> <month> 30 </month>
Reference-contexts: The question of how does the data grow needs to be better understood and tools for coping with it need to be developed. (See articles by P. Huber, by Fayyad & Smyth, and by others in <ref> [60] </ref>) 13. Develop a theory and techniques for assessing significance in the presence of the large. Traditional techniques for assessing statistical significance were designed to handle the small sample case: in the presence of large data sets, these measures lose their intended "filtering" power. 14.
Reference: [61] <author> K. Koller and M. Sahami. </author> <title> Toward optimal feature selection. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: Having a minimal number of features often leads to better generalization and simpler models that can be more easily interpreted. This problem is also addressed by statistical <ref> [45, 61] </ref>, machine learning [57, 62] as well as other mathematical programming formulations [20, 72]. In general, it should be noted that the problem trading off the simplicity of a model with how well it fits the training data is a well-studied problem.
Reference: [62] <author> Y. le Cun, J. S. Denker, and S. A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems II (Denver 1989), </booktitle> <pages> pages 598-605, </pages> <address> San Mateo, California, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Having a minimal number of features often leads to better generalization and simpler models that can be more easily interpreted. This problem is also addressed by statistical [45, 61], machine learning <ref> [57, 62] </ref> as well as other mathematical programming formulations [20, 72]. In general, it should be noted that the problem trading off the simplicity of a model with how well it fits the training data is a well-studied problem.
Reference: [63] <author> Edward E. Leamer. </author> <title> Specification searches: ad hoc inference with nonexperimental data. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: The earliest uses of the term come from statistics and its usage in most settings was negative with connotations of blind exploration of data without a priori hypotheses to be verified. However, notable exceptions can be found. For example, as early as 1978 <ref> [63] </ref>, the term is used in a positive sense in a demonstration of how generalized linear regression can be used to solve problems that are very difficult for humans and the traditional statistical techniques of that time to solve.
Reference: [64] <author> W. Li. </author> <title> Sharp condition constants for feasible and optimal solutions of a perturbed linear program. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 187 </volume> <pages> 15-40, </pages> <year> 1993. </year>
Reference-contexts: Further study of the problem and other approaches constitute important research areas. 5. Effect of data perturbation on derived models. Perturbation of mathematical programs is a widely studied area, for example <ref> [89, 88, 5, 38, 64] </ref>. Applying these results to specific data mining problems where the data is constantly being augmented and revised without having to rebuild the mathematical programming model would be a valuable contribution. 6. Modeling noise.
Reference: [65] <author> M. Litzkow and M. Livny. </author> <title> Experience with the condor distributed batch system. </title> <booktitle> In Proceedings of the IEEE Workshop on Experimental Distributed Systems, </booktitle> <pages> pages 97-101, </pages> <address> Hunstville, AL, October 1990. </address> <publisher> IEEE Compter Society Press. </publisher>
Reference-contexts: Scaling to large sets of constraints and variables. 25 (a) Decomposing constraints and variables into subsets. The parallel constraint distribution approach of [39] is one way of decomposing problem constraints among parallel processors or parallel virtual machines (PVM) <ref> [46, 65] </ref>, while the parallel variable distribution of [40] can be similarly applied when dealing with databases with a very large number of attributes. (b) Algorithms for dealing with constraints and variables sequentially.
Reference: [66] <author> Z.-Q. Luo, J.-S. Pang, and D. Ralph. </author> <title> Mathematical Programs with Equilibrium Constraints. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1996. </year>
Reference-contexts: + Aw efl e) = 0 s T (v Bw + efl e) = 0 u 0 v 0 (12) Since problem (12) is a linear program with equilibrium constraints (LPEC), it is a special case of the more general mathematical program with equilibrium constraints (MPEC) studied in detail in <ref> [66] </ref>. Being linear, (12) is endowed with some features not possessed by the more general MPECs, principally exactness of a penalty formulation without boundedness of the feasible region and without assuming nondegeneracy. These properties and an algorithm for solving (12) are given in [70].
Reference: [67] <author> O. L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: Modeling rare events (e.g. low probability items like fraud or rare diseases). This is an important problem which can be possibly addressed by a careful use of an appropriate norm in the context of a separation problem. For example the use of the infinity norm as originally proposed in <ref> [67, 68] </ref> can be used for capturing such rare events. <p> Allowing a variable weight in the interval [0; 1] for example would introduce a scaling of the problem that could enhance separation either by a linear or nonlinear separating approach <ref> [67, 68, 9] </ref>. 7 Concluding Remarks KDD holds the promise of an enabling technology that could unlock the knowledge lying dormant in huge databases.
Reference: [68] <author> O. L. Mangasarian. </author> <title> Multi-surface method of pattern separation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:801-807, </volume> <year> 1968. </year>
Reference-contexts: For example the robust linear separator of 26 [9] has a very simple geometric representation as a plane separating most or all the elements of two sets, which has been used to advantage in medical applications [78, 77]. Similarly the multisurface separation method <ref> [68, 74] </ref> can be geometrically depicted as placing the sets to be separated into distinct polyhedral compartments. 8. Local versus global methods. Because of size considerations we often do not want to model the whole data set. We therefore need to pre-partition the data and model it locally. <p> Modeling rare events (e.g. low probability items like fraud or rare diseases). This is an important problem which can be possibly addressed by a careful use of an appropriate norm in the context of a separation problem. For example the use of the infinity norm as originally proposed in <ref> [67, 68] </ref> can be used for capturing such rare events. <p> Allowing a variable weight in the interval [0; 1] for example would introduce a scaling of the problem that could enhance separation either by a linear or nonlinear separating approach <ref> [67, 68, 9] </ref>. 7 Concluding Remarks KDD holds the promise of an enabling technology that could unlock the knowledge lying dormant in huge databases.
Reference: [69] <author> O. L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: the generalization error of the classifier [84]. 12 A related linear programming formulation of problem (18) is the following: minimize w;fl;y;z k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks <ref> [69, 8, 11, 16] </ref>, decision tree construction [6, 11, 69, 7] and calculating nonlinear discriminants by nonlinearly transforming the given data [102, 23, 84]. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate <p> 12 A related linear programming formulation of problem (18) is the following: minimize w;fl;y;z k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks [69, 8, 11, 16], decision tree construction <ref> [6, 11, 69, 7] </ref> and calculating nonlinear discriminants by nonlinearly transforming the given data [102, 23, 84]. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate has a continuous output in contrast to
Reference: [70] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: Next, we propose a precise mathematical programming formulation of the nonconvex problem of minimizing the number of such misclassified points <ref> [70] </ref>. This is done by first proposing a simple linear complementarity formulation of the step function (Lemma 4.1) and then using this result to formulate the misclassification minimization problem as a linear program with equilibrium (linear complementarity) constraints (LPEC). <p> fl) = cardinality 8 &gt; &gt; : fi fi fi fi A i w fl 1 &lt; 0; 1 i m; 9 &gt; &gt; ; We note that (9) is always solvable since there exists only a finite number of twofold partitions of A [ B that are linearly separable <ref> [70] </ref>. Any such partition that minimizes the number of points misclassified also minimizes (9). We reduce (9) to an LPEC by first representing the step function () fl as a complementarity condition via the plus function () + , which is a re-statement of [73, Equation 2.11]. <p> ones in R m : r = (a) fl ; u = (a) + , (r; u) 2 arg min fi fi 0 r ? u a 0; We now combine Lemma 4.1 and the minimization problem (9) to obtain the following misclassification minimization characterization which is a re-statement of <ref> [70, Proposition 2.2] </ref>. Proposition 4.2 Misclassification Minimization as a Linear Program with Equilibrium Constraints (LPEC). <p> Being linear, (12) is endowed with some features not possessed by the more general MPECs, principally exactness of a penalty formulation without boundedness of the feasible region and without assuming nondegeneracy. These properties and an algorithm for solving (12) are given in <ref> [70] </ref>. We note that a hybrid algorithm is proposed in [26] addressing the misclassification minimization problem (9) which performs the following two steps at each iteration.
Reference: [71] <author> O. L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1994. </year>
Reference-contexts: In keeping with the notation already introduced, the margin is weighted by 2 [0; 1) and a measure of misclassification error is weighted by (1 ): minimize w;fl;y;z 2 kwk 2 subject to Aw + efl + e y; y 0; z 0: The Wolfe dual <ref> [71] </ref> of this quadratic programming problem is usually solved. Points A i 2 A and B i 2 B corresponding to inequality constraints of (18) with positive dual variables constitute the support vectors of the problem.
Reference: [72] <author> O. L. Mangasarian. </author> <title> Machine learning via polyhedral concave minimization. </title> <editor> In H. Fischer, B. Ried-mueller, and S. Schae*er, editors, </editor> <booktitle> Applied Mathematics and Parallel Computing - Festschrift for Klaus Ritter, </booktitle> <pages> pages 175-188. </pages> <publisher> Physica-Verlag A Springer-Verlag Company, </publisher> <address> Heidelberg, </address> <year> 1996. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-20.ps.Z. </note>
Reference-contexts: Having a minimal number of features often leads to better generalization and simpler models that can be more easily interpreted. This problem is also addressed by statistical [45, 61], machine learning [57, 62] as well as other mathematical programming formulations <ref> [20, 72] </ref>. In general, it should be noted that the problem trading off the simplicity of a model with how well it fits the training data is a well-studied problem. <p> Because of the discontinuity of the step function term e T v fl , we approximate it by a concave exponential on the nonnegative real line <ref> [72] </ref>. We note that the step function can be approximated using Lemma 4.1 with the resulting problem being an LPEC. For a further discussion of this LPEC formulation as well as a formulation that approximates the step function by a sigmoid, see [19]. <p> This approach, which leads to minimization of a concave function on (often) a polyhedral set <ref> [72] </ref>, has been used very effectively for feature selection [19] and noise suppression [17]. Although the polyhedral concave minimization problem is a difficult problem, fast and finite algorithms for finding very useful stationary points make it a very effective tool for data reduction.
Reference: [73] <author> O. L. Mangasarian. </author> <title> Mathematical programming in machine learning. </title> <editor> In G. Di Pillo and F. Giannessi, editors, </editor> <booktitle> Nonlinear Optimization and Applications, </booktitle> <pages> pages 283-295, </pages> <address> New York, 1996. </address> <publisher> Plenum Publishing. </publisher>
Reference-contexts: Any such partition that minimizes the number of points misclassified also minimizes (9). We reduce (9) to an LPEC by first representing the step function () fl as a complementarity condition via the plus function () + , which is a re-statement of <ref> [73, Equation 2.11] </ref>. Lemma 4.1 Characterization of the step function () fl .
Reference: [74] <author> O. L. Mangasarian, R. Setiono, and W. H. Wolberg. </author> <title> Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Large-Scale Numerical Optimization, </booktitle> <pages> pages 22-31, </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1990. </year> <title> SIAM. </title> <booktitle> Proceedings of the Workshop on Large-Scale Numerical Optimization, </booktitle> <institution> Cornell University, </institution> <address> Ithaca, New York, </address> <month> October 19-20, </month> <year> 1989. </year>
Reference-contexts: For example the robust linear separator of 26 [9] has a very simple geometric representation as a plane separating most or all the elements of two sets, which has been used to advantage in medical applications [78, 77]. Similarly the multisurface separation method <ref> [68, 74] </ref> can be geometrically depicted as placing the sets to be separated into distinct polyhedral compartments. 8. Local versus global methods. Because of size considerations we often do not want to model the whole data set. We therefore need to pre-partition the data and model it locally.
Reference: [75] <author> O. L. Mangasarian and M. V. Solodov. </author> <title> Backpropagation convergence via deterministic nonmonotone perturbed minimization. </title> <editor> In G. Tesauro J. D. Cowan and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems -6-, </booktitle> <pages> pages 383-390, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Mathematical programming approaches that deal with problem data in a sequential manner are very useful for processing large databases. Such incremental approaches have already been developed for establishing convergence of the online back-propagation algorithm that trains neural networks incrementally <ref> [76, 75] </ref> as well as for handling more general types of problems [100, 95]. 2. Approximate solutions. (a) The problem of identifying constraints which are inactive and will remain inactive as optimization proceeds is an important and interesting practical problem.
Reference: [76] <author> O. L. Mangasarian and M. V. Solodov. </author> <title> Serial and parallel backpropagation convergence via nonmono-tone perturbed minimization. </title> <journal> Optimization Methods and Software, </journal> <volume> 4(2) </volume> <pages> 103-116, </pages> <year> 1994. </year>
Reference-contexts: Mathematical programming approaches that deal with problem data in a sequential manner are very useful for processing large databases. Such incremental approaches have already been developed for establishing convergence of the online back-propagation algorithm that trains neural networks incrementally <ref> [76, 75] </ref> as well as for handling more general types of problems [100, 95]. 2. Approximate solutions. (a) The problem of identifying constraints which are inactive and will remain inactive as optimization proceeds is an important and interesting practical problem.
Reference: [77] <author> O. L. Mangasarian, W. N. Street, and W. H. Wolberg. </author> <title> Breast cancer diagnosis and prognosis via linear programming. </title> <journal> Operations Research, </journal> <volume> 43(4) </volume> <pages> 570-577, </pages> <month> July-August </month> <year> 1995. </year>
Reference-contexts: The formulation (5) is equivalent to the following robust linear programming formulation (RLP) proposed in [8] and effectively used to solve problems from real-world domains <ref> [77] </ref>: min m e T z jAw + efl + e 5 y; Bw efl + e 5 z; y = 0; z = 0 : (6) The linear program (6) or, equivalently, the formulation (5) define a separating plane P that approximately satisfies the conditions (4). <p> In [83] a more general treatment of EM and the underlying optimization problem can be found. 19 5 Case Studies 5.1 Classification Using Linear Programming The formulation (6) has been successfully used to determine a classification function that objectively performs breast cancer diagnosis <ref> [77] </ref>. Specifically, cellular features of a breast mass are used to classify the mass as either benign or malignant. This classification function is a central component of the Xcyt image analysis program. This case study is detailed in [77] and is briefly summarized here. <p> used to determine a classification function that objectively performs breast cancer diagnosis <ref> [77] </ref>. Specifically, cellular features of a breast mass are used to classify the mass as either benign or malignant. This classification function is a central component of the Xcyt image analysis program. This case study is detailed in [77] and is briefly summarized here. First, a fluid sample from the breast mass is obtained by an outpatient procedure involving a small gauge needle, known as a fine needle aspirate (FNA). The fluid is placed on a slide and stained to highlight the cellular nuclei of the constituent cells. <p> Predicted tenfold cross-validation accuracy was 97.5%. This level of accuracy is as good as the best results achieved at specialized cancer institutions <ref> [77] </ref>. 5.2 Classification with Support Vector Machines We present a "Support Vector Machine approach for detecting vertically oriented and unoccluded frontal views of human faces in grey level images" [84]. This case study is detailed in [84] and is briefly summarized here. <p> Many mathematical programming models possess this property. For example the robust linear separator of 26 [9] has a very simple geometric representation as a plane separating most or all the elements of two sets, which has been used to advantage in medical applications <ref> [78, 77] </ref>. Similarly the multisurface separation method [68, 74] can be geometrically depicted as placing the sets to be separated into distinct polyhedral compartments. 8. Local versus global methods. Because of size considerations we often do not want to model the whole data set.
Reference: [78] <author> O. L. Mangasarian and W. H. Wolberg. </author> <title> Cancer diagnosis via linear programming. </title> <journal> SIAM News, </journal> <volume> 23:1 & 18, </volume> <year> 1990. </year>
Reference-contexts: Many mathematical programming models possess this property. For example the robust linear separator of 26 [9] has a very simple geometric representation as a plane separating most or all the elements of two sets, which has been used to advantage in medical applications <ref> [78, 77] </ref>. Similarly the multisurface separation method [68, 74] can be geometrically depicted as placing the sets to be separated into distinct polyhedral compartments. 8. Local versus global methods. Because of size considerations we often do not want to model the whole data set.
Reference: [79] <author> H. Mannila, H. Toivonen, and A.I. Verkamo. </author> <title> Discovery of frequent episodes in event sequence. Data Mining and Knowledge Discovery, </title> <type> 1(3), </type> <year> 1997. </year>
Reference-contexts: The distinguishing feature of this class of methods is that ordering of observations is important and must be accounted for. Scalable methods for finding frequent sequences in databases, while in the worst-case exponential in complexity, do appear to execute efficiently given sparseness in real-world transactional databases <ref> [79] </ref>. 4 Mathematical Programming Formulations In this section we provide example formulations of some data mining problems as mathematical programs. The formulations are intended as general guidelines and do not necessarily represent best-possible formulations.
Reference: [80] <author> MATLAB. </author> <title> User's Guide. The MathWorks, </title> <publisher> Inc., </publisher> <year> 1992. </year>
Reference-contexts: Mathematical programming languages geared for data mining problems. Although there are languages specifically designed for setting up and solving mathematical programs, such as AMPL [43] and GAMS [22, 55], and languages that can be easily adapted for the same purpose, such as MATLAB <ref> [80] </ref>, no specific language has been designed for mathematical programming applications to data mining. Such a language could make such applications more widely accepted by the data mining community. 4. Data reduction techniques.
Reference: [81] <author> M. Mehta, R. Agrawal, and J. Rissanen. Sliq: </author> <title> a fast scalable classifier for data mining. </title> <booktitle> In Proceedings of EDBT-96. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: While many of these techniques have been historically defined to work over memory-resident data and not much attention has been given to integrating them with database systems. Some of these techniques are beginning to be scaled to operate on large databases. Examples in classification (Section 3.1) include decision trees <ref> [81] </ref>, in summarization (Section 3.3) association rules [3], and in clustering [107] 3.1 Predictive Modeling The goal is to predict some field (s) in a database based on other fields.
Reference: [82] <author> S. Murthy, S. Kasif, S. Salzberg, and R. Beigel. </author> <title> OC1: Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 322-327, </pages> <address> Cambridge, MA 02142, 1993. </address> <publisher> The AAAI Press/The MIT Press. </publisher>
Reference-contexts: Local versus global methods. Because of size considerations we often do not want to model the whole data set. We therefore need to pre-partition the data and model it locally. One way to achieve this is via a decision tree approach <ref> [6, 82] </ref> in which the data is partitioned into local regions and modeled individually. A regression approach to this is given by Regression trees in CART [21]. Another example is to cluster the data first, then model each cluster locally. 9.
Reference: [83] <author> R. M. Neal and G. E. Hinton. </author> <title> A view of the em algorithm that justifies incremental, sparse, and other variants. </title> <type> Technical report, </type> <institution> Dept. of Statistics and Dept. of Computer Science, University of Toronto, Toronto, </institution> <address> Ontario, Canada, </address> <year> 1997. </year>
Reference-contexts: The EM algorithm has been shown to converge to a local minima of (48) <ref> [83] </ref>. We now summarize the EM algorithm for the Gaussian mixture model. Algorithm 4.6 Expectation-Maximization Algorithm. <p> The relationship between the hard assignment method of the k-Mean algorithm (Algorithm 4.5) and the EM Algorithm can be found in [59]. In <ref> [83] </ref> a more general treatment of EM and the underlying optimization problem can be found. 19 5 Case Studies 5.1 Classification Using Linear Programming The formulation (6) has been successfully used to determine a classification function that objectively performs breast cancer diagnosis [77].
Reference: [84] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Training support vector machines: an application to face detection. </title> <booktitle> In CVPR 1997, </booktitle> <year> 1997. </year>
Reference-contexts: stationary point which satisfies the minimum principle necessary optimality condition for problem (17) [19, Theorem 2.2] and leads to a sparse w with good generalization properties. 4.3 Predictive Modeling: Support Vector Machines The previous discussion of feature selection naturally leads to a strongly related framework: the support vector machine (SVM) <ref> [102, 91, 84] </ref>. The SVM can be seen as a way of determining the relevant parameters in polynomial, neural network, or radial basis function classifiers. <p> These points are the only data points that are relevant for determining the optimal separating plane. Their number is usually small and it is proportional to the generalization error of the classifier <ref> [84] </ref>. 12 A related linear programming formulation of problem (18) is the following: minimize w;fl;y;z k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks [69, 8, 11, 16], decision tree construction <p> k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks [69, 8, 11, 16], decision tree construction [6, 11, 69, 7] and calculating nonlinear discriminants by nonlinearly transforming the given data <ref> [102, 23, 84] </ref>. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate has a continuous output in contrast to a discrete output for the classification problem. <p> This level of accuracy is as good as the best results achieved at specialized cancer institutions [77]. 5.2 Classification with Support Vector Machines We present a "Support Vector Machine approach for detecting vertically oriented and unoccluded frontal views of human faces in grey level images" <ref> [84] </ref>. This case study is detailed in [84] and is briefly summarized here. In the face detection problem, input is an arbitrary image. The task is then to determine whether or not there are any human faces in the image. If so, return their location. <p> of accuracy is as good as the best results achieved at specialized cancer institutions [77]. 5.2 Classification with Support Vector Machines We present a "Support Vector Machine approach for detecting vertically oriented and unoccluded frontal views of human faces in grey level images" <ref> [84] </ref>. This case study is detailed in [84] and is briefly summarized here. In the face detection problem, input is an arbitrary image. The task is then to determine whether or not there are any human faces in the image. If so, return their location. <p> Experimental results presented in <ref> [84] </ref> used two sets of images. Set A contained 313 high-quality images with one face per image. Set B contained 23 images of mixed quality with a total of 155 faces. ": : : it is important to state that set A involved 4,669,960 pattern windows, while set B 5,383,682." [84]. <p> <ref> [84] </ref> used two sets of images. Set A contained 313 high-quality images with one face per image. Set B contained 23 images of mixed quality with a total of 155 faces. ": : : it is important to state that set A involved 4,669,960 pattern windows, while set B 5,383,682." [84]. The system is compared with [98]: 3 Available from UCI Repository of Machine Learning Databases, University of California, Irvine.
Reference: [85] <author> M. L. Overton. </author> <title> A quadratically convergent method for minimizing a sum of Euclidean norms. </title> <journal> Mathematical Programming, </journal> <volume> 27 </volume> <pages> 34-63, </pages> <year> 1983. </year>
Reference-contexts: Stop when c `;j = c `;j+1 ; ` = 1; : : : ; k. If the 2-norm (not 2-norm squared) is used in the objective of (38), the cluster center update subproblem becomes the considerably harder Weber problem <ref> [29, 85] </ref> which locates a center in R n closest in sum of Euclidean distances to a finite set of given points.
Reference: [86] <editor> G. Piatetsky-Shapiro and W. Frawley, editors. </editor> <title> Knowledge Discovery in Databases. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1991. </year>
Reference-contexts: The term KDD was coined at the first KDD workshop in 1989 <ref> [86] </ref> to emphasize that "knowledge" is the end product of a data-driven discovery. In our view KDD refers to the overall process of discovering useful knowledge from data while data mining refers to a particular step in this process. <p> Models of causality can be probabilistic (as in deriving some statement about the probability distribution governing the data) or they can be deterministic as in deriving functional dependencies between fields in the data <ref> [86] </ref>. Density estimation methods in general fall under this category, so do methods for explicit causal modeling (e.g. [48] and [51]). 3.5 Change and Deviation Detection These methods account for sequence information, be it time-series or some other ordering (e.g. protein sequencing in genome mapping).
Reference: [87] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <pages> pages 465-471, </pages> <year> 1978. </year>
Reference-contexts: In statistics this is known as the bias-variance tradeoff [44], in Bayesian inference it is known as penalized likelihood [13, 51], and in pattern recognition/machine learning it manifests itself as the minimum message length (MML) [105] problem. The MML framework, also called minimum description length (MDL) <ref> [87] </ref> dictates that the best model for a given data set is one that minimizes the coding length of the data and the model combined. If a model fits the data exactly, then the data need not be encoded and the cost is that of coding the model.
Reference: [88] <author> S. M. Robinson. </author> <title> Bounds for error in the solution of a perturbed linear program. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 6 </volume> <pages> 69-81, </pages> <year> 1973. </year>
Reference-contexts: Further study of the problem and other approaches constitute important research areas. 5. Effect of data perturbation on derived models. Perturbation of mathematical programs is a widely studied area, for example <ref> [89, 88, 5, 38, 64] </ref>. Applying these results to specific data mining problems where the data is constantly being augmented and revised without having to rebuild the mathematical programming model would be a valuable contribution. 6. Modeling noise.
Reference: [89] <author> S. M. Robinson. </author> <title> Perturbations in finite-dimensional systems of linear inequalities and equations. </title> <type> Technical Summary Report No. 1357, </type> <institution> Mathematics Research Center, University of Wisconsin, Madison, Wisconsin, </institution> <month> August </month> <year> 1973. </year>
Reference-contexts: Further study of the problem and other approaches constitute important research areas. 5. Effect of data perturbation on derived models. Perturbation of mathematical programs is a widely studied area, for example <ref> [89, 88, 5, 38, 64] </ref>. Applying these results to specific data mining problems where the data is constantly being augmented and revised without having to rebuild the mathematical programming model would be a valuable contribution. 6. Modeling noise.
Reference: [90] <author> C. Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: This is the case of overfitting the training data. While biasing a classification algorithm to construct ^g from a less complex function class often improves generalization ability, it may not be desirable in all problem domains <ref> [90] </ref>. Overtraining can also lead to poor generalization even when the complexity of the function class from which ^g is constructed is optimal [14]. The key to good generalization is correctly estimating the complexity of the true mapping g while avoiding overtraining.
Reference: [91] <author> B. Scholkopf, C. Burges, and V. Vapnik. </author> <title> Extracting support data for a given task. </title> <booktitle> In Proceedings, First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 252 - 257, </pages> <address> Menlo Park, CA, 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: stationary point which satisfies the minimum principle necessary optimality condition for problem (17) [19, Theorem 2.2] and leads to a sparse w with good generalization properties. 4.3 Predictive Modeling: Support Vector Machines The previous discussion of feature selection naturally leads to a strongly related framework: the support vector machine (SVM) <ref> [102, 91, 84] </ref>. The SVM can be seen as a way of determining the relevant parameters in polynomial, neural network, or radial basis function classifiers.
Reference: [92] <author> S. Z. Selim and M. A. Ismail. </author> <title> K-Means-Type algorithms: a generalized convergence theorem and characterization of local optimality. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6:81-87, </volume> <year> 1984. </year>
Reference-contexts: This property makes the centers computed by the k-Mean algorithm less robust to outliers in the data. We note that k-Mean convergence has been shown in <ref> [92] </ref> and a discussion relating to the convergence proof is presented in [4]. We note here that clustering can also be done using density estimation. Given k clusters, one can associate a probability density function (model) with each of the clusters.
Reference: [93] <author> J. W. Shavlik and T. G. Dietterich (editors). </author> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Often it is possible to allow an algorithm to construct ^g from a sufficiently complex function class so that ^g approximates g arbitrarily well on the training set. But this complex ^g usually approximates g poorly on points not in the training set <ref> [93] </ref>. This is the case of overfitting the training data. While biasing a classification algorithm to construct ^g from a less complex function class often improves generalization ability, it may not be desirable in all problem domains [90].
Reference: [94] <author> A. Silberschatz and A. Tuzhilin. </author> <title> On subjective measures of interestingness in knowledge discovery. </title> <editor> In U. Fayyad and R. Uthurusamy, editors, </editor> <booktitle> Proceedings of KDD-95: First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 275-281, </pages> <address> Menlo Park, CA, 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: In certain contexts understandability can be estimated by simplicity (e.g., the number of bits to describe a pattern). Sometimes the measures are combined under a single interestingness measure (e.g., see <ref> [94] </ref> and references within). Interestingness functions can be explicitly defined or can be manifested implicitly via an ordering placed by the KDD system on the discovered patterns or models.
Reference: [95] <author> M. V. Solodov. </author> <title> Incremental gradient algorithms with stepsizes bounded away from zero. </title> <note> Computational Optimization and Applications, 1998. To appear. </note>
Reference-contexts: Such incremental approaches have already been developed for establishing convergence of the online back-propagation algorithm that trains neural networks incrementally [76, 75] as well as for handling more general types of problems <ref> [100, 95] </ref>. 2. Approximate solutions. (a) The problem of identifying constraints which are inactive and will remain inactive as optimization proceeds is an important and interesting practical problem. Computational algorithms towards this goal have appeared in [24, 42, 106, 12].
Reference: [96] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: m e T z fi fi fi fi Aw + efl + e y; y 0; z 0; 9 &gt; &gt; ; This feature selection problem will be solved for a value of 2 [0; 1) for which the resulting classification function estimate ^g (13) generalizes best, estimated by cross-validation <ref> [96] </ref>. Typically this will be achieved 11 in a feature space of reduced dimensionality, that is e T v fl &lt; n (i.e. the number of features used is less than n). <p> Approximation (PLNA) which "chooses" the fewest elements of F such that ^g approximates our sampling of the true regression function [17]: min (1 )kAw bk 1 + (e T jwj fl ); 2 [0; 1): (26) The value of is chosen to maximize a measure of generalization, estimated by cross-validation <ref> [96] </ref>, for instance. <p> Actual diagnostic outcome for these 569 patients is known. Malignant cases were verified by biopsy and benign cases confirmed by biopsy or subsequent examinations. The diagnostic tags determine the sets A and B for the classification problem. Best results, determined by 10-fold cross-validation <ref> [96] </ref>, were obtained with a separating plane calculated by (6) using 3 of the 30 dimensions: extreme area, extreme smoothness and mean texture. Predicted tenfold cross-validation accuracy was 97.5%.
Reference: [97] <author> W. N. Street and O. L. Mangasarian. </author> <title> Improved generalization via tolerant training. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 96(2) </volume> <pages> 259-279, </pages> <month> February </month> <year> 1998. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-11.ps.Z. 32 </note>
Reference-contexts: the following loss functional [101]: jj * = 0 if jj &lt; *; jj * otherwise: (33) Problem (32) can then be formulated as the following constrained quadratic program: minimize w;y N (e T y) + (w T w) subject to y *e Aw b y + *e; (34) In <ref> [97] </ref> a similar Tolerant Training Method is proposed in which the following quadratic program is solved for some nonnegative tolerance t (which is analogous to * above) and some small value of : minimize w;y;z 2 kyk 2 2 + 2 subject to z t e Aw b y + t <p> Modeling noise. Mathematical programming models that purposely tolerate error, either because there is noise in the data or because the model is an inaccurate representation of the real problem, are likely to perform better. One such approach, tolerant training <ref> [97] </ref>, purposely tolerates such inaccuracies in the model, and often leads to better predictive results. Extending this tolerant mathematical programming model to a wider class of problems would be an important and practical contribution. 7. Visualization and understandability of derived models.
Reference: [98] <author> K. Sung and T. Poggio. </author> <title> Example-based learning for view-based human face detection. </title> <editor> A. I. Lab, A. I. </editor> <volume> Memo 1521, </volume> <publisher> MIT, </publisher> <address> Cambridge, MA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: Set B contained 23 images of mixed quality with a total of 155 faces. ": : : it is important to state that set A involved 4,669,960 pattern windows, while set B 5,383,682." [84]. The system is compared with <ref> [98] </ref>: 3 Available from UCI Repository of Machine Learning Databases, University of California, Irvine.
Reference: [99] <author> Charles W. Therrien. </author> <title> Discrete Random Signals and Statistical Signal Processing. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: This case study appears in [17] and is summarized here. The linear systems used are based upon ideas related to signal processing <ref> [47, 99] </ref> and more specifically to an example in [2, Equation (8)].
Reference: [100] <author> P. Tseng. </author> <title> Incremental Gradient(-Projection) method with momentum term and adaptive stepsize rule. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 8, </volume> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: Such incremental approaches have already been developed for establishing convergence of the online back-propagation algorithm that trains neural networks incrementally [76, 75] as well as for handling more general types of problems <ref> [100, 95] </ref>. 2. Approximate solutions. (a) The problem of identifying constraints which are inactive and will remain inactive as optimization proceeds is an important and interesting practical problem. Computational algorithms towards this goal have appeared in [24, 42, 106, 12].
Reference: [101] <author> V. Vapnik, S. E. Golowich, and A. Smola. </author> <title> Support vector method for function approximation, regression estimation, and signal processing. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. </publisher>
Reference-contexts: square fit error plus a 2-norm penalization term on the coefficient vector: 14 min (1 ) kAw bk 2 kwk 2 Similarly, the Basis Pursuit De-Noising [27] refers to a solution of: min (1 ) kAw bk 2 The Support Vector Method has also been extended to the regression problem <ref> [32, 101] </ref>. <p> In our setting, this problem can then be formulated as: min (1 ) (e T (jAw bj * ) + kwk 2 Here j j * is the following loss functional <ref> [101] </ref>: jj * = 0 if jj &lt; *; jj * otherwise: (33) Problem (32) can then be formulated as the following constrained quadratic program: minimize w;y N (e T y) + (w T w) subject to y *e Aw b y + *e; (34) In [97] a similar Tolerant Training
Reference: [102] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: As the number of dimensions grow, the number of choice combinations for dimensionality reduction explode. Furthermore, a projection to lower dimensions could easily transform a relatively easy discrimination problem into one that is extremely difficult. In fact, some mining algorithms (e.g. support vector machines <ref> [102] </ref> discussed later in this paper) employ a reverse techniques where dimensionality is purposefully increased to render the classification problem easy (linear). <p> stationary point which satisfies the minimum principle necessary optimality condition for problem (17) [19, Theorem 2.2] and leads to a sparse w with good generalization properties. 4.3 Predictive Modeling: Support Vector Machines The previous discussion of feature selection naturally leads to a strongly related framework: the support vector machine (SVM) <ref> [102, 91, 84] </ref>. The SVM can be seen as a way of determining the relevant parameters in polynomial, neural network, or radial basis function classifiers. <p> The SVM can be seen as a way of determining the relevant parameters in polynomial, neural network, or radial basis function classifiers. While most approaches are based on the idea of minimizing an error in separating the given data (i.e. minimizing training set error), SVMs incorporate structured risk minimization <ref> [102, 23] </ref> which minimizes an upper bound on the generalization error. For a more detailed discussion of SVMs, see [102, 23, 104]. Consider the simple case when the sets A and B are linearly separable. <p> While most approaches are based on the idea of minimizing an error in separating the given data (i.e. minimizing training set error), SVMs incorporate structured risk minimization [102, 23] which minimizes an upper bound on the generalization error. For a more detailed discussion of SVMs, see <ref> [102, 23, 104] </ref>. Consider the simple case when the sets A and B are linearly separable. The idea is to determine, among the infinite number of planes correctly separating A from B, the one which will have smallest generalization error. <p> k + e T z subject to Aw + efl + e y; y 0; z 0: These mathematical programming formulations have been extended to constructively training neural networks [69, 8, 11, 16], decision tree construction [6, 11, 69, 7] and calculating nonlinear discriminants by nonlinearly transforming the given data <ref> [102, 23, 84] </ref>. 4.4 Predictive Modeling: Regression We now discuss the regression problem which differs from the classification problem in that the function g which we are trying to estimate has a continuous output in contrast to a discrete output for the classification problem.
Reference: [103] <author> G. Wahba. </author> <title> Spline Models for Observational Data. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1990. </year>
Reference-contexts: Other methods for addressing the regression problem, which are also optimization problems, include neural networks [52] and smoothing splines <ref> [103] </ref>. We present mathematical programming formulations that attempt to estimate the true, unknown regression function g by a linear combination of functions from some pre-defined function set. We relax the notion that elements of the function set be linearly independent.
Reference: [104] <author> G. Wahba. </author> <title> Support vector machines, reproducing kernel hilbert spaces and the randomized gacv. </title> <type> Technical report no. 984, </type> <institution> Department of Statistics, University of Wisconsin, Madison, WI 53706, </institution> <year> 1997. </year> <note> Available at: ftp://ftp.stat.wisc.edu/pub/wahba/index.html. </note>
Reference-contexts: While most approaches are based on the idea of minimizing an error in separating the given data (i.e. minimizing training set error), SVMs incorporate structured risk minimization [102, 23] which minimizes an upper bound on the generalization error. For a more detailed discussion of SVMs, see <ref> [102, 23, 104] </ref>. Consider the simple case when the sets A and B are linearly separable. The idea is to determine, among the infinite number of planes correctly separating A from B, the one which will have smallest generalization error.
Reference: [105] <author> C.S. Wallace and J.D. Patrick. </author> <title> Coding decision trees. </title> <type> Technical Report TR 151, </type> <institution> Monash University, </institution> <address> Melbourne, Australia, </address> <year> 1991. </year>
Reference-contexts: In statistics this is known as the bias-variance tradeoff [44], in Bayesian inference it is known as penalized likelihood [13, 51], and in pattern recognition/machine learning it manifests itself as the minimum message length (MML) <ref> [105] </ref> problem. The MML framework, also called minimum description length (MDL) [87] dictates that the best model for a given data set is one that minimizes the coding length of the data and the model combined.
Reference: [106] <author> S. J. Wright. </author> <title> Identifiable surfaces in constrained optimization. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 31 </volume> <pages> 1063-1079, </pages> <year> 1993. </year>
Reference-contexts: Approximate solutions. (a) The problem of identifying constraints which are inactive and will remain inactive as optimization proceeds is an important and interesting practical problem. Computational algorithms towards this goal have appeared in <ref> [24, 42, 106, 12] </ref>. A possible simple approach for this problem is the use of the classical exterior quadratic penalty function [41] and dropping constraints for which the product of the penalty parameter (which is increasing to infinity) times the constraint violation is less than some tolerance.
Reference: [107] <author> T. Zhang, R. Ramakrishnan, and M. Livny. </author> <title> Birch: A new data clustering algorithm and its applications. Data Mining and Knowledge Discovery, </title> <type> 1(2), </type> <year> 1997. </year> <month> 33 </month>
Reference-contexts: Some of these techniques are beginning to be scaled to operate on large databases. Examples in classification (Section 3.1) include decision trees [81], in summarization (Section 3.3) association rules [3], and in clustering <ref> [107] </ref> 3.1 Predictive Modeling The goal is to predict some field (s) in a database based on other fields. If the field being predicted is a numeric (continuous) variable (such as a physical measurement of e.g. height) then the prediction problem is a regression problem.
References-found: 107

