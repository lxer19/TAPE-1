URL: http://www.ultimode.com/wray/MLNet/MLNet.ps.Z
Refering-URL: http://www.ultimode.com/wray/MLNet/MLNet.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: wray@kronos.arc.nasa.gov  
Title: MLnet Summer School on Machine Learning and Knowledge Acquisition: LEARNING AND PROBABILITIES  for input.  
Author: Wray Buntine, RIACS Thanks to Padhraic Smyth and Peter Cheeseman 
Note: Not all slides will be covered in the presentation.  
Address: MS 269-2 Moffet Field, CA 94035-1000  
Affiliation: NASA Ames Research Center,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Angluin and C.H. Smith. </author> <title> Inductive inference: Theories and methods. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <year> 1983. </year>
Reference: [2] <author> S. Ben-David and M. Jacovi. </author> <title> On learning in the limit and non-uniform (*; ffi)- learning. </title> <editor> In L. Pitt, editor, COLT'93: </editor> <booktitle> Workshop on Computational Learning Theory, </booktitle> <pages> pages 209-217. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference: [3] <author> J. O. Berger. </author> <title> Statistical analysis and the illusion of objectivity. </title> <journal> American Scientist, </journal> <volume> 76(March-April):159-165, </volume> <year> 1988. </year>
Reference-contexts: its Bayesian interpretation [12]. * Sample complexity, VC dimension, and other measures of problem complexity (see recent COLT work). * Monte Carlo sampling theory [68]. * Interpretations of probability and rationality, standard "paradoxes" and their resolution. * Broader issues such as Occam's razor, subjectivity vs. objectivity, the principle of indifference <ref> [3, 51] </ref>.
Reference: [4] <author> J.O. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference: [5] <author> J.M. Bernardo and A.F.M. Smith. </author> <title> Bayesian Theory. </title> <publisher> John Wiley, </publisher> <address> Chichester, </address> <year> 1994. </year>
Reference-contexts: deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes [30, 31]. * Methods for handling priors <ref> [5] </ref>. * Subsampling to handle large datasets [67].
Reference: [6] <author> J. Besag, J. York, and A. Mollie. </author> <title> Bayesian image restoration with two applications in spatial statistics. </title> <journal> Ann. Inst. Statist. Math., </journal> <volume> 43(1) </volume> <pages> 1-59, </pages> <year> 1991. </year>
Reference-contexts: SS #52 LEGEND FOR GRAPHICAL MODELS For graphical models in general: SS #53 UNSUPERVISED LEARNING FOR IMAGES Spatial coherence models: Figure (a) shows a popular Markov model used for creating spatially coherent classifications <ref> [6] </ref>. Mixed pixels: Pixels in an image represent a mixture of class types (30% grassland, 70% woodland); figure (b) shows the popular linear mixture model used for this.
Reference: [7] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: Essentials: * check list of representations; * check list of methods; * check list of theory; * check list of fields (see Section Ib). SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees <ref> [75, 7, 11] </ref>, rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50].
Reference: [8] <author> G.L. Bretthorst. </author> <title> An introduction to model selection using probability theory as logic. </title> <editor> In G. Heidbreder, editor, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic, </publisher> <year> 1994. </year> <title> Proceedings, </title> <institution> at Santa Barbara, </institution> <year> 1993. </year>
Reference: [9] <author> W.L. Buntine. </author> <title> A critique of the Valiant model. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 837-842, </pages> <address> Detroit, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: OP T + * j N; S; ) 1 ffi : This is equivalent to, no matter what prior over (S; ) is used, p (KL PAC (sample) &lt; OP T + * j N) 1 ffi : Because this ignores the details of the sample, it answers the question <ref> [9, 42] </ref>: if I obtain a sample of size N, how confident can I expect to be about learning (irrespective of the prior/truth)? NOT how confident can I be with my current sample? SS #94 APPROXIMATE BAYESIAN THEORY The Bayesian Maximum A Posteriori (MAP) approximation is a constructive, asymptotically optimal scheme
Reference: [10] <author> W.L. Buntine. </author> <title> Classifiers: A theoretical and empirical study. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [11] <author> W.L. Buntine. </author> <title> Learning classification trees. In D.J. </title> <editor> Hand, editor, </editor> <booktitle> Artificial Intelligence Frontiers in Statistics, </booktitle> <pages> pages 182-201. </pages> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1991. </year>
Reference-contexts: Essentials: * check list of representations; * check list of methods; * check list of theory; * check list of fields (see Section Ib). SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees <ref> [75, 7, 11] </ref>, rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50]. <p> SS #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors <ref> [11, 14] </ref>. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making
Reference: [12] <author> W.L. Buntine. </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> University of Technology, </institution> <address> Sydney, </address> <year> 1991. </year> <note> Written in 1990, awarded in 1992. </note>
Reference-contexts: SS #97 PERCEPTRONS, cont. SS #98 LEARNING THEORIES: SUMMARY * Theories can be interpreted and compared with language of probability and decision theory <ref> [42, 12] </ref>. PAC/PAB addresses the question: "how much data should I obtain?" in a prior independent way, not, "how do I learn with the current sample?" Bayesian MAP and MDL methods related. <p> SS #102 CHECK LIST OF THEORY * Asymptotic, and large sample results (convergence, order of magnitude, etc). * Transformations between MDL, Bayesian methods [99], and others. * PAC and its Bayesian interpretation <ref> [12] </ref>. * Sample complexity, VC dimension, and other measures of problem complexity (see recent COLT work). * Monte Carlo sampling theory [68]. * Interpretations of probability and rationality, standard "paradoxes" and their resolution. * Broader issues such as Occam's razor, subjectivity vs. objectivity, the principle of indifference [3, 51].
Reference: [13] <author> W.L. Buntine. </author> <title> Theory refinement of Bayesian networks. </title> <editor> In D'Ambrosio et al. </editor> <volume> [24]. </volume>
Reference-contexts: It provides <ref> [13, 21] </ref>: An improved (over maximum likelihood) measure to optimize: Choose the structure S that maximizes the posterior log. probability p (Sjsample) p (Sjsample) / p (S; sample) = p (S) Z p (samplejS; ) p (jS) d ; Domain specific information can be used to tune the measure through priors
Reference: [14] <author> W.L. Buntine. </author> <title> Learning with graphical models. </title> <type> Technical Report FIA-94-02, </type> <institution> Artificial Intelligence Research Branch, NASA Ames Research Center, </institution> <year> 1994. </year>
Reference-contexts: SS #55 GRAPHICAL MODELS Other problems can be represented in graphical models using: * known and unknown variables, * deterministic nodes [88], * standard probabilities functions at nodes (Gaussian, multinomial, Dirichlet, lo gistic, etc.), * mixed directed and undirected arcs [32], * optional arcs (indicating alternative models) <ref> [14] </ref>. * plates representing samples [14]. Graphical models offer a unified framework for representing a problem (prior knowledge and data), performing problem decomposition, specifying a knowledge refinement task, etc. SS #56 GRAPHICAL MODELS, cont. <p> GRAPHICAL MODELS Other problems can be represented in graphical models using: * known and unknown variables, * deterministic nodes [88], * standard probabilities functions at nodes (Gaussian, multinomial, Dirichlet, lo gistic, etc.), * mixed directed and undirected arcs [32], * optional arcs (indicating alternative models) <ref> [14] </ref>. * plates representing samples [14]. Graphical models offer a unified framework for representing a problem (prior knowledge and data), performing problem decomposition, specifying a knowledge refinement task, etc. SS #56 GRAPHICAL MODELS, cont. <p> p (jx 1 ; : : : ; x N ; M) : * Convergence slow near a local maxima so some implementations switch to con jugate gradient or other methods [63] when near solution. * To understand the general approach, you need to consider the exponential family of distributions <ref> [16, 14] </ref>, although it applies more generally (e.g., [53]). <p> SS #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors <ref> [11, 14] </ref>. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making
Reference: [15] <author> W.L. </author> <title> Buntine and A.S. Weigend. Bayesian back-propagation. </title> <journal> Complex Systems, </journal> <volume> 5(1) </volume> <pages> 603-643, </pages> <year> 1991. </year>
Reference-contexts: The graphical model representing this learning problem, assuming Gaussians inde pendent between x 1 and x 2 , is given below. SS #60 MIXTURE MODELS: MOTIVATION Mixture models are ubiquitous in data analysis [96, 62]. They model: * Missing values in other problems (trees, feed-forward networks, etc.) <ref> [15, 92] </ref>. * Latent or hidden variables, e.g., medical syndromes. * Unsupervised learning and clustering, e.g., Autoclass [20]. * Supervised learning and multivariate splits in trees [53]. * Robust regression [57]. * Non-parametric density estimation (i.e., equivalent to Kernel density estimation and nearest neighbor). * Rule-based systems with multi-firing probabilistic rules. <p> their core. * In general, missing data is handled with mixture models [96]. * We have a case with class class, known data, given-fields, and missing data missing-fields. fields = (given-fields; missing-fields) : * How do we modify the learning algorithm? For most classes of algorithms, well understood alternatives exist <ref> [95, 74, 15, 90] </ref>. <p> SS #83 METHODS: FRACTIONAL EXAMPLES * Like the fill-in methods but do multiple fill-ins, and assign each a probability (denoted p (case) in the figure). * Better approximates the sum. * See <ref> [74, 15, 90] </ref>. OTHER METHODS All the usual methods for handling mixture models. * EM algorithm. * Gibbs sampling. * Incremental versions of both [95]. SS #85 SECTION VIb.
Reference: [16] <author> G. Casella and R.L. Berger. </author> <title> Statistical Inference. </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Belmont, CA, </address> <year> 1990. </year>
Reference-contexts: p (jx 1 ; : : : ; x N ; M) : * Convergence slow near a local maxima so some implementations switch to con jugate gradient or other methods [63] when near solution. * To understand the general approach, you need to consider the exponential family of distributions <ref> [16, 14] </ref>, although it applies more generally (e.g., [53]).
Reference: [17] <author> B. Cestnik and I. Bratko. </author> <title> On estimating probabilities in tree pruning. </title> <booktitle> In Pro--ceedings of the Sixth European Working Session on Learning, Porto, </booktitle> <address> Portugal, 1991. </address> <publisher> Pitman Publishing. </publisher>
Reference: [18] <author> J.M. Chambers and T.J. Hastie, </author> <title> editors. Statistical Models in S. </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Pacific Grove, California, </address> <year> 1992. </year>
Reference: [19] <author> E. Charniak. </author> <title> Bayesian networks without tears. </title> <journal> AI Magazine, </journal> <volume> 12(4) </volume> <pages> 50-63, </pages> <year> 1991. </year>
Reference: [20] <author> P. Cheeseman, M. Self, J. Kelly, W. Taylor, D. Freeman, and J. Stutz. </author> <title> Bayesian classification. </title> <booktitle> In Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 607-611, </pages> <address> Saint Paul, Minnesota, </address> <year> 1988. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: SS #51 THE BAYESIAN NETWORK FOR AUTOCLASS III The Autoclass III <ref> [20] </ref> application to the IRAS star catalogue looked at intensities measured for different wavelengths in the infra-red region. This simple model assumes independence between intensities. A more complete model would assume correlation between neighboring wavelengths, and dependence between the hidden class and the star position. <p> SS #60 MIXTURE MODELS: MOTIVATION Mixture models are ubiquitous in data analysis [96, 62]. They model: * Missing values in other problems (trees, feed-forward networks, etc.) [15, 92]. * Latent or hidden variables, e.g., medical syndromes. * Unsupervised learning and clustering, e.g., Autoclass <ref> [20] </ref>. * Supervised learning and multivariate splits in trees [53]. * Robust regression [57]. * Non-parametric density estimation (i.e., equivalent to Kernel density estimation and nearest neighbor). * Rule-based systems with multi-firing probabilistic rules. * Related to hidden Markov models.
Reference: [21] <author> P.C. Cheeseman. </author> <title> On finding the most probable model. </title> <editor> In J. Shrager and P. Langley, editors, </editor> <title> Computational Models of Discovery and Theory Formation. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: It provides <ref> [13, 21] </ref>: An improved (over maximum likelihood) measure to optimize: Choose the structure S that maximizes the posterior log. probability p (Sjsample) p (Sjsample) / p (S; sample) = p (S) Z p (samplejS; ) p (jS) d ; Domain specific information can be used to tune the measure through priors <p> SS #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) <ref> [73, 21] </ref>. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method
Reference: [22] <author> B. Cheng and D.M. Titterington. </author> <title> Neural networks: A review from a statistical perspective. </title> <journal> Statistical Science, </journal> <volume> 9 </volume> <pages> 2-54, </pages> <year> 1994. </year> <title> with comments and rejoinder. </title>
Reference: [23] <author> G.F. Cooper and E.H. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <editor> In D'Ambrosio et al. </editor> <volume> [24], </volume> <pages> pages 86-94. </pages>
Reference: [24] <editor> B.D. D'Ambrosio, P. Smets, and P.P. Bonissone, editors. </editor> <booktitle> Uncertainty in Artificial Intelligence: Proceedings of the Seventh Conference, </booktitle> <address> Los Angeles, CA, </address> <year> 1991. </year>
Reference: [25] <author> G.R. Dattatreya and L.N. Kanal. </author> <title> Decision trees in pattern recognition. In L.N. </title> <editor> Kanal and A. Rosenfeld, editors, </editor> <booktitle> Progress in Pattern Recognition 2, </booktitle> <pages> pages 189-239. </pages> <publisher> Elsevier Science Publishers B.V., North Holland, </publisher> <year> 1985. </year>
Reference: [26] <author> M.H. </author> <title> DeGroot. Optimal Statistical Decisions. </title> <publisher> McGraw-Hill, </publisher> <year> 1970. </year>
Reference: [27] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: SS #62 ALGORITHMS ON MIXTURES Many general purpose algorithms exist for learning with mixtures [96, 92]. Incremen tal versions of each of these algorithms also exist (e.g., [95]). These are: 1. k-means clustering and related algorithms [29], 2. The Expectation-Maximization (EM) algorithm <ref> [27] </ref>. 3. Gibbs sampling [49, 35], and more general purpose Markov chain Monte Carlo algorithms [39, 68, 79]. These are listed in terms of: * increasing computational complexity, * increasing statistical sophistication, * increasing accuracy, * decreasing bias. SS #63 K-MEANS ALGORITHM Initialize: class centers. Repeat: until converges, 1. <p> SS #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models <ref> [92, 58, 27] </ref>. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes [30, 31]. * Methods for handling priors [5]. * Subsampling to
Reference: [28] <author> L. Devroye. </author> <title> A Course in Density Estimation. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1987. </year>
Reference: [29] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: SS #62 ALGORITHMS ON MIXTURES Many general purpose algorithms exist for learning with mixtures [96, 92]. Incremen tal versions of each of these algorithms also exist (e.g., [95]). These are: 1. k-means clustering and related algorithms <ref> [29] </ref>, 2. The Expectation-Maximization (EM) algorithm [27]. 3. Gibbs sampling [49, 35], and more general purpose Markov chain Monte Carlo algorithms [39, 68, 79]. These are listed in terms of: * increasing computational complexity, * increasing statistical sophistication, * increasing accuracy, * decreasing bias.
Reference: [30] <author> B. Efron and R. Tibshirani. </author> <title> Statistical data analysis in the computer age. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 390-395, </pages> <year> 1991. </year>
Reference-contexts: 54, 94]. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes <ref> [30, 31] </ref>. * Methods for handling priors [5]. * Subsampling to handle large datasets [67].
Reference: [31] <author> B. Efron and R.J. Tibshirani. </author> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman & Hall, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: 54, 94]. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes <ref> [30, 31] </ref>. * Methods for handling priors [5]. * Subsampling to handle large datasets [67].
Reference: [32] <author> M. Frydenberg. </author> <title> The chain graph Markov property. </title> <journal> Scandinavian Journal of Statistics, </journal> <year> 1991. </year>
Reference-contexts: The DAG also models the error as a Gaussian. SS #55 GRAPHICAL MODELS Other problems can be represented in graphical models using: * known and unknown variables, * deterministic nodes [88], * standard probabilities functions at nodes (Gaussian, multinomial, Dirichlet, lo gistic, etc.), * mixed directed and undirected arcs <ref> [32] </ref>, * optional arcs (indicating alternative models) [14]. * plates representing samples [14]. Graphical models offer a unified framework for representing a problem (prior knowledge and data), performing problem decomposition, specifying a knowledge refinement task, etc. SS #56 GRAPHICAL MODELS, cont.
Reference: [33] <author> S. Geman, E. Bienenstock, and Rene Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference: [34] <author> S. Geman and D. Geman. </author> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian relation of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6, </volume> <year> 1984. </year>
Reference-contexts: SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. <ref> [34, 78, 102, 45] </ref> * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50].
Reference: [35] <author> W.R. Gilks, D.G. Clayton, D.J. Spiegelhalter, N.G. Best, A.J. McNeil, L.D. Sharples, and A.J. Kirby. </author> <title> Modelling complexity: applications of Gibbs sampling in medicine. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 55 </volume> <pages> 39-102, </pages> <year> 1993. </year>
Reference-contexts: SS #62 ALGORITHMS ON MIXTURES Many general purpose algorithms exist for learning with mixtures [96, 92]. Incremen tal versions of each of these algorithms also exist (e.g., [95]). These are: 1. k-means clustering and related algorithms [29], 2. The Expectation-Maximization (EM) algorithm [27]. 3. Gibbs sampling <ref> [49, 35] </ref>, and more general purpose Markov chain Monte Carlo algorithms [39, 68, 79]. These are listed in terms of: * increasing computational complexity, * increasing statistical sophistication, * increasing accuracy, * decreasing bias. SS #63 K-MEANS ALGORITHM Initialize: class centers. Repeat: until converges, 1. <p> M) [68, 79]. * Gibbs sampling corresponds to simulated annealing with fixed temperature [97]. * Local repair/search methods [52, 65] correspond to simulated annealing with zero temperature, so Gibbs sampling is probabilistic local search. * Gibbs sampling can be applied more generally to many learning and data anal ysis problems <ref> [35] </ref>. SS #71 K-MEANS vs. EM vs. GIBBS The three algorithms differ in how they perform Steps 1 and 2. Algorithm Step 1 Step 2 k-means MAP or ML MAP or ML EM posterior mean MAP or mean Gibbs sample sample SS #72 SECTION VIa. <p> #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods <ref> [68, 79, 35, 36] </ref> * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes [30, 31]. * Methods for handling priors [5]. * Subsampling to handle large datasets [67].
Reference: [36] <author> W.R. Gilks, A. Thomas, </author> <title> and D.J. Spiegelhalter. A language and program for complex Bayesian modelling. </title> <booktitle> The Statistician, </booktitle> <year> 1993. </year>
Reference-contexts: #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods <ref> [68, 79, 35, 36] </ref> * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes [30, 31]. * Methods for handling priors [5]. * Subsampling to handle large datasets [67].
Reference: [37] <author> D.J. </author> <title> Hand. Kernel Discriminant Analysis. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1982. </year>
Reference: [38] <author> T. Hastie and R. Tibshirani. </author> <title> Generalized additive models. </title> <journal> Statistical Science, </journal> <volume> 1 </volume> <pages> 297-318, </pages> <year> 1986. </year>
Reference-contexts: SS #99 SECTION VII. Essentials: * check list of representations; * check list of methods; * check list of theory; * check list of fields (see Section Ib). SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models <ref> [38, 60] </ref> trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62,
Reference: [39] <author> W.K. Hastings. </author> <title> Monte Carlo sampling methods using Markov chains and their applications. </title> <journal> Biometrika, </journal> <volume> 57(1) </volume> <pages> 97-109, </pages> <year> 1970. </year>
Reference-contexts: Incremen tal versions of each of these algorithms also exist (e.g., [95]). These are: 1. k-means clustering and related algorithms [29], 2. The Expectation-Maximization (EM) algorithm [27]. 3. Gibbs sampling [49, 35], and more general purpose Markov chain Monte Carlo algorithms <ref> [39, 68, 79] </ref>. These are listed in terms of: * increasing computational complexity, * increasing statistical sophistication, * increasing accuracy, * decreasing bias. SS #63 K-MEANS ALGORITHM Initialize: class centers. Repeat: until converges, 1. Assign cases to their most likely class. 2.
Reference: [40] <author> D. Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36(2) </volume> <pages> 177-222, </pages> <year> 1988. </year>
Reference: [41] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Control, </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <month> September </month> <year> 1992. </year>
Reference: [42] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information thjeory and the VC dimension. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 83-113, </pages> <year> 1994. </year>
Reference-contexts: OP T + * j N; S; ) 1 ffi : This is equivalent to, no matter what prior over (S; ) is used, p (KL PAC (sample) &lt; OP T + * j N) 1 ffi : Because this ignores the details of the sample, it answers the question <ref> [9, 42] </ref>: if I obtain a sample of size N, how confident can I expect to be about learning (irrespective of the prior/truth)? NOT how confident can I be with my current sample? SS #94 APPROXIMATE BAYESIAN THEORY The Bayesian Maximum A Posteriori (MAP) approximation is a constructive, asymptotically optimal scheme <p> SS #97 PERCEPTRONS, cont. SS #98 LEARNING THEORIES: SUMMARY * Theories can be interpreted and compared with language of probability and decision theory <ref> [42, 12] </ref>. PAC/PAB addresses the question: "how much data should I obtain?" in a prior independent way, not, "how do I learn with the current sample?" Bayesian MAP and MDL methods related.
Reference: [43] <author> David Haussler and Manfred Warmuth. </author> <title> The probably approximately correct (pac) and other learning models. </title> <editor> In A. Meyrowitz and S. Chipman, editors, </editor> <title> Machine Learning: Induction, Analogy and Discovery. </title> <year> 1993. </year>
Reference: [44] <author> M. Henrion, J.S. Breese, and E.J. Horvitz. </author> <title> Decision analysis and expert systems. </title> <journal> AI Magazine, </journal> <volume> 12(4) </volume> <pages> 64-91, </pages> <year> 1991. </year>
Reference: [45] <author> J.A. Hertz, A.S. Krogh, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. <ref> [34, 78, 102, 45] </ref> * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50].
Reference: [46] <author> K.-U. Hoffgen. </author> <title> Learning and robust learning of product distributions. </title> <note> Research Report Nr. 464, revised May 1993, </note> <institution> Fachbereich Informatik, Universitat Dortmund, </institution> <year> 1993. </year>
Reference-contexts: PAC theory generalized to the noisy case provides results on <ref> [46] </ref>: Sample size bounds: To learn with confidence 1 ffi that your network has Kullback-Leibler distance that is within * of the optimum for a network with k parents, then use a sample of size N = 2 2k+13 n 2 n 2 These bounds are not tight, and probably can
Reference: [47] <author> E.J. Horvitz, D.E. Heckerman, </author> <title> and C.P. Langlotz. A framework for comparing alternative formalisms for plausible reasoning. </title> <booktitle> In Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 210-214, </pages> <address> Philadelphia, </address> <year> 1986. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference: [48] <author> R.A. Howard. </author> <title> Decision analysis: perspectives on inference, decision, </title> <booktitle> and exper-imentation. Proceedings of the IEEE, </booktitle> <volume> 58(5), </volume> <year> 1970. </year>
Reference: [49] <author> T. Hrycej. </author> <title> Gibbs sampling in Bayesian networks. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 351-363, </pages> <year> 1990. </year>
Reference-contexts: SS #62 ALGORITHMS ON MIXTURES Many general purpose algorithms exist for learning with mixtures [96, 92]. Incremen tal versions of each of these algorithms also exist (e.g., [95]). These are: 1. k-means clustering and related algorithms [29], 2. The Expectation-Maximization (EM) algorithm [27]. 3. Gibbs sampling <ref> [49, 35] </ref>, and more general purpose Markov chain Monte Carlo algorithms [39, 68, 79]. These are listed in terms of: * increasing computational complexity, * increasing statistical sophistication, * increasing accuracy, * decreasing bias. SS #63 K-MEANS ALGORITHM Initialize: class centers. Repeat: until converges, 1.
Reference: [50] <author> R.A. Jacobs, M.I. Jordan, S.J. Nowlan, and G.E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models <ref> [96, 62, 50] </ref>.
Reference: [51] <author> W.H. Jefferys and J.O. Berger. </author> <title> Ockham's razor and Bayesian analysis. </title> <journal> American Scientist, </journal> <volume> 80(Jan-Feb):64-72, </volume> <year> 1992. </year>
Reference-contexts: its Bayesian interpretation [12]. * Sample complexity, VC dimension, and other measures of problem complexity (see recent COLT work). * Monte Carlo sampling theory [68]. * Interpretations of probability and rationality, standard "paradoxes" and their resolution. * Broader issues such as Occam's razor, subjectivity vs. objectivity, the principle of indifference <ref> [3, 51] </ref>.
Reference: [52] <author> D.S. Johnson, C.H. Papdimitriou, and M. Yannakakis. </author> <title> How easy is local search? In FOCS'85, </title> <address> pages 39-42, </address> <year> 1985. </year>
Reference-contexts: N ; M) : * Sampling generates dependent samples of that nevertheless have large sample properties similar to the posterior distribution p (jx 1 ; : : : ; x N ; M) [68, 79]. * Gibbs sampling corresponds to simulated annealing with fixed temperature [97]. * Local repair/search methods <ref> [52, 65] </ref> correspond to simulated annealing with zero temperature, so Gibbs sampling is probabilistic local search. * Gibbs sampling can be applied more generally to many learning and data anal ysis problems [35]. SS #71 K-MEANS vs. EM vs.
Reference: [53] <author> M.I. Jordan and R.I. Jacobs. </author> <title> Supervised learning and divide-and-conquer: A statistical approach. </title> <booktitle> In ML10 [66], </booktitle> <pages> pages 159-166. </pages>
Reference-contexts: They model: * Missing values in other problems (trees, feed-forward networks, etc.) [15, 92]. * Latent or hidden variables, e.g., medical syndromes. * Unsupervised learning and clustering, e.g., Autoclass [20]. * Supervised learning and multivariate splits in trees <ref> [53] </ref>. * Robust regression [57]. * Non-parametric density estimation (i.e., equivalent to Kernel density estimation and nearest neighbor). * Rule-based systems with multi-firing probabilistic rules. * Related to hidden Markov models. <p> x N ; M) : * Convergence slow near a local maxima so some implementations switch to con jugate gradient or other methods [63] when near solution. * To understand the general approach, you need to consider the exponential family of distributions [16, 14], although it applies more generally (e.g., <ref> [53] </ref>).
Reference: [54] <author> R.E. Kass and A.E. Raftery. </author> <title> Bayes factors and model uncertainty. </title> <type> Technical Report #571, </type> <institution> Department of Statistics, Carnegie Mellon University, </institution> <address> PA, </address> <year> 1993. </year> <note> Submitted to Jnl. </note> <institution> of American Statistical Association. </institution>
Reference-contexts: SS #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values <ref> [92, 54, 94] </ref>. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes
Reference: [55] <author> Ron Kohavi. </author> <title> Bottom-up induction of oblivious, read-once decision graphs : Strengths and limitations. </title> <booktitle> In Twelfth National Conference on Artificial Intelligence, </booktitle> <year> 1994. </year> <note> Paper available by anonymous ftp from Starry.Stanford.EDU:pub/ronnyk/aaai94.ps. </note>
Reference-contexts: SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees [75, 7, 11], rules, and graph models <ref> [69, 55] </ref>, feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50].
Reference: [56] <author> R. Kraft and W.L. Buntine. </author> <title> Initial exploration of the ASRS database. </title> <booktitle> In Seventh International Symposium on Avaition Psychology, </booktitle> <address> Columbus, Ohio, </address> <year> 1993. </year>
Reference-contexts: SS #86 HYBRID CLUSTERING AND KNOWLEDGE DISCOVERY The US Aviation Safety Reporting System is a national resource for aircraft safety maintaining a database of aircraft incidents. Initial clustering/unsupervised learning of the database, was done using the model below <ref> [56] </ref>. Aviation psychologists, reviewing the results, in many cases said, "so what?" * We already know wide-body aircraft don't go on unscheduled flights (joy rides). * We already know that if an aircraft has four pilots it must be military.
Reference: [57] <author> K. Lange and J.S. Sinsheimer. </author> <title> Normal/independent distributions and their applications in robust regression. </title> <journal> Journal of Computational and Graphical Statistics, </journal> <volume> 2(2), </volume> <year> 1993. </year>
Reference-contexts: They model: * Missing values in other problems (trees, feed-forward networks, etc.) [15, 92]. * Latent or hidden variables, e.g., medical syndromes. * Unsupervised learning and clustering, e.g., Autoclass [20]. * Supervised learning and multivariate splits in trees [53]. * Robust regression <ref> [57] </ref>. * Non-parametric density estimation (i.e., equivalent to Kernel density estimation and nearest neighbor). * Rule-based systems with multi-firing probabilistic rules. * Related to hidden Markov models.
Reference: [58] <author> S.L. Lauritzen. </author> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis, </journal> <note> 1993. To appear; available as Tech. Rep. </note> <institution> R 91-05, Institute for Electronic Systems, Aalborg University. </institution>
Reference-contexts: SS #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models <ref> [92, 58, 27] </ref>. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes [30, 31]. * Methods for handling priors [5]. * Subsampling to
Reference: [59] <author> M. Li and P. Vitanyi. </author> <title> Inductive reasoning and Kolmogorov complexity. </title> <journal> Journal of Computer and Systems Science, </journal> <volume> 44(2) </volume> <pages> 343-384, </pages> <year> 1992. </year>
Reference: [60] <author> P. McCullagh and J.A. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: SS #99 SECTION VII. Essentials: * check list of representations; * check list of methods; * check list of theory; * check list of fields (see Section Ib). SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models <ref> [38, 60] </ref> trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62,
Reference: [61] <author> G. McLachlan. </author> <title> Discriminant Analysis and Statistical Pattern Recognition. </title> <address> Wi-ley, New York, </address> <year> 1992. </year>
Reference: [62] <author> G. J. McLachlan and K. E. Basford. </author> <title> Mixture Models: Inference and Applications to Clustering. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: SS #59 MIXTURE MODELS: EXAMPLE, cont. The graphical model representing this learning problem, assuming Gaussians inde pendent between x 1 and x 2 , is given below. SS #60 MIXTURE MODELS: MOTIVATION Mixture models are ubiquitous in data analysis <ref> [96, 62] </ref>. <p> models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models <ref> [96, 62, 50] </ref>.
Reference: [63] <author> I. Meilijson. </author> <title> A fast improvement to the EM algorithm on its own terms. </title> <journal> J. Roy. Statist. Soc. B, </journal> <volume> 51(1) </volume> <pages> 127-138, </pages> <year> 1989. </year>
Reference-contexts: It computes the MAP for . p (jx 1 ; : : : ; x N ; M) : * Convergence slow near a local maxima so some implementations switch to con jugate gradient or other methods <ref> [63] </ref> when near solution. * To understand the general approach, you need to consider the exponential family of distributions [16, 14], although it applies more generally (e.g., [53]).
Reference: [64] <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor, editors. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horword, </publisher> <address> Hertfordshire, England, </address> <year> 1994. </year>
Reference: [65] <author> S. Minton, M.D. Johnson, A.B. Philips, and P. Laird. </author> <title> Solving large-scale constraint-satisfaction and scheduling problems using a heuristic repair method. </title> <booktitle> In Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 17-24, </pages> <address> Boston, Massachusetts, </address> <year> 1990. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: N ; M) : * Sampling generates dependent samples of that nevertheless have large sample properties similar to the posterior distribution p (jx 1 ; : : : ; x N ; M) [68, 79]. * Gibbs sampling corresponds to simulated annealing with fixed temperature [97]. * Local repair/search methods <ref> [52, 65] </ref> correspond to simulated annealing with zero temperature, so Gibbs sampling is probabilistic local search. * Gibbs sampling can be applied more generally to many learning and data anal ysis problems [35]. SS #71 K-MEANS vs. EM vs.
Reference: [66] <editor> Machine Learning: </editor> <booktitle> Proc. of the Tenth International Conference, </booktitle> <address> Amherst, Mas-sachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [67] <author> R. Musick, J. Catlett, and S. Russell. </author> <title> Decision theoretic subsampling for induction on large databases. </title> <booktitle> In ML10 [66]. </booktitle>
Reference-contexts: Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes [30, 31]. * Methods for handling priors [5]. * Subsampling to handle large datasets <ref> [67] </ref>. <p> decision lists and devise a MAP algorithm using local search to learn decision lists; * do Kernel density or nearest neighbor type density estimation with a proba bilistic mixture model; SS #104 SOME RESEARCH QUESTIONS Adapt the standard generic algorithms to: * handle large samples more efficiently e.g., by subsampling <ref> [67] </ref>; * run on parallel computers, e.g., easy for Gibbs; * incorporate and adjust for missing values efficiently; * approximate/estimate sample and run-time complexity for a given problem specification relative to a particular algorithm (EM, Gibbs, etc.). SS #105
Reference: [68] <author> R.M. Neal. </author> <title> Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: Incremen tal versions of each of these algorithms also exist (e.g., [95]). These are: 1. k-means clustering and related algorithms [29], 2. The Expectation-Maximization (EM) algorithm [27]. 3. Gibbs sampling [49, 35], and more general purpose Markov chain Monte Carlo algorithms <ref> [39, 68, 79] </ref>. These are listed in terms of: * increasing computational complexity, * increasing statistical sophistication, * increasing accuracy, * decreasing bias. SS #63 K-MEANS ALGORITHM Initialize: class centers. Repeat: until converges, 1. Assign cases to their most likely class. 2. <p> the distribution p (jx 1 ; h 1 ; : : : ; x N ; h N ; M) : * Sampling generates dependent samples of that nevertheless have large sample properties similar to the posterior distribution p (jx 1 ; : : : ; x N ; M) <ref> [68, 79] </ref>. * Gibbs sampling corresponds to simulated annealing with fixed temperature [97]. * Local repair/search methods [52, 65] correspond to simulated annealing with zero temperature, so Gibbs sampling is probabilistic local search. * Gibbs sampling can be applied more generally to many learning and data anal ysis problems [35]. <p> #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods <ref> [68, 79, 35, 36] </ref> * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes [30, 31]. * Methods for handling priors [5]. * Subsampling to handle large datasets [67]. <p> LIST OF THEORY * Asymptotic, and large sample results (convergence, order of magnitude, etc). * Transformations between MDL, Bayesian methods [99], and others. * PAC and its Bayesian interpretation [12]. * Sample complexity, VC dimension, and other measures of problem complexity (see recent COLT work). * Monte Carlo sampling theory <ref> [68] </ref>. * Interpretations of probability and rationality, standard "paradoxes" and their resolution. * Broader issues such as Occam's razor, subjectivity vs. objectivity, the principle of indifference [3, 51].
Reference: [69] <author> J.J. Oliver. </author> <title> Decision graphs an extension of decision trees. </title> <booktitle> In Proceedings of the Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 343-350, </pages> <year> 1993. </year> <note> Extended version available as TR 173, </note> <institution> Department of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, AUSTRALIA. </address>
Reference-contexts: SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees [75, 7, 11], rules, and graph models <ref> [69, 55] </ref>, feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50].
Reference: [70] <author> R.M. Oliver and J.Q. Smith, </author> <title> editors. Influence Diagrams, Belief Nets and Decision Analysis. </title> <publisher> Wiley, </publisher> <year> 1990. </year>
Reference-contexts: Conditional multivariate distributions: various linear models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes <ref> [72, 70] </ref> * Mixture models [96, 62, 50].
Reference: [71] <author> M. Opper and D. Haussler. </author> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> In COLT'91: 1991 Workshop on Computational Learning Theory, </booktitle> <pages> pages 75-87. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: randomly chooses the weights w for a perceptron according to its posterior probability p (wjsample). * BAYES algorithm does the full Bayesian approach: making a new prediction by doing a posterior weighted average of all weights w. * Average learning curve for GIBBS (upper solid) and BAYES (lower) reproduced from <ref> [71] </ref>. SS #97 PERCEPTRONS, cont. SS #98 LEARNING THEORIES: SUMMARY * Theories can be interpreted and compared with language of probability and decision theory [42, 12].
Reference: [72] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Conditional multivariate distributions: various linear models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes <ref> [72, 70] </ref> * Mixture models [96, 62, 50].
Reference: [73] <author> S.J. </author> <title> Press. Bayesian Statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: SS #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) <ref> [73, 21] </ref>. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method
Reference: [74] <author> J.R. Quinlan. </author> <title> Unknown attribute values in induction. </title> <editor> In A.M. Segre, editor, </editor> <booktitle> Proceedings of the Sixth International Machine Learning Workshop, </booktitle> <publisher> Cornell, </publisher> <address> New York, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: their core. * In general, missing data is handled with mixture models [96]. * We have a case with class class, known data, given-fields, and missing data missing-fields. fields = (given-fields; missing-fields) : * How do we modify the learning algorithm? For most classes of algorithms, well understood alternatives exist <ref> [95, 74, 15, 90] </ref>. <p> SS #83 METHODS: FRACTIONAL EXAMPLES * Like the fill-in methods but do multiple fill-ins, and assign each a probability (denoted p (case) in the figure). * Better approximates the sum. * See <ref> [74, 15, 90] </ref>. OTHER METHODS All the usual methods for handling mixture models. * EM algorithm. * Gibbs sampling. * Incremental versions of both [95]. SS #85 SECTION VIb.
Reference: [75] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Essentials: * check list of representations; * check list of methods; * check list of theory; * check list of fields (see Section Ib). SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees <ref> [75, 7, 11] </ref>, rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50].
Reference: [76] <author> J.R. Quinlan and R.L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference: [77] <author> B. D. Ripley. </author> <title> Neural networks and related methods for classification. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 56(3), </volume> <year> 1994. </year>
Reference-contexts: SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks <ref> [82, 77] </ref>. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50].
Reference: [78] <author> B.D. Ripley. </author> <title> Spatial Statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. <ref> [34, 78, 102, 45] </ref> * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50].
Reference: [79] <author> B.D. Ripley. </author> <title> Stochastic Simulation. </title> <publisher> John Wiley & Sons, </publisher> <year> 1987. </year>
Reference-contexts: Incremen tal versions of each of these algorithms also exist (e.g., [95]). These are: 1. k-means clustering and related algorithms [29], 2. The Expectation-Maximization (EM) algorithm [27]. 3. Gibbs sampling [49, 35], and more general purpose Markov chain Monte Carlo algorithms <ref> [39, 68, 79] </ref>. These are listed in terms of: * increasing computational complexity, * increasing statistical sophistication, * increasing accuracy, * decreasing bias. SS #63 K-MEANS ALGORITHM Initialize: class centers. Repeat: until converges, 1. Assign cases to their most likely class. 2. <p> the distribution p (jx 1 ; h 1 ; : : : ; x N ; h N ; M) : * Sampling generates dependent samples of that nevertheless have large sample properties similar to the posterior distribution p (jx 1 ; : : : ; x N ; M) <ref> [68, 79] </ref>. * Gibbs sampling corresponds to simulated annealing with fixed temperature [97]. * Local repair/search methods [52, 65] correspond to simulated annealing with zero temperature, so Gibbs sampling is probabilistic local search. * Gibbs sampling can be applied more generally to many learning and data anal ysis problems [35]. <p> #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods <ref> [68, 79, 35, 36] </ref> * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes [30, 31]. * Methods for handling priors [5]. * Subsampling to handle large datasets [67].
Reference: [80] <author> B.D. Ripley. </author> <title> Statistical aspects of neural networks. </title> <type> In Invited lectures for Sem--Stat (Seminaire Europeen de Statistique), </type> <address> Sandbjerg, Denmark, 1993. </address> <publisher> Chapman and Hall. </publisher>
Reference: [81] <author> J. Rissanen. </author> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 49(3) </volume> <pages> 223-239, </pages> <year> 1987. </year>
Reference: [82] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In David E. Rumelhart, James L. McClelland, and the PDP Research Group, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> page 318. </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks <ref> [82, 77] </ref>. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50].
Reference: [83] <author> S.R. Safavian and D. Landgrebe. </author> <title> A survey of decision tree classification methodology. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(3), </volume> <year> 1991. </year>
Reference: [84] <author> C. Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 13(1), </volume> <year> 1993. </year>
Reference: [85] <author> D. W. Scott. </author> <title> Multivariate Density Estimation: Theory, Practice, and Visualization. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1992. </year>
Reference: [86] <author> H.S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Learning curves in large neural networks. </title> <booktitle> In COLT'91: Workshop on Computational Learning Theory. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [87] <author> H.S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Physical Review A, </journal> <volume> 45 </volume> <pages> 6056-6091, </pages> <year> 1992. </year>
Reference: [88] <author> R.D. Shachter. </author> <title> An ordered examination of influence diagrams. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 535-563, </pages> <year> 1990. </year>
Reference-contexts: The DAG also models the error as a Gaussian. SS #55 GRAPHICAL MODELS Other problems can be represented in graphical models using: * known and unknown variables, * deterministic nodes <ref> [88] </ref>, * standard probabilities functions at nodes (Gaussian, multinomial, Dirichlet, lo gistic, etc.), * mixed directed and undirected arcs [32], * optional arcs (indicating alternative models) [14]. * plates representing samples [14].
Reference: [89] <author> R.D. Shachter and D. Heckerman. </author> <title> Thinking backwards for knowledge acquisition. </title> <journal> AI Magazine, </journal> <volume> 8(Fall):55-61, </volume> <year> 1987. </year>
Reference: [90] <author> D.J. Spiegelhalter and S.L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 579-605, </pages> <year> 1990. </year>
Reference-contexts: their core. * In general, missing data is handled with mixture models [96]. * We have a case with class class, known data, given-fields, and missing data missing-fields. fields = (given-fields; missing-fields) : * How do we modify the learning algorithm? For most classes of algorithms, well understood alternatives exist <ref> [95, 74, 15, 90] </ref>. <p> SS #83 METHODS: FRACTIONAL EXAMPLES * Like the fill-in methods but do multiple fill-ins, and assign each a probability (denoted p (case) in the figure). * Better approximates the sum. * See <ref> [74, 15, 90] </ref>. OTHER METHODS All the usual methods for handling mixture models. * EM algorithm. * Gibbs sampling. * Incremental versions of both [95]. SS #85 SECTION VIb.
Reference: [91] <editor> S.M. Stigler. </editor> <booktitle> The History of Statistics: The Measurement of Uncertainty before 1900. </booktitle> <publisher> Belknap Press of Harvard Uni. Press, </publisher> <address> Cambridge, Massachussetts, </address> <year> 1986. </year>
Reference: [92] <author> M.A. Tanner. </author> <title> Tools for Statistical Inference. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1993. </year>
Reference-contexts: The graphical model representing this learning problem, assuming Gaussians inde pendent between x 1 and x 2 , is given below. SS #60 MIXTURE MODELS: MOTIVATION Mixture models are ubiquitous in data analysis [96, 62]. They model: * Missing values in other problems (trees, feed-forward networks, etc.) <ref> [15, 92] </ref>. * Latent or hidden variables, e.g., medical syndromes. * Unsupervised learning and clustering, e.g., Autoclass [20]. * Supervised learning and multivariate splits in trees [53]. * Robust regression [57]. * Non-parametric density estimation (i.e., equivalent to Kernel density estimation and nearest neighbor). * Rule-based systems with multi-firing probabilistic rules. <p> SS #62 ALGORITHMS ON MIXTURES Many general purpose algorithms exist for learning with mixtures <ref> [96, 92] </ref>. Incremen tal versions of each of these algorithms also exist (e.g., [95]). These are: 1. k-means clustering and related algorithms [29], 2. The Expectation-Maximization (EM) algorithm [27]. 3. Gibbs sampling [49, 35], and more general purpose Markov chain Monte Carlo algorithms [39, 68, 79]. <p> SS #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values <ref> [92, 54, 94] </ref>. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes <p> SS #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values [92, 54, 94]. * EM, ICM and other deterministic Gibbs models <ref> [92, 58, 27] </ref>. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes [30, 31]. * Methods for handling priors [5]. * Subsampling to
Reference: [93] <author> H.H. Thodberg. </author> <title> Bayesian backprop in action: Pruning, ensembles, error bars and applications to spectroscopy. </title> <booktitle> In Advances in Neural Information Processing Systems 5 (NIPS*93). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: [94] <author> L. Tierney and J.B. Kadane. </author> <title> Accurate approximations for posterior moments and marginal densities. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 81(393) </volume> <pages> 82-86, </pages> <year> 1986. </year>
Reference-contexts: SS #101 CHECK LIST OF METHODS * Maximum A Posterior (MAP) [73, 21]. * Exact Bayes factors [11, 14]. * Laplace's method, approximate Bayes factors, marginals and expected values <ref> [92, 54, 94] </ref>. * EM, ICM and other deterministic Gibbs models [92, 58, 27]. * Gibbs sampling and other Monte Carlo methods [68, 79, 35, 36] * Differentiation, i.e., back-prop, (for Laplace's method and MAP). * Methods for making the above parallel or on-line. * Cross validation, bootstrap and empirical Bayes
Reference: [95] <author> D.M. Titterington. </author> <title> Recursive parameter estimation using incomplete data. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 46(2) </volume> <pages> 257-267, </pages> <year> 1984. </year>
Reference-contexts: SS #62 ALGORITHMS ON MIXTURES Many general purpose algorithms exist for learning with mixtures [96, 92]. Incremen tal versions of each of these algorithms also exist (e.g., <ref> [95] </ref>). These are: 1. k-means clustering and related algorithms [29], 2. The Expectation-Maximization (EM) algorithm [27]. 3. Gibbs sampling [49, 35], and more general purpose Markov chain Monte Carlo algorithms [39, 68, 79]. <p> their core. * In general, missing data is handled with mixture models [96]. * We have a case with class class, known data, given-fields, and missing data missing-fields. fields = (given-fields; missing-fields) : * How do we modify the learning algorithm? For most classes of algorithms, well understood alternatives exist <ref> [95, 74, 15, 90] </ref>. <p> OTHER METHODS All the usual methods for handling mixture models. * EM algorithm. * Gibbs sampling. * Incremental versions of both <ref> [95] </ref>. SS #85 SECTION VIb. Specialist topics: * missing values; * knowledge discovery and refine ment; knowledge discovery requires prototyping and refinement of prob abilistic models; * connections between theories.
Reference: [96] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1985. </year>
Reference-contexts: SS #59 MIXTURE MODELS: EXAMPLE, cont. The graphical model representing this learning problem, assuming Gaussians inde pendent between x 1 and x 2 , is given below. SS #60 MIXTURE MODELS: MOTIVATION Mixture models are ubiquitous in data analysis <ref> [96, 62] </ref>. <p> SS #62 ALGORITHMS ON MIXTURES Many general purpose algorithms exist for learning with mixtures <ref> [96, 92] </ref>. Incremen tal versions of each of these algorithms also exist (e.g., [95]). These are: 1. k-means clustering and related algorithms [29], 2. The Expectation-Maximization (EM) algorithm [27]. 3. Gibbs sampling [49, 35], and more general purpose Markov chain Monte Carlo algorithms [39, 68, 79]. <p> Why? they all have a common model of the data likelihood at their core. * In general, missing data is handled with mixture models <ref> [96] </ref>. * We have a case with class class, known data, given-fields, and missing data missing-fields. fields = (given-fields; missing-fields) : * How do we modify the learning algorithm? For most classes of algorithms, well understood alternatives exist [95, 74, 15, 90]. <p> models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. [34, 78, 102, 45] * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models <ref> [96, 62, 50] </ref>.
Reference: [97] <author> P.J.M. van Laarhoven and E.H.L. Aarts. </author> <title> Simulated Annealing: Theory and Applications. </title> <address> D. </address> <publisher> Reidel, </publisher> <address> Dordrecht, </address> <year> 1987. </year>
Reference-contexts: ; x N ; h N ; M) : * Sampling generates dependent samples of that nevertheless have large sample properties similar to the posterior distribution p (jx 1 ; : : : ; x N ; M) [68, 79]. * Gibbs sampling corresponds to simulated annealing with fixed temperature <ref> [97] </ref>. * Local repair/search methods [52, 65] correspond to simulated annealing with zero temperature, so Gibbs sampling is probabilistic local search. * Gibbs sampling can be applied more generally to many learning and data anal ysis problems [35]. SS #71 K-MEANS vs. EM vs.
Reference: [98] <author> V. Vapnik. </author> <title> Estimation of Dependencies Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference: [99] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact encoding. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 49(3) </volume> <pages> 240-265, </pages> <year> 1987. </year>
Reference-contexts: This is approximately equivalent to the Bayesian MAP approach if we make the transformation <ref> [99] </ref>: DL (S) = log p (S) ; DL (samplejS; ) = log p (samplejS; ) ; DL (precision ()jS) 1 log det V ariance (jS; sample) SS #96 STATISTICAL PHYSICS APPLIED TO PERCEPTRONS (nothing on Bayesian networks available!) * Input is n binary variables and the output is a binary <p> SS #102 CHECK LIST OF THEORY * Asymptotic, and large sample results (convergence, order of magnitude, etc). * Transformations between MDL, Bayesian methods <ref> [99] </ref>, and others. * PAC and its Bayesian interpretation [12]. * Sample complexity, VC dimension, and other measures of problem complexity (see recent COLT work). * Monte Carlo sampling theory [68]. * Interpretations of probability and rationality, standard "paradoxes" and their resolution. * Broader issues such as Occam's razor, subjectivity vs.
Reference: [100] <author> C.S. Wallace and J.D. Patrick. </author> <title> Coding decision trees. </title> <booktitle> Machine Learning, </booktitle> <year> 1993. </year>
Reference: [101] <author> S.M. Weiss and C.A. </author> <title> Kulikowski. Computer Systems That Learn. </title> <publisher> Morgan-Kaufmann, </publisher> <year> 1991. </year>
Reference: [102] <author> J. Whittaker. </author> <title> Graphical Models in Applied Multivariate Statistics. </title> <publisher> Wiley, </publisher> <year> 1990. </year>
Reference-contexts: SS #100 CHECK LIST OF REPRESENTATIONS * Basic distributions: Gaussians, uniform, multinomial. * Conditional multivariate distributions: various linear models [38, 60] trees [75, 7, 11], rules, and graph models [69, 55], feed-forward networks [82, 77]. * Undirected graphical models, i.e., Markov random fields, for vision, etc. <ref> [34, 78, 102, 45] </ref> * Directed models, i.e., influence diagrams and Bayesian networks, including de terministic nodes [72, 70] * Mixture models [96, 62, 50].
Reference: [103] <author> D. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(2) </volume> <pages> 241-260, </pages> <year> 1992. </year>
References-found: 103

