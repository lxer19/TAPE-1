URL: http://www-csag.cs.uiuc.edu/papers/fmdcs-usenixnt.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Email: (-mbbuchan,achien-@cs.uiuc.edu)  
Title: Coordinated Thread Scheduling for Workstation Clusters Under Windows NT  
Author: Matt Buchanan and Andrew A. Chien 
Affiliation: Concurrent Systems Architecture Group Department of Computer Science, University of Illinois  
Abstract: Coordinated thread scheduling is a critical factor in achieving good performance for tightly-coupled parallel jobs on workstation clusters. We are building a coordinated scheduling system that coexists with the Windows NT scheduler which both provides coordinated scheduling and can generalize to provide a wide range of resource abstractions. We describe the basic approach, called demand-based coscheduling, and implementation in the context of Windows NT. We report preliminary performance data characterizing the effectiveness of our approach and describe benefits and limitations of our approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ousterhout, J. K. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of the 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: 1. Introduction Coordinated scheduling for parallel jobs across the nodes of a multiprocessor is well-known to produce benefits in both system and individual job efficiency <ref> [1, 5, 6] </ref>. Without coordinated scheduling, parallel jobs suffer high communication latencies between constituent threads due to context switching. This effect is exacerbated if the thread scheduling for individual nodes is done by independent timesharing schedulers.
Reference: [2] <author> Von Eicken, T, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference: [3] <author> Russinovich, M. </author> <title> Differences between Windows NT Workstation and Server. </title> <note> Available from http://www.ntinternals.com/tune.txt. </note>
Reference-contexts: With high performance networks that achieve latencies in the range of tens of microseconds, the scheduling and context switching latency can increase communication latency by several orders of magnitude. For example, under Windows NT, CPU quanta vary from 20 ms to 120 ms <ref> [3] </ref>, implying that uncoordinated scheduling can cause best-case latencies on the order of 10 ms to explode by three to four orders of magnitude, nullifying many benefits of fast communication. <p> is true: 2 (T C - T P ) + C T q R where T C is the current time, T P is the time the host was last interrupted to schedule this thread, T q is the length of a CPU quantum (120 ms under Windows NT Server <ref> [3] </ref>), R is the number of threads waiting for the CPU, E and C are constants chosen to balance fairness and performance.
Reference: [4] <author> Sobalvarro, P. G. </author> <title> Demand-based coscheduling of parallel jobs on multiprogrammed multiprocessors. </title> <type> Ph.D. thesis, </type> <institution> Laboratory for Computer Science, Mas-sachusetts Institute of Technology, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: Furthermore, in a cluster environment, kernel replacement is difficult at best, so we restrict ourselves to approaches that involve augmentation of existing operating system infrastructure. Our approach to coordinated scheduling is Demand-based Coscheduling (DCS) <ref> [4, 7] </ref> which achieves coordination by observing the communication between threads. The essence of this approach is the observation that only those threads which are communicating need be coscheduled, and this admits a bottom-up, emergent scheduling approach. <p> The LCP sets a flag if a communicating thread is running, and when a packet arrives, evaluates the following condition: !threadIsRunning && fairToPreempt () If the condition is true, then the LCP interrupts the host. The fairness criteria is critical and we adopt the approach taken in <ref> [4] </ref> to decide whether to interrupt the host.
Reference: [5] <author> Feitelson, D. G. and L. Rudolph. </author> <title> Coscheduling based on runtime identification of activity working sets. </title> <journal> In International Journal of parallel Programming, </journal> <volume> Vol. 23, No. 2, </volume> <pages> pages 135-160, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: 1. Introduction Coordinated scheduling for parallel jobs across the nodes of a multiprocessor is well-known to produce benefits in both system and individual job efficiency <ref> [1, 5, 6] </ref>. Without coordinated scheduling, parallel jobs suffer high communication latencies between constituent threads due to context switching. This effect is exacerbated if the thread scheduling for individual nodes is done by independent timesharing schedulers.
Reference: [6] <author> Dusseau, A. C., R. H. Arpaci, and D. E. Culler. </author> <title> Effective distributed scheduling of parallel work-loads. </title> <booktitle> In ACM SIGMETRICS 96 Conference on the Measurement and Modeling of Computer Systems, </booktitle> <year> 1996. </year>
Reference-contexts: 1. Introduction Coordinated scheduling for parallel jobs across the nodes of a multiprocessor is well-known to produce benefits in both system and individual job efficiency <ref> [1, 5, 6] </ref>. Without coordinated scheduling, parallel jobs suffer high communication latencies between constituent threads due to context switching. This effect is exacerbated if the thread scheduling for individual nodes is done by independent timesharing schedulers.
Reference: [7] <author> Sobalvarro, P. G. and W. E. Weihl. </author> <title> Demand-based coscheduling of parallel jobs on multipro-grammed multiprocessors. </title> <booktitle> In Proceedings of the Parallel Job Scheduling Workshop at IPPS 95, </booktitle> <year> 1995. </year> <note> Available in Springer-Verlag Lecture Notes in Computer Science, Vol. 949. </note>
Reference-contexts: Furthermore, in a cluster environment, kernel replacement is difficult at best, so we restrict ourselves to approaches that involve augmentation of existing operating system infrastructure. Our approach to coordinated scheduling is Demand-based Coscheduling (DCS) <ref> [4, 7] </ref> which achieves coordination by observing the communication between threads. The essence of this approach is the observation that only those threads which are communicating need be coscheduled, and this admits a bottom-up, emergent scheduling approach.
Reference: [8] <author> Von Eicken, T., A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> a user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995.. </year>
Reference: [9] <author> Tezuka, H. A. Hori, and Y. Ishikawa. </author> <title> Design and implementation of PM: a communication library for workstation clusters. </title> <booktitle> In JSPP, </booktitle> <year> 1996. </year>
Reference: [10] <author> Microsoft. </author> <title> Windows NT device driver kit documentation. </title>
Reference-contexts: The scheduler uses several other criteria in addition to the callbacks return value in choosing a thread to run, including the time the thread has been waiting for a CPU and its processor affinity <ref> [10] </ref>. Of course, if a competitor thread has a higher priority than a communicating thread, the scheduler may never propose a communicating thread to the callback. Thus, there is no guarantee that a communicating thread will be offered, much less scheduled at an appropriate time. <p> However, since the Windows NT kernel does not export a well-defined interface to device drivers for modifying the priorities of arbitrary threads, (only for boosting the priority of driver created system threads <ref> [10] </ref>), we were forced to use an undocumented internal interface for thread priority modification.
Reference: [11] <author> Russinovich, M. and B. Cogswell. NTExport documentation. </author> <note> Available from http://www.ntinternals.com.. </note>
Reference-contexts: By using a tool called NTExport <ref> [11] </ref> that uses the symbol information distributed with every build of Windows NT (intended for kernel debugging support) to build an export library for the kernel, we exported the appropriate calls to our driver, enabling thread priority modification. (We hope to find a more portable yet equally effective approach to solve
Reference: [12] <author> Custer, H. </author> <title> Inside Windows NT. </title> <publisher> Microsoft Press (Redmond, </publisher> <address> WA), </address> <year> 1993. </year>
Reference: [13] <editor> Boden, N., et. al. </editor> <booktitle> Myrineta gigabit-per-second local-area network. In IEEE Micro, </booktitle> <pages> pages 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Our implementation of DCS in Windows NT coexists with release binaries of the operating system require a customized device driver for the network card (in this case the Myrinet <ref> [13] </ref> card). This driver memory maps the network device into the user address space. The device driver, combined with special Myrinet firmware, influences the operating system schedulers decisions by boosting thread priorities, based on communication traffic and system thread scheduling.
Reference: [14] <author> Pakin, S., Karamcheti, V. and Chien, A. A. </author> <title> Fast Messages: Efficient, Portable Communication for Workstation Clusters and MPPs, </title> <journal> IEEE Concurrency 5(2), </journal> <month> April </month> <year> 1997, </year> <pages> pages 60-73. </pages>
Reference-contexts: Such layers are essential because they make available the high performance of the underlying network hardware to applications. Illinois Fast Messages (FM) is such a messaging layer <ref> [14] </ref>, and is a key part of the Concurrent System Architecture Groups High Performance Virtual Machines (HPVM) project [15], which seeks to leverage clusters of commodity workstations running Windows NT to run high-performance parallel and distributed applications.
Reference: [15] <author> Chien, A., et. al. </author> <title> High Performance Virtual Machines (HPVM): Clusters with Supercomputing Performance and APIs, </title> <booktitle> Proceedings of the Eighth SIAM Conference on Parallel Processing, </booktitle> <address> March 1997, Minneapolis, Minnesota. </address>
Reference-contexts: Such layers are essential because they make available the high performance of the underlying network hardware to applications. Illinois Fast Messages (FM) is such a messaging layer [14], and is a key part of the Concurrent System Architecture Groups High Performance Virtual Machines (HPVM) project <ref> [15] </ref>, which seeks to leverage clusters of commodity workstations running Windows NT to run high-performance parallel and distributed applications. Fast Messages can deliver userspace to userspace communication latencies as low as 8 ms and overheads of a few ms.
References-found: 15

