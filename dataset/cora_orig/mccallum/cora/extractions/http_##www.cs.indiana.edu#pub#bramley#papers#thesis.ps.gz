URL: http://www.cs.indiana.edu/pub/bramley/papers/thesis.ps.gz
Refering-URL: http://www.cs.indiana.edu/pub/bramley/papers/
Root-URL: http://www.cs.indiana.edu
Title: ROW PROJECTION METHODS FOR LINEAR SYSTEMS  
Author: Randall Barry Bramley S.-P. Han, H. Lotz, A. Peressini, B. Reznick, and J. Uhl. W. Zaring 
Note: Over the years the faculty in the  has also given both professional and personal support. In particular I thank  deserves special mention for the good advice and willing ear he has provided not only to myself but also to several hundred graduate students. I also thank the people who gave me a shove in the right direction at  
Date: 1981  
Address: Arlington,  1984  
Affiliation: B.S., University of Texas at  M.S., University of Illinois at Urbana-Champaign,  Mathematics Department at the University of Illinois  
Abstract: This thesis would not have been possible without the generous help of Ahmed Sameh, my graduate advisor. The many hours of conversation and consultation he provided are beyond value, and his careful editing has spared the reader some difficult reading. I also thank the rest of the thesis committee, W. Gear, R. Skeel, T. Kerkhoven, and S. Gallopoulos for the time and help they have given. The Center for Supercomputing Research and Development has provided me with financial support and more importantly, an access to both hardware and people that is unmatched at any university. In particular, Mike Berry, Sy-Shin Lo, Ulrike Meier, and Youcef Saad have generously given me valuable help and information in many conversations, and Jeannie Covert, Deb Hud-son, and Mark Washburn have conscientiously kept the necessary equipment running. Many past and present students have also helped me. Ed Anderson optimized for the Alliant FX/8 the PCGPAK library used for some of the test results, Hsin-Chu Chen taught me the ropes around CSRD and much more, Ding-Kai Chen has been a valuable source of answers to machine architecture questions, Dave Semararo has provided useful conversations that sometimes involved CFD, and Tom Conte developed and answered questions about the Latex macros used for writing this thesis. I especially thank Mei-Qin Chen and Xiaoge Wang for helpful mathematical discussions and for being willing to listen to long pointless stories. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Afriat, </author> <title> Orthogonal and oblique projectors and the characteristics of pairs of vector spaces, </title> <journal> Proc. Camb. Phil. Soc., </journal> <pages> 800-816, 53(1957). </pages>
Reference-contexts: 0.4 0.6 0.8 1.0 0.0 0.4 0.8 The above algorithm generates residuals r k that satisfy r k = p k (I Q)r 0 (5.7) where p k () = T k ffi ! fl (5.8) and T k is the k th Chebyshev polynomial of the first kind on <ref> [1; 1] </ref>. The shifted and normalized Chebyshev polynomial p k is thus a residual polynomial as defined in Chapter 1, i.e., p k (0) = 1. Figure 5.1 shows p 6 for - = 0:05. <p> In 1985 Y. Saad, A. Sameh, and P. Saylor [49] proposed a scheme called block Stiefel (BST) acceleration to elude this obstacle. Suppose that in Chebyshev (x 0 ; -; K) acceleration is chosen larger than 1 . Eigencomponents of r k corresponding to eigenvalues in <ref> [-; 1] </ref> are reduced even faster than if - = 1 were chosen, while eigencomponents of r k corresponding to eigenvalues in (0; -) are reduced at a much slower rate. <p> Figure 6.5 shows the residual norm for each iteration for Problem 5 with n = 13824. The curve levels off as the eigencomponents in <ref> [-; 1] </ref> become small. Then the projection step does not cause a large decrease in the residual because of the concommittent increase in the eigencomponents corresponding to the large eigenvalues. The estimate is adjusted and then the process starts over. For well conditioned problems the method behaves like Chebyshev acceleration.
Reference: [2] <author> R. Ansorge, </author> <title> Connections between the Cimmino-methods and the Kaczmarz-methods for the solution of singular and regular systems of equations, </title> <booktitle> Computing, </booktitle> <pages> 367-375, 33(1984). </pages>
Reference: [3] <author> A. Ben-Israel, T. Greville, </author> <title> Generalized Inverses: Theory and Applications, </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> (1974). </year>
Reference: [4] <author> A. Berman, R. Plemmons, </author> <title> Nonnegative Matrices in the Mathematical Sciences, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> (1979). </year>
Reference: [5] <author> G. Birkhhoff, R. Lynch, </author> <title> Numerical Solution of Elliptic Problems, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> (1984). </year>
Reference: [6] <author> A. Bjorck, T. Elfving, </author> <title> Accelerated projection methods for computing pseudo-inverse solutions of systems of linear equations, </title> <journal> BIT, </journal> <pages> 145-163, </pages> <year> 19(1979). </year>
Reference-contexts: However, for problems with a few extremely small eigenvalues, BST acceleration can potentially outperform Chebyshev acceleration. 5.3 CG Acceleration CG acceleration for RP methods was proposed in <ref> [6] </ref> and tested in [38, 39, 9]. CG acceleration requires no parameter estimation and is an optimal process, implicitly finding the optimal residual polynomial. <p> x k1 ] (5.54) In contrast CG acceleration of the Kaczmarz system minimizes k S T V (x k x fl ) k over the subspace S V S T V flspan [x 0 x fl ; : : : ; x k1 x fl ], a result established in <ref> [6] </ref>. Chapter 6 Testing Results The RP algorithms developed and proposed in this thesis have the goals of robustness and efficiency, in that order. This chapter presents numerical results that show both goals are achieved and provides the basis for further analysis of RP methods.
Reference: [7] <author> A. Bjorck, G. Golub, </author> <title> Numerical methods for computing angles between linear subspaces, </title> <journal> Math. Comp. </journal> <pages> 579-594, </pages> <month> 27 </month> <year> (1973). </year>
Reference: [8] <author> J. Bollen, </author> <title> Round-off error analysis of descent methods for solving linear equations, </title> <type> Ph.D Thesis, </type> <institution> Technische Hogeschool Eindhoven, </institution> <year> (1980). </year>
Reference-contexts: This section does not present an error analysis of the acceleration methods used since such analysis has been presented for the conjugate gradient method in <ref> [8] </ref> and for Chebyshev acceleration in [25]. Block Stiefel acceleration is discussed in detail in the next chapter. 4.2.1 Perturbational Analysis Let A T = [A 1 ; A 2 ; : : : ; A m ], and set P i = orthogonal projector onto range (A i ).
Reference: [9] <author> R. Bramley, A. Sameh, </author> <title> A robust parallel solver for block tridiag--onal systems, </title> <type> CSRD Tech. </type> <address> Rept. 806, </address> <year> (1988). </year>
Reference-contexts: However, for problems with a few extremely small eigenvalues, BST acceleration can potentially outperform Chebyshev acceleration. 5.3 CG Acceleration CG acceleration for RP methods was proposed in [6] and tested in <ref> [38, 39, 9] </ref>. CG acceleration requires no parameter estimation and is an optimal process, implicitly finding the optimal residual polynomial. <p> For k &gt; 0, d k1 2 range (I P 1 ), so A T 1 (x k1 + ff k d k1 ) = A T 1 x 0 = b 1 (5.51) In <ref> [9] </ref> it was shown how to use these results in the m = 2 case to update the true residual and not require additional multiplications by A in doing so. <p> When combined with this preconditioner CGNE is denoted simply as ICCG. Note that this is not the same as forming the normal equations A T A and then performing an incomplete Cholesky factorization of A T A. Earlier work <ref> [9] </ref> with such a preconditioner for CGNE applied to two dimensional problems showed that it also suffered robustness problems. All of the methods tested, along with their additional memory requirements, are listed in Table 6.1.
Reference: [10] <author> Y. Censor, </author> <title> Row-action methods for huge and sparse systems and their applications, </title> <journal> SIAM Rev., </journal> <pages> 444-466 23(1981). </pages>
Reference: [11] <author> Y. Censor, P. Eggermont, D. Gordon, </author> <title> Strong underrelaxation in Kaczmarz's method for inconsistent systems, </title> <journal> Numer. Math., </journal> <pages> 83-92, 41(1983). </pages>
Reference: [12] <author> G. </author> <type> Cimmino, </type> <institution> Calcolo approssimato per le soluzioni dei sistemi di equazioni lineari, Ric. Sci. Progr. tecn. econom. </institution> <month> naz., </month> <pages> 326-333, 9(1939). </pages>
Reference: [13] <author> P. Concus, G. Golub, </author> <title> A generalized conjugate gradient method for nonsymmetric systems of linear equations, </title> <booktitle> Lecture Notes in Economics and Mathematical Systems: Computing Methods in Applied Sciences and Engineering, </booktitle> <pages> 56-65, </pages> <address> Berlin 134(1976). </address>
Reference: [14] <author> F. Deutsch, </author> <title> Rate of convergence of method of alternating projections, Parametric Optimization and Approximations, </title> <editor> ed. Brosowsfi and Deutsch, </editor> <publisher> Birkhauser Basel: </publisher> <address> ISNM 72, </address> <month> 96-107 </month> <year> (1985). </year>
Reference: [15] <author> J. Dongarra, C. Moler, J. Bunch, G. Stewart, </author> <title> Linpack User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> (1979). </year>
Reference-contexts: The second estimate is that obtained by applying Gerschgorin estimates numerically as the problem is generated, with the estimate for min (C) replaced by that of Corollary 4.1 whenever it is larger. The third estimate is computed using the Linpack routine DPOCO <ref> [15] </ref>. For Problems 2,3, and 5 the theoretical bound is actually smaller than the computed condition number.
Reference: [16] <author> P. Eggermont, G. Herman, A. Lent, </author> <title> Iterative algorithms for large partitioned linear systems, with applications to image reconstruction, </title> <journal> Lin. Alg. Appl., </journal> <pages> 37-67, 40(1981). </pages>
Reference: [17] <author> T. Elfving, </author> <title> Group-iterative methods for consistent and inconsistent linear equations, </title> <institution> Rept. LITH-MAT-R-1977-11, Dept. Math. Linkoping Univ., </institution> <year> (1977). </year>
Reference: [18] <author> T. Elfving, </author> <title> Block iterative methods for consistent and inconsistent linear equations, </title> <journal> Numer. Math., </journal> <pages> 1-12, 35(1980). </pages>
Reference: [19] <author> H. Elman, G. Golub, </author> <title> Iterative methods for cyclically reduced non-self-adjoint linear systems, </title> <institution> UMIACS-TR-88-87, CS-TR-2145, University of Maryland, </institution> <year> (1988). </year> <month> 190 </month>
Reference: [20] <author> H. Elman, </author> <title> Iterative methods for large, sparse, nonsymmetric sys-tems of linear equations, </title> <institution> Yale Univ. Res. Rept. </institution> <month> 229, </month> <year> (1982). </year>
Reference: [21] <author> D. Evans, </author> <title> Parallel algorithms in computational linear algebra, </title> <booktitle> Parallel Computers and Computations, </booktitle> <pages> 55-79, </pages> <editor> ed. J. van Leeuwen, J. Lenstra, </editor> <publisher> Centrum voor Wiskunde en Informatica (1985). </publisher>
Reference: [22] <author> R. Fletcher, </author> <title> Conjugate gradient methods for indefinite systems, </title> <booktitle> Numerical Analysis Dundee 1975, </booktitle> <pages> 73-89, </pages> <editor> ed. G. Watson, </editor> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> (1976). </year>
Reference: [23] <author> J. George, J. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems Prentice-hall, </title> <address> Engelwood Cliffs, N.J. </address> <year> (1981). </year>
Reference: [24] <author> P. Gilbert, </author> <title> Iterative methods for the three-dimensional reconstruction of an object from projections, </title> <journal> J. Theor. Biol., </journal> <pages> 105-117, 36(1972). </pages>
Reference: [25] <author> G. Golub, M. Overton, </author> <title> Convergence of inexact Chebyshev and Richardson iterative methods for solving linear systems, </title> <journal> Numer. Math., </journal> <pages> 571-593, 53(1988). </pages>
Reference-contexts: This section does not present an error analysis of the acceleration methods used since such analysis has been presented for the conjugate gradient method in [8] and for Chebyshev acceleration in <ref> [25] </ref>. Block Stiefel acceleration is discussed in detail in the next chapter. 4.2.1 Perturbational Analysis Let A T = [A 1 ; A 2 ; : : : ; A m ], and set P i = orthogonal projector onto range (A i ).
Reference: [26] <author> G. Golub, C. Van Loan, </author> <title> Matrix Computations, </title> <publisher> John Hopkins University Press, </publisher> <address> Baltimore (1983). </address>
Reference-contexts: This is true for any mesh size even though the overall condition number of A in this case becomes arbitrarily large as N increases. Some bounds on the subproblem condition numbers can be made immediately. Since C j is a submatrix of A, the minimax characterization of singular values <ref> [26, Cor. 8.3-3] </ref> implies that the smallest singular values min satisfy min (C j ) min (A) and that the largest singular values max satisfy max (C j ) max (A), so that (C j ) (A). <p> For this the errors incurred on the separate steps of forming the normal equations and computing the projections using their Cholesky decompositions must be examined. The rounding error analyses of <ref> [26] </ref> and [63] are used, and rounding errors incurred in vector subtraction and matrix-vector multiplications are omitted since the resulting relative errors are minor in comparison. Let t be the machine precision, that is, let t be the number of bits in the mantissa.
Reference: [27] <author> R. Gordon, R. Bender, G. Herman, </author> <title> Algebraic reconstruction techniques (ART) for three-dimensional electron microscopy and x-ray photography, </title> <journal> J. Theoret. Biol., </journal> <pages> 471-481 29(1970). </pages>
Reference: [28] <author> I. Gustafsson, </author> <title> Modified incomplete Cholesky methods, Preconditioning Methods: Theory and Applications, </title> <editor> ed. D. Evans, </editor> <address> 265-293, </address> <publisher> Gordon and Breach, </publisher> <address> New York, </address> <year> (1983). </year>
Reference-contexts: GMRES (10) is implemented also with ILU and MILU preconditioners. The versions of these preconditioners are the ones that require an additional amount of storage equal to the number of nonzeros in the matrix A, as opposed to the versions that require only one additional vector of storage. See <ref> [28] </ref> for a description of these preconditioners, where the high versus low 101 memory requirement versions are distinguished by using asterisk, for exam-ple, ILU versus ILU fl .
Reference: [29] <author> I. Halperin, </author> <title> The product of projection operators, </title> <journal> Acta Sci. Math. </journal> <volume> (Szeged), </volume> <pages> 96-99 23(1962). </pages>
Reference: [30] <author> M. Hanke, W. Niethammer, </author> <title> On the acceleration of Kaczmarz's method for inconsistent linear systems, </title> <institution> Institut fur Praktische Mathematik, Universitat Karlsruhe, </institution> <year> (1988). </year> <month> 191 </month>
Reference: [31] <author> L. Hageman, D. Young, </author> <title> Applied Iterative Methods, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> (1981). </year>
Reference-contexts: Finally, the pseudo-residual is ~ b (I Q)x, in contrast to the true residual b Ax. 5.1 Chebyshev Acceleration The properties and techniques for Chebyshev acceleration have been thoroughly examined <ref> [31, 43, 44] </ref> and are summarized here for RP methods primarily to establish notation and concepts used in Section 5.2. The largest eigenvalue of I Q is assumed to be 1, so that only an estimate of the smallest eigenvalue need be made.
Reference: [32] <author> G. Herman, A. Lent, P. Lutz, </author> <title> Relaxation methods for image reconstruction, </title> <journal> Comm. ACM, </journal> <volume> 152-158, 21(2), </volume> <year> (1978). </year>
Reference: [33] <author> G. Herman, </author> <title> Image Reconstruction from Projections, </title> <publisher> Academic Press, </publisher> <address> New York (1980). </address>
Reference: [34] <author> M. Hestenes, E. </author> <title> Stiefel, Methods of conjugate gradients for solving linear systems, </title> <institution> J. Research Nat. Bur. Standards, </institution> <note> Paper 2379, #6 49(1952). </note>
Reference: [35] <author> D. Jespersen, P. Buning, </author> <title> Accelerating an iterative process by explicit annihilation, </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <pages> 639-651, 6(1985). </pages>
Reference: [36] <author> C. Johnson, </author> <title> A Gersgorin-type lower bound for the smallest singular value, Linear Algebra and its Applications, </title> <type> 1-7, </type> <month> 112 </month> <year> (1989). </year>
Reference-contexts: Although the results shown here are particular for the m = 9 partitioning applied to matrices generated by the seven point central difference operator applied to elliptic problems on the unit cube, for more general matrices A and partitioning schemes a theorem from C. Johnson <ref> [36] </ref> is presented that 63 provides lower bounds on the smallest singular values of the submatrices A i . <p> Bounds on the smallest singular values of rectangular matrices allow choosing partitions of more general matrices that also are provably well conditioned without choosing m = N. Three such bounds are presented in <ref> [36] </ref>, the best of which is Theorem 4.3 Let C = (c ij ) be an s fi n matrix with s n.
Reference: [37] <author> S. Kaczmarz, Angenaherte Auflosung von Systemen linearer Gle-ichungen, Bull. </author> <note> intern. </note> <institution> Acad. polonaise Sci. lettres (Cracouie); Class sci. math. natur.: Seira A. Sci. Math., </institution> <month> 355-357 </month> <year> (1939). </year>
Reference: [38] <author> C. Kamath, </author> <title> Solution of nonsymmetric systems of equations on a multiprocessor, </title> <journal> CSRD Rept. </journal> <volume> 591, </volume> <year> (1986). </year>
Reference-contexts: However, for problems with a few extremely small eigenvalues, BST acceleration can potentially outperform Chebyshev acceleration. 5.3 CG Acceleration CG acceleration for RP methods was proposed in [6] and tested in <ref> [38, 39, 9] </ref>. CG acceleration requires no parameter estimation and is an optimal process, implicitly finding the optimal residual polynomial.
Reference: [39] <author> C. Kamath, A. Sameh, </author> <title> A projection method for solving nonsymmetric linear systems on multiprocessors, </title> <booktitle> Parallel Computing, </booktitle> <pages> 291-312, 9(1988/1989). </pages>
Reference-contexts: However, for problems with a few extremely small eigenvalues, BST acceleration can potentially outperform Chebyshev acceleration. 5.3 CG Acceleration CG acceleration for RP methods was proposed in [6] and tested in <ref> [38, 39, 9] </ref>. CG acceleration requires no parameter estimation and is an optimal process, implicitly finding the optimal residual polynomial.
Reference: [40] <author> D. Kincaid, D. Young, </author> <title> Adapting iterative algorithms developed for symmetric systems to nonsymmetric systems, Elliptic Problem Solvers, 353-359, </title> <editor> ed. M. Schultz, </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> (1981). </year>
Reference: [41] <author> A. Kydes, R. Tewarson, </author> <title> An iterative method for solving partitioned linear equations, </title> <journal> Computing 357-363, </journal> <volume> 15(1975). </volume>
Reference: [42] <author> A. Lakshminarayanan, A. Lent, </author> <title> Methods of least squares and SIRT in reconstruction, </title> <journal> J. Theor. Biol., </journal> <volume> 267-295, 76(1979). </volume> <pages> 192 </pages>
Reference: [43] <author> T. Manteuffel, </author> <title> The Tchebychev iteration for nonsymmetric linear systems, </title> <journal> Numer. Math., </journal> <pages> 307-327, 28(1977). </pages>
Reference-contexts: Finally, the pseudo-residual is ~ b (I Q)x, in contrast to the true residual b Ax. 5.1 Chebyshev Acceleration The properties and techniques for Chebyshev acceleration have been thoroughly examined <ref> [31, 43, 44] </ref> and are summarized here for RP methods primarily to establish notation and concepts used in Section 5.2. The largest eigenvalue of I Q is assumed to be 1, so that only an estimate of the smallest eigenvalue need be made.
Reference: [44] <author> T. Manteuffel, </author> <title> Adaptive procedure for estimating parameters for the nonsymmetric Tchebychev iteration, </title> <journal> Numer. Math., </journal> <pages> 183-208, 31(1978). </pages>
Reference-contexts: Finally, the pseudo-residual is ~ b (I Q)x, in contrast to the true residual b Ax. 5.1 Chebyshev Acceleration The properties and techniques for Chebyshev acceleration have been thoroughly examined <ref> [31, 43, 44] </ref> and are summarized here for RP methods primarily to establish notation and concepts used in Section 5.2. The largest eigenvalue of I Q is assumed to be 1, so that only an estimate of the smallest eigenvalue need be made.
Reference: [45] <author> J. Meijerink, H. van der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix, </title> <journal> Math. Comp, </journal> <pages> 148-162, 31(1977). </pages>
Reference-contexts: Because only GMRES (k) is implemented with these preconditioners, in the sequel the combination of GMRES (10) with ILU preconditioning is abbreviated ILU here, and mutatis mutandi for MILU. Although these two preconditioners can be shown to exist for M-matrices <ref> [45] </ref>, that need not be the case for the test problems. Even when the precondition-ers exist, the preconditioned system may actually be more difficult to solve than the unpreconditioned one. This is shown to occur in the test results.
Reference: [46] <author> S. Nelson, M. Neumann, </author> <title> Generalizations of the projection method with applications to SOR theory for Hermitian positive semidefi-nite linear systems, </title> <journal> Numer. Math. </journal> <pages> 123-141, </pages> <month> 51 </month> <year> (1987). </year>
Reference: [47] <author> W. Peters, </author> <title> Losung linearer Gleichungssysteme durch Projektion auf Schnittraume von Hyperebenen und Berechnung einer verall-gemeinerten Inversen, </title> <journal> Beit. Numer. Math., </journal> <pages> 129-146, 5(1976). </pages>
Reference: [48] <author> J. Reid, </author> <title> On the use of conjugate gradients for the solution of large sparse systems of linear equations, Large Sparse Sets of Linear Equations, </title> <editor> ed. J. </editor> <booktitle> Reid, </booktitle> <pages> 231-254, </pages> <publisher> Academic Press (1971). </publisher>
Reference: [49] <author> Y. Saad, A. Sameh, P. </author> <title> Saylor, Solving elliptic differential equations on a linear array of processors, </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <pages> 1049-1063, 6(1985). </pages>
Reference-contexts: In 1985 Y. Saad, A. Sameh, and P. Saylor <ref> [49] </ref> proposed a scheme called block Stiefel (BST) acceleration to elude this obstacle. Suppose that in Chebyshev (x 0 ; -; K) acceleration is chosen larger than 1 . <p> When r k = v i , the factor is zero and hence the i th eigencomponent is eliminated, which is precisely the motivation given for BST in <ref> [49] </ref>.
Reference: [50] <author> Y. Saad, M. Schultz, </author> <title> Conjugate gradient-like algorithms for solving nonsymmetric linear systems, </title> <journal> Math. Comp, </journal> <pages> 417-424, 44(1985). </pages>
Reference: [51] <author> P. </author> <title> Saylor, Private communication (1988). </title>
Reference: [52] <institution> Scientific Computing Associates, Inc., PCGPAK User's Guide, </institution> <address> New Haven, CT (1987). </address>
Reference-contexts: The latter is refered to as CGNE. GMRES (10) is from the PCGPAK library and has been optimized for the Alliant FX/8 by E. Anderson and for the Cray-2 by Scientific Computing Associates. More details about these implementations of GM-RES (10) can be found in <ref> [52] </ref>.
Reference: [53] <author> A. Sherman, </author> <title> An empirical investigation of methods for nonsymmetric linear systems, Elliptic Problem Solvers, 429-434, </title> <editor> ed. M. Schultz, </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> (1981). </year>
Reference: [54] <author> G. Smith, </author> <title> Numerical Solution of Partial Differential Equations: Finite Difference Methods, </title> <publisher> Clarendon Press, Oxford, </publisher> <year> (1978). </year> <month> 193 </month>
Reference: [55] <author> G. Stewart, </author> <title> Introduction to Matrix Computations, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> (1973). </year>
Reference-contexts: Using the bounds on inner product computations in <ref> [55] </ref>, the computed inner product f l (x T y) satisfies f l (x T y) = (1 + ff)x T y; jffj &lt; ~n where ~n is the length of the inner product.
Reference: [56] <author> G. Stewart, </author> <title> On the perturbation of pseudo-inverses, projections, and linear least squares problems, </title> <journal> SIAM Rev., </journal> <pages> 634-662, </pages> <year> 19(1977). </year>
Reference-contexts: The effects of perturbations of A on P i were analyzed thoroughly by G. Stew-art in <ref> [56] </ref>, and those results are simply summarized here and applied to the row projection methods. Let B i = A i + E i and let ~ P i = the orthogonal projector onto range (B i ).
Reference: [57] <author> K. Tanabe, </author> <title> A projection method for solving a singular system of linear equations, </title> <journal> Numer. Math., </journal> <pages> 203-214 17(1971). </pages>
Reference: [58] <author> A. Trummer, </author> <title> Reconstructing pictures from projections: on the convergence of the ART algorithm with relaxation, </title> <booktitle> Computing, </booktitle> <pages> 189-195, 26(1981). </pages>
Reference: [59] <author> A. Trummer, </author> <title> A note on the ART of relaxation, </title> <booktitle> Computing, </booktitle> <pages> 349-352, 33(1984). </pages>
Reference: [60] <author> P. Vinsome, Orthomin, </author> <title> an iterative method for solving sparse sets of simultaneous linear equations, </title> <booktitle> Proc. Fourth Symp. on Reservoir Simulation, </booktitle> <pages> 149-159, </pages> <institution> Society of Petr. Eng. of AIME, </institution> <year> (1976). </year>
Reference: [61] <author> R. Wainwright, R. Keller, </author> <title> Algorithms for projection methods for solving linear systems of equations, </title> <journal> Comp. Math. Appl., </journal> <pages> 235-245, 3(1977). </pages>
Reference: [62] <author> T. Whitney, R. Meany, </author> <title> Two algorithms related to the method of steepest descent, </title> <journal> SIAM J. Numer. Anal., </journal> <pages> 109-118, 4(1967). </pages>
Reference: [63] <author> J. Wilkinson, </author> <title> Modern error analysis, </title> <journal> SIAM Rev., </journal> <pages> 548-568, 13(1971). </pages>
Reference-contexts: For this the errors incurred on the separate steps of forming the normal equations and computing the projections using their Cholesky decompositions must be examined. The rounding error analyses of [26] and <ref> [63] </ref> are used, and rounding errors incurred in vector subtraction and matrix-vector multiplications are omitted since the resulting relative errors are minor in comparison. Let t be the machine precision, that is, let t be the number of bits in the mantissa. <p> Computing an Individual Projection The computation of a projection relies on finding the Cholesky decomposition C T j C j = R T R and then solving two triangular systems given by R and R T . In <ref> [63] </ref> a complete analysis of the Cholesky decomposition is given. <p> j satisfies R T R + E = C T 74 where k E k 2 fl n k C T and h p i n h i Furthermore, if 10 &lt; n &lt; 0:1 1 , then k E k 2 2n 3=2 k C T Proof: : See <ref> [63] </ref>. 2 Note that it is n and not N = n 3 that appears in the above bounds.
References-found: 63

