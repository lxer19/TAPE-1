URL: http://www.research.att.com/~schapire/papers/SchapireFrBaLe98.ps.Z
Refering-URL: http://www.research.att.com/~schapire/publist.html
Root-URL: 
Email: schapire@research.att.com  yoav@research.att.com  Peter.Bartlett@anu.edu.au  w-lee@ee.adfa.oz.au  
Title: Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods  
Author: Robert E. Schapire Yoav Freund Peter Bartlett Wee Sun Lee 
Date: May 7, 1998  
Address: 180 Park Avenue, Room A279 Florham Park, NJ 07932-0971 USA  180 Park Avenue, Room A205 Florham Park, NJ 07932-0971 USA  Canberra, ACT 0200 Australia  Canberra ACT 2600 Australia  
Affiliation: AT&T Labs  AT&T Labs  Dept. of Systems Engineering RSISE, Aust. National University  School of Electrical Engineering University College UNSW Australian Defence Force Academy  
Note: The Annals of Statistics, to appear.  
Abstract: One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew R. Barron. </author> <title> Universal approximation bounds for superposition of a sigmoidal function. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(3) </volume> <pages> 930-945, </pages> <year> 1993. </year>
Reference-contexts: We define the classification margin for the example as the difference between the weight assigned to the correct label and the maximal weight assigned to any single incorrect label. It is easy to see that the margin is a number in the range <ref> [1; 1] </ref> and that an example is classified correctly if and only if its margin is positive. A large positive margin can be interpreted as a confident correct classification. Now consider the distribution of the margin over the whole set of training examples. <p> A large positive margin can be interpreted as a confident correct classification. Now consider the distribution of the margin over the whole set of training examples. To visualize this distribution, we plot the fraction of examples whose margin is at most x as a function of x 2 <ref> [1; 1] </ref>. We refer to these graphs as margin distribution graphs. At the bottom of Figure 1, we show the margin distribution graphs that correspond to the experiments described above. <p> Then we can rewrite Equation (10) as f (x) = k~ffk 1 where jj~ffjj 1 = P T t=1 jff t j is the l 1 norm of ~ff. In our analysis, we use the fact that all of the components of ~ h (x) are in the range <ref> [1; +1] </ref>, or, in other words that the max or l 1 norm of ~ h (x) is bounded by 1: k ~ h (x)k 1 = max T Viewed this way, the connection between maximal margin classifiers and boosting becomes clear. <p> Investigating the relevance of this type of bound when applying boosting to real-world problems is an interesting open research direction. 7 Other loss functions We describe briefly related work which has been done on loss functions other than the 0-1 loss. For quadratic loss, Jones [24] and Barron <ref> [1] </ref> have shown that functions in the convex hull of a class H of real-valued functions can be approximated by a convex combination of N elements of H to an accuracy of O (1=N ) by iteratively adding the member of H which minimizes the residual error to the existing convex <p> In results analogous to those presented here, they showed that the generalization error can be bounded in terms of the sum of the absolute values of the output weights (when the members of H are normalized to have output values in the interval <ref> [1; 1] </ref>), rather than in terms of the number of components in the convex combination. Similar work on iterative convex approximation in L p spaces was presented by Donahue et al. [14].
Reference: [2] <author> Peter L. Bartlett. </author> <title> The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. </title> <journal> IEEE Transactions on Information Theory, </journal> <note> 1998 (to appear). </note>
Reference-contexts: In this paper, we present an alternative theoretical analysis of voting methods, applicable, for instance, to bagging, boosting, arcing [8] and ECOC [13]. Our approach is based on a similar result presented by Bartlett <ref> [2] </ref> in a different context. We prove rigorous, non-asymptotic upper bounds on the generalization error of voting methods in terms of a measure of performance of the combined classifier on the training set. <p> In Section 6, we discuss the relation between our work and Vapnik's in greater detail. Shawe-Taylor et al. [38] gave bounds on the generalization error of support-vector classifiers in terms of the margins, and Bartlett <ref> [2] </ref> used related techniques to give a similar bound for neural networks with small weights. A consequence of Bartlett's result is a bound on the generalization error of a voting classifier in terms of the fraction of training examples with small margin. <p> This bound does not depend on the number of classifiers that are combined in the vote. The approach we take is similar to that of Shawe-Taylor et al. [38] and Bartlett <ref> [2] </ref>, but the proof here is simpler and more direct. A slightly weaker version of Theorem 1 is a special case of Bartlett's main result. We give a proof for the special case in which there are just two possible labels f1; +1g. <p> Standard techniques yield the following theorem (the proof is essentially identical to that of Theorem 2 in Bartlett <ref> [2] </ref>). Theorem 4 Let F be a class of real-valued functions defined on the instance space X . Let D be a distribution over X fi f1; 1g, and let S be a sample of m examples chosen independently at random according to D. <p> In addition, this result leads to a slight improvement (by log factors) of the main result of Bartlett <ref> [2] </ref>, which gives bounds on generalization error for neural networks with real outputs in terms of the size of the network weights and the margin distribution. 3 The Effect of Boosting on Margin Distributions We now give theoretical evidence that Freund and Schapire's [20] AdaBoost algorithm is especially suited to the
Reference: [3] <author> Eric Bauer and Ron Kohavi. </author> <title> An empirical comparison of voting classification algorithms: Bagging, boosting, and variants. </title> <type> Unpublished manuscript, </type> <year> 1997. </year>
Reference-contexts: For example, several researchers have reported significant improvements in performance using voting methods with decision-tree learning algorithms such as C4.5 or CART as well as with neural networks <ref> [3, 6, 8, 12, 13, 16, 18, 29, 31, 37] </ref>. We refer to each of the classifiers that is combined in the vote as a base classifier and to the final voted classifier as the combined classifier.
Reference: [4] <author> Eric B. Baum and David Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: Indeed, such an analysis of boosting (which could also be applied to bagging) was carried out by Freund and Schapire [20] using the methods of Baum and Haussler <ref> [4] </ref>. This analysis predicts that the test error eventually will increase as the number of base classifiers combined increases. Such a prediction is clearly incorrect in the case of the experiments described above, as was pointed out by Quinlan [31] and Breiman [8].
Reference: [5] <author> Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. </author> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-152, </pages> <year> 1992. </year>
Reference-contexts: The idea that maximizing the margin can improve the generalization error of a classifier was previously suggested and studied by Vapnik [42] and led to his work with Cortes on support-vector classifiers [10], and with Boser and Guyon <ref> [5] </ref> on optimal margin classifiers. In Section 6, we discuss the relation between our work and Vapnik's in greater detail. <p> support vectors; Vapnik shows that ~ff can always be written as a linear combination of the support vectors. 6 Relation to Vapnik's Maximal Margin Classifiers The use of the margins of real-valued classifiers to predict generalization error was previously studied by Vapnik [42] in his work with Boser and Guyon <ref> [5] </ref> and Cortes [10] on optimal margin classifiers. We start with a brief overview of optimal margin classifiers. One of the main ideas behind this method is that some nonlinear classifiers on a low dimensional space can be treated as linear classifiers over a high dimensional space.
Reference: [6] <author> Leo Breiman. </author> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24(2) </volume> <pages> 123-140, </pages> <year> 1996. </year>
Reference-contexts: For example, several researchers have reported significant improvements in performance using voting methods with decision-tree learning algorithms such as C4.5 or CART as well as with neural networks <ref> [3, 6, 8, 12, 13, 16, 18, 29, 31, 37] </ref>. We refer to each of the classifiers that is combined in the vote as a base classifier and to the final voted classifier as the combined classifier. <p> As examples of the effectiveness of these methods, consider the results of the following two experiments using the letter dataset. (All datasets are described in Appendix B.) In the first experiment, 1 we used Breiman's bagging method <ref> [6] </ref> on top of C4.5 [32], a decision-tree learning algorithm similar to CART [9]. That is, we reran C4.5 many times on random bootstrap subsamples and combined the computed trees using simple voting.
Reference: [7] <author> Leo Breiman. </author> <title> Prediction games and arcing classifiers. </title> <type> Technical Report 504, </type> <institution> Statistics Department, University of California at Berkeley, </institution> <year> 1997. </year> <month> 23 </month>
Reference-contexts: From a practical standpoint, the fact that the bounds we give are so loose suggests that there might exist criteria different from the one used in Theorem 1 which are better in predicting the performance of voting classifiers. Breiman <ref> [7] </ref> and Grove and Schuurmans [23] experimented with maximizing the minimal margin, that is, the smallest margin achieved on the training set. <p> Related to this, the optimal margin method uses quadratic programming for its optimization, whereas the boosting algorithm can be seen as a method for approximate linear programming <ref> [7, 19, 21, 23] </ref>. Both boosting and support vector machines aim to find a linear classifier in a very high dimensional space.
Reference: [8] <author> Leo Breiman. </author> <title> Arcing classifiers. </title> <journal> Annals of Statistics, </journal> <note> to appear. </note>
Reference-contexts: For example, several researchers have reported significant improvements in performance using voting methods with decision-tree learning algorithms such as C4.5 or CART as well as with neural networks <ref> [3, 6, 8, 12, 13, 16, 18, 29, 31, 37] </ref>. We refer to each of the classifiers that is combined in the vote as a base classifier and to the final voted classifier as the combined classifier. <p> Similar improvements in test error have been demonstrated on many other benchmark problems (see Figure 2). These error curves reveal a remarkable phenomenon, first observed by Drucker and Cortes [16], and later by Quinlan [31] and Breiman <ref> [8] </ref>. Ordinarily, as classifiers become more and more complex, we expect their generalization error eventually to degrade. Yet these curves reveal that test error does not increase for either method even after 1000 trees have been combined (by which point, the combined classifier involves more than two million decision-tree nodes). <p> This analysis predicts that the test error eventually will increase as the number of base classifiers combined increases. Such a prediction is clearly incorrect in the case of the experiments described above, as was pointed out by Quinlan [31] and Breiman <ref> [8] </ref>. The apparent contradiction is especially stark in the boosting experiment in which the test error continues to decrease even after the training error has reached zero. Breiman [8] and others have proposed definitions of bias and variance for classification, and have argued that voting methods work primarily by reducing the <p> a prediction is clearly incorrect in the case of the experiments described above, as was pointed out by Quinlan [31] and Breiman <ref> [8] </ref>. The apparent contradiction is especially stark in the boosting experiment in which the test error continues to decrease even after the training error has reached zero. Breiman [8] and others have proposed definitions of bias and variance for classification, and have argued that voting methods work primarily by reducing the variance of a learning algorithm. <p> However, as argued in Section 5.4, the complexity of such combined classifiers can be much greater than that of the base classifiers and can result in over-fitting. In this paper, we present an alternative theoretical analysis of voting methods, applicable, for instance, to bagging, boosting, arcing <ref> [8] </ref> and ECOC [13]. Our approach is based on a similar result presented by Bartlett [2] in a different context. We prove rigorous, non-asymptotic upper bounds on the generalization error of voting methods in terms of a measure of performance of the combined classifier on the training set. <p> This can be seen in 1. Contrast this with the margin distribution graphs after 1000 iterations of bagging in which as many 16 Kong & Dietterich [26] definitions Breiman <ref> [8] </ref> definitions stumps C4.5 stumps C4.5 error pseudoloss error error pseudoloss error name boost bag boost bag boost bag boost bag boost bag boost bag waveform bias 26.0 3.8 22.8 0.8 11.9 1.5 0.5 1.4 19.2 2.6 15.7 0.5 7.9 0.9 0.3 1.4 var 5.6 2.8 4.1 3.8 8.6 14.9 3.7 <p> While the details of these definitions differ from author to author <ref> [8, 25, 26, 40] </ref>, they are all attempts to capture the following quantities: The bias term measures the persistent error of the learning algorithm, in other words, the error that would remain even if we had an infinite number of independently trained classifiers. <p> This simple observation suggests that it may be inherently more difficult or even impossible to find a bias-variance decomposition for classification as natural and satisfying as in the quadratic regression case. This difficulty is reflected in the myriad definitions that have been proposed for bias and variance <ref> [8, 25, 26, 40] </ref>. Rather than discussing each one separately, for the remainder of this section, except where noted, we follow the definitions given by Kong and Dietterich [26], and referred to as Definition 0 by Breiman [8]. (These definitions are given in Appendix C.) 5.2 Bagging and variance reduction. <p> Rather than discussing each one separately, for the remainder of this section, except where noted, we follow the definitions given by Kong and Dietterich [26], and referred to as Definition 0 by Breiman <ref> [8] </ref>. (These definitions are given in Appendix C.) 5.2 Bagging and variance reduction. The notion of variance certainly seems to be helpful in understanding bagging; empirically, bagging appears to be most effective for learning algorithms with large variance. <p> Thus, in this case, the behavior of bagging is very different from its expected behavior on truly independent training sets. Boosting, on the same data, achieved a test error of 0:6%. 5.3 Boosting and variance reduction. Breiman <ref> [8] </ref> argued that boosting is primarily a variance-reducing procedure. Some of the evidence for this comes from the observed effectiveness of boosting when used with C4.5 or CART, algorithms known empirically to have high variance. <p> However, our experiments show that boosting can also be highly effective when used with learning algorithms whose error tends to be dominated by bias rather than variance. 5 We ran boosting and bagging on four artificial datasets described by Breiman <ref> [8] </ref>, as well as the artificial problem studied by Kong and Dietterich [26]. Following previous authors, we used training sets of size 200 for the latter problem and 300 for the others. For the base learning algorithm, we tested C4.5. <p> We then estimated bias, variance and average error of these algorithms by rerunning them 1000 times each, and evaluating them on a test set of 10,000 examples. For these experiments, we used both the bias-variance definitions given by Kong and Dietterich [26] and those proposed more recently by Breiman <ref> [8] </ref>. (Definitions are given in Appendix C.) For multiclass problems, following Freund and Schapire [18], we tested both error-based and pseudoloss-based versions of bagging and boosting. For two-class problems, only the error-based versions were used. The results are summarized in Table 1. Clearly, boosting is doing more than reducing variance.
Reference: [9] <author> Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: As examples of the effectiveness of these methods, consider the results of the following two experiments using the letter dataset. (All datasets are described in Appendix B.) In the first experiment, 1 we used Breiman's bagging method [6] on top of C4.5 [32], a decision-tree learning algorithm similar to CART <ref> [9] </ref>. That is, we reran C4.5 many times on random bootstrap subsamples and combined the computed trees using simple voting.
Reference: [10] <author> Corinna Cortes and Vladimir Vapnik. </author> <title> Support-vector networks. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 273-297, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: The idea that maximizing the margin can improve the generalization error of a classifier was previously suggested and studied by Vapnik [42] and led to his work with Cortes on support-vector classifiers <ref> [10] </ref>, and with Boser and Guyon [5] on optimal margin classifiers. In Section 6, we discuss the relation between our work and Vapnik's in greater detail. <p> shows that ~ff can always be written as a linear combination of the support vectors. 6 Relation to Vapnik's Maximal Margin Classifiers The use of the margins of real-valued classifiers to predict generalization error was previously studied by Vapnik [42] in his work with Boser and Guyon [5] and Cortes <ref> [10] </ref> on optimal margin classifiers. We start with a brief overview of optimal margin classifiers. One of the main ideas behind this method is that some nonlinear classifiers on a low dimensional space can be treated as linear classifiers over a high dimensional space.
Reference: [11] <author> Luc Devroye. </author> <title> Bounds for the uniform deviation of empirical measures. </title> <journal> Journal of Multivariate Analysis, </journal> <volume> 12 </volume> <pages> 72-79, </pages> <year> 1982. </year>
Reference-contexts: satisfies the following bound for all &gt; 0: P D yf (x) 0 P S yf (x) + O @ p 2 + log (1=ffi) 1 The proof of this theorem uses the following uniform convergence result, which is a refinement of the Vapnik and Chervonenkis result due to Devroye <ref> [11] </ref>.
Reference: [12] <author> Thomas G. Dietterich. </author> <title> An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. </title> <type> Unpublished manuscript, </type> <year> 1998. </year>
Reference-contexts: For example, several researchers have reported significant improvements in performance using voting methods with decision-tree learning algorithms such as C4.5 or CART as well as with neural networks <ref> [3, 6, 8, 12, 13, 16, 18, 29, 31, 37] </ref>. We refer to each of the classifiers that is combined in the vote as a base classifier and to the final voted classifier as the combined classifier.
Reference: [13] <author> Thomas G. Dietterich and Ghulum Bakiri. </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 263-286, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: For example, several researchers have reported significant improvements in performance using voting methods with decision-tree learning algorithms such as C4.5 or CART as well as with neural networks <ref> [3, 6, 8, 12, 13, 16, 18, 29, 31, 37] </ref>. We refer to each of the classifiers that is combined in the vote as a base classifier and to the final voted classifier as the combined classifier. <p> However, as argued in Section 5.4, the complexity of such combined classifiers can be much greater than that of the base classifiers and can result in over-fitting. In this paper, we present an alternative theoretical analysis of voting methods, applicable, for instance, to bagging, boosting, arcing [8] and ECOC <ref> [13] </ref>. Our approach is based on a similar result presented by Bartlett [2] in a different context. We prove rigorous, non-asymptotic upper bounds on the generalization error of voting methods in terms of a measure of performance of the combined classifier on the training set. <p> Brief descriptions of these are given in Appendix B. Note that all three of these learning problems are multiclass with 26, 6 and 4 classes, respectively. Voting methods. In addition to bagging and boosting, we used a variant of Dietterich and Bakiri's <ref> [13] </ref> method of error-correcting output codes (ECOC), which can be viewed as a voting method. This approach was designed to handle multiclass problems using only a two-class learning algorithm.
Reference: [14] <author> M. J. Donahue, L. Gurvits, C. Darken, and E. Sontag. </author> <title> Rates of convex approximation in non-Hilbert spaces. Constructive Approximation, </title> <booktitle> 13 </booktitle> <pages> 187-220, </pages> <year> 1997. </year>
Reference-contexts: Similar work on iterative convex approximation in L p spaces was presented by Donahue et al. <ref> [14] </ref>. To the best of our knowledge, similar iterative schemes for combining functions have not been studied for the log loss. Extensions of boosting to solve regression problems have been suggested by Freund [17] and Freund and Schapire [20]. These extensions are yet to be tested in practice.
Reference: [15] <author> Harris Drucker. </author> <title> Improving regressors using boosting techniques. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <pages> pages 107-115, </pages> <year> 1997. </year>
Reference-contexts: To the best of our knowledge, similar iterative schemes for combining functions have not been studied for the log loss. Extensions of boosting to solve regression problems have been suggested by Freund [17] and Freund and Schapire [20]. These extensions are yet to be tested in practice. Drucker <ref> [15] </ref> experimented with a different extension of boosting for regression and reported some encouraging results. 8 Open Problems The methods in this paper allow us to upper bound the generalization error of a voted classifier based on simple statistics which can be measured using the training data.
Reference: [16] <author> Harris Drucker and Corinna Cortes. </author> <title> Boosting decision trees. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 479-485, </pages> <year> 1996. </year>
Reference-contexts: For example, several researchers have reported significant improvements in performance using voting methods with decision-tree learning algorithms such as C4.5 or CART as well as with neural networks <ref> [3, 6, 8, 12, 13, 16, 18, 29, 31, 37] </ref>. We refer to each of the classifiers that is combined in the vote as a base classifier and to the final voted classifier as the combined classifier. <p> Note that boosting drives the test error down even further to just 3.1%. Similar improvements in test error have been demonstrated on many other benchmark problems (see Figure 2). These error curves reveal a remarkable phenomenon, first observed by Drucker and Cortes <ref> [16] </ref>, and later by Quinlan [31] and Breiman [8]. Ordinarily, as classifiers become more and more complex, we expect their generalization error eventually to degrade.
Reference: [17] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <journal> Information and Computation, </journal> <volume> 121(2) </volume> <pages> 256-285, </pages> <year> 1995. </year>
Reference-contexts: there is structure in the data and in the base learning algorithm that is not taken into account in the assumptions of a general theory. 5 In fact, the original goal of boosting was to reduce the error of so-called weak learning algorithms which tend to have very large bias. <ref> [17, 20, 34] </ref> 19 5.4 Why averaging can increase complexity In this section, we challenge a common intuition which says that when one takes the majority vote over several base classifiers the generalization error of the resulting classifier is likely to be lower than the average generalization error of the base <p> This analysis is preferable to the analysis that depends on the size of the margin when only a few of the training examples are support vectors. Previous work <ref> [17] </ref> has suggested that boosting also can be used as a method for selecting a small number of informative examples from the training set. <p> Similar work on iterative convex approximation in L p spaces was presented by Donahue et al. [14]. To the best of our knowledge, similar iterative schemes for combining functions have not been studied for the log loss. Extensions of boosting to solve regression problems have been suggested by Freund <ref> [17] </ref> and Freund and Schapire [20]. These extensions are yet to be tested in practice.
Reference: [18] <author> Yoav Freund and Robert E. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 148-156, </pages> <year> 1996. </year>
Reference-contexts: For example, several researchers have reported significant improvements in performance using voting methods with decision-tree learning algorithms such as C4.5 or CART as well as with neural networks <ref> [3, 6, 8, 12, 13, 16, 18, 29, 31, 37] </ref>. We refer to each of the classifiers that is combined in the vote as a base classifier and to the final voted classifier as the combined classifier. <p> Our bounds also depend on the number of training examples and the complexity of the base classifiers, but do not depend explicitly on the number of base classifiers. Although too loose 3 reported by Freund and Schapire <ref> [18] </ref>. Each point in each scatter plot shows the test error rate of the two competing algorithms on a single benchmark. <p> We also used a simple algorithm for finding the best single-node, binary-split decision tree (a decision stump). Since this latter algorithm is very weak, we used the pseudoloss versions of boosting and bagging, as described above. (See Freund and Schapire <ref> [20, 18] </ref> for details.) Results. Figures 4 and 5 show error curves and margin distribution graphs for the three datasets, three voting methods and two base learning algorithms. Note that each figure corresponds only to a single run of each algorithm. <p> For these experiments, we used both the bias-variance definitions given by Kong and Dietterich [26] and those proposed more recently by Breiman [8]. (Definitions are given in Appendix C.) For multiclass problems, following Freund and Schapire <ref> [18] </ref>, we tested both error-based and pseudoloss-based versions of bagging and boosting. For two-class problems, only the error-based versions were used. The results are summarized in Table 1. Clearly, boosting is doing more than reducing variance.
Reference: [19] <author> Yoav Freund and Robert E. Schapire. </author> <title> Game theory, on-line prediction and boosting. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 325-332, </pages> <year> 1996. </year>
Reference-contexts: Related to this, the optimal margin method uses quadratic programming for its optimization, whereas the boosting algorithm can be seen as a method for approximate linear programming <ref> [7, 19, 21, 23] </ref>. Both boosting and support vector machines aim to find a linear classifier in a very high dimensional space.
Reference: [20] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 55(1) </volume> <pages> 119-139, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: The test error of bagging 1000 trees is 6.6%, a significant improvement. (Both of these error rates are indicated in the figure as horizontal grid lines.) In the second experiment, we used Freund and Schapire's AdaBoost algorithm <ref> [20] </ref> on the same dataset, also using C4.5. This method is similar to bagging in that it reruns the base learning algorithm C4.5 many times and combines the computed trees using voting. <p> Typically, both in theory and in practice, the difference between the training error and the test error increases when the complexity of the classifier increases. Indeed, such an analysis of boosting (which could also be applied to bagging) was carried out by Freund and Schapire <ref> [20] </ref> using the methods of Baum and Haussler [4]. This analysis predicts that the test error eventually will increase as the number of base classifiers combined increases. <p> factors) of the main result of Bartlett [2], which gives bounds on generalization error for neural networks with real outputs in terms of the size of the network weights and the margin distribution. 3 The Effect of Boosting on Margin Distributions We now give theoretical evidence that Freund and Schapire's <ref> [20] </ref> AdaBoost algorithm is especially suited to the task of maximizing the number of training examples with large margin. We briefly review their algorithm. We adopt the notation used in the previous section, and restrict our attention to the binary case. <p> This quantity is exactly the margin of the combined classifier computed up to this point. Freund and Schapire <ref> [20] </ref> prove that if the training error rates of all the base classifiers are bounded below 1=2 for all D t so that * t 1=2 fl for some fl &gt; 0, then the training error of the combined classifier decreases exponentially fast with the number of base classifiers that are <p> However, if this increase is sufficiently slow the bound of Theorem 5 is still useful. Characterizing the conditions under which the increase is slow is an open problem. Although this theorem applies only to binary classification problems, Freund and Schapire <ref> [20] </ref> and others [35, 36] give extensive treatment to the multiclass case (see also Section 4). <p> Since ECOC is a voting method, we can measure margins just as we do for boosting and bagging. As noted above, we used three multiclass learning problems in our experiments, whereas the version of boosting given in Section 3 only handles two-class data. Freund and Schapire <ref> [20] </ref> describe a straightforward adaption of this algorithm to the multiclass case. The problem with this algorithm is that it still requires that the accuracy of each base classifier exceed 1=2. <p> For fairly powerful base learners, such as C4.5, this does not seem to be a problem. However, the accuracy 1=2 requirement can often be difficult for less powerful base learning algorithms which may be unable to generate classifiers with small training errors. Freund and Schapire <ref> [20] </ref> provide one solution to this problem by modifying the form of the base classifiers and refining the goal of the base learner. In this approach, rather than predicting a single class for each example, the base classifier chooses a set of plausible labels for each example. <p> We also used a simple algorithm for finding the best single-node, binary-split decision tree (a decision stump). Since this latter algorithm is very weak, we used the pseudoloss versions of boosting and bagging, as described above. (See Freund and Schapire <ref> [20, 18] </ref> for details.) Results. Figures 4 and 5 show error curves and margin distribution graphs for the three datasets, three voting methods and two base learning algorithms. Note that each figure corresponds only to a single run of each algorithm. <p> there is structure in the data and in the base learning algorithm that is not taken into account in the assumptions of a general theory. 5 In fact, the original goal of boosting was to reduce the error of so-called weak learning algorithms which tend to have very large bias. <ref> [17, 20, 34] </ref> 19 5.4 Why averaging can increase complexity In this section, we challenge a common intuition which says that when one takes the majority vote over several base classifiers the generalization error of the resulting classifier is likely to be lower than the average generalization error of the base <p> To the best of our knowledge, similar iterative schemes for combining functions have not been studied for the log loss. Extensions of boosting to solve regression problems have been suggested by Freund [17] and Freund and Schapire <ref> [20] </ref>. These extensions are yet to be tested in practice.
Reference: [21] <author> Yoav Freund and Robert E. Schapire. </author> <title> Adaptive game playing using multiplicative weights. Games and Economic Behavior, </title> <note> (to appear). </note>
Reference-contexts: Related to this, the optimal margin method uses quadratic programming for its optimization, whereas the boosting algorithm can be seen as a method for approximate linear programming <ref> [7, 19, 21, 23] </ref>. Both boosting and support vector machines aim to find a linear classifier in a very high dimensional space.
Reference: [22] <author> Jerome H. Friedman. </author> <title> On bias, variance, </title> <journal> 0/1-loss, and the curse-of-dimensionality. </journal> <note> Available electronically from http://stat.stanford.edu/~jhf. </note>
Reference-contexts: Both bias and variance are always nonnegative and averaging decreases the variance term without changing the bias term. One would naturally hope that this beautiful analysis would carry over from quadratic regression to classification. Unfortunately, as has been observed before us, (see, for instance, Friedman <ref> [22] </ref>) taking the majority vote over several classification rules can sometimes result in an increase in the expected classification error.
Reference: [23] <author> Adam J. Grove and Dale Schuurmans. </author> <title> Boosting in the limit: Maximizing the margin of learned ensembles. </title> <booktitle> In Proceedings of the Fifteenth National Conference on Artificial Intelligence, </booktitle> <year> 1998. </year>
Reference-contexts: From a practical standpoint, the fact that the bounds we give are so loose suggests that there might exist criteria different from the one used in Theorem 1 which are better in predicting the performance of voting classifiers. Breiman [7] and Grove and Schuurmans <ref> [23] </ref> experimented with maximizing the minimal margin, that is, the smallest margin achieved on the training set. The advantage of using this criterion is that maximizing the minimal margin can be done efficiently (if the set of base classifiers is not too large) using linear programming. <p> Related to this, the optimal margin method uses quadratic programming for its optimization, whereas the boosting algorithm can be seen as a method for approximate linear programming <ref> [7, 19, 21, 23] </ref>. Both boosting and support vector machines aim to find a linear classifier in a very high dimensional space.
Reference: [24] <author> Lee K. Jones. </author> <title> A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training. </title> <journal> Annals of Statistics, </journal> <volume> 20(1) </volume> <pages> 608-613, </pages> <year> 1992. </year>
Reference-contexts: Investigating the relevance of this type of bound when applying boosting to real-world problems is an interesting open research direction. 7 Other loss functions We describe briefly related work which has been done on loss functions other than the 0-1 loss. For quadratic loss, Jones <ref> [24] </ref> and Barron [1] have shown that functions in the convex hull of a class H of real-valued functions can be approximated by a convex combination of N elements of H to an accuracy of O (1=N ) by iteratively adding the member of H which minimizes the residual error to
Reference: [25] <author> Ron Kohavi and David H. Wolpert. </author> <title> Bias plus variance decomposition for zero-one loss functions. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 275-283, </pages> <year> 1996. </year>
Reference-contexts: While the details of these definitions differ from author to author <ref> [8, 25, 26, 40] </ref>, they are all attempts to capture the following quantities: The bias term measures the persistent error of the learning algorithm, in other words, the error that would remain even if we had an infinite number of independently trained classifiers. <p> This simple observation suggests that it may be inherently more difficult or even impossible to find a bias-variance decomposition for classification as natural and satisfying as in the quadratic regression case. This difficulty is reflected in the myriad definitions that have been proposed for bias and variance <ref> [8, 25, 26, 40] </ref>. Rather than discussing each one separately, for the remainder of this section, except where noted, we follow the definitions given by Kong and Dietterich [26], and referred to as Definition 0 by Breiman [8]. (These definitions are given in Appendix C.) 5.2 Bagging and variance reduction.
Reference: [26] <author> Eun Bae Kong and Thomas G. Dietterich. </author> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 313-321, </pages> <year> 1995. </year>
Reference-contexts: This can be seen in 1. Contrast this with the margin distribution graphs after 1000 iterations of bagging in which as many 16 Kong & Dietterich <ref> [26] </ref> definitions Breiman [8] definitions stumps C4.5 stumps C4.5 error pseudoloss error error pseudoloss error name boost bag boost bag boost bag boost bag boost bag boost bag waveform bias 26.0 3.8 22.8 0.8 11.9 1.5 0.5 1.4 19.2 2.6 15.7 0.5 7.9 0.9 0.3 1.4 var 5.6 2.8 4.1 3.8 <p> While the details of these definitions differ from author to author <ref> [8, 25, 26, 40] </ref>, they are all attempts to capture the following quantities: The bias term measures the persistent error of the learning algorithm, in other words, the error that would remain even if we had an infinite number of independently trained classifiers. <p> This simple observation suggests that it may be inherently more difficult or even impossible to find a bias-variance decomposition for classification as natural and satisfying as in the quadratic regression case. This difficulty is reflected in the myriad definitions that have been proposed for bias and variance <ref> [8, 25, 26, 40] </ref>. Rather than discussing each one separately, for the remainder of this section, except where noted, we follow the definitions given by Kong and Dietterich [26], and referred to as Definition 0 by Breiman [8]. (These definitions are given in Appendix C.) 5.2 Bagging and variance reduction. <p> This difficulty is reflected in the myriad definitions that have been proposed for bias and variance [8, 25, 26, 40]. Rather than discussing each one separately, for the remainder of this section, except where noted, we follow the definitions given by Kong and Dietterich <ref> [26] </ref>, and referred to as Definition 0 by Breiman [8]. (These definitions are given in Appendix C.) 5.2 Bagging and variance reduction. The notion of variance certainly seems to be helpful in understanding bagging; empirically, bagging appears to be most effective for learning algorithms with large variance. <p> experiments show that boosting can also be highly effective when used with learning algorithms whose error tends to be dominated by bias rather than variance. 5 We ran boosting and bagging on four artificial datasets described by Breiman [8], as well as the artificial problem studied by Kong and Dietterich <ref> [26] </ref>. Following previous authors, we used training sets of size 200 for the latter problem and 300 for the others. For the base learning algorithm, we tested C4.5. We also used the decision-stump base-learning algorithm described in Section 4. <p> We then estimated bias, variance and average error of these algorithms by rerunning them 1000 times each, and evaluating them on a test set of 10,000 examples. For these experiments, we used both the bias-variance definitions given by Kong and Dietterich <ref> [26] </ref> and those proposed more recently by Breiman [8]. (Definitions are given in Appendix C.) For multiclass problems, following Freund and Schapire [18], we tested both error-based and pseudoloss-based versions of bagging and boosting. For two-class problems, only the error-based versions were used. The results are summarized in Table 1.
Reference: [27] <author> Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. </author> <title> Efficient agnostic learning of neural networks with bounded fan-in. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(6) </volume> <pages> 2118-2132, </pages> <year> 1996. </year>
Reference-contexts: Lee, Bartlett and Williamson <ref> [27] </ref> extended this result to show that the 22 procedure will converge to the best approximation in the convex hull of H even when the target function is not in the convex hull of H. <p> Lee, Bartlett and Williamson [27] extended this result to show that the 22 procedure will converge to the best approximation in the convex hull of H even when the target function is not in the convex hull of H. Lee, Bartlett and Williamson <ref> [27, 28] </ref> also studied the generalization error when this procedure is used for learning.
Reference: [28] <author> Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. </author> <title> The importance of convexity in learning with squared loss. </title> <journal> IEEE Transactions on Information Theory, </journal> <note> to appear. 24 </note>
Reference-contexts: Lee, Bartlett and Williamson [27] extended this result to show that the 22 procedure will converge to the best approximation in the convex hull of H even when the target function is not in the convex hull of H. Lee, Bartlett and Williamson <ref> [27, 28] </ref> also studied the generalization error when this procedure is used for learning.
Reference: [29] <author> Richard Maclin and David Opitz. </author> <title> An empirical evaluation of bagging and boosting. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 546-551, </pages> <year> 1997. </year>
Reference-contexts: For example, several researchers have reported significant improvements in performance using voting methods with decision-tree learning algorithms such as C4.5 or CART as well as with neural networks <ref> [3, 6, 8, 12, 13, 16, 18, 29, 31, 37] </ref>. We refer to each of the classifiers that is combined in the vote as a base classifier and to the final voted classifier as the combined classifier.
Reference: [30] <author> C. J. Merz and P. M. Murphy. </author> <title> UCI repository of machine learning databases, </title> <note> 1998. http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference: [31] <author> J. R. Quinlan. Bagging, </author> <title> boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 725-730, </pages> <year> 1996. </year>
Reference-contexts: For example, several researchers have reported significant improvements in performance using voting methods with decision-tree learning algorithms such as C4.5 or CART as well as with neural networks <ref> [3, 6, 8, 12, 13, 16, 18, 29, 31, 37] </ref>. We refer to each of the classifiers that is combined in the vote as a base classifier and to the final voted classifier as the combined classifier. <p> Note that boosting drives the test error down even further to just 3.1%. Similar improvements in test error have been demonstrated on many other benchmark problems (see Figure 2). These error curves reveal a remarkable phenomenon, first observed by Drucker and Cortes [16], and later by Quinlan <ref> [31] </ref> and Breiman [8]. Ordinarily, as classifiers become more and more complex, we expect their generalization error eventually to degrade. <p> This analysis predicts that the test error eventually will increase as the number of base classifiers combined increases. Such a prediction is clearly incorrect in the case of the experiments described above, as was pointed out by Quinlan <ref> [31] </ref> and Breiman [8]. The apparent contradiction is especially stark in the boosting experiment in which the test error continues to decrease even after the training error has reached zero.
Reference: [32] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: As examples of the effectiveness of these methods, consider the results of the following two experiments using the letter dataset. (All datasets are described in Appendix B.) In the first experiment, 1 we used Breiman's bagging method [6] on top of C4.5 <ref> [32] </ref>, a decision-tree learning algorithm similar to CART [9]. That is, we reran C4.5 many times on random bootstrap subsamples and combined the computed trees using simple voting.
Reference: [33] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory Series A, </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: Since the VC-dimension of H is d, Sauer's lemma <ref> [33, 41] </ref> states that jfhh (x 1 ); : : : ; h (x m )i : h 2 Hgj d X i em d for m d 1.
Reference: [34] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: there is structure in the data and in the base learning algorithm that is not taken into account in the assumptions of a general theory. 5 In fact, the original goal of boosting was to reduce the error of so-called weak learning algorithms which tend to have very large bias. <ref> [17, 20, 34] </ref> 19 5.4 Why averaging can increase complexity In this section, we challenge a common intuition which says that when one takes the majority vote over several base classifiers the generalization error of the resulting classifier is likely to be lower than the average generalization error of the base
Reference: [35] <author> Robert E. Schapire. </author> <title> Using output codes to boost multiclass learning problems. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <pages> pages 313-321, </pages> <year> 1997. </year>
Reference-contexts: However, if this increase is sufficiently slow the bound of Theorem 5 is still useful. Characterizing the conditions under which the increase is slow is an open problem. Although this theorem applies only to binary classification problems, Freund and Schapire [20] and others <ref> [35, 36] </ref> give extensive treatment to the multiclass case (see also Section 4).
Reference: [36] <author> Robert E. Schapire and Yoram Singer. </author> <title> Improved boosting algorithms using confidence-rated predictions. </title> <booktitle> In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, </booktitle> <year> 1998. </year>
Reference-contexts: However, if this increase is sufficiently slow the bound of Theorem 5 is still useful. Characterizing the conditions under which the increase is slow is an open problem. Although this theorem applies only to binary classification problems, Freund and Schapire [20] and others <ref> [35, 36] </ref> give extensive treatment to the multiclass case (see also Section 4).
Reference: [37] <author> Holger Schwenk and Yoshua Bengio. </author> <title> Training methods for adaptive boosting of neural networks for character recognition. </title> <booktitle> In Advances in Neural Information Processing Systems 10, </booktitle> <year> 1998. </year>
Reference-contexts: For example, several researchers have reported significant improvements in performance using voting methods with decision-tree learning algorithms such as C4.5 or CART as well as with neural networks <ref> [3, 6, 8, 12, 13, 16, 18, 29, 31, 37] </ref>. We refer to each of the classifiers that is combined in the vote as a base classifier and to the final voted classifier as the combined classifier.
Reference: [38] <author> John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. </author> <title> A framework for structural risk minimisation. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 68-76, </pages> <year> 1996. </year>
Reference-contexts: In Section 6, we discuss the relation between our work and Vapnik's in greater detail. Shawe-Taylor et al. <ref> [38] </ref> gave bounds on the generalization error of support-vector classifiers in terms of the margins, and Bartlett [2] used related techniques to give a similar bound for neural networks with small weights. <p> This bound does not depend on the number of classifiers that are combined in the vote. The approach we take is similar to that of Shawe-Taylor et al. <ref> [38] </ref> and Bartlett [2], but the proof here is simpler and more direct. A slightly weaker version of Theorem 1 is a special case of Bartlett's main result. We give a proof for the special case in which there are just two possible labels f1; +1g. <p> However, typically, the expected value of the minimal margin is not known. Shawe-Taylor et al. <ref> [38] </ref> used techniques from the theory of learning real-valued functions to give bounds on generalization error in terms of margins on the training examples. Shawe-Taylor et al. [39] also gave related results for arbitrary real classes.
Reference: [39] <author> John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. </author> <title> Structural risk minimization over data-dependent hierarchies. </title> <type> Technical Report NC-TR-96-053, </type> <institution> Neurocolt, </institution> <year> 1996. </year>
Reference-contexts: However, typically, the expected value of the minimal margin is not known. Shawe-Taylor et al. [38] used techniques from the theory of learning real-valued functions to give bounds on generalization error in terms of margins on the training examples. Shawe-Taylor et al. <ref> [39] </ref> also gave related results for arbitrary real classes. Consider the relation between Equation (10) and the argument of the minimum in Equation (12).
Reference: [40] <author> Robert Tibshirani. </author> <title> Bias, variance and prediction error for classification rules. </title> <type> Technical report, </type> <institution> University of Toronto, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: While the details of these definitions differ from author to author <ref> [8, 25, 26, 40] </ref>, they are all attempts to capture the following quantities: The bias term measures the persistent error of the learning algorithm, in other words, the error that would remain even if we had an infinite number of independently trained classifiers. <p> This simple observation suggests that it may be inherently more difficult or even impossible to find a bias-variance decomposition for classification as natural and satisfying as in the quadratic regression case. This difficulty is reflected in the myriad definitions that have been proposed for bias and variance <ref> [8, 25, 26, 40] </ref>. Rather than discussing each one separately, for the remainder of this section, except where noted, we follow the definitions given by Kong and Dietterich [26], and referred to as Definition 0 by Breiman [8]. (These definitions are given in Appendix C.) 5.2 Bagging and variance reduction.
Reference: [41] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year>
Reference-contexts: Since the VC-dimension of H is d, Sauer's lemma <ref> [33, 41] </ref> states that jfhh (x 1 ); : : : ; h (x m )i : h 2 Hgj d X i em d for m d 1.
Reference: [42] <author> Vladimir N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: By simple, we mean that the classifier is chosen from a restricted space of classifiers. When the space is finite, we use its cardinality as the measure of complexity and when it is infinite we use the VC dimension <ref> [42] </ref> which is often closely related to the number of parameters that define the classifier. Typically, both in theory and in practice, the difference between the training error and the test error increases when the complexity of the classifier increases. <p> The idea that maximizing the margin can improve the generalization error of a classifier was previously suggested and studied by Vapnik <ref> [42] </ref> and led to his work with Cortes on support-vector classifiers [10], and with Boser and Guyon [5] on optimal margin classifiers. In Section 6, we discuss the relation between our work and Vapnik's in greater detail. <p> The circled instances are the support vectors; Vapnik shows that ~ff can always be written as a linear combination of the support vectors. 6 Relation to Vapnik's Maximal Margin Classifiers The use of the margins of real-valued classifiers to predict generalization error was previously studied by Vapnik <ref> [42] </ref> in his work with Boser and Guyon [5] and Cortes [10] on optimal margin classifiers. We start with a brief overview of optimal margin classifiers. <p> Without loss of generality, we can assume that R = 1. 21 Vapnik <ref> [42] </ref> showed that the VC dimension of all linear classifiers with minimum margin at least is upper bounded by 1= 2 . <p> However, computationally, they are very different: support vector machines use the method of kernels to perform computations in the high dimensional space while boosting relies on a base learning algorithm which explores the high dimensional space one coordinate at a time. Vapnik <ref> [42] </ref> gave an alternative analysis of optimal margin classifiers, based on the number of support vectors, i.e., the number of examples that define the final classifier.
References-found: 42

