URL: http://www.speech.sri.com/people/victor/papers/eurospeech.97.abrash.ps
Refering-URL: http://www.speech.sri.com/people/victor/publications.html
Root-URL: 
Email: victor@speech.sri.com  
Title: MIXTURE INPUT TRANSFORMATIONS FOR ADAPTATION OF HYBRID CONNECTIONIST SPEECH RECOGNIZERS  
Author: Victor Abrash 
Address: Menlo Park, California 94025 U.S.A.  
Affiliation: Speech Technology And Research Laboratory SRI International  
Abstract: We extend the input transformation approach for adapting hybrid connectionist speech recognizers to allow multiple transformations to be trained. Previous work has shown the efficacy of the linear input transformation approach for speaker adaptation [1][2][3], but has focused only on training global transformations. This approach is clearly suboptimal since it assumes that a single transformation is appropriate for every region in the acoustic feature input space, that is, for every phonetic class, microphone, and noise level. In this paper, we propose a new algorithm to train mixtures of transformation networks (MTNs) in the hybrid connectionist recognition framework. This approach is based on the idea of partitioning the acoustic feature space into R regions and training an input transformation for each region. The transformations are combined probabilistically according to the degree to which the acoustic features belong to each region, where the combination weights are derived from a separate acoustic gating network (AGN). We apply the new algorithm to nonnative speaker adaptation, and present recognition results for the 1994 WSJ Spoke 3 development set. The MTN technique can also be used for noise or microphone robust recognition or for other nonspeech neural network pattern recognition problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Franco, V. Abrash, M. Cohen, A. Sankar, and M. Wein-traub, </author> <title> Hybrid HMM/MLP speech recognition, </title> <booktitle> in ARPA Artificial Neural Network Technology 1994 Program Review, </booktitle> <address> (Key West, FL), </address> <month> December 68 </month> <year> 1994. </year>
Reference: [2] <author> V. Abrash, H. Franco, A. Sankar, and M. Cohen, </author> <title> Connectionist speaker normalization and adaptation, </title> <booktitle> in Proceedings of the 4th European Conference on Speech Communication and Technology, (Madrid, Spain), </booktitle> <pages> pp. 21832186, </pages> <month> Sept. </month> <year> 1995. </year>
Reference: [3] <author> J. Neto, L. Almeida, M. Hochberg, C. Martins, L. Nunes, S. Renals, and T. Robinson, </author> <title> Speaker-adaptation for hybrid hmm-ann continuous speech recognition system, </title> <booktitle> in Proceedings of the 4th European Conference on Speech Communication and Technology, (Madrid, Spain), </booktitle> <pages> pp. 21712174, </pages> <month> Sept. </month> <year> 1995. </year>
Reference: [4] <author> M. Cohen, H. Franco, N. Morgan, D. Rumelhart, and V. Abrash, </author> <title> Hybrid neural network/hidden markov model continuous-speech recognition, </title> <booktitle> in Proceedings of the International Conference on Spoken Language Processing, </booktitle> <address> (Banff, Alberta, Canada), </address> <pages> pp. 915918, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: In this case, the transformed MLP input is Y 0 t4 ; : : : ; y 0 t+4 g, where each column y 0 t+i is computed as y 0 R X w r (t) TN1 r (y 0 for i 2 <ref> [4; 4] </ref>. The overall MTN using this type of TN is denoted by MTN1 (R). 2.3. Training Each TN is trained only for data falling into its acoustic region with nonzero probability.
Reference: [5] <author> H. Bourlard and N. Morgan, </author> <title> Connectionist Speech Recognition: A Hybrid Approach. </title> <address> Massachusetts: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference: [6] <author> V. Digalakis, R. Rtischev, and L. Neumeyer, </author> <title> Fast speaker adaptation using constrained estimation of Gaussian mixtures, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 3, </volume> <pages> pp. 357366, </pages> <month> Sept. </month> <year> 1995. </year>
Reference: [7] <author> C. Leggetter and P. Woodland, </author> <title> Maximum likelihood linear regression for speaker adaptation of continous density HMMs, </title> <booktitle> Computer Speech and Language Processing, </booktitle> <volume> vol. 9, </volume> <pages> pp. 171186, </pages> <year> 1995. </year>
Reference: [8] <author> L. Neumeyer and M. Weintraub, </author> <title> Probabilistic optimum filtering for robust speech recognition, </title> <booktitle> in Proceedings of the International Conference on Acoustics, Speech and Signal Processing, (Adelaide, AU), </booktitle> <pages> pp. 417420, </pages> <month> apr </month> <year> 1994. </year>
Reference-contexts: The AGN may use acoustic features different from those used by the MLP; for example, microphone or SNR-dependent features may be more useful for region selection in channel or noise robustness applications <ref> [8] </ref>. 2.2. Transformation Network Architecture The transformation component of the MTN architecture is very flexible, and many types of TN can be trained. Each TN receives the same input. <p> Currently, the AGN weights are not adapted. * Training halts when phonetic frame classification performance on an independent cross-validation dataset ceases to improve. 2.4. Comparison with Other Algorithms The MTN architecture is similar to both Huang's CDNN architecture [9] and to Neumeyer's POF algorithm <ref> [8] </ref>. Compared to CDNN, we implement a piecewise linear transformation rather than piecewise nonlinear, and use soft decision boundaries rather than hard VQ decisions for combining the sub-networks.
Reference: [9] <author> X. Huang, </author> <title> Minimizing speaker variation effects for speaker-independent speech recognition, </title> <booktitle> in DARPA Speech and Natural Language Workshop, </booktitle> <address> (Harriman, NY), </address> <pages> pp. </pages> <address> 191196, </address> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: Currently, the AGN weights are not adapted. * Training halts when phonetic frame classification performance on an independent cross-validation dataset ceases to improve. 2.4. Comparison with Other Algorithms The MTN architecture is similar to both Huang's CDNN architecture <ref> [9] </ref> and to Neumeyer's POF algorithm [8]. Compared to CDNN, we implement a piecewise linear transformation rather than piecewise nonlinear, and use soft decision boundaries rather than hard VQ decisions for combining the sub-networks.
Reference: [10] <author> S. Waterhouse, D. Kershaw, and T. Robinson, </author> <title> Smoothed local adaptation of connectionist systems, </title> <booktitle> in Proceedings of the International Conference on Spoken Language Processing, </booktitle> <address> (Philadelphia, PA), </address> <year> 1996. </year>
Reference-contexts: The MTN architecture is part of the MLP phonetic classifier and is trained to optimize phonetic frame classification performance, which is related to recognition word error. Waterhouse <ref> [10] </ref> recently introduced a different method to adapt connectionist recognition systems using multiple input transformations. The recurrent neural network (RNN) acoustic model was duplicated and a single linear input transformation was paired with each duplicate copy.
Reference: [11] <author> F. Kubala et al., </author> <title> The hub and spoke paradigm for CSR evaluation, </title> <booktitle> in ARPA Spoken Language Technology Workshop, </booktitle> <address> (Plainsboro, NJ), </address> <pages> pp. 914, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: EXPERIMENTAL RESULTS SRI's DECIPHER speech recognizer was used for all experiments. Supervised adaptation experiments were performed on the male subset of the 1994 Spoke 3 (S3) development set of the Wall Street Journal (WSJ) speech corpus <ref> [11] </ref>. This data consists of read speech from nonnative speakers of American English, and was divided into 40 adaptation and 40 test sentences. The standard 5,000-word, closed-vocabulary bigram language model was used for recognition.
References-found: 11

