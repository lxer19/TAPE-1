URL: http://www.ee.technion.ac.il/publications/Neural-Networks/tseries.ps
Refering-URL: http://www.ultimode.com/stevew/moe/refs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Non-linear Models for Time Series Using Mixtures of Experts  
Author: Assaf J. Zeevi Ronny Meir yz Robert J. Adler 
Date: May 1996  
Abstract: We consider a novel non-linear model for time series analysis. The study of this model emphasizes both theoretical aspects as well as practical applicability. The architecture of the model is demonstrated to be sufficiently rich, in the sense of approximating unknown functional forms, yet it retains some of the simple and intuitive characteristics of linear models. A comparison to some more established non-linear models will be emphasized, and theoretical issues are backed by prediction results for benchmark time series, as well as computer generated data sets. Efficient estimation algorithms are seen to be applicable, made possible by the mixture based structure of the model. Large sample properties of the estimators are discussed as well, in both well specified as well as misspecified settings. We also demonstrate how inference pertaining to the data structure may be made from the parameterization of the model, resulting in a better, more intuitive, understanding of the structure and performance of the model.
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <title> (1974) "A New Look at the Statistical Model Identification", </title> <journal> IEEE Trans. AC, </journal> <volume> vol. 19:6, </volume> <pages> 716-723. </pages>
Reference: <author> Billingsley, P. </author> <title> (1962) Statistical Inference for Markov Processes, </title> <publisher> Holt, </publisher> <address> New York. </address>
Reference: <author> Brockwell, P.J. and Davis, R.A. </author> <title> (1991) Time Series: Theory and Methods, Second Edition, </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference-contexts: Obviously by considering the reduced likelihood we induce an estimation error, not present in (18). It is well known <ref> (e.g. Brockwell and Davis, 1991) </ref> that in many cases, asymptotically, the maximizer of the reduced likelihood exhibits the same limiting normal distribution, and is consistent. <p> A third expert allows for rapid changes to be tracked smoothly. Results comparing the one step prediction performance of the MEM (3; 6), TAR (2; 8; 3) (see Tong 1983), and AR (7) <ref> (Brockwell and Davis 1991) </ref> are given first for the learning set (1821-1920), followed by the results for the prediction set (1921-1934). The latter includes results for a bilinear model and an AR (2) model with random coefficients from Brockwell and Davis (1991).
Reference: <author> Dempster, A.P., Laird, N.M. and Rubin, </author> <title> D.B. (1977) "Maximum Likelihood from Incomplete Data via the EM Algorithm", </title> <journal> J. Roy. Statist. Soc., </journal> <volume> vol. </volume> <pages> B39, 1-38. </pages>
Reference-contexts: It turns out however, that under a broad range of conditions, there exists a very efficient estimation procedure, the so-called Expectation-Maximization (EM) algorithm <ref> (Dempster et al. 1977) </ref>. This algorithm has been extensively studied and utilized in related contexts. (See Titterington et al. (1985) for a comprehensive summary.) The algorithm was given by Jordan and Jacobs (1994) in the context of a regression problem (for i.i.d. data), using the MEM as a parameteric estimator. <p> By defining this step of steepest ascent we are actually in the framework of the GEM (generalized EM) algorithm <ref> (Dempster et al. 1977) </ref>, though for the sake of clarity and consistency we continue referring to the algorithm simply as EM. Having formulated the estimation problem as an EM algorithm, one can utilize the theoretical results concerning the convergence properties of the algorithm. <p> By global convergence we refer to the convergence of the algorithm from any initial condition. While it is well known that the likelihood function is non-decreasing in the course of the application of the EM algorithm <ref> (Dempster et al. 1977) </ref> this observation alone is insufficient to establish convergence of the parameter values to a limit point. The algorithm may end up cycling through a sequence of equi-likelihood points.
Reference: <author> Domowitz, I. and White, H. </author> <title> (1982) "Misspecified Models with Dependent Observations", </title> <journal> Journal of Econometrics, </journal> <volume> vol. 20: </volume> <pages> 35-58. </pages>
Reference: <author> Everitt, B.S. and Hand, </author> <title> D.J. (1981) Finite Mixture Distributions, </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Jacobs, R.A., Jordan, M.I., Nowlan, S.J. and Hinton, G.E. </author> <title> (1991) "Adaptive Mixtures of Local Experts", </title> <journal> Neural Computation, </journal> <volume> vol. 3 </volume> <pages> 79-87. </pages>
Reference: <author> Jordan, M. and Jacobs, R. </author> <title> (1994) "Hierarchical Mixtures of Experts and the EM Algorithm", </title> <journal> Neural Computation, </journal> <volume> vol. 6, </volume> <pages> pp. 181-214. </pages>
Reference: <author> Jordan, M.I. and Xu, L. </author> <title> "Convergence Results for the EM Approach to Mixtures of Experts Architectures", Neural Networks, </title> <publisher> in press. </publisher>
Reference: <author> Hamilton, J.D. </author> <title> (1994) Time Series Analysis, </title> <publisher> M.I.T. Press, </publisher> <address> Boston. </address>
Reference: <author> Hamilton, J.D. </author> <title> (1990) "Analysis of Time Series Subject to Changes in Regime", </title> <journal> Journal of Econometrics, </journal> <volume> vol. 45, </volume> <pages> pp. 39-70. </pages>
Reference-contexts: By partitioning the input space and assigning a separate model to each part, a more efficient analysis can be performed, which is `tailored' to the data structure. These concepts appear also in Hamilton's recent work on Markov switching models <ref> (Hamilton, 1990) </ref>, and regime switching models, summarized in Hamilton (1994). In the past few years, since their re-introduction, neural networks have become a popular tool in both modeling and forecasting in many fields.
Reference: <author> Luenberger, </author> <title> D.J. (1969) Optimization by Vector Space Methods, </title> <publisher> Wiley, </publisher> <address> NY. </address>
Reference-contexts: Addressing this issue, we have made a simple modification of the EM algorithm, based on the general class of space step methods <ref> (see Luenberger, 1969) </ref>. The addition of this step will guarantee that the likelihood function is strictly increasing, except possibly at a stationary point. Thus, we obtain a globally convergent algorithm, without sacrificing any of the favorable local convergence properties.
Reference: <author> McCullagh, P. and Nelder, J.A. </author> <title> (1989) Generalized Linear Models, Second Edition, </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: the following relation g j (x; g ) expf T g j x + g j;0 g i=1 expf T g i x + g i;0 g Thus the function g j () yields a smooth partitioning of the input space in the form of a multiple output logistic regression <ref> (see McCullagh and Nelder, 1989) </ref>. The objective of the maximization step is to maximize Q, and consequently (see the proofs in Dempster et al. (1977) and Wu (1983)), the likelihood (21) is maximized as well.
Reference: <author> Meyn, S.P. and Tweedie, </author> <title> R.L. (1993) Markov Chains and Stochastic Stability, </title> <publisher> Springer-Verlag, London. </publisher>
Reference: <author> Mhaskar, H. </author> <title> (1996) "Neural Networks for Optimal Approximation of Smooth and Analytic Functions", </title> <booktitle> Neural Computation vol. </booktitle> <volume> 8(1), </volume> <pages> pp. 164-177. </pages>
Reference: <author> Priestley M.B. </author> <title> (1988) Non-linear and Non-stationary Time Series Analysis, </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: We note in passing that in the more general case, where the domain of the nonlinear mapping '() includes lagged values of the noise process f" t g (i.e. a non-linear extension of the ARMA model), (16) can be straightforwardly extended <ref> (see Priestley, 1988) </ref> to include a summation over the lagged portion of f" t g, thus generalizing the ARMA model a well. Obviously, the SDM includes the linear ARMA models, bilinear models, and the TAR model as restricted cases (see Priestley (1988) for a more elaborate discussion).
Reference: <author> Priestley, </author> <title> M.B. (1980) "State-dependent Models: a General Approach to Non-linear Time Series Analysis", </title> <journal> J. Time Series Anal., </journal> <volume> vol. 1, </volume> <pages> pp. 47-71 Redner, </pages> <editor> R.A. and Walker, H.F. </editor> <title> (1984) "Mixture Densities, Maximum Likelihood and the EM Algorithm", </title> <journal> SIAM Review, </journal> <volume> vol. 26, </volume> <pages> pp. 195-239. </pages> <note> 28 Rudin, </note> <author> W. </author> <title> (1987) Real and Complex Analysis, Second Edition, </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Titterington, D.M. , Smith, A.F.M. and Makov, U.E. </author> <title> (1985) Statistical Analysis of Finite Mixture Distributions. </title> <publisher> Wiley & Sons. </publisher>
Reference: <author> Tong, H. </author> <title> (1983) Threshold Models in Non-linear Time Series Analysis, </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference-contexts: Here n denotes the number of linear regions utilized by the model. The vectors a (i) are the coefficients corresponding to the local linear approximation of ' by the TAR model. (For further discussion we refer the reader to Tong's monograph <ref> (Tong 1983) </ref> which is primarily dedicated to the study of this model.). <p> A third expert allows for rapid changes to be tracked smoothly. Results comparing the one step prediction performance of the MEM (3; 6), TAR (2; 8; 3) <ref> (see Tong 1983) </ref>, and AR (7) (Brockwell and Davis 1991) are given first for the learning set (1821-1920), followed by the results for the prediction set (1921-1934). The latter includes results for a bilinear model and an AR (2) model with random coefficients from Brockwell and Davis (1991).
Reference: <author> Tong, H. and Lim, </author> <title> K.S. (1980) "Threshold Autoregression, Limit Cycles and Cyclical Data", </title> <journal> J. R. Stat. Soc. B, </journal> <volume> vol. 42, </volume> <pages> 245-252. </pages>
Reference-contexts: These were Tong's TAR (threshold autoregressive) model <ref> (Tong and Lim, 1980) </ref>, and the more general SDM (state dependent models) introduced by Priestley (1980). The latter models can be reduced to a TAR model by imposing a more restrictive structure (for further details see Priestley (1988)) .
Reference: <author> Waterhouse, S.R. and Robinson, A.J. </author> <title> (1994) "Non-linear Prediction of Acoustic Vectors Using Hierarchical Mixtures of Experts", </title> <booktitle> in Neural Information Processing Systems 7, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Weigend, </author> <title> A.S. and Gershenfeld, N.A. (1994) Time Series Prediction: Forecasting the Future and Understanding the Past, </title> <booktitle> Santa Fe Institute Studies in the Science of Complexity, Proc. </booktitle> <volume> Vol. </volume> <pages> XV. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Since neural nets have gained popularity as models for time series analysis and forecasting, they have been applied to series ranging from the benchmark sunspot activity data set (Weigend et al. 1990) to the most chaotic and large data sets of the recent competition at the Santa Fe institute <ref> (Weigend and Gershenfeld 1994) </ref>. Results concerning the universality of these networks established their theoretical basis. Our Proposition 2.1 demonstrates the equivalence (in the sense of the approximation power) of the MEM to neural net models.
Reference: <author> Weigend, A.S Huberman, B.A. and Rumelhart, D.E. </author> <title> (1990) "Predicting the Future: A Connectionist Approach", </title> <booktitle> Intl. Jour. of Neural Systems, </booktitle> <volume> vol. 1, </volume> <pages> pp. 193-209. </pages>
Reference-contexts: Since neural nets have gained popularity as models for time series analysis and forecasting, they have been applied to series ranging from the benchmark sunspot activity data set <ref> (Weigend et al. 1990) </ref> to the most chaotic and large data sets of the recent competition at the Santa Fe institute (Weigend and Gershenfeld 1994). Results concerning the universality of these networks established their theoretical basis. <p> The TAR model was optimized via a similar approach (see Akaike's (1974) original paper, or Tong's monograph for a review and application to model selection for the TAR model). Neural network models were model selected by weight elimination and cross validation <ref> (Weigend et al. 1990) </ref>. In contrast, the MEM was optimized in quite a crude manner, using the following decision tree. We initialized the model with two experts (local AR models). Following the estimation process, we examined whether the parameterization of the two experts was sufficiently different. <p> For the sunpsot data the MEM yielded results which were comparable to the TAR model and slightly inferior to those reported recently using neural networks <ref> (Weigend et al. 1990) </ref>. In both cases, the data set was divided into a learning set, i.e. the part that was used to estimate the model's parameters, and a prediction set, used to assess the prediction error of the competing models. <p> The rise to peaks tends to be sharper than the subsequent falling pattern, leading to the assumption of nonlinear structure. We shall compare the MEM performance with that of the TAR model as well as some recent results reported for neural network models <ref> (Weigend et al. 1990) </ref>. As a reference point we shall bring the best available results for ARMA models reported in Brockwell and Davis (1991). Figure 4 describes the results obtained for an MEM (3; 12).
Reference: <author> Weigend, A.S. and Mangeas, M. </author> <title> (1995) "Non-linear Gated Experts for Time Series: Discovering Regimes and Avoiding Overfitting", </title> <type> Technical report, </type> <institution> Dept. of Computer Science University of Colorado. </institution>
Reference: <author> White, H. </author> <title> (1982) "Maximum Likelihood Estimation of Misspecified Models", </title> <journal> Econometrica, </journal> <volume> vol. 50, </volume> <pages> pp. 1-25. </pages>
Reference-contexts: Assuming the sample size, N , is sufficiently large we are assured under the assumptions of the Theorem that p fl1=2 n ( ^ n;N n ) converges in distribution to a standard normal random variable <ref> (c.f. Corollary 3.3 in Do mowitz and White, 1982) </ref> where A fl n td ; fl o n;N = N 8 : t=d+1 td ; fl 9 ; td ; ) = [X t f n (X t1 td ; )] 2 .
Reference: <author> White, H. </author> <title> (1994) Estimation, Inference and Specification Analysis, </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: our hopes of learning the underlying structure, we shall still be able to obtain results concerning the estimator's characteristic behavior, i.e. asymptotic normality and consistency, with respect to a certain parameter value which is `best' in a sense which will be made precise. (The reader is referred to White's monograph <ref> (White 1994) </ref> for a precise definition and elaborate discussion of these concepts.) 3.2 Theory of Maximum Likelihood Estimation for the Mixture of Experts Model Consider the problem of estimating the parameters of the transition kernel characterizing the underlying stochastic process by the method of maximum likelihood, given a sample set D
Reference: <author> White, H. and Domowitz, I. </author> <title> (1984) "Non-linear Regression with Dependent Observations", </title> <journal> Econometrica, </journal> <volume> vol. 52: </volume> <pages> 143-161. </pages>
Reference: <author> Wu, C.F.J., </author> <title> (1983) "On the Convergence Properties of the EM Algorithm", </title> <journal> Ann. Statist., </journal> <volume> vol. 11, </volume> <pages> pp. 95-103. </pages>
Reference-contexts: Although the local convergence properties of the algorithm are well understood it has been a little more difficult to establish general conditions under which the algorithm is globally 17 convergent <ref> (see Wu 1983) </ref>. By global convergence we refer to the convergence of the algorithm from any initial condition.
Reference: <author> Yule, G.U. </author> <title> (1927) "On a Method of Investigating Periodicities in Disturbed Series with Special Reference to Wolfer's Sunspot Numbers", </title> <journal> Philos. Trans. Roy. Soc. A, </journal> <volume> vol. 226, </volume> <month> pp.267-298. </month>
Reference: <author> Zeevi, A.J., Meir, R. and Maiorov, V. </author> <title> (1995) "Error Bounds for Functional Approximation and Estimation Using Mixtures of Experts", </title> <note> submitted to IEEE Transactions on Information Theory also in EE Pub. No. 113, </note> <institution> Electrical Engineering Department, Technion. </institution> <month> 29 </month>
Reference-contexts: We pursue this task in the framework of least squares (LS) estimation. The results presented herein are an extension to recent results in the context of non-linear regression <ref> (Zeevi et al. 1995) </ref>. The main difference is in the assumptions we make pertaining to the probabilistic structure of the random variables since the observations are dependent in the case of time series.
References-found: 30

