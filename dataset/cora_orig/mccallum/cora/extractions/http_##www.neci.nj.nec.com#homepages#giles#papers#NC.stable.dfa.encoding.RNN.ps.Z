URL: http://www.neci.nj.nec.com/homepages/giles/papers/NC.stable.dfa.encoding.RNN.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Title: Stable Encoding of Large Finite-State Automata in Recurrent Neural Networks with Sigmoid Discriminants  
Author: Christian W. Omlin a;b C. Lee Giles a;c 
Address: 4 Independence Way, Princeton, NJ 08540  Troy, NY 12180  College Park, MD 20742  
Affiliation: a NEC Research Institute,  b CS Department, Rensselaer Polytechnic Institute,  c UMIACS, U. of Maryland,  
Abstract: We propose an algorithm for encoding deterministic finite-state automata (DFAs) in second-order recurrent neural networks with sigmoidal discriminant function and we prove that the languages accepted by the constructed network and the DFA are identical. The desired finite-state network dynamics is achieved by programming a small subset of all weights. A worst case analysis reveals a relationship between the weight strength and the maximum allowed network size which guarantees finite-state behavior of the constructed network. We illustrate the method by encoding random DFAs with 10, 100, and 1,000 states. While the theory predicts that the weight strength scales with the DFA size, we find the weight strength to be almost constant for all the experiments. These results can be explained by noting that the generated DFAs represent average cases. We empirically demonstrate the existence of extreme DFAs for which the weight strength scales with DFA size.
Abstract-found: 1
Intro-found: 1
Reference: [Alon et al., 1991] <author> Alon, N., Dewdney, A., and Ott, T. </author> <year> (1991). </year> <title> Efficient simulation of finite automata by neural nets. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 38(2) </volume> <pages> 495-514. </pages>
Reference-contexts: The internal representation of learned DFA states can deteriorate due to the dynamical nature of recurrent networks making predictions about the generalization performance of trained recurrent networks difficult [Zeng et al., 1993]. Methods for constructing DFAs in recurrent networks with hard-limiting neurons discriminant functions have been proposed <ref> [Alon et al., 1991, Horne and Hush, 1994, Minsky, 1967] </ref>; fl To appear in Neural Computation. 1 methods for constructing networks with sigmoidal and radial-basis discriminant functions are discussed in [Alquezar and Sanfeliu, 1995, Frasconi et al., 1993, Gori et al., 1994, Giles and Omlin, 1993].
Reference: [Alquezar and Sanfeliu, 1995] <author> Alquezar, R. and Sanfeliu, A. </author> <year> (1995). </year> <title> An algebraic framework to represent finite-state machines in single-layer recurrent neural networks. Neural Computation. </title> <note> To appear. </note>
Reference: [Elman, 1990] <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211. </pages>
Reference: [Frasconi et al., 1991] <author> Frasconi, P., Gori, M., Maggini, M., and Soda, G. </author> <year> (1991). </year> <title> A unified approach for integrating explicit knowledge and learning by example in recurrent networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> page 811. </pages> <publisher> IEEE 91CH3049-4. </publisher>
Reference: [Frasconi et al., 1993] <author> Frasconi, P., Gori, M., and Soda, G. </author> <year> (1993). </year> <title> Injecting nondeterministic finite state automata into recurrent networks. </title> <type> Technical report, </type> <institution> Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy, Florence, Italy. </institution>
Reference: [Giles et al., 1992] <author> Giles, C., Miller, C., Chen, D., Chen, H., Sun, G., and Lee, Y. </author> <year> (1992). </year> <title> Learning and extracting finite state automata with second-order recurrent neural networks. Neural Computation, </title> <publisher> 4(3):380. </publisher>
Reference: [Giles and Omlin, 1993] <author> Giles, C. and Omlin, C. </author> <year> (1993). </year> <title> Extraction, insertion and refinement of symbolic rules in dynamically driven recurrent neural networks. </title> <journal> Connection Science, </journal> <volume> 5(3 </volume> & 4):307-337. 
Reference: [Gori et al., 1994] <author> Gori, M., Maggini, M., and Soda, G. </author> <year> (1994). </year> <title> Insertion of finite state automata in recurrent radial basis function networks. </title> <type> Technical report, </type> <institution> Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy, Florence, Italy. </institution>
Reference: [Hopcroft and Ullman, 1979] <author> Hopcroft, J. and Ullman, J. </author> <year> (1979). </year> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <address> Reading, MA. </address>
Reference-contexts: We hypothesize that dense neural DFA construction is a NP-complete problem <ref> [Hopcroft and Ullman, 1979] </ref>. 7 ACKNOWLEDGMENT We would like to acknowledge useful discussions with B.G. Horne, L. R. Leerink, and T. Lin. We would especially like to acknowledge helpful suggestions from the reviewer. 17
Reference: [Horne and Hush, 1994] <author> Horne, B. and Hush, D. </author> <year> (1994). </year> <title> Bounds on the complexity of recurrent neural network implementations of finite state machines. </title> <editor> In Cowen, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 359-366. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The internal representation of learned DFA states can deteriorate due to the dynamical nature of recurrent networks making predictions about the generalization performance of trained recurrent networks difficult [Zeng et al., 1993]. Methods for constructing DFAs in recurrent networks with hard-limiting neurons discriminant functions have been proposed <ref> [Alon et al., 1991, Horne and Hush, 1994, Minsky, 1967] </ref>; fl To appear in Neural Computation. 1 methods for constructing networks with sigmoidal and radial-basis discriminant functions are discussed in [Alquezar and Sanfeliu, 1995, Frasconi et al., 1993, Gori et al., 1994, Giles and Omlin, 1993].
Reference: [Minsky, 1967] <author> Minsky, M. </author> <year> (1967). </year> <title> Computation: Finite and Infinite Machines, </title> <booktitle> chapter 3, </booktitle> <pages> pages 32-66. </pages> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: The internal representation of learned DFA states can deteriorate due to the dynamical nature of recurrent networks making predictions about the generalization performance of trained recurrent networks difficult [Zeng et al., 1993]. Methods for constructing DFAs in recurrent networks with hard-limiting neurons discriminant functions have been proposed <ref> [Alon et al., 1991, Horne and Hush, 1994, Minsky, 1967] </ref>; fl To appear in Neural Computation. 1 methods for constructing networks with sigmoidal and radial-basis discriminant functions are discussed in [Alquezar and Sanfeliu, 1995, Frasconi et al., 1993, Gori et al., 1994, Giles and Omlin, 1993].
Reference: [Omlin and Giles, 1994] <author> Omlin, C. and Giles, C. </author> <year> (1994). </year> <title> Constructing deterministic finite-state automata in sparse recurrent neural networks. </title> <type> Technical Report 94-3, </type> <institution> Computer Science Department, Rensselaer Polytechnic Institute. </institution>
Reference-contexts: Due to space limitations, we only give the proofs of the theorems which establish our results; for proofs of auxiliary lemmas see <ref> [Omlin and Giles, 1994] </ref>. 3.1 Fixed Point Analysis for Sigmoidal Discriminant Function Recall that the recurrent network changes its state according to equation (1). <p> We state here without proof the following facts about fixed points of the function h (:); the proofs can be found in <ref> [Omlin and Giles, 1994] </ref>. Lemma 3.1.1 For 0 &lt; H &lt; 4, h (x; H) has the following fixed point: 0 = 0:5 Furthermore, h (x; H) converge to 0 for any choice of a start value x 0 .
Reference: [Pollack, 1991] <author> Pollack, J. </author> <year> (1991). </year> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 227-252. 18 </pages>
Reference: [Servan-Schreiber et al., 1991] <author> Servan-Schreiber, D., Cleeremans, A., and McClelland, J. </author> <year> (1991). </year> <title> Graded state machine: The representation of temporal contingencies in simple recurrent networks. Machine Learning, </title> <publisher> 7:161. </publisher>
Reference: [Tino, 1994] <author> Tino, P. </author> <year> (1994). </year> <title> Private communication. </title>
Reference-contexts: The graph shows the minimum value of H for correct classification of 100 strings of length 100. H increases up to = 75%; for &gt; 75%, the DFA becomes degenerated causing H to decrease again. dynamical attractors often has a value 6 <ref> [Tino, 1994] </ref>.] However, there exist DFAs which exhibit the scaling behavior that is predicted by the theory. We will briefly discuss such DFAs.
Reference: [Watrous and Kuhn, 1992] <author> Watrous, R. and Kuhn, G. </author> <year> (1992). </year> <title> Induction of finite-state languages using second-order recurrent networks. Neural Computation, </title> <publisher> 4(3):406. </publisher>
Reference: [Zeng et al., 1993] <author> Zeng, Z., Goodman, R., and Smyth, P. </author> <year> (1993). </year> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 976-990. 19 </pages>
Reference-contexts: The internal representation of learned DFA states can deteriorate due to the dynamical nature of recurrent networks making predictions about the generalization performance of trained recurrent networks difficult <ref> [Zeng et al., 1993] </ref>.
References-found: 17

