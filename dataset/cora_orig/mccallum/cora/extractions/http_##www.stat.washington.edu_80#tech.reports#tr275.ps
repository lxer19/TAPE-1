URL: http://www.stat.washington.edu:80/tech.reports/tr275.ps
Refering-URL: http://www.stat.washington.edu:80/tech.reports/
Root-URL: 
Title: Trees and Splines in Survival Analysis  
Author: Orna Intrator Charles Kooperberg 
Address: Jerusalem, Israel  Seattle, USA  
Affiliation: Department of Statistics Hebrew University  Department of Statistics University of Washington  
Abstract: Technical Report No. 275 Revised March 30, 1995 University of Washington Department of Statistics Seattle, Washington 98195 Abstract During the past few years several nonparametric alternatives to the Cox proportional hazards model have appeared in the literature. These methods extend techniques that are well known from regression analysis to the analysis of censored survival data. In this paper we discuss methods based on (partition) trees and (polynomial) splines, analyze two datasets using both Survival Trees[1] and HARE[2], and compare the strengths and weaknesses of the two methods. One of the strengths of HARE is that its model fitting procedure has an implicit check for proportionality of the underlying hazards model. It also provides an explicit model for the conditional hazards function, which makes it very convenient to obtain graphical summaries. On the other hand, the tree-based methods automatically partition a dataset into groups of cases that are similar in survival history. Results obtained by survival trees and HARE are often complimentary. Trees and splines in survival analysis should provide the data analyst with two useful tools when analyzing survival data.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Intrator O. </author> <title> Exploratory trees for semi-markov processes. </title> <booktitle> In Proceedings of the 23rd symposium on the interface of computing science and statistics, </booktitle> <year> 1991; </year> <month> 352-55. </month>
Reference-contexts: One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences <ref> [1, 20, 21, 22, 24, 23] </ref>. Gordon and Olshen [15] presented the first extension of CART to censored survival data, which involved a distance measure (the Wasserstein metric) between Kaplan-Meier curves and certain point masses. Their approach amounts to assuming a piecewise exponential model with one data-determined knot. <p> He introduced a totally nonparametric tree algorithm, basing the partitioning on between-node separation using the Harrington-Fleming [25] class of two sample rank statistics. Pruning based on between-node separation, as discussed in <ref> [1, 20, 23] </ref>, is conceptually harder. 2.2 Review of CART The Classification and Regression Trees (CART) method of Breiman et al.[3] addresses the classification and regression problem by building a binary decision tree according to some splitting rule based on the covariates. <p> In regression the error denoted by R (T ) is the average squared error in the terminal nodes. Tree selection can also be done in an exploratory manner by examining trees in sequence. 2.3 Survival Trees Based on Between-Node Separation Segal [20], Intrator <ref> [1] </ref> and LeBlanc and Crowley [23] use as the prediction rule the Kaplan-Meier estimate of the survival distribution, and as the splitting rule a test for measuring differences between distributions adapted to censored data such as the log-rank test or Wilcoxon test, and more generally the G ;fl class of rank <p> For example, Intrator <ref> [1] </ref> chooses g (x) = x 1: We get an increasing sequence of the critical pruning values: for tree T j1 in the sequence we define a j as a j = min t2T j m (t)=g (j e T t ): Tree T j is then tree T j1 pruned <p> The sequence of a's defines a nested sequence of pruned subtrees. As in CART, exploratory tree selection can be done by examining the plot of values of the penalty parameter a versus tree size. Tree selection may be based on goodness-of-prediction measures. Intrator <ref> [1] </ref> explores the mea sure given by PE (T a ) = t2 e T a where S ts (t) is the estimated survival curve at node t based on a test sample, S ls (t) is that based on the learning sample, and 1 v (; ) is the P <p> Many other useful features of CART can be incorporated in survival trees. For example, Intrator <ref> [1] </ref> explores tree robustness using a cross-validation approach while growing the tree, testing all possible splits on several samples and selecting the split that was the best on most samples. <p> Of the 2404 cases, 1116 were uncensored and 1288 were censored. There were no missing values for any of the covariates. Survival Trees analysis In Sections 2.3 and 2.4 we briefly described a number of survival tree methods <ref> [1, 15, 20, 17, 18, 23] </ref>. In the examples section we mainly present the results from Intrator's method [1], and briefly describe results from Davis and Anderson's method [17] and LeBlanc and Crowley's method [23]. 9 Variables analyzed in survival trees are always reduced to dichotomies. <p> There were no missing values for any of the covariates. Survival Trees analysis In Sections 2.3 and 2.4 we briefly described a number of survival tree methods [1, 15, 20, 17, 18, 23]. In the examples section we mainly present the results from Intrator's method <ref> [1] </ref>, and briefly describe results from Davis and Anderson's method [17] and LeBlanc and Crowley's method [23]. 9 Variables analyzed in survival trees are always reduced to dichotomies. <p> The possible split points for size were 10mm, 15mm, : : : , 65mm. The possible BMI split points were 20, 22.5, 25, 29, and 33 kg/m . The age split points examined were 30, 40, 50, 60 and 70 years. The survival trees program of Intrator <ref> [1] </ref> was run using all splits, a partial set of splits, and on different randomly selected subsamples to test for tree robustness and competing structures. A variety of pairs of values for and fl were used for emphasizing early, middle and late differences between survival experiences. <p> An analysis using Intrator's method <ref> [1] </ref> validated Carmelli's results, using various rank tests ( and fl combinations) for splitting and the pruning algorithm presented above, which is different from the pruning algorithm used by Carmelli et al. <p> The basic ingredients that are involved in actual applications using these methods have been introduced here. Details about the programs can be found in the papers where the methods were introduced. HARE and HEFT are available from statlib (statlib@stat.cmu.edu). Intrator's program <ref> [1] </ref> is available by sending email to msorna@olive.huji.ac.il.
Reference: [2] <author> Kooperberg C, Stone CJ, Truong YK. </author> <title> Hazard regression. </title> <journal> Journal of the American Statistical Association 1995; 90: </journal> <pages> 78-94. </pages>
Reference-contexts: In the context of survival analysis, Etezadi-Amoli and Ciampi [43], Efron [44] and Abrahamo-wicz, Ciampi and Ramsay [45] use polynomial splines to model either the unconditional distribution of the survival times or the baseline hazard function within a proportional hazards model. Kooper-berg, Stone and Truong <ref> [2] </ref> develop hazard regression (HARE), in which the conditional log-hazard function is modeled using polynomial splines. Kooperberg [46] extends the HARE methodology to handle interval-censored data. <p> Kooperberg [46] extends the HARE methodology to handle interval-censored data. Under suitable conditions Kooperberg, Stone and Truong [47] obtain the L 2 rate of convergence for a nonadaptive version of HARE. 3.2 HARE In hazard regression (HARE, Kooperberg, Stone and Truong <ref> [2] </ref>, hereafter referred to as KST), polynomial splines are used to estimate the conditional log-hazard function based on possibly censored survival data and one or more covariates. An automatic procedure involving maximum likelihood, stepwise addition, stepwise deletion and BIC is used to select the final model. <p> In Figure 2 we show estimates for the survival functions for the nine terminal nodes. We show both the usual Kaplan-Meier curves (left side) as well as estimates using Hazard Estimation with Flexible Tails <ref> [2] </ref> (HEFT). HEFT employs cubic splines and has some additional log terms that make it possible to estimate tails more flexibly; otherwise HEFT is very similar to HARE. See KST for more details. 10 Table 1. Cost-complexity and tree size for the breast cancer data.
Reference: [3] <author> Breiman L, Friedman JH, Olshen RA, Stone CJ. </author> <title> Classification and Regression Trees. </title> <address> Pacific Grove, California: </address> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: The nonparametric methods discussed in this paper can give insight into data that the traditional methods fail to provide. The use of Classification and Regression Trees <ref> [3] </ref> (hereafter referred to as CART) and other recursive partitioning methods have allowed a more thorough examination of effects of variables fl Address for correspondence: C. Kooperberg, Department of Statistics GN-22, University of Washington, WA 98195, USA on the survival distribution. <p> The uses of surrogate splits for handling missing data and variable importance, ideas initially presented in CART <ref> [3] </ref>, are also extended to the survival trees setting. 2.4 Survival Trees Based on Within-Node Homogeneity Tree building and pruning based on within-node homogeneity allows for trivial inheritance of the CART algorithm.
Reference: [4] <author> Friedman JH. </author> <title> Multivariate regression splines (with discussion). </title> <booktitle> The Annals of Statistics 1988; 19: </booktitle> <pages> 1-141. </pages>
Reference-contexts: All these features are highly desirable for exploratory analyses. Polynomial splines form a versatile tool for function estimation. They have been used in many situations such as multiple regression <ref> [4] </ref>, density estimation [5], estimation of the spectral distribution [6], and polychotomous regression and classification [7]. In the polynomial spline approach, an unknown function is modeled in a linear space. Stepwise algorithms make it possible to determine this space adaptively. <p> Stepwise algorithms make it possible to determine this space more adaptively than in the smoothing spline approach, which involves only a few smoothing parameters. Adaptive algorithms for polynomial splines were first introduced in a regression context [42]. Other applications include multiple regression (MARS <ref> [4] </ref>), density estimation (LOGSPLINE [5]), and spectral distribution estimation (LSPEC [6]). <p> When a proportional hazards model is appropriate, HARE, especially when used in conjunction with HEFT, provides a smooth estimate for the underlying baseline hazard function. It also provides a MARS-like <ref> [4] </ref> model for the conditional hazards function. Another strength is a graphical interface that makes it very easy to look at curves such as conditional hazards functions. Thus a HARE model is potentially useful for a health care practitioner in coming up with a prognosis for a particular patient.
Reference: [5] <author> Kooperberg C, Stone CJ. </author> <title> Logspline density estimation for censored data. </title> <journal> Journal of Computational and Graphical Statistics 1992; 1: </journal> <pages> 301-28. </pages>
Reference-contexts: All these features are highly desirable for exploratory analyses. Polynomial splines form a versatile tool for function estimation. They have been used in many situations such as multiple regression [4], density estimation <ref> [5] </ref>, estimation of the spectral distribution [6], and polychotomous regression and classification [7]. In the polynomial spline approach, an unknown function is modeled in a linear space. Stepwise algorithms make it possible to determine this space adaptively. <p> Stepwise algorithms make it possible to determine this space more adaptively than in the smoothing spline approach, which involves only a few smoothing parameters. Adaptive algorithms for polynomial splines were first introduced in a regression context [42]. Other applications include multiple regression (MARS [4]), density estimation (LOGSPLINE <ref> [5] </ref>), and spectral distribution estimation (LSPEC [6]). In the context of survival analysis, Etezadi-Amoli and Ciampi [43], Efron [44] and Abrahamo-wicz, Ciampi and Ramsay [45] use polynomial splines to model either the unconditional distribution of the survival times or the baseline hazard function within a proportional hazards model.
Reference: [6] <author> Kooperberg C, Stone CJ, Truong YK. </author> <title> Logspline estimation of a possibly mixed spectral distribution. </title> <journal> Journal of Time Series Analysis, </journal> <note> 1995; to appear. 21 </note>
Reference-contexts: All these features are highly desirable for exploratory analyses. Polynomial splines form a versatile tool for function estimation. They have been used in many situations such as multiple regression [4], density estimation [5], estimation of the spectral distribution <ref> [6] </ref>, and polychotomous regression and classification [7]. In the polynomial spline approach, an unknown function is modeled in a linear space. Stepwise algorithms make it possible to determine this space adaptively. <p> Adaptive algorithms for polynomial splines were first introduced in a regression context [42]. Other applications include multiple regression (MARS [4]), density estimation (LOGSPLINE [5]), and spectral distribution estimation (LSPEC <ref> [6] </ref>). In the context of survival analysis, Etezadi-Amoli and Ciampi [43], Efron [44] and Abrahamo-wicz, Ciampi and Ramsay [45] use polynomial splines to model either the unconditional distribution of the survival times or the baseline hazard function within a proportional hazards model.
Reference: [7] <author> Bose S, Kooperberg C, Stone CJ. </author> <title> Polychotomous regression. </title> <type> Manuscript, </type> <year> 1995. </year>
Reference-contexts: All these features are highly desirable for exploratory analyses. Polynomial splines form a versatile tool for function estimation. They have been used in many situations such as multiple regression [4], density estimation [5], estimation of the spectral distribution [6], and polychotomous regression and classification <ref> [7] </ref>. In the polynomial spline approach, an unknown function is modeled in a linear space. Stepwise algorithms make it possible to determine this space adaptively. In the proportional hazards model [8] the conditional log-hazard function is an additive function of time and the vector of covariates.
Reference: [8] <author> Cox DR. </author> <title> Regression models and life tables (with discussion). </title> <journal> Journal of the Royal Statistical Society Series B 1972; 34: </journal> <pages> 187-220. </pages>
Reference-contexts: In the polynomial spline approach, an unknown function is modeled in a linear space. Stepwise algorithms make it possible to determine this space adaptively. In the proportional hazards model <ref> [8] </ref> the conditional log-hazard function is an additive function of time and the vector of covariates. Traditionally, in this model the dependence of the survival time on the covariates is modeled fully parametrically, so that the regression function can be estimated independently of the baseline hazard function. <p> Before pursuing these issues further, we should point out that if none of the basis functions of G depend on both t and x, then (2) is a proportional hazards model <ref> [8] </ref>. It is a particular interesting 7 feature of HARE that the model selection procedure may or may not result in such a model. <p> Nevertheless, if the three restricted models were tested against the unrestricted model, relying on standard 2 statistics, then all three smaller models would be strongly rejected in favor of the larger model. Proportional Hazards analysis We also fit a Cox proportional hazards model <ref> [8] </ref> to the breast cancer data, using a backwards stepwise algorithm. There were two initial sets of covariates that 13 Table 3. Summary of several HARE models for the transformed breast cancer data. Model Full Additive Prop. Haz.
Reference: [9] <author> Kwak, LW, Halpern J, Olshen RA, Horning SJ. </author> <title> Prognostic significance of actual dose intensity in diffuse large cell lymphoma: Results of tree-structured survival analysis. </title> <journal> Journal of Clinical Oncology, </journal> <volume> 1990; 8: </volume> <pages> 963-77. </pages>
Reference-contexts: The prediction rule for survival analysis is typically based on the estimate of the distribution function, which implies that the three other ingredients all work nonparametrically on the space of distributions. Applications of survival trees may be found in several articles <ref> [9, 10, 11, 12, 13, 14] </ref>. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures.
Reference: [10] <author> Carmelli D, Halpern J, Swan GE, </author> <title> Dame A, McElroy M, Gelb AB, Rosenman RH. 27-Year mortality in the Western Collaborative Group Study: construction of risk groups by recursive partitioning. </title> <journal> Journal of Clinical Epidemiology 1991; 44: </journal> <pages> 1341-51. </pages>
Reference-contexts: The prediction rule for survival analysis is typically based on the estimate of the distribution function, which implies that the three other ingredients all work nonparametrically on the space of distributions. Applications of survival trees may be found in several articles <ref> [9, 10, 11, 12, 13, 14] </ref>. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures. <p> that future versions of HARE will have such an ability.) As in the previous example, we present the survival tree analysis followed by the HARE analysis and a proportional hazards analysis and we conclude the example with a comparison between the three methods. 15 Survival Tree Analysis Carmelli et al. <ref> [10] </ref> presented their survival tree analysis of the CHD data. <p> If a linear proportional hazards model with just these four variables is fit, the coefficients are virtually the same as in Table 5. It is 19 interesting to note that in the survival tree approach <ref> [10] </ref> smoking did not end up in the model. Comparison Whereas for our first example the main results from the survival tree analysis were confirmed by the HARE analysis, this is much less the case for the second example.
Reference: [11] <author> Piette JD, Intrator O, Zierler S, Mor V, Stein M. </author> <title> Differences in case fatality rates for Aids patients: application of a new methodology for survival research. </title> <booktitle> Epidemiology 1992; 3: </booktitle> <pages> 310-18. </pages>
Reference-contexts: The prediction rule for survival analysis is typically based on the estimate of the distribution function, which implies that the three other ingredients all work nonparametrically on the space of distributions. Applications of survival trees may be found in several articles <ref> [9, 10, 11, 12, 13, 14] </ref>. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures.
Reference: [12] <author> Mor V, Intrator O, Laliberte LL. </author> <title> Factors affecting conversion rates to Medicaid among new admissions to nursing homes. </title> <booktitle> Health Services Research 1993; 28: </booktitle> <pages> 1-25. </pages>
Reference-contexts: The prediction rule for survival analysis is typically based on the estimate of the distribution function, which implies that the three other ingredients all work nonparametrically on the space of distributions. Applications of survival trees may be found in several articles <ref> [9, 10, 11, 12, 13, 14] </ref>. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures.
Reference: [13] <author> Schumacher M, Schmoor C, Sauerbrei W, Schauer A, Ummenhofer L, Gatzemeier W, Rauschecker H. </author> <title> The prognostic effect of histological tumor grade in node negative breast cancer patients. </title> <booktitle> Breast Cancer Research and Treatment 1993; 25: </booktitle> <pages> 235-45. </pages>
Reference-contexts: The prediction rule for survival analysis is typically based on the estimate of the distribution function, which implies that the three other ingredients all work nonparametrically on the space of distributions. Applications of survival trees may be found in several articles <ref> [9, 10, 11, 12, 13, 14] </ref>. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures.
Reference: [14] <author> Curran WJ, Scott CB, Horton J, Nelson JS, Weinstein AS, Fischbach AJ, Chang CH, Rotman M, Asbell SO, Krisch RE. </author> <title> Recursive partitioning analysis of prognostic factors in three radiation therapy oncology group malignant glioma trials. </title> <booktitle> Journal of the National Cancer Institute 1993; 85:9: </booktitle> <pages> 704-710. </pages>
Reference-contexts: The prediction rule for survival analysis is typically based on the estimate of the distribution function, which implies that the three other ingredients all work nonparametrically on the space of distributions. Applications of survival trees may be found in several articles <ref> [9, 10, 11, 12, 13, 14] </ref>. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures.
Reference: [15] <author> Gordon L, Olshen RA. </author> <title> Tree-structured survival analysis. Cancer Treatment Reports, </title> <booktitle> 1985; 69: </booktitle> <pages> 1065-69. </pages>
Reference-contexts: Applications of survival trees may be found in several articles [9, 10, 11, 12, 13, 14]. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node <ref> [15, 16, 17, 18, 19] </ref>. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences [1, 20, 21, 22, 24, 23]. <p> The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences [1, 20, 21, 22, 24, 23]. Gordon and Olshen <ref> [15] </ref> presented the first extension of CART to censored survival data, which involved a distance measure (the Wasserstein metric) between Kaplan-Meier curves and certain point masses. Their approach amounts to assuming a piecewise exponential model with one data-determined knot. <p> Of the 2404 cases, 1116 were uncensored and 1288 were censored. There were no missing values for any of the covariates. Survival Trees analysis In Sections 2.3 and 2.4 we briefly described a number of survival tree methods <ref> [1, 15, 20, 17, 18, 23] </ref>. In the examples section we mainly present the results from Intrator's method [1], and briefly describe results from Davis and Anderson's method [17] and LeBlanc and Crowley's method [23]. 9 Variables analyzed in survival trees are always reduced to dichotomies. <p> They used log-rank statistics to define the splitting rule, while the pruning and tree selection were as in Gordon and Olshen <ref> [15] </ref>, in which the risk at a node is defined by the fourth power Wasserstein distance between the Kaplan-Meier survival curve of the subjects in a node and a piecewise exponential model with one knot. <p> Davis' program [17] is available by sending email directly to rdaids@sdac.harvard.edu, LeBlanc and Crowley's program [23] is available by sending email to mikel@orca.fhcrc.org, Segal's program [20] is vailable by sending email to mark@segal.ucsf.edu, Zhang's program [19] is available by sending email to heping@peace.med.yale.edu, and Gordon and Olshen's program <ref> [15] </ref> is available solely for academic use by sending email to dstein@saturn.sdsu.edu. Hazard regression and survival trees are two new methods for the analysis of survival data that deserve a place in the toolbox of the survival analyst. These methods have their specific strengths.
Reference: [16] <author> Ciampi A, Chang, CH, Hogg S, McKinney S. </author> <title> Recursive Partitioning: a versatile method for exploratory data analysis in Biostatistics. </title> <editor> In: MacNeil IB, Umphrey G eds. </editor> <booktitle> Proceedings from Joshi Festschrift. </booktitle> <address> Amsterdam: </address> <publisher> North Holland. </publisher>
Reference-contexts: Applications of survival trees may be found in several articles [9, 10, 11, 12, 13, 14]. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node <ref> [15, 16, 17, 18, 19] </ref>. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences [1, 20, 21, 22, 24, 23]. <p> Gordon and Olshen [15] presented the first extension of CART to censored survival data, which involved a distance measure (the Wasserstein metric) between Kaplan-Meier curves and certain point masses. Their approach amounts to assuming a piecewise exponential model with one data-determined knot. Ciampi et al.'s method <ref> [16] </ref> is based on a parametric model and likelihood ratio 2 statistics. Davis and Anderson [17] suggest a method based on the observed likelihood at a node, while assuming an exponential model for the baseline hazard function.
Reference: [17] <author> Davis RB, Anderson JR. </author> <title> Exponential survival trees. </title> <journal> Statistics in Medicine, </journal> <volume> 1989; 8: </volume> <pages> 947-61. </pages>
Reference-contexts: Applications of survival trees may be found in several articles [9, 10, 11, 12, 13, 14]. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node <ref> [15, 16, 17, 18, 19] </ref>. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences [1, 20, 21, 22, 24, 23]. <p> Their approach amounts to assuming a piecewise exponential model with one data-determined knot. Ciampi et al.'s method [16] is based on a parametric model and likelihood ratio 2 statistics. Davis and Anderson <ref> [17] </ref> suggest a method based on the observed likelihood at a node, while assuming an exponential model for the baseline hazard function. LeBlanc and Crowley [18] use deviance residuals based on the Cox proportional hazards model for the splitting rule. <p> When L 2 Wassestein distances are used, the homogeneity corresponds to the variance of the Kaplan-Meier estimate. Davis and Anderson <ref> [17] </ref> define within-node homogeneity based on the negative log-likelihood of an exponential model at the node. <p> Of the 2404 cases, 1116 were uncensored and 1288 were censored. There were no missing values for any of the covariates. Survival Trees analysis In Sections 2.3 and 2.4 we briefly described a number of survival tree methods <ref> [1, 15, 20, 17, 18, 23] </ref>. In the examples section we mainly present the results from Intrator's method [1], and briefly describe results from Davis and Anderson's method [17] and LeBlanc and Crowley's method [23]. 9 Variables analyzed in survival trees are always reduced to dichotomies. <p> Survival Trees analysis In Sections 2.3 and 2.4 we briefly described a number of survival tree methods [1, 15, 20, 17, 18, 23]. In the examples section we mainly present the results from Intrator's method [1], and briefly describe results from Davis and Anderson's method <ref> [17] </ref> and LeBlanc and Crowley's method [23]. 9 Variables analyzed in survival trees are always reduced to dichotomies. <p> The two next splits were also very similar: the node with (nodes 3) was split at (size 52) and the note with ER positive was split on menopausal state. Splits further down the tree were more different. The tree obtained using the method of Davis and Anderson <ref> [17] </ref> split at the root node to the left on (nodes 4), after which the left node split on (nodes 6) and the right node split on (size 25). This presents a model similar to that presented in Figure 1, where the smaller number of nodes is split on size. <p> Trees from the methods of LeBlanc and Crowley [23], and of Davis and Anderson <ref> [17] </ref> were obtained. Leblanc and Crowley's method produced a tree in which the first split is on age, followed by several splits (in both branches) on SBP follow. Actually, after four splits we have the following five nodes: 1. a decision node for which (age 48) and (SBP 151). <p> Details about the programs can be found in the papers where the methods were introduced. HARE and HEFT are available from statlib (statlib@stat.cmu.edu). Intrator's program [1] is available by sending email to msorna@olive.huji.ac.il. Davis' program <ref> [17] </ref> is available by sending email directly to rdaids@sdac.harvard.edu, LeBlanc and Crowley's program [23] is available by sending email to mikel@orca.fhcrc.org, Segal's program [20] is vailable by sending email to mark@segal.ucsf.edu, Zhang's program [19] is available by sending email to heping@peace.med.yale.edu, and Gordon and Olshen's program [15] is available solely for
Reference: [18] <author> LeBlanc M, Crowley J. </author> <title> Relative risk trees for censored data. </title> <booktitle> Biometrics 1992; 48: </booktitle> <pages> 411-25. </pages>
Reference-contexts: Applications of survival trees may be found in several articles [9, 10, 11, 12, 13, 14]. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node <ref> [15, 16, 17, 18, 19] </ref>. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences [1, 20, 21, 22, 24, 23]. <p> Ciampi et al.'s method [16] is based on a parametric model and likelihood ratio 2 statistics. Davis and Anderson [17] suggest a method based on the observed likelihood at a node, while assuming an exponential model for the baseline hazard function. LeBlanc and Crowley <ref> [18] </ref> use deviance residuals based on the Cox proportional hazards model for the splitting rule. These extensions of CART are based on a definition of a within-node homogeneity measure. <p> LeBlanc and Crowley <ref> [18] </ref> assume a semiparametric proportional hazard model, where the hazard (tjz i ) at time t for individual i with covariates z i is the product of a baseline hazard that depends only on time and a structural component that depends on the individual through its covariates 0 (t)(z i ). <p> Of the 2404 cases, 1116 were uncensored and 1288 were censored. There were no missing values for any of the covariates. Survival Trees analysis In Sections 2.3 and 2.4 we briefly described a number of survival tree methods <ref> [1, 15, 20, 17, 18, 23] </ref>. In the examples section we mainly present the results from Intrator's method [1], and briefly describe results from Davis and Anderson's method [17] and LeBlanc and Crowley's method [23]. 9 Variables analyzed in survival trees are always reduced to dichotomies.
Reference: [19] <author> Zhang HP. </author> <title> Splitting Criteria in Survival Trees. </title> <booktitle> In 10th Workshop on Statistical Modeling. Lecture Notes in Statistics Series. </booktitle> <publisher> Springer Verlag. </publisher> <year> 1995. </year>
Reference-contexts: Applications of survival trees may be found in several articles [9, 10, 11, 12, 13, 14]. The extensions of CART to survival data fall into two groups. One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node <ref> [15, 16, 17, 18, 19] </ref>. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences [1, 20, 21, 22, 24, 23]. <p> The use of within-node homogeneity based on likelihood statistics allows the inheritance of all subsequent CART methodology, since the measures defined are all subadditive, allowing comparison between subtrees. However, except for Gordon and Olshen's work, these methods are not devoid of parametric assumptions about the underlying hazards model. Zhang <ref> [19] </ref> uses a within-node homogeneity measure that is based on the idea that a homogeneous node should consist of subjects whose observed failure times are close and who are mostly censored or mostly uncensored. <p> Lastly, Zhang <ref> [19] </ref> introduces a totally different concept of splitting. He argues that a homogeneous node should consist of subjects whose observed failure times are close and who are mostly censored or mostly uncensored. <p> Intrator's program [1] is available by sending email to msorna@olive.huji.ac.il. Davis' program [17] is available by sending email directly to rdaids@sdac.harvard.edu, LeBlanc and Crowley's program [23] is available by sending email to mikel@orca.fhcrc.org, Segal's program [20] is vailable by sending email to mark@segal.ucsf.edu, Zhang's program <ref> [19] </ref> is available by sending email to heping@peace.med.yale.edu, and Gordon and Olshen's program [15] is available solely for academic use by sending email to dstein@saturn.sdsu.edu.
Reference: [20] <author> Segal MR. </author> <title> Regression trees for censored data. </title> <booktitle> Biometrics 1988; 44: </booktitle> <pages> 35-47. </pages>
Reference-contexts: One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences <ref> [1, 20, 21, 22, 24, 23] </ref>. Gordon and Olshen [15] presented the first extension of CART to censored survival data, which involved a distance measure (the Wasserstein metric) between Kaplan-Meier curves and certain point masses. Their approach amounts to assuming a piecewise exponential model with one data-determined knot. <p> Zhang [19] uses a within-node homogeneity measure that is based on the idea that a homogeneous node should consist of subjects whose observed failure times are close and who are mostly censored or mostly uncensored. Segal <ref> [20] </ref> argued that tests for between-node separation can tell more about the important prognostic factors associated with the survival phenomenon under study than within-node homogeneity. He introduced a totally nonparametric tree algorithm, basing the partitioning on between-node separation using the Harrington-Fleming [25] class of two sample rank statistics. <p> He introduced a totally nonparametric tree algorithm, basing the partitioning on between-node separation using the Harrington-Fleming [25] class of two sample rank statistics. Pruning based on between-node separation, as discussed in <ref> [1, 20, 23] </ref>, is conceptually harder. 2.2 Review of CART The Classification and Regression Trees (CART) method of Breiman et al.[3] addresses the classification and regression problem by building a binary decision tree according to some splitting rule based on the covariates. <p> In regression the error denoted by R (T ) is the average squared error in the terminal nodes. Tree selection can also be done in an exploratory manner by examining trees in sequence. 2.3 Survival Trees Based on Between-Node Separation Segal <ref> [20] </ref>, Intrator [1] and LeBlanc and Crowley [23] use as the prediction rule the Kaplan-Meier estimate of the survival distribution, and as the splitting rule a test for measuring differences between distributions adapted to censored data such as the log-rank test or Wilcoxon test, and more generally the G ;fl class <p> The quality m (t) measures how much the risk decreases if we use the best possible split on node t, relative to not splitting node t at all. Segal <ref> [20] </ref> used a similar measure of quality m (t), the maximal chi-square statistic of the branch T t , but he did so without weighting by the probability of the node. <p> Of the 2404 cases, 1116 were uncensored and 1288 were censored. There were no missing values for any of the covariates. Survival Trees analysis In Sections 2.3 and 2.4 we briefly described a number of survival tree methods <ref> [1, 15, 20, 17, 18, 23] </ref>. In the examples section we mainly present the results from Intrator's method [1], and briefly describe results from Davis and Anderson's method [17] and LeBlanc and Crowley's method [23]. 9 Variables analyzed in survival trees are always reduced to dichotomies. <p> HARE and HEFT are available from statlib (statlib@stat.cmu.edu). Intrator's program [1] is available by sending email to msorna@olive.huji.ac.il. Davis' program [17] is available by sending email directly to rdaids@sdac.harvard.edu, LeBlanc and Crowley's program [23] is available by sending email to mikel@orca.fhcrc.org, Segal's program <ref> [20] </ref> is vailable by sending email to mark@segal.ucsf.edu, Zhang's program [19] is available by sending email to heping@peace.med.yale.edu, and Gordon and Olshen's program [15] is available solely for academic use by sending email to dstein@saturn.sdsu.edu.
Reference: [21] <author> Segal MR, Bloch DA. </author> <title> A comparison of estimated proportional hazards models and regression trees. </title> <booktitle> Statistics in Medicine 1989; 8: </booktitle> <pages> 539-50. </pages>
Reference-contexts: One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences <ref> [1, 20, 21, 22, 24, 23] </ref>. Gordon and Olshen [15] presented the first extension of CART to censored survival data, which involved a distance measure (the Wasserstein metric) between Kaplan-Meier curves and certain point masses. Their approach amounts to assuming a piecewise exponential model with one data-determined knot.
Reference: [22] <author> Butler JH, Gilpin E, Gordon L, Olshen RA. </author> <title> Tree structured survival analysis, II. </title> <type> Technical Report, </type> <institution> Stanford University. </institution> <year> 1991. </year>
Reference-contexts: One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences <ref> [1, 20, 21, 22, 24, 23] </ref>. Gordon and Olshen [15] presented the first extension of CART to censored survival data, which involved a distance measure (the Wasserstein metric) between Kaplan-Meier curves and certain point masses. Their approach amounts to assuming a piecewise exponential model with one data-determined knot.
Reference: [23] <author> LeBlanc M, Crowley J. </author> <title> A review of tree-based prognostic models. In in New Advances in the Design and Analysis of Clinical Trials Data. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences <ref> [1, 20, 21, 22, 24, 23] </ref>. Gordon and Olshen [15] presented the first extension of CART to censored survival data, which involved a distance measure (the Wasserstein metric) between Kaplan-Meier curves and certain point masses. Their approach amounts to assuming a piecewise exponential model with one data-determined knot. <p> He introduced a totally nonparametric tree algorithm, basing the partitioning on between-node separation using the Harrington-Fleming [25] class of two sample rank statistics. Pruning based on between-node separation, as discussed in <ref> [1, 20, 23] </ref>, is conceptually harder. 2.2 Review of CART The Classification and Regression Trees (CART) method of Breiman et al.[3] addresses the classification and regression problem by building a binary decision tree according to some splitting rule based on the covariates. <p> In regression the error denoted by R (T ) is the average squared error in the terminal nodes. Tree selection can also be done in an exploratory manner by examining trees in sequence. 2.3 Survival Trees Based on Between-Node Separation Segal [20], Intrator [1] and LeBlanc and Crowley <ref> [23] </ref> use as the prediction rule the Kaplan-Meier estimate of the survival distribution, and as the splitting rule a test for measuring differences between distributions adapted to censored data such as the log-rank test or Wilcoxon test, and more generally the G ;fl class of rank statistics [26, 27]. <p> Estimation of goodness-of-prediction may also be done using a cross-validation scheme. Another approach is to validate the complexity parameters. The idea here is to test the credibility of the pruning and not the goodness-of-prediction directly. This approach is currently being investigated by Intrator. Alternatively, LeBlanc and Crowley <ref> [23] </ref> measure the quality of a branch T t as the sum of the log-rank test statistic values (chi-square values) of the decision nodes of the branch, denoted by G (T t ). <p> Of the 2404 cases, 1116 were uncensored and 1288 were censored. There were no missing values for any of the covariates. Survival Trees analysis In Sections 2.3 and 2.4 we briefly described a number of survival tree methods <ref> [1, 15, 20, 17, 18, 23] </ref>. In the examples section we mainly present the results from Intrator's method [1], and briefly describe results from Davis and Anderson's method [17] and LeBlanc and Crowley's method [23]. 9 Variables analyzed in survival trees are always reduced to dichotomies. <p> In the examples section we mainly present the results from Intrator's method [1], and briefly describe results from Davis and Anderson's method [17] and LeBlanc and Crowley's method <ref> [23] </ref>. 9 Variables analyzed in survival trees are always reduced to dichotomies. The present version of the program developed by Intrator does not automatically test all possible break points for continuous variables, and a set of binary variables representing ranks must be provided to it. <p> Thus our analysis, combining survival trees and HEFT, enable us to identify different groups of people with similar survival experiences. The tree obtained using the method of LeBlanc and Crowley <ref> [23] </ref> was very similar to the tree we presented in Figure 1. It also first split on nodes at (nodes 6). After which the left side was split at (nodes 3) and the right side was split on ER. <p> Trees from the methods of LeBlanc and Crowley <ref> [23] </ref>, and of Davis and Anderson [17] were obtained. Leblanc and Crowley's method produced a tree in which the first split is on age, followed by several splits (in both branches) on SBP follow. <p> Details about the programs can be found in the papers where the methods were introduced. HARE and HEFT are available from statlib (statlib@stat.cmu.edu). Intrator's program [1] is available by sending email to msorna@olive.huji.ac.il. Davis' program [17] is available by sending email directly to rdaids@sdac.harvard.edu, LeBlanc and Crowley's program <ref> [23] </ref> is available by sending email to mikel@orca.fhcrc.org, Segal's program [20] is vailable by sending email to mark@segal.ucsf.edu, Zhang's program [19] is available by sending email to heping@peace.med.yale.edu, and Gordon and Olshen's program [15] is available solely for academic use by sending email to dstein@saturn.sdsu.edu.
Reference: [24] <author> Ciampi A, Thiffault J, Nakache J-P, </author> <title> Asselain B Stratification by stepwise regression, correspondence analysis and recursive partition. </title> <booktitle> Computational Statistics and Data Analysis 1986; 4: </booktitle> <pages> 185-204. </pages>
Reference-contexts: One approach uses a statistic that determines within-node homogeneity: how similar are the survival experiences of observations in a node [15, 16, 17, 18, 19]. The alternative approach is based on separation measures. The main ingredient is now a (test) statistic that distinguishes between survival experiences <ref> [1, 20, 21, 22, 24, 23] </ref>. Gordon and Olshen [15] presented the first extension of CART to censored survival data, which involved a distance measure (the Wasserstein metric) between Kaplan-Meier curves and certain point masses. Their approach amounts to assuming a piecewise exponential model with one data-determined knot.
Reference: [25] <author> Harrington DP, Fleming TR. </author> <title> A class of rank test procedures for censored survival data. </title> <booktitle> Biometrika 1982; 69: </booktitle> <pages> 553-66. 22 </pages>
Reference-contexts: Segal [20] argued that tests for between-node separation can tell more about the important prognostic factors associated with the survival phenomenon under study than within-node homogeneity. He introduced a totally nonparametric tree algorithm, basing the partitioning on between-node separation using the Harrington-Fleming <ref> [25] </ref> class of two sample rank statistics.
Reference: [26] <author> Fleming TR, Augustine GA, Elcombe SA, </author> <title> Offord KP. The SURVDIF procedure. </title> <note> SAS/SUGI Users Manual, version 5 1986. </note>
Reference-contexts: LeBlanc and Crowley [23] use as the prediction rule the Kaplan-Meier estimate of the survival distribution, and as the splitting rule a test for measuring differences between distributions adapted to censored data such as the log-rank test or Wilcoxon test, and more generally the G ;fl class of rank statistics <ref> [26, 27] </ref>. These statistics are weighted versions of the log-rank statistic, where the weights allow flexibility in emphasizing differences between two survival curves for early times (the left tail of the distribution), middle times or late times (the right tail of the distribution).
Reference: [27] <author> Fleming TR, Harrington DP, O'Sullivan M. </author> <title> Supremum versions of the log-rank and generalized Wilcoxon statistics. </title> <journal> Journal of the American Statistical Association 1987; 82: </journal> <pages> 312-20. </pages>
Reference-contexts: LeBlanc and Crowley [23] use as the prediction rule the Kaplan-Meier estimate of the survival distribution, and as the splitting rule a test for measuring differences between distributions adapted to censored data such as the log-rank test or Wilcoxon test, and more generally the G ;fl class of rank statistics <ref> [26, 27] </ref>. These statistics are weighted versions of the log-rank statistic, where the weights allow flexibility in emphasizing differences between two survival curves for early times (the left tail of the distribution), middle times or late times (the right tail of the distribution).
Reference: [28] <author> Breslow, N. </author> <title> Contribution to the discussion of a paper by D.R, Cox. </title> <journal> Journal of the Royal Statistical Society, Series B 1972; 34: </journal> <pages> 216-17. </pages>
Reference-contexts: LeBlanc and Crowley use a single step estimate of the deviance, which is based on the Breslow <ref> [28] </ref> estimate of the baseline hazard using the Nelson [29] estimate of the structural component (which is one for all individuals).
Reference: [29] <author> Nelson, W. </author> <title> On estimating the distribution of a random vector when only the coordinate is observable. </title> <booktitle> Technometrics 1969; 12, </booktitle> <pages> 923-24. </pages>
Reference-contexts: LeBlanc and Crowley use a single step estimate of the deviance, which is based on the Breslow [28] estimate of the baseline hazard using the Nelson <ref> [29] </ref> estimate of the structural component (which is one for all individuals).
Reference: [30] <author> Eubank RL. </author> <title> Spline Smoothing and Nonparametric Regression. </title> <address> New York: </address> <publisher> Marcel Dekker, </publisher> <year> 1988. </year>
Reference-contexts: The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function <ref> [30, 31, 32] </ref>. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan [36, 37], 6 Gray [38], Hastie and Tibshirani [31, 39] and Gu [40, 41].
Reference: [31] <author> Hastie TJ, Tibshirani R. </author> <title> Generalized Additive Models. </title> <publisher> London: Chapman and Hall, </publisher> <year> 1990. </year>
Reference-contexts: The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function <ref> [30, 31, 32] </ref>. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan [36, 37], 6 Gray [38], Hastie and Tibshirani [31, 39] and Gu [40, 41]. <p> The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function [30, 31, 32]. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan [36, 37], 6 Gray [38], Hastie and Tibshirani <ref> [31, 39] </ref> and Gu [40, 41]. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients. <p> However, in a proposed commercial implementation HARE should be able to deal with missing values in a way that is similar to the function no.gam.replace () in S [50], and it will have an importance measure for the covariates based on an ANOVA decomposition <ref> [31] </ref>. There is a similarity between the procedure by which variables are selected through the splitting and pruning algorithm of survival trees and the stepwise addition and deletion algorithm of HARE. But there are also major differences between survival trees and HARE.
Reference: [32] <author> Wahba G. </author> <title> Spline Models for Observational Data. </title> <address> Philadelphia: </address> <publisher> SIAM, </publisher> <year> 1990. </year>
Reference-contexts: The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function <ref> [30, 31, 32] </ref>. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan [36, 37], 6 Gray [38], Hastie and Tibshirani [31, 39] and Gu [40, 41].
Reference: [33] <author> Anderson JA, Senthilselvan A. </author> <title> Smooth estimates for the hazard function. </title> <journal> Journal of the Royal Statistical Society Series B 1980; 42: </journal> <pages> 322-27. </pages>
Reference-contexts: The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function [30, 31, 32]. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan <ref> [33] </ref>, Whittemore and Keller [34], Senthilselvan [35], O'Sullivan [36, 37], 6 Gray [38], Hastie and Tibshirani [31, 39] and Gu [40, 41]. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients.
Reference: [34] <author> Whittemore AS, Keller JB. </author> <title> Survival estimation using splines. </title> <booktitle> Biometrics 1986; 42: </booktitle> <pages> 495-506. </pages>
Reference-contexts: The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function [30, 31, 32]. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller <ref> [34] </ref>, Senthilselvan [35], O'Sullivan [36, 37], 6 Gray [38], Hastie and Tibshirani [31, 39] and Gu [40, 41]. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients.
Reference: [35] <author> Senthilselvan A. </author> <title> Penalized likelihood estimation of hazard and intensity functions. </title> <journal> Journal of the Royal Statistical Society Series B 1987; 49: </journal> <pages> 170-74. </pages>
Reference-contexts: The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function [30, 31, 32]. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan <ref> [35] </ref>, O'Sullivan [36, 37], 6 Gray [38], Hastie and Tibshirani [31, 39] and Gu [40, 41]. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients.
Reference: [36] <author> O'Sullivan F. </author> <title> Nonparametric estimation of relative risk using splines and cross-validation. </title> <journal> SIAM Journal on Scientific and Statistical Computing 1988; 9: </journal> <pages> 531-42. </pages>
Reference-contexts: The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function [30, 31, 32]. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan <ref> [36, 37] </ref>, 6 Gray [38], Hastie and Tibshirani [31, 39] and Gu [40, 41]. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients.
Reference: [37] <author> O'Sullivan F. </author> <title> Fast computation of fully automated log-density and log-hazard estimators. </title> <journal> SIAM Journal of Scientific and Statistical Computing 1988; 9: </journal> <pages> 363-79. </pages>
Reference-contexts: The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function [30, 31, 32]. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan <ref> [36, 37] </ref>, 6 Gray [38], Hastie and Tibshirani [31, 39] and Gu [40, 41]. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients.
Reference: [38] <author> Gray RJ. </author> <title> Flexible methods for analyzing survival data using splines, with applications to breast cancer prognosis. </title> <journal> Journal of the American Statistical Association 1992; 87: </journal> <pages> 942-51. </pages>
Reference-contexts: The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function [30, 31, 32]. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan [36, 37], 6 Gray <ref> [38] </ref>, Hastie and Tibshirani [31, 39] and Gu [40, 41]. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients. <p> In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan [36, 37], 6 Gray <ref> [38] </ref>, Hastie and Tibshirani [31, 39] and Gu [40, 41]. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients. Gu [41] models the complete log-hazard function, though the computational demands seem too formidable to be applicable in situations with many covariates such as our examples in Section 4. To make the computations more feasible, Gray [38] uses a B-spline approximation to <p> Gray <ref> [38] </ref> and Hastie and Tibshirani [39] use time-varying coefficients. Gu [41] models the complete log-hazard function, though the computational demands seem too formidable to be applicable in situations with many covariates such as our examples in Section 4. To make the computations more feasible, Gray [38] uses a B-spline approximation to the smoothing spline problem. In the polynomial spline approach, an unknown function is modeled in a linear space. Stepwise algorithms make it possible to determine this space more adaptively than in the smoothing spline approach, which involves only a few smoothing parameters. <p> It has been analyzed in Gray <ref> [38] </ref> using a hybrid of penalized likelihood and polynomial splines (see Section 3) and in KST using HARE (see Section 3.1). In this subsection we present a survival tree analysis, followed by a summary and extension of the HARE analysis. <p> Note that the model in Table 2 is not a proportional hazards model because of the presence of the basis functions ((0:514 ^q 0 (t)) + fi ER) and ((0:194 ^q 0 (t)) + fi size). Gray <ref> [38] </ref> noted nonproportionality with respect to ER using time-varying coefficients. In his analysis, he felt that a proportional hazards model with respect to size was appropriate. We investigate this further below. <p> Secondly, it is very convenient to fit and compare linear proportional hazards models, additive proportional hazards models, proportional hazards models with time-varying coefficients, and nonparametric proportional hazards models. As mentioned above, in the analysis of Gray <ref> [38] </ref> the effect of size was modeled proportionally, while in the HARE analysis an interaction between time and size ended up in the model. To investigate this further, we applied the HARE algorithm forcing an additive model for the log-hazard function.
Reference: [39] <author> Hastie TJ, Tibshirani R. </author> <title> Varying-coefficient models (with discussion). </title> <journal> Journal of the Royal Statistical Society Series B 1993; 55: </journal> <pages> 757-800. </pages>
Reference-contexts: The smoothing spline solution to a function estimation problem is typically the maximizer of a penalized likelihood function [30, 31, 32]. In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan [36, 37], 6 Gray [38], Hastie and Tibshirani <ref> [31, 39] </ref> and Gu [40, 41]. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients. <p> Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani <ref> [39] </ref> use time-varying coefficients. Gu [41] models the complete log-hazard function, though the computational demands seem too formidable to be applicable in situations with many covariates such as our examples in Section 4. To make the computations more feasible, Gray [38] uses a B-spline approximation to the smoothing spline problem.
Reference: [40] <author> Gu C. </author> <title> Penalized likelihood hazard estimation. </title> <type> Technical Report No. 91-58, </type> <institution> Dept. of Statistics, Purdue University 1991. </institution>
Reference-contexts: In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan [36, 37], 6 Gray [38], Hastie and Tibshirani [31, 39] and Gu <ref> [40, 41] </ref>. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients.
Reference: [41] <author> Gu C. </author> <title> Structural multivariate function estimation: some automatic density and hazard estimates. </title> <type> Manuscript 1994. </type>
Reference-contexts: In survival analysis, smoothing splines have been used by Anderson and Senthilselvan [33], Whittemore and Keller [34], Senthilselvan [35], O'Sullivan [36, 37], 6 Gray [38], Hastie and Tibshirani [31, 39] and Gu <ref> [40, 41] </ref>. Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients. <p> Most of these papers use splines within the framework of the proportional hazards model. Gray [38] and Hastie and Tibshirani [39] use time-varying coefficients. Gu <ref> [41] </ref> models the complete log-hazard function, though the computational demands seem too formidable to be applicable in situations with many covariates such as our examples in Section 4. To make the computations more feasible, Gray [38] uses a B-spline approximation to the smoothing spline problem.
Reference: [42] <author> Smith PL. </author> <title> Curve fitting and modeling with splines using statistical variable selection methods. </title> <type> NASA Report 166034, </type> <institution> NASA, Langley Research Center, Hampla, VA, </institution> <year> 1982. </year>
Reference-contexts: Stepwise algorithms make it possible to determine this space more adaptively than in the smoothing spline approach, which involves only a few smoothing parameters. Adaptive algorithms for polynomial splines were first introduced in a regression context <ref> [42] </ref>. Other applications include multiple regression (MARS [4]), density estimation (LOGSPLINE [5]), and spectral distribution estimation (LSPEC [6]).
Reference: [43] <author> Etezadi-Amoli J, Ciampi A. </author> <title> Extended hazard regression for censored survival data with covari-ates: A spline approximation for the baseline hazard function. </title> <booktitle> Biometrics 1987; 43: </booktitle> <pages> 181-92. </pages>
Reference-contexts: Adaptive algorithms for polynomial splines were first introduced in a regression context [42]. Other applications include multiple regression (MARS [4]), density estimation (LOGSPLINE [5]), and spectral distribution estimation (LSPEC [6]). In the context of survival analysis, Etezadi-Amoli and Ciampi <ref> [43] </ref>, Efron [44] and Abrahamo-wicz, Ciampi and Ramsay [45] use polynomial splines to model either the unconditional distribution of the survival times or the baseline hazard function within a proportional hazards model.
Reference: [44] <author> Efron B. </author> <title> Logistic regression, survival analysis and the Kaplan-Meier curve. </title> <journal> Journal of American Statistical Association 1988; 83: </journal> <pages> 414-25. </pages>
Reference-contexts: Adaptive algorithms for polynomial splines were first introduced in a regression context [42]. Other applications include multiple regression (MARS [4]), density estimation (LOGSPLINE [5]), and spectral distribution estimation (LSPEC [6]). In the context of survival analysis, Etezadi-Amoli and Ciampi [43], Efron <ref> [44] </ref> and Abrahamo-wicz, Ciampi and Ramsay [45] use polynomial splines to model either the unconditional distribution of the survival times or the baseline hazard function within a proportional hazards model. Kooper-berg, Stone and Truong [2] develop hazard regression (HARE), in which the conditional log-hazard function is modeled using polynomial splines.
Reference: [45] <author> Abrahamowicz M, Ciampi A, Ramsay JO. </author> <title> Nonparametric density estimation for censored survival data: Regression-spline approach. </title> <journal> The Canadian Journal of Statistics 1991; 20: </journal> <pages> 171-85. </pages>
Reference-contexts: Adaptive algorithms for polynomial splines were first introduced in a regression context [42]. Other applications include multiple regression (MARS [4]), density estimation (LOGSPLINE [5]), and spectral distribution estimation (LSPEC [6]). In the context of survival analysis, Etezadi-Amoli and Ciampi [43], Efron [44] and Abrahamo-wicz, Ciampi and Ramsay <ref> [45] </ref> use polynomial splines to model either the unconditional distribution of the survival times or the baseline hazard function within a proportional hazards model. Kooper-berg, Stone and Truong [2] develop hazard regression (HARE), in which the conditional log-hazard function is modeled using polynomial splines.
Reference: [46] <author> Kooperberg C. </author> <title> Hazard regression for interval censored data. </title> <booktitle> In: The Proceedings of the International Biometrics Conference, </booktitle> <address> Hamilton Ontario, </address> <year> 1994; </year> <month> 81-96. 23 </month>
Reference-contexts: Kooper-berg, Stone and Truong [2] develop hazard regression (HARE), in which the conditional log-hazard function is modeled using polynomial splines. Kooperberg <ref> [46] </ref> extends the HARE methodology to handle interval-censored data. <p> In Kooperberg <ref> [46] </ref>, where HARE was applied to some datasets with large amounts of interval censoring, linear proportional hazards models were obtained. There it was hypothesized that this was because of the `lack of signal' due to the large amount of censoring.
Reference: [47] <author> Kooperberg C, Stone CJ, Truong YK. </author> <title> The L 2 rate of convergence of hazard regression. </title> <note> Scan--dinavian Journal of Statistics 1995; to appear. </note>
Reference-contexts: Kooper-berg, Stone and Truong [2] develop hazard regression (HARE), in which the conditional log-hazard function is modeled using polynomial splines. Kooperberg [46] extends the HARE methodology to handle interval-censored data. Under suitable conditions Kooperberg, Stone and Truong <ref> [47] </ref> obtain the L 2 rate of convergence for a nonadaptive version of HARE. 3.2 HARE In hazard regression (HARE, Kooperberg, Stone and Truong [2], hereafter referred to as KST), polynomial splines are used to estimate the conditional log-hazard function based on possibly censored survival data and one or more covariates.
Reference: [48] <author> Miller RG. </author> <title> Survival Analysis. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1981. </year>
Reference-contexts: Note that the partial likelihood corresponding to Y i = y i , ffi i , x i and fi equals [f (y i jx i ; fi)] ffi i [1 F (y i jx i ; fi)] 1ffi i (page 16 of Miller <ref> [48] </ref>), so the log-likelihood is given by (y i ; ffi i jx i ; fi) = ffi i ff (y i jx i ; fi) 0 The log-likelihood function corresponding to the observed data is given by `(fi) = i It is straightforward to compute the corresponding score S (fi)
Reference: [49] <author> Rao CR. </author> <title> Linear Statistical Inference and Its Applications, second edition. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: Since it is computationally too time consuming to evaluate each candidate for a new basis function by recomputing the maximum likelihood fit, we choose among the various candidates by a heuristic search that is designed approximately to maximize the absolute value of the corresponding Rao statistic <ref> [49] </ref>.
Reference: [50] <author> Chambers JM, Hastie TJ. </author> <title> Statistical Models in S. </title> <address> Pacific Grove, California: </address> <publisher> Wadsworth, </publisher> <year> 1992. </year>
Reference-contexts: This is similar to what is sometimes done for generalized linear models; see, for example, the function step.glm in S and the discussion in Chambers and Hastie <ref> [50] </ref> (page 235). (After the new basis function has been added to the model, it is refitted using maximum likelihood and the Newton-Raphson algorithm.) As mentioned earlier, the candidate basis functions of G are piecewise linear functions (splines) in the covariates, piecewise linear functions in t, and tensor products of two <p> We select the model corresponding to the value ^- of that minimizes BIC - = AIC log n;- . A program for implementing HARE has been written in C, and an interface based on the statistical package S <ref> [50, 51] </ref> has also been developed. For a more detailed discussion of the HARE procedure and its interface, see KST. <p> However, in a proposed commercial implementation HARE should be able to deal with missing values in a way that is similar to the function no.gam.replace () in S <ref> [50] </ref>, and it will have an importance measure for the covariates based on an ANOVA decomposition [31]. There is a similarity between the procedure by which variables are selected through the splitting and pruning algorithm of survival trees and the stepwise addition and deletion algorithm of HARE.
Reference: [51] <author> Becker RA, Chambers JM, Wilks AR. </author> <title> The New S Language. </title> <address> Pacific Grove, California: </address> <publisher> Wadsworth, </publisher> <year> 1988. </year>
Reference-contexts: We select the model corresponding to the value ^- of that minimizes BIC - = AIC log n;- . A program for implementing HARE has been written in C, and an interface based on the statistical package S <ref> [50, 51] </ref> has also been developed. For a more detailed discussion of the HARE procedure and its interface, see KST.
Reference: [52] <author> Lindley DV. </author> <title> Contribution to the discussion of Regression models and life tables. </title> <journal> Journal of the Royal Statistical Society Series B 1972; 34: </journal> <pages> 208-209. </pages>
Reference-contexts: However, the structure is much smaller. 11 HARE analysis Before applying HARE it is often advantageous to use (unconditional) hazard estimation to transform time so that the transformed unconditional hazard function will be approximately equal to one <ref> [52] </ref>. The main advantage of such a transformation is that because of the piecewise linear nature of HARE, the (baseline) hazard functions may have big jumps in the first derivative.
Reference: [53] <author> Rosenman RH, Friedman M, Strauss R, Wurm M, Kotichek R, Hahn W, Werthessen, </author> <title> NT. A predictive study of coronary heart disease: The Western Collaborative Group Study. </title> <journal> Journal of the American Medical Association 1964; 189: </journal> <pages> 15-22. </pages>
Reference: [54] <author> Rosenman RH, Brand RJ, Sholtz RI, Friedman M. </author> <title> Multivariate prediction of coronary heart disease during 8.5 year follow-up in the Western Collaborative Group Study. </title> <journal> American Journal of Cardiology 1976; 37: </journal> <pages> 903-10. </pages>
Reference: [55] <author> Ragland DR, </author> <title> Brand RJ. Coronary heart disease mortality in the Western Collaborative Group Study: follow-up experience of 22 years. </title> <journal> American Journal of Epidemiology 1988; 127: </journal> <pages> 462-75. </pages>
Reference-contexts: Since we are only concerned with CHD here, 2740 cases (86.8%) are censored. The study is described in Rosenman et al.[53], Rosenman et al.[54] and Ragland and Brand <ref> [55] </ref>.
Reference: [56] <author> Cox DR, Oakes D. </author> <title> Survival Analysis. </title> <publisher> London: Chapman and Hall, </publisher> <year> 1983. </year> <month> 24 </month>
Reference-contexts: In both HEFT estimates about 81% of the probability mass is beyond 28 years. Since the Western Collaborative Group Study is ongoing, the censoring is essentially Type I <ref> [56] </ref>. In particular, we note that of the 2121 men that have not died, 2091 (98.6%) have been in the study for over 20 years, while of the 415 men that died of CHD 229 (55.2%) died before they had been in the study for 20 years.
References-found: 56

