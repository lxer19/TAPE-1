URL: http://www.cs.nmsu.edu/~jcook/papers/djournal.ps.gz
Refering-URL: http://www.cs.nmsu.edu/~jcook/papers/
Root-URL: http://www.cs.nmsu.edu
Email: fjcook,alwg@cs.colorado.edu  
Title: Discovering Models of Software Processes from Event-Based Data  
Author: Jonathan E. Cook and Alexander L. Wolf 
Address: Boulder, CO 80309 USA  
Affiliation: Software Engineering Research Laboratory Department of Computer Science University of Colorado  
Date: November 1, 1996 13:53  
Note: DRAFT:  This work was supported in part by the National Science Foundation under grant CCR-93-02739 and the Air Force Material Command, Rome Laboratory, and the Advanced Research Projects Agency under Contract Number F30602-94-C-0253. The content of the information does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred.  
Abstract: University of Colorado Department of Computer Science Technical Report CU-CS-8??-96 November 1996 Many software process methods and tools presuppose the existence of a formal model of a process. Unfortunately, developing a formal model for an on-going, complex process can be difficult, costly, and error prone. This presents a practical barrier to the adoption of process technologies. The barrier would be lowered by automating the creation of formal models. We have developed techniques that can use basic event data captured from an on-going process to generate a formal model of process behavior. We term this kind of data analysis process discovery. This paper describes three methods for process discovery that we have developed, implemented, and applied in an industrial case study. These methods span the range from purely algorithmic, to algorithmic and statistical, to purely statistical (neural net). We show that not only is process discovery possible, it is practical and effective in real-world situations. c fl 1996 Jonathan E. Cook and Alexander L. Wolf
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of the ACM-SIGMOD 1993 International Conference on Managment of Data, </booktitle> <pages> pages 207-216. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: The techniques we developed will be a beginning for assisting the process engineers in discovering, building, and evolving process models. Other techniques that look at different types of process data will ultimately be useful as well. For example, the area of data mining (e.g., <ref> [1] </ref>) may have useful techniques for discovering software product relationships. 4 Process ModelEvent DataExecuting Process Collection Inference Event 1 Event 2 Event 3 Event 76 Event 77 Event 78 4 Problem Statement and Approach Our goals in this work are to take a trace of the process execution, in the form
Reference: [2] <author> H. Ahonen, K. Mannila, and E. Nikunen. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 153-167. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This machine is then merged according to some rules until a final state machine is output as the learned language. Examples are: * Ahonen et al. <ref> [2] </ref> describe a prefix tree method where states are merged based on previous contexts.
Reference: [3] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference-contexts: The survey by Angluin and Smith [6] is a broad look at inductive inference learning. Pitt's survey [46] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in <ref> [3, 35, 40, 41, 48] </ref>. Angluin and Smith, in their survey, acknowledge the gap between the theoretical results that have been achieved and the practical application of inference methods.
Reference: [4] <author> D. Angluin. </author> <title> Inference of reversible languages. </title> <journal> Journal of the ACM, </journal> <volume> 29(3) </volume> <pages> 741-765, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Examples are: * Ahonen et al. [2] describe a prefix tree method where states are merged based on previous contexts. That is, if two states are entered from the same k-length contexts, then they are merged. 8 * Angluin <ref> [4] </ref> merges states of a prefix tree based on a notion of k-reversibility, which restricts the class of languages her algorithm infers.
Reference: [5] <author> D. Angluin. </author> <title> Learning regular sets from queries and counter-examples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: Results for grammars, including DFAs, are more negative, but less definite. As Pitt [46] reports, If DFAs are polynomially approximately predictable, then there is a probabilistic polynomial time algorithm for inverting the RSA encryption function, for factoring Blum integers, and for deciding quadratic residues. Angluin <ref> [5] </ref> phrases the learning problem in terms of an oracle.
Reference: [6] <author> D. Angluin and C.H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> ACM Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: To develop our initial set of methods, we have cast the process discovery problem in terms of another, previously investigated FSM discovery problem. That problem is the discovery of a grammar for a regular language given example sentences in that language <ref> [6] </ref>. This area of research historically uses the term grammar inference for this problem. If one interprets events as tokens and event streams as sentences in the language of the process, then a natural fit between these areas is seen. <p> We call the sequence s in hs; li positive if l = 1 (i.e., it is in L) and negative otherwise. L is defined 1 This presentation is consistent with others in the field <ref> [6, 46, 49] </ref>. 2 L = fl is not a very interesting language. 6 as the possibly infinite set of all positive sequences, and L is defined as the possibly infinite set of all negative sequences. For learning purposes, an algorithm needs some presentation of data. <p> With this framework, Angluin gave a polynomial-time algorithm for learning DFAs, and others have extended this to other classes of languages. There is much work describing the computational complexities of various learning paradigms and classes of languages or formulas. The survey by Angluin and Smith <ref> [6] </ref> is a broad look at inductive inference learning. Pitt's survey [46] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in [3, 35, 40, 41, 48].
Reference: [7] <author> G.S. Avrunin, U.A. Buy, J.C. Corbett, L.K. Dillon, and J.C. Wileden. </author> <title> Automated analysis of concurrent systems with the constrained expression toolset. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(11) </volume> <pages> 1204-1222, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Using event data to characterize behavior is widely accepted in other areas of software engi 3 neering, such as program visualization [42], concurrent-system analysis <ref> [7] </ref>, and distributed debug-ging [10, 24]. We feel it is applicable to software process as well. One can visualize a process execution as completely represented by an event stream containing events from all of the possible event types that might be generated during that process.
Reference: [8] <author> S. Bandinelli, A. Fuggetta, and C. Ghezzi. </author> <title> Software Process Model Evolution in the SPADE Environe-ment. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1128-1144, December 1993. 
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [9, 26, 45, 52], process analysis [32, 33, 37, 47], and process evolution <ref> [8, 34] </ref>, assume the existence of some sort of formal model of a process in order for those technologies to be applied. The need to develop a formal model as a prerequisite to using a new technology is a daunting prospect to the managers of large, on-going projects.
Reference: [9] <author> N.S. Barghouti and G.E. Kaiser. </author> <title> Scaling Up Rule-based Development Environments. </title> <booktitle> In Proceedings of the Third European Software Engineering Conference, number 550 in Lecture Notes in Computer Science, </booktitle> <pages> pages 380-395. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The issues of managing and improving the process of developing and maintaining software have come to the forefront of software engineering research. In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [9, 26, 45, 52] </ref>, process analysis [32, 33, 37, 47], and process evolution [8, 34], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [10] <author> P. Bates. </author> <title> Debugging heterogenous systems using event-based models of behavior. </title> <booktitle> In Proceedings of a Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 11-22. </pages> <publisher> ACM Press, </publisher> <month> January </month> <year> 1989. </year>
Reference-contexts: Using event data to characterize behavior is widely accepted in other areas of software engi 3 neering, such as program visualization [42], concurrent-system analysis [7], and distributed debug-ging <ref> [10, 24] </ref>. We feel it is applicable to software process as well. One can visualize a process execution as completely represented by an event stream containing events from all of the possible event types that might be generated during that process.
Reference: [11] <author> A.W. Biermann and J.A. Feldman. </author> <title> On the Synthesis of Finite State Machines from Samples of Their Behavior. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 21(6) </volume> <pages> 592-597, </pages> <month> June </month> <year> 1972. </year>
Reference-contexts: While 0-reversible languages can be inferred in near-linear time, higher k values cause the algorithm to be cubic in the length of the input. * Bierman and Feldman <ref> [11] </ref> describe a prefix tree method where states are merged based on k-length future behaviors. <p> The Ktail method is a purely algorithmic approach that looks at the future behavior to compute a possible current state. We modified the theoretical description given in <ref> [11] </ref> to make it less dependent on multiple sequences, and extended it to allow for handling noisy data. In addition, we added some post-analysis steps that remove some common overly complex constructs that the basic algorithm tends to leave in a discovered model. Our implementation was from scratch. 3. <p> The plethora of tuning parameters and the lack of guidelines in setting them are a drawback as well. 6.2 Ktail The next method is purely algorithmic and based on work by Biermann and Feldman <ref> [11] </ref>. Their original presentation was formulated in terms of sample strings and output values for the FSM at the end of the strings. Our formulation of this algorithm does not make use of the output values and is thus presented as just operating on the sample strings themselves.
Reference: [12] <author> A. Brazma. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 260-271. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: In this sense, these algorithms make a pass through a sequence, find repetitive patterns, replace all occurrences with the regular expression, then start all over until it is decided that they are done. Two examples are: * The work of Brazma et al. <ref> [13, 12, 14] </ref> is most representative of this work. They have been able to construct fast algorithms that learn restricted regular expressions. The restrictions they place on the inferred regular expressions are that both the or operator and the *-nesting are limited and fixed.
Reference: [13] <author> A. Brazma and K. Cerans. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 76-90. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: In this sense, these algorithms make a pass through a sequence, find repetitive patterns, replace all occurrences with the regular expression, then start all over until it is decided that they are done. Two examples are: * The work of Brazma et al. <ref> [13, 12, 14] </ref> is most representative of this work. They have been able to construct fast algorithms that learn restricted regular expressions. The restrictions they place on the inferred regular expressions are that both the or operator and the *-nesting are limited and fixed.
Reference: [14] <author> A. Brazma, I. Jonassen, I. Eidhammer, and D. Gilbert. </author> <title> Approaches to the automatic discovery of patterns in biosequences. </title> <type> Technical Report TCU/CS/1995/18, </type> <institution> City University (London), </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: In this sense, these algorithms make a pass through a sequence, find repetitive patterns, replace all occurrences with the regular expression, then start all over until it is decided that they are done. Two examples are: * The work of Brazma et al. <ref> [13, 12, 14] </ref> is most representative of this work. They have been able to construct fast algorithms that learn restricted regular expressions. The restrictions they place on the inferred regular expressions are that both the or operator and the *-nesting are limited and fixed.
Reference: [15] <author> R.C. Carrasco and J. Oncina. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 139-152. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Although their method is not directly described as prefix tree merging, it does fall in this class. * Carrasco and Oncina <ref> [15] </ref> describe a statistical method for learning stochastic regular languages, based on state merging from a prefix tree, where states are merged based on statistical likelihood calculations.
Reference: [16] <author> J. Carrol and D. </author> <title> Long. Theory of Finite Automata. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: While FSMs may be somewhat deficient for prescribing software processes, they are quite convenient and sufficiently powerful for describing historical patterns of actual behavior. In this paper we do not present a formal description of finite state machines. A good reference for this background is Carrol and Long <ref> [16] </ref>. We use the term FSM to refer to a nondeterministic, transition-labeled state machine.
Reference: [17] <author> F. Casacuberta. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 119-129. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year> <month> 39 </month>
Reference-contexts: * Sanchez and Benedi [50] use a specific inference mechanism (that only finds loop-free automata) to provide a model structure and initial transition probabilities to a forward-backward parameter estimation algorithm. * Chen [20] describes a corpus-based SCFG inference method using bayesian probabilities and compares it to ngram models. * Casacuberta <ref> [17] </ref> provides a transformation algorithm that takes a CFG and translates it to a CFG in Chomsky Normal Form, which is required for parameter estimation using the 10 inside-outside algorithm. 5.5 Inference Using Neural Nets The neural network community has also looked at the problem of grammar inference from positive samples.
Reference: [18] <author> A. Castellanos, I. Galiano, and E. Vidal. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 93-105. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Running time is maximally n 3 ; however they claim actual times are near-linear. * Castellanos et al. <ref> [18] </ref> apply a method of this class to learning natural language translators. Another class of techniques is directed more towards iteratively building up a regular expression from the sequences, and then translating that into an FSM (if necessary).
Reference: [19] <author> E. Charniak. </author> <title> Statistical Language Learning. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: The algorithms mentioned above for parameter estimation on both the HMMs and SCFGs are types of expectation maximization algorithms. All of them are gradient descent techniques that find a local maximum. Hence, initializations are important for them. A good reference on these algorithms is Charniak's book <ref> [19] </ref>. To solve the structure inference problem for statistical models, the standard algorithmic techniques for inferring an FSM or CFG can be applied to finding a structure, and then the well-known algorithms can be used to assign the correct probabilities to the transitions or production rules.
Reference: [20] <author> S.F. Chen. </author> <title> Bayesian grammar induction for language modeling. </title> <type> Technical Report TR-01-95, </type> <institution> Harvard University, Center for Research in Computing Technology, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: His results show a good improvement over parameter estimation beginning with a fully connected model. * Sanchez and Benedi [50] use a specific inference mechanism (that only finds loop-free automata) to provide a model structure and initial transition probabilities to a forward-backward parameter estimation algorithm. * Chen <ref> [20] </ref> describes a corpus-based SCFG inference method using bayesian probabilities and compares it to ngram models. * Casacuberta [17] provides a transformation algorithm that takes a CFG and translates it to a CFG in Chomsky Normal Form, which is required for parameter estimation using the 10 inside-outside algorithm. 5.5 Inference Using
Reference: [21] <author> J.E. Cook and A.L. Wolf. Balboa: </author> <title> A framework for event-based process data analysis. </title> <type> Technical Report CU-CS-8??-96, </type> <institution> Department of Computer Science, University of Colorado, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: RNet was adapted from an implementation by Das [25], while the others were implemented from scratch. Additionally, a common user interface, other common code (startup and graph operations), and an interface to the Balboa framework (see <ref> [21] </ref>) are all implemented. Table 2 shows the size (LOC) of the various discovery tools. The discovery methods output their discovered FSM in a format compatible with the dot graph layout program [39].
Reference: [22] <author> J.E. Cook and A.L. Wolf. </author> <title> Process discovery and validation through event-data analysis. </title> <type> Technical Report CU-CS-817-96, </type> <institution> Department of Computer Science, University of Colorado, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: This is one reason why we do not view process discovery as being able to discover a complete and completely correct model. For a more detailed presentation of event data, systems that can collect event data, and using event data for analysis purposes, please see <ref> [22] </ref>. 3 Related Process Work In the software process domain, there has been little work that deals with techniques for discovering process models from current process execution behavior. Three pieces of work that are related are: 1.
Reference: [23] <author> J.E. Cook and A.L. Wolf. </author> <title> Software process validation: Quantitatively measuring the correspondence of a process to a model using event-based data. </title> <type> Technical Report CU-CS-8??-96, </type> <institution> Department of Computer Science, University of Colorado, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: The goal of the study was to statistically identify process behaviors that correlated with the acceptance or rejectance of the product. The correlation of behaviors was performed by using process validation techniques that we have developed (see <ref> [23] </ref>). This correlation was done first using an existing process model, but it was shown that this process model did not capture even half of the behavior that was being exhibited.
Reference: [24] <author> J. Cuny, G. Forman, A. Hough, J. Kundu, C. Lin, L. Snyder, and D. Stemple. </author> <title> The adriane debugger: Scalable application of event-based abstraction. </title> <booktitle> In Proceedings of the ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 85-95. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: Using event data to characterize behavior is widely accepted in other areas of software engi 3 neering, such as program visualization [42], concurrent-system analysis [7], and distributed debug-ging <ref> [10, 24] </ref>. We feel it is applicable to software process as well. One can visualize a process execution as completely represented by an event stream containing events from all of the possible event types that might be generated during that process.
Reference: [25] <author> S. Das and M.C. Mozer. </author> <title> A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction. </title> <booktitle> In Proceedings of the 1993 Conference, number 6 in Advances in Neural Information Processing Systems, </booktitle> <pages> pages 19-26. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Recent representative work in this field is <ref> [25, 55] </ref>. The various methods all consist of defining a recurrent network architecture, and then analyzing the hidden neuron activity to discover the states and transitions for the resulting grammar. The difference between these methods is how they inspect the hidden neurons to infer state information. <p> The methods can be characterized by the method in which they examine those samples. 1. The RNet method is a purely statistical (neural network) approach that looks at the past behavior to characterize a state. For this method we have extended an implementation by Das <ref> [25] </ref>. Our extensions allow this method to handle more event types (theirs was restricted to two), and enable the easier extraction of the discovered model from the net. 2. The Ktail method is a purely algorithmic approach that looks at the future behavior to compute a possible current state. <p> This recently developed method is due to Das and Mozer <ref> [25] </ref>. We refer to it here as the RNet method. In a standard feed-forward neural network, neurons are split into layers, with all the outputs of the neurons in one layer feeding forward into all the neurons of the next layer (see Figure 3a). <p> RNet was adapted from an implementation by Das <ref> [25] </ref>, while the others were implemented from scratch. Additionally, a common user interface, other common code (startup and graph operations), and an interface to the Balboa framework (see [21]) are all implemented. Table 2 shows the size (LOC) of the various discovery tools.
Reference: [26] <author> W. Deiters and V. Gruhn. </author> <title> Managing Software Processes in the Environment MELMAC. </title> <booktitle> In SIGSOFT '90: Proceedings of the Fourth Symposium on Software Development Environments, </booktitle> <pages> pages 193-205. </pages> <booktitle> ACM SIGSOFT, </booktitle> <month> December </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The issues of managing and improving the process of developing and maintaining software have come to the forefront of software engineering research. In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [9, 26, 45, 52] </ref>, process analysis [32, 33, 37, 47], and process evolution [8, 34], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [27] <author> K. El Emam, N. Moukheiber, and N.H. Madhavji. </author> <title> An evaluation of the G/Q/M method. </title> <type> Technical Report MCGILL/SE-94-11, </type> <institution> McGill University, </institution> <year> 1994. </year>
Reference-contexts: This work is more along the lines of a process post-mortem, to analyze by discussion the changes that a process should undergo for the next cycle. 3. Madhavji et al. <ref> [27] </ref> describe a method for eliciting process models from currently executing processes. The method is basically a plan for how a person would enter a development organization, understand their process, and describe the process in a formal way. There is no notion of automation or tool support in any way.
Reference: [28] <author> P.K. Garg and S. Bhansali. </author> <title> Process programming by hindsight. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 280-293. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: Three pieces of work that are related are: 1. Garg and Bhansali <ref> [28] </ref> describe a method using explanation-based learning to discover aspects and fragments of the underlying process model from process history data and rules of operations and their effects. This work centers on using a rule base and goals to derive a generalized execution flow from a specific process history.
Reference: [29] <author> P.K. Garg, M. Jazayeri, </author> <title> and M.L. Creech. A meta-process for software reuse, process discovery and evolution. </title> <booktitle> In Proceedings of the 6th International Workshop on Software Reuse, </booktitle> <pages> page [need pages], </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This work centers on using a rule base and goals to derive a generalized execution flow from a specific process history. By having enough rules, they showed that a complete and correct process fragment could be generated from execution data. 2. Garg et al. <ref> [29] </ref> employ process history analysis, mostly human-centered data validation and analysis, in the context of a meta-process for creating and validating domain specific process and software kits.
Reference: [30] <author> E.M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: Now, the grammar inference problem can be phrased as, given some presentation of all or part of fi L , can one infer a good grammar G describing L? 5.2 Complexity Results Gold's "identification in the limit" was the first framework for analyzing the grammar inference problem <ref> [30] </ref>. Identifying in the limit frames the problem in terms of looking at the complete presentation of fi L .
Reference: [31] <author> E.M. Gold. </author> <title> Complexity of automatic identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: If, after some finite presentation of pairs from fi L , the algorithm guesses G correctly, and does not change its guess as the presentation continues, then the algorithm is said to identify G in the limit. Gold showed <ref> [31] </ref> that finding a DFA with a minimum number of states with a presentation of fi L is NP-hard, and also that it is impossible with just a positive presentation ( L ).
Reference: [32] <author> R.M. Greenwood. </author> <title> Using CSP and System Dynamics as Process Engineering Tools. </title> <booktitle> In Proceedings of the Second European Workshop on Software Process Technology, number 635 in Lecture Notes in Computer Science, </booktitle> <pages> pages 138-145. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [9, 26, 45, 52], process analysis <ref> [32, 33, 37, 47] </ref>, and process evolution [8, 34], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [33] <author> V. Gruhn and R. Jegelka. </author> <title> An Evaluation of FUNSOFT Nets. </title> <booktitle> In Proceedings of the Second European Workshop on Software Process Technology, number 635 in Lecture Notes in Computer Science, </booktitle> <pages> pages 196-214. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [9, 26, 45, 52], process analysis <ref> [32, 33, 37, 47] </ref>, and process evolution [8, 34], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [34] <author> M.L. Jaccheri and R. Conradi. </author> <title> Techniques for Process Model Evolution in EPOS. </title> <journal> IEEE Transactions on Software Engineering, </journal> 19(12) 1145-1156, December 1993. <volume> 40 </volume>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [9, 26, 45, 52], process analysis [32, 33, 37, 47], and process evolution <ref> [8, 34] </ref>, assume the existence of some sort of formal model of a process in order for those technologies to be applied. The need to develop a formal model as a prerequisite to using a new technology is a daunting prospect to the managers of large, on-going projects.
Reference: [35] <author> S. Jain and A. Sharma. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intel--ligence (subseries of LNCS), </booktitle> <pages> pages 349-364. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The survey by Angluin and Smith [6] is a broad look at inductive inference learning. Pitt's survey [46] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in <ref> [3, 35, 40, 41, 48] </ref>. Angluin and Smith, in their survey, acknowledge the gap between the theoretical results that have been achieved and the practical application of inference methods.
Reference: [36] <author> L.G. Votta J.E. Cook and A.L. Wolf. </author> <title> Does following prescribed processes lead to better products? Technical Report CU-CS-8??-96, </title> <institution> Department of Computer Science, University of Colorado, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: An in-depth presentation of this study can be found in <ref> [36] </ref>; we will just highlight the use of the discovery tools here. The study focussed on a change request process for a large telecommunications software system.
Reference: [37] <author> M.I. Kellner. </author> <title> Software Process Modeling Support for Management Planning and Control. </title> <booktitle> In Proceedings of the First International Conference on the Software Process, </booktitle> <pages> pages 8-28. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [9, 26, 45, 52], process analysis <ref> [32, 33, 37, 47] </ref>, and process evolution [8, 34], assume the existence of some sort of formal model of a process in order for those technologies to be applied. <p> The example in this section is taken from the ISPW 6/7 process problem [38]. We describe the process using an FSM that is based on Kellner's Statemate solution <ref> [37] </ref>. Our version is shown in Figure 8. The idea is to see how well the methods perform at reproducing this FSM and, thereby, discovering the process. At a high level, the ISPW 6/7 process proceeds as follows.
Reference: [38] <author> M.I. Kellner, P.H. Feiler, A. Finkelstein, T. Katayama, L.J. Osterweil, M.H. Penedo, and H.D. Rombach. </author> <title> Software Process Modeling Example Problem. </title> <booktitle> In Proceedings of the 6th International Software Process Workshop, </booktitle> <pages> pages 19-29, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The example in this section is taken from the ISPW 6/7 process problem <ref> [38] </ref>. We describe the process using an FSM that is based on Kellner's Statemate solution [37]. Our version is shown in Figure 8. The idea is to see how well the methods perform at reproducing this FSM and, thereby, discovering the process.
Reference: [39] <author> E. </author> <title> Koutsofios and S.C. North. Drawing Graphs with Dot. </title> <institution> AT&T Bell Laboratories, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: What is important is that the methods produce an FSM model that reflects the structure inherent in the sample|that is, the A-B-C and B-A-C loops. In this chapter, all of the graphical representations presented have been automatically generated by the dot directed-graph drawing tool <ref> [39] </ref> from the output of the discovery tools. 12 Input Layer Hidden Layer Output Layer (a) (b) In Section 7, we demonstrate the methods on a more complex example, exposing the relative performance and possible strengths and weaknesses of each method. 6.1 RNet The first method we describe comes from the <p> Table 2 shows the size (LOC) of the various discovery tools. The discovery methods output their discovered FSM in a format compatible with the dot graph layout program <ref> [39] </ref>. This leveraging of an existing graph layout tool is central to the successful application of the discovery tools. It provides an end-to-end solution, going from event data directly to a visual display of the discovered process model. Discovery, seen in Figure 12, is the process discovery tool of Balboa.
Reference: [40] <author> S. Lange, J. Nessel, and R. Wiehagen. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 423-437. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The survey by Angluin and Smith [6] is a broad look at inductive inference learning. Pitt's survey [46] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in <ref> [3, 35, 40, 41, 48] </ref>. Angluin and Smith, in their survey, acknowledge the gap between the theoretical results that have been achieved and the practical application of inference methods.
Reference: [41] <author> S. Lange and P. Watson. </author> <title> Algorithmic Learning Theory, </title> <booktitle> volume 872 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 438-452. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The survey by Angluin and Smith [6] is a broad look at inductive inference learning. Pitt's survey [46] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in <ref> [3, 35, 40, 41, 48] </ref>. Angluin and Smith, in their survey, acknowledge the gap between the theoretical results that have been achieved and the practical application of inference methods.
Reference: [42] <author> R.J. LeBlanc and A.D. Robbins. </author> <title> Event-Driven Monitoring of Distributed Programs. </title> <booktitle> In Proceedings of the Fifth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 515-522. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1985. </year>
Reference-contexts: Using event data to characterize behavior is widely accepted in other areas of software engi 3 neering, such as program visualization <ref> [42] </ref>, concurrent-system analysis [7], and distributed debug-ging [10, 24]. We feel it is applicable to software process as well. One can visualize a process execution as completely represented by an event stream containing events from all of the possible event types that might be generated during that process.
Reference: [43] <author> L. Miclet. </author> <title> Syntactic and Structural Pattern Recognition: Theory and Applications, </title> <booktitle> volume 7 of Series in Computer Science, chapter 9: Grammatical Inference, </booktitle> <pages> pages 237-290. </pages> <publisher> World Scientific, </publisher> <address> New Jersey, </address> <year> 1990. </year>
Reference-contexts: They have extended this work to handle noisy strings by treating the noise as an edit distance problem, although currently they only allow for a single-token edit, not multi-token gaps. Their work is geared towards analyzing biosequence (DNA/RNA, protein) data. * Miclet <ref> [43, Section 3.3.3] </ref> describes a method he calls the uv k w algorithm, where each pass looks for repetitions of substrings (the v k ), chooses the best candidate, and replaces with a *-expression; then the next pass is started on the reduced string. <p> This method is well-adapted for loops, but has problems with the or operator as well. Miclet's survey <ref> [43] </ref> describes several other techniques for inferring (N)DFAs from positive samples, and is, in general, a good reference for practical inference techniques. 5.4 Statistical Model Inference Techniques Statistical models are models of languages that incorporate some notion of the likelihood of a sequence or subsequence occurring in the language. <p> In practice, this approach has been too time-consuming to be practical, and also requires an assumption on the number of states in the model <ref> [43] </ref>. The next level above HMMs is Stochastic Context-Free Grammars (SCFG). An SCFG is just a CFG where each production rule has some associated probability of firing. As with HMMs, inferring an SCFG usually means inferring the probabilities on the given production rules.
Reference: [44] <author> L. Miclet and J. Quinqueton. </author> <title> Syntactic and Structural Pattern Recognition, </title> <booktitle> volume 45 of NATO ASI Series F: Computer and Systems Sciences, </booktitle> <pages> pages 153-171. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: This method uses the concept of Markov models to find the most probable event sequence productions, and algorithmically converts those probabilities into states and state transitions. Although our method is new, there is previous work that has used similar methods. For example, Miclet and Quinqueton <ref> [44] </ref> use sequence probabilities to create FSM recognizers of protein sequences, and then use the Markov models to predict the center point of new protein sequences.
Reference: [45] <author> B. Peuschel and W. Schafer. </author> <title> Concepts and Implementation of a Rule-based Process Engine. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 262-279. </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction The issues of managing and improving the process of developing and maintaining software have come to the forefront of software engineering research. In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [9, 26, 45, 52] </ref>, process analysis [32, 33, 37, 47], and process evolution [8, 34], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [46] <author> L. Pitt. </author> <title> Analogical and Inductive Inference, </title> <booktitle> volume 397 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 18-44. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: We call the sequence s in hs; li positive if l = 1 (i.e., it is in L) and negative otherwise. L is defined 1 This presentation is consistent with others in the field <ref> [6, 46, 49] </ref>. 2 L = fl is not a very interesting language. 6 as the possibly infinite set of all positive sequences, and L is defined as the possibly infinite set of all negative sequences. For learning purposes, an algorithm needs some presentation of data. <p> Classes of boolean formulas and decision trees, and some geometric and algebraic concepts have been shown to be PAC-learnable. Results for grammars, including DFAs, are more negative, but less definite. As Pitt <ref> [46] </ref> reports, If DFAs are polynomially approximately predictable, then there is a probabilistic polynomial time algorithm for inverting the RSA encryption function, for factoring Blum integers, and for deciding quadratic residues. Angluin [5] phrases the learning problem in terms of an oracle. <p> There is much work describing the computational complexities of various learning paradigms and classes of languages or formulas. The survey by Angluin and Smith [6] is a broad look at inductive inference learning. Pitt's survey <ref> [46] </ref> is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in [3, 35, 40, 41, 48].
Reference: [47] <author> M. Saeki, T. Kaneko, and M. Sakamoto. </author> <title> A Method for Software Process Modeling and Description Using LOTOS. </title> <booktitle> In Proceedings of the First International Conference on the Software Process, </booktitle> <pages> pages 90-104. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation [9, 26, 45, 52], process analysis <ref> [32, 33, 37, 47] </ref>, and process evolution [8, 34], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [48] <author> Y. Sakakibara. </author> <title> Efficient learning of context-free grammars from positive structural examples. </title> <journal> Information and Computation, </journal> <volume> 97 </volume> <pages> 23-60, </pages> <year> 1992. </year>
Reference-contexts: The survey by Angluin and Smith [6] is a broad look at inductive inference learning. Pitt's survey [46] is a very good starting point for understanding the theoretical complexities involved in learning regular grammars. Further theoretical work on language learning is found in <ref> [3, 35, 40, 41, 48] </ref>. Angluin and Smith, in their survey, acknowledge the gap between the theoretical results that have been achieved and the practical application of inference methods.
Reference: [49] <author> Y. Sakakibara. </author> <title> Grammatical inference: An old and new paradigm. </title> <type> Technical Report ISIS-RR-95-9E, </type> <institution> Institute for Social Information Science, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: We call the sequence s in hs; li positive if l = 1 (i.e., it is in L) and negative otherwise. L is defined 1 This presentation is consistent with others in the field <ref> [6, 46, 49] </ref>. 2 L = fl is not a very interesting language. 6 as the possibly infinite set of all positive sequences, and L is defined as the possibly infinite set of all negative sequences. For learning purposes, an algorithm needs some presentation of data.
Reference: [50] <author> J.A. Sanchez and J.M. Benedi. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelligence (subseries of LNCS), </booktitle> <pages> pages 130-138. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year> <month> 41 </month>
Reference-contexts: This has been done in several pieces of work. * Stolke [51] using a prefix tree method of FSM inference to provide an input HMM to a parameter estimation routine. His results show a good improvement over parameter estimation beginning with a fully connected model. * Sanchez and Benedi <ref> [50] </ref> use a specific inference mechanism (that only finds loop-free automata) to provide a model structure and initial transition probabilities to a forward-backward parameter estimation algorithm. * Chen [20] describes a corpus-based SCFG inference method using bayesian probabilities and compares it to ngram models. * Casacuberta [17] provides a transformation algorithm
Reference: [51] <author> A. Stolcke. </author> <title> Grammatical Inference and Applications, </title> <booktitle> volume 862 of Lecture Notes in Artificial Intelli--gence (subseries of LNCS), </booktitle> <pages> pages 106-118. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This has been done in several pieces of work. * Stolke <ref> [51] </ref> using a prefix tree method of FSM inference to provide an input HMM to a parameter estimation routine.
Reference: [52] <author> S.M. Sutton, Jr., H. Ziv, D. Heimbigner, H.E. Yessayan, M. Maybee, , L.J. Osterweil, and X. Song. </author> <title> Programming a Software Requirements-specification Process. </title> <booktitle> In Proceedings of the First International Conference on the Software Process, </booktitle> <pages> pages 68-89. </pages> <publisher> IEEE Computer Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The issues of managing and improving the process of developing and maintaining software have come to the forefront of software engineering research. In response, new methods and tools for supporting various aspects of the software process have been devised. Many of the technologies, including process automation <ref> [9, 26, 45, 52] </ref>, process analysis [32, 33, 37, 47], and process evolution [8, 34], assume the existence of some sort of formal model of a process in order for those technologies to be applied.
Reference: [53] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: The PAC model for learning concepts from examples was proposed by Valiant <ref> [53] </ref>. PAC stands for "Probably Approximately Correct", which may not sound rigorous but is: probably is defined as within some probability 1 ffi, and approximately is defined as within some 1 * of the correct solution.
Reference: [54] <author> A.L. Wolf and D.S. Rosenblum. </author> <title> A Study in Software Process Data Capture and Analysis. </title> <booktitle> In Proceedings of the Second International Conference on the Software Process, </booktitle> <pages> pages 115-124. </pages> <publisher> IEEE Computer Society, </publisher> <month> February </month> <year> 1993. </year>
Reference-contexts: This does not mean that other aspects of a process are not worthy of study; it is just that the issues we have chosen to investigate are those having to do with behavior rather than structure. Following Wolf and Rosenblum <ref> [54] </ref>, we use an event-based model of process actions, where an event is used to characterize the dynamic behavior of a process in terms of identifiable, instantaneous actions, such as invoking a development tool or deciding upon the next activity to be performed. `Instantaneous' is relative to the time granularity that
Reference: [55] <author> Z. Zeng, R.M. Goodman, and P. Smyth. </author> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Nueral Computation, </journal> <volume> 5 </volume> <pages> 976-990, </pages> <year> 1993. </year> <month> 42 </month>
Reference-contexts: Recent representative work in this field is <ref> [25, 55] </ref>. The various methods all consist of defining a recurrent network architecture, and then analyzing the hidden neuron activity to discover the states and transitions for the resulting grammar. The difference between these methods is how they inspect the hidden neurons to infer state information.
References-found: 55

