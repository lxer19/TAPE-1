URL: http://www.research.digital.com/SRC/personal/Michael_Mitzenmacher/NEWWORK/postscripts/tstamp-jver.ps.gz
Refering-URL: http://www.research.digital.com/SRC/personal/Michael_Mitzenmacher/NEWWORK/papers.html
Root-URL: http://www.research.digital.com
Title: Average Case Analyses of List Update Algorithms, with Applications to Data Compression  
Author: Susanne Albers Michael Mitzenmacher 
Keyword: Online Algorithms, Competitive Analysis, List Update Problem, Probability Distribution, Data Compression, Entropy.  
Abstract: We study the performance of the Timestamp(0) (TS(0)) algorithm for self-organizing sequential search on discrete memoryless sources. We demonstrate that TS(0) is better than Move-to-front on such sources, and determine performance ratios for TS(0) against the optimal o*ine and static adversaries in this situation. Previous work on such sources compared online algorithms only to static adversaries. One practical motivation for our work is the use of the Move-to-front heuristic in various compression algorithms. Our theoretical results suggest that in many cases using TS(0) in place of Move-to-front in schemes that use the latter should improve compression. Tests using implementations on a standard corpus of test documents demonstrate that TS(0) leads to improved compression. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Albers. </author> <title> Improved randomized on-line algorithms for the list update problem. </title> <booktitle> In Proc. of the 6th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 412-419, </pages> <year> 1995. </year>
Reference-contexts: Our main contribution is to show that there is an algorithm that has an even better overall performance than MTF. The algorithm we analyze belongs to the Timestamp (p) family of algorithms <ref> [1] </ref> that were introduced in the context of randomized online algorithms and are defined for any real number p 2 [0; 1]. <p> The algorithm we analyze belongs to the Timestamp (p) family of algorithms [1] that were introduced in the context of randomized online algorithms and are defined for any real number p 2 <ref> [0; 1] </ref>. For p = 0, the algorithm is deterministic and can be formulated as follows: * Algorithm TS (0): Insert the requested item, say x, in front of the first item in the list that has been requested at most once since the last request to x. <p> Items x 4 and x 6 were requested at most once since the last request to x 1 , whereas x 2 and x 3 were both requested twice. Thus, TS (0) will insert x 1 immediately in front of x 4 in the list. In <ref> [1] </ref> it was shown that TS (0) achieves a competitive ratio of 2 on any request sequence, as does Move-to-front [17]. <p> These statements were shown in <ref> [1] </ref>. <p> This technique of evaluating cost by considering pairs of items was also used in <ref> [3, 11, 1] </ref>. In the second part of the analysis we show that, for each pair fx; yg, the asymptotic expected cost paid by TS (0) is at most 3 2 times the asymptotic expected cost incurred by OPT. <p> Let C T S (oe xy ) be the cost incurred by TS (0) if it serves oe xy on a two item list that consists of only x and y. In <ref> [1] </ref> it was shown that if TS (0) serves oe on the long list, then the relative position of x and y changes in the same way as if TS (0) serves oe xy on the two item list. <p> In particular, we have shown that on all distributions, the expected cost incurred by TS (0) is at most 1.5 times the expected cost incurred by the optimal (dynamic) o*ine algorithm. We note that a similar analysis can also be used to study the Timestamp (p) algorithms <ref> [1] </ref>, but TS (0) yields the best competitive ratio against distributions.
Reference: [2] <author> R. Bachrach and R. El-Yaniv. </author> <title> Online list accessing algorithms and their applications: Recent empirical evidence. </title> <booktitle> In Proceedings of the 8th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 53-62, </pages> <year> 1997. </year>
Reference-contexts: In almost all of our tests, TS (0) encoding achieves a better compression ratio than MTF encoding. We note that further experiments on the performance of compression schemes based on list update algorithms, including TS (0) and MTF, have recently been performed by <ref> [2] </ref>. 4 2 Analyses for the list update problem In this section, we begin by demonstrating that the asymptotic expected cost of TS (0) is always at most that of MTF on discrete memoryless sources.
Reference: [3] <author> J.L. Bentley and C.C. McGeoch. </author> <title> Amortized analyses of self-organizing sequential search heuristics. </title> <journal> Communication of the ACM, </journal> <volume> 28 </volume> <pages> 404-411, </pages> <year> 1985. </year>
Reference-contexts: This technique of evaluating cost by considering pairs of items was also used in <ref> [3, 11, 1] </ref>. In the second part of the analysis we show that, for each pair fx; yg, the asymptotic expected cost paid by TS (0) is at most 3 2 times the asymptotic expected cost incurred by OPT.
Reference: [4] <author> J.L. Bentley, D.S. Sleator, R.E. Tarjan and V.K. Wei. </author> <title> A locally adaptive data compression scheme. </title> <journal> Communication of the ACM, </journal> <volume> 29 </volume> <pages> 320-330, </pages> <year> 1986. </year>
Reference-contexts: by comparing TS (0) with the optimal static and dynamic o*ine algorithms. 2 Since our results show that TS (0) performs better than MTF on distributions, we consider applying the algorithm in the context of data compression, where MTF has been used to develop a locally adaptive data compression scheme <ref> [4] </ref>. Here we prove that for all distributions ~p = (p 1 ; p 2 ; : : : ; p n ), the expected number of bits needed by a TS (0)-based encoding scheme to encode one symbol is linear in the entropy of the source. <p> The string S consists of symbols, where each symbol is an element in the alphabet = fx 1 ; x 2 ; : : : ; x n g. Each symbol is equal to x i with probability p i . Bentley et al. <ref> [4] </ref> showed how any list update algorithm can be used to develop a data compression scheme. The idea is to convert the string S of symbols into a string I of integers. <p> We also prove that, for any string S, the average number of bits needed by TS (0) to encode one symbol in S are linear in the empirical entropy of S. Our bound is the same as that given for MTF by <ref> [4] </ref>. Moreover, we provide evidence that TS (0)-based compression schemes can outperform MTF-based compression schemes in practical situations by implementing these compression algorithms and testing them on the standard Calgary Compression Corpus files [19]. <p> It also suggests that perhaps the bound could be improved by avoiding this technical difficulty. 3 Analyses and simulations for data compression The MTF algorithm has proved useful in the development of the locally adaptive compression scheme of <ref> [4] </ref>. Motivated by this result, we consider a similar algorithm based on TS (0). We assume the reader is somewhat familiar with the system of [4], which was briefly described in the introduction. 11 3.1 Theoretical results Let B T S (~p) be the expected number of bits that TS (0) <p> technical difficulty. 3 Analyses and simulations for data compression The MTF algorithm has proved useful in the development of the locally adaptive compression scheme of <ref> [4] </ref>. Motivated by this result, we consider a similar algorithm based on TS (0). We assume the reader is somewhat familiar with the system of [4], which was briefly described in the introduction. 11 3.1 Theoretical results Let B T S (~p) be the expected number of bits that TS (0) needs to encode one symbol in an input sequence that is generated by ~p = (p 1 ; p 2 ; : : : ; <p> In order to analyze B T S (~p), we have to specify how an integer j should be encoded. We use a variable length prefix code by Elias [7] which encodes the integer j using 1 + blog jc + 2blog (1 + log j)c bits. Bentley et al. <ref> [4] </ref> showed that, using this prefix code, the expected number of bits needed by the MTF algorithm is B MT F (~p) 1 + H (~p) + 2 log (1 + H (~p)); for all ~p. <p> Similarly, let A MT F (S) be the average number of bits needed by the MTF algorithm. Again, we assume that an integer j is encoded by means of the Elias encoding that requires 1 + blog jc + 2blog (1 + log j)c bits. Bentley et al. <ref> [4] </ref> show that for any input sequence S, A MT F (S) 1 + H (S) + 2 log (1 + H (S)) where H (S) = P n m i m i the "empirical entropy" of S. <p> Also, for convenience, we placed no memory limitation on the compressor or decompressor; that is, the length of the list was allowed to grow as large as necessary. In practice one might wish to devise a more memory-efficient scheme, using the list as a cache as in <ref> [4] </ref>. The results of Table 2 reflect the compression achieved 1 , including only the token stream and not the dictionary. As one might expect, the gains from TS (0) in this situation are less dramatic, but still noticeable.
Reference: [5] <author> M. Burrows and D.J. Wheeler. </author> <title> A block-sorting lossless data compression algorithm. </title> <note> DEC SRC Research Report 124, 1994. 17 </note>
Reference-contexts: They implemented the new compression scheme but their simulations do not show an explicit comparison between MTF and MTF with secondary lists. Also recently, a fast and efficient compression scheme that uses MTF encoding as a subroutine has been developed <ref> [5] </ref>. <p> We have also attempted to use TS (0) encoding in place of MTF encoding in the data compression algorithm recently presented by Burrows and Wheeler <ref> [5] </ref>. Unfortunately, the results here show less promise. In some cases, TS (0) led to improved compression, but in most cases MTF encoding yielded better results.
Reference: [6] <author> F.R.K. Chung, </author> <title> D.J. Hajela and P.D. Seymour. Self-organizing sequential search and Hilbert's inequality. </title> <booktitle> Proc. 17th Annual Symposium on the Theory of Computing, </booktitle> <pages> pages 217-223, </pages> <year> 1985. </year>
Reference-contexts: Rivest [14] showed that for all ~p, E F C (~p)=E ST AT (~p) = 1. However, the algorithm FC has the drawback that it adapts very slowly to changing probability distributions. Chung et al. <ref> [6] </ref> analyzed the MTF rule and proved E MT F (~p)=E ST AT (~p) 2 1:5708 for all ~p. <p> in a request sequence generated by ~p = (p 1 ; p 2 ; : : : ; p n ) is 1 + i j6=i j + 3p 2 i X p i p j (p 2 j ) 1 : We can now adapt the techniques presented in <ref> [6] </ref> to bound the ratio between E T S (~p) and E ST AT (~p). We assume p 1 p 2 : : : p n . <p> y) is non-negative and homogeneous of degree 1, and that Z 1 K (x; 1)x 1=p dx = 0 Then Z 1 0 Z 1 K (x; y)g (y)dy C q 0 Proof of Lemma 2: Set F (x) = R x 1 f (x)dx: Then, by Lemma 2 of <ref> [6] </ref>, R 1 R 1 0 min (x; y)f (x)f (y)dxdy = R 1 0 F 2 (x)dx.
Reference: [7] <author> P. Elias. </author> <title> Universal codeword sets and the representation of the integers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 21 </volume> <pages> 194-203, </pages> <year> 1975. </year>
Reference-contexts: We assume p i &gt; 0 for all i. In order to analyze B T S (~p), we have to specify how an integer j should be encoded. We use a variable length prefix code by Elias <ref> [7] </ref> which encodes the integer j using 1 + blog jc + 2blog (1 + log j)c bits. <p> The compression is performed by turning the document into a token stream. The tokens are then encoded by their position in the list using standard variable-length prefix encodings given by Elias <ref> [7] </ref>; each integer j requires 1 + 2blog jc bits. This prefix code is different from the code we used in the analyses of the previous section; in our tests, it leads to slightly better compression.
Reference: [8] <author> G.H. Gonnet, J.I. Munro and H. Suwanda. </author> <title> Exegesis of self-organizing linear search. </title> <journal> SIAM Journal on Computing, </journal> <volume> 10 </volume> <pages> 613-637, </pages> <year> 1981. </year>
Reference-contexts: However, the algorithm FC has the drawback that it adapts very slowly to changing probability distributions. Chung et al. [6] analyzed the MTF rule and proved E MT F (~p)=E ST AT (~p) 2 1:5708 for all ~p. This bound is tight because Gonnet et al. <ref> [8] </ref> showed that one can find ~p 0 with E MT F (~p 0 )=E ST AT (~p 0 ) ff for any ff arbitrarily close to 2 .
Reference: [9] <author> D. Grinberg, S. Rajagopalan, R. Venkatesan and V.K. Wei. </author> <title> Splay trees for data compression. </title> <booktitle> In Proc. of the 6th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 522-530, </pages> <year> 1995. </year>
Reference-contexts: Bentley et al. also showed that, for any string S, the 3 average number of bits needed by MTF to encode one symbol in S is linear in the "empirical entropy" of the string. Recently, Grinberg et al. <ref> [9] </ref> proposed a modification of the MTF encoding, which they call MTF encoding with secondary lists. They implemented the new compression scheme but their simulations do not show an explicit comparison between MTF and MTF with secondary lists.
Reference: [10] <author> G.H. Hardy, J.E. Littlewood and G. Polya. </author> <title> Inequalities. </title> <publisher> Cambridge University Press, </publisher> <address> Cam-bridge, England, </address> <year> 1994. </year>
Reference-contexts: We calculate the required integral (using Maple) to find Z 1 H + (x; 1)x 1=2 dx 1:3390:::: We now move to the proof of Lemma 2. The proof depends on Holder's inequality Z Z 1=p Z 1=q and the following version of Hilbert's inequality (see <ref> [10] </ref>): 10 Theorem 5 (Hilbert's inequality) For p; q &gt; 1 satisfying 1 p + 1 q = 1, suppose that K (x; y) is non-negative and homogeneous of degree 1, and that Z 1 K (x; 1)x 1=p dx = 0 Then Z 1 0 Z 1 K (x; y)g <p> For j = 1; : : : ; n, let q ij be the asymptotic probability that x i is at position j in TS (0)'s list. The expected number of bits to encode the symbol x i is P n j=1 q ij f (j), which, by Jensen's <ref> [10] </ref> inequality, is at most f ( P n j=1 q ij j).
Reference: [11] <author> S. Irani. </author> <title> Two results on the list update problem. </title> <journal> Information Processing Letters, </journal> <volume> 38 </volume> <pages> 301-306, </pages> <year> 1991. </year>
Reference-contexts: This technique of evaluating cost by considering pairs of items was also used in <ref> [3, 11, 1] </ref>. In the second part of the analysis we show that, for each pair fx; yg, the asymptotic expected cost paid by TS (0) is at most 3 2 times the asymptotic expected cost incurred by OPT.
Reference: [12] <author> N. Kahale. </author> <title> Large Deviation Bounds for Markov Chains. </title> <type> DIMACS Technical Report 94-39. </type>
Reference-contexts: Let Z be the random number of such transitions on oe. Abusing notation somewhat, let E [Z] be the asymptotic expected number of such transitions. We now make use of standard large deviation bounds on finite state Markov chains (see, for example, [16, Lemma 7.6] or <ref> [12, Corollary 4.2] </ref>), which yield Chernoff-like bounds on the deviation of the number of transition from the asymptotic expected number of transitions.
Reference: [13] <author> R. Karp and P. Raghavan. </author> <title> From a personal communication cited in [15]. </title>
Reference-contexts: They also showed that the algorithms T and FC are not c-competitive for any constant c that is independent of the list size n. The competitive ratio of 2 is the best ratio that a deterministic online algorithm for the list update problem can achieve <ref> [13] </ref>. In classical data compression theory, it is often assumed that a discrete memoryless source generates a string S to be compressed.
Reference: [14] <author> R. Rivest. </author> <title> On self-organizing sequential search heuristics. </title> <journal> Communication of the ACM, </journal> <volume> 19 </volume> <pages> 63-67, </pages> <year> 1976. </year>
Reference-contexts: For any algorithm A, let E A (~p) denote the asymptotic expected cost incurred by algorithm A in serving one request in a request sequence generated by the distribution ~p. Rivest <ref> [14] </ref> showed that for all ~p, E F C (~p)=E ST AT (~p) = 1. However, the algorithm FC has the drawback that it adapts very slowly to changing probability distributions. <p> p j 2 (p i + p j ) 2 : Corollary 1 For any probability distribution ~p = (p 1 ; p 2 ; : : : ; p n ), E MT F (~p) E T S (~p) = 1ijn (p i p j ) 2 Proof: Rivest <ref> [14] </ref> showed E MT F (~p) = P 2p i p j p i +p j : Using part b) of Theorem 1, the result follows immediately. 6 2.2 Performance against dynamic o*ine algorithms Theorem 2 For any probability distribution ~p = (p 1 ; p 2 ; : : :
Reference: [15] <author> N. Reingold, J. Westbrook and D.D. Sleator. </author> <title> Randomized competitive algorithms for the list update problem. </title> <journal> Algorithmica, </journal> <volume> 11(1) </volume> <pages> 15-32, </pages> <year> 1994. </year>
Reference: [16] <author> A. Shwartz and A. Weiss. </author> <title> Large Deviations for Performance Analysis : Queues, communications, and computing. </title> <publisher> Chapman and Hall, </publisher> <address> London, England, </address> <year> 1995. </year>
Reference-contexts: Let Z be the random number of such transitions on oe. Abusing notation somewhat, let E [Z] be the asymptotic expected number of such transitions. We now make use of standard large deviation bounds on finite state Markov chains (see, for example, <ref> [16, Lemma 7.6] </ref> or [12, Corollary 4.2]), which yield Chernoff-like bounds on the deviation of the number of transition from the asymptotic expected number of transitions.
Reference: [17] <author> D.D. Sleator and R.E. Tarjan. </author> <title> Amortized efficiency of list update and paging rules. </title> <journal> Communication of the ACM, </journal> <volume> 28 </volume> <pages> 202-208, </pages> <year> 1985. </year>
Reference-contexts: Thus, TS (0) will insert x 1 immediately in front of x 4 in the list. In [1] it was shown that TS (0) achieves a competitive ratio of 2 on any request sequence, as does Move-to-front <ref> [17] </ref>. <p> More recent research on the list update problem was inspired by Sleator and Tarjan <ref> [17] </ref> who suggested to compare the performance of an online algorithm to that of an optimal o*ine algorithm. An optimal o*ine algorithm knows the entire request sequence in advance and can serve it with minimum cost.
Reference: [18] <author> B. Teia. </author> <title> A lower bound for randomized list update algorithms. </title> <journal> Information Processing Letters, </journal> <volume> 47 </volume> <pages> 5-9, </pages> <year> 1993. </year>
Reference-contexts: It is worthwhile to notice that 1.5 is the best lower bound currently known on the competitiveness that can be achieved by randomized list update algorithms against the oblivious adversary <ref> [18] </ref>. Thus, the performance ratio of TS (0) on distributions is at least as good as the performance ratio of randomized algorithms on any input. Finally we evaluate TS (0) against the optimal static ordering and show, for all ~p, E T S (~p)=E ST AT (~p) 1:34.
Reference: [19] <author> I.H. Witten and T. Bell. </author> <title> The Calgary/Canterbury text compression corpus. </title> <note> Anonymous ftp from ftp.cpsc.ucalgary.ca : /pub/text.compression/corpus/text.compression.corpus.tar.Z. 18 </note>
Reference-contexts: Our bound is the same as that given for MTF by [4]. Moreover, we provide evidence that TS (0)-based compression schemes can outperform MTF-based compression schemes in practical situations by implementing these compression algorithms and testing them on the standard Calgary Compression Corpus files <ref> [19] </ref>. In almost all of our tests, TS (0) encoding achieves a better compression ratio than MTF encoding. <p> We have tested our theoretical results by implementing simple versions of TS (0)-encoders and decoders for text compression. Our tests use standard documents from the Calgary Compression Corpus <ref> [19] </ref>. The current goal of these tests is not to develop an all-purpose functional compression system, but merely to demonstrate the potential gains from using TS (0) in place of MTF. The compression is performed by turning the document into a token stream.
References-found: 19

