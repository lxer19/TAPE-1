URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1996/96-44.ps.gz
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1996.html
Root-URL: http://www.cs.rutgers.edu
Email: Email: marek@cs.uni-bonn.de  Email: ajm@maths.ox.ac.uk  
Title: Polynomial Bounds for VC Dimension of Sigmoidal and General Pfaffian Neural Networks  
Author: by Marek Karpinski Angus Macintyre 
Note: DIMACS is a partnership of Rutgers University, Princeton University, AT&T Research, Bellcore, and Bell Laboratories. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology. Research partially supported by the International Computer Science Institute, Berkeley, by the DFG Grant KA 673/4-1, and by the ESPRIT BR Grants 7097 and ECUS 030. Research partially done while visiting  Research supported in part by a Senior Research ellowship of the EPSRC.  
Address: 53117 Bonn  Oxford OX1 3LB  Princeton University.  
Affiliation: Dept. of Computer Science University of Bonn  Mathematical Institute University of Oxford  Dept. of Computer Science at  
Abstract: DIMACS Technical Report 96-44 
Abstract-found: 1
Intro-found: 1
Reference: [AB92] <author> M. Anthony, N. Biggs, </author> <title> Computational Learning Theory: An Introduction, </title> <publisher> Cam-bridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: We develop a new method for proving explicit upper bounds for a wide class of analog neural networks with general Pfaffian activation functions. The most commonly used activation function in various neural networks applications is the sigmoid (y) = 1=1 + e y (cf. [HKP91]). We refer to <ref> [AB92] </ref>, [M93a], and [MS93] for all the necessary background on the computation by neural networks and the VC dimension (particularly, to the connection between their computational power, and the sample complexity). <p> We refer to <ref> [AB92] </ref>, [GJ93], and [MS93] for all notions required for the VC Dimension of neural networks, and to [H76] for all notions of differential geometry. The paper was inspired by the work of Goldberg and Jerrum [GJ93], who could deal with polynomial activation functions. <p> Observe that also the VC Dimension of A restricted to the boolean functions is bounded by d. We have ln (jB A j) O (kd) (cf., e.g., <ref> [AB92] </ref>). Our O ((`m) 2 ) upper bounds on the VC Dimension d of A entail now the following formula for the number jB A j of different boolean functions computed by A: jB A j 2 O (k` 2 m 2 ) . 4.7 Multivariate activation.
Reference: [AS93] <author> M. Anthony, J. Shawe-Taylor, </author> <title> A Result of Vapnik with Applications, </title> <journal> Discrete Applied Math. </journal> <volume> 47 (1993), </volume> <pages> pp. 207-217. </pages>
Reference: [BT90] <author> A. Borodin, P. Tiwari, </author> <title> On the Decidability of Sparse Univariate Polynomial Interpolation, </title> <booktitle> Proc. 22nd ACM STOC (1990), </booktitle> <pages> pp. 535-545. </pages>
Reference: [D92] <author> L. van den Dries, </author> <title> Tame Topology and 0-minimal Structures, </title> <type> preprint, </type> <institution> University of Illinois, Urbana, </institution> <note> 1992; to appear as a book. </note>
Reference: [DMM94] <author> L. van den Dries, A.Macintyre and D.Marker, </author> <title> The Elementary Theory of Restricted Analytic Fields with Exponentation, </title> <booktitle> Annals of Mathematics 140 (1994), </booktitle> <pages> pp 183-205. </pages>
Reference: [GJ93] <author> P. Goldberg and M. Jerrum, </author> <title> Bounding the Vapnik Chervonenkis Dimension of Concept Classes Parametrized by Real Numbers. </title> <journal> Machine Learning, </journal> <note> 1994 (to appear). - 14 - A preliminary version appeared in Proc. 6th ACM Workshop on Computational Learning Theory, pp. 361-369, </note> <year> 1993. </year>
Reference-contexts: It is perhaps worth nothing that slightly more general analytic increasing activation functions do not always have finite VC-dimension [S92]. In Maass's 1993 lecture notes [M93a] (see also <ref> [GJ93] </ref> and [MS93]), Open Problem 10 asks: Is the VC Dimension of analog neural nets with the sigmoid activation function (y) = 1=1 + e y bounded by a polynomial in the number of programmable parameters? In this paper we give an affirmative answer, with a polynomial bound in the number <p> We refer to [AB92], <ref> [GJ93] </ref>, and [MS93] for all notions required for the VC Dimension of neural networks, and to [H76] for all notions of differential geometry. The paper was inspired by the work of Goldberg and Jerrum [GJ93], who could deal with polynomial activation functions. A reference in [GJ93] to Warren's paper [W68] was <p> We refer to [AB92], <ref> [GJ93] </ref>, and [MS93] for all notions required for the VC Dimension of neural networks, and to [H76] for all notions of differential geometry. The paper was inspired by the work of Goldberg and Jerrum [GJ93], who could deal with polynomial activation functions. A reference in [GJ93] to Warren's paper [W68] was of particular importance. The paper is organized as follows. In Section 1, we introduce the necessary formalism for the describing formulas, as well as all preparatory algebraic and topological facts. <p> We refer to [AB92], <ref> [GJ93] </ref>, and [MS93] for all notions required for the VC Dimension of neural networks, and to [H76] for all notions of differential geometry. The paper was inspired by the work of Goldberg and Jerrum [GJ93], who could deal with polynomial activation functions. A reference in [GJ93] to Warren's paper [W68] was of particular importance. The paper is organized as follows. In Section 1, we introduce the necessary formalism for the describing formulas, as well as all preparatory algebraic and topological facts. <p> the class C = f ~ fi : ~ fi 2 IR ` g for ~ fi = fx 2 IR k : (x; ~ fi) &gt; 0g the partition of IR k by A according to the weight assignment ~ fi. (The general reader is referred to [MS93] and <ref> [GJ93] </ref> for definitions and basic properties of Vapnik-Chervonenkis (VC) dimension. We say a set S IR k is shattered by C if fS T C 2 C g = P (S). <p> So either V &lt; 2 log B, or V 16l. 2 3 Applications 3.1 If we now work with polynomials, and Milnor's bound for B, we get the results from <ref> [GJ93] </ref>. 3.2 An example involving exponentiation. Fix q and linear functions fl 1 ; : : : fl q of v; ~y. Let t i (v; ~y), 1 i s, be polynomials, of total degree d i , in v; ~y and the e fl i 's.
Reference: [H12] <author> G. H. Hardy, </author> <title> Properties of Logarithmic-Exponential Functions, </title> <journal> Proc. London Math. Soc. </journal> <volume> 10 (1912), </volume> <pages> pp. 54-90. </pages>
Reference: [H92] <author> D. Haussler, </author> <title> Decision Theoretic Generalizations of the PAC Model for Neural Net and other Learning Applications, Information an Computation 100, </title> <booktitle> (1992), </booktitle> <pages> pp. 78-150. </pages>
Reference: [HKP91] <author> J. Hertz, A. Krogh and R. G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: We develop a new method for proving explicit upper bounds for a wide class of analog neural networks with general Pfaffian activation functions. The most commonly used activation function in various neural networks applications is the sigmoid (y) = 1=1 + e y (cf. <ref> [HKP91] </ref>). We refer to [AB92], [M93a], and [MS93] for all the necessary background on the computation by neural networks and the VC dimension (particularly, to the connection between their computational power, and the sample complexity).
Reference: [H76] <author> M. W. Hirsch, </author> <title> Differential Topology, </title> <publisher> Springer-Verlag, </publisher> <year> 1976. </year>
Reference-contexts: We refer to [AB92], [GJ93], and [MS93] for all notions required for the VC Dimension of neural networks, and to <ref> [H76] </ref> for all notions of differential geometry. The paper was inspired by the work of Goldberg and Jerrum [GJ93], who could deal with polynomial activation functions. A reference in [GJ93] to Warren's paper [W68] was of particular importance. The paper is organized as follows.
Reference: [KM94] <author> M. Karpinski and A. Macintyre, </author> <title> Polynomial Bounds for VC Dimension of Sig-moidal Neural Networks, </title> <booktitle> Proc. 27th ACM STOC (1995), </booktitle> <address> pp.200-208. </address>
Reference-contexts: The result is a special case of much more general result about the VC Dimension of the classes defined by certain formulas. In contrast to <ref> [KM94] </ref>, this paper does not use o-minimality and therefore can be applied to more general situations like the Pfaffian functions for which o-minimality is not yet even established (!).
Reference: [KM95] <author> M. Karpinski and A. Macintyre, </author> <title> Bounding VC Dimension for Neural Metworks: </title> <booktitle> Progress and Prospects (Invited Lecture), Proc. EuroCOLT'95, Lecture Notes in Artificial Intelligence Vol.904, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995, </year> <pages> pp. 337-341. </pages>
Reference: [KW93] <author> M. Karpinski and T. Werther, </author> <title> VC Dimension and Uniform Learnability of Sparse Polynomials and Rational Functions, </title> <journal> SIAM J. Computing 22 (1993), </journal> <pages> pp 1276-1285. </pages>
Reference: [K91] <author> A. G. Khovanski, </author> <title> Fewnomials, </title> <publisher> American Mathematical Society, </publisher> <address> Providence, R.I., </address> <year> 1991. </year>
Reference-contexts: Fix such a B henceforth. 1.4 Examples. a) The t i are polynomials of degree d in ~y. Then B can be taken as 2 (2d) ` by a result of Milnor [M64]. b) Khovanski <ref> [K91, p. 91, Corollary 3] </ref> proved a basic result about exponential polynomials, namely: Theorem 1. Suppose l m. <p> log S + ql log l: c) If the t i are definable in an o-minimal expansion of the real field [KPS86], the existence of a B is guaranteed, but good bounds are not. d) Examples for which o-minimality is unknown but where our method applies involved Pfaffian functions (cf. <ref> [K91, p. 91, Example 3] </ref>). <p> Since Khovanski's <ref> [K91] </ref> one has known how to use Finiteness Theorems about exponentiation to give uniform estimates in problems involving - 8 - families of polynomials where there is an absolute bound to the number of nonzero coefficients occurring, but none on the degrees involved. <p> An alternative approach to the above result works directly with the function f N w (v; ~y), and uses the fact that f N w is a Pfaffian function. For the fundamental work on Pfaffian functions one should consult <ref> [K91] </ref>. (x) is Pfaffian, since 0 (x) = (x) ((x)) 2 . Clearly f N (v; ~y) is Pfaffian, for N an input node, for f N (v; ~y) = v i , where v i is the input variable corresponding to N . <p> There is still a large gap between (l 2 ) lower bound and our O (l 4 ) upper bound for sigmoidal and Pfaffian activation functions. The current bound on B in our paper comes because of Khovanski's technique of removing one variable at a time (cf. <ref> [K91, p.13] </ref>). We are looking closely at a method for getting to a kind of Bezout's estimate in one step, removing all variables simultaneously. 2 Acknowledgement. We thank Gregory Cherlin, Mark Jerrum and Eduardo Sontag for stimulating remarks and discussions.
Reference: [KPS86] <author> J. Knight, A. Pillay and C. Steinhorn, </author> <title> Definable Sets and Ordered Structures II, </title> <journal> Trans. American Mathematical Society 295 (1986), pp.593-605. </journal>
Reference-contexts: So (ql)(ql 1)=2 + l logd + l (q + 1) log S + ql log l: c) If the t i are definable in an o-minimal expansion of the real field <ref> [KPS86] </ref>, the existence of a B is guaranteed, but good bounds are not. d) Examples for which o-minimality is unknown but where our method applies involved Pfaffian functions (cf. [K91, p. 91, Example 3]).
Reference: [KS95] <author> P. Koiran and E.D. Sontag, </author> <title> Neural Networks with Quadratic VC Dimension to appear in Advances in Neural Information Processing Systems (NIPS '95), </title> <year> 1995. </year>
Reference-contexts: Obviously this would improve our upper bounds on the VC Dimension. The best lower bound on the VC Dimension of neural networks is (l log l) (cf. [M93a], [M94]) for the threshold, and (l 2 ) (cf. <ref> [KS95] </ref>) for piecewise polynomial and sigmoidal activation functions. There is still a large gap between (l 2 ) lower bound and our O (l 4 ) upper bound for sigmoidal and Pfaffian activation functions.
Reference: [L92] <author> M. C. Laskowski, </author> <title> Vapnik-Chervonenkis Classes od Definable Sets, </title> <journal> J.London Math. Society 45 (1992), </journal> <pages> pp 377-384. </pages>
Reference-contexts: Let (v; ~y) be Then (by definition) the V C-dimension of A is the V C-dimension of C . By <ref> [L92] </ref> (which appeals to [W94]) this dimension is finite, since is definable in +; ; ; 0; 1; &lt;; e x . We now apply our method to get a good polynomial bound for V C dim (A).
Reference: [M93a] <author> W. Maass, </author> <booktitle> Perspectives of Current Research about the Complexity of Learning on Neural Nets, in: Theoretical Advances in Neural Computation and Learning, </booktitle> <editor> V. P. Roychowdhury, K. Y. Siu, A. Orlitsky (Editors), </editor> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994, </year> <pages> pp. 295-336. </pages>
Reference-contexts: We develop a new method for proving explicit upper bounds for a wide class of analog neural networks with general Pfaffian activation functions. The most commonly used activation function in various neural networks applications is the sigmoid (y) = 1=1 + e y (cf. [HKP91]). We refer to [AB92], <ref> [M93a] </ref>, and [MS93] for all the necessary background on the computation by neural networks and the VC dimension (particularly, to the connection between their computational power, and the sample complexity). <p> It is perhaps worth nothing that slightly more general analytic increasing activation functions do not always have finite VC-dimension [S92]. In Maass's 1993 lecture notes <ref> [M93a] </ref> (see also [GJ93] and [MS93]), Open Problem 10 asks: Is the VC Dimension of analog neural nets with the sigmoid activation function (y) = 1=1 + e y bounded by a polynomial in the number of programmable parameters? In this paper we give an affirmative answer, with a polynomial bound <p> Section 2 contains the Main Result, and Sections 3 and 4 the applications. - 2 - 1 The setting 1.1 We shall consider a standard model of a feedforward network architecture A with the activation function (cf., e. g., <ref> [M93a] </ref>, [MS93]) with k inputs, m computational nodes, and ` weights (the number of programmable parameters). We assume (for simplicity) that the output gate of A has range f0; 1g. <p> Since the pseudo-dimension of an architecture A is bounded by the VC-Dimension of a new architecture A 0 (see [MS93]) got directly from A, we get polynomial bounds for the pseudo-dimension. This answers affirmatively the second part of Problem 10 in <ref> [M93a] </ref>. 4.6 Boolean Functions. We are interested now in computation of boolean functions f : f0; 1g k ! f0; 1g by neural networks (cf. [MSS91], [M93b]). <p> Obviously this would improve our upper bounds on the VC Dimension. The best lower bound on the VC Dimension of neural networks is (l log l) (cf. <ref> [M93a] </ref>, [M94]) for the threshold, and (l 2 ) (cf. [KS95]) for piecewise polynomial and sigmoidal activation functions. There is still a large gap between (l 2 ) lower bound and our O (l 4 ) upper bound for sigmoidal and Pfaffian activation functions.
Reference: [M93b] <author> W. Maass, </author> <title> Bounds for the Computational Power and Learning Complexity of Analog Neural Nets, </title> <booktitle> Proc. 25th ACM STOC (1993), </booktitle> <pages> pp. 335-344. - 15 </pages> - 
Reference-contexts: In contrast to [KM94], this paper does not use o-minimality and therefore can be applied to more general situations like the Pfaffian functions for which o-minimality is not yet even established (!). In the case of boolean functions computed by sigmoidal neural networks (cf. [MSS91], <ref> [M93b] </ref>), our result entails, also for the first time, by a simple counting argument, the fact that not every boolean function can be computed by a single polynomial size sigmoidal or general Pfaffian neural network with an appropriate weight assignment. <p> This answers affirmatively the second part of Problem 10 in [M93a]. 4.6 Boolean Functions. We are interested now in computation of boolean functions f : f0; 1g k ! f0; 1g by neural networks (cf. [MSS91], <ref> [M93b] </ref>). It is known that applying some single non-boolean activation functions enhances, sometimes dramatically, the computational power of a neural network (cf. [MSS91]) even if restricted to the boolean functions.
Reference: [M94] <author> W. Maass, </author> <title> Neural Nets with Superlinear VC-Dimension, </title> <booktitle> Proc. of the International Conference on Artificial Neural Networks 1994 (ICANN '94), </booktitle> <publisher> Springer (Berlin 1994), </publisher> <pages> pp. </pages> <note> 581-584; journal version appeared in Neural Computation 6 (1994), pp. 875-882. </note>
Reference-contexts: Obviously this would improve our upper bounds on the VC Dimension. The best lower bound on the VC Dimension of neural networks is (l log l) (cf. [M93a], <ref> [M94] </ref>) for the threshold, and (l 2 ) (cf. [KS95]) for piecewise polynomial and sigmoidal activation functions. There is still a large gap between (l 2 ) lower bound and our O (l 4 ) upper bound for sigmoidal and Pfaffian activation functions.
Reference: [MSS91] <author> W. Maass, G. Schnitger and E. D. Sontag, </author> <title> On the Computational Power of Sig-moidal versus Boolean Threshold Circuits, </title> <booktitle> Proc. 32nd IEEE FOCS (1991), </booktitle> <pages> pp. 767-776. </pages>
Reference-contexts: In contrast to [KM94], this paper does not use o-minimality and therefore can be applied to more general situations like the Pfaffian functions for which o-minimality is not yet even established (!). In the case of boolean functions computed by sigmoidal neural networks (cf. <ref> [MSS91] </ref>, [M93b]), our result entails, also for the first time, by a simple counting argument, the fact that not every boolean function can be computed by a single polynomial size sigmoidal or general Pfaffian neural network with an appropriate weight assignment. <p> This answers affirmatively the second part of Problem 10 in [M93a]. 4.6 Boolean Functions. We are interested now in computation of boolean functions f : f0; 1g k ! f0; 1g by neural networks (cf. <ref> [MSS91] </ref>, [M93b]). It is known that applying some single non-boolean activation functions enhances, sometimes dramatically, the computational power of a neural network (cf. [MSS91]) even if restricted to the boolean functions. <p> We are interested now in computation of boolean functions f : f0; 1g k ! f0; 1g by neural networks (cf. <ref> [MSS91] </ref>, [M93b]). It is known that applying some single non-boolean activation functions enhances, sometimes dramatically, the computational power of a neural network (cf. [MSS91]) even if restricted to the boolean functions. However it has been open for sometime now how much this increase in computational power of a neural network could be.
Reference: [MS93] <author> A. J. Macintyre and E. D. Sontag, </author> <title> Finiteness results for Sigmoidal Neural Networks, </title> <booktitle> Proc. 25th ACM STOC (1993), </booktitle> <address> pp.325-334. </address>
Reference-contexts: The most commonly used activation function in various neural networks applications is the sigmoid (y) = 1=1 + e y (cf. [HKP91]). We refer to [AB92], [M93a], and <ref> [MS93] </ref> for all the necessary background on the computation by neural networks and the VC dimension (particularly, to the connection between their computational power, and the sample complexity). In [MS93] the finiteness of VC Dimension of sigmoidal neural networks has been established for the first time using a deep result in <p> We refer to [AB92], [M93a], and <ref> [MS93] </ref> for all the necessary background on the computation by neural networks and the VC dimension (particularly, to the connection between their computational power, and the sample complexity). In [MS93] the finiteness of VC Dimension of sigmoidal neural networks has been established for the first time using a deep result in model theory. It is perhaps worth nothing that slightly more general analytic increasing activation functions do not always have finite VC-dimension [S92]. <p> It is perhaps worth nothing that slightly more general analytic increasing activation functions do not always have finite VC-dimension [S92]. In Maass's 1993 lecture notes [M93a] (see also [GJ93] and <ref> [MS93] </ref>), Open Problem 10 asks: Is the VC Dimension of analog neural nets with the sigmoid activation function (y) = 1=1 + e y bounded by a polynomial in the number of programmable parameters? In this paper we give an affirmative answer, with a polynomial bound in the number of programmable <p> We refer to [AB92], [GJ93], and <ref> [MS93] </ref> for all notions required for the VC Dimension of neural networks, and to [H76] for all notions of differential geometry. The paper was inspired by the work of Goldberg and Jerrum [GJ93], who could deal with polynomial activation functions. <p> Section 2 contains the Main Result, and Sections 3 and 4 the applications. - 2 - 1 The setting 1.1 We shall consider a standard model of a feedforward network architecture A with the activation function (cf., e. g., [M93a], <ref> [MS93] </ref>) with k inputs, m computational nodes, and ` weights (the number of programmable parameters). We assume (for simplicity) that the output gate of A has range f0; 1g. <p> dimension of the class C = f ~ fi : ~ fi 2 IR ` g for ~ fi = fx 2 IR k : (x; ~ fi) &gt; 0g the partition of IR k by A according to the weight assignment ~ fi. (The general reader is referred to <ref> [MS93] </ref> and [GJ93] for definitions and basic properties of Vapnik-Chervonenkis (VC) dimension. We say a set S IR k is shattered by C if fS T C 2 C g = P (S). <p> Combining the ideas of 3.3. and 4.1. we easily get for log B a bound with dominant term quadratic in ln, and this is of course dominant in the VC-dimension bound for the A's in the family. 4.5 Haussler's Pseudodimension. We refer to <ref> [MS93] </ref> for the definition of the pseudo-dimension of an architecture. Since the pseudo-dimension of an architecture A is bounded by the VC-Dimension of a new architecture A 0 (see [MS93]) got directly from A, we get polynomial bounds for the pseudo-dimension. <p> We refer to <ref> [MS93] </ref> for the definition of the pseudo-dimension of an architecture. Since the pseudo-dimension of an architecture A is bounded by the VC-Dimension of a new architecture A 0 (see [MS93]) got directly from A, we get polynomial bounds for the pseudo-dimension. This answers affirmatively the second part of Problem 10 in [M93a]. 4.6 Boolean Functions. We are interested now in computation of boolean functions f : f0; 1g k ! f0; 1g by neural networks (cf. [MSS91], [M93b]).
Reference: [M64] <author> J. Milnor, </author> <title> On the Betti Numbers of Real Varieties, </title> <booktitle> Proc. of the American Mathematical Society 15 (1964), </booktitle> <pages> pp 275-280. </pages>
Reference-contexts: Fix such a B henceforth. 1.4 Examples. a) The t i are polynomials of degree d in ~y. Then B can be taken as 2 (2d) ` by a result of Milnor <ref> [M64] </ref>. b) Khovanski [K91, p. 91, Corollary 3] proved a basic result about exponential polynomials, namely: Theorem 1. Suppose l m.
Reference: [M65] <author> J. Milnor, </author> <title> Topology from the Differentiable Viewpoint, </title> <address> Univ.Press, Virginia, </address> <year> 1965. </year>
Reference-contexts: Choose fi 1 ; : : : ; fi r (r `) from among these, and let F : IR ` ! IR r be defined by F (~y) = hfi 1 (~y); : : : ; fi r (~y)i: By Sard's Theorem <ref> [M65] </ref>, the set of nonregular values h* 1 ; : : : ; * r i of F in IR r has Lebesgue measure 0. <p> = * ij g [ fy : t i (a j ; ~y) = * ij g (1 i s ; 1 j V ) has at least 2 V connected components (V; a j ; * are fixed as above). 2.3 This can now be combined with Sard [S42], <ref> [M65] </ref>, and a combinatorial idea of Warren [W68], to give Theorem 2. We use the following cases of Sard's Theorem. We have a C 1 map F : IR m ! IR n .
Reference: [S42] <author> A. Sard, </author> <title> The Measure of the Critical Points of Differentiable Maps, </title> <journal> Bull. Amer. Math. Soc. </journal> <volume> 48 (1942), </volume> <pages> pp. 883-890. </pages>
Reference-contexts: ~y) = * ij g [ fy : t i (a j ; ~y) = * ij g (1 i s ; 1 j V ) has at least 2 V connected components (V; a j ; * are fixed as above). 2.3 This can now be combined with Sard <ref> [S42] </ref>, [M65], and a combinatorial idea of Warren [W68], to give Theorem 2. We use the following cases of Sard's Theorem. We have a C 1 map F : IR m ! IR n . <p> of q in IR n which are not regular values of F has Lebesgue measure 0. - 5 - (It is easily seen that the normal definition of regular value, in terms of F 1 (p) containing no critical points, is equivalent to that given above.) Now we apply Sard <ref> [S42] </ref>. Let P = f&lt; i; j &gt; : 1 i s ; 1 j V g. For &lt; i; j &gt;2 P , let t i;j (~y) = t i (a j ; ~y).
Reference: [S-T94] <author> J. Shawe-Taylor, </author> <title> Sample Sizes for Sigmoidal Neural Networks, </title> <type> Preprint, </type> <institution> University of London, </institution> <year> 1994, </year> <note> to appear in Proc. ACM COLT, </note> <year> 1995. </year>
Reference: [S92] <author> E. D. Sontag, </author> <title> Feedforward Nets for Interpolation and Classification, </title> <journal> J. Comp. Syst. Sci. </journal> <volume> 45 (1992), </volume> <pages> pp. 20-48. </pages>
Reference-contexts: In [MS93] the finiteness of VC Dimension of sigmoidal neural networks has been established for the first time using a deep result in model theory. It is perhaps worth nothing that slightly more general analytic increasing activation functions do not always have finite VC-dimension <ref> [S92] </ref>.
Reference: [TV94] <author> G. Turan and F. Vatan, </author> <title> On the Computation of Boolean Functions by Analog Circuits of Bounded Fan-in, </title> <booktitle> Proc. 35th IEEE FOCS (1994), </booktitle> <pages> pp. 553-564. </pages>
Reference: [W68] <author> H. E. Warren, </author> <title> Lower Bounds for Approximation by Non-linear Manifolds, Trans. </title> <booktitle> of the AMS 133 (1968), </booktitle> <pages> pp. 167-178. </pages>
Reference-contexts: The paper was inspired by the work of Goldberg and Jerrum [GJ93], who could deal with polynomial activation functions. A reference in [GJ93] to Warren's paper <ref> [W68] </ref> was of particular importance. The paper is organized as follows. In Section 1, we introduce the necessary formalism for the describing formulas, as well as all preparatory algebraic and topological facts. <p> t i (a j ; ~y) = * ij g (1 i s ; 1 j V ) has at least 2 V connected components (V; a j ; * are fixed as above). 2.3 This can now be combined with Sard [S42], [M65], and a combinatorial idea of Warren <ref> [W68] </ref>, to give Theorem 2. We use the following cases of Sard's Theorem. We have a C 1 map F : IR m ! IR n . <p> Note finally, before we approach Theorem 2 via a theorem of Warren, that for ~* in 0 , if A 1 A 2 and f 1 f 2 then Z (A 2 ; f 2 )(~*) is a submanifold of Z (A 1 ; f 1 )(~*). Warren <ref> [W68] </ref> proved: Theorem 6. <p> Let b j (0 j n) be the number of connected components among all intersections of any j n of the M i . Then M S n i=1 M i has no more than P n j=0 b j connected components. Proof. See <ref> [W68, Theorem 1] </ref>. - 6 - We want to apply this by fixing ~* in 0 , taking M = IR l , and the M i as the zerosets of the t i;j (~y) * ij .
Reference: [W94] <author> A. J. Wilkie, </author> <title> Model Completeness Results of Restricted Pfaffian Functions and the Exponential Function; to appear in Journal of the AMS, </title> <year> 1994. </year>
Reference-contexts: Let (v; ~y) be Then (by definition) the V C-dimension of A is the V C-dimension of C . By [L92] (which appeals to <ref> [W94] </ref>) this dimension is finite, since is definable in +; ; ; 0; 1; &lt;; e x . We now apply our method to get a good polynomial bound for V C dim (A).
References-found: 30

