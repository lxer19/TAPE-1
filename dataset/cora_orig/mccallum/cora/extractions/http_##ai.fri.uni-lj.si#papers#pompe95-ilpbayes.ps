URL: http://ai.fri.uni-lj.si/papers/pompe95-ilpbayes.ps
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Email: e-mail: furos.pompe, igor.kononenkog@fer.uni-lj.si  
Phone: tel: +386-61-1768 386  
Title: Naive Bayesian classifier within ILP-R  
Author: Uros Pompe, Igor Kononenko 
Date: April, 1995  
Address: Trzaska 25, SI-61001 Ljubljana, Slovenia  
Affiliation: University of Ljubljana, Faculty of electrical engineering computer science,  
Abstract: When dealing with the classification problems, current ILP systems often lag behind state-of-the-art attributional learners. Part of the blame can be ascribed to a much larger hypothesis space which, therefore, cannot be as thoroughly explored. However, sometimes it is due to the fact that ILP systems do not take into account the probabilistic aspects of hypotheses when classifying unseen examples. This paper proposes just that. We developed a naive Bayesian classifier within our ILP-R first order learner. The learner itself uses a clever RELIEF based heuristic which is able to detect strong dependencies within the literal space when such dependencies exist. We conducted a series of experiments on artificial and real-world data sets. The results show that the combination of ILP-R together with the naive Bayesian classifier sometimes significantly improves the classification of unseen instances as measured by both classification accuracy and average information score.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K. M. & Pazzani, M. J. </author> <year> (1993). </year> <title> Hydra: A noise-tolerant relational concept learning algorithm. </title> <booktitle> In Proceedings of the 13 th International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1064-1070). </pages> <address> Chambery, France. </address>
Reference-contexts: The second reason might be that propositional learners take into account additional information by probabilistically analyzing the hypothesis over the training set. 2 ILP-95, Leuven The probabilistic aspects of the hypothesis were considered before within ILP. For instance, the HYDRA system <ref> (Ali & Pazzani, 1993) </ref> uses probabilistical evaluation of clauses to improve classification accuracy. However, its winner-takes-all strategy results in returning the class of the most reliable clause that was satisfied by the test example. Therefore, it does not exploit all available information.
Reference: <author> Cestnik, B. </author> <year> (1990). </year> <title> Estimating probabilities: A crucial task in machine learning. </title> <booktitle> In Proceedings of European Conference on Artificial Intelligence (pp. </booktitle> <pages> 147-149). </pages> <address> Stockholm. </address>
Reference-contexts: Finally, given the training set E + of labeled (classified) examples, the probability P (F i ) is computed as relative frequency of class F i . Conditional probability P (F i jC j ) is assessed using the m-estimate <ref> (Cestnik, 1990) </ref>. It is computed over the set Cov (C j ; E + ). When classifying the example T the coverage factor j is determined by the outcome of trying to cover T with C j .
Reference: <author> Dolsak, B. & Muggleton, S. </author> <year> (1992). </year> <title> The application of inductive logic programming to finite elements mesh design. </title> <editor> In S. Muggleton (Ed.), </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press. </publisher>
Reference-contexts: ILP-95, Leuven 11 4.3 Relational domains MESH (relational): We tested the performance of ILP-R on the problem of determining the number of elements for each of the edges of an object in the finite element mesh design problem <ref> (Dolsak & Muggleton, 1992) </ref>. There are five objects for which the human experts have constructed the appropriate meshes. In each of five experiments one object is used for testing and the other four for learning and the results are averaged.
Reference: <author> Dzeroski, S. </author> <year> (1991). </year> <title> Handling noise in inductive logic programming. </title> <type> Master's thesis, </type> <institution> University of Ljubljana, Faculty of electrical engineering and computer science, Ljubljana, Slovenia. </institution>
Reference: <author> Kira, K. & Rendell, L. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <editor> In D. </editor> <address> Sleeman & P. </address>
Reference-contexts: However, these measures are unable to detect strong dependencies among literals and are therefore unable to reveal a good hypothesis, when such dependencies exist. To attack the problem we developed an ILP learner called ILP-R (Pompe & Kononenko, 1994). Its RELIEF <ref> (Kira & Rendell, 1992) </ref> based heuristic was shown to be very effective in estimating the quality of literals when they are strongly dependent. For example, parity problems of various degrees were successfully solved, even when a significant number of irrelevant arguments are used within the target predicate.
Reference: <editor> Edwards (Eds.), </editor> <booktitle> Proceedings of International Conference on Machine Learning (pp. </booktitle> <pages> 249-256). </pages> <address> Aberdeen: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of relief. </title> <editor> In L. De Raedt & F. Bergadano (Eds.), </editor> <booktitle> Proceedings of European Conference on Machine Learning Catania: </booktitle> <publisher> Springer Verlag. </publisher>
Reference-contexts: However, these measures are unable to detect strong dependencies among literals and are therefore unable to reveal a good hypothesis, when such dependencies exist. To attack the problem we developed an ILP learner called ILP-R <ref> (Pompe & Kononenko, 1994) </ref>. Its RELIEF (Kira & Rendell, 1992) based heuristic was shown to be very effective in estimating the quality of literals when they are strongly dependent. <p> As a result we get the conditional probabilities of classes given the new instance. The orthogonality of the approach separates the selection of the learner from the use of our naive classifier, giving us an extra degree of freedom. We have chosen ILP-R <ref> (Pompe & Kononenko, 1994) </ref> as our first order learner. Apart from being relatively efficient, its RELIEF (Kira & Rendell, 1992; Kononenko, 1994) based literal quality estimator is able to point out literal dependencies where greedy algorithms using myopic impurity functions (for instance information gain) would fail. <p> This can lead to an unmanageable exponential growth of the training set when there are non-determinate literals present in the background knowledge B. To eliminate this problem ILP-R uses a language bias based on the d-dependable language L d <ref> (Pompe & Kononenko, 1994) </ref>. In essence, this bias prohibits the cross-referencing of the variables in the variable dependency tree. This is a tree where each node denotes a set of variables introduced by some literal L 2 C and the edge from the predecessor node is marked with L. <p> such bias 1 during the initialization and updating of the literal space (Algorithm 1: lines 4 & 12), we can change the representation of the training data in such a way that we can keep the growth of the training set within linear bounds with respect to the clause length <ref> (Pompe & Kononenko, 1994) </ref>. 2.3 Assessing the quality of literals Finally, we are going to describe the core of the ILP-R. If we look again at the Algorithm 1 it can be observed that the source of all effectiveness and efficiency is essentially the Select function. <p> Those two neighbours together with the given example are then used to compute the quality of the literal. It was shown in <ref> (Kononenko, 1994) </ref> that it is sometimes desirable to look at somewhat larger neighbourhood. Computing the quality statistics over the larger set of neighbours can improve its reliability which is important when learning in the presence of noise. <p> The details can be found in <ref> (Pompe & Kononenko, 1994) </ref>. 3 Classification with the induced hypothesis When we obtain the hypothesis we are still faced with the problem of how to get the most from it. <p> The tables were taken from <ref> (Kononenko et al., 1994) </ref>. The descriptions of individual systems can be found there as well.
Reference: <author> Kononenko, I. & Bratko, I. </author> <year> (1991). </year> <title> Information based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 67-80. </pages> <note> ILP-95, Leuven 15 Kononenko, </note> <author> I., Simec, E., & Robnik, M. </author> <year> (1994). </year> <title> Overcoming the myopia of inductive learning algorithms with RELIEF. </title> <type> Technical report, </type> <institution> University of Ljubljana, Faculty of electrical engineering and computer science. </institution>
Reference-contexts: Partly because we wanted to test the performance of our algorithm when dealing with similarly constraint language, but mostly because the recursion would not be feasible since target predicates had quite a large arity. Besides the classification accuracy, we measured also the average information score <ref> (Kononenko & Bratko, 1991) </ref>. This measure eliminates the influence of prior probabilities and appropriately treats probabilistic answers of a classifier.
Reference: <author> Lloyd, J. W. </author> <year> (1987). </year> <booktitle> Foundations of Logic Programming. </booktitle> <address> Berlin, Germany: </address> <publisher> Springer Verlag, second edition. </publisher>
Reference-contexts: In the following sections two different approaches are described. 3.1 Procedural classification Common way of using induced hypothesis is to run it through the Prolog interpreter and collect the results. What this effectively means is that we interpret the hypothesis in a procedural way. More formally <ref> (Lloyd, 1987) </ref>, given a hypothesis H, which is in our case a normal program without function symbols except for the constant, a background knowledge B and an unclassified example T , we are looking for the first computed answer fi c which we encounter during the SLDNF-resolution of the goal T
Reference: <editor> Muggleton, S., Ed. </editor> <booktitle> (1992). Inductive Logic Programming. </booktitle> <address> London, England: </address> <publisher> Academic Press. </publisher>
Reference-contexts: ILP-95, Leuven 11 4.3 Relational domains MESH (relational): We tested the performance of ILP-R on the problem of determining the number of elements for each of the edges of an object in the finite element mesh design problem <ref> (Dolsak & Muggleton, 1992) </ref>. There are five objects for which the human experts have constructed the appropriate meshes. In each of five experiments one object is used for testing and the other four for learning and the results are averaged.
Reference: <author> Pompe, U. & Kononenko, I. </author> <year> (1994). </year> <title> Linear space induction in first order logic with relief. </title> <booktitle> In R. </booktitle>
Reference-contexts: However, these measures are unable to detect strong dependencies among literals and are therefore unable to reveal a good hypothesis, when such dependencies exist. To attack the problem we developed an ILP learner called ILP-R <ref> (Pompe & Kononenko, 1994) </ref>. Its RELIEF (Kira & Rendell, 1992) based heuristic was shown to be very effective in estimating the quality of literals when they are strongly dependent. <p> As a result we get the conditional probabilities of classes given the new instance. The orthogonality of the approach separates the selection of the learner from the use of our naive classifier, giving us an extra degree of freedom. We have chosen ILP-R <ref> (Pompe & Kononenko, 1994) </ref> as our first order learner. Apart from being relatively efficient, its RELIEF (Kira & Rendell, 1992; Kononenko, 1994) based literal quality estimator is able to point out literal dependencies where greedy algorithms using myopic impurity functions (for instance information gain) would fail. <p> This can lead to an unmanageable exponential growth of the training set when there are non-determinate literals present in the background knowledge B. To eliminate this problem ILP-R uses a language bias based on the d-dependable language L d <ref> (Pompe & Kononenko, 1994) </ref>. In essence, this bias prohibits the cross-referencing of the variables in the variable dependency tree. This is a tree where each node denotes a set of variables introduced by some literal L 2 C and the edge from the predecessor node is marked with L. <p> such bias 1 during the initialization and updating of the literal space (Algorithm 1: lines 4 & 12), we can change the representation of the training data in such a way that we can keep the growth of the training set within linear bounds with respect to the clause length <ref> (Pompe & Kononenko, 1994) </ref>. 2.3 Assessing the quality of literals Finally, we are going to describe the core of the ILP-R. If we look again at the Algorithm 1 it can be observed that the source of all effectiveness and efficiency is essentially the Select function. <p> The details can be found in <ref> (Pompe & Kononenko, 1994) </ref>. 3 Classification with the induced hypothesis When we obtain the hypothesis we are still faced with the problem of how to get the most from it.
Reference: <author> Kruse, R. Viertl, & G. Della Riccia (Eds.), </author> <note> CISM Lecture notes, (to appear). </note> <institution> Udine, </institution> <address> Italy: </address> <publisher> Springer Verlag. </publisher> <address> Presented at: </address> <booktitle> International School for the Synthesis of Expert Knowledge. </booktitle>
Reference: <author> Pompe, U., Kovacic, M., & Kononenko, I. </author> <year> (1993). </year> <title> Sfoil: Stochastic approach to inductive logic programming. </title> <booktitle> In Proceedings of the Second Electrotechnical and Computer Science Conference ERK'93 (pp. </booktitle> <pages> 189-192). </pages> <address> Portoroz, Slovenia. </address>
Reference-contexts: The results reported by Dzeroski (1991) for various ILP systems are 12 % classification accuracy for FOIL, 22 % for mFOIL and 29 % for GOLEM and the result reported by <ref> (Pompe et al., 1993) </ref> is 28 % for SFOIL. The description of the MESH problem is appropriate for ILP systems. For attribute learners only relations with arity 1 (i.e. attributes) can be used to describe the problem.
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference-contexts: The Select function is heuristically guided. The details are left to the Section 2.3. First we are going to describe how the literal space is formed. 2.2 Language bias Internally, ILP-R uses FOIL-like representation of instances <ref> (Quinlan, 1990) </ref>. The training data are represented as a set of tuples holding variable bindings. If new variables are introduced by some literal L then tuples are expanded to reflect the newly available information.
Reference: <author> Quinlan, J. R. </author> <year> (1991). </year> <title> Determinate literals in inductive logic programming. </title> <booktitle> In Proceedings of International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 746-750). </pages> <address> Sydney, Australia. </address>
Reference-contexts: In addition, the system uses more extensive search algorithm when building a clause. Finally, it tries to alleviate the combinatorial explosion (space wise) when there are non-determinate literals <ref> (Quinlan, 1991) </ref> present in the background knowledge. To achieve such a goal, the system uses a language bias which enables it to keep the growth of the training set within the linear bounds with respect to the clause length.
References-found: 15

