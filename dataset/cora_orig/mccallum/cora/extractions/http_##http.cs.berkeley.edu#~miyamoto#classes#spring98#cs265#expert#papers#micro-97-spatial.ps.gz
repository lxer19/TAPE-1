URL: http://http.cs.berkeley.edu/~miyamoto/classes/spring98/cs265/expert/papers/micro-97-spatial.ps.gz
Refering-URL: http://http.cs.berkeley.edu/~miyamoto/classes/spring98/cs265/expert/index.html
Root-URL: http://www.cs.berkeley.edu
Email: ftjohnson,merten,hwug@crhc.uiuc.edu  
Title: Run-time Spatial Locality Detection and Optimization  
Author: Teresa L. Johnson Matthew C. Merten Wen-mei W. Hwu 
Address: IL 61801  
Affiliation: Center for Reliable and High-Performance Computing University of Illinois, Urbana-Champaign,  
Abstract: As the disparity between processor and main memory performance grows, the number of execution cycles spent waiting for memory accesses to complete also increases. As a result, latency hiding techniques are critical for improved application performance on future processors. We present a microarchitecture scheme which detects and adapts to varying spatial locality, dynamically adjusting the amount of data fetched on a cache miss. The Spatial Locality Detection Table, introduced in this paper, facilitates the detection of spatial locality across adjacent cached blocks. Results from detailed simulations of several integer programs show significant speedups. The improvements are due to the reduction of conflict and capacity misses by utilizing small blocks and small fetch sizes when spatial locality is absent, and the prefetching effect of large fetch sizes when spatial locality exists. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Boland and A. Dollas, </author> <title> "Predicting and precluding problems with memory latency," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 59-66, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: 1 Introduction This paper introduces an approach to solving the growing memory latency problem <ref> [1] </ref> by intelligently exploiting spatial locality. Spatial locality refers to the tendency for neighboring memory locations to be referenced close together in time. Traditionally there have been two main approaches used to exploit spatial locality. The first approach is to use larger cache blocks, which have a natural prefetching effect.
Reference: [2] <author> T. L. Johnson and W. W. Hwu, </author> <title> "Run-time adaptive cache hierarchy management via reference analysis," </title> <booktitle> in Proceedings of the 24th International Symposium on Computer Architecture, </booktitle> <pages> pp. 315-326, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: We introduce a new hardware mechanism called the Spatial Locality Detection Table (SLDT). Each SLDT entry tracks the accesses to multiple adjacent cache blocks, facilitating detection of spatial locality across those blocks while they are cached. The resulting information is later recorded in the Memory Address Table <ref> [2] </ref> for long-term tracking of larger regions called macroblocks. We show that these extensions to the cache microarchitecture significantly improve the performance of integer applications, achieving up to 17% and 26% improvements for 100 and 200-cycle memory latencies, respectively. This scheme is fully compatible with existing Instruction Set Architectures (ISA). <p> Also, dynamic schemes do not require profiling, which many users are unwilling to perform, or ISA changes. 4 Techniques 4.1 Overview of Prior Work In this section we briefly overview the concept of a mac-roblock, as well as the Memory Address Table (MAT), introduced in an earlier paper <ref> [2] </ref> and utilized in this work. We showed that cache bypassing decisions could be effectively made at run-time, based on the previous usage of the memory address being accessed. Other bypassing schemes include [19][20][16][21]. In particular, our scheme dynamically kept track of the accessing frequencies of memory regions called macroblocks. <p> To support dynamic bypassing decisions, each entry in the table contains a saturating counter, where the counter value represents the frequency of accesses to the corresponding mac-roblock. For details on the MAT bypassing scheme see <ref> [2] </ref>. Also introduced in that paper was an optimization geared towards improving the efficiency of L1 bypasses, by tracking the spatial locality of bypassed data using the MAT, and using that information to determine how much data to fetch on an L1 bypass. <p> We examined the performance improvement achieved by integrating our spatial locality optimizations with intelligent bypassing, using 8-bit access counters in each MAT entry <ref> [2] </ref>. The 4-way set-associative buffers used to hold the bypassed data at the L1 and L2 caches contain 128 8-byte entries and 512 32-byte entries, respectively. <p> Pcode is the only benchmark for which the performance degrades significantly when reducing the MAT size, however, 1K-entry MATs can still outperform the doubled caches. Comparing Figure 7 to the bypassing improvements in <ref> [2] </ref> shows that often significant improvements can be achieved by intelligently controlling the fetch sizes into the data caches and bypass buffers. It can be shown that our optimizations require 26% and 44% less tags and data than doubling the data caches at the L1 and L2 levels, respectively [18].
Reference: [3] <author> S. Przybylski, </author> <title> "The performance impact of block sizes and fetch strategies," </title> <booktitle> in Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pp. 160-169, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: general spatial locality issues; Section 4 discusses hardware techniques; Section 5 presents simulation results; and Section 6 concludes with future directions. 2 Related Work Several studies have examined the performance effects of cache block sizes <ref> [3] </ref>[4]. One of the studies allowed multiple consecutive blocks to be fetched with one request [3], and found that for data caches the optimal statically-determined fetch size was generally twice the block size. In this work we also examine fetch sizes larger than the block size, however, we allow the fetch size to vary based on the detected spatial locality.
Reference: [4] <author> A. J. Smith, </author> <title> "Line (block) size choice for cpu cache memories," </title> <journal> IEEE Transaction on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 1063-1075, </pages> <year> 1987. </year>
Reference: [5] <author> F. Dahlgren, M. Dubois, and P. Strenstrom, </author> <title> "Fixed and adaptive sequential prefetching in shared memory multiprocessors," </title> <booktitle> in Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pp. 56-63, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In this work we also examine fetch sizes larger than the block size, however, we allow the fetch size to vary based on the detected spatial locality. Another method allows the number of blocks fetched on a miss to vary across program execution, but not across different data <ref> [5] </ref>. Hardware [6][7][8][9][10] and software [11][12][13] prefetch-ing methods for uniprocessor machines have been proposed. However, many of these methods focus on prefetching regular array accesses within well-structured loops, which are access patterns primarily found in numeric codes.
Reference: [6] <author> A. J. Smith, </author> <title> "Cache memories," </title> <journal> Computing Surveys, </journal> <volume> vol. 14, no. 3, </volume> <pages> pp. 473-530, </pages> <year> 1982. </year>
Reference: [7] <author> N. P. Jouppi, </author> <title> "Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers," </title> <booktitle> in Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pp. 364-373, </pages> <month> June </month> <year> 1990. </year>
Reference: [8] <author> J.-L. Baer and T.-F. Chen, </author> <title> "An effective on-chip preload-ing scheme to reduce data access penalty," </title> <booktitle> in Proceeding of Supercomputing '91, </booktitle> <pages> pp. 176-186, </pages> <month> Nov. </month> <year> 1991. </year>
Reference: [9] <author> S. Mehrotra and L. Harrison, </author> <title> "Quantifying the performance potential of a data prefetch mechanism for pointer-intensive and numeric programs," </title> <type> Tech. Rep. 1458, </type> <institution> CSRD, Univ. of Illinois, </institution> <month> November </month> <year> 1995. </year>
Reference: [10] <author> J. W. C. Fu, J. H. Patel, and B. L. Janssens, </author> <title> "Stride directed prefetching in scalar processors," </title> <booktitle> in Proc. 25th Ann. Conference on Microprogramming and Microarchitectures, </booktitle> <month> Dec. </month> <year> 1992. </year>
Reference: [11] <author> A. K. Porterfield, </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, Houston, TX, </institution> <year> 1989. </year>
Reference: [12] <author> T. C. Mowry, M. S. Lam, and A. Gupta, </author> <title> "Design and evaluation of a compiler algorithm for prefetching," </title> <booktitle> in Proc. Fifth Int'l Conf. on Architectural Support for Prog. Lang. and Operating Systems., </booktitle> <pages> pp. 62-73, </pages> <month> Oct. </month> <year> 1992. </year>
Reference: [13] <author> W. Y. Chen, S. A. Mahlke, P. P. Chang, and W. W. Hwu, </author> <title> "Data access microarchitectures for superscalar processors with compiler-assisted data prefetching," </title> <booktitle> in Proceedings of the 24th Annual International Symposium on Microarchi-tecture, </booktitle> <pages> pp. 69-73, </pages> <month> November </month> <year> 1991. </year>
Reference: [14] <author> C.-K. Luk and T. C. Mowry, </author> <title> "Compiler-based prefetching for recursive data structures," </title> <booktitle> in Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 222-233, </pages> <month> September </month> <year> 1996. </year>
Reference: [15] <author> M. H. Lipasti, W. J. Schmidth, S. R. Kunkel, and R. R. Roediger, "SPAID: </author> <title> Software prefetching in pointer- and call-intensive environments," </title> <booktitle> in Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pp. 231-236, </pages> <month> December </month> <year> 1995. </year>
Reference: [16] <author> A. Gonzalez, C. Aliagas, and M. Valero, </author> <title> "A data cache with multiple caching strategies tuned to different types of locality," </title> <booktitle> in Proc. International Conference on Supercomputing, </booktitle> <pages> pp. 338-347, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Other methods geared towards integer codes [14][15] focus on compiler-inserted prefetching of pointer targets, and could be used in conjunction with our techniques. The dual data cache <ref> [16] </ref> attempts to intelligently exploit both spatial and temporal locality, however the temporal and spatial data must be placed in separate structures, and therefore the relative amounts of each type of data must be determined a priori.
Reference: [17] <author> V. Milutinovic, B. Markovic, M. Tomasevic, and M. Trem-blay, </author> <title> "The split temporal/spatial cache: Initial performance analysis," </title> <booktitle> in Proceedings of the SCIzzL-5, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: Also, the spatial locality detection method was tuned to numeric codes with constant stride vectors. In integer codes, the spatial locality patterns may not be as regular. The split temporal/spatial cache <ref> [17] </ref> is similar in structure to the dual data cache, however, the run-time locality detection mechanism is quite different than that of both the dual data cache and this paper. 3 Spatial Locality Caches seek to exploit the principle of locality.
Reference: [18] <author> T. L. Johnson, M. C. Merten, and W. W. Hwu, </author> <title> "Run-time spatial locality detection and optimization," </title> <type> Tech. Rep. </type> <institution> IMPACT-97-02, University of Illinois, Urbana, </institution> <address> IL (http://www.crhc.uiuc.edu/IMPACT/papers/tech.html), Sept. </address> <year> 1997. </year>
Reference-contexts: Traditionally, exploitation of spatial locality is achieved through either larger block sizes or prefetching of additional blocks. For a 32-byte cache block, we found that over half the time the extra data fetched into the cache simply wasted bus bandwidth and cache space <ref> [18] </ref>. Therefore, it would be beneficial to tune the amount of data fetched and cached on a miss to the spatial locality available in the data. <p> Also, we found routines in benchmarks such as SPEC92 gcc where the amount of spatial locality in data fetched by a particular load instruction varied depending on the function arguments <ref> [18] </ref>. As such, neither static analysis (if even possible) nor profiling will result in definitive or accurate spatial locality information for the load instructions. Dynamic analysis of the spatial locality in the data offers greater promise. <p> In practice this spatial locality optimization should be performed in combination with bypassing, in order to achieve the best possible performance, as well as to amortize the cost of the MAT hardware. The cost of the combined hardware is addressed elsewhere <ref> [18] </ref> due to space constraints. 4.2 Support for Varying Fetch Sizes The varying fetch size optimization could be supported using subblocks. <p> It also eliminates conflict misses resulting from accesses to different subblocks. However, this approach makes detection of spatial reuses much more difficult, as will be described in Section 4.3. Also, smaller block sizes increase the tag array cost <ref> [18] </ref>. In our scheme, the max fetch size data is always aligned to max fetch size boundaries. As a result, our techniques will fetch data on either side of the accessed element, depending on the location of the element within the max fetch size block. <p> It can be shown that our optimizations require 26% and 44% less tags and data than doubling the data caches at the L1 and L2 levels, respectively <ref> [18] </ref>. The cost of our optimizations includes both the tag and data costs of the reorganized data caches (which have larger tag costs than the base configuration), the SLDTs, the MATs, and the bypass buffers.
Reference: [19] <author> R. A. Sugumar and S. G. Abraham, </author> <title> "Efficient simulation of caches under optimal replacement with applications to miss characterization," </title> <type> Tech. Rep. </type> <institution> CSE-TR-143-92, Univ. of Michigan, </institution> <year> 1992. </year>
Reference: [20] <author> G. Tyson, M. Farrens, J. Matthews, and A. R. Pleszkun, </author> <title> "A modified approach to data cache management," </title> <booktitle> in Proceedings of the 28th Annual International Symposium on Mi-croarchitecture, </booktitle> <pages> pp. 93-103, </pages> <month> December </month> <year> 1995. </year>
Reference: [21] <author> J. A. Rivers and E. S. Davidson, </author> <title> "Reducing conflicts in direct-mapped caches with a temporality-based design," </title> <booktitle> in Proceedings of the 1996 International Conference on Parallel Processing, </booktitle> <pages> pp. 151-162, </pages> <month> August </month> <year> 1996. </year>
Reference: [22] <author> J. W. C. Fu and J. H. Patel, </author> <title> "Data prefetching in multiprocessor vector cache memories," </title> <booktitle> in Proc. 18th Ann. Int'l Symp. Computer Architecture, </booktitle> <pages> pp. 54-63, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Instead, we use a cache with small lines, equal to the smaller fetch size, and optionally fill in multiple, consecutive blocks when the larger fetch size is chosen. This approach is similar to that used in some prefetching strategies <ref> [22] </ref>. As a result, the cache can be fully utilized, even when the smaller sizes are fetched. It also eliminates conflict misses resulting from accesses to different subblocks. However, this approach makes detection of spatial reuses much more difficult, as will be described in Section 4.3.
Reference: [23] <author> P. P. Chang, S. A. Mahlke, W. Y. Chen, N. J. Warter, and W. W. Hwu, </author> <title> "IMPACT: An architectural framework for multiple-instruction-issue processors," </title> <booktitle> in Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pp. 266-275, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The last two benchmarks consist of modules from the IMPACT compiler <ref> [23] </ref> that we felt were representative of many real-world integer applications. Pcode, the front end of IMPACT, is run performing dependence analysis with the internal representation of the combine.c file from GNU CC as input. lmdes2 customizer, a machine description optimizer, is run optimizing the Su-perSPARC machine description. <p> These optimizations operate over linked list and complex data structures, and utilize hash tables for efficient access to the information. In order to provide a realistic evaluation of our technique for future high-performance, high-issue rate systems, we first optimized the code using the IMPACT compiler <ref> [23] </ref>. Classical optimizations were applied, then optimizations were performed which increase instruction level parallelism. The code was scheduled, register allocated and optimized for an eight-issue, scoreboarded, superscalar processor with register renaming. The ISA is an extension of the HP PA-RISC instruction set to support compile-time speculation.
Reference: [24] <author> J. W. C. Fu and J. H. Patel, </author> <title> "How to simulate 100 billion references cheaply," </title> <type> Tech. Rep. </type> <institution> CRHC-91-30, Center for Reliable and High-Performance Computing, University of Illi-nois, Urbana, IL, </institution> <year> 1991. </year>
Reference-contexts: The instruction latencies used are those of a Hewlett-Packard PA-RISC 7100. The base machine configuration is described in Table 2. Since simulating the entire applications at this level of detail would be impractical, uniform sampling is used to reduce simulation time <ref> [24] </ref>, however emulation is still performed between samples. The simulated samples are 200,000 instructions in length and are spaced evenly every 20,000,000 instructions, yielding a 1% sampling ratio. For smaller applications, the time between samples is reduced to maintain at least 50 samples (10,000,000 instructions).
References-found: 24

