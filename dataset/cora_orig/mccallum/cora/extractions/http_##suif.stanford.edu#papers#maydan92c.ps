URL: http://suif.stanford.edu/papers/maydan92c.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Title: ACCURATE ANALYSIS OF ARRAY REFERENCES  
Author: Dror Eliezer Maydan 
Degree: a dissertation submitted to the department of computer science and the committee on graduate studies of stanford university in partial fulfillment of the requirements for the degree of doctor of philosophy By  
Date: September 1992  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> H. Abelson, G. J. Sussman, and J. Sussman. </author> <title> Structure and Interpretation of Computer Programs. </title> <publisher> The MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: a [i + 10] = a [i]+3 end do do j = 1 to 20 end do collapse to this one: do i = 1 to 10 end do 4 Memoization is a technique to remember the results of previous computations, previously used to implement call-by-need arguments in LISP compilers <ref> [1] </ref>. CHAPTER 2. AFFINE MEMORY DISAMBIGUATION 31 Two cases that before appeared to be different, now are identical. In Table 2 we show the results of both our simple scheme and our improved one applied to the PERFECT Club.
Reference: [2] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Given a use of x, data-flow analyzers do not care about all the definitions to x, they care about the reaching definitions of x, those definitions that produce a value for x <ref> [2] </ref>. Similarly, we showed in the last chapter that memory disambiguation is not sufficient for parallelization. We need to extend our analysis to perform data-flow analysis on individual array elements, what we call data-flow dependence analysis.
Reference: [3] <author> R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <institution> Technical Report Rice COMP TR86-42, Department of Computer Science, Rice University, </institution> <month> Nov </month> <year> 1986. </year>
Reference-contexts: Non-scientific codes do not exhibit much loop level parallelism, and we feel that library routines tend to be simpler than full programs. Table 1 shows how many times each test was called per program. The first column represents array constants, for example a <ref> [3] </ref> versus a [4]. These cases are handled without dependence testing. We merely include them to show that their frequency can skew statistics if we apply general dependence routines to them. The second column represents the cases where GCD returns independent. <p> We then return the set union of all the data-flow dependence vectors calculated at each leaf. The concept of dependence levels can be used to optimize our algorithm for calculating data-flow dependence vectors. We define the level of a dependence as follows <ref> [3] </ref>. <p> We say that such a write does not "self-interfere". For example: do i = 11 to 20 end do a <ref> [3] </ref> = In the first loop, we are writing to different locations in each iteration so the write statement does not self-interfere, but in the second loop, we are writing to the same location, a [3], in each iteration so the write statement does self-interfere. <p> For example: do i = 11 to 20 end do a <ref> [3] </ref> = In the first loop, we are writing to different locations in each iteration so the write statement does not self-interfere, but in the second loop, we are writing to the same location, a [3], in each iteration so the write statement does self-interfere. A write that never self-interferes will write at most one value into any location.
Reference: [4] <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> Oct </month> <year> 1987. </year>
Reference-contexts: Non-scientific codes do not exhibit much loop level parallelism, and we feel that library routines tend to be simpler than full programs. Table 1 shows how many times each test was called per program. The first column represents array constants, for example a [3] versus a <ref> [4] </ref>. These cases are handled without dependence testing. We merely include them to show that their frequency can skew statistics if we apply general dependence routines to them. The second column represents the cases where GCD returns independent. For these cases, we do not need to call the exact routines. <p> For example, the following code passes an array starting at the fourth element of a to the subroutine. program prog dimension a (m) ... call fred (a <ref> [4] </ref>) end subroutine fred (a) dimension a (m) ... return end To summarize the main program, we must calculate the union of all its references to a with all of the subroutine's references to a.
Reference: [5] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with do loops. </title> <booktitle> In Proceedings of the ACM SIGPLAN PPOPP, </booktitle> <year> 1991. </year>
Reference-contexts: If the write loop is degenerate, every read will read an uninitialized value, and the correct LWT will be ?. Ancourt has developed an algorithm for loops with affine loop bounds based on Fourier-Motzkin elimination that can be used to detect possibly degenerate loops <ref> [5] </ref>. For full degeneracies, we can use the algorithm to derive the conditions under which a degeneracy will not occur, i.e. n 1 in the above example. We merely insert these conditions at the top of our LWT.
Reference: [6] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data acess and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN PLDI, </booktitle> <pages> pages 41-53, </pages> <year> 1989. </year>
Reference-contexts: In this variation of RSDs, each dimension of each array is summarized separately. Each dimension is represented by either a constant symbolic expression; a range consisting of a lower bound, upper bound and step size; or ?, signifying that the entire dimension is accessed. Balasundaram advocates simple sections <ref> [6] </ref>. A simple section is a convex polytope with simple boundaries. A simple boundary is a hyperplane of the form x i = c or x i x j = c.
Reference: [7] <author> U. Banerjee. </author> <title> Speedup of Ordinary Programs. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: If after going through all the constraints, lb i &gt; ub i for any variable t i , the system is inconsistent. Otherwise it is consistent. This test is a superset of the well-known single loop, single dimension exact test <ref> [7] </ref>. It is quite clear that with one loop and single-dimensional arrays one cannot get constraints with more than one free variable.
Reference: [8] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: We describe them in the order in which they are applied by our system. 2.3.1 Extended GCD Test We use the Extended GCD Test <ref> [8] </ref> as a preprocessing step for our other tests. While the test itself is not exact, it allows us to transform our problem into a simpler and smaller CHAPTER 2. AFFINE MEMORY DISAMBIGUATION 17 form, increasing the applicability of our other tests. <p> the Single Variable Per Constraint Test, only applies in cases where each constraint has at most one variable. 2.3.2 Single Variable Per Constraint Test (SVPC) Banerjee shows that if the solution to the generalized GCD Test has at most 1 free variable then one can solve the exact problem easily <ref> [8] </ref>. Each constraint is merely an CHAPTER 2. AFFINE MEMORY DISAMBIGUATION 19 upper or lower bound for the free variable. One merely goes through each constraint calculating the appropriate lower or upper bound and storing the best ones found. <p> Unfortunately it is very difficult to define such a model since the amount of parallelism depends on the optimizations performed. Nonetheless, to give some comparison, we implemented two standard algorithms: the Simple GCD Test (algorithm 5.4.1 in <ref> [8] </ref>) and the Trapezoidal Banerjee Test (algorithm 4.3.1 in [8]). Not computing direction vectors, these algorithms found 415 out of 482 independent pairs, missing 16%. For direction vectors, we used the Simple GCD Test followed by Wolfe's extension to Banerjee's rectangular test (2.5.2 in [53]). <p> Unfortunately it is very difficult to define such a model since the amount of parallelism depends on the optimizations performed. Nonetheless, to give some comparison, we implemented two standard algorithms: the Simple GCD Test (algorithm 5.4.1 in <ref> [8] </ref>) and the Trapezoidal Banerjee Test (algorithm 4.3.1 in [8]). Not computing direction vectors, these algorithms found 415 out of 482 independent pairs, missing 16%. For direction vectors, we used the Simple GCD Test followed by Wolfe's extension to Banerjee's rectangular test (2.5.2 in [53]). <p> The Smith normal form [44] can be used to solve F w ~ i w = F r ~ i r in the general case. In a process similar to the extended GCD Test <ref> [8] </ref>, which is similar to an integral version of LU decomposition, we can find matrices P , Q and S such that P F w Q = S, P and Q are unimodular matrices and S is of CHAPTER 4.
Reference: [9] <author> T. Brandes. </author> <title> The importance of direct dependences for automatic parallelism. </title> <booktitle> In Proceedings of International Conference on Supercomputing, 1988. </booktitle> <volume> 129 BIBLIOGRAPHY 130 </volume>
Reference-contexts: Since, we know when we are exact, we can also always use Feautrier's algorithm as a backup in the rare cases when we fail. Several other researchers have worked on similar problems. Brandes <ref> [9] </ref>, Ribas [42] and Pugh and Wonnacott [41] take similar approaches to us in that they extend memory disambiguation techniques to compute data-flow dependence information. Brandes's approach does not apply if dependence distances are coupled or if the loop is non-rectangular. Ribas only discusses constant-distance dependences in perfectly nested loops.
Reference: [10] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 162-175, </pages> <year> 1986. </year>
Reference-contexts: Each direction vector is a set of simple linear constraints on the loop indices. We simply add these constraints to our system and solve as before. The standard approach, based on Burke and Cytron <ref> [10] </ref>, uses a hierarchical system to avoid testing all possible direction vectors. Rather than testing each possible vector it first tests (fl; fl; . . . ; fl). Recall that fl represents a dependence with any direction. <p> This is exactly the same formulation as for the affine memory disambiguation problem. We can solve for the data-flow vectors in the same way that dependence analysis solves for direction vectors using a variation of Burke and Cytron's method <ref> [10] </ref>. We then return the set union of all the data-flow dependence vectors calculated at each leaf. The concept of dependence levels can be used to optimize our algorithm for calculating data-flow dependence vectors. We define the level of a dependence as follows [3]. <p> If a loop is not parallelized, it is not possible to know if the loop is inherently serial or if the summarizer was too inaccurate. As an alternative approach, Burke and Cytron keep a list of all the different array accesses <ref> [10] </ref>. They do not find the union of different accesses to the same array. While possibly as accurate as inlining, this approach could be just as inefficient. The number of array references should grow just as the object size grows with inlining. CHAPTER 5.
Reference: [11] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedureal side effects in a parallel programming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference: [12] <author> G. Dantzig and B. C. Eaves. </author> <title> Fourier-motzkin elimination and its dual. </title> <journal> Journal of Combinatorial Theory, </journal> <volume> A(14):288-297, </volume> <year> 1973. </year>
Reference-contexts: Therefore the system is inconsistent. 3 For clarity we use the term Simple Loop Residue Test to refer to the Loop Residue Test without Shostak's extensions. CHAPTER 2. AFFINE MEMORY DISAMBIGUATION 26 2.3.5 Fourier-Motzkin Test Our last algorithm, Fourier-Motzkin <ref> [12] </ref>, is a backup inexact test. It solves the general non-integer linear programming case exactly. If it returns independent, we know that the integer case is also independent. If it returns dependent, we can use it to return a sample solution. <p> . ., 0 F r (~x 2;...;n ) This system has a solution iff 9~x 2;...;n such that D i (~x 2;...;n ) E j (~x 2;...;n ), i = (1; . . . ; p), j = (1; . . . ; q) A proof can be found in <ref> [12] </ref>. Thus, one can eliminate one variable at a time until there are none left. At each step, the number of constraints grows by pq p q. While this can lead to exponential behavior in the worst case, when p and q are small, each step might actually eliminate constraints.
Reference: [13] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four perfect-benchmark programs. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug </month> <year> 1991. </year>
Reference-contexts: Given a use of an array element, we do not want to compute all the definitions to the same array element location, only the reaching definitions. Other researchers have come to similar conclusions. Eigenmann et al. <ref> [13] </ref> discuss the techniques used to hand-parallelize four of the PERFECT Club programs. They show that commercial parallelizers are not powerful enough to parallelize these real programs. In particular, they find array privatization to be useful in all of the programs. <p> DATA-FLOW DEPENDENCE ANALYSIS 88 or of a dependence constraint with a copy of T . 4.6 Array Privatization: An Optimization We will demonstrate the necessity of more powerful data dependence representations with the array privatization example. As we have mentioned, recent research by Eigenmann et al. <ref> [13] </ref>, by Singh and Hennessy [47], and by us [38] has shown that array privatization is a critical transformation in the suite of parallelizing transformations. To privatize arrays, traditional data dependence techniques are not sufficient; data-flow dependence vectors are required.
Reference: [14] <author> M. Berry et al. </author> <title> The PERFECT Club benchmarks: effective performance evaluation of supercomputers. </title> <type> Technical Report UIUCSRD Rep. No. 827, </type> <institution> University of Illinois Urbana-Champaign, </institution> <year> 1989. </year>
Reference-contexts: If the input is not of the appropriate form for an algorithm, then we try the next one. Using a series of tests allows us to be exact for a wider range of inputs. We evaluated our algorithms on the PERFECT Club <ref> [14] </ref>, a set of 13 scientific benchmarks, and found the algorithms to be exact in every case. Cascading exact tests can also be much more efficient than cascading inexact ones. <p> We then ran them on the PERFECT Club benchmarks. These are a set of 13 scientific Fortran programs ranging in size from 500 to 18,000 lines collected at the University of Illinois at Urbana-Champaign <ref> [14] </ref>. We feel that these are a fair set of benchmarks. Non-scientific codes do not exhibit much loop level parallelism, and we feel that library routines tend to be simpler than full programs. Table 1 shows how many times each test was called per program.
Reference: [15] <author> P. Feautrier. </author> <title> Array expansion. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 429-442, </pages> <month> Jul </month> <year> 1988. </year>
Reference: [16] <author> P. Feautrier. </author> <title> Parametric integer programming. </title> <type> Technical Report 209, </type> <institution> Laboratoire Methodologie and Architecture Des Systemes Informatiques, </institution> <month> Jan </month> <year> 1988. </year>
Reference: [17] <author> P. Feautrier. </author> <title> Dataflow analysis of array and scalar references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(1) </volume> <pages> 23-52, </pages> <month> Feb </month> <year> 1991. </year>
Reference-contexts: Each parametric integer programming problem itself requires solving a large number of integer programming problems. As an example, Feautrier timed his algorithm on the following test input case <ref> [17] </ref>. do i = 1 to l do k = 2 to n 1 end do end do Feautrier found that his algorithm takes 1.7 seconds to solve this input on a low end SPARC [17]. <p> As an example, Feautrier timed his algorithm on the following test input case <ref> [17] </ref>. do i = 1 to l do k = 2 to n 1 end do end do Feautrier found that his algorithm takes 1.7 seconds to solve this input on a low end SPARC [17]. We have also implemented Feautrier's algorithms; we find them complex and obtain similar timing figures. We believe that the algorithm is much too inefficient for use on real programs. Our algorithm described below is designed to exploit the inherent simplicity in real code.
Reference: [18] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN PLDI, </booktitle> <pages> pages 15-29, </pages> <year> 1991. </year>
Reference-contexts: Some work has been done on algorithms that are guaranteed to be exact for special case inputs. Simple loop residue [46], Li and Yew's work [31], the I Test [25] and the CHAPTER 2. AFFINE MEMORY DISAMBIGUATION 16 Delta Test <ref> [18] </ref> fall into this category. Little work has been done to analyze either the accuracy or the efficiency of these algorithms in practice. None of these algorithms, as far as we know, has been definitively shown to be both "accurate enough" and "efficient enough".
Reference: [19] <author> E.D. Granston and A.V. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 854-865, </pages> <year> 1991. </year>
Reference: [20] <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <year> 1990. </year> <note> BIBLIOGRAPHY 131 </note>
Reference: [21] <author> M. R. Haghighat. </author> <title> Symbolic dependence analysis for high performance parallelizing compilers. </title> <booktitle> In 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1990. </year>
Reference-contexts: We use a scalar prepass to discover which variables can be expressed as affine functions of the induction variables, and we treat any other variables as unbound induction variables. Our treatment of symbolic variables is less general than advocated by Haghighat <ref> [21] </ref> and by Lichnewsky and Thomasset [33]. Both groups advocate finding the convex hull of all scalar constraints interprocedurally and annotating the array references with these constraints. They then use these annotations to help prove independence. This approach enables them to solve more systems exactly than our approach.
Reference: [22] <author> M. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> Apr </month> <year> 1991. </year>
Reference-contexts: Our experiment may slightly exaggerate the effects of inlining. Hall argues that scalar optimizers reduce the size of inlined object code by eliminating dead code and creating more specialized code <ref> [22] </ref>. She compares how much optimizers reduce object code size for inlined programs versus uninlined program. In her best examples, the optimizer is able to shrink inlined code by about 10% more than it was able to shrink uninlined code. <p> Thus all practical inliners must be restrictive and not inline every call. Hall utilizes two approaches <ref> [22] </ref>. For certain optimizations she restricts inlining based on the size of the code; large subroutines are not inlined. For other optimizations, she uses goal directed inlining; she attempts to inline sites that are likely to aid further optimization. Both these approaches might be very reasonable. <p> In general, anything short of full inlining may be insufficient if we want to support every possible optimization. We see two possible approaches. Hall develops interprocedural loop transformations such as loop embedding that move loops across subroutine calls <ref> [22] </ref>. Once all the loops are moved, further loop transformations can be performed intraprocedurally. Alternatively, current loop transformation techniques tend to apply only in situations that are much more restrictive than simple parallelization. CHAPTER 5.
Reference: [23] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> Jul </month> <year> 1991. </year>
Reference: [24] <author> R. Kannan. </author> <title> Minkowski's convex body theorem and integer programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 415-440, </pages> <month> Aug </month> <year> 1987. </year>
Reference-contexts: The size of the coefficients, though, could conceivably be very large. The complexity of most common algorithms (branch and bound, cutting plane) depend on the size of the coefficients in the worst case. Lenstra [29] and Kannan <ref> [24] </ref> have developed algorithms that do not depend on the coefficients, but in the worst case, Kannan's algorithm is O (n O (n) ) where n is the number of variables.
Reference: [25] <author> X. Kong, D. Klappholz, and K. Psarris. </author> <title> The i test: an improved dependence test for automatic parallelization and vectorization. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 342-349, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: Li, Yew and Zhu, for example, consider Fourier-Motzkin to be too expensive [32]. Some work has been done on algorithms that are guaranteed to be exact for special case inputs. Simple loop residue [46], Li and Yew's work [31], the I Test <ref> [25] </ref> and the CHAPTER 2. AFFINE MEMORY DISAMBIGUATION 16 Delta Test [18] fall into this category. Little work has been done to analyze either the accuracy or the efficiency of these algorithms in practice.
Reference: [26] <author> D. Kuck. </author> <title> The Structure of Computers and Computations, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <year> 1978. </year>
Reference: [27] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M.J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Eigth ACM Symposium on the Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1981. </year>
Reference: [28] <author> James R. Larus. </author> <title> Estimating the potential parallelism in programs. </title> <editor> In Aland David Padua, editor, </editor> <booktitle> Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, chapter 17, </booktitle> <pages> pages 331-349. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Solving this problem exactly allows us to judge how effectively affine memory disambiguation approximates data dependence analysis. In Chapter 3, we develop a method for judging the effectiveness of this approximation. We use the llpp system developed by Larus <ref> [28] </ref>, a dynamic trace-based system, to find all the intraprocedural data dependences dynamically. By comparing the results of our system to this dynamic system, we show that while the affine approximation is reasonable, memory disambiguation is not. <p> The question remains how accurately this domain approximates the full data dependence problem. Solving the affine memory disambiguation problem exactly allows us to judge the effectiveness of the domain restriction. Using a dynamic, trace-based system developed by Larus <ref> [28] </ref>, we can find a solution to the full data dependence problem for a given input set. By comparing our exact affine memory disambiguator to Larus's system, we can discover the limitations of the domain. We show that while the affine approximation is reasonable, the memory disambiguation approximation is not. <p> It solves exactly the affine memory disambiguation problem with an extension for symbolic analysis. We use llpp, a system developed by Larus, to dynamically find all the loop-carried 44 CHAPTER 3. EFFECTIVENESS OF AFFINE MEMORY DISAMBIGUATION 45 true data-flow dependences in a program for a given input set <ref> [28] </ref>. Recall that a loop-carried true data-flow dependence occurs when we read a value in iteration i r that was written in an earlier iteration i w . Larus developed his system to dynamically estimate the amount of loop level parallelism in a program.
Reference: [29] <author> H.W. Lenstra. </author> <title> Integer programming with a fixed number of variables. </title> <journal> Mathematics of Operations Research, </journal> <volume> 8(4) </volume> <pages> 538-548, </pages> <year> 1983. </year>
Reference-contexts: Therefore being exponential in the number of variables and constraints could be acceptable. The size of the coefficients, though, could conceivably be very large. The complexity of most common algorithms (branch and bound, cutting plane) depend on the size of the coefficients in the worst case. Lenstra <ref> [29] </ref> and Kannan [24] have developed algorithms that do not depend on the coefficients, but in the worst case, Kannan's algorithm is O (n O (n) ) where n is the number of variables.
Reference: [30] <author> Z. Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proceedings of the International Conference on Supercomputing (ACM), </booktitle> <address> Washington, D.C., </address> <year> 1992. </year> <note> BIBLIOGRAPHY 132 </note>
Reference-contexts: If the summary representation is not detailed enough to describe the fact that each write only writes every other location, then parallelization would not be possible. CHAPTER 4. DATA-FLOW DEPENDENCE ANALYSIS 93 Li has used a similar approach in developing an algorithm specifically for the array privatization problem <ref> [30] </ref>. His algorithm handles more general control flow than ours but less general array references. We believe that the best solution is to develop an integrated framework that combines our approach with summary based approaches.
Reference: [31] <author> Z. Li and P. Yew. </author> <title> Practical methods for exact data dependency analysis. </title> <booktitle> In Proceedings of the Second Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1989. </year>
Reference-contexts: Li, Yew and Zhu, for example, consider Fourier-Motzkin to be too expensive [32]. Some work has been done on algorithms that are guaranteed to be exact for special case inputs. Simple loop residue [46], Li and Yew's work <ref> [31] </ref>, the I Test [25] and the CHAPTER 2. AFFINE MEMORY DISAMBIGUATION 16 Delta Test [18] fall into this category. Little work has been done to analyze either the accuracy or the efficiency of these algorithms in practice.
Reference: [32] <author> Z. Li, P. Yew, and C. Zhu. </author> <title> An efficient data dependence analysis for parallelizing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 26-34, </pages> <month> Jan </month> <year> 1990. </year>
Reference-contexts: Wallace has developed a variation of simplex extended to look for integer solutions [52]. To guarantee termination, it must assume dependence after a certain number of iterations. The Lambda Test extends Banerjee's inequalities to better deal with coupled subscripts <ref> [32] </ref>. The Power Test [54] combines the GCD Test with Fourier-Motzkin. While some of these algorithms may be accurate in most cases, they all have the problem that their approximations are implicit. If such an algorithm returns dependent, we do not know if an approximation was made. <p> If such an algorithm returns dependent, we do not know if an approximation was made. In addition, there is evidence that some of these algorithms are too inefficient. Li, Yew and Zhu, for example, consider Fourier-Motzkin to be too expensive <ref> [32] </ref>. Some work has been done on algorithms that are guaranteed to be exact for special case inputs. Simple loop residue [46], Li and Yew's work [31], the I Test [25] and the CHAPTER 2. AFFINE MEMORY DISAMBIGUATION 16 Delta Test [18] fall into this category. <p> The cost of this algorithm is a matter of debate. Theoretically it can be exponential. Experimentally, Triolet has implemented this approach and seems to be satisfied with its efficiency [51], but Li, Yew and Zhu consider Triolet's numbers to be too expensive <ref> [32] </ref>. Nonetheless, we are required to call this algorithm so few times that its accrued expense is very reasonable. In the first step, we eliminate the first variable, x 1 , from the set of constraints. <p> We have argued that using special case exact tests is inherently efficient since most problems can be solved using only one test. We can further improve the efficiency of our approach. It has been said that dependence testing is performing a large number of tests on relatively small inputs <ref> [32] </ref>. We show that in actuality it is CHAPTER 2. <p> We chose our other tests because of their efficiency, applicability and compatibility. They all expect their data in the same form: A~x ~ b . Thus there is no need to convert data from one form to another. Some tests, like the Lambda Test <ref> [32] </ref> expect their data in a different form. All the tests succeed in finding independent references in practice. We checked how many times each test returned independent (counting each direction vector tested) for the tests in Table 5.
Reference: [33] <author> A. Lichnewsky and F. Thomasset. </author> <title> Introducing symbolic problem solving techniques in dependence testing phases of a vectorizer. </title> <booktitle> In Proceedings of Supercomputing 88, </booktitle> <pages> pages 396-406, </pages> <year> 1988. </year>
Reference-contexts: We use a scalar prepass to discover which variables can be expressed as affine functions of the induction variables, and we treat any other variables as unbound induction variables. Our treatment of symbolic variables is less general than advocated by Haghighat [21] and by Lichnewsky and Thomasset <ref> [33] </ref>. Both groups advocate finding the convex hull of all scalar constraints interprocedurally and annotating the array references with these constraints. They then use these annotations to help prove independence. This approach enables them to solve more systems exactly than our approach.
Reference: [34] <author> V. Maslov. Delinearization: </author> <title> an efficient way to break multiloop dependence equations. </title> <booktitle> In Proceedings of the SIGPLAN PLDI, </booktitle> <pages> pages 152-161, </pages> <year> 1992. </year>
Reference-contexts: Thus, as long as abs (j w j r + c c 2 ) &lt; n for all j w ; j r , we can safely delinearize the two references. Maslov has independently developed a more formal algorithm for delinearization <ref> [34] </ref>. His algorithm is more detailed and little bit more general than ours. Other examples, which at first do not appear to be linearized, are equivalent to our linearized example.
Reference: [35] <author> D.E. Maydan, S. P. Amarasinghe, and M.S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Conference Record of 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 283-292, </pages> <year> 1992. </year>
Reference-contexts: We therefore close this chapter by presenting an algorithm for array privatization. A description of the algorithm is available in [36]. A description of how data-flow dependence analysis fits in with more standard data dependence information can be found in <ref> [35] </ref>. 4.1 Data-flow Dependence Vectors Distance and direction vectors provide us with a representation to summarize a set of dependent iteration pairs.
Reference: [36] <author> D.E. Maydan, S. P. Amarasinghe, and M.S. Lam. </author> <title> Array data flow analysis and its application in array privatization. </title> <note> In to appear in Proceedings of ACM POPL, </note> <year> 1993. </year>
Reference-contexts: In order to utilize the data-flow dependence information to parallelize loops, we must privatize arrays. We therefore close this chapter by presenting an algorithm for array privatization. A description of the algorithm is available in <ref> [36] </ref>. A description of how data-flow dependence analysis fits in with more standard data dependence information can be found in [35]. 4.1 Data-flow Dependence Vectors Distance and direction vectors provide us with a representation to summarize a set of dependent iteration pairs. <p> Our intention is to illustrate the necessity and sufficiency of data-flow dependence analysis for array privatization in this domain. A more detailed and formal description of our algorithm is found in <ref> [36] </ref>. Consider the example in Figure 13 extracted from the PERFECT Club benchmark program OCS. We will use this example throughout this section to illustrate the array pri-vatization problem. Every iteration of the outer loop reads and writes the same elements of the array work.
Reference: [37] <author> D.E. Maydan, J.L. Hennessy, and M.S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the SIGPLAN PLDI, </booktitle> <pages> pages 1-14, </pages> <year> 1991. </year>
Reference-contexts: Traditional approaches restrict the data dependence domain to the simpler, decidable, problem of affine memory disambiguation [8][53]. Pairs of references that cannot be proved independent in this domain are assumed dependent. In this chapter, we present an approach to solving the affine memory disambiguation problem <ref> [37] </ref>. While in the worst case affine memory disambiguation is too expensive to solve exactly, we show that in the cases seen in practice we can be both exact and efficient.
Reference: [38] <author> D.E. Maydan, J.L. Hennessy, and M.S. Lam. </author> <title> Effectiveness of data dependence analysis. </title> <booktitle> In Proceedings of the NSF-NCRD Workshop on Advanced Compilation Techniques for Novel Architectures, </booktitle> <year> 1992. </year>
Reference-contexts: By comparing our exact affine memory disambiguator to Larus's system, we can discover the limitations of the domain. We show that while the affine approximation is reasonable, the memory disambiguation approximation is not. This experiment was first described in <ref> [38] </ref>. 3.1 Experimental System In the last chapter we described our dependence analysis system. It solves exactly the affine memory disambiguation problem with an extension for symbolic analysis. We use llpp, a system developed by Larus, to dynamically find all the loop-carried 44 CHAPTER 3. <p> As we have mentioned, recent research by Eigenmann et al. [13], by Singh and Hennessy [47], and by us <ref> [38] </ref> has shown that array privatization is a critical transformation in the suite of parallelizing transformations. To privatize arrays, traditional data dependence techniques are not sufficient; data-flow dependence vectors are required.
Reference: [39] <author> V.R. Pratt. </author> <title> Two easy theories whose combination is hard. </title> <type> Technical report, </type> <institution> Mass Institue of Technology, </institution> <month> Sept </month> <year> 1977. </year>
Reference-contexts: Pratt developed a simple algorithm that can be used for data dependence testing that works when all constraints are of the form t i t j + c <ref> [39] </ref>. One creates a graph with a node for each variable. For this inequality, we place a directed arc with value c from node t i to node t j . Assume we have another constraint t j t k + d.
Reference: [40] <author> W. Pugh. </author> <title> The omega test: a fast and practical integer programming algorithm for dependence analysis. </title> <booktitle> In Supercomputing '91, </booktitle> <year> 1991. </year> <note> BIBLIOGRAPHY 133 </note>
Reference-contexts: Nonetheless, these types of examples are fairly rare and the extended GCD Test should be sufficient for the common constant-distance cases. Pugh has developed a more accurate method for computing distance vectors <ref> [40] </ref>. For each pair of loop indices, (i j ,i 0 j ), he adds to his system of constraints a new variable d j and the constraint that d j = i 0 j i j . <p> It is quite possible that other tests could be added and some eliminated without significantly changing our results. In fact, since we first presented these results, Pugh has come up with an alternative exact approach <ref> [40] </ref>. He uses a variation of the Extended GCD Test followed by an extension to the Fourier-Motzkin algorithm. His approach may be a little more expensive than ours. <p> He claims that his test requires O (CV 2 ) worst case time for the common cases when the SVPC Test applies, while we have shown that the SVPC Test requires O (C + V ) time. Pugh does not believe in memoization <ref> [40] </ref>. He makes the incorrect claim that the cost of a memoization hit would be about 2 to 4 times the scanning, or copying, cost of a problem. We have shown that in fact the hit cost could be less than the scanning cost. <p> For these rather uncommon cases, we can use a memory disambiguator to determine the unique t values <ref> [40] </ref>. Note that degenerate loops are not a problem for this case.
Reference: [41] <author> W. Pugh and D. Wonnacott. </author> <title> Eliminating false data dependences using the omega test. </title> <booktitle> In Proceedings of the SIGPLAN PLDI, </booktitle> <pages> pages 140-151, </pages> <year> 1992. </year>
Reference-contexts: Since, we know when we are exact, we can also always use Feautrier's algorithm as a backup in the rare cases when we fail. Several other researchers have worked on similar problems. Brandes [9], Ribas [42] and Pugh and Wonnacott <ref> [41] </ref> take similar approaches to us in that they extend memory disambiguation techniques to compute data-flow dependence information. Brandes's approach does not apply if dependence distances are coupled or if the loop is non-rectangular. Ribas only discusses constant-distance dependences in perfectly nested loops.
Reference: [42] <author> H. Ribas. </author> <title> Obtaining dependence vectors for nested-loop computations. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1990. </year>
Reference-contexts: Since, we know when we are exact, we can also always use Feautrier's algorithm as a backup in the rare cases when we fail. Several other researchers have worked on similar problems. Brandes [9], Ribas <ref> [42] </ref> and Pugh and Wonnacott [41] take similar approaches to us in that they extend memory disambiguation techniques to compute data-flow dependence information. Brandes's approach does not apply if dependence distances are coupled or if the loop is non-rectangular. Ribas only discusses constant-distance dependences in perfectly nested loops.
Reference: [43] <author> C. Rosend. </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1990. </year>
Reference: [44] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley & Sons, </publisher> <year> 1986. </year>
Reference-contexts: Unfortunately, affine memory disambiguation in general is exactly equivalent to integer programming, a well-studied problem. The standard integer programming problem <ref> [44] </ref> is to find max ~x such that A~x &lt; ~ b , ~x integral (3) It is clear that (2) is a special case of (3) so affine memory disambiguation can be reduced to integer programming. Another polynomially equivalent version of integer programming is [44] 9 ~x such that A~x <p> The standard integer programming problem <ref> [44] </ref> is to find max ~x such that A~x &lt; ~ b , ~x integral (3) It is clear that (2) is a special case of (3) so affine memory disambiguation can be reduced to integer programming. Another polynomially equivalent version of integer programming is [44] 9 ~x such that A~x = ~ b , ~x 0, ~x integral (4) If A is an m fi n matrix, one can reduce (4) to affine memory disambiguation by constructing the following program: do x 1 = 0 to unknown . . . do x n = 0 <p> In the general case, we are interested in integral solutions instead of real ones, and the matrix F w is not always invertible. The Smith normal form <ref> [44] </ref> can be used to solve F w ~ i w = F r ~ i r in the general case.
Reference: [45] <author> Z. Shen, Z. Li, and P. Yew. </author> <title> An empirical study on array subscripts and data dependencies. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, pages II-145 to II-152, </booktitle> <year> 1989. </year>
Reference-contexts: None of these algorithms, as far as we know, has been definitively shown to be both "accurate enough" and "efficient enough". In fact, Shen, Li and Yew found that cases such as coupled subscripts appear frequently and cannot be analyzed accurately using traditional algorithms such as Banerjee's <ref> [45] </ref>. Our approach is to use a series of special case exact tests. If the input is not of the appropriate form for an algorithm, then we try the next one. Using a series of tests allows us to be exact for a wider range of inputs.
Reference: [46] <author> R. Shostak. </author> <title> Deciding linear inequalities by computing loop residues. </title> <journal> ACM Journal, </journal> <volume> 28(4) </volume> <pages> 769-779, </pages> <month> Oct </month> <year> 1981. </year>
Reference-contexts: In addition, there is evidence that some of these algorithms are too inefficient. Li, Yew and Zhu, for example, consider Fourier-Motzkin to be too expensive [32]. Some work has been done on algorithms that are guaranteed to be exact for special case inputs. Simple loop residue <ref> [46] </ref>, Li and Yew's work [31], the I Test [25] and the CHAPTER 2. AFFINE MEMORY DISAMBIGUATION 16 Delta Test [18] fall into this category. Little work has been done to analyze either the accuracy or the efficiency of these algorithms in practice. <p> Since our graph has one vertex for every variable and one edge for every constraint, the complexity of the Simple Loop Residue Test is O (CV ). CHAPTER 2. AFFINE MEMORY DISAMBIGUATION 25 Shostak <ref> [46] </ref> extends this algorithm first to deal with inequalities of the form at i bt j + c and then to handle cases with more than two variables. 3 Unfortunately these extensions make the algorithm inexact.
Reference: [47] <author> J.P. Singh and J.L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiporcessing: </booktitle> <address> Tokyo, Japan, </address> <year> 1991. </year>
Reference-contexts: To privatize arrays, memory disambiguation is not sufficient; data-flow dependence analysis is required. Singh and 58 CHAPTER 4. DATA-FLOW DEPENDENCE ANALYSIS 59 Hennessy perform a similar study on two of the PERFECT Club benchmarks and also find privatization to be important <ref> [47] </ref>. In this chapter, we introduce the concept of data-flow dependence vectors. Data-flow dependence vectors provide us with a clean and simple representation to extend standard direction and distance vectors. We will describe a set of algorithms developed by Feautrier, which we can adapt to compute data-flow dependence vectors [16][15][17]. <p> As we have mentioned, recent research by Eigenmann et al. [13], by Singh and Hennessy <ref> [47] </ref>, and by us [38] has shown that array privatization is a critical transformation in the suite of parallelizing transformations. To privatize arrays, traditional data dependence techniques are not sufficient; data-flow dependence vectors are required.
Reference: [48] <author> R. Tarjan. </author> <title> Data Structures and Network Algorithms. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1983. </year>
Reference-contexts: Otherwise it is consistent. While a graph may have an exponential number of cycles, checking if a graph has a negative cycle can be done in time proportional to the number of edges times the number of vertices using Bellman-Ford's algorithm <ref> [48] </ref>. Since our graph has one vertex for every variable and one edge for every constraint, the complexity of the Simple Loop Residue Test is O (CV ). CHAPTER 2.
Reference: [49] <author> S. Tjiang, M. Wolf, M. Lam, K. Pieper, and J. Hennessy. </author> <title> Integrated scalar optimization and parallelization. </title> <booktitle> In Proc. 4th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: AFFINE MEMORY DISAMBIGUATION 28 one might have to cut off the process after an arbitrary number of steps and assume dependence. We have not found any cases that require us to use explicit branch and bound. 2.4 Effectiveness of Algorithms We have implemented these algorithms in the SUIF system <ref> [49] </ref>, a compiler system developed at Stanford. We then ran them on the PERFECT Club benchmarks. These are a set of 13 scientific Fortran programs ranging in size from 500 to 18,000 lines collected at the University of Illinois at Urbana-Champaign [14].
Reference: [50] <author> Steven W.K. Tjiang and John L. Hennessy. </author> <title> Sharlit a tool for building optimizers. </title> <booktitle> In Proceedings of the SIGPLAN PLDI, </booktitle> <pages> pages 82-93, </pages> <year> 1992. </year>
Reference: [51] <author> R. Triolet. </author> <title> Interprocedural analysis for program restructuring with parafrase. </title> <type> Technical Report CSRD Rep. No. 538, </type> <institution> University of Illinois Urbana-Champaign, </institution> <month> Dec. </month> <year> 1985. </year> <note> BIBLIOGRAPHY 134 </note>
Reference-contexts: When some of the loop bounds are triangular or trapezoidal or when multiple dimensions are coupled such as a [i][i], Banerjee further approximates and does not solve the real (non-integer) problem exactly either. Other researchers have developed more accurate may algorithms. Triolet uses the Fourier-Motzkin algorithm <ref> [51] </ref>. Fourier-Motzkin solves the real system exactly, even when there are triangular and trapezoidal bounds. Wallace has developed a variation of simplex extended to look for integer solutions [52]. To guarantee termination, it must assume dependence after a certain number of iterations. <p> The cost of this algorithm is a matter of debate. Theoretically it can be exponential. Experimentally, Triolet has implemented this approach and seems to be satisfied with its efficiency <ref> [51] </ref>, but Li, Yew and Zhu consider Triolet's numbers to be too expensive [32]. Nonetheless, we are required to call this algorithm so few times that its accrued expense is very reasonable. In the first step, we eliminate the first variable, x 1 , from the set of constraints. <p> Thus a simple boundary is either parallel to a coordinate axis or at a 45 degree angle to a pair of coordinate axes. Triolet advocates a more accurate approach. He finds the convex hull of all the array references <ref> [51] </ref>. While this approach is more accurate, handling convex hulls can be considerably more expensive than the other two approaches. All three of these approaches approximate the accessed regions. The regions described may be larger than the actual regions accessed.
Reference: [52] <author> D. R. Wallace. </author> <title> Dependence of multi-dimensional array references. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 418-428, </pages> <year> 1988. </year>
Reference-contexts: Other researchers have developed more accurate may algorithms. Triolet uses the Fourier-Motzkin algorithm [51]. Fourier-Motzkin solves the real system exactly, even when there are triangular and trapezoidal bounds. Wallace has developed a variation of simplex extended to look for integer solutions <ref> [52] </ref>. To guarantee termination, it must assume dependence after a certain number of iterations. The Lambda Test extends Banerjee's inequalities to better deal with coupled subscripts [32]. The Power Test [54] combines the GCD Test with Fourier-Motzkin.
Reference: [53] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: One common method to exploit this parallelism is to execute the loop iterations concurrently. In this first section, we give an overview describing the type of information that is required to parallelize loops. We base our framework on material found in <ref> [53] </ref> and [55]. Their descriptions are based on a long line of work originating from Kuck, Lamport and their associates. In the next section, we discuss calculating the information necessary for 1 CHAPTER 1. INTRODUCTION 2 parallelization. <p> The values being read were written before the first iteration of the loop. It is therefore possible to execute the iterations concurrently. Two array references a and a' are said to be dependent if any of the locations accessed by reference a are also accessed by reference a' <ref> [53] </ref>. Otherwise, the two references are independent. <p> Wolfe showed that the bounds constraints on ~x can be expressed as constraints on ~ t <ref> [53] </ref>. <p> Not computing direction vectors, these algorithms found 415 out of 482 independent pairs, missing 16%. For direction vectors, we used the Simple GCD Test followed by Wolfe's extension to Banerjee's rectangular test (2.5.2 in <ref> [53] </ref>). We eliminated unused variables so that a [i] versus a [i-1] would return the one direction vector (fl; +) CHAPTER 2.
Reference: [54] <author> M. J. Wolfe and C.-W. Tseng. </author> <title> The Power test for data dependence. </title> <type> Technical Report CS/E 90-015, </type> <institution> Dept. of Computer Science and Engineering, Oregon Graduate Institute, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Wallace has developed a variation of simplex extended to look for integer solutions [52]. To guarantee termination, it must assume dependence after a certain number of iterations. The Lambda Test extends Banerjee's inequalities to better deal with coupled subscripts [32]. The Power Test <ref> [54] </ref> combines the GCD Test with Fourier-Motzkin. While some of these algorithms may be accurate in most cases, they all have the problem that their approximations are implicit. If such an algorithm returns dependent, we do not know if an approximation was made.
Reference: [55] <author> H. Zima and Barbara Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: One common method to exploit this parallelism is to execute the loop iterations concurrently. In this first section, we give an overview describing the type of information that is required to parallelize loops. We base our framework on material found in [53] and <ref> [55] </ref>. Their descriptions are based on a long line of work originating from Kuck, Lamport and their associates. In the next section, we discuss calculating the information necessary for 1 CHAPTER 1. INTRODUCTION 2 parallelization. <p> Current compiler systems conservatively guarantee this condition by requiring that parallelization maintain the serial execution order between every write operation and every other write or read operation to the same memory location <ref> [55] </ref>. Parallelizing loops under this model requires the compiler to analyze array reference patterns. <p> Any dependent iteration pair such that ~ i = ~ i 0 is a loop-independent dependence, while a dependent iteration pair such that ~ i 6= ~ i 0 is a loop-carried dependence <ref> [55] </ref>. All the iterations of a loop nesting can be run in parallel if and only if there are no loop-carried dependences between any two references in the loop. To parallelize a subset of the loop nestings, we need to know more than if there are loop-carried dependences. <p> This is both infeasible and unnecessary for most optimization techniques. Distance vectors and direction vectors are standard representations CHAPTER 1. INTRODUCTION 4 that allow us to summarize the set of dependent iteration pairs <ref> [55] </ref>. Distance vectors represent the vector difference between the two iteration elements in a dependent iteration pair. <p> We use the term dependence vector to refer to either distance or direction vectors. Standard compiler systems require parallelization to preserve the order between all write operations and all read/write operations to the same location <ref> [55] </ref>. Given this model, distance and direction vectors are sufficient representations for parallelization. This model, though, is too restrictive. Not all dependences are equally harmful.
References-found: 55

