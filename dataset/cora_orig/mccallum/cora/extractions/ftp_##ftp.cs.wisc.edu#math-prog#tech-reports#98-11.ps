URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/98-11.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: http://www.cs.wisc.edu
Title: MATHEMATICAL PROGRAMMING APPROACHES TO MACHINE LEARNING AND DATA MINING  
Author: By Paul S. Bradley 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1998  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> The NETLIB test problem set. </institution> <note> www.enseeiht.fr/netlib/index.html. </note>
Reference-contexts: The second set consists of "bad" radar returns, those that do not show evidence of structure; the signals pass through the ionosphere. There are 351 instances with 35 features per instance. Feature 1 takes continuous values in <ref> [0; 1] </ref>. Feature 2 is zero for all instances. Features 3 through 34 take continuous values on [1; 1]. The set A consists of features 1 through 34 corresponding to "good" radar returns. The set B consists of the same features corresponding to "bad" radar returns. <p> There are 351 instances with 35 features per instance. Feature 1 takes continuous values in [0; 1]. Feature 2 is zero for all instances. Features 3 through 34 take continuous values on <ref> [1; 1] </ref>. The set A consists of features 1 through 34 corresponding to "good" radar returns. The set B consists of the same features corresponding to "bad" radar returns. Notice that feature 2 is irrelevant. <p> The set A consists of features 1 through 34 corresponding to "good" radar returns. The set B consists of the same features corresponding to "bad" radar returns. Notice that feature 2 is irrelevant. Similarly, we augmented this dataset by adding 6 random features sampled from a uniform distribution on <ref> [1; 1] </ref>. <p> Upon the suggestion of Philip E. Gill [66], the LPC Algorithm 2.3.1 was applied to the WOODW linear programming problem. The WOODW problem data is publically available from the Netlib repository <ref> [1] </ref>. The primal linear program consists of 1085 equality constraints and 13 inequality constraints. There are 8405 variables. The optimal objective value is 1.3044763331. The constraints are 0.04% dense with 37474 66 nonzeros and the constraint matrix is structured (see Figure 9). <p> By letting range over the interval <ref> [0; 1] </ref>, the number of nonzero elements in the solution x varies from n to 0, while the error kAx bk 1 monotonically increases. Depending on the problem at hand, a solution for a 2 [0; 1) will be most desirable. <p> The linear systems used are based upon ideas related to signal processing [69, 155] and more specifically to an example in [2, Equation (8)]. We consider the following true signal g (t) : <ref> [0; 1] </ref> ! R: g (t) = j=1 We assume that the true signal g (t) cannot be sampled precisely, but that the following observed signal can be sampled: 119 ~g (t) = (g (t) + error); sampled at times : t i = i 4 t; 4t = 0:04; i <p> The LPC Algorithm was tested on an expanded version of the WOODW dual problem from the NETLIB repository <ref> [1] </ref>. It was motivated and further tested on the classification task over massive datasets that may not fit into resident machine memory. The algorithm uses support vector ideas by keeping only essential data points needed for determining a separating plane.
Reference: [2] <author> T. J. Abatzoglou, J. M. Mendel, and G. A. Harada. </author> <title> The constrained total least squares technique and its application to harmonic superposition. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39 </volume> <pages> 1070-1087, </pages> <year> 1991. </year>
Reference-contexts: One can relate this to a machine learning framework by treating the observed system as a training set, and the true system as a testing set [75]. The linear systems used are based upon ideas related to signal processing [69, 155] and more specifically to an example in <ref> [2, Equation (8)] </ref>. <p> 4 t; 4t = 0:04; i = 0; 1; : : : ; 25: (88) We further assume that we do not know the true signal g (t) (87), and we attempt to model it as: ^g (t) = j=1 The parameter values a in (89) were chosen as in <ref> [2, Equation (8)] </ref>. The problem now is to compute the coefficients x j ; j = 1; : : : ; 10; of ^g (t) (89) so that we can adequately recover g (t), given only the noisy data ~g (t i ) (88).
Reference: [3] <author> K. Al-Sultan. </author> <title> A Tabu search approach to the clustering problem. </title> <journal> Pattern Recognition, </journal> <volume> 28(9) </volume> <pages> 1443-1451, </pages> <year> 1995. </year>
Reference-contexts: Many approaches to this problem include statistical [81, 63], machine learning [60], integer and mathematical programming <ref> [147, 3, 135] </ref>.
Reference: [4] <author> H. Almuallin and T. G. Deitterich. </author> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 547-552, </pages> <year> 1991. </year>
Reference-contexts: In contrast, the wrapper model encompasses algorithms which utilize the estimator ^g to evaluate a given feature subset. Instances of the filter model include the FOCUS algorithm <ref> [4] </ref>, the Relief algorithm [85] and a method of feature selection based upon Information Theory [88]. The FOCUS algorithm [4] performs an exhaustive search through the possible feature subsets and returns the minimal subset sufficient for classification. <p> In contrast, the wrapper model encompasses algorithms which utilize the estimator ^g to evaluate a given feature subset. Instances of the filter model include the FOCUS algorithm <ref> [4] </ref>, the Relief algorithm [85] and a method of feature selection based upon Information Theory [88]. The FOCUS algorithm [4] performs an exhaustive search through the possible feature subsets and returns the minimal subset sufficient for classification. FOCUS may not be able to identify irrelevant features when noise is present in the training set, or if the training set is not representative of future data [83].
Reference: [5] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illinois, </address> <year> 1992. </year>
Reference-contexts: Examples include a separating-plane based function [96, 97], the backpropagation algorithm for artificial neural networks (ANNs) [77, 111], decision tree construction algorithms utilizing various node-decision criteria <ref> [29, 134, 5] </ref>, spline methods for classification [165, 162] and probabilistic graphical dependency models [76, 32]. Evaluating an estimate ^g of g in terms of how well it performs on data not included in the training set, or how well ^g generalizes, is paramount.
Reference: [6] <author> K. P. Bennett and J. A. </author> <title> Blue. A support vector machine approach to decision trees. </title> <institution> Department of Mathematical Sciences Math Report No. 97-100, Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1997. </year> <note> http://www.math.rpi.edu/~bennek/. </note>
Reference-contexts: &gt; &gt; &gt; &gt; = ; We note that an optimization formulation was proposed and implemented in [97] for computing a separating plane by forcing the bounding planes to be as far apart as possible. 48 Usually the support vector machine problem is formulated using the 2-norm in the objective <ref> [161, 6] </ref>. Since the 2-norm is dual to itself, it follows that the margin is also measured in the 2-norm when this formulation is used. <p> to (71), the objective function consist of weighting the 1-norm of the residual vector by (1 ) and weighting by a term which suppresses the components of x, which is our Parsimonious Least Norm Approximation (PLNA) problem: min For comparative purposes we shall also employ Vapnik's support vector machine approach <ref> [161, 6] </ref> of minimizing the size of the solution vector x as well as the error kAx bk 1 , thereby decreasing the VC-Dimension [161, p 76] (a capacity measure) and improving generalization.
Reference: [7] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher> <pages> 146 </pages>
Reference-contexts: The formulation (6) is equivalent to the following robust linear programming formulation (RLP) proposed in <ref> [7] </ref> and effectively used to solve problems from real-world domains [109]: min m e 0 z fi fi 19 Non-symmetric misclassification costs can easily be handled by the RLP formulation (7) by introducing parameters fi 1 and fi 2 into the objective.
Reference: [8] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: Two principal reasons for choosing the 1-norm in (6) are: (i) Problem (6) is then reducible to a linear program (7) with many important theoretical properties making it an effective computational tool <ref> [8] </ref>. (ii) The 1-norm is less sensitive to outliers such as those occurring when the underlying data distributions have pronounced tails, hence (6) has a similar effect to that of robust regression [78],[75, pp 82-87]. <p> In contrast, the formulations (7), (12), (13), (16) measure average separation error. Minimizing average separation error in (7) ensures that the solution w = 0 occurs iff e 0 A = k , in which case it is not unique <ref> [8, Theorem 2.5] </ref>. 2.2.2 Numerical Comparison with FSV and RLP Numerical tests were conducted to investigate feature suppression and generalization ability of the support vector machine approaches (34) - (36), the concave minimization FSV problem (13) and the robust linear program (7) [22].
Reference: [9] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization & Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: To avoid this difficulty, (15) is reformulated as a parametric bilinear program which can be easily handled by solving a finite sequence of linear programs that terminate at a stationary point <ref> [9, Algorithm 2.1] </ref>. <p> We now turn our attention to solving the FSB problem (16). Bilinear Algorithm Due to the bilinear nature of the objective function of the FSB problem (16), we consider the fast, simple bilinear algorithm of <ref> [9, Algorithm 2.1] </ref>. We apply the algorithm to solve the FSB problem (16) as follows. Algorithm 2.1.5 Bilinear Algorithm for FSB (16) Choose 2 [0; 1); 2 (0; 1). <p> Hence the Uncoupled Bilinear Program Algorithm UBPA <ref> [9, Algorithm 2.1] </ref> is applicable. Simply stated, this algorithm alternates between solving a linear program in the variable T and a linear program in the variables (C; D). The algorithm terminates finitely at a stationary point satisfying the minimum principle necessary 83 optimality condition for problem (52) [9, Theorem 2.1]. <p> Simply stated, this algorithm alternates between solving a linear program in the variable T and a linear program in the variables (C; D). The algorithm terminates finitely at a stationary point satisfying the minimum principle necessary 83 optimality condition for problem (52) <ref> [9, Theorem 2.1] </ref>. Further, because of the simple structure of the bilinear program (52), the two linear programs can be solved explicitly in closed form. This leads us to the following algorithmic implementation. <p> The FSV problem (13) is solved by solving a sequence of linear programs via the Successive Linearization Algorithm 2.1.3, which is a general tool for solving problems with a concave objective over a polyhedral region [103]. The FSV problem (16) is solved by <ref> [9, Algorithm 2.1] </ref>. Classifiers produced by the support vector machine [161, 33] (SVM) approach were presented. Minimizing an upper bound on generalization performance [161, 33] motivates the SVM formulation which attempts to separate the training data A and B with a maximum margin.
Reference: [10] <author> M. W. Berry, S. T. Dumais, and G. W. O'Brein. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year> <note> http://www.cs.utk.edu/~berry. </note>
Reference-contexts: A different clustering approach, latent semantic indexing, is given in <ref> [10] </ref> that also uses singular value decomposition. We end this section by establishing the finiteness of the k-Plane Algorithm 3.2.1. Theorem 3.2.8 (Finite Termination of the k-Plane Algorithm 3.2.1) The k-Plane Algorithm 3.2.1 terminates in a finite number of steps at a cluster assignment that is locally optimal.
Reference: [11] <author> C. M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: Depending on the goals of the user, different data mining methods may be employed in the KDD process. Two primary data mining methods are classification and clustering [57]. 1.3.1 Massive Dataset Issues Many popular classification and clustering algorithms <ref> [148, 63, 11, 81] </ref> require accessing individual data items an arbitrary number of times, or require data to be resident in memory for effective processing. These approaches play powerful roles as data mining 7 algorithms over data sets of modest size.
Reference: [12] <author> V. Blanz, B. Scholkopf, H. Bulthoff, C. Burges, V. Vapnik, and T. Vetter. </author> <title> Comparison of view-based object recognition algorithms using realistic 3d models. </title> <booktitle> In Artificial Neural Networks - ICANN96, </booktitle> <pages> pages 251-256, </pages> <address> Berlin, </address> <year> 1996. </year> <booktitle> Springer Lecture Notes in Computer Science, </booktitle> <volume> Vol. </volume> <pages> 1112. </pages>
Reference-contexts: In the case where the training data are linearly inseparable (training error &gt; 0), one attempts to minimize the separation error and the norm of w simultaneously. Applications of support vector machine classifiers include isolated handwritten digit recognition [46, 144, 145, 34], object recognition <ref> [12] </ref>, face detection [127] and text categorization [82].
Reference: [13] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: This problem is compounded by the fact that we have only a finite sampling of g, which, in addition, may be corrupted by noise. To avoid overfitting, many learning algorithms utilize the Occam's Razor bias <ref> [13] </ref> 3 (or equivalently, minimum description length [136] or minimum message length [166]). This bias favors the simplest model possible that still achieves some level of performance on the training data. Common techniques to avoid overtraining are early stopping and weight decay [16]. <p> Thus, with a large number of original problem features and a finite amount of training data, many different estimators may accurately separate the training data, but only a small fraction may generalize well. In the classification task described above, one realization of the Occam's Razor bias <ref> [13] </ref> is to "choose" a small number of predictive features and utilize these to construct the estimator ^g, discarding irrelevant or redundant features. Removal of irrelevant or redundant features usually speeds the learning process, the constructed estimator usually generalizes better and often lends itself to easier interpretation [117]. <p> This approach has been successfully used in such machine learning problems as misclassification minimization [101], feature selection [27] and data mining [26, 105]. The idea behind using as few columns of A as possible to span b is motivated by the Occam's Razor bias <ref> [13] </ref>.
Reference: [14] <author> M. Boddy and T. Dean. </author> <title> Decision-theoretic deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67(2) </volume> <pages> 245-286, </pages> <year> 1994. </year>
Reference-contexts: Early termination, if appropriate, is highly 8 desirable. * On-line "anytime" behavior <ref> [14] </ref>: a "best" solution is always available, with status information on progress provided to allow the user to terminate the process if the current solution is adequate. * Suspendable, stoppable, resumable: incremental progress is saved to resume a stopped job. * Ability to incrementally incorporate additional data with existing models efficiently.
Reference: [15] <author> S. Bos. </author> <title> A realizable learning task which shows overfitting. </title> <booktitle> In Advances in Neural 147 Information Processing Systems 8, </booktitle> <pages> pages 218-224, </pages> <address> Cambridge, MA, 1996. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Overtraining can also lead to poor generalization even when the complexity of the function class from which ^g is constructed is optimal <ref> [15] </ref>. The key to good generalization is correctly estimating the complexity of the true mapping g while avoiding overtraining. This problem is compounded by the fact that we have only a finite sampling of g, which, in addition, may be corrupted by noise.
Reference: [16] <author> S. Bos and M. Opper. </author> <title> An exact description of early stopping and weight decay. Lab for Information Representation, </title> <address> RIKEN, Wako-shi Saitama 351-01, Japan. </address> <month> November 21, </month> <year> 1996. </year> <note> See http://www.bip.riken.go.jp/irl/boes/boes.html. </note>
Reference-contexts: This bias favors the simplest model possible that still achieves some level of performance on the training data. Common techniques to avoid overtraining are early stopping and weight decay <ref> [16] </ref>. The latter can also be viewed as an overfitting avoidance technique. Generalization ability can be effectively estimated by leave-one-out testing [90], where ^g is constructed by a learning algorithm using all but one of the available training examples and is tested on the point "left out".
Reference: [17] <author> L. Bottou and Y. Bengio. </author> <title> Convergence properties of the k-means algorithms. </title> <booktitle> In Advances in Neural Information Processing Systems -7-, </booktitle> <pages> pages 585-592, </pages> <address> Cam-bridge, MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Hard clustering algorithms can often be placed in the probabilistic framework of soft clustering <ref> [18, 17] </ref>. Formal methodology for evaluating clustering algorithms is lacking, in contrast to the fairly standard methodologies of leave-one-out testing and ten-fold cross-validation used in the supervised classification problem. Evaluation procedures include distortion (if applicable), information gain [18], classification accuracy (if classes are known) and holdout likelihood [112].
Reference: [18] <author> P. S. Bradley and U. M. Fayyad. </author> <title> Refining initial points for k-means clustering. </title> <editor> In J. Shavlik, editor, </editor> <booktitle> Proceedings of the Fifteenth International Conference on Machine Learning (ICML '98), </booktitle> <pages> pages 91-99, </pages> <address> San Francisco, CA, 1998. </address> <publisher> Morgan Kaufmann. </publisher> <address> http://www.cs.wisc.edu/~paulb/papers.html. </address>
Reference-contexts: Hard clustering algorithms can often be placed in the probabilistic framework of soft clustering <ref> [18, 17] </ref>. Formal methodology for evaluating clustering algorithms is lacking, in contrast to the fairly standard methodologies of leave-one-out testing and ten-fold cross-validation used in the supervised classification problem. Evaluation procedures include distortion (if applicable), information gain [18], classification accuracy (if classes are known) and holdout likelihood [112]. <p> Formal methodology for evaluating clustering algorithms is lacking, in contrast to the fairly standard methodologies of leave-one-out testing and ten-fold cross-validation used in the supervised classification problem. Evaluation procedures include distortion (if applicable), information gain <ref> [18] </ref>, classification accuracy (if classes are known) and holdout likelihood [112]. Generalization ability is usually not an explicit issue in unsupervised learning. Although generalization ability is not paramount, clustering methods still implement a bias when defining clusters. <p> For datasets with a massive number of examples, the straightforward way to "scale" existing algorithms is by random sampling. One selects a random sample that fits into resident memory and constructs models over this sample. Models constructed over random samples may be inadequate <ref> [19, 18] </ref>. These drawbacks may be overcome by employing more complex sampling or "averaging" schemes. <p> The algorithms presented above are shown to be effective data mining tools over moderately sized datasets, but have serious computational drawbacks when applied to massive datasets. In <ref> [18, 59] </ref>, an algorithm is proposed for refining a given clustering initialization targeted at clustering massive datasets. In [19, 20], a scalable clustering framework is presented and instantiated on the k-Mean and EM algorithms. A scalable clustering approach is also presented in [168]. <p> Note that the k-Median Algorithm 3.1.3 computes a solution which is locally opti mal. We cannot guaranteed a solution which is globally optimal. The issue of initializing iterative clustering approaches is discussed in <ref> [18, 59] </ref>. <p> Useful solutions were computed via the Successive Linearization Algorithms 2.1.3 and 4.4.1 and the k-Median Algorithm 3.1.3 with random initializations. Knowledge about the particular problem domain in which these algorithms are applied may lead to a better initialization procedure. We note that issues regarding clustering initialization are provided in <ref> [18, 59] </ref>. 144 Analysis of the classifiers computed by the support vector machine approach utilizing different norms to measure the margin of separation is needed.
Reference: [19] <author> P. S. Bradley, U. M. Fayyad, and C. Reina. </author> <title> Scaling clustering to large databases. </title> <booktitle> In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> KDD98, </address> <year> 1998. </year> <note> http://www.cs.wisc.edu/~paulb/papers.html. </note>
Reference-contexts: For datasets with a massive number of examples, the straightforward way to "scale" existing algorithms is by random sampling. One selects a random sample that fits into resident memory and constructs models over this sample. Models constructed over random samples may be inadequate <ref> [19, 18] </ref>. These drawbacks may be overcome by employing more complex sampling or "averaging" schemes. <p> Models constructed over random samples may be inadequate [19, 18]. These drawbacks may be overcome by employing more complex sampling or "averaging" schemes. To deal efficiently with massive datasets, we propose that data mining algorithms be developed which satisfy (to the extent possible) the following <ref> [19] </ref>: * Require one scan of the dataset if possible: since even a single data scan over a massive database can be costly. <p> The algorithms presented above are shown to be effective data mining tools over moderately sized datasets, but have serious computational drawbacks when applied to massive datasets. In [18, 59], an algorithm is proposed for refining a given clustering initialization targeted at clustering massive datasets. In <ref> [19, 20] </ref>, a scalable clustering framework is presented and instantiated on the k-Mean and EM algorithms. A scalable clustering approach is also presented in [168].
Reference: [20] <author> P. S. Bradley, U. M. Fayyad, and C. Reina. </author> <title> Scaling EM clustering to large databases. </title> <type> Technical report, </type> <institution> Microsoft Research, </institution> <address> Redmond, WA, </address> <year> 1998. </year> <note> http://www.cs.wisc.edu/~paulb/papers.html. </note>
Reference-contexts: The algorithms presented above are shown to be effective data mining tools over moderately sized datasets, but have serious computational drawbacks when applied to massive datasets. In [18, 59], an algorithm is proposed for refining a given clustering initialization targeted at clustering massive datasets. In <ref> [19, 20] </ref>, a scalable clustering framework is presented and instantiated on the k-Mean and EM algorithms. A scalable clustering approach is also presented in [168].
Reference: [21] <author> P. S. Bradley, Usama M. Fayyad, and O. L. Mangasarian. </author> <title> Data mining: Overview and optimization opportunities. </title> <type> Technical Report 98-01, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <note> January 148 1998. INFORMS Journal on Computing, submitted. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/998-01.ps.Z. </note>
Reference-contexts: Evaluation criteria include certainty (estimated predictive accuracy on unseen data or generalization ability) or utility (gain, perhaps in dollars saved due to better predictions or speed-up in response time of a system). Other criteria such as novelty and understandability are much more subjective and difficult to define <ref> [21] </ref>. Application of a visualization tool may aid in evaluating the output of the data mining algorithm. See Figure 1. <p> For example, one may select, clean and reduce data only to discover after mining that one or several of the previous steps need to be redone <ref> [21] </ref>. Depending on the goals of the user, different data mining methods may be employed in the KDD process. <p> There are straightforward ways to "scale" current algorithms to deal with large scale or massive datasets. A standard approach to dealing with high-dimensional data is to project it into a low-dimensional space and attempt to construct models in this space <ref> [21] </ref>. Three drawbacks to this scheme are: (1) as the number of dimensions grow, the combinatorial choice of projection into a space of lower dimension becomes overwhelming; (2) projection to a space of lower dimension could transform a relatively easy modeling problem into one that is extremely difficult [21]; (3) the <p> this space <ref> [21] </ref>. Three drawbacks to this scheme are: (1) as the number of dimensions grow, the combinatorial choice of projection into a space of lower dimension becomes overwhelming; (2) projection to a space of lower dimension could transform a relatively easy modeling problem into one that is extremely difficult [21]; (3) the projection may result in loss of accuracy or "quality", resulting in a poor model or a difficult problem (as in (2)). For datasets with a massive number of examples, the straightforward way to "scale" existing algorithms is by random sampling.
Reference: [22] <author> P. S. Bradley and O. L. Mangasarian. </author> <title> Feature selection via concave minimization and support vector machines. </title> <editor> In J. Shavlik, editor, </editor> <booktitle> Machine Learning Proceedings of the Fifteenth International Conference(ICML '98), </booktitle> <pages> pages 82-90, </pages> <address> San Francisco, California, 1998. </address> <publisher> Morgan Kaufmann. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/98-03.ps.Z. </publisher>
Reference-contexts: We begin with the feature selection problem which consists of eliminating as many features from the original n-dimensional features space as possible, while still accurately estimating g over the training set. We then focus on the support vector machine <ref> [22, 33, 161] </ref> approach to classification. Computational comparison is made between a feature selection approach via concave minimization and the support vector machine formulations. A technique for solving general linear programs with a massive number of constraints is considered. <p> , in which case it is not unique [8, Theorem 2.5]. 2.2.2 Numerical Comparison with FSV and RLP Numerical tests were conducted to investigate feature suppression and generalization ability of the support vector machine approaches (34) - (36), the concave minimization FSV problem (13) and the robust linear program (7) <ref> [22] </ref>. The six publicly available datasets used in this comparison are discussed next. 50 Datasets We used two variants of the Wisconsin Prognostic Breast Cancer Database (see Section 2.1.4).
Reference: [23] <author> P. S. Bradley and O. L. Mangasarian. </author> <title> k-Plane clustering. </title> <type> Technical Report 98-08, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> August </month> <year> 1998. </year> <month> ftp://ftp.cs.wisc.edu/math-prog/tech-reports/98-08.ps.Z. </month>
Reference-contexts: We cannot guaranteed a solution which is globally optimal. The issue of initializing iterative clustering approaches is discussed in [18, 59]. We now focus on a novel clustering approach in which the clusters are represented by planes in contrast to points in R n <ref> [23] </ref>. 3.2 k-Plane Clustering We change the characterization of a cluster from a centroid or point in R n to a plane in R n . The justification for this approach is that data sometimes naturally falls into clusters grouped around flat surfaces such as planes. <p> The k-Mean clusters were better on the Ionosphere dataset. On both the BUPA and Ionosphere datasets, the k-Plane algorithm converged faster than k-Mean, as much as 6.21 times faster on the BUPA dataset <ref> [23] </ref>. The novel k-Plane Algorithm 3.2.1 provides the analyst with a clustering tool which 109 Table 10: 10-fold Cross-Validation Results Ionosphere BUPA Ave. Test k-Plane 0.641 0.011 0.603 0.047 Correct. k-Mean 0.706 0.100 0.556 0.032 k-Median 0.661 0.109 0.565 0.033 Ave.
Reference: [24] <author> P. S. Bradley and O. L. Mangasarian. </author> <title> Massive data discrimination via linear support vector machines. </title> <type> Technical Report 98-05, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> May </month> <year> 1998. </year> <month> ftp://ftp.cs.wisc.edu/math-prog/tech-reports/98-03.ps.Z. </month>
Reference-contexts: These results again indicate the utility of the FSV approach to the important data mining problem of classification. Next, we consider an approach for solving general linear programs with a massive number of constraints <ref> [24] </ref>. This approach effectively scales the linear-programming-based classification formulations and makes them useful in large-scale KDD applications (see Section 1.3.1). 2.3 Massive Datasets The approach discussed is a method for solving linear programs with a massive number of constraints. <p> We state now our chunking algorithm <ref> [24, Algorithm 3.1] </ref> and establish its finite termination [24, Theorem 3.2] for the linear program (37), where m may be orders of magnitude larger than n. <p> We state now our chunking algorithm [24, Algorithm 3.1] and establish its finite termination <ref> [24, Theorem 3.2] </ref> for the linear program (37), where m may be orders of magnitude larger than n. <p> This significant result is the basis of a fundamental computational approach for handling linear programs with massive constraints for which the subproblems (38) of the LPC Algorithm 2.3.1 have vertex solutions. To establish its validity we first prove a lemma <ref> [24, Lemma 3.3] </ref>.
Reference: [25] <author> P. S. Bradley, O. L. Mangasarian, and J. B. Rosen. </author> <title> Parsimonious least norm approximation. </title> <journal> Computational Optimization and Applications, </journal> (1):5-21, October 1998. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-03.ps.Z. 
Reference-contexts: The approach was generalized in [107] for a nondifferentiable f using its supergradient (2). Existence of a vertex solution for a finite smoothing parameter to the problem with differentiable f and negative exponential which also solves (19) was established in <ref> [25] </ref>. The result is generalized for nondifferentiable f in [106]. We restate [25, Theorem 2.1]. <p> Existence of a vertex solution for a finite smoothing parameter to the problem with differentiable f and negative exponential which also solves (19) was established in [25]. The result is generalized for nondifferentiable f in [106]. We restate <ref> [25, Theorem 2.1] </ref>.
Reference: [26] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Clustering via concave minimization. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems -9-, </booktitle> <pages> pages 368-374, </pages> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-03.ps.Z. </publisher> <pages> 149 </pages>
Reference-contexts: This is clearly a combinatorial problem that we shall solve by minimizing a concave function on polyhedral 113 set. This approach has been successfully used in such machine learning problems as misclassification minimization [101], feature selection [27] and data mining <ref> [26, 105] </ref>. The idea behind using as few columns of A as possible to span b is motivated by the Occam's Razor bias [13].
Reference: [27] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Feature selection via mathematical programming. </title> <journal> INFORMS Journal on Computing, </journal> <volume> 10(2) </volume> <pages> 209-217, </pages> <year> 1998. </year> <month> ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-21.ps.Z. </month>
Reference-contexts: Initial centers for the k-Median and k-Mean algorithms were deterministically chosen by dividing the coordinate axes into six intervals over the range of the data and choosing two centers as the midpoints of the densest and second densest intervals on the axes <ref> [27] </ref>. We utilized three publicly available databases from the UCI ML Repository [119] and one dataset that appears in [126]. The Wisconsin Diagnostic Breast Cancer Dataset (WDBC) consists of 569 instances each having 32 features. Feature 1 is an identification and was discarded. <p> This is clearly a combinatorial problem that we shall solve by minimizing a concave function on polyhedral 113 set. This approach has been successfully used in such machine learning problems as misclassification minimization [101], feature selection <ref> [27] </ref> and data mining [26, 105]. The idea behind using as few columns of A as possible to span b is motivated by the Occam's Razor bias [13].
Reference: [28] <author> E. J. Bredensteiner and K. P. Bennett. </author> <title> Feature minimization within decision trees. </title> <journal> Computational Optimizations and Applications, </journal> <volume> 10 </volume> <pages> 111-126, </pages> <year> 1998. </year>
Reference-contexts: In [118], stepwise techniques are utilized to avoid exhaustive enumeration of all possible feature subsets. In [38], dynamic programming is used for the same purpose. In [123], a branch and bound method is presented that effectively rejects suboptimal subsets without direct evaluation and yields a global solution. In <ref> [28] </ref>, 17 the feature selection problem is formulated as a linear program with equilibrium (complementarity) constraints (LPEC), but both the LPEC formulation and its method of solution differ from those presented here.
Reference: [29] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Publishing Company, </publisher> <address> Belmont, CA, </address> <year> 1983. </year>
Reference-contexts: Examples include a separating-plane based function [96, 97], the backpropagation algorithm for artificial neural networks (ANNs) [77, 111], decision tree construction algorithms utilizing various node-decision criteria <ref> [29, 134, 5] </ref>, spline methods for classification [165, 162] and probabilistic graphical dependency models [76, 32]. Evaluating an estimate ^g of g in terms of how well it performs on data not included in the training set, or how well ^g generalizes, is paramount.
Reference: [30] <author> C. E. Brodley and P. E. Utgoff. </author> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19(1) </volume> <pages> 45-77, </pages> <year> 1995. </year>
Reference-contexts: These searches must be monitored to prevent cycling. There are further generalizations of these basic procedures such as backward stepwise elimination-SLASH (BSE-SLASH) [36] and Greedy Sequential Backward Elimination (GBSE) <ref> [30] </ref>. Optimal Brain Damage (OBD) [92] is a procedure to set the weights of an artificial neural network (ANN) to zero, but when applied to weights associated with input features, this is synonymous with feature selection.
Reference: [31] <author> A. Brooke, D. Kendrick, and A. Meeraus. </author> <title> GAMS: A User's Guide. </title> <publisher> The Scientific Press, </publisher> <address> South San Francisco, CA, </address> <year> 1988. </year>
Reference-contexts: These features are deemed relevant to the prognosis problem. All linear programs formulations were solved using the CPLEX package [48] called from within MATLAB [110]. The quadratic programming problem (36) was solved using the MINOS optimization package [120] called from the GAMS modeling environment <ref> [31] </ref>. Table 7 summarizes average solve times for the Ionosphere dataset, the largest in this numerical comparison. Table 7: Average running times: Ionosphere data set.
Reference: [32] <author> W. L. Buntine. </author> <title> Graphical models for discovering knowledge. </title> <booktitle> In Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 59-81, </pages> <address> Menlo Park, CA, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Examples include a separating-plane based function [96, 97], the backpropagation algorithm for artificial neural networks (ANNs) [77, 111], decision tree construction algorithms utilizing various node-decision criteria [29, 134, 5], spline methods for classification [165, 162] and probabilistic graphical dependency models <ref> [76, 32] </ref>. Evaluating an estimate ^g of g in terms of how well it performs on data not included in the training set, or how well ^g generalizes, is paramount. <p> Probabilistic clustering methods include the COBWEB algorithm [60], AutoClass [39], the Expectation-Maximization (EM) algorithm [50, 124] and, more recently, probabilistic graphical approaches <ref> [76, 32] </ref>. "Hard" clustering algorithms, such as k-Mean, assign data items to a single cluster whereas "soft" clustering algorithms, such as EM, assign a given data point to all clusters with a certain probability of membership.
Reference: [33] <author> C. J. C. Burges. </author> <title> A tutorial on support vector machines. Data Mining and Knowledge Discovery, </title> <type> 2, </type> <year> 1998. </year>
Reference-contexts: We begin with the feature selection problem which consists of eliminating as many features from the original n-dimensional features space as possible, while still accurately estimating g over the training set. We then focus on the support vector machine <ref> [22, 33, 161] </ref> approach to classification. Computational comparison is made between a feature selection approach via concave minimization and the support vector machine formulations. A technique for solving general linear programs with a massive number of constraints is considered. <p> The principal motivation is a probabilistic bound on the generalization error [160] of a classifier consisting of the average error of the classifier over a finite training set plus a term dependent upon the VC-Dimension <ref> [161, 33] </ref> of the particular set of classification functions considered. We are interested in the set of classification functions defined by separating planes, w 0 x = fl on R n . <p> In practice, one usually solves (36) by way of its dual [98]. In this formulation, the data enter only as inner products which are computed in the transformed space via a kernel function K (x; y) = (x) (y) <ref> [33, 161, 164] </ref>. We note that separation errors in (34) - (36) are weighted equally conforming to the SVM formulations in [33, 161]. In contrast, the formulations (7), (12), (13), (16) measure average separation error. <p> In this formulation, the data enter only as inner products which are computed in the transformed space via a kernel function K (x; y) = (x) (y) [33, 161, 164]. We note that separation errors in (34) - (36) are weighted equally conforming to the SVM formulations in <ref> [33, 161] </ref>. In contrast, the formulations (7), (12), (13), (16) measure average separation error. <p> The FSV problem (16) is solved by [9, Algorithm 2.1]. Classifiers produced by the support vector machine <ref> [161, 33] </ref> (SVM) approach were presented. Minimizing an upper bound on generalization performance [161, 33] motivates the SVM formulation which attempts to separate the training data A and B with a maximum margin. <p> The FSV problem (16) is solved by [9, Algorithm 2.1]. Classifiers produced by the support vector machine <ref> [161, 33] </ref> (SVM) approach were presented. Minimizing an upper bound on generalization performance [161, 33] motivates the SVM formulation which attempts to separate the training data A and B with a maximum margin.
Reference: [34] <author> C. J. C. Burges and B. Scholkopf. </author> <title> Improving the accuracy and speed of support vector machines. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems -9-, </booktitle> <pages> pages 375-381, </pages> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. </publisher> <pages> 150 </pages>
Reference-contexts: In the case where the training data are linearly inseparable (training error &gt; 0), one attempts to minimize the separation error and the norm of w simultaneously. Applications of support vector machine classifiers include isolated handwritten digit recognition <ref> [46, 144, 145, 34] </ref>, object recognition [12], face detection [127] and text categorization [82].
Reference: [35] <author> C. L. Carter, C. Allen, and D. E. Henson. </author> <title> Relation of tumor size, lymph node status, and survival in 24,740 breast cancer cases. </title> <journal> Cancer, </journal> <volume> 63 </volume> <pages> 181-187, </pages> <year> 1989. </year> <title> SEER: Surveillence, Epidemiology and End Results Program of the National Cancer Institute. </title>
Reference-contexts: These two features were then normalized to have mean = 0 and standard deviation = 1. In this data-mining context, when referring to the WPBC 86 dataset, we are referring to this set of 194 points in R 2 . We also used a subset of the SEER database <ref> [35] </ref> consisting of the two features tumor size and nodes positive for 21,960 instances. Each of these features is encoded in a non-intuitive fashion as an integer value in f0; : : : ; 8g. Tumor size = 0 corresponds to "no tumor found or microscopic tumor only".
Reference: [36] <author> R. Caruana and D. Freitag. </author> <title> Greedy attribute selection. </title> <booktitle> In Machine Learning: Proc 11th Intl Conf, </booktitle> <pages> pages 28-36, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Distance here is the KL-distance [89] or equivalently, cross-entropy. A few examples of the wrapper model are forward selection (backward elimination), forward stepwise selection (backward stepwise elimination), Optimal Brain Damage and SET-GEN. One of the simplest hillclimbing feature selection methods is forward selection <ref> [36] </ref>. Initially, the candidate set of features is empty. At each iteration, one feature is greedily added to the candidate set as the one that provides the maximum increase in a generalization estimate . Backward elimination [36] begins with the candidate set of features consisting of all original problem features. <p> One of the simplest hillclimbing feature selection methods is forward selection <ref> [36] </ref>. Initially, the candidate set of features is empty. At each iteration, one feature is greedily added to the candidate set as the one that provides the maximum increase in a generalization estimate . Backward elimination [36] begins with the candidate set of features consisting of all original problem features. At each iteration, one feature is greedily removed from the candidate set so that the resulting smaller subset corresponds to a maximum increase in estimated generalization. Iterations cease when a given generalization increase is not observed. <p> Note that the method of [88] is a backward-elimination scheme which scores a given feature subset via cross-entropy or KL-distance [89] instead of utilizing the performance 15 of the classifier computed by the induction algorithm. Forward stepwise selection <ref> [36] </ref> is a form of bidirectional hillclimbing in that at each step, one feature is allowed to be added and one feature is removed from the candidate subset. Forward stepwise selection differs from backward stepwise elimination [36] only in that the former begins with an empty candidate set and the latter <p> Forward stepwise selection <ref> [36] </ref> is a form of bidirectional hillclimbing in that at each step, one feature is allowed to be added and one feature is removed from the candidate subset. Forward stepwise selection differs from backward stepwise elimination [36] only in that the former begins with an empty candidate set and the latter with a candidate set consisting of all original features. These searches must be monitored to prevent cycling. There are further generalizations of these basic procedures such as backward stepwise elimination-SLASH (BSE-SLASH) [36] and Greedy Sequential Backward <p> from backward stepwise elimination <ref> [36] </ref> only in that the former begins with an empty candidate set and the latter with a candidate set consisting of all original features. These searches must be monitored to prevent cycling. There are further generalizations of these basic procedures such as backward stepwise elimination-SLASH (BSE-SLASH) [36] and Greedy Sequential Backward Elimination (GBSE) [30]. Optimal Brain Damage (OBD) [92] is a procedure to set the weights of an artificial neural network (ANN) to zero, but when applied to weights associated with input features, this is synonymous with feature selection.
Reference: [37] <author> T. Cavalier and B. Melloy. </author> <title> An iterative linear programming solution to the Euclidean regression model. </title> <journal> Computers and Operations Research, </journal> <volume> 28 </volume> <pages> 781-793, </pages> <year> 1995. </year>
Reference-contexts: It is the latter computation, which is a one step replacement of an algorithm for the Euclidean Regression Problem <ref> [150, 37] </ref> which does not use squared distances and cannot be solved in one step, that makes the following k-Plane clustering algorithm possible.
Reference: [38] <author> C. Y. Chang. </author> <title> Dynamic programming as applied to feature subset selection in a pattern recognition system. </title> <journal> IEEE Trans. Syst. Man Cybern., </journal> <volume> SMC-3:166-171, </volume> <month> March </month> <year> 1973. </year>
Reference-contexts: Formulating the feature selection problem as a mathematical program has been extensively studied. In [118], stepwise techniques are utilized to avoid exhaustive enumeration of all possible feature subsets. In <ref> [38] </ref>, dynamic programming is used for the same purpose. In [123], a branch and bound method is presented that effectively rejects suboptimal subsets without direct evaluation and yields a global solution.
Reference: [39] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (autoclass): Theory and results. </title> <booktitle> In Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180, </pages> <address> Menlo Park, CA, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Probabilistic clustering methods include the COBWEB algorithm [60], AutoClass <ref> [39] </ref>, the Expectation-Maximization (EM) algorithm [50, 124] and, more recently, probabilistic graphical approaches [76, 32]. "Hard" clustering algorithms, such as k-Mean, assign data items to a single cluster whereas "soft" clustering algorithms, such as EM, assign a given data point to all clusters with a certain probability of membership.
Reference: [40] <author> Chunhui Chen and O. L. Mangasarian. </author> <title> Hybrid misclassification minimization. </title> <booktitle> Advances in Computational Mathematics, </booktitle> <volume> 5(2) </volume> <pages> 127-136, </pages> <year> 1996. </year> <month> ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-05.ps.Z. </month>
Reference-contexts: In <ref> [40] </ref>, the decision problem associated with minimizing the number of misclassifications is shown to be NP-complete and a hybrid algorithm is proposed to compute an approximate solution.
Reference: [41] <author> S. S. Chen, D. L. Donoho, and M. A. Saunders. </author> <title> Atomic decomposition by basis pursuit. </title> <type> Technical Report 479, </type> <institution> Department of Statistics, Stan-ford University, Stanford, </institution> <address> California 94305, </address> <month> February </month> <year> 1996. </year> <note> Available by http://playfair.stanford.EDU/reports/chen s/BasisPursuit.ps.Z. 151 </note>
Reference-contexts: We now turn our attention to a more general problem of obtaining a minimum support solution [106] to a linear system Ax = b. 110 Chapter 4 Parsimonious Approximation A wide range of important applications, including those in machine learning <ref> [163, 41] </ref>, can be reduced to the problem of estimating a vector x 2 R n by minimizing some norm of the residual vector Ax b arising from a system of linear equations: Ax = b; (71) where A 2 R mfin and b 2 R m , both A and
Reference: [42] <author> K. J. Cherkauer and J. W. Shavlik. </author> <title> Growing simpler decision trees to facilitate knowledge discoverey. </title> <booktitle> In Proceedings, Second International Conference on Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: With the given parameters set to zero, steps (1)-(4) are repeated. SET-GEN <ref> [42] </ref> employs a genetic algorithm [71] to search candidate feature subsets. For each candidate subset, a C4.5 decision tree [134] is constructed using the given candidate input features and generalization ability of the decision tree is estimated by 16 ten-fold cross-validation [152].
Reference: [43] <author> S.-J. Chung. </author> <title> NP-completeness of the linear complementarity problem. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 60 </volume> <pages> 393-399, </pages> <year> 1989. </year>
Reference-contexts: However, the FSL problem (15) is computationally difficult, in fact LPECs in general are NP-hard since they subsume the general linear complementarity problem which is NP-complete <ref> [43] </ref>. To avoid this difficulty, (15) is reformulated as a parametric bilinear program which can be easily handled by solving a finite sequence of linear programs that terminate at a stationary point [9, Algorithm 2.1]. <p> If the 2-norm or p-norm, p 6= 1; 1, is used, the objective function will be neither concave nor convex. Nevertheless, minimizing a piecewise-linear concave function on a polyhedral set is NP-hard, because the general linear complementarity problem, which is NP-complete <ref> [43] </ref>, can be reduced to such a problem [99, Lemma 1]. Given this fact, we focus our attention on effective methods for processing (49). We propose reformulating problem (49) as a bilinear program.
Reference: [44] <author> V. Chvatal. </author> <title> Linear Programming. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: In its dual form our algorithm can be interpreted as a block-column generation method related to column 57 generation methods of Gilmore-Gomory [68], Dantzig-Wolfe [49], <ref> [44, pp 198-200,428-429] </ref> and others [122, pp 243-248].
Reference: [45] <author> F. Cordellier and J. Ch. Fiorot. </author> <title> On the Fermat-Weber problem with convex cost functionals. </title> <journal> Mathematical Programming, </journal> <volume> 14 </volume> <pages> 295-311, </pages> <year> 1978. </year>
Reference-contexts: Without the squared distance term, the subproblem of the k-Mean algorithm becomes the considerably harder Weber problem <ref> [128, 45] </ref> which locates a center in R n closest in sum of Euclidean distances (not their squares) to a given finite set of points. The Weber problem has no closed form solution.
Reference: [46] <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-279, </pages> <year> 1995. </year>
Reference-contexts: In the case where the training data are linearly inseparable (training error &gt; 0), one attempts to minimize the separation error and the norm of w simultaneously. Applications of support vector machine classifiers include isolated handwritten digit recognition <ref> [46, 144, 145, 34] </ref>, object recognition [12], face detection [127] and text categorization [82].
Reference: [47] <author> R. W. Cottle, J.-S. Pang, and R. E. Stone. </author> <title> The Linear Complementarity Problem. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: &gt; &gt; &gt; = ; We make use of the following lemma [103, Lemma 2.3] in order to model the step function of (9) exactly, as a linear program with equilibrium constraints (LPEC) [101, 94]. (The "equilibrium" terminology refers to the term involving the ? condition which characterizes complementarity problems <ref> [47] </ref>.) Lemma 2.1.1 Let a 2 R m .
Reference: [48] <author> CPLEX Optimization Inc., </author> <title> Incline Village, Nevada. Using the CPLEX(TM) Linear Optimizer and CPLEX(TM) Mixed Integer Optimizer (Version 2.0), </title> <year> 1992. </year>
Reference-contexts: These features are deemed relevant to the prognosis problem. All linear programs formulations were solved using the CPLEX package <ref> [48] </ref> called from within MATLAB [110]. The quadratic programming problem (36) was solved using the MINOS optimization package [120] called from the GAMS modeling environment [31]. Table 7 summarizes average solve times for the Ionosphere dataset, the largest in this numerical comparison. Table 7: Average running times: Ionosphere data set. <p> The time needed by each approach to compute a solution was determined by performing a single run on a Sun SparcStation 20 with 96 megabytes of memory running MATLAB 5.1, using the commands "tic" and "toc" [110]. All linear programs were solved with CPLEX <ref> [48] </ref> interfaced with MATLAB. Solving the PLNA problem with = 0:5 with initial point (93) and ff = 5 took 0.4603 seconds. Solving the LLNA problem with = 0:5 took 0.1978 seconds. Determining the least squares solution by Algorithm 4.5.1 with t = 0:0001 took 0.0224 seconds.
Reference: [49] <author> G. B. Dantzig and P. Wolfe. </author> <title> Decomposition principle for linear programs. </title> <journal> Operations Research, </journal> <volume> 8 </volume> <pages> 101-111, </pages> <year> 1960. </year>
Reference-contexts: In its dual form our algorithm can be interpreted as a block-column generation method related to column 57 generation methods of Gilmore-Gomory [68], Dantzig-Wolfe <ref> [49] </ref>, [44, pp 198-200,428-429] and others [122, pp 243-248].
Reference: [50] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the em algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year> <month> 152 </month>
Reference-contexts: Probabilistic clustering methods include the COBWEB algorithm [60], AutoClass [39], the Expectation-Maximization (EM) algorithm <ref> [50, 124] </ref> and, more recently, probabilistic graphical approaches [76, 32]. "Hard" clustering algorithms, such as k-Mean, assign data items to a single cluster whereas "soft" clustering algorithms, such as EM, assign a given data point to all clusters with a certain probability of membership.
Reference: [51] <author> P. A. Devijver and J. Kittler. </author> <title> Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: While known for being a very fast procedure, ID3 suffers when the true relation between input/output pairs utilizes interactions between features <ref> [51, 129] </ref> since it does not make use of more than one feature in any given decision. 2.1.2 Mathematical Programming Approaches As mentioned in Section 2.1, our task is to discriminate between two given sets in an n-dimensional feature space using as few of the given features as possible.
Reference: [52] <author> J. L. Devore. </author> <title> Probability and Statistics for Engineering and the Sciences. </title> <publisher> Wadsworth Publishing Company, </publisher> <address> Belmont, California, </address> <year> 1995. </year> <note> Fourth Edition. </note>
Reference-contexts: Bold indicates best. Value following "" is one standard deviation. 1 Specifically, this is the p-value of a two-tailed paired t-test testing the null hypothesis that the difference in "Test" errors for the feature selection and RLP classifiers is zero <ref> [52] </ref>. 39 Table 2: Average number of features used on the WPBC dataset. <p> On the six data sets tested, the FSV classifiers 2 Specifically, this is the p-value of a two-tailed paired t-test testing the null hypothesis that the difference in "Test" errors for the FSV and SVM 1-norm classifiers is zero <ref> [52] </ref>. 54 had a smaller test set error than the SVM-1 classifiers on three of the datasets and vice-versa on the remaining three (Table 6). The minimum p-value is 0.1246 indicating that classifiers obtained by the FSV (13) and the SVM 1-norm (35) approaches have similar generalization properties.
Reference: [53] <author> T. G. Dietterich. </author> <title> Approximate statistical tests for comparing supervised classification learning algorithms. </title> <booktitle> Neural Computation, </booktitle> <year> 1998. </year> <note> http://www.cs.orst.edu/~tgd/cv/pubs.html. </note>
Reference-contexts: A further discussion of statistical tests for comparing different learning algorithms which construct estimators ^g can be found in <ref> [53] </ref>. 1.2 Machine Learning: Unsupervised Learning The fundamental difference between an unsupervised learning task and a supervised learning task is that the elements from the input space X are not tagged with their corresponding element in the output space Y in unsupervised learning. <p> We compare generalization ability by the re-sampled paired t test <ref> [53] </ref>. This statistical test, which is most popular in the machine learning literature, consists of conducting 30 trials. <p> The second is between the SVM-2 classifiers and the RLP (7) classifiers with p-value = 0.0071. Note that applying the paired t-test to 10-fold cross validation results may indicate a difference in the average test set correctness when one is not present <ref> [53] </ref>. Thus the results of these experiments may be more similar than indicated by the p-values.
Reference: [54] <author> H. Drucker, C. J. C. Burges, L. Kaufman, A. Smola, and V. Vapnik. </author> <title> Support vector regression machines. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems -9-, </booktitle> <pages> pages 155-161, </pages> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Applications of support vector machine classifiers include isolated handwritten digit recognition [46, 144, 145, 34], object recognition [12], face detection [127] and text categorization [82]. The support vector machine has been extended beyond classification tasks to regression problems <ref> [149, 159, 54] </ref> and principle component analysis (PCA) [143, 146]. 2.2.1 SVM Mathematical Programs The support vector machine classifier is obtained by solving an optimization problem with an objective function which balances a term forcing separation between A and B 45 and a term maximizing the margin of separation or equivalently,
Reference: [55] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Clustering each of these datasets on the Computer Sciences Department Ironsides Cluster (see Section 2.3.2) requires time on the order of seconds. Hence a "repetition with random restarts" strategy was employed <ref> [55] </ref> to address the problem of local optimality (recall both the k-Median and k-Mean solutions are dependent upon the 87 initial cluster centers). Sets of initial centers were generated by sampling 20 sets of k = 3 cluster centers from a uniform distribution on the range of the data. <p> Although a global solution to the 142 problem cannot be guaranteed, the finite and simple k-Median Algorithm 3.1.3 is able to compute useful clustering solutions when initialized using a random reset strategy <ref> [55] </ref>. The utility of the k-Median Algorithm 3.1.3 and k-Mean Algorithm 3.1.4 as data mining tools were evaluated over moderately sized medical and other datasets. Both algorithms were able to identify three populations in both the Wisconsin Prognostic Breast Cancer Database (WPBC) and the SEER database with distinct survival characteristics.
Reference: [56] <author> U. Fayyad, D. Haussler, and P. Stolorz. </author> <title> KDD for science data analysis: Issues and examples. </title> <booktitle> In Proceedings, Second International Conference on Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: This multi-disciplinary field is driven by the need to extract useful information from enormous amounts of data collected in areas ranging from astronomy <ref> [56] </ref> to business and industrial domains [130]. The KDD process consists of numerous interactive and iterative steps enabling "knowledge" to evolve from a given store of "raw" data.
Reference: [57] <author> U. Fayyad, G. Piatetsky-Shapiro, and Padhraic Smyth. </author> <title> Knowledge discovery and data mining: Towards a unifying framework. </title> <booktitle> In Proceedings, Second International Conference on Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: introduce a process for extracting useful information from 5 collected data utilizing supervised and unsupervised machine learning methods. 1.3 Knowledge Discovery in Databases Knowledge discovery in databases (KDD) has been defined as ": : : the nontrivial process of identifying valid, novel, potentially useful and ultimately understandable patterns in data" <ref> [57] </ref>. This multi-disciplinary field is driven by the need to extract useful information from enormous amounts of data collected in areas ranging from astronomy [56] to business and industrial domains [130]. <p> Depending on the goals of the user, different data mining methods may be employed in the KDD process. Two primary data mining methods are classification and clustering <ref> [57] </ref>. 1.3.1 Massive Dataset Issues Many popular classification and clustering algorithms [148, 63, 11, 81] require accessing individual data items an arbitrary number of times, or require data to be resident in memory for effective processing.
Reference: [58] <author> U. M. Fayyad. </author> <title> Editorial. Data Mining and Knowledge Discovery, </title> <booktitle> 1(1) </booktitle> <pages> 5-10, </pages> <year> 1997. </year> <month> 153 </month>
Reference-contexts: These steps include data selection, preprocessing and transformation to put the raw data in a form suitable for the application of one (or more) data mining algorithms. Any algorithm that enumerates patterns from, or fits models, to data is a data mining algorithm <ref> [58] </ref>. Many algorithms from pattern recognition, statistics, databases, machine learning and 6 optimization qualify as data mining algorithms and give the field of KDD its multidisciplinary character. Related fields further include high performance and parallel computing, knowledge modeling, management of uncertainty, and data visualization [58]. <p> data is a data mining algorithm <ref> [58] </ref>. Many algorithms from pattern recognition, statistics, databases, machine learning and 6 optimization qualify as data mining algorithms and give the field of KDD its multidisciplinary character. Related fields further include high performance and parallel computing, knowledge modeling, management of uncertainty, and data visualization [58]. After the data mining algorithm has identified patterns (or computed models) of possible interest, this output must be interpreted or evaluated.
Reference: [59] <author> U. M. Fayyad, C. Reina, and P. S. Bradley. </author> <title> Initialization of iterative refinement clustering algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> KDD98, </address> <year> 1998. </year> <note> http://www.cs.wisc.edu/~paulb/papers.html. </note>
Reference-contexts: The algorithms presented above are shown to be effective data mining tools over moderately sized datasets, but have serious computational drawbacks when applied to massive datasets. In <ref> [18, 59] </ref>, an algorithm is proposed for refining a given clustering initialization targeted at clustering massive datasets. In [19, 20], a scalable clustering framework is presented and instantiated on the k-Mean and EM algorithms. A scalable clustering approach is also presented in [168]. <p> Note that the k-Median Algorithm 3.1.3 computes a solution which is locally opti mal. We cannot guaranteed a solution which is globally optimal. The issue of initializing iterative clustering approaches is discussed in <ref> [18, 59] </ref>. <p> Useful solutions were computed via the Successive Linearization Algorithms 2.1.3 and 4.4.1 and the k-Median Algorithm 3.1.3 with random initializations. Knowledge about the particular problem domain in which these algorithms are applied may lead to a better initialization procedure. We note that issues regarding clustering initialization are provided in <ref> [18, 59] </ref>. 144 Analysis of the classifiers computed by the support vector machine approach utilizing different norms to measure the margin of separation is needed.
Reference: [60] <author> D. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: Probabilistic clustering methods include the COBWEB algorithm <ref> [60] </ref>, AutoClass [39], the Expectation-Maximization (EM) algorithm [50, 124] and, more recently, probabilistic graphical approaches [76, 32]. "Hard" clustering algorithms, such as k-Mean, assign data items to a single cluster whereas "soft" clustering algorithms, such as EM, assign a given data point to all clusters with a certain probability of membership. <p> Many approaches to this problem include statistical [81, 63], machine learning <ref> [60] </ref>, integer and mathematical programming [147, 3, 135].
Reference: [61] <author> E. W. Forgy. </author> <title> Cluster analysis of multivariate data: Efficiency versus interpretability of classifications. </title> <journal> Biometric Soc. </journal> <note> Meetings, Riverside CA (Abstract in Bio-metrics 21, No. 3, 768), </note> <year> 1965. </year>
Reference-contexts: In the clustering problem [81, 63], the goal is to group or cluster the data into sets of "like" points. One hopes to obtain clusters revealing some sort of high-level characterization of the points belonging to individual clusters. "Exemplar" or "prototype"-based clustering approaches include Forgy's method <ref> [61] </ref>, the MacQueen algorithm [95] (commonly referred to as "batch" and "online" k-Mean clustering), Kohonen maps [87], and competitive learning [141].
Reference: [62] <author> M. Frank and P. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: The method which we propose for solving the FSV problem is the successive linearization approximation (SLA) method of minimizing a concave function on a polyhedral set which is a finitely terminating stepless Frank-Wolfe algorithm <ref> [62] </ref>. In [103] finite termination of the SLA was established for a differentiable concave function, and in [107] for a nondifferentiable concave function using its supergradient 31 (2). We now state the algorithm. Algorithm 2.1.3 Successive Linearization Algorithm (SLA) for FSV (13). Choose 2 [0; 1). <p> We now turn our attention to solving (83) by a finitely terminating successive linearization algorithm. 4.4 The Concave Minimization Algorithm The finite method that we propose is the successive linear approximation (SLA) method of minimizing a concave function on a polyhedral set which is a finitely terminating stepless Frank-Wolfe algorithm <ref> [62] </ref>, similar to Algorithm 2.1.3. We state now the SLA for problem (83) which has a differentiable concave objective. Algorithm 4.4.1 Successive Linearization Algorithm (SLA) for (83). Choose 2 [0; 1). Start with a random x 0 2 R n .
Reference: [63] <author> K. Fukunaga. </author> <title> Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: The motivation for an unsupervised learning algorithm is to "look for regularities" in the training examples fx i g M i=1 X [148]. Unsupervised learning algorithms can be divided into general "discovery" systems (e.g. AM [93], BACON [91]) and those which perform 4 "clustering" [148]. In the clustering problem <ref> [81, 63] </ref>, the goal is to group or cluster the data into sets of "like" points. <p> Depending on the goals of the user, different data mining methods may be employed in the KDD process. Two primary data mining methods are classification and clustering [57]. 1.3.1 Massive Dataset Issues Many popular classification and clustering algorithms <ref> [148, 63, 11, 81] </ref> require accessing individual data items an arbitrary number of times, or require data to be resident in memory for effective processing. These approaches play powerful roles as data mining 7 algorithms over data sets of modest size. <p> The clustering problem is first formulated as minimizing a piecewise linear concave function over a polyhedral set resulting in the k-Median algorithm. The k-Mean algorithm <ref> [147, 81, 63] </ref> computes a local solution to the problem of minimizing a nonconvex objective over a polyhedral set. We examine a novel approach, k-Plane clustering, where clusters are characterized by planes in R n , obtainable by solving an eigenvalue problem for each cluster. <p> We next consider methods in which the clusters themselves are described by a centroid in R n . 80 3.1 Clustering to Centroids We consider the unsupervised assignment of elements of a given set to groups or clusters of like points. Many approaches to this problem include statistical <ref> [81, 63] </ref>, machine learning [60], integer and mathematical programming [147, 3, 135]. <p> In contrast the k-Mean algorithm uses squares of 2-norm distances to generate cluster centers, which may be sensitive to outliers. We note that clustering algorithms based on statistical assumptions that minimize some function of scatter matrices do not appear to have convergence proofs <ref> [63, pp. 508-515] </ref>. However convergence of a class of k-means-type algorithms to local solutions is given in [147]. <p> This algorithm requires supergradients of the objective which are available in (49) and the resulting procedure reduces to Algorithm 3.1.3. We next discuss the k-Mean approach to clustering <ref> [147, 81, 63] </ref> . 3.1.2 k-Mean Algorithm The k-Mean algorithm is a standard technique for clustering.
Reference: [64] <author> P. P. Gallo. </author> <title> Consistency of regression estimates when some variables are subject to error. </title> <journal> Comm. Statist. Theory Methods, </journal> <volume> 11 </volume> <pages> 973-983, </pages> <year> 1982. </year>
Reference-contexts: If [A; b] = [A 0 + 4A; b 0 + 4b], where [4A; 4b] are uncorrelated random variables with zero mean and equal variance, it can be shown <ref> [64, 70] </ref> that x TLS estimates x 0 := A 0 b 0 consistently in the sense that x TLS converges to x 0 as m tends to infinity.
Reference: [65] <author> U. M. Garcia Palomares and O. L. Mangasarian. </author> <title> Superlinearly convergent Quasi-Newton algorithms for nonlinearly constrained optimization problems. </title> <journal> Mathematical Programming, </journal> <volume> 11 </volume> <pages> 1-13, </pages> <year> 1976. </year>
Reference-contexts: All but the 140 OBD Algorithm 2.1.6 either improve generalization or produce classifiers with "equivalent" performance on the Ionosphere classification task, while utilizing a fraction of the original problem features. The FSS problem (12) is solved by iterative quadratic programming <ref> [65, 74, 67] </ref>. The FSV problem (13) is solved by solving a sequence of linear programs via the Successive Linearization Algorithm 2.1.3, which is a general tool for solving problems with a concave objective over a polyhedral region [103]. The FSV problem (16) is solved by [9, Algorithm 2.1].
Reference: [66] <author> P. E. Gill. </author> <title> Private communication, </title> <month> July 16 </month> <year> 1998. </year>
Reference-contexts: Upon the suggestion of Philip E. Gill <ref> [66] </ref>, the LPC Algorithm 2.3.1 was applied to the WOODW linear programming problem. The WOODW problem data is publically available from the Netlib repository [1]. The primal linear program consists of 1085 equality constraints and 13 inequality constraints. There are 8405 variables. The optimal objective value is 1.3044763331.
Reference: [67] <author> P. E. Gill, W. Murray, and M. H. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1981. </year> <month> 154 </month>
Reference-contexts: All but the 140 OBD Algorithm 2.1.6 either improve generalization or produce classifiers with "equivalent" performance on the Ionosphere classification task, while utilizing a fraction of the original problem features. The FSS problem (12) is solved by iterative quadratic programming <ref> [65, 74, 67] </ref>. The FSV problem (13) is solved by solving a sequence of linear programs via the Successive Linearization Algorithm 2.1.3, which is a general tool for solving problems with a concave objective over a polyhedral region [103]. The FSV problem (16) is solved by [9, Algorithm 2.1].
Reference: [68] <author> P. C. Gilmore and R. E. Gomory. </author> <title> A linear programming approach to the cutting stock problem. </title> <journal> Operations Research, </journal> <volume> 9 </volume> <pages> 849-859, </pages> <year> 1961. </year>
Reference-contexts: In its dual form our algorithm can be interpreted as a block-column generation method related to column 57 generation methods of Gilmore-Gomory <ref> [68] </ref>, Dantzig-Wolfe [49], [44, pp 198-200,428-429] and others [122, pp 243-248].
Reference: [69] <author> A. A. Giordano. </author> <title> Least Square Estimation With Applications to Digital Signal Processing. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: One can relate this to a machine learning framework by treating the observed system as a training set, and the true system as a testing set [75]. The linear systems used are based upon ideas related to signal processing <ref> [69, 155] </ref> and more specifically to an example in [2, Equation (8)].
Reference: [70] <author> L. J. Gleser. </author> <title> Estimation in a multivariate "errors in variables" regression model: Large sample results. </title> <journal> Annals of Statistics, </journal> <volume> 9 </volume> <pages> 24-44, </pages> <year> 1981. </year>
Reference-contexts: If [A; b] = [A 0 + 4A; b 0 + 4b], where [4A; 4b] are uncorrelated random variables with zero mean and equal variance, it can be shown <ref> [64, 70] </ref> that x TLS estimates x 0 := A 0 b 0 consistently in the sense that x TLS converges to x 0 as m tends to infinity.
Reference: [71] <author> D. E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading MA, </address> <year> 1989. </year>
Reference-contexts: With the given parameters set to zero, steps (1)-(4) are repeated. SET-GEN [42] employs a genetic algorithm <ref> [71] </ref> to search candidate feature subsets. For each candidate subset, a C4.5 decision tree [134] is constructed using the given candidate input features and generalization ability of the decision tree is estimated by 16 ten-fold cross-validation [152].
Reference: [72] <author> G. H. Golub and C. F. Van Loan. </author> <title> An analysis of the total least squares problem. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17 </volume> <pages> 883-893, </pages> <year> 1980. </year>
Reference-contexts: The total least squares (TLS) problem <ref> [72, 80] </ref> (or orthogonal regression, or errors-in-variables regression) addresses the following optimization problem: min n fi fi ^ b 2 R ( ^ A) ; (73) where k k F is the Frobenius norm of a matrix and R ( ^ A) is the column space of the matrix ^ A.
Reference: [73] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The John Hopkins University Press, </publisher> <address> Baltimore, Maryland, </address> <year> 1983. </year>
Reference-contexts: In addition, in some applications, errors may only occur in certain elements of A, in which case E is sparse. But the computational method most often employed to solve the TLS problem is based on the singular value decomposition (SVD) <ref> [73] </ref>, which is not in general sparsity preserving or structure preserving. In contrast structured total least norm (STLN) methods [138, 79] permit a known structure in A and [A; b] to be preserved in A + E and [A + E; b + r].
Reference: [74] <author> S.-P. Han. </author> <title> Superlinearly convergent variable metric algorithms for general nonlinear programming problems. </title> <journal> Mathematical Programming, </journal> <volume> 11 </volume> <pages> 263-282, </pages> <year> 1976. </year>
Reference-contexts: All but the 140 OBD Algorithm 2.1.6 either improve generalization or produce classifiers with "equivalent" performance on the Ionosphere classification task, while utilizing a fraction of the original problem features. The FSS problem (12) is solved by iterative quadratic programming <ref> [65, 74, 67] </ref>. The FSV problem (13) is solved by solving a sequence of linear programs via the Successive Linearization Algorithm 2.1.3, which is a general tool for solving problems with a concave objective over a polyhedral region [103]. The FSV problem (16) is solved by [9, Algorithm 2.1].
Reference: [75] <author> M. H. Hassoun. </author> <title> Fundamentals of Artificial Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1995. </year>
Reference-contexts: One can relate this to a machine learning framework by treating the observed system as a training set, and the true system as a testing set <ref> [75] </ref>. The linear systems used are based upon ideas related to signal processing [69, 155] and more specifically to an example in [2, Equation (8)].
Reference: [76] <author> D. Heckerman. </author> <title> Bayesian networks for knowledge discovery. </title> <booktitle> In Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 273-305, </pages> <address> Menlo Park, CA, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Examples include a separating-plane based function [96, 97], the backpropagation algorithm for artificial neural networks (ANNs) [77, 111], decision tree construction algorithms utilizing various node-decision criteria [29, 134, 5], spline methods for classification [165, 162] and probabilistic graphical dependency models <ref> [76, 32] </ref>. Evaluating an estimate ^g of g in terms of how well it performs on data not included in the training set, or how well ^g generalizes, is paramount. <p> Probabilistic clustering methods include the COBWEB algorithm [60], AutoClass [39], the Expectation-Maximization (EM) algorithm [50, 124] and, more recently, probabilistic graphical approaches <ref> [76, 32] </ref>. "Hard" clustering algorithms, such as k-Mean, assign data items to a single cluster whereas "soft" clustering algorithms, such as EM, assign a given data point to all clusters with a certain probability of membership.
Reference: [77] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year> <month> 155 </month>
Reference-contexts: Examples include a separating-plane based function [96, 97], the backpropagation algorithm for artificial neural networks (ANNs) <ref> [77, 111] </ref>, decision tree construction algorithms utilizing various node-decision criteria [29, 134, 5], spline methods for classification [165, 162] and probabilistic graphical dependency models [76, 32]. <p> given point sets A and B in R n , we shall mean a plane that attempts to separate R n into two half spaces such that 10 each open halfspace contains points mostly of A or B: Alternatively, such a plane can also be interpreted as a classical perceptron <ref> [140, 77, 100] </ref>. * For a function f : R n ! R that is concave on R n , the supergradient @f (x) of f at x is a vector in R n satisfying f (y) f (x) @f (x)(y x) (2) for any y 2 R n . <p> But, with a large number of features and a fixed number of training examples, one encounters the "curse of dimensionality"; a high-dimensional space with a modest number of data points is almost empty <ref> [77] </ref>. Thus, with a large number of original problem features and a finite amount of training data, many different estimators may accurately separate the training data, but only a small fraction may generalize well. <p> Since the function e 0 v fl is discontinuous on the non-negative orthant, we make a continuous approximation of it using either the standard sigmoid function of neural networks <ref> [139, 77] </ref> or by a concave exponential on the nonnegative real line [103].
Reference: [78] <author> P. J. Huber. </author> <title> Robust Statistics. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: [79] <author> S. Van Huffel, H. Park, and J. B. Rosen. </author> <title> Formulation and solution of structured total least norm problems for parameter estimation. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 44 </volume> <pages> 2464-2474, </pages> <year> 1996. </year>
Reference-contexts: But the computational method most often employed to solve the TLS problem is based on the singular value decomposition (SVD) [73], which is not in general sparsity preserving or structure preserving. In contrast structured total least norm (STLN) methods <ref> [138, 79] </ref> permit a known structure in A and [A; b] to be preserved in A + E and [A + E; b + r]. Furthermore, the problem can be formulated so as to minimize the 1-norm, 1-norm or the Frobenius norm used in the TLS problem.
Reference: [80] <author> S. Van Huffel and J. Vandewalle. </author> <title> The Total Least Squares Problem, Computational Aspects and Analysis. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1991. </year>
Reference-contexts: These include least squares, total least squares and structured total least norm. The least squares (LS) solution, x LS to (71) is obtained by solving the following problem <ref> [80] </ref>: 111 x2R n kAx bk 2 : (72) Suppose that the RHS b is corrupted by uncorrelated random variables with zero mean and equal variance, then the estimate x LS has the smallest variance in the class of estimation methods which fulfill the following two conditions: (1) the estimate is <p> RHS b is corrupted by uncorrelated random variables with zero mean and equal variance, then the estimate x LS has the smallest variance in the class of estimation methods which fulfill the following two conditions: (1) the estimate is unbiased, and (2) the estimate is a linear function of b <ref> [80] </ref>. <p> The total least squares (TLS) problem <ref> [72, 80] </ref> (or orthogonal regression, or errors-in-variables regression) addresses the following optimization problem: min n fi fi ^ b 2 R ( ^ A) ; (73) where k k F is the Frobenius norm of a matrix and R ( ^ A) is the column space of the matrix ^ A. <p> Here A 0 is the pseudo-inverse of A 0 obtained from the singular value decomposition of A 0 <ref> [80] </ref>. 112 We formulate the total least squares problem as in [138] which leads us to a description of the structured total least norm problem.
Reference: [81] <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: The motivation for an unsupervised learning algorithm is to "look for regularities" in the training examples fx i g M i=1 X [148]. Unsupervised learning algorithms can be divided into general "discovery" systems (e.g. AM [93], BACON [91]) and those which perform 4 "clustering" [148]. In the clustering problem <ref> [81, 63] </ref>, the goal is to group or cluster the data into sets of "like" points. <p> Depending on the goals of the user, different data mining methods may be employed in the KDD process. Two primary data mining methods are classification and clustering [57]. 1.3.1 Massive Dataset Issues Many popular classification and clustering algorithms <ref> [148, 63, 11, 81] </ref> require accessing individual data items an arbitrary number of times, or require data to be resident in memory for effective processing. These approaches play powerful roles as data mining 7 algorithms over data sets of modest size. <p> The clustering problem is first formulated as minimizing a piecewise linear concave function over a polyhedral set resulting in the k-Median algorithm. The k-Mean algorithm <ref> [147, 81, 63] </ref> computes a local solution to the problem of minimizing a nonconvex objective over a polyhedral set. We examine a novel approach, k-Plane clustering, where clusters are characterized by planes in R n , obtainable by solving an eigenvalue problem for each cluster. <p> We next consider methods in which the clusters themselves are described by a centroid in R n . 80 3.1 Clustering to Centroids We consider the unsupervised assignment of elements of a given set to groups or clusters of like points. Many approaches to this problem include statistical <ref> [81, 63] </ref>, machine learning [60], integer and mathematical programming [147, 3, 135]. <p> This algorithm requires supergradients of the objective which are available in (49) and the resulting procedure reduces to Algorithm 3.1.3. We next discuss the k-Mean approach to clustering <ref> [147, 81, 63] </ref> . 3.1.2 k-Mean Algorithm The k-Mean algorithm is a standard technique for clustering.
Reference: [82] <author> T. Joachims. </author> <title> Text categorization with support vector machines. </title> <type> Technical Report LS VII Number 23, </type> <institution> University of Dortmund, </institution> <year> 1997. </year> <month> ftp://ftp-ai.infomatik.uni-dortmund.de/pub/Reports/report23.ps.Z. </month>
Reference-contexts: Applications of support vector machine classifiers include isolated handwritten digit recognition [46, 144, 145, 34], object recognition [12], face detection [127] and text categorization <ref> [82] </ref>.
Reference: [83] <author> G. H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pages 121-129, </pages> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: of eliminating as many features from the original n-dimensional feature space as possible, while still accurately estimating g to some extent over the training set. 13 2.1.1 Machine Learning Approaches Machine learning approaches to feature subset selection can basically be divided along the lines of filter models and wrapper models <ref> [83] </ref>. In the filter model, feature selection is done as a preprocessing step which ignores the effects of the selected subset on the performance of the estimator constructed by a given algorithm. In contrast, the wrapper model encompasses algorithms which utilize the estimator ^g to evaluate a given feature subset. <p> The FOCUS algorithm [4] performs an exhaustive search through the possible feature subsets and returns the minimal subset sufficient for classification. FOCUS may not be able to identify irrelevant features when noise is present in the training set, or if the training set is not representative of future data <ref> [83] </ref>. The Relief algorithm [85] assigns a weight to each feature which corresponds to the "relevance" of that feature as represented in the training set. <p> We consider the algorithms which compute solutions to the FSS, FSV and FSB problems to be related to wrapper models of feature selection <ref> [83] </ref>. As the FSS, FSV and FSB problems are being solved, the benefit of removing a problem feature (i.e. setting a given w j to zero) is balanced by an increase (if any) in the term measuring separation error of the separating plane w 0 x = fl. <p> Then ffi 2 in Step (ii) is chosen as . Thus has the effect of suppressing all components of w when = 1 and suppressing no components when = 0. This OBD adaptation is considered a wrapper model <ref> [83] </ref>. 2.1.4 Numerical Results We evaluated the proposed algorithms for solving the FSS problem (12), the FSV problem (13), the FSB problem (16), and the OBD adaptation (Algorithm 2.1.6), in 35 comparison with solutions produced by the RLP (7).
Reference: [84] <author> E. L. Kaplan and P. Meier. </author> <title> Nonparametric estimation from incomplete observations. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 53 </volume> <pages> 457-481, </pages> <year> 1958. </year>
Reference-contexts: It is precisely the mean that is used in the k-Mean algorithm subproblem. We focus on evaluating the k-Median Algorithm 3.1.3 and the k-Mean Algorithm 3.1.4 in three clustering tasks. 3.1.3 Data Mining Survival Curves In many medical domains, survival curves <ref> [84] </ref> are important prognostic tools. Our goal was use the k-Median Algorithm 3.1.3 and the k-Mean Algorithm 3.1.4 as data mining tools in a KDD process over two medical datasets to identify groups with distinct survival characteristics. We used an altered version of the WPBC dataset [119]. <p> When referring to the SEER dataset, we are referring to this set of 21,960 points in R 2 . We applied the k-Median and k-Mean algorithms with k = 3, as data mining tools, to the WPBC and SEER datasets. Survival curves <ref> [84] </ref> were then constructed for each cluster, representing expected percent of surviving patients as a function of time, for patients in that cluster. <p> In the second set of tests the ability to recover class labels by clustering unlabeled data was tested. The first set of tests consisted of applying the k-Plane algorithm to a subset of the WPBC dataset [119] with goal of obtaining separated survival curves <ref> [84] </ref>. The two features of tumor size and lymph node status were used (see Section 3.1.3). Both features were normalized to have zero mean and standard deviation = 1. This dataset consists of 198 points in R 2 .
Reference: [85] <author> K. Kira and L. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 129-134, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In contrast, the wrapper model encompasses algorithms which utilize the estimator ^g to evaluate a given feature subset. Instances of the filter model include the FOCUS algorithm [4], the Relief algorithm <ref> [85] </ref> and a method of feature selection based upon Information Theory [88]. The FOCUS algorithm [4] performs an exhaustive search through the possible feature subsets and returns the minimal subset sufficient for classification. <p> FOCUS may not be able to identify irrelevant features when noise is present in the training set, or if the training set is not representative of future data [83]. The Relief algorithm <ref> [85] </ref> assigns a weight to each feature which corresponds to the "relevance" of that feature as represented in the training set. <p> The relevance of a feature is then an average of these values over the sampled points from the training set. Since each feature relevance is determined independently of other features, Relief is unable to identify redundant features <ref> [85] </ref>.
Reference: [86] <author> David G. Kleinbaum. </author> <title> Survival Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1996. </year> <month> 156 </month>
Reference-contexts: Evaluating whether the survival characteristics are distinct for each cluster could also be performed by comparing p-values for pairwise log rank statistics computed from each survival curve <ref> [86] </ref>. The survival curves of clusters determined by the k-Median Algorithm 3.1.3 (Figure 13) are more separated than those determined by the k-Mean Algorithm 3.1.4 (Figure 14). These results indicate the utility of these clustering approaches as data mining tools.
Reference: [87] <author> T. Kohonen. </author> <title> Self-organized formation of typologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69, </pages> <year> 1982. </year>
Reference-contexts: One hopes to obtain clusters revealing some sort of high-level characterization of the points belonging to individual clusters. "Exemplar" or "prototype"-based clustering approaches include Forgy's method [61], the MacQueen algorithm [95] (commonly referred to as "batch" and "online" k-Mean clustering), Kohonen maps <ref> [87] </ref>, and competitive learning [141].
Reference: [88] <author> K. Koller and M. Sahami. </author> <title> Toward optimal feature selection. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <address> San Mateo, CA, 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In contrast, the wrapper model encompasses algorithms which utilize the estimator ^g to evaluate a given feature subset. Instances of the filter model include the FOCUS algorithm [4], the Relief algorithm [85] and a method of feature selection based upon Information Theory <ref> [88] </ref>. The FOCUS algorithm [4] performs an exhaustive search through the possible feature subsets and returns the minimal subset sufficient for classification. <p> In a domain in which a relevant group of features are highly correlated, Relief will assign to each of the correlated features a large relevance value and not notice the redundancy. 14 The feature selection method proposed in <ref> [88] </ref> attempts to compute a reduced feature set such that the probability distribution of the class given the reduced feature set is "near" to the distribution of the class given the full feature set. <p> Notice that in forward selection, once a feature has been added to the candidate set, it will not be removed. Similarly, in backward elimination, once a feature has been removed, it cannot again re-enter the candidate set. Note that the method of <ref> [88] </ref> is a backward-elimination scheme which scores a given feature subset via cross-entropy or KL-distance [89] instead of utilizing the performance 15 of the classifier computed by the induction algorithm.
Reference: [89] <author> S. Kullback and R. A. Leibler. </author> <title> On information and sufficiency. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22 </volume> <pages> 76-86, </pages> <year> 1951. </year>
Reference-contexts: The idea is to remove a feature such that the probability distribution of the class given the reduced feature subset is as "near" to the distribution of the class given the full feature subset. Distance here is the KL-distance <ref> [89] </ref> or equivalently, cross-entropy. A few examples of the wrapper model are forward selection (backward elimination), forward stepwise selection (backward stepwise elimination), Optimal Brain Damage and SET-GEN. One of the simplest hillclimbing feature selection methods is forward selection [36]. Initially, the candidate set of features is empty. <p> Similarly, in backward elimination, once a feature has been removed, it cannot again re-enter the candidate set. Note that the method of [88] is a backward-elimination scheme which scores a given feature subset via cross-entropy or KL-distance <ref> [89] </ref> instead of utilizing the performance 15 of the classifier computed by the induction algorithm. Forward stepwise selection [36] is a form of bidirectional hillclimbing in that at each step, one feature is allowed to be added and one feature is removed from the candidate subset.
Reference: [90] <author> P. A. Lahenbruch and R. M. Mickey. </author> <title> Estimation of error rates in discriminant analysis. </title> <journal> Technometrics, </journal> <volume> 10 </volume> <pages> 1-11, </pages> <year> 1968. </year>
Reference-contexts: Common techniques to avoid overtraining are early stopping and weight decay [16]. The latter can also be viewed as an overfitting avoidance technique. Generalization ability can be effectively estimated by leave-one-out testing <ref> [90] </ref>, where ^g is constructed by a learning algorithm using all but one of the available training examples and is tested on the point "left out". This procedure is repeated for each data point in the training set. <p> Estimate classification correctness using the class label of the classified exam ples. (ii) Determine a separating plane (3) using the the classified examples, by solving the RLP (7). Classify individual examples by the classification function defined by the separating plane. Estimate classification correctness by a leave-one-out strategy <ref> [90] </ref> using the classified examples. 95 We simulate the situation in which only a fraction of a database has been classified by splitting the WDBC dataset into two subsets S 1 and S 2 . <p> The WDBC dataset used here is the same as described in Section 3.1.4. Test set correctness for the estimated classification function determined by solving the RLP (7) is estimated by a leave-one-out strategy <ref> [90] </ref> on set S 1 . testing subset), as a percentage of the size of the entire WDBC dataset. As expected, the performance of the classification function determined by solving the robust linear program (7) improves as the size of S 1 increases.
Reference: [91] <author> P. Langley, H. A. Simon, and G. L. Bradshaw. </author> <title> Heuristics for empirical discovery. In Computational Models of Learning, </title> <address> Berlin, 1987. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The motivation for an unsupervised learning algorithm is to "look for regularities" in the training examples fx i g M i=1 X [148]. Unsupervised learning algorithms can be divided into general "discovery" systems (e.g. AM [93], BACON <ref> [91] </ref>) and those which perform 4 "clustering" [148]. In the clustering problem [81, 63], the goal is to group or cluster the data into sets of "like" points.
Reference: [92] <author> Y. le Cun, J. S. Denker, and S. A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems II (Den-ver 1989), </booktitle> <pages> pages 598-605, </pages> <address> San Mateo, California, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These searches must be monitored to prevent cycling. There are further generalizations of these basic procedures such as backward stepwise elimination-SLASH (BSE-SLASH) [36] and Greedy Sequential Backward Elimination (GBSE) [30]. Optimal Brain Damage (OBD) <ref> [92] </ref> is a procedure to set the weights of an artificial neural network (ANN) to zero, but when applied to weights associated with input features, this is synonymous with feature selection. <p> that the following complementarity condition holds at termination: (r i+1 ) 0 v i+1 + e 0 u i+1 = (r i+1 ) 0 (u i+1 v i+1 ) + (u i+1 ) 0 (r i+1 + e) = 0: (32) OBD Adaptation We consider the Optimal Brain Damage method <ref> [92] </ref> for reducing neural network complexity and adapt it to problem (6). Because the objective function of (6) is piecewise-linear, the second derivatives on which the OBD method is based do not exist.
Reference: [93] <author> D. B. Lenat. </author> <title> The ubiquity of discovery. </title> <journal> Artificial Intelligence, </journal> <volume> 9 </volume> <pages> 257-285, </pages> <year> 1977. </year>
Reference-contexts: The motivation for an unsupervised learning algorithm is to "look for regularities" in the training examples fx i g M i=1 X [148]. Unsupervised learning algorithms can be divided into general "discovery" systems (e.g. AM <ref> [93] </ref>, BACON [91]) and those which perform 4 "clustering" [148]. In the clustering problem [81, 63], the goal is to group or cluster the data into sets of "like" points.
Reference: [94] <author> Z.-Q. Luo, J.-S. Pang, D. Ralph, and S.-Q. Wu. </author> <title> Mathematical programs with equilibrium constraints. </title> <journal> Mathematical Programming, </journal> <volume> 75 </volume> <pages> 19-76, </pages> <year> 1996. </year>
Reference-contexts: fi fi fi fi Bw efl + e z; v w v &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; = ; We make use of the following lemma [103, Lemma 2.3] in order to model the step function of (9) exactly, as a linear program with equilibrium constraints (LPEC) <ref> [101, 94] </ref>. (The "equilibrium" terminology refers to the term involving the ? condition which characterizes complementarity problems [47].) Lemma 2.1.1 Let a 2 R m .
Reference: [95] <author> J. B. MacQueen. </author> <title> Some methods for classification and analysis of multivariate observations. </title> <booktitle> In Proc. Symp. Math. Statist. and Probability, 5th, Berkeley, </booktitle> <volume> volume 1, </volume> <pages> pages 281-297, </pages> <address> Berkeley, CA, </address> <year> 1967. </year> <institution> Univ. of California Press. </institution> <type> AD 66981. 157 </type>
Reference-contexts: One hopes to obtain clusters revealing some sort of high-level characterization of the points belonging to individual clusters. "Exemplar" or "prototype"-based clustering approaches include Forgy's method [61], the MacQueen algorithm <ref> [95] </ref> (commonly referred to as "batch" and "online" k-Mean clustering), Kohonen maps [87], and competitive learning [141].
Reference: [96] <author> O. L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: We assume the true classification function g has the following form: 2 8 &gt; &gt; : 0 if x 2 B: Many different algorithms exist for constructing the approximation ^g of g and the approximation may have many different functional forms. Examples include a separating-plane based function <ref> [96, 97] </ref>, the backpropagation algorithm for artificial neural networks (ANNs) [77, 111], decision tree construction algorithms utilizing various node-decision criteria [29, 134, 5], spline methods for classification [165, 162] and probabilistic graphical dependency models [76, 32]. <p> from [108] that a solution of (36) is a solution to the following problem: min 1 kwk 2 as well as of 49 w;fl;y;z2 X 2 Nonlinear separating surfaces, which are linear in their parameters, can also easily be handled by the formulations (7), (12), (13), (16), (34) and (35) <ref> [96] </ref>. If the data are mapped nonlinearly via : R n ! R ` , a nonlinear separating surface in R n is easily computed as a linear separator in R ` . In practice, one usually solves (36) by way of its dual [98].
Reference: [97] <author> O. L. Mangasarian. </author> <title> Multi-surface method of pattern separation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:801-807, </volume> <year> 1968. </year>
Reference-contexts: We assume the true classification function g has the following form: 2 8 &gt; &gt; : 0 if x 2 B: Many different algorithms exist for constructing the approximation ^g of g and the approximation may have many different functional forms. Examples include a separating-plane based function <ref> [96, 97] </ref>, the backpropagation algorithm for artificial neural networks (ANNs) [77, 111], decision tree construction algorithms utilizing various node-decision criteria [29, 134, 5], spline methods for classification [165, 162] and probabilistic graphical dependency models [76, 32]. <p> min 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; : e 0 s fi fi fi fi fi fi fi Bw efl + e z; s w s: &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; = ; We note that an optimization formulation was proposed and implemented in <ref> [97] </ref> for computing a separating plane by forcing the bounding planes to be as far apart as possible. 48 Usually the support vector machine problem is formulated using the 2-norm in the objective [161, 6].
Reference: [98] <author> O. L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year> <note> Reprint: SIAM Classic in Applied Mathematics 10, 1994, Philadelphia. </note>
Reference-contexts: The iterates determined by (26) generate a strictly decreasing sequence of objective function values for the FSV 32 problem (13) and terminate at an iteration i with a stationary point (which may also be a global minimum solution) that satisfies the following minimum principle necessary optimality criterion <ref> [98] </ref>. (1 )( m i ) + k i )) + ff (" ffv i i ) 0; 8 feasible (w; fl; y; z; v): (28) We solve the FSV problem (13) by Algorithm 2.1.3. We now turn our attention to solving the FSB problem (16). <p> If the data are mapped nonlinearly via : R n ! R ` , a nonlinear separating surface in R n is easily computed as a linear separator in R ` . In practice, one usually solves (36) by way of its dual <ref> [98] </ref>. In this formulation, the data enter only as inner products which are computed in the transformed space via a kernel function K (x; y) = (x) (y) [33, 161, 164]. <p> To establish its validity we first prove a lemma [24, Lemma 3.3]. Lemma 2.3.3 If x solves the linear program (37) and (x; u) 2 R n+m is a Karush-Kuhn-Tucker (KKT) <ref> [98] </ref> point (i.e. a primal-dual optimal pair) such that u I &gt; 0 where I f1; : : : ; mg and u J = 0, J f1; : : : ; mg, I [ J = f1; : : : ; mg, then x 2 arg minfc 0 x jH <p> Proof The KKT conditions <ref> [98] </ref> for (37) satisfied by (x; u) are: c = H 0 u; u 0; u 0 (H x b) = 0; H x b 0; which under the condition u I &gt; 0 imply that H I x = b I ; u J = 0; H J x b <p> The primal problem has the following form: min fc 0 x j Ax = b; Bx d; x 0 g : (45) Since the LPC Algorithm 2.3.1 deals with linear programs with massive constraints, we applied it to the dual formulation <ref> [98] </ref> of (45) having 8405 constraints and 1098 variables: min fb 0 u + d 0 v j A 0 u B 0 v c; v 0 g : (46) LPC Numerical Results for Expanded WOODW Problem It was discovered empirically that selecting an initial dual subproblem with less than 6958 <p> This is a difficult problem since a local minimum is not necessarily a global minimum. By converting this problem to an equivalent bilinear program, a fast successive-linearization k-Median algorithm terminates after a few iterations at a point satisfying a minimum principle necessary optimality condition <ref> [98] </ref>. Although there is no guarantee that such a point is a global solution, numerical tests reveal that the k-Median Algorithm 3.1.3 computes useful clustering solutions in diverse domains. <p> Hence e 0 D i` bounds the 1-norm distance between A i and center C ` . Note that since the objective function of (49) is the sum of the minima of k linear (and hence concave) functions, it is a piecewise-linear concave function <ref> [98, Corollary 4.1.14] </ref>. If the 2-norm or p-norm, p 6= 1; 1, is used, the objective function will be neither concave nor convex. <p> The iterates determined by (84) generate a strictly decreasing sequence of objective function values for (83) and terminates at an iteration i with a stationary point (which may also be a global minimum solution) that satisfies the following minimum principle necessary optimality criterion <ref> [98] </ref>, (1 )e 0 (y y ) 0 (z z i ) 0; 8 feasible (x; y; z): (86) We now turn our attention to numerical testing of Algorithm 4.4.1 and the linear programming formulation (78). 4.5 Application and Numerical Testing We wish to determine whether x-component suppression or x-norm reduction
Reference: [99] <author> O. L. Mangasarian. </author> <title> Characterization of linear complementarity problems as linear programs. </title> <journal> Mathematical Programming Study, </journal> <volume> 7 </volume> <pages> 74-87, </pages> <year> 1978. </year>
Reference-contexts: If the 2-norm or p-norm, p 6= 1; 1, is used, the objective function will be neither concave nor convex. Nevertheless, minimizing a piecewise-linear concave function on a polyhedral set is NP-hard, because the general linear complementarity problem, which is NP-complete [43], can be reduced to such a problem <ref> [99, Lemma 1] </ref>. Given this fact, we focus our attention on effective methods for processing (49). We propose reformulating problem (49) as a bilinear program.
Reference: [100] <author> O. L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: given point sets A and B in R n , we shall mean a plane that attempts to separate R n into two half spaces such that 10 each open halfspace contains points mostly of A or B: Alternatively, such a plane can also be interpreted as a classical perceptron <ref> [140, 77, 100] </ref>. * For a function f : R n ! R that is concave on R n , the supergradient @f (x) of f at x is a vector in R n satisfying f (y) f (x) @f (x)(y x) (2) for any y 2 R n .
Reference: [101] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: for example, by minimizing some norm of the average violations of (5) such as min f (w; fl) := min 1 k (Aw + efl + e) + k 1 + k Another way of satisfying (5) consists of minimizing the number of points misclassified by the separating plane P <ref> [101, 121] </ref>. In [40], the decision problem associated with minimizing the number of misclassifications is shown to be NP-complete and a hybrid algorithm is proposed to compute an approximate solution. <p> fi fi fi fi Bw efl + e z; v w v &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; = ; We make use of the following lemma [103, Lemma 2.3] in order to model the step function of (9) exactly, as a linear program with equilibrium constraints (LPEC) <ref> [101, 94] </ref>. (The "equilibrium" terminology refers to the term involving the ? condition which characterizes complementarity problems [47].) Lemma 2.1.1 Let a 2 R m . <p> Given this fact, we focus our attention on effective methods for processing (49). We propose reformulating problem (49) as a bilinear program. Similar reformulations have been very effective in computationally solving NP-complete linear complementarity problems [102] as well as other difficult machine learning <ref> [101] </ref> and optimization problems with equilibrium constraints [101]. In order to carry out this reformulation we need the following simple lemma. 82 Lemma 3.1.1 Let a 2 R k . <p> We propose reformulating problem (49) as a bilinear program. Similar reformulations have been very effective in computationally solving NP-complete linear complementarity problems [102] as well as other difficult machine learning <ref> [101] </ref> and optimization problems with equilibrium constraints [101]. In order to carry out this reformulation we need the following simple lemma. 82 Lemma 3.1.1 Let a 2 R k . <p> This is clearly a combinatorial problem that we shall solve by minimizing a concave function on polyhedral 113 set. This approach has been successfully used in such machine learning problems as misclassification minimization <ref> [101] </ref>, feature selection [27] and data mining [26, 105]. The idea behind using as few columns of A as possible to span b is motivated by the Occam's Razor bias [13].
Reference: [102] <author> O. L. Mangasarian. </author> <title> The linear complementarity problem as a separable bilinear program. </title> <journal> Journal of Global Optimization, </journal> <volume> 6 </volume> <pages> 153-161, </pages> <year> 1995. </year>
Reference-contexts: Given this fact, we focus our attention on effective methods for processing (49). We propose reformulating problem (49) as a bilinear program. Similar reformulations have been very effective in computationally solving NP-complete linear complementarity problems <ref> [102] </ref> as well as other difficult machine learning [101] and optimization problems with equilibrium constraints [101]. In order to carry out this reformulation we need the following simple lemma. 82 Lemma 3.1.1 Let a 2 R k .
Reference: [103] <author> O. L. Mangasarian. </author> <title> Machine learning via polyhedral concave minimization. </title> <editor> In H. Fischer, B. Riedmueller, and S. Schae*er, editors, </editor> <booktitle> Applied Mathematics and Parallel Computing - Festschrift for Klaus Ritter, </booktitle> <pages> pages 175-188. </pages> <publisher> Physica-Verlag </publisher>
Reference-contexts: Since the function e 0 v fl is discontinuous on the non-negative orthant, we make a continuous approximation of it using either the standard sigmoid function of neural networks [139, 77] or by a concave exponential on the nonnegative real line <ref> [103] </ref>. <p> &gt; &gt; &gt; &lt; (1 )( m e 0 z ) + (n e 0 " ffv ) fi fi fi fi fi fi fi Bw efl + e z; v w v &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; = ; We make use of the following lemma <ref> [103, Lemma 2.3] </ref> in order to model the step function of (9) exactly, as a linear program with equilibrium constraints (LPEC) [101, 94]. (The "equilibrium" terminology refers to the term involving the ? condition which characterizes complementarity problems [47].) Lemma 2.1.1 Let a 2 R m . <p> z 0; 0 r ? u v 0 9 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; ; 2 [0; 1): (15) One consequence of this LPEC formulation of the FS problem (9) is the existence of a solution to the FS problem <ref> [103, Proposition 2.5] </ref>. However, the FSL problem (15) is computationally difficult, in fact LPECs in general are NP-hard since they subsume the general linear complementarity problem which is NP-complete [43]. <p> The successive linearization approach computes a locally optimal solution to the concave problem by identifying vertices of the feasible region via a sequence of linear programs. The successive linearization approach for obtaining minimum support solutions of (18) was introduced in <ref> [103] </ref> for a differentiable f utilizing the negative exponential 28 (11) to approximate the step function. The approach was generalized in [107] for a nondifferentiable f using its supergradient (2). <p> The method which we propose for solving the FSV problem is the successive linearization approximation (SLA) method of minimizing a concave function on a polyhedral set which is a finitely terminating stepless Frank-Wolfe algorithm [62]. In <ref> [103] </ref> finite termination of the SLA was established for a differentiable concave function, and in [107] for a nondifferentiable concave function using its supergradient 31 (2). We now state the algorithm. Algorithm 2.1.3 Successive Linearization Algorithm (SLA) for FSV (13). Choose 2 [0; 1). <p> w v &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; = (26) Stop if (w i ; fl i ; y i ; z i ; v i ) is feasible and (1 )( m e 0 (z i+1 z i ) ) + ff (" ffv i We restate <ref> [103, Theorem 4.2] </ref> which establishes finite termination for Algorithm 2.1.3 at a stationary point, which may be a global solution as well. <p> arg min 8 &gt; : ) 0 z fi fi fi y Ax b y; 9 &gt; ; (84) Stop when (x i ; y i ; z i ) is feasible and (1 )e 0 y i + ff (" ffz i ) 0 z i+1 : (85) By <ref> [103, Theorem 4.2] </ref> we have the following finite termination result for the SLA (Algorithm 4.4.1). 118 Theorem 4.4.2 Finite Termination for SLA applied to (83). <p> The FSS problem (12) is solved by iterative quadratic programming [65, 74, 67]. The FSV problem (13) is solved by solving a sequence of linear programs via the Successive Linearization Algorithm 2.1.3, which is a general tool for solving problems with a concave objective over a polyhedral region <ref> [103] </ref>. The FSV problem (16) is solved by [9, Algorithm 2.1]. Classifiers produced by the support vector machine [161, 33] (SVM) approach were presented.
References-found: 103

