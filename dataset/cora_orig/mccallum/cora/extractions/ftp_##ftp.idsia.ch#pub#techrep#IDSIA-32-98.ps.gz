URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-32-98.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Email: nic@idsia.ch  
Title: Slope Centering: Making Shortcut Weights Effective May 9, 1998  
Author: Nicol N. Schraudolph 
Web: http://www.idsia.ch/  
Address: Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Abstract: Shortcut connections are a popular architectural feature of multi-layer perceptrons. It is generally assumed that by implementing a linear sub-mapping, shortcuts assist the learning process in the remainder of the network. Here we find that this is not always the case: shortcut weights may also act as distractors that slow down convergence and can lead to inferior solutions. This problem can be addressed with slope centering, a particular form of gradient factor centering [2]. By removing the linear component of the error signal at a hidden node, slope centering effectively decouples that node from the shortcuts that bypass it. This eliminates the possibility of destructive interference from shortcut weights, and thus ensures that the benefits of shortcut connections are fully realized. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. N. Schraudolph, </author> <title> "Slope centering: Making shortcut weights effective", </title> <booktitle> in Proceedings of the 8th International Conference on Artificial Neural Networks, </booktitle> <editor> L. Niklasson, M. Boden, and T. Ziemke, Eds., Skovde, </editor> <booktitle> Sweden, 1998, Perspectives in Neural Computing, </booktitle> <pages> pp. 523-528, </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, ftp://ftp.idsia.ch/pub/nic/ slope.ps.gz fl. </address>
Reference-contexts: This should not come as a surprise | after all, the simultaneous adaptation of additional parameters (the shortcut weights) at non-infinitesimal step sizes fl Reprinted from Proc. 8th International Conference on Artificial Neural Networks <ref> [1] </ref>. must necessarily add noise to the error signal. The shortcuts add degrees of freedom to which the bypassed hidden nodes are also coupled, and such redundant parametrization impedes optimization by gradient descent.
Reference: [2] <author> N. N. Schraudolph, </author> <title> "Centering neural network gradient factors", </title> <booktitle> In Orr and Muller [19], </booktitle> <pages> pp. 207-226, </pages> <address> ftp://ftp.idsia.ch/pub/nic/center.ps.gz fl. </address>
Reference-contexts: The network was trained on all 256 patterns, using a cross-entropy loss function and error centering <ref> [2, 10] </ref>, until the root-mean-square error of its output fell below 0.01. Since the complement of a symmetric bit pattern is also symmetric, the symmetry detection task has no linear component at all | we therefore expected shortcuts to be of minimal benefit here. <p> Using an already accelerated gradient method as a baseline, we found in two benchmarks that while shortcuts alone worsened performance, their combination with slope centering further sped up learning significantly. Slope centering is one example of the more general gradient factor centering approach <ref> [2] </ref>. It has long been known that centering input and hidden unit activities is beneficial [17, 18], and recently we have extended this notion to the centering of error signals [10].
Reference: [3] <author> S. Shah, F. Palmieri, and M. Datum, </author> <title> "Optimal filtering algorithms for fast learning in feedforward neural networks", </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 779-787, </pages> <year> 1992. </year>
Reference-contexts: Each experiment consisted of a number of runs starting from different initial weights drawn from a zero-mean Gaussian distribution with standard deviation 0.3. Training was done in batch mode; the hidden-to-output weights of the network were updated before backpropagating error through them <ref> [3] </ref>.
Reference: [4] <author> R. Neuneier and H. G. Zimmermann, </author> <title> "How to train neural networks", </title> <booktitle> In Orr and Muller [19], </booktitle> <pages> pp. 373-423. </pages>
Reference-contexts: In order to make the optimization as efficient as possible, we tested a number of acceleration methods, then chose the combination of two (vario-j and bold driver ) that yielded the fastest reliable convergence overall. 2 Vario-j <ref> [4, 5, page 48] </ref> sets the local learning rate for each weight inversely to the standard deviation of its stochastic gradient: w ij = % + oe (g ij ) @E ; oe (u) j hu 2 i hui ; (5) with the small positive constant % = 0:1 preventing division
Reference: [5] <editor> H. G. Zimmermann, "Neuronale Netze als Entscheidungskalkul", in Neurona-le Netze in der Okonomie: Grundlagen und finanzwirtschaftliche Anwendungen, H. Rehkugler and H. G. Zimmermann, </editor> <booktitle> Eds., </booktitle> <pages> pp. 1-87. </pages> <publisher> Vahlen Verlag, </publisher> <address> Munich, </address> <year> 1994. </year>
Reference-contexts: In order to make the optimization as efficient as possible, we tested a number of acceleration methods, then chose the combination of two (vario-j and bold driver ) that yielded the fastest reliable convergence overall. 2 Vario-j <ref> [4, 5, page 48] </ref> sets the local learning rate for each weight inversely to the standard deviation of its stochastic gradient: w ij = % + oe (g ij ) @E ; oe (u) j hu 2 i hui ; (5) with the small positive constant % = 0:1 preventing division
Reference: [6] <author> A. Lapedes and R. Farber, </author> <title> "A self-optimizing, nonsymmetrical neural net for content addressable memory and pattern recognition", </title> <journal> Physica, </journal> <volume> D 22 </volume> <pages> 247-259, </pages> <year> 1986. </year>
Reference-contexts: The global learning rate j was adjusted by the bold driver technique <ref> [6, 7, 8, 9] </ref>: after each batch in which the error did not increase by more than " = 10 10 (for numerical stability), j is increased by 2%. Otherwise, the last weight change is undone, and j decreased by 50%.
Reference: [7] <author> T. P. Vogl, J. K. Mangis, A. K. Rigler, W. T. Zink, and D. L. Alkon, </author> <title> "Accelerating the convergence of the back-propagation method", </title> <journal> Biological Cybernetics, </journal> <volume> 59 </volume> <pages> 257-263, </pages> <year> 1988. </year>
Reference-contexts: The global learning rate j was adjusted by the bold driver technique <ref> [6, 7, 8, 9] </ref>: after each batch in which the error did not increase by more than " = 10 10 (for numerical stability), j is increased by 2%. Otherwise, the last weight change is undone, and j decreased by 50%.
Reference: [8] <author> R. Battiti, </author> <title> "Accelerated back-propagation learning: Two optimization methods", </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 331-342, </pages> <year> 1989. </year>
Reference-contexts: The global learning rate j was adjusted by the bold driver technique <ref> [6, 7, 8, 9] </ref>: after each batch in which the error did not increase by more than " = 10 10 (for numerical stability), j is increased by 2%. Otherwise, the last weight change is undone, and j decreased by 50%.
Reference: [9] <author> R. Battiti, </author> <title> "First- and second-order methods for learning: Between steepest descent and Newton's method", </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 141-166, </pages> <year> 1992. </year>
Reference-contexts: The global learning rate j was adjusted by the bold driver technique <ref> [6, 7, 8, 9] </ref>: after each batch in which the error did not increase by more than " = 10 10 (for numerical stability), j is increased by 2%. Otherwise, the last weight change is undone, and j decreased by 50%.
Reference: [10] <author> N. N. Schraudolph and T. J. Sejnowski, </author> <title> "Tempering backpropagation networks: Not all weights are created equal", </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, Eds. </editor> <booktitle> 1996, </booktitle> <volume> vol. 8, </volume> <pages> pp. 563-569, </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The network was trained on all 256 patterns, using a cross-entropy loss function and error centering <ref> [2, 10] </ref>, until the root-mean-square error of its output fell below 0.01. Since the complement of a symmetric bit pattern is also symmetric, the symmetry detection task has no linear component at all | we therefore expected shortcuts to be of minimal benefit here. <p> Slope centering is one example of the more general gradient factor centering approach [2]. It has long been known that centering input and hidden unit activities is beneficial [17, 18], and recently we have extended this notion to the centering of error signals <ref> [10] </ref>. In future work, we will investigate the centering of additional gradient factors, such as those that occur in networks with multiplicative nodes. Acknowledgment This work was supported by the Swiss National Science Foundation under grant numbers 2100-045700.95/1 and 2000-052678.97/1.
Reference: [11] <author> D. H. Deterding, </author> <title> Speaker Normalisation for Automatic Speech Recognition, </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
Reference-contexts: is no surprise: since slope centering projects the backpropagated gradient into the null space of (linear) shortcut weights, the hidden nodes can no longer reduce the linear component of the error signal on their own. 3.3 Vowel Recognition Problem We also tested our approach on Deterding's speaker-independent vowel recognition data <ref> [11] </ref>, which has been adopted [12] as a popular [13, 14, 15, 16] neural network benchmark. The task is to recognize the eleven steady-state vowels of British English in a speaker-independent fashion, given 10 spectral features of the speech signal.
Reference: [12] <author> A. J. Robinson, </author> <title> Dynamic Error Propagation Networks, </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
Reference-contexts: centering projects the backpropagated gradient into the null space of (linear) shortcut weights, the hidden nodes can no longer reduce the linear component of the error signal on their own. 3.3 Vowel Recognition Problem We also tested our approach on Deterding's speaker-independent vowel recognition data [11], which has been adopted <ref> [12] </ref> as a popular [13, 14, 15, 16] neural network benchmark. The task is to recognize the eleven steady-state vowels of British English in a speaker-independent fashion, given 10 spectral features of the speech signal.
Reference: [13] <author> M. Finke and K.-R. Muller, </author> <title> "Estimating a-posteriori probabilities using stochastic network models", </title> <booktitle> in Proceedings of the 1993 Connectionist Models Summer School,, </booktitle> <editor> M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A. S. Weigend, Eds., </editor> <address> Boulder, CO, 1994, </address> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: gradient into the null space of (linear) shortcut weights, the hidden nodes can no longer reduce the linear component of the error signal on their own. 3.3 Vowel Recognition Problem We also tested our approach on Deterding's speaker-independent vowel recognition data [11], which has been adopted [12] as a popular <ref> [13, 14, 15, 16] </ref> neural network benchmark. The task is to recognize the eleven steady-state vowels of British English in a speaker-independent fashion, given 10 spectral features of the speech signal.
Reference: [14] <author> M. Herrmann, </author> <title> "On the merits of topography in neural maps", </title> <booktitle> in Proceedings of the Workshop on Self-Organizing Maps, </booktitle> <institution> T. Kohonen, Ed. Helsinki University of Technology, </institution> <year> 1997, </year> <pages> pp. 112-117. </pages>
Reference-contexts: gradient into the null space of (linear) shortcut weights, the hidden nodes can no longer reduce the linear component of the error signal on their own. 3.3 Vowel Recognition Problem We also tested our approach on Deterding's speaker-independent vowel recognition data [11], which has been adopted [12] as a popular <ref> [13, 14, 15, 16] </ref> neural network benchmark. The task is to recognize the eleven steady-state vowels of British English in a speaker-independent fashion, given 10 spectral features of the speech signal.
Reference: [15] <author> S. Hochreiter and J. Schmidhuber, </author> <title> "Unsupervised coding with lococode", </title> <booktitle> in Proceedings of the 7th International Conference on Artificial Neural Networks, </booktitle> <address> Lausanne, Switzerland, </address> <year> 1997, </year> <pages> pp. 655-660, </pages> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: gradient into the null space of (linear) shortcut weights, the hidden nodes can no longer reduce the linear component of the error signal on their own. 3.3 Vowel Recognition Problem We also tested our approach on Deterding's speaker-independent vowel recognition data [11], which has been adopted [12] as a popular <ref> [13, 14, 15, 16] </ref> neural network benchmark. The task is to recognize the eleven steady-state vowels of British English in a speaker-independent fashion, given 10 spectral features of the speech signal.
Reference: [16] <author> G. W. Flake, </author> <title> "Square unit augmented, radially extended, multilayer percep-trons", </title> <booktitle> In Orr and Muller [19], </booktitle> <pages> pp. 145-163. </pages>
Reference-contexts: gradient into the null space of (linear) shortcut weights, the hidden nodes can no longer reduce the linear component of the error signal on their own. 3.3 Vowel Recognition Problem We also tested our approach on Deterding's speaker-independent vowel recognition data [11], which has been adopted [12] as a popular <ref> [13, 14, 15, 16] </ref> neural network benchmark. The task is to recognize the eleven steady-state vowels of British English in a speaker-independent fashion, given 10 spectral features of the speech signal.
Reference: [17] <author> B. Widrow, J. M. McCool, M. G. Larimore, and C. R. Johnson, Jr., </author> <title> "Stationary and nonstationary learning characteristics of the LMS adaptive filter", </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 64(8) </volume> <pages> 1151-1162, </pages> <year> 1976. </year>
Reference-contexts: Slope centering is one example of the more general gradient factor centering approach [2]. It has long been known that centering input and hidden unit activities is beneficial <ref> [17, 18] </ref>, and recently we have extended this notion to the centering of error signals [10]. In future work, we will investigate the centering of additional gradient factors, such as those that occur in networks with multiplicative nodes.
Reference: [18] <author> Y. LeCun, I. Kanter, and S. A. Solla, </author> <title> "Eigenvalues of covariance matrices: Application to neural-network learning", </title> <journal> Physical Review Letters, </journal> <volume> 66(18) </volume> <pages> 2396-2399, </pages> <year> 1991. </year>
Reference-contexts: Slope centering is one example of the more general gradient factor centering approach [2]. It has long been known that centering input and hidden unit activities is beneficial <ref> [17, 18] </ref>, and recently we have extended this notion to the centering of error signals [10]. In future work, we will investigate the centering of additional gradient factors, such as those that occur in networks with multiplicative nodes.
Reference: [19] <author> G. B. Orr and K.-R. Muller, Eds., </author> <title> Neural Networks: Tricks of the Trade, </title> <booktitle> vol. 1524 of Lecture Notes in Computer Science, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1998. </year>
References-found: 19

