URL: http://mikeb.www.media.mit.edu/people/mikeb/ipps95_final.ps
Refering-URL: http://mikeb.www.media.mit.edu/people/mikeb/publications.html
Root-URL: http://www.media.mit.edu
Email: fap,carlag@cs.duke.edu fdfk,nilsg@cs.dartmouth.edu mikeb@media.mit.edu  
Title: Characterizing Parallel File-access Patterns on a Large-scale Multiprocessor  
Author: A. Purakayastha and Carla Ellis David Kotz and Nils Nieuwejaar Michael L. Best 
Address: Durham, NC 27708 Hanover, NH 03755 Cambridge, MA 02139  
Affiliation: Dept. of Computer Science Dept. of Computer Science Media Laboratory Duke University Dartmouth College MIT  
Abstract: High-performance parallel file systems are needed to satisfy tremendous I/O requirements of parallel scientific applications. The design of such high-performance parallel file systems depends on a comprehensive understanding of the expected workload, but so far there have been very few usage studies of multiprocessor file systems. This paper is part of the CHARISMA project, which intends to fill this void by measuring real file-system workloads on various production parallel machines. In particular, here we present results from the CM-5 at the National Center for Supercomputing Applications. Our results are unique because we collect information about nearly every individual I/O request from the mix of jobs running on the machine. Analysis of the traces leads to various recommendations for parallel file-system design. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Personal communication with NCSA consulting staff and NCSA CM-5 systems staff, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: A single file system resides on the SDA. The logical block size of this file system is 29.5 KB and the physical disk block size is 59 KB. There are roughly 1000 user accounts on this machine. The CMF users dominate the CMMD users by roughly 7 to 3 <ref> [1] </ref>. 3.1 Trace Collection The CHARISMA project is a multiplatform tracing project. We defined a generic set of trace records that logged events such as open, close, read, write, truncate/extend, link/unlink, etc. The actual format of the records differed slightly depending on the platform and programming model.
Reference: [2] <author> R. Bagrodia, A. Chien, Y. Hsu, and D. Reed. Input/output: </author> <title> Instrumentation, characterization, modeling and management policy, </title> <note> 1994. On WWW at http://www.ccsf.caltech.edu/SIO/SIO.html. </note>
Reference-contexts: Cypher et al. [8] studied selected parallel scientific applications, mainly to establish temporal patterns in I/O rates. Gal-breath et al. [15] used anecdotal evidence to provide a high level picture of I/O from some parallel applications. Bagro-dia et al. proposed using Pablo to analyze and characterize specific applications <ref> [2] </ref>. The only file system workload study of a production parallel scientific computation environment was that of Kotz and Nieuwejaar [19], as part of the CHARISMA project. They instrumented an iPSC/860 at NASA Ames.
Reference: [3] <author> M. G. Baker, J. H. Hartman, M. D. Kupfer, K. W. Shirriff, and J. K. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <month> Oct </month> <year> 1991. </year>
Reference-contexts: Smith [30] studied file-access behavior of IBM mainframes. Porcar [25] analyzed dynamic trace data for files in an IBM batch environment. Floyd and Ellis [12, 13] and Ousterhout et al. [23] studied file-access patterns from isolated Unix workstations. Baker et al. <ref> [3] </ref> studied access patterns in Sprite, a distributed Unix system. Ramakrishnan et al. [28] studied file access patterns in a commercial computing environment, on a VAX/VMS platform. There have been a few studies of I/O from scientific workloads.
Reference: [4] <author> M. L. Best, A. Greenberg, C. Stanfill, and L. W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: Typically jobs can access files in different I/O modes, which determine how a file pointer is shared among clients running on individual nodes <ref> [7, 4, 18, 14, 22] </ref>. The Hurricane [20] and KSR1 [17] file systems use a memory-mapped interface. The nCUBE [9] and Vesta [5] file systems allow more user control over data layout by providing per-process logical views of the data. <p> CMMD allows multiple threads of control, one for each PN. CMMD I/O provides a variety of I/O modes - in some, action is taken by a single PN; in others, all PNs co-operatively perform parallel I/O <ref> [4] </ref>. 3 Tracing Methodology The 512-node NCSA CM-5 is generally divided into 5 partitions of size 32, 32, 64, 128 and 256 nodes; at times the machine is reconfigured as a single 512-node partition.
Reference: [5] <author> P. F. Corbett, D. G. Feitelson, J.-P. Prost, and S. J. Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference-contexts: Typically jobs can access files in different I/O modes, which determine how a file pointer is shared among clients running on individual nodes [7, 4, 18, 14, 22]. The Hurricane [20] and KSR1 [17] file systems use a memory-mapped interface. The nCUBE [9] and Vesta <ref> [5] </ref> file systems allow more user control over data layout by providing per-process logical views of the data. In PIFS (Bridge) [11], the file system controls which processor handles which part of the file to exploit memory locality. 2.3 The CM-5 The CM-5 is a scalable message-passing multiprocessor.
Reference: [6] <author> T. H. Cormen and D. Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Jensen and Reed traced file archive activity on a Cray at NCSA [16]. Experimental studies of I/O from parallel scientific programs running on multiprocessors have been rather limited. Crockett [7] and Kotz and Ellis [18] described hypothetical characterizations of a parallel scientific file system workload. Cormen and Kotz <ref> [6] </ref> discussed desirable characteristics of parallel I/O algorithms. Reddy et al. [29] studied I/O from parallelized sequential applications, but their applications were handpicked and I/O was not parallel. Cypher et al. [8] studied selected parallel scientific applications, mainly to establish temporal patterns in I/O rates.
Reference: [7] <author> T. W. Crockett. </author> <title> File concepts for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: Miller and Katz [21] and Pasquale and Polyzos [24] studied I/O-intensive Cray applications. Jensen and Reed traced file archive activity on a Cray at NCSA [16]. Experimental studies of I/O from parallel scientific programs running on multiprocessors have been rather limited. Crockett <ref> [7] </ref> and Kotz and Ellis [18] described hypothetical characterizations of a parallel scientific file system workload. Cormen and Kotz [6] discussed desirable characteristics of parallel I/O algorithms. Reddy et al. [29] studied I/O from parallelized sequential applications, but their applications were handpicked and I/O was not parallel. <p> Typically jobs can access files in different I/O modes, which determine how a file pointer is shared among clients running on individual nodes <ref> [7, 4, 18, 14, 22] </ref>. The Hurricane [20] and KSR1 [17] file systems use a memory-mapped interface. The nCUBE [9] and Vesta [5] file systems allow more user control over data layout by providing per-process logical views of the data.
Reference: [8] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <year> 1993. </year>
Reference-contexts: Cormen and Kotz [6] discussed desirable characteristics of parallel I/O algorithms. Reddy et al. [29] studied I/O from parallelized sequential applications, but their applications were handpicked and I/O was not parallel. Cypher et al. <ref> [8] </ref> studied selected parallel scientific applications, mainly to establish temporal patterns in I/O rates. Gal-breath et al. [15] used anecdotal evidence to provide a high level picture of I/O from some parallel applications. Bagro-dia et al. proposed using Pablo to analyze and characterize specific applications [2].
Reference: [9] <author> E. DeBenedictis and J. M. del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications, </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Typically jobs can access files in different I/O modes, which determine how a file pointer is shared among clients running on individual nodes [7, 4, 18, 14, 22]. The Hurricane [20] and KSR1 [17] file systems use a memory-mapped interface. The nCUBE <ref> [9] </ref> and Vesta [5] file systems allow more user control over data layout by providing per-process logical views of the data.
Reference: [10] <author> J. M. del Rosario and A. Choudhary. </author> <title> High performance I/O for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Baker et al. [3] studied access patterns in Sprite, a distributed Unix system. Ramakrishnan et al. [28] studied file access patterns in a commercial computing environment, on a VAX/VMS platform. There have been a few studies of I/O from scientific workloads. Del Rosario and Choudhary <ref> [10] </ref> provided an informal characterization of some grand challenge applications. Powell [26] concentrated mainly on file sizes on a Cray-1. Miller and Katz [21] and Pasquale and Polyzos [24] studied I/O-intensive Cray applications. Jensen and Reed traced file archive activity on a Cray at NCSA [16].
Reference: [11] <author> P. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The Hurricane [20] and KSR1 [17] file systems use a memory-mapped interface. The nCUBE [9] and Vesta [5] file systems allow more user control over data layout by providing per-process logical views of the data. In PIFS (Bridge) <ref> [11] </ref>, the file system controls which processor handles which part of the file to exploit memory locality. 2.3 The CM-5 The CM-5 is a scalable message-passing multiprocessor. It may contain from tens to thousands of processing nodes (PNs) and a few Control Processors (CPs). Each PN has only private memory.
Reference: [12] <author> R. Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical Report 177, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: Smith [30] studied file-access behavior of IBM mainframes. Porcar [25] analyzed dynamic trace data for files in an IBM batch environment. Floyd and Ellis <ref> [12, 13] </ref> and Ousterhout et al. [23] studied file-access patterns from isolated Unix workstations. Baker et al. [3] studied access patterns in Sprite, a distributed Unix system. Ramakrishnan et al. [28] studied file access patterns in a commercial computing environment, on a VAX/VMS platform. <p> Very few files (5.8% of those accessed by CMF jobs and 5.9% of those accessed by CMMD jobs) were used simultaneously for both read and write, which is consistent with observations in Unix file systems made by Floyd <ref> [12] </ref>, and with the iPSC results. It is also not surprising, since co-ordinating parallel read-writes from several nodes is difficult. 25% of CMF jobs did not open any SDA files at all, and 63% of CMF jobs opened 1-4 files on the SDA.
Reference: [13] <author> R. Floyd and C. Ellis. </author> <title> Directory reference patterns in hierarchical file systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(2) </volume> <pages> 238-247, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Smith [30] studied file-access behavior of IBM mainframes. Porcar [25] analyzed dynamic trace data for files in an IBM batch environment. Floyd and Ellis <ref> [12, 13] </ref> and Ousterhout et al. [23] studied file-access patterns from isolated Unix workstations. Baker et al. [3] studied access patterns in Sprite, a distributed Unix system. Ramakrishnan et al. [28] studied file access patterns in a commercial computing environment, on a VAX/VMS platform. <p> For CMMD jobs, more than 50% of files were kept open for more than 1 hour. For CMF jobs about 15% of files were kept open for more than 1 hour. On the whole, file-open durations are much larger than observed in Floyd's Unix file system study <ref> [13] </ref>. 4.3 I/O Request Sizes CMF jobs, the sizes of 90% of write requests were less than 1000 bytes. For CMMD jobs, the sizes of 90% of write requests were less than 400 bytes.
Reference: [14] <author> J. C. French, T. W. Pratt, and M. Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 Hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference-contexts: Typically jobs can access files in different I/O modes, which determine how a file pointer is shared among clients running on individual nodes <ref> [7, 4, 18, 14, 22] </ref>. The Hurricane [20] and KSR1 [17] file systems use a memory-mapped interface. The nCUBE [9] and Vesta [5] file systems allow more user control over data layout by providing per-process logical views of the data.
Reference: [15] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: Reddy et al. [29] studied I/O from parallelized sequential applications, but their applications were handpicked and I/O was not parallel. Cypher et al. [8] studied selected parallel scientific applications, mainly to establish temporal patterns in I/O rates. Gal-breath et al. <ref> [15] </ref> used anecdotal evidence to provide a high level picture of I/O from some parallel applications. Bagro-dia et al. proposed using Pablo to analyze and characterize specific applications [2].
Reference: [16] <author> D. W. Jensen and D. A. Reed. </author> <title> File archive activity in a supercomputing environment. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 387-396, </pages> <year> 1993. </year>
Reference-contexts: Del Rosario and Choudhary [10] provided an informal characterization of some grand challenge applications. Powell [26] concentrated mainly on file sizes on a Cray-1. Miller and Katz [21] and Pasquale and Polyzos [24] studied I/O-intensive Cray applications. Jensen and Reed traced file archive activity on a Cray at NCSA <ref> [16] </ref>. Experimental studies of I/O from parallel scientific programs running on multiprocessors have been rather limited. Crockett [7] and Kotz and Ellis [18] described hypothetical characterizations of a parallel scientific file system workload. Cormen and Kotz [6] discussed desirable characteristics of parallel I/O algorithms.
Reference: [17] <institution> Kendall Square Research. KSR1 technology background, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Typically jobs can access files in different I/O modes, which determine how a file pointer is shared among clients running on individual nodes [7, 4, 18, 14, 22]. The Hurricane [20] and KSR1 <ref> [17] </ref> file systems use a memory-mapped interface. The nCUBE [9] and Vesta [5] file systems allow more user control over data layout by providing per-process logical views of the data.
Reference: [18] <author> D. Kotz. </author> <title> Multiprocessor file system interfaces. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 194-201, </pages> <year> 1993. </year>
Reference-contexts: Miller and Katz [21] and Pasquale and Polyzos [24] studied I/O-intensive Cray applications. Jensen and Reed traced file archive activity on a Cray at NCSA [16]. Experimental studies of I/O from parallel scientific programs running on multiprocessors have been rather limited. Crockett [7] and Kotz and Ellis <ref> [18] </ref> described hypothetical characterizations of a parallel scientific file system workload. Cormen and Kotz [6] discussed desirable characteristics of parallel I/O algorithms. Reddy et al. [29] studied I/O from parallelized sequential applications, but their applications were handpicked and I/O was not parallel. <p> Typically jobs can access files in different I/O modes, which determine how a file pointer is shared among clients running on individual nodes <ref> [7, 4, 18, 14, 22] </ref>. The Hurricane [20] and KSR1 [17] file systems use a memory-mapped interface. The nCUBE [9] and Vesta [5] file systems allow more user control over data layout by providing per-process logical views of the data.
Reference: [19] <author> D. Kotz and N. Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: The first results of the CHARISMA project involve a tracing study done on an Intel iPSC/860 at NASA's Ames research facility <ref> [19] </ref>. This paper describes results from a different platform, the CM-5 at the National Center for Supercomputing Applications (NCSA). <p> Bagro-dia et al. proposed using Pablo to analyze and characterize specific applications [2]. The only file system workload study of a production parallel scientific computation environment was that of Kotz and Nieuwejaar <ref> [19] </ref>, as part of the CHARISMA project. They instrumented an iPSC/860 at NASA Ames.
Reference: [20] <author> O. Krieger and M. Stumm. </author> <title> HFS: a flexible file system for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 6-14, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Typically jobs can access files in different I/O modes, which determine how a file pointer is shared among clients running on individual nodes [7, 4, 18, 14, 22]. The Hurricane <ref> [20] </ref> and KSR1 [17] file systems use a memory-mapped interface. The nCUBE [9] and Vesta [5] file systems allow more user control over data layout by providing per-process logical views of the data.
Reference: [21] <author> E. L. Miller and R. H. Katz. </author> <title> Input/Output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: There have been a few studies of I/O from scientific workloads. Del Rosario and Choudhary [10] provided an informal characterization of some grand challenge applications. Powell [26] concentrated mainly on file sizes on a Cray-1. Miller and Katz <ref> [21] </ref> and Pasquale and Polyzos [24] studied I/O-intensive Cray applications. Jensen and Reed traced file archive activity on a Cray at NCSA [16]. Experimental studies of I/O from parallel scientific programs running on multiprocessors have been rather limited.
Reference: [22] <author> B. Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Typically jobs can access files in different I/O modes, which determine how a file pointer is shared among clients running on individual nodes <ref> [7, 4, 18, 14, 22] </ref>. The Hurricane [20] and KSR1 [17] file systems use a memory-mapped interface. The nCUBE [9] and Vesta [5] file systems allow more user control over data layout by providing per-process logical views of the data.
Reference: [23] <author> J. Ousterhout, H. DaCosta, D. Harrison, J. Kunze, M. Kupfer, and J. Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of 10th Symposium on Operating System Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Smith [30] studied file-access behavior of IBM mainframes. Porcar [25] analyzed dynamic trace data for files in an IBM batch environment. Floyd and Ellis [12, 13] and Ousterhout et al. <ref> [23] </ref> studied file-access patterns from isolated Unix workstations. Baker et al. [3] studied access patterns in Sprite, a distributed Unix system. Ramakrishnan et al. [28] studied file access patterns in a commercial computing environment, on a VAX/VMS platform. There have been a few studies of I/O from scientific workloads.
Reference: [24] <author> B. K. Pasquale and G. C. Polyzos. </author> <title> A static analysis of I/O characteristics of scientific applications in a production workload. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 388-397, </pages> <year> 1993. </year>
Reference-contexts: There have been a few studies of I/O from scientific workloads. Del Rosario and Choudhary [10] provided an informal characterization of some grand challenge applications. Powell [26] concentrated mainly on file sizes on a Cray-1. Miller and Katz [21] and Pasquale and Polyzos <ref> [24] </ref> studied I/O-intensive Cray applications. Jensen and Reed traced file archive activity on a Cray at NCSA [16]. Experimental studies of I/O from parallel scientific programs running on multiprocessors have been rather limited. Crockett [7] and Kotz and Ellis [18] described hypothetical characterizations of a parallel scientific file system workload.
Reference: [25] <author> J. Porcar. </author> <title> File migration in distributed computer systems. </title> <type> Technical Report LBL-14763, </type> <institution> Lawrence Berkeley Lab, </institution> <month> July </month> <year> 1982. </year>
Reference-contexts: Smith [30] studied file-access behavior of IBM mainframes. Porcar <ref> [25] </ref> analyzed dynamic trace data for files in an IBM batch environment. Floyd and Ellis [12, 13] and Ousterhout et al. [23] studied file-access patterns from isolated Unix workstations. Baker et al. [3] studied access patterns in Sprite, a distributed Unix system.
Reference: [26] <author> M. L. Powell. </author> <title> The DEMOS file system. </title> <booktitle> In Proceedings of the Sixth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 33-42, </pages> <month> November </month> <year> 1977. </year>
Reference-contexts: Ramakrishnan et al. [28] studied file access patterns in a commercial computing environment, on a VAX/VMS platform. There have been a few studies of I/O from scientific workloads. Del Rosario and Choudhary [10] provided an informal characterization of some grand challenge applications. Powell <ref> [26] </ref> concentrated mainly on file sizes on a Cray-1. Miller and Katz [21] and Pasquale and Polyzos [24] studied I/O-intensive Cray applications. Jensen and Reed traced file archive activity on a Cray at NCSA [16]. Experimental studies of I/O from parallel scientific programs running on multiprocessors have been rather limited.
Reference: [27] <author> A. Purakayastha, C. S. Ellis, D. Kotz, N. Nieuwejaar, and M. </author> <title> Best. Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <type> Technical Report CS-1994-33, </type> <institution> Dept. of Computer Science, Duke University, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: We defined a generic set of trace records that logged events such as open, close, read, write, truncate/extend, link/unlink, etc. The actual format of the records differed slightly depending on the platform and programming model. Detailed formats of event records are listed in <ref> [27] </ref>. We specifically considered user-program I/O only to and from the SDA. Serial NFS I/O was not considered because we expected that it would have much less data traffic due to limited bandwidth. We instrumented the run-time CMF I/O libraries to collect traces.
Reference: [28] <author> K. K. Ramakrishnan, P. Biswas, and R. Karedla. </author> <title> Analysis of file I/O traces in commercial computing environments. </title> <booktitle> In Proceedings of ACM SIGMETRICS and PERFORMANCE '92, </booktitle> <pages> pages 78-90, </pages> <year> 1992. </year>
Reference-contexts: Porcar [25] analyzed dynamic trace data for files in an IBM batch environment. Floyd and Ellis [12, 13] and Ousterhout et al. [23] studied file-access patterns from isolated Unix workstations. Baker et al. [3] studied access patterns in Sprite, a distributed Unix system. Ramakrishnan et al. <ref> [28] </ref> studied file access patterns in a commercial computing environment, on a VAX/VMS platform. There have been a few studies of I/O from scientific workloads. Del Rosario and Choudhary [10] provided an informal characterization of some grand challenge applications. Powell [26] concentrated mainly on file sizes on a Cray-1.
Reference: [29] <author> A. Reddy and P. Banerjee. </author> <title> A study of I/O behavior of Perfect Benchmarks on a multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 312-321, </pages> <year> 1990 1990. </year>
Reference-contexts: Experimental studies of I/O from parallel scientific programs running on multiprocessors have been rather limited. Crockett [7] and Kotz and Ellis [18] described hypothetical characterizations of a parallel scientific file system workload. Cormen and Kotz [6] discussed desirable characteristics of parallel I/O algorithms. Reddy et al. <ref> [29] </ref> studied I/O from parallelized sequential applications, but their applications were handpicked and I/O was not parallel. Cypher et al. [8] studied selected parallel scientific applications, mainly to establish temporal patterns in I/O rates.
Reference: [30] <author> A. Smith. </author> <title> Analysis of long term file reference patterns and their applications to file migration algorithms. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> SE-7(4):403-417, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Smith <ref> [30] </ref> studied file-access behavior of IBM mainframes. Porcar [25] analyzed dynamic trace data for files in an IBM batch environment. Floyd and Ellis [12, 13] and Ousterhout et al. [23] studied file-access patterns from isolated Unix workstations. Baker et al. [3] studied access patterns in Sprite, a distributed Unix system.
References-found: 30

