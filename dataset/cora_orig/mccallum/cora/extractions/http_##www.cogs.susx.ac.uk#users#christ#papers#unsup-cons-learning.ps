URL: http://www.cogs.susx.ac.uk/users/christ/papers/unsup-cons-learning.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)1273 678856  
Title: Unsupervised Constructive Learning  
Author: Chris Thornton 
Date: February 18, 1997  
Web: WWW: http://www.cogs.susx.ac.uk  
Address: Brighton BN1 9QH  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: In constructive induction (CI), the learner's problem representation is modified as a normal part of the learning process. This is useful when the initial representation is inadequate or inappropriate. In this paper, I argue that the distinction between constructive and non-constructive methods is unclear. I propose a theoretical model which allows (a) a clean distinction to be made and (b) the process of CI to be properly motivated. I also show that although constructive induction has been used almost exclusively in the context of supervised learning, there is no reason why it cannot form a part of an unsupervised regime.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michalski, R. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <editor> In R. Michalski, J. Carbonell and T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference-contexts: 1 Introduction Constructive induction (CI) is of use when the initial representation for a problem obstructs the application of ordinary inductive methods <ref> [1] </ref>. Wnek and Michalski [2] have divided constructive induction methods into several types including hypothesis-driven (HCI) methods, data-driven (DCI) methods and knowledge-driven (KCI) methods. Practical methods introduced in recent years include FRINGE [3], AQ17-HCI [2] and CN2-MCI [4].
Reference: [2] <author> Wnek, J. and Michalski, R. </author> <year> (1994). </year> <title> Hypothesis-driven constructive induction in AQ17-HCI: a method and experiments. </title> <booktitle> Machine Learning, </booktitle> <address> 14 (p. 139). Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: 1 Introduction Constructive induction (CI) is of use when the initial representation for a problem obstructs the application of ordinary inductive methods [1]. Wnek and Michalski <ref> [2] </ref> have divided constructive induction methods into several types including hypothesis-driven (HCI) methods, data-driven (DCI) methods and knowledge-driven (KCI) methods. Practical methods introduced in recent years include FRINGE [3], AQ17-HCI [2] and CN2-MCI [4]. Almost all CI methods seek to transform the initial representation space by introducing new features. <p> Wnek and Michalski <ref> [2] </ref> have divided constructive induction methods into several types including hypothesis-driven (HCI) methods, data-driven (DCI) methods and knowledge-driven (KCI) methods. Practical methods introduced in recent years include FRINGE [3], AQ17-HCI [2] and CN2-MCI [4]. Almost all CI methods seek to transform the initial representation space by introducing new features. However, in the literature, the term `feature' has been used ambiguously.
Reference: [3] <author> Pagallo, G. </author> <year> (1989). </year> <title> Learning DNF by decision trees. </title> <booktitle> Proceedings of The Eleventh Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 639-644). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Wnek and Michalski [2] have divided constructive induction methods into several types including hypothesis-driven (HCI) methods, data-driven (DCI) methods and knowledge-driven (KCI) methods. Practical methods introduced in recent years include FRINGE <ref> [3] </ref>, AQ17-HCI [2] and CN2-MCI [4]. Almost all CI methods seek to transform the initial representation space by introducing new features. However, in the literature, the term `feature' has been used ambiguously.
Reference: [4] <author> Kramer, S. </author> <year> (1994). </year> <title> CN2-MCI: a two-step method for constructive induction. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference-contexts: Wnek and Michalski [2] have divided constructive induction methods into several types including hypothesis-driven (HCI) methods, data-driven (DCI) methods and knowledge-driven (KCI) methods. Practical methods introduced in recent years include FRINGE [3], AQ17-HCI [2] and CN2-MCI <ref> [4] </ref>. Almost all CI methods seek to transform the initial representation space by introducing new features. However, in the literature, the term `feature' has been used ambiguously. <p> Oliveira and Sangiovanni-Vincentelli [12], for example, describe a system that effectively searches for a minimal set of features each of which tests for a conjunctive property of the input variables. Kramer <ref> [4] </ref> describes a system that tries to detect and exploit relations over variables which tend to appear together in useful rules. Matheus [28] describes the CITRE system which, to a first approximation, tries to capitalize on the presence of disjunctive regions in decision tree descriptions.
Reference: [5] <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, this usage cannot be taken too literally since all supervised learning algorithms attempt to implement the target partitioning on the space and would thus all be potentially classified as constructive. Explicitly discounting such degenerate cases still leaves us with methods such as C4.5 <ref> [5] </ref>, Backpropagation [6] and CN2 [7, 8] which all make use of intermediate constructs (i.e., constructs not directly involved in production of output) that identify partitions on the representation space. 1 These methods would seem to satisfy the criterion of being `feature generators' in a non-degenerate sense and yet they are
Reference: [6] <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <booktitle> Nature, </booktitle> <pages> 323 (pp. 533-6). </pages>
Reference-contexts: However, this usage cannot be taken too literally since all supervised learning algorithms attempt to implement the target partitioning on the space and would thus all be potentially classified as constructive. Explicitly discounting such degenerate cases still leaves us with methods such as C4.5 [5], Backpropagation <ref> [6] </ref> and CN2 [7, 8] which all make use of intermediate constructs (i.e., constructs not directly involved in production of output) that identify partitions on the representation space. 1 These methods would seem to satisfy the criterion of being `feature generators' in a non-degenerate sense and yet they are typically described
Reference: [7] <author> Clark, P. and Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning, </booktitle> <pages> 3 (pp. 261-283). </pages>
Reference-contexts: However, this usage cannot be taken too literally since all supervised learning algorithms attempt to implement the target partitioning on the space and would thus all be potentially classified as constructive. Explicitly discounting such degenerate cases still leaves us with methods such as C4.5 [5], Backpropagation [6] and CN2 <ref> [7, 8] </ref> which all make use of intermediate constructs (i.e., constructs not directly involved in production of output) that identify partitions on the representation space. 1 These methods would seem to satisfy the criterion of being `feature generators' in a non-degenerate sense and yet they are typically described as `selective' and
Reference: [8] <author> Clark, P. and Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: some recent improvements. </title> <editor> In Y. Kodratoff (Ed.), </editor> <booktitle> Proceedings of the Fifth European Working Session on Learning. No. 482 of Lecture Notes in Artificial Intelligence (pp. </booktitle> <pages> 151-163). </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: However, this usage cannot be taken too literally since all supervised learning algorithms attempt to implement the target partitioning on the space and would thus all be potentially classified as constructive. Explicitly discounting such degenerate cases still leaves us with methods such as C4.5 [5], Backpropagation [6] and CN2 <ref> [7, 8] </ref> which all make use of intermediate constructs (i.e., constructs not directly involved in production of output) that identify partitions on the representation space. 1 These methods would seem to satisfy the criterion of being `feature generators' in a non-degenerate sense and yet they are typically described as `selective' and
Reference: [9] <author> Sazonov, V. and Wnek, J. </author> <year> (1994). </year> <title> A hypothesis-driven constructive induction approach to expanding neural networks. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference: [10] <author> Pfahringer, B. </author> <year> (1994). </year> <title> Cipf 2.0: a robust constructive induction system. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference: [11] <author> Japkowicz, N. and Hirsh, H. </author> <year> (1994). </year> <title> Towards a bootstrapping approach to constructive induction. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference: [12] <author> Oliveira, A. and Sangiovanni-Vincentelli, A. </author> <year> (1992). </year> <title> Constructive induction using a non-greedy strategy for feature selection. </title> <editor> In D. Sleeman and P. Edwards (Eds.), </editor> <booktitle> Proceedings of the Ninth International Workshop on Machine Machine Learning (ML92) (pp. </booktitle> <pages> 355-360). </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann Publishers. </publisher> <pages> 7 </pages>
Reference-contexts: The feature implemented by the function will therefore tend to instantiate a complex, non-local partition of the space. The proposed model also allows us to understand why practical CI methods typically involve some sort of search for relational functions. Oliveira and Sangiovanni-Vincentelli <ref> [12] </ref>, for example, describe a system that effectively searches for a minimal set of features each of which tests for a conjunctive property of the input variables. Kramer [4] describes a system that tries to detect and exploit relations over variables which tend to appear together in useful rules.
Reference: [13] <author> Gold, E. </author> <year> (1967). </year> <title> Language identification in the limit. </title> <booktitle> Information and Con--trol, </booktitle> <pages> 10 (pp. 447-74). </pages>
Reference-contexts: Early work by Gold <ref> [13] </ref> and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., [17, 18, 19, 20, 21, 21, 22].
Reference: [14] <author> Valiant, L. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <pages> 27 (pp. 1134-42). </pages>
Reference: [15] <author> Valiant, L. </author> <year> (1985). </year> <title> Learning disjunctions of conjunctions. </title> <booktitle> Proceedings of the Ninth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 560-566). </pages> <address> Los Altos: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [16] <author> Vapnik, V. and Chervonenkis, A. </author> <year> (1971). </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theor. Probab. Appl., </journal> <volume> 16, No. </volume> <pages> 2 (pp. 264-280). </pages>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension <ref> [16] </ref> and led to the theoretical advances of Haussler and others, e.g., [17, 18, 19, 20, 21, 21, 22].
Reference: [17] <author> Haussler, D. </author> <year> (1986). </year> <title> Quantifying the inductive bias in concept learning. </title> <institution> UCSC-CRL-86-25, University of California at Santa Cruz. </institution>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [18] <author> Blumer, A., Ehrenfeucht, A., Haussler, D. and Warmuth, M. </author> <year> (1987). </year> <title> Oc-cam's razor. </title> <journal> Information Processing Letters, </journal> <pages> 24 (pp. 377-380). </pages>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [19] <author> Haussler, D. </author> <year> (1988). </year> <title> Quantifying inductive bias: AI learning and valiant's learning framework. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 36 (pp. 177-221). </pages>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [20] <author> Haussler, D. </author> <year> (1987). </year> <title> Bias, version spaces and valiant's learning framework. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 324-336). </pages> <institution> University of California, Irvine: </institution> <month> (June 22-25). </month>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [21] <author> Baum, E. and Haussler, D. </author> <year> (1990). </year> <title> What size net gives valid generalization?. </title> <editor> In J.W. Shavlik and T.G. Dietterich (Eds.), </editor> <booktitle> Readings In Machine Learning (pp. </booktitle> <pages> 258-262). </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [22] <author> Haussler, D., Kearns, M. and Schapire, R. </author> <year> (1992). </year> <title> Bounds on the sample complexity of bayesian learning using information theory and the VC dimension. </title> <institution> UCSC-CRL-91-44, University of California at Santa Cruz. </institution>
Reference-contexts: Early work by Gold [13] and Valiant [14,15] established a tradition which grew to encompass theoretical constructs such as VC-dimension [16] and led to the theoretical advances of Haussler and others, e.g., <ref> [17, 18, 19, 20, 21, 21, 22] </ref>. Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic [23].
Reference: [23] <author> Kearns, M. </author> <year> (1990). </year> <title> The Computational Complexity of Machine Learning. </title> <publisher> The MIT Press. </publisher>
Reference-contexts: Much of this work is directed towards the goal of analyzing the complexity of learning but, at the time of writing, measuring the hardness of arbitrary learning problems (e.g., specific training sets) remains problematic <ref> [23] </ref>. However, it turns out that a useful, qualitative measure of problem hardness can be obtained via a Bayesian analysis of justification sources for generalisation. Consider the following example. D is the body of data shown in Table 1.
Reference: [24] <author> Shavlik, J. and Dietterich, T. (Eds.) </author> <year> (1990). </year> <booktitle> Readings in Machine Learning. </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There are a large number of these, see <ref> [24, 25, 26] </ref>. 3 Statistical v. relational learning The analysis of justification sources allows us to divide methods of inductive learning into two basic types. A method that attempts to exploit either of the first two forms of probability confronts a tractable task.
Reference: [25] <editor> Michalski, R., Carbonell, J. and Mitchell, T. (Eds.) </editor> <booktitle> (1983). Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference-contexts: There are a large number of these, see <ref> [24, 25, 26] </ref>. 3 Statistical v. relational learning The analysis of justification sources allows us to divide methods of inductive learning into two basic types. A method that attempts to exploit either of the first two forms of probability confronts a tractable task.
Reference: [26] <editor> Michalski, R., Carbonell, J. and Mitchell, T. (Eds.) </editor> <booktitle> (1986). Machine Learning: An Artificial Intelligence Approach: Vol II. </booktitle> <address> Los Altos: </address> <publisher> Morgan Kauf-mann. </publisher> <pages> 8 </pages>
Reference-contexts: There are a large number of these, see <ref> [24, 25, 26] </ref>. 3 Statistical v. relational learning The analysis of justification sources allows us to divide methods of inductive learning into two basic types. A method that attempts to exploit either of the first two forms of probability confronts a tractable task.
Reference: [27] <author> Stone, J. and Thornton, C. </author> <year> (1995). </year> <title> Can artificial neural networks discover useful regularities?. </title> <booktitle> Proceedings of ICANN-95. </booktitle> <address> Cambridge. </address>
Reference-contexts: Assuming a finite range is tantamount to assuming an arbitrary boundary on the space of possible relationships. 4 i.e., they tend to exploit probabilities of the first and second form, rather than of the third form. <ref> [27] </ref> Interestingly, we can deduce that the evaluation function used in the third form must measure a relational property of its inputs. To understand why, we need to think about the way in which the function differentiates different types of input.
Reference: [28] <author> Matheus, C. </author> <year> (1990). </year> <title> Adding domain knowledge to SBL through feature construction. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 803-808). </pages> <address> Boston, MA.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Kramer [4] describes a system that tries to detect and exploit relations over variables which tend to appear together in useful rules. Matheus <ref> [28] </ref> describes the CITRE system which, to a first approximation, tries to capitalize on the presence of disjunctive regions in decision tree descriptions. Other systems involve a search (i.e., operator-based) process working with some sort of relational description language, e.g., [29, 11,10].
Reference: [29] <author> Aronis, J. and Provost, F. </author> <year> (1994). </year> <title> Efficiently constructing relational features&lt;br&gt;from background knowledge for inductive machine learning. </title> <booktitle> Proceedings of ML-COLT'94. </booktitle>
Reference-contexts: Matheus [28] describes the CITRE system which, to a first approximation, tries to capitalize on the presence of disjunctive regions in decision tree descriptions. Other systems involve a search (i.e., operator-based) process working with some sort of relational description language, e.g., <ref> [29, 11,10] </ref>. In several recent cases, this type of approach has focussed on what are known as `counting' or M-of-N features, i.e., features which effectively count the number of occurrences of a particular variable value, cf. [30,31,2,9 32,33].
Reference: [30] <author> Spackman, K. </author> <year> (1988). </year> <title> Learning categorical decision criteria in biomedical domains. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 36-46). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann,. </publisher>
Reference: [31] <author> Fawcett, T. and Utgoff, P. </author> <year> (1991). </year> <title> A hybrid method for feature generation. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 137-141). </pages> <address> Evanston, Ill. </address>
Reference: [32] <author> Seshu, R. </author> <year> (1989). </year> <title> Solving the Parity Problem. </title> <institution> University of Illinois at Urbana-Champaign, Inductive Learning Group. </institution>
Reference: [33] <author> Murphy, P. and Pazzani, M. </author> <year> (1991). </year> <title> ID2-of-3: constructive induction of m-of-n concepts for discriminators in decision trees. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning (ML91). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 9 </pages>
References-found: 33

