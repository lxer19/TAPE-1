URL: http://www.cs.ucsd.edu/users/walfredo/parallelIO.ps
Refering-URL: http://www.cs.ucsd.edu/users/walfredo/resume.html
Root-URL: http://www.cs.ucsd.edu
Title: On Interfaces to Parallel I/O Subsystems  
Author: Walfredo Cirne 
Web: http://www-cse.ucsd.edu/users/walfredo  
Address: La Jolla, CA 92093-0114 USA  
Affiliation: University of California San Diego Department of Computer Science and Engineering  
Abstract: Computer performance has improved exponentially for the last 20 years. However, different components have improved at substantially different rates. Processing power has grown much faster than memory speed in general, but specially than secondary memory (i. e., disks). This discrepancy produces unbalanced systems, which dont fully deliver the performance improvement of their enhanced processors. Parallel I/O has been seen as a response to this problem, especially on parallel machines, which multiply the gap between processors and disks. However, I/O requests have to be expressed in a nontraditional way in order to expose more parallelism and thus enable parallel I/O operations. This paper surveys some of the work currently being carried out the quest of interfaces to better explore parallel I/O. 
Abstract-found: 1
Intro-found: 1
Reference: [Anderson 95] <author> Tom Anderson, Michael Dahlin, Jeanna Neefe, David Patterson, Drew Roselli, and Randy Wang. </author> <title> Serverless Network File Systems. </title> <booktitle> 15th Symposium on Operating Systems Principles, ACM Transactions on Computer Systems, </booktitle> <year> 1995. </year> <note> http://now.cs.berkeley.edu/Papers/Papers/sosp95.ps </note>
Reference-contexts: This approach has also been successfully explored in distributed file systems, such as xFS, which aggregates disks on different machines to create distributed and parallel virtual disks <ref> [Anderson 95] </ref>. Parallel I/O (as a big and fast virtual disk) has been quite successful as a response for the performance gap between processors and disks.
Reference: [Arpaci 97] <author> Andrea C. Arpaci- Dusseau, Remzi H. Arpaci- Dusseau, David E. Culler, Joseph M. Hellerstein, David A. Patterson. </author> <title> High-Performance Sorting on Networks of Workstations. </title> <booktitle> SIGMOD '97, </booktitle> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year> <note> http://now.cs.berkeley.edu/NowSort/abstract.html </note>
Reference-contexts: Parallel I/O is likely to remain a major technique to deal with the I/O performance gap. As a matter of fact, the field is in its infancy. The fact that the fastest sort (on disks) available <ref> [Arpaci 97] </ref> uses parallel I/O but makes direct and completely ad-hoc access to the distributed disks shows both the potential and the necessity of work in the area.
Reference: [Bershad 94] <author> B. Bershad et al. </author> <title> Operating System Support for High-Performance Parallel I/O Systems. Scalable I/O Initiative. </title> <address> http://www.ccsf.caltech.edu/SIO/SIO_osfs.ps </address>
Reference-contexts: Other efforts deal with efficient use of archives (huge data repositories stored in tertiary memory devices). Comprehensive coverage of the I/O gap problem can be found in recent reports that discuss research opportunities in the area [Gibson 96] <ref> [Bershad 94] </ref>. Of course, one of the major I/O problems found in modern machines is accessing data in secondary storage.
Reference: [Blumofe 95] <author> R. D. Blumofe et al. Cilk: </author> <title> An Efficient Multithreaded Runtime System. </title> <booktitle> 5th ACM SIGPLAN PPoPP, </booktitle> <pages> pp. 207-216, </pages> <address> Santa Barbara, CA, </address> <month> July 19-21, </month> <year> 1995. </year> <month> ftp://theory.lcs.mit.edu/pub/cilk/PPoPP95.ps.Z </month>
Reference-contexts: Another concern about the interface design is that all works we know about assume some message-based programming model. Apparently, there is no research on how to express parallel I/O request on thread-based models, such as Cilk <ref> [Blumofe 95] </ref>, COOL [Chandra 93], or OpenMP [OpenMP 97]. Since some parallel applications are much easier to program by creating threads on the fly, how such applications should express parallel I/O operation is an important (and untouched) question.
Reference: [Bordawekar 95a] <author> Rajesh Bordawekar, and Alok Choudhary. </author> <title> Communication Strategies for Out-of-core Programs on Distributed Memory Machines. </title> <booktitle> In Proc. of 9th ACM Intl. Conference on Supercomputing, </booktitle> <month> July </month> <year> 1995. </year> <note> http://www.npac.syr.edu/techreports/html/0650/abs-0667.html </note>
Reference-contexts: Passion-compiled programs are able to access global out-of-core arrays very much in the same way regular HPF programs deal with global (in-core) arrays. The global out-of-core arrays are partitioned in many local out-of-core arrays (which are stored in the different processor). Figure 2.2 portrays the Passion architecture [Thakur 94] <ref> [Bordawekar 95a] </ref>. In addition to communication, the Passion compiler also takes care of swapping the pieces of the global array in and out of memory. The generated code calls the Passion runtime library, which performs some optimizations, such as prefetching and data sieve [Thakur 94]. <p> On the other hand, when the communication is small compared to the total amount of data, interleaving computation, communication, and I/O delivers the best execution time. <ref> [Bordawekar 95a] </ref> Experimental results have shown that the Passion compiler can produce code that runs about twice as fast as the regular HPF code (using virtual memory to support out-of-core computation) on applications such as LU factorization and red-black relaxation [Bordawekar 95b]. 2.4.
Reference: [Bordawekar 95b] <author> Rajesh Bordawekar, Alok Choudhary, Ken Kennedy, Charles Koelbel and Michael Paleczny. </author> <title> A Model and Compilation Strategy for Out-of-Core Data Parallel Programs . In Proc. </title> <booktitle> of the Fifth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year> <note> http://www.npac.syr.edu/techreports/html/0650/abs-0696.html </note>
Reference-contexts: interleaving computation, communication, and I/O delivers the best execution time. [Bordawekar 95a] Experimental results have shown that the Passion compiler can produce code that runs about twice as fast as the regular HPF code (using virtual memory to support out-of-core computation) on applications such as LU factorization and red-black relaxation <ref> [Bordawekar 95b] </ref>. 2.4. Shore (http://www.cs.wisc.edu/shore/) Shore (Scalable Heterogeneous Object Repository) is a persistent object system developed at the University of Wisconsin. It combines many services usually provided separately by objectoriented databases (OODB) and file systems.
Reference: [Carey 94] <author> M. Carey, D. DeWitt, J. Naughton, M. Solomon et. al. </author> <title> Shoring Up Persistent Applications . Proc. </title> <booktitle> of the 1994 ACM SIGMOD Conference, </booktitle> <address> Minneapolis, MN, </address> <month> May </month> <year> 1994. </year> <month> file://ftp.cs.wisc.edu/tech-reports/reports/94/tr1222.ps.Z </month>
Reference-contexts: The OODB features include concurrency control, crash recovery, type checking, and pool of anonymous objects (which provides a lightweight storage mechanism for small objects). Besides putting together features usually isolated, Shore also introduces some novel ideas, such as the extensible value-added server facility. <ref> [Carey 94] </ref> Users (i. e., programmers) interface with the system through SDL (Shore Data Language). SDL is used to specify the persistent objects that are stored by the system. SDL is language-independent, but currently there is binding only to C++.
Reference: [Chandra 93] <author> R. Chandra, A. Gupta, and J. L. </author> <booktitle> Hennessy . Data Locality and Load Balancing in COOL . Fourth ACM SIGPLAN PPoPP, </booktitle> <pages> pp. 249-259, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <note> http://www-flash.stanford.edu/cool/ppopp.ps </note>
Reference-contexts: Another concern about the interface design is that all works we know about assume some message-based programming model. Apparently, there is no research on how to express parallel I/O request on thread-based models, such as Cilk [Blumofe 95], COOL <ref> [Chandra 93] </ref>, or OpenMP [OpenMP 97]. Since some parallel applications are much easier to program by creating threads on the fly, how such applications should express parallel I/O operation is an important (and untouched) question. Yet other key and yet unanswered question is how to combine performance and portability.
Reference: [Chen 94] <author> P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, and D. A. Patterson. </author> <title> RAID: High-Performance, Reliable Secondary Storage. </title> <journal> ACM Computing Surveys 26, </journal> <volume> 2, </volume> <pages> 145-185, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: A RAID is a virtual disk made of many physical disks that processes requests in parallel in a completely transparent fashion (i. e., the application sees only a single large and fast disk) <ref> [Chen 94] </ref>. This approach has also been successfully explored in distributed file systems, such as xFS, which aggregates disks on different machines to create distributed and parallel virtual disks [Anderson 95].
Reference: [Chen 97] <author> Y. Chen, I. Foster, J. Nieplocha, and M. Winslett. </author> <title> Optimizing collective I/O performance on parallel computers: A multisystem study. </title> <booktitle> In Proceedings of 11th ACM International Conference on Supercomputing, </booktitle> <address> Vienna, </address> <month> July </month> <year> 1997. </year> <note> ACM Press. http://drl.cs.uiuc.edu/pubs/ics97.html </note>
Reference-contexts: of MPI, the problem seems to be much worse in MPI-IO because even similar machines can have their I/O subsystem configured in completely different ways and high performance often requires that the user choose optimal values for system parameters such as disk array distribution and the number of I/O nodes <ref> [Chen 97] </ref>. Although an untuned ported MPI-IO application is expected to run correctly, it can be very far from its optimal performance in the new platform . Whether different MPI-IO implementations will be able to really provide portability and performance is a question yet to be answered. <p> Yet other key and yet unanswered question is how to combine performance and portability. MPI-IO is likely to represent an important step towards portability. However, recent studies have shown the importance of carefully tune a great number of parameters to achieve near-peak performance on a distributed I/O subsystem <ref> [Chen 97] </ref>. Besides representing a big burden to the user, the manual adjustment of such parameters precludes portability. We expect to see considerable research effort on how to automatically tune such system parameters.
Reference: [Colvin 97] <author> Alex Colvin and Thomas H. Cormen. </author> <title> ViC*: A Compiler for Virtual-Memory C*. </title> <institution> Dartmouth College Computer Science Technical Report PCS-TR97-303. </institution> <note> Submitted to HIPS '98. </note> <month> November </month> <year> 1997. </year> <note> http://www.cs.dartmouth.edu/reports/abstracts/TR97-323/ </note>
Reference-contexts: The ViC* compiler transforms the source program to remove many of the parallel disk accesses, by performing loop optimizations (such as loop fusion [Hennessy 96]). The ViC* runtime system invokes efficient algorithms to perform parallel I/O operations and overlaps I/O with in-core computation whenever possible <ref> [Colvin 97] </ref>. However, we found no further details on the implementation of the ViC* runtime system. <p> data sets (more precisely, about three times bigger than the in-core memory), a ViC* synthetic benchmark (which computes in parallel the terms 1/ k for 0 &lt; k &lt; K, calls an external function, and then returns their sum) outperforms the virtual memory implementation by a factor of approximately three <ref> [Colvin 97] </ref>. 3. Future Directions The positive results reached by projects such as Passion and ViC* on automatically generating collective I/O calls for out-of-core computation show the effectiveness of compiler-based approaches.
Reference: [Cormen 94] <author> Thomas H. Cormen, and Alex Colvin. </author> <title> ViC*: A Preprocessor for Virtual-Memory C*. </title> <institution> Dartmouth PCS-TR94-243, </institution> <year> 1994. </year> <month> ftp://ftp.cs.dartmouth.edu/TR/TR94-243.ps.Z </month>
Reference-contexts: The underlying computer multiplexes a set of physical processors among the virtual processors to support the parallel model. ViC* adds only one new construct to C*: the outofcore shape modifier and therefore exposes a very simple interface for the programmer. <ref> [Cormen 94] </ref> The ViC* latest version outputs plain C code, whose object is to be linked against the ViC* runtime library. The ViC* compiler transforms the source program to remove many of the parallel disk accesses, by performing loop optimizations (such as loop fusion [Hennessy 96]).
Reference: [Fink 96] <author> S. J. Fink, S. R. Kohn, and S. B. Baden. </author> <title> Flexible Communication Mechanisms for Dynamic Structured Applications. Third International Workshop on Parallel Algorithms for Irregularly Structured Problems (IRREGULAR '96), </title> <address> Santa Barbara, CA, </address> <pages> pp 203-215, </pages> <month> August </month> <year> 1996. </year> <month> file://ftp.cs.ucsd.edu/pub/baden/tr/kelp.ps.gz </month>
Reference-contexts: While out-of-core computation has been the main goal on these projects, perhaps different classes of parallel I/O-intensive application could also benefit from a language-based approach. Some of the programming tools that target specific classes of applications could be extended to express parallel I/O. For example, KeLPs MotionPlans <ref> [Fink 96] </ref> can in principle be extended to describe irregular blocked communication between the memories and disks. Tackling the problem in the language level is attractive because high-level I/O patterns can, in principle, be more easily expressed.
Reference: [Gibson 96] <author> G. A. Gibson, J. V. Vitter, J. Wilkes et al. </author> <booktitle> Report of the Working Group on Storage I/O for Large-Scale Computing . ACM Computing Surbeys, </booktitle> <volume> 28 (4), </volume> <month> December </month> <year> 1996. </year> <note> http://www.acm.org/pubs/citations/journals/surveys/1996-28-4/p779-gibson/ </note>
Reference-contexts: Other efforts deal with efficient use of archives (huge data repositories stored in tertiary memory devices). Comprehensive coverage of the I/O gap problem can be found in recent reports that discuss research opportunities in the area <ref> [Gibson 96] </ref> [Bershad 94]. Of course, one of the major I/O problems found in modern machines is accessing data in secondary storage.
Reference: [Hennessy 96] <author> J. L. Hennessy, and D. A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach . Second Edition. </title> <publisher> Morgan Kaufmann. </publisher> <year> 1996. </year>
Reference-contexts: The ViC* compiler transforms the source program to remove many of the parallel disk accesses, by performing loop optimizations (such as loop fusion <ref> [Hennessy 96] </ref>). The ViC* runtime system invokes efficient algorithms to perform parallel I/O operations and overlaps I/O with in-core computation whenever possible [Colvin 97]. However, we found no further details on the implementation of the ViC* runtime system.
Reference: [Huber 95] <author> James V. Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A High Performance Portable Parallel File System . International Conference on Supercomputing (ICS), </title> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year> <note> http://www-pablo.cs.uiuc.edu/Papers/ICS95-ppfs.html </note>
Reference-contexts: Maybe a fruitful approach is to build specialized user-friendly interfaces (i. e., interfaces that hide most configuration details) on top of an extremely flexible interface (i. e., those which allows a great deal of configuration). A good example of the latter kind of interface is PPFS <ref> [Huber 95] </ref>. PPFS users have fine control on caching, prefetching, data placement, and data sharing policies. It is then possible to build higher-level (and probably specialized) interfaces on top of PPFS. The advantage is that such interfaces would not have to cope with low-level parallel I/O details.
Reference: [IBM 97] <institution> International Bussiness Machines. </institution> <note> MPI-IO/PIOFS Web Page . 1997. http://www.research.ibm.com/people/p/prost/sections/mpiio.html </note>
Reference-contexts: Nevertheless, the I/O part of MPI 2.0 has been implemented as an extension to MPI 1.1 by the Argone National Laboratory [ROMIO 97], by the Lawrence Livermore National Laboratory [Jones 96], and by the IBM T. J. Watson Research Center <ref> [IBM 97] </ref>. In particular, the initial performance results obtained with ROMIO show improvements of two orders of magnitude on an astrophysics application that performs I/O on tridimensional (BLOCK, BLOCK, BLOCK) distributed arrays (see tables below) [ROMIO 97].
Reference: [Jones 96] <author> Terry Jones, Richard Mark, Jeanne Martin, John May, Elsie Pierce, and Linda Stanberry. </author> <title> An MPI-IO Interface to HPSS . Goddard Conference on Mass Storage Systems. </title> <note> 1996. http://www.llnl.gov/people/trj/goddard/ </note>
Reference-contexts: Nevertheless, the I/O part of MPI 2.0 has been implemented as an extension to MPI 1.1 by the Argone National Laboratory [ROMIO 97], by the Lawrence Livermore National Laboratory <ref> [Jones 96] </ref>, and by the IBM T. J. Watson Research Center [IBM 97]. In particular, the initial performance results obtained with ROMIO show improvements of two orders of magnitude on an astrophysics application that performs I/O on tridimensional (BLOCK, BLOCK, BLOCK) distributed arrays (see tables below) [ROMIO 97].
Reference: [Madhyastha 97] <author> Tara M. Madhyastha, and Daniel A. Reed. </author> <title> Exploiting Global Input/Output Access Pattern Classification . In Proceedings of Supercomputing '97, </title> <address> San Jose, CA, </address> <month> November </month> <year> 1997. </year> <note> http://www-pablo.cs.uiuc.edu/People/tara/sc97.ps.Z </note>
Reference-contexts: The advantage is that such interfaces would not have to cope with low-level parallel I/O details. Still regarding automatic tuning, there are promising results on runtime identification of patterns in the I/O requests generated by parallel applications <ref> [Madhyastha 97] </ref>. Since the runtime system might be able to automatically identify some collective I/O behavior, this approach can be used to simplify the I/O interface for parallel applications by omitting details that can be inferred by the system, providing therefore a better balance between performance and simplicity of use.
Reference: [MPI 97] <editor> Message Passing Interface Forum . MPI-2: </editor> <booktitle> Extensions to the Message-Passaing Interface. </booktitle> <year> 1997. </year> <note> http://www.mpi-forum.org/docs/mpi-20.ps.Z </note>
Reference-contexts: The MPI 1.0 specification was finished in May 1994 and it is safe to say MPI is the standard message-based API for parallel programming, having broad vendor support and also independent implementations. Such an acceptance has motivated the development of MPI 2.0 <ref> [MPI 97] </ref>, which extends the original version in many ways, one of which is parallel I/O. MPI-IO is based on the concept of two-phase I/O. The idea is to trade computation and communication for I/O. <p> However, some restrictions apply on nonblocking collective call, such as each file may have at most one active nonblocking collective operation at any time and no collective I/O operations are permitted on a file concurrently with a split collective access on that file <ref> [MPI 97] </ref>. Table 2.1 presents all the MPI data access routines grouped by how they behave regarding positioning, synchronism, and coordination. The user can specify weaker consistency semantics and provide hints (such as file access patterns and file system specific information) in order to ease optimizations. <p> MPI_FILE_WRITE_AT MPI_FILE_READ_AT_ALL MPI_FILE_WRITE_AT_ALL explicit offsets nonblocking MPI_FILE_IREAD_AT MPI_FILE_IWRITE_AT MPI_FILE_READ_AT_ALL_BEGIN MPI_FILE_READ_AT_ALL_END MPI_FILE_WRITE_AT_ALL_BEING MPI_FILE_WRITE_AT_ALL_END blocking MPI_FILE_READ MPI_FILE_WRITE MPI_FILE_READ_ALL MPI_FILE_WRITE_ALL individual file pointers nonblocking MPI_FILE_IREAD MPI_FILE_IWRITE MPI_FILE_READ_ALL_BEGIN MPI_FILE_READ_ALL_END MPI_FILE_WRITE_ALL_BEGIN MPI_FILE_WRITE_ALL_END blocking MPI_FILE_READ MPI_FILE_WRITE MPI_FILE_READ_ORDERED MPI_FILE_WRITE_ORDERED shared file pointers nonblocking MPI_FILE_IREAD_SHARED MPI_FILE_IWRITE_SHARED MPI_FILE_READ_ORDERED_BEGIN MPI_FILE_READ_ORDERED_END MPI_FILE_WRITE_ ORDERED_BEGIN MPI_FILE_WRITE_ORDERED_END Table 2.1 MPI-IO Data Access Routines (from <ref> [MPI 97] </ref>) MPI 2.0 was released in July 1997 and there is no complete implementation available yet.
Reference: [OpenMP 97] <author> OpenMP Consortium. OpenMP: </author> <title> A Proposed Standard API for Shared Memory Programming. </title> <month> October </month> <year> 1997. </year> <note> http://www.openmp.org/mp-documents/paper/paper.ps [Pablo 95] -. Pablo Web Page. 1995. http://bugle.cs.uiuc.edu/Projects/Pablo/pablo.html [ROMIO 97] -. ROMIO Web Page. 1997. http://www.mcs.anl.gov/home/thakur/romio/ </note>
Reference-contexts: Another concern about the interface design is that all works we know about assume some message-based programming model. Apparently, there is no research on how to express parallel I/O request on thread-based models, such as Cilk [Blumofe 95], COOL [Chandra 93], or OpenMP <ref> [OpenMP 97] </ref>. Since some parallel applications are much easier to program by creating threads on the fly, how such applications should express parallel I/O operation is an important (and untouched) question. Yet other key and yet unanswered question is how to combine performance and portability.
Reference: [Seamons 94] <author> K.E. Seamons, and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O . In Proceedings of Supercomputing '94, </title> <address> pages 650-659, Washington, DC, November 1994. </address> <publisher> IEEE Computer Society Press. </publisher> <address> http://drl.cs.uiuc.edu/pubs/super94.ps </address>
Reference-contexts: The timestamp file is read either by having the application selecting which timestamp should be in memory at a given moment or by providing a paged-on-demand higher dimensional array (time is the added dimension). <ref> [Seamons 94] </ref> Arrays can be grouped and the operations can be performed on a group of arrays. This provides an even higher-level interface for collective I/O, which enables the server to better optimize the physical I/O requests.
Reference: [Seamons 95] <author> K.E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda . In Proceedings of Supercomputing '95, </title> <address> San Diego, CA, December 1995. </address> <publisher> IEEE Computer Society Press. </publisher> <address> http://www.supercomp.org/sc95/proceedings/520_SEAM/SC95.HTM </address>
Reference-contexts: Arrays are distributed across both compute and I/O nodes using HPF-style BLOCK and *-based array schema. This system has two basic components: clients (which run on the compute nodes) and servers (which run on the I/O nodes). Figure 2.1 depicts Pandas architecture <ref> [Seamons 95] </ref>. Panda only supports collective I/O. In other words, Panda works within the SPMD paradigm and assumes all clients reach the collective I/O call at (approximately) the same time. The interface supports checkpointing (and restoring) an array and also timestamping an array onto an append-only (distributed) file. <p> Panda achieves throughputs close to the full capacity of the underlying AIX file system, using a variety of array sizes, number of nodes, and disk schemas. <ref> [Seamons 95] </ref> 2.3. Passion (http://www.cat.syr.edu/passion.html) The Passion (Parallel and Scalable Software for Input-Output) project aims to support data parallel out-of-core computation. The user writes programs in HPF (a similar language could also be supported) and the compiler obtains distribution information from the regular HPF directives.
Reference: [Sunderam 90] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. In Concurrency: </title> <journal> Practice and Experience, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: Objects can belong to ParSets. Methods can be invoked on every element of a ParSet via a single operation. The system is free to execute methods in parallel (it can, for example, run the invoked method in parallel on the machines that actually store the objects). PShore uses PVM <ref> [Sunderam 90] </ref> as the communication infrastructure. The programmer invokes a ParSet operation in a single thread. While this can be convenient for many applications, the lack of an interface for collective I/O might preclude some optimizations.
Reference: [Thakur 94] <author> Rajeev Thakur, Rajesh Bordawekar, Alok Choudhary, Ravi Ponnusamy, and Tarvinder Singh. </author> <title> Passion Runtime Library for Parallel I/O . In Proc. </title> <booktitle> of the Scalable Parallel Libraries Conference, </booktitle> <month> October </month> <year> 1994. </year> <month> ftp://ftp.npac.syr.edu/pub/users/thakur/papers/splc94_passion_runtime.ps.Z </month>
Reference-contexts: Passion-compiled programs are able to access global out-of-core arrays very much in the same way regular HPF programs deal with global (in-core) arrays. The global out-of-core arrays are partitioned in many local out-of-core arrays (which are stored in the different processor). Figure 2.2 portrays the Passion architecture <ref> [Thakur 94] </ref> [Bordawekar 95a]. In addition to communication, the Passion compiler also takes care of swapping the pieces of the global array in and out of memory. The generated code calls the Passion runtime library, which performs some optimizations, such as prefetching and data sieve [Thakur 94]. <p> 2.2 portrays the Passion architecture <ref> [Thakur 94] </ref> [Bordawekar 95a]. In addition to communication, the Passion compiler also takes care of swapping the pieces of the global array in and out of memory. The generated code calls the Passion runtime library, which performs some optimizations, such as prefetching and data sieve [Thakur 94]. Prefetching overlaps I/O and computation, hiding the I/O latency. Data sieve blocks spaced accesses into a large single access, eliminating the unneeded data in memory. This reduces the number of I/O operations and therefore the software and hardware overhead associated with starting an I/O operation.
Reference: [TMC 93] <institution> Thinking Machines Corporation. </institution> <note> C* Programming Guide. </note> <year> 1993. </year>
Reference-contexts: ViC* (http://www.cs.dartmouth.edu/~thc/ViC.html) ViC* (Virtual memory C*) is an extension of C* to support out-of-core computation. C* is a data parallel language designed by Thinking Machines <ref> [TMC 93] </ref>. A fundamental concept in C* is shape. It describes the structure of parallel variables and specifies how many virtual processors are necessary to perform an operation over the parallel variable. The underlying computer multiplexes a set of physical processors among the virtual processors to support the parallel model.
References-found: 26

