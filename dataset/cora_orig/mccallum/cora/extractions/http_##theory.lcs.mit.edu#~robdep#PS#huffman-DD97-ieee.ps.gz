URL: http://theory.lcs.mit.edu/~robdep/PS/huffman-DD97-ieee.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~robdep/papers.html
Root-URL: 
Email: Email: robdep@theory.lcs.mit.edu.  Email: ads@dia.unisa.it  
Title: A new bound for the data expansion of Huffman codes  
Author: Roberto De Prisco Alfredo De Santis 
Keyword: Index Terms Huffman codes, data expansion of optimal codes, source coding, re dundancy of optimal codes.  
Note: Corresponding author.  
Address: 545 Technology Square, Cambridge, MA 02139 USA.  84081 Baronissi (SA) Italy.  
Affiliation: MIT Laboratory for Computer Science,  Dipartimento di Informatica ed Applicazioni, Universita di Salerno,  
Abstract: In this paper we prove that the maximum data expansion ffi of Huffman codes is upper bounded by ffi &lt; 1:39. This bound improves on the previous best known upper bound ffi &lt; 2. We also provide some characterizations of the maximum data expansion of optimal codes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. M. Capocelli and A. De Santis, </author> <title> "Tight upper bounds on the redundancy of Huffman codes", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. </volume> <editor> IT-35, n. </editor> <volume> 5, </volume> <pages> pp. 1084-1091, </pages> <month> Sept </month> <year> 1989. </year> <month> 12 </month>
Reference-contexts: We refer the reader to [1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 15, 16, 17]. In particular the following bound on r (see <ref> [1, 4, 8, 12, 14] </ref>), as a function of the most likely source letter probability p 1 , holds. r &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 2 H (p 1 ) p 1 ; if 0:5 p 1 &lt; 1 1 + 0:5 (1 <p> However we use bound (2) since it is easier to state 4 and it is sufficient for our proofs. A simple analysis of the above bound (we refer the reader to <ref> [1, 14] </ref> for graphics showing the bound) proves the following facts. Fact 1 If p 1 &lt; 2=9 then r &lt; 0:3082. Fact 2 If p 1 &lt; 0:40 then r &lt; 0:4151. <p> If we consider optimal codes, for which r &lt; 1, we have that ffi &lt; 2: (3) Many upper bounds on the redundancy as functions of probabilities of the source are known (see <ref> [1, 2, 3, 4, 6, 8, 10, 12, 14] </ref>). These bounds on the redundancy immediately yield bounds on the data expansion by means of Theorem 1. 5 Given a source S we can construct another source S 0 by deleting one symbol of S (and normalizing the remaining probabilities).
Reference: [2] <author> R. M. Capocelli and A. De Santis, </author> <title> "A Note on D-ary Huffman codes", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. </volume> <editor> IT-37, n. </editor> <volume> 1, </volume> <pages> pp. 174-179, </pages> <month> Jan </month> <year> 1991. </year>
Reference-contexts: If we consider optimal codes, for which r &lt; 1, we have that ffi &lt; 2: (3) Many upper bounds on the redundancy as functions of probabilities of the source are known (see <ref> [1, 2, 3, 4, 6, 8, 10, 12, 14] </ref>). These bounds on the redundancy immediately yield bounds on the data expansion by means of Theorem 1. 5 Given a source S we can construct another source S 0 by deleting one symbol of S (and normalizing the remaining probabilities).
Reference: [3] <author> R. M. Capocelli and A. De Santis, </author> <title> "New bounds on the redundancy of Huffman codes", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. </volume> <editor> IT-37, n. </editor> <volume> 4, </volume> <pages> pp. 1095-1104, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: If we consider optimal codes, for which r &lt; 1, we have that ffi &lt; 2: (3) Many upper bounds on the redundancy as functions of probabilities of the source are known (see <ref> [1, 2, 3, 4, 6, 8, 10, 12, 14] </ref>). These bounds on the redundancy immediately yield bounds on the data expansion by means of Theorem 1. 5 Given a source S we can construct another source S 0 by deleting one symbol of S (and normalizing the remaining probabilities).
Reference: [4] <author> R. M. Capocelli, R. Giancarlo, and I. J. Taneja, </author> <title> "Bounds on the redundancy of Huffman codes", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-32, </volume> <editor> n. </editor> <volume> 6, </volume> <pages> pp. 854-857, </pages> <month> Dec </month> <year> 1986. </year>
Reference-contexts: We refer the reader to [1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 15, 16, 17]. In particular the following bound on r (see <ref> [1, 4, 8, 12, 14] </ref>), as a function of the most likely source letter probability p 1 , holds. r &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 2 H (p 1 ) p 1 ; if 0:5 p 1 &lt; 1 1 + 0:5 (1 <p> If we consider optimal codes, for which r &lt; 1, we have that ffi &lt; 2: (3) Many upper bounds on the redundancy as functions of probabilities of the source are known (see <ref> [1, 2, 3, 4, 6, 8, 10, 12, 14] </ref>). These bounds on the redundancy immediately yield bounds on the data expansion by means of Theorem 1. 5 Given a source S we can construct another source S 0 by deleting one symbol of S (and normalizing the remaining probabilities).
Reference: [5] <author> J-F. Cheng, S. Dolinar, M. Effros, R. </author> <title> McEliece, "Data expansion with Huffman codes", </title> <booktitle> Proc. of ISIT 95, </booktitle> <address> Whistler, Canada (BC), pag.325, </address> <month> Sept </month> <year> 1995. </year>
Reference-contexts: The problem of studying the maximum data expansion has been introduced by Cheng et al. <ref> [5] </ref>. They proved that ffi N 4 for all N . Hollmann [9] provided an upper bound of ffi N &lt; 1 + r, where r is the redundancy of the code used for the encoding. This bound holds for any (not necessarily optimal) uniquely decodable code. <p> Fact 1 If p 1 &lt; 2=9 then r &lt; 0:3082. Fact 2 If p 1 &lt; 0:40 then r &lt; 0:4151. We remark that there is a source with p 1 = 1=3 whose redundancy is r ' 0:4151, thus the above bound is tight. Cheng et al. <ref> [5] </ref> proved that the maximum data expansion is upper bounded by ffi &lt; 4. An interesting result that relates the maximum data expansion and the redundancy has been recently proved by Hollmann [9]. He proved that the data expansion and the redundancy are related by the following theorem. <p> Cheng et al. <ref> [5] </ref> conjecture a best possible upper bound ffi 4=5. It is likely that the upper bound ffi &lt; 1:39 provided in this paper can be further improved. Acknowledgments. We thank the anonymous referees for helpful comments.
Reference: [6] <author> R. De Prisco and A. De Santis, </author> <title> "On the redundancy achieved by Huffman codes", </title> <journal> Information Sciences, </journal> <volume> Vol. 88, </volume> <pages> nn. 1-4, pp. 131-148, </pages> <month> Jan </month> <year> 1996. </year>
Reference-contexts: If we consider optimal codes, for which r &lt; 1, we have that ffi &lt; 2: (3) Many upper bounds on the redundancy as functions of probabilities of the source are known (see <ref> [1, 2, 3, 4, 6, 8, 10, 12, 14] </ref>). These bounds on the redundancy immediately yield bounds on the data expansion by means of Theorem 1. 5 Given a source S we can construct another source S 0 by deleting one symbol of S (and normalizing the remaining probabilities).
Reference: [7] <author> R. De Prisco and A. De Santis, </author> <title> "On lower bounds for the redundancy of optimal codes", </title> <type> manuscript, </type> <year> 1996. </year>
Reference: [8] <author> R. G. Gallager, </author> <title> "Variation on a theme by Huffman", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-24, </volume> <editor> n. </editor> <volume> 6, </volume> <pages> pp. 668-674, </pages> <month> Dec </month> <year> 1978. </year>
Reference-contexts: We refer the reader to [1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 15, 16, 17]. In particular the following bound on r (see <ref> [1, 4, 8, 12, 14] </ref>), as a function of the most likely source letter probability p 1 , holds. r &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 2 H (p 1 ) p 1 ; if 0:5 p 1 &lt; 1 1 + 0:5 (1 <p> If we consider optimal codes, for which r &lt; 1, we have that ffi &lt; 2: (3) Many upper bounds on the redundancy as functions of probabilities of the source are known (see <ref> [1, 2, 3, 4, 6, 8, 10, 12, 14] </ref>). These bounds on the redundancy immediately yield bounds on the data expansion by means of Theorem 1. 5 Given a source S we can construct another source S 0 by deleting one symbol of S (and normalizing the remaining probabilities). <p> Furthermore, if for some m 1 it holds 1 p 1 2 m+1 + 1 then any optimal code for S must assign a codeword of length m or of length m + 1 to the most likely source letter. Gallager <ref> [8] </ref> has proved the following sibling property for Huffman codes.
Reference: [9] <author> D. H. Hollmann, </author> <title> "A simple proof of finite data expansion per symbol with Huffman codes", </title> <type> Personal communication, </type> <year> 1996. </year>
Reference-contexts: The problem of studying the maximum data expansion has been introduced by Cheng et al. [5]. They proved that ffi N 4 for all N . Hollmann <ref> [9] </ref> provided an upper bound of ffi N &lt; 1 + r, where r is the redundancy of the code used for the encoding. This bound holds for any (not necessarily optimal) uniquely decodable code. <p> Cheng et al. [5] proved that the maximum data expansion is upper bounded by ffi &lt; 4. An interesting result that relates the maximum data expansion and the redundancy has been recently proved by Hollmann <ref> [9] </ref>. He proved that the data expansion and the redundancy are related by the following theorem. Theorem 1 [9] The data expansion and the redundancy of any uniquely decodable (not necessarily optimal) code are related by ffi &lt; 1 + r: In this paper we provide an extension of Hollmann's bound. <p> An interesting result that relates the maximum data expansion and the redundancy has been recently proved by Hollmann <ref> [9] </ref>. He proved that the data expansion and the redundancy are related by the following theorem. Theorem 1 [9] The data expansion and the redundancy of any uniquely decodable (not necessarily optimal) code are related by ffi &lt; 1 + r: In this paper we provide an extension of Hollmann's bound.
Reference: [10] <author> Y. Horibe, </author> <title> "An improved bound for weight-balanced trees", </title> <journal> Inform. Contr., </journal> <volume> vol. 24, </volume> <editor> n. </editor> <volume> 2, </volume> <pages> pp. 148-151, </pages> <year> 1977. </year>
Reference-contexts: If we consider optimal codes, for which r &lt; 1, we have that ffi &lt; 2: (3) Many upper bounds on the redundancy as functions of probabilities of the source are known (see <ref> [1, 2, 3, 4, 6, 8, 10, 12, 14] </ref>). These bounds on the redundancy immediately yield bounds on the data expansion by means of Theorem 1. 5 Given a source S we can construct another source S 0 by deleting one symbol of S (and normalizing the remaining probabilities).
Reference: [11] <author> D. A. Huffman, </author> <title> "A Method for the construction of minimum redundancy codes", </title> <journal> Proc. IRE, </journal> <volume> 40, n.2, </volume> <pages> pp. 1098-1101, </pages> <year> 1952. </year>
Reference-contexts: In particular they are equal when N is a power of 2. Thus any upper bound given on ffi (C) holds also for (1). For the sake of simplicity we drop the ceiling. Given a source S, the Huffman encoding algorithm <ref> [11] </ref> constructs an optimal prefix code for S. The encoding is optimal in the sense that codeword lengths minimize the redundancy. Throughout this paper, unless otherwise specified, we always refer to optimal codes and thus "redundancy" ("data expansion") should be read as redundancy (data expansion) of optimal codes.
Reference: [12] <author> O. Johnsen, </author> <title> "On the redundancy of Huffman codes", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. </volume> <editor> IT-26, n. </editor> <volume> 2, </volume> <pages> pp. 220-222, </pages> <month> Mar </month> <year> 1980. </year>
Reference-contexts: We refer the reader to [1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 15, 16, 17]. In particular the following bound on r (see <ref> [1, 4, 8, 12, 14] </ref>), as a function of the most likely source letter probability p 1 , holds. r &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 2 H (p 1 ) p 1 ; if 0:5 p 1 &lt; 1 1 + 0:5 (1 <p> If we consider optimal codes, for which r &lt; 1, we have that ffi &lt; 2: (3) Many upper bounds on the redundancy as functions of probabilities of the source are known (see <ref> [1, 2, 3, 4, 6, 8, 10, 12, 14] </ref>). These bounds on the redundancy immediately yield bounds on the data expansion by means of Theorem 1. 5 Given a source S we can construct another source S 0 by deleting one symbol of S (and normalizing the remaining probabilities). <p> Let r 0 and ffi 0 be the redundancy and the maximum data expansion of S 0 . Notice that from (3) we have that both ffi and ffi 0 are less than 2. The following result has been obtained by Johnsen <ref> [12] </ref>. Lemma 1 [12] If the most likely source letter probability is greater than 0.4, then a Huffman code assigns a codeword of length 1 to the most likely source letter. Montgomery and Kumar provided a generalization [16]. <p> Let r 0 and ffi 0 be the redundancy and the maximum data expansion of S 0 . Notice that from (3) we have that both ffi and ffi 0 are less than 2. The following result has been obtained by Johnsen <ref> [12] </ref>. Lemma 1 [12] If the most likely source letter probability is greater than 0.4, then a Huffman code assigns a codeword of length 1 to the most likely source letter. Montgomery and Kumar provided a generalization [16].
Reference: [13] <author> G.O.H. Katona and T.O.H. Nemetz, </author> <title> "Huffman codes and self-information", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. </volume> <editor> IT-22, n. </editor> <volume> 3, </volume> <pages> pp. </pages> <month> 337-340 May </month> <year> 1976. </year>
Reference: [14] <author> D.Manstetten, </author> <title> "Tight bounds on the redundancy of Huffman codes", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. </volume> <editor> IT-38, n. </editor> <volume> 1, </volume> <pages> pp. </pages> <month> 144-151 Jan </month> <year> 1992. </year>
Reference-contexts: We refer the reader to [1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 15, 16, 17]. In particular the following bound on r (see <ref> [1, 4, 8, 12, 14] </ref>), as a function of the most likely source letter probability p 1 , holds. r &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 2 H (p 1 ) p 1 ; if 0:5 p 1 &lt; 1 1 + 0:5 (1 <p> The above upper bound is tight only for p 1 2 . We remark that the tight upper bound on the redundancy as a function of p 1 is known for every possible value of p 1 (see <ref> [14] </ref>). However we use bound (2) since it is easier to state 4 and it is sufficient for our proofs. A simple analysis of the above bound (we refer the reader to [1, 14] for graphics showing the bound) proves the following facts. <p> However we use bound (2) since it is easier to state 4 and it is sufficient for our proofs. A simple analysis of the above bound (we refer the reader to <ref> [1, 14] </ref> for graphics showing the bound) proves the following facts. Fact 1 If p 1 &lt; 2=9 then r &lt; 0:3082. Fact 2 If p 1 &lt; 0:40 then r &lt; 0:4151. <p> If we consider optimal codes, for which r &lt; 1, we have that ffi &lt; 2: (3) Many upper bounds on the redundancy as functions of probabilities of the source are known (see <ref> [1, 2, 3, 4, 6, 8, 10, 12, 14] </ref>). These bounds on the redundancy immediately yield bounds on the data expansion by means of Theorem 1. 5 Given a source S we can construct another source S 0 by deleting one symbol of S (and normalizing the remaining probabilities).
Reference: [15] <author> B.L.Montgomery and J.Abrahams, </author> <title> "On the redundancy of optimal prefix-condition codes for finite and infinite sources", </title> <journal> IEEE Trans. Inform. Theory, vol.IT-33, n. </journal> <volume> 1, </volume> <pages> pp. 156-160, </pages> <month> Jan </month> <year> 1987. </year> <month> 13 </month>
Reference: [16] <author> B.L.Montgomery and B.V.K.V.Kumar, </author> <title> "On the average codeword length of optimal binary codes for extended sources", </title> <journal> IEEE Trans. Inform. Theory , vol. IT-33, n. </journal> <volume> 2, </volume> <pages> pp. 293-296, </pages> <month> Mar </month> <year> 1987. </year>
Reference-contexts: The following result has been obtained by Johnsen [12]. Lemma 1 [12] If the most likely source letter probability is greater than 0.4, then a Huffman code assigns a codeword of length 1 to the most likely source letter. Montgomery and Kumar provided a generalization <ref> [16] </ref>. Lemma 2 [16] Let p 1 be the most likely source letter probability p 1 of a source S. <p> The following result has been obtained by Johnsen [12]. Lemma 1 [12] If the most likely source letter probability is greater than 0.4, then a Huffman code assigns a codeword of length 1 to the most likely source letter. Montgomery and Kumar provided a generalization <ref> [16] </ref>. Lemma 2 [16] Let p 1 be the most likely source letter probability p 1 of a source S.
Reference: [17] <author> R. Yeung, </author> <title> "The redundancy theorem and new bounds of the expected length of the Huffman code", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. </volume> <editor> IT-37, n. </editor> <volume> 3, </volume> <pages> pp. </pages> <month> 687-691 May </month> <year> 1991. </year> <month> 14 </month>
References-found: 17

