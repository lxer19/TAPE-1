URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-95-18/MP-TR-95-18.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-95-18/
Root-URL: http://www.cs.wisc.edu
Title: A variable-penalty alternating directions method for convex optimization  
Author: Spyridon Kontogiorgis Robert R. Meyer 
Keyword: parallel computing, alternating direction methods, decomposition, block angular programs. Abbreviated title: variable-penalty ADI.  
Abstract: We study a generalized version of the method of alternating directions as applied to the minimization of the sum of two convex functions subject to linear constraints. The method consists of solving consecutively in each iteration two optimization problems which contain in the objective function both Lagrangian and proximal terms. The minimizers determine the new proximal terms and a simple update of the Lagrangian terms follows. We prove a convergence theorem which extends existing results by relaxing the assumption of uniqueness of minimizers. Another novelty is that we allow penalty matrices, and these may vary per iteration. This can be beneficial in applications, since it allows additional tuning of the method to the problem and can lead to faster convergence relative to fixed penalties. As an application, we derive a decomposition scheme for block angular optimization and present computational results on a class of dual block angular problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.A. Assad. </author> <title> Multicommodity network flows: A survey. </title> <journal> Networks, </journal> <volume> 8 </volume> <pages> 37-91, </pages> <year> 1978. </year>
Reference-contexts: +1 otherwise and write problem (41) as min G 1 (x 1 ; x 2 ; x 3 ) + G 2 (x 4 ) subject to x 3 + x 4 = 1 This is in the form of problem (1), with A = [0 0 1], B = <ref> [1] </ref> and b = 1. Observe that A has not full column rank. Both G 1 and G 2 are convex, proper, closed functions. We take, for simplicity, H t = I; 8t. <p> If this condition is used as a stopping rule in the example in section 3, the algorithm (for nonnegative start) terminates finitely at an optimal point. 5 An ADI decomposition scheme for block angular problems Several classes of models in applied optimization, including multicommodity network flow <ref> [1] </ref> and stochastic scenario analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x <p> the algorithm (for nonnegative start) terminates finitely at an optimal point. 5 An ADI decomposition scheme for block angular problems Several classes of models in applied optimization, including multicommodity network flow <ref> [1] </ref> and stochastic scenario analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = <p> (for nonnegative start) terminates finitely at an optimal point. 5 An ADI decomposition scheme for block angular problems Several classes of models in applied optimization, including multicommodity network flow <ref> [1] </ref> and stochastic scenario analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : <p> models in applied optimization, including multicommodity network flow <ref> [1] </ref> and stochastic scenario analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : :; K: 13 Each function f [i] is finite-valued, convex and continuous, and, in many applications, quadratic in the vector <p> applied optimization, including multicommodity network flow <ref> [1] </ref> and stochastic scenario analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : :; K: 13 Each function f [i] is finite-valued, convex and continuous, and, in many applications, quadratic in the vector x [i] <p> multicommodity network flow <ref> [1] </ref> and stochastic scenario analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : :; K: 13 Each function f [i] is finite-valued, convex and continuous, and, in many applications, quadratic in the vector x [i] , i.e. f <p> In mapping CBA onto problem (1), we choose to incorporate the block constraints in the definition of G 1 and represent the coupling constraints in the definition of G 2 and as explicit linear equality constraints. (Other mappings are discussed in [30].) We define G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 and 8 &gt; : K X d [i] d +1 otherwise Problem CBA can be written as min G 1 x [1] ; : : : ; x [K] + G 2 d [1] ; : : : ; d [K] <p> G 2 and as explicit linear equality constraints. (Other mappings are discussed in [30].) We define G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 and 8 &gt; : K X d [i] d +1 otherwise Problem CBA can be written as min G 1 x [1] ; : : : ; x [K] + G 2 d [1] ; : : : ; d [K] subject to D [i] x [i] = d [i] ; i = 1; : : : ; K which is in the form of problem (1), with the correspondences b 0, <p> in [30].) We define G 1 x <ref> [1] </ref> ; : : : ; x [K] := i=1 and 8 &gt; : K X d [i] d +1 otherwise Problem CBA can be written as min G 1 x [1] ; : : : ; x [K] + G 2 d [1] ; : : : ; d [K] subject to D [i] x [i] = d [i] ; i = 1; : : : ; K which is in the form of problem (1), with the correspondences b 0, A D := diag , and B I. <p> We let the diagonal matrix fl t K consist of K copies of fl t placed along the diagonal. We write in shorthand x for the concatenation of the vectors x <ref> [1] </ref> ; x [2] ; : : : ; x [K] , and similarly for d and p. <p> Specialized algorithms for this problem are reviewed in [35]. To cast it in a format suitable for ADI, we introduce auxiliary vectors of variables x <ref> [1] </ref> ; : : : ; x [K] , the combination of which plays the role of the x variables, and rewrite it in a dual block angular form, 16 as in [16, section 3.7.3] min K X a i fl fl fl subject to x [i] = z b [i] <p> For each class we generated 49 random problems. The weights a i were uniformly distributed in <ref> [1; 10] </ref>, while the components of b were uniformly distributed in [10; 100]. In all runs we chose initial values z 0 = p 0 = 0.
Reference: [2] <author> D.P. Bertsekas. </author> <title> Multiplier methods: A survey. </title> <journal> Automatica, </journal> <volume> 12 </volume> <pages> 133-145, </pages> <year> 1976. </year>
Reference-contexts: From standard duality theory, if (x fl ; z fl ; p fl ) is a saddlepoint of the augmented Lagrangian, then (x fl ; z fl ) is a primal solution of (1) and p fl is an associated dual multiplier. In the method of multipliers <ref> [26, 38, 39, 2] </ref> a saddlepoint is located by an iterative process, consisting of a minimization of the augmented Lagrangian, followed by a steepest-ascent update of the multipliers. (x t+1 ; z t+1 ) 2 argmin x; z p t+1 = p t + r p L (x t+1 ; z <p> finitely at an optimal point. 5 An ADI decomposition scheme for block angular problems Several classes of models in applied optimization, including multicommodity network flow [1] and stochastic scenario analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : :; K: 13 <p> an optimal point. 5 An ADI decomposition scheme for block angular problems Several classes of models in applied optimization, including multicommodity network flow [1] and stochastic scenario analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : :; K: 13 Each function <p> flow [1] and stochastic scenario analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : :; K: 13 Each function f [i] is finite-valued, convex and continuous, and, in many applications, quadratic in the vector x [i] , i.e. f [i] (x <p> and stochastic scenario analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : :; K: 13 Each function f [i] is finite-valued, convex and continuous, and, in many applications, quadratic in the vector x [i] , i.e. f [i] (x [i] ) <p> analysis [41], require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f <ref> [2] </ref> (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A [K] x [K] = b [K] 0 x [i] u [i] ; i = 1; : : :; K: 13 Each function f [i] is finite-valued, convex and continuous, and, in many applications, quadratic in the vector x [i] , i.e. f [i] (x [i] ) = c [i] <p> We let the diagonal matrix fl t K consist of K copies of fl t placed along the diagonal. We write in shorthand x for the concatenation of the vectors x [1] ; x <ref> [2] </ref> ; : : : ; x [K] , and similarly for d and p.
Reference: [3] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: Such assumptions are: A has full column rank and G 2 is the sum of a closed, proper, convex function and a strictly convex C 1 function [16, chapter 3]; A has full column rank [9, 12], <ref> [3, chapter 3] </ref>. The dual algorithm in [17] requires that both primal and dual problems be feasible and that the solution set of the primal be bounded. We will show here that it is possible to dispense with the uniqueness assumption and still obtain a solution of (1) by ADI. <p> Finally, lemma 2.8 shows how to obtain a primal solution for (1) by solving two 4 minimization problems which employ the limits of , Bz t as fixed terms in the objective. The proof employs the saddlepoint approach of <ref> [3, chapter 3] </ref> and [20, chapter 3]. An important difference is that we do not assume minimizer uniqueness for problems (1), (6) or (7). Also, the use of variable penalty requires a more complex argument, involving lemma 2.4.
Reference: [4] <author> J. </author> <title> Cea. Lectures on Optimization Theory and Algorithms. </title> <journal> Lectures on Mathematics and Physics, </journal> <volume> vol. 53. </volume> <publisher> Tata Institute, </publisher> <address> Bombay, 1978. </address> <publisher> Distributed by Springer Verlag, </publisher> <address> Berlin. </address> <month> 21 </month>
Reference-contexts: We will use the following minimum principle lemma, which specializes <ref> [4, theorem 2.3] </ref>. Lemma 2.3 Let X 0 be a convex subset of IR n .
Reference: [5] <author> G. Cheng and M. Teboulle. </author> <title> A proximal-based decomposition method for convex minimization problems. </title> <journal> Mathematical Programming, Series A, </journal> <volume> 64(1) </volume> <pages> 81-110, </pages> <year> 1994. </year>
Reference-contexts: Eckstein and Bertsekas [12] and Cheng and Teboulle <ref> [5] </ref> have constructed ADI variants which allow for inexact minimization. The former also permits relaxation of the primal iterates; in the latter, quadratic proximal terms replace the augmented Lagrangian penalty terms. In the Peaceman-Rachford variant [18], based on [36], a multiplier update is interpolated between 2 the two problems.
Reference: [6] <author> Y.C. Cheng. </author> <title> On the gradient-projection method for solving the nonsymmetric linear complementarity problem. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 43 </volume> <pages> 527-541, </pages> <year> 1984. </year>
Reference-contexts: If x minimizes J, then J 1 (x) J 1 (x) + rJ 2 (x) T We will also use the following lemma, due to Cheng <ref> [6, lemma 2.1] </ref>. Lemma 2.4 Let and * t be two sequences of nonnegative numbers, with P 1 and a t+1 a t + * t . Then converges. In order to prove theorem 2.1, we state and prove a collection of lemmas.
Reference: [7] <author> G. Cohen and D.L. Zhu. </author> <title> Decomposition coordination methods in large scale optimization problems: The nondifferentiable case and the case of augmented lagrangians. </title> <editor> In J.B. Cruz, editor, </editor> <booktitle> Advances in Large Scale Systems Theory and Applications, </booktitle> <pages> pages 203-266. </pages> <publisher> JAI Press, </publisher> <address> Greenwich, CT, </address> <year> 1983. </year>
Reference-contexts: On the theoretical level, an open issue is whether the convergence properties are preserved if the quadratic penalty is replaced by other penalty functions, such as Bregman's [45, 11], or the class of strongly convex functions employed in the auxiliary problem method <ref> [7] </ref>.
Reference: [8] <author> J. Douglas and H.H. Rachford Jr. </author> <title> On the numerical solution of heat conduction problems in two-and three-space variables. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 82 </volume> <pages> 421-439, </pages> <year> 1956. </year>
Reference-contexts: The connection to the proximal point algorithm is discussed in Rockafellar [40]. It has been shown [32] that the method is an instance of the Douglas-Rachford splitting <ref> [8] </ref> for finding a zero of a maximal monotone operator.
Reference: [9] <author> J. Eckstein. </author> <title> Splitting Methods for Monotone Operators with Applications to Parallel Optimization. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Civil Engineering, </institution> <year> 1989. </year> <note> Available as Technical Report LIDS-TH-1877, MIT 1989. </note>
Reference-contexts: The method has been studied extensively in the theoretical frameworks of both Lagrangian functions (Glowinski with Chan, Marrocco, Fortin and Le Tallec [16, 20]), and maximal monotone operators (Lions and Mercier [32], Gabay [18], Eckstein and Bertsekas <ref> [9, 12] </ref>). The connection to the proximal point algorithm is discussed in Rockafellar [40]. It has been shown [32] that the method is an instance of the Douglas-Rachford splitting [8] for finding a zero of a maximal monotone operator. <p> Several other decomposition schemes, such as the algorithm of Han and Lou [24], Spingarn's method of partial inverses [42, 43], the progressive hedging algorithm of Rockafellar and Wets [41] and Golshtein's block method of convex programming [21, 22] are also instances of the Douglas-Rachford splitting (see <ref> [9] </ref> for a demonstration). Eckstein and Bertsekas [12] and Cheng and Teboulle [5] have constructed ADI variants which allow for inexact minimization. The former also permits relaxation of the primal iterates; in the latter, quadratic proximal terms replace the augmented Lagrangian penalty terms. <p> Such assumptions are: A has full column rank and G 2 is the sum of a closed, proper, convex function and a strictly convex C 1 function [16, chapter 3]; A has full column rank <ref> [9, 12] </ref>, [3, chapter 3]. The dual algorithm in [17] requires that both primal and dual problems be feasible and that the solution set of the primal be bounded. <p> In this case the computational performance depends strongly on the value of the penalty. Experience on a variety of applications [16, chapter 5], <ref> [9, chapter 7] </ref>, [17] has shown that if the penalty is chosen too small or too large the solution time can significantly increase. For certain simple problems an optimal value of can be found by spectral techniques [16, chapter 1].
Reference: [10] <author> J. Eckstein. </author> <title> The alternating step method for monotropic programming on the Connection Machine CM-2. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(1) </volume> <pages> 293-318, </pages> <year> 1993. </year>
Reference-contexts: In network flow problems, accelerated convergence has been observed with heuristics that vary the penalty for finitely many iterations, or even use a separate penalty for each linear constraint <ref> [34, 10, 31] </ref>. In this article we present convergence results for a general variable penalty algorithm, in which a symmetric positive definite (spd) matrix H t is employed in the Lagrangian and proximal term, to allow for linear transformations of the constraints. <p> For each class we generated 49 random problems. The weights a i were uniformly distributed in <ref> [1; 10] </ref>, while the components of b were uniformly distributed in [10; 100]. In all runs we chose initial values z 0 = p 0 = 0. <p> For each class we generated 49 random problems. The weights a i were uniformly distributed in [1; 10], while the components of b were uniformly distributed in <ref> [10; 100] </ref>. In all runs we chose initial values z 0 = p 0 = 0. We terminated a run when all components of two successive (z; p) iterates agreed to at least D significant digits, for D = 6 and D = 8.
Reference: [11] <author> J. Eckstein. </author> <title> Nonlinear proximal point algorithms using Bregman functions, with applications to convex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 18 </volume> <pages> 202-226, </pages> <year> 1993. </year>
Reference-contexts: On the theoretical level, an open issue is whether the convergence properties are preserved if the quadratic penalty is replaced by other penalty functions, such as Bregman's <ref> [45, 11] </ref>, or the class of strongly convex functions employed in the auxiliary problem method [7].
Reference: [12] <author> J. Eckstein and D.P. Bertsekas. </author> <title> On the Douglas-Rachford splitting method and the proximal point method for maximal monotone operators. </title> <journal> Mathematical Programming, Series A, </journal> <volume> 55(3) </volume> <pages> 293-318, </pages> <year> 1992. </year>
Reference-contexts: The method has been studied extensively in the theoretical frameworks of both Lagrangian functions (Glowinski with Chan, Marrocco, Fortin and Le Tallec [16, 20]), and maximal monotone operators (Lions and Mercier [32], Gabay [18], Eckstein and Bertsekas <ref> [9, 12] </ref>). The connection to the proximal point algorithm is discussed in Rockafellar [40]. It has been shown [32] that the method is an instance of the Douglas-Rachford splitting [8] for finding a zero of a maximal monotone operator. <p> Eckstein and Bertsekas <ref> [12] </ref> and Cheng and Teboulle [5] have constructed ADI variants which allow for inexact minimization. The former also permits relaxation of the primal iterates; in the latter, quadratic proximal terms replace the augmented Lagrangian penalty terms. <p> Such assumptions are: A has full column rank and G 2 is the sum of a closed, proper, convex function and a strictly convex C 1 function [16, chapter 3]; A has full column rank <ref> [9, 12] </ref>, [3, chapter 3]. The dual algorithm in [17] requires that both primal and dual problems be feasible and that the solution set of the primal be bounded.
Reference: [13] <author> J. Eckstein and M. Ferris. </author> <title> Operator splitting methods for monotone linear complementarity problems. </title> <type> Technical Report TMC-239, </type> <institution> Thinking Machines Corporation, Cambridge, Massachusetts, </institution> <year> 1992. </year>
Reference-contexts: This algorithm requires more stringent assumptions for convergence and is less robust numerically [16]. Fukushima [17] presents an ADI method for the dual problem of (1). ADI methods have been constructed for several other classes of problems, such as variational inequalities [19] and the monotone linear complementarity problem <ref> [13] </ref>. The existing literature on the basic ADI algorithm (3)-(5) treats the slightly simpler case where B = I and b = 0. Convergence has been proved under assumption 1.1 and additional ones that guarantee that the original problem and/or the ADI problems are uniquely solvable.
Reference: [14] <author> J. Eckstein and M. Fukushima. </author> <title> Some reformulations and applications of the alternating direction method of multipliers. In W.W. Hager, </title> <editor> D.W. Hearn, and P.M. Pardalos, editors, </editor> <title> Large Scale Optimization: </title> <booktitle> State of the Art, </booktitle> <pages> pages 115-134. </pages> <publisher> Kluwer Academic, </publisher> <address> Dordrecht, The Netherlands, </address> <year> 1994. </year>
Reference-contexts: Appropriate definitions of G 1 and G 2 can lead to a fine grain (activity-level) decomposition scheme <ref> [14] </ref>. The choice of granularity depends on the architecture of the target computing environment: a coarse grain method may perform better in a cluster of workstations, while a fine grain one may be better suited to a massively parallel system.
Reference: [15] <author> W. Findeisen, F.N. Bailey, M. Brdys, K. Malinowski, P. Tatjewski, and A. Wozniak. </author> <title> Control and Coordination in Hierarchical Systems. IIASA series. </title> <publisher> John Wiley & Sons, </publisher> <year> 1980. </year>
Reference-contexts: Such updates, although computationally expensive, may yield faster convergence. When applied to linear block angular problems, the coarse grain ADI decomposition schemes require solving quadratic problems at each iteration. To overcome this computational drawback we may iteratively linearize the quadratic term, as done in [44, 33], <ref> [15, section 2.5.1] </ref> for the method of multipliers, or replace it with a piecewise linear local approximation, as done in the convex optimization methods in [29, 28].
Reference: [16] <author> M. Fortin and R. Glowinski, </author> <title> editors. Augmented Lagrangian Methods: Applications to the Numerical Solution of Boundary-Value Problems. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1983. </year>
Reference-contexts: Arbitrary p 0 and z 0 can be chosen as starting point, and, in the basic case, the penalty is constant. The method has been studied extensively in the theoretical frameworks of both Lagrangian functions (Glowinski with Chan, Marrocco, Fortin and Le Tallec <ref> [16, 20] </ref>), and maximal monotone operators (Lions and Mercier [32], Gabay [18], Eckstein and Bertsekas [9, 12]). The connection to the proximal point algorithm is discussed in Rockafellar [40]. <p> In the Peaceman-Rachford variant [18], based on [36], a multiplier update is interpolated between 2 the two problems. This algorithm requires more stringent assumptions for convergence and is less robust numerically <ref> [16] </ref>. Fukushima [17] presents an ADI method for the dual problem of (1). ADI methods have been constructed for several other classes of problems, such as variational inequalities [19] and the monotone linear complementarity problem [13]. <p> Such assumptions are: A has full column rank and G 2 is the sum of a closed, proper, convex function and a strictly convex C 1 function <ref> [16, chapter 3] </ref>; A has full column rank [9, 12], [3, chapter 3]. The dual algorithm in [17] requires that both primal and dual problems be feasible and that the solution set of the primal be bounded. <p> In this case the computational performance depends strongly on the value of the penalty. Experience on a variety of applications <ref> [16, chapter 5] </ref>, [9, chapter 7], [17] has shown that if the penalty is chosen too small or too large the solution time can significantly increase. For certain simple problems an optimal value of can be found by spectral techniques [16, chapter 1]. <p> Experience on a variety of applications [16, chapter 5], [9, chapter 7], [17] has shown that if the penalty is chosen too small or too large the solution time can significantly increase. For certain simple problems an optimal value of can be found by spectral techniques <ref> [16, chapter 1] </ref>. In the general case the choice of a good value of is a question of considerable experimentation and of familiarity with the characteristics of the problem. In such cases an appropriate variable penalty heuristic can result in computational savings. <p> To cast it in a format suitable for ADI, we introduce auxiliary vectors of variables x [1] ; : : : ; x [K] , the combination of which plays the role of the x variables, and rewrite it in a dual block angular form, 16 as in <ref> [16, section 3.7.3] </ref> min K X a i fl fl fl subject to x [i] = z b [i] ; i = 1; : : :; K We pair a multiplier vector p [i] with each block of constraints and use a separate penalty value i for each block. <p> x t+1 2 a i i fl [i] fl 2 5 fi t [i] := z t b [i] + p t z t+1 = ( i=1 i ) 1 i=1 i b [i] + x t+1 [i] (59) For a single penalty , the updates agree with (7.40)-(7.43) in <ref> [16, section 3.7.3] </ref>. <p> In this case, by (59) and (57) we have that P K [i] = 0, for t 0, and thus (59) simplifies to z t+1 = K i=1 b [i] + x t+1 i We note that the earlier theory in <ref> [16, chapter 3] </ref> cannot characterize the convergence of this iterative scheme, because the objective function in (54) is not strictly convex. In contrast, our corollary 4.2 guarantees that converges to a primal solution of the problem.
Reference: [17] <author> M. Fukushima. </author> <title> Application of the alternating direction method of multipliers to separable convex programming problems. </title> <journal> Computational Optimization and Applications, </journal> <volume> 1 </volume> <pages> 93-111, </pages> <year> 1992. </year>
Reference-contexts: In the Peaceman-Rachford variant [18], based on [36], a multiplier update is interpolated between 2 the two problems. This algorithm requires more stringent assumptions for convergence and is less robust numerically [16]. Fukushima <ref> [17] </ref> presents an ADI method for the dual problem of (1). ADI methods have been constructed for several other classes of problems, such as variational inequalities [19] and the monotone linear complementarity problem [13]. <p> Such assumptions are: A has full column rank and G 2 is the sum of a closed, proper, convex function and a strictly convex C 1 function [16, chapter 3]; A has full column rank [9, 12], [3, chapter 3]. The dual algorithm in <ref> [17] </ref> requires that both primal and dual problems be feasible and that the solution set of the primal be bounded. We will show here that it is possible to dispense with the uniqueness assumption and still obtain a solution of (1) by ADI. <p> In this case the computational performance depends strongly on the value of the penalty. Experience on a variety of applications [16, chapter 5], [9, chapter 7], <ref> [17] </ref> has shown that if the penalty is chosen too small or too large the solution time can significantly increase. For certain simple problems an optimal value of can be found by spectral techniques [16, chapter 1].
Reference: [18] <author> D. Gabay. </author> <title> Applications of the method of multipliers to variational inequalities. </title> <editor> In M. Fortin and R. Glowinski, editors, </editor> <title> Augmented Lagrangian Methods: Applications to the Numerical Solution of Boundary-Valued Problems, </title> <address> pages 299-331. </address> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1983. </year>
Reference-contexts: The method has been studied extensively in the theoretical frameworks of both Lagrangian functions (Glowinski with Chan, Marrocco, Fortin and Le Tallec [16, 20]), and maximal monotone operators (Lions and Mercier [32], Gabay <ref> [18] </ref>, Eckstein and Bertsekas [9, 12]). The connection to the proximal point algorithm is discussed in Rockafellar [40]. It has been shown [32] that the method is an instance of the Douglas-Rachford splitting [8] for finding a zero of a maximal monotone operator. <p> Eckstein and Bertsekas [12] and Cheng and Teboulle [5] have constructed ADI variants which allow for inexact minimization. The former also permits relaxation of the primal iterates; in the latter, quadratic proximal terms replace the augmented Lagrangian penalty terms. In the Peaceman-Rachford variant <ref> [18] </ref>, based on [36], a multiplier update is interpolated between 2 the two problems. This algorithm requires more stringent assumptions for convergence and is less robust numerically [16]. Fukushima [17] presents an ADI method for the dual problem of (1).
Reference: [19] <author> D. Gabay and B. Mercier. </author> <title> A dual algorithm for the solution of nonlinear variational problems via finite element approximation. </title> <journal> Computers and Mathematics with Applications, </journal> <volume> 2 </volume> <pages> 17-40, </pages> <year> 1976. </year>
Reference-contexts: This algorithm requires more stringent assumptions for convergence and is less robust numerically [16]. Fukushima [17] presents an ADI method for the dual problem of (1). ADI methods have been constructed for several other classes of problems, such as variational inequalities <ref> [19] </ref> and the monotone linear complementarity problem [13]. The existing literature on the basic ADI algorithm (3)-(5) treats the slightly simpler case where B = I and b = 0.
Reference: [20] <author> R. Glowinski and P. Le Tallec. </author> <title> Augmented Lagrangian and Operator-Splitting Methods in Nonlinear Mechanics. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1989. </year>
Reference-contexts: Arbitrary p 0 and z 0 can be chosen as starting point, and, in the basic case, the penalty is constant. The method has been studied extensively in the theoretical frameworks of both Lagrangian functions (Glowinski with Chan, Marrocco, Fortin and Le Tallec <ref> [16, 20] </ref>), and maximal monotone operators (Lions and Mercier [32], Gabay [18], Eckstein and Bertsekas [9, 12]). The connection to the proximal point algorithm is discussed in Rockafellar [40]. <p> Finally, lemma 2.8 shows how to obtain a primal solution for (1) by solving two 4 minimization problems which employ the limits of , Bz t as fixed terms in the objective. The proof employs the saddlepoint approach of [3, chapter 3] and <ref> [20, chapter 3] </ref>. An important difference is that we do not assume minimizer uniqueness for problems (1), (6) or (7). Also, the use of variable penalty requires a more complex argument, involving lemma 2.4.
Reference: [21] <author> Ye. G. Golshtein. </author> <title> The block method of convex programming. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 33 </volume> <pages> 584-587, </pages> <year> 1986. </year>
Reference-contexts: Several other decomposition schemes, such as the algorithm of Han and Lou [24], Spingarn's method of partial inverses [42, 43], the progressive hedging algorithm of Rockafellar and Wets [41] and Golshtein's block method of convex programming <ref> [21, 22] </ref> are also instances of the Douglas-Rachford splitting (see [9] for a demonstration). Eckstein and Bertsekas [12] and Cheng and Teboulle [5] have constructed ADI variants which allow for inexact minimization.
Reference: [22] <author> Ye. G. Golshtein. </author> <title> A general approach to decomposition of optimization systems. </title> <journal> Soviet Journal of Computer and Systems Sciences, </journal> <volume> 25(3) </volume> <pages> 105-114, </pages> <year> 1987. </year> <month> 22 </month>
Reference-contexts: Several other decomposition schemes, such as the algorithm of Han and Lou [24], Spingarn's method of partial inverses [42, 43], the progressive hedging algorithm of Rockafellar and Wets [41] and Golshtein's block method of convex programming <ref> [21, 22] </ref> are also instances of the Douglas-Rachford splitting (see [9] for a demonstration). Eckstein and Bertsekas [12] and Cheng and Teboulle [5] have constructed ADI variants which allow for inexact minimization.
Reference: [23] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: It is based on the following inequality, due to Weyl. We let k (A), k = 1; : : : ; m, denote the k-th largest eigenvalue of a real symmetric m fi m matrix A, i.e. 1 (A) 2 (A) : : : m (A). Lemma 2.9 <ref> [23, lemma 8.1.3] </ref> Let A and E be real symmetric m fi m matrices. Then k (A) + m (E) k (A + E) k (A) + 1 (E); k = 1; : : : ; m: The following lemma describes the iterative construction.
Reference: [24] <author> S.P. Han and G. Lou. </author> <title> A parallel algorithm for a class of convex problems. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 26 </volume> <pages> 345-355, </pages> <year> 1988. </year>
Reference-contexts: The connection to the proximal point algorithm is discussed in Rockafellar [40]. It has been shown [32] that the method is an instance of the Douglas-Rachford splitting [8] for finding a zero of a maximal monotone operator. Several other decomposition schemes, such as the algorithm of Han and Lou <ref> [24] </ref>, Spingarn's method of partial inverses [42, 43], the progressive hedging algorithm of Rockafellar and Wets [41] and Golshtein's block method of convex programming [21, 22] are also instances of the Douglas-Rachford splitting (see [9] for a demonstration).
Reference: [25] <author> P. Henrici. </author> <title> Elements of Numerical Analysis. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: A possible strategy is to choose H t such that the quadratic proximal terms are approximately diagonalized. Techniques for the local acceleration of linear convergence, such as Aitken's 2 -method <ref> [25, section 5.9] </ref>, may also be beneficial.
Reference: [26] <author> M. Hestenes. </author> <title> Multiplier and gradient methods. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 4 </volume> <pages> 303-320, </pages> <year> 1969. </year>
Reference-contexts: From standard duality theory, if (x fl ; z fl ; p fl ) is a saddlepoint of the augmented Lagrangian, then (x fl ; z fl ) is a primal solution of (1) and p fl is an associated dual multiplier. In the method of multipliers <ref> [26, 38, 39, 2] </ref> a saddlepoint is located by an iterative process, consisting of a minimization of the augmented Lagrangian, followed by a steepest-ascent update of the multipliers. (x t+1 ; z t+1 ) 2 argmin x; z p t+1 = p t + r p L (x t+1 ; z
Reference: [27] <author> R.A. Horn and C.R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1993. </year>
Reference-contexts: Since the matrix H t H t+1 is ultimately positive semidefinite, so ultimately is (H t+1 ) 1 (H t ) 1 <ref> [27, corollary 7.7.4] </ref>, and therefore * t 0, ultimately. A telescopic summation shows that k X * t = kp fl k (H k+1 ) 1 kp fl k (H 0 ) 1 ; 8k 0 Because of assumption 1.3, these partial sums are uniformly bounded from above.
Reference: [28] <author> P.V. Kamesam and R.R. Meyer. </author> <title> Multipoint methods for separable nonlinear networks. </title> <journal> Mathematical Programming Study, </journal> <volume> 22 </volume> <pages> 185-205, </pages> <year> 1984. </year>
Reference-contexts: To overcome this computational drawback we may iteratively linearize the quadratic term, as done in [44, 33], [15, section 2.5.1] for the method of multipliers, or replace it with a piecewise linear local approximation, as done in the convex optimization methods in <ref> [29, 28] </ref>. On the theoretical level, an open issue is whether the convergence properties are preserved if the quadratic penalty is replaced by other penalty functions, such as Bregman's [45, 11], or the class of strongly convex functions employed in the auxiliary problem method [7].
Reference: [29] <author> C.Y. Kao and R.R. Meyer. </author> <title> Secant approximation methods for convex optimization. </title> <journal> Mathematical Programming Study, </journal> <volume> 14 </volume> <pages> 143-162, </pages> <year> 1981. </year>
Reference-contexts: To overcome this computational drawback we may iteratively linearize the quadratic term, as done in [44, 33], [15, section 2.5.1] for the method of multipliers, or replace it with a piecewise linear local approximation, as done in the convex optimization methods in <ref> [29, 28] </ref>. On the theoretical level, an open issue is whether the convergence properties are preserved if the quadratic penalty is replaced by other penalty functions, such as Bregman's [45, 11], or the class of strongly convex functions employed in the auxiliary problem method [7].
Reference: [30] <author> S. Kontogiorgis. </author> <title> Alternating Directions Methods for the Parallel Solution of Large-Scale Block-Structured Optimization Problems. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, Department of Computer Sciences, </institution> <year> 1994. </year> <note> Available as Report MP 94-13. </note>
Reference-contexts: In mapping CBA onto problem (1), we choose to incorporate the block constraints in the definition of G 1 and represent the coupling constraints in the definition of G 2 and as explicit linear equality constraints. (Other mappings are discussed in <ref> [30] </ref>.) We define G 1 x [1] ; : : : ; x [K] := i=1 and 8 &gt; : K X d [i] d +1 otherwise Problem CBA can be written as min G 1 x [1] ; : : : ; x [K] + G 2 d [1] ;
Reference: [31] <author> S. Kontogiorgis, R. De Leone, and R.R. Meyer. </author> <title> Alternating direction splittings for block angular parallel optimization. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 90(1), </volume> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: In network flow problems, accelerated convergence has been observed with heuristics that vary the penalty for finitely many iterations, or even use a separate penalty for each linear constraint <ref> [34, 10, 31] </ref>. In this article we present convergence results for a general variable penalty algorithm, in which a symmetric positive definite (spd) matrix H t is employed in the Lagrangian and proximal term, to allow for linear transformations of the constraints. <p> This is a resource proximization (RP) splitting <ref> [31] </ref>, in which the activities x t [i] always satisfy the block constraints and the target resource allocations d t [i] always satisfy the coupling constraints. <p> The choice of granularity depends on the architecture of the target computing environment: a coarse grain method may perform better in a cluster of workstations, while a fine grain one may be better suited to a massively parallel system. In <ref> [31] </ref> we present computational results for an ultimately-fixed-penalty variant of the coarse grain RP decomposition on the Connection Machine 5 parallel supercomputer. The CM-5 can be viewed as a cluster of powerful processors linked by fast networks.
Reference: [32] <author> P.L. Lions and B. Mercier. </author> <title> Splitting algorithms for the sum of two nonlinear operators. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16(6) </volume> <pages> 964-979, </pages> <year> 1979. </year>
Reference-contexts: The method has been studied extensively in the theoretical frameworks of both Lagrangian functions (Glowinski with Chan, Marrocco, Fortin and Le Tallec [16, 20]), and maximal monotone operators (Lions and Mercier <ref> [32] </ref>, Gabay [18], Eckstein and Bertsekas [9, 12]). The connection to the proximal point algorithm is discussed in Rockafellar [40]. It has been shown [32] that the method is an instance of the Douglas-Rachford splitting [8] for finding a zero of a maximal monotone operator. <p> extensively in the theoretical frameworks of both Lagrangian functions (Glowinski with Chan, Marrocco, Fortin and Le Tallec [16, 20]), and maximal monotone operators (Lions and Mercier <ref> [32] </ref>, Gabay [18], Eckstein and Bertsekas [9, 12]). The connection to the proximal point algorithm is discussed in Rockafellar [40]. It has been shown [32] that the method is an instance of the Douglas-Rachford splitting [8] for finding a zero of a maximal monotone operator. <p> Previous theoretical work on (3)-(5) has focused on the case of a fixed penalty . It has been proven that, under additional assumptions of coercivity and Lipschitz continuity, the rate of convergence is linear. Then an optimal value of exists and is related to the constants in these properties <ref> [32, section 1.3.3] </ref>. In the general case, a good value of is determined empirically, after experimentation and examination of the characteristics of the problem.
Reference: [33] <author> J.M. Mulvey and A. Ruszczynski. </author> <title> A diagonal quadratic approximation method for large scale linear programs. </title> <journal> Operations Research Letters, </journal> <volume> 12 </volume> <pages> 205-215, </pages> <year> 1992. </year>
Reference-contexts: Such updates, although computationally expensive, may yield faster convergence. When applied to linear block angular problems, the coarse grain ADI decomposition schemes require solving quadratic problems at each iteration. To overcome this computational drawback we may iteratively linearize the quadratic term, as done in <ref> [44, 33] </ref>, [15, section 2.5.1] for the method of multipliers, or replace it with a piecewise linear local approximation, as done in the convex optimization methods in [29, 28].
Reference: [34] <author> J.M. Mulvey and H. Vladimirou. </author> <title> Solving multistage stochastic networks: An application of scenario aggregation. </title> <journal> Networks, </journal> <volume> 21(6) </volume> <pages> 619-643, </pages> <year> 1990. </year>
Reference-contexts: In network flow problems, accelerated convergence has been observed with heuristics that vary the penalty for finitely many iterations, or even use a separate penalty for each linear constraint <ref> [34, 10, 31] </ref>. In this article we present convergence results for a general variable penalty algorithm, in which a symmetric positive definite (spd) matrix H t is employed in the Lagrangian and proximal term, to allow for linear transformations of the constraints.
Reference: [35] <author> L.M. Ostresh Jr. </author> <title> On the convergence of a class of iterative methods for solving the Weber location problem. </title> <journal> Operations Research, </journal> <volume> 26 </volume> <pages> 597-609, </pages> <year> 1978. </year>
Reference-contexts: Specialized algorithms for this problem are reviewed in <ref> [35] </ref>. <p> All runs were done on an IBM RS-6000/590 workstation using double precision arithmetic. We also solved the problems with the special-purpose Weiszfeld algorithm as emended in <ref> [35] </ref>, and compared results. The objective function values at termination agreed to 6 7 digits. 17 10 1000 iterations Value of fixed penalty Performance of fixed penalty ADI D = 6 K = 15, n = 4, for termination accuracy of D = 6 and D = 8 significant digits.
Reference: [36] <author> D.W. Peaceman and H.H. Rachford Jr. </author> <title> The numerical solution of parabolic and elliptic differential equations. </title> <journal> SIAM Journal, </journal> <volume> 3 </volume> <pages> 28-42, </pages> <year> 1955. </year>
Reference-contexts: Eckstein and Bertsekas [12] and Cheng and Teboulle [5] have constructed ADI variants which allow for inexact minimization. The former also permits relaxation of the primal iterates; in the latter, quadratic proximal terms replace the augmented Lagrangian penalty terms. In the Peaceman-Rachford variant [18], based on <ref> [36] </ref>, a multiplier update is interpolated between 2 the two problems. This algorithm requires more stringent assumptions for convergence and is less robust numerically [16]. Fukushima [17] presents an ADI method for the dual problem of (1).
Reference: [37] <author> B.T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <year> 1987. </year>
Reference-contexts: A characteristic of the extended algorithm is that, although 3 the primal iterates may not converge, they are feasible in the limit, and also the limit of the objective value is optimal. (This is in common with other algorithms, such as the subgradient method for nondifferentiable optimization in <ref> [37, section 5.3.2] </ref>.) We present an example illustrating this characteristic in section 3. In section 4 we provide a sequence of corollaries that mainly address primal convergence and finite termination.
Reference: [38] <author> M.J.D. Powell. </author> <title> A method for nonlinear constraints in minimization problems. </title> <editor> In R. Fletcher, editor, </editor> <booktitle> Optimization, </booktitle> <pages> pages 283-298. </pages> <publisher> Academic Press, </publisher> <year> 1969. </year>
Reference-contexts: From standard duality theory, if (x fl ; z fl ; p fl ) is a saddlepoint of the augmented Lagrangian, then (x fl ; z fl ) is a primal solution of (1) and p fl is an associated dual multiplier. In the method of multipliers <ref> [26, 38, 39, 2] </ref> a saddlepoint is located by an iterative process, consisting of a minimization of the augmented Lagrangian, followed by a steepest-ascent update of the multipliers. (x t+1 ; z t+1 ) 2 argmin x; z p t+1 = p t + r p L (x t+1 ; z
Reference: [39] <author> R.T. Rockafellar. </author> <title> The multiplier method of Hestenes and Powell applied to convex programming. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 12 </volume> <pages> 555-562, </pages> <year> 1973. </year>
Reference-contexts: From standard duality theory, if (x fl ; z fl ; p fl ) is a saddlepoint of the augmented Lagrangian, then (x fl ; z fl ) is a primal solution of (1) and p fl is an associated dual multiplier. In the method of multipliers <ref> [26, 38, 39, 2] </ref> a saddlepoint is located by an iterative process, consisting of a minimization of the augmented Lagrangian, followed by a steepest-ascent update of the multipliers. (x t+1 ; z t+1 ) 2 argmin x; z p t+1 = p t + r p L (x t+1 ; z
Reference: [40] <author> R.T. Rockafellar. </author> <title> Augmented lagrangians and applications of the proximal point algorithm in convex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 1 </volume> <pages> 97-116, </pages> <year> 1976. </year>
Reference-contexts: The connection to the proximal point algorithm is discussed in Rockafellar <ref> [40] </ref>. It has been shown [32] that the method is an instance of the Douglas-Rachford splitting [8] for finding a zero of a maximal monotone operator.
Reference: [41] <author> R.T. Rockafellar and R. J.-B. Wets. </author> <title> Scenarios and policy aggregation in optimization under uncertainty. </title> <journal> Mathematics of Operations Research, </journal> <volume> 16 </volume> <pages> 119-147, </pages> <year> 1991. </year>
Reference-contexts: Several other decomposition schemes, such as the algorithm of Han and Lou [24], Spingarn's method of partial inverses [42, 43], the progressive hedging algorithm of Rockafellar and Wets <ref> [41] </ref> and Golshtein's block method of convex programming [21, 22] are also instances of the Douglas-Rachford splitting (see [9] for a demonstration). Eckstein and Bertsekas [12] and Cheng and Teboulle [5] have constructed ADI variants which allow for inexact minimization. <p> this condition is used as a stopping rule in the example in section 3, the algorithm (for nonnegative start) terminates finitely at an optimal point. 5 An ADI decomposition scheme for block angular problems Several classes of models in applied optimization, including multicommodity network flow [1] and stochastic scenario analysis <ref> [41] </ref>, require solving convex block angular problems (CBA) of the following form: min f [1] (x [1] ) + f [2] (x [2] ) + : : : + f [K] (x [K] ) subject to A [1] x [1] = b [1] A [2] x [2] = b [2] A
Reference: [42] <author> J.E. Spingarn. </author> <title> Partial inverse of a monotone operator. </title> <journal> Applied Mathematics and Optimization, </journal> <volume> 10 </volume> <pages> 247-265, </pages> <year> 1983. </year>
Reference-contexts: It has been shown [32] that the method is an instance of the Douglas-Rachford splitting [8] for finding a zero of a maximal monotone operator. Several other decomposition schemes, such as the algorithm of Han and Lou [24], Spingarn's method of partial inverses <ref> [42, 43] </ref>, the progressive hedging algorithm of Rockafellar and Wets [41] and Golshtein's block method of convex programming [21, 22] are also instances of the Douglas-Rachford splitting (see [9] for a demonstration). Eckstein and Bertsekas [12] and Cheng and Teboulle [5] have constructed ADI variants which allow for inexact minimization. <p> Then, for any saddlepoint (x fl ; z fl ; p fl ), fff t g would be nonincreasing and Fejer-monotone, meaning that no step increases the distance to any solution point. This is also a property of the proximal point algorithm <ref> [42] </ref>. Lemma 2.6 Let the assumptions of theorem 2.1 hold. Then, (i) p t+1 p t ! 0 . converges to the optimal objective value for problem (1).
Reference: [43] <author> J.E. Spingarn. </author> <title> Applications of the method of partial inverses to convex programming: Decomposition. </title> <journal> Mathematical Programming, </journal> <volume> 32 </volume> <pages> 199-223, </pages> <year> 1985. </year> <month> 23 </month>
Reference-contexts: It has been shown [32] that the method is an instance of the Douglas-Rachford splitting [8] for finding a zero of a maximal monotone operator. Several other decomposition schemes, such as the algorithm of Han and Lou [24], Spingarn's method of partial inverses <ref> [42, 43] </ref>, the progressive hedging algorithm of Rockafellar and Wets [41] and Golshtein's block method of convex programming [21, 22] are also instances of the Douglas-Rachford splitting (see [9] for a demonstration). Eckstein and Bertsekas [12] and Cheng and Teboulle [5] have constructed ADI variants which allow for inexact minimization.
Reference: [44] <author> G. Stephanopoulos and A.W. Westerberg. </author> <title> The use of Hestenes' method of multipliers to resolve dual gaps in engineering system optimization. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 15 </volume> <pages> 285-309, </pages> <year> 1975. </year>
Reference-contexts: Such updates, although computationally expensive, may yield faster convergence. When applied to linear block angular problems, the coarse grain ADI decomposition schemes require solving quadratic problems at each iteration. To overcome this computational drawback we may iteratively linearize the quadratic term, as done in <ref> [44, 33] </ref>, [15, section 2.5.1] for the method of multipliers, or replace it with a piecewise linear local approximation, as done in the convex optimization methods in [29, 28].
Reference: [45] <author> P. Tseng and D.P. Bertsekas. </author> <title> On the convergence of the exponential multiplier method for convex programming. </title> <journal> Mathematical Programming, </journal> <volume> 60 </volume> <pages> 1-19, </pages> <year> 1993. </year> <month> 24 </month>
Reference-contexts: On the theoretical level, an open issue is whether the convergence properties are preserved if the quadratic penalty is replaced by other penalty functions, such as Bregman's <ref> [45, 11] </ref>, or the class of strongly convex functions employed in the auxiliary problem method [7].
References-found: 45

