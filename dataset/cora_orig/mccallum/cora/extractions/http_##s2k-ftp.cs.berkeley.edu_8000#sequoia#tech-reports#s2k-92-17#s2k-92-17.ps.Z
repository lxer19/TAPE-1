URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-17/s2k-92-17.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-17/
Root-URL: http://www.cs.berkeley.edu
Email: (kfall@ucsd.edu)  (pasquale@ucsd.edu)  
Title: Exploiting In-Kernel Data Paths to Improve I/O Throughput and CPU Availability  
Author: Kevin Fall Joseph Pasquale 
Note: Nov, 1992  
Address: La Jolla, CA 92093-0114  
Affiliation: Computer Systems Laboratory Department of Computer Science and Engineering University of California, San Diego  
Abstract: We present the motivation, design, implementation, and performance evaluation of a UNIX kernel mechanism capable of establishing fast in-kernel data pathways between I/O objects. A new system call, splice() moves data asynchronously and without user-process intervention to and from I/O objects specified by file descriptors. Performance measurements indicate improved I/O throughput and increased CPU availability attributable to reduce context switch and data copying overhead. 
Abstract-found: 1
Intro-found: 1
Reference: [Cor90] <author> D. E. Corporation, </author> <title> ``DECStation 5000/200 KN02 System Module Functional Specification, Rev 1.3'', Workstation Systems Engineering, </title> <address> Palo Alto, CA, </address> <month> Aug, </month> <year> 1990. </year>
Reference-contexts: The DecStation 5000/200 MIPS R3000 processor is clocked at 25 Mhz and includes a 64 KByte instruction and 64KByte write-through data cache. Cached memory read throughput is 21 MB/s, uncached CPU read rate is 10 MB/s, and partial-page write throughput is 20 MB/s <ref> [Cor90] </ref>. We used Digital's RZ56 and RZ58 SCSI disks for performance measurements. The RZ56 provides an average rotational latency of 8.3 ms, average seek time of 16 ms, and a to/from media peak data transfer rate of 1.66 MB/s.
Reference: [Cus93] <author> H. Custer, </author> <title> Inside Windows NT, </title> <publisher> Microsoft Press, </publisher> <address> Redmond, WA, </address> <year> 1993. </year>
Reference-contexts: Consequently, the kernel can decide how to best transfer data between devices, given its knowledge of the source and destination transfer characteristics and device interfaces. Second, the call operates asynchronously in a fashion similar to the asynchronous I/O calls present in several current versions of UNIX, and Windows NT <ref> [Cus93] </ref>. A calling process may continue user-mode execution while I/O is proceeding between objects. The process may regain control of the splice execution periodically by carefully adjusting the transfer size parameter (described below), but does not need to create threads or call specialized ``async I/O'' functions as several systems require.
Reference: [DeA92] <author> R. W. Dean and F. </author> <title> Armand, ``Data Movement in Kernelized Systems'', </title> <booktitle> USENIX Workshop on Micro-kernels and Other Kernel Architectures, </booktitle> <address> Seattle, WA, </address> <month> Apr, </month> <year> 1992, </year> <pages> 243-261. </pages>
Reference-contexts: We plan to investigate these areas, as well as the performance of our SCSI device driver, with the expectation of higher performance. 7. Related Work The work described here relates to the general problems of system overhead encountered with large throughput I/O loading. Dean and Armand <ref> [DeA92] </ref> explore the effect of microkernel-based operating system design on data movement performance, suggesting the desirability of including device manipulation code directly in user processes to avoid copying. Similar suggestions are made by Forin et. al in [FGB91], who suggest the mapping of device registers directly to user-level processes.
Reference: [FGB91] <author> A. Forin, D. Golub and B. Bershad, </author> <title> ``An I/O System for Mach 3.0'', </title> <booktitle> Proc. Usenix Mach Symposium, </booktitle> <address> Monterey, CA, </address> <month> Nov, </month> <year> 1991, </year> <pages> 163-176. </pages>
Reference-contexts: Dean and Armand [DeA92] explore the effect of microkernel-based operating system design on data movement performance, suggesting the desirability of including device manipulation code directly in user processes to avoid copying. Similar suggestions are made by Forin et. al in <ref> [FGB91] </ref>, who suggest the mapping of device registers directly to user-level processes. Govindan and Anderson [GoA91] describe ``memory-mapped streams'' as a mechanism for moving continuous media data between address spaces using shared memory.
Reference: [GoA91] <author> R. Govindan and D. P. Anderson, </author> <title> ``Scheduling and IPC Mechanisms for Continuous Media'', </title> <booktitle> Proc. 13th Symp. on Operating System Prin., </booktitle> <address> Pacific Grove, CA, </address> <month> Oct, </month> <year> 1991, </year> <pages> 68-80. </pages>
Reference-contexts: Similar suggestions are made by Forin et. al in [FGB91], who suggest the mapping of device registers directly to user-level processes. Govindan and Anderson <ref> [GoA91] </ref> describe ``memory-mapped streams'' as a mechanism for moving continuous media data between address spaces using shared memory. These approaches require the interface of I/O devices to appear as memory objects, and must therefore be mappable to a process' address space.
Reference: [LMK89] <author> S. J. Leffler, M. K. McKusick, M. J. Karels and J. S. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: This paper assumes basic familiarity with these functions. They are discussed in more detail in <ref> [LMK89] </ref>. 5.2. Implementation and Operational Details Assuming an entire file is to be copied, splice operates generally as follows. First the size of the source file is determined from information present in the gnode (Ultrix terminology for generic filesystem node).
Reference: [PCM91] <author> M. Pasieka, P. Crumley, A. Marks and A. Infortuna, </author> <title> ``Distributed Multimedia: How Can the Necessary Data Rates be Supported?'', </title> <booktitle> Proc. Usenix Summer Conference, </booktitle> <address> Nashville, TN, </address> <month> June, </month> <year> 1991, </year> <pages> 169-182. </pages>
Reference-contexts: Several architectures restrict the ability to map devices, especially to user address space. Furthermore, we believe the data transfer size granularity should be specified by the application, rather than being constrained by details of the VM hardware. In an approach similar to ours, Pasieka et. al <ref> [PCM91] </ref> suggest the UNIX ioctl be used to pass handles between source and destination devices, referring to kernel-level data objects. Their scheme decouples data movement from the application but requires user process execution to effect a data transfer between devices. 8.
Reference: [Pas92] <author> J. Pasquale, </author> <title> ``I/O System Design for Intensive Multimedia I/O'', </title> <booktitle> Proc. IEEE Workshop on Workstation Operating Systems, </booktitle> <address> Key Biscayne, FL, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Concepts useful for improving I/O system performance for these applications include minimization of data movement within memory, and separating I/O control from I/O data transfer <ref> [Pas92] </ref>. With UNIX being the primary operating system available for most scientific and high performance computing platforms today, evaluation and improvement of UNIX system performance when exposed to I/O intensive workloads is important to ensure a viable future execution environment.
Reference: [PrR85] <author> D. Presotto and D. Ritchie, </author> <title> ``Interprocess Communication in the 8th Edition Unix System'', </title> <booktitle> Proc. Usenix Winter Conference, </booktitle> <month> Dec </month> <year> 1985, </year> <pages> 309-316. </pages>
Reference-contexts: The mechanism we present focuses on improving performance for I/O intensive applications performing no direct manipulation of the transferred data. However, the mechanism does not preclude the use of ``stacked'' kernel processing modules to perform common functions required by applications, in a fashion similar to Streams <ref> [PrR85] </ref>. Processing modules may be implemented in hardware where appropriate; for example, compression/decompression. Today's workstation technology is not capable of providing the high-bandwidth I/O demanded by applications like HDTV-quality video or even NTSC-quality video, when moved about uncompressed and processed by the CPU in real time. <p> An old-style telephone operator ``patching together'' two communicating parties is an appropriate analogy for the operation of splice. Splice may also be thought of as providing the ``reverse'' capability of the original Streams IPC pseudoterminal (PT) [Rit84] or the streams-based pipe implementation in 8th Edition UNIX <ref> [PrR85] </ref>. The PT in Ritchie's streams and pipes in 8th Edition provide IPC by cross-connecting file descriptors within the kernel. Splice, in contrast, provides the cross-connection of devices within the kernel as specified by the calling process. 3.
Reference: [Rit84] <author> D. M. Ritchie, </author> <title> ``A Stream Input-Output System'', </title> <journal> ATT Bell Laboratories Technical Journal 63, </journal> <month> 8 (October </month> <year> 1984), </year> <pages> 1897-1910. </pages>
Reference-contexts: An old-style telephone operator ``patching together'' two communicating parties is an appropriate analogy for the operation of splice. Splice may also be thought of as providing the ``reverse'' capability of the original Streams IPC pseudoterminal (PT) <ref> [Rit84] </ref> or the streams-based pipe implementation in 8th Edition UNIX [PrR85]. The PT in Ritchie's streams and pipes in 8th Edition provide IPC by cross-connecting file descriptors within the kernel. Splice, in contrast, provides the cross-connection of devices within the kernel as specified by the calling process. 3.
Reference: [DEC92] <author> Digital Equipment Corporation, </author> <title> ``Information and Configuration Guide for Digital's Desktop Storage: Focus on the RZ Series of SCSI Disk Drives'', Disk and Subsystems Group, </title> <address> Palo Alto, CA, </address> <month> Feb, </month> <year> 1992. </year> <month> 10 </month>
Reference-contexts: The RZ56 provides 64 KB of read-ahead cache, and the RZ58 provides 256 KB of read-ahead cache segmented into 4 read-ahead requests <ref> [DEC92] </ref>. The performance improvement of splice is most pronounced when applied to devices producing or consuming data at high rates relative to the CPU execution rate. To determine how splice would perform when using fast devices, we also implemented a RAM disk.
References-found: 11

