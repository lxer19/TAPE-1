URL: http://www.cs.msu.edu/~huangyih/Publications/IC3N96.ps
Refering-URL: http://www.cs.msu.edu/~huangyih/Publications/index.html
Root-URL: http://www.cs.msu.edu
Email: ftsaiyi, huangyih, mckinley,g@cps.msu.edu  
Title: Performance Evaluation of Barrier Synchronization in ATM Networks  
Author: Y.-j. Tsai, Y. Huang, and P. K. McKinley 
Address: East Lansing, Michigan 48824  
Affiliation: Department of Computer Science Michigan State University  
Abstract: In this paper, we apply a new methodology to the design of barrier synchronization in ATM networks. To improve performance, a small set of core subopera-tions is moved to the device driver or to the network adapter firmware. Three different barrier algorithms are studied. The results of analysis and simulation indicate which algorithms are best suited for the proposed methods and show that implementations using this approach outperform conventional implementations by a wide margin. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Message Passing Interface Forum, </author> <title> "MPI: A message-passing interface standard," </title> <type> tech. rep., </type> <institution> Department of Computer Science, University of Tennessee, Knoxville, Tennessee, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Besides traditional multicast operations, in which data is transmitted from one sender to multiple destinations, such applications often require the more general class of collective operations, which include data movement operations, global operations on distributed data, and barrier synchronization <ref> [1] </ref>. Since collective operations tend to synchronize the participating processes, their efficiency is critical to the performance of the distributed application. However, the increases in raw network bandwidth that have made possible high-performance distributed computing, have also revealed bottlenecks in communication subsystems of workstations and PCs.
Reference: [2] <author> E. Cooper, P. Steenkiste, R. Sansom, and B. Zill, </author> <title> "Protocol implementation on the Nectar communication processor.," </title> <booktitle> in ACM SIGCOMM'90 Symposium, (Philadelphia), </booktitle> <pages> pp. 135-144, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The problem of reducing system overhead has been addressed in many ways, including improved buffer management, restructured OS kernels, and streamlining of the protocol stack. Another solution considered previously is to move the protocol stack (wholly or partially) to the network adapter <ref> [2] </ref>. The intent is to off-load work from the host processor, reduce the rate of interrupts to the host processor, and improve system throughput. Such a downward migration of functionality is not without pitfalls.
Reference: [3] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels, "U-Net: </author> <title> A user-level network interface for parallel and distributed computing," </title> <booktitle> in Proc. of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <address> (Copper Mountain, Colorado), </address> <pages> pp. 40-53, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Such a downward migration of functionality is not without pitfalls. Since the host processor is typically much more powerful than the adapter processor, the potential advantage of off-loading can be diminished by protracted processing on the adapter <ref> [3] </ref>. However, our results show that there are special cases, namely, collective operations, in which significant performance gain can be achieved when certain high-level commu-nication functions are supported in the network interface (NI) [4]. <p> The per switch latency for a cell is assumed to be 12 sec [13]. The performance of the AAL-level implementation depends on the efficiency of segmentation and reassembly. The values published in <ref> [3] </ref> showed that the round-trip time of a single-cell AAL-5 packet is approximately 65 sec, using Sun SPARCstation-20 workstations, Fore SBA-200 adapters, and the Fore ASX-200 switch. To approximate the user-level overhead per cell, we use the UDP overhead for very short messages, which was reported in [14]. <p> The two NI implementations achieve better performance than does the user-level implementation. Moreover, since even the NI implementation involves one round of transport-to-NI overhead, which is approximately 300 sec, the use of modern light-weight message transportation systems, such as U-net <ref> [3] </ref>, should be able to further lower the latencies of the NI-supported implementations. The performance of ring-based implementations are plotted in Figure 10. Not surprisingly, ring-based implementations incur execution times proportional to the group size. However, NI-supported implementations dramatically reduce the growth rate of execution time.
Reference: [4] <author> Y. Huang and P. K. McKinley, </author> <title> "Efficient collective operations with ATM network interface support," </title> <booktitle> in Proceedings of the 1996 International Conference on Parallel Processing, </booktitle> <address> (Bloomingdale, Illinois), </address> <month> August </month> <year> 1996. </year> <note> accepted to appear. </note>
Reference-contexts: However, our results show that there are special cases, namely, collective operations, in which significant performance gain can be achieved when certain high-level commu-nication functions are supported in the network interface (NI) <ref> [4] </ref>. Instead of placing entire collective operations in the NI, our methodology identifies a set of high-priority tasks in collective operations, moves these tasks to the NI, and leaves more complex but less time-critical functions to higher-level software. <p> When the token returns to the origin, the entire group has reached the barrier and a release token is then circulated along the ring, the receipt of which signals the completion of the barrier operation to each node. 3 Methodology In <ref> [4] </ref>, we proposed an NI-support methodology that seeks to avoid the pitfalls of NI support for high-level functions while ensuring the feasibility of low-level implementation. In short, the methodology restricts the functions supported at the NI to a small number of arithmetic operations and logical tests per message.
Reference: [5] <author> T. S. Axelrod, </author> <title> "Effects of synchronization barriers on multiprocessor performance," </title> <journal> Parallel Computing, </journal> <volume> vol. 3, </volume> <pages> pp. 129-140, </pages> <year> 1986. </year>
Reference-contexts: A synchronization barrier is a logical point in the control flow of an algorithm at which all the members of a subset of the processes must arrive before any of the processes in the subset may proceed further <ref> [5] </ref>. Different algorithms may be used to implement barrier synchronization. Our results indicate which algorithms are best suited for the proposed methods and demonstrate that implementations using this approach outperform conventional implementations by a wide margin. The remainder of this paper is organized as follows.
Reference: [6] <author> Y. Huang, C. Huang, and P. K. McKinley, </author> <title> "Multi-cast virtual topologies for collective communication in MPCs and ATM clusters," </title> <booktitle> in Proceedings of Supercomputing'95, </booktitle> <address> (San Diego, California), </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Second, the connection-oriented nature of ATM supports the use of virtual topologies, in which a set of pre-established connections is used repeatedly during the lifetime of the application <ref> [6] </ref>. Third, the ATM User Network Interface (UNI) standard 3.1 [7] supports VCs with multiple destinations, or multicast VCs. Cells transmitted on multicast VCs are replicated as needed by network switches, resulting in a tree-like connection from the source to the destinations. <p> Cells transmitted on multicast VCs are replicated as needed by network switches, resulting in a tree-like connection from the source to the destinations. Since many collective operations include data distribution phases, hardware supported multicast can greatly improve performance <ref> [6, 8] </ref>. Barrier Synchronization. Barrier synchronization occurs frequently in parallel and distributed programs, and has been studied extensively in the past for multiprocessors. The increasing use of workstation clusters for parallel computing has generated interest in fast barrier operations for distributed environments.
Reference: [7] <author> ATM Forum, </author> <title> ATM User-Network Interface (UNI) Specification Version 3.1. </title> <publisher> Prentice Hall, </publisher> <month> September </month> <year> 1994. </year>
Reference-contexts: Second, the connection-oriented nature of ATM supports the use of virtual topologies, in which a set of pre-established connections is used repeatedly during the lifetime of the application [6]. Third, the ATM User Network Interface (UNI) standard 3.1 <ref> [7] </ref> supports VCs with multiple destinations, or multicast VCs. Cells transmitted on multicast VCs are replicated as needed by network switches, resulting in a tree-like connection from the source to the destinations. Since many collective operations include data distribution phases, hardware supported multicast can greatly improve performance [6, 8].
Reference: [8] <author> C. C. Huang and P. K. McKinley, </author> <title> "Communication issues in parallel computing across ATM networks," </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 73-86, </pages> <year> 1994. </year>
Reference-contexts: Cells transmitted on multicast VCs are replicated as needed by network switches, resulting in a tree-like connection from the source to the destinations. Since many collective operations include data distribution phases, hardware supported multicast can greatly improve performance <ref> [6, 8] </ref>. Barrier Synchronization. Barrier synchronization occurs frequently in parallel and distributed programs, and has been studied extensively in the past for multiprocessors. The increasing use of workstation clusters for parallel computing has generated interest in fast barrier operations for distributed environments.
Reference: [9] <author> D. Johnson, D. Lilja, and J. Riedl, </author> <title> "A circulating active barrier synchronization mechanism," </title> <booktitle> in Proceedings of the 1995 International Conference on Parallel Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 202-209, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The increasing use of workstation clusters for parallel computing has generated interest in fast barrier operations for distributed environments. Besides a number of MPI implementations for clusters that provide software barrier algorithms, some hardware solutions have been proposed. For example, Johnson et al. <ref> [9] </ref> recently proposed a circulating active barrier, which uses a separate conductor connecting a set of workstations in a ring, and a cluster controller to generate and manage barrier packets.
Reference: [10] <author> Y. Tsai, Y. Huang, and P. K. McKinley, </author> <title> "Performance evaluation of barrier synchronization on ATM networks," </title> <type> Tech. Rep. </type> <institution> MSU-CPS-96-24, Department of Computer Science, Michigan State University, East Lansing, Michigan, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: Using this approach, a reliable multicast layer in the protocol stack is not necessary. Star-based Barrier. Due to its similarity to the tree based approach, the details of star-based NI support are omitted here, but can be found in <ref> [10] </ref>. Ring-Based Barrier. The responsibility of the user process to support a ring-based barrier is iden Upon receiving a barrier token D, DO IF (D is received from local process) IF (old val D:val) /* This is a retransmission. */ IF (I am the root) Broadcast D. <p> When the barrier operation is implemented at user level, we assume that barrier tokens are transmitted using UDP/IP over ATM. The resultant value of 308 sec is used for user-level overhead. The analytical results were closely matched by the simulation results <ref> [10] </ref>; due to space limitations, only a subset of the simulation results are presented here. To take into account the effect of network topology, every simulation session was repeated over 20 randomly generated graphs, in which each vertex represents an ATM switch.
Reference: [11] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Shauser, E. Santos, R. Subramonian, and T. von Eicken, </author> <title> "LogP: Towards a realistic model of parallel computation," </title> <booktitle> in Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOPP), </booktitle> <address> (San Diego, </address> <publisher> Califor-nia), </publisher> <pages> pp. 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: ENDIF ENDDO 5 Performance Comparisons We studied the performance of the three barrier algorithms through both analysis and simulation. Three levels of implementation are considered: ATM cell level, AAL level, and user-space. For the analysis, we used a modified version of the LogP model <ref> [11] </ref>. The LogP model assumes a fully-connected topology with network latency L, a per message overhead o involved in each send and receive action, and a gap g between two consecutive sends or two consecutive receives.
Reference: [12] <author> H. D. Schwetman, "Csim: </author> <title> A C-based, process-oriented simulation language," </title> <type> Tech. Rep. </type> <institution> PP-080-85, Microelectronics and Computer Technology Corporation, </institution> <year> 1985. </year>
Reference-contexts: We have defined another parameter, , to reflect the cost of computation performed by a node on a received message. The simulator is based on CSIM <ref> [12] </ref>, an event driven simulation package. The simulations take into account the effects of topology, serialization problems, and the asynchronous nature of barrier commits by each process, which are not reflected in the analysis. For both the analysis and simulation, we instantiated the parameters for specific ATM network environments.
Reference: [13] <author> R. Mandeville, </author> <title> "The ATM stress test," </title> <journal> Data Communications, </journal> <pages> pp. 69-82, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: We also assume that the full bandwidth can be achieved, and thus we set the gap g equal to 3 sec. The per switch latency for a cell is assumed to be 12 sec <ref> [13] </ref>. The performance of the AAL-level implementation depends on the efficiency of segmentation and reassembly. The values published in [3] showed that the round-trip time of a single-cell AAL-5 packet is approximately 65 sec, using Sun SPARCstation-20 workstations, Fore SBA-200 adapters, and the Fore ASX-200 switch.
Reference: [14] <author> K. K. Keeton, T. E. Anderson, and D. A. Patterson, </author> <title> "LogP quantified: The case of low-overhead local area networks," in presented at Hot Interconnects III: A Symposium on High Performance Interconnects, </title> <month> August </month> <year> 1995. </year>
Reference-contexts: To approximate the user-level overhead per cell, we use the UDP overhead for very short messages, which was reported in <ref> [14] </ref>. The reason for using figures of UDP instead of TCP is that the proposed methods handle reliability at the user level, rather than at the transport layer. When the barrier operation is implemented at user level, we assume that barrier tokens are transmitted using UDP/IP over ATM.
References-found: 14

