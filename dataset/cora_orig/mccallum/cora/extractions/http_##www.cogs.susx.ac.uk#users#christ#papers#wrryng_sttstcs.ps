URL: http://www.cogs.susx.ac.uk/users/christ/papers/wrryng_sttstcs.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Title: The Worrying Statistics of Connectionist Representation  
Author: Chris Thornton 
Address: Brighton BN1 9QN  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Pubnum: CSRP  
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)273 678856  
Date: December 14, 1994  362  
Abstract: The paper looks at how the hidden-vector cluster analyses associated with Elman and others seemed to provide a potentially important link between the symbolically-oriented level of analysis and the connectionist level of analysis | a link that might one day help to explain how higher mental processes are grounded in neural architectures. The paper goes on to reconsider the implications of these analyses in light of some recent work by Finch and Chater which shows that linguistically meaningful categories (of the type derived from hidden-vector analyses) are directly evidenced in the N-gram statistics of natural language. The implication of this work seems to be that hidden-vector analyses do not primarily address the link between the symbolic and connectionist levels of explanation but rather tell us something about the statistical properties of the training environments used. The consequences of this result for cognitive science are lightly sketched in. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Newell, A. and Simon, H. </author> <year> (1963). </year> <title> The logic theory machine. </title> <editor> In .E. Feigenbaum, Feldman and J. (Eds.), </editor> <booktitle> Computers and Thought. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: [2] <author> Roberts, L. </author> <year> (1965). </year> <title> Machine perception of three-dimensional solids. </title> <editor> In J. Tippett, D. Berkowitz, L. Clapp, C. Keoster and A. Vanderburgh (Eds.), </editor> <booktitle> Optical and Electro-Optical Information Processing. </booktitle> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher> <pages> 6 </pages>
Reference-contexts: The effect was, perhaps, particularly noticeable in the area of vision research. Vision researchers of the 1960s, e.g., Roberts <ref> [2] </ref>, were largely concerned with systems which sorted out neat, perspective drawings using rules whose good sense could easily be comprehended. cf. Waltz filtering [3].
Reference: [3] <author> Waltz, D. </author> <year> (1975). </year> <title> Understanding line drawings of scenes with shadows. </title> <editor> In P. Winston (Ed.), </editor> <booktitle> The Psychology of Computer Vision (pp. </booktitle> <pages> 19-92). </pages> <publisher> Mcgraw-Hill. </publisher>
Reference-contexts: The effect was, perhaps, particularly noticeable in the area of vision research. Vision researchers of the 1960s, e.g., Roberts [2], were largely concerned with systems which sorted out neat, perspective drawings using rules whose good sense could easily be comprehended. cf. Waltz filtering <ref> [3] </ref>.
Reference: [4] <author> Scott, G. </author> <year> (1988). </year> <title> Local and Global Interpretation of Moving Images. </title> <booktitle> Research Notes In Artificial Intelligence, </booktitle> <address> London: </address> <publisher> Pitman. </publisher>
Reference-contexts: Waltz filtering [3]. But by the late 1970s researchers had begun to work with systems which dealt primarily in abstruse mathematical constructs having nothing obvious to do with the mind or the real world, cf. <ref> [4] </ref>. 1 It was perhaps inevitable that there would be some kind of reaction to AI's gradual descent into high technology. And when it came, it came in a rush with the most obvious manifestation being John Searle's vigorous attack on `strong AI'.
Reference: [5] <author> Thornton, C. </author> <year> (1985). </year> <title> A response to searle's thesis. </title> <journal> AISB Quarterly, </journal> <volume> No. </volume> <pages> 52 (pp. 32-33). </pages>
Reference: [6] <author> Sloman, A. </author> <year> (1985). </year> <title> Strong strong and weak strong AI. </title> <journal> AISB Quarterly, </journal> <volume> No. </volume> <pages> 52 (pp. 26-31). </pages>
Reference-contexts: Whatever Searle demonstrated about the intentional properties of AI systems he did stimulate a much more thorough scrutiny of the functionalist assumptions underlying strong (and not so strong) AI. This scrutiny was, as Sloman pointed out at the time <ref> [6] </ref>, urgently needed. Functionalism is essentially the idea that intentional states have nothing to do with architectural substrates. Being a functionalist implies believing that all that is required for intentional states is the right program.
Reference: [7] <author> Dennett, D. </author> <year> (1987). </year> <title> The Intentional Stance. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: It suggests that the important thing is to capture the phenomenon at a computational level of description. Later on the algorithmic and implementation details can be worked out. Functionalism effectively urges a top-down program of research. The image is that of a `triumphant cascade through Marr's three levels.' <ref> [7, p. 227] </ref> But the validity of the functionalist stance is open to question. And, of course, it is not just a question of computer simulations of mental processes: the issue is much more general than that. It has to do with the validity of simulations in general.
Reference: [8] <author> Brooks, R. </author> <year> (1991). </year> <title> Intelligence without representation. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 47 (pp. 139-159). </pages>
Reference-contexts: Certainly, in the years following the attack new approaches began to emerge which were centrally concerned with issues of architecture and environment. Examples include the develop of geneticism (genetic algorithms, classifier systems etc.), the development of the reactive systems movement stemming from Brooks work on robot creatures <ref> [8; 9] </ref> and finally, of course, connectionism, a paradigm whose stated aim was to take the low-level architecture of the brain seriously [10].
Reference: [9] <author> Brooks, R. </author> <year> (1991). </year> <title> Intelligence without reason. </title> <booktitle> Proceedings of the Twelth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 569-595). </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Certainly, in the years following the attack new approaches began to emerge which were centrally concerned with issues of architecture and environment. Examples include the develop of geneticism (genetic algorithms, classifier systems etc.), the development of the reactive systems movement stemming from Brooks work on robot creatures <ref> [8; 9] </ref> and finally, of course, connectionism, a paradigm whose stated aim was to take the low-level architecture of the brain seriously [10].
Reference: [10] <author> Hinton, G. and Anderson, J. (Eds.) </author> <year> (1981). </year> <title> Parallel Models of Associative Memory. </title> <address> Hillsdale, N.J.: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Examples include the develop of geneticism (genetic algorithms, classifier systems etc.), the development of the reactive systems movement stemming from Brooks work on robot creatures [8; 9] and finally, of course, connectionism, a paradigm whose stated aim was to take the low-level architecture of the brain seriously <ref> [10] </ref>. The emergence of these new approaches seems, in retrospect, to have been something of a mixed blessing for those on the philosophical side of the fence who, like Searle, believed AI to have over-extended the functionalist position.
Reference: [11] <author> Sejnowski, T. and Rosenberg, C. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce english text. </title> <journal> Complex Systems, </journal> <pages> 1 (pp. 145-68). </pages>
Reference-contexts: The first widely-published usage of these techniques was in Sejnowski and Rosenberg's <ref> [11] </ref> work on the NETtalk system. These two researchers showed how a cluster analysis of the hidden-vectors of a backpropagation network (trained to convert text to speech) showed up linguistically meaningful groupings.
Reference: [12] <author> Elman, J. </author> <year> (1989). </year> <title> Representation and structure in connectionist models. </title> <type> CRL Technical Report 8903, </type> <institution> San Diego: Center for Research in Language (UCLA). </institution>
Reference-contexts: Recently, it has been used to particularly good effect by Elman who showed how a copy-back network trained to do word-prediction (given only a diet of raw English sentences), formed an internal hierarchy that captured lexical and semantic categories <ref> [12] </ref>. For anyone dreaming wistfully of a bottom-up, `reverse-cascade', this new work by Elman and others looked very promising. The notion that the behaviours of connectionist systems embodied tacit rules was fairly well accepted especially in light of Rumelhart and McClelland's work on the learning of past tenses [13]. <p> The main gist of their results is that the `linguistically meaningful' hierarchical structures which can be obtained by clustering the hidden-vectors of, say, an Elman-style, copy-back network trained to do word-prediction <ref> [12] </ref>, can also be obtained by a fairly straightforward statistical anylysis of a large corpus of English sentences. Finch and Chater have carried out a whole range of experiments using a large corpus of text derived from electronic `news' discussion groups.
Reference: [13] <author> Rumelhart, D. and McClelland, J. </author> <year> (1986). </year> <title> On learning the past tenses of english verbs. </title> <editor> In D. Rumelhart, J. McClelland and the PDP Research Group (Eds.), </editor> <title> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. Vols I and II. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: For anyone dreaming wistfully of a bottom-up, `reverse-cascade', this new work by Elman and others looked very promising. The notion that the behaviours of connectionist systems embodied tacit rules was fairly well accepted especially in light of Rumelhart and McClelland's work on the learning of past tenses <ref> [13] </ref>. But with the new hidden-vector analyses one could now say much more precisely what form the terms of these rules might take. In effect, the hidden-vector analyses provided an initial step-up on the reverse cascade.
Reference: [14] <author> Clark, A. </author> <year> (1990). </year> <title> Connectionism, competence, and explanation. In M.A. </title> <editor> Boden (Ed.), </editor> <booktitle> The Philosophy of Artificial Intelligence. </booktitle> <publisher> Oxford: Oxford University Press. </publisher>
Reference-contexts: Andy Clark was quick to see the potential of this new method. In discussing the implications of hidden-vector analyses, he suggested that `a fully interpreted cluster-analysis . . . constitutes the nearest connectionist analogue to a classical competence theory.' <ref> [14] </ref> Of course, by this date, cluster analysis had only managed to `reify' fairly primitive types of class definition (eg. lexical categories) but it was easy to imagine how it might be possible to build one cluster-analysis on top of another and perhaps produce in the end a chain of connections
Reference: [15] <author> Finch, S. and Chater, N. </author> <year> (1991). </year> <title> A hybrid approach to the automatic learning of linguistic cateogories. </title> <editor> In S. Torrance (Ed.), AISB Quarterly, </editor> <volume> No. </volume> <pages> 78 (pp. 16-24). </pages>
Reference-contexts: It would certainly satisfy those emphasising the need for firm grounding. It would also satisfy those wanting to know how higher-level mental objects such as rules and concepts correspond to lower-level neural processes. 4 Finch, Chater and the statistics of English Unfortunately, some recent work by Finch and Chater <ref> [15] </ref> seems to suggest that such a cascade of connections | were it ever to be derived | might not tell us any more than we could have found out using GOFSA | good, old-fashioned statistical analysis. <p> Finch and Chater have carried out a whole range of experiments using a large corpus of text derived from electronic `news' discussion groups. They tried various approaches and the details are described in their various papers <ref> [15, 16; 17] </ref>. The method they used involved sampling N-gram statistics and then using cluster analysis to discover groupings of words with similar probability distributions. The analysis produced groupings and structures which have a very close correspondence to known syntactic and semantic categories. <p> As they note, `an N-gram is an ordered sequence of N symbols. The frequencies of occurrence of each N-gram in a continuous stream of data constitutes the N-gram statistics of the data set.' <ref> [15] </ref>. Their aim was to look at the number of times that particular words were observed to appear as the last-but-one, last-but-two, next-but-one and next-but-two neighbours of every other, commonly occurring word. As we have seen, this approach was able to show up important class divisions.
Reference: [16] <author> Finch, S. and Chater, N. </author> <title> (forthcoming). Bootstrapping Syntactic Categories Using Statistical Methods. </title>
Reference-contexts: Finch and Chater have carried out a whole range of experiments using a large corpus of text derived from electronic `news' discussion groups. They tried various approaches and the details are described in their various papers <ref> [15, 16; 17] </ref>. The method they used involved sampling N-gram statistics and then using cluster analysis to discover groupings of words with similar probability distributions. The analysis produced groupings and structures which have a very close correspondence to known syntactic and semantic categories.
Reference: [17] <author> Chater, N. and Conkey, P. </author> <title> (forthcoming). Finding Linguistic Structure with Recurrent Neural Networks. </title>
Reference-contexts: Finch and Chater have carried out a whole range of experiments using a large corpus of text derived from electronic `news' discussion groups. They tried various approaches and the details are described in their various papers <ref> [15, 16; 17] </ref>. The method they used involved sampling N-gram statistics and then using cluster analysis to discover groupings of words with similar probability distributions. The analysis produced groupings and structures which have a very close correspondence to known syntactic and semantic categories. <p> They conclude that their statistical work shows that the `copy-back scheme is sampling these [N-gram] statistics successfully.' The go on to say that `these results suggest that the hidden unit patterns that recurrent neural networks develop can be viewed as reflecting quite directly the statistical structure of the sequences learnt.' <ref> [17] </ref> By showing that the internal structures formed by copy-back word-prediction networks closely resemble the structures derived from a particular statistical analysis, they have effectively shown that the networks are sampling the relevant statistic. In a sense, they have provided a type-1 theory [18] for the behaviour of these networks.
Reference: [18] <author> Marr, D. </author> <year> (1977). </year> <title> Artificial intelligence: a personal view. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 9 (pp. 37-48). 7 </pages>
Reference-contexts: In a sense, they have provided a type-1 theory <ref> [18] </ref> for the behaviour of these networks. The theory says that the network is performing a particular computation and it characterizes this computation without making any reference to implementation issues.
References-found: 18

