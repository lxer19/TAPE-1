URL: http://www-white.media.mit.edu/~popat/smthesis.ps.Z
Refering-URL: http://www-white.media.mit.edu/people/popat/smthesis.html
Root-URL: http://www.media.mit.edu
Title: SCALAR QUANTIZATION WITH ARITHMETIC CODING  
Author: by Ashok Chhabedia Popat David H. Staelin Arthur C. Smith 
Degree: Submitted in Partial fulfillment of the Requirements of the Degree of MASTER OF SCIENCE in Electrical Engineering and Computer Science at the  All rights reserved Signature of Author  Certified by  Professor of Electrical Engineering Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: (1986)  June 1990  1990  March 28, 1990  
Affiliation: S.B. Massachusetts Institute of Technology  Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Shannon, C.E., and Weaver, W., </author> <title> The Mathematical Theory of Communication. </title> <institution> Urbana: University of Illinois Press, </institution> <year> 1963. </year>
Reference-contexts: The roots of arithmetic coding can be traced to the original work of Shannon <ref> [1] </ref>, although the technique was made practical only about a decade ago [16]. For an introduction to quantization and its history, the reader is referred to Ger 14 sho [17]. An extremely simple treatment of the basic concepts of quantization appears in [18, Chapter 2]. <p> In a later paper they investigate an implementation of entropy-constrained quantization [11]. Practical arithmetic coding has a relatively short history. Although the principle of arithmetic coding was described by Shannon in 1948 <ref> [1] </ref> in proving a source coding theorem, the first practical arithmetic codes did not appear until 1976 with the work of Rissanen [29] and, as reported by Langdon [16], Pasco [30]. <p> Shannon describes a similar mapping in proving a source coding theorem <ref> [1, pp. 60-61] </ref>. 54 above by accomplishing the mapping and inverse-mapping in a letter-by-letter fashion.
Reference: [2] <author> Shannon, C.E., </author> <title> "Coding Theorems for a Discrete Source with a Fidelity Criterion," </title> <journal> IRE Nat. Conv. Rec., </journal> <volume> pt. 4, </volume> <pages> pp. 142-163, </pages> <month> Mar. </month> <year> 1959. </year>
Reference: [3] <author> Kolmogorov, </author> <title> A.N., "On the Shannon Theory of Information in the Case of Continuous Signals," </title> <journal> IRE Trans. Inform. Theory, </journal> <volume> vol. IT-2, </volume> <pages> pp. 102-108, </pages> <year> 1956. </year>
Reference-contexts: In other words, a probabilistic model of the source must be specified, and the fidelity measure must be defined in terms of that probabilistic model. The most obvious such definition is some sort of average distortion incurred in the encoding/decoding process. Shannon [1][2] and Kolmogorov <ref> [3] </ref> have shown that if a distortion measure is suitably defined, then there exists a bounding function, known as the rate-distortion function, which specifies the minimum rate R (D) required by any encoding/decoding system to represent a given source with distortion not exceeding D.
Reference: [4] <author> Gray, R.M., and Davisson, L.D., </author> <title> "A Mathematical Theory of Data Compression?" Proc. </title> <journal> IEEE Int. Conf. Commun., </journal> <pages> pp. </pages> <address> 40-A-1|40-A-5, </address> <year> 1974. </year>
Reference-contexts: Equivalently, there exists a distortion-rate function, which specifies the minimum possible distortion D (R) attainable when a source is encoded at a rate not exceeding R bits per sample. An introduction to rate-distortion theory can be found in <ref> [4] </ref> or [5, Appendix D], while a more in-depth treatments are given in [6] and [7]. The most commonly used distortion measures are single-letter distortion measures.
Reference: [5] <author> Jayant, N.S., and Noll, P., </author> <title> Digital Coding of Waveforms | Principles and Applications to Speech and Video, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1984. </year>
Reference-contexts: Equivalently, there exists a distortion-rate function, which specifies the minimum possible distortion D (R) attainable when a source is encoded at a rate not exceeding R bits per sample. An introduction to rate-distortion theory can be found in [4] or <ref> [5, Appendix D] </ref>, while a more in-depth treatments are given in [6] and [7]. The most commonly used distortion measures are single-letter distortion measures. <p> For an introduction to quantization and its history, the reader is referred to Ger 14 sho [17]. An extremely simple treatment of the basic concepts of quantization appears in [18, Chapter 2]. In-depth summaries of the important results of quantization research can be found in <ref> [5, Chapter 4] </ref> and [19]. In the entropy-constrained case, the most relevant prior work is by Goblick and Holsinger [20], Gish and Pierce [21], Berger [22][23], Wood [24], Noll and Zelinski [25], and Farvardin and Modestino [11][26]. <p> In early investigations into uniform quantization [23][25], consideration was restricted to the case = 0. Uniform quantizers of this type are often termed midriser, as their characteristic staircase functions rise at the origin. At the other extreme are uniform quantizers with = =2, which are often termed midtread <ref> [5] </ref>. <p> First, it should be noted that in general, direct calculation of the distortion-rate function of a continuous-amplitude source is extremely difficult. In some cases, the Calculus of Variations can be successfully applied [6][43]. In general, however, the distortion-rate function is much more easily bounded than computed <ref> [5, pp. 639-640] </ref>. In contrast, in the case of a discrete-amplitude source, an algorithm devised independently by Arimoto [44] and by Blahut [45][46] always converges to the distortion-rate function. <p> The results of the technique described above agree with those previously published for the Gaussian, Laplacian, and gamma MSE case <ref> [5] </ref>, and the Laplacian mean-absolute error case [6]. 42 In the next section, the distortion-rate functions computed in the above manner are used as a reference by which to gauge the performance of alphabet-constrained and entropy-constrained quantization for Gaussian, Laplacian, and gamma sources. 2.8 Performance of Locally-Optimal and Uniform Quantization This
Reference: [6] <author> Berger, T., </author> <title> Rate Distortion Theory: A Mathematical Basis for Data Compression, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1971 </year>
Reference-contexts: An introduction to rate-distortion theory can be found in [4] or [5, Appendix D], while a more in-depth treatments are given in <ref> [6] </ref> and [7]. The most commonly used distortion measures are single-letter distortion measures. A single-letter distortion measure is an average of some nonnegative function of each source letter and the corresponding reproducing letter (rather than of the two sequences of letters). <p> In the present study, the approach to calculating the distortion-rate 10 Analytical expressions for the distortion-rate function are available in the Gaussian MSE case [7, p. 477] and in the Laplacian mean-absolute error case <ref> [6, p. 95] </ref>, while numerically obtained values are available in the Laplacian and gamma MSE cases [5][25]. <p> The results of the technique described above agree with those previously published for the Gaussian, Laplacian, and gamma MSE case [5], and the Laplacian mean-absolute error case <ref> [6] </ref>. 42 In the next section, the distortion-rate functions computed in the above manner are used as a reference by which to gauge the performance of alphabet-constrained and entropy-constrained quantization for Gaussian, Laplacian, and gamma sources. 2.8 Performance of Locally-Optimal and Uniform Quantization This section presents distortion-rate performance curves for uniform
Reference: [7] <author> Gallager, R.G., </author> <title> Information Theory and Reliable Communication. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: An introduction to rate-distortion theory can be found in [4] or [5, Appendix D], while a more in-depth treatments are given in [6] and <ref> [7] </ref>. The most commonly used distortion measures are single-letter distortion measures. A single-letter distortion measure is an average of some nonnegative function of each source letter and the corresponding reproducing letter (rather than of the two sequences of letters). <p> In the present study, the approach to calculating the distortion-rate 10 Analytical expressions for the distortion-rate function are available in the Gaussian MSE case <ref> [7, p. 477] </ref> and in the Laplacian mean-absolute error case [6, p. 95], while numerically obtained values are available in the Laplacian and gamma MSE cases [5][25]. <p> That is, define P i (U i ) = Prfu i = U i j U i1 g: (3:1) By the chain rule of probability <ref> [7, p. 22] </ref>, the probability P u N (U N ) that u N assumes the value U N can be written P u N (U N ) = Prfu N = U N g = P 1 (U 1 )P 2 (U 2 ) P N (U N ): (3:2) <p> An alternative approach, devised independently by Elias, Gallager, and Shannon, all in unpublished work [56][57, pp. 61-62][58], dispenses with the first limitation cited 6 A prefix-condition code has the property that no one codeword is a prefix of another codeword. 7 This mapping is described by Gallager in an exercise <ref> [7, pp. 513-514] </ref>. Shannon describes a similar mapping in proving a source coding theorem [1, pp. 60-61]. 54 above by accomplishing the mapping and inverse-mapping in a letter-by-letter fashion.
Reference: [8] <author> Gray, </author> <title> R.M. "Vector Quantization," </title> <journal> IEEE ASSP Magazine, </journal> <pages> pp. 4-29, </pages> <month> Apr. </month> <year> 1984. </year>
Reference-contexts: In general, the performance of an encoding/decoding system can approach the rate-distortion bound only if many source letters are encoded at once. Vector quantization is one such form of encoding <ref> [8] </ref>, and trellis coding is another [9]. Alternatively, if successive source letters are encoded independently of one another, then the encoding process is called scalar quantization with entropy-coding, or often simply quantization (particularly in the special case of fixed-rate encoding).
Reference: [9] <author> Viterbi, A.J., and Omura, J.K., </author> <title> "Trellis Encoding of Memoryless Discrete-Time Sources with a Fidelity Criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-20, no. 3, </volume> <pages> pp. 325-332, </pages> <year> 1974. </year>
Reference-contexts: In general, the performance of an encoding/decoding system can approach the rate-distortion bound only if many source letters are encoded at once. Vector quantization is one such form of encoding [8], and trellis coding is another <ref> [9] </ref>. Alternatively, if successive source letters are encoded independently of one another, then the encoding process is called scalar quantization with entropy-coding, or often simply quantization (particularly in the special case of fixed-rate encoding).
Reference: [10] <author> Pratt, </author> <title> W.K., Digital Image Processing, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1978. </year>
Reference: [11] <author> Farvardin, N., and Modestino, J.W., </author> <title> "Adaptive Buffer-Instrumented Entropy-Coded Quantizer Performance for Memoryless Sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-32, no. 1, </volume> <month> Jan. </month> <year> 1986. </year>
Reference-contexts: These sources, often used in modeling signals encountered in speech-and image-coding practice [5][10], can be regarded as instances of the "generalized Gaussian" density as defined in <ref> [11] </ref>. <p> In a related vein, if the entropy-coder is significantly inefficient, then the number of bits used will be greater than the entropy. In such a situation it might make sense to optimize the quantizer and entropy-coder jointly; this problem is considered by Farvardin and Modestino <ref> [11] </ref>. However, one of the main results of this thesis is that it is in fact possible to encode Q (x) in such a way that the alphabet constraint is inactive, while at the same time, the coding efficiency is sufficiently high to warrant the lone assumption of constrained entropy. <p> Farvardin and Modestino investigate in detail the problem of designing locally-optimal quantizers for memoryless sources subject to an entropy constraint [26]. In a later paper they investigate an implementation of entropy-constrained quantization <ref> [11] </ref>. Practical arithmetic coding has a relatively short history. <p> The other is the suitability of the technique in the presence of transmission errors. The former topic is omitted because it is adequately treated elsewhere <ref> [11] </ref>[33]. In particular, Farvardin and Modestino [11] show that little need be lost in quantizer performance by buffering, even if the size of the buffer is kept rather small. <p> It should be emphasized that even in the comparatively simple case of alphabet-constrained quantization, little is known in general about the global optimality of 5 The same numbers appear again in a later paper by the same authors (see Table II of <ref> [11] </ref>). 25 -10 -6 -2 2 6 10 Decision levels Reconstruction levels -10 -6 -2 2 6 10 Decision levels Reconstruction levels -10 -6 -2 2 6 10 Decision levels Reconstruction levels Error exponent n Decision and reconstruction levels (c) gamma (b) Laplacian (a) Gaussian sources, versus error exponent -. <p> The present application requires that the average bit-rate occasionally be less than one bit per letter; therefore, direct Huffman coding is inappropriate. Farvardin and Modestino <ref> [11] </ref> suggest an entropy-coding scheme that involves blocking the source letters into L-tuples, then applying a Huffman code to the resulting sequence of L-tuples. This blocking results in a new source alphabet that consists of K L letters. <p> Second, as K becomes large (as is required at higher rates), the codebook becomes very large (perhaps prohibitively large) even when L = 2. These two considerations indicate that the scheme described in <ref> [11] </ref> is less appropriate in the present application than arithmetic coding. Gallager [55] has suggested that the simplest and most natural form of entropy coding to use in the quantization application is a variant of run-length coding.
Reference: [12] <author> Lloyd, </author> <title> S.P., "Least Squares Quantization in PCM," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-28, no. 2, </volume> <pages> pp. 129-137, </pages> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: In the MSE case the problem simplifies considerably, and efficient methods of solution are described by Lloyd <ref> [12] </ref> and Max [13]. In the general case, any of a number of well-known procedures for nonlinear optimization can be used [39][40][41]. In the present study, the simple ad hoc procedure presented in Appendix A was found to be effective in all cases.
Reference: [13] <author> Max, J., </author> <title> "Quantizing for Minimum Distortion," </title> <journal> IRE Trans. Inform. Theory, </journal> <volume> vol. IT-6, </volume> <pages> pp. 7-12, </pages> <month> Mar. </month> <year> 1960. </year>
Reference-contexts: In the MSE case the problem simplifies considerably, and efficient methods of solution are described by Lloyd [12] and Max <ref> [13] </ref>. In the general case, any of a number of well-known procedures for nonlinear optimization can be used [39][40][41]. In the present study, the simple ad hoc procedure presented in Appendix A was found to be effective in all cases.
Reference: [14] <author> Huffman, D., </author> <title> "A Method for Construction of Minimum Redundancy Codes," </title> <booktitle> Proc. IRE, </booktitle> <pages> pp. 1098-1101, </pages> <month> Sept. </month> <year> 1952. </year>
Reference-contexts: A method of entropy-coding that allows this is arithmetic coding. 1.4 Arithmetic Coding The usual approaches to entropy-coding involve mapping a fixed-length string of source letters to a variable-length codeword (by the method of Huffman <ref> [14] </ref>), or 13 mapping a variable-length string of source letters to a fixed-length codeword (by the method of Tunstall [15]), or a combination of the two (e.g., run-length coding). <p> Note that when is larger than roughly 14, the inefficiency is negligible (independent of which distortion exponent is used). 3.7 Alternative Coding Procedures For many, the technique that comes to mind when the term entropy coding is mentioned is Huffman coding <ref> [14] </ref>. In Huffman coding, each source letter is represented by a distinct binary codeword. The codewords are of variable length; more probable source letters are assigned shorter codewords, while less probable letters are assigned longer codewords.
Reference: [15] <author> Tunstall, B., </author> <title> "Synthesis of Noiseless Compression Codes," </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering, Georgia Institute of Technology, </institution> <year> 1968. </year> <month> 97 </month>
Reference-contexts: that allows this is arithmetic coding. 1.4 Arithmetic Coding The usual approaches to entropy-coding involve mapping a fixed-length string of source letters to a variable-length codeword (by the method of Huffman [14]), or 13 mapping a variable-length string of source letters to a fixed-length codeword (by the method of Tunstall <ref> [15] </ref>), or a combination of the two (e.g., run-length coding). In the first approach, compression is achieved by minimizing the average length of the codeword used to represent each source string; in the second, it is achieved by maximizing the number of source letters represented by each codeword.
Reference: [16] <author> Langdon, G.G., </author> <title> "An Introduction to Arithmetic Coding," </title> <journal> IBM J. Res. Develop., </journal> <pages> pp. 135-149, </pages> <month> Mar. </month> <year> 1984. </year>
Reference-contexts: The roots of arithmetic coding can be traced to the original work of Shannon [1], although the technique was made practical only about a decade ago <ref> [16] </ref>. For an introduction to quantization and its history, the reader is referred to Ger 14 sho [17]. An extremely simple treatment of the basic concepts of quantization appears in [18, Chapter 2]. <p> Practical arithmetic coding has a relatively short history. Although the principle of arithmetic coding was described by Shannon in 1948 [1] in proving a source coding theorem, the first practical arithmetic codes did not appear until 1976 with the work of Rissanen [29] and, as reported by Langdon <ref> [16] </ref>, Pasco [30]. Since then, most of the work in arithmetic coding has been in the form of refinements to the basic technique. Most of the published work | including the recent work on the Q-coder [31][32] | has focused on the problem of encoding a binary source. <p> The material presented in this chapter is intended to be self-contained and precise, but not tutorial. Sections 3.1-3, in particular, should be read slowly. For a simplified treatment of arithmetic coding, along with a description of its origins, the reader is referred to Langdon <ref> [16] </ref>. More theoretically oriented treatments are given by Gallager 49 in [49] and Rissanen and Langdon in [50] and [51]. The particular arithmetic code described in this chapter is based on a strategy outlined in [51], although the derivation and analysis of the code presented here are original.
Reference: [17] <author> Gersho, A. </author> <title> "Principles of Quantization," </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> vol. CAS-25, no. 7, </volume> <month> July </month> <year> 1978, </year> <pages> pp. 427-436. </pages>
Reference-contexts: The roots of arithmetic coding can be traced to the original work of Shannon [1], although the technique was made practical only about a decade ago [16]. For an introduction to quantization and its history, the reader is referred to Ger 14 sho <ref> [17] </ref>. An extremely simple treatment of the basic concepts of quantization appears in [18, Chapter 2]. In-depth summaries of the important results of quantization research can be found in [5, Chapter 4] and [19].
Reference: [18] <author> Lynch, T.J., </author> <title> Data Compression Techniques and Applications, </title> <publisher> Wadsworth, </publisher> <address> Bel-mont, California, </address> <year> 1985. </year>
Reference-contexts: For an introduction to quantization and its history, the reader is referred to Ger 14 sho [17]. An extremely simple treatment of the basic concepts of quantization appears in <ref> [18, Chapter 2] </ref>. In-depth summaries of the important results of quantization research can be found in [5, Chapter 4] and [19].
Reference: [19] <author> Clarke, </author> <title> R.J., Transform Coding of Images, </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida, </address> <year> 1985. </year>
Reference-contexts: For an introduction to quantization and its history, the reader is referred to Ger 14 sho [17]. An extremely simple treatment of the basic concepts of quantization appears in [18, Chapter 2]. In-depth summaries of the important results of quantization research can be found in [5, Chapter 4] and <ref> [19] </ref>. In the entropy-constrained case, the most relevant prior work is by Goblick and Holsinger [20], Gish and Pierce [21], Berger [22][23], Wood [24], Noll and Zelinski [25], and Farvardin and Modestino [11][26].
Reference: [20] <author> Goblick, T.J., and Holsinger, J.L., </author> <title> "Analog Source Digitization: A Comparison of Theory and Practice," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-13, </volume> <pages> pp. 323-326, </pages> <month> Apr. </month> <year> 1967. </year>
Reference-contexts: An extremely simple treatment of the basic concepts of quantization appears in [18, Chapter 2]. In-depth summaries of the important results of quantization research can be found in [5, Chapter 4] and [19]. In the entropy-constrained case, the most relevant prior work is by Goblick and Holsinger <ref> [20] </ref>, Gish and Pierce [21], Berger [22][23], Wood [24], Noll and Zelinski [25], and Farvardin and Modestino [11][26]. Goblick and Holsinger [20] show that for a Gaussian memoryless source and some small given value of MSE, the entropy of a uniform quantizer exceeds the rate-distortion bound by only 0.25 bits/sample. <p> In the entropy-constrained case, the most relevant prior work is by Goblick and Holsinger <ref> [20] </ref>, Gish and Pierce [21], Berger [22][23], Wood [24], Noll and Zelinski [25], and Farvardin and Modestino [11][26]. Goblick and Holsinger [20] show that for a Gaussian memoryless source and some small given value of MSE, the entropy of a uniform quantizer exceeds the rate-distortion bound by only 0.25 bits/sample.
Reference: [21] <author> Gish, H., and Pierce, J.N., </author> <title> "Asymptotically Efficient Quantizing," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-14, </volume> <pages> pp. 676-683, </pages> <month> Sept. </month> <year> 1968. </year>
Reference-contexts: In-depth summaries of the important results of quantization research can be found in [5, Chapter 4] and [19]. In the entropy-constrained case, the most relevant prior work is by Goblick and Holsinger [20], Gish and Pierce <ref> [21] </ref>, Berger [22][23], Wood [24], Noll and Zelinski [25], and Farvardin and Modestino [11][26]. Goblick and Holsinger [20] show that for a Gaussian memoryless source and some small given value of MSE, the entropy of a uniform quantizer exceeds the rate-distortion bound by only 0.25 bits/sample. Gish and Pierce [21] establish <p> Pierce <ref> [21] </ref>, Berger [22][23], Wood [24], Noll and Zelinski [25], and Farvardin and Modestino [11][26]. Goblick and Holsinger [20] show that for a Gaussian memoryless source and some small given value of MSE, the entropy of a uniform quantizer exceeds the rate-distortion bound by only 0.25 bits/sample. Gish and Pierce [21] establish the asymptotic optimality, in the case of low distortion, of quantizers that have uniformly spaced decision levels (hereinafter called uniform quantizers), under relatively mild assumptions about the pdf of the source and for a broad class of distortion measures. <p> They further show that for a difference distortion measure D (v) , the difference between the performance of uniform entropy-constrained quantization and the rate-distortion bound vanishes in the case of large -, again only in the case of asymptotically low distortion. The treatment in <ref> [21] </ref> is analytical and involves some elegant approximations. The near-optimality of the uniform quantizer is confirmed in the case of a Gaussian source and MSE criterion by Wood [24], who, like Gish and Pierce, uses an analytical approach. <p> to compute the partial derivatives f@D (-) k =@d k g, in all cases except - = 2 (where analytical solutions are available). 2.2 Uniform Entropy-Constrained Quantization With a constraint on entropy, uniform quantization is known to be asymptotically optimal in the case of low distortion under relatively general conditions <ref> [21] </ref>, and nearly MSE-optimal at all rates for some sources [24][22][25]. A significant part of the present study is investigation of the performance of uniform entropy-constrained quantization with respect to --power distortion, relative to locally-optimal quantizers and the rate-distortion bound. <p> However, the approach taken here is to use a uniform midtread quantizer of the appropriate entropy as the starting point in each case. The reason for this choice is threefold: first, uniform quantization is known to be optimal in the asymptotic case of low distortion <ref> [21] </ref>; second, such quantizers are known to perform well in the MSE sense at all rates for the sources of present interest [22]; and third, using such a quantizer as a starting point provides a means of testing the degree to which the uniform midtread quantizer is locally-optimal (by comparing it <p> Another observation is that the performance of entropy-constrained quantization appears to approach the distortion-rate bound as the distortion exponent increases, for all rates. This had been previously established only in the asymptotic case of low distortion <ref> [21] </ref>. It was observed in the previous section that based on the results of a randomized search, the globally optimal quantizers appear to be asymmetric at low rates in the Gaussian mean-absolute error case. The same conclusion can be reached in a different way.
Reference: [22] <author> Berger, T., </author> <title> "Minimum Entropy Quantizers and Permutation Codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-28, no. 2, </volume> <pages> pp. 149-156, </pages> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: In the alphabet-constrained case, Fleischer [27] and Trushkin [28] have obtained sufficient conditions for optimality for certain sources and distortion measures, but corresponding conditions in the entropy-constrained case have yet to be obtained. In <ref> [22] </ref>, Berger uses numerical techniques to show that in both the Gaussian and Laplacian case, uniform quantizers appear to be nearly optimal. <p> Thus, uniform quantizers are in fact suboptimal in the case of entropy-constrained quantization of a Laplacian source with respect to the MSE criterion. It is important to note that Berger himself never claims in [23] that the uniform midriser quantizers he considers are optimal, and in a subsequent paper <ref> [22] </ref>, he concludes that indeed they must not be optimal. It is remarkable then, that the erroneous belief of exact optimality persists within the research community. 3 The dependence of k on for these innermost levels is most conveniently demonstrated numerically. <p> The reason for this choice is threefold: first, uniform quantization is known to be optimal in the asymptotic case of low distortion [21]; second, such quantizers are known to perform well in the MSE sense at all rates for the sources of present interest <ref> [22] </ref>; and third, using such a quantizer as a starting point provides a means of testing the degree to which the uniform midtread quantizer is locally-optimal (by comparing it with the result of the search).
Reference: [23] <author> Berger, T., </author> <title> "Optimum Quantizers and Permutation Codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-18, no. 6, </volume> <pages> pp. 759-765, </pages> <month> Nov. </month> <year> 1972. </year>
Reference-contexts: A necessary condition for optimality of an entropy-constrained quantizer | which is simply the Lagrange multiplier condition | was first proposed by Berger <ref> [23] </ref>. In the alphabet-constrained case, Fleischer [27] and Trushkin [28] have obtained sufficient conditions for optimality for certain sources and distortion measures, but corresponding conditions in the entropy-constrained case have yet to be obtained. <p> midriser uniform quantizer can never have an entropy of less than one bit per sample, since the maximum possible probability of a quantization region in that case is 1=2. 2.3 MSE-Suboptimality of Uniform Quantization for Laplacian Sources It has been asserted by several authors [19][25][36][37][38], all of whom cite Berger <ref> [23] </ref>, 22 that when a Laplacian source is to be quantized subject to an entropy constraint, the uniform quantizer is exactly optimal in the MSE sense for all values of entropy. <p> For the d k 's that do border the region containing zero, k exhibits a strong dependence on and assumes the common value of k for the other d k 's only as approaches zero. 3 In this case, which Berger considers exclusively in <ref> [23] </ref>, the condition (2:10) holds for all k; i.e., the Lagrange multiplier condition is satisfied. This does not imply that these quantizers are optimal | the Lagrange multiplier condition is necessary, but not sufficient, for optimality. <p> Thus, uniform quantizers are in fact suboptimal in the case of entropy-constrained quantization of a Laplacian source with respect to the MSE criterion. It is important to note that Berger himself never claims in <ref> [23] </ref> that the uniform midriser quantizers he considers are optimal, and in a subsequent paper [22], he concludes that indeed they must not be optimal.
Reference: [24] <author> Wood, </author> <title> R.C., "On Optimum Quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-15, </volume> <pages> pp. 248-252, </pages> <month> Mar. </month> <year> 1969. </year>
Reference-contexts: In-depth summaries of the important results of quantization research can be found in [5, Chapter 4] and [19]. In the entropy-constrained case, the most relevant prior work is by Goblick and Holsinger [20], Gish and Pierce [21], Berger [22][23], Wood <ref> [24] </ref>, Noll and Zelinski [25], and Farvardin and Modestino [11][26]. Goblick and Holsinger [20] show that for a Gaussian memoryless source and some small given value of MSE, the entropy of a uniform quantizer exceeds the rate-distortion bound by only 0.25 bits/sample. <p> The treatment in [21] is analytical and involves some elegant approximations. The near-optimality of the uniform quantizer is confirmed in the case of a Gaussian source and MSE criterion by Wood <ref> [24] </ref>, who, like Gish and Pierce, uses an analytical approach. A necessary condition for optimality of an entropy-constrained quantizer | which is simply the Lagrange multiplier condition | was first proposed by Berger [23].
Reference: [25] <author> Noll, P., and Zelinski, R., </author> <title> "Bounds on Quantizer Performance in the Low Bit-Rate Region," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. COM-26, no. 2, </volume> <pages> pp. 300-305, </pages> <month> Feb. </month> <year> 1978. </year>
Reference-contexts: In-depth summaries of the important results of quantization research can be found in [5, Chapter 4] and [19]. In the entropy-constrained case, the most relevant prior work is by Goblick and Holsinger [20], Gish and Pierce [21], Berger [22][23], Wood [24], Noll and Zelinski <ref> [25] </ref>, and Farvardin and Modestino [11][26]. Goblick and Holsinger [20] show that for a Gaussian memoryless source and some small given value of MSE, the entropy of a uniform quantizer exceeds the rate-distortion bound by only 0.25 bits/sample. <p> In [22], Berger uses numerical techniques to show that in both the Gaussian and Laplacian case, uniform quantizers appear to be nearly optimal. Noll and Zelinski <ref> [25] </ref> also use a numerical technique to show that uniform quantization appears to be nearly 15 MSE-optimal for several memoryless sources, but in their study they unnecessarily restrict consideration to only those quantizers that have a decision level at zero, as noted in [26] and elsewhere.
Reference: [26] <author> Farvardin, N., and Modestino, J.W., </author> <title> "Optimum Quantizer Performance for a Class of Non-Gaussian Memoryless Sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-30, no. 3, </volume> <month> May </month> <year> 1984. </year>
Reference-contexts: Noll and Zelinski [25] also use a numerical technique to show that uniform quantization appears to be nearly 15 MSE-optimal for several memoryless sources, but in their study they unnecessarily restrict consideration to only those quantizers that have a decision level at zero, as noted in <ref> [26] </ref> and elsewhere. Farvardin and Modestino investigate in detail the problem of designing locally-optimal quantizers for memoryless sources subject to an entropy constraint [26]. In a later paper they investigate an implementation of entropy-constrained quantization [11]. Practical arithmetic coding has a relatively short history. <p> MSE-optimal for several memoryless sources, but in their study they unnecessarily restrict consideration to only those quantizers that have a decision level at zero, as noted in <ref> [26] </ref> and elsewhere. Farvardin and Modestino investigate in detail the problem of designing locally-optimal quantizers for memoryless sources subject to an entropy constraint [26]. In a later paper they investigate an implementation of entropy-constrained quantization [11]. Practical arithmetic coding has a relatively short history. <p> of (adjusting as needed to keep H constant). 24 It is worth mentioning that the investigation in this section was inspired by the following experimental finding: better quantizers than those described as "optimal" for a Laplacian source by Farvardin and Modestino (column 6 of the table on p. 495 of <ref> [26] </ref>) 5 are easily obtainable using either a randomized search or the technique to be described in Section 2.5. 2.4 Design of Locally-Optimal Alphabet-Constrained Quantizers When no entropy-constraint is imposed, the design problem is conceptually very simple: locate a relative minimum of a nonlinear scalar function, D (-) , of a <p> While asymmetric optimal quantizers have been previously reported for symmetric sources in the alphabet-constrained case 39 [37][42] and the joint alphabet- and entropy-constrained case <ref> [26] </ref>, none has previously been reported in the purely entropy-constrained case. The result strikes the author as mildly counterintuitive. An interesting feature of the decision-level scatter plots is the occasional doubling-up of decision levels | that is, two or more decision levels lying very close to one another. <p> Asymmetry in the alphabet-constrained case has been reported previously by Sharma [37] and Kabal [42], and in the entropy-constrained case, but with a severe restriction on the number of reconstruction levels, by Farvardin <ref> [26] </ref>. In the latter work, asymmetry 83 is also reported in some cases when the number of reconstruction levels is large, but it is asserted there that in every such case an optimal symmetric quantizer can also be found.
Reference: [27] <author> Fleischer, P.E., </author> <title> "Sufficient Conditions for Achieving Minimum Distortion in a Quantizer," </title> <booktitle> IEEE Int. Conv. Rec., part 1, </booktitle> <year> 1964, </year> <pages> pp. 104-111. </pages>
Reference-contexts: A necessary condition for optimality of an entropy-constrained quantizer | which is simply the Lagrange multiplier condition | was first proposed by Berger [23]. In the alphabet-constrained case, Fleischer <ref> [27] </ref> and Trushkin [28] have obtained sufficient conditions for optimality for certain sources and distortion measures, but corresponding conditions in the entropy-constrained case have yet to be obtained.
Reference: [28] <author> Trushkin, </author> <title> A.V., "Sufficient Conditions for Uniqueness of a Locally Optimal Quantizer for a Class of Convex Error Weighting Functions," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-28, </volume> <pages> pp. 187-198, </pages> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: A necessary condition for optimality of an entropy-constrained quantizer | which is simply the Lagrange multiplier condition | was first proposed by Berger [23]. In the alphabet-constrained case, Fleischer [27] and Trushkin <ref> [28] </ref> have obtained sufficient conditions for optimality for certain sources and distortion measures, but corresponding conditions in the entropy-constrained case have yet to be obtained. In [22], Berger uses numerical techniques to show that in both the Gaussian and Laplacian case, uniform quantizers appear to be nearly optimal.
Reference: [29] <author> Rissanen, J.J., </author> <title> "Generalized Kraft Inequality and Arithmetic Coding," </title> <journal> IBM J. Res. Develop., </journal> <volume> vol. 20, no. </volume> <pages> pp. 198-203, </pages> <year> 1976. </year>
Reference-contexts: Practical arithmetic coding has a relatively short history. Although the principle of arithmetic coding was described by Shannon in 1948 [1] in proving a source coding theorem, the first practical arithmetic codes did not appear until 1976 with the work of Rissanen <ref> [29] </ref> and, as reported by Langdon [16], Pasco [30]. Since then, most of the work in arithmetic coding has been in the form of refinements to the basic technique.
Reference: [30] <author> Pasco, R., </author> <title> "Source Coding Algorithms for Fast Data compression," </title> <type> Ph.D. Thesis, </type> <institution> Department of Electrical Engineering, Stanford University, California, </institution> <year> 1976. </year>
Reference-contexts: Although the principle of arithmetic coding was described by Shannon in 1948 [1] in proving a source coding theorem, the first practical arithmetic codes did not appear until 1976 with the work of Rissanen [29] and, as reported by Langdon [16], Pasco <ref> [30] </ref>. Since then, most of the work in arithmetic coding has been in the form of refinements to the basic technique. Most of the published work | including the recent work on the Q-coder [31][32] | has focused on the problem of encoding a binary source.
Reference: [31] <author> Mitchell, J.L., and Pennebaker, W.B., </author> <title> "Optimal Hardware and Software Arithmetic Coding Procedures for the Q-Coder," </title> <note> draft submitted for publication to IBM J. </note> <institution> Res. Dev., </institution> <month> Aug. </month> <year> 1988. </year> <month> 98 </month>
Reference: [32] <author> Pennebaker, W.B., and Mitchell, J.L., </author> <title> "An Overview of the Basic Principles of the Q-Coder Adaptive Binary Arithmetic Coder," </title> <note> draft submitted for publication to IBM J. </note> <institution> Res. Dev., </institution> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Second, it allows researchers involved in designing a new 20 Their approach is analogous to that used in by Langdon and Rissanen in [47] and by Pennebaker and Mitchell in <ref> [32] </ref>, but the extension to the K-ary case as described in [61] is, in the present author's opinion, not at all obvious. <p> Define a new type, "codereg," for these registers: */ typedef unsigned long codereg; /* variables: */ static int k; /* size of source alphabet */ static int pr; /* precision of arithmetic operations (e.g., 16) */ static int ccw; /* width of carry-control field (e.g., 8) */ static codereg twotothe <ref> [32] </ref>; /* array of masks that isolate individual bits */ static double *p; /* source probabilities */ static codereg q [MAXK]; /* rounded source probabilities */ static codereg cumq [MAXK]; /* cumulative rounded source probabilities */ static int verbose = 0; /* set to zero to make silent */ /* routines:
Reference: [33] <author> Jelinek, F., </author> <title> "Buffer Overflow in Variable Length Coding of Fixed Rate Sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-14, no. 3, </volume> <month> May </month> <year> 1968. </year>
Reference: [34] <author> Thomas, </author> <title> G.B., and Finney, R.L., Calculus and Analytic Geometry, fifth ed., </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1980. </year>
Reference: [35] <author> Simmons, </author> <title> G.F., Calculus with Analytic Geometry, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: It should be noted that finding the r k 's that satisfy (2:5) is generally not a simple matter, particularly when is odd. A related problem is the difficulty of evaluating the partial derivatives f@D (-) k =@d k g via Leibniz' formula <ref> [35] </ref> | such evaluation requires, ultimately, differentiation of the r k 's, which are not generally available in analytic form.
Reference: [36] <author> Netravali, A.N., and Saigal, R., </author> <title> "Optimum Quantizer Design Using a Fixed-Point Algorithm," </title> <journal> Bell System Technical Journal, </journal> <volume> vol. 55, no. 9, </volume> <month> Nov. </month> <year> 1976, </year> <pages> pp. 1423-1435. </pages>
Reference: [37] <author> Sharma, D.K., </author> <title> "Design of Absolutely Optimal Quantizers for a Wide Class of Distortion Measures," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-24, no. 6, </volume> <month> Nov. </month> <year> 1978, </year> <pages> pp. 693-702. </pages>
Reference-contexts: One of the more surprising findings of the present work is the asymmetry of optimum entropy-constrained quantizers in the case of a Gaussian source and mean-absolute distortion measures, for rates less than one bit per sample. Asymmetry in the alphabet-constrained case has been reported previously by Sharma <ref> [37] </ref> and Kabal [42], and in the entropy-constrained case, but with a severe restriction on the number of reconstruction levels, by Farvardin [26].
Reference: [38] <author> Netravali, </author> <title> A.N., and Limb, J.O., "Picture Coding: </title> <journal> A Review,"Proceedings of the IEEE, </journal> <volume> vol. 68, no. 3, </volume> <month> Mar. </month> <year> 1980, </year> <pages> pp. 366-406. </pages>
Reference: [39] <author> Luenberger, D.G., </author> <title> Optimization by Vector Space Methods, </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1969. </year>
Reference: [40] <author> Luenberger, D.G., </author> <title> Linear and Nonlinear Programming, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1984. </year>
Reference-contexts: , is given by x k = x k1 ff k g k1 ; (A:1) where the adaptive step size ff k is computed as ff k = 0:1 kx 0 k if k = 1; fi fl otherwise. (A:2) This procedure is similar to the method of steepest descent <ref> [40, pp. 214-225] </ref>, but differs in one important respect: instead of performing a line minimization at every step, the new point is obtained by simply stepping in the direction of the negative gradient by a small amount ff k , where the amount is determined by the recent consistency of the
Reference: [41] <author> Shanno, D.F., and Phu, K.H., </author> <title> "Minimization of Unconstrained Multivariable Functions," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> No. 6, </volume> <pages> pp. 618-622, </pages> <month> Dec. </month> <year> 1980. </year>
Reference-contexts: In each of these applications, the procedure presented here was found to converge more reliably and occasionally more quickly than the TOMS algorithm #500 <ref> [41] </ref> and the Fletcher-Reeves-Polak-Ribiere algorithm [65, pp. 317-323], although this might be attributable to an inappropriate choice of parameters in the latter algorithms. 88 Appendix B: Software to calculate moments of densities This appendix presents C-language software to compute the definite integral Z b X i f x (X)dX; where i
Reference: [42] <author> Kabal, P., </author> <title> "Quantizers for the Gamma Distribution and Other Symmetrical Distributions," </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> vol. ASSP-32, no. 4, </volume> <pages> pp. 836-841, </pages> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: The continuous lines are obtained by connecting corresponding decision levels computed for integer values of -. 26 a particular quantizer for a given source. However, a few special cases have been successfully handled [27][28]<ref> [42] </ref>. In particular, Kabal [42] shows that in the case of a gamma source and mean-square error, the symmetric locally-optimal alphabet-constrained quantizers obtained in the present study are not globally optimal. It was observed at the beginning of the present section that the problem of obtaining a locally-optimal alphabet-constrained quantizer is conceptually very simple. <p> Note that entropy-constrained quantization performs substantially better than alphabet-constrained quantization in most cases, particularly for large values of the error exponent -. It should be noted that the locally-optimal alphabet-constrained quantizers in the gamma MSE case shown in Figure 2.14 are not globally-optimal (this has been shown by Kabal <ref> [42] </ref>). <p> Asymmetry in the alphabet-constrained case has been reported previously by Sharma [37] and Kabal <ref> [42] </ref>, and in the entropy-constrained case, but with a severe restriction on the number of reconstruction levels, by Farvardin [26].
Reference: [43] <author> Girod, B., </author> <type> private communication, </type> <institution> Massachusetts Institute of Technology, </institution> <month> Fall </month> <year> 1988. </year>
Reference: [44] <author> Arimoto, S., </author> <title> "An Algorithm for Computing the Capacity of Arbitrary Discrete Memoryless Channels," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-18, </volume> <pages> pp. 14-20, </pages> <month> Jan. </month> <year> 1972. </year>
Reference-contexts: In some cases, the Calculus of Variations can be successfully applied [6][43]. In general, however, the distortion-rate function is much more easily bounded than computed [5, pp. 639-640]. In contrast, in the case of a discrete-amplitude source, an algorithm devised independently by Arimoto <ref> [44] </ref> and by Blahut [45][46] always converges to the distortion-rate function.
Reference: [45] <author> Blahut, R.E., </author> <title> "Computation of Channel Capacity and Rate-Distortion Functions," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-18, no. 4, </volume> <month> July, </month> <year> 1972. </year>
Reference: [46] <author> Humblet, P., </author> <title> "The Arimoto-Blahut Algorithm to find Channel Capacity," class handout for MIT course 6.441, </title> <address> Cambridge, Mass., spring term, </address> <year> 1983. </year> <month> 99 </month>
Reference: [47] <author> Langdon, G., and Rissanen, J.J., </author> <title> "Compression of Black-White Images with Arithmetic Coding," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. COM-29, no. 6, </volume> <pages> pp. 858-867, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Thus, carry-trapping must be disabled as long as the carry-control field contains a carry-trap bit. This consideration leads to the two-state carry-trapping mechanism as indicated in Figure 3.19. This method of controlling the propagation of carry-overs was devised by Langdon and Rissanen <ref> [47] </ref>, and may have been based on a similar technique devised by Rubin [60]. 3.6 Performance of Arithmetic Coding Applied to Quantization 75 0 10 20 30 40 50 + + + + + + + + + + + + + + + + + + + + + + <p> In the former case, the additional burden of having to compute the probabilities would far outweigh any computational simplification that might result from employing a binary rather than K-ary arithmetic code. In the latter case, the number of probabilities (or, if the approach described in <ref> [47] </ref> is used, the number of "skew numbers") that would have to be stored is increased by a factor of dlog 2 Ke. <p> First, it essentially solves the source coding problem for entropy-coded quantization (at least in the case of reliable channels and moderately fast hardware). Second, it allows researchers involved in designing a new 20 Their approach is analogous to that used in by Langdon and Rissanen in <ref> [47] </ref> and by Pennebaker and Mitchell in [32], but the extension to the K-ary case as described in [61] is, in the present author's opinion, not at all obvious.
Reference: [48] <author> Langdon, G., and Rissanen, J.J., </author> <title> "A Simple General Binary Source Code," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-28, no. 5, </volume> <month> Sep. </month> <year> 1982. </year>
Reference: [49] <author> Gallager, R.G., </author> <title> "Arithmetic Coding," class handout for MIT course 6.441, </title> <address> Cam-bridge, Mass., </address> <month> Feb. 26, </month> <year> 1989. </year>
Reference-contexts: Sections 3.1-3, in particular, should be read slowly. For a simplified treatment of arithmetic coding, along with a description of its origins, the reader is referred to Langdon [16]. More theoretically oriented treatments are given by Gallager 49 in <ref> [49] </ref> and Rissanen and Langdon in [50] and [51]. The particular arithmetic code described in this chapter is based on a strategy outlined in [51], although the derivation and analysis of the code presented here are original. <p> for every i, with (U 1 ) (U 2 ) : : : (U N ). 10 In this case, the limitation can be replaced by the slightly less severe limitation that it might be necessary to wait until many letters have been encoded before the first can be decoded <ref> [49] </ref>. Some difficulty of this type is inherent to the class of codes in which variable-length source strings are mapped to either variable- or fixed-length code strings, (this class includes arithmetic codes).
Reference: [50] <author> Rissanen, J.J., and Langdon, G.G., </author> <title> "Arithmetic Coding," </title> <journal> IBM J. Res. Develop., </journal> <volume> vol. 23, no. 2, </volume> <pages> pp. 149-162, </pages> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: Sections 3.1-3, in particular, should be read slowly. For a simplified treatment of arithmetic coding, along with a description of its origins, the reader is referred to Langdon [16]. More theoretically oriented treatments are given by Gallager 49 in [49] and Rissanen and Langdon in <ref> [50] </ref> and [51]. The particular arithmetic code described in this chapter is based on a strategy outlined in [51], although the derivation and analysis of the code presented here are original.
Reference: [51] <author> Rissanen, J.J. </author> <title> "Universal Modelling and Coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-27, no. 1, </volume> <pages> pp. 12-23, </pages> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: Sections 3.1-3, in particular, should be read slowly. For a simplified treatment of arithmetic coding, along with a description of its origins, the reader is referred to Langdon [16]. More theoretically oriented treatments are given by Gallager 49 in [49] and Rissanen and Langdon in [50] and <ref> [51] </ref>. The particular arithmetic code described in this chapter is based on a strategy outlined in [51], although the derivation and analysis of the code presented here are original. <p> More theoretically oriented treatments are given by Gallager 49 in [49] and Rissanen and Langdon in [50] and <ref> [51] </ref>. The particular arithmetic code described in this chapter is based on a strategy outlined in [51], although the derivation and analysis of the code presented here are original. The main contribution of the present work lies in the application of arithmetic coding to scalar quantization in a way that essentially solves the source coding problem in that application. <p> Secondary contributions of this work are * in supplying various important details not given in <ref> [51] </ref> or elsewhere in the literature (these details were worked out by the present author.) * in providing an algorithm for rounding-off source letter probabilities to admissible values (finite-precision values that allow unique decod-ability) in a way that minimizes the attendant increase in the average number of code bits. 3.1 Notation
Reference: [52] <author> Brusewitz, H., </author> <title> "Quantization and Entropy Coding," </title> <institution> Royal Inst. </institution> <type> Tech. internal report TRITA-TTT 8304, </type> <address> Stockholm, </address> <month> Dec. </month> <year> 1983. </year>
Reference-contexts: The main contribution of the present work lies in the application of arithmetic coding to scalar quantization in a way that essentially solves the source coding problem in that application. The suitability of arithmetic coding in the quantization application was apparently first recognized by Brusewitz <ref> [52] </ref> in 1983, then independently by the author [53] in 1985, and most recently by Chou et al. in 1989. 1 It should be noted that Brusewitz' and Chou's discussions of arithmetic coding applied to quantization consist of a single paragraph and a single sentence, respectively; neither explores implementation or performance
Reference: [53] <author> Popat, </author> <title> A.C., unpublished work, </title> <institution> Massachusetts Institute of Technology, </institution> <year> 1985. </year>
Reference-contexts: The suitability of arithmetic coding in the quantization application was apparently first recognized by Brusewitz [52] in 1983, then independently by the author <ref> [53] </ref> in 1985, and most recently by Chou et al. in 1989. 1 It should be noted that Brusewitz' and Chou's discussions of arithmetic coding applied to quantization consist of a single paragraph and a single sentence, respectively; neither explores implementation or performance issues in any detail.
Reference: [54] <author> LeGall, D., and Tabatabai, A., </author> <title> "Sub-band coding of Digital Images Using Symmetric Short Kernel Filters and Arithmetic Coding Techniques," </title> <booktitle> Proc. IEEE Int. Conf. ASSP 1988, </booktitle> <pages> pp 761-763. </pages>
Reference-contexts: The sequence u N can be regarded as a random selection from the superalphabet A N = fa N 1 ; : : : ; a N K N g, where each a N l represents one of the K N possible 1 Recently LeGall and Tabatabai <ref> [54] </ref> have suggested applying a hybrid PCM and self-adapting binary arithmetic code to the high-frequency subbands in an image coding system.
Reference: [55] <author> Gallager, R.G., </author> <title> private communication, </title> <year> 1985. </year>
Reference-contexts: Their scheme is similar to a run length code proposed by Gallager <ref> [55] </ref>, and has similar limitations (i.e., it works well at extremely low bit-rates, but becomes either prohibitively complicated or else inefficient at medium bit-rates | see Section 3.7). <p> Second, as K becomes large (as is required at higher rates), the codebook becomes very large (perhaps prohibitively large) even when L = 2. These two considerations indicate that the scheme described in [11] is less appropriate in the present application than arithmetic coding. Gallager <ref> [55] </ref> has suggested that the simplest and most natural form of entropy coding to use in the quantization application is a variant of run-length coding. It is 78 appropriate to question, then, why in this thesis arithmetic coding was chosen over this more intuitive run-length approach.
Reference: [56] <author> Gallager, R.G., </author> <title> information given in lecture, </title> <publisher> MIT course 6.441, </publisher> <month> Feb. 26, </month> <year> 1989. </year>
Reference: [57] <author> Abramson, N., </author> <title> Information Theory and Coding, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference: [58] <author> Elias, P., </author> <title> in a conversation with the author, </title> <month> Spring </month> <year> 1986. </year>
Reference: [59] <author> Ibaraki, T., and Katoh, N., </author> <title> Resource Allocation Problems, </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1989. </year>
Reference-contexts: In general, this approach yields good but suboptimal coding efficiency. Alternatively, the problem can be regarded as a discrete resource allocation problem with a separable and convex objective function. An optimal solution to this type of problem is obtainable by an incremental or greedy algorithm <ref> [59, p. 54] </ref>. Such an algorithm is shown in Figure 3.15 (a). Uniform quantization of a Gaussian source provides a simple example to illustrate the difference between the best-case efficiencies of the K-ary arithmetic code when the naive and optimal algorithms are used.
Reference: [60] <author> Rubin, F., </author> <title> "Arithmetic Stream Coding Using Fixed Precision Registers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <pages> pp. 672-675, </pages> <month> Nov. </month> <year> 1979. </year>
Reference-contexts: This consideration leads to the two-state carry-trapping mechanism as indicated in Figure 3.19. This method of controlling the propagation of carry-overs was devised by Langdon and Rissanen [47], and may have been based on a similar technique devised by Rubin <ref> [60] </ref>. 3.6 Performance of Arithmetic Coding Applied to Quantization 75 0 10 20 30 40 50 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +
Reference: [61] <author> Rissanen, J.J., and Mohiuddin, K.M., </author> <title> "A Multiplication-Free Multialphabet Arithmetic Code," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 37, no. 2, </volume> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: Recall from Section 3.4 that encoding requires two fixed-precision multiplications per source letter, and that 80 decoding requires on the order of log 2 K fixed-precision multiplications per source letter (under the assumption that a binary search is used). Rissanen and Mohiuddin <ref> [61] </ref> have recently shown that it is possible to substitute shifting operations for multiplications in a K-ary arithmetic code while retaining good efficiency, at least for some sources. 20 This multiplication-free approach would be appropriate when extremely low-cost hardware is to be used (e.g., when general-purpose microprocessors are to be used <p> multiplications are no more expensive than other operations, and the code described in this chapter would be more appropriate because of its near optimality in terms of data compression, and because it appears to require two fewer comparison operations per encoding and decoding cycle than the multiplication-free approach described in <ref> [61] </ref>. The foregoing list of alternative coding techniques is far from exhaustive, but is thought to be representative. <p> Second, it allows researchers involved in designing a new 20 Their approach is analogous to that used in by Langdon and Rissanen in [47] and by Pennebaker and Mitchell in [32], but the extension to the K-ary case as described in <ref> [61] </ref> is, in the present author's opinion, not at all obvious.
Reference: [62] <author> Pearlman, W., </author> <title> "New Algorithms for Optimum Quantization and -Distance," </title> <booktitle> 1986 IEEE International Symposium on Information Theory, </booktitle> <address> Ann Arbor, Michi-gan, </address> <month> Oct. </month> <year> 1986, </year> <month> p.9. </month>
Reference-contexts: While the random search technique described in Chapter 2 provides evidence about the character of optimal quantizers, the evidence is not, by its nature, conclusive. An alternative method of estimating the best performance attainable by entropy-constrained quantization is suggested by Pearlman <ref> [62] </ref>. The basic idea is to compute the rate-distortion performance indirectly, by calculating the rate-distortion function of the source with an appropriate constraint on reconstruction levels (or, in Pearlman's terms, reproduction alphabet). This approach requires an efficient means of finding the optimal reproducing letters at each step of the algorithm. <p> This approach requires an efficient means of finding the optimal reproducing letters at each step of the algorithm. It is claimed in <ref> [62] </ref> that by modifying the distortion measure at each iteration, the performance of the optimal entropy-constrained quantizer can be found for a given number of re 84 construction levels.
Reference: [63] <author> Popat, A.C., and Zeger, K., </author> <title> "Robust Quantization of Memoryless Sources using Dispersive FIR Filters," </title> <journal> IEEE Trans. on Communications, </journal> <note> publication pending. 100 </note>
Reference: [64] <author> Popat, </author> <title> A.C., "A Note on QMF Design," </title> <institution> Advanced Television Research Project memorandum, MIT Research Laboratory of Electronics, </institution> <address> Cambridge, Mas-sachusetts, </address> <month> Dec. </month> <year> 1988. </year>
Reference: [65] <editor> Press, W.H., et al., </editor> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <address> Cam-bridge, UK, </address> <year> 1988. </year> <month> 101 </month>
Reference-contexts: In each of these applications, the procedure presented here was found to converge more reliably and occasionally more quickly than the TOMS algorithm #500 [41] and the Fletcher-Reeves-Polak-Ribiere algorithm <ref> [65, pp. 317-323] </ref>, although this might be attributable to an inappropriate choice of parameters in the latter algorithms. 88 Appendix B: Software to calculate moments of densities This appendix presents C-language software to compute the definite integral Z b X i f x (X)dX; where i = 1; : : :
References-found: 65

