URL: ftp://wol.ra.phy.cam.ac.uk/pub/www/mackay/alpha.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: mackay@mrao.cam.ac.uk  
Title: Comparison of Approximate Methods for Handling Hyperparameters  
Author: David J.C. MacKay 
Note: Submitted to Neural Computation  
Address: Cambridge, CB3 0HE. United Kingdom.  
Affiliation: Cavendish Laboratory,  
Abstract: I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models which include unknown hyperparameters such as regularization constants and noise levels. In the `evidence framework' the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized hyperparameters are used to define a Gaussian approximation to the posterior distribution. In the alternative `MAP' method, the true posterior probability is found by integrating over the hyperparameters. The true posterior is then maximized over the model parameters, and a Gaussian approximation is made. The similarities of the two approaches, and their relative merits, are discussed, and comparisons are made with the ideal hierarchical Bayesian solution. In moderately ill-posed problems, integration over hyperparameters yields a probability distribution with a skew peak which causes significant biases to arise in the MAP method. In contrast, the evidence framework is shown to introduce negligible predictive error, under straightforward conditions. General lessons are drawn concerning inference in many dimensions.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bishop, C. M. </author> <title> (1995) Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press. </publisher>
Reference: <author> Box, G. E. P., and Tiao, G. C. </author> <title> (1973) Bayesian inference in statistical analysis. </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> Bretthorst, G. </author> <title> (1988) Bayesian spectrum analysis and parameter estimation. Springer. </title> <note> Also available at bayes.wustl.edu. </note>
Reference: <author> Bryan, R. </author> <title> (1990) Solving oversampled data problems by Maximum Entropy. In Maximum Entropy and Bayesian Methods, </title> <institution> Dartmouth, U.S.A., </institution> <note> 1989 , ed. by P. Fougere, pp. 221-232. Kluwer. 23 Buntine, </note> <author> W., and Weigend, A. </author> <title> (1991) Bayesian back-propagation. </title> <booktitle> Complex Systems 5: </booktitle> <pages> 603-643. </pages>
Reference-contexts: MPjff MP = fl (54) @ 3 log P (Djff; H) @(log ff) 3 fi fi ff MP MPjff MP = fl : (55) The first derivative is exact, assuming that the eigenvalues a are independent of ff, which is true in the case of a Gaussian prior on w <ref> (Bryan 1990) </ref>. The second and third derivatives are approximate, with terms proportional to n e being omitted.
Reference: <author> Dempster, A., Laird, N., and Rubin, D. </author> <title> (1977) Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B 39: </journal> <pages> 1-38. </pages>
Reference-contexts: Thus the evidence-maximizing estimate replaces k by the effective number of well determined parameters fl: 2 w (MP) = (w MPjff ) 2 =fl. The free energy minimization approach is like an EM algorithm <ref> (Dempster et al. 1977) </ref>, in which we wish to find the most probable ff and do this by introducing an E-step in which a distribution over w is obtained (Neal and Hinton 1993).
Reference: <author> Feynman, R. P. </author> <title> (1972) Statistical Mechanics. </title> <editor> W. A. </editor> <publisher> Benjamin, Inc. </publisher>
Reference-contexts: The objective function chosen to measure the quality of the approximation is a variational free energy <ref> (Feynman 1972) </ref>, F () = d k w Q (w; ) log Q (w; ) The free energy F () is bounded below by log P (DjH) and only attains this value for Q (w; ) = P (wjD; H).
Reference: <author> Gull, S. F. </author> <title> (1988) Bayesian inductive inference and maximum entropy. </title> <booktitle> In Maximum Entropy and Bayesian Methods in Science and Engineering, </booktitle> <volume> vol. 1: </volume> <booktitle> Foundations, </booktitle> <editor> ed. by G. Erickson and C. </editor> <volume> Smith, </volume> <pages> pp. 53-74, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference-contexts: There are typically far fewer regularization constants and other hyperparameters than there are `level 1' parameters. (2) If practical Bayesian methods involve approximations such as fitting a Gaussian to a posterior distribution, then one should think twice before integrating out hyperparameters <ref> (Gull 1988) </ref>. The probability density which results from such an integration typically has a skew peak; a Gaussian fitted at the peak may not approximate the distribution well.
Reference: <author> Gull, S. F. </author> <title> (1989) Developments in maximum entropy data analysis. In Maximum Entropy and Bayesian Methods, Cambridge 1988 , ed. by J. </title> <booktitle> Skilling, </booktitle> <pages> pp. 53-71, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference-contexts: In this paper I compare the approximate strategies of MacKay (1991) and Buntine and Weigend (1991) for handling hyperparameters, assuming a Bayesian approach to neural networks. This comparison is also relevant to other ill-posed problems such as image reconstruction problems <ref> (Gull 1989) </ref>. For simplicity I will concentrate on the case of a single hyperparameter ff, and I will assume that the prior is Gaussian over w, and that the likelihood function is also a Gaussian function of w. <p> The variables ff and fi are known as hyperparameters. Problems for which models can be written in the form (2) include linear interpolation with a fixed basis set (Gull 1988; MacKay 1992a), nonlinear regression with a neural network (MacKay 1992c), nonlinear classification (MacKay 1992b), and image deconvolution <ref> (Gull 1989) </ref>. <p> For each data analysis problem, one may evaluate the critical ff max above which the posterior would be measurably affected by the large ff tail of the evidence <ref> (Gull 1989) </ref>. Often, as Gull points out, this critical value of ff max has bizarrely large magnitude. Even if a flat prior between appropriate ff min and ff max is used, it is possible in principle for the posterior P (log ffjD; H) to be multi-modal.
Reference: <author> Hinton, G. E., and Sejnowski, T. J. </author> <title> (1986) Learning and relearning in Boltzmann machines. In Parallel Distributed Processing, </title> <editor> ed. by D. E. Rumelhart and J. E. </editor> <booktitle> McClelland, </booktitle> <pages> pp. 282-317. </pages> <address> Cambridge Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: A more principled approach to overfitting, and one that is less implementation-dependent, is to change the objective function by adding one or more regularizers that penalize complex functions. There are various 1 regularizers, the simplest and most popular being `weight decay' <ref> (Hinton and Sejnowski 1986) </ref> (also known as `ridge regression').
Reference: <author> Hinton, G. E., and van Camp, D. </author> <title> (1993) Keeping neural networks simple by minimizing the description length of the weights. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pp. 5-13. </pages> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference-contexts: The free energy minimization approach is like an EM algorithm (Dempster et al. 1977), in which we wish to find the most probable ff and do this by introducing an E-step in which a distribution over w is obtained <ref> (Neal and Hinton 1993) </ref>. This distribution takes into account the k fl ill-determined parameters by assigning each of them a variance of 2 w in the matrix .
Reference: <author> MacKay, D. J. C. </author> <title> (1991) Bayesian Methods for Adaptive Models. </title> <institution> California Institute of Technology dissertation. </institution>
Reference: <author> MacKay, D. J. C. </author> <title> (1992a) Bayesian interpolation. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 415-447. </pages>
Reference-contexts: In principle, there may be multiple optima in ff, but this is not the typical case for a model well matched to the data. Under general conditions, the error bars on log ff are log ffjD ' p 2=fl <ref> (MacKay 1992a) </ref> (see section 8). Thus log ff is well-determined by the data if fl 1.
Reference: <author> MacKay, D. J. C. </author> <title> (1992b) The evidence framework applied to classification networks. </title> <booktitle> Neural Computation 4 (5): </booktitle> <pages> 698-714. </pages>
Reference-contexts: The variables ff and fi are known as hyperparameters. Problems for which models can be written in the form (2) include linear interpolation with a fixed basis set (Gull 1988; MacKay 1992a), nonlinear regression with a neural network (MacKay 1992c), nonlinear classification <ref> (MacKay 1992b) </ref>, and image deconvolution (Gull 1989).
Reference: <author> MacKay, D. J. C. </author> <title> (1992c) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 448-472. </pages>
Reference-contexts: The variables ff and fi are known as hyperparameters. Problems for which models can be written in the form (2) include linear interpolation with a fixed basis set (Gull 1988; MacKay 1992a), nonlinear regression with a neural network <ref> (MacKay 1992c) </ref>, nonlinear classification (MacKay 1992b), and image deconvolution (Gull 1989). <p> When a nonlinear model has multiple local optima, one can approximate the posterior by a sum of Gaussians, one fitted at each optimum. There is then an analogous choice between either (a) optimizing ff separately at each local optimum in w, and using a Gaussian approximation conditioned on ff <ref> (MacKay 1992c) </ref>; or (b) fitting multiple Gaussians to local maxima of the true posterior with the hyperparameter ff integrated out. The results of this paper shed light on this choice. <p> We can therefore describe the MAP method thus: MAP method (improper prior over ff): find a self-consistent solution fw MP ; ff eff g such that w MP maximizes P (wjD; ff eff ; H) and ff eff satisfies equation (24). This procedure is suggested in <ref> (MacKay 1992c) </ref> as a `quick and dirty' approximation to the evidence frame work.
Reference: <author> MacKay, D. J. C. </author> <title> (1995) Developments in probabilistic modelling with neural networks|ensemble learning. </title> <booktitle> In Neural Networks: Artificial Intelligence and Industrial Applications. Proceedings of the 3rd Annual Symposium on Neural Networks, </booktitle> <address> Nijmegen, Netherlands, </address> <month> 14-15 September </month> <year> 1995 </year> <month> , pp. </month> <pages> 191-198, </pages> <address> Berlin. </address> <publisher> Springer. </publisher>
Reference-contexts: with respect to the ensemble's parameters, can be evaluated. [This is the main reason for choosing the objective function F () rather than some other measure of distance between Q (w; ) and P (wjD; H).] A longer review of Ensemble Learning including references to applications may be found in <ref> (MacKay 1995) </ref>. In this section I demonstrate that a free energy approximation for the model studied in this paper reproduces the method of the evidence framework precisely.
Reference: <author> MacKay, D. J. C. </author> <title> (1996) Bayesian non-linear modelling for the 1993 energy prediction competition. In Maximum Entropy and Bayesian Methods, </title> <editor> Santa Barbara 1993 , ed. by G. </editor> <booktitle> Heidbreder, </booktitle> <pages> pp. 221-234, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference: <author> Neal, R. M. </author> <title> (1993a) Bayesian learning via stochastic dynamics. </title> <booktitle> In Advances in Neural Information Processing Systems 5 , ed. </booktitle> <editor> by C. L. Giles, S. J. Hanson, and J. D. </editor> <booktitle> Cowan, </booktitle> <pages> pp. 475-482, </pages> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Neal, R. M. </author> <title> (1993b) Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference: <author> Neal, R. M. </author> <title> (1996) Bayesian Learning for Neural Networks. </title> <booktitle> Number 118 in Lecture Notes in Statistics. </booktitle> <address> New York: </address> <publisher> Springer. </publisher>
Reference: <author> Neal, R. M., and Hinton, G. E. </author> <title> (1993) A new view of the EM algorithm that justifies incremental and other variants. </title> <journal> Biometrika. </journal> <note> submitted. </note>
Reference-contexts: The free energy minimization approach is like an EM algorithm (Dempster et al. 1977), in which we wish to find the most probable ff and do this by introducing an E-step in which a distribution over w is obtained <ref> (Neal and Hinton 1993) </ref>. This distribution takes into account the k fl ill-determined parameters by assigning each of them a variance of 2 w in the matrix .
Reference: <author> Reif, F. </author> <title> (1965) Fundamentals of statistical and thermal physics. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: For example, the most probable microstate under the canonical ensemble is always the ground state, for any temperature 1=fi 0; whereas its probability under the microcanonical ensemble is zero. But if the system has a large number of degrees of freedom, it is well known <ref> (Reif 1965) </ref> that for most macroscopic purposes, the two distributions are indistinguishable, because most of the probability mass of the canonical ensemble is concentrated in the states in a small interval around E.
Reference: <author> Ripley, B. D. </author> <title> (1995) Pattern Recognition and Neural Networks. </title> <publisher> Cambridge. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <title> (1986) Learning representations by back-propagating errors. </title> <booktitle> Nature 323: </booktitle> <pages> 533-536. </pages>
Reference-contexts: 1 The overfitting problem and hyperparameters in neural networks Feedforward neural networks are often trained to solve regression and classification problems using algorithms that minimize an error function, a measure of goodness of fit to the training data <ref> (Rumelhart et al. 1986) </ref>. If nothing is done to control the complexity of the resulting neural network, an inevitable consequence of error-minimization will be overfitting | the neural network will learn a function which fits spurious details and noise in the data.
Reference: <author> Skilling, J. </author> <title> (1993) Bayesian numerical analysis. In Physics and Probability, </title> <editor> ed. by W. T. Grandy, Jr. and P. Milonni, </editor> <publisher> Cambridge. C.U.P. </publisher>
Reference: <author> Strauss, C. E. M., Wolpert, D. H., and Wolf, D. R. </author> <title> (1993) Alpha, evidence, and the entropic prior. In Maximum Entropy and Bayesian Methods, Paris 1992 , ed. by A. </title> <publisher> Mohammed-Djafari, Dordrecht. Kluwer. </publisher>
Reference: <author> Thodberg, H. H. </author> <title> (1993) Ace of Bayes: application of neural networks with pruning. </title> <type> Technical Report 1132 E, </type> <institution> Danish meat research institute. </institution>
Reference: <author> Wahba, G. </author> <title> (1975) A comparison of GCV and GML for choosing the smoothing parameter in the generalized spline smoothing problem. </title> <journal> Numer. Math. </journal> <volume> 24: </volume> <pages> 383-393. </pages> <note> 24 Weigend, </note> <author> A. S., Rumelhart, D. E., and Huberman, B. A. </author> <title> (1991) Generalization by weight-elimination with applica-tions to forecasting. </title> <booktitle> In Advances in Neural Information Processing Systems 3 , ed. </booktitle> <editor> by R. P. L. et. </editor> <booktitle> al., </booktitle> <pages> pp. 875-882. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The evidence is then maximized over the regularization constant ff and noise level fi. A Gaussian approximation is then made with the hyperparameters fixed to their optimized values. This relates closely to the `generalized maximum likelihood' or `MLII' method in statistics <ref> (Wahba 1975) </ref>. This method can be applied to nonlinear models by making appropriate local linearizations (so that the integral over the parameters is made approximately rather than exactly) and has been used successfully in image reconstruction (Gull 1989; Weir 1991) and in neural networks (MacKay 1992c; Thodberg 1993; MacKay 1996).
Reference: <author> Weir, N. </author> <title> (1991) Applications of maximum entropy techniques to HST data. </title> <booktitle> In Proceedings of the ESO/ST-ECF Data Analysis Workshop, </booktitle> <address> April 1991 , ed. </address> <note> by P. </note> <editor> Grosbol and R. </editor> <booktitle> Warmels, </booktitle> <pages> pp. 115-129, </pages> <institution> Garching. European Southern Observatory/Space Telescope - European Coordinating Facility. </institution>
Reference: <author> Wolpert, D. H. </author> <title> (1993) On the use of evidence in neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 5 , ed. </booktitle> <editor> by C. L. Giles, S. J. Hanson, and J. D. </editor> <booktitle> Cowan, </booktitle> <pages> pp. 539-546, </pages> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher> <month> November 19, </month> <note> 1997 | Version 3.4 25 </note>
References-found: 29

