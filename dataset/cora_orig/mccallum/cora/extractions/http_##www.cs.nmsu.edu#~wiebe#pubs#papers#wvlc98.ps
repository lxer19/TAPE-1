URL: http://www.cs.nmsu.edu/~wiebe/pubs/papers/wvlc98.ps
Refering-URL: http://www.cs.nmsu.edu/~wiebe/pubs/index.html
Root-URL: http://www.cs.nmsu.edu
Email: e-mail: wiebe, kmckeeve@cs.nmsu.edu  e-mail: bruce@cs.unca.edu  
Title: Mapping Collocational Properties into Machine Learning Features Proc. 6th Workshop on Very Large Corpora (WVLC-98)  
Author: Janyce M. Wiebey and Kenneth J. McKeevery and Rebecca F. Brucez 
Address: Las Cruces, NM 88003  Asheville, NC 28804-3299  
Affiliation: Department of Computer Science and the Computing Research Laboratory New Mexico State University  zDepartment of Computer Science University of North Carolina at Asheville  
Abstract: This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning. In experiments performing an event categorization task, Wiebe et al. (1997a) found that different organizations are best for different properties. This paper presents a statistical analysis of the results across different machine learning algorithms. In the experiments, the relationship between property and organization was strikingly consistent across algorithms. This prompted further analysis of this relationship, and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments. While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Badsberg, J. </author> <year> 1995. </year> <title> An Environment for Graphical Models. </title> <type> Ph.D. </type> <institution> diss., Aalborg University. </institution>
Reference: <author> Bishop, Y. M.; Fienberg, S.; and Holland, P. </author> <year> 1975. </year> <title> Discrete Multivariate Analysis: Theory and Practice. </title> <publisher> (Cambridge: The MIT Press). </publisher>
Reference-contexts: Specifically, the model of independence between each word w (when satisfying a constraint in S j ) and the classification variable is assessed, using the likelihood ratio statistic, G 2 <ref> (Bishop et al. 1975) </ref>. Those with the top N G 2 values, i.e., for which independence is a poor fit, are chosen 1 .
Reference: <author> Bruce, R.; Wiebe, J., and Pedersen, T. </author> <year> 1996. </year> <title> The measure of a model. </title> <booktitle> Proc. EMNLP-1, </booktitle> <pages> pp. 101-112. </pages>
Reference: <author> Bruce, R. and Wiebe, J. </author> <year> 1994. </year> <title> Word-Sense Disambiguation Using Decomposable Models. </title> <booktitle> Proc. 32nd Annual Meeting of the Assoc. for Comp. Linguistics (ACL-94), </booktitle> <pages> pp. 139-146. </pages>
Reference-contexts: et al. (1994) of the StatLog project comparing machine learning algorithms. (1) PEBLS, a K-Nearest Neighbor algorithm (Cost and Salzberg 1993); (2) C4.5, a decision tree algorithm (Quinlan 1994); (3) Ripper, an inductive rule based classifier (Cohen 1996); (4) the Naive Bayes classifier; and (5), a probabilistic model search procedure <ref> (Bruce & Wiebe 1994) </ref> using the public domain software CoCo (Bads-berg 1995). Linear discriminant classifiers are omitted because they are not appropriate for categorical data. Neural network classifiers are omitted as well. 6 Results machine learning algorithms on each combination of collocational property and feature organization.
Reference: <author> Chafe, Wallace. </author> <title> 1986 Evidentiality in English Conversation and Academic Writing. </title> <editor> In: Chafe, Wallace and Nichols, Johanna, Eds., Ev-identiality: </editor> <title> The Linguistic Coding of Epistemology. </title> <publisher> Ablex, </publisher> <address> Norwood, NJ: </address> <pages> 261-272. </pages>
Reference-contexts: The study of interaction between property and organization is presented in section 7. 2 The Event Categorization Task This work is part of a larger project on processing newspaper articles to support automatic segmentation and summarization. A fundamental component of reporting is evidentiality <ref> (Chafe 1986, van Dijk 1988) </ref>: What source does the reporter give for his information? Is the information being presented as fact, opinion, or speculation? Our end application is a segmentation of the text into factual and non-factual segments, to include in a document profile for summarization and retrieval.
Reference: <author> Cohen, W. </author> <year> 1996. </year> <title> Learning Trees and Rules with Set-Valued Features. </title> <booktitle> Proc. AAAI-96, </booktitle> <pages> pp. 709-717. </pages>
Reference-contexts: algorithms included in this study are representative of the major types suggested by Michie et al. (1994) of the StatLog project comparing machine learning algorithms. (1) PEBLS, a K-Nearest Neighbor algorithm (Cost and Salzberg 1993); (2) C4.5, a decision tree algorithm (Quinlan 1994); (3) Ripper, an inductive rule based classifier <ref> (Cohen 1996) </ref>; (4) the Naive Bayes classifier; and (5), a probabilistic model search procedure (Bruce & Wiebe 1994) using the public domain software CoCo (Bads-berg 1995). Linear discriminant classifiers are omitted because they are not appropriate for categorical data.
Reference: <author> Cost, S. and Salzberg, S. </author> <year> 1993. </year> <title> A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features, </title> <booktitle> Machine Learning 10 (1): </booktitle> <pages> 57-78. </pages> <editor> van Dijk, T.A. </editor> <year> (1988). </year> <title> News as Discourse. </title> <address> (Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum). </publisher>
Reference-contexts: number of collocation words to be increased without a corresponding increase in complexity. 5 The Machine Learning Algorithms The algorithms included in this study are representative of the major types suggested by Michie et al. (1994) of the StatLog project comparing machine learning algorithms. (1) PEBLS, a K-Nearest Neighbor algorithm <ref> (Cost and Salzberg 1993) </ref>; (2) C4.5, a decision tree algorithm (Quinlan 1994); (3) Ripper, an inductive rule based classifier (Cohen 1996); (4) the Naive Bayes classifier; and (5), a probabilistic model search procedure (Bruce & Wiebe 1994) using the public domain software CoCo (Bads-berg 1995).
Reference: <author> Gale, W.; Church, K.; and Yarowsky, D. </author> <year> 1992. </year>
References-found: 8

