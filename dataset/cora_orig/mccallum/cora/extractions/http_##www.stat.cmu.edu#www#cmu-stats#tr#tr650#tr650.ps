URL: http://www.stat.cmu.edu/www/cmu-stats/tr/tr650/tr650.ps
Refering-URL: 
Root-URL: 
Title: Model Selection for Consumer Loan Application Data  
Author: Herbert Lee 
Keyword: Bayes Factor, Model Uncertainty, Markov Chain Monte Carlo, Logistic Regression  
Date: April 26, 1996  
Abstract: Loan applications at banks are often long, requiring the applicant to provide large amounts of data. Is all of it necessary? Can we save the applicant some frustration and the bank some expense by using only a subset of the relevant variables? To answer this question, I have attempted to model the current loan approval process at a particular bank. I have used several model selection techniques for logistic regression, including stepwise regression, Occam's Window, Markov Chain Monte Carlo Model Composition (Raftery, Madigan, and Hoeting, 1993), and Bayesian Random Searching. The resulting models largely agree upon a subset of only one-third of the original variables. fl This paper was completed in partial fulfillment of the Ph.D. data analysis requirement. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Chambers, J. and Hastie, T. </author> <year> (1992). </year> <title> Statistical Models in S. Wadsworth & Brooks/Cole Advanced Books & Software, </title> <address> Pacific Grove, CA. </address>
Reference-contexts: Eventually I found an appealing model. Henceforth I will refer to this model as the "exploratory model". The next technique applied to this data set was stepwise model selection <ref> (Chambers and Hastie, 1992) </ref>. S-plus has a built-in function called "step.glm" which is a reasonably efficient routine to select variables based on the AIC.
Reference: <author> Clyde, M., DeSimone, H. and Parmigiani, G. </author> <year> (1994). </year> <title> Prediction via Orthogonalized Model Mixing. </title> <type> ISDS DP 94-32, </type> <institution> Duke University. </institution>
Reference-contexts: In addition to the more established techniques|tree-based methods, the Akaike Information Criterion (AIC), or the Bayesian Information Criterion (BIC or Schwarz criterion)|newer ideas include Oc-cam's Window and Markov Chain Monte Carlo Model Composition (MC 3 ) (Raftery et al., 1993), new approximations to Bayes Factors (Raftery, 1993), Orthogonalization <ref> (Clyde, DeS-imone, and Parmigiani, 1994) </ref>, and Stochastic Search Variable Selection (SSVS) (George and McCulloch, 1993). Since the point of this analysis is to model a process at a bank, interpretability of results is an important issue.
Reference: <author> Gelman, A., Carlin, J., Stern, H. and Rubin, D. </author> <year> (1995). </year> <title> Bayesian Data Analysis. </title> <publisher> Chapman and Hall, </publisher> <address> Cambridge. </address>
Reference-contexts: In this analysis, I used trees, AIC and BIC, Occam's Window, MC 3 , and a random search based on Bayes Factors. The resulting models were checked for fit and compared to each other using both cross-validation and posterior predictive p-values, a Bayesian goodness-of-fit test <ref> (Gelman et al., 1995) </ref>. All of the analyses were done in S-Plus. Many of the routines are either built-in or are available from Statlib. The data set is large (initially 9140 records), but some records contain missing data. <p> The proportion of matches for each model was computed. A newer Bayesian method for checking goodness of fit and prediction is posterior predictive p-values <ref> (Gelman et al., 1995) </ref>. The idea is to compare a test quantity to its posterior distribution under the model. Using the notation of Gelman et al., let T (y; ) be a test statistic (here we are interested in goodness of fit).
Reference: <author> George, E. and McCulloch, R. </author> <year> (1993). </year> <title> Variable Selection Via Gibbs Sampling. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 88, </volume> <pages> 881-889. </pages>
Reference-contexts: Akaike Information Criterion (AIC), or the Bayesian Information Criterion (BIC or Schwarz criterion)|newer ideas include Oc-cam's Window and Markov Chain Monte Carlo Model Composition (MC 3 ) (Raftery et al., 1993), new approximations to Bayes Factors (Raftery, 1993), Orthogonalization (Clyde, DeS-imone, and Parmigiani, 1994), and Stochastic Search Variable Selection (SSVS) <ref> (George and McCulloch, 1993) </ref>. Since the point of this analysis is to model a process at a bank, interpretability of results is an important issue. Orthogonalization procedures may result in nice fits, but they leave the user with an uninterpretable transformation of the original parameters.
Reference: <author> Hastie, T. and Tibshirani, R. </author> <year> (1990). </year> <title> Generalized Additive Models. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: The distributions exhibit clustering near zero and a long right tail, both of which are reduced by the log transformation. To further investigate possible transformations, I fit several generalized additive models with smoothing splines using "gam" and "s" in S-Plus <ref> (Hastie and Tibshirani 1990) </ref>. The resulting models showed that log transformations were not unreasonable. 8 Only 21% of the applications had a co-applicant.
Reference: <author> Hoeting, J., Raftery, A. and Madigan, D. </author> <year> (1995). </year> <title> Simultaneous Variable and Transformation Selection in Linear Regression. </title> <note> Submitted to Statistics and Computing. </note>
Reference: <author> Kass, R. and Raftery, A. </author> <year> (1995). </year> <title> Bayes Factors [Review Paper]. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 90, </volume> <pages> 773-795. </pages>
Reference: <author> McCullagh, P. and Nelder, J. </author> <year> (1989). </year> <title> Generalized Linear Models. 2nd edition. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Raftery, A., Madigan, D. and Hoeting, J. </author> <year> (1993). </year> <title> Model Selection and Accounting for Model Uncertainty in Linear Regression Models. </title> <type> Technical Report 262, </type> <institution> Department of Statistics, University of Washington. </institution> <note> 28 Raftery, </note> <author> A. </author> <year> (1993). </year> <title> Approximate Bayes Factors and Accounting for Model Uncertainty in Generalized Linear Models. </title> <type> Technical Report 255, </type> <institution> Department of Statistics, University of Washington. </institution>
Reference-contexts: In addition to the more established techniques|tree-based methods, the Akaike Information Criterion (AIC), or the Bayesian Information Criterion (BIC or Schwarz criterion)|newer ideas include Oc-cam's Window and Markov Chain Monte Carlo Model Composition (MC 3 ) <ref> (Raftery et al., 1993) </ref>, new approximations to Bayes Factors (Raftery, 1993), Orthogonalization (Clyde, DeS-imone, and Parmigiani, 1994), and Stochastic Search Variable Selection (SSVS) (George and McCulloch, 1993). Since the point of this analysis is to model a process at a bank, interpretability of results is an important issue. <p> In addition to the more established techniques|tree-based methods, the Akaike Information Criterion (AIC), or the Bayesian Information Criterion (BIC or Schwarz criterion)|newer ideas include Oc-cam's Window and Markov Chain Monte Carlo Model Composition (MC 3 ) (Raftery et al., 1993), new approximations to Bayes Factors <ref> (Raftery, 1993) </ref>, Orthogonalization (Clyde, DeS-imone, and Parmigiani, 1994), and Stochastic Search Variable Selection (SSVS) (George and McCulloch, 1993). Since the point of this analysis is to model a process at a bank, interpretability of results is an important issue. <p> They were designed in the context of Bayesian Model Averaging, where to find the posterior distribution of a quantity of interest, one averages over the posterior distributions given each of the available models, weighting the average by the posterior probabilities of each model <ref> (Raftery, Madigan and Hoeting 1993) </ref>. Occam's Window and Markov Chain Monte Carlo Model Composition were designed at the University of Washington and are more fully described in the aforementioned paper. The Bayesian Random Search is a modification of MC 3 which should be more accurate. <p> So the first two Bayesian models are almost identical. For all four models, there is high agreement on AIC, BIC, cross-validation score, and posterior predictive p-value. While the fact that no model dominates in terms of posterior probability suggests that one should consider Bayesian Model Averaging <ref> (Raftery et al., 1993) </ref>, I will argue against 24 it here for two reasons. First, the two best models are almost identical, and the top four models are very similar. Thus the predictions should not vary by much between the models.
Reference: <author> Raftery, A. </author> <year> (1994). </year> <title> Bayesian Model Selection in Social Research. </title> <note> To be published in Sociological Methodology. </note>
Reference-contexts: Raftery (1993) describes several approximations to Bayes Factors, including the BIC. He has submitted to Statlib S-plus code called "bic.logit" which implements Occam's Window using the BIC approximation for Bayes Factors <ref> (Raftery 1994) </ref>. I ran this code and will refer to the top models selected as the "Bayes models". <p> Raftery gives a detailed justification for the choice of 1.65 = e :5 , and the other choices of prior parameters are rather intuitive. Statlib contains S-plus code by Raftery, called "glib", for computing Bayes Factors and posterior distribution quantities using this approximation <ref> (Raftery 1994) </ref>. I have used sections of this code in my analysis. MC 3 may be simulation-consistent, but I am not aware of any mention in the literature of how long the simulation needs to run to reach its equilibrium state.
Reference: <author> Schwarz, G. </author> <year> (1978). </year> <title> Estimating the Dimension of a Model. </title> <journal> Annals of Statistics, </journal> <volume> 6, </volume> <pages> 461-464. </pages>
Reference-contexts: a first step in model selection for logistic regression, I tried a number of different models, looked at t-statistics for coefficients, deviance reductions, and AIC and BIC values, where AIC = deviance + 2 fl number of parameters, and BIC = deviance + number of parameters fl log (sample size) <ref> (Schwarz 1978) </ref>. Eventually I found an appealing model. Henceforth I will refer to this model as the "exploratory model". The next technique applied to this data set was stepwise model selection (Chambers and Hastie, 1992). <p> Thus I ran a backward stepwise regression to achieve the model I will refer to as the "stepwise AIC model". As the AIC is known to be inconsistent for model selection, while the BIC is consistent <ref> (Schwarz 1978) </ref>, I modified the stepwise glm algorithm to use the BIC to test for model improvement. One expects the resulting model to have fewer parameters than the stepwise AIC, since the penalty for including additional terms in the model is larger for the BIC.

References-found: 11

