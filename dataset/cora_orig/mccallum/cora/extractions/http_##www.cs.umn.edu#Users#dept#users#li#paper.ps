URL: http://www.cs.umn.edu/Users/dept/users/li/paper.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/li/
Root-URL: http://www.cs.umn.edu
Email: ftnnguyen,lig@cs.umn.edu  
Title: Interprocedural Analysis to Support Static Scheduling of Parallel Loops and Data Allocation  
Author: Trung N. Nguyen Zhiyuan Li 
Address: 200 Union St. SE Minneapolis, MN 55455  
Affiliation: Department of Computer Science University of Minnesota  
Abstract: We present an interprocedural program analysis to support static loop scheduling and data allocation with the objective of reducing remote memory references on CC-NUMA multiprocessors. Given a program which consists of parallel regions in the form of DOALL loops and sequential regions, we build an interprocedural control flow graph and annotate it with array reference information. We then construct a few decision graphs which represent the potential penalty, in terms of the number of remote memory references, associated with misaligned assignments of DOALL loop iterations and array data to the processors. These graphs are then reduced to a subgraph which aligns the DOALL loop iterations as well as the array data. The number of remote references is much reduced compared with misaligned assignments. Experiments performed in a research parallelizing compiler indicate that the proposed interprocedural analysis can be performed efficiently. Moreover, simulation results suggest that, with the support of the proposed analysis, static loop scheduling can achieve much better parallel execution speed over dynamic scheduling for parallel loops with regular workload. fl This work is supported in part by the National Science Foundation, Grants NSF/CCR-9210913 and NSF/CCR-950254. 
Abstract-found: 1
Intro-found: 1
Reference: [AKN93] <author> A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Automatic partitioning of parallel loops and data arrays for distributed shared memory multiprocessors. </title> <booktitle> In Proc. International Conference on Parallel Processing, volume I: Architecture, </booktitle> <pages> pages 2-11, </pages> <address> St. Charles, IL, </address> <year> 1993. </year>
Reference-contexts: Likewise, processors in the systems are mapped to a processor space. By solving several potentially large systems of equations, array data and loop iterations are mapped to processors. Agarwal et al. also model this problem in a linear algebra framework <ref> [AKN93] </ref>. Loop iterations and array data are mapped to a hyperplane. The loop iterations are then partitioned into optimal hyperparallelepiped tiles that minimize the interprocessor communication. This scheme, however, only considers a single loop nest. Li and Pingali take a different approach to the assignment problem [LP93].
Reference: [AL93] <author> J. M. Anderson and M. S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proc. ACM SIGPLAN Conf. on Prog. Lang. Design and Imp., </booktitle> <pages> pages 112-125, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: It has been shown that static scheduling can enhance data locality by assigning loop iterations which frequently access the same memory locations to the same processor <ref> [AL93, JD92, SG91, LC90, HS92, KKBP91, Tse89] </ref>. On cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data allocation can also be performed in conjunction with static loop scheduling to increase the chance of local memory references in the event of cache misses. <p> In Section 6, we present simulation results and summarize this work. 2 Related Work Several researchers have proposed algorithms for allocating data and parallel loop iterations on CC-NUMA architectures. Anderson and Lam model this problem in a linear algebra framework <ref> [AL93] </ref>. They assume that loop bounds and array subscripts are affine functions of the loop indices. Parallel loop iterations are mapped to a polytope in an l-dimensional iteration space, where l is the depth of a parallel loop nest. An n-dimensional array is mapped to an n-dimensional rectangular space. <p> For our purpose, we regard each VP as having its own memory module as well. Concurrent with the assignment of parallel loop iterations, we allocate array data to memory modules, similar to the strategy used by several previous works <ref> [AL93, RS91] </ref>. For the sake of discussions, we assume in this paper that all DOALL loops are normalized to have lower loop limits of one and index steps of one. Similarly, we assume the lower bound of each array dimension to be one. <p> The graph construction algorithms presented in this work can be extended to accommodate a wider class of array references if such extension is called for in practice. As Anderson and Lam pointed out <ref> [AL93] </ref>, the alignment problem can be broken down into two optimization phases: 1. Phase I: Optimizing the orientation of the data allocation and the assignment of the parallel iterations. Suppose the VP's are numbered from 1 to P . <p> If a displacement is small, then its effect is not as important as choosing a good orientation because its effect usually diminishes due to large data cache lines in practice. 4.1 Constructing the connection graph Before starting the two optimizing phases, we create a connection graph <ref> [AL93] </ref> to allow the compiler to know which DOALL loops and arrays should be considered together. A connection graph is denoted by CG (d; a; e) where d and a are two sets of vertices and e is a set of undirected edges.
Reference: [BCK + 89] <author> M. Berry, D. Chen, P. Koss, D. Kuck, and S. Lo. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report CSRD-827, </type> <institution> University of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: In the current implementation of the alignment algorithm, the first two cases are handled. The last case which requires dividing an array is not yet implemented. If such a case is encountered (as in mgrid and some programs in the Perfect benchmarking suite <ref> [BCK + 89] </ref>), our current implementation ignores these array references. 6 Simulation Results and Summary Through execution-driven simulations, we examine the effect of static scheduling of parallel loops and data allocation on a CC-NUMA architecture.
Reference: [CHK92] <author> K. Cooper, M. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proc. IEEE International Conference on Computer Language, </booktitle> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Hence, the memory location represented by a formal parameter may vary from one invocation of the subroutine to another. Subroutine in-lining [CHT91, CHT92, Hol91] can eliminate all formal parameters, and subroutine cloning <ref> [CHK92] </ref> can result in a unique virtual address for every formal parameter. However, in-lining and cloning may make the resulting object code overly large and, thus, are performed to limited extent in practice.
Reference: [CHT91] <author> K. Cooper, M. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software-Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In this example, A and B map to X and Y respectively at the first call statement and to C and D respectively at the second call statement. Hence, the memory location represented by a formal parameter may vary from one invocation of the subroutine to another. Subroutine in-lining <ref> [CHT91, CHT92, Hol91] </ref> can eliminate all formal parameters, and subroutine cloning [CHK92] can result in a unique virtual address for every formal parameter. However, in-lining and cloning may make the resulting object code overly large and, thus, are performed to limited extent in practice.
Reference: [CHT92] <author> K. Cooper, M. Hall, and L. Torczon. </author> <title> Unexpected side effects of inline substitution a case study. </title> <journal> Letters on Programming Languages and Systems, </journal> <volume> 1(1), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: In this example, A and B map to X and Y respectively at the first call statement and to C and D respectively at the second call statement. Hence, the memory location represented by a formal parameter may vary from one invocation of the subroutine to another. Subroutine in-lining <ref> [CHT91, CHT92, Hol91] </ref> can eliminate all formal parameters, and subroutine cloning [CHK92] can result in a unique virtual address for every formal parameter. However, in-lining and cloning may make the resulting object code overly large and, thus, are performed to limited extent in practice.
Reference: [ENBT94] <author> G. W. Elsesser, V. N. Ngo, S. Bhattacharya, and W.-T. Tsai. </author> <title> Processor preallocation and load balancing of doall loops. </title> <journal> Journal of Supercomputing, </journal> <volume> 8 </volume> <pages> 135-161, </pages> <year> 1994. </year>
Reference-contexts: However, many parallel loops in well-known benchmarking programs are found to have little variance in the workload of different iterations and such loops can be recognized by the compiler <ref> [ENBT94, LN94] </ref>. This paper presents an interprocedural program analysis to support static scheduling of parallel loops and data allocation on cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors.
Reference: [FYTZ87] <author> Z. Fang, P. Yew, P. Tang, and C. Zhu. </author> <title> Dynamic processor self-scheduling for general parallel nested loops. </title> <booktitle> In 1987 Int. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1987. </year>
Reference-contexts: Moreover, dynamic scheduling is known to incur a heavier scheduling overhead due to its need to access and to modify a global iteration queue. Dynamic scheduling, on the other hand, can potentially achieve a more balanced workload among the processors <ref> [KW85, HSF92, PK87, FYTZ87, TN91, ML92, LTSS93] </ref>. In applications where the workload is irregularly distributed among the parallel loop iterations, it is quite possible that a program's total execution time can be shorter under dynamic scheduling.
Reference: [GLL97] <author> J. Gu, Z. Li, and G. Lee. </author> <title> Experience with efficient array data-flow analysis for array privatization. </title> <booktitle> In Proc. 6th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: The misalignment penalty is estimated statically. Each graph is then reduced by rejecting assignment decisions which incur high penalties. We perform experiments within an advanced interprocedural parallelizing compiler, Panorama <ref> [GLL97] </ref>, and conduct execution-driven simulations using a detailed CC-NUMA simulator. The techniques developed in this work can be applied to bus-based multiprocessors as well. In the rest of this paper, we first review related work in Section 2 and give an overview of Panorama in Section 3. <p> Instead, we construct decision graphs and reduce such graphs based on misalignment penalty information which is collected interprocedurally. We present statistics regarding such graphs. 3 3 The Panorama parallelizing compiler Panorama is a source-to-source parallelizing Fortran compiler <ref> [GLL97] </ref> developed at the University of Minnesota. It uses a hierarchical supergraph (HSG), similar to the HSCG of the the PIPS project [IJT91], to represent program control flow both intraprocedurally and interpro-cedurally. <p> As far as we know, the use of an interprocedural control flow graph and its associated array reference information for static loop scheduling and data allocation is unique to this work. Panorama performs efficient interprocedural array data flow analysis to enable array priva-tization and loop parallelization <ref> [GLL97] </ref>. Loop compound nodes are marked as DOALL loops if the represented loops do not contain loop-carried dependences after program transformations. A set of symbolic evaluation utility routines are implemented to support array data flow analysis.
Reference: [Hol91] <author> A. Holler. </author> <title> A Study of the Effects of Subprogram Inlining. </title> <type> PhD thesis, </type> <institution> University of Virginia, </institution> <address> Charlottesville, VA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: In this example, A and B map to X and Y respectively at the first call statement and to C and D respectively at the second call statement. Hence, the memory location represented by a formal parameter may vary from one invocation of the subroutine to another. Subroutine in-lining <ref> [CHT91, CHT92, Hol91] </ref> can eliminate all formal parameters, and subroutine cloning [CHK92] can result in a unique virtual address for every formal parameter. However, in-lining and cloning may make the resulting object code overly large and, thus, are performed to limited extent in practice.
Reference: [HS92] <author> C.H. Huang and P. Sadayappan. </author> <title> Communication-free hyperplane positioning of nested loops. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 186-200. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <month> 33 </month>
Reference-contexts: It has been shown that static scheduling can enhance data locality by assigning loop iterations which frequently access the same memory locations to the same processor <ref> [AL93, JD92, SG91, LC90, HS92, KKBP91, Tse89] </ref>. On cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data allocation can also be performed in conjunction with static loop scheduling to increase the chance of local memory references in the event of cache misses.
Reference: [HSF92] <author> S. F. Hummel, E. Schonberg, and L. E. Flynn. </author> <title> Factoring: A method for scheduling parallel loops. </title> <journal> CACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Moreover, dynamic scheduling is known to incur a heavier scheduling overhead due to its need to access and to modify a global iteration queue. Dynamic scheduling, on the other hand, can potentially achieve a more balanced workload among the processors <ref> [KW85, HSF92, PK87, FYTZ87, TN91, ML92, LTSS93] </ref>. In applications where the workload is irregularly distributed among the parallel loop iterations, it is quite possible that a program's total execution time can be shorter under dynamic scheduling.
Reference: [IJT91] <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical interprocedural parallelization: An overview of the pips project. </title> <booktitle> In Proc. Int. Conf. on Supercomputing, </booktitle> <pages> pages 244-251, </pages> <year> 1991. </year>
Reference-contexts: We present statistics regarding such graphs. 3 3 The Panorama parallelizing compiler Panorama is a source-to-source parallelizing Fortran compiler [GLL97] developed at the University of Minnesota. It uses a hierarchical supergraph (HSG), similar to the HSCG of the the PIPS project <ref> [IJT91] </ref>, to represent program control flow both intraprocedurally and interpro-cedurally. The HSG provides sufficient interprocedural information to the compiler such that static scheduling and data allocation can be coordinated across procedure boundaries. The HSG is a composition of the control flow subgraphs of all routines in a program.
Reference: [Inc] <author> Silicon Graphics, Inc. </author> <title> POWER Fortran Accelerator User's Guide. </title>
Reference-contexts: Static scheduling methods perform such assignment at compile time, while dynamic scheduling methods leave the assignment decision to program execution time. The Silicon Graphics Challenge multiprocessor, for example, provides two static scheduling methods, simple and interleave, and two dynamic scheduling methods, gss and dynamic <ref> [Inc] </ref>. The simple method divides the loop iterations by the number of processors and then assigns each chunk of consecutive iterations to one processor.
Reference: [JD92] <author> Y. Ju and H. Dietz. </author> <title> Reduction of cache coherence overhead by compiler data layout and loop transformation. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 344-358. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: It has been shown that static scheduling can enhance data locality by assigning loop iterations which frequently access the same memory locations to the same processor <ref> [AL93, JD92, SG91, LC90, HS92, KKBP91, Tse89] </ref>. On cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data allocation can also be performed in conjunction with static loop scheduling to increase the chance of local memory references in the event of cache misses.
Reference: [KH92] <author> R. E. Kessler and M. D. Hill. </author> <title> Page placement algorithms for large real-indixed caches. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 10(4), </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: Using this information, the simulator changes the virtual addresses of array data on the fly. In addition, a page-coloring page mapping scheme <ref> [KH92] </ref> is adopted for the mapping from a virtual page to a physical page.
Reference: [KK95] <author> K. Kennedy and U. Kremer. </author> <title> Automatic data layout for High Performance Fortran. </title> <booktitle> In Proc. Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: This access normalization process transforms the loop nest into a new loop nest with better data locality. Recent work by Kremer et al. uses integer programming to allocate data on distributed memory multicomputers <ref> [KK95] </ref>. Compared to previous work, our work emphasizes the aspect of interprocedural analysis to support the alignment of parallel loop iterations and the allocation of array data, taking advantage of the interprocedural control flow graph and the associated array reference information in Panorama.
Reference: [KKBP91] <author> D. Kulkarni, K.G. Kuman, A. Basu, and A. Paulraj. </author> <title> Loop partitioning for distributed memory multiprocessors as unimodular transformations. </title> <booktitle> In ACM Int. Conf. on Supercomputing, </booktitle> <pages> pages 206-215, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: It has been shown that static scheduling can enhance data locality by assigning loop iterations which frequently access the same memory locations to the same processor <ref> [AL93, JD92, SG91, LC90, HS92, KKBP91, Tse89] </ref>. On cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data allocation can also be performed in conjunction with static loop scheduling to increase the chance of local memory references in the event of cache misses.
Reference: [KW85] <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-11(10):1001-1016, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: Moreover, dynamic scheduling is known to incur a heavier scheduling overhead due to its need to access and to modify a global iteration queue. Dynamic scheduling, on the other hand, can potentially achieve a more balanced workload among the processors <ref> [KW85, HSF92, PK87, FYTZ87, TN91, ML92, LTSS93] </ref>. In applications where the workload is irregularly distributed among the parallel loop iterations, it is quite possible that a program's total execution time can be shorter under dynamic scheduling.
Reference: [LC90] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers '90: The Third Sym. on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 424-432. </pages> <publisher> IEEE, </publisher> <month> October </month> <year> 1990. </year>
Reference-contexts: It has been shown that static scheduling can enhance data locality by assigning loop iterations which frequently access the same memory locations to the same processor <ref> [AL93, JD92, SG91, LC90, HS92, KKBP91, Tse89] </ref>. On cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data allocation can also be performed in conjunction with static loop scheduling to increase the chance of local memory references in the event of cache misses.
Reference: [LC91] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> J. Par. and Dist. Computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year>
Reference-contexts: The orientation graph augments the component affinity graph of J. Li and M. Chen <ref> [LC91] </ref> by adding nodes which represent parallel loops to handle loop iteration assignment. We also introduce a "demand-driven" algorithm for the construction of the OG, which reduces the size of the graph, an important consideration when we analyze large programs.
Reference: [LN94] <author> Z. Li and T. N. Nguyen. </author> <title> An empricial study of the work load distribution under static scheduling. </title> <booktitle> In Proc. International Conference on Parallel Processing, volume II: Software, </booktitle> <address> St. Charles, IL, </address> <year> 1994. </year>
Reference-contexts: However, many parallel loops in well-known benchmarking programs are found to have little variance in the workload of different iterations and such loops can be recognized by the compiler <ref> [ENBT94, LN94] </ref>. This paper presents an interprocedural program analysis to support static scheduling of parallel loops and data allocation on cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors.
Reference: [LP93] <author> W. Li and K. Pingali. </author> <title> Access normalization: Loop restructuring for NUMA computers. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 11(4), </volume> <month> November </month> <year> 1993. </year> <month> 34 </month>
Reference-contexts: Loop iterations and array data are mapped to a hyperplane. The loop iterations are then partitioned into optimal hyperparallelepiped tiles that minimize the interprocessor communication. This scheme, however, only considers a single loop nest. Li and Pingali take a different approach to the assignment problem <ref> [LP93] </ref>. Access patterns of arrays in each loop nest are summarized in a data access matrix. After calculating or estimating an invertible matrix of the data access matrix, they use it as a basis for transforming and normalizing the loop nest.
Reference: [LTSS93] <author> H. Li, S. Tandri, M. Stumm, and K. C. Sevcik. </author> <title> Locality and loop scheduling on NUMA multiprocessors. </title> <booktitle> In Proc. International Conference on Parallel Processing, volume II: Software, </booktitle> <pages> pages 140-147, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Moreover, dynamic scheduling is known to incur a heavier scheduling overhead due to its need to access and to modify a global iteration queue. Dynamic scheduling, on the other hand, can potentially achieve a more balanced workload among the processors <ref> [KW85, HSF92, PK87, FYTZ87, TN91, ML92, LTSS93] </ref>. In applications where the workload is irregularly distributed among the parallel loop iterations, it is quite possible that a program's total execution time can be shorter under dynamic scheduling.
Reference: [ML92] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <booktitle> In Supercomputing 92, </booktitle> <pages> pages 104-113, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Moreover, dynamic scheduling is known to incur a heavier scheduling overhead due to its need to access and to modify a global iteration queue. Dynamic scheduling, on the other hand, can potentially achieve a more balanced workload among the processors <ref> [KW85, HSF92, PK87, FYTZ87, TN91, ML92, LTSS93] </ref>. In applications where the workload is irregularly distributed among the parallel loop iterations, it is quite possible that a program's total execution time can be shorter under dynamic scheduling.
Reference: [Ngu96] <author> T. N. Nguyen. </author> <title> Interprocedural compiler analysis for reducing memory latency. </title> <type> Technical report, PhD thesis, </type> <institution> University of Minnesota, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: This problem is NP-hard and we use a heuristics for its solution. The heuristics itself is not the emphasis of this paper and the interested readers are referred to <ref> [Ngu96] </ref>. Obviously, if we include all possible orientation nodes for each array or each parallel loop in the OG, then there exist a number of possible SOG's which are all equivalent. <p> The shaded nodes in Figure 7 show a final SDG. We use a linear-time heuristic algorithm to find a SDG <ref> [Ngu96] </ref>, but we leave out the algorithm because it is not the focus of this paper. 4.4 Estimating the mismatch costs The mismatch cost associated with each edge is the main factor in deciding what nodes are included in SOG and SDG. <p> The computation is dominated by the construction of DG whose time complexity was discussed earlier. (Our heuristic algorithm for finding the SOG's and the SDG's is a linear algorithm with respect to the number of nodes and edges <ref> [Ngu96] </ref>.) Table 1 shows the size of the connection graph and the number of connected components found in the connection graph for several numerical programs. We will give more descriptions of some of these programs in Section 6. <p> These results are intended as an overview to show that, as expected, alignment of parallel loops and array data can improve the performance considerably. More detailed discussions can be found in <ref> [Ngu96] </ref>. The Panorama parallelizing compiler (c.f. Section 3) is used to parallelize and instrument the benchmarking programs for the experiments. It also provides the framework for static scheduling and data allocation. <p> This process continues until the interpreted code runs to stop. Since Panorama is a source-to-source compiler, it does not generate virtual addresses for the machine code. Instead, in the NUMAsim simulator, we re-map the virtual addresses generated by f77 to reflect the selected data allocation scheme <ref> [Ngu96] </ref>. As mentioned previously, Panorama inserts directives to allow the simulator to identify the starting address and the size of each array and what data allocation scheme to be used for that array. Using this information, the simulator changes the virtual addresses of array data on the fly.
Reference: [PFTV89] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes: The Art of Scientific Computing (Fortran Version). </title> <publisher> Cambridge University Press, </publisher> <year> 1989. </year>
Reference-contexts: These programs are chosen for their relatively high degree of parallelism and their reasonable simulation time. This section provides a short descriptions of these test programs. ADI is a numerical algorithm used to solve partial differential equations <ref> [PFTV89] </ref>. The main data in ADI is nine 256x256 arrays of double words. There are two main parallel loop nests with 254 iterations at each level. One array is swept along the columns in one loop nest and then along the rows in the other.
Reference: [PK87] <author> C. Polychronopoulos and D. J. Kuck. </author> <title> Guided self scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: With dynamic scheduling, the iterations are also divided into CHUNK-sized chunks. As each processor finishes a chunk, however, it enters a critical section to grab the next available chunk. With gss scheduling <ref> [PK87] </ref> , the chunk size is varied, depending on the number of iterations remaining. The execution speed of a parallel loop on shared memory machines can be affected by several factors, such as program parallelism, data locality, scheduling overhead, and load balance. <p> Moreover, dynamic scheduling is known to incur a heavier scheduling overhead due to its need to access and to modify a global iteration queue. Dynamic scheduling, on the other hand, can potentially achieve a more balanced workload among the processors <ref> [KW85, HSF92, PK87, FYTZ87, TN91, ML92, LTSS93] </ref>. In applications where the workload is irregularly distributed among the parallel loop iterations, it is quite possible that a program's total execution time can be shorter under dynamic scheduling.
Reference: [RS91] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-time techniques for data distribution in distributed memory machines. </title> <journal> IEEE Trans. on Par. and Dist. Systems, </journal> <volume> 2(4), </volume> <year> 1991. </year>
Reference-contexts: For our purpose, we regard each VP as having its own memory module as well. Concurrent with the assignment of parallel loop iterations, we allocate array data to memory modules, similar to the strategy used by several previous works <ref> [AL93, RS91] </ref>. For the sake of discussions, we assume in this paper that all DOALL loops are normalized to have lower loop limits of one and index steps of one. Similarly, we assume the lower bound of each array dimension to be one.
Reference: [SG91] <author> V. Sarkar and G. R. Gao. </author> <title> Optimization of array accesses by collective loop transformations. </title> <booktitle> In ACM Int. Conf.s on Supercomputing, </booktitle> <pages> pages 194-204, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: It has been shown that static scheduling can enhance data locality by assigning loop iterations which frequently access the same memory locations to the same processor <ref> [AL93, JD92, SG91, LC90, HS92, KKBP91, Tse89] </ref>. On cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data allocation can also be performed in conjunction with static loop scheduling to increase the chance of local memory references in the event of cache misses.
Reference: [TN91] <author> Ten H. Tzen and Lionel M. Ni. </author> <title> Dynamic loop scheduling for shared memory multiprocessors. </title> <booktitle> In Int. Conf. on Parallel Processing, </booktitle> <pages> pages 247-250, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Moreover, dynamic scheduling is known to incur a heavier scheduling overhead due to its need to access and to modify a global iteration queue. Dynamic scheduling, on the other hand, can potentially achieve a more balanced workload among the processors <ref> [KW85, HSF92, PK87, FYTZ87, TN91, ML92, LTSS93] </ref>. In applications where the workload is irregularly distributed among the parallel loop iterations, it is quite possible that a program's total execution time can be shorter under dynamic scheduling.
Reference: [Tse89] <author> P.-S. Tseng. </author> <title> A parallelizing compiler for distributed memory parallel computers. </title> <type> Technical Report CMU-CS-91-121, PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: It has been shown that static scheduling can enhance data locality by assigning loop iterations which frequently access the same memory locations to the same processor <ref> [AL93, JD92, SG91, LC90, HS92, KKBP91, Tse89] </ref>. On cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data allocation can also be performed in conjunction with static loop scheduling to increase the chance of local memory references in the event of cache misses.
Reference: [VF93] <author> J. E. Veenstra and R. J. Fowler. </author> <title> MINT tutorial and user manual. </title> <type> Technical Report 452, </type> <institution> University of Rochester, </institution> <month> June </month> <year> 1993. </year> <institution> Dep. of Computer Science. </institution> <month> 35 </month>
Reference-contexts: The SGI Challenge provides run-time libraries which implement the loop scheduling methods mentioned in the introduction. The executable object code is simulated by NUMASim, an execution-driven CC-NUMA cache simulator. NUMAsim consists of two main parts: the MINT event generator front-end <ref> [VF93] </ref> and our detailed CC-NUMA cache simulator back-end. The MINT front-end interprets each instruction in the executable object code and schedules events such as loads and stores for the back-end to simulate.
References-found: 33

