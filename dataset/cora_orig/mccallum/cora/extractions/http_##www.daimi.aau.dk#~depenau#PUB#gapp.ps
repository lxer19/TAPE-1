URL: http://www.daimi.aau.dk/~depenau/PUB/gapp.ps
Refering-URL: http://www.daimi.aau.dk/~depenau/PUB/pub.html
Root-URL: http://www.daimi.aau.dk
Email: Email: jd.terma@login.dkuug.dk  Email: depenau@daimi.aau.dk  
Phone: Phone: +45 8622  Phone: +45 89423371,  
Title: The MS-oe Error Function The Scaled Conjugate Gradient with an "offset term" where E is
Author: Jan Depenau and Martin Moller and @w ij @E(U Li p @U Li (T p V Li )(g (U Li p @w ij @E(U Li p @U Li (T p V Li )(g (U Li oe)V 
Affiliation: DAIMI, Computer Science Department, Aarhus University,  
Address: 2000,  Ny Munkegade, Bldg. 540, DK-8000 Aarhus C  
Web: DK-8520 Lystrup  
Note: TERMA Elektronik AS, Hovmarken 4,  GF.1 Introduction p  ))V (L1)j (1)  (L1)j (2)  
Abstract: Training the output units of a neural network into saturations means that the derivative will be zero and that the training is caught in a local mimima, also called "flat spot". Adding a small offset term to the derivative in a back-propagation method, such as gradient descent and Quick-Prop, is known to eliminate flat spots and increase learning speed [Fahlman 88]. The same simple trick cannot be directly applied to second order methods like the Scaled Conjugate Gradient, (SCG) [Moller 93a], as they use not only the derivative but also the error function itself. By simply integrating the derivative which includes the offset term, a formulation of the error function that corresponds to this derivative is given. Being able to calculate the derivative as well as the actual error, the "offset approach" is now directly applicable in second order methods. An example of the SCG with offset is given. One of the big problems with training a neural network is that it is often caught in a local minimum. Using a Mean Square error function (MS) and standard gradient descent, the update of a weight w ij to an output unit i is proportional to the negative gradient given by: 
Abstract-found: 1
Intro-found: 0
Reference: [Depenau 94] <author> J. Depenau, </author> <title> Evaluation of the Cascade Correlation Algorithm, 1994, </title> <type> Technical Report, </type> <note> TERMA Elektronik AS, pp. 21. </note>
Reference: [Fahlman 88] <author> S. E. Fahlman, </author> <title> of Learning Speed in Back-Propagation Networks, </title> <year> 1988, </year> <month> CMU-CS-88-162. </month>
Reference: [Fahlman et al. 90] <author> S. E. Fahlman and C. Lebiere, </author> <booktitle> The Cascade-Correlation Learning Architecture, in Neural Information Processing Systems 2, </booktitle> <editor> Editors D. Touretzky, </editor> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Denver, Colorado, </address> <year> 1990, </year> <pages> pp. 524-532. </pages>
Reference: [Moller et al. 94] <author> M. Moller and J. Depenau, </author> <title> Soft-Monotonic Error Functions, </title> <booktitle> Proceedings from the World Congress on Neural Network, </booktitle> <address> San Diego 1994, </address> <pages> pp 444-449. </pages>
Reference: [Moller 93a] <author> M. Moller, </author> <title> A Scaled Conjugate Gradient Algorithm for Fast Supervised Learning, Neural Networks, </title> <booktitle> 1993, </booktitle> <volume> 6(4): </volume> <pages> pp 525-533. </pages>
Reference: [Moller 93b] <author> M. Moller, </author> <title> Supervised Learning on Large redundant Training Sets, </title> <journal> International Journal of Neural Systems, 1993, </journal> <volume> 4(1): </volume> <pages> pp 15-25. </pages>
Reference: [Prechelt 94] <author> L. Prechelt, </author> <title> Proben 1 A Set of Neural Network Benchmark Problems and Benchmark-ing Rules. , 1994, Found on connectionist ftp list. </title> <type> 665 </type>
References-found: 7

