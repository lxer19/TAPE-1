URL: http://www.cs.utexas.edu/users/lorenzo/corsi/cs395t/96F/papers/sosp95.ps
Refering-URL: http://www.cs.utexas.edu/users/lorenzo/corsi/cs395t/96F/fs.html
Root-URL: 
Title: Serverless Network File Systems  
Author: Thomas E. Anderson, Michael D. Dahlin, Jeanna M. Neefe, David A. Patterson, Drew S. Roselli, and Randolph Y. Wang 
Date: (1996).  
Note: Versions of this work appear in the 15th ACM Symposium on Operating Systems Principles (December 1995) and the ACM Transactions on Computer Systems  
Address: Berkeley  
Affiliation: Computer Science Division University of California at  
Abstract: Copyright 1995 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works, requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept, ACM Inc., 1515 Broadway, New York, NY 10036 USA, fax +1 (212) 869-0481, or &lt;permissions@acm.org&gt;. Abstract In this paper, we propose a new paradigm for network file system design, serverless network file systems. While traditional network file systems rely on a central server machine, a serverless system utilizes workstations cooperating as peers to provide all file system services. Any machine in the system can store, cache, or control any block of data. Our approach uses this location independence, in combination with fast local area networks, to provide better performance and scalability than traditional file systems. Further, because any machine in the system can assume the responsibilities of a failed component, our serverless design also provides high availability via redundant data storage. To demonstrate our approach, we have implemented a prototype serverless network file system called xFS. Preliminary performance measurements suggest that our architecture achieves its goal of scalability. For instance, in a 32-node xFS system with 32 active clients, each client receives nearly as much read or write throughput as it would see if it were the only active client. 
Abstract-found: 1
Intro-found: 1
Reference: [Ande95] <author> T. Anderson, D. Culler, D. Patterson, </author> <title> and the NOW team. A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 5464, </pages> <month> February </month> <year> 1995. </year> <month> 32 </month>
Reference-contexts: For instance, continuous media workloads will increase file system demands; even a few workstations simultaneously running video applications would swamp a traditional central server [Rash94]. Coordinated Networks of Workstations (NOWs) allow users to migrate jobs among many machines and also permit networked workstations to run parallel jobs <ref> [Doug91, Litz92, Ande95] </ref>. By increasing the peak processing power available to users, NOWs increase peak demands on the file system [Cyph93]. Unfortunately, current centralized file system designs fundamentally limit performance and availability since all read misses and all disk writes go through the central server.
Reference: [Bake91] <author> M. Baker, J. Hartman, M. Kupfer, K. Shirriff, and J. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proc. of the 13th Symp. on Operating Systems Principles, </booktitle> <pages> pages 198212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Next generation networks not only enable serverlessness, they require it by allowing applications to place increasing demands on the file system. The I/O demands of traditional applications have been increasing over time <ref> [Bake91] </ref>; new applications enabled by fast networks such as multimedia, process migration, and parallel processing will further pressure file systems to provide increased performance. For instance, continuous media workloads will increase file system demands; even a few workstations simultaneously running video applications would swamp a traditional central server [Rash94]. <p> Unfortunately, small writes are common in many environments <ref> [Bake91] </ref>, and larger caches increase the percentage of writes in disk workload mixes over time. We expect cooperative caching using workstation memory as a global cache to further this workload trend. <p> Distributing Utilization Status xFS assigns the burden of maintaining each segments utilization status to the client that wrote the segment. This approach provides parallelism by distributing the bookkeeping, and it provides good locality; because clients seldom write-share data <ref> [Bake91, Kist92, Blaz93] </ref> a clients writes usually affect only local segments utilization status. We simulated this policy to examine how well it reduced the overhead of maintaining utilization information.
Reference: [Bake92] <author> M. Baker, S. Asami, E. Deprit, J. Ousterhout, and M. Seltzer. </author> <title> Non-Volatile Memory for Fast, Reliable File Systems. </title> <booktitle> In ASPLOS-V, </booktitle> <pages> pages 1022, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: the simulator assumes that blocks survive in clients write buffers for 30 seconds or until overwritten, whichever is sooner; this assumption allows the simulated system to avoid communication more often than a real system since it does not account for segments that are written to disk early due to syncs <ref> [Bake92] </ref>. (Unfortunately, syncs are not visible in our Auspex traces.) Finally, under the Distributed policy, each client tracks the status of blocks that it writes so that it needs no network messages when modifying a block for which it was the last writer. <p> Of course, a key to good NFS server performance is to efficiently implement synchronous writes; our prototype does not yet exploit the non-volatile RAM optimization found in most commercial NFS servers <ref> [Bake92] </ref>, so for best performance, NFS clients should mount these partitions using the unsafe option to allow xFS to buffer writes in memory. 7. xFS Prototype This section describes the state of the xFS prototype as of August 1995 and presents preliminary performance results measured on a 32 node cluster of <p> These servers also use a Prestoserve NVRAM card that acts as a buffer for disk writes <ref> [Bake92] </ref>. We did not use an NVRAM buffer for the xFS machines, but xFSs log buffer provides similar performance benefits. 25 A high-speed, switched Myrinet network [Bode95] connects the machines.
Reference: [Bake94] <author> M. Baker. </author> <title> Fast Crash Recovery in Distributed File Systems. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: Manager Recovery Once the storage servers have recovered, the managers can recover their manager map, disk location metadata, and cache consistency metadata. Manager map recovery uses a consensus algorithm as described above for stripe group recovery. Cache consistency recovery relies on server-driven polling <ref> [Bake94, Nels88] </ref>: a recovering manager contacts the clients, and each client returns a list of the blocks that it is caching or for which it has write ownership from that managers portion of the index number space. <p> this can require a total of O (N 2 ) messages, each manager or client only needs to contact N storage server groups, and all of the managers and clients can proceed in parallel, provided that they take steps to avoid having many machines simultaneously contact the same storage server <ref> [Bake94] </ref>; we plan use randomization to accomplish this goal. Similar considerations apply to the phases where managers read their checkpoints, clients roll forward, and managers query clients for their cache consistency state. 22 6.
Reference: [Basu95] <author> A. Basu, V. Buch, W. Vogels, and T. von Eicken. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proc. of the 15th Symp. on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: In contrast, shared media networks such as Ethernet or FDDI allow only one client or server to transmit at a time. In addition, the move towards low latency network interfaces <ref> [vE92, Basu95] </ref> enables closer cooperation between 2 machines than has been possible in the past. The result is that a LAN can be used as an I/O backplane, harnessing physically distributed processors, memory, and disks into a single system.
Reference: [Birr93] <author> A. Birrell, A. Hisgen, C. Jerian, T. Mann, and G. Swart. </author> <title> The Echo Distributed File System. </title> <type> Technical Report 111, </type> <institution> Digital Equipment Corp. Systems Research Center, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Although this made sense on an Ethernet, on todays fast LANs fetching data from local disk can be an order of magnitude slower than from server memory or remote striped disk. Similarly, a central server represents a single point of failure, requiring server replication <ref> [Walk83, Kaza89, Pope90, Lisk91, Kist92, Birr93] </ref> for high availability. Replication increases the cost and complexity of central servers, and can also increase latency on writes since the system must replicate data at multiple servers.
Reference: [Blac95] <author> T. Blackwell, J. Harris, and M. Seltzer. </author> <title> Heuristic Cleaning Algorithms in Log-Structured File Systems. </title> <booktitle> In Proc. of the 1995 Winter USENIX, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: We do not expect checkpoints to ever limit performance. However, thorough future investigation will be needed to evaluate the impact of distributed cleaning under a wide range workloads; other researchers have measured sequential cleaning overheads from a few percent <ref> [Rose92, Blac95] </ref> to as much as 40% [Selt95], depending on the workload. Also, the current prototype implementation suffers from three inefficiencies, all of which we will attack in the future. 1.xFS is currently implemented as a set of user-level processes by redirecting vnode layer calls.
Reference: [Blau94] <author> M. Blaum, J. Brady, J. Bruck, and J. Menon. EVENODD: </author> <title> An Optimal Scheme for Tolerating Double Disk Failures in RAID Architectures. </title> <booktitle> In Proc. of the 21st Symp. on Computer Architecture, </booktitle> <pages> pages 245254, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: it can reconstruct the contents of a failed disk by taking the exclusive-OR of the remaining data blocks and the parity block. xFS uses single parity disk striping to achieve the same benefits; in the future we plan to cope with multiple workstation or disk failures using multiple parity blocks <ref> [Blau94] </ref>. RAIDs suffer from two limitations. <p> We plan to investigate using multiple parity fragments to allow recovery when there are multiple failures within a stripe group <ref> [Blau94] </ref>. Less widespread changes to xFS membership such as when an authorized machine asks to join the system, when a machine notifies the system that it is withdrawing, or when a machine cannot be contacted because of a crash or network failure trigger similar reconfiguration steps.
Reference: [Blaz93] <author> M. </author> <title> Blaze. Caching in Large-Scale Distributed File Systems. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Distributing Utilization Status xFS assigns the burden of maintaining each segments utilization status to the client that wrote the segment. This approach provides parallelism by distributing the bookkeeping, and it provides good locality; because clients seldom write-share data <ref> [Bake91, Kist92, Blaz93] </ref> a clients writes usually affect only local segments utilization status. We simulated this policy to examine how well it reduced the overhead of maintaining utilization information.
Reference: [Bode95] <author> N. Boden, D. Cohen, R. Felderman, A. Kulawik, C. Seitz, J. Seizovic, and W. Su. </author> <title> Myrinet A Gigabit-per-Second Local-Area Network. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 2936, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Three factors motivate our work on serverless network file systems: the opportunity provided by fast switched LANs, the expanding demands of users, and the fundamental limitations of central server systems. The recent introduction of switched local area networks such as ATM or Myrinet <ref> [Bode95] </ref> enables serverlessness by providing aggregate bandwidth that scales with the number of machines on the network. In contrast, shared media networks such as Ethernet or FDDI allow only one client or server to transmit at a time. <p> These servers also use a Prestoserve NVRAM card that acts as a buffer for disk writes [Bake92]. We did not use an NVRAM buffer for the xFS machines, but xFSs log buffer provides similar performance benefits. 25 A high-speed, switched Myrinet network <ref> [Bode95] </ref> connects the machines. Although each link of the physical network has a peak bandwidth of 80 MB/s, RPC and TCP/IP protocol overheads place a much lower limit on the throughput actually achieved [Keet95].
Reference: [Cabr91] <author> L. Cabrera and D. Long. Swift: </author> <title> A Storage Architecture for Large Objects. </title> <booktitle> In Proc. Eleventh Symp. on Mass Storage Systems, </booktitle> <pages> pages 123128, </pages> <month> Oct </month> <year> 1991. </year>
Reference-contexts: Other parallel systems have implemented redundant data storage intended for restricted workloads consisting entirely of large files, where per-file striping is appropriate and where large file accesses reduce stress on their centralized manager architectures. For instance, Swift <ref> [Cabr91] </ref> and SFS [LoVe93] provide redundant distributed data storage for parallel environments, and Tiger [Rash94] services multimedia workloads. TickerTAIP [Cao93], SNS [Lee95], and AutoRAID [Wilk95] implement RAID-derived storage systems.
Reference: [Cao93] <author> P. Cao, S. Lim, S. Venkataraman, and J. Wilkes. </author> <title> The TickerTAIP Parallel RAID Architecture. </title> <booktitle> In Proc. of the 20th Symp. on Computer Architecture, </booktitle> <pages> pages 5263, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For instance, Swift [Cabr91] and SFS [LoVe93] provide redundant distributed data storage for parallel environments, and Tiger [Rash94] services multimedia workloads. TickerTAIP <ref> [Cao93] </ref>, SNS [Lee95], and AutoRAID [Wilk95] implement RAID-derived storage systems. These systems could provide services similar to xFSs storage servers, but they would require serverless management to provide a scalable and highly available file system interface to augment their simpler disk block interfaces.
Reference: [Chai91] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In ASP-LOS-IV Proceedings, </booktitle> <pages> pages 224234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This paper makes two sets of contributions. First, xFS synthesizes a number of recent innovations that, taken together, provide a basis for serverless file system design. xFS relies on previous work in areas such as scalable cache consistency (DASH [Leno90] and Alewife <ref> [Chai91] </ref>), cooperative caching, disk striping (RAID and Zebra), and log structured file systems (Sprite LFS [Rose92] and BSD LFS [Selt93]). Second, in addition to borrowing techniques developed in other projects, we have refined them to work well in our serverless system. <p> Multiprocessor Cache Consistency Network file systems resemble multiprocessors in that both provide a uniform view of storage across the system, requiring both to track where blocks are cached. This information allows them to maintain cache consistency by invalidating stale cached copies. Multiprocessors such as DASH [Leno90] and Alewife <ref> [Chai91] </ref> scalably distribute this task by dividing the systems physical memory evenly among processors; each processor manages the cache consistency state for its own physical memory locations. 1 log across the storage servers. Clients compute parity for segments, not for individual files. Client Memories 1 2 3 . . .
Reference: [Chen94] <author> P. Chen, E. Lee, G. Gibson, R. Katz, and D. Patterson. </author> <title> RAID: High-Performance, Reliable Secondary Storage. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(2):145188, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: First, xFS dynamically distributes control processing across the system on a per-file granularity by utilizing a new serverless management scheme. Second, xFS distributes its data storage across storage server disks by implementing a software RAID <ref> [Patt88, Chen94] </ref> using log-based network striping similar to Zebras [Hart95]. Finally, xFS eliminates central server 3 caching by taking advantage of cooperative caching [Leff91, Dahl94b] to harvest portions of client memory as a large, global file cache. This paper makes two sets of contributions. <p> Finally, since xFS has evolved from our initial proposal [Wang93], we describe the relationship of the design presented here to previous versions of the xFS design. 2.1. RAID xFS exploits RAID-style disk striping to provide high performance and highly available disk storage <ref> [Patt88, Chen94] </ref>. A RAID partitions a stripe of data into N-1 data blocks and a parity block the exclusive-OR of the corresponding bits of the data blocks. It stores each data and parity block on a different disk. <p> To improve performance and availability when using large numbers of storage servers, rather than stripe each segment over all storage servers in the system, xFS implements stripe groups as have been proposed for large RAIDs <ref> [Chen94] </ref>.
Reference: [Corb93] <author> P. Corbett, S. Baylor, and D. Feitelson. </author> <title> Overview of the Vesta Parallel File System. Computer Architecture News, </title> <address> 21(5):714, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Related Work Section 2 discussed a number of projects that provide an important basis for xFS. This section describes several other efforts to build decentralized file systems and then discusses the dynamic management hierarchies used in some MPPs. Several file systems, such as CFS [Pier89], Bridge [Dibb89], and Vesta <ref> [Corb93] </ref>, distribute data over multiple storage servers to support parallel workloads; however, they lack mechanisms to provide availability across component failures.
Reference: [Cris91] <author> F. Cristian. </author> <title> Reaching Agreement on Processor Group Membership in Synchronous Distributed Systems. </title> <booktitle> Distributed Computing, </booktitle> <address> 4:175187, </address> <year> 1991. </year>
Reference-contexts: Each storage server recovers this information independently from a private checkpoint, so this stage can proceed in parallel across all storage servers. Storage servers next regenerate their stripe group map. First, the storage servers use a distributed consensus algorithm <ref> [Cris91, Ricc91, Schr91] </ref> to determine group membership and to elect a group leader.
Reference: [Cyph93] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural Requirements of Parallel Scientific Applications with Explicit Communication. </title> <booktitle> In Proc. of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 213, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Coordinated Networks of Workstations (NOWs) allow users to migrate jobs among many machines and also permit networked workstations to run parallel jobs [Doug91, Litz92, Ande95]. By increasing the peak processing power available to users, NOWs increase peak demands on the file system <ref> [Cyph93] </ref>. Unfortunately, current centralized file system designs fundamentally limit performance and availability since all read misses and all disk writes go through the central server. To address such performance limitations, users resort to costly schemes to try to scale these fundamentally unscalable file systems.
Reference: [Dahl94a] <author> M. Dahlin, C. Mather, R. Wang, T. Anderson, and D. Patterson. </author> <title> A Quantitative Analysis of Cache Policies for Scalable Network File Systems. </title> <booktitle> In Proc. of 1994 SIGMETRICS, </booktitle> <pages> pages 150160, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Further, as we show in Section 3.2.4, by directly controlling which machines manage which data, we can improve locality and reduce network communication. 2.5. Previous xFS Work The design of xFS has evolved considerably since our original proposal <ref> [Wang93, Dahl94a] </ref>. The original design stored all system data in client disk caches and managed cache consistency using a hierarchy of metadata servers rooted at a central server. Our new implementation eliminates client disk caching in favor of network striping to take advantage of high speed, switched LANs. <p> We examined management policies by simulating xFSs behavior under a seven day trace of 236 clients NFS accesses to an Auspex file server in the Berkeley Computer Science Division <ref> [Dahl94a] </ref>. We warmed the simulated caches through the first day of the trace and gathered statistics through the rest. Since we would expect other workloads to yield different results, evaluating a wider range of workloads remains important work. <p> Instead, data may be stored in any cache, and a hierarchy of directories allows any node to locate any data by searching successively higher and more globally-complete directories. While an early xFS study simulated the effect of a hierarchical approach to metadata for file systems <ref> [Dahl94a] </ref>, we now instead support location-independence using a manager-map-based approach for three reasons. First, our approach eliminates the root manager that must track all data; such a root would bottleneck performance and reduce availability.
Reference: [Dahl94b] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson. </author> <title> Cooperative Caching: Using Remote Client Memory to Improve File System Performance. </title> <booktitle> In Proc. of the First Symp. on Operating Systems Design and Implementation, </booktitle> <pages> pages 267 280, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Second, xFS distributes its data storage across storage server disks by implementing a software RAID [Patt88, Chen94] using log-based network striping similar to Zebras [Hart95]. Finally, xFS eliminates central server 3 caching by taking advantage of cooperative caching <ref> [Leff91, Dahl94b] </ref> to harvest portions of client memory as a large, global file cache. This paper makes two sets of contributions. <p> Moreover, our new design eliminates the central management server in favor of a distributed metadata manager to provide better scalability, locality, and availability. We have also previously examined cooperative caching using client memory as a global file cache via simulation <ref> [Dahl94b] </ref> and therefore focus only on the issues raised by integrating cooperative caching with the rest of the serverless system. 3. Serverless File Service The RAID, LFS, Zebra, and multiprocessor cache consistency work discussed in the previous section leaves three basic problems unsolved.
Reference: [Dibb89] <author> P. Dibble and M. Scott. </author> <title> Beyond Striping: The Bridge Multiprocessor File System. </title> <journal> Computer Architechture News, </journal> <volume> 17(5):3239, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: Related Work Section 2 discussed a number of projects that provide an important basis for xFS. This section describes several other efforts to build decentralized file systems and then discusses the dynamic management hierarchies used in some MPPs. Several file systems, such as CFS [Pier89], Bridge <ref> [Dibb89] </ref>, and Vesta [Corb93], distribute data over multiple storage servers to support parallel workloads; however, they lack mechanisms to provide availability across component failures.
Reference: [Doug91] <author> F. Douglis and J. Ousterhout. </author> <title> Transparent Process Migration: Design Alternatives and the Sprite Implementation. </title> <journal> Software: Practice and Experience, </journal> <volume> 21(7), </volume> <month> July </month> <year> 1991. </year> <month> 33 </month>
Reference-contexts: For instance, continuous media workloads will increase file system demands; even a few workstations simultaneously running video applications would swamp a traditional central server [Rash94]. Coordinated Networks of Workstations (NOWs) allow users to migrate jobs among many machines and also permit networked workstations to run parallel jobs <ref> [Doug91, Litz92, Ande95] </ref>. By increasing the peak processing power available to users, NOWs increase peak demands on the file system [Cyph93]. Unfortunately, current centralized file system designs fundamentally limit performance and availability since all read misses and all disk writes go through the central server.
Reference: [Hage92] <author> E. Hagersten, A. Landin, and S. </author> <title> Haridi. </title> <booktitle> DDMA Cache-Only Memory Architecture. IEEE Computer, </booktitle> <address> 25(9):4554, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: Due to a hardware failure, we ran this experiment with three groups of eight storage servers and 31 clients. 31 Several MPP designs have used dynamic hierarchies to avoid the fixed-home approach used in traditional directory-based MPPs. The KSR1 [Rost93] machine, based on the DDM proposal <ref> [Hage92] </ref>, avoids associating data with fixed home nodes. Instead, data may be stored in any cache, and a hierarchy of directories allows any node to locate any data by searching successively higher and more globally-complete directories.
Reference: [Hart95] <author> J. Hartman and J. Ousterhout. </author> <title> The Zebra Striped Network File System. </title> <journal> ACM Trans. on Computer Systems, </journal> <month> August </month> <year> 1995. </year>
Reference-contexts: First, xFS dynamically distributes control processing across the system on a per-file granularity by utilizing a new serverless management scheme. Second, xFS distributes its data storage across storage server disks by implementing a software RAID [Patt88, Chen94] using log-based network striping similar to Zebras <ref> [Hart95] </ref>. Finally, xFS eliminates central server 3 caching by taking advantage of cooperative caching [Leff91, Dahl94b] to harvest portions of client memory as a large, global file cache. This paper makes two sets of contributions. <p> Although Rosenblums original measurements found relatively low cleaner overheads, even a small overhead can make the cleaner a bottleneck in a distributed environment. Further, some workloads, such as transaction processing, incur larger cleaning overheads [Selt93, Selt95]. 2.3. Zebra Zebra <ref> [Hart95] </ref> provides a way to combine LFS and RAID so that both work well in a distributed environment. Zebra uses a software RAID on commodity hardware (workstation, disks, and networks) to address RAIDs cost disadvantage, and LFSs batched writes provide efficient access to a network RAID. <p> By leaving the obsolete entries in the map, xFS allows clients to read data previously written to the groups without first transferring the data from obsolete groups to current groups. Over time, the cleaner will move data from obsolete groups to current groups <ref> [Hart95] </ref>. When the cleaner removes the last block of live data from an obsolete group, xFS deletes its entry from the stripe group map. 12 3.2. System Operation This section describes how xFS uses the various maps we described in the previous section. <p> Managers use the deltas to update their imaps and index nodes as they do during normal operation; version numbers in the deltas allow managers to chronologically order different clients modifications to the same files <ref> [Hart95] </ref>. 5.1.3. Cleaner Recovery Clients checkpoint the segment utilization information needed by cleaners in standard xFS files, called s-files. Because these checkpoints are stored in standard files, they are automatically recovered by the storage server and manager phases of recovery.
Reference: [Howa88] <author> J. Howard, M. Kazar, S. Menees, D. Nichols, M. Satyanarayanan, R. Sidebotham, and M. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 6(1):5181, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: 1. Introduction A serverless network file system distributes storage, cache, and control over cooperating workstations. This approach contrasts with traditional file systems such as Netware [Majo94], NFS [Sand85], Andrew <ref> [Howa88] </ref>, and Sprite [Nels88] where a central server machine stores all data and satisfies all client cache misses. Such a central server is both a performance and reliability bottleneck. <p> It is also expensive, since it requires the (human) system manager to effectively become part of the file system moving users, volumes, and disks among servers to balance load. Finally, Andrew <ref> [Howa88] </ref> attempts to improve scalability by caching data on client disks. Although this made sense on an Ethernet, on todays fast LANs fetching data from local disk can be an order of magnitude slower than from server memory or remote striped disk. <p> Instead, the system locates the blocks of the ifile using manager checkpoints described in Section 5.1. 3.2.3. Cache Consistency xFS utilizes a token-based cache consistency scheme similar to Sprite [Nels88] and Andrew <ref> [Howa88] </ref> except that xFS manages consistency on a per-block rather than per-file basis. Before a client modifies a block, it must 14 acquire write ownership of that block. The client sends a message to the blocks manager. <p> This code provides xFSs interface to the Solaris v-node layer and also accesses the kernel buffer cache. We implemented the remaining three pieces of xFS as daemons outside of the kernel address space to facilitate debugging <ref> [Howa88] </ref>. If the xFS kernel module cannot satisfy a request using the buffer cache, then it sends the request to the client daemon. The client daemons provide the rest of xFSs functionality by accessing the manager daemons and the storage server daemons over the network. <p> We use the same hardware to compare xFS with two central-server architectures, NFS [Sand85] and AFS (a commercial version of the Andrew file system <ref> [Howa88] </ref>). We use NFS as our baseline system for practical reasons NFS is mature, widely available, and well-tuned, allowing easy comparison and a good frame of reference but its limitations with respect to scalability are well known [Howa88]. <p> architectures, NFS [Sand85] and AFS (a commercial version of the Andrew file system <ref> [Howa88] </ref>). We use NFS as our baseline system for practical reasons NFS is mature, widely available, and well-tuned, allowing easy comparison and a good frame of reference but its limitations with respect to scalability are well known [Howa88]. <p> We also use Satyanarayanans Andrew benchmark <ref> [Howa88] </ref> as a simple evaluation of application-level performance. In the future, we plan to compare the systems performance under more demanding applications. 7.3.1. Scalability Figures 8 through 10 illustrate the scalability of xFSs performance for large writes, large reads, and small writes. <p> A surprising result is that NFS outperforms AFS when there are a large number of clients; this is because in-memory file caches have grown dramatically since this comparison was first made <ref> [Howa88] </ref>, and the working set of the benchmark now fits in the NFS clients in-memory caches, reducing the benefit of AFSs on-disk caches. 7.3.2.
Reference: [Kaza89] <author> M. Kazar. Ubik: </author> <title> Replicated Servers Made Easy. </title> <booktitle> In Proc. of the Second Workshop on Workstation Operating Systems, </booktitle> <pages> pages 6067, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Although this made sense on an Ethernet, on todays fast LANs fetching data from local disk can be an order of magnitude slower than from server memory or remote striped disk. Similarly, a central server represents a single point of failure, requiring server replication <ref> [Walk83, Kaza89, Pope90, Lisk91, Kist92, Birr93] </ref> for high availability. Replication increases the cost and complexity of central servers, and can also increase latency on writes since the system must replicate data at multiple servers.
Reference: [Keet95] <author> K. Keeton, T. Anderson, and D. Patterson. </author> <title> LogP Quantified: The Case for Low-Overhead Local Area Networks. </title> <booktitle> In Proc. 1995 Hot Interconnects, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: Although each link of the physical network has a peak bandwidth of 80 MB/s, RPC and TCP/IP protocol overheads place a much lower limit on the throughput actually achieved <ref> [Keet95] </ref>. The throughput for fast networks such as the Myrinet depends heavily on the version and patch level of the Solaris operating system used. For our xFS measurements, we used a kernel that we compiled from the Solaris 2.4 source release.
Reference: [Kist92] <author> J. Kistler and M. Satyanarayanan. </author> <title> Disconnected Operation in the Coda File System. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 10(1):325, </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: Although this made sense on an Ethernet, on todays fast LANs fetching data from local disk can be an order of magnitude slower than from server memory or remote striped disk. Similarly, a central server represents a single point of failure, requiring server replication <ref> [Walk83, Kaza89, Pope90, Lisk91, Kist92, Birr93] </ref> for high availability. Replication increases the cost and complexity of central servers, and can also increase latency on writes since the system must replicate data at multiple servers. <p> Distributing Utilization Status xFS assigns the burden of maintaining each segments utilization status to the client that wrote the segment. This approach provides parallelism by distributing the bookkeeping, and it provides good locality; because clients seldom write-share data <ref> [Bake91, Kist92, Blaz93] </ref> a clients writes usually affect only local segments utilization status. We simulated this policy to examine how well it reduced the overhead of maintaining utilization information.
Reference: [Kubi93] <author> J. Kubiatowicz and A. Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proc. of the 7th Internat. Conf. on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: Similarly, xFS could be used within a group or department where fast LANs connect machines and where uniform system administration and physical building security allow machines to trust one another. A file system based on serverless principles would also be appropriate for scalable server architectures currently being researched <ref> [Kubi93, Kusk94] </ref>. Untrusted clients can also benefit from the scalable, reliable, and cost-effective file service provided by a core of xFS machines via a more restrictive protocol such as NFS. <p> Even in environments that do not trust all desktop machines, xFS could still be used within a trusted core of desktop machines and servers, among physically secure compute servers and file servers in a machine room, or within one of the parallel server architectures now being researched <ref> [Kubi93, Kusk94] </ref>. In these cases, the xFS core could still provide scalable, reliable, and cost-effective file service to less trusted fringe clients running more restrictive protocols. The downside is that the core system can not exploit the untrusted CPUs, memories, and disks located in the fringe.
Reference: [Kusk94] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proc. of the 21st Internat. Symp. on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Similarly, xFS could be used within a group or department where fast LANs connect machines and where uniform system administration and physical building security allow machines to trust one another. A file system based on serverless principles would also be appropriate for scalable server architectures currently being researched <ref> [Kubi93, Kusk94] </ref>. Untrusted clients can also benefit from the scalable, reliable, and cost-effective file service provided by a core of xFS machines via a more restrictive protocol such as NFS. <p> Even in environments that do not trust all desktop machines, xFS could still be used within a trusted core of desktop machines and servers, among physically secure compute servers and file servers in a machine room, or within one of the parallel server architectures now being researched <ref> [Kubi93, Kusk94] </ref>. In these cases, the xFS core could still provide scalable, reliable, and cost-effective file service to less trusted fringe clients running more restrictive protocols. The downside is that the core system can not exploit the untrusted CPUs, memories, and disks located in the fringe.
Reference: [Lee95] <author> E. Lee. </author> <title> Highly-Available, Scalable Network Storage. </title> <booktitle> In Proc. of COMPCON 95, </booktitle> <year> 1995. </year>
Reference-contexts: For instance, Swift [Cabr91] and SFS [LoVe93] provide redundant distributed data storage for parallel environments, and Tiger [Rash94] services multimedia workloads. TickerTAIP [Cao93], SNS <ref> [Lee95] </ref>, and AutoRAID [Wilk95] implement RAID-derived storage systems. These systems could provide services similar to xFSs storage servers, but they would require serverless management to provide a scalable and highly available file system interface to augment their simpler disk block interfaces.
Reference: [Leff91] <author> A. Leff, P. Yu, and J. Wolf. </author> <title> Policies for Efficient Memory Utilization in a Remote Caching Architecture. </title> <booktitle> In Proc. of the First Internat. Conf. on Parallel and Distributed Information Systems, </booktitle> <pages> pages 198207, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Second, xFS distributes its data storage across storage server disks by implementing a software RAID [Patt88, Chen94] using log-based network striping similar to Zebras [Hart95]. Finally, xFS eliminates central server 3 caching by taking advantage of cooperative caching <ref> [Leff91, Dahl94b] </ref> to harvest portions of client memory as a large, global file cache. This paper makes two sets of contributions.
Reference: [Leno90] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proc. of the 17th Internat. Symp. on Computer Architecture, </booktitle> <pages> pages 148159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This paper makes two sets of contributions. First, xFS synthesizes a number of recent innovations that, taken together, provide a basis for serverless file system design. xFS relies on previous work in areas such as scalable cache consistency (DASH <ref> [Leno90] </ref> and Alewife [Chai91]), cooperative caching, disk striping (RAID and Zebra), and log structured file systems (Sprite LFS [Rose92] and BSD LFS [Selt93]). Second, in addition to borrowing techniques developed in other projects, we have refined them to work well in our serverless system. <p> Multiprocessor Cache Consistency Network file systems resemble multiprocessors in that both provide a uniform view of storage across the system, requiring both to track where blocks are cached. This information allows them to maintain cache consistency by invalidating stale cached copies. Multiprocessors such as DASH <ref> [Leno90] </ref> and Alewife [Chai91] scalably distribute this task by dividing the systems physical memory evenly among processors; each processor manages the cache consistency state for its own physical memory locations. 1 log across the storage servers. Clients compute parity for segments, not for individual files.
Reference: [Lisk91] <author> B. Liskov, S. Ghemawat, R. Gruber, P. Johnson, L. Shrira, and M. Williams. </author> <title> Replication in the Harp File System. </title> <booktitle> In Proc. of the 13th Symp. on Operating Systems Principles, </booktitle> <pages> pages 226238, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Although this made sense on an Ethernet, on todays fast LANs fetching data from local disk can be an order of magnitude slower than from server memory or remote striped disk. Similarly, a central server represents a single point of failure, requiring server replication <ref> [Walk83, Kaza89, Pope90, Lisk91, Kist92, Birr93] </ref> for high availability. Replication increases the cost and complexity of central servers, and can also increase latency on writes since the system must replicate data at multiple servers.
Reference: [Litz92] <author> M. Litzkow and M. Solomon. </author> <title> Supporting Checkpointing and Process Migration Outside the UNIX Kernel. </title> <booktitle> In Proc. of the Winter 1992 USENIX, </booktitle> <pages> pages 283290, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: For instance, continuous media workloads will increase file system demands; even a few workstations simultaneously running video applications would swamp a traditional central server [Rash94]. Coordinated Networks of Workstations (NOWs) allow users to migrate jobs among many machines and also permit networked workstations to run parallel jobs <ref> [Doug91, Litz92, Ande95] </ref>. By increasing the peak processing power available to users, NOWs increase peak demands on the file system [Cyph93]. Unfortunately, current centralized file system designs fundamentally limit performance and availability since all read misses and all disk writes go through the central server.
Reference: [LoVe93] <author> S. LoVerso, M. Isman, A. Nanopoulos, W. Nesheim, E. Milne, and R. Wheeler. sfs: </author> <title> A Parallel File System for the CM-5. </title> <booktitle> In Proc. of the Summer 1993 Usenix, </booktitle> <pages> pages 291305, </pages> <year> 1993. </year>
Reference-contexts: Other parallel systems have implemented redundant data storage intended for restricted workloads consisting entirely of large files, where per-file striping is appropriate and where large file accesses reduce stress on their centralized manager architectures. For instance, Swift [Cabr91] and SFS <ref> [LoVe93] </ref> provide redundant distributed data storage for parallel environments, and Tiger [Rash94] services multimedia workloads. TickerTAIP [Cao93], SNS [Lee95], and AutoRAID [Wilk95] implement RAID-derived storage systems.
Reference: [Majo94] <author> D. Major, G. Minshall, and K. Powell. </author> <title> An Overview of the NetWare Operating System. </title> <booktitle> In Proc. of the 1994 Winter USENIX, </booktitle> <pages> pages 35572, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: 1. Introduction A serverless network file system distributes storage, cache, and control over cooperating workstations. This approach contrasts with traditional file systems such as Netware <ref> [Majo94] </ref>, NFS [Sand85], Andrew [Howa88], and Sprite [Nels88] where a central server machine stores all data and satisfies all client cache misses. Such a central server is both a performance and reliability bottleneck.
Reference: [McKu84] <author> M. McKusick, W. Joy, S. Leffler, and R. Fabry. </author> <title> A Fast File System for UNIX. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 2(3):181197, </volume> <month> August </month> <year> 1984. </year>
Reference-contexts: LFSs solution to this problem provides a general mechanism to handle location-independent data storage. LFS uses per-file inodes, similar to those of the Fast File System (FFS) <ref> [McKu84] </ref>, to store pointers to the systems data blocks. However, where FFSs inodes reside in fixed locations, LFSs inodes move to the end of the log each time they are modified.
Reference: [Nels88] <author> M. Nelson, B. Welch, and J. Ousterhout. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 6(1), </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: 1. Introduction A serverless network file system distributes storage, cache, and control over cooperating workstations. This approach contrasts with traditional file systems such as Netware [Majo94], NFS [Sand85], Andrew [Howa88], and Sprite <ref> [Nels88] </ref> where a central server machine stores all data and satisfies all client cache misses. Such a central server is both a performance and reliability bottleneck. <p> The system treats the ifile like any other file with one exception: the ifile has no index nodes. Instead, the system locates the blocks of the ifile using manager checkpoints described in Section 5.1. 3.2.3. Cache Consistency xFS utilizes a token-based cache consistency scheme similar to Sprite <ref> [Nels88] </ref> and Andrew [Howa88] except that xFS manages consistency on a per-block rather than per-file basis. Before a client modifies a block, it must 14 acquire write ownership of that block. The client sends a message to the blocks manager. <p> Manager Recovery Once the storage servers have recovered, the managers can recover their manager map, disk location metadata, and cache consistency metadata. Manager map recovery uses a consensus algorithm as described above for stripe group recovery. Cache consistency recovery relies on server-driven polling <ref> [Bake94, Nels88] </ref>: a recovering manager contacts the clients, and each client returns a list of the blocks that it is caching or for which it has write ownership from that managers portion of the index number space.
Reference: [Patt88] <author> D. Patterson, G. Gibson, and R. Katz. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> In Internat. Conf. on Management of Data, </booktitle> <pages> pages 109116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: First, xFS dynamically distributes control processing across the system on a per-file granularity by utilizing a new serverless management scheme. Second, xFS distributes its data storage across storage server disks by implementing a software RAID <ref> [Patt88, Chen94] </ref> using log-based network striping similar to Zebras [Hart95]. Finally, xFS eliminates central server 3 caching by taking advantage of cooperative caching [Leff91, Dahl94b] to harvest portions of client memory as a large, global file cache. This paper makes two sets of contributions. <p> Finally, since xFS has evolved from our initial proposal [Wang93], we describe the relationship of the design presented here to previous versions of the xFS design. 2.1. RAID xFS exploits RAID-style disk striping to provide high performance and highly available disk storage <ref> [Patt88, Chen94] </ref>. A RAID partitions a stripe of data into N-1 data blocks and a parity block the exclusive-OR of the corresponding bits of the data blocks. It stores each data and parity block on a different disk. <p> These systems could provide services similar to xFSs storage servers, but they would require serverless management to provide a scalable and highly available file system interface to augment their simpler disk block interfaces. In contrast with the log-based striping approach taken by Zebra and xFS, TickerTAIPs RAID level 5 <ref> [Patt88] </ref> architecture makes calculating parity for small writes expensive when disks are distributed over the network. SNS combats this problem by using a RAID level 1 (mirrored) architecture, but this approach approximately doubles the space overhead for storing redundant data.
Reference: [Pier89] <author> P. Pierce. </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsystem. </title> <booktitle> In Proc. of the Fourth Conf. on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 155160, </pages> <year> 1989. </year>
Reference-contexts: Related Work Section 2 discussed a number of projects that provide an important basis for xFS. This section describes several other efforts to build decentralized file systems and then discusses the dynamic management hierarchies used in some MPPs. Several file systems, such as CFS <ref> [Pier89] </ref>, Bridge [Dibb89], and Vesta [Corb93], distribute data over multiple storage servers to support parallel workloads; however, they lack mechanisms to provide availability across component failures.
Reference: [Pope90] <author> G. Popek, R. Guy, T. Page, and J. Heidemann. </author> <title> Replication in the Ficus Distributed File System. </title> <booktitle> In Proc. of the Workshop on the Management of Replicated Data, </booktitle> <pages> pages 510, </pages> <month> November </month> <year> 1990. </year> <month> 34 </month>
Reference-contexts: Although this made sense on an Ethernet, on todays fast LANs fetching data from local disk can be an order of magnitude slower than from server memory or remote striped disk. Similarly, a central server represents a single point of failure, requiring server replication <ref> [Walk83, Kaza89, Pope90, Lisk91, Kist92, Birr93] </ref> for high availability. Replication increases the cost and complexity of central servers, and can also increase latency on writes since the system must replicate data at multiple servers.
Reference: [Rash94] <author> R. Rashid. </author> <title> Microsofts Tiger Media Server. </title> <booktitle> In The First Networks of Workstations Workshop Record, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: For instance, continuous media workloads will increase file system demands; even a few workstations simultaneously running video applications would swamp a traditional central server <ref> [Rash94] </ref>. Coordinated Networks of Workstations (NOWs) allow users to migrate jobs among many machines and also permit networked workstations to run parallel jobs [Doug91, Litz92, Ande95]. By increasing the peak processing power available to users, NOWs increase peak demands on the file system [Cyph93]. <p> For instance, Swift [Cabr91] and SFS [LoVe93] provide redundant distributed data storage for parallel environments, and Tiger <ref> [Rash94] </ref> services multimedia workloads. TickerTAIP [Cao93], SNS [Lee95], and AutoRAID [Wilk95] implement RAID-derived storage systems. These systems could provide services similar to xFSs storage servers, but they would require serverless management to provide a scalable and highly available file system interface to augment their simpler disk block interfaces.
Reference: [Ricc91] <author> A. Ricciardi and K. Birman. </author> <title> Using Process Groups to Implement Failure Detection in Asynchronous Environments. </title> <booktitle> In Proc. Tenth Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 341353, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Each storage server recovers this information independently from a private checkpoint, so this stage can proceed in parallel across all storage servers. Storage servers next regenerate their stripe group map. First, the storage servers use a distributed consensus algorithm <ref> [Cris91, Ricc91, Schr91] </ref> to determine group membership and to elect a group leader.
Reference: [Rose92] <author> M. Rosenblum and J. Ousterhout. </author> <title> The Design and Implementation of a Log-Structured File System. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 10(1):2652, </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: First, xFS synthesizes a number of recent innovations that, taken together, provide a basis for serverless file system design. xFS relies on previous work in areas such as scalable cache consistency (DASH [Leno90] and Alewife [Chai91]), cooperative caching, disk striping (RAID and Zebra), and log structured file systems (Sprite LFS <ref> [Rose92] </ref> and BSD LFS [Selt93]). Second, in addition to borrowing techniques developed in other projects, we have refined them to work well in our serverless system. For instance, we have transformed DASHs scalable cache consistency approach into a more general, distributed control system that is also fault tolerant. <p> A second drawback of commercially available hardware RAID systems is that they are significantly more expensive than non-RAID commodity disks because the commercial RAIDs add special-purpose hardware to compute parity. 2.2. LFS xFS implements log-structured storage based on the Sprite and BSD LFS prototypes <ref> [Rose92, Selt93] </ref> because this approach provides high-performance writes, simple recovery, and a flexible method to locate file data stored on disk. <p> Our design therefore provides a distributed cleaner. An LFS cleaner, whether centralized or distributed, has three main tasks. First, the system must keep utilization status about old segments how many holes they contain and how recently these holes appeared to make wise decisions about which segments to clean <ref> [Rose92] </ref>. Second, the system must examine this bookkeeping information to select segments to clean. Third, the cleaner reads the live blocks from old log segments and writes those blocks to new segments. The rest of this section describes how xFS distributes cleaning. <p> We do not expect checkpoints to ever limit performance. However, thorough future investigation will be needed to evaluate the impact of distributed cleaning under a wide range workloads; other researchers have measured sequential cleaning overheads from a few percent <ref> [Rose92, Blac95] </ref> to as much as 40% [Selt95], depending on the workload. Also, the current prototype implementation suffers from three inefficiencies, all of which we will attack in the future. 1.xFS is currently implemented as a set of user-level processes by redirecting vnode layer calls.
Reference: [Rost93] <author> E. Rosti, E. Smirni, T. Wagner, A. Apon, and L. Dowdy. </author> <title> The KSR1: Experimentation and Modeling of Poststore. </title> <booktitle> In Proc. of 1993 SIGMETRICS, </booktitle> <pages> pages 7485, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Due to a hardware failure, we ran this experiment with three groups of eight storage servers and 31 clients. 31 Several MPP designs have used dynamic hierarchies to avoid the fixed-home approach used in traditional directory-based MPPs. The KSR1 <ref> [Rost93] </ref> machine, based on the DDM proposal [Hage92], avoids associating data with fixed home nodes. Instead, data may be stored in any cache, and a hierarchy of directories allows any node to locate any data by searching successively higher and more globally-complete directories.
Reference: [Sand85] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and Implementation of the Sun Network Filesystem. </title> <booktitle> In Proc. of the Summer 1985 USENIX, </booktitle> <pages> pages 119130, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: 1. Introduction A serverless network file system distributes storage, cache, and control over cooperating workstations. This approach contrasts with traditional file systems such as Netware [Majo94], NFS <ref> [Sand85] </ref>, Andrew [Howa88], and Sprite [Nels88] where a central server machine stores all data and satisfies all client cache misses. Such a central server is both a performance and reliability bottleneck. <p> Uniprocessor 50 MHz SS-20s and SS-10s have SPECInt92 ratings of 74 and 65, and can copy large blocks of data from memory to memory at 27 MB/s and 20 MB/s, respectively. We use the same hardware to compare xFS with two central-server architectures, NFS <ref> [Sand85] </ref> and AFS (a commercial version of the Andrew file system [Howa88]). We use NFS as our baseline system for practical reasons NFS is mature, widely available, and well-tuned, allowing easy comparison and a good frame of reference but its limitations with respect to scalability are well known [Howa88].
Reference: [Schr91] <author> M. Schroeder, A. Birrell, M. Burrows, H. Murray, R. Needham, T. Rodeheffer, E. Satterthwaite, and C. Thacker. Au-tonet: </author> <title> A High-Speed, Self-Configuring Local Area Network Using Point-to-Point Links. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> 9(8):13181335, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: Each storage server recovers this information independently from a private checkpoint, so this stage can proceed in parallel across all storage servers. Storage servers next regenerate their stripe group map. First, the storage servers use a distributed consensus algorithm <ref> [Cris91, Ricc91, Schr91] </ref> to determine group membership and to elect a group leader.
Reference: [Selt93] <author> M. Seltzer, K. Bostic, M. McKusick, and C. Staelin. </author> <title> An Implementation of a Log-Structured File System for UNIX. </title> <booktitle> In Proc. of the 1993 Winter USENIX, </booktitle> <pages> pages 307326, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: number of recent innovations that, taken together, provide a basis for serverless file system design. xFS relies on previous work in areas such as scalable cache consistency (DASH [Leno90] and Alewife [Chai91]), cooperative caching, disk striping (RAID and Zebra), and log structured file systems (Sprite LFS [Rose92] and BSD LFS <ref> [Selt93] </ref>). Second, in addition to borrowing techniques developed in other projects, we have refined them to work well in our serverless system. For instance, we have transformed DASHs scalable cache consistency approach into a more general, distributed control system that is also fault tolerant. <p> A second drawback of commercially available hardware RAID systems is that they are significantly more expensive than non-RAID commodity disks because the commercial RAIDs add special-purpose hardware to compute parity. 2.2. LFS xFS implements log-structured storage based on the Sprite and BSD LFS prototypes <ref> [Rose92, Selt93] </ref> because this approach provides high-performance writes, simple recovery, and a flexible method to locate file data stored on disk. <p> The overhead associated with log cleaning is the primary drawback of LFS. Although Rosenblums original measurements found relatively low cleaner overheads, even a small overhead can make the cleaner a bottleneck in a distributed environment. Further, some workloads, such as transaction processing, incur larger cleaning overheads <ref> [Selt93, Selt95] </ref>. 2.3. Zebra Zebra [Hart95] provides a way to combine LFS and RAID so that both work well in a distributed environment. <p> We discuss deltas in more detail in Section 5.1. As in BSD LFS <ref> [Selt93] </ref>, each manager caches its portion of the imap in memory and stores it on disk in a special file called the ifile. The system treats the ifile like any other file with one exception: the ifile has no index nodes.
Reference: [Selt95] <author> M. Seltzer, K. Smith, H. Balakrishnan, J. Chang, S. McMains, and V. Padmanabhan. </author> <title> File System Logging Versus Clustering: A Performance Comparison. </title> <booktitle> In Proc. of the 1995 Winter USENIX, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: The overhead associated with log cleaning is the primary drawback of LFS. Although Rosenblums original measurements found relatively low cleaner overheads, even a small overhead can make the cleaner a bottleneck in a distributed environment. Further, some workloads, such as transaction processing, incur larger cleaning overheads <ref> [Selt93, Selt95] </ref>. 2.3. Zebra Zebra [Hart95] provides a way to combine LFS and RAID so that both work well in a distributed environment. <p> We do not expect checkpoints to ever limit performance. However, thorough future investigation will be needed to evaluate the impact of distributed cleaning under a wide range workloads; other researchers have measured sequential cleaning overheads from a few percent [Rose92, Blac95] to as much as 40% <ref> [Selt95] </ref>, depending on the workload. Also, the current prototype implementation suffers from three inefficiencies, all of which we will attack in the future. 1.xFS is currently implemented as a set of user-level processes by redirecting vnode layer calls.
Reference: [Smit77] <author> A. Smith. </author> <title> Two Methods for the Efficient Analysis of Memory Address Trace Data. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-3(1):94101, </volume> <month> January </month> <year> 1977. </year>
Reference-contexts: By omitting requests that resulted in local hits, the trace inflates the average number of network hops needed to satisfy a read request. Because we simulate larger caches than those of the traced system, this factor does not alter the total number of network requests for each policy <ref> [Smit77] </ref>, which is the relative metric we use for comparing policies. The second limitation of the trace is that its finite length does not allow us to determine a files First Writer with certainty for references to files created before the beginning of the trace.
Reference: [vE92] <author> T. von Eicken, D. Culler, S. Goldstein, and K. E. Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of 1992 ASPLOS, </booktitle> <pages> pages 256266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In contrast, shared media networks such as Ethernet or FDDI allow only one client or server to transmit at a time. In addition, the move towards low latency network interfaces <ref> [vE92, Basu95] </ref> enables closer cooperation between 2 machines than has been possible in the past. The result is that a LAN can be used as an I/O backplane, harnessing physically distributed processors, memory, and disks into a single system. <p> To fix this limitation, we are working to move xFS into the kernel. (Note that AFS shares this handicap.) 2.RPC and TCP/IP overheads severely limit xFSs network performance. We are porting xFSs communications layer to Active Messages <ref> [vE92] </ref> to address this issue. 3.We have done little profiling and tuning. As we do so, we expect to find and fix inefficiencies. As a result, the absolute performance is much less than we expect for the well-tuned xFS.
Reference: [Walk83] <author> B. Walker, G. Popek, R. English, C. Kline, and G. Thiel. </author> <title> The LOCUS distributed operating system. </title> <booktitle> In Proc. of the 5th Symp. on Operating Systems Principles, </booktitle> <pages> pages 4969, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: Although this made sense on an Ethernet, on todays fast LANs fetching data from local disk can be an order of magnitude slower than from server memory or remote striped disk. Similarly, a central server represents a single point of failure, requiring server replication <ref> [Walk83, Kaza89, Pope90, Lisk91, Kist92, Birr93] </ref> for high availability. Replication increases the cost and complexity of central servers, and can also increase latency on writes since the system must replicate data at multiple servers.
Reference: [Wang93] <author> R. Wang and T. Anderson. xFS: </author> <title> A Wide Area Mass Storage File System. </title> <booktitle> In Fourth Workshop on Workstation Operating Systems, </booktitle> <pages> pages 7178, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: To distribute control across the network, xFS draws inspiration from several multiprocessor cache consistency designs. Finally, since xFS has evolved from our initial proposal <ref> [Wang93] </ref>, we describe the relationship of the design presented here to previous versions of the xFS design. 2.1. RAID xFS exploits RAID-style disk striping to provide high performance and highly available disk storage [Patt88, Chen94]. <p> Further, as we show in Section 3.2.4, by directly controlling which machines manage which data, we can improve locality and reduce network communication. 2.5. Previous xFS Work The design of xFS has evolved considerably since our original proposal <ref> [Wang93, Dahl94a] </ref>. The original design stored all system data in client disk caches and managed cache consistency using a hierarchy of metadata servers rooted at a central server. Our new implementation eliminates client disk caching in favor of network striping to take advantage of high speed, switched LANs.
Reference: [Wilk95] <author> J. Wilkes, R. Golding, C. Staelin, and T. Sullivan. </author> <title> The HP AutoRAID Hierarchical Storage System. </title> <booktitle> In Proc. of the 15th Symp. on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: For instance, Swift [Cabr91] and SFS [LoVe93] provide redundant distributed data storage for parallel environments, and Tiger [Rash94] services multimedia workloads. TickerTAIP [Cao93], SNS [Lee95], and AutoRAID <ref> [Wilk95] </ref> implement RAID-derived storage systems. These systems could provide services similar to xFSs storage servers, but they would require serverless management to provide a scalable and highly available file system interface to augment their simpler disk block interfaces.
Reference: [Wolf89] <author> J. Wolf. </author> <title> The Placement Optimization Problem: A Practical Solution to the Disk File Assignment Problem. </title> <booktitle> In Proc. of 1989 SIGMETRICS, </booktitle> <pages> pages 110, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: This approach only moderately improves scalability because its coarse distribution often results in hot spots when the partitioning allocates heavily used files and directory trees to a single server <ref> [Wolf89] </ref>. It is also expensive, since it requires the (human) system manager to effectively become part of the file system moving users, volumes, and disks among servers to balance load. Finally, Andrew [Howa88] attempts to improve scalability by caching data on client disks.
References-found: 55

