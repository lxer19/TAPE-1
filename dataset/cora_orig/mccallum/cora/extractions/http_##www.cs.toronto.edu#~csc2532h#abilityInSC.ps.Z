URL: http://www.cs.toronto.edu/~csc2532h/abilityInSC.ps.Z
Refering-URL: http://www.cs.toronto.edu/~csc2532h/
Root-URL: http://www.cs.toronto.edu
Email: -lesperan,hector,,scherl-@ai.toronto.edu  
Title: Ability and Knowing How in the Situation Calculus  
Author: Yves Lesp erance, Hector J. Levesque, Fangzhen Lin, and Richard B. Scherl 
Note: This research received financial support from the  and the Natural Science and Engineering Research Council (Canada). Fellow of the Canadian Institute for Advanced Research. Current address:  
Date: January 1995  
Address: Toronto, ON, M5S 1A4 Canada  (Ontario, Canada),  Heights, Newark, NJ 07102 USA  
Affiliation: Department of Computer Science University of Toronto  Information Technology Research Center  the Institute for Robotics and Intelligent Systems (Canada),  Department of Computer and Information Science, New Jersey Institute of Technology, University  
Abstract: Most agents can acquire information about their environments as they operate. A good plan for such an agent is one that not only achieves the goal, but is also executable, i.e., ensures that the agent has enough information at every step to know what to do next. In this paper, we presents a formal account of what it means for an agent to know how to execute a plan and to be able to achieve a goal. Such a theory is a prerequisite for producing specifications of planners for agents that can acquire information at run time. It is also essential to account for cooperation among agents. Our account is more general than previous proposals, handles while loops properly, and incorporates an approach to the frame problem. It can also be used to prove programs containing sensing actions correct. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Philip E. Agre and David Chapman. </author> <title> What are plans for? Robotics and Autonomous Systems, </title> <booktitle> 6 </booktitle> <pages> 17-34, </pages> <year> 1990. </year>
Reference-contexts: Others have suggested that the right role for plans is as advice to a relatively smart improvisation module <ref> [1] </ref>. Also, multi-agent systems are becoming more common and typically involve agents at different levels of smartness. All this suggests studying what knowing how or ability means for agents with varying levels of intelligence.
Reference: [2] <author> Ernest Davis. </author> <title> Knowledge preconditions for plans. </title> <type> Technical Report 637, </type> <institution> Computer Science Department, </institution> <address> New York University, </address> <year> 1993. </year>
Reference-contexts: But as argued in the concluding section, the framework we propose provides a useful foundation for further exploration of this space. We will discuss related work as it becomes relevant. It is worth singling out, however, the very similar work of Ernest Davis <ref> [2] </ref>. Like us, Davis develops accounts of knowing how to execute a plan for both smart and dumb executors. However in [2], he fails to show that his account really handles unbounded iteration, a key problem area in earlier work. <p> We will discuss related work as it becomes relevant. It is worth singling out, however, the very similar work of Ernest Davis <ref> [2] </ref>. Like us, Davis develops accounts of knowing how to execute a plan for both smart and dumb executors. However in [2], he fails to show that his account really handles unbounded iteration, a key problem area in earlier work. Nor does he discuss ability to achieve a goal and its relation to knowing how. <p> This is one point over which our account differs from Davis's <ref> [2] </ref>, so the proposition would not hold in his system. The above result might suggest a simpler way of defining ability: use the above equivalence as an axiom to somehow define Can. Unfortunately, this approach does not seem to work.
Reference: [3] <author> Oren Etzioni, Steve Hanks, Daniel Weld, Denise Draper, Neal Lesh, and Mike Williamson. </author> <title> An approach to planning with incomplete information. </title> <editor> In Bernhard Nebel, Charles Rich, and William Swartout, editors, </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the Third International Conference, </booktitle> <pages> pages 115-125, </pages> <address> Cambridge, MA, 1992. </address> <publisher> Morgan Kaufmann Publishing. </publisher>
Reference-contexts: Such assumptions cannot be sustained in most real applications (e.g., robotics, information gathering agents); there, agents need to acquire knowledge at execution time by sensing their environment. Some recent work, for instance <ref> [3] </ref>, has attempted to generalize classical planning techniques to deal with this. But a key problem is that in such domains, it is not even clear what a plan is and when it is a solution to a particular planning problem.
Reference: [4] <author> C.C. Green. </author> <title> Theorem proving by resolution as a basis for question-answering systems. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 4, </volume> <pages> pages 183-205. </pages> <publisher> American Elsevier, </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: What would be ideal in this case would be a way of synthesizing a suitable program from a proof of Can (OE; s); that is, from a proof of 9oe Know (CanGet (OE; oe; now); s): This is can be thought of as a generalization of answer extraction <ref> [4] </ref> that would somehow convert a selection function into a program of the appropriate sort. We can also imagine a variety of types of programs for agents of varying power.
Reference: [5] <author> Andrew R. Haas. </author> <title> The case for domain-specific frame axioms. In F.M. </title> <editor> Brown, editor, </editor> <booktitle> The Frame Problem in Artificial Intelligence: Proceedings of the 1987 Workshop, </booktitle> <pages> pages 343-348, </pages> <publisher> Lawrence, KA, </publisher> <address> April 1987. </address> <publisher> Morgan Kaufmann Publishing. </publisher> <pages> 14 </pages>
Reference-contexts: Our approach incorporates a treatment of the frame problem due to Reiter [12] (who extends previous proposals by Pednault [11], Schubert [14] and Haas <ref> [5] </ref>). The basic idea behind this is to collect all effects axioms about a given fluent and assume that they specify all the ways the value of the fluent may change.
Reference: [6] <author> Yves Lesperance, Hector J. Levesque, F. Lin, Daniel Marcu, Raymond Reiter, and Richard B. Scherl. </author> <title> A logical approach to high-level robot programming a progress report. In Benjamin Kuipers, editor, Control of the Physical World by Intelligent Agents, </title> <booktitle> Papers from the 1994 AAAI Fall Symposium, </booktitle> <pages> pages 79-85, </pages> <address> New Orleans, LA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: But there is no real reason to restrict our attention to this picture. In some cases, planning from scratch may be so hard that it is better to try to build a smart executor that the user can program at a high level we pursue this in <ref> [6] </ref>. Others have suggested that the right role for plans is as advice to a relatively smart improvisation module [1]. Also, multi-agent systems are becoming more common and typically involve agents at different levels of smartness.
Reference: [7] <author> Fangzhen Lin and Raymond Reiter. </author> <title> State constraints revisited. </title> <journal> Journal of Logic and Computation, </journal> <volume> 4(5) </volume> <pages> 655-678, </pages> <year> 1994. </year>
Reference-contexts: Therefore, DOWN has the same truth value in all worlds s fl such that 2 This discussion ignores the ramification problem; a treatment compatible with our approach has been proposed by Lin and Reiter <ref> [7] </ref>. 5 K (s fl ; do (SENSEDOWN; s)), and so KWhether (DOWN; do (SENSEDOWN; s)) holds. Similar reasoning explains why we must have 9c Know (COMBOFSAFE = c; do (READCOMBOFSAFE; s)).
Reference: [8] <author> John McCarthy and Patrick Hayes. </author> <title> Some philosophical problems from the standpoint of artificial intelligence. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 4, </volume> <pages> pages 463-502. </pages> <publisher> Edinburgh University Press, Edinburgh, </publisher> <address> UK, </address> <year> 1979. </year>
Reference-contexts: We point out some of the differences as they become pertinent. 2 A Theory of Action Our theory is based on an extended version of the situation calculus <ref> [8] </ref>, a predicate calculus dialect for representing dynamically changing worlds. In this formalism, the world is taken to be in a certain state (or situation). That state can only change as a result of an agent doing an action. <p> The above axioms are not sufficient if one wants to reason about change. It is usually necessary to add frame axioms that specify when fluents remain unchanged by actions. The frame problem <ref> [8] </ref> arises because the number of these frame axioms is of the order of the product of the number of fluents and the number of actions.
Reference: [9] <author> Robert C. Moore. </author> <title> A formal theory of knowledge and action. </title> <editor> In J. R. Hobbs and Robert C. Moore, editors, </editor> <booktitle> Formal Theories of the Common Sense World, </booktitle> <pages> pages 319-358. </pages> <publisher> Ablex Publishing, </publisher> <address> Norwood, NJ, </address> <year> 1985. </year>
Reference-contexts: after doing READCOMBOFSAFE an agent might know what the combination of the safe he is trying to open is: P oss (READCOMBOFSAFE; s) oe 9c Know (COMBOFSAFE = c; do (READCOMBOFSAFE; s)): Knowledge is represented by adapting the possible world model to the situation calculus (as first done by Moore <ref> [9] </ref>). K (s 0 ; s) represents the fact that in state s, the agent thinks the state of the world could be s 0 . Know (OE; s) is an abbreviation for the formula 8s 0 (K (s 0 ; s) oe OE (s 0 )). <p> To our knowledge, this is the first time an account has been shown to handle both ability and inability in cases involving unbounded iteration. The earlier accounts of Moore <ref> [9] </ref> and Morgenstern [10] have problems with such cases; we explain their inadequacies in the next section. Let us now examine some properties of our definition of ability and see how some alternative definitions fail to handle important cases. <p> immediate consequence of the definition is that if an agent knows how to smartly execute ffi, then he knows that ffi has a terminating state: Proposition 10 SKH (ffi; s) oe Know (9s 0 Do (ffi; now; s 0 ); s): We mentioned earlier that the accounts proposed by Moore <ref> [9] </ref> and Morgenstern [10] are inadequate for dealing with unbounded iteration. The problem arises with non-terminating programs such as while T rue do a. Intuitively, we would want to say that no agent knows how to execute such a program, as it is impossible to bring it to termination.
Reference: [10] <author> Leora Morgenstern. </author> <title> Knowledge preconditions for actions and plans. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 867-874, </pages> <address> Milan, Italy, August 1987. </address> <publisher> Morgan Kaufmann Publishing. </publisher>
Reference-contexts: To our knowledge, this is the first time an account has been shown to handle both ability and inability in cases involving unbounded iteration. The earlier accounts of Moore [9] and Morgenstern <ref> [10] </ref> have problems with such cases; we explain their inadequacies in the next section. Let us now examine some properties of our definition of ability and see how some alternative definitions fail to handle important cases. <p> the definition is that if an agent knows how to smartly execute ffi, then he knows that ffi has a terminating state: Proposition 10 SKH (ffi; s) oe Know (9s 0 Do (ffi; now; s 0 ); s): We mentioned earlier that the accounts proposed by Moore [9] and Morgenstern <ref> [10] </ref> are inadequate for dealing with unbounded iteration. The problem arises with non-terminating programs such as while T rue do a. Intuitively, we would want to say that no agent knows how to execute such a program, as it is impossible to bring it to termination.
Reference: [11] <author> E. P. D. Pednault. </author> <title> ADL: Exploring the middle ground between STRIPS and the situation calculus. </title> <editor> In R.J. Brachman, H.J. Levesque, and R. Reiter, editors, </editor> <booktitle> Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 324-332, </pages> <address> Toronto, ON, May 1989. </address> <publisher> Morgan Kaufmann Publishing. </publisher>
Reference-contexts: The frame problem [8] arises because the number of these frame axioms is of the order of the product of the number of fluents and the number of actions. Our approach incorporates a treatment of the frame problem due to Reiter [12] (who extends previous proposals by Pednault <ref> [11] </ref>, Schubert [14] and Haas [5]). The basic idea behind this is to collect all effects axioms about a given fluent and assume that they specify all the ways the value of the fluent may change.
Reference: [12] <author> Raymond Reiter. </author> <title> The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression. </title> <editor> In Vladimir Lifschitz, editor, </editor> <booktitle> Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy, </booktitle> <pages> pages 359-380. </pages> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1991. </year>
Reference-contexts: The frame problem [8] arises because the number of these frame axioms is of the order of the product of the number of fluents and the number of actions. Our approach incorporates a treatment of the frame problem due to Reiter <ref> [12] </ref> (who extends previous proposals by Pednault [11], Schubert [14] and Haas [5]). The basic idea behind this is to collect all effects axioms about a given fluent and assume that they specify all the ways the value of the fluent may change.
Reference: [13] <author> Richard B. Scherl and Hector J. Levesque. </author> <title> The frame problem and knowledge-producing actions. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 689-695, </pages> <address> Washington, DC, July 1993. </address> <publisher> AAAI Press/The MIT Press. </publisher>
Reference-contexts: This treatment avoids the proliferation of axioms, as it only requires a single successor state axiom per fluent and a single precondition axiom per action. 2 Scherl and Levesque <ref> [13] </ref> have generalized this account to handle knowledge-producing actions. Such actions affect the mental state of the agent rather than the state of the external world.
Reference: [14] <author> L.K. Schubert. </author> <title> Monotonic solution to the frame problem in the situation calculus: An efficient method for worlds with fully specified actions. In H.E. </title> <editor> Kyberg, R.P. Loui, and G.N. Carlson, editors, </editor> <booktitle> Knowledge Representation and Defeasible Reasoning, </booktitle> <pages> pages 23-67. </pages> <publisher> Kluwer Academic Press, </publisher> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: Our approach incorporates a treatment of the frame problem due to Reiter [12] (who extends previous proposals by Pednault [11], Schubert <ref> [14] </ref> and Haas [5]). The basic idea behind this is to collect all effects axioms about a given fluent and assume that they specify all the ways the value of the fluent may change.
References-found: 14

