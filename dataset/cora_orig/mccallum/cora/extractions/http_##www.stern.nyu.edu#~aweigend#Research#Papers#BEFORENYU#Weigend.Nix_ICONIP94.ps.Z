URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/Weigend.Nix_ICONIP94.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: andreas@cs.colorado.edu  dnix@cs.colorado.edu  
Title: Predictions with Confidence Intervals (Local Error Bars)  
Author: Andreas S. Weigend David A. Nix 
Address: Boulder, CO 80309-0430  Boulder, CO 80309-0430  
Affiliation: Department of Computer Science and Institute of Cognitive Science University of Colorado  Department of Computer Science and Institute of Cognitive Science University of Colorado  
Note: CU-CS-724-94 (May 1994). Submitted to ICONIP'94-Seoul.  
Abstract: We present a new method for obtaining local error bars, i.e., estimates of the confidence in the predicted value that depend on the input. We approach this problem of nonlinear regression in a maximum likelihood framework. We demonstrate our technique first on computer generated data with locally varying, normally distributed target noise. We then apply it to the laser data from the Santa Fe Time Series Competition. Finally, we extend the technique to estimate error bars for iterated predictions, and apply it to the exact competition task where it gives the best performance to date.
Abstract-found: 1
Intro-found: 1
Reference: <author> W.L. </author> <title> Buntine and A.S. Weigend. (1991) "Bayesian Backpropagation." </title> <journal> Complex Systems, </journal> <volume> 5: </volume> <pages> 603-643. </pages>
Reference-contexts: 1 Obtaining Error Bars Using a Maximum Likelihood Framework 1.1 Motivation and Concept Feed-forward artificial neural networks are widely used and well-suited for nonlinear regression. They can be interpreted as predicting the expected value of the conditional target distribution as a function of (or "conditioned on") the input pattern <ref> (e.g., Buntine & Weigend, 1991) </ref>. This target distribution in response to each input may also be viewed as an error model (Rumelhart et al., 1994). Often an estimate of the mean of this conditional target distribution suffices; this is typically done using one output unit.
Reference: <author> N.A. </author> <title> Gershenfeld and A.S. Weigend. (1994) "The Future of Time Series." In Time Series Prediction: Forecasting the Future and Understanding the Past, A.S. </title> <editor> Weigend and N.A. Gershenfeld, eds., </editor> <publisher> Addison-Wesley, </publisher> <pages> pp. 1-70. </pages>
Reference-contexts: If the distance between either of these two predictions is greater than the single-step uncertainty estimate, this distance is used as the current uncertainty estimate. Because the competition log likelihood measure is highly sensitive to even a single error bar that is too small <ref> (Gershenfeld & Weigend, 1994, pp. 64-65) </ref>, we impose a lower bound of 4.0 on the variance in calculating the iterated uncertainties. Table 2 summarizes the results.
Reference: <author> A.M. Fraser and A. Dimitriadis. </author> <title> (1994) "Forecasting Probability Densities Using Hidden Markov Models with Mixed States." In Time Series Prediction: Forecasting the Future and Understanding the Past, A.S. </title> <editor> Weigend and N.A. Gershenfeld, eds., </editor> <publisher> Addison-Wesley, </publisher> <pages> pp. 265-282. </pages>
Reference-contexts: Such additional information could be obtained by attempting to estimate the entire conditional target distribution with connectionist methods (e.g., "fractional binning," Srivastava & Weigend, 1994) or with non-connectionist methods such as a Monte Carlo on a Hidden Markov Model <ref> (Fraser & Dimitriadis, 1994) </ref>. Nonparametric estimates of the shape of a conditional target distribution require large quantities of data.
Reference: <author> D.E. Rumelhart, R. Durbin, R. Golden, and Y. Chauvin. </author> <title> (1994) "Backpropagation: The Basic Theory." In Backpropagation: Theory, Architectures and Applications, </title> <editor> Y. Chauvin and D.E. Rumelhart, eds., </editor> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: They can be interpreted as predicting the expected value of the conditional target distribution as a function of (or "conditioned on") the input pattern (e.g., Buntine & Weigend, 1991). This target distribution in response to each input may also be viewed as an error model <ref> (Rumelhart et al., 1994) </ref>. Often an estimate of the mean of this conditional target distribution suffices; this is typically done using one output unit. However, we here present a method that gives more information than just the mean of that distribution.
Reference: <author> T. Sauer. </author> <title> (1994) "Time Series Prediction by Using Delay Coordinate Embedding." In Time Series Prediction: Forecasting the Future and Understanding the Past, A.S. </title> <editor> Weigend and N.A. Gershenfeld, eds., </editor> <publisher> Addison-Wesley, </publisher> <pages> pp. 175-193. </pages>
Reference: <author> A.N. Srivastava and A.S. Weigend. </author> <title> (1994) "Computing the Probability Density in Connectionist Regression." In Proceedings of the ICANN'94, </title> <note> to be published. </note>
Reference: <author> R. Tibshirani. </author> <title> (1994) "A comparison of some error estimates for neural network models." </title> <institution> University of Toronto preprint. </institution>
Reference: <author> E.A. Wan. </author> <title> (1994) "Time Series Prediction by Using a Connectionist Network with Internal Delay Lines." In Time Series Prediction: Forecasting the Future and Understanding the Past, A.S. </title> <editor> Weigend and N.A. Gershenfeld, eds., </editor> <publisher> Addison-Wesley, </publisher> <pages> pp. 195-217. </pages>
Reference: <author> A.S. Weigend and B. LeBaron. </author> <title> (1994) "Evaluating Neural Network Predictors by Bootstrapping." </title> <note> Technical Report CU-CS-725-94 (May 1994), </note> <institution> Computer Science Department, University of Colorado. </institution>
Reference-contexts: If the distance between either of these two predictions is greater than the single-step uncertainty estimate, this distance is used as the current uncertainty estimate. Because the competition log likelihood measure is highly sensitive to even a single error bar that is too small <ref> (Gershenfeld & Weigend, 1994, pp. 64-65) </ref>, we impose a lower bound of 4.0 on the variance in calculating the iterated uncertainties. Table 2 summarizes the results.
Reference: <author> A.S. Weigend and N.A. Gershenfeld, eds., </author> <title> (1994) Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison-Wesley. </publisher> <pages> 6 </pages>
Reference-contexts: If the distance between either of these two predictions is greater than the single-step uncertainty estimate, this distance is used as the current uncertainty estimate. Because the competition log likelihood measure is highly sensitive to even a single error bar that is too small <ref> (Gershenfeld & Weigend, 1994, pp. 64-65) </ref>, we impose a lower bound of 4.0 on the variance in calculating the iterated uncertainties. Table 2 summarizes the results.
References-found: 10

