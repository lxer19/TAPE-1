URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-349.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: ftpminka, picardg@media.mit.edu  
Title: Interactive learning using a "society of models"  
Author: T. P. Minka and R. W. Picard 
Address: 20 Ames Street; Cambridge, MA 02139  
Affiliation: Vision and Modeling Group MIT Media Laboratory  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 349 Submitted to Special Issue of Pattern Recognition on Image Database: Classification and Retrieval Abstract Digital library access is driven by features, but features are often context-dependent and noisy, and their relevance for a query is not always obvious. This paper describes an approach for utilizing many data-dependent, user-dependent, and task-dependent features in a semi-automated tool. Instead of requiring universal similarity measures or manual selection of relevant features, the approach provides a learning algorithm for selecting and combining groupings of the data, where groupings can be induced by highly specialized and context-dependent features. The selection process is guided by a rich example-based interaction with the user. The inherent combinatorics of using multiple features is reduced by a multistage grouping generation, weighting, and collection process. The stages closest to the user are trained fastest and slowly propagate their adaptations back to earlier stages. The weighting stage adapts the collection stage's search space across uses, so that, in later interactions, good groupings are found given few examples from the user. Described is an interactive-time implementation of this architecture for semi-automatic within-image segmentation and across-image labeling, driven by concurrently active color models, texture models, or manually-provided group ings.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Mao and A. K. Jain, </author> <title> "Texture classification and segmentation using multiresolution simultaneous autore-gressive models," </title> <booktitle> Patt. Rec., </booktitle> <volume> vol. 25, no. 2, </volume> <pages> pp. 173-188, </pages> <year> 1992. </year>
Reference-contexts: Similarity measures are also user and task dependent, as demonstrated by Figure 3. Unfortunately, these dependencies are not, at this point, understood well enough, especially by the typical digital library user, to permit careful selection of the optimal measure beforehand. Note that the multi-resolution simultaneous auto-regressive (MRSAR) model of <ref> [1] </ref>, which fares poorly compared to the shift-invariant eigenvector (EV) model in the above two examples, scores clearly above the EV model on the standard Brodatz database [2] [3]. (On the same test data, but for a perceptually motivated similarity criteria based on periodicity, directionality, and randomness, both the EV and <p> Then the learning goal is to cover all of P but none of N , given only a few examples from each. The heuristic used in FourEyes for the prior weight w 2 <ref> [0; 1] </ref> of a grouping G given weighting unit b is w (Gjb) = ff (b) + 2 fi (Gjb) + 1 ) (1) The first term of w (Gjb) is an estimate of the expected fraction of P contained in G and the second term is an estimate of the
Reference: [2] <author> P. Brodatz, </author> <title> Textures: A Photographic Album for Artists and Designers. </title> <address> New York: </address> <publisher> Dover, </publisher> <year> 1966. </year>
Reference-contexts: Note that the multi-resolution simultaneous auto-regressive (MRSAR) model of [1], which fares poorly compared to the shift-invariant eigenvector (EV) model in the above two examples, scores clearly above the EV model on the standard Brodatz database <ref> [2] </ref> [3]. (On the same test data, but for a perceptually motivated similarity criteria based on periodicity, directionality, and randomness, both the EV and MRSAR models are beat by a new Wold-based model [4].) Attempts to use intuitive texture features, like coarseness, contrast, and directionality [5] [6], are appropriate in some <p> Thus the prior weights directly influence the learner's inductive bias. The prior weights are determined from statistics collected over multiple learning sessions, which will be described in Section 6. for learning texture classes in the Brodatz <ref> [2] </ref> album. Each of the 112 textures in the album was equally divided into 9 128x128 non-overlapping images; the desired classification corresponds to the 112 original texture classes. The learner begins with all images unclassified.
Reference: [3] <author> R. W. Picard, T. Kabir, and F. Liu, </author> <title> "Real-time recognition with the entire Brodatz texture database," </title> <booktitle> in Proc. IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <address> (New York), </address> <pages> pp. 638-639, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Note that the multi-resolution simultaneous auto-regressive (MRSAR) model of [1], which fares poorly compared to the shift-invariant eigenvector (EV) model in the above two examples, scores clearly above the EV model on the standard Brodatz database [2] <ref> [3] </ref>. (On the same test data, but for a perceptually motivated similarity criteria based on periodicity, directionality, and randomness, both the EV and MRSAR models are beat by a new Wold-based model [4].) Attempts to use intuitive texture features, like coarseness, contrast, and directionality [5] [6], are appropriate in some cases, <p> The MRSAR has demonstrated excellent matching performance on this database in earlier experiments <ref> [3] </ref>, so we would expect learning to proceed even faster. This was indeed the case; the learner reached 100% accuracy after 487 examples.
Reference: [4] <author> F. Liu and R. W. </author> <title> Picard, "Periodicity, directionality, and randomness: Wold features for image modeling and retrieval," </title> <journal> IEEE T. Patt. Analy. and Mach. Intell., </journal> <note> To appear. Also MIT Media Laboratory Perceptual Computing TR#320. </note>
Reference-contexts: model in the above two examples, scores clearly above the EV model on the standard Brodatz database [2] [3]. (On the same test data, but for a perceptually motivated similarity criteria based on periodicity, directionality, and randomness, both the EV and MRSAR models are beat by a new Wold-based model <ref> [4] </ref>.) Attempts to use intuitive texture features, like coarseness, contrast, and directionality [5] [6], are appropriate in some cases, but do not fully determine all the qualities people might use in judging similarity. Thus an a priori optimal context-dependent selection among similarity measures, either by human or computer, seems unlikely.
Reference: [5] <author> H. Tamura, S. Mori, and T. Yamawaki, </author> <title> "Textural features corresponding to visual perception," </title> <journal> IEEE T. Sys., Man and Cyber., </journal> <volume> vol. SMC-8, no. 6, </volume> <pages> pp. 460-473, </pages> <year> 1978. </year>
Reference-contexts: on the standard Brodatz database [2] [3]. (On the same test data, but for a perceptually motivated similarity criteria based on periodicity, directionality, and randomness, both the EV and MRSAR models are beat by a new Wold-based model [4].) Attempts to use intuitive texture features, like coarseness, contrast, and directionality <ref> [5] </ref> [6], are appropriate in some cases, but do not fully determine all the qualities people might use in judging similarity. Thus an a priori optimal context-dependent selection among similarity measures, either by human or computer, seems unlikely. Next, the scope of queries that databases need to address is immense.
Reference: [6] <author> W. Niblack, R. Barber, W. Equitz, M. Flickner, E. Glasman, D. Petkovic, P. Yanker, C. Faloutsos, and G. Taubin, </author> <title> "The QBIC project: Querying images by content using color, texture, and shape," in Storage and Retrieval for Image and Video Databases (W. Niblack, </title> <editor> ed.), </editor> <address> (San Jose, CA), </address> <pages> pp. 173-181, SPIE, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: the standard Brodatz database [2] [3]. (On the same test data, but for a perceptually motivated similarity criteria based on periodicity, directionality, and randomness, both the EV and MRSAR models are beat by a new Wold-based model [4].) Attempts to use intuitive texture features, like coarseness, contrast, and directionality [5] <ref> [6] </ref>, are appropriate in some cases, but do not fully determine all the qualities people might use in judging similarity. Thus an a priori optimal context-dependent selection among similarity measures, either by human or computer, seems unlikely. Next, the scope of queries that databases need to address is immense. <p> The learner could approach the theoretical limit of 7 examples or 650:1 savings if ideal across-image groupings also became available, or were learned. 8 Related work Some recent systems which perform retrieval on image data are QBIC <ref> [6] </ref>, SWIM [32], Photobook [33], and CORE [34]. A notable quality of these systems is that they present many different ways of organizing the data but offer little assistance in actually choosing one of these organizations or making a 12 new one.
Reference: [7] <author> D. Romer, </author> <title> "The Kodak picture exchange," </title> <month> April </month> <year> 1995. </year> <institution> seminar at MIT Media Lab. </institution> <month> 13 </month>
Reference-contexts: Desirable queries also extend to subjective content ("give me a scene of a romantic forest"), task-specific content ("I need something with open space, to place text"), collaborative content ("show me pictures children like"), and more <ref> [7] </ref>. Answering such queries requires a variety of features, or meta-data, to be attached to the data in a digital library, some of which may not be computable directly from the data.
Reference: [8] <author> F. Cohen and D. Cooper, </author> <title> "Simple parallel hierarchi-cal and relaxation algorithms for segmenting noncausal Markovian random fields," </title> <journal> IEEE T. Patt. Analy. and Mach. Intell., </journal> <volume> vol. PAMI-9, </volume> <pages> pp. 195-219, </pages> <month> Mar. </month> <year> 1987. </year>
Reference-contexts: Examples include the doubly-stochastic Markov random field (MRF) segmentations of <ref> [8] </ref> and [9], the auto-regressive model interiors and MRF model boundaries of [10], the Gaussian model interiors and active contour boundaries of [11], and the cooperative robust estimation of [12].
Reference: [9] <author> J. K. Goutsias and J. M. Mendel, </author> <title> "Simultaneous optimal segmentation and model estimation of nonstationary noisy images," </title> <journal> IEEE T. Patt. Analy. and Mach. In-tell., </journal> <volume> vol. II, </volume> <pages> pp. 990-998, </pages> <month> Sept. </month> <year> 1989. </year>
Reference-contexts: Examples include the doubly-stochastic Markov random field (MRF) segmentations of [8] and <ref> [9] </ref>, the auto-regressive model interiors and MRF model boundaries of [10], the Gaussian model interiors and active contour boundaries of [11], and the cooperative robust estimation of [12].
Reference: [10] <author> C. Bouman and B. Liu, </author> <title> "Multiple resolution segmentation of textured images," </title> <journal> IEEE T. Patt. Analy. and Mach. Intell., </journal> <volume> vol. PAMI-13, no. 2, </volume> <pages> pp. 99-113, </pages> <year> 1991. </year>
Reference-contexts: Examples include the doubly-stochastic Markov random field (MRF) segmentations of [8] and [9], the auto-regressive model interiors and MRF model boundaries of <ref> [10] </ref>, the Gaussian model interiors and active contour boundaries of [11], and the cooperative robust estimation of [12].
Reference: [11] <author> S. C. Zhu, T. S. Lee, and A. L. Yuille, </author> <title> "Region competition: Unifying snakes, region growing, energy/Bayes/MDL for multi-band image segmentation," </title> <booktitle> in Int. Conf. on Computer Vision, </booktitle> <address> (Boston, MA), </address> <pages> pp. 416-423, </pages> <year> 1995. </year>
Reference-contexts: Examples include the doubly-stochastic Markov random field (MRF) segmentations of [8] and [9], the auto-regressive model interiors and MRF model boundaries of [10], the Gaussian model interiors and active contour boundaries of <ref> [11] </ref>, and the cooperative robust estimation of [12]. The basic idea of treating segment boundaries separately from their interiors is also at the heart of second-generation image coding techniques [13], where a variety of multiple-model strategies continue to be under investigation.
Reference: [12] <author> T. Darrell and A. P. Pentland, </author> <title> "Cooperative robust estimation using layers of support," </title> <editor> IEEE T. Patt. Analy. </editor> <booktitle> and Mach. Intell., </booktitle> <year> 1995. </year>
Reference-contexts: Examples include the doubly-stochastic Markov random field (MRF) segmentations of [8] and [9], the auto-regressive model interiors and MRF model boundaries of [10], the Gaussian model interiors and active contour boundaries of [11], and the cooperative robust estimation of <ref> [12] </ref>. The basic idea of treating segment boundaries separately from their interiors is also at the heart of second-generation image coding techniques [13], where a variety of multiple-model strategies continue to be under investigation. This joint optimization approach has an unfavorably large tradeoff of computation for accuracy.
Reference: [13] <author> M. Kunt, A. Ikonomopoulos, and M. Kocher, </author> <title> "Second-generation image-coding techniques," </title> <journal> Proc. IEEE, </journal> <volume> vol. 73, no. 4, </volume> <pages> pp. 549-574, </pages> <year> 1985. </year>
Reference-contexts: The basic idea of treating segment boundaries separately from their interiors is also at the heart of second-generation image coding techniques <ref> [13] </ref>, where a variety of multiple-model strategies continue to be under investigation. This joint optimization approach has an unfavorably large tradeoff of computation for accuracy.
Reference: [14] <author> T. M. Strat and M. A. Fischler, </author> <title> "Context-based vision: Recognizing objects using information from both 2-d and 3-d imagery," </title> <journal> IEEE T. Patt. Analy. and Mach. In-tell., </journal> <volume> vol. 13, </volume> <pages> pp. 1050-1065, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: The amount of approximations needed to make these work interactively (quickly and with little training) may defeat the benefits of using multiple models in the first place. The rule-based blackboard for model selection has been advocated for "context-based vision" <ref> [14] </ref>. The method reduces the complexity of model selection via explicit, user-provided rules that determine when changes may be made to the blackboard (i.e. which models should be used at a given time) and what segmentation hypotheses should be removed from further consideration.
Reference: [15] <author> E. Saund and T. P. Moran, </author> <title> "Perceptual organization in an interactive sketch editing application," </title> <booktitle> in Proc. Fifth International Conference on Computer Vision, </booktitle> <address> (Cam-bridge, MA), </address> <pages> pp. 597-604, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The paradigm is similar to that of the perceptually organized editing program PerSketch <ref> [15] </ref>. 3 three stages learn. Under this paradigm, the single object hierarchy of conventional paint programs is traded for multiple, possibly conflicting organizations. The amount of structure imposed by the system is mediated by an example-based interaction with the user.
Reference: [16] <author> R. A. Jarvis and E. A. Patrick, </author> <title> "Clustering using a similarity measure based on shared near neighbors," </title> <editor> IEEE T. </editor> <booktitle> Comp., </booktitle> <pages> pp. 1025-1034, </pages> <month> Nov. </month> <year> 1973. </year>
Reference-contexts: Multiple hierarchies are used to contain the sets. Hierarchies allow efficient expression of sets which are the union of other sets and are the natural output of many clustering algorithms. The particular clustering algorithm used by FourEyes is based on shared neighbors <ref> [16] </ref>; it is a single-link method that tends to group areas of similar density in feature space. The method was chosen since it avoids the seemingly arbitrary cuts through regions of constant density made by complete-link methods, which try to minimize an aggregate, rather than local, error.
Reference: [17] <author> A. K. Jain and R. C. Dubes, </author> <title> Algorithms for Clustering Data. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: This advantage of single-link clustering, which seems most appropriate for perceptual problems, has been demonstrated in the literature; see e.g. <ref> [17] </ref>, [18]. In the experiments described here, k t (the shared neighbor threshold) was zero and k (the number of neighbors) was steadily increased from 1 until all points formed a single cluster.
Reference: [18] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley-Interscience, </publisher> <year> 1973. </year>
Reference-contexts: This advantage of single-link clustering, which seems most appropriate for perceptual problems, has been demonstrated in the literature; see e.g. [17], <ref> [18] </ref>. In the experiments described here, k t (the shared neighbor threshold) was zero and k (the number of neighbors) was steadily increased from 1 until all points formed a single cluster.
Reference: [19] <author> E. Saber, A. M. Tekalp, R. Eschbach, and K. Knox, </author> <title> "Annotation of natural scenes using adaptive color segmentation," IS&T/SPIE Electronic Imaging, </title> <address> Feb. 1995. San Jose, CA. </address>
Reference-contexts: If the within-image groupings have different scales, it is up to the across-image features to remove scale dependence, if desired. The advantage of incorporating within-image relationships for across-image annotation is described in <ref> [19] </ref>. For color-based annotation of image regions, that work demonstrated a clear quality improvement when scene-adaptive class thresholds, based on preserving the continuity of the within-image class-likelihood histogram, were used instead of fixed, universally optimized thresholds. FourEyes approximates this behavior by forming its across-image groupings from within-image groupings. <p> FourEyes approximates this behavior by forming its across-image groupings from within-image groupings. Moreover, the shared neighbor clustering algorithm used by FourEyes behaves similarly to the histogram splitting used in <ref> [19] </ref>, so the within-image groupings generated by both methods similarly preserve class-likelihood continuity. This is a major difference with our previous annotation system [20], which did not use within-image groupings. Another difference is the ability to learn weights on groupings and to self-improve, as described in Section 6. <p> FourEyes differs from that work in three important ways. First, FourEyes does not perform its analysis strictly on lone pixels. By using within-image groupings as the analysis elements, it addresses the need for spatial context as outlined in <ref> [19] </ref>. Second, FourEyes can incorporate information from multi-dimensional or non-numerical features such as subjective clusterings provided by the user. Third, and most important as the number of features gets large, FourEyes can learn a strong bias on groupings.
Reference: [20] <author> R. W. Picard and T. P. Minka, </author> <title> "Vision texture for annotation," </title> <journal> Journal of Multimedia Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 3-14, </pages> <year> 1995. </year>
Reference-contexts: Moreover, the shared neighbor clustering algorithm used by FourEyes behaves similarly to the histogram splitting used in [19], so the within-image groupings generated by both methods similarly preserve class-likelihood continuity. This is a major difference with our previous annotation system <ref> [20] </ref>, which did not use within-image groupings. Another difference is the ability to learn weights on groupings and to self-improve, as described in Section 6. The within-image and across-image groupings are computed off-line, before the user begins interaction with the system. <p> The algorithm used in FourEyes differs from AQ in its evaluation of the next grouping to add. Instead of choosing the grouping which simply maximizes the number of positive examples (as in our previous work <ref> [20] </ref>), it maximizes the product of this number and the prior weight of the grouping. This means that, e.g., a grouping with twice the prior weight can cover half as many positive examples before it is chosen. Thus the prior weights directly influence the learner's inductive bias.
Reference: [21] <author> T. Minka, </author> <title> "An image database browser that learns from user interaction," </title> <type> Master's thesis, </type> <institution> MIT, </institution> <year> 1996. </year>
Reference-contexts: In a later version of the program, a background task continuously eliminates groupings with low weight (a forgetting mechanism) and replaces them with new ones. This adds a link from the second stage to the first and is described in <ref> [21] </ref>. Since the later stages of the system only see groupings, not feature values, it is not necessary for numerical similarity features to be used. For example, this is advantageous for incorporating subjective associations among content. <p> The latter case is examined in Section 6; the former case could arise through adaptation of the grouping generation stage, as explored in <ref> [21] </ref>. The dominance of some models over others is obvious in these four experiments, but it need not be so in general.
Reference: [22] <author> T. M. Mitchell, </author> <title> "The need for biases in learning generalizations," </title> <type> Tech. Rep. </type> <institution> CBM-TR-117, Rutgers University, </institution> <month> May </month> <year> 1980. </year>
Reference-contexts: The performance of any learner is crucially dependent on its inductive bias: "any basis for choosing one generalization over another, other than strict consistency with the observed training examples" <ref> [22] </ref>. Bias is determined by both the extent of a learner's concept space as well as the relative weights assigned a priori to different concepts. The latter has a close correspondence with the prior in Bayesian learning [23].
Reference: [23] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <journal> ch. </journal> <volume> 3, </volume> <editor> p. </editor> <volume> 58. </volume> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: Bias is determined by both the extent of a learner's concept space as well as the relative weights assigned a priori to different concepts. The latter has a close correspondence with the prior in Bayesian learning <ref> [23] </ref>. These two components of bias may be expressed procedurally (by an algorithm) or declaratively (say, by weights). Either may change during the problem or across different problems. The approach taken in FourEyes is to use a simple concept language (pure disjunctions, i.e. set union) with an adaptive weighting mechanism.
Reference: [24] <author> J. R. Quinlan, </author> <title> "Induction of decision trees," </title> <journal> Machine Learning, </journal> <volume> vol. 1, </volume> <pages> pp. 81-106, </pages> <year> 1986. </year>
Reference-contexts: This makes a great deal of the inductive bias declarative and hence easy to change dynamically (i.e. the learner is "malleable"). This is in contrast to a learner with a powerful concept language but limited weighting mechanism, such as ID3 <ref> [24] </ref> or CART [25], which can simulate arbitrary set operations but can only change their bias via splitting or pruning parameters, and so are difficult 6 to steer in desired directions. The learning algorithm used in FourEyes descends from AQ [26].
Reference: [25] <author> L. Breiman, J. H. Freidman, R. A. Olshen, and C. J. Stone, </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks/Cole Advanced Books & Software, </publisher> <year> 1984. </year>
Reference-contexts: This makes a great deal of the inductive bias declarative and hence easy to change dynamically (i.e. the learner is "malleable"). This is in contrast to a learner with a powerful concept language but limited weighting mechanism, such as ID3 [24] or CART <ref> [25] </ref>, which can simulate arbitrary set operations but can only change their bias via splitting or pruning parameters, and so are difficult 6 to steer in desired directions. The learning algorithm used in FourEyes descends from AQ [26].
Reference: [26] <author> R. S. Michalski, </author> <title> "A theory and methodology of inductive learning," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 20, no. 2, </volume> <pages> pp. 111-161, </pages> <year> 1983. </year>
Reference-contexts: The learning algorithm used in FourEyes descends from AQ <ref> [26] </ref>. AQ is a greedy method that collects groupings one at a time, such that each one includes no negative examples but their union includes all positive examples. Starting from an empty union, the grouping which adds the most positive examples but no negative ones is iteratively added.
Reference: [27] <author> R. W. Picard and T. Kabir, </author> <title> "Finding similar patterns in large image databases," </title> <booktitle> in Proc. IEEE Conf. on Acoustics, Speech, and Signal Proc., </booktitle> <address> (Minneapolis, MN), </address> <pages> pp. </pages> <address> V-161-V-164, </address> <year> 1993. </year>
Reference-contexts: This hierarchy had 632 groupings containing more than one element. Given this feeble bias, it required all 1008 examples to reach zero error. The second experiment had available the same hierarchy plus a hierarchy generated by clustering the images by EV features <ref> [27] </ref> (the hierarchy contained 427 groupings).
Reference: [28] <author> T. Chang and C. C. J. Kuo, </author> <title> "Texture analysis and classification with tree-structured wavelet transform," </title> <type> Tech. Rep. </type> <institution> USC-SIPI198, University of Southern California, </institution> <address> Los Angeles, CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: The dominance of some models over others is obvious in these four experiments, but it need not be so in general. For example, if two roughly equally performing models, say a Eu-clidean gray-level histogram distance and the tree-structured wavelet (TSW) transform <ref> [28] </ref> are used, the result is better than either one alone (from 916 and 858, respectively, to 785 examples to reach zero error). These experiments demonstrate the ability of the learner to tolerate grouping noise and quickly locate the most useful groupings for generalization.
Reference: [29] <author> T. Kohonen, </author> <title> Self-Organization and Associative Memory. </title> <address> Berlin, Heidelberg: </address> <publisher> Springer, </publisher> <year> 1984. </year> <note> 3rd ed. </note> <year> 1989. </year>
Reference-contexts: An important issue here is the comparison between weight-vectors in order to determine when two learning tasks are similar; this is s (b) given below. 6.1 Modeling weight-space FourEyes classifies learning problems by clustering weight-space. Currently this is done via a self-organizing map (SOM) <ref> [29] </ref>. During user interaction, each SOM unit (stored vector of weights) competes for consistency with the user's examples; the winning unit propagates its weights over the groupings. When the user is satisfied with the output of the learner, the winning unit is updated to more closely match the examples. <p> A method for adding new units which avoids monopoly, e.g. "wincount" [30], could also be used. Another possible extension is the relaxation of the winner-take-all constraint, to allow multiple units to contribute and/or be updated, e.g. via a neighborhood around each unit <ref> [29] </ref>, which would provide output interpolation. A mechanism for the elimination of unnecessary units (forget ting) may also be useful.
Reference: [30] <author> T. Uchiyama and M. A. Arbib, </author> <title> "An algorithm for competitive learning in clustering problems," </title> <journal> Pattern Recognition, </journal> <volume> vol. 27, </volume> <pages> pp. 1415-1421, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In this case, a new unit is created and initialized with the current example counts (ff gets ff fl , fi gets fi fl ). A method for adding new units which avoids monopoly, e.g. "wincount" <ref> [30] </ref>, could also be used. Another possible extension is the relaxation of the winner-take-all constraint, to allow multiple units to contribute and/or be updated, e.g. via a neighborhood around each unit [29], which would provide output interpolation.
Reference: [31] <author> Y.-I. Ohta, T. Kanade, and T. Sakai, </author> <title> "Color information for region segmentation," Comp. Graph. </title> <journal> and Img. Proc., </journal> <volume> vol. 13, </volume> <pages> pp. 222-241, </pages> <year> 1980. </year>
Reference-contexts: Third, within-image groupings computed from the Euclidean distance between unnormalized histograms of 32 fi 32-pixel patches in the Ohta color space <ref> [31] </ref> were added (1663 groupings), which raised the savings to 2.4:1. When run again on the same problem, the weights stored in the SOM raised the savings to 2.9:1, which is therefore the most that can be expected with these two models.
Reference: [32] <author> H.-J. Zhang and S. W. Smoliar, </author> <title> "Developing power tools for video indexing and retrieval," in Proceedings SPIE Storage and Retrieval for Image and Video Databases II (W. </title> <editor> Niblack and R. C. Jain, eds.), </editor> <address> (San Jose, CA), </address> <pages> pp. 140-149, SPIE, </pages> <month> Feb. </month> <year> 1994. </year> <note> Vol. 2185. </note>
Reference-contexts: The learner could approach the theoretical limit of 7 examples or 650:1 savings if ideal across-image groupings also became available, or were learned. 8 Related work Some recent systems which perform retrieval on image data are QBIC [6], SWIM <ref> [32] </ref>, Photobook [33], and CORE [34]. A notable quality of these systems is that they present many different ways of organizing the data but offer little assistance in actually choosing one of these organizations or making a 12 new one.
Reference: [33] <author> A. Pentland, R. W. Picard, and S. Sclaroff, "Photo-book: </author> <title> Tools for content-based manipulation of image databases," </title> <booktitle> in SPIE Storage and Retrieval of Image & Video Databases II, </booktitle> <address> (San Jose, CA), </address> <pages> pp. 34-47, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: The learner could approach the theoretical limit of 7 examples or 650:1 savings if ideal across-image groupings also became available, or were learned. 8 Related work Some recent systems which perform retrieval on image data are QBIC [6], SWIM [32], Photobook <ref> [33] </ref>, and CORE [34]. A notable quality of these systems is that they present many different ways of organizing the data but offer little assistance in actually choosing one of these organizations or making a 12 new one.
Reference: [34] <author> J. K. Wu, A. D. Narasimhalu, B. M. Mehtre, C. P. Lam, and Y. J. Gao, </author> <title> "CORE: a content-based retrieval engine for multimedia information systems," </title> <journal> Multimedia Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 25-41, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: The learner could approach the theoretical limit of 7 examples or 650:1 savings if ideal across-image groupings also became available, or were learned. 8 Related work Some recent systems which perform retrieval on image data are QBIC [6], SWIM [32], Photobook [33], and CORE <ref> [34] </ref>. A notable quality of these systems is that they present many different ways of organizing the data but offer little assistance in actually choosing one of these organizations or making a 12 new one.
Reference: [35] <author> R. L. Delanoy and R. J. Sasiela, </author> <title> "Machine learning for a toolkit for image mining," </title> <institution> Lincoln Laboratory 1017, MIT, Lexington, </institution> <address> MA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Since intentions can vary greatly and features can be very opaque, another solution is needed. The example-based interaction in FourEyes, coupled with a learning element that selects and constructs organizations, provides such an alternative. The need for a learning component between the user and image features is described in <ref> [35] </ref>. In that work, positive and negative pixels were used to define a classification rule for new pixels. The classification rule was a conjunction of thresholds on one-dimensional feature values, where the thresholds and features are chosen to maximize the separation between positive and negative. <p> Second, FourEyes can incorporate information from multi-dimensional or non-numerical features such as subjective clusterings provided by the user. Third, and most important as the number of features gets large, FourEyes can learn a strong bias on groupings. FourEyes' groupings implicitly quantize and the weightings prioritize the thresholds used in <ref> [35] </ref>. This allows FourEyes to improve its performance over time and over new problems, despite growth in the number of features. FourEyes employs hierarchically-organized sets, produced by off-line clustering, for efficient retrieval of plausible groupings.
Reference: [36] <author> H.-J. Zhang and D. Zhong, </author> <title> "A scheme for visual feature based image indexing," </title> <booktitle> in SPIE Conference on Storage and Retrieval for Image and Video Databases, </booktitle> <address> (San Jose, CA), </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: This allows FourEyes to improve its performance over time and over new problems, despite growth in the number of features. FourEyes employs hierarchically-organized sets, produced by off-line clustering, for efficient retrieval of plausible groupings. A possible alternative is the hierarchical self-organizing map discussed in <ref> [36] </ref>, which can reduce high-dimensional vector spaces into arbitrary hierarchical topologies (a hierarchy of two dimensional topologies was used in that paper). The principal advantage of the algorithm is that it is trained on-line and might be modified to optimize a classification criterion, as in LVQ [37].
Reference: [37] <author> T. Kohonen, </author> <title> "Learning Vector Quantization," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 1, no. Supplement 1, </volume> <editor> p. </editor> <volume> 303, </volume> <year> 1988. </year> <title> 14 images, and assigned them the label "sky." Within-image groupings allowed FourEyes to grow those labeled patches into larger "sky" regions (indicated by cross-hatching). Across-image groupings allowed FourEyes to also place tentative labels on the two left images. The menu buttons allow the user to control which sets of groupings are available to the learner. 15 "car" is yellow, "grass" is green, "leaves" is cyan, "person" is red, "sky" is blue, and "water" is purple. Unlabeled regions are white. </title> <type> 16 17 </type>
Reference-contexts: The principal advantage of the algorithm is that it is trained on-line and might be modified to optimize a classification criterion, as in LVQ <ref> [37] </ref>. This admits the possibility of modifying the groupings based on information obtained by the learner and the memory of weights, without a full recluster-ing step.
References-found: 37

