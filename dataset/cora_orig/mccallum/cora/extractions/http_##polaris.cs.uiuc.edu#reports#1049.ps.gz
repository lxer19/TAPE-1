URL: http://polaris.cs.uiuc.edu/reports/1049.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: MULTIPROCESSOR SPARSE SVD ALGORITHMS AND APPLICATIONS  
Author: BY MICHAEL WAITSEL BERRY 
Degree: 1983 THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Address: 1981  1991 Urbana, Illinois  
Affiliation: B.S., University of Georgia,  M.S., North Carolina State University,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [Aase71] <author> J. O. Aasen. </author> <title> On the reduction of a symmetric matrix to tridiagonal form. </title> <journal> BIT, </journal> <volume> 11 </volume> <pages> 233-242, </pages> <year> 1971. </year>
Reference: [Alli87a] <institution> FX/Series Architecture Manual. Alliant Computer Systems Corporation, Littleton, </institution> <address> MA, </address> <month> September </month> <year> 1987. </year>
Reference-contexts: operations (see [DBMS79]) must be synchronized across the 8 processors of the Alliant FX/80, and large numbers of memory fetches for vectors of length 1033 easily stress the memory hierarchy of the Alliant FX/80 which supports a 4-way interleaved 128 kilobyte cache between main memory and the 8 processors (see <ref> [Alli87a] </ref>). The assembly language level-1 BLAS kernels used by LASVD on the Alliant FX/80 were supplied by the Alliant FX/Series Scientific Library ([Alli87b]).
Reference: [Alli87b] <institution> FX/Series Scientific Library. Alliant Computer Systems Corporation, </institution> <address> Little-ton, MA, </address> <month> September </month> <year> 1987. </year>
Reference: [Baue57] <author> F. L. Bauer. </author> <title> Das Verfahren der Treppeniteration und verwandte Verfahren zur Losung algebraischer Eigenwertprobleme. </title> <journal> ZAMP, </journal> <volume> 8 </volume> <pages> 214-235, </pages> <year> 1957. </year>
Reference: [BeSa86] <author> M. Berry and A. Sameh. </author> <title> Multiprocessor Jacobi schemes for dense symmetric eigenvalue and singular value decompositions. </title> <booktitle> Proceedings of ICPP 86, </booktitle> <address> St. Charles, IL, 433-440, </address> <year> 1986. </year>
Reference-contexts: Thus, the decomposition in (1.1) is realized after determining the matrix ~ V in (3.3). The matrix ~ V is constructed via the (i; j) plane rotations discussed in <ref> [BeSa86] </ref> so that r T where r i designates the i-th column of R R ~ V . Note that (3.7) requires the columns of ~ Q to decrease in norm from left to right, and hence the resulting i to be in monotonic non-increasing order. <p> Also, the inequality in (3.7) can be reversed if the singular values are desired in non-decreasing order. Several parallel schemes for ordering the (i; j) plane rotations may be used in orthogonalizing the columns of R. One possible orthogonal-ization scheme is given in <ref> [BeSa86] </ref> and [BeSa88].
Reference: [BeSa88] <author> M. Berry and A. Sameh. </author> <title> Parallel algorithms for the singular value and dense symmetric eigenvalue problems. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 27 </volume> <pages> 191-213, </pages> <year> 1989. </year>
Reference-contexts: Also, the inequality in (3.7) can be reversed if the singular values are desired in non-decreasing order. Several parallel schemes for ordering the (i; j) plane rotations may be used in orthogonalizing the columns of R. One possible orthogonal-ization scheme is given in [BeSa86] and <ref> [BeSa88] </ref>. <p> (and modification by Chan), this Jacobi-based method will also suffer from fill-in if the original matrix A is very sparse. 25 Although the Berry-Sameh-Jacobi method can be twice as fast as the GKR method for suitably rectangular matrices on multiprocessor architectures such as the Alliant FX/8 and Cray X-MP/416 (see <ref> [BeSa88] </ref>), the storage requirements to produce the SVD in (3.6) for matrices in Table 2.1, for example, would be constraining. 26 Chapter 4 SINGULAR VALUE DECOMPOSITION OF SPARSE MATRICES In this chapter, we show how canonical sparse symmetric eigenvalue problems can be used to (indirectly) compute the sparse singular value decomposition. <p> To improve upon the two-sided Jacobi algorithm originally suggested 34 by Rutishauser ([Ruti69]) for the spectral decomposition (step (4)) in ritzit, one may employ a parallel two- or one-sided Jacobi method (see <ref> [BeSa88] </ref>) on a multiprocessor. In fact, the one-sided Jacobi scheme (3.3), when appropriately adapted for symmetric positive definite matrices (see [BeSa88]), is quite appropriate for step (4) provided the dimension of the current subspace, s, is not too large (e.g., s 100). <p> upon the two-sided Jacobi algorithm originally suggested 34 by Rutishauser ([Ruti69]) for the spectral decomposition (step (4)) in ritzit, one may employ a parallel two- or one-sided Jacobi method (see <ref> [BeSa88] </ref>) on a multiprocessor. In fact, the one-sided Jacobi scheme (3.3), when appropriately adapted for symmetric positive definite matrices (see [BeSa88]), is quite appropriate for step (4) provided the dimension of the current subspace, s, is not too large (e.g., s 100).
Reference: [BGLS87] <author> R. Bording, A. Gertsztenkorn, L. Lines, J. Scales, and S. Treitel. </author> <title> Applications of seismic travel-time tomography. Geophys. </title> <journal> J. R. astr. Soc., </journal> <volume> 90(2) </volume> <pages> 285-304, </pages> <year> 1987. </year>
Reference-contexts: In recent literature (see [ScDG90], [ScGe88], and <ref> [BGLS87] </ref>), great interest has been expressed in the ability to compute either the complete sparse SVD or a few of the smallest singular values and corresponding singular vectors.
Reference: [BuKa77] <author> J. R. Bunch and L. Kaufman. </author> <title> Some stable methods for calculating inertia and solving symmetric linear systems. </title> <journal> Math. Comp., </journal> <volume> 31 </volume> <pages> 162-179, </pages> <year> 1977. </year>
Reference: [BiVL85] <author> C. Bischoff and C. Van Loan. </author> <title> The WY representation for products of Householder Matrices. </title> <type> Technical Report TR 85-681, </type> <institution> Cornell University, Department of Computer Science, </institution> <year> 1985. </year>
Reference-contexts: While efficient implementations of such orthogonal factorizations using block generalizations of Householder transformations on hierarchical- and distributed-memory architectures ([BeSa88] and <ref> [BiVL85] </ref>) have been achieved, these orthogonal transformations do not preserve the nonzero structure of a sparse matrix A.
Reference: [Chan82] <author> T. F. Chan. </author> <title> An improved algorithm for computing the singular value decomposition. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 8(1) </volume> <pages> 72-83, </pages> <year> 1982. </year> <month> 164 </month>
Reference: [Cray89a] <institution> Cray-2 Multitasking Programmers' Manual. Publication SN-2026C, Cray Research Inc., Mendota Heights, MN, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: This particular Cray-2S uses the UNICOS 5:1 operating system, and supports autotasking (see <ref> [Cray89a] </ref>). Using autotasking, we allow the Cray-2S/4128 Fortran pre-processor to generate multiple-CPU programs for each of our four SVD methods.
Reference: [Cray89b] <institution> UNICOS Fortran Library Reference Manual. Publication SR-2079 5.0, Cray Research Inc., Mendota Heights, MN, </institution> <month> February </month> <year> 1989. </year>
Reference-contexts: To obtain these profiles, we invoke the flowtrace compiler option (see <ref> [Cray89b] </ref>) on only 1 CPU (profiling is not currently available for multiple-CPU programs). We terminate each method when residuals (4.2) less than or equal to 10 3 are achieved (no loss of accuracy associated with the use of the A T A operator observed in this case).
Reference: [Cray89c] <institution> UNICOS Math and Scientific Library Reference Manual. Publication SR-2081 5.0, Cray Research Inc., Mendota Heights, MN, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: The assembly language level-1 BLAS kernels used by LASVD on the Alliant FX/80 were supplied by the Alliant FX/Series Scientific Library ([Alli87b]). On the Cray-2S/4128, optimized Cray Assembly Language (CAL) implementations of the level-1 BLAS kernels (see <ref> [Cray89c] </ref>) 96 required less than 1% of the total CPU time for each method, and hence do not appear in the profiles presented in Figures 5.1 through 5.4. time) is spent in the level-2 (matrix-vector) and level-3 (matrix-matrix) BLAS kernels.
Reference: [CuWi85] <author> J. K. Cullum and R. A. Willoughby. </author> <title> Lanczos Algorithm for Large Symmetric Eigenvalue Computations. Volume 1 Theory, </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1985. </year>
Reference-contexts: The multiplicity of the zero eigenvalue of B is m + n 2r, where r=rank (A). The following 27 Lemma (see <ref> [CuWi85] </ref> for proof) demonstrates how the SVD of A is generated from the eigenvalues and eigenvectors of the the matrix B in (4.1). Lemma 4.1 Let A be an m fi n (m n) matrix and B defined by (4.1). 1. <p> , is then obtained as v i = 1 i Computing the SVD (A) via the eigensystems of either A T A or AA T may be adequate for determining the largest singular triplets of A, but the loss of accuracy can be severe for the smallest singular triplets (see <ref> [CuWi85] </ref>). <p> Theoretically, it is easy to prove that for the basic Lanczos recursion that orthogonal-ization with respect to only the two most recently-generated Lanczos vectors is sufficient to guarantee that each succeeding Lanczos vector is orthogonal with respect to all previously generated Lanczos vectors (see <ref> [CuWi85] </ref> or [Parl80]). In one sense, the Lanczos procedure can be viewed as the Gram-Schmidt orthogonalization of the set of Krylov vectors w 1 ; Bw 1 ; :::; B k1 w 1 . <p> The resulting single-vector Lanczos SVD procedure, however, can only compute the distinct singular values of an mfin matrix A and not their multiplicities. As shown in <ref> [CuWi85] </ref>, these multiplicities can be recovered along with the suppression of zero eigenvalues of the matrix B caused by m 6= n.
Reference: [DDFL90] <author> S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harsh-man. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <note> 1990. In press. </note>
Reference-contexts: Given the growing availability of multiprocessor computer systems, there has been great interest in the development of parallel implementations of the singular value decomposition, in general. In applications such as information retrieval ([DuFL88], <ref> [DDFL90] </ref>), the data matrix whose SVD is sought is usually large and sparse. It is this particular case that motivates our research effort. Hence, we are primarily interested in SVD methods which can be used to determine singular values and singular vectors of large sparse matrices on multiprocessors. <p> In the following sections, we discuss the need for computing singular triplets of large sparse matrices as suggested by recent research in engineering, physical and social science. 2.1 Latent Semantic Indexing In [DuFL88] and <ref> [DDFL90] </ref> a new approach to automatic indexing and retrieval is discussed. It is designed to overcome a fundamental problem that plagues existing information retrieval techniques that try to match words of queries with words of documents. <p> Hence, the LSI system returns all relevant titles and no irrelevant ones. We recall that the ordinary keyword technique missed titles c3 and c5 in its response to the query. Although this sample database is quite small, it does support recent claims ([DuFL88] and <ref> [DDFL90] </ref>) that LSI using the sparse SVD is more robust and economical than straight term overlap methods. However, in practice, one must compute at least 100-200 largest singular values and corresponding singular vectors of sparse matrices having similar characteristics to those matrices in Table 2.1.
Reference: [DBMS79] <author> J. Dongarra, J. Bunch, C. Moler, and G. W. Stewart. </author> <title> LINPACK Users' Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: As implemented in the LINPACK library <ref> [DBMS79] </ref>, this algorithm consists of two phases. <p> On the Cray-2S/4128, the level-1 BLAS operations (used within the conjugate gradient (CG) algorithm) consumed no more than 5% of the total CPU time (see Figure 5.4). Such vector operations (see <ref> [DBMS79] </ref>) must be synchronized across the 8 processors of the Alliant FX/80, and large numbers of memory fetches for vectors of length 1033 easily stress the memory hierarchy of the Alliant FX/80 which supports a 4-way interleaved 128 kilobyte cache between main memory and the 8 processors (see [Alli87a]).
Reference: [DDHH88] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> An extended set of Fortran basic linear algebra subprograms. </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference: [DoSo87] <author> J. Dongarra and D. Sorensen. </author> <title> A fast algorithm for the symmetric eigenvalue problem. </title> <note> SIAM J. </note> <institution> Sci. Stat. Comput., 8(2):s139-s154, </institution> <year> 1987. </year>
Reference-contexts: We note 40 that on a multiprocessor architecture, step (2) in Table 4.3 may benefit from any available optimized library routine that solves the symmetric tridiagonal eigenvalue problem (eg., multisectioning in [LoPS87] or divide and conquer in <ref> [DoSo87] </ref>). Theoretically, it is easy to prove that for the basic Lanczos recursion that orthogonal-ization with respect to only the two most recently-generated Lanczos vectors is sufficient to guarantee that each succeeding Lanczos vector is orthogonal with respect to all previously generated Lanczos vectors (see [CuWi85] or [Parl80]).
Reference: [DuGG89] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference: [DuFL88] <author> S. T. Dumais, G. W. Furnas, and T. K. Landauer. </author> <title> Using latent semantic analysis to improve access to textual information. </title> <booktitle> In Proceedings of Computer Human Interaction '88, </booktitle> <year> 1988. </year>
Reference-contexts: In the following sections, we discuss the need for computing singular triplets of large sparse matrices as suggested by recent research in engineering, physical and social science. 2.1 Latent Semantic Indexing In <ref> [DuFL88] </ref> and [DDFL90] a new approach to automatic indexing and retrieval is discussed. It is designed to overcome a fundamental problem that plagues existing information retrieval techniques that try to match words of queries with words of documents.
Reference: [GaJM87] <author> K. Gallivan, W. Jalby, and U. Meier. </author> <title> The use of BLAS3 in linear algebra on a parallel processor with a hierarchical memory. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 18(6) </volume> <pages> 1079-1084, </pages> <year> 1987. </year>
Reference-contexts: On multiprocessor architectures, especially those having hierarchical memories such as the Alliant FX/80 and Cray-2, one may achieve high performance (with a slight increase in the total number of arithmetic operations) by using either a block Gram-Schmidt or block Householder orthogonalization method in step (2). As discussed in <ref> [GaJM87] </ref>, significant improvements in the algorithmic performance of fundamental linear algebra kernels may be gained through the improved data locality associated with block-based methods. <p> To form a section of (4.33) as specified by (4.34), we must first orthogonalize the columns of Y k , i.e., y (k) can apply a (block) modified Gram-Schmidt orthogonalization procedure (see <ref> [GaJM87] </ref>) to orthogonalize the columns of Y k , and then obtain (4.34) by determining the spectral decomposition of F k = Y T k W T F k W = ~ ; (4.47) where W is an orthogonal matrix of order s.
Reference: [GoKa65] <author> G. Golub and W. Kahan. </author> <title> Calculating the singular values and pseudoinverse of a matrix. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 2(3) </volume> <pages> 205-224, </pages> <year> 1965. </year>
Reference-contexts: method easily parallelizable, neither of these methods can exploit the sparsity of the matrix A in (1.1). 3.1 Golub-Kahan-Reinsch Method Perhaps the most well-known and widely used algorithm for computing the singular value decomposition of an m fi n (m n) rectangular matrix is the Golub-Kahan-Reinsch algorithm (see [GoRe71] and <ref> [GoKa65] </ref>). As implemented in the LINPACK library [DBMS79], this algorithm consists of two phases. <p> Combining (4.15) and (4.16) then produces ([Simo84]) fi i+1 w T To motivate the block Lanczos SVD method discussed in the succeeding section we consider the application of the Lanczos procedure outlined in Table 4.3 to the 2-cyclic matrix B in (4.1) with a particular starting vector (see <ref> [GoKa65] </ref>). The resulting single-vector Lanczos SVD procedure, however, can only compute the distinct singular values of an mfin matrix A and not their multiplicities. As shown in [CuWi85], these multiplicities can be recovered along with the suppression of zero eigenvalues of the matrix B caused by m 6= n.
Reference: [GoLO81] <author> G. H. Golub, F. T. Luk, and M. L. Overton. </author> <title> A block Lanczos method for computing the singular values and corresponding singular vectors of a matrix. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(2) </volume> <pages> 149-169, </pages> <year> 1981. </year>
Reference-contexts: There are a few options for the reduction of J k to the bidiagonal matrix, B k . Golub, Luk, and Overton in <ref> [GoLO81] </ref> advocated the use of either band Householder or band Givens methods which in effect chase off (or zero) elements on the diagonals above the first super-diagonal of J k . <p> This technique was suggested by Golub, Luk, and Overton in <ref> [GoLO81] </ref>. Given a block size b (usually b p, where p is the desired number of triplets), the number of diagonal blocks, d, for J k , is defined as bc=bc, where bc denotes truncation of the mantissa.
Reference: [GoRe71] <author> G. Golub and C. Reinsch. </author> <title> Singular value decomposition and least squares solutions. In Handbook for Automatic Computation II, Linear Algebra, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1971. </year> <month> 165 </month>
Reference-contexts: The three results in Theorem 1.1 can be combined to yield the following quantification of matrix rank deficiency (see <ref> [GoRe71] </ref> for a proof): Theorem 1.2 [Eckart and Young] Let the SVD of A be given by (1.1) with r = rank (A) p = min (m,n) and define: A k = i=1 i with k &lt; r ; then min kA Bk = kA A k k = k+1 r <p> the latter method easily parallelizable, neither of these methods can exploit the sparsity of the matrix A in (1.1). 3.1 Golub-Kahan-Reinsch Method Perhaps the most well-known and widely used algorithm for computing the singular value decomposition of an m fi n (m n) rectangular matrix is the Golub-Kahan-Reinsch algorithm (see <ref> [GoRe71] </ref> and [GoKa65]). As implemented in the LINPACK library [DBMS79], this algorithm consists of two phases. <p> The second phase reduces B k to diagonal form by a modified QR algorithm. This diagonalization procedure is discussed in detail in <ref> [GoRe71] </ref>. The resulting diagonalized B k will yield the approximate singular values of A and the corresponding left and right 48 singular vectors are determined by products of all the left and right transformations (respectively) used in both phases of the SVD of J k .
Reference: [GoUn77] <author> G. H. Golub, and R. R. Underwood. </author> <title> The block Lanczos method for computing eigenvalues. </title> <booktitle> In Mathematical Software III, </booktitle> <publisher> Academic Press, </publisher> <address> New York, 361-377, </address> <year> 1977. </year>
Reference-contexts: Following the block Lanczos recursion for the sparse symmetric 46 eigenvalue problem ([Unde75], <ref> [GoUn77] </ref>), (4.17) may be represented in matrix form as A T ^ U k = ^ V k J T A ^ V k = ^ U k J k + ~ Z k ; (4.18) where ^ U k = [u 1 ; ; u k ], ^ V k
Reference: [GoVL89] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations, Second Edition, </title> <publisher> Johns Hopkins, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: The method of choice in this case proved to be a band Givens algorithm using fast Givens rotations (see <ref> [GoVL89] </ref>) which required approximately 8 (bk) 3 =3 multiplications to determine the bidiagonal matrix, B k , from J k with accumulation of orthogonal transformations. <p> To estimate the eigenvalue spectrum of J T k J k we can employ the Gershgorin's theorem (see <ref> [GoVL89] </ref>, p.341) after the formation of each upper block bidiagonal matrix, J k . <p> Following [SaWi82], we may solve for h (k) Q T ~ BQ 2 h j = Q T ~ By j ; (4.42) via the conjugate gradient method, <ref> [GoVL89] </ref>, and recover d (k) d j = Q k ~ d j = Q 2 h j : (4.43) Using the fact that I Q 1 Q T 1 = Q 2 Q T 2 is the orthogonal projector onto the nullspace of Y k and (4.43), the conjugate gradient
Reference: [Grca81] <author> J. Grcar. </author> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1981. </year>
Reference-contexts: Table 4.3: Single-Vector Lanczos Recursion. As with subspace iteration, the matrix B is only referenced through matrix-vector multiplication in Table 4.3. At each iteration, the basic Lanczos recursion requires only the two most recently-generated vectors, although for finite-precision arithmetic modifications suggested by Grcar <ref> [Grca81] </ref>, Parlett and Scott [PaSc79], and Simon [Simo84] require additional Lanczos vectors to be readily accessible via secondary storage.
Reference: [Hest58] <author> M. R. Hestenes. </author> <title> Inversion of matrices by biorthogonalization and related results. </title> <journal> J. Soc. Indust. Appl. Math., </journal> <volume> 6 </volume> <pages> 51-90, </pages> <year> 1958. </year>
Reference: [Kani66] <author> S. Kaniel. </author> <title> Estimates for some computational techniques in linear algebra. </title> <journal> Math. Comp., </journal> <volume> 20 </volume> <pages> 369-378, </pages> <year> 1966. </year>
Reference: [KDLS86] <author> D. Kuck, E. Davidson, D. Lawrie, and A. Sameh. </author> <title> Parallel supercomputing today and the CEDAR approach. </title> <journal> Science, </journal> <volume> 231 </volume> <pages> 967-974, </pages> <year> 1986. </year>
Reference: [Lanc61] <author> C. </author> <title> Lanczos. Linear Differential Operators. </title> <publisher> Van Nostrand, </publisher> <address> New York, </address> <year> 1961. </year>
Reference: [Luen73] <author> D. G. Luenberger. </author> <title> Introduction to Linear and Nonliner Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: The corrections k in this case are selected to be orthogonal to the previous estimates Y k (4.36), i.e., so that (see <ref> [Luen73] </ref>) T We then recast (4.37) as 0 B @ Y T 1 C A B B d j 1 C A 0 B @ (k) 0 C C ; j = 1; 2; : : : ; p; (4.39) where 2l is a vector of order p reflecting the Lagrange
Reference: [LoPS87] <author> S. Lo, B. Phillipe, and A. Sameh. </author> <title> A multiprocessor algorithm for the symmetric tridiagonal eigenvalue problem. </title> <note> SIAM J. </note> <institution> Sci. Stat. Comput., 8(2):s155-s165, </institution> <year> 1987. </year>
Reference-contexts: We note 40 that on a multiprocessor architecture, step (2) in Table 4.3 may benefit from any available optimized library routine that solves the symmetric tridiagonal eigenvalue problem (eg., multisectioning in <ref> [LoPS87] </ref> or divide and conquer in [DoSo87]).
Reference: [MaWi68] <author> R. S. Martin and J. H. Wilkinson. </author> <title> The implicit QL Algorithm. </title> <journal> Num. Math., </journal> <volume> 12 </volume> <pages> 377-383, </pages> <year> 1968. </year>
Reference-contexts: It is also interesting to note that the computation of eigenval-ues and accumulation of orthogonal transformations (for recovering eigenvectors) of the symmetric tridiagonal matrix T j in (4.10) as determined by the implicit QL method (see <ref> [MaWi68] </ref>) and implemented by an optimized version of the EISPACK routine, IMTQL2, requires approximately 12% of the total CPU time when the eigensystem of the order 1033 matrix A T A is determined but as much as 34% of the total CPU time if we are computing the eigensystem of the
Reference: [Paig76] <author> C. C. Paige. </author> <title> Error analysis of the Lanczos algorithms for tridiagonalizing a symmetric matrix. </title> <journal> J. Inst. Math. Appl., </journal> <volume> 18 </volume> <pages> 341-349, </pages> <year> 1976. </year>
Reference: [Parl80] <author> B. N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: Difficulties in approximating the smallest singular values by any of the three equivalent symmetric eigenvalue problems will be presented in Chapter 6. 4.2 Subspace Iteration Subspace iteration is perhaps one of the simplest algorithms used to solve large sparse eigenvalue problems. As discussed in <ref> [Parl80] </ref>, it can be viewed as a block generalization 30 of the classical power method. <p> Theoretically, it is easy to prove that for the basic Lanczos recursion that orthogonal-ization with respect to only the two most recently-generated Lanczos vectors is sufficient to guarantee that each succeeding Lanczos vector is orthogonal with respect to all previously generated Lanczos vectors (see [CuWi85] or <ref> [Parl80] </ref>). In one sense, the Lanczos procedure can be viewed as the Gram-Schmidt orthogonalization of the set of Krylov vectors w 1 ; Bw 1 ; :::; B k1 w 1 . <p> Table 4.8: Modified Gram-Schmidt Orthogonalization Procedure for Section Formation with Polynomial Acceleration in TRSVD. 71 4.5.3 Shifting Strategy for TRSVD As discussed in [SaWi82], we can also accelerate the convergence of the Y k to eigen-vectors of ~ B (and hence singular vectors of A) by incorporating Ritz shifts (see <ref> [Parl80] </ref>) into TRSVD. <p> However, the strategy outlined in Table 4.9 has been quite successful in maintaining global convergence with shifting. This strategy is based upon the result of the following theorem (see <ref> [Parl80] </ref>, p. 318): Theorem 4.4 For an arbitrary nonzero vector u and scalar -, 9 an exact singular value of A, , such that jfl -j k ~ Bk=kuk : 72 Since we may encounter a nondescent step in any conjugate gradient (CG) iteration when solving P ( ~ B - <p> This is acceptable if ~ l ~ j &lt; fl j ~ j &lt; kr j k ; where r (k) j is the j-th residual vector (4.2) for ~ (k) (k) l . As demonstrated in <ref> [Parl80] </ref> and [Wilk65], optimal acceleration (cubic rate of convergence) will occur if ~ l is used as a shift for y (k) l (see Step (2) of Table 4.9). 73 Let i = minfj j (~ (k) (k) j ) has not convergedg. <p> We point out, however, that alternative operators of the form (shift and 120 invert) (A T A ~ 2 I) 1 ; where ~ is a good approximation to an exact singular value, , of A, can also be used to determine the p-smallest singular values of A (see <ref> [Parl80] </ref> and [Ruti70]). However, means of accurately determining either the Cholesky factorization of A T A when A is a large and sparse rectangular matrix is not considered in this investigation.
Reference: [PaRe70] <author> B. N. Parlett and J. K. Reid. </author> <title> On the solution of a system of linear equations whose system is symmetric but not definite. </title> <journal> BIT, </journal> <volume> 10 </volume> <pages> 386-397, </pages> <year> 1970. </year>
Reference: [PaSc79] <author> B. N. Parlett and D. S. Scott. </author> <title> The Lanczos algorithm with selective reorthog-onalization. </title> <journal> Math. Comp., </journal> <volume> 33 </volume> <pages> 217-238, </pages> <year> 1979. </year>
Reference-contexts: Table 4.3: Single-Vector Lanczos Recursion. As with subspace iteration, the matrix B is only referenced through matrix-vector multiplication in Table 4.3. At each iteration, the basic Lanczos recursion requires only the two most recently-generated vectors, although for finite-precision arithmetic modifications suggested by Grcar [Grca81], Parlett and Scott <ref> [PaSc79] </ref>, and Simon [Simo84] require additional Lanczos vectors to be readily accessible via secondary storage. <p> Eigenpairs of A T A are used to approximate the 10-largest singular triplets of A in each method. Notice that we have deliberately omitted LASVD from Figure 5.13. This is primarily due to the fact that the single-vector Lanczos procedure (LANSO, see <ref> [PaSc79] </ref>) we have used for LASVD does not produce corresponding left and right singular vectors (Step (4) in Table 4.3) until all approximations to the singular values of A (eigenvalues of B) have converged. Thus, we are not able to assess the residual (4.2) after each Lanczos step.
Reference: [PaSa75] <author> C. C. Paige and M. A. Saunders. </author> <title> Solution of sparse indefinite systems of linear equations. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 12(4) </volume> <pages> 617-629, </pages> <year> 1975. </year>
Reference: [PaSa82] <author> C. C. Paige and M. A. Saunders. </author> <title> LSQR: an algorithm for sparse linear equations and sparse least squares. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 8(1) </volume> <pages> 43-71, </pages> <year> 1982. </year> <month> 166 </month>
Reference: [PeWi79] <author> G. Peters and J. H. Wilkinson. </author> <title> Inverse iteration, ill-conditioned equations and Newton's method. </title> <journal> SIAM Review, </journal> <volume> 21(3) </volume> <pages> 339-360, </pages> <year> 1979. </year>
Reference: [Ruti69] <author> H. </author> <title> Rutishauser. Computational Aspects of F.L. Bauer's Simultaneous Iteration Method. </title> <journal> Numer. Math., </journal> <volume> 13 </volume> <pages> 4-13, </pages> <year> 1969. </year>
Reference-contexts: As illustrated in <ref> [Ruti69] </ref>, the existence of z = lim E k ; is assured and the angle, (k) j , between the j-th eigenvector of ~ B and E k would be O s + fl ! k where j is the j-th largest singular value of the m fi n matrix A.
Reference: [Ruti70] <author> H. </author> <title> Rutishauser. Simultaneous iteration method for symmetric matrices. </title> <journal> Nu-mer. Math., </journal> <volume> 16 </volume> <pages> 205-223, </pages> <year> 1970. </year>
Reference-contexts: Hence, one should choose fl to be as small as possible. 31 The most sophisticated implementation of subspace iteration (which is used in this thesis) is that of Rutishauser's ritzit program (see <ref> [Ruti70] </ref>). This particular algorithm incorporates both a Rayleigh-Ritz procedure and acceleration via Chebyshev polynomials. The iteration which embodies the ritzit program is given in Table 4.1. The Rayleigh Quotient matrix, H k , in step (3) is essentially the projection of ~ B 2 onto the span (Z k1 ). <p> point out, however, that alternative operators of the form (shift and 120 invert) (A T A ~ 2 I) 1 ; where ~ is a good approximation to an exact singular value, , of A, can also be used to determine the p-smallest singular values of A (see [Parl80] and <ref> [Ruti70] </ref>). However, means of accurately determining either the Cholesky factorization of A T A when A is a large and sparse rectangular matrix is not considered in this investigation.
Reference: [Saad80] <author> Y. Saad. </author> <title> On the rates of convergence of the Lanczos and the block-Lanczos methods. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 17 </volume> <pages> 687-706, </pages> <year> 1980. </year>
Reference-contexts: Then, 0 i i ( i inf ) K i tan ( i ; y L ) ; (4.14) where T q (x) is the Chebyshev polynomial of order q given by (4.4). The proof of the theorem follows from the analysis given in <ref> [Saad80] </ref> where the optimal choice of the parameter p is discussed. We note that inspection of the bound in (4.14) reveals that the tighter bounds which minimize the right-hand-side occur for i in neighborhoods of inf or max , where max = max f i g. <p> This result directly follows from the bounds by Saad in <ref> [Saad80] </ref> and Lemma 4.2 for the equivalent symmetric eigenvalue problem A T Av i = 2 where v i is the right singular vector corresponding to the i-th singular value of A, i . 55 Theorem 4.2 Let i be a singular value of an mfin matrix A with its corresponding
Reference: [SaWi82] <author> A. H. Sameh and J. A. Wisniewski. </author> <title> A trace minimization algorithm for the generalized eigenvalue problem. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19(6) </volume> <pages> 1243-1259, </pages> <year> 1982. </year>
Reference-contexts: hybrid block Lanczos SVD method for computing the 20-largest singular triplets of the ADI matrix in Table 2:1 on 1 CPU of the CRAY-2S/4128. 61 4.5 Trace Minimization Method The final SVD scheme for sparse matrices we consider in this thesis is based upon the trace minimization algorithm discussed in <ref> [SaWi82] </ref> and [Wisn81] for the generalized eigenvalue problem Hx = Gx ; (4.32) where H and G are symmetric and G is also positive definite. <p> From Theorem 4.3, the matrix Y in (4.34) which minimizes trace (Y T ~ BY ) is the matrix of ~ B-eigenvectors associated with the p-smallest eigenvalues of the problem (4.33). As discussed in <ref> [SaWi82] </ref> and [Wisn81], F (Y ) can be chosen so that global convergence is assured. <p> Since the original matrix A is assumed to be large, sparse, and without any particular sparsity structure (pattern of nonzeros) we have chosen an iterative method (conjugate gradient) for the systems in (4.39). Following <ref> [SaWi82] </ref> and dropping the subscripts in (4.39), let Y k = Q k R = [Q 1 ; Q 2 ]R ; (4.40) 64 be the orthogonal factorization of Y k so that R T = [ ~ R T ; 0], ~ R is pfip and upper triangular, Q T <p> If we partition ~ d (k) (k) (k) j ] so that g (k) j is a vector of order p, then we must have g (k) j = 0 since R T ~ d (k) j = 0 and ~ R is nonsingular. Following <ref> [SaWi82] </ref>, we may solve for h (k) Q T ~ BQ 2 h j = Q T ~ By j ; (4.42) via the conjugate gradient method, [GoVL89], and recover d (k) d j = Q k ~ d j = Q 2 h j : (4.43) Using the fact that <p> We note that the first m and last n components of y (k) j approximate left and right singular vectors 66 of A, respectively. Following the discussion of convergence rates for trace minimization in <ref> [SaWi82] </ref> when applied to (4.32), we have t (y j ) = ( j = p ) 2 ; which corresponds to the convergence rate of subspace iteration in (4.3). <p> Table 4.8: Modified Gram-Schmidt Orthogonalization Procedure for Section Formation with Polynomial Acceleration in TRSVD. 71 4.5.3 Shifting Strategy for TRSVD As discussed in <ref> [SaWi82] </ref>, we can also accelerate the convergence of the Y k to eigen-vectors of ~ B (and hence singular vectors of A) by incorporating Ritz shifts (see [Parl80]) into TRSVD.
Reference: [ScDG90] <author> J. A. Scales, P. Dochery, and A. Gerszternkorn. </author> <title> Regularization of nonlinear inverse problems: imaging the near-surface weathering layer. Inverse Problems, </title> <booktitle> 6(1) </booktitle> <pages> 115-131, </pages> <year> 1990. </year>
Reference-contexts: In recent literature (see <ref> [ScDG90] </ref>, [ScGe88], and [BGLS87]), great interest has been expressed in the ability to compute either the complete sparse SVD or a few of the smallest singular values and corresponding singular vectors. <p> Data Columns Rows Nonzeros Density c r AMOCO1 330 1436 35210 7.43 106.7 24.5 AMOCO2 8754 9855 1159116 1.34 132.4 117.6 Table 2.2: Sparse Jacobian Matrix Specifications from Seismic Tomography . As discussed in <ref> [ScDG90] </ref>, the linear least squares solution for (2.3) is usually deter mined using the pseudo-inverse D y = V k 1 k ; where k = rank (D), k = diag ( 1 ; 2 ; :::; k ), and U T k U k = V T k V k <p> Since long-wavelength rather than short-wavelength trends are usually observed from seismic travel times, inversion for velocity trends alone using travel times, t (r), will force the vectors of the smallest singular triplets to be high frequency. However, as discussed in <ref> [ScDG90] </ref> and [ScGe88], inversion for subsurface velocity ((x; y; z)) and depths-to-reflectors from travel times involves a fundamental trade-off between velocity and reflector position: increasing (decreasing) the velocity above a reflector while simultaneously increasing (decreasing) position does not affect the travel times.
Reference: [ScGe88] <author> J. A. Scales and A. Gerszternkorn. </author> <title> Robust methods in inverse theory. Inverse Problems, </title> <type> 4 </type> <month> 1071-1091 </month> <year> 1988. </year>
Reference-contexts: In recent literature (see [ScDG90], <ref> [ScGe88] </ref>, and [BGLS87]), great interest has been expressed in the ability to compute either the complete sparse SVD or a few of the smallest singular values and corresponding singular vectors. <p> Since long-wavelength rather than short-wavelength trends are usually observed from seismic travel times, inversion for velocity trends alone using travel times, t (r), will force the vectors of the smallest singular triplets to be high frequency. However, as discussed in [ScDG90] and <ref> [ScGe88] </ref>, inversion for subsurface velocity ((x; y; z)) and depths-to-reflectors from travel times involves a fundamental trade-off between velocity and reflector position: increasing (decreasing) the velocity above a reflector while simultaneously increasing (decreasing) position does not affect the travel times.
Reference: [Simo84] <author> H. Simon. </author> <title> Analysis of the symmetric Lanczos algorithm with reorthogonal-ization methods. </title> <journal> Lin. Alg. and Its Appl., </journal> <volume> 61 </volume> <pages> 101-131, </pages> <year> 1984. </year>
Reference-contexts: As with subspace iteration, the matrix B is only referenced through matrix-vector multiplication in Table 4.3. At each iteration, the basic Lanczos recursion requires only the two most recently-generated vectors, although for finite-precision arithmetic modifications suggested by Grcar [Grca81], Parlett and Scott [PaSc79], and Simon <ref> [Simo84] </ref> require additional Lanczos vectors to be readily accessible via secondary storage. We note 40 that on a multiprocessor architecture, step (2) in Table 4.3 may benefit from any available optimized library routine that solves the symmetric tridiagonal eigenvalue problem (eg., multisectioning in [LoPS87] or divide and conquer in [DoSo87]). <p> In this thesis, we employ the most recent version of a single-vector Lanczos algorithm (4.9) equipped with a selective reorthogonalization strategy, LANSO, designed by Par-lett and his colleagues at The University of California at Berkeley ([PaSc79], <ref> [Simo84] </ref>). This particular method is primarily designed for the standard and generalized symmetric eigenvalue problem. We simply apply it to either B = A T A or the 2-cyclic matrix B defined in (4.1). The selective reorthogonalization strategy used in this case is well motivated in [Simo84]. <p> California at Berkeley ([PaSc79], <ref> [Simo84] </ref>). This particular method is primarily designed for the standard and generalized symmetric eigenvalue problem. We simply apply it to either B = A T A or the 2-cyclic matrix B defined in (4.1). The selective reorthogonalization strategy used in this case is well motivated in [Simo84].
Reference: [SBDG76] <author> B. Smith, J. Boyce, J. Dongarra, B. Garbow, Y. Ikebe, V. Klema, and C. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide. </title> <publisher> Springer, Berlin, </publisher> <editor> 2nd ed., </editor> <year> 1976. </year>
Reference: [Unde75] <author> R. R. Underwood. </author> <title> An iterative block Lanczos method for the solution of large sparse symmetric eigenproblems. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> May </month> <year> 1975. </year>
Reference-contexts: Although the bound in (4.29) is somewhat tighter than that which was considered by Underwood in <ref> [Unde75] </ref> for the symmetric eigenvalue problem, both results clearly indicate the desire for b (block size) to be chosen so that i i+b is as large as possible.
Reference: [VaDM88] <author> J. Vanderwalle and B. De Moor. </author> <title> A variety of applications of singular value decomposition in identification and signal processing. In SVD and Signal Processing, Algorithms, Applications, and Architectures, </title> <publisher> Elsevier, </publisher> <address> Amster-dam, </address> <year> 1988. </year>
Reference-contexts: This particular matrix was supplied by Amoco Research Center 4 . 2.4 Other Applications As outlined in <ref> [VaDM88] </ref>, the SVD is rich in its application in several areas of engineering.
Reference: [Varg62] <author> R. S. Varga. </author> <title> Matrix Iterative Analysis. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1962. </year>
Reference-contexts: The following Lemma illustrates the fundamental relations between these symmetric eigenvalue problems and the SVD. 1 A non-negative irreducible matrix B which is 2-cyclic has 2 eigenvalues of modulus (B), where (B) is the spectral radius of B. See Definition 2.2 on page 35 in <ref> [Varg62] </ref>. 28 Lemma 4.2 Let A be an m fi n (m n) matrix with rank (A) = r. 1.
Reference: [Wilk65] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1965. </year>
Reference-contexts: Accordingly, our appropriate trace minimization SVD scheme (TRSVD) is then based upon the following theorem which is a direct consequence of the Courant-Fischer theorem (see <ref> [Wilk65] </ref>). Without loss of generality, let us assume that H = ~ B, G = I m+n and consider the associated symmetric eigensystem of order m + n. <p> This is acceptable if ~ l ~ j &lt; fl j ~ j &lt; kr j k ; where r (k) j is the j-th residual vector (4.2) for ~ (k) (k) l . As demonstrated in [Parl80] and <ref> [Wilk65] </ref>, optimal acceleration (cubic rate of convergence) will occur if ~ l is used as a shift for y (k) l (see Step (2) of Table 4.9). 73 Let i = minfj j (~ (k) (k) j ) has not convergedg.
Reference: [Wilk72] <author> J. H. Wilkinson. </author> <title> Inverse iteration in theory and in practice. in Symposia Mathematica X, </title> <publisher> Academic Press, London, </publisher> <pages> 361-379, </pages> <year> 1972. </year> <month> 167 </month>
Reference-contexts: In other words, we simply use our most recent approximations to the eigenvalues of ~ B from our k-th section within TRSVD as Ritz shifts. As was shown by Wilkinson in <ref> [Wilk72] </ref>, the Rayleigh quotient iteration associated with (4.49) will ultimately achieve cubic convergence to fl j , where j is an exact singular value of A, provided - (k) j is sufficiently close to fl j .
Reference: [Wisn81] <author> J. A. Wisniewski. </author> <title> On solving the large sparse generalized eigenvalue problem. </title> <type> PhD thesis, </type> <institution> The University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1981. </year> <month> 168 </month>
Reference-contexts: Lanczos SVD method for computing the 20-largest singular triplets of the ADI matrix in Table 2:1 on 1 CPU of the CRAY-2S/4128. 61 4.5 Trace Minimization Method The final SVD scheme for sparse matrices we consider in this thesis is based upon the trace minimization algorithm discussed in [SaWi82] and <ref> [Wisn81] </ref> for the generalized eigenvalue problem Hx = Gx ; (4.32) where H and G are symmetric and G is also positive definite. <p> From Theorem 4.3, the matrix Y in (4.34) which minimizes trace (Y T ~ BY ) is the matrix of ~ B-eigenvectors associated with the p-smallest eigenvalues of the problem (4.33). As discussed in [SaWi82] and <ref> [Wisn81] </ref>, F (Y ) can be chosen so that global convergence is assured.
References-found: 55

