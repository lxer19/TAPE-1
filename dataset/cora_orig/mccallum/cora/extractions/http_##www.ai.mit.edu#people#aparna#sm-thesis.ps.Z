URL: http://www.ai.mit.edu/people/aparna/sm-thesis.ps.Z
Refering-URL: http://www.ai.mit.edu/people/aparna/aparna.html
Root-URL: 
Title: The Role of Fixation and Visual Attention in Object Recognition  
Author: Aparna Lakshmi Ratan 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology,  
Date: 1529 January, 1995  1995  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Technical Report No.  
Abstract: This research project is a study of the role of fixation and visual attention in object recognition. In this project, we build an active vision system which can recognize a target object in a cluttered scene efficiently and reliably. Our system integrates visual cues like color and stereo to perform figure/ground separation, yielding candidate regions on which to focus attention. Within each image region, we use stereo to extract features that lie within a narrow disparity range about the fixation position. These selected features are then used as input to an Alignment-style recognition system. We show that visual attention and fixation significantly reduce the complexity and the false identifications in model-based recognition using Alignment methods. We also demonstrate that stereo can be used effectively as a figure/ground separator without the need for accurate camera calibration. This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for this research was provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-94-01-0994 and AFOSR contract F49620-93-1-0604. The author was also supported by NSF-9407173-IRI. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Aloimonos, </author> <title> "Active Perception", </title> <publisher> Lawrence Erlbaum Assoc., Publishers, </publisher> <year> 1993. </year>
Reference-contexts: An active (animate) vision framework takes advantage of fixation and keeps the fovea over a given spatial target (gaze control), changes focus and changes point of view while investigating a scene. The "active vision" paradigm has been discussed in papers such as <ref> [1] </ref>, [6],[5] among others. Most of the work in the field of active vision has been concerned with low level tasks like gaze control [1], [52], [51], [15], [16]. <p> The "active vision" paradigm has been discussed in papers such as <ref> [1] </ref>, [6],[5] among others. Most of the work in the field of active vision has been concerned with low level tasks like gaze control [1], [52], [51], [15], [16]. The importance of camera movements and adjustment of imaging parameters in stereo vision has been investigated by Ballard in [7], Abbot and Ahuja in [2] and Bajcsy in [4]. Knowledge of verging geometry has been used by Krotkov et al. [35] to address calibration issues.
Reference: [2] <author> A.L. Abbott and Narendra Ahuja, </author> <title> "Surface reconstruction by dyanamic integration of focus, camera vergence, and stereo", </title> <month> ICCV </month> <year> 1988. </year>
Reference-contexts: Most of the work in the field of active vision has been concerned with low level tasks like gaze control [1], [52], [51], [15], [16]. The importance of camera movements and adjustment of imaging parameters in stereo vision has been investigated by Ballard in [7], Abbot and Ahuja in <ref> [2] </ref> and Bajcsy in [4]. Knowledge of verging geometry has been used by Krotkov et al. [35] to address calibration issues. A system that integrates information from focus, vergence angle, and stereo disparity over multiple fixations to get accurate depth estimates was proposed by Abbot and Ahuja [2]. <p> and Ahuja in <ref> [2] </ref> and Bajcsy in [4]. Knowledge of verging geometry has been used by Krotkov et al. [35] to address calibration issues. A system that integrates information from focus, vergence angle, and stereo disparity over multiple fixations to get accurate depth estimates was proposed by Abbot and Ahuja [2]. Vergence control has been used by Olson [50] to simplify stereopsis by limiting the disparity range to provide relative depth information over single fixations to be used in building qualitative descriptions for recognition. Controlled eye movements have been used to obtain geometric information for camera calibration [10].
Reference: [3] <author> N. Ayache and B. Faverjon, </author> <title> "Efficient registration of stereo images by matching graph descriptions of edge segments", </title> <booktitle> IJCV 1(2), </booktitle> <pages> 107-131, </pages> <year> 1987. </year>
Reference-contexts: One solution to this problem is to use attributes of nearby features <ref> [3] </ref>, [46], [40] and another is to alter the control strategy. 51 Since we are interested in finding roughly contiguous 3D regions to select out groups of image features that are likely to come from a single object, we use a control method fixates a target, searches for matching features in <p> This is similar to the working of the human stereo system where the fusible range of disparities is restricted around the fixation point (Panum's area). The stereo algorithm implemented here is a modified version of Grimson's stereo matcher ([20]). It is similar to earlier stereo algorithms <ref> [3] </ref> and [46], [40], [37], [41] and uses ideas about the human stereo system, Panum's area and the role of eye movements in stereopsis as discussed in [37], [41], [24] and [50].
Reference: [4] <author> R. </author> <title> Bajcsy, "Active Perception vs Passive Perception", </title> <booktitle> Proc. Third IEEE Workshop on Computer Vision, </booktitle> <volume> 55 - 59, </volume> <month> Oct </month> <year> 1985. </year>
Reference-contexts: The importance of camera movements and adjustment of imaging parameters in stereo vision has been investigated by Ballard in [7], Abbot and Ahuja in [2] and Bajcsy in <ref> [4] </ref>. Knowledge of verging geometry has been used by Krotkov et al. [35] to address calibration issues. A system that integrates information from focus, vergence angle, and stereo disparity over multiple fixations to get accurate depth estimates was proposed by Abbot and Ahuja [2].
Reference: [5] <author> R. Bajcsy and M. Campos, </author> <title> "Active and Exploratory Vision", CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> Vol. 56, </volume> <pages> 31-40, </pages> <month> July </month> <year> 1992. </year>
Reference: [6] <author> D. Ballard, </author> <title> "Animate Vision." </title> <journal> Artificial Intelligence, </journal> <volume> Vol 48, </volume> <pages> 57-86, </pages> <year> 1991 </year>
Reference-contexts: Thus, we see that visual attention gives a convenient way to combine and integrate information provided by several visual cues in order to perform selection. 17 1.2.2 Active Vision Fixation plays an important role in biological and machine vision, especially in binocular stereo. As Ballard mentions in <ref> [6] </ref>, the human eye is different from cameras in that it has much better resolution in a small region around the optical axis. This region is called the fovea. The resolution over the fovea is much better than in the periphery.
Reference: [7] <author> D. Ballard, </author> <title> "Eye Movement and Visual Cognition", Proc. Work. on Spatial Reasoning and Multi Sensor Fusion, </title> <year> 1987 </year>
Reference-contexts: Most of the work in the field of active vision has been concerned with low level tasks like gaze control [1], [52], [51], [15], [16]. The importance of camera movements and adjustment of imaging parameters in stereo vision has been investigated by Ballard in <ref> [7] </ref>, Abbot and Ahuja in [2] and Bajcsy in [4]. Knowledge of verging geometry has been used by Krotkov et al. [35] to address calibration issues.
Reference: [8] <author> M. Bober, P.Hoad, J.Mataas, P. Remagnino, J.Kittler, J. Illingworth, </author> <title> "Control of perception in an Active Vision System: Sensing and Interpretation", </title> <booktitle> IROS, </booktitle> <year> 1993. </year>
Reference-contexts: In their paper, they give a framework for combining feature maps using active vision techniques. However they focused more on the low level issues of building the head and gaze control. Bober et al. <ref> [8] </ref> actively control the sensor based on the goal to be accomplished. <p> He exploits knowledge about the environment to simplify visual and motor processing in an agent that performs the specific tasks of navigation and place recognition. 1.3 Our Approach Our system is similar in spirit to Ferrier and Clark [15] and Bober et al. <ref> [8] </ref> in that it investigates the role of fixation and visual attention to perform the higher level task of object recognition.
Reference: [9] <author> R.C. Bolles and R.A. Cain, </author> <title> "Recognizing and locating partially visible objects: The local feature-focus method", </title> <journal> International Journal of Robotics Research, </journal> <volume> 1(3) </volume> <pages> 57-82 48, </pages> <year> 1982. </year>
Reference-contexts: Thus, we have to account for errors in 57 the selection mechanism and see how it affects the recognition process in terms of false positives and false negatives. 5.2 The Recognition System There are a number of recognition systems in the literature [29], [21], [22], [64], <ref> [9] </ref> that recognize rigid objects using a geometric description of the model. These systems have a geometric description of the model in terms of features like points and lines. <p> The difference between the various recognition methods lies in the way in which they approach the combinatorics that results from examining all matches between model and image features to get the correct transformation. We have correspondence-space based methods [21], [22], <ref> [9] </ref> that explore the space of all possible matches between the model and data features and pruned the search space by using geometric constraints on the model and image features [21] or by using distinctive features on the model to guide the search [9]. <p> We have correspondence-space based methods [21], [22], <ref> [9] </ref> that explore the space of all possible matches between the model and data features and pruned the search space by using geometric constraints on the model and image features [21] or by using distinctive features on the model to guide the search [9]. Another set of methods for recognition are alignment-based methods [29], [64] that explore only a part of the interpretation by matching a small number of model and image features that are sufficient to compute the transform that aligns the model features with the image features.
Reference: [10] <author> C. Brown, </author> <title> "Progress in Image Understanding at University Of Rochester", </title> <booktitle> DARPA IU Workshop, </booktitle> <pages> 73-77, </pages> <year> 1988. </year>
Reference-contexts: Vergence control has been used by Olson [50] to simplify stereopsis by limiting the disparity range to provide relative depth information over single fixations to be used in building qualitative descriptions for recognition. Controlled eye movements have been used to obtain geometric information for camera calibration <ref> [10] </ref>.
Reference: [11] <author> W. Ching, P. Toh, K. Chan and M. </author> <title> Er, "Robust Vergence with Concurrent Detection of Occlusion and Specular Highlights", </title> <journal> ICCV, </journal> <volume> 384 - 394, </volume> <year> 1993. </year>
Reference: [12] <author> R. Deriche and G. Giraudon, </author> <title> "Accurate corner detection: An analytic study", </title> <booktitle> ICCV, </booktitle> <pages> 66-70, </pages> <year> 1990. </year> <month> 101 </month>
Reference: [13] <author> F. Ennesser and G. Medioni, </author> <title> "Finding Waldo, or Focus of Attention Using Local Color Information", </title> <address> CVPR 711-7112, </address> <year> 1993. </year>
Reference: [14] <author> O.D. Faugeras, </author> <title> "What can be seen in three dimensions with an uncalibrated camera rig?", </title> <booktitle> Second ECCV, Italy, </booktitle> <pages> 563-578, </pages> <year> 1992. </year>
Reference-contexts: Faugeras <ref> [14] </ref> has argued that a scene around a moving robot can be constructed and maintained without careful camera calibration. They avoid reconstruction by using relative coordinate systems. In this work, we would like to illustrate a similar idea for the role of stereo in recognition.
Reference: [15] <author> J. J. Clark and N. Ferrier, </author> <title> "Modal control of an attentive vision system", </title> <booktitle> Second ICCV, </booktitle> <volume> 524 - 523, </volume> <year> 1988. </year>
Reference-contexts: This observation has motivated the use of visual attention in object recognition. Hurlbert and Poggio in [28] suggest how the concept of visual attention can be used to reduce the combinatorial search in recognition. There have been a number of computational models [59], <ref> [15] </ref>, [34] of attention proposed in the literature that use this idea. All these models are based on the model of visual attention proposed by Treisman in [62] as a result of psychophysical experiments. <p> All these models are based on the model of visual attention proposed by Treisman in [62] as a result of psychophysical experiments. The Treisman model consists of several low level feature maps which could be combined using a selection filter. The computational models of attention ([34], <ref> [15] </ref> and [59]) mentioned above use different strategies to combine and control the feature maps. In Koch and Ullman's model, the feature maps are combined using a "Winner Take All" mechanism where the network locates the region that differs the most from its neighbors with respect to some property. <p> All the "conspicuity" values are combined into a global saliency map and the network finds the maximum conspicuity value in the global map. The most conspicuous location is where attention is focussed. Clark and Ferrier <ref> [15] </ref> combined the feature maps by assigning a weight to each feature map and combining them using a linear combination of these weighted features. Syeda-Mahmood [59] uses an arbiter module that combines the feature maps and maintains separate saliency maps until the arbiter stage. <p> The "active vision" paradigm has been discussed in papers such as [1], [6],[5] among others. Most of the work in the field of active vision has been concerned with low level tasks like gaze control [1], [52], [51], <ref> [15] </ref>, [16]. The importance of camera movements and adjustment of imaging parameters in stereo vision has been investigated by Ballard in [7], Abbot and Ahuja in [2] and Bajcsy in [4]. Knowledge of verging geometry has been used by Krotkov et al. [35] to address calibration issues. <p> Recognition can be more robust using active and attentive vision since we have the ability to obtain multiple views and can ignore irrelevant information. Ferrier and Clark in <ref> [15] </ref> suggest a form of "active-attentive" vision to focus attention on parts of the scene that is important to the task at hand. In their paper, they give a framework for combining feature maps using active vision techniques. <p> He exploits knowledge about the environment to simplify visual and motor processing in an agent that performs the specific tasks of navigation and place recognition. 1.3 Our Approach Our system is similar in spirit to Ferrier and Clark <ref> [15] </ref> and Bober et al. [8] in that it investigates the role of fixation and visual attention to perform the higher level task of object recognition.
Reference: [16] <author> D.J. Coombs, </author> <title> "Real Time Gaze Holding in Binocular Robot Vision", </title> <type> PhD. Thesis, </type> <institution> Univ Of Rochester, </institution> <year> 1992 </year>
Reference-contexts: The "active vision" paradigm has been discussed in papers such as [1], [6],[5] among others. Most of the work in the field of active vision has been concerned with low level tasks like gaze control [1], [52], [51], [15], <ref> [16] </ref>. The importance of camera movements and adjustment of imaging parameters in stereo vision has been investigated by Ballard in [7], Abbot and Ahuja in [2] and Bajcsy in [4]. Knowledge of verging geometry has been used by Krotkov et al. [35] to address calibration issues.
Reference: [17] <author> W.E.L. </author> <title> Grimson, "The combinatorics of object recognition in cluttered environments using constrained search", </title> <booktitle> Proc. of the International Conference on Computer Vision, </booktitle> <year> 1988. </year>
Reference-contexts: Previous work suggests that selection is one of the key problems in recognition (<ref> [17] </ref>, [18]) since it reduces the expected complexity of recognition and keeps the false positives under control. Grimson shows in [17] that the expected search complexity (using a method called constrained search) can be reduced from exponential to a low order polynomial when all the edge features are known to come from a single object. <p> In Chapter 1, we began with a discussion of the effect of scene clutter on recognition systems. We saw that scene clutter impedes the performance of recognition methods (e.g. <ref> [17] </ref>, [29], [64] among others) and also contributes to the number of false alarms that have to be handled. We argued using previous results [18] that a key component in efficient object recognition is selection or figure/ground separation before model matching.
Reference: [18] <author> W.E.L. </author> <title> Grimson, "Object Recognition by Computer: The Role of Geometric Constraints." </title> <publisher> Cambridge: MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: We now discuss the effects of clutter on the performance of some of these recognition methods. Methods that explore a tree of interpretations using constrained search techniques to find consistent interpretations of the data relative to the model (e.g. <ref> [18] </ref>) have an exponential expected case complexity in the presence of scene clutter. If the clutter can be made relatively small, however, the expected search complexity is reduced to a low order polynomial [18]. <p> interpretations using constrained search techniques to find consistent interpretations of the data relative to the model (e.g. <ref> [18] </ref>) have an exponential expected case complexity in the presence of scene clutter. If the clutter can be made relatively small, however, the expected search complexity is reduced to a low order polynomial [18]. There are other recognition methods known as minimal alignment methods (e.g. [29], [64]), which find a small number of corresponding features between model and data 11 and use the associated transformation to align the model with the data for verifica-tion. <p> The complexity is still a function of scene clutter, however, so in practice clutter can slow down these methods significantly. In both cases, scene clutter also contributes to the number of false alarms that must be handled [23]. All the studies (e.g. <ref> [18] </ref>) on the search space complexity and the effects of scene clutter on it suggest that we need a way to reduce the number of features in the scene and restrict the search to relevant data subsets in the scene while avoiding extraneous information provided by clutter. <p> Previous work suggests that selection is one of the key problems in recognition ([17], <ref> [18] </ref>) since it reduces the expected complexity of recognition and keeps the false positives under control. <p> As we saw in Chapter 1, the problem of isolating regions belonging to a single object in an image is termed the selection (figure/ground separation) problem and has been recognized as a crucial problem in model-based recognition ([17], <ref> [18] </ref>). <p> We saw that scene clutter impedes the performance of recognition methods (e.g. [17], [29], [64] among others) and also contributes to the number of false alarms that have to be handled. We argued using previous results <ref> [18] </ref> that a key component in efficient object recognition is selection or figure/ground separation before model matching.
Reference: [19] <author> W.E.L. </author> <title> Grimson, "A computer implementation of a theory of human stereo vision", </title> <journal> Phil. Trans. Roy. Soc. London, </journal> <volume> vol B 292, </volume> <pages> 217-253, </pages> <year> 1981. </year>
Reference: [20] <author> W.E.L. </author> <title> Grimson, "Computational experiments with feature based stereo algorithm", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol 7, </volume> <month> Jan </month> <year> 1985. </year>
Reference-contexts: If we have an ideal situation with perfect image data of an object isolated from the background, then there are many techniques (e.g. <ref> [20] </ref>, [29] among others) for recognizing the object and its pose. <p> to align all the points of the first model view with the new view and the alignment can then be verified. 5.5 Picking features for recognition In order to benefit from the alignment method, we need a few distinguishing features that are relatively stable and are sufficient for performing alignment <ref> [20] </ref>. If we consider computing alignments using all points along the contour of the model and 62 data as features, then we have to try a large number of alignments even for a simple model.
Reference: [21] <author> W.E.L. Grimson and T. Lozano-Perez, </author> <title> "Model based recognition and localization from sparse range or tactile data", </title> <journal> Intl. Journal Of Robotics Research , 3(3): </journal> <volume> 3 - 35, </volume> <year> 1984. </year>
Reference-contexts: Thus, we have to account for errors in 57 the selection mechanism and see how it affects the recognition process in terms of false positives and false negatives. 5.2 The Recognition System There are a number of recognition systems in the literature [29], <ref> [21] </ref>, [22], [64], [9] that recognize rigid objects using a geometric description of the model. These systems have a geometric description of the model in terms of features like points and lines. <p> The difference between the various recognition methods lies in the way in which they approach the combinatorics that results from examining all matches between model and image features to get the correct transformation. We have correspondence-space based methods <ref> [21] </ref>, [22], [9] that explore the space of all possible matches between the model and data features and pruned the search space by using geometric constraints on the model and image features [21] or by using distinctive features on the model to guide the search [9]. <p> We have correspondence-space based methods <ref> [21] </ref>, [22], [9] that explore the space of all possible matches between the model and data features and pruned the search space by using geometric constraints on the model and image features [21] or by using distinctive features on the model to guide the search [9].
Reference: [22] <author> W.E.L. Grimson and T. Lozano-Perez, </author> <title> "Localizing overlapping parts by searching the interpretation tree", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 9(4) </volume> <pages> 469-482, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Thus, we have to account for errors in 57 the selection mechanism and see how it affects the recognition process in terms of false positives and false negatives. 5.2 The Recognition System There are a number of recognition systems in the literature [29], [21], <ref> [22] </ref>, [64], [9] that recognize rigid objects using a geometric description of the model. These systems have a geometric description of the model in terms of features like points and lines. <p> The difference between the various recognition methods lies in the way in which they approach the combinatorics that results from examining all matches between model and image features to get the correct transformation. We have correspondence-space based methods [21], <ref> [22] </ref>, [9] that explore the space of all possible matches between the model and data features and pruned the search space by using geometric constraints on the model and image features [21] or by using distinctive features on the model to guide the search [9].
Reference: [23] <author> W.E.L. </author> <title> Grimson and D.P. Huttenlocher, "On the sensistivity of the Hough Transform for Object Recognition", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(3) </volume> <pages> 255-274, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The complexity is still a function of scene clutter, however, so in practice clutter can slow down these methods significantly. In both cases, scene clutter also contributes to the number of false alarms that must be handled <ref> [23] </ref>.
Reference: [24] <author> W.E.L Grimson, </author> <title> "Why stereo vision is not always about 3D reconstruction", </title> <institution> MIT AI Lab Memo 1435, </institution> <year> 1993. </year>
Reference-contexts: We demonstrate this as follows: * we show that rough color measures can be used to roughly segment the data without the need for a complex color constancy model. * We also show that stereo can be used effectively as a figure/ground separator 19 without calculating absolute depths <ref> [24] </ref>. Thus, we don't need accurate camera calibration. If we consider selection to be the important part of recognition, and do 3D recognition from 2D by using techniques like linear combination of views [64], then we don't need accurate 3D data for correspondence. <p> This means that we can use relative depths to get feature subsets in the same depth range and avoid extracting absolute depth information entirely. This is useful and interesting for several reasons <ref> [24] </ref>. 1. There has been some physiological evidence to show that the human system does 3D recognition from 2D views. 2. As shown by Grimson in [24] and Olson in [50], small inaccuracies in camera parameters can lead to large errors in depth. <p> This is useful and interesting for several reasons <ref> [24] </ref>. 1. There has been some physiological evidence to show that the human system does 3D recognition from 2D views. 2. As shown by Grimson in [24] and Olson in [50], small inaccuracies in camera parameters can lead to large errors in depth. If we are interested in finding roughly contiguous 3D regions then it is useful to fixate on a target and search for matching features within some disparity range about that point. <p> Keep only the features that fall within the salient regions extracted above. 7. Potential matches between features in the left and right images are computed in the following way using a stereo matcher <ref> [24] </ref> that is described in greater detail in Chapter 4. <p> The methods suggested to find the camera parameters (e.g. [63]) have been shown to be unstable [65]. 4.3 Sensitivity of depth to camera calibration We note the main results of Grimson's analysis of the sensitivity of depth reconstruction from stereo disparities to changes in camera parameters in <ref> [24] </ref>. <p> Grimson uses a perturbation analysis in <ref> [24] </ref> to show that three parameters can lead to large errors. These parameters are * The location of the two principal points. * The focal length. * Gaze angles. <p> From sections 4.3 and 4.4 we can conclude that 47 units of interocular separation). Graphs represent errors in computing gaze angles of 1, 0.5 and 0.25 degrees, from top to bottom. This figure is taken from <ref> [24] </ref>. * small inaccuracies in measuring camera parameters result in large errors in depth. * Selection is a critical part of object recognition and we can avoid explicitly computing 3D distances if we use stereo for selection. <p> It is similar to earlier stereo algorithms [3] and [46], [40], [37], [41] and uses ideas about the human stereo system, Panum's area and the role of eye movements in stereopsis as discussed in [37], [41], <ref> [24] </ref> and [50]. <p> The system illustrates how simple color measures can be used to roughly segment the image into regions that are likely to contain the target object. It also shows that stereo can be used effectively as a figure/ground separator without the need for explicit depth calculations and accurate camera calibration <ref> [24] </ref>. The results in Chapter 6 show that the system performs reliably in cluttered scenes with different objects under varying lighting conditions. Thus, this system demonstrates a method for doing efficient selection which reduces the complexity of the recognition process significantly and keeps the false identifications under control.
Reference: [25] <author> W.E.L. Grimson, A. Lakshmi Ratan, P.A. O'Donnell, G. Klanderman, </author> <title> "An Active Visual Attention System to Play "Where's Waldo"?", </title> <booktitle> Proc. of the Workshop on Visual Behaviors, </booktitle> <pages> 85-90, </pages> <year> 1994. </year>
Reference: [26] <author> I.D. Horswill, "Polly: </author> <title> A vision based Artificial Agent", </title> <booktitle> Eleventh Natl. Conf on AI, </booktitle> <volume> 824 - 829, </volume> <year> 1993. </year>
Reference-contexts: Their system architecture divides a visual task into the categories of camera control, focus of attention control and selection of a suitable recognition strategy and they stress the close interaction between the goal, sensor control and the visual task. Horswill in <ref> [26] </ref> uses a task based approach to perform a higher level task.
Reference: [27] <author> R. Horaud and T. Skordas, </author> <title> "Stereo correspondence through feature grouping and maximal cliques", </title> <journal> IEEE Pattern Analysis and Machine Intelligence, </journal> <volume> 11, </volume> <pages> 1168-1180, </pages> <year> 1989. </year>
Reference: [28] <author> A.Hurlbert and T. Poggio, </author> <title> "Visual attention in brains and computers", </title> <type> Technical Report, </type> <institution> Aritificial Intelligence Lab, M.I.T., AI-Memo-915, </institution> <month> June </month> <year> 1986. </year>
Reference-contexts: This ability of humans to select out relevant parts of a scene relating to a particular task is known as visual attention. This observation has motivated the use of visual attention in object recognition. Hurlbert and Poggio in <ref> [28] </ref> suggest how the concept of visual attention can be used to reduce the combinatorial search in recognition. There have been a number of computational models [59], [15], [34] of attention proposed in the literature that use this idea.
Reference: [29] <author> D.P. Huttenlocher and S.Ullman, </author> <title> "Object Recognition Using Alignment", </title> <booktitle> Proc. First Intl. Conf.Comp. Vision, </booktitle> <pages> 109-111, </pages> <year> 1987. </year>
Reference-contexts: If the clutter can be made relatively small, however, the expected search complexity is reduced to a low order polynomial [18]. There are other recognition methods known as minimal alignment methods (e.g. <ref> [29] </ref>, [64]), which find a small number of corresponding features between model and data 11 and use the associated transformation to align the model with the data for verifica-tion. <p> If we have an ideal situation with perfect image data of an object isolated from the background, then there are many techniques (e.g. [20], <ref> [29] </ref> among others) for recognizing the object and its pose. <p> This kind of selection greatly reduces the search in the correspondence stage where the image data is matched with model data using schemes like Alignment <ref> [29] </ref> or Linear Combination of Views [64]. 16 1.2.1 Visual Attention While there is enough evidence to prove that object selection is a complex task for a machine to perform, it is interesting to note that humans seem to have no difficulty in selecting out parts of a scene that contain <p> These line segments are the features that we use for recognition. As we discussed earlier in this chapter, we want to find the model object in this scene by using Alignment-style recognition techniques (e.g. <ref> [29] </ref>) where we find 3 corresponding points between the model (Figure 1-3) and the image (Figure 1-4 (b)) to compute the transformation that aligns the model with a hypothesized instance of the object in 21 the image and then verify that hypothesis by comparing the transformed model with the image data. <p> Consider a case where we have m = 50 model features and n = 500 data features. If we use Alignment-style recognition techniques (e.g. <ref> [29] </ref>) where we find 3 corresponding points between the model and the image to compute the transformation that aligns the model with a hypothesized instance of the object in the image and then verify the hypothesis by comparing the transformed model with the image data, then the number of alignments that <p> All edges that have a match at this narrow depth of field, together with their neighboring edges (edges that lie close to them) in the image form the input to the recognition engine. 10. Alignment <ref> [29] </ref> is used to determine if the target object is present among the selected edges. The results of the alignment are saved and step 8 is repeated with the next trigger feature for the cameras to fixate (from step 7). <p> Save the selected features and fixate on the next feature obtained from step 3. 8. Once all the candidate features from step 3 have been explored and the respective features collected in step 5, we pass the groups of features obtained in step 5 to a recognition engine <ref> [29] </ref> that aligns the model with the selected feature set and verifies if the object is present in the image or not. 56 Chapter 5 The Recognition System The previous chapters discussed the development of a selection mechanism using color and stereo cues. <p> Thus, we have to account for errors in 57 the selection mechanism and see how it affects the recognition process in terms of false positives and false negatives. 5.2 The Recognition System There are a number of recognition systems in the literature <ref> [29] </ref>, [21], [22], [64], [9] that recognize rigid objects using a geometric description of the model. These systems have a geometric description of the model in terms of features like points and lines. <p> Another set of methods for recognition are alignment-based methods <ref> [29] </ref>, [64] that explore only a part of the interpretation by matching a small number of model and image features that are sufficient to compute the transform that aligns the model features with the image features. We used an alignment-based recognition system. <p> These methods tend to have problems in cluttered environments. We show the advantages of using attentional selection while using alignment-methods to recognize objects in cluttered environments. 5.3 Recognition Using Alignment The recognition system we built uses an alignment-based method developed by Hut-tenlocher and Ullman <ref> [29] </ref>. The design of the recognition system involved picking 58 features to match, building the model and choosing a method for verification. 5.3.1 Alignment Method In this method, the model is represented as a list of 3D-points. The description of the alignment method follows [29]. <p> method developed by Hut-tenlocher and Ullman <ref> [29] </ref>. The design of the recognition system involved picking 58 features to match, building the model and choosing a method for verification. 5.3.1 Alignment Method In this method, the model is represented as a list of 3D-points. The description of the alignment method follows [29]. <p> Definition 2 Given three non-collinear points a m , b m and c m in the plane and three corresponding points a i b i and c i in the plane, it is shown in <ref> [29] </ref> that there exists a unique transformation, Q (x) = U x + b, where U is a symmetric matrix and b is a translation vector, such that (Q (a 0 m )) = a i , (Q (b 0 m )) = c i , where v 0 = (x; <p> Computing the transformation As shown by Huttenlocher in <ref> [29] </ref>, we can use the following algorithm to compute Q and the two-dimensional affine transform A given three pairs of corresponding points (a m ; a i ), (b m ; b i ), (c m ; c i ) where the image points are in two-dimensional image coordinates and the <p> In our system, we approximate the curves in the edge image by line segments and use the junction points where two line segments meet is considered a corner feature. We also use the orientations of edge segments to induce virtual corners <ref> [29] </ref>. Figure 5-1 shows an example of a virtual corner induced at the point of intersection of two extended edge contours. <p> Let a and b be two data points with orientation vectors a i and b i and A is the line passing through a in the direction a i , B is the line through b in the direction b i . It has been shown in <ref> [29] </ref> that if the distance from the two edge points a and b to the intersection point c is large then a small error in either of the two orientation vectors causes a large positional error in the location of c. <p> We have described two alignment style recognition techniques here. We implemented both since the linear combinations method breaks down if we have planar objects. 5.8.1 Model Representation The model representation varies depending on whether we use the alignment <ref> [29] </ref> or the linear combination of views [64]. 1. Alignment for planar objects: The model is a set of line segments that represent the measured contours of the model object. 2. Linear Combination Of Views: The model is represented as a set of two views of the object. <p> In Chapter 1, we began with a discussion of the effect of scene clutter on recognition systems. We saw that scene clutter impedes the performance of recognition methods (e.g. [17], <ref> [29] </ref>, [64] among others) and also contributes to the number of false alarms that have to be handled. We argued using previous results [18] that a key component in efficient object recognition is selection or figure/ground separation before model matching.
Reference: [30] <author> D.P. Huttenlocher, </author> <title> "Three-dimesional recognition of solid objects from a two-dimensional image", </title> <type> PhD thesis, </type> <institution> Artifical Intelligence Lab, MIT, AI-TR-1045, </institution> <year> 1988. </year>
Reference-contexts: Selection can be used to improve the performance of other recognition methods (e.g. <ref> [30] </ref> among others) as well. The aim of this project is to investigate the role of visual attention and fixation in the selection phase of object recognition. Visual attention refers to selecting out portions of the scene on which to focus the resources of visual processing.
Reference: [31] <author> N.H. Kim and A.C. Bovik, </author> <title> "A contour based stereo matching algorithm using disparity continuity", </title> <journal> Pattern Recognition, </journal> <volume> 21, </volume> <pages> 515-524, </pages> <year> 1988. </year>
Reference: [32] <author> L. Kitchen and A. Rosenfeld, </author> <title> "Gray-level corner detection", </title> <journal> Pattern Recognition Letters, </journal> <pages> 95-102, </pages> <month> December </month> <year> 1982. </year>
Reference: [33] <author> G.J. Klinker, S.A. Shafer, and T. Kanade, </author> <title> "Using a color refection model to seperate highlights from object color", </title> <address> ICCV, </address> <year> 1987. </year>
Reference-contexts: Specularities occur as bright white streaks in images of objects with shiny surfaces (e.g. metallic surfaces) under normal lighting conditions. There are methods suggested in the literature that can remove specularities <ref> [33] </ref> by analyzing the clusters formed when a specular region and its adjacent color region are projected into color space. Another problem with using color for segmentation is one of achieving color constancy or a stable perception of color of varying lighting conditions.
Reference: [34] <author> C. Koch and S. Ullman, </author> <title> "Selecting one among the many: A simple network implementing shifts in selective visual attention", </title> <type> Technical Report, </type> <institution> Aritificial Intelligence Lab, M.I.T., AI-memo-770, </institution> <month> Jan </month> <year> 1984. </year>
Reference-contexts: This observation has motivated the use of visual attention in object recognition. Hurlbert and Poggio in [28] suggest how the concept of visual attention can be used to reduce the combinatorial search in recognition. There have been a number of computational models [59], [15], <ref> [34] </ref> of attention proposed in the literature that use this idea. All these models are based on the model of visual attention proposed by Treisman in [62] as a result of psychophysical experiments.
Reference: [35] <author> E.Krotkov, K. Henriksen and R. Kories, </author> <title> "Stereo ranging with verging cameras", </title> <journal> IEEE Trans. on Pattern Analysis and machine Intelligence, </journal> <pages> 1489-1510, </pages> <year> 1989. </year>
Reference-contexts: The importance of camera movements and adjustment of imaging parameters in stereo vision has been investigated by Ballard in [7], Abbot and Ahuja in [2] and Bajcsy in [4]. Knowledge of verging geometry has been used by Krotkov et al. <ref> [35] </ref> to address calibration issues. A system that integrates information from focus, vergence angle, and stereo disparity over multiple fixations to get accurate depth estimates was proposed by Abbot and Ahuja [2].
Reference: [36] <author> R.K. Lenz and R.Y. Tsai, </author> <title> "Techniques for Calibration of the Scale Factor and Image Center for High Accuracy 3-D Machine Vision Metrology", </title> <journal> IEEE PAMI, </journal> <volume> 10(5), </volume> <pages> 713-720, </pages> <year> 1988. </year>
Reference-contexts: Errors in locating the principal points lead to large errors in computed depth, e.g. for an object that is 1 meter away from the camera, errors on the order of 10 pixels lead to 10% errors in depth. The current methods for computing principal points <ref> [36] </ref> have residual errors of about 6 pixels. Errors in computing focal length result in small errors in relative depth for nearby objects ( Z 2b 10). Larger disparities lead to larger errors.
Reference: [37] <author> D.Marr, </author> <title> "Vision", </title> <publisher> Freeman :SanFrancisco, </publisher> <year> 1982. </year>
Reference-contexts: Syeda-Mahmood [59] uses an arbiter module that combines the feature maps and maintains separate saliency maps until the arbiter stage. The idea of using feature maps to represent low level processing of information can be traced back to Marr <ref> [37] </ref> where he uses the primal sketch to expose low level image features and Triesman [62] in her model of attention among others. <p> The stereo algorithm implemented here is a modified version of Grimson's stereo matcher ([20]). It is similar to earlier stereo algorithms [3] and [46], [40], <ref> [37] </ref>, [41] and uses ideas about the human stereo system, Panum's area and the role of eye movements in stereopsis as discussed in [37], [41], [24] and [50]. <p> The stereo algorithm implemented here is a modified version of Grimson's stereo matcher ([20]). It is similar to earlier stereo algorithms [3] and [46], [40], <ref> [37] </ref>, [41] and uses ideas about the human stereo system, Panum's area and the role of eye movements in stereopsis as discussed in [37], [41], [24] and [50].
Reference: [38] <author> J.E.W. Mayhew and J.P. </author> <title> Frisby, 3D Model Recognition from Stereoscopic Cues, </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference: [39] <author> E.S. McVey and J.W. Lee, </author> <title> "Some accuracy and resolution aspects of computer vision distance measurements", </title> <journal> IEEE Pattern Analysis and Machine Intelligence 4(6) </journal> <pages> 646-649, </pages> <year> 1982. </year>
Reference: [40] <author> S.B. Pollard, J.E.W. Mayhew, and J.P. Frisby, "PMF: </author> <title> A stereo correspondence algorithm using a disparity gradient limit", </title> <booktitle> Perception 14, </booktitle> <pages> 449-470, </pages> <year> 1985. </year>
Reference-contexts: One solution to this problem is to use attributes of nearby features [3], [46], <ref> [40] </ref> and another is to alter the control strategy. 51 Since we are interested in finding roughly contiguous 3D regions to select out groups of image features that are likely to come from a single object, we use a control method fixates a target, searches for matching features in some narrow <p> This is similar to the working of the human stereo system where the fusible range of disparities is restricted around the fixation point (Panum's area). The stereo algorithm implemented here is a modified version of Grimson's stereo matcher ([20]). It is similar to earlier stereo algorithms [3] and [46], <ref> [40] </ref>, [37], [41] and uses ideas about the human stereo system, Panum's area and the role of eye movements in stereopsis as discussed in [37], [41], [24] and [50].
Reference: [41] <author> D.Marr and T. Poggio, </author> <title> "A Theory Of Human Stereo Vision", </title> <journal> Proc. Royal Society Of London B 204, </journal> <pages> 301-328. 103 </pages>
Reference-contexts: In addition to the matching constraints given above, the algorithm takes advantage of the following global constraints <ref> [41] </ref> in order to get a focal edge in the left image with a unique match in the right image. (a) The continuity constraint which says that the world consists of piecewise smooth surfaces. <p> The stereo algorithm implemented here is a modified version of Grimson's stereo matcher ([20]). It is similar to earlier stereo algorithms [3] and [46], [40], [37], <ref> [41] </ref> and uses ideas about the human stereo system, Panum's area and the role of eye movements in stereopsis as discussed in [37], [41], [24] and [50]. <p> The stereo algorithm implemented here is a modified version of Grimson's stereo matcher ([20]). It is similar to earlier stereo algorithms [3] and [46], [40], [37], <ref> [41] </ref> and uses ideas about the human stereo system, Panum's area and the role of eye movements in stereopsis as discussed in [37], [41], [24] and [50].
Reference: [42] <author> L.T. Maloney and B. Wandell, </author> <title> "Color constancy: a method for reccovering sur-face spectral reflectance", </title> <journal> J. Opt. Soc. Amer. A, </journal> <volume> 3(1), </volume> <pages> 29-33. </pages>
Reference: [43] <author> J.E.W. Mayhew and J.P. </author> <title> Frisby, "Psychophysical and computational studies towards a theory of human stereopsis", </title> <booktitle> Artificial Intelligence 17(1-3): </booktitle> <pages> 379-386, </pages> <year> 1981. </year>
Reference: [44] <author> J.E.W. Mayhew and J.P. </author> <title> Frisby, 3D Model Recognition from Stereoscopic Cues, </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference: [45] <author> E.S. McVey and J.W. Lee, </author> <title> "Some accuracy and resolution aspects of computer vision distance measurements", </title> <journal> IEEE Pattern Analysis and Machine Intelligence 4(6) </journal> <pages> 646-649, </pages> <year> 1982. </year>
Reference: [46] <author> G. Medioni and R. Nevatia, </author> <title> "Segment-based stereo matching", Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 31: </volume> <pages> 2-18, </pages> <year> 1985. </year>
Reference-contexts: One solution to this problem is to use attributes of nearby features [3], <ref> [46] </ref>, [40] and another is to alter the control strategy. 51 Since we are interested in finding roughly contiguous 3D regions to select out groups of image features that are likely to come from a single object, we use a control method fixates a target, searches for matching features in some <p> This is similar to the working of the human stereo system where the fusible range of disparities is restricted around the fixation point (Panum's area). The stereo algorithm implemented here is a modified version of Grimson's stereo matcher ([20]). It is similar to earlier stereo algorithms [3] and <ref> [46] </ref>, [40], [37], [41] and uses ideas about the human stereo system, Panum's area and the role of eye movements in stereopsis as discussed in [37], [41], [24] and [50].
Reference: [47] <author> C.L. Novak and S.A. Shafer, </author> <title> "Supervised color constancy using a color chart", </title> <institution> Carnegie Mellon University, </institution> <type> Technical Report, </type> <institution> CUM-CS-90-140. </institution>
Reference-contexts: Another problem with using color for segmentation is one of achieving color constancy or a stable perception of color of varying lighting conditions. There has been some work in the literature to correct for the chromaticity of the illuminant ([42], <ref> [47] </ref> among others). We have tried to avoid using a complex color constancy model in our method since we are interested in a quick way to roughly segment the scene into regions that could contain the model object.
Reference: [48] <author> D. Shoham and S. Ullman, </author> <title> "Aligning a model to an image using minimal information", </title> <booktitle> Proc. 2nd Intl. Conf. of Computer Vision, </booktitle> <year> 1988. </year>
Reference-contexts: The corner features give us a reasonable set of features for alignment. However, since our selected data consists of a group of line segments, we could have used a combination of points and lines to compute the alignment transform (e.g. <ref> [48] </ref>) as well. Once the alignment transform has been computed we use the line segments as features to verify the alignment.
Reference: [49] <author> K. Nagao and W.E.L. </author> <title> Grimson, "Object Recognition by Alignment using Invariant Projection of Planar Surfaces", </title> <booktitle> Intl. Conf. on Computer Vision, </booktitle> <volume> Vol. 1, </volume> <pages> 861-864, </pages> <year> 1994. </year>
Reference-contexts: performance in scenes which have little color information. * We can refine the final pose of a solution further by ranking the alignment features and using the best features in a least squares minimization. * We could also use other features besides edges (e.g. centroids of data clusters) in alignment <ref> [49] </ref>. * We can take advantage of additional constraints, like a rough estimate of ground plane for example, to make the recognition more robust. 1 Alignment-style recognition techniques ([29], [64]) find a small number of corresponding features between the model and the image to compute the transformation that aligns the model
Reference: [50] <institution> Olson, "Stereopsis Of Verging Systems", ICCV, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: A system that integrates information from focus, vergence angle, and stereo disparity over multiple fixations to get accurate depth estimates was proposed by Abbot and Ahuja [2]. Vergence control has been used by Olson <ref> [50] </ref> to simplify stereopsis by limiting the disparity range to provide relative depth information over single fixations to be used in building qualitative descriptions for recognition. Controlled eye movements have been used to obtain geometric information for camera calibration [10]. <p> This is useful and interesting for several reasons [24]. 1. There has been some physiological evidence to show that the human system does 3D recognition from 2D views. 2. As shown by Grimson in [24] and Olson in <ref> [50] </ref>, small inaccuracies in camera parameters can lead to large errors in depth. If we are interested in finding roughly contiguous 3D regions then it is useful to fixate on a target and search for matching features within some disparity range about that point. <p> It is similar to earlier stereo algorithms [3] and [46], [40], [37], [41] and uses ideas about the human stereo system, Panum's area and the role of eye movements in stereopsis as discussed in [37], [41], [24] and <ref> [50] </ref>.
Reference: [51] <author> K. Pahlavan and J. Eklundh, </author> <title> "A Head-Eye System Analysis and Design", CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> Vol 56, </volume> <pages> 41-56, </pages> <year> 1992. </year>
Reference-contexts: The "active vision" paradigm has been discussed in papers such as [1], [6],[5] among others. Most of the work in the field of active vision has been concerned with low level tasks like gaze control [1], [52], <ref> [51] </ref>, [15], [16]. The importance of camera movements and adjustment of imaging parameters in stereo vision has been investigated by Ballard in [7], Abbot and Ahuja in [2] and Bajcsy in [4]. Knowledge of verging geometry has been used by Krotkov et al. [35] to address calibration issues.
Reference: [52] <editor> K.Pahlavan, T.Uhlin, J.O.Eklundh, "Dyanamic Fixation", </editor> <booktitle> Fourth ICCV 412 - 419, </booktitle> <year> 1993. </year>
Reference-contexts: The "active vision" paradigm has been discussed in papers such as [1], [6],[5] among others. Most of the work in the field of active vision has been concerned with low level tasks like gaze control [1], <ref> [52] </ref>, [51], [15], [16]. The importance of camera movements and adjustment of imaging parameters in stereo vision has been investigated by Ballard in [7], Abbot and Ahuja in [2] and Bajcsy in [4]. Knowledge of verging geometry has been used by Krotkov et al. [35] to address calibration issues.
Reference: [53] <author> T. Pavlidis, </author> <title> Structural Pattern Recognition, </title> <publisher> Springer-Verlag: </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: that point to get regions that probably come from the same object in 3D-space at a finer level of resolution (Figure 4-5). 4.6.2 Features for the stereo matching process The features used for the stereo match are line segments obtained from intensity edges by running a split and merge algorithm <ref> [53] </ref>. <p> Get the intensity edges from the left and right images. Get segments from the intensity edges by running a split and merge algorithm <ref> [53] </ref> on the edge images. 2. Each segment is described by its end points, HSV (hue, saturation and intensity) values on either side of the segment and the distance to neighboring segments 3.
Reference: [54] <author> T. Poggio et al., </author> <title> The MIT Vision Machine, </title> <type> AI Tech Report </type>
Reference: [55] <author> L. Robert and O.D. Faugeras, </author> <title> "Curve-based Stereo: Figural Continuity and Curvature", </title> <booktitle> CVPR, </booktitle> <pages> 57-62, </pages> <year> 1991. </year>
Reference: [56] <author> I. D. Reid and D. W. Murray, </author> <title> "Tracking Foveated Corner Clusters using Affine Structure", </title> <journal> ICCV, </journal> <volume> 76 - 83, </volume> <year> 1993. </year>
Reference: [57] <author> W.S. Rutowski and A. Rosenfeld, </author> <title> "A comparison of corner detection techniques for chain coded curves", </title> <type> Technical Report, </type> <institution> Univ. Of Maryland, </institution> <type> Tech. Report No. 263, </type> <year> 1977. </year>
Reference: [58] <author> M.J. Swain and D. H. Ballard, </author> <title> "Indexing via Color Histograms", </title> <journal> ICCV, </journal> <volume> 623 - 630, </volume> <year> 1990. </year>
Reference-contexts: Psychophysical experiments conducted by Treisman [62] show that color is used in preattentive visual processing. * it is useful in segmentation since it gives region information and if specified correctly can be relatively stable to changes in orientation and illumination 35 conditions, as mentioned by Swain and Ballard in <ref> [58] </ref>. * a color region in an image tends to come from a single object and thus features within a color region can be grouped together to describe an instance of the object in the image. <p> We use a color-based description of a model object to locate color regions in the image that satisfy that description. Color information in a model has been used to search for instances of the model in an image in works such as <ref> [58] </ref> and [60] among others. Swain and Ballard [58] represent the model and the image by color histograms and perform a match of these histograms to locate objects. Syeda-Mahmood [59] developed a model of color saliency to perform data and model driven selection. <p> Color information in a model has been used to search for instances of the model in an image in works such as <ref> [58] </ref> and [60] among others. Swain and Ballard [58] represent the model and the image by color histograms and perform a match of these histograms to locate objects. Syeda-Mahmood [59] developed a model of color saliency to perform data and model driven selection. <p> While this simple algorithm has been sufficient to illustrate the importance of color in selecting regions to focus attention, 36 we can generalize it to histogram matching approaches to color segmentation (e.g. <ref> [58] </ref>) or other color saliency algorithms (e.g [60]) to obtain the same results. 3.2.1 Algorithm * Input: A model color description, input HSV Image (p) * Output: A list of ellipses represented by their center, area, orientation, major and minor axes, and an image of labeled regions (out) * Description: The
Reference: [59] <author> T.F. Syeda Mahmood, </author> <type> AI Tech Report, </type> <year> 1992. </year>
Reference-contexts: This observation has motivated the use of visual attention in object recognition. Hurlbert and Poggio in [28] suggest how the concept of visual attention can be used to reduce the combinatorial search in recognition. There have been a number of computational models <ref> [59] </ref>, [15], [34] of attention proposed in the literature that use this idea. All these models are based on the model of visual attention proposed by Treisman in [62] as a result of psychophysical experiments. <p> All these models are based on the model of visual attention proposed by Treisman in [62] as a result of psychophysical experiments. The Treisman model consists of several low level feature maps which could be combined using a selection filter. The computational models of attention ([34], [15] and <ref> [59] </ref>) mentioned above use different strategies to combine and control the feature maps. In Koch and Ullman's model, the feature maps are combined using a "Winner Take All" mechanism where the network locates the region that differs the most from its neighbors with respect to some property. <p> The most conspicuous location is where attention is focussed. Clark and Ferrier [15] combined the feature maps by assigning a weight to each feature map and combining them using a linear combination of these weighted features. Syeda-Mahmood <ref> [59] </ref> uses an arbiter module that combines the feature maps and maintains separate saliency maps until the arbiter stage. <p> Swain and Ballard [58] represent the model and the image by color histograms and perform a match of these histograms to locate objects. Syeda-Mahmood <ref> [59] </ref> developed a model of color saliency to perform data and model driven selection. We use a simple blob-coloring algorithm to roughly segment the image into connected components with color properties similar to the color properties of the model. <p> We then went on to show that effective and efficient selection can be achieved when several independent cues are used in conjunction. In this project, we have used visual attention mechanisms <ref> [59] </ref> to integrate the visual cues of color and stereo in order to perform selection and focus the resources of the recognition engines onto relevant data subsets and we have used active vision techniques to direct the selection process.
Reference: [60] <author> T.F. Syeda Mahmood, </author> <title> "Data and Model driven Selection using color regions", </title> <booktitle> Proceedings of the European Conference on Computer Vision, </booktitle> <pages> 321-327, </pages> <year> 1992. </year>
Reference-contexts: We use a color-based description of a model object to locate color regions in the image that satisfy that description. Color information in a model has been used to search for instances of the model in an image in works such as [58] and <ref> [60] </ref> among others. Swain and Ballard [58] represent the model and the image by color histograms and perform a match of these histograms to locate objects. Syeda-Mahmood [59] developed a model of color saliency to perform data and model driven selection. <p> While this simple algorithm has been sufficient to illustrate the importance of color in selecting regions to focus attention, 36 we can generalize it to histogram matching approaches to color segmentation (e.g. [58]) or other color saliency algorithms (e.g <ref> [60] </ref>) to obtain the same results. 3.2.1 Algorithm * Input: A model color description, input HSV Image (p) * Output: A list of ellipses represented by their center, area, orientation, major and minor axes, and an image of labeled regions (out) * Description: The input image (p) is an HSV image.
Reference: [61] <author> M. Tistarelli and G. </author> <title> Sandini, "Dyanamic Aspects of Active Vision", CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> Vol 56, </volume> <pages> 108-129, </pages> <month> July </month> <year> 1992. </year>
Reference: [62] <author> A. Treisman, </author> <title> "Selective Attention in Man", </title> <journal> Brit. Med. Bulletin., </journal> <volume> 20 </volume> <pages> 12-16, </pages> <year> 1964. </year>
Reference-contexts: There have been a number of computational models [59], [15], [34] of attention proposed in the literature that use this idea. All these models are based on the model of visual attention proposed by Treisman in <ref> [62] </ref> as a result of psychophysical experiments. The Treisman model consists of several low level feature maps which could be combined using a selection filter. The computational models of attention ([34], [15] and [59]) mentioned above use different strategies to combine and control the feature maps. <p> The idea of using feature maps to represent low level processing of information can be traced back to Marr [37] where he uses the primal sketch to expose low level image features and Triesman <ref> [62] </ref> in her model of attention among others. <p> Psychophysical experiments conducted by Treisman <ref> [62] </ref> show that color is used in preattentive visual processing. * it is useful in segmentation since it gives region information and if specified correctly can be relatively stable to changes in orientation and illumination 35 conditions, as mentioned by Swain and Ballard in [58]. * a color region in an
Reference: [63] <author> R.Y. Tsai, </author> <title> "A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses", </title> <journal> IJRA, </journal> <volume> 3(4), </volume> <pages> 323-344, </pages> <year> 1987. </year>
Reference-contexts: This is true provided we have ways to determine the camera parameters accurately. The methods suggested to find the camera parameters (e.g. <ref> [63] </ref>) have been shown to be unstable [65]. 4.3 Sensitivity of depth to camera calibration We note the main results of Grimson's analysis of the sensitivity of depth reconstruction from stereo disparities to changes in camera parameters in [24].
Reference: [64] <author> S.Ullman and R.Basri, </author> <title> "Recognition by Linear Combination of Models", </title> <journal> IEEE Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10), 992 - 1006, </volume> <year> 1991. </year>
Reference-contexts: If the clutter can be made relatively small, however, the expected search complexity is reduced to a low order polynomial [18]. There are other recognition methods known as minimal alignment methods (e.g. [29], <ref> [64] </ref>), which find a small number of corresponding features between model and data 11 and use the associated transformation to align the model with the data for verifica-tion. <p> This kind of selection greatly reduces the search in the correspondence stage where the image data is matched with model data using schemes like Alignment [29] or Linear Combination of Views <ref> [64] </ref>. 16 1.2.1 Visual Attention While there is enough evidence to prove that object selection is a complex task for a machine to perform, it is interesting to note that humans seem to have no difficulty in selecting out parts of a scene that contain relevant or interesting information with regard <p> Thus, we don't need accurate camera calibration. If we consider selection to be the important part of recognition, and do 3D recognition from 2D by using techniques like linear combination of views <ref> [64] </ref>, then we don't need accurate 3D data for correspondence. This means that we can use relative depths to get feature subsets in the same depth range and avoid extracting absolute depth information entirely. This is useful and interesting for several reasons [24]. 1. <p> Details on why we chose these cues and how we decided to combine them in an active vision framework are described in later chapters. The various stages in the working of our system are described below. 1 Alignment-style recognition techniques ([29], <ref> [64] </ref>) find a small number of corresponding features between the model and the image to compute the transformation that aligns the model with a hypothesized instance of the object in the image and then verifies the hypothesis by comparing the transformed model with the image data. 29 30 2.1.1 The various <p> We have argued in Chapter 1 that selection plays an important role in recognition. If stereo is used for selection instead of 3D reconstruction then we could avoid explicit 3D input for 3D object recognition by using view based recognition schemes like <ref> [64] </ref>. These view based recognition schemes use stored 2D views of a model to generate a hypothesized image that can be compared to the observed image. From sections 4.3 and 4.4 we can conclude that 47 units of interocular separation). <p> Thus, we have to account for errors in 57 the selection mechanism and see how it affects the recognition process in terms of false positives and false negatives. 5.2 The Recognition System There are a number of recognition systems in the literature [29], [21], [22], <ref> [64] </ref>, [9] that recognize rigid objects using a geometric description of the model. These systems have a geometric description of the model in terms of features like points and lines. <p> Another set of methods for recognition are alignment-based methods [29], <ref> [64] </ref> that explore only a part of the interpretation by matching a small number of model and image features that are sufficient to compute the transform that aligns the model features with the image features. We used an alignment-based recognition system. <p> This is the recognition method using linear combination of views <ref> [64] </ref>. 5.4.1 Linear Combination Of Views In this method, the object is represented as a small set (3) of 2D-views and full correspondence is provided between these views. A description of the method follows. Let O be a rigid object. <p> We have described two alignment style recognition techniques here. We implemented both since the linear combinations method breaks down if we have planar objects. 5.8.1 Model Representation The model representation varies depending on whether we use the alignment [29] or the linear combination of views <ref> [64] </ref>. 1. Alignment for planar objects: The model is a set of line segments that represent the measured contours of the model object. 2. Linear Combination Of Views: The model is represented as a set of two views of the object. Four corresponding points between the two views are specified. <p> In Chapter 1, we began with a discussion of the effect of scene clutter on recognition systems. We saw that scene clutter impedes the performance of recognition methods (e.g. [17], [29], <ref> [64] </ref> among others) and also contributes to the number of false alarms that have to be handled. We argued using previous results [18] that a key component in efficient object recognition is selection or figure/ground separation before model matching. <p> in a least squares minimization. * We could also use other features besides edges (e.g. centroids of data clusters) in alignment [49]. * We can take advantage of additional constraints, like a rough estimate of ground plane for example, to make the recognition more robust. 1 Alignment-style recognition techniques ([29], <ref> [64] </ref>) find a small number of corresponding features between the model and the image to compute the transformation that aligns the model with a hypothesized instance of the object in the image and then verifies the hypothesis by comparing the transformed model with the image data. 99 7.2 Future Directions We
Reference: [65] <author> L.B. Wolff, </author> <title> "Accurate Measurement of Orientation from Stereo using Line Correspondence", </title> <booktitle> CVPR, </booktitle> <pages> 410-415, </pages> <year> 1989. </year>
Reference-contexts: This is true provided we have ways to determine the camera parameters accurately. The methods suggested to find the camera parameters (e.g. [63]) have been shown to be unstable <ref> [65] </ref>. 4.3 Sensitivity of depth to camera calibration We note the main results of Grimson's analysis of the sensitivity of depth reconstruction from stereo disparities to changes in camera parameters in [24].
Reference: [66] <author> M.S. Wu, and J.J. </author> <title> Leou "A Bipartite Matching Approach for Feature Correspondence in Stereo Vision", </title> <journal> Pattern Recognition Letters, </journal> <volume> Vol. 16, No. 1, </volume> <pages> 23-31, </pages> <year> 1995. </year>
References-found: 66

