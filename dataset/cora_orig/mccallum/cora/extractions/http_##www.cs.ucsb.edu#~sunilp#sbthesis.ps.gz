URL: http://www.cs.ucsb.edu/~sunilp/sbthesis.ps.gz
Refering-URL: http://www.cs.ucsb.edu/~sunilp/
Root-URL: http://www.cs.ucsb.edu
Title: Efficient Techniques for Overcoming the I/O Bottleneck.  
Author: Sunil K. Prabhakar 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science by  Committee in charge: Professor Divyakant Agrawal, Co-Chair Professor Amr El Abbadi, Co-Chair Professor Ambuj Singh Professor Terence Smith  
Date: March 1998  
Affiliation: UNIVERSITY of CALIFORNIA Santa Barbara  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K. A. S. Abdel-Ghaffar and A. El Abbadi. </author> <title> Optimal disk allocation for partial match queries. </title> <journal> Proc. ACM Symp. on Transactions of Database Systems, </journal> <volume> 18(1) </volume> <pages> 132-156, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Two approaches based upon error correcting codes are [27] and <ref> [1] </ref>. Two schemes that have been proposed for only two-dimensional data are the Fib [18] and HalfM [2] approaches. <p> This is the case in the relational database setting where the data is structured in tables and accesses are via range or partial match queries. Various schemes have been proposed to improve the performance of these queries. In <ref> [1] </ref> Abdel-Ghaffar and El Abbadi develop provably optimal allocation algorithms for partial match queries. In [30], three alternative declustering schemes are compared for shared-nothing multiprocessor database machines. Another multidimensional declustering method, called Coordinate Modulo Distribution (CMD), is developed in [47].
Reference: [2] <author> K. A. S. Abdel-Ghaffar and A. El Abbadi. </author> <title> Optimal allocation of two-dimensional data. </title> <booktitle> In International Conference on Database Theory, </booktitle> <pages> pages 409-418, </pages> <address> Delphi, Greece, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: As a result, more detailed maps can be retrieved, or in general, any information related to the Sierra Nevada mountains, for example, amount of rainfall, elevation information, or even papers studying the mountain range. Recently in <ref> [2] </ref>, it was shown that strictly optimal solutions (defined shortly) for rectangular queries exist only under very restrictive conditions on the tiling or for very few I/O devices (namely 1,2,3 and 5). In this Chapter we explore allocation methods where no strictly optimal solution exists. <p> Two approaches based upon error correcting codes are [27] and [1]. Two schemes that have been proposed for only two-dimensional data are the Fib [18] and HalfM <ref> [2] </ref> approaches. <p> Clearly, given M disks, the minimal or optimal cost is given by d A M e, where A is the number of buckets to be retrieved. An allocation that results in optimal cost for all queries is said to be strictly optimal. In <ref> [2] </ref>, it was shown that strictly optimal allocations exist in only very special cases for two-dimensional data. The necessary and sufficient conditions for the existence of strictly optimal allocation schemes are derived. <p> M N 1 N 2 2, or M =8, and N 1 =N 2 =4, or M=N 1 N 2 4 and minfN 1 ; N 2 g = 3). Strictly optimal allocations for each of these cases are also defined in <ref> [2] </ref>. Of these cases, only the second case is of general interest. To demonstrate sufficiency of these conditions, a strictly optimal allocation method is given, which allocates tile (i; j) to device (b M 2 c)i + j) mod M . <p> Overall, we observe that there is no single existing approach that gives the best performance for all or even most values of M . It should be noted that as expected from the results obtained in <ref> [2] </ref>, none of the policies produces CHAPTER 2. 28 strictly optimal results, except for the special cases M = 2; 3 and 5. DM is strictly optimal for M= 2 and 3, FX is strictly optimal for M= 2 and HalfM is strictly optimal for M= 2; 3 and 5. <p> We did not consider HCAM in this comparison due to its performance sensitivity to different query types. 2.6 Discussion It has been shown that strictly optimal allocations for two-dimensional data on different I/O devices exist only in very limited cases <ref> [2] </ref>. We proposed a general class of cyclic declustering methods applicable in these cases. We have shown that the Disk Modulo [22] and the Fibonacci [18] allocation methods are special cases of this class. <p> In each graph the lower bound that can be achieved by any allocation scheme is shown. CHAPTER 4. 102 For two-dimensional range queries, it has been shown that strictly optimal declustering can be achieved only in very rare cases <ref> [2] </ref>. We expect that for nearest-neighbor searching too, in general, optimal declustering will be achievable only in very few cases. The case of 2-way partitioning is discussed first. In Figures 4.8 and 4.9 the performance of the various schemes for 2-way partitioning with 15 dimensions is shown.
Reference: [3] <author> R. Agrawal, C. Faloutsos, and A. Swami. </author> <title> Efficient similarity search in sequence databases. </title> <booktitle> In 4th Int. Conference on Foundations of Data Organization and Algorithms, </booktitle> <pages> pages 69-84, </pages> <year> 1993. </year>
Reference-contexts: The number of dimensions necessary for a satisfactory mapping can be very large, and dimensionality CHAPTER 1. 6 reduction techniques are first employed to reduce the number of dimensions to smaller values <ref> [3] </ref>. Searching for similar objects therefore transforms into a problem of locating the nearest points. A nearest-neighbor query is evaluated as follows. <p> Some examples of these include the R-tree [34], R fl -tree [7], the X-tree [9], HB-tree [48], GiST [35] and others [81, 68, 42, 33]. The performance of these structures is good for few dimensions, but as the number of dimensions increases beyond 10, the performance degrades significantly <ref> [3] </ref>. As in the case of range queries, an effective technique for improving the performance of the index structures is to employ parallelism.
Reference: [4] <author> S. Akyurek and K. Salem. </author> <title> Adaptive block rearrangement. </title> <journal> ACM Trans. on Comp. Systems, </journal> <volume> 13(2) </volume> <pages> 89-121, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: In Section 6.6 we present and discuss the results from the simulations. Section 6.7 presents experiments with real disks and Section 6.8 summarizes the chapter. 6.2 Related Work Data placement has been an important research topic in the context of database systems <ref> [4, 10, 19] </ref>. The technique of striping data across multiple disks to overcome the I/O bottleneck has been extensively studied [57, 15]. The improvements in performance are essentially because large requests are serviced in parallel by multiple disks and multiple small requests are handled concurrently. <p> Another multidimensional declustering method, called Coordinate Modulo Distribution (CMD), is developed in [47]. When the structure of the data or the access patterns are not well understood, other approaches have been taken to improve I/O performance. In these cases data is generally viewed as independent files. In <ref> [4] </ref> a technique which dynamically copies blocks that are frequently accessed to a reserved space in the center of the disk to reduce seek delays is described.
Reference: [5] <author> A. D. Alexandrov, W. Y. Ma, A. El Abbadi, and B. S. Manjunath. </author> <title> Adaptive filtering and indexing for image databases. </title> <booktitle> In Proc. of the 258 BIBLIOGRAPHY 259 SPIE Int. Conf. on Storage and Retrieval for Image and Video Databases - III, </booktitle> <pages> pages 12-23, </pages> <address> San Jose, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: These vectors have the property that images that are similar in content will be placed closer in the multidimensional space defined by the vectors. Hence the Euclidean distance between a pair of images gives a quantitative measure of the degree of similarity of their image contents <ref> [5] </ref>. 6.4 Placement Strategies The objective of our study is to propose and evaluate several different schemes for the placement of wavelet decomposed image data. We decided to use multiples of four disks for this study with most of the experiments using only four disks.
Reference: [6] <author> M. Andrews, M. A. Bender, and L. Zhang. </author> <title> New algorithms for the disk scheduling problem. </title> <booktitle> In Foundations of Computer Science, </booktitle> <year> 1996. </year>
Reference-contexts: We therefore have to work with variable task processing times which is a problem that has not been addressed in earlier studies. The robotic library scheduling problem is also related to the problem of disk scheduling which has been studied extensively in the operating systems community <ref> [80, 6] </ref>. The major difference between disk scheduling and library scheduling is that tertiary storage media such as tapes or optical disks are removable as opposed to fixed magnetic disks. The algorithms developed for disks all assume that the medium is never switched which is not true for robotic libraries. <p> The technique of scheduling batches of I/O requests is based upon the fact that I/O traffic is bursty in nature <ref> [6, 69] </ref>, i.e., I/O requests are generated in a short period of time followed by periods during which little or no requests are generated. This technique has also been used for scheduling batch I/O requests for single media such as linear tapes [46], serpentine tapes [37] and magnetic disks [6]. <p> This technique has also been used for scheduling batch I/O requests for single media such as linear tapes [46], serpentine tapes [37] and magnetic disks <ref> [6] </ref>. The rest of the chapter is organized as follows. In Section 8.2 an analytic evaluation of introducing extra media exchanges is presented. Section 8.3 presents and analyses results of experiments conducted to validate our analysis.
Reference: [7] <author> N. Beckmann, H. Kriegel, R. Schneider, and B. Seeger. </author> <title> The R* tree: An efficient and robust access method for points and rectangl es. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 322-331, </pages> <month> May 23-25 </month> <year> 1990. </year>
Reference-contexts: Searching for the nearest points in high-dimensional space for large databases is a very computation and I/O intensive operation. Several index structures have been proposed in the literature that significantly improve the search operation in multidimensional spaces. Some examples of these include the R-tree [34], R fl -tree <ref> [7] </ref>, the X-tree [9], HB-tree [48], GiST [35] and others [81, 68, 42, 33]. The performance of these structures is good for few dimensions, but as the number of dimensions increases beyond 10, the performance degrades significantly [3].
Reference: [8] <author> S. Berchtold, C. Bohm, B. Braunmuller, D. A. Keim, and H-P. Kriegel. </author> <title> Fast parallel similarity search in multimedia databases. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 1-12, </pages> <address> Arizona, U.S.A., </address> <year> 1997. </year>
Reference-contexts: However, for nearest-neighbor queries, it is dependent upon the algorithm used to determine the nearest neighbors. Several declustering schemes have been proposed in the literature for range queries [22, 40, 26, 24]. More recently, declustering for improving the performance of nearest-neighbor queries was proposed in <ref> [8] </ref>. In this part of the thesis, we present a new class of declustering techniques called Cyclic Allocation Schemes which provide significant parallel I/O for range and nearest-neighbor queries in multidimensional databases. <p> The HalfM approach was developed to show the sufficiency of strictly optimal (to be defined shortly) solutions for two-dimensional data. Recently, in <ref> [8] </ref>, a new allocation technique was developed that optimizes the performance of nearest-neighbor queries that are typically used to search for similar objects. We now show how each of these approaches allocates buckets to disks. <p> The Hilbert sequence maps a multidimensional space into a linear order. Figure 2.1 (d) shows the allocation generated by HCAM for N 1 =N 2 =8, M=5. The linear path corresponding to the Hilbert curve is also shown. CHAPTER 2. 23 The allocation scheme developed in <ref> [8] </ref> for improving the performance of nearest-neighbor queries requires that each dimension be divided into exactly two parts. Thus the value of each coordinate is either 0 or 1. Each bucket can therefore be identified by a binary number, where each bit identifies the bucket in each dimension. <p> In this chapter we show that the cyclic schemes are a good declustering technique for nearest-neighbor queries. Their performance is compared with that of the schemes used in the previous chapters (DM, FX, HCAM) as well as the new scheme defined in <ref> [8] </ref>. This new scheme, which we shall call NoD is designed for nearest-neighbor queries. The performance of HCAM for the nearest-neighbor queries is found to be poor (achieving a speed-up of only 3 with 16 disks) [8]. The new approach developed in [8] limits the number of divisions along each dimension <p> the previous chapters (DM, FX, HCAM) as well as the new scheme defined in <ref> [8] </ref>. This new scheme, which we shall call NoD is designed for nearest-neighbor queries. The performance of HCAM for the nearest-neighbor queries is found to be poor (achieving a speed-up of only 3 with 16 disks) [8]. The new approach developed in [8] limits the number of divisions along each dimension to 1. Also, the availability of additional disks does not improve the performance of the declustering. The other approaches do not suffer from these limitations. <p> as well as the new scheme defined in <ref> [8] </ref>. This new scheme, which we shall call NoD is designed for nearest-neighbor queries. The performance of HCAM for the nearest-neighbor queries is found to be poor (achieving a speed-up of only 3 with 16 disks) [8]. The new approach developed in [8] limits the number of divisions along each dimension to 1. Also, the availability of additional disks does not improve the performance of the declustering. The other approaches do not suffer from these limitations. <p> This is typically achieved by dividing each dimension into several parts. Each bucket is then identified by a set of numbers or coordinates corresponding to each dimension. We borrow and extend the definitions of direct and indirect neighbors from <ref> [8] </ref> (the model in their paper does not allow splitting any dimension into more than two parts). Definition 2 Two buckets are direct neighbors if their coordinates differ in exactly one dimension. Moreover, the magnitude of the difference in the coordinates should be 1. <p> Note that these definitions hold for any number of dimensions. Similarly, buckets that differ by 1 in exactly three dimensions can be said to be doubly indirect neighbors and so on. As was established in <ref> [8] </ref>, for d dimensions, the likelihood that data will be clustered around the (d 1)-dimensional surface increases as d becomes larger. Consequently, the need to access direct and indirect neighbors when searching for nearest-neighbors becomes greater. Therefore it is important to effectively decluster direct and indirect neighbors. <p> If the query is to find the k nearest-neighbors, then the circle (or hyper sphere) that determines which buckets should be examined is the one that passes through the current kth nearest-neighbor. In <ref> [8] </ref>, an allocation scheme for declustering buckets to optimize nearest-neighbor queries is presented. This scheme requires that each dimension be divided into exactly two parts, thus the value of each coordinate is either 0 or 1. <p> They define such an allocation to be near-optimal. It is shown that DM, FX and HCAM do not achieve near-optimal declustering. Given a bucket (x 0 ; x 1 ; ; x d1 ), CHAPTER 4. 87 the <ref> [8] </ref> scheme assigns it to disk 0 B d1 M 8 &gt; &gt; : 0 otherwise 1 C 10 where is the bitwise exclusive-OR operator. This allocation guarantees near-optimal declustering, assuming that there are enough disks available. The number of disks required is given by 2 dlog 2 (d+1)e . <p> However, if there are more disks available then the required number, the NoD approach does not use the extra disks thus the extra disks are wasted. Through experiments, it is shown in <ref> [8] </ref> that the performance of 10 nearest-neighbors queries is improved by almost a linear factor in the number of disks through declustering using NoD. The HCAM approach is able to achieve only a two fold improvement using 16 disks. <p> We start by using a metric that measures the degree of near-optimal declustering since in <ref> [8] </ref> achieving near-optimal declustering was the main criterion for predicting the performance of an allocation scheme. However, achieving near-optimal declustering in itself does not accurately reflect the degree of parallelism achieved. <p> The evaluations are based upon both criteria: 1. Count this is a measure of how far the allocation is from being a near-optimal allocation, as defined in <ref> [8] </ref>. The count for an allocation scheme is the sum of the count for each bucket. The count for a bucket is the number of direct and indirect neighbors of the bucket that are allocated to the same disk as the bucket itself. <p> CHAPTER 9. 250 Next, declustering for parallel I/O in the context of nearest-neighbor or similarity queries for multidimensional data was studied. For similarity queries a new scheme proposed in <ref> [8] </ref> by Berchtold et. al has been shown to be the best declustering scheme. We call this scheme NoD because its goal is to achieve near-optimal declustering. The performance of the new cyclic schemes was shown to be competitive with respect to the existing schemes.
Reference: [9] <author> S. Berchtold, D. A. Keim, and H. P. Kreigel. </author> <title> The X-tree: An index structure for high-dimensional data. </title> <booktitle> In 22nd. Conference on Very Large Databases, </booktitle> <pages> pages 28-39, </pages> <address> Bombay, India, </address> <year> 1996. </year>
Reference-contexts: Several index structures have been proposed in the literature that significantly improve the search operation in multidimensional spaces. Some examples of these include the R-tree [34], R fl -tree [7], the X-tree <ref> [9] </ref>, HB-tree [48], GiST [35] and others [81, 68, 42, 33]. The performance of these structures is good for few dimensions, but as the number of dimensions increases beyond 10, the performance degrades significantly [3].
Reference: [10] <author> D. Bitton and J. Gray. </author> <title> Disk shadowing. </title> <booktitle> In Proceedings of the Int. Conf. on Very Large Data Bases, </booktitle> <pages> pages 331-338, </pages> <address> Los Angeles CA., </address> <month> September </month> <year> 1988. </year>
Reference-contexts: In Section 6.6 we present and discuss the results from the simulations. Section 6.7 presents experiments with real disks and Section 6.8 summarizes the chapter. 6.2 Related Work Data placement has been an important research topic in the context of database systems <ref> [4, 10, 19] </ref>. The technique of striping data across multiple disks to overcome the I/O bottleneck has been extensively studied [57, 15]. The improvements in performance are essentially because large requests are serviced in parallel by multiple disks and multiple small requests are handled concurrently.
Reference: [11] <author> J. Bruno and P. Downey. </author> <title> Complexity of task sequencing with dead BIBLIOGRAPHY 260 lines, set-up times and changeover costs. </title> <journal> SIAM Journal of Computing, </journal> <volume> 7(4) </volume> <pages> 393-404, </pages> <month> November </month> <year> 1978. </year>
Reference-contexts: The latencies can be viewed as setup times, however latencies for requests are dependent on the order in which requests are processed. Hence this variation is also not applicable. Another variation of the setup times problem was studied in <ref> [11] </ref> where tasks are grouped into classes and a setup time has to be paid for a job CHAPTER 7. 192 if the preceding job is not from the same class. This problem too does not apply because it is too restrictive.
Reference: [12] <author> M. J. Carey, L. M. Haas, and M. Livny. </author> <title> Tapes hold data, too: Challenges of tuples on tertiary store. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 413-7, </pages> <address> Washington, DC, </address> <year> 1993. </year>
Reference-contexts: Although tertiary storage, in particular magnetic tape, has historically been used primarily for archival or backup purposes, the exploding storage requirements and the high cost of secondary storage are forcing computer architects and designers to re-evaluate the role of tertiary storage <ref> [12] </ref>. Commercially available automatic tape and disk libraries or jukeboxes (e.g. [23, 78, 67]) provide automated access to large amounts of tertiary storage. These libraries can hold hundreds or thousands of tapes or disks.
Reference: [13] <author> K. R. Castleman. </author> <title> Digital Image Processing. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1996. </year>
Reference-contexts: Supporting multi-resolution images can be done by storing each image at multiple resolutions and accessing the appropriate copy as required. However this method wastes a lot of storage space. Image decomposition using Wavelets <ref> [13] </ref> represents a space-efficient method of supporting multi-resolution images. Using wavelets, each image is decomposed into several coefficients, different subsets of which are required to produce copies of the original image at different resolutions. The wavelets technique is superior because it does not incur any extra storage overhead. <p> Due to the multiple copies the storage requirements of this scheme are high. A more storage efficient scheme is to use image decomposition techniques such as wavelets <ref> [13] </ref>. Wavelets transform an image into multiple image coefficients without incurring any additional storage overhead, and one of the coefficients has visual similarity to the original image. Thus, the visually similar coefficient can be thought of as a thumbnail or icon of the original image. <p> From a browsing point of view, we incorporate content based techniques to determine appropriate placement strategies. 6.3 Review of Image Processing Techniques The wavelets transform <ref> [13] </ref> decomposes images into progressively lower resolutions as follows. Beginning with an original image of size N fi M (for simplicity, assume that N and M are powers of 2), the image is decomposed into four parts, each of size N=2 fi M=2, as shown in Figure 6.1.
Reference: [14] <author> L. T. Chen, R. Drach, M. Keating, S. Louise, D. Rotem, and A. Shoshani. </author> <title> Efficient organization and access of multi-dimensional datasets on tertiary storage systems. </title> <booktitle> In Information Systems, </booktitle> <volume> volume 20, </volume> <pages> pages 155-83. </pages> <publisher> Elsevier Science, </publisher> <year> 1995. </year>
Reference-contexts: In [71, 72], modifications to CHAPTER 7. 194 the architecture of database management systems are proposed for efficiently accessing tertiary storage directly. Other studies have looked at the problem of reorganization of data that is stored on tertiary media in order to improve retrieval performance <ref> [14, 74] </ref>. Both studies focus on large arrays with known access patterns and make optimizations for these accesses. 7.2.3 Problem Specification From the above discussion it is clear that the general problem of robotic library scheduling is difficult (possibly NP-complete).
Reference: [15] <author> P. M. Chen and D. A. Patterson. </author> <title> Maximizing performance in a striped disk-array. </title> <booktitle> In Proc. of the 17th Int. Sym. on Comp. Architecture, </booktitle> <pages> pages 322-331, </pages> <year> 1990. </year>
Reference-contexts: The technique of striping data across multiple disks to overcome the I/O bottleneck has been extensively studied <ref> [57, 15] </ref>. The improvements in performance are essentially because large requests are serviced in parallel by multiple disks and multiple small requests are handled concurrently. Extensive studies of both the performance and fault tolerance CHAPTER 6. 138 aspects of RAID have been conducted.
Reference: [16] <author> X. Cheng, R. Dolin, M. Neary, S. Prabhakar, K. Ravikanth, D. Wu, D. Agrawal, A. El Abbadi, M. Freeston, A. Singh, T. Smith, and J. Su. </author> <title> Scalable access within the context of digital libraries. </title> <booktitle> In IEEE International Conference on Advances in Digital Libraries, ADL, </booktitle> <pages> pages 70-81, </pages> <address> Washington, D.C., </address> <year> 1997. </year>
Reference-contexts: Identifying objects that are similar to each other is a challenging problem for many applications, such as multimedia repositories or digital libraries. Examples of such applications are the QBIC project [54] and the Alexan-dria Digital Library <ref> [16] </ref>. In order to automate similarity searching, it is first necessary to define a similarity measure. <p> Two examples of such repositories are the Chabot project [55] and the IBM Al-maden Center's QBIC project [54]. This need is particularly critical for the experts working in the fields of remote-sensing and earth sciences. The Alexandria project <ref> [16] </ref> at UC Santa Barbara has been initiated to build a digital library for maps and image data, typically stored in raster or vec 134 CHAPTER 6. 135 tor format.
Reference: [17] <author> T. Chiueh and R. H. Katz. </author> <title> Multi-resolution video representation for BIBLIOGRAPHY 261 parallel disk arrays. </title> <booktitle> ACM Transaction on Multimedia, </booktitle> <pages> pages 401-409, </pages> <year> 1993. </year>
Reference-contexts: With the knowledge of this structure and the expected access patterns, we can potentially do better than the more general solutions for independent files. In a similar setting, Chiueh and Katz <ref> [17] </ref> discuss pyramidal coding schemes for multiresolution video data and a storage layout for this data on parallel disk arrays to provide real time jitter-free video retrieval. They do not however discuss the issue of the relative placement of stripes on each disk for better performance.
Reference: [18] <author> B. Chor, C. E. Leiserson, R. L. Rivest, and J. B. Shearer. </author> <title> An application of number theory to the organization of raster-graphics memory. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 33(1) </volume> <pages> 86-104, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Two approaches based upon error correcting codes are [27] and [1]. Two schemes that have been proposed for only two-dimensional data are the Fib <ref> [18] </ref> and HalfM [2] approaches. <p> The method is motivated by a declustering method based upon certain properties of Fibonacci CHAPTER 2. 35 numbers proposed by Chor, Leiserson, Rivest and Shearer <ref> [18] </ref>, for the allocation of two-dimensional screen pixels to memory chips. We will call this method FIB. The FIB method is applicable only to very few values of M , in particular, only for values of M that are odd order Fibonacci numbers, i.e. <p> We proposed a general class of cyclic declustering methods applicable in these cases. We have shown that the Disk Modulo [22] and the Fibonacci <ref> [18] </ref> allocation methods are special cases of this class. <p> The first heuristic is based upon the GFIB scheme for two-dimensional data. The GFIB scheme is an extension of the Fib scheme proposed in <ref> [18] </ref>. The original scheme is restrictive because it limits the number of disks to odd order Fibonacci numbers. The GFIB scheme works as follows. Let us first assume that the value of M is the ith Fibonacci number.
Reference: [19] <author> G. Copeland, W. Alexander, E. Boughter, and T. Keller. </author> <title> Data placement in Bubba. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 99-108, </pages> <address> Chicago, </address> <year> 1988. </year>
Reference-contexts: In Section 6.6 we present and discuss the results from the simulations. Section 6.7 presents experiments with real disks and Section 6.8 summarizes the chapter. 6.2 Related Work Data placement has been an important research topic in the context of database systems <ref> [4, 10, 19] </ref>. The technique of striping data across multiple disks to overcome the I/O bottleneck has been extensively studied [57, 15]. The improvements in performance are essentially because large requests are serviced in parallel by multiple disks and multiple small requests are handled concurrently. <p> The notion of "Disk Cooling" is introduced in [75], where data is dynamically redistributed based on the access patterns to yield higher I/O performance. Data placement and declustering in highly parallel systems is studied in <ref> [19] </ref>. The authors show that the static data placement problem is NP-complete. They introduce the CHAPTER 6. 139 notion of the heat of a disk and use it to develop a greedy heuristic for data allocation on disks.
Reference: [20] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This problem in general has been shown to be NP-complete [29]. However, if we limit the maximum degree of each vertex to d max , then a coloring using CHAPTER 6. 145 d max + 1 colors exists <ref> [20] </ref>. For such a graph a simple greedy algorithm can be used to find such a coloring. This algorithm assigns colors to nodes in a sequential traversal trying to assign the lowest possible color that is different from all colors assigned to its neighbors.
Reference: [21] <author> A. L. Drapeau and R. H. Katz. </author> <title> Striping in large tape libraries. </title> <booktitle> In Proc. of Supercomputing, </booktitle> <pages> pages 378-387, </pages> <address> Portland, Oregon, 1993. </address> <publisher> ACM. </publisher>
Reference-contexts: Instead of generating four independent requests to the disks in the second phase, Separated generates a single coarser request to the fourth disk. This reduces the level of contention and the average queueing delays. Similar effects were also observed by Drapeau and Katz <ref> [21] </ref> in their experiments with striping in tape libraries. To test this hypothesis, we ran tests where the expansion of the thumbnails was requested one image at a time rather than as a set. <p> Increasing the number of disks reduced the level of contention and limited the gains of the Separated strategy. This implies that the number of disks should be scaled with the degree of concurrency. The experiments of Drapeau and Katz <ref> [21] </ref> on tape libraries also bore out similar conclusions. The influence of disk caching was also investigated by increasing the disk cache size to 128 Kilobytes the maximum size for the disks simulated. <p> Myllymaki and Livny have investigated the benefits of executing tape and disk I/O in parallel [53]. In [28], Ford and Myllymaki have proposed a log structured file system for tertiary media. The benefits of striping in tape based systems has been studied by Drapeau and Katz <ref> [21] </ref> and also by Golubchik and Muntz [31]. In [31] a general open system model for striped libraries is developed. In [71, 72], modifications to CHAPTER 7. 194 the architecture of database management systems are proposed for efficiently accessing tertiary storage directly.
Reference: [22] <author> H. C. Du and J. S. Sobolewski. </author> <title> Disk allocation for cartesian product files on multiple-disk systems. </title> <journal> ACM Transactions of Database Systems, </journal> <volume> 7(1) </volume> <pages> 82-101, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: For range queries CHAPTER 1. 7 the set is well defined by the query itself. However, for nearest-neighbor queries, it is dependent upon the algorithm used to determine the nearest neighbors. Several declustering schemes have been proposed in the literature for range queries <ref> [22, 40, 26, 24] </ref>. More recently, declustering for improving the performance of nearest-neighbor queries was proposed in [8]. In this part of the thesis, we present a new class of declustering techniques called Cyclic Allocation Schemes which provide significant parallel I/O for range and nearest-neighbor queries in multidimensional databases. <p> Several different techniques have been proposed for relational databases including the Disk Modulo or DM approach <ref> [22] </ref> also known as the CMD approach [47], the Fieldwise eXclusive or FX approach [40], the Gray code approach [32] and the HCAM approach [26]. Two approaches based upon error correcting codes are [27] and [1]. <p> The approach was later extended by Li, Srivastava and Rotem for range queries and dynamic files using the same allocation strategy [47]. The Disk Modulo or DM approach <ref> [22] </ref> is defined as: DM (x 0 ; x 1 ; ; x d1 ) = ( j=0 An example of the allocation generated by DM is shown in Figure 2.1 (b) for the case N 1 = N 2 = 8, M = 5. <p> A generalization of the DM method, the Generalized Disk Modulo or GMD, was also developed in <ref> [22] </ref>, which allocates tile (x 0 ; x 1 ; ; x d1 ) to device (a 0 x 0 + a 1 x 1 + + a d1 x d1 ) mod M , where a 0 a d1 are integers. <p> We call this a class of cyclic declustering methods. This class is a subset of the class of allocations defined by the GDM method introduced in <ref> [22] </ref>. As in the cyclic methods, the GDM approach, also requires the two hop values, a and b, to be relatively prime with respect to M . However, it does not require that a and b be relatively prime with respect to each other. <p> We proposed a general class of cyclic declustering methods applicable in these cases. We have shown that the Disk Modulo <ref> [22] </ref> and the Fibonacci [18] allocation methods are special cases of this class. <p> The first part the thesis explored the use of parallel I/O, beginning with range and similarity queries for multidimensional data sets. We began by considering the special case of two-dimensional range queries. First some of the important existing schemes (DM <ref> [22] </ref>, FX [40] and HCAM [26]) were described and their performance was evaluated. Next, we presented the intuition behind our new cyclic declustering schemes. Based upon these ideas, we developed three schemes - RPHM, GFIB and EXH.
Reference: [23] <author> EXABYTE. </author> <title> Products. </title> <address> http://www.Exabyte.CO M:80/Products/, </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Commercially available automatic tape and disk libraries or jukeboxes (e.g. <ref> [23, 78, 67] </ref>) provide automated access to large amounts of tertiary storage. These libraries can hold hundreds or thousands of tapes or disks. <p> We now substitute some numbers for a commercial tape drive, the Exabyte EXB-Mammoth <ref> [23] </ref> using 170mm, 20GB tapes. The size of a block is chosen to be 512 KB, which gives almost 40,000 blocks per tape. Let x and z be 1 block each. The tape switch time for the mammoth drive is around 40 seconds. <p> The values of the model parameters used for the experiments are shown in Table 7.2. These values are based on the Exabyte EXB-480 Library configured with Exabyte EXB Mammoth drives <ref> [23] </ref>. Most of the values have been chosen to closely match the published figures for the products modeled. For some experiments, however, the value of one parameter was altered.
Reference: [24] <author> C. Faloutsos. </author> <title> Gray codes for partial match and range queries. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(10) </volume> <pages> 1381-1393, </pages> <month> October </month> <year> 1988. </year> <note> BIBLIOGRAPHY 262 </note>
Reference-contexts: For range queries CHAPTER 1. 7 the set is well defined by the query itself. However, for nearest-neighbor queries, it is dependent upon the algorithm used to determine the nearest neighbors. Several declustering schemes have been proposed in the literature for range queries <ref> [22, 40, 26, 24] </ref>. More recently, declustering for improving the performance of nearest-neighbor queries was proposed in [8]. In this part of the thesis, we present a new class of declustering techniques called Cyclic Allocation Schemes which provide significant parallel I/O for range and nearest-neighbor queries in multidimensional databases.
Reference: [25] <author> C. Faloutsos, R. Barber, M. Flickner, J. Hafner, W. Niblack, D. Petkovic, and W. Equitz. </author> <title> Efficient and effective querying by image content. </title> <journal> In Journal of Intelligent Information Systems, </journal> <volume> volume 3, </volume> <pages> pages 231-262, </pages> <year> 1994. </year>
Reference-contexts: This results in relatively fast browsing of thumbnails, at the cost of slower retrieval of higher resolution images. Another similar project, Query by Image Content at IBM Almaden <ref> [54, 25] </ref> handles querying of images by color, shape and texture. It does not however address the issues of image placement for performance improvement. There appears to be a general lack of placement techniques for image data.
Reference: [26] <author> C. Faloutsos and P. Bhagwat. </author> <title> Declustering using fractals. </title> <booktitle> In Proceedings of the 2nd International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 18 - 25, </pages> <address> San Diego, CA, </address> <month> Jan </month> <year> 1993. </year>
Reference-contexts: For range queries CHAPTER 1. 7 the set is well defined by the query itself. However, for nearest-neighbor queries, it is dependent upon the algorithm used to determine the nearest neighbors. Several declustering schemes have been proposed in the literature for range queries <ref> [22, 40, 26, 24] </ref>. More recently, declustering for improving the performance of nearest-neighbor queries was proposed in [8]. In this part of the thesis, we present a new class of declustering techniques called Cyclic Allocation Schemes which provide significant parallel I/O for range and nearest-neighbor queries in multidimensional databases. <p> Several different techniques have been proposed for relational databases including the Disk Modulo or DM approach [22] also known as the CMD approach [47], the Fieldwise eXclusive or FX approach [40], the Gray code approach [32] and the HCAM approach <ref> [26] </ref>. Two approaches based upon error correcting codes are [27] and [1]. Two schemes that have been proposed for only two-dimensional data are the Fib [18] and HalfM [2] approaches. <p> Figure 2.1 (c) shows an example of the allocation generated by the FX method for N 1 =N 2 =8, M=5. The HCAM method <ref> [26] </ref> is based upon the Hilbert space filling curves. Hilbert curves can be used to convert a discrete multi-dimensional array into a linear sequence such that spatial proximity in the original array is preserved. <p> RPHM, GFIB and EXH are still the best allocation strategies. We now consider the special case of queries which have the same number of CHAPTER 2. 46 rows and columns, called square queries. This is an interesting case because the HCAM approach <ref> [26] </ref> has been shown to have very good performance for square queries. Figure 2.12 gives the performance of the various approaches for square queries. We observe that the HCAM approach performs better than DM and FX for almost all values of M . <p> The comparisons are made with the FX, DM and HCAM schemes. The HCAM scheme has been shown to give the best performance among existing schemes <ref> [26] </ref>. Due to the large number of possible queries, the evaluations are based on the performance for a set of randomly chosen queries. To achieve a high degree of confidence in the results, five different random sets of 1000 queries each was used. <p> The first part the thesis explored the use of parallel I/O, beginning with range and similarity queries for multidimensional data sets. We began by considering the special case of two-dimensional range queries. First some of the important existing schemes (DM [22], FX [40] and HCAM <ref> [26] </ref>) were described and their performance was evaluated. Next, we presented the intuition behind our new cyclic declustering schemes. Based upon these ideas, we developed three schemes - RPHM, GFIB and EXH. The new schemes were shown to give better performance than any of the existing schemes.
Reference: [27] <author> C. Faloutsos and D. Metaxas. </author> <title> Declustering using error correcting codes. </title> <booktitle> In Proc. ACM Symp. on Principles of Database Systems, </booktitle> <pages> pages 253-258, </pages> <year> 1989. </year>
Reference-contexts: Several different techniques have been proposed for relational databases including the Disk Modulo or DM approach [22] also known as the CMD approach [47], the Fieldwise eXclusive or FX approach [40], the Gray code approach [32] and the HCAM approach [26]. Two approaches based upon error correcting codes are <ref> [27] </ref> and [1]. Two schemes that have been proposed for only two-dimensional data are the Fib [18] and HalfM [2] approaches.
Reference: [28] <author> D. A. Ford and J. Myllymaki. </author> <title> A log-structured organization for tertiary storage. </title> <booktitle> In Proceedings of the Twelfth International Conference on Data Engineering, </booktitle> <pages> pages 20-7, </pages> <address> New Orleans, Louisiana, </address> <year> 1996. </year>
Reference-contexts: Other researchers have studied various aspects of tertiary storage systems. Myllymaki and Livny have investigated the benefits of executing tape and disk I/O in parallel [53]. In <ref> [28] </ref>, Ford and Myllymaki have proposed a log structured file system for tertiary media. The benefits of striping in tape based systems has been studied by Drapeau and Katz [21] and also by Golubchik and Muntz [31]. In [31] a general open system model for striped libraries is developed.
Reference: [29] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability, A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: The problem of task or job scheduling has been extensively studied in the literature [58, 51, 56]. Several variations of the problem of scheduling tasks on processors have been investigated <ref> [56, 29] </ref>. The problem of robotic library scheduling, however, is not addressed by any of the existing studies. The drives and processors play analogous roles, as do the requests and tasks. The processing time for tasks however, does not easily map onto any simple concept for the library. <p> Similarly the graph k-colorability problem is to assign colors to nodes in a graph such that no two adjacent nodes have the same color using at most k colors. This problem in general has been shown to be NP-complete <ref> [29] </ref>. However, if we limit the maximum degree of each vertex to d max , then a coloring using CHAPTER 6. 145 d max + 1 colors exists [20]. For such a graph a simple greedy algorithm can be used to find such a coloring. <p> The processor scheduling problem seems to be very similar to our library scheduling problem. Several variations of the problem of scheduling tasks on processors have been investigated <ref> [56, 29] </ref>. The various parameters studied include single versus multiple processors, preemptive versus non preemptive tasks, tasks with and without release times and deadlines, precedence constraints between tasks, tasks with weights or priorities and tasks with setup times.
Reference: [30] <author> S. Ghandeharizadeh and D. J. DeWitt. </author> <title> A multiuser performance analysis of alternative declustering strategies. </title> <booktitle> In Proc. Int. Conf. Data Engineering, </booktitle> <pages> pages 466-475, </pages> <address> Los Angeles, California., </address> <month> February </month> <year> 1990. </year>
Reference-contexts: Various schemes have been proposed to improve the performance of these queries. In [1] Abdel-Ghaffar and El Abbadi develop provably optimal allocation algorithms for partial match queries. In <ref> [30] </ref>, three alternative declustering schemes are compared for shared-nothing multiprocessor database machines. Another multidimensional declustering method, called Coordinate Modulo Distribution (CMD), is developed in [47]. When the structure of the data or the access patterns are not well understood, other approaches have been taken to improve I/O performance.
Reference: [31] <author> L. Golubchik and R. Muntz. </author> <title> Analysis of striping techniques in robotic BIBLIOGRAPHY 263 storage libraries. </title> <booktitle> In Proceedings of the Fourteenth IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 225-38, </pages> <address> Monterey, CA, </address> <year> 1995. </year>
Reference-contexts: In [28], Ford and Myllymaki have proposed a log structured file system for tertiary media. The benefits of striping in tape based systems has been studied by Drapeau and Katz [21] and also by Golubchik and Muntz <ref> [31] </ref>. In [31] a general open system model for striped libraries is developed. In [71, 72], modifications to CHAPTER 7. 194 the architecture of database management systems are proposed for efficiently accessing tertiary storage directly. <p> In [28], Ford and Myllymaki have proposed a log structured file system for tertiary media. The benefits of striping in tape based systems has been studied by Drapeau and Katz [21] and also by Golubchik and Muntz <ref> [31] </ref>. In [31] a general open system model for striped libraries is developed. In [71, 72], modifications to CHAPTER 7. 194 the architecture of database management systems are proposed for efficiently accessing tertiary storage directly.
Reference: [32] <author> J. Gray, B. Horst, and M. Walker. </author> <title> Parity striping of disc arrays: Low-cost reliable storage with acceptable throughput. </title> <booktitle> In Proceedings of the Int. Conf. on Very Large Data Bases, </booktitle> <pages> pages 148-161, </pages> <address> Washington DC., </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Several different techniques have been proposed for relational databases including the Disk Modulo or DM approach [22] also known as the CMD approach [47], the Fieldwise eXclusive or FX approach [40], the Gray code approach <ref> [32] </ref> and the HCAM approach [26]. Two approaches based upon error correcting codes are [27] and [1]. Two schemes that have been proposed for only two-dimensional data are the Fib [18] and HalfM [2] approaches. <p> The improvements in performance are essentially because large requests are serviced in parallel by multiple disks and multiple small requests are handled concurrently. Extensive studies of both the performance and fault tolerance CHAPTER 6. 138 aspects of RAID have been conducted. In <ref> [32] </ref> for example, a variation of RAID where only the parity and not the data is striped across disks is shown to yield lower response times and higher throughput. Further improvements in performance have been achieved when the structure of the data and the access patterns are well understood.
Reference: [33] <author> O. Gunther. </author> <title> The design of the cell tree: An object-oriented index structure for geometric databases. </title> <booktitle> In Proc. Int. Conf. Data Engineering, </booktitle> <pages> pages 598-605, </pages> <year> 1989. </year>
Reference-contexts: Several index structures have been proposed in the literature that significantly improve the search operation in multidimensional spaces. Some examples of these include the R-tree [34], R fl -tree [7], the X-tree [9], HB-tree [48], GiST [35] and others <ref> [81, 68, 42, 33] </ref>. The performance of these structures is good for few dimensions, but as the number of dimensions increases beyond 10, the performance degrades significantly [3]. As in the case of range queries, an effective technique for improving the performance of the index structures is to employ parallelism.
Reference: [34] <author> A. Guttman. R-trees: </author> <title> A dynamic index structure for spatial searching. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 47-57, </pages> <year> 1984. </year>
Reference-contexts: Searching for the nearest points in high-dimensional space for large databases is a very computation and I/O intensive operation. Several index structures have been proposed in the literature that significantly improve the search operation in multidimensional spaces. Some examples of these include the R-tree <ref> [34] </ref>, R fl -tree [7], the X-tree [9], HB-tree [48], GiST [35] and others [81, 68, 42, 33]. The performance of these structures is good for few dimensions, but as the number of dimensions increases beyond 10, the performance degrades significantly [3].
Reference: [35] <author> J. Hellerstein, J. Naughton, and A. Pfeffer. </author> <title> Generalized search trees for database systems. </title> <booktitle> In Proceedings of the Int. Conf. on Very Large Data Bases, </booktitle> <pages> pages 562-573, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Several index structures have been proposed in the literature that significantly improve the search operation in multidimensional spaces. Some examples of these include the R-tree [34], R fl -tree [7], the X-tree [9], HB-tree [48], GiST <ref> [35] </ref> and others [81, 68, 42, 33]. The performance of these structures is good for few dimensions, but as the number of dimensions increases beyond 10, the performance degrades significantly [3].
Reference: [36] <author> B. K. Hillyer and A. Silberschatz. </author> <title> On the modeling and performance characteristics of a serpentine tape. </title> <booktitle> In SIGMETRICS, </booktitle> <pages> pages 170-9, </pages> <address> Canada, </address> <year> 1996. </year>
Reference-contexts: The order of processing the requests optimally on a medium is not important for our discussion. This order can be as simple as the order of the block numbers for linear tapes or more complex such as those suggested by Hillyer and Silberschatz <ref> [36] </ref> for serpentine tapes or any disk scheduling policy for optical disks. <p> Note also that the order of processing of requests for a given medium is not restricted, i.e. any order can be used. For example for serpentine tapes, the scheduling policies developed by Hillyer and Silberschatz <ref> [36] </ref> can be used for scheduling the requests on each tape. For the medium scheduling it is necessary only to be able to determine the seek and transfer times for each request.
Reference: [37] <author> B. K. Hillyer and A. Silberschatz. </author> <title> Random I/O scheduling in online BIBLIOGRAPHY 264 tertiary storage. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Canada, </address> <year> 1996. </year>
Reference-contexts: The algorithms developed for disks all assume that the medium is never switched which is not true for robotic libraries. More recent work on tape scheduling <ref> [37, 46] </ref> has studied efficient processing schedules for I/O requests for single tapes. Sarawagi and Stonebraker have worked on the problem of ordering I/O requests for tertiary storage media when tertiary storage is directly incorporated in a relational database [73]. <p> The algorithms developed for disks all assume that the medium is never switched which is not true for robotic libraries. More recent work on tape scheduling <ref> [37, 46] </ref> has studied efficient processing schedules for I/O requests for single tapes. In [37] Hillyer and Silberschatz have analyzed algorithms for scheduling batched requests for a single serpentine tape that is loaded on the drive. Li and Orji [46] have studied efficient scheduling policies for linear tapes. <p> The algorithms developed for disks all assume that the medium is never switched which is not true for robotic libraries. More recent work on tape scheduling [37, 46] has studied efficient processing schedules for I/O requests for single tapes. In <ref> [37] </ref> Hillyer and Silberschatz have analyzed algorithms for scheduling batched requests for a single serpentine tape that is loaded on the drive. Li and Orji [46] have studied efficient scheduling policies for linear tapes. Both these studies do not consider scheduling requests for more than one medium. <p> This technique has also been used for scheduling batch I/O requests for single media such as linear tapes [46], serpentine tapes <ref> [37] </ref> and magnetic disks [6]. The rest of the chapter is organized as follows. In Section 8.2 an analytic evaluation of introducing extra media exchanges is presented. Section 8.3 presents and analyses results of experiments conducted to validate our analysis.
Reference: [38] <author> H. V. </author> <title> Jagdish. A retrieval technique for similar shapes. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 208-217, </pages> <year> 1991. </year>
Reference-contexts: The most common approach is to map each data object into a high-dimensional space such that the distance between two such points is a measure of the similarity between the corresponding data objects the closer the points in the high-dimensional space, the more similar the data objects <ref> [50, 38] </ref>. The number of dimensions necessary for a satisfactory mapping can be very large, and dimensionality CHAPTER 1. 6 reduction techniques are first employed to reduce the number of dimensions to smaller values [3]. Searching for similar objects therefore transforms into a problem of locating the nearest points.
Reference: [39] <author> K. Keeton and R. H. Katz. </author> <title> Evaluating video layout strategies for a high-performance storage server. </title> <journal> Multimedia Systems, </journal> <volume> 3:43 - 52, </volume> <year> 1995. </year>
Reference-contexts: They do not however discuss the issue of the relative placement of stripes on each disk for better performance. Placement techniques for multiresolution video data based upon the RAID-II prototype have been studied in <ref> [39] </ref>. Multiresolution video data is obtained through sub-band video coding which, like wavelets for images, generates several bands (coefficients) without data replication. Both these studies, however, are not directly applicable outside the context of video data. <p> For low concurrency, the strategies that decluster the image coefficients perform better and for high concurrency the strategies that do not decluster coefficients perform better. It is interesting to note that Keeton and Katz <ref> [39] </ref> observed similar results for multiresolution video data. In particular, they found that declustering coefficients or subbands for video resulted in better performance only for low or moderate concurrencies.
Reference: [40] <author> M. H. Kim and S. Pramanik. </author> <title> Optimal file distribution for partial match retrieval. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 173-182, </pages> <address> Chicago, </address> <year> 1988. </year>
Reference-contexts: For range queries CHAPTER 1. 7 the set is well defined by the query itself. However, for nearest-neighbor queries, it is dependent upon the algorithm used to determine the nearest neighbors. Several declustering schemes have been proposed in the literature for range queries <ref> [22, 40, 26, 24] </ref>. More recently, declustering for improving the performance of nearest-neighbor queries was proposed in [8]. In this part of the thesis, we present a new class of declustering techniques called Cyclic Allocation Schemes which provide significant parallel I/O for range and nearest-neighbor queries in multidimensional databases. <p> Several different techniques have been proposed for relational databases including the Disk Modulo or DM approach [22] also known as the CMD approach [47], the Fieldwise eXclusive or FX approach <ref> [40] </ref>, the Gray code approach [32] and the HCAM approach [26]. Two approaches based upon error correcting codes are [27] and [1]. Two schemes that have been proposed for only two-dimensional data are the Fib [18] and HalfM [2] approaches. <p> The FX method proposed by Kim and Pramanik <ref> [40] </ref>, requires that all the N i values and M be powers of 2. Each tile is allocated to the device given by the lowest log 2 M bits of the bit-wise exclusive-OR of the binary representations of the co-ordinates. <p> To do this, we consider the CHAPTER 2. 22 dlog 2 N i e bits of the binary representations of the tile coordinates to perform the bit-wise exclusive-OR operation and then convert the result to modulo M . The Fieldwise eXclusive or FX approach <ref> [40] </ref> is defined as: F X (x 0 ; x 1 ; ; x d1 ) = (b 0 b 1 b d1 ) mod M where b j is the binary representation of x j and represents the bitwise exclusive-OR operator. <p> The first part the thesis explored the use of parallel I/O, beginning with range and similarity queries for multidimensional data sets. We began by considering the special case of two-dimensional range queries. First some of the important existing schemes (DM [22], FX <ref> [40] </ref> and HCAM [26]) were described and their performance was evaluated. Next, we presented the intuition behind our new cyclic declustering schemes. Based upon these ideas, we developed three schemes - RPHM, GFIB and EXH. The new schemes were shown to give better performance than any of the existing schemes.
Reference: [41] <author> B. Kobler, J. Berbert, P. Caulk, and P. C. Hariharan. </author> <title> Architechture and design of storage and data management for the NASA Earth Observing System Data and Information System (EOSDIS). </title> <booktitle> In Proceedings of the 14th IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 65-76, </pages> <address> Los Alamitos, CA, USA, Sept 1995. </address> <publisher> IEEE Computer Soc. Press. </publisher>
Reference-contexts: The Earth Observing System Data and Information System (EOSDIS) <ref> [41] </ref>, for example, has an estimated data generation rates of well over a terabyte a day.
Reference: [42] <author> C. Kolovson and M. Stonebraker. </author> <title> Segment indexes: Dynamic indexing techniques for multi-dimensional interval data. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 138-147, </pages> <year> 1991. </year>
Reference-contexts: Several index structures have been proposed in the literature that significantly improve the search operation in multidimensional spaces. Some examples of these include the R-tree [34], R fl -tree [7], the X-tree [9], HB-tree [48], GiST [35] and others <ref> [81, 68, 42, 33] </ref>. The performance of these structures is good for few dimensions, but as the number of dimensions increases beyond 10, the performance degrades significantly [3]. As in the case of range queries, an effective technique for improving the performance of the index structures is to employ parallelism.
Reference: [43] <author> D. Kotz, T. B. Song, and S. Radhakrishnan. </author> <title> A detailed simulation of the HP 97560 disk drive. </title> <type> Technical Report TR94-220, </type> <institution> Comp. Sci. Dept., Dartmouth College., </institution> <year> 1994. </year> <note> BIBLIOGRAPHY 265 </note>
Reference-contexts: Use of a simulator provides much greater flexibility. Some of the experiments are repeated with real disks. These results are discussed in Section 6.7. The simulator that we use has been developed by Kotz et al. <ref> [43] </ref> based on the model developed by Ruemmler and Wilkes of HP laboratories [70]. The model is sophisticated and is capable of simulating multiple disks connected CHAPTER 6. 150 to multiple I/O buses. placement, it is necessary to have a model of the usage patterns. <p> In particular, four separate disks are used in order to investigate inter-disk parallelism for seeking. The simulator models the HP 97560 disk. Some of the parameters of these disks are given in Table 6.1. Further details of the disk and the model can be found in [70] and <ref> [43] </ref>. Although the specified size of the cache for the HP 97560 is 128 Kilobytes, we limited the cache to 4 Kilobytes, in order to study the placement strategies in the absence of caching. Note that 4 Kilobytes is the size of the thumbnail and each coefficient of the images.
Reference: [44] <author> E. L. Lawler, J. K. Lenstra, A. H. G. Rinnooy Kan, and D. B. Shmoys. </author> <title> The Traveling Salesman Problem. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1985. </year>
Reference: [45] <author> J.K. Lenstra, A. K. G. Rinnooy Kan, and P. Brucker. </author> <title> Complexity of machine scheduling problems. </title> <journal> Annals of Discrete Mathematics, </journal> <volume> 1 </volume> <pages> 343-362, </pages> <year> 1977. </year>
Reference-contexts: Proof: We will prove the above theorem by showing that a known NP-complete problem can be reduced to the above problem (M F M S). We will reduce the problem of Scheduling to Minimize Weighted Completion Time (M W CT ) <ref> [45] </ref>, which is defined as follows, to M F M S. <p> CHAPTER 7. 211 QUESTION: Is there an m-processor schedule for S such that the sum, over all t 2 S, of ((t) + l (t)) w (t) is no more than J 0 ? The M W CT problem has been shown to be NP-complete in <ref> [45] </ref>. Given any instance of the M W CT problem, we create an instance of the M F M S problem as follows.
Reference: [46] <author> J. Li and C. Orji. </author> <title> I/O scheduling in tape-based tertiary systems. </title> <journal> Journal of Mathematical Modelling and Scientific Computing, </journal> <volume> 6, </volume> <year> 1996. </year>
Reference-contexts: The algorithms developed for disks all assume that the medium is never switched which is not true for robotic libraries. More recent work on tape scheduling <ref> [37, 46] </ref> has studied efficient processing schedules for I/O requests for single tapes. Sarawagi and Stonebraker have worked on the problem of ordering I/O requests for tertiary storage media when tertiary storage is directly incorporated in a relational database [73]. <p> The algorithms developed for disks all assume that the medium is never switched which is not true for robotic libraries. More recent work on tape scheduling <ref> [37, 46] </ref> has studied efficient processing schedules for I/O requests for single tapes. In [37] Hillyer and Silberschatz have analyzed algorithms for scheduling batched requests for a single serpentine tape that is loaded on the drive. Li and Orji [46] have studied efficient scheduling policies for linear tapes. <p> More recent work on tape scheduling [37, 46] has studied efficient processing schedules for I/O requests for single tapes. In [37] Hillyer and Silberschatz have analyzed algorithms for scheduling batched requests for a single serpentine tape that is loaded on the drive. Li and Orji <ref> [46] </ref> have studied efficient scheduling policies for linear tapes. Both these studies do not consider scheduling requests for more than one medium. Sarawagi and Stonebraker have worked on the problem of ordering I/O requests for tertiary storage media when tertiary storage is directly incorporated in a relational database [73]. <p> This technique has also been used for scheduling batch I/O requests for single media such as linear tapes <ref> [46] </ref>, serpentine tapes [37] and magnetic disks [6]. The rest of the chapter is organized as follows. In Section 8.2 an analytic evaluation of introducing extra media exchanges is presented. Section 8.3 presents and analyses results of experiments conducted to validate our analysis.
Reference: [47] <author> J. Li, J. Srivastava, and D. Rotem. CMD: </author> <title> a multidimensional declus-tering method for parallel database systems. </title> <booktitle> In Proceedings of the Int. Conf. on Very Large Data Bases, </booktitle> <pages> pages 3-14, </pages> <address> Vancouver, Canada, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Several different techniques have been proposed for relational databases including the Disk Modulo or DM approach [22] also known as the CMD approach <ref> [47] </ref>, the Fieldwise eXclusive or FX approach [40], the Gray code approach [32] and the HCAM approach [26]. Two approaches based upon error correcting codes are [27] and [1]. Two schemes that have been proposed for only two-dimensional data are the Fib [18] and HalfM [2] approaches. <p> The Disk Modulo approach proposed by Du and Sobolewski was designed to allocate Cartesian product files to multiple disks to improve the performance of partial match queries. The approach was later extended by Li, Srivastava and Rotem for range queries and dynamic files using the same allocation strategy <ref> [47] </ref>. <p> In [1] Abdel-Ghaffar and El Abbadi develop provably optimal allocation algorithms for partial match queries. In [30], three alternative declustering schemes are compared for shared-nothing multiprocessor database machines. Another multidimensional declustering method, called Coordinate Modulo Distribution (CMD), is developed in <ref> [47] </ref>. When the structure of the data or the access patterns are not well understood, other approaches have been taken to improve I/O performance. In these cases data is generally viewed as independent files.
Reference: [48] <author> D. B. Lomet and B. Salzberg. </author> <title> The hb-tree: A multi-attribute indexing method with good guaranteed performance. </title> <journal> Proc. ACM Symp. on Transactions of Database Systems, </journal> <volume> 15(4) </volume> <pages> 625-658, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Several index structures have been proposed in the literature that significantly improve the search operation in multidimensional spaces. Some examples of these include the R-tree [34], R fl -tree [7], the X-tree [9], HB-tree <ref> [48] </ref>, GiST [35] and others [81, 68, 42, 33]. The performance of these structures is good for few dimensions, but as the number of dimensions increases beyond 10, the performance degrades significantly [3].
Reference: [49] <author> W. Y. Ma and B. S. Manjunath. </author> <title> Pictorial queries: Combining feature extraction with database search. </title> <type> Technical Report CIPR 94-18, </type> <institution> Univ. of California, Santa Barbara, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: This method is undesirable because of the slow speed of the annotation process and also the dependence on and limitation of the subjective interpretation of the expert. Fortunately, there exist several image processing techniques for quantifying the image content [54, 55]. We used the Gabor CHAPTER 6. 142 Transform <ref> [49] </ref> to generate a set of features for each image. The features for each image are viewed as a vector in a multidimensional space. These vectors have the property that images that are similar in content will be placed closer in the multidimensional space defined by the vectors. <p> Multiresolution image browsing is a very efficient means of allowing users to determine the images that are of interest from a large image database. Image processing techniques such as Gabor filters <ref> [49] </ref> are used to capture the content of images. We proposed two orthogonal declustering schemes - thumbnail declustering and coefficient declustering. The first scheme declus-ters the image thumbnails based upon similarity since similar images are likely to be fetched together.
Reference: [50] <author> B. S. Manjunath and W. Y. Ma. </author> <title> Texture features for browsing and retrieval of image data. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 18(8) </volume> <pages> 837-42, </pages> <month> August </month> <year> 1996. </year> <note> BIBLIOGRAPHY 266 </note>
Reference-contexts: The most common approach is to map each data object into a high-dimensional space such that the distance between two such points is a measure of the similarity between the corresponding data objects the closer the points in the high-dimensional space, the more similar the data objects <ref> [50, 38] </ref>. The number of dimensions necessary for a satisfactory mapping can be very large, and dimensionality CHAPTER 1. 6 reduction techniques are first employed to reduce the number of dimensions to smaller values [3]. Searching for similar objects therefore transforms into a problem of locating the nearest points.
Reference: [51] <author> T. E. Morton and D. W. Pentico. </author> <title> Heuristic Scheduling Systems with applications to Production Systems and Project Management. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Since the major concern for tertiary I/O is the high latency, we choose the optimization function to be the average waiting time seen by the set of requests. The problem of task or job scheduling has been extensively studied in the literature <ref> [58, 51, 56] </ref>. Several variations of the problem of scheduling tasks on processors have been investigated [56, 29]. The problem of robotic library scheduling, however, is not addressed by any of the existing studies. The drives and processors play analogous roles, as do the requests and tasks. <p> This measure, also known as the optimization function, could be one of several possibilities. Some of the popular optimization functions that have been studied in the literature include <ref> [58, 51] </ref>: * Makespan: the time at which the last request is completed. * TWCT: the Total Weighted Completion Time (each job has an as signed weight). * TWT: the Total Waiting Time for all jobs. * Throughput: the average number of requests serviced per second. <p> These two contradictory factors make the task of finding an optimal schedule for the requests very difficult. CHAPTER 7. 190 7.2.2 Related Work The problem of task or job scheduling has been extensively studied in the literature <ref> [58, 51, 56] </ref>. Related areas are discussed here, with emphasis on the applicability of existing results to our problem. Scheduling tasks are traditionally categorized as processor scheduling or shop scheduling. Processor scheduling refers to problems where a set of tasks has to be executed by a set of processors.
Reference: [52] <author> J. Myllymaki and M. Livny. </author> <title> Disk-tape joins: Synchronizing disk and tape access. </title> <booktitle> In Joint International Conference on Measurement and Modeling of ComputerSystems. SIGMETRICS '95/PERFORMANCE '95, </booktitle> <pages> pages 279-90, </pages> <address> Ottawa, Canada, </address> <year> 1995. </year>
Reference-contexts: Their work is not applicable outside the relational database framework. Optimization of the performance of join operations in databases has also been studied by Myllymaki and Livny <ref> [52] </ref> under the assumption that one relation CHAPTER 1. 13 is on tape and the other on disk. Chapter 7 gives a more detailed description of related work. Further details can be found in [59]. <p> Their work is not applicable outside the relational database framework. Optimization of the performance of join operations in databases has also been studied by Myllymaki and Livny <ref> [52] </ref> under the assumption that one relation is on tape and the other on disk. Other researchers have studied various aspects of tertiary storage systems. Myllymaki and Livny have investigated the benefits of executing tape and disk I/O in parallel [53].
Reference: [53] <author> J. Myllymaki and M. Livny. </author> <title> Efficient buffering for concurrent disk and tape I/O. </title> <booktitle> In Proceedings of Performance '96, Int. Conf. on Performance Theory, Measurement and Evaluation of Computer Communication Systems, </booktitle> <address> Lausanne, Switzerland, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Other researchers have studied various aspects of tertiary storage systems. Myllymaki and Livny have investigated the benefits of executing tape and disk I/O in parallel <ref> [53] </ref>. In [28], Ford and Myllymaki have proposed a log structured file system for tertiary media. The benefits of striping in tape based systems has been studied by Drapeau and Katz [21] and also by Golubchik and Muntz [31].
Reference: [54] <author> W. Niblack, R. Barber, W. Equitz, M. Flickner, E. Glasman, D. Petkovic, and P. Yanker. </author> <title> The QBIC project: Querying images by content using color, texture and shape. </title> <booktitle> In Proc. of the SPIE Conf. 1908 on Storage and Retrieval for Image and Video Databases, volume 1908, </booktitle> <pages> pages 173-187, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Alternatively, the problem can be viewed as one of maximizing the parallelism with a given number of disks. Identifying objects that are similar to each other is a challenging problem for many applications, such as multimedia repositories or digital libraries. Examples of such applications are the QBIC project <ref> [54] </ref> and the Alexan-dria Digital Library [16]. In order to automate similarity searching, it is first necessary to define a similarity measure. <p> Chapter 6 Browsing and Placement of Multiresolution Images 6.1 Introduction With rapid advances in computer and communication technologies, there is an increasing demand to build and maintain large image repositories. Two examples of such repositories are the Chabot project [55] and the IBM Al-maden Center's QBIC project <ref> [54] </ref>. This need is particularly critical for the experts working in the fields of remote-sensing and earth sciences. <p> Multiresolution image retrieval seems to be the key to providing efficient browsing of image data. This retrieval model is employed both in the QBIC <ref> [54] </ref> system as well as the Chabot [55] project. One method of providing images at multiple resolutions is storing multiple CHAPTER 6. 136 copies of images at various resolutions, as in the Chabot project. Due to the multiple copies the storage requirements of this scheme are high. <p> This results in relatively fast browsing of thumbnails, at the cost of slower retrieval of higher resolution images. Another similar project, Query by Image Content at IBM Almaden <ref> [54, 25] </ref> handles querying of images by color, shape and texture. It does not however address the issues of image placement for performance improvement. There appears to be a general lack of placement techniques for image data. <p> In this paper, the process of reconstructing an image by one level is referred to as expansion of the image. Content based browsing refers to retrieving images based on the similarity of the visual information contained in the images <ref> [54, 55] </ref>. For example, having seen a specific hurricane image, a user may want to see similar images of hurricanes. In order to evaluate the similarity of images, one would first quantify the image content using several possible techniques. <p> This method is undesirable because of the slow speed of the annotation process and also the dependence on and limitation of the subjective interpretation of the expert. Fortunately, there exist several image processing techniques for quantifying the image content <ref> [54, 55] </ref>. We used the Gabor CHAPTER 6. 142 Transform [49] to generate a set of features for each image. The features for each image are viewed as a vector in a multidimensional space.
Reference: [55] <author> V. E. Ogle and M. Stonebraker. Chabot: </author> <title> Retrieval from a relational database of images. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 41-48, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Chapter 6 Browsing and Placement of Multiresolution Images 6.1 Introduction With rapid advances in computer and communication technologies, there is an increasing demand to build and maintain large image repositories. Two examples of such repositories are the Chabot project <ref> [55] </ref> and the IBM Al-maden Center's QBIC project [54]. This need is particularly critical for the experts working in the fields of remote-sensing and earth sciences. <p> Multiresolution image retrieval seems to be the key to providing efficient browsing of image data. This retrieval model is employed both in the QBIC [54] system as well as the Chabot <ref> [55] </ref> project. One method of providing images at multiple resolutions is storing multiple CHAPTER 6. 136 copies of images at various resolutions, as in the Chabot project. Due to the multiple copies the storage requirements of this scheme are high. <p> Multiresolution video data is obtained through sub-band video coding which, like wavelets for images, generates several bands (coefficients) without data replication. Both these studies, however, are not directly applicable outside the context of video data. The Chabot project currently underway at UC Berkeley <ref> [55] </ref> is also concerned with the retrieval of multiresolution images (although each image is stored at different resolution levels, incurring additional storage overhead as compared to wavelets). <p> In this paper, the process of reconstructing an image by one level is referred to as expansion of the image. Content based browsing refers to retrieving images based on the similarity of the visual information contained in the images <ref> [54, 55] </ref>. For example, having seen a specific hurricane image, a user may want to see similar images of hurricanes. In order to evaluate the similarity of images, one would first quantify the image content using several possible techniques. <p> This method is undesirable because of the slow speed of the annotation process and also the dependence on and limitation of the subjective interpretation of the expert. Fortunately, there exist several image processing techniques for quantifying the image content <ref> [54, 55] </ref>. We used the Gabor CHAPTER 6. 142 Transform [49] to generate a set of features for each image. The features for each image are viewed as a vector in a multidimensional space. <p> The Separated placement strategy differs from the others in that the thumbnails are placed on only three disks and all the other three coefficients are placed together on the fourth disk. This storage organization is similar to the placement of data in the Chabot project at UC Berkeley <ref> [55] </ref> where the lowest-resolution images or thumbnails are stored on disks and all higher resolution images are stored in tertiary storage (which is in contrast to the fourth disk in the Separated placement strategy).
Reference: [56] <author> G. R. Parker. </author> <title> Deterministic Scheduling Theory. </title> <publisher> Chapman & Hall, 2-6 Boundary Row, </publisher> <address> London SE1 8HN, UK, </address> <year> 1995. </year> <note> BIBLIOGRAPHY 267 </note>
Reference-contexts: Since the major concern for tertiary I/O is the high latency, we choose the optimization function to be the average waiting time seen by the set of requests. The problem of task or job scheduling has been extensively studied in the literature <ref> [58, 51, 56] </ref>. Several variations of the problem of scheduling tasks on processors have been investigated [56, 29]. The problem of robotic library scheduling, however, is not addressed by any of the existing studies. The drives and processors play analogous roles, as do the requests and tasks. <p> The problem of task or job scheduling has been extensively studied in the literature [58, 51, 56]. Several variations of the problem of scheduling tasks on processors have been investigated <ref> [56, 29] </ref>. The problem of robotic library scheduling, however, is not addressed by any of the existing studies. The drives and processors play analogous roles, as do the requests and tasks. The processing time for tasks however, does not easily map onto any simple concept for the library. <p> These two contradictory factors make the task of finding an optimal schedule for the requests very difficult. CHAPTER 7. 190 7.2.2 Related Work The problem of task or job scheduling has been extensively studied in the literature <ref> [58, 51, 56] </ref>. Related areas are discussed here, with emphasis on the applicability of existing results to our problem. Scheduling tasks are traditionally categorized as processor scheduling or shop scheduling. Processor scheduling refers to problems where a set of tasks has to be executed by a set of processors. <p> The processor scheduling problem seems to be very similar to our library scheduling problem. Several variations of the problem of scheduling tasks on processors have been investigated <ref> [56, 29] </ref>. The various parameters studied include single versus multiple processors, preemptive versus non preemptive tasks, tasks with and without release times and deadlines, precedence constraints between tasks, tasks with weights or priorities and tasks with setup times.
Reference: [57] <author> D. A. Patterson, G. Gibson, and R. H. Katz. </author> <title> A case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 109-116, </pages> <address> Chicago, </address> <year> 1988. </year>
Reference-contexts: The technique of striping data across multiple disks to overcome the I/O bottleneck has been extensively studied <ref> [57, 15] </ref>. The improvements in performance are essentially because large requests are serviced in parallel by multiple disks and multiple small requests are handled concurrently. Extensive studies of both the performance and fault tolerance CHAPTER 6. 138 aspects of RAID have been conducted.
Reference: [58] <author> M. Pinedo. </author> <title> Scheduling Theory, </title> <booktitle> Algorithms and Systems. Prentice-Hall International Series in Industrial and Systems Engineering. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year>
Reference-contexts: Since the major concern for tertiary I/O is the high latency, we choose the optimization function to be the average waiting time seen by the set of requests. The problem of task or job scheduling has been extensively studied in the literature <ref> [58, 51, 56] </ref>. Several variations of the problem of scheduling tasks on processors have been investigated [56, 29]. The problem of robotic library scheduling, however, is not addressed by any of the existing studies. The drives and processors play analogous roles, as do the requests and tasks. <p> This measure, also known as the optimization function, could be one of several possibilities. Some of the popular optimization functions that have been studied in the literature include <ref> [58, 51] </ref>: * Makespan: the time at which the last request is completed. * TWCT: the Total Weighted Completion Time (each job has an as signed weight). * TWT: the Total Waiting Time for all jobs. * Throughput: the average number of requests serviced per second. <p> These two contradictory factors make the task of finding an optimal schedule for the requests very difficult. CHAPTER 7. 190 7.2.2 Related Work The problem of task or job scheduling has been extensively studied in the literature <ref> [58, 51, 56] </ref>. Related areas are discussed here, with emphasis on the applicability of existing results to our problem. Scheduling tasks are traditionally categorized as processor scheduling or shop scheduling. Processor scheduling refers to problems where a set of tasks has to be executed by a set of processors. <p> The analogous algorithm for the minimum weighted completion time problem (M W CT )[45] has been shown to have an upper bound. In particular the algorithm is guaranteed to produce a solution that is within (1 + p (approximately within 20%) of the optimal for M W CT <ref> [58] </ref>. This bound does not necessarily hold for the M F M S problem due to the other constant factors in the cost. Due to its good performance guarantee for the M W CT problem, we evaluated this algorithm for the M F M S problem.
Reference: [59] <author> S. Prabhakar. </author> <title> An overview of current tertiary storage technology and research. </title> <type> Master's thesis, </type> <institution> University of California, Santa Barbara, </institution> <year> 1998. </year>
Reference-contexts: Chapter 7 gives a more detailed description of related work. Further details can be found in <ref> [59] </ref>. Since we expect that the tertiary scheduling problem will be difficult, we began by making the following simplifying assumption. Since the major component of the latency is the switch time, we focus only on those schedules that eliminate media switches as much as possible.
Reference: [60] <author> S. Prabhakar, K. Abdel-Ghaffar, D. Agrawal, and A. El Abbadi. </author> <title> Cyclic allocation of two-dimensional data. </title> <type> Technical Report TRCS97-08, </type> <institution> Dept. of Computer Science, Univ. of Calilfornia, Santa Barbara, </institution> <year> 1997. </year> <note> http://www.cs.ucsb.edu/TRs /TRCS97-08.ps. </note>
Reference-contexts: For more clarity, colored versions of the graphs shown in this chapter and other graphs for N 1 =N 2 =64 are available over in a techreport <ref> [60] </ref> (http://www.cs.ucsb.edu/TRs/techreports/TRCS97-08.ps). Also for clarity, in the remainder of the chapter, the Random method is not shown because of its poor performance. The performance of FX for values of M that are not powers of 2 is observed to be quite poor.
Reference: [61] <author> S. Prabhakar, K. Abdel-Ghaffar, D. Agrawal, and A. El Abbadi. </author> <title> Cyclic allocation of two-dimensional data. </title> <booktitle> In International Conference on Data Engineering, </booktitle> <pages> pages 94-101, </pages> <address> Orlando, Florida, </address> <month> Feb </month> <year> 1998. </year>
Reference-contexts: As the number of disks increases, the performance gradually becomes worse and in general is worse than DM. It should be pointed out CHAPTER 3. 74 that the FX scheme was originally defined for the cases where the number of disks is a power of 2, and extended in <ref> [61] </ref> to other values. It is interesting to note that this erratic behavior is not seen with 8 dimensions, for which both FX and DM have very similar performance. <p> CHAPTER 4. 89 4.3 Cyclic declustering for similarity search In this section we develop a new disk allocation method for optimizing the performance of nearest-neighbor queries based upon Cyclic allocation. The Cyclic schemes developed in <ref> [61] </ref> were optimized for declustering two-dimensional data for range or partial match queries. The first step is extends the schemes to make them applicable to more than two dimensions.
Reference: [62] <author> S. Prabhakar, D. Agrawal, and A. El Abbadi. </author> <title> Impact of media exchanges on robotic libraries. </title> <type> Technical Report TRCS97-07, </type> <institution> Dept. of Computer Science, Univ. of Calilfornia, Santa Barbara, </institution> <year> 1997. </year> <note> http://www.cs.ucsb.edu/TRs /TRCS97-07.ps. </note>
Reference-contexts: We are also able to establish that for optical disk technology, introducing extra media exchanges is not beneficial. The results are also available as a technical report <ref> [62] </ref>. Part I Efficient I/O for Data on Secondary Storage 15 Chapter 2 Cyclic Declustering of Two-Dimensional Data 2.1 Introduction In this chapter we introduce the notion of Cyclic Allocation Schemes by considering the restricted but important case of two-dimensional data.
Reference: [63] <author> S. Prabhakar, D. Agrawal, A. El Abbadi, and A. Singh. </author> <title> Scheduling BIBLIOGRAPHY 268 tertiary I/O in database applications. </title> <booktitle> In Eighth International Workshop on Database and Expert Systems Applications, </booktitle> <pages> pages 722-727, </pages> <address> Toulouse, France, </address> <month> September </month> <year> 1997. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: For multiple drives, the problem is shown to be NP-complete and a simple and effective (experimentally) heuristic is proposed. The details of this study are given in Chapter 7. The results of the study have been published in <ref> [63] </ref>. The assumption that minimizing media switches leads to optimal solutions is not necessarily true. This is particularly the case for certain tape technolo CHAPTER 1. 14 gies which can have very long seek and rewind times (exceeding the switch time).
Reference: [64] <author> S. Prabhakar, D. Agrawal, A. El Abbadi, A. Singh, and T. Smith. </author> <title> Browsing and placement of multiresolution images on secondary storage. </title> <type> Technical Report TRCS96-22, </type> <institution> Dept. of Computer Science, Univ. of Califor-nia, Santa Barbara, </institution> <year> 1996. </year> <note> http://www.cs.ucsb.edu/TRs/TRCS96-22.ps. </note>
Reference-contexts: As expected, the crossover point where Separated performs better than Bundled shifted in the direction of larger number of clients. Further details can be CHAPTER 6. 171 found in <ref> [64] </ref>. To study the impact of the number of disks, the number of disks to was increased to eight. Also, the size of each thumbnail (and coefficient) was doubled so that the amount of disk used on each disk was the same as in the setup with four disks. <p> Zonal and Interleaved do not have as many cache hits because the location of items that are accessed by any one client are usually on different disks, hence read ahead is not effective. Further details of these experiments can be found in <ref> [64] </ref>. 6.6.3 Discussion In all the experiments, we observe that the Distance based strategies perform better than the corresponding Round Robin strategies (not shown for clarity). From this we conclude that it is always beneficial to decluster the images based upon their content.
Reference: [65] <author> S. Prabhakar, D. Agrawal, A. El Abbadi, A. Singh, and T. Smith. </author> <title> Browsing and placement of images on secondary storage. </title> <booktitle> In IEEE International Conference on Multimedia Computing and Systems (ICMCS'97), </booktitle> <pages> pages 636-7, </pages> <address> Ottawa, Canada, June 1997. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: It is interesting to note that the results for the two settings are not the same. Further details of this study are presented in Chapter 6. The results have also been published in CHAPTER 1. 10 <ref> [65, 66] </ref>. 1.3 I/O Scheduling in Tertiary Libraries Applications such as Digital Libraries and multimedia repositories which store very large volumes of data are compelled by the current economic factors to employ tertiary storage as the primary storage medium.
Reference: [66] <author> S. Prabhakar, D. Agrawal, A. El Abbadi, A. Singh, and T. Smith. </author> <title> Browsing and placement of multiresolution images on parallel disks. </title> <booktitle> In 5th Annual Workshop on I/O in Parallel and Distributed Systems, (IOPADS'97), </booktitle> <pages> pages 102-113, </pages> <address> San Jose, U.S.A., </address> <month> November </month> <year> 1997. </year> <note> ACM Press. </note>
Reference-contexts: It is interesting to note that the results for the two settings are not the same. Further details of this study are presented in Chapter 6. The results have also been published in CHAPTER 1. 10 <ref> [65, 66] </ref>. 1.3 I/O Scheduling in Tertiary Libraries Applications such as Digital Libraries and multimedia repositories which store very large volumes of data are compelled by the current economic factors to employ tertiary storage as the primary storage medium.
Reference: [67] <institution> Quantum. Products and technology. </institution> <address> http://www.quantum.com/products, Oct. </address> <year> 1996. </year>
Reference-contexts: Commercially available automatic tape and disk libraries or jukeboxes (e.g. <ref> [23, 78, 67] </ref>) provide automated access to large amounts of tertiary storage. These libraries can hold hundreds or thousands of tapes or disks. <p> For example, the SONY OSL-2001 jukebox can switch optical disks in about 5 seconds and the average seek time for optical disks is 28 - 40 milliseconds [78]. The DLT4700 mini tape library has a load/unload cycle of 29 seconds and the average seek time is about 68 seconds <ref> [67] </ref>. Therefore for the optical disks the seek time is much smaller than the switch time and for the tapes the seek time is larger than the switch time.
Reference: [68] <author> J. T. Robinson. </author> <title> The kdb-tree: A search structure for large multidimensional dynamic indexes. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 10-18, </pages> <year> 1981. </year> <note> BIBLIOGRAPHY 269 </note>
Reference-contexts: Several index structures have been proposed in the literature that significantly improve the search operation in multidimensional spaces. Some examples of these include the R-tree [34], R fl -tree [7], the X-tree [9], HB-tree [48], GiST [35] and others <ref> [81, 68, 42, 33] </ref>. The performance of these structures is good for few dimensions, but as the number of dimensions increases beyond 10, the performance degrades significantly [3]. As in the case of range queries, an effective technique for improving the performance of the index structures is to employ parallelism.
Reference: [69] <author> C. Ruemmler and J. Wilkes. </author> <title> UNIX disk access patterns. </title> <booktitle> In USENIX, </booktitle> <pages> pages 405-420, </pages> <month> Winter </month> <year> 1993. </year>
Reference-contexts: The technique of scheduling batches of I/O requests is based upon the fact that I/O traffic is bursty in nature <ref> [6, 69] </ref>, i.e., I/O requests are generated in a short period of time followed by periods during which little or no requests are generated. This technique has also been used for scheduling batch I/O requests for single media such as linear tapes [46], serpentine tapes [37] and magnetic disks [6].
Reference: [70] <author> C. Ruemmler and J. Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Use of a simulator provides much greater flexibility. Some of the experiments are repeated with real disks. These results are discussed in Section 6.7. The simulator that we use has been developed by Kotz et al. [43] based on the model developed by Ruemmler and Wilkes of HP laboratories <ref> [70] </ref>. The model is sophisticated and is capable of simulating multiple disks connected CHAPTER 6. 150 to multiple I/O buses. placement, it is necessary to have a model of the usage patterns. Since traces of actual user access patterns for image repositories are not available, we developed approximate models. <p> In particular, four separate disks are used in order to investigate inter-disk parallelism for seeking. The simulator models the HP 97560 disk. Some of the parameters of these disks are given in Table 6.1. Further details of the disk and the model can be found in <ref> [70] </ref> and [43]. Although the specified size of the cache for the HP 97560 is 128 Kilobytes, we limited the cache to 4 Kilobytes, in order to study the placement strategies in the absence of caching.
Reference: [71] <author> S. Sarawagi. </author> <title> Database systems for efficient access to tertiary memory. </title> <booktitle> In Proceedings of the Fourteenth IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 120-6, </pages> <address> Monterey, California, 1995. </address> <publisher> IEEE Comput. Soc. Press. </publisher>
Reference-contexts: The benefits of striping in tape based systems has been studied by Drapeau and Katz [21] and also by Golubchik and Muntz [31]. In [31] a general open system model for striped libraries is developed. In <ref> [71, 72] </ref>, modifications to CHAPTER 7. 194 the architecture of database management systems are proposed for efficiently accessing tertiary storage directly. Other studies have looked at the problem of reorganization of data that is stored on tertiary media in order to improve retrieval performance [14, 74].
Reference: [72] <author> S. Sarawagi. </author> <title> Query processing in tertiary memory databases. </title> <booktitle> In Proc. of the 21st Int. Conf. on Very Large Data Bases, </booktitle> <pages> pages 585-596, </pages> <address> San Francisco, California, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The benefits of striping in tape based systems has been studied by Drapeau and Katz [21] and also by Golubchik and Muntz [31]. In [31] a general open system model for striped libraries is developed. In <ref> [71, 72] </ref>, modifications to CHAPTER 7. 194 the architecture of database management systems are proposed for efficiently accessing tertiary storage directly. Other studies have looked at the problem of reorganization of data that is stored on tertiary media in order to improve retrieval performance [14, 74].
Reference: [73] <author> S. Sarawagi and M. Stonebraker. </author> <title> Single query opimization for tertiary memory. </title> <type> Technical Report s2k-94-45, </type> <institution> Computer Science Div. U.C. Berkeley, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: More recent work on tape scheduling [37, 46] has studied efficient processing schedules for I/O requests for single tapes. Sarawagi and Stonebraker have worked on the problem of ordering I/O requests for tertiary storage media when tertiary storage is directly incorporated in a relational database <ref> [73] </ref>. The reordering is based upon knowledge of the state of the tertiary storage and the semantics of the requests that are made for performing join operations. Their work is not applicable outside the relational database framework. <p> Both these studies do not consider scheduling requests for more than one medium. Sarawagi and Stonebraker have worked on the problem of ordering I/O requests for tertiary storage media when tertiary storage is directly incorporated in a relational database <ref> [73] </ref>. The reordering is based upon knowledge of the state of the tertiary storage and the semantics of the requests that are made for performing join operations. Their work is not applicable outside the relational database framework.
Reference: [74] <author> S. Sarawagi and M. Stonebraker. </author> <title> Efficient organization of large multidimensional arrays. </title> <booktitle> In IEEE Int. Conf. on Data Engineering, </booktitle> <pages> pages 328-336, </pages> <address> Houston, TX, USA, Feb. 1994. </address> <publisher> IEEE Comput. Soc. Press. </publisher>
Reference-contexts: In [71, 72], modifications to CHAPTER 7. 194 the architecture of database management systems are proposed for efficiently accessing tertiary storage directly. Other studies have looked at the problem of reorganization of data that is stored on tertiary media in order to improve retrieval performance <ref> [14, 74] </ref>. Both studies focus on large arrays with known access patterns and make optimizations for these accesses. 7.2.3 Problem Specification From the above discussion it is clear that the general problem of robotic library scheduling is difficult (possibly NP-complete).
Reference: [75] <author> P. Scheuermann, G. Weikum, and P. Zabback. </author> <title> "Disk Cooling" in parallel disk systems. </title> <journal> Bulletin of the Technical Committee on Data Engineering, </journal> <volume> 17(3) </volume> <pages> 29-40, </pages> <month> September </month> <year> 1994. </year> <note> BIBLIOGRAPHY 270 </note>
Reference-contexts: In these cases data is generally viewed as independent files. In [4] a technique which dynamically copies blocks that are frequently accessed to a reserved space in the center of the disk to reduce seek delays is described. The notion of "Disk Cooling" is introduced in <ref> [75] </ref>, where data is dynamically redistributed based on the access patterns to yield higher I/O performance. Data placement and declustering in highly parallel systems is studied in [19]. The authors show that the static data placement problem is NP-complete.
Reference: [76] <author> H. D. Schwetman. CSIM: </author> <title> A C-based, process-oriented simulation language. </title> <booktitle> In Proceedings of the 1986 Winter Simulation Conference, </booktitle> <pages> pages 387-396, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Next the benefits of Full Medium scheduling are presented. These are followed by the experiments with single and multiple drive settings. 7.5.1 Experimental Setup The algorithms were evaluated using an event driven simulator for robotic libraries developed using CSIM <ref> [76] </ref>, a simulation library package. A simulation model of a general robotic library consisting of a variable number of drives, robot arms and media has been developed. The numbers of each of these is configurable.
Reference: [77] <author> T. R. Smith and J. Frew. </author> <title> Alexandria digital library. </title> <journal> Communications of the ACM, </journal> <volume> 38(4) </volume> <pages> 61-62, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Efficient retrieval of information defined in a two-dimensional space is gaining in importance. An increasingly popular and user-friendly approach for retrieving geographically referenced information is to request the user to define a rectangular bounding box on a map indicating the area of interest. In the Alexandria Digital Library project <ref> [77] </ref>, we are exploring various methods 16 CHAPTER 2. 17 for supporting such geographically referenced data. For example, if a user is interested in more information about the Sierra Nevada mountains, the user can draw a rectangle around the area on a map of the United States.
Reference: [78] <author> SONY. </author> <title> Recording media products. </title> <address> http://sony.sosin.com.sg/record-media/, Oct. </address> <year> 1996. </year>
Reference-contexts: Commercially available automatic tape and disk libraries or jukeboxes (e.g. <ref> [23, 78, 67] </ref>) provide automated access to large amounts of tertiary storage. These libraries can hold hundreds or thousands of tapes or disks. <p> For example, the SONY OSL-2001 jukebox can switch optical disks in about 5 seconds and the average seek time for optical disks is 28 - 40 milliseconds <ref> [78] </ref>. The DLT4700 mini tape library has a load/unload cycle of 29 seconds and the average seek time is about 68 seconds [67].
Reference: [79] <author> M. Stonebraker, L. A. Rowe, and M. Hirohama. </author> <title> The implementation of POSTGRES. </title> <journal> In IEEE Trans. on Knowledge and Data Eng., </journal> <volume> volume 2, </volume> <pages> pages 125-142, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The allocation strategy followed in Chabot is to place all thumbnails and metadata about thumbnails on disk using POSTGRES <ref> [79] </ref>, and all higher CHAPTER 6. 140 resolution images on tertiary storage. This results in relatively fast browsing of thumbnails, at the cost of slower retrieval of higher resolution images.
Reference: [80] <author> T. J. Teorey. </author> <title> Properties of disk scheduling policies in multiprogrammed computer systems. </title> <booktitle> In Proceedings of the AFIPS Fall Joint Computer Conference, </booktitle> <pages> pages 1-11, </pages> <year> 1972. </year>
Reference-contexts: We therefore have to work with variable task processing times which is a problem that has not been addressed in earlier studies. The robotic library scheduling problem is also related to the problem of disk scheduling which has been studied extensively in the operating systems community <ref> [80, 6] </ref>. The major difference between disk scheduling and library scheduling is that tertiary storage media such as tapes or optical disks are removable as opposed to fixed magnetic disks. The algorithms developed for disks all assume that the medium is never switched which is not true for robotic libraries. <p> For disk scheduling, a number of requests need to be serviced on different locations. The drive has to seek between requests that are not contiguously located. Numerous algorithms have been proposed for efficiently servicing disk requests, such as LOOK, C-LOOK, C-SCAN <ref> [80] </ref> etc. The major difference between disk scheduling and library scheduling is that tertiary storage media such as tapes or optical disks are removable as opposed CHAPTER 7. 193 to fixed magnetic disks.
Reference: [81] <author> D. White and R. Jain. </author> <title> Similarity indexing with the SS-tree. </title> <booktitle> In Proc. Int. Conf. Data Engineering, </booktitle> <pages> pages 516-523, </pages> <year> 1996. </year>
Reference-contexts: Several index structures have been proposed in the literature that significantly improve the search operation in multidimensional spaces. Some examples of these include the R-tree [34], R fl -tree [7], the X-tree [9], HB-tree [48], GiST [35] and others <ref> [81, 68, 42, 33] </ref>. The performance of these structures is good for few dimensions, but as the number of dimensions increases beyond 10, the performance degrades significantly [3]. As in the case of range queries, an effective technique for improving the performance of the index structures is to employ parallelism.
References-found: 81

