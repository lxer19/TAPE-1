URL: file://ftp.cs.unc.edu/pub/projects/proteus/reports/hpc94.ps.gz
Refering-URL: http://www.cs.unc.edu/Research/proteus/proteus-publications.html
Root-URL: http://www.cs.unc.edu
Title: SOFTWARE ISSUES IN HIGH-PERFORMANCE COMPUTING AND A FRAMEWORK FOR THE DEVELOPMENT OF HPC APPLICATIONS  
Author: PETER H. MILLS, LARS S. NYLAND, JAN F. PRINS, AND JOHN H. REIF 
Abstract: We identify the following key problems faced by HPC software: (1) the large gap between HPC design and implementation models in application development, (2) achieving high performance for a single application on different HPC platforms, and (3) accommodating constant changes in both problem specification and target architecture as computational methods and architectures evolve. To attack these problems, we suggest an application development methodology in which high-level architecture-independent specifications are elaborated, through an iterative refinement process which introduces architectural detail, into a form which can be translated to efficient low-level architecture-specific programming notations. A tree-structured development process permits multiple architectures to be targeted with implementation strategies appropriate to each architecture, and also provides a systematic means to accommodate changes in specification and target architecture. We describe the Proteus system, an application development system based on a wide-spectrum programming notation coupled with a notion of program refinement. This system supports the above development methodology via: (1) the construction of the specification and the successive designs in a uniform notation, which can be interpreted to provide early feedback on functionality and performance, (2) migration of the design towards specific architectures using formal methods of program refinement, (3) techniques for performance assessment in which the computational model varies with the level of refinement, and (4) the automatic translation of suitably refined programs to low-level parallel virtual machine codes for efficient execution. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G. Blelloch, S. Chatterjee, J. Sipelstein, and M. Zahga. CVL: </author> <title> A C vector library. </title> <type> Draft Technical Note, </type> <institution> Carnegie Mellon University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Low-level parallel virtual machines. The final translation techniques can gain wider applicability by targeting low-level parallel virtual machines that are efficiently implemented on classes of parallel architectures, rather than machine-specific languages. For example the C language along with libraries such as the vector library CVL <ref> [1] </ref>, PVM [11] or POSIX threads might be appropriate as low-level parallel virtual machine targets. 2.4. Software development process. Figure 2 illustrates the development process we have in mind.
Reference: 2. <author> G. E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: For example, at the coarsest level performance prediction may be done using the interpreter to derive simple approximations of total work. As the program is refined into data-parallel code one might employ the VRAM vector model <ref> [2] </ref>, or for a shared-memory version a model akin to the APRAM might be used for performance evaluation.
Reference: 3. <author> G. E. Blelloch, S. Chatterjee, J. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <booktitle> In Proc. 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. ACM, </booktitle> <year> 1993. </year>
Reference-contexts: In this camp might also be said to fall several functional (or equational) languages. The parallelism is typically implicit and is primarily data-parallelism. For example, a notable effort in this area is NESL (Nested Sequence Language) <ref> [3] </ref>, a data-parallel language that supports the expression of nested data parallelism and is compiled to a widely implemented lower-level vector language VCODE. Id and SISAL are other functional languages which employ a single-assignment property to enforce determinate behavior.
Reference: 4. <author> Micholas Carriero and David Gelernter. </author> <title> Coordination languages and their significance. </title> <journal> Communications of the ACM, </journal> <volume> 35(2) </volume> <pages> 96-107, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This might be said to characterize some coordination languages with simple but widely translatable logical models such as the distributed data structures of Linda <ref> [4] </ref>. In this camp might also be said to fall several functional (or equational) languages. The parallelism is typically implicit and is primarily data-parallelism.
Reference: 5. <author> Rohit Chandra, Anoop Gupta, and John Hennessy. </author> <title> Integrating concurrency and data abstraction in the COOL parallel programming language. </title> <type> Technical Report CSL-TR-92-511, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <address> Ca., </address> <year> 1992. </year>
Reference-contexts: Process (or task) parallel computations can also be succinctly expressed with a small set of process creation and synchronization primitives similar to those adopted in recent languages such as PCN [7], CC++ [6], and COOL <ref> [5] </ref>. In particular, communication is through a shared object model in which the access to shared state is controlled through object methods and class directives which constrain mutual exclusion of methods [16].
Reference: 6. <author> K. Mani Chandy and Carl Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <booktitle> In Proc. of the 4th Workshop on Parallel Computing and Compilers. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Process (or task) parallel computations can also be succinctly expressed with a small set of process creation and synchronization primitives similar to those adopted in recent languages such as PCN [7], CC++ <ref> [6] </ref>, and COOL [5]. In particular, communication is through a shared object model in which the access to shared state is controlled through object methods and class directives which constrain mutual exclusion of methods [16].
Reference: 7. <author> K. Mani Chandy and Stephen Taylor. </author> <title> An Introduction to Parallel Programming. </title> <publisher> Jones and Bartlett, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: Process (or task) parallel computations can also be succinctly expressed with a small set of process creation and synchronization primitives similar to those adopted in recent languages such as PCN <ref> [7] </ref>, CC++ [6], and COOL [5]. In particular, communication is through a shared object model in which the access to shared state is controlled through object methods and class directives which constrain mutual exclusion of methods [16].
Reference: 8. <author> Marina C. Chen, Young il Choo, and Jinke Li. </author> <title> Crystal: Theory and pragmatics of generating efficient parallel code. </title> <editor> In Boleslaw K. Szymanski, editor, </editor> <booktitle> Parallel Functional Languages and Compilers, chapter 7, </booktitle> <pages> pages 255-308. </pages> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: Fairly sophisticated translation strategies are used in these cases to bridge the gap from high-level language to machine and so achieve a measure of architecture independence. Several high-level parallel languages rely on transformation from high-level specification to realize efficient execution. Notable efforts include Crystal <ref> [8] </ref> and variants of the Bird-Meertens functional formalism [21]. Another noteworthy effort is Maude [15], a language based on rewriting logic which can be transformed into a parallel sublanguage (Simple Maude) which can then be compiled.
Reference: 9. <author> Richard Cole and Ofer Zajicek. </author> <title> The APRAM: Incorporating asynchrony into the PRAM model. </title> <booktitle> In Proc. of the First ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 169-178. </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: Models and resource metrics for parallel computation. In response to the first need there have been proposed a variety of models which extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the APRAM <ref> [9] </ref>), communication costs, such as network latency and bandwidth restrictions (e.g., the LogP model [10]), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM [23]).
Reference: 10. <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proc. 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. ACM, </booktitle> <year> 1993. </year>
Reference-contexts: In response to the first need there have been proposed a variety of models which extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the APRAM [9]), communication costs, such as network latency and bandwidth restrictions (e.g., the LogP model <ref> [10] </ref>), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM [23]).
Reference: 11. <author> Jack Dongarra, G. A. Geist, Robert Manchek, and V. S. Sundaram. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <journal> J. Computers in Physics, </journal> <volume> 7(2) </volume> <pages> 166-175, </pages> <year> 1993. </year>
Reference-contexts: Low-level parallel virtual machines. The final translation techniques can gain wider applicability by targeting low-level parallel virtual machines that are efficiently implemented on classes of parallel architectures, rather than machine-specific languages. For example the C language along with libraries such as the vector library CVL [1], PVM <ref> [11] </ref> or POSIX threads might be appropriate as low-level parallel virtual machine targets. 2.4. Software development process. Figure 2 illustrates the development process we have in mind.
Reference: 12. <author> Allen Goldberg, Jan Prins, John Reif, Rik Faith, Zhiyong Li, Peter Mills, Lars Nyland, Dan Palmer, James Riely, and Stephen Westfold. </author> <title> The Proteus System for the Development of Parallel Applications. </title> <month> April </month> <year> 1994. </year>
Reference-contexts: In particular, communication is through a shared object model in which the access to shared state is controlled through object methods and class directives which constrain mutual exclusion of methods [16]. Predefined classes such as for single-assignment objects which synchronize a producer with a consumer <ref> [12] </ref>, together with provisions for private state with barrier synchronization [17], allow the expression of a wide range of parallel computing paradigms. 3.2. Transformation of data and process parallelism.
Reference: 13. <author> James R. Larus, Satish Chandra, and David A. Wood. CICO: </author> <title> A practical shared-memory programming performance model. In Ferrante and Hey, editors, Portability and Performance for Parallel Processors. </title> <year> 1994. </year>
Reference-contexts: Such a refined model could be instrumented into parallel code through the use of annotations which incorporate explicit details of memory locality. A related approach has been used for cache-coherent shared-memory multiprocessors in the CICO project <ref> [13] </ref>, where annotations serve both for performance prediction and to guide more efficient code generation. SOFTWARE ISSUES AND DEVELOPMENT OF HPC APPLICATIONS 9 3.
Reference: 14. <author> Zhiyong Li, Peter H. Mills, and John H. Reif. </author> <title> Models and resource metrics for parallel and distributed computation. </title> <type> Technical Report, </type> <institution> Department of Computer Science, Duke University, </institution> <year> 1994. </year>
Reference-contexts: As a simple example of the process of developing improved performance models, consider a new hybrid model of parallel computation, the LogP-HMM model <ref> [14] </ref>, which extends a network model (the LogP) with a sequential hierarchical memory model (the HMM). Such a refined model could be instrumented into parallel code through the use of annotations which incorporate explicit details of memory locality.
Reference: 15. <author> Jose Meseguer. </author> <title> A logical theory of concurrent objects and its realization in the Maude language. </title> <editor> In Gul Agha, Peter Wegner, and Akinori Yonezawa, editors, </editor> <booktitle> Research Directions in Concurrent Object-Oriented Programming, </booktitle> <pages> pages 314-390. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Several high-level parallel languages rely on transformation from high-level specification to realize efficient execution. Notable efforts include Crystal [8] and variants of the Bird-Meertens functional formalism [21]. Another noteworthy effort is Maude <ref> [15] </ref>, a language based on rewriting logic which can be transformed into a parallel sublanguage (Simple Maude) which can then be compiled. In these cases the refinement steps are justified formally through inference steps or algebraic transformations.
Reference: 16. <author> Peter H. Mills. </author> <title> Parallel programming using linear variables. </title> <type> Draft Technical Report, </type> <institution> Department of Computer Science, Duke University, </institution> <year> 1994. </year> <booktitle> SOFTWARE ISSUES AND DEVELOPMENT OF HPC APPLICATIONS 13 </booktitle>
Reference-contexts: In particular, communication is through a shared object model in which the access to shared state is controlled through object methods and class directives which constrain mutual exclusion of methods <ref> [16] </ref>. Predefined classes such as for single-assignment objects which synchronize a producer with a consumer [12], together with provisions for private state with barrier synchronization [17], allow the expression of a wide range of parallel computing paradigms. 3.2. Transformation of data and process parallelism.
Reference: 17. <author> Peter H. Mills, Lars S. Nyland, Jan F. Prins, John H. Reif, and Robert A. Wagner. </author> <title> Prototyping parallel and distributed programs in Proteus. </title> <booktitle> In Proc. of the 3rd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 10-19. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Predefined classes such as for single-assignment objects which synchronize a producer with a consumer [12], together with provisions for private state with barrier synchronization <ref> [17] </ref>, allow the expression of a wide range of parallel computing paradigms. 3.2. Transformation of data and process parallelism.
Reference: 18. <author> Lars S. Nyland, Jan F. Prins, and John H. Reif. </author> <title> A data-parallel implementation of the adaptive fast multipole algorithm. </title> <booktitle> In Proc. of the 1993 DAGS/PC Symposium, </booktitle> <institution> Dartmouth College, </institution> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The goal of our experiments with Proteus has been to explore this space. Our experiments have identified new adaptive problem decompositions that yield good performance even in complex settings where bodies are not uniformly distributed <ref> [18] </ref>. Further descriptions of the language, implementation, and demonstrations are available from the Proteus WWW information server at http://www.cs.unc.edu/proteus.html. 4.
Reference: 19. <author> Helmut A. Partsch. </author> <title> Specification and Transformation of Programs: A Formal Approach to Software Development. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: express the refinement steps in a formal manner, for example as program transformations, then there is the possibility of applying these transformations 4 MILLS, NYLAND, PRINS, AND REIF automatically in the form of development "tactics", as is done in program synthesis systems such as KIDS [22], and the CIP system <ref> [19] </ref>. An automated approach can be particularly important in this setting because there are more versions to develop from a specification than is typical in the synthesis of a conventional (sequential) application.
Reference: 20. <author> Jan F. Prins and Daniel W. Palmer. </author> <title> Transforming high-level data-parallel programs into vector operations. </title> <booktitle> In Proc. of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 119-128. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: Transformation of data and process parallelism. Proteus data-parallel expressions in a functional subset involving nested sequence datatypes can currently 10 MILLS, NYLAND, PRINS, AND REIF be transformed and translated to C with vector operations (CVL) using the Kestrel Data-Type Refinement System (DTRE3) <ref> [20] </ref>.
Reference: 21. <author> D.B. Skillicorn. </author> <title> Architecture-independent parallel computation. </title> <journal> IEEE Computer, </journal> <volume> 23(12) </volume> <pages> 38-50, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Several high-level parallel languages rely on transformation from high-level specification to realize efficient execution. Notable efforts include Crystal [8] and variants of the Bird-Meertens functional formalism <ref> [21] </ref>. Another noteworthy effort is Maude [15], a language based on rewriting logic which can be transformed into a parallel sublanguage (Simple Maude) which can then be compiled. In these cases the refinement steps are justified formally through inference steps or algebraic transformations.
Reference: 22. <author> Douglas R. Smith. </author> <title> KIDS a semi-automactic program development system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: If we express the refinement steps in a formal manner, for example as program transformations, then there is the possibility of applying these transformations 4 MILLS, NYLAND, PRINS, AND REIF automatically in the form of development "tactics", as is done in program synthesis systems such as KIDS <ref> [22] </ref>, and the CIP system [19]. An automated approach can be particularly important in this setting because there are more versions to develop from a specification than is typical in the synthesis of a conventional (sequential) application.
Reference: 23. <author> J. S. Vitter and E. A. M. Shriver. </author> <title> Algorithms for parallel memory II: Hierarchical multilevel memories. </title> <journal> Algorithmica, </journal> <note> 1993. (P. Mills, J. </note> <institution> Reif) Dept. of Computer Science, Duke University, Durham, N.C. 27708-0129. E-mail address: fphm,reifg@cs.duke.edu (L. Nyland, J. Prins) Dept. of Computer Science, University of North Carolina, </institution> <address> Chapel Hill, N.C. 27599-3175. </address> <publisher> E-mail address: fnyland,prinsg@cs.unc.edu </publisher>
Reference-contexts: aspects such as asynchrony of processes (e.g., the APRAM [9]), communication costs, such as network latency and bandwidth restrictions (e.g., the LogP model [10]), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory and disk I/O (e.g., the P-HMM <ref> [23] </ref>). The most prevalent and promising recent models are parameterized (or generic) models, which abstract the architectural details into several generic parameters which we call resource metrics.
References-found: 23

