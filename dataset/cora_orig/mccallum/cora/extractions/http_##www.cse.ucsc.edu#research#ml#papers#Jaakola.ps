URL: http://www.cse.ucsc.edu/research/ml/papers/Jaakola.ps
Refering-URL: http://www.cse.ucsc.edu/~haussler/pubs.html
Root-URL: http://www.cse.ucsc.edu
Email: ftommi,hausslerg@cse.ucsc.edu  
Title: Exploiting generative models in discriminative classifiers  
Author: Tommi S. Jaakkola and David Haussler and Isaac Newton 
Address: Santa Cruz, CA 95064  20 Clarkson Road Cambridge, CB3 0EH, U.K.  
Affiliation: Department of Computer Science University of California  Institute for Mathematical Sciences University of Cambridge  
Abstract: Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.-I. Amari. </author> <title> Natural gradient works efficiently in learning. </title> <journal> Neural Computation, </journal> <volume> 10 </volume> <pages> 251-276, </pages> <year> 1998. </year>
Reference-contexts: This class of probability models defines a Riemannian manifold M fi with a local metric given by the Fisher information matrix I, where I = E X fU X U T X g, U X = r log P (Xj), and the expectation is over P (Xj) (see e.g. <ref> [1] </ref>). For simplicity we have suppressed the dependence of I and U X on the parameter setting , or equivalently, on the position in the manifold. The gradient of the log-likelihood, U X , is called the Fisher score, and plays a fundamental role in our development. <p> This latter gradient is known as the natural gradient (see e.g. <ref> [1] </ref>) and is obtained from the ordinary gradient via X = I 1 U X . We will call the mapping X ! X the natural mapping of examples into feature vectors. Again, we have supressed dependence on the parameter setting here.
Reference: [2] <author> P. Baldi, Y. Chauvin, T. Hunkapillar, and M. McClure. </author> <title> Hidden Markov models of biological primary sequence information. </title> <journal> PNAS, </journal> <volume> 91 </volume> <pages> 1059-1063, </pages> <year> 1994. </year>
Reference-contexts: In the second and more serious application of the combined classifier, we consider the well-known problem of recognizing remote homologies (evolutionary/structural similarities) between protein sequences 8 that have low residue identity. Considerable recent work has been done in refining hidden Markov models for this purpose <ref> [7, 2, 3, 6] </ref>, and such models current achieve the best performance 9 . We use these state-of-the-art HMMs as comparison cases and also as sources for deriving the kernel function.
Reference: [3] <author> S. Eddy. </author> <title> Multiple alignment using hidden Markov models. </title> <editor> In C. Rallings et al., editors, </editor> <booktitle> ISMB-95, </booktitle> <pages> pages 114-120, </pages> <address> Menlo Park, CA, July 1995. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: In the second and more serious application of the combined classifier, we consider the well-known problem of recognizing remote homologies (evolutionary/structural similarities) between protein sequences 8 that have low residue identity. Considerable recent work has been done in refining hidden Markov models for this purpose <ref> [7, 2, 3, 6] </ref>, and such models current achieve the best performance 9 . We use these state-of-the-art HMMs as comparison cases and also as sources for deriving the kernel function.
Reference: [4] <author> T. Hubbard, A. Murzin, S. Brenner, and C. Chothia. Scop: </author> <title> a structural classification of proteins database. </title> <journal> NAR, </journal> <volume> 25(1) </volume> <pages> 236-9, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: The experiment was set up as follows. We picked a particular superfamily (glycosyl-transferases) from the TIM-barrel fold in the SCOP protein struture classification <ref> [4] </ref>, and left out one of the four major families in this superfamily for testing while training the HMM as well as the combined classifier on sequences corresponding to the remaining three families.
Reference: [5] <author> T. S. Jaakkola and D. Haussler. </author> <title> Probabilistic kernel methods. </title> <note> 1998. Manuscript in preparation. </note>
Reference-contexts: Consequently, the penalized log-likelihood function can be also written entirely in terms of i ; the resulting likelihood function specifies how the coefficients are to be optimized. This optimization problem has a unique solution and can be put into a generic form (see <ref> [5] </ref> for details). Also, the form of the kernel function that establishes the connection between the logistic regression model and a kernel classifier is rather specific, i.e., has the inner product form K (X i ; X) = X T i X.
Reference: [6] <author> K. Karplus, Kimmen Sjolander, C. Barrett, M. Cline, D. Haussler, R. Hughey, L. Holm, and C. Sander. </author> <title> Predicting protein structure using hidden Markov models. Proteins: Structure, Function, </title> <journal> and Genetics, </journal> <volume> Supplement 1(1) </volume> <pages> 134-139, </pages> <year> 1997. </year>
Reference-contexts: In the second and more serious application of the combined classifier, we consider the well-known problem of recognizing remote homologies (evolutionary/structural similarities) between protein sequences 8 that have low residue identity. Considerable recent work has been done in refining hidden Markov models for this purpose <ref> [7, 2, 3, 6] </ref>, and such models current achieve the best performance 9 . We use these state-of-the-art HMMs as comparison cases and also as sources for deriving the kernel function.
Reference: [7] <author> A. Krogh, M. Brown, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Hidden Markov models in computational biology: Applications to protein modeling. </title> <journal> JMB, </journal> <volume> 235 </volume> <pages> 1501-1531, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: In the second and more serious application of the combined classifier, we consider the well-known problem of recognizing remote homologies (evolutionary/structural similarities) between protein sequences 8 that have low residue identity. Considerable recent work has been done in refining hidden Markov models for this purpose <ref> [7, 2, 3, 6] </ref>, and such models current achieve the best performance 9 . We use these state-of-the-art HMMs as comparison cases and also as sources for deriving the kernel function.
Reference: [8] <author> D. J. C. MacKay. </author> <title> Introduction to gaussian processes. </title> <note> 1997. Available from http://wol.ra.phy.cam.ac.uk/mackay/. </note>
Reference-contexts: However, many discriminative methods, which directly estimate a posterior probability for a class label (as in Gaussian process classifiers <ref> [8] </ref>) or a discriminant function for the class label (as in support vector machines [9]) have in other areas proven to be superior to generative models for classification problems. <p> Here we propose a general method for extracting these discriminatory features using a generative model. The features we propose are generally applicable but are most naturally suited to kernel methods for discriminative classification. 2 Kernel methods Here we provide a brief introduction to kernel methods; see, e.g., [9] <ref> [8] </ref> for more details. Suppose now that we have a training set of examples X i and corresponding binary labels S i (1). In kernel methods, as we define them, the label for a new example X is obtained from a weighted sum of the training labels.
Reference: [9] <author> V. Vapnik. </author> <title> The nature of statistical learning theory. </title> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: However, many discriminative methods, which directly estimate a posterior probability for a class label (as in Gaussian process classifiers [8]) or a discriminant function for the class label (as in support vector machines <ref> [9] </ref>) have in other areas proven to be superior to generative models for classification problems. The problem is that there has been no systematic way to extract features or metric relations between examples for use with discriminative methods in the context of difficult data types such as those listed above. <p> Here we propose a general method for extracting these discriminatory features using a generative model. The features we propose are generally applicable but are most naturally suited to kernel methods for discriminative classification. 2 Kernel methods Here we provide a brief introduction to kernel methods; see, e.g., <ref> [9] </ref> [8] for more details. Suppose now that we have a training set of examples X i and corresponding binary labels S i (1). In kernel methods, as we define them, the label for a new example X is obtained from a weighted sum of the training labels.
Reference: [10] <author> G. Wahba. </author> <title> Spline models for observational data. </title> <booktitle> CBMS-NSF Regional Conference Series in Applied Mathematics, </booktitle> <year> 1990. </year>
Reference-contexts: For a general kernel function to be valid, roughly speaking it only needs to be positive semi-definite (see e.g. <ref> [10] </ref>).
References-found: 10

