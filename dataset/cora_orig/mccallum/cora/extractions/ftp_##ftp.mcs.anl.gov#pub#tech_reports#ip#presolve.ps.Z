URL: ftp://ftp.mcs.anl.gov/pub/tech_reports/ip/presolve.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/otc/InteriorPoint/abstracts/Gondzio.html
Root-URL: http://www.mcs.anl.gov
Email: e-mail: gondzio@divsun.unige.ch  
Title: Presolve Analysis of Linear Programs Prior to Applying an Interior Point Method  
Author: Jacek Gondzio 
Keyword: Key words. Presolving, linear programs, interior point methods.  
Address: 102 Bd Carl Vogt, CH-1211 Geneva 4, Switzerland  
Affiliation: Logilab, HEC Geneva, Section of Management Studies, University of Geneva,  
Abstract: Technical Report 1994.3 February 7, 1994, revised December 20, 1994 Abstract Several issues concerning an analysis of large and sparse linear programming problems prior to solving them with an interior point based optimizer are addressed in this paper. Three types of presolve procedures are distinguished. Routines from the first class repeatedly analyze an LP problem formulation: eliminate empty or singleton rows and columns, look for primal and dual forcing or dominated constraints, tighten bounds for variables and shadow prices or just the opposite, relax them to find implied free variables. The second type of analysis aims at reducing a fill-in of the Cholesky factor of the normal equations matrix used to compute orthogonal projections and includes a heuristic for increasing the sparsity of the LP constraint matrix and a technique of splitting dense columns in it. Finally, routines from the third class detect, and remove, different linear dependecies of rows and columns in a constraint matrix. Computational results on problems from the Netlib collection, including some recently added infeasible ones, are given. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adler I., N. Karmarkar, M.G.C. Resende and G. Veiga. </author> <title> An implementation of Karmarkar's algorithm for linear programming, </title> <booktitle> Mathematical Programming 44 (1989) pp. </booktitle> <pages> 297-336. </pages>
Reference-contexts: Such columns create (subject to a symmetric row and column permutation) completely dense blocks dense windows in AfiA T matrices and in their Cholesky factors. Dense columns may be handled by the Schur complement mechanism [8], iterative method <ref> [1] </ref> or a combination of the two [19]. <p> Splitting has a nice property as it does not introduce rank deficiency into ~ A, a feature that neither the Schur complement mechanism [8] nor the preconditioned conjugate gradient approach <ref> [1] </ref> share. On the other hand, our experience indicates that its applicability is restricted to a case when the number of dense columns is small. The crucial issue of its implementation is the appropriate distribution of nonzeros of d in d i of (37).
Reference: [2] <author> Adler I., N. Karmarkar, M.G.C. Resende and G. Veiga. </author> <title> Data structures and programming techniques for the implementation of Karmarkar's algorithm, </title> <note> ORSA Journal on Computing 1 (1989) pp. 84-106. </note>
Reference-contexts: Unless the problem is irreducible, time spent in the presolve analysis will almost surely payoff in terms of overall CPU time. The issue of presolve analysis has been addressed by many authors <ref> [2, 4, 6, 20, 29] </ref>. In this paper we discuss several known presolve techniques, we extend some of them, simplify the others and, finally, show how they all work in practice. We distinguish three groups among them. <p> In its full generality, the problem of finding the sequence of Gaussian elimination operators that reduce a matrix A to its sparsest form, is known to be an NP complete problem <ref> [2] </ref>, [22]. The same problem applied to AA T , as it is required in interior point methods, is even more difficult. <p> However, it is considerably less efficient, in the average, [12]. To our knowledge, Adler et al. <ref> [2] </ref> were the first to improve the sparsity of L by increasing the sparsity of A. Independently, [7] and [22] addressed a more general problem of finding a nonsingular matrix M such that M A is the sparsest possible. Both [2] and [7] propose rather expensive algorithms to reduce the number <p> To our knowledge, Adler et al. <ref> [2] </ref> were the first to improve the sparsity of L by increasing the sparsity of A. Independently, [7] and [22] addressed a more general problem of finding a nonsingular matrix M such that M A is the sparsest possible. Both [2] and [7] propose rather expensive algorithms to reduce the number of nonzeros of A. We propose a considerably simpler, easy to implement heuristic that makes A sparser after relatively little analysis effort. <p> Consequently, our heuristic is unable to find two rows with different sparsity patterns (none of them being subset of the other), such that the linear combination (36) produces limited fill-in and, at the same time, cancells more nonzero entries <ref> [2, 7] </ref>. <p> The average reductions of the number of rows vary from 10% to 20% and this surely gratifies the presolve effort. Adler et al. <ref> [2] </ref> give results of their presolve (CLEAN and SPARSE) for only 35 Netlib problems (see Table I of [2]). They are concerned with inversion of A T A matrices and apply their presolve procedure to differently defined A. <p> The average reductions of the number of rows vary from 10% to 20% and this surely gratifies the presolve effort. Adler et al. <ref> [2] </ref> give results of their presolve (CLEAN and SPARSE) for only 35 Netlib problems (see Table I of [2]). They are concerned with inversion of A T A matrices and apply their presolve procedure to differently defined A. <p> Our presolver obtains better nonzero reductions on 27 problems (for three of them: AGG, BEACONFD and SCORPION the number of nonzeros in a new matrix A is more than two times smaller than that of <ref> [2] </ref>) and looses on 5 problems. This improvement results mainly from our sophisticated CLEAN routine. Chang and McCormick [7] report in their Table 7.1 results of HASP (Hierarchical Algorithm for a Sparsity Problem) run after using their less involved CLEAN on 67 Netlib problems.
Reference: [3] <author> Altman A. and J. Gondzio. </author> <title> An efficient implementation of a higher order primal-dual interior point method for large sparse linear programs, </title> <journal> Archives of Control Sciences 2 (1993), </journal> <volume> No 1/2, </volume> <pages> pp. 23-40. </pages>
Reference-contexts: Although the most of operations that perform this task are straightforward, we find it useful to recall them. We discuss this problem in Section 5. Presolve analysis techniques discussed in this paper have all been incorporated into our higher order primal-dual interior point code HOPDM <ref> [3] </ref>; this results in a considerable improvement 2 of the efficiency of its new version 2.0. From now on, we shall always refer to version 2.0 of HOPDM. <p> Next, depending on the sign of z w, one of the variables z or w is set to zero and the other to the nonnegative value jz wj. 6 Primal-dual method All presolve techniques presented in this paper have been incorporated into a new version 2.0 of the HOPDM <ref> [3] </ref> code. In this section we shall very briefly address several issues of a primal-dual logarithmic barrier method an interior point algorithm implemented in the HOPDM optimizer. The first theoretical results for this method come from [18] and [24]. Early implementations of it can be found in [8, 23]. <p> Next, a sparse Cholesky factorization [16] of AfiA T is computed and we use it twice in solves for the predictor and corrector components of (52). We still use a starting point that approximately solves an auxilliary QP problem <ref> [3] </ref>.
Reference: [4] <author> Andersen E.D. and K.D. Andersen. </author> <title> Presolving in linear programming, </title> <type> Technical Report, </type> <institution> Department of Mathematics and Computer Science, Odense University, Denmark, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Unless the problem is irreducible, time spent in the presolve analysis will almost surely payoff in terms of overall CPU time. The issue of presolve analysis has been addressed by many authors <ref> [2, 4, 6, 20, 29] </ref>. In this paper we discuss several known presolve techniques, we extend some of them, simplify the others and, finally, show how they all work in practice. We distinguish three groups among them. <p> Let us mention that some of the techniques discussed here already appeared in the paper of Brearley, Mitra and Williams [6]. We survey them all and propose useful extensions, e.g.: improving bounds on shadow prices, eliminating weakly dominated columns, detecting implied free variables (see e.g., <ref> [4] </ref>) and generating finite bounds for free variables (a particularly useful technique for an interior point optimizer). 2.1 Empty rows and columns Depending on the type of the constraint and of the sign of a right hand side component, an empty row must either be redundant or infeasible. <p> The analysis presented above is correct under the assumption that b i and l k or u k for k 2 P ifl or k 2 N ifl , respectively, are finite. To our knowlegde, this is the way in which it has been implemented in <ref> [4] </ref>, [6] and [20]. We go a step further and show that in some cases finite implied bounds can be derived from (7) and (10) even when b i is not finite. <p> It is thus unable to generate an implied bound for a free variable. With our approach we do generate such bounds; we eliminate free variables from all problems in the Netlib collection, except three models: PEROLD, PILOT-JA and PILOT4. (The presolver of <ref> [4] </ref> leaves free variables in twelve problems.) 2.5 Implied free variables The implied bounds determined by the technique described in the previous section, can be used in a different manner. <p> Constraint (24) is always sasfied with z j 0. * c j c j &lt; c j . The variable x j cannot be eliminated. If the third case occurs, the variable x j a called weakly dominated. It is claimed in <ref> [4] </ref> that the variable can be eliminated like a strongly dominated variable (c j &lt; c j ). Below, we prove a more general condition that qualifies a weekly dominated variable for elimination. <p> In our implementation the assumptions are checked while computing limits (27) and (28). On the other hand, we do not bother where the bounds on the shadow prices come from. They do not necessarily have to be generated by singleton columns, as it was required in <ref> [4] </ref>. The column j is thus weakly dominated by some, in general unknown, subset of columns. An analogous condition can be given for the second type of weak domination (c j = c j ). Proposition 2. <p> Several authors have thus tried to detect at least the simplest linearly dependent rows, i.e., duplicate rows <ref> [4, 29] </ref>, as a search for them can take full advantage of the sparsity of A. <p> The improvement surely results from our better CLEAN. Some impressive presolve reductions for the Netlib problems have been obtained by An-dersens <ref> [4] </ref>. A comparison of our results with those of Tables 6 and 7 of [4] reveals that our presolver reaches better nonzeros reductions for 45 of 88 problems and looses on 16 problems. The presolver of [4] is of the CLEAN type, so our improvement mainly results from the presence of <p> The improvement surely results from our better CLEAN. Some impressive presolve reductions for the Netlib problems have been obtained by An-dersens <ref> [4] </ref>. A comparison of our results with those of Tables 6 and 7 of [4] reveals that our presolver reaches better nonzeros reductions for 45 of 88 problems and looses on 16 problems. The presolver of [4] is of the CLEAN type, so our improvement mainly results from the presence of the heuristic to make A sparser. <p> Some impressive presolve reductions for the Netlib problems have been obtained by An-dersens <ref> [4] </ref>. A comparison of our results with those of Tables 6 and 7 of [4] reveals that our presolver reaches better nonzeros reductions for 45 of 88 problems and looses on 16 problems. The presolver of [4] is of the CLEAN type, so our improvement mainly results from the presence of the heuristic to make A sparser. We would like to further comment on the reductions due to the use of the heuristic to make A sparser (MKSP column in Table 1).
Reference: [5] <author> Bixby R. </author> <title> Implementing the simplex method: the initial basis, </title> <note> ORSA Journal on Computing 4 (1992) pp. 267-284. </note>
Reference-contexts: If a simplex based optimizer is used, this effect is beneficial since free variables enter the basis at the beginning and stay in it until optimum is reached <ref> [5] </ref>. It would undoubtedly be highly undesirable for an interior point solver (at least the one using normal equations approach to compute orthogonal projections).
Reference: [6] <author> Brearley A.L., G. Mitra and H.P. Williams. </author> <title> Analysis of mathematical programming problems prior to applying the simplex algorithm, </title> <booktitle> Mathematical Programming 15 (1975) pp. </booktitle> <pages> 54-83. </pages>
Reference-contexts: Unless the problem is irreducible, time spent in the presolve analysis will almost surely payoff in terms of overall CPU time. The issue of presolve analysis has been addressed by many authors <ref> [2, 4, 6, 20, 29] </ref>. In this paper we discuss several known presolve techniques, we extend some of them, simplify the others and, finally, show how they all work in practice. We distinguish three groups among them. <p> Most of these techniques are already known, at least since the paper of Brearley et al. <ref> [6] </ref>. We survey them and discuss several extensions in Section 2. We also show that some of them, although they lead to a problem size reduction, may result in an LP formulation that is less suitable for an interior point solver. <p> Let us mention that some of the techniques discussed here already appeared in the paper of Brearley, Mitra and Williams <ref> [6] </ref>. <p> The analysis presented above is correct under the assumption that b i and l k or u k for k 2 P ifl or k 2 N ifl , respectively, are finite. To our knowlegde, this is the way in which it has been implemented in [4], <ref> [6] </ref> and [20]. We go a step further and show that in some cases finite implied bounds can be derived from (7) and (10) even when b i is not finite.
Reference: [7] <author> Chang S.F. and S.T. McCormick. </author> <title> A hierarchical algorithm for making sparse matrices sparser, </title> <booktitle> Mathematical Programming 56 (1992) pp. </booktitle> <pages> 1-30. </pages>
Reference-contexts: However, it is considerably less efficient, in the average, [12]. To our knowledge, Adler et al. [2] were the first to improve the sparsity of L by increasing the sparsity of A. Independently, <ref> [7] </ref> and [22] addressed a more general problem of finding a nonsingular matrix M such that M A is the sparsest possible. Both [2] and [7] propose rather expensive algorithms to reduce the number of nonzeros of A. <p> To our knowledge, Adler et al. [2] were the first to improve the sparsity of L by increasing the sparsity of A. Independently, <ref> [7] </ref> and [22] addressed a more general problem of finding a nonsingular matrix M such that M A is the sparsest possible. Both [2] and [7] propose rather expensive algorithms to reduce the number of nonzeros of A. We propose a considerably simpler, easy to implement heuristic that makes A sparser after relatively little analysis effort. The normal equations approach used to compute orthogonal projections considerably suffers from the presence of dense columns in A. <p> Consequently, our heuristic is unable to find two rows with different sparsity patterns (none of them being subset of the other), such that the linear combination (36) produces limited fill-in and, at the same time, cancells more nonzero entries <ref> [2, 7] </ref>. <p> This improvement results mainly from our sophisticated CLEAN routine. Chang and McCormick <ref> [7] </ref> report in their Table 7.1 results of HASP (Hierarchical Algorithm for a Sparsity Problem) run after using their less involved CLEAN on 67 Netlib problems. They include objective nonzeros to A, which again makes comparison of their results with ours inconsistent.
Reference: [8] <author> Choi I.C., C.L. Monma and D.F. Shanno. </author> <title> Further development of a primal-dual interior point method, </title> <note> ORSA Journal on Computing 2 (1990) pp. 304-311. </note>
Reference-contexts: Such columns create (subject to a symmetric row and column permutation) completely dense blocks dense windows in AfiA T matrices and in their Cholesky factors. Dense columns may be handled by the Schur complement mechanism <ref> [8] </ref>, iterative method [1] or a combination of the two [19]. <p> Splitting has a nice property as it does not introduce rank deficiency into ~ A, a feature that neither the Schur complement mechanism <ref> [8] </ref> nor the preconditioned conjugate gradient approach [1] share. On the other hand, our experience indicates that its applicability is restricted to a case when the number of dense columns is small. <p> In this section we shall very briefly address several issues of a primal-dual logarithmic barrier method an interior point algorithm implemented in the HOPDM optimizer. The first theoretical results for this method come from [18] and [24]. Early implementations of it can be found in <ref> [8, 23] </ref>. For a current state-of-the-art implementations of the primal-dual method see [19, 20, 25, 26, 11]. Originally, the version 1.0 of the HOPDM code implemented the higher order primal-dual interior point method of Mehrotra [26] and used second or third order central trajectory approx imations.
Reference: [9] <author> Duff I.S., A.M. Erisman and J.K. Reid. </author> <title> Direct methods for sparse matrices, </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: = ~c; and the optimal dual variables of the original problem are y fl = M T ~y fl : The reader interested in more detail in the organization of the computations that involve a sequence of Gaussian elementary operations is referred to the excellent book of Duff et al. <ref> [9] </ref>. To complete the discussion of the postsolve procedure let us observe that some particular presolve modifications need some additional variable transformations to be done. The dual variables corresponding to empty rows can be set to any value, for example 0.
Reference: [10] <author> Duff I.S., N.I.M. Gould, J.K. Reid, J.A. Scott and K. Turner. </author> <title> The factorization of sparse symmetric indefinite matrices, </title> <note> IMA Journal of Numerical Analysis 11 (1991) pp. 181-204. </note>
Reference-contexts: We assume that a normal equations approach is used to find the orthogonal projections. We also assume that Cholesky decomposition LL T = AfiA T ; where fi is some diagonal scaling matrix, has to be computed at every iteration of the algorithm. A competitive | augmented system approach <ref> [10] </ref>, [12], [30], [32] | has some advantages over the normal equations one: it naturally handles free variables and dense columns of A and easily extends to nonseparable quadratic programming. However, it is considerably less efficient, in the average, [12].
Reference: [11] <author> Forrest J.J.H. and J.A. Tomlin. </author> <title> Implementing interior point linear programming methods in the Optimization Subroutine Library, </title> <journal> IBM Systems Journal 31 (1992) pp.26-38. </journal>
Reference-contexts: The reader interested in more detail in the implementation of this technique is referred to [15]. Let us observe that any equality type doubleton row can be transformed to the form (38). Some LP codes, e.g., OSL of IBM <ref> [11] </ref>, offer as an option, a presolve technique to eliminate doubleton rows. <p> The first theoretical results for this method come from [18] and [24]. Early implementations of it can be found in [8, 23]. For a current state-of-the-art implementations of the primal-dual method see <ref> [19, 20, 25, 26, 11] </ref>. Originally, the version 1.0 of the HOPDM code implemented the higher order primal-dual interior point method of Mehrotra [26] and used second or third order central trajectory approx imations. Its new version 2.0, uses a second order predictor-corrector technique implemented in a manner like [19].
Reference: [12] <author> Fourer R. and S. Mehrotra. </author> <title> Solving symmetric indefinite systems in an interior point method for linear programming, </title> <booktitle> Mathematical Programming 62 (1993) pp. </booktitle> <pages> 15-39. </pages>
Reference-contexts: We also assume that Cholesky decomposition LL T = AfiA T ; where fi is some diagonal scaling matrix, has to be computed at every iteration of the algorithm. A competitive | augmented system approach [10], <ref> [12] </ref>, [30], [32] | has some advantages over the normal equations one: it naturally handles free variables and dense columns of A and easily extends to nonseparable quadratic programming. However, it is considerably less efficient, in the average, [12]. <p> A competitive | augmented system approach [10], <ref> [12] </ref>, [30], [32] | has some advantages over the normal equations one: it naturally handles free variables and dense columns of A and easily extends to nonseparable quadratic programming. However, it is considerably less efficient, in the average, [12]. To our knowledge, Adler et al. [2] were the first to improve the sparsity of L by increasing the sparsity of A. Independently, [7] and [22] addressed a more general problem of finding a nonsingular matrix M such that M A is the sparsest possible.
Reference: [13] <author> Gay D.M. </author> <title> Electronic mail distribution of linear programming test problems, </title> <journal> Mathematical Programming Society COAL Newsletter 13 (1985) pp. </journal> <pages> 10-12. </pages>
Reference-contexts: From now on, we shall always refer to version 2.0 of HOPDM. In Section 6, we briefly describe our implementation of the primal-dual method and in Section 7 we show that when run on the Netlib <ref> [13] </ref> suite of 91 problems, our code looses only 74 iterations to OB1.60 (a state-of-the-art implementation of Lustig et al. [20], version of September 1993 of a predictor-corrector primal-dual method of Mehrotra [25]). <p> The algorithm terminates when 8-digit accurate solution is obtained, i.e., when jc T x (b T y u T w)j 10 8 : (56) 7 Numerical results We shall demonstrate in this section practical performance of our presolve procedure applied to solving linear problems from the Netlib collection <ref> [13] </ref>, including the recently added infeasible ones. In fact, we ran HOPDM on a larger suite of tests, some of them of a considerably larger size than the biggest Netlib models and we sometimes obtained spectacular reductions.
Reference: [14] <author> George, A. and J.W.H. Liu. </author> <title> The evolution of the minimum degree ordering algorithm, </title> <note> SIAM Review 31 (1989) pp. 1-19. </note>
Reference-contexts: When reporting CPU times for HOPDM, we have excluded the time for model input and solution output. The time spent in the presolve analysis and in the preprocessing for the Cholesky factorization (with the multiple minimum degree ordering of <ref> [14] </ref> and with the symmetric factorization [16]) is included in the overall solution time. For OB1.60, we report only the number of iterations to reach the optimum. These results show that the presolve analysis gives, in the average, reduction of the solution time.
Reference: [15] <author> Gondzio J. </author> <title> Splitting dense columns of constraint matrix in interior point methods for large scale linear programming, </title> <note> Optimization 24 (1992) pp. 285-297. </note>
Reference-contexts: The pivot row is then used to eliminate nonzero entries from all rows with the superset sparsity pattern. Such an approach can never produce fill-in. We also recall in Section 3 the technique of splitting dense columns <ref> [15] </ref>, [31], and later show limits of its application. Finally, in Section 4 we address the problem of identifying linear dependencies in the matrix A. We show that our heuristic for making A sparser naturally eliminates linearly dependent rows. <p> Dense columns may be handled by the Schur complement mechanism [8], iterative method [1] or a combination of the two [19]. In our implementation we split <ref> [15] </ref>, [31] dense columns, i.e., we cut them into shorter pieces. 3.1 Making A sparser The basic idea of our heuristic for improving the sparsity of A is to analyze every equality type row, say, a ifl and to look for all LP constraints with the sparsity pattern being the superset <p> When applied to general linear programs <ref> [15] </ref>, [31] it consists of cutting dense columns into shorter pieces and replacing one large dense window of AfiA T with several smaller ones that can be easier accommodated by the Cholesky decomposition. <p> Further reduction of the number of nonzero entries in the adjacency structure ~ A ~ fi ~ A T and in the Cholesky factor can be obtained after careful analysis of the sparsity pattern of a sparse part of matrix A <ref> [15] </ref>. A heuristic implemented in our code is based on such an analysis. It looks for a compromise between the following two goals. <p> Secondly, we look at the number of ~ A ~ fi ~ A T nonzero entries that do not belong to the new dense blocks and keep this number as small as possible. The reader interested in more detail in the implementation of this technique is referred to <ref> [15] </ref>. Let us observe that any equality type doubleton row can be transformed to the form (38). Some LP codes, e.g., OSL of IBM [11], offer as an option, a presolve technique to eliminate doubleton rows. <p> There are three major modules to be distinguished in the presolve analysis: CLEAN implements all techniques of the logical analysis discussed in Section 2 and the search for duplicate columns (Section 4.2); MAKESPARSER implements the heuristic of Section 3.1; and SPLIT implements the heuristic recalled in Section 3.2 (see <ref> [15] </ref> for more detail). These modules are called in the following sequence CLEAN, MAKESPARSER, CLEAN, SPLIT. The second call of CLEAN is clearly skipped if the algorithm to make A sparser could not remove a single nonzero entry from A.
Reference: [16] <author> Gondzio J. </author> <title> Implementing Cholesky factorization for interior point methods of linear programming, </title> <note> Optimization 27 (1993) pp. 121-140. </note>
Reference-contexts: In our implementation, (51) is reduced to the normal equations form AfiA T y = h; (55) where fi = (X 1 Z + S 1 W ) 1 . Next, a sparse Cholesky factorization <ref> [16] </ref> of AfiA T is computed and we use it twice in solves for the predictor and corrector components of (52). We still use a starting point that approximately solves an auxilliary QP problem [3]. <p> When reporting CPU times for HOPDM, we have excluded the time for model input and solution output. The time spent in the presolve analysis and in the preprocessing for the Cholesky factorization (with the multiple minimum degree ordering of [14] and with the symmetric factorization <ref> [16] </ref>) is included in the overall solution time. For OB1.60, we report only the number of iterations to reach the optimum. These results show that the presolve analysis gives, in the average, reduction of the solution time.
Reference: [17] <author> Karmarkar N. </author> <title> A new polynomial-time algorithm for linear programming, </title> <note> Combinatorica 4 (1984) 373-395. </note>
Reference: [18] <author> Kojima M., S. Mizuno and A. Yoshise. </author> <title> A primal-dual interior point algorithm for linear programming, </title> <editor> in: N. Megiddo ed., </editor> <booktitle> Progress in Mathematical Programming, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989, </year> <pages> pp. 29-48. </pages>
Reference-contexts: In this section we shall very briefly address several issues of a primal-dual logarithmic barrier method an interior point algorithm implemented in the HOPDM optimizer. The first theoretical results for this method come from <ref> [18] </ref> and [24]. Early implementations of it can be found in [8, 23]. For a current state-of-the-art implementations of the primal-dual method see [19, 20, 25, 26, 11].
Reference: [19] <author> Lustig I.J., R.E. Marsten and D.F. Shanno. </author> <title> On implementing Mehrotra's predictor-corrector interior point method for linear programming, </title> <note> SIAM Journal on Optimization 2 (1992) pp. 435-449. </note>
Reference-contexts: Such columns create (subject to a symmetric row and column permutation) completely dense blocks dense windows in AfiA T matrices and in their Cholesky factors. Dense columns may be handled by the Schur complement mechanism [8], iterative method [1] or a combination of the two <ref> [19] </ref>. <p> The first theoretical results for this method come from [18] and [24]. Early implementations of it can be found in [8, 23]. For a current state-of-the-art implementations of the primal-dual method see <ref> [19, 20, 25, 26, 11] </ref>. Originally, the version 1.0 of the HOPDM code implemented the higher order primal-dual interior point method of Mehrotra [26] and used second or third order central trajectory approx imations. Its new version 2.0, uses a second order predictor-corrector technique implemented in a manner like [19]. <p> Originally, the version 1.0 of the HOPDM code implemented the higher order primal-dual interior point method of Mehrotra [26] and used second or third order central trajectory approx imations. Its new version 2.0, uses a second order predictor-corrector technique implemented in a manner like <ref> [19] </ref>. <p> For this reason we restrict the analysis to the Netlib collection. In all runs reported in this paper we use the same stopping criterion (56) as <ref> [19] </ref> and [25], i.e., we terminate after an 8-digit accurate solution is obtained.
Reference: [20] <author> Lustig I.J., R.E. Marsten and D.F. Shanno. </author> <title> Interior point methods for linear programming computational state of the art, </title> <note> ORSA Journal on Computing 6 (1994) 1-14. 21 </note>
Reference-contexts: Unless the problem is irreducible, time spent in the presolve analysis will almost surely payoff in terms of overall CPU time. The issue of presolve analysis has been addressed by many authors <ref> [2, 4, 6, 20, 29] </ref>. In this paper we discuss several known presolve techniques, we extend some of them, simplify the others and, finally, show how they all work in practice. We distinguish three groups among them. <p> In Section 6, we briefly describe our implementation of the primal-dual method and in Section 7 we show that when run on the Netlib [13] suite of 91 problems, our code looses only 74 iterations to OB1.60 (a state-of-the-art implementation of Lustig et al. <ref> [20] </ref>, version of September 1993 of a predictor-corrector primal-dual method of Mehrotra [25]). This means that we loose in the average one iteration per problem, a loss of efficiency less than 5%. <p> The analysis presented above is correct under the assumption that b i and l k or u k for k 2 P ifl or k 2 N ifl , respectively, are finite. To our knowlegde, this is the way in which it has been implemented in [4], [6] and <ref> [20] </ref>. We go a step further and show that in some cases finite implied bounds can be derived from (7) and (10) even when b i is not finite. <p> The first theoretical results for this method come from [18] and [24]. Early implementations of it can be found in [8, 23]. For a current state-of-the-art implementations of the primal-dual method see <ref> [19, 20, 25, 26, 11] </ref>. Originally, the version 1.0 of the HOPDM code implemented the higher order primal-dual interior point method of Mehrotra [26] and used second or third order central trajectory approx imations. Its new version 2.0, uses a second order predictor-corrector technique implemented in a manner like [19]. <p> The value of u 0 = 10 2 has been found to be a good compromise and it is now used in a default presolve analysis of HOPDM. Table 3 compares the efficiency of HOPDM and the OB1.60 code of <ref> [20] </ref>, version of September 1993. The results for OB1.60 come from the paper of Xu et al. [33], Table I, as we have no direct access to this implementation.
Reference: [21] <author> Lustig I.J., J.M. Mulvey and T.J. Carpenter. </author> <title> Formulating two-stage stochastic programs for interior point methods, </title> <note> Operations Research 39 (1991) pp. 757-770. </note>
Reference-contexts: do not exploit such a possibility (its detection would involve floating point operations and would require considerably more effort), a simple heuristic proposed in this paper produces comparably good results as more involved algorithms. 3.2 Splitting dense columns The idea of splitting dense columns has its origin in stochastic programming <ref> [21] </ref>, [27]. When applied to general linear programs [15], [31] it consists of cutting dense columns into shorter pieces and replacing one large dense window of AfiA T with several smaller ones that can be easier accommodated by the Cholesky decomposition.
Reference: [22] <author> McCormick S.T. </author> <title> Making sparse matrices sparser: computational results, </title> <booktitle> Mathematical Programming 49 (1990) pp. </booktitle> <pages> 91-111. </pages>
Reference-contexts: In its full generality, the problem of finding the sequence of Gaussian elimination operators that reduce a matrix A to its sparsest form, is known to be an NP complete problem [2], <ref> [22] </ref>. The same problem applied to AA T , as it is required in interior point methods, is even more difficult. <p> However, it is considerably less efficient, in the average, [12]. To our knowledge, Adler et al. [2] were the first to improve the sparsity of L by increasing the sparsity of A. Independently, [7] and <ref> [22] </ref> addressed a more general problem of finding a nonsingular matrix M such that M A is the sparsest possible. Both [2] and [7] propose rather expensive algorithms to reduce the number of nonzeros of A.
Reference: [23] <author> McShane K.A., C.L. Monma and D.F. Shanno. </author> <title> An implementation of a primal-dual interior point method for linear programming, </title> <note> ORSA Journal on Computing 1 (1989) pp. 70-89. </note>
Reference-contexts: In this section we shall very briefly address several issues of a primal-dual logarithmic barrier method an interior point algorithm implemented in the HOPDM optimizer. The first theoretical results for this method come from [18] and [24]. Early implementations of it can be found in <ref> [8, 23] </ref>. For a current state-of-the-art implementations of the primal-dual method see [19, 20, 25, 26, 11]. Originally, the version 1.0 of the HOPDM code implemented the higher order primal-dual interior point method of Mehrotra [26] and used second or third order central trajectory approx imations.
Reference: [24] <author> Megiddo N. </author> <title> Pathways to the optimal set in linear programming, </title> <editor> in: N. Megiddo ed., </editor> <booktitle> Progress in Mathematical Programming, </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989, </year> <pages> pp. 131-158. </pages>
Reference-contexts: In this section we shall very briefly address several issues of a primal-dual logarithmic barrier method an interior point algorithm implemented in the HOPDM optimizer. The first theoretical results for this method come from [18] and <ref> [24] </ref>. Early implementations of it can be found in [8, 23]. For a current state-of-the-art implementations of the primal-dual method see [19, 20, 25, 26, 11].
Reference: [25] <author> Mehrotra S. </author> <title> On the implementation of a primal-dual interior point method, </title> <note> SIAM Journal on Optimization 2 (1992) pp. 575-601. </note>
Reference-contexts: our implementation of the primal-dual method and in Section 7 we show that when run on the Netlib [13] suite of 91 problems, our code looses only 74 iterations to OB1.60 (a state-of-the-art implementation of Lustig et al. [20], version of September 1993 of a predictor-corrector primal-dual method of Mehrotra <ref> [25] </ref>). This means that we loose in the average one iteration per problem, a loss of efficiency less than 5%. <p> The first theoretical results for this method come from [18] and [24]. Early implementations of it can be found in [8, 23]. For a current state-of-the-art implementations of the primal-dual method see <ref> [19, 20, 25, 26, 11] </ref>. Originally, the version 1.0 of the HOPDM code implemented the higher order primal-dual interior point method of Mehrotra [26] and used second or third order central trajectory approx imations. Its new version 2.0, uses a second order predictor-corrector technique implemented in a manner like [19]. <p> Mehrotra <ref> [25] </ref> composed the direction step (x; s; y; z; w) (denoted with for short) from two parts = a + c : (52) The affine scaling predictor direction a solves a linear system like (51) for a right hand side equal to the current violation of the first order optimality conditions <p> For this reason we restrict the analysis to the Netlib collection. In all runs reported in this paper we use the same stopping criterion (56) as [19] and <ref> [25] </ref>, i.e., we terminate after an 8-digit accurate solution is obtained.
Reference: [26] <author> Mehrotra S. </author> <title> Higher order methods and their performance, </title> <type> Technical Report 90-16R1, </type> <institution> Department of Industrial Engineering and Management Sciences, Northwestern University, </institution> <address> Evanston, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: The first theoretical results for this method come from [18] and [24]. Early implementations of it can be found in [8, 23]. For a current state-of-the-art implementations of the primal-dual method see <ref> [19, 20, 25, 26, 11] </ref>. Originally, the version 1.0 of the HOPDM code implemented the higher order primal-dual interior point method of Mehrotra [26] and used second or third order central trajectory approx imations. Its new version 2.0, uses a second order predictor-corrector technique implemented in a manner like [19]. <p> Early implementations of it can be found in [8, 23]. For a current state-of-the-art implementations of the primal-dual method see [19, 20, 25, 26, 11]. Originally, the version 1.0 of the HOPDM code implemented the higher order primal-dual interior point method of Mehrotra <ref> [26] </ref> and used second or third order central trajectory approx imations. Its new version 2.0, uses a second order predictor-corrector technique implemented in a manner like [19].
Reference: [27] <author> Mulvey J.M. and A. Ruszczynski. </author> <title> A diagonal quadratic approximation method for large scale linear programs, </title> <note> Operations Research Letters (1992). </note>
Reference-contexts: not exploit such a possibility (its detection would involve floating point operations and would require considerably more effort), a simple heuristic proposed in this paper produces comparably good results as more involved algorithms. 3.2 Splitting dense columns The idea of splitting dense columns has its origin in stochastic programming [21], <ref> [27] </ref>. When applied to general linear programs [15], [31] it consists of cutting dense columns into shorter pieces and replacing one large dense window of AfiA T with several smaller ones that can be easier accommodated by the Cholesky decomposition.
Reference: [28] <author> Murtagh B. </author> <title> Advanced Linear Programming: Computation and Practice, </title> <address> McGrew Hill, New York, </address> <year> 1981. </year>
Reference-contexts: A similar analysis applies to a "greater than or equal to" row. Let us also observe that any equality type constraint can be replaced (for the purpose of this analysis only) with two inequalities, like an inequality row with a finite range (see e.g., <ref> [28] </ref>). We then assume that row i of (2) originally had the form X j The following four possibilities may occur: * b i &lt; b i . Constraint (10) cannot be satisfied; the problem is infeasible. * b i = b i . Constraint (10) is forcing.
Reference: [29] <author> Tomlin J. and J.S. Welch. </author> <title> Finding duplicate rows in a linear program, </title> <note> Operations Research Letters 5 (1986) pp. 7-11. </note>
Reference-contexts: Unless the problem is irreducible, time spent in the presolve analysis will almost surely payoff in terms of overall CPU time. The issue of presolve analysis has been addressed by many authors <ref> [2, 4, 6, 20, 29] </ref>. In this paper we discuss several known presolve techniques, we extend some of them, simplify the others and, finally, show how they all work in practice. We distinguish three groups among them. <p> Several authors have thus tried to detect at least the simplest linearly dependent rows, i.e., duplicate rows <ref> [4, 29] </ref>, as a search for them can take full advantage of the sparsity of A. <p> Several authors have thus tried to detect at least the simplest linearly dependent rows, i.e., duplicate rows [4, 29], as a search for them can take full advantage of the sparsity of A. By duplicate rows, we mean, as in <ref> [29] </ref>, two rows a ifl and a kfl for which a real ff exists such 11 that a kfl = ffa ifl : (40) Let us observe that our heuristic of Section 3.1 to make A sparser naturally detects such a situation if at least one of the constraints i and
Reference: [30] <author> Turner K. </author> <title> Computing projections for the Karmarkar algorithm, </title> <note> Linear Algebra and its Applications 152 (1991) pp. 141-154. </note>
Reference-contexts: We also assume that Cholesky decomposition LL T = AfiA T ; where fi is some diagonal scaling matrix, has to be computed at every iteration of the algorithm. A competitive | augmented system approach [10], [12], <ref> [30] </ref>, [32] | has some advantages over the normal equations one: it naturally handles free variables and dense columns of A and easily extends to nonseparable quadratic programming. However, it is considerably less efficient, in the average, [12].
Reference: [31] <author> Vanderbei R.J. </author> <title> Splitting dense columns in sparse linear systems, </title> <note> Linear Algebra and its Applications 152 (1991) pp. 107-117. </note>
Reference-contexts: The pivot row is then used to eliminate nonzero entries from all rows with the superset sparsity pattern. Such an approach can never produce fill-in. We also recall in Section 3 the technique of splitting dense columns [15], <ref> [31] </ref>, and later show limits of its application. Finally, in Section 4 we address the problem of identifying linear dependencies in the matrix A. We show that our heuristic for making A sparser naturally eliminates linearly dependent rows. <p> Dense columns may be handled by the Schur complement mechanism [8], iterative method [1] or a combination of the two [19]. In our implementation we split [15], <ref> [31] </ref> dense columns, i.e., we cut them into shorter pieces. 3.1 Making A sparser The basic idea of our heuristic for improving the sparsity of A is to analyze every equality type row, say, a ifl and to look for all LP constraints with the sparsity pattern being the superset of <p> When applied to general linear programs [15], <ref> [31] </ref> it consists of cutting dense columns into shorter pieces and replacing one large dense window of AfiA T with several smaller ones that can be easier accommodated by the Cholesky decomposition. <p> The crucial issue of its implementation is the appropriate distribution of nonzeros of d in d i of (37). Arbitrary splitting into columns that are no longer than a given threshold has been proposed in <ref> [31] </ref>. Further reduction of the number of nonzero entries in the adjacency structure ~ A ~ fi ~ A T and in the Cholesky factor can be obtained after careful analysis of the sparsity pattern of a sparse part of matrix A [15].
Reference: [32] <author> Vanderbei R. and T.J. Carpenter. </author> <title> Symmetric indefinite systems for interior point methods, </title> <type> Technical Report SOR 91-7, </type> <institution> Department of Civil Engineering and Operations Research, Princeton University, Princeton, </institution> <address> New Jersey, </address> <year> 1991. </year>
Reference-contexts: We also assume that Cholesky decomposition LL T = AfiA T ; where fi is some diagonal scaling matrix, has to be computed at every iteration of the algorithm. A competitive | augmented system approach [10], [12], [30], <ref> [32] </ref> | has some advantages over the normal equations one: it naturally handles free variables and dense columns of A and easily extends to nonseparable quadratic programming. However, it is considerably less efficient, in the average, [12].

References-found: 32

