URL: http://www.cs.utah.edu/~retrac/papers/sigmetrics89.ps.Z
Refering-URL: http://www.cs.utah.edu/~retrac/papers.html
Root-URL: 
Title: Optimistic Implementation of Bulk Data Transfer Protocols range of load conditions the optimistic implementation outperforms
Author: John B. Carter Willy Zwaenepoel 
Note: This research was supported in part by the National Science Foundation under Grant CCR-8716914 and by a National Science Foundation Fellowship.  
Address: Houston, Texas  
Affiliation: Department of Computer Science Rice University  
Abstract: During a bulk data transfer over a high speed network, there is a high probability that the next packet received from the network by the destination host is the next packet in the transfer. An optimistic implementation of a bulk data transfer protocol takes advantage of this observation by instructing the network interface on the destination host to deposit the data of the next packet immediately into its anticipated final location. No copying of the data is required in the common case, and overhead is greatly reduced. Our optimistic implementation of the V kernel bulk data transfer protocols on SUN-3/50 workstations connected by a 10 megabit Ethernet achieves peak process-to-process data rates of 8.3 megabits per second for 1-megabyte transfers, and 6.8 megabits per second for 8-kilobyte transfers, compared to 6.1 and 5.0 megabits per second for the pessimistic implementation. When the reception of a bulk data transfer is interrupted by the arrival of unexpected packets at the destination, the worst-case performance of the optimistic implementation is only 15 percent less than that of the pessimistic implementation. Measurements and simulation indicate that for a wide 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.R. Boggs, J.C. Mogul, and C.A. Kent. </author> <title> Measured capacity of an Ethernet: Myths and reality. </title> <booktitle> In Proceedings of the 1988 Sigcomm Symposium, </booktitle> <pages> pages 222-234, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Figure 4 provides a graphical comparison of the throughput achieved by the various implementations. When two 1-megabyte data transfers are taking place simultaneously between two pairs of machines, each transfer receives roughly half the total available network throughput (See also <ref> [1] </ref>). When 1-megabyte data transfers from two different machines are directed simultaneously at the same destination machine, the transfers achieve an average total throughput of 6.5 megabits per second. 35 percent of these blasts are interrupted by a blast packet from a different blast.
Reference: [2] <author> D.R. Cheriton. VMTP: </author> <title> A transport protocol for the next generation of communication systems. </title> <booktitle> In Proceedings of the 1986 Sigcomm Symposium, </booktitle> <pages> pages 406-415, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Selective retransmission is used to deal with missed packets. Flow control measures may be needed to reduce packet loss when going from a fast to a slow machine <ref> [2] </ref>. The advantages of a blast protocol over more conventional protocols such as stop-and-wait and sliding window derive from the reduced number of acknowledgements, and from the fact that protocol and transmission overhead on the sender and the receiver occur in parallel [11].
Reference: [3] <author> D.R. Cheriton and C.L. Williamson. </author> <title> Network measurement of the VMTP request-response protocol in the V distributed system. </title> <booktitle> In Proceedings of the 1987 ACM Sigmetrics Conference, </booktitle> <pages> pages 128-140, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Data Rate 4 7.8 0.4 4.2 45% 32 52.3 0.9 5.0 53% 1024 4 1035 | 8.1 86% Table 2 Worst-Case Optimistic Implementation 3 Each 32-kilobyte blast is interrupted. 4 Only one 32-kilobyte blast is interrupted. 5 These measurements are better than those reported in <ref> [3] </ref>, because the latter implementation did not use the gather capability of the interface during transmission, while ours does. 6 A memory-to-memory copy takes 0.37 milliseconds per kilobyte. a pessimistic implementation, the data must always be copied from the kernel receive buffer area to its destination. <p> received. * The number of interrupted blasts, and the type of packet that caused the blast to be interrupted. * The average blast size. * The number of non-blast packets received, including non-blast V, other unicast, and broad cast packets. 8 For a detailed account of V network traffic see <ref> [3] </ref>. 6.2 Normal Operation To assess performance during normal operation, we have collected statistics for different periods of 10,000 seconds each. While the exact numbers differ somewhat from one session to another, the overall shape of the figures remains the same. <p> This significantly increases the traffic to the file server, and hence the number of interrupted blasts. 7 Related Work Locality in network traffic has been noted earlier <ref> [3, 8, 10] </ref>. However, to the best of our knowledge, no protocol implementation has taken advantage of this phenomenon to the extent described here. <p> The blast protocols discussed here were developed as part of the V interkernel protocol [4]. Similar protocols are now part of the VMTP protocol definition <ref> [3] </ref>. VMTP claims 4.5 megabits per second process-to-process data rates on SUN-3/75s (slightly faster than our SUN-3/50s), with no performance improvements when increasing the segment size beyond 16 kilobytes. The current VTMP implementation uses neither the scatter nor the gather feature of the network interfaces on the SUN.
Reference: [4] <author> D.R. Cheriton and W. Zwaenepoel. </author> <title> The distributed V operating system and its performance for diskless workstations. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 128-140, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: The small packets used for 32-byte message transactions in the V interkernel protocol fit entirely into the header <ref> [4] </ref>. When the first packet of a blast arrives, the kernel redirects the odd-numbered buffer descriptors to point to where consecutive blast packets should deposit their data in user memory. 1 The even-numbered buffer descriptors for the headers are not changed (See Figure 3). <p> In the expected case, Jacobson's TCP implementation avoids invoking the general purpose packet reception code, but the data must still be copied to user space. The blast protocols discussed here were developed as part of the V interkernel protocol <ref> [4] </ref>. Similar protocols are now part of the VMTP protocol definition [3]. VMTP claims 4.5 megabits per second process-to-process data rates on SUN-3/75s (slightly faster than our SUN-3/50s), with no performance improvements when increasing the segment size beyond 16 kilobytes.
Reference: [5] <author> Intel Corporation. </author> <title> Intel i82856 interface reference guide. </title>
Reference-contexts: Finally, the data portion is copied to its final destination. An optimistic implementation takes full advantage of the scatter-gather capabilities of network interfaces such as the AMD LANCE [6] and the Intel i82856 <ref> [5] </ref> present on various models of SUN workstations. These interfaces allow portions of an incoming packet to be delivered to noncontiguous areas of memory. The bulk data transfer protocol studied in this paper is a blast protocol [11].
Reference: [6] <author> Advanced Micro Devices. </author> <title> Am 7990: Local area network controller for Ethernet (LANCE). </title>
Reference-contexts: Finally, the data portion is copied to its final destination. An optimistic implementation takes full advantage of the scatter-gather capabilities of network interfaces such as the AMD LANCE <ref> [6] </ref> and the Intel i82856 [5] present on various models of SUN workstations. These interfaces allow portions of an incoming packet to be delivered to noncontiguous areas of memory. The bulk data transfer protocol studied in this paper is a blast protocol [11].
Reference: [7] <author> V. Jacobson. </author> <title> Note on TCP/IP mailing list, </title> <address> tcp-ip@SRI-NIC.ARPA, </address> <month> March </month> <year> 1988. </year>
Reference-contexts: However, to the best of our knowledge, no protocol implementation has taken advantage of this phenomenon to the extent described here. In his efforts to improve TCP performance, Jacobson predicts that the next incoming TCP packet on a given connection will be the next packet on that connection <ref> [7] </ref>. This prediction is less aggressive than ours, which assumes that the next packet that is received at a host is the next in the current bulk data transfer.
Reference: [8] <author> R. Jain and S. Ruthier. </author> <title> Packet trains: Measurements and a new model for computer network traffic. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> SAC-4(6):986-995, </volume> <month> Septem-ber </month> <year> 1986. </year>
Reference-contexts: This significantly increases the traffic to the file server, and hence the number of interrupted blasts. 7 Related Work Locality in network traffic has been noted earlier <ref> [3, 8, 10] </ref>. However, to the best of our knowledge, no protocol implementation has taken advantage of this phenomenon to the extent described here.
Reference: [9] <author> J.K. Ousterhout, A.R. Cherenson, F. Douglis, M.N. Nelson, and B.B. Welch. </author> <title> The Sprite network operating system. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The current VTMP implementation uses neither the scatter nor the gather feature of the network interfaces on the SUN. Sprite uses blast protocols (called implicit acknowledgement and fragmentation) for multi-packet RPCs <ref> [9] </ref>. Sprite performance measurements, also on SUN-3/75s, indicate a kernel-to-kernel data rate of approximately 6 megabits per second, and a process-to-process data rate of 3.8 megabits per second.
Reference: [10] <author> J.F. Shoch and J.A. Hupp. </author> <title> Measured performance of an Ethernet local network. </title> <journal> Communications of the ACM, </journal> <volume> 23(12) </volume> <pages> 711-721, </pages> <month> Decem-ber </month> <year> 1980. </year>
Reference-contexts: This significantly increases the traffic to the file server, and hence the number of interrupted blasts. 7 Related Work Locality in network traffic has been noted earlier <ref> [3, 8, 10] </ref>. However, to the best of our knowledge, no protocol implementation has taken advantage of this phenomenon to the extent described here.
Reference: [11] <author> W. Zwaenepoel. </author> <title> Protocols for large data transfers over local area networks. </title> <booktitle> In Proceedings of the 9th Data Communications Symposium, </booktitle> <pages> pages 22-32, </pages> <month> September </month> <year> 1985. </year>
Reference-contexts: These interfaces allow portions of an incoming packet to be delivered to noncontiguous areas of memory. The bulk data transfer protocol studied in this paper is a blast protocol <ref> [11] </ref>. The data to be transferred is divided into one or more blasts of fixed maximum size. For each blast, the sender transmits the required number of packets, and then waits for an acknowledgement. <p> The advantages of a blast protocol over more conventional protocols such as stop-and-wait and sliding window derive from the reduced number of acknowledgements, and from the fact that protocol and transmission overhead on the sender and the receiver occur in parallel <ref> [11] </ref>. The ideas presented in this paper are not specific to blast protocols, and can be applied to any bulk data transfer protocol in which the following two properties hold: 1.
References-found: 11

