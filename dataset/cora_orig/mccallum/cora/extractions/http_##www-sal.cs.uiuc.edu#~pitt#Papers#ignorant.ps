URL: http://www-sal.cs.uiuc.edu/~pitt/Papers/ignorant.ps
Refering-URL: http://www-sal.cs.uiuc.edu/~pitt/
Root-URL: http://www.cs.uiuc.edu
Email: frazier@cs.acu.edu  sg@cs.wustl.edu  nmishra@uiuc.edu  pitt@cs.uiuc.edu  
Title: Learning From a Consistently Ignorant Teacher  
Author: Michael Frazier Abilene Christian Sally Goldman Nina Mishra Leonard Pitt 
Note: "Consistency requires you to be as ignorant today as you were a year ago." Bernard Berenson (1865-1959) Supported in part by NSF Grant IRI-9014840, and by NASA grant NAG 1-613. This work was completed while  Supported in part by NSF Grant CCR-9110108 and an NSF NYI Grant CCR-9357707 with matching funds provided by Xerox Corporation, Palo Alto Research Center. Supported in part by NSF Grant IRI-9014840.  
Address: Abilene, TX 79699  St. Louis, MO 63130  Urbana, IL 61801  Urbana, IL 61801  New Orleans.  
Affiliation: Computer Science Dept.  University  Dept. of Computer Science Washington University  Dept. of Computer Science University of Illinois  Dept. of Computer Science University of Illinois  at the University of Illinois at Urbana-Champaign, and at Southern University at  
Abstract: One view of computational learning theory is that of a learner acquiring the knowledge of a teacher. We introduce a formal model of learning capturing the idea that teachers may have gaps in their knowledge. In particular, we consider learning from a teacher who labels examples "+" (a positive instance of the concept being learned), "" (a negative instance of the concept being learned), and "?" (an instance with unknown classification), in such a way that knowledge of the concept class and all the positive and negative examples is not sufficient to determine the labelling of any of the examples labelled with "?". The goal of the learner is not to compensate for the ignorance of the teacher by attempting to infer "+" or "" labels for the examples labelled with "?", but is rather to learn (an approximation to) the ternary labelling presented by the teacher. Thus, the goal of the learner is still to acquire the knowledge of the teacher, but now the learner must also identify the gaps. This is the notion of learning from a consistently ignorant teacher. We present general results describing when known learning algorithms can be used to obtain algorithms that learn from a consistently ignorant teacher. We investigate the learnability of a variety of concept classes in this model, including monomials, monotone DNF formulas, Horn sentences, decision trees, DFAs, and axis-parallel boxes in Euclidean space, among others. Both learnability and non-learnability results are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [AFP92] <author> D. Angluin, M. Frazier, and L. Pitt. </author> <title> Learning conjunctions of Horn clauses. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 147-164, </pages> <year> 1992. </year>
Reference-contexts: The learnability of DNF formulas remains a centrally studied unsolved problem; thus, while l-term DNF formulas, decision trees, and Horn sentences are learnable in standard models <ref> [Ang87a, Bsh95, AFP92] </ref>, learning these types of concepts from a consistently ignorant teacher would appear to be much more difficult. We also show in Section 5 that the problem of learning blurry DFAs is intractable, given standard cryptographic assumptions. <p> Classes known to be learnable under one or both of these models include, for example, deterministic finite automata [Ang87b], read-once formulas over various bases [AHK93, BHH95, BHH92], and propositional boolean formulas representable in the following forms: k-term DNF [Ang87a, BR92], read-twice DNF [AP91, Han91, PR95], and Horn sentences <ref> [AFP92] </ref>. In contrast, Angluin and Kharitonov [AK95] have shown that, under cryptographic assumptions, read-thrice formulas, nondeterministic finite automata, and context-free grammars cannot be learned in the PAC-memb model, and that membership queries do not help in learning the general class of DNF formulas. <p> Corollary 7 For C 2 fHorn clauses, 1-DNFs containing at most O (log n) literals, Classic sentencesg, C ? is polynomially PAC-memb learnable. Proof: For C the class of Horn clauses, C " is the class of Horn sentences which is known to be PAC-memb learnable <ref> [AFP92] </ref>. For C the class of 1-DNF formulas containing at most O (log n) literals, C " is the class of O (log n)-CNF expressions, which is known to be PAC-memb learnable [Bsh95]. <p> be learnable from an omniscient teacher is necessarily learnable from a consistently ignorant teacher. (In other words, the learnability of C may not imply the learnability of C ? .) In particular, while the classes of `-term DNF formulas, decision trees, and Horn sentences are known to be PAC-memb learnable <ref> [Ang87a, Bsh95, AFP92] </ref>, we show here that learning their blurry counterparts is as hard as learning (non-blurry) DNF. Since the learnability of DNF is a widely attacked open problem in computational learning theory, we have evidence that learning blurry unrestricted versions of these classes may be hard.
Reference: [AHK93] <author> D. Angluin, L. Hellerstein, and M. Karpinski. </author> <title> Learning read-once formulas with queries. </title> <journal> J. ACM, </journal> <volume> 40 </volume> <pages> 185-210, </pages> <year> 1993. </year>
Reference-contexts: Much work has been directed towards understanding what concept classes are efficiently learnable in each of these membership query models. Classes known to be learnable under one or both of these models include, for example, deterministic finite automata [Ang87b], read-once formulas over various bases <ref> [AHK93, BHH95, BHH92] </ref>, and propositional boolean formulas representable in the following forms: k-term DNF [Ang87a, BR92], read-twice DNF [AP91, Han91, PR95], and Horn sentences [AFP92]. <p> We show C + [ is the class of unate DNF formulas and hence PAC-memb learnable by <ref> [AHK93] </ref>. To see that C + [ is the class of unate DNF formulas, note that for F + C, since there is an example x that Intersect F + labels positive (by the definition of C + ? ), x satisfies every monomial in F + .
Reference: [AHP92] <author> H. Aizenstein, L. Hellerstein, and L. Pitt. </author> <title> Read-thrice DNF is hard to learn with membership and equivalence queries. </title> <booktitle> In Proc. of the 33rd Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 523-532. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1992. </year> <title> (A revised manuscript, with additional results, is Aizenstein, Hegedus, Hellerstein, and Pitt, "Complexity Theoretic Hardness Results for Query Learning", to appear in Computational Complexity.) 10 A formula is a read-k sat-j DNF if each variable occurs at most k times and each positive example satisfies at most j terms. </title> <type> 38 </type>
Reference-contexts: Recently, techniques have been developed showing that some classes are not learnable in the exact-memb model <ref> [AHP92, PR94] </ref>. Related work on learning with incomplete information: Most of the work in both the PAC and exact models, both with and without membership queries, assumes that examples are labeled either positive or negative. In these situations the border between the positive and negative examples is well defined.
Reference: [AK94] <author> D. Angluin and M. Kri~kis. </author> <title> Learning with malicious membership queries and exceptions. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 57-66. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Thus, unlike our approach, the way that "don't know" examples are classified is unimportant. Other investigations have considered learning concept classes when membership query responses are incorrect (as opposed to "don't know"): Angluin and Kri~kis <ref> [AK94] </ref>, and Angluin [Ang94] consider learning with a bounded number of such erroneous responses, and Frazier and Pitt [FP94] consider learning when such incorrect responses occur randomly with probability at most 1 2 .
Reference: [AK95] <author> D. Angluin and M. </author> <title> Kharitonov. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50, </volume> <booktitle> 1995. (Special issue for the 23rd Annual ACM Symposium on Theory of Computing.) </booktitle>
Reference-contexts: In contrast, Angluin and Kharitonov <ref> [AK95] </ref> have shown that, under cryptographic assumptions, read-thrice formulas, nondeterministic finite automata, and context-free grammars cannot be learned in the PAC-memb model, and that membership queries do not help in learning the general class of DNF formulas. <p> In order to demonstrate this, we need the notion of a prediction preserving reduction with membership queries <ref> [AK95] </ref> (see also [PW90]). While the definition of a prediction-preserving reduction is somewhat involved, it in essence captures the idea that one can sometimes use an efficient learning algorithm for one concept class to construct an efficient learning algorithm for a different concept class. <p> For concept classes C 1 and C 2 , we use the notation L (C 1 ) L (C 2 ) to mean that PAC-memb learning C 1 reduces to PAC-memb learning C 2 , in the sense of <ref> [AK95] </ref>. We will present such reductions informally | namely by showing how an an efficient algorithm for PAC-memb learning C 2 can be used to efficiently PAC-memb learn C 1 . <p> Observation 19 Blurry DFAs are not learnable under standard cryptographic assumptions. 9 Proof: For C the class of DFAs, we show how an algorithm A for C ? can be used to learn the union of DFAs. Since learning the union of DFAs is not possible under cryptographic assumptions <ref> [AK95] </ref>, the result follows. 9 For example, assuming the intractability of inverting RSA encryption, factoring Blum integers, or determining quadratic residuosity. 31 To learn the union of DFAs, M 1 ; : : : ; M t , we run the algorithm A for C ? on the target f ? <p> It follows from this sequence of reductions that PAC-memb learning the agreement of Horn sentences when the set of positive examples is samplable is as hard as PAC-memb learning the class of DNF formulas. Finally, we strengthen this result by using the hardness result of Angluin and Kharitonov <ref> [AK95] </ref> which shows, under the assumption that one-way functions exist, that membership queries do not help in learning DNF formulas.
Reference: [AKMW95] <author> P. Auer, S. Kwek, W. Maass, and M. Warmuth. </author> <title> On-line prediction of depth two linear threshold circuits. </title> <note> Manuscript in preparation. </note>
Reference-contexts: Bshouty et al. [BGGM94, BCH94] present an algorithm to exactly learn the class of geometric concepts defined by s hyperplanes of known slopes for d constant. Auer et al. <ref> [AKMW95] </ref> present an algorithm for learning the class of depth two linear threshold circuits with a polynomial number of threshold gates and variables with the fan-in at the input gates bound by a constant.
Reference: [AL88] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [AL88, Lai88, Slo88, SV88, SS92, Kea93, KL93, GS95, RR95] </ref>. In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [Ang87a] <author> D. Angluin. </author> <title> Learning k-term DNF formulas using queries and counterexamples. </title> <type> Technical Report YALEU/DCS/RR-559, </type> <institution> Department of Computer Science, Yale University, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: The learnability of DNF formulas remains a centrally studied unsolved problem; thus, while l-term DNF formulas, decision trees, and Horn sentences are learnable in standard models <ref> [Ang87a, Bsh95, AFP92] </ref>, learning these types of concepts from a consistently ignorant teacher would appear to be much more difficult. We also show in Section 5 that the problem of learning blurry DFAs is intractable, given standard cryptographic assumptions. <p> Classes known to be learnable under one or both of these models include, for example, deterministic finite automata [Ang87b], read-once formulas over various bases [AHK93, BHH95, BHH92], and propositional boolean formulas representable in the following forms: k-term DNF <ref> [Ang87a, BR92] </ref>, read-twice DNF [AP91, Han91, PR95], and Horn sentences [AFP92]. <p> be learnable from an omniscient teacher is necessarily learnable from a consistently ignorant teacher. (In other words, the learnability of C may not imply the learnability of C ? .) In particular, while the classes of `-term DNF formulas, decision trees, and Horn sentences are known to be PAC-memb learnable <ref> [Ang87a, Bsh95, AFP92] </ref>, we show here that learning their blurry counterparts is as hard as learning (non-blurry) DNF. Since the learnability of DNF is a widely attacked open problem in computational learning theory, we have evidence that learning blurry unrestricted versions of these classes may be hard.
Reference: [Ang87b] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Inform. Comput., </journal> <volume> 75(2) </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: We also show in Section 5 that the problem of learning blurry DFAs is intractable, given standard cryptographic assumptions. Once again, while DFAs are learnable in standard models <ref> [Ang87b] </ref>, their blurry counterparts seem much harder to learn. In Section 6 we consider the extension of the model of a consistently ignorant teacher to the exact learning model using equivalence (and membership) queries, and demonstrate that analogous results hold in this more demanding learning model. <p> We refer to these models as the PAC-memb and exact-memb models. Much work has been directed towards understanding what concept classes are efficiently learnable in each of these membership query models. Classes known to be learnable under one or both of these models include, for example, deterministic finite automata <ref> [Ang87b] </ref>, read-once formulas over various bases [AHK93, BHH95, BHH92], and propositional boolean formulas representable in the following forms: k-term DNF [Ang87a, BR92], read-twice DNF [AP91, Han91, PR95], and Horn sentences [AFP92]. <p> Since both ` and k are constant, each function in C k " and C k [ is efficiently representable as an ` 0 - term DNF formula, for ` 0 constant. Moreover, since for each constant ` 0 , ` 0 -term DNF formulas are PAC-memb learnable <ref> [Ang87b, BR92] </ref>, C k " and C k [ are efficiently learnable. For C the class of decision trees, consider as an example the case when k = 2. <p> Since each function in C k " and C k is efficiently representable as a DFA, and DFAs are efficiently PAC-memb learnable <ref> [Ang87b] </ref>, C k " [ are also efficiently learnable. <p> Since the learnability of DNF is a widely attacked open problem in computational learning theory, we have evidence that learning blurry unrestricted versions of these classes may be hard. And, while DFAs are learnable from omniscient teachers <ref> [Ang87b] </ref>, we show that blurry DFAs are not learnable under widely accepted cryptographic assumptions. Recall that Theorem 11 showed that for decision trees and DFAs, the learning problem is no easier even when the set of positive examples is guaranteed to be nonempty.
Reference: [Ang88] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: A well-investigated alternative model of learning is that of exact learning from equivalence queries <ref> [Ang88] </ref>. In this model, the learner proposes as a hypothesis some h 2 C, and in response is told "yes" if h = f , or otherwise is given a counterexample x such that h (x) 6= f (x). <p> It is known that any class learnable exactly from equivalence queries can be learned in the PAC setting, via a simple transformation turning an algorithm in the former setting to one in the latter <ref> [Ang88] </ref>. The converse does not hold [Blu94]. <p> While the details of algorithm AS are beyond the scope of this brief discussion, one key aspect of the algorithm involves the simulation of an earlier algorithm of Angluin <ref> [Ang88] </ref> (call it A) that learns monotone DNF using (complete) membership queries. <p> Observe that the "lower" boundary is expressed by the function Union F , which is representable as a monotone DNF formula of size P f2F jf j. Thus, A runs the monotone DNF learning algorithm <ref> [Ang88] </ref> treating all "?" examples as "+" examples, and obtains some hypothesis h that (with probability at least 1 ffi) correctly (within *) classifies examples as does Union F . To classify an example x, A outputs "?" if h (x) = "+", otherwise A outputs "".
Reference: [Ang90] <author> D. Angluin. </author> <title> Negative results for equivalence queries. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 121-150, </pages> <year> 1990. </year>
Reference-contexts: Evidence suggests that only relatively simple types of concepts can be learned passively in this way <ref> [Ang90, KV94, PW90] </ref>. Consequently, researchers have considered augmenting 1 If the demand of polynomial-time computation below is replaced with expected polynomial-time computation, then the learning algorithm need not be given the parameter s, but could "guess" it instead [HKLW91]. 6 this learning protocol by allowing the learner to perform experiments.
Reference: [Ang94] <author> D. Angluin. </author> <title> Exact learning of -DNF formulas with malicious membership queries. </title> <type> Technical Report YALEU/DCS/TR-1020, </type> <institution> Yale University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Thus, unlike our approach, the way that "don't know" examples are classified is unimportant. Other investigations have considered learning concept classes when membership query responses are incorrect (as opposed to "don't know"): Angluin and Kri~kis [AK94], and Angluin <ref> [Ang94] </ref> consider learning with a bounded number of such erroneous responses, and Frazier and Pitt [FP94] consider learning when such incorrect responses occur randomly with probability at most 1 2 . In other related work, Kearns and Schapire [KS94] generalized the PAC setting to non-binary values using Haussler's framework [Hau89].
Reference: [AP91] <author> H. Aizenstein and L. Pitt. </author> <title> Exact learning of read-twice DNF formulas. </title> <booktitle> In Proc. 32th Annu. IEEE Sympos. Found. Comput. Sci., </booktitle> <pages> pages 170-179. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Classes known to be learnable under one or both of these models include, for example, deterministic finite automata [Ang87b], read-once formulas over various bases [AHK93, BHH95, BHH92], and propositional boolean formulas representable in the following forms: k-term DNF [Ang87a, BR92], read-twice DNF <ref> [AP91, Han91, PR95] </ref>, and Horn sentences [AFP92]. In contrast, Angluin and Kharitonov [AK95] have shown that, under cryptographic assumptions, read-thrice formulas, nondeterministic finite automata, and context-free grammars cannot be learned in the PAC-memb model, and that membership queries do not help in learning the general class of DNF formulas.
Reference: [AS94] <author> D. Angluin and D. </author> <title> Slonim. Randomly fallible teachers: Learning monotone DNF with an incomplete membership oracle. </title> <journal> Machine Learning, </journal> <volume> 14(7) </volume> <pages> 7-26, </pages> <year> 1994. </year> <title> Special issue for the Fourth Ann. </title> <booktitle> Workshop on Comput. Learning Theory (Santa Cruz, </booktitle> <address> CA., </address> <month> July </month> <year> 1991). </year>
Reference-contexts: There has also been some work considering learning from noisy membership queries [GKS93, Sak91]. Angluin and Slonim <ref> [AS94] </ref> introduced a model of incomplete membership queries in which each membership query is answered "don't know" with a given probability. Furthermore, this information is persistent|repeatedly making a query that was answered "don't know" always results in a "don't know" answer. <p> Indeed, Angluin and Slonim note that their algorithm for learning monotone DNF with an incomplete membership oracle can be used to learn monotone DNF with random (false negative) one-sided errors. Sloan and Turan [ST94] consider a variant of <ref> [AS94] </ref> in which a limited membership oracle labels a polynomial number of examples "don't know". The learner's performance is measured only on the examples for which the limited membership oracle knows the answer. Thus, unlike our approach, the way that "don't know" examples are classified is unimportant.
Reference: [AV79] <author> D. Angluin and L. G. Valiant. </author> <title> Fast probabilistic algorithms for hamiltonian circuits and matchings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18(2) </volume> <pages> 155-193, </pages> <year> 1979. </year>
Reference-contexts: By applying a version of Chernoff bounds presented in <ref> [AV79] </ref>, we know: LE (p; m; pm=2) e mp=8 (2) It is easily verified that m BP Q ( * 2 d+1 ; ffi 32 d ) pm LBA 2 =2.
Reference: [BCH94] <author> N. Bshouty, Z. Chen, and S. Homer. </author> <title> On learning discretized geometric concepts. </title> <booktitle> In Proc. of the 35th Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 54-63, </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos California, </address> <month> November </month> <year> 1994. </year> <month> 39 </month>
Reference-contexts: Recently, there have been several independent results (using very different techniques) to exactly learn this class using only equivalence queries 2 with time and sample complexity polynomial in d, s, and log n for either d constant <ref> [CH94a, BGGM94, BCH94, MW95] </ref>. or s constant [MW95]. One noteworthy difference is that the algorithms of Maass and Warmuth [MW95] have a sample complexity that is polynomial in log n, s, and d. <p> Bshouty et al. <ref> [BGGM94, BCH94] </ref> present an algorithm to exactly learn the class of geometric concepts defined by s hyperplanes of known slopes for d constant.
Reference: [BEHW89] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> J. ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: 0 &lt; p (x) &lt; 1. (If a written numeral is sometimes identified as "4" and sometimes as "9", the learner just wants to 8 know this|it does not need to determine what percentage of the population calls the numeral each value.) Related work on learning boxes: Blumer et al. <ref> [BEHW89] </ref> present an algorithm to PAC-learn an s-fold union of boxes in E d by drawing a sufficiently large sample of size m = poly * ; log 1 and then choosing a greedy covering from the boxes consistent with the sample. <p> Hence, jIntersect F k j = O (d) and is efficiently learnable by results of <ref> [BEHW89] </ref>. <p> To aid in learning the agreement of boxes, we first learn the intersection region (which is itself a box). We can approximate the intersection box by treating all "?" examples in the sample as negative examples and running a known algorithm to learn one d-dimensional box <ref> [BEHW89] </ref>. Learning the intersection box allows us to distinguish between positive and non-positive examples. <p> The VC-dimension 8 of BPQ [ (s) grows polynomially with s and d (namely, it is at most 2ds log 3s). It then follows from Theorem 2.1 of Blumer et al. <ref> [BEHW89] </ref> that if LearnBPQ chooses at least m BP Q = max 4 ffi ; * log 13 o random examples, then with probability at least 1 ffi, it will output a hypothesis h with error at most *. 8 The VC-dimension is a combinatorial parameter of a concept class that <p> ; * log 13 o random examples, then with probability at least 1 ffi, it will output a hypothesis h with error at most *. 8 The VC-dimension is a combinatorial parameter of a concept class that directly relates to the number of examples necessary (and sufficient) for sufficient generalization <ref> [BEHW89] </ref>. 25 Proof of Part 1: We first show that LearnBPQ produces a hypothesis that is consistent with the sample S. <p> This completes the proof of Part 1. Proof of Part 2: Note that the VC-dimension of BPQ is at most d (this is easily shown), and by Lemma 3.2.3 of Blumer et al. <ref> [BEHW89] </ref>, the VC-dimension of BPQ [ (s) is at most 2ds log (3s). This completes the proof of Part 2 and hence of the theorem. <p> Proof of Part 2: Observe that the intersection region is a d-dimensional axis-parallel box and the VC-dimension of a d-dimensional axis-parallel box is 2d. Hence, by a direct application of Theorem 2.1 in Blumer et al. <ref> [BEHW89] </ref>, the probability that the procedure OneBox (T ) (in step 5) produces a hypothesis that has error more than * 2 is at most ffi 3 , provided that jT j max 8 ffi ; 16d * : But jT j = jS 2 j = m LBA 2 ,
Reference: [BGGM94] <author> N. Bshouty, P. Goldberg, S. Goldman, and D. Mathias. </author> <title> Exact learning of discretized geometric concepts. </title> <type> Technical Report WUCS-94-19, Technical Report, </type> <institution> Washington University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Recently, there have been several independent results (using very different techniques) to exactly learn this class using only equivalence queries 2 with time and sample complexity polynomial in d, s, and log n for either d constant <ref> [CH94a, BGGM94, BCH94, MW95] </ref>. or s constant [MW95]. One noteworthy difference is that the algorithms of Maass and Warmuth [MW95] have a sample complexity that is polynomial in log n, s, and d. <p> If the learning algorithm can also use membership queries then there is single algorithm to exactly learn this class in polynomial time when either s or d are constant <ref> [GGM94, BGGM94] </ref>. (See also [CM92, Che93, GGM94] for earlier work.) There has also been recent work that addresses learning more complex geometric concepts. Bshouty et al. [BGGM94, BCH94] present an algorithm to exactly learn the class of geometric concepts defined by s hyperplanes of known slopes for d constant. <p> Bshouty et al. <ref> [BGGM94, BCH94] </ref> present an algorithm to exactly learn the class of geometric concepts defined by s hyperplanes of known slopes for d constant.
Reference: [BGM95] <author> N. Bshouty, S. Goldman, and D. Mathias. </author> <title> Noise-tolerant parallel learning of geometric concepts. </title> <booktitle> In Proc. 8th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 345-352. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1995. </year>
Reference-contexts: Pitt [KP95] give a algorithm to learn in the PAC-memb model the intersection of s halfspaces in d dimensions that has time and sample complexity polynomial in both s and d given there is a lower bound on the minimum distance between any positive and any negative point. (See also <ref> [BGM95] </ref> for earlier work.) There has also been a lot of work on exactly learning unions of s boxes in the discretized space f1; : : :; ng d .
Reference: [BGMST95] <author> N. Bshouty, S. Goldman, D. Mathias, S. Suri, and H. Tamaki. </author> <title> Noise-tolerant distribution-free learning of general geometric concepts. </title> <note> Manuscript in preparation. </note>
Reference-contexts: Both the time and sample complexity of their algorithm depend polynomially on (2d) s ; 1 * , and log 1 ffi . Recently, Bshouty et al. <ref> [BGMST95] </ref> present a noise-tolerant PAC-algorithm to learn any geometric concept defined by a boolean combination of s halfspaces for d constant, and Kwek and Pitt [KP95] give a algorithm to learn in the PAC-memb model the intersection of s halfspaces in d dimensions that has time and sample complexity polynomial in
Reference: [BHH92] <author> N. Bshouty, T. Hancock, and L. Hellerstein. </author> <title> Learning Boolean read-once formulas with arbitrary symmetric and constant fan-in gates. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 1-15. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Much work has been directed towards understanding what concept classes are efficiently learnable in each of these membership query models. Classes known to be learnable under one or both of these models include, for example, deterministic finite automata [Ang87b], read-once formulas over various bases <ref> [AHK93, BHH95, BHH92] </ref>, and propositional boolean formulas representable in the following forms: k-term DNF [Ang87a, BR92], read-twice DNF [AP91, Han91, PR95], and Horn sentences [AFP92].
Reference: [BHH95] <author> N. Bshouty, T. Hancock, and L. Hellerstein. </author> <title> Learning arithmetic read-once formulas. </title> <journal> SIAM Journal of Computing, </journal> <volume> 24(4) </volume> <pages> 706-735, </pages> <year> 1995. </year>
Reference-contexts: Much work has been directed towards understanding what concept classes are efficiently learnable in each of these membership query models. Classes known to be learnable under one or both of these models include, for example, deterministic finite automata [Ang87b], read-once formulas over various bases <ref> [AHK93, BHH95, BHH92] </ref>, and propositional boolean formulas representable in the following forms: k-term DNF [Ang87a, BR92], read-twice DNF [AP91, Han91, PR95], and Horn sentences [AFP92].
Reference: [Blu94] <author> A. Blum. </author> <title> Separating distribution-free and mistake-bound learning models over the Boolean domain. </title> <journal> SIAM J. Comput., </journal> <volume> 23(5) </volume> <pages> 990-1000, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: It is known that any class learnable exactly from equivalence queries can be learned in the PAC setting, via a simple transformation turning an algorithm in the former setting to one in the latter [Ang88]. The converse does not hold <ref> [Blu94] </ref>.
Reference: [BR92] <author> A. Blum and S. Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proc. of the 24th Symposium on Theory of Computing, </booktitle> <pages> pages 382-389. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Classes known to be learnable under one or both of these models include, for example, deterministic finite automata [Ang87b], read-once formulas over various bases [AHK93, BHH95, BHH92], and propositional boolean formulas representable in the following forms: k-term DNF <ref> [Ang87a, BR92] </ref>, read-twice DNF [AP91, Han91, PR95], and Horn sentences [AFP92]. <p> Since both ` and k are constant, each function in C k " and C k [ is efficiently representable as an ` 0 - term DNF formula, for ` 0 constant. Moreover, since for each constant ` 0 , ` 0 -term DNF formulas are PAC-memb learnable <ref> [Ang87b, BR92] </ref>, C k " and C k [ are efficiently learnable. For C the class of decision trees, consider as an example the case when k = 2.
Reference: [Bsh95] <author> N. Bshouty. </author> <title> Exact learning Boolean functions via the monotone theory. </title> <journal> Inform. Com-put. </journal> <volume> 123(1) </volume> <pages> 146-153, </pages> <month> 15 November </month> <year> 1995. </year>
Reference-contexts: The learnability of DNF formulas remains a centrally studied unsolved problem; thus, while l-term DNF formulas, decision trees, and Horn sentences are learnable in standard models <ref> [Ang87a, Bsh95, AFP92] </ref>, learning these types of concepts from a consistently ignorant teacher would appear to be much more difficult. We also show in Section 5 that the problem of learning blurry DFAs is intractable, given standard cryptographic assumptions. <p> For C the class of 1-DNF formulas containing at most O (log n) literals, C " is the class of O (log n)-CNF expressions, which is known to be PAC-memb learnable <ref> [Bsh95] </ref>. In both cases, C [ is the class of 1-DNF expressions, which is known to be PAC learnable [Val84]. Hence, for C the class of Horn clauses and 1-DNF formulas with at most O (log n) literals, by Theorem 6, C ? is PAC-memb learnable. <p> Since each function in C k " and C k efficiently representable as a decision tree, and the class of decision trees is efficiently PAC-memb learnable <ref> [Bsh95] </ref>, C k " and C k [ are also efficiently learnable. <p> be learnable from an omniscient teacher is necessarily learnable from a consistently ignorant teacher. (In other words, the learnability of C may not imply the learnability of C ? .) In particular, while the classes of `-term DNF formulas, decision trees, and Horn sentences are known to be PAC-memb learnable <ref> [Ang87a, Bsh95, AFP92] </ref>, we show here that learning their blurry counterparts is as hard as learning (non-blurry) DNF. Since the learnability of DNF is a widely attacked open problem in computational learning theory, we have evidence that learning blurry unrestricted versions of these classes may be hard.
Reference: [CH94a] <author> Z. Chen and S. Homer. </author> <title> The bounded injury priority method and the learnability of unions of rectangles. </title> <type> Unpublished, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: Recently, there have been several independent results (using very different techniques) to exactly learn this class using only equivalence queries 2 with time and sample complexity polynomial in d, s, and log n for either d constant <ref> [CH94a, BGGM94, BCH94, MW95] </ref>. or s constant [MW95]. One noteworthy difference is that the algorithms of Maass and Warmuth [MW95] have a sample complexity that is polynomial in log n, s, and d.
Reference: [CH94b] <author> W. Cohen and H. Hirsh. </author> <title> Learning the CLASSIC description logic: Theoretical and experimental results. </title> <booktitle> In Principles of Knowledge Representation and Reasoning: Proceedings of the Fourth International Conference (KR94). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: The Classic description logic is a first-order logic used for representing objects and their relationships. A description of Classic is beyond the scope of this paper; we note only that positive results for the learnability of Classic sentences have recently been given <ref> [CH94c, CH94b, FP94] </ref>. It is helpful to view the example space X n as a lattice with componentwise "or" and "and" as the lattice operators. The top element is the vector f1g n and the bottom element is the vector f0g n .
Reference: [CH94c] <author> W. Cohen and H. Hirsh. </author> <title> Learnability of description logics with equality constraints. </title> <journal> Machine Learning, </journal> 17(2/3):169-199, November/December 1994. Special issue for the Fifth Ann. Workshop on Comput. Learning Theory (Pittsburgh, PA., July 1992). <volume> 40 </volume>
Reference-contexts: The Classic description logic is a first-order logic used for representing objects and their relationships. A description of Classic is beyond the scope of this paper; we note only that positive results for the learnability of Classic sentences have recently been given <ref> [CH94c, CH94b, FP94] </ref>. It is helpful to view the example space X n as a lattice with componentwise "or" and "and" as the lattice operators. The top element is the vector f1g n and the bottom element is the vector f0g n .
Reference: [Che93] <author> Z. Chen. </author> <title> Learning unions of two rectangles in the plane with equivalence queries. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 243-252. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: If the learning algorithm can also use membership queries then there is single algorithm to exactly learn this class in polynomial time when either s or d are constant [GGM94, BGGM94]. (See also <ref> [CM92, Che93, GGM94] </ref> for earlier work.) There has also been recent work that addresses learning more complex geometric concepts. Bshouty et al. [BGGM94, BCH94] present an algorithm to exactly learn the class of geometric concepts defined by s hyperplanes of known slopes for d constant.
Reference: [CM92] <author> Z. Chen and W. Maass. </author> <title> On-line learning of rectangles. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 16-28. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: If the learning algorithm can also use membership queries then there is single algorithm to exactly learn this class in polynomial time when either s or d are constant [GGM94, BGGM94]. (See also <ref> [CM92, Che93, GGM94] </ref> for earlier work.) There has also been recent work that addresses learning more complex geometric concepts. Bshouty et al. [BGGM94, BCH94] present an algorithm to exactly learn the class of geometric concepts defined by s hyperplanes of known slopes for d constant. <p> As we saw in the analysis of LearnBoxesAgreement, in each quadrant we have at most s origin-incident boxes. We can use the algorithm of Chen and Maass <ref> [CM92] </ref> to learn C S and the algorithm described above to learn C G .
Reference: [FP94] <author> M. Frazier and L. Pitt. </author> <title> CLASSIC learning. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 23-34. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year> <note> To appear, Machine Learning. </note>
Reference-contexts: The Classic description logic is a first-order logic used for representing objects and their relationships. A description of Classic is beyond the scope of this paper; we note only that positive results for the learnability of Classic sentences have recently been given <ref> [CH94c, CH94b, FP94] </ref>. It is helpful to view the example space X n as a lattice with componentwise "or" and "and" as the lattice operators. The top element is the vector f1g n and the bottom element is the vector f0g n . <p> Other investigations have considered learning concept classes when membership query responses are incorrect (as opposed to "don't know"): Angluin and Kri~kis [AK94], and Angluin [Ang94] consider learning with a bounded number of such erroneous responses, and Frazier and Pitt <ref> [FP94] </ref> consider learning when such incorrect responses occur randomly with probability at most 1 2 . In other related work, Kearns and Schapire [KS94] generalized the PAC setting to non-binary values using Haussler's framework [Hau89]. <p> Hence, for C the class of Horn clauses and 1-DNF formulas with at most O (log n) literals, by Theorem 6, C ? is PAC-memb learnable. The class of Classic sentences is known to be polynomially PAC-memb learnable <ref> [FP94] </ref>. Further, since the syntax of Classic admits an "AND" construct, the intersection of any two Classic sentences is itself a Classic sentence of size that is the sum of the sizes of the sentences being intersected. It follows that intersections of Classic sentences are polynomially PAC-memb learnable. <p> It follows that intersections of Classic sentences are polynomially PAC-memb learnable. There are different possible semantics for the "union" of Classic sentences; in a recent extension of <ref> [FP94] </ref>, it is shown that a "weak union" of Classic sentences are PAC-memb learnable, and that this is sufficient to show that agreements of Classic sentences are learnable, hence Classic is learnable from a consistently ignorant teacher. 5 The corollary above also applies to the corresponding dual class, i.e., when C <p> Note that f ? evaluates to "?" on x iff f (x) = 1, and 5 The result that a weak union of Classic sentences is learnable, and that Classic is learnable from a consistently ignorant teacher, appear as Theorem 17 and Corollary 18, respectively, in the full version of <ref> [FP94] </ref>. 18 evaluates to 0 on x iff f (x) = 0. Thus, any algorithm that learns the agreement of monomials can be used to learn (boolean-valued) DNF by simply interpreting all positive examples of the DNF algorithm as "?" examples of the agreement of monomials algorithm.
Reference: [GGM94] <author> P. Goldberg, S. Goldman, and D. Mathais. </author> <title> Learning unions of boxes with membership and equivalence queries. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 198-207. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: If the learning algorithm can also use membership queries then there is single algorithm to exactly learn this class in polynomial time when either s or d are constant <ref> [GGM94, BGGM94] </ref>. (See also [CM92, Che93, GGM94] for earlier work.) There has also been recent work that addresses learning more complex geometric concepts. Bshouty et al. [BGGM94, BCH94] present an algorithm to exactly learn the class of geometric concepts defined by s hyperplanes of known slopes for d constant. <p> If the learning algorithm can also use membership queries then there is single algorithm to exactly learn this class in polynomial time when either s or d are constant [GGM94, BGGM94]. (See also <ref> [CM92, Che93, GGM94] </ref> for earlier work.) There has also been recent work that addresses learning more complex geometric concepts. Bshouty et al. [BGGM94, BCH94] present an algorithm to exactly learn the class of geometric concepts defined by s hyperplanes of known slopes for d constant.
Reference: [GKS93] <author> S. Goldman, M. Kearns, and R. Schapire. </author> <title> Exact identification of circuits using fixed points of amplification functions. </title> <journal> SIAM J. Comput., </journal> <volume> 22(4) </volume> <pages> 705-726, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example. There has also been some work considering learning from noisy membership queries <ref> [GKS93, Sak91] </ref>. Angluin and Slonim [AS94] introduced a model of incomplete membership queries in which each membership query is answered "don't know" with a given probability. Furthermore, this information is persistent|repeatedly making a query that was answered "don't know" always results in a "don't know" answer.
Reference: [GNPS91] <author> C. Gunter, T. Ngair, P. Panangaden, and D. Subramanian. </author> <title> The common order-theoretic structure of version spaces and atms's. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 500-505, </pages> <address> Anaheim, CA, July 1991. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: Note also that not every version space can be represented by the boundary sets S and G <ref> [GNPS91, Hir91, Mit78] </ref>. 14 induce a ternary function, which we call VS [S;G] defined as follows: VS [S;G] (x) = &gt; &gt; &gt; &lt; + if s (x) = "+" for all s 2 S; ? otherwise: For a concept class C, it can be shown that the agreement of a
Reference: [GS95] <author> S. Goldman and R. Sloan. </author> <title> Can PAC learning algorithms tolerate random noise. </title> <journal> Algo-rithmica, </journal> <volume> 14(1), </volume> <year> 1995. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [AL88, Lai88, Slo88, SV88, SS92, Kea93, KL93, GS95, RR95] </ref>. In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [Han91] <author> T. Hancock. </author> <title> Learning 2-DNF formulas and k decision trees. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 199-209, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Classes known to be learnable under one or both of these models include, for example, deterministic finite automata [Ang87b], read-once formulas over various bases [AHK93, BHH95, BHH92], and propositional boolean formulas representable in the following forms: k-term DNF [Ang87a, BR92], read-twice DNF <ref> [AP91, Han91, PR95] </ref>, and Horn sentences [AFP92]. In contrast, Angluin and Kharitonov [AK95] have shown that, under cryptographic assumptions, read-thrice formulas, nondeterministic finite automata, and context-free grammars cannot be learned in the PAC-memb model, and that membership queries do not help in learning the general class of DNF formulas.
Reference: [Hau89] <author> D. Haussler. </author> <title> Generalizing the PAC model for neural net and other learning applications. </title> <type> Technical Report UCSC-CRL-89-30, </type> <institution> University of California Santa Cruz, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: In other related work, Kearns and Schapire [KS94] generalized the PAC setting to non-binary values using Haussler's framework <ref> [Hau89] </ref>. They define a p-concept in which each example x 2 X has some probability p (x) of being classified as positive. In their model, the goal of the learner is to make optimal predictions, or more commonly, to accurately predict p (x) for all x 2 X .
Reference: [Hir91] <author> H. Hirsh. </author> <title> Theoretical underpinnings of version spaces. </title> <booktitle> In Proceedings of the Twelfth Joint International Conference on Artificial Intelligence, </booktitle> <pages> pages 665-670, </pages> <address> Sydney, Aus-tralia, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Note also that not every version space can be represented by the boundary sets S and G <ref> [GNPS91, Hir91, Mit78] </ref>. 14 induce a ternary function, which we call VS [S;G] defined as follows: VS [S;G] (x) = &gt; &gt; &gt; &lt; + if s (x) = "+" for all s 2 S; ? otherwise: For a concept class C, it can be shown that the agreement of a
Reference: [HKLW91] <author> D. Haussler, M. Kearns, N. Littlestone, and M. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Inform. Comput., </journal> <volume> 95(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year> <month> 41 </month>
Reference-contexts: On the other hand, an improper (or representation-independent) learning algorithm may output any polynomial-time algorithm as a hypothesis. The less constrained model of improper learning is equivalent to polynomial "PAC-predictability" <ref> [HLW94, HKLW91] </ref> Throughout this paper, we are concerned mostly with improper learning, and the default interpretation of "learnable" should be that of improper learning. A well-investigated alternative model of learning is that of exact learning from equivalence queries [Ang88]. <p> Consequently, researchers have considered augmenting 1 If the demand of polynomial-time computation below is replaced with expected polynomial-time computation, then the learning algorithm need not be given the parameter s, but could "guess" it instead <ref> [HKLW91] </ref>. 6 this learning protocol by allowing the learner to perform experiments.
Reference: [HLW94] <author> D. Haussler, N. Littlestone, and M. Warmuth. </author> <title> Predicting f0,1g functions on randomly drawn points. </title> <journal> Inform. Comput., </journal> <volume> 115(2) </volume> <pages> 284-293, </pages> <year> 1994. </year>
Reference-contexts: On the other hand, an improper (or representation-independent) learning algorithm may output any polynomial-time algorithm as a hypothesis. The less constrained model of improper learning is equivalent to polynomial "PAC-predictability" <ref> [HLW94, HKLW91] </ref> Throughout this paper, we are concerned mostly with improper learning, and the default interpretation of "learnable" should be that of improper learning. A well-investigated alternative model of learning is that of exact learning from equivalence queries [Ang88].
Reference: [HU79] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachussetts, </address> <year> 1979. </year>
Reference-contexts: We assume basic familiarity with the definitions of deterministic finite automata (DFAs) <ref> [HU79] </ref>. The Classic description logic is a first-order logic used for representing objects and their relationships. A description of Classic is beyond the scope of this paper; we note only that positive results for the learnability of Classic sentences have recently been given [CH94c, CH94b, FP94]. <p> Since each function in C k " and C k efficiently representable as a decision tree, and the class of decision trees is efficiently PAC-memb learnable [Bsh95], C k " and C k [ are also efficiently learnable. Finally, for C the class of DFAs, standard arguments <ref> [HU79] </ref> show that the intersection and union of a constant k number of DFAs is representable as a DFA of size the product of the sizes of the DFAs in the intersection or union (but exponential in k).
Reference: [Jac94] <author> J. Jackson. </author> <title> Learning DNF under the uniform distribution. </title> <booktitle> In Proc. of the 35th Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 42-53, </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamitos California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In particular, we construct an algorithm A to learn C ? given an algorithm A + that learns C + ? . Suppose that f ? is represented 6 However, DNF formulas are PAC-memb learnable with respect to the uniform distribution <ref> [Jac94] </ref>. 20 as an agreement of concepts f 1 ; : : : ; f t in C over variables v 1 ; : : :; v n .
Reference: [Kea93] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proc. 25th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 392-401, </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [AL88, Lai88, Slo88, SV88, SS92, Kea93, KL93, GS95, RR95] </ref>. In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [KL93] <author> M. Kearns and M. Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM J. Comput., </journal> <volume> 22 </volume> <pages> 807-837, </pages> <year> 1993. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [AL88, Lai88, Slo88, SV88, SS92, Kea93, KL93, GS95, RR95] </ref>. In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [KP95] <author> S. Kwek and L. Pitt. </author> <title> Polynomial-time learning of intersections of halfspaces, </title> <note> 1995. Manuscript in preparation. </note>
Reference-contexts: Recently, Bshouty et al. [BGMST95] present a noise-tolerant PAC-algorithm to learn any geometric concept defined by a boolean combination of s halfspaces for d constant, and Kwek and Pitt <ref> [KP95] </ref> give a algorithm to learn in the PAC-memb model the intersection of s halfspaces in d dimensions that has time and sample complexity polynomial in both s and d given there is a lower bound on the minimum distance between any positive and any negative point. (See also [BGM95] for
Reference: [KS94] <author> M. Kearns and R. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <journal> J. of Comput. Syst. Sci., </journal> <volume> 48(3) </volume> <pages> 464-497, </pages> <year> 1994. </year>
Reference-contexts: In other related work, Kearns and Schapire <ref> [KS94] </ref> generalized the PAC setting to non-binary values using Haussler's framework [Hau89]. They define a p-concept in which each example x 2 X has some probability p (x) of being classified as positive.
Reference: [KV94] <author> M. Kearns and L. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> J. ACM, </journal> <volume> 41(1) </volume> <pages> 67-95, </pages> <year> 1994. </year>
Reference-contexts: Evidence suggests that only relatively simple types of concepts can be learned passively in this way <ref> [Ang90, KV94, PW90] </ref>. Consequently, researchers have considered augmenting 1 If the demand of polynomial-time computation below is replaced with expected polynomial-time computation, then the learning algorithm need not be given the parameter s, but could "guess" it instead [HKLW91]. 6 this learning protocol by allowing the learner to perform experiments.
Reference: [Lai88] <author> P. Laird. </author> <title> Learning from good and bad data. </title> <booktitle> In Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [AL88, Lai88, Slo88, SV88, SS92, Kea93, KL93, GS95, RR95] </ref>. In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [LW94] <author> P. Long and M. Warmuth. </author> <title> Composite geometric concepts and polynomial predictability. </title> <journal> Inform. Comput., </journal> <volume> 113(2) </volume> <pages> 230-252, </pages> <year> 1994. </year>
Reference-contexts: The number of such boxes considered is shown to be at most em 2d , so, for d constant, this algorithm runs in polynomial time. Long and Warmuth <ref> [LW94] </ref> present an algorithm to PAC-learn this same class by again drawing a sufficiently large sample and constructing a hypothesis consistent with the sample that consists of at most s (2d) s boxes. <p> Observe that by results of <ref> [LW94] </ref>, C k [ is PAC learnable in time polynomial in d, 1 * , and 1 ffi .
Reference: [Mit78] <author> T. Mitchell. </author> <title> Version Spaces: An Approach to Concept Learning. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1978. </year>
Reference-contexts: Note also that not every version space can be represented by the boundary sets S and G <ref> [GNPS91, Hir91, Mit78] </ref>. 14 induce a ternary function, which we call VS [S;G] defined as follows: VS [S;G] (x) = &gt; &gt; &gt; &lt; + if s (x) = "+" for all s 2 S; ? otherwise: For a concept class C, it can be shown that the agreement of a
Reference: [Mit82] <author> T. Mitchell. </author> <title> Generalization as search. Art. </title> <journal> Int., </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: The notion of an agreement of base concepts has independent interest, as it models a type of unanimous vote of independent agents. Finally, there is an interesting relationship between Mitchell's definition of version spaces <ref> [Mit82] </ref> and agreements. Given a concept class C of boolean-valued functions and a set of examples M , the version space V is the set of concepts in C consistent with M .
Reference: [MW95] <author> W. Maass and M. Warmuth. </author> <title> Efficient learning with virtual threshold gates. </title> <booktitle> In Proc. XII International Conf. on Machine Learning, </booktitle> <pages> pages 378-386, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: Recently, there have been several independent results (using very different techniques) to exactly learn this class using only equivalence queries 2 with time and sample complexity polynomial in d, s, and log n for either d constant <ref> [CH94a, BGGM94, BCH94, MW95] </ref>. or s constant [MW95]. One noteworthy difference is that the algorithms of Maass and Warmuth [MW95] have a sample complexity that is polynomial in log n, s, and d. <p> Recently, there have been several independent results (using very different techniques) to exactly learn this class using only equivalence queries 2 with time and sample complexity polynomial in d, s, and log n for either d constant [CH94a, BGGM94, BCH94, MW95]. or s constant <ref> [MW95] </ref>. One noteworthy difference is that the algorithms of Maass and Warmuth [MW95] have a sample complexity that is polynomial in log n, s, and d. <p> results (using very different techniques) to exactly learn this class using only equivalence queries 2 with time and sample complexity polynomial in d, s, and log n for either d constant [CH94a, BGGM94, BCH94, MW95]. or s constant <ref> [MW95] </ref>. One noteworthy difference is that the algorithms of Maass and Warmuth [MW95] have a sample complexity that is polynomial in log n, s, and d.
Reference: [PR94] <author> K. Pillaipakkamnatt and V. Raghavan. </author> <title> On the limits of proper learnability of subclasses of DNF formulas. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 118-129. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year> <note> To appear, Machine Learning. 42 </note>
Reference-contexts: Recently, techniques have been developed showing that some classes are not learnable in the exact-memb model <ref> [AHP92, PR94] </ref>. Related work on learning with incomplete information: Most of the work in both the PAC and exact models, both with and without membership queries, assumes that examples are labeled either positive or negative. In these situations the border between the positive and negative examples is well defined.
Reference: [PR95] <author> K. Pillaipakkamnatt and V. Raghavan. </author> <title> Read-twice DNF formulas are properly learn-able. </title> <journal> Inform. Comput. </journal> <volume> 122(2) </volume> <pages> 236-267, </pages> <month> November, </month> <year> 1995. </year>
Reference-contexts: Classes known to be learnable under one or both of these models include, for example, deterministic finite automata [Ang87b], read-once formulas over various bases [AHK93, BHH95, BHH92], and propositional boolean formulas representable in the following forms: k-term DNF [Ang87a, BR92], read-twice DNF <ref> [AP91, Han91, PR95] </ref>, and Horn sentences [AFP92]. In contrast, Angluin and Kharitonov [AK95] have shown that, under cryptographic assumptions, read-thrice formulas, nondeterministic finite automata, and context-free grammars cannot be learned in the PAC-memb model, and that membership queries do not help in learning the general class of DNF formulas.
Reference: [PW90] <author> L. Pitt and M. Warmuth. </author> <title> Prediction preserving reducibility. </title> <journal> J. of Comput. Syst. Sci., </journal> <volume> 41(3) </volume> <pages> 430-467, </pages> <month> December </month> <year> 1990. </year> <booktitle> Special issue for the Third Annual Conference of Structure in Complexity Theory (Washington, </booktitle> <address> DC., </address> <month> June </month> <year> 1988). </year>
Reference-contexts: Evidence suggests that only relatively simple types of concepts can be learned passively in this way <ref> [Ang90, KV94, PW90] </ref>. Consequently, researchers have considered augmenting 1 If the demand of polynomial-time computation below is replaced with expected polynomial-time computation, then the learning algorithm need not be given the parameter s, but could "guess" it instead [HKLW91]. 6 this learning protocol by allowing the learner to perform experiments. <p> In order to demonstrate this, we need the notion of a prediction preserving reduction with membership queries [AK95] (see also <ref> [PW90] </ref>). While the definition of a prediction-preserving reduction is somewhat involved, it in essence captures the idea that one can sometimes use an efficient learning algorithm for one concept class to construct an efficient learning algorithm for a different concept class.
Reference: [RR95] <author> D. Ron and R. Rubinfeld. </author> <title> Learning fallible deterministic finite automata. </title> <journal> Machine Learning, </journal> 18(2/3):149-186, 1995. Special issue for the Sixth Ann. Workshop on Com-put. Learning Theory (Santa Cruz, CA., July 1993). 
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [AL88, Lai88, Slo88, SV88, SS92, Kea93, KL93, GS95, RR95] </ref>. In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [Sak91] <author> Y. Sakakibara. </author> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Inform. Proc. Lett., </journal> <volume> 37(5) </volume> <pages> 279-284, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example. There has also been some work considering learning from noisy membership queries <ref> [GKS93, Sak91] </ref>. Angluin and Slonim [AS94] introduced a model of incomplete membership queries in which each membership query is answered "don't know" with a given probability. Furthermore, this information is persistent|repeatedly making a query that was answered "don't know" always results in a "don't know" answer.
Reference: [SS92] <author> Y. Sakakibara and R. Siromoney. </author> <title> A noise model on learning sets of strings. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 295-302, </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [AL88, Lai88, Slo88, SV88, SS92, Kea93, KL93, GS95, RR95] </ref>. In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [Slo88] <author> R. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 91-96, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [AL88, Lai88, Slo88, SV88, SS92, Kea93, KL93, GS95, RR95] </ref>. In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [ST94] <author> R. H. Sloan and G. Turan. </author> <title> Learning with queries but incomplete information. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 237-245. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Indeed, Angluin and Slonim note that their algorithm for learning monotone DNF with an incomplete membership oracle can be used to learn monotone DNF with random (false negative) one-sided errors. Sloan and Turan <ref> [ST94] </ref> consider a variant of [AS94] in which a limited membership oracle labels a polynomial number of examples "don't know". The learner's performance is measured only on the examples for which the limited membership oracle knows the answer.
Reference: [SV88] <author> G. Shackelford and D. Volper. </author> <title> Learning k-DNF with Noise in the Attributes. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 97-103. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: In these situations the border between the positive and negative examples is well defined. There has been work addressing the issue of mislabeled training examples <ref> [AL88, Lai88, Slo88, SV88, SS92, Kea93, KL93, GS95, RR95] </ref>. In these situations, the border between the positive and negative examples may appear blurry to the learner, but this is just the result of the noise process that has been applied to the properly labeled example.
Reference: [Val84] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Commun. ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: The elements are partially ordered by , where x y if and only if for all i, x i y i . If x y, we say, x is below y or y is above x. Standard learning models: In Valiant's distribution-free, or probably approximately correct (PAC) learning model <ref> [Val84] </ref>, the learner's goal is to infer how an unknown target function f , chosen from some known concept class C, classifies all examples from the domain X . Often C is decomposed into subclasses C n according to some natural dimension measure n. <p> In both cases, C [ is the class of 1-DNF expressions, which is known to be PAC learnable <ref> [Val84] </ref>. Hence, for C the class of Horn clauses and 1-DNF formulas with at most O (log n) literals, by Theorem 6, C ? is PAC-memb learnable. The class of Classic sentences is known to be polynomially PAC-memb learnable [FP94]. <p> The class C + " is learnable since the intersection of an arbitrary number of monomials can be represented as a monomial of length at most n (where n is the number of variables) and the class of monomials is known to be PAC learnable <ref> [Val84] </ref>. We show C + [ is the class of unate DNF formulas and hence PAC-memb learnable by [AHK93]. <p> The size of this representation is at most P f2F k jf j. Thus, since each function in C k " and C k [ is efficiently representable as a monotone DNF, and the class C of monotone DNF formulas is PAC-memb learnable <ref> [Val84] </ref>, C k [ are PAC-memb learnable. For C the class of `-term DNF formulas, ` constant, let F k C be such that the cardinality of F k is k.
References-found: 62

