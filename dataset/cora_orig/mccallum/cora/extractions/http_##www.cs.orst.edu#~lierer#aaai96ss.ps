URL: http://www.cs.orst.edu/~lierer/aaai96ss.ps
Refering-URL: http://www.cs.orst.edu:80/~tadepall/research/publications.html
Root-URL: 
Email: lierer@mail.cs.orst.edu tadepall@research.cs.orst.edu  
Title: The Use of Active Learning in Text Categorization  
Author: Ray Liere Prasad Tadepalli 
Address: Dearborn Hall 303, Corvallis, OR 97331-3202, USA  
Affiliation: Department of Computer Science, Oregon State University,  
Abstract: With the advent of large distributed and dynamic document collections (such as are on the World Wide Web), it is becoming increasingly important to automate the task of text categorization. The use of machine learning in text categorization is difficult due to characteristics of the domain, including a very large number of input features, noise, and the problems associated with semantic analysis of text. As a result, the use of supervised learning requires a relatively large number of labeled examples. We explore the possibility of using (almost) unsupervised learning and propose some novel approaches to using machine learning in this domain. 
Abstract-found: 1
Intro-found: 1
Reference: [Apte94] <author> Chidanand Apt, Fred Damerau, </author> <title> Automated Learning of Decision Rules for Text Categorization, </title> <booktitle> ACM TOIS 12(2) </booktitle> <pages> 233-251, </pages> <month> July </month> <year> 1994 </year>
Reference-contexts: One of the most impressive results in applying machine learning to text categorization is by Apt and Damerau, who used optimized rule-based induction and reported an 80.5% recall-precision breakev en point using the Reuters-22173 collection <ref> [Apte94] </ref>. However, to achieve this result, over 10,000 labeled examples were used. Typically, large numbers of labeled examples, usually in the thousands, are needed in order to obtain good results with supervised learning.
Reference: [Board87] <author> Raymond A. Board, Leonard Pitt, </author> <title> Semi-Supervised Learning, </title> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <note> Report No. UIUCDCS-R-87-1372, </note> <month> September </month> <year> 1987 </year>
Reference-contexts: Recently there have been some promising results in the active learning area by: (1) Board and Pitt (semi-supervised learning) <ref> [Board87] </ref>; (2) Freund, Seung, Shamir, and Tishby (user selection of examples for labeling) [Freund92]; (3) Lewis and Gale (uncertainty sampling) [Lewis94]; (4) Cohn, Atlas, and Ladner (selective sampling) [Cohn94]; and (5) Dagan and Engelson (committee-based sampling) [Dagan95].
Reference: [Castelli95] <author> Vittorio Castelli, Thomas M. </author> <title> Cover, On the Exponential Value of Labeled Samples, </title> <journal> Pattern Recog nition Letters 16(1) </journal> <pages> 105-111, </pages> <month> January </month> <year> 1995 </year>
Reference-contexts: Knowledge engineering methods, on the other hand, require large amounts of engineering and usually large amounts of computational effort on the front end. 2.1 Labeled versus Unlabeled Examples Castelli and Cover use Bayesian analysis to determine the relative worth of labeled versus unlabeled examples <ref> [Castelli95] </ref>. They conclude that labeled examples are exponentially more valuable than unlabeled examples. This indicates that one will save a great deal of (computational) learning effort by using labeled rather than unlabeled examples. Unfortunately, examples do not label themselves --- a human must do that.
Reference: [Cheeseman95] <author> Peter Cheeseman, John Stutz, </author> <title> Bayesian Classification (AutoClass): Theory and Results, in Advances in Knowledge Discovery and Data Mining, </title> <publisher> The AAAI Press: </publisher> <address> Menlo Park, expected March 1996 </address>
Reference-contexts: This is emphatically not a criticism of AutoClass C. We are using AutoClass C in a domain in which it is not normally used. Most problems on which AutoClass C is used deal with far fewer features <ref> [Cheeseman95] </ref>.
Reference: [Cohn94] <author> David Cohn, Les Atlas, Richard Ladner, </author> <title> Improving Generalization with Active Learning, </title> <booktitle> Machine Learning 15(2) </booktitle> <pages> 201-221, </pages> <month> May </month> <year> 1994 </year>
Reference-contexts: Recently there have been some promising results in the active learning area by: (1) Board and Pitt (semi-supervised learning) [Board87]; (2) Freund, Seung, Shamir, and Tishby (user selection of examples for labeling) [Freund92]; (3) Lewis and Gale (uncertainty sampling) [Lewis94]; (4) Cohn, Atlas, and Ladner (selective sampling) <ref> [Cohn94] </ref>; and (5) Dagan and Engelson (committee-based sampling) [Dagan95]. While approaches and results vary, all of these studies concluded that these various forms of active learning improved learning efficiency by significant amounts.
Reference: [Cook] <author> AutoClass C, </author> <note> version 2.7, </note> <author> software and documentation; Diane Cook, Joseph Potts, Will Taylor, Peter Cheeseman, </author> <note> and John Stutz; available via ftp from: csr.uta.edu:/pub/autoclass-c.tar.Z </note>
Reference-contexts: We are also planning to extend AutoClass C to make it applicable to more general kinds of tasks such as overlapping categories and multi-layered Bayesian networks. These are all challenging problems. 5. Acknowledgements The availability of AutoClass C <ref> [Cook] </ref> and the Reuters-22173 corpus [Reuters] has greatly assisted in our research to date.
Reference: [Dagan95] <author> Ido Dagan, Sean P. Engelson, </author> <title> Committee-Based Sampling for Training Probabilistic Classifiers, </title> <booktitle> ML95, 1995, p. </booktitle> <pages> 150-157 </pages>
Reference-contexts: promising results in the active learning area by: (1) Board and Pitt (semi-supervised learning) [Board87]; (2) Freund, Seung, Shamir, and Tishby (user selection of examples for labeling) [Freund92]; (3) Lewis and Gale (uncertainty sampling) [Lewis94]; (4) Cohn, Atlas, and Ladner (selective sampling) [Cohn94]; and (5) Dagan and Engelson (committee-based sampling) <ref> [Dagan95] </ref>. While approaches and results vary, all of these studies concluded that these various forms of active learning improved learning efficiency by significant amounts.
Reference: [Freund92] <author> Yoav Freund, H. Sebastian Seung, Eli Shamir, Naftali Tishby, </author> <title> Information, Prediction, and Query by Committee, </title> <booktitle> NIPS92, p. </booktitle> <pages> 483-490 </pages>
Reference-contexts: Recently there have been some promising results in the active learning area by: (1) Board and Pitt (semi-supervised learning) [Board87]; (2) Freund, Seung, Shamir, and Tishby (user selection of examples for labeling) <ref> [Freund92] </ref>; (3) Lewis and Gale (uncertainty sampling) [Lewis94]; (4) Cohn, Atlas, and Ladner (selective sampling) [Cohn94]; and (5) Dagan and Engelson (committee-based sampling) [Dagan95]. While approaches and results vary, all of these studies concluded that these various forms of active learning improved learning efficiency by significant amounts.
Reference: [Fung90] <author> Robert M. Fung, Stuart L. Crawford, Lee A. Appelbaum, Richard M. Tong, </author> <title> An Architecture for Probabilistic Concept-Based Information Retrieval, </title> <booktitle> SIGIR'90, 1990, p. </booktitle> <pages> 455-467 </pages>
Reference-contexts: It is quite general and is much more efficient to use than the exhaustive examination of the full probability distribution in situations where some degree of locality applies. Work using Bayesian networks in text categorization has been done by (1) Fung, Crawford, Appelbaum, and Tong <ref> [Fung90] </ref>; (2) Fung and Del Fav ero [Fung95]; (3) Turtle and Croft [Turtle90]; and (4) Haines and Croft [Haines93].
Reference: [Fung95] <author> Robert Fung, Brendan Del Fav ero, </author> <title> Applying Bayesian Networks to Information Retrieval, </title> <journal> Commu nications of the ACM, </journal> <volume> 38(3) </volume> <pages> 42-48, </pages> <month> March </month> <year> 1995 </year>
Reference-contexts: This research was partially supported by the National Science Foundation under grant number IRI-9520243. Most existing methods of text categorization fall into one of 3 categories: boolean, probabilistic, or vector space <ref> [Fung95] </ref>. Existing methods use either a knowledge engineering or a machine learning technology. Admittedly there are many situations when the dividing line between methods or technologies is fuzzy at best, and many approaches draw from multiple methods and technologies. <p> Work using Bayesian networks in text categorization has been done by (1) Fung, Crawford, Appelbaum, and Tong [Fung90]; (2) Fung and Del Fav ero <ref> [Fung95] </ref>; (3) Turtle and Croft [Turtle90]; and (4) Haines and Croft [Haines93]. These approaches typically use a Bayesian network with an at least partially predefined structure, and then employ supervised learning to determine the structure details and the probability distributions of the nodes in the network.
Reference: [Haines93] <author> David Haines, W. Bruce Croft, </author> <title> Relevance Feedback and Inference Networks, </title> <booktitle> SIGIR'93, p. </booktitle> <pages> 2-11 </pages>
Reference-contexts: Work using Bayesian networks in text categorization has been done by (1) Fung, Crawford, Appelbaum, and Tong [Fung90]; (2) Fung and Del Fav ero [Fung95]; (3) Turtle and Croft [Turtle90]; and (4) Haines and Croft <ref> [Haines93] </ref>. These approaches typically use a Bayesian network with an at least partially predefined structure, and then employ supervised learning to determine the structure details and the probability distributions of the nodes in the network.
Reference: [Lebowitz87] <author> Michael Lebowitz, </author> <title> Experiments with Incremental Concept Formation: </title> <booktitle> UNIMEM, Machine Learning 2(2) </booktitle> <pages> 103-138, </pages> <year> 1987 </year>
Reference-contexts: How-ev er, most unsupervised conceptual clustering algorithms partition the data instances into classes. We are only familiar with two that do not have this property --- OLOC [Martin94] and UNIMEM <ref> [Lebowitz87] </ref>. Supervised learning gets around this problem by effectively training a system for each category. For each category, the system gives a "yes" or "no". The fact that a document can be in more than (or less than) one category does not ever need to be directly confronted. 4.
Reference: [Lewis94] <author> David D. Lewis, William A. Gale, </author> <title> A Sequential Algorithm for Training Text Classifiers, </title> <booktitle> SIGIR'94, p. </booktitle> <pages> 3-12 </pages>
Reference-contexts: Recently there have been some promising results in the active learning area by: (1) Board and Pitt (semi-supervised learning) [Board87]; (2) Freund, Seung, Shamir, and Tishby (user selection of examples for labeling) [Freund92]; (3) Lewis and Gale (uncertainty sampling) <ref> [Lewis94] </ref>; (4) Cohn, Atlas, and Ladner (selective sampling) [Cohn94]; and (5) Dagan and Engelson (committee-based sampling) [Dagan95]. While approaches and results vary, all of these studies concluded that these various forms of active learning improved learning efficiency by significant amounts.
Reference: [Martin94] <author> Joel D. Martin, Dorrit O. Billman, </author> <title> Acquiring and Combining Overlapping Concepts, </title> <booktitle> Machine Learning 16(1-2):121-155, </booktitle> <month> July/August </month> <year> 1994 </year>
Reference-contexts: How-ev er, most unsupervised conceptual clustering algorithms partition the data instances into classes. We are only familiar with two that do not have this property --- OLOC <ref> [Martin94] </ref> and UNIMEM [Lebowitz87]. Supervised learning gets around this problem by effectively training a system for each category. For each category, the system gives a "yes" or "no".
Reference: [Pearl88] <author> Judea Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, </title> <publisher> Mor gan Kaufmann, </publisher> <year> 1988 </year>
Reference-contexts: We wanted a firm basis on which to build. The Bayesian network formalism has a strong theoretical (as well as empirical) foundation <ref> [Pearl88] </ref>. It is quite general and is much more efficient to use than the exhaustive examination of the full probability distribution in situations where some degree of locality applies.
Reference: [Reuters] <author> Reuters-22173 corpus, </author> <title> a collection of 22,173 indexed documents appearing on the Reuters newswire in 1987; Reuters Ltd, </title> <institution> Carnegie Group, David Lewis, </institution> <note> Information Retrieval Laboratory at the University of Massachusetts; available via ftp from: ciir-ftp.- cs.umass.edu:/pub/reuters1/corpus.tar.Z </note>
Reference-contexts: We are also planning to extend AutoClass C to make it applicable to more general kinds of tasks such as overlapping categories and multi-layered Bayesian networks. These are all challenging problems. 5. Acknowledgements The availability of AutoClass C [Cook] and the Reuters-22173 corpus <ref> [Reuters] </ref> has greatly assisted in our research to date.
Reference: [Turtle90] <author> Howard Turtle, W. Bruce Croft, </author> <title> Inference Net works for Document Retrieval, </title> <booktitle> SIGIR'90, p. </booktitle> <pages> 1-24 </pages>
Reference-contexts: Work using Bayesian networks in text categorization has been done by (1) Fung, Crawford, Appelbaum, and Tong [Fung90]; (2) Fung and Del Fav ero [Fung95]; (3) Turtle and Croft <ref> [Turtle90] </ref>; and (4) Haines and Croft [Haines93]. These approaches typically use a Bayesian network with an at least partially predefined structure, and then employ supervised learning to determine the structure details and the probability distributions of the nodes in the network.
References-found: 17

