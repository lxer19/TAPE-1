URL: ftp://ftp.cs.colorado.edu/users/mozer/papers/dubm.ps
Refering-URL: http://www.cs.colorado.edu/~mozer/papers/dubm.html
Root-URL: http://www.cs.colorado.edu
Title: Lending Direction to Neural Networks  
Author: Richard S. Zemel Christopher K. I. Williams Michael C. Mozer 
Address: 10010 North Torrey Pines Rd. La Jolla, CA 92037  Toronto, ONT M5S 1A4 CANADA  Boulder, CO 80309-0430  
Affiliation: CNL, The Salk Institute  Department of Computer Science University of Toronto  Department of Computer Science Institute of Cognitive Science University of Colorado  
Abstract: We present a general formulation for a network of stochastic directional units. This formulation is an extension of the Boltzmann machine in which the units are not binary, but take on values on a cyclic range, between 0 and 2 radians. This measure is appropriate to many domains, representing cyclic or angular values, e.g., wind direction, days of the week, phases of the moon. The state of each unit in a Directional-Unit Boltzmann Machine (DUBM) is described by a complex variable, where the phase component specifies a direction; the weights are also complex variables. We associate a quadratic energy function, and corresponding probability, with each DUBM configuration. The conditional distribution of a unit's stochastic state is a circular version of the Gaussian probability distribution, known as the von Mises distribution. In a mean-field approximation to a stochastic dubm, the phase component of a unit's state represents its mean direction, and the magnitude component specifies the degree of certainty associated with this direction. This combination of a value and a certainty provides additional representational power in a unit. We present a proof that the settling dynamics for a mean-field DUBM cause convergence to a free energy minimum. Finally, we describe a learning algorithm and simulations that demonstrate a mean-field DUBM's ability to learn interesting mappings. fl To appear in: Neural Networks.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D. H., Hinton, G. E., and Sejnowski, T. J. </author> <year> (1985). </year> <title> A learning algorithm for Boltzmann machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169. </pages>
Reference-contexts: We have devised a general formulation of networks of stochastic directional units. This formulation is a novel generalization of a Boltzmann machine <ref> (Ackley, Hinton and Sejnowski, 1985) </ref> in which the units are not binary, but instead take on directional values between 0 and 2 radians. This paper presents the theoretical basis of a directional-unit Boltzmann machine (dubm), including an energy function and underlying probabilistic model.
Reference: <author> Almeida, L. B. </author> <year> (1987). </year> <title> A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. </title> <booktitle> In Proceedings, 1st First International Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 609-618, </pages> <address> San Diego, CA. </address> <publisher> IEEE. </publisher>
Reference: <author> Baldi, P. and Meir, R. </author> <year> (1990). </year> <title> Computing with arrays of coupled oscillators: An application to preattentive texture discrimination. </title> <journal> Neural Computation, </journal> <volume> 2(4) </volume> <pages> 458-471. </pages>
Reference: <author> Baldi, P. and Pineda, F. </author> <year> (1991). </year> <title> Contrastive learning and neural oscillations. </title> <journal> Neural Computation, </journal> <volume> 3(4) </volume> <pages> 526-533. </pages>
Reference: <author> Benvenuto, N. and Piazza, F. </author> <year> (1992). </year> <title> On the complex backpropagation algorithm. </title> <journal> IEEE Transactions On Signal Processing, </journal> <volume> 40(4) </volume> <pages> 967-969. </pages>
Reference: <author> Eckhorn, R., Bauer, R., Jordan, W., Brosch, M., Kruse, W., Munk, M., and Reitboek, H. K. </author> <year> (1988). </year> <title> Coherent oscillations: A mechanism of feature linking in the visual cortex. </title> <journal> Biological Cybernetics, </journal> <volume> 60 </volume> <pages> 121-130. </pages>
Reference: <author> Fradkin, E., Huberman, B. A., and Shenker, S. H. </author> <year> (1978). </year> <title> Gauge symmetries in random magnetic systems. </title> <journal> Physical Review B, </journal> <volume> 18(9) </volume> <pages> 4789-4814. </pages>
Reference: <author> Galland, C. C. </author> <year> (1992). </year> <title> Self-organization using Deterministic Boltzmann Machine learning. </title> <type> PhD thesis, </type> <institution> Physics Department of the University of Toronto. </institution> <note> 17 Gislen, </note> <author> L., Peterson, C., and Soderberg, B. </author> <year> (1992). </year> <title> Rotor neurons: Basic formalism and dynamics. </title> <journal> Neural Computation, </journal> <volume> 4(5) </volume> <pages> 737-745. </pages>
Reference-contexts: Radford Neal (personal communication, 1992) has suggested a method for combining binary and directional units. This expanded representation may be 5 Note that deterministic dubms share the problem of deterministic binary-unit Boltzmann machines, in that their performance drops off in deep networks <ref> (Galland, 1992) </ref>. This is largely due to the fact that the mean-field approximation breaks down as multiple layers introduce increasing correlations among the units. 16 useful in domains with directional data that is not present everywhere.
Reference: <author> Gray, C. M., Koenig, P., Engel, A. K., and Singer, W. </author> <year> (1989). </year> <title> Oscillatory responses in cat visual cortex exhibiting intercolumnar synchronization which reflects global properties. </title> <journal> Nature (London), </journal> <volume> 338 </volume> <pages> 334-337. </pages>
Reference: <author> Hinton, G. E. </author> <year> (1989). </year> <title> Deterministic Boltzmann learning performs steepest descent in weight-space. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 143-150. </pages>
Reference: <author> Hopfield, J. J. </author> <year> (1982). </year> <title> Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Sciences U.S.A., </booktitle> <volume> 79 </volume> <pages> 2554-2558. </pages>
Reference-contexts: In the Hopfield spin-system <ref> (Hopfield, 1982) </ref> and in a Boltzmann machine, the units are binary, and the energy is defined to be the sum of the pairwise products of the units' states and their connecting weights.
Reference: <author> Hopfield, J. J. </author> <year> (1984). </year> <title> Neurons with graded response have collective computational properties like those of two-state neurons. </title> <booktitle> Proceedings of the National Academy of Sciences U.S.A., </booktitle> <volume> 81 </volume> <pages> 3088-3092. </pages>
Reference: <author> Kammen, D. M., Holmes, P. J., and Koch, C. </author> <year> (1989). </year> <title> Cortical oscillations in neuronal networks: Feed-back versus local coupling. </title> <booktitle> In Models of Brain Function, </booktitle> <pages> pages 273-284. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference: <author> Mardia, K. V. </author> <year> (1972). </year> <title> Statistics of Directional Data. </title> <publisher> Academic Press, London. </publisher>
Reference-contexts: state is proportional to: p (Z j = z j ) / e fiE (Z j =z j ) = e fia j cos (t j ff j ) (5) This probability distribution for a unit's state corresponds to a distribution known as the von Mises, or circular normal, distribution <ref> (Mardia, 1972) </ref> that in many ways acts on a circular range as the Gaussian distribution acts on a linear (i.e., non-circular) range. 1 Two parameters completely characterize this distribution: a mean direction t = (0; 2] and a concentration parameter m &gt; 0 that behaves like the reciprocal of the variance
Reference: <author> Mozer, M. C., Zemel, R. S., and Behrmann, M. </author> <year> (1992). </year> <title> Learning to segment images using dynamic feature binding. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 436-443. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: This approach greatly simplifies the derivation of the deterministic system dynamics. Our work demonstrates that networks composed of directional units can learn interesting mappings. magic, as described in <ref> (Mozer et al., 1992) </ref>, is a large-scale dubm that learns to segment novel images composed of a wide range of overlapping geometric objects. One extension of the current model is appropriate for tasks that involve learning a mapping from a set of input patterns to a set of output patterns.
Reference: <author> Mozer, M. C., Zemel, R. S., Behrmann, M., and Williams, C. K. I. </author> <year> (1992). </year> <title> Learning to segment images using dynamic feature binding. </title> <journal> Neural Computation, </journal> <volume> 4(5) </volume> <pages> 650-665. </pages>
Reference-contexts: This approach greatly simplifies the derivation of the deterministic system dynamics. Our work demonstrates that networks composed of directional units can learn interesting mappings. magic, as described in <ref> (Mozer et al., 1992) </ref>, is a large-scale dubm that learns to segment novel images composed of a wide range of overlapping geometric objects. One extension of the current model is appropriate for tasks that involve learning a mapping from a set of input patterns to a set of output patterns.
Reference: <author> Noest, A. J. </author> <year> (1988). </year> <title> Phasor neural networks. </title> <booktitle> In Neural Information Processing Systems, </booktitle> <pages> pages 584-591, </pages> <address> New York. </address> <publisher> AIP. </publisher>
Reference: <author> Peterson, C. and Anderson, J. R. </author> <year> (1987). </year> <title> A mean field theory learning algorithm for neural networks. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 995-1019. </pages>
Reference: <author> Pineda, F. J. </author> <year> (1987). </year> <title> Generalization of back propagation to recurrent and higher order neural networks. </title> <booktitle> In Proceedings of IEEE Conference on Neural Information Processing Systems, </booktitle> <address> Denver, Colorado. </address> <note> IEEE. </note> <author> von der Malsburg, C. </author> <year> (1981). </year> <title> The correlation theory of brain function. </title> <type> Technical Report Internal report 81-2, </type> <institution> Department of Neurobiology, Max Planck Institute for Biophysical Chemistry, Goettingen. </institution>
Reference: <author> Widrow, B., McCool, J., and Ball, M. </author> <year> (1975). </year> <title> The complex LMS algorithm. </title> <booktitle> Proc. IEEE, </booktitle> <pages> pages 719-720. 18 </pages>
References-found: 20

