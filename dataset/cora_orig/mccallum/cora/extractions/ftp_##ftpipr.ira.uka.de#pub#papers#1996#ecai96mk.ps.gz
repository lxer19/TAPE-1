URL: ftp://ftpipr.ira.uka.de/pub/papers/1996/ecai96mk.ps.gz
Refering-URL: ftp://ftpipr.ira.uka.de/.public_html/papersna.html
Root-URL: 
Title: Communication as the Basis for Learning in Multi-Agent Systems  
Author: M. Kaiser, R. Dillmann, O. Rogalla 
Address: D-76128 Karlsruhe, Germany  
Affiliation: University of Karlsruhe Institute for Real-Time Computer Systems Robotics  
Note: in: ECAI-96 Workshop on Learning in Distributed AI Systems, Budapest, Hungary,  
Email: E-Mail: kaiser@ira.uka.de  
Phone: Phone: +49 721 6084051 Fax: +49 721 606740  
Date: 1996  
Abstract: This paper discusses the significance of communication between individual agents that are embedded into learning Multi-Agent Systems. For several learning tasks occurring within a Multi-Agent System, communication activities are investigated and the need for a mutual understanding of agents participating in the learning process is made explicit. Thus, the need for a common ontology to exchange learning-related information is shown. Building this ontology is an additional learning task that is not only extremely important, but also extremely difficult. We propose a solution that is motivated by the human ability to understand each other even in the absence of a common language by using alternative communication channels, such as gestures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. D. Beer. </author> <title> A dynamical systems perspective on agent-environment interaction. </title> <journal> Artificial Intelligence, </journal> <volume> 72 </volume> <pages> 173-215, </pages> <year> 1995. </year>
Reference-contexts: To provide the formal basis for these investigations, we'll employ state space models, following previous work by Beer <ref> [1] </ref> and ourselves [9]. 2 Modeling Multi-Agent Systems 2.1 Single agent model Our model of an agent a is that of a skilled subsystem. A single agent is able to perform competent actions that are related to its locally (possibly internally) defined goal and facilitate goal-oriented state transitions.
Reference: [2] <author> A. I. Cypher. </author> <title> Watch what I do Programming by Demonstration. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: Obviously, observing other agents' actions and matching the observations against one's internal representations is not a trivial task. However, many techniques developed within the framework of Programming by Demonstration <ref> [2] </ref> may prove useful to support it.
Reference: [3] <author> A. Farquhar, R. Fikes, W. Pratt, and J. Rice. </author> <title> Collaborative ontology construction for information integration. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1995. </year>
Reference-contexts: All these things can happen despite a common communication language and a common content language. This problem, for which currently solutions are being sought via tools for creating, accessing, and maintaining ontologies <ref> [3] </ref>, results therefore in another learning task, the task to learn the meaning of symbols. Throughout this paper, we will analyze the learning tasks existing within a Multi-Agent System with respect to their requirements regarding the communication between the agents involved.
Reference: [4] <author> Tim Finin, Rich Fritzson, Don McKay, and Robin McEntire. </author> <title> KQML A language and protocol for knowledge and information exchange. </title> <type> Technical Report CS-94-02, </type> <institution> Computer Science Department, University of Maryland and Valley Forge Engineering Center, Unisys Corporation, Computer Science Department, University of Maryland, </institution> <address> UMBC Baltimore MD 21228, </address> <year> 1994. </year>
Reference-contexts: However, when designing such a language we experience the same dilemma that initially motivated the use of learning: If we have a known, agent-independent "area" that we want to describe by a language, both syntax and semantics of that language can be defined a--priori. KQML <ref> [4] </ref> is a good example of such a language that has been designed specifically to facilitate communication in a content-independent manner. If the area or function we want to describe is not a-priori given or sufficiently complex, a general-purpose language such as KIF [5] is a reasonable choice.
Reference: [5] <author> M. R. Genesereth, R. E. Fikes, et al. </author> <title> Knowledge interchange format, version 3.0 reference manual. </title> <type> Technical Report Logic-92-1, </type> <institution> Computer Science Department, Stanford University, </institution> <year> 1992. </year>
Reference-contexts: KQML [4] is a good example of such a language that has been designed specifically to facilitate communication in a content-independent manner. If the area or function we want to describe is not a-priori given or sufficiently complex, a general-purpose language such as KIF <ref> [5] </ref> is a reasonable choice.
Reference: [6] <author> J. H. Gennari, P. Langley, and D. Fisher. </author> <title> Models of incremental concept formation. </title> <journal> Artificial Intelligence, </journal> <volume> 40:11 - 61, </volume> <year> 1989. </year>
Reference-contexts: In this sense, the symbol represents a concept that is to be learned in a supervised manner, for example by methods such as those described in <ref> [6, 11] </ref>. In addition, the Multi-Agent System's task space must be extended by the new subgoal. Task-specifying external agents and the Multi-Agent System itself must incorporate the new symbol into their task-description language.
Reference: [7] <author> Stevan Harnad. </author> <title> The symbol grounding problem. </title> <journal> Physica D, </journal> <volume> 42 </volume> <pages> 335-346, </pages> <year> 1990. </year>
Reference-contexts: To understand the meaning of symbols used for communication, these symbols must be grounded on the representation primitives of each agent its state and its action vector <ref> [14, 7] </ref>. Such symbols can be general, such that they describe phenomena that can only be observed within a team or on the system level and are only partially understandable for most individual agents.
Reference: [8] <author> S. Jain and A. Sharma. </author> <title> Team learning of formal languages. In Agents that learn from other agents: </title> <booktitle> Proceedings of the ICML'95 Workshop, </booktitle> <address> Tahoe City, California, </address> <year> 1995. </year>
Reference-contexts: be able to map the teacher's advice onto its own action space. 1 It should be noted that we use the term "learning to act as a team" instead of "team learning," in order to distinguish the "team learning" scenario (multiple agents trying to learn the same concept/language, as in <ref> [8] </ref>) from the situation considered here. Both requirements are not trivial, especially if agents should learn from other agents that are not structurally identical. For learning on the basis of a scalar reward, the situation is very similar.
Reference: [9] <author> M. Kaiser and R. Dillmann. </author> <title> Building elementary robot skills from human demonstration. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <address> Minneapolis, Minnesota, USA, </address> <year> 1996. </year>
Reference-contexts: To provide the formal basis for these investigations, we'll employ state space models, following previous work by Beer [1] and ourselves <ref> [9] </ref>. 2 Modeling Multi-Agent Systems 2.1 Single agent model Our model of an agent a is that of a skilled subsystem. A single agent is able to perform competent actions that are related to its locally (possibly internally) defined goal and facilitate goal-oriented state transitions. <p> This setting is typical for transferring skills from one agent (e.g., a human) to another (e.g., a robot) <ref> [9] </ref>. 3.1.1 Communication issues in isolated learning In the isolated learning case, a single agent (the instructed agent or the "pupil") receives feedback from another agent. This agent may be an artificial one (a softbot), or a human supervisor.
Reference: [10] <author> M. Kaiser, A. Retey, and R. Dillmann. </author> <title> Designing neural networks for adaptive control. </title> <booktitle> In IEEE International Conference on Decision and Control (34th CDC), </booktitle> <year> 1995. </year>
Reference-contexts: As in adaptive control, agents may also receive an indication of the direction u into which to alter their actions, instead of an optimal action <ref> [10] </ref>. Another prototypical setting is that of reinforcement learning, in which the agent receives a possibly delayed reward r as feedback (see, for example, [13]) and alters its actions in order to maximize the reward.
Reference: [11] <author> Volker Klingspor, Katharina Morik, and Anke Rieger. </author> <title> Learning concepts from sensor data of a mobile robot. </title> <booktitle> Machine Learning, </booktitle> <year> 1996. </year>
Reference-contexts: In this sense, the symbol represents a concept that is to be learned in a supervised manner, for example by methods such as those described in <ref> [6, 11] </ref>. In addition, the Multi-Agent System's task space must be extended by the new subgoal. Task-specifying external agents and the Multi-Agent System itself must incorporate the new symbol into their task-description language.
Reference: [12] <author> Th. Langle, T. C. Luth, and U. Rembold. </author> <title> A distributed control architecture for autonomous robot systems. </title> <editor> In T. Kanade H. Bunke, H. Noltemeier, editor, </editor> <title> Modelling and Planning for Sensor Based Intelligent Robot Systems. </title> <publisher> World Scientific, </publisher> <year> 1995. </year>
Reference-contexts: In a Multi-Agent System, also an agent that coordinates a team of agents as well as agents cooperating in a team must be able to understand their respective counterparts - independent on the coordination/negotiation technique (see <ref> [19, 12] </ref> for examples) that is actually used. They must be able to understand what they are expected to do (or how they could contribute), and must be prepared to formulate their requests in an understandable manner.
Reference: [13] <author> L. J. Lin. </author> <title> Reinforcement learning for robots using neural networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1993. </year>
Reference-contexts: As in adaptive control, agents may also receive an indication of the direction u into which to alter their actions, instead of an optimal action [10]. Another prototypical setting is that of reinforcement learning, in which the agent receives a possibly delayed reward r as feedback (see, for example, <ref> [13] </ref>) and alters its actions in order to maximize the reward. This kind of learning requires exploration, i.e., the systematic alteration of the calculated action in order to estimate the optimal action.
Reference: [14] <author> C. Malcolm and T. Smithers. </author> <title> Symbol grounding via a hybrid architecture in an autonomous assembly system. </title> <booktitle> Robotics and Autonomous Systems Special Issue on Designing Autonomous Agents, </booktitle> <address> 6(1,2), </address> <year> 1990. </year>
Reference-contexts: To understand the meaning of symbols used for communication, these symbols must be grounded on the representation primitives of each agent its state and its action vector <ref> [14, 7] </ref>. Such symbols can be general, such that they describe phenomena that can only be observed within a team or on the system level and are only partially understandable for most individual agents.
Reference: [15] <author> P. Reignier, V. Hansen, and J.L. Crowley. </author> <title> Incremental supervised learning for mobile robot reactive control. </title> <booktitle> In Intelligent Autonomous Systems 4 (IAS-4), </booktitle> <pages> pages 287 - 294. </pages> <publisher> IOS Press, </publisher> <year> 1995. </year>
Reference-contexts: the agent to contribute to a subgoal h: In the first case, each learning agent obtains specific feedback that can be used to alter its action u: In the most simple case, the optimal action u fl is communicated to the agent, such that incremental supervised learning may take place. <ref> [15] </ref> describes such a situation for an agent (a robot) that learns directly from user demonstrations. As in adaptive control, agents may also receive an indication of the direction u into which to alter their actions, instead of an optimal action [10].
Reference: [16] <author> J. Schmidhuber. </author> <title> A general method for multi-agent reinforcement learning in unrestricted environments. </title> <editor> In S. Sen, editor, </editor> <booktitle> AAAI Spring Symposium on Adaptation, Coevolution and Learning in Multiagent Systems. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: It should, however, be noted that in some cases additional constraints are to be observed, since a single agent's environment (and, possibly, its target function) is continuously changing <ref> [16] </ref>. 3.2.1 Communication issues in learning to act as a team When learning to act as a team, the teacher instructs or evaluates a group of agents that cooperate towards a common goal g: Here, the same communication requirements as in the isolated learning case exist (see section 3.1.1).
Reference: [17] <editor> S. Sen, editor. </editor> <booktitle> AAAI Spring Symposium on Adaptation, Coevolution and Learning in Multiagent Systems. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Learning in Multi-Agent Systems has become a major research field within Distributed Artificial Intelligence and Machine Learning <ref> [20, 17] </ref>. It is motivated by the insight that it is impossible to determine a-priori the complete knowledge that must exist within each component of a distributed, heterogeneous system in order to allow satisfactory performance of that system.
Reference: [18] <author> K. T. Simsarian and M. J. Mataric. </author> <title> Learning to cooperate using two six-legged mobile robots. </title> <editor> In M. Kaiser, editor, </editor> <booktitle> Proceedings of the 3rd European Workshop on Learning Robots (EWLR-3), </booktitle> <address> Heraklion, Crete, Greece, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Once credit assignment has taken place and subgoals g a j (or h j or h; respectively) are known for any of the team members, team learning is reduced to several parallel steps of isolated learning (see Fig. 2 and, for example, <ref> [18] </ref>).
Reference: [19] <author> R. G. Smith and R. Davis. </author> <title> Frameworks for cooperation in distributed problem solving. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 11(1) </volume> <pages> 61-70, </pages> <year> 1981. </year>
Reference-contexts: In a Multi-Agent System, also an agent that coordinates a team of agents as well as agents cooperating in a team must be able to understand their respective counterparts - independent on the coordination/negotiation technique (see <ref> [19, 12] </ref> for examples) that is actually used. They must be able to understand what they are expected to do (or how they could contribute), and must be prepared to formulate their requests in an understandable manner.
Reference: [20] <author> G. Weiss and S. Sen, </author> <title> editors. Adaptation and Learning in Multi-Agent Systems. </title> <publisher> Springer-Verlag Berlin, </publisher> <address> Heidelberg, New York, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction Learning in Multi-Agent Systems has become a major research field within Distributed Artificial Intelligence and Machine Learning <ref> [20, 17] </ref>. It is motivated by the insight that it is impossible to determine a-priori the complete knowledge that must exist within each component of a distributed, heterogeneous system in order to allow satisfactory performance of that system.
References-found: 20

