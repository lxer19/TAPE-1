URL: http://physics.www.media.mit.edu/publications/papers/entropy.ps
Refering-URL: http://physics.www.media.mit.edu/publications/
Root-URL: http://www.media.mit.edu
Email: (neilg@media.mit.edu)  
Title: Information in Dynamics  
Author: Neil Gershenfeld 
Date: 8  
Note: a message can be sent through a channel. Information theory has since flourished in engineering practice.  example of the physical meaning of information. 14 Assume that a physical system is described by a  
Address: 20 Ames St, Cambridge, MA 02139  
Affiliation: MIT Media Laboratory  
Abstract: The historical search for the fundamental meaning of thermodynamic entropy lead to the discovery of the connection between entropy and information about microscopic dynamics, which in turn motivated the development of the theory of information in communications. In this paper I will review how information theory can profitably be applied back to its roots in dynamics in order to characterize the essential properties of a measured time series. Entropy was introduced in the familiar modern form (dS = ffiQ=T ) by Clausius in 1854, building on work by Carnot (1824) and Kelvin (~1850) to understand the nature of heat and irreversability in thermodynamic systems. 1;2 Boltzmann was dedicated to understanding the microscopic meaning of this macroscopic entropy; influenced by Maxwell's kinetic theory of gases (~1860) he introduced the relationship H = f (r; p; t) log f (r; p; t)d 3 rd 3 p (where f is the velocity distribution in a gas) in 1872, and then the more general form S = k log W (where W is the num In 1871 Maxwell created his demon, 3 which appeared to violate the second law of thermodynamics by intelligent action. Szilard, in 1929, made the significant step of considering a one-molecule gas that could be on either side of a partition; this introducing the idea of a binary bit of information (which side of the partition the molecule is in) and of using entropy to measure information. 4 Although Szi-lard missed the crucial role of erase in explaining the demon's (mis)behavior (this was first recognized by Landauer 5 ), he had laid the foundation for the development of both reversible computation 6 and of information theory. 7 In 1948 Shannon applied entropy to measure the information content in an arbitrary message, independent of its physical origin, and was thereby able to solve significant outstanding communications problems such as the maximum rate at which The study of ergodic systems has both benefited from, and contributed to, information theory. 9 More recently, Shaw pointed out the connection between information and dissipative dynamics, 10 and Fraser extended this to develop a framework that I will describe here for characterizing the structure in time series produced by non-linear systems. 11 Although a dynamical system's global structure can perform nontrivial computations, 12;13 analyzing the information evolution associated with the much simpler local behavior is sufficient to answer deep questions about the complexity and predictability of a system. In this paper I will will explain how to understand and measure such information. As well as being quite useful in practice, this application of information theory back to its roots in dynamics provides a simple but clear state vector ~x and governing equations d~x=dt = ~ f (~x) (or ~x n+1 = ~ f (~x n )). This need not imply that the system is finite-dimensional; the underlying governing equations may be infinite-dimensional partial differential equations which reduce to a finite-dimensional mode expansion due to dissipation. Let y(~x(t)) be a scalar experimentally-accessible quantity that is a function of the state of the system (such as the temperature or velocity at a point in a fluid convection cell, or the concentration of a particular species in a chemical reaction). The goal is to learn as much as possible about the underlying system given only the time series y(t). The necessary connection between the observed time series and its physical origin is provided by state-space reconstruction. 1518 Construct a new vector out of lagged copies of the observable ~z t = (y t ; y tt ; : : :; y t(d1)t ), where the time lag t and the dimension d are parameters that will be discussed shortly. If d is large enough, then for almost any choice of the governing equations ~ f , the observable y(~x), and the time delay t , the motion of the ber of available states) around 1877.
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> S.G. Brush, </author> <title> The Kind of Motion we call Heat, </title> <journal> Studies in Statistical Mechanics, </journal> <volume> Vol. </volume> <publisher> VI (North-Holland, </publisher> <year> 1976). </year>
Reference: [2] <author> R. Balian, </author> <note> From Microphysics to Macrophysics, Vol. I (Springer-Verlag, 1991), p. 123. </note>
Reference: [3] <author> H.S. Leff and A.F. Rex, Maxwell's Demon: </author> <title> Entropy, Information, </title> <publisher> Computing (Princeton University Press, </publisher> <year> 1990). </year>
Reference: [4] <author> L. Szilard, Z. f. </author> <type> Physik 53, </type> <month> 840 </month> <year> (1929). </year>
Reference: [5] <author> R. </author> <title> Landauer, </title> <journal> IBM J. Res. Dev. </journal> <volume> 5, </volume> <month> 183 </month> <year> (1961). </year>
Reference: [6] <author> C.H. Bennett, IBM J. </author> <title> Res. </title> <journal> Dev. </journal> <volume> 17, </volume> <month> 525 </month> <year> (1973). </year>
Reference: [7] <author> C.E. </author> <title> Shannon, </title> <journal> Bell Syst. Tech. J. </journal> <volume> 27, </volume> <month> 379 </month> <year> (1948). </year>
Reference: [8] <author> D. </author> <title> Slepian, </title> <booktitle> Key Papers in the Development of Information Theory (IEEE Press, </booktitle> <year> 1974). </year>
Reference: [9] <author> K. </author> <title> Petersen, </title> <publisher> Ergodic Theory (Cambridge University Press, </publisher> <year> 1989). </year>
Reference: [10] <author> R. Shaw, Z. Naturforsch. 36a, </author> <month> 80 </month> <year> (1981). </year>
Reference: [11] <author> A.M. </author> <title> Fraser, </title> <journal> IEEE Trans. Inf. </journal> <note> Theory 35 245 (1989). </note>
Reference: [12] <author> C. Moore, </author> <title> Phys. </title> <journal> Rev. Lett. </journal> <volume> 64, </volume> <month> 2354 </month> <year> (1990). </year>
Reference: [13] <author> E. Fredkin and T. Toffoli, </author> <title> Int. </title> <journal> J. Theor. Phys. </journal> <volume> 21, </volume> <month> 905 </month> <year> (1982). </year>
Reference: [14] <author> R. </author> <title> Landauer, </title> <booktitle> Physics Today 44, </booktitle> <month> 23 </month> <year> (1991). </year>
Reference: [15] <editor> F. Takens, </editor> <booktitle> Springer Lecture Notes in Mathematics, </booktitle> <volume> 898, </volume> <month> 366 </month> <year> (1981). </year>
Reference: [16] <author> N.H. Packard, J.P. Crutchfield, J.D. Farmer, </author> <title> and R.S. Shaw, </title> <journal> Phys. Rev. Lett. </journal> <volume> 45, </volume> <month> 712 </month> <year> (1980). </year>
Reference: [17] <author> D. Ruelle, </author> <type> personal communication. </type>
Reference: [18] <author> T. Sauer, J. Yorke and M. Casdagli, J. </author> <title> Stat. </title> <journal> Phys. </journal> <volume> 65, </volume> <month> 579 </month> <year> (1991). </year>
Reference: [19] <author> M. Palus, </author> <title> in Predicting the Future and Understanding the Past, A.S. </title> <editor> Weigend and N.A. Ger-shenfeld eds. </editor> <publisher> (Addison-Wesley, </publisher> <year> 1993). </year>
Reference: [20] <author> J.P. Eckmann and D. Ruelle, </author> <title> Rev. Mod. </title> <journal> Phys., </journal> <volume> 57, </volume> <month> 617 </month> <year> (1985). </year>
Reference: [21] <author> N.A. </author> <title> Gershenfeld and A.S. </title> <type> Weigend, preprint, </type> <year> 1992. </year>
Reference: [22] <author> F.P. Preparata and M.I. Shamos, </author> <title> Computational Geometry, An Introduction (Springer-Verlag, </title> <year> 1985). </year>
References-found: 22

