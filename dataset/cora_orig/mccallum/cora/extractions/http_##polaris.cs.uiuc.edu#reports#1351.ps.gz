URL: http://polaris.cs.uiuc.edu/reports/1351.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: (harrison@csrd.uiuc.edu)  (mehrotra@csrd.uiuc.edu)  
Title: A data prefetch mechanism for accelerating general-purpose computation  and Connected Components Corporation  
Author: Luddy Harrison Sharad Mehrotra 
Date: 8 May 1994 (Last revised 9 March 1995)  
Affiliation: UI Dept. of Computer Science  CSRD and UI Dept. of Computer Science  
Abstract-found: 0
Intro-found: 0
Reference: [BC91] <author> Jean-Loup Baer and Tien-Fu Chen. </author> <title> An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 176-186, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Unlike another data prefetch scheme that uses speculative execution to generate prefetches early along the most likely path of execution at considerable extra hardware cost <ref> [BC91] </ref>, the IRB design makes a trade-off towards simplicity of design. The IRB hardware makes no attempt to detect the most likely path of execution through a program's control flow graph of its own.
Reference: [BL94] <author> Ricardo Bianchini and Thomas J. LeBlanc. </author> <title> A Preliminary Evaluation of Cache-Miss-Initiated Prefetching Techniques in Scalable Multiprocessors. </title> <type> Technical Report 515, </type> <institution> University of Rochestor, Computer Science Department, Rochester, </institution> <address> NY 14627, </address> <month> May </month> <year> 1994. </year>
Reference: [CB92] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-61, </pages> <month> October </month> <year> 1992. </year>
Reference: [CB94a] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> A Performance Study of Software and Hardware Data Prefetching Schemes. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 223-232, </pages> <month> April </month> <year> 1994. </year>
Reference: [CB94b] <author> Zarka Cvetanovic and Dileep Bhandarkar. </author> <title> Characterization of Alpha AXP Performance Using TP and SPEC Workloads. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 60-70, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It is also distinguishable from previous designs in its seamless integration of linear and indirect address prefetching. The IRB operates on load instructions only, because they make up 20% to 40% of the instructions executed by typical programs 1 <ref> [CB94b, HP90] </ref>. Because the IRB acquires knowledge about individual load accesses exclusively at runtime, it should also be useful in prefetching data references in operating system and transaction processing codes.
Reference: [Chi94a] <author> Chi-Hung Chi. </author> <title> Compiler Optimization Technique for Data Cache Prefetching Using a Small CAM Array. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 263-266, </pages> <month> August </month> <year> 1994. </year>
Reference: [Chi94b] <author> Tzi-cker Chiueh. </author> <title> Sunder: A Programmable Hardware Prefetch Architecture for Numerical Loops. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 488-497, </pages> <month> November </month> <year> 1994. </year>
Reference: [CKP91] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year> <note> REFERENCES 23 </note>
Reference: [CMCH91] <author> William Y. Chen, Scott A. Mahlke, Pohua P. Chang, and Wen-mei W. Hwu. </author> <title> Data Access Microarchitectures for Superscalar Processors with Compiler-Assisted Data Prefetching. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Microarchi-tecture, </booktitle> <pages> pages 69-73, </pages> <month> November </month> <year> 1991. </year>
Reference: [CMH + 92] <author> William Y. Chen, Scott A. Mahlke, Wen-mei W. Hwu, Tokuzo Kiyohara, and Pohua P. Chang. </author> <title> Tolerating data access latency with register preloading. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference: [CR94] <author> Mark J. Charney and Anthony P. Reeves. </author> <title> Correlation-Based Hardware Prefetching. </title> <note> Submitted to IEEE Transactions on Computers, </note> <month> September </month> <year> 1994. </year>
Reference: [DDS93] <author> Fredrik Dahlgren, Michel Dubois, and Per Stenstrom. </author> <title> Fixed and Adaptive Sequential Prefetching in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 56-63, </pages> <month> August </month> <year> 1993. </year>
Reference: [DS95] <author> Fredrik Dahlgren and Per Stenstrom. </author> <title> Effectiveness of Hardware-Based Stride and Sequential Prefetching in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the first IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 68-77, </pages> <month> January </month> <year> 1995. </year>
Reference: [EV93] <author> Richard J. Eickemeyer and S. Vassiliadis. </author> <title> A load-instruction unit for pipelined processors. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 37(4) </volume> <pages> 547-564, </pages> <month> July </month> <year> 1993. </year>
Reference: [FP91] <author> John W. C. Fu and Janak H. Patel. </author> <title> Data prefetching in Multiprocessor Vector Cache Memories. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1991. </year>
Reference: [FPJ92] <author> John W. C. Fu, Janak H. Patel, and Bob L. Janssens. </author> <title> Stride Directed Prefetching in Scalar Processors. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 102-110, </pages> <month> December </month> <year> 1992. </year>
Reference: [GGV90] <author> Edward H. Gornish, Elana D. Granston, and Alexander V. Vei-denbaum. </author> <title> Compiler-directed data prefetching in multiprocessors with memory hierarchies. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <pages> pages 354-368, </pages> <institution> Department of Computer Science, Urbana, </institution> <address> IL 61801, </address> <month> June </month> <year> 1990. </year>
Reference: [Gor95] <author> Edward H. Gornish. </author> <title> Adaptive and integrated data cache prefetch-ing for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, Urbana, </institution> <address> IL 61801, </address> <month> January </month> <year> 1995. </year> <note> REFERENCES 24 </note>
Reference: [Gwe92] <author> Linley Gwennap. </author> <title> Microprocessor Developments Beyond 1995: Experts See Limits to Superscalar Benefits | Memory Becomes Paramount. </title> <type> Microprocessor Report, </type> <month> 9 December </month> <year> 1992. </year>
Reference-contexts: 1 INVENTION BACKGROUND 3 1 Invention Background The ever-increasing gap between microprocessor and memory speeds has been well documented <ref> [Gwe92, HP90] </ref>. Instruction and data cache hierarchies have become the principal means of bridging this speed discrepancy. Typically, first-level caches are small (8K to 32K bytes in size), direct mapped, and integrated on CPU chips.
Reference: [HP90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA 94403, </address> <year> 1990. </year>
Reference-contexts: 1 INVENTION BACKGROUND 3 1 Invention Background The ever-increasing gap between microprocessor and memory speeds has been well documented <ref> [Gwe92, HP90] </ref>. Instruction and data cache hierarchies have become the principal means of bridging this speed discrepancy. Typically, first-level caches are small (8K to 32K bytes in size), direct mapped, and integrated on CPU chips. <p> It is also distinguishable from previous designs in its seamless integration of linear and indirect address prefetching. The IRB operates on load instructions only, because they make up 20% to 40% of the instructions executed by typical programs 1 <ref> [CB94b, HP90] </ref>. Because the IRB acquires knowledge about individual load accesses exclusively at runtime, it should also be useful in prefetching data references in operating system and transaction processing codes. <p> These programs are characterized by large data footprints and high 1 Techniques such as write buffers and dirty line buffers are already very effective for store instructions <ref> [HP90] </ref>. 2 THE IRB | SOME ILLUSTRATIVE EXAMPLES 4 rates of process switching [MDO94], which cause high turnover of data in the primary cache. However, even in such programs, many data references are due to traversals of pointer-linked data structures. <p> line. 6 Determined by whatever techniques the CPU is using, including speculative execution. 4 IRB IMPLEMENTATION 17 4 IRB IMPLEMENTATION 18 5 IRB EVALUATION 19 4.1 Incorporating the IRB into a CPU pipeline Let us consider how the IRB might be integrated into the well known five stage RISC pipeline <ref> [HP90] </ref>. This pipeline is typical of early RISC processors, and is also a reasonable approximation to the pipelines used for individual function units in a modern superscalar CPU 7 .
Reference: [Jou90] <author> Norman P. Jouppi. </author> <title> Improving Direct-mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference: [JT93] <author> Ivan Jegou and Olivier Temam. </author> <title> Speculative Prefetching. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <pages> pages 57 - 66, </pages> <month> July </month> <year> 1993. </year>
Reference: [KL91] <author> Alexander C. Klaiber and Henry M. Levy. </author> <title> An architecture for software-controlled data prefetching. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 43-53, </pages> <month> May </month> <year> 1991. </year>
Reference: [LS84] <author> Johnny K. F. Lee and Alan J. Smith. </author> <title> Branch Prediction Strategies and Branch Target Buffer Design. </title> <journal> IEEE Computer, </journal> <volume> 17(1) </volume> <pages> 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: The RRU hardware resembles a Branch Target Buffer commonly used in pipelined processors to reduce branch misprediction penalty <ref> [LS84] </ref>. The RRU is indexed by virtual addresses of load instructions. Each RRU entry consists of several fields, the first of which is the instruction address. The second field is the virtual operand address last issued by this load instruction.
Reference: [LYL87] <author> Roland L. Lee, Pen-Chung Yew, and Duncan H. Lawrie. </author> <title> Data Prefetching in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 28-31, </pages> <month> August </month> <year> 1987. </year>
Reference: [McF92] <author> Scott McFarling. </author> <title> Cache Replacement with Dynamic Exclusion. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 191-200, </pages> <month> May </month> <year> 1992. </year>
Reference: [MDO94] <author> Ann Marie Grizzaffi Maynard, Colette M. Donnelly, and Bret R. Olszewski. </author> <title> Contrasting Characteristics and Cache Performance of Technical and Multi-User Commercial Workloads. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 145-156, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: While scientific codes are an important class of applications, modern CPUs need to efficiently execute a much wider spectrum of programs, including pointer-intensive and sparse matrix computations. The latter two categories of codes tend to exhibit much poorer cache locality than their regular, numeric counterparts <ref> [MDO94] </ref>, and are much less amenable to compile-time restructuring. The IRB extends data prefetching to memory address sequences (first order linear recurrences of indirection) that characterize symbolic and sparse matrix computation, while also handling dense scientific codes. <p> These programs are characterized by large data footprints and high 1 Techniques such as write buffers and dirty line buffers are already very effective for store instructions [HP90]. 2 THE IRB | SOME ILLUSTRATIVE EXAMPLES 4 rates of process switching <ref> [MDO94] </ref>, which cause high turnover of data in the primary cache. However, even in such programs, many data references are due to traversals of pointer-linked data structures. An IRB can detect these patterns, and generate prefetches for future accesses, thus turning prospective cache misses into hits.
Reference: [MG91] <author> Todd C. Mowry and Anoop Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference: [MLG92] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceed REFERENCES 25 ings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference: [PK94] <author> Subbarao Palacharla and Richard E. Kessler. </author> <title> Evaluating Stream Buffers as a Secondary Cache Replacement. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 24-33, </pages> <month> April </month> <year> 1994. </year>
Reference: [Sel92] <author> Charles William Selvidge. </author> <title> Compilation-Based Prefetching for Memory Latency Tolerance. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <address> Cambridge, MA 02139, </address> <month> May </month> <year> 1992. </year>
Reference: [SH92] <author> James E. Smith and Wei-Chung Hsu. </author> <title> Prefetching in Supercomputer Instruction Caches. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 588-597, </pages> <month> November </month> <year> 1992. </year>
Reference: [Skl92] <author> Ivan Sklenar. </author> <title> Prefetch unit for vector operations on scalar computers. </title> <journal> Computer Architecture News, </journal> <volume> 20(4) </volume> <pages> 31-37, </pages> <month> September </month> <year> 1992. </year>
Reference: [SMH94] <author> Rafael H. Saavedra, Weihua Mao, and Kai Hwang. </author> <title> Performance and Optimization of Data Prefetching Strategies in Scalable Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 427-448, </pages> <month> September </month> <year> 1994. </year>
Reference: [Smi78] <author> Alan Jay Smith. </author> <title> Sequential Program Prefetching in Memory Hierarchies. </title> <journal> IEEE Computer, </journal> <volume> 11(12) </volume> <pages> 7-21, </pages> <month> December </month> <year> 1978. </year>
Reference: [SR88] <author> Kimming So and Rudolph N. Rechtschaffen. </author> <title> Cache Operations by MRU Change. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(6) </volume> <pages> 700-709, </pages> <month> June </month> <year> 1988. </year>
Reference: [TE93] <author> Dean M. Tullsen and Susan J. Eggers. </author> <title> Limitations of Cache Prefetching on a Bus-Based Multiprocessor. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 278-288, </pages> <month> May </month> <year> 1993. </year>
Reference: [VS92] <author> Anujan Varma and Gunjan K. Sinha. </author> <title> A Class of Prefetch Schemes for On-Chip Data Caches. </title> <booktitle> Poster in Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference: [YGHH94] <author> Yoji Yamada, John Gyllenhall, Grant Haab, and Wen-mei W. Hwu. </author> <title> Data Relocation and Prefetching for Programs with Large Data Sets. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1994. </year>
Reference: [Zla91] <author> Zahari Zlatev. </author> <title> Computational Methods for General Sparse Matrices. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA 02061, </address> <year> 1991. </year>
Reference-contexts: Figure 1 illustrates this phenomenon. Numerical Fortran codes that operate on sparse matrices experience similar performance degradation, since these matrices are stored in memory as arrays with subscripted subscripts that contain only the non-zero elements <ref> [Zla91] </ref>. There isn't much we can do to a cache's design to affect the above phenomenon, since a conventional cache relies, in part, on spatial locality for its performance gains 2 . Thus, reducing the cache miss penalty via prefetching becomes a viable alternative.
References-found: 40

