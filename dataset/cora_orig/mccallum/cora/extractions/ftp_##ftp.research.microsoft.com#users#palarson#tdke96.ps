URL: ftp://ftp.research.microsoft.com/users/palarson/tdke96.ps
Refering-URL: http://www.research.microsoft.com/~palarson/
Root-URL: http://www.research.microsoft.com
Title: Speeding up External Mergesort  
Author: LuoQuan Zheng and Per -Ake Larson 
Keyword: Index terms: sorting, external sorting, mergesort, run placement, buffering strategy  
Abstract: External mergesort is normally implemented so that each run is stored contiguously on disk and blocks of data are read exactly in the order they are needed during merging. We investigate two ideas for improving the performance of external mergesort: interleaved layout and a new reading strategy. Interleaved layout places blocks from different runs in consecutive disk addresses. This is done in the hope that interleaving will reduce seek overhead during merging. The new reading strategy precomputes the order in which data blocks are to be read according to where they are located on disk and when they are needed for merging. Extra buffer space makes it possible to read blocks in an order that reduces seek overhead, instead of reading them exactly in the order they are needed for merging. A detailed simulation model was used to compare the two layout strategies and three reading strategies. The effects of using multiple work disks were also investigated. We found that, in most cases, interleaved layout does not improve performance, but that the new reading strategy consistently performs better than double buffering and forecasting. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aggarwal, A., Vitter, J.S., </author> <title> The Input/Output Complexity of Sorting and Related Problems, </title> <journal> Communications of the ACM, </journal> <volume> 31(9), </volume> <year> 1988, </year> <pages> pp. 1116-1127. 18 </pages>
Reference-contexts: Combining the outputs of all the processors gives the final results. Quinn [12], Iyer et al. [7] and Varman et al. [15] investigated algorithms for partitioning. Aggarwal and Vitter <ref> [1] </ref> showed that mergesort is an optimal external sorting method (up to a constant factor) in the total number of I/O operations required. They also studied the use of P disks to obtain I/O concurrency.
Reference: [2] <author> Akl, S.G., </author> <title> Parallel Sorting Algorithms, </title> <publisher> Academic Press Inc., </publisher> <address> Orlando, Florida, </address> <year> 1985. </year>
Reference-contexts: Her recommendation is to use all available memory for input buffers and to have two input buffers per run. Since sorting is such a time consuming task, exploiting parallelism is a natural next step. A good introduction to parallel sorting can be found in <ref> [2] </ref> and [5]. One of the strategies proposed to improve the performance of the merge phase is to have many processors organized into a tree.
Reference: [3] <author> Baer, J.L., </author> <title> Computer Systems Architecture, </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1980, </year> <pages> pp. 255-257. </pages>
Reference: [4] <author> Beck, M., Bitton, D., Wilkinson, </author> <title> W.K., Sorting Large Files on a Backend Multiprocessor, </title> <journal> IEEE Trans. Comput., </journal> <volume> 37, </volume> <year> 1988, </year> <pages> pp. 769-778. </pages>
Reference-contexts: The final result is obtained at the root. Parallelism is exploited both through pipelining merge steps between levels of the tree, and through concurrent merging performed by processors on the same level. Beck et al. <ref> [4] </ref> implemented this idea on a number of backend processors. Due to hardware limitations, very small buffers (1Kb) were used. They also studied different layout strategies. However, their findings are heavily influenced by the small buffer size.
Reference: [5] <author> Bitton, D., </author> <title> Design, Analysis, and Implementation of Parallel External Sorting Algorithms, </title> <type> Ph.D. dissertation, </type> <institution> Univ. Wisconsin-Madison, </institution> <type> TR 464, </type> <month> Jan. </month> <year> 1982. </year>
Reference-contexts: Her recommendation is to use all available memory for input buffers and to have two input buffers per run. Since sorting is such a time consuming task, exploiting parallelism is a natural next step. A good introduction to parallel sorting can be found in [2] and <ref> [5] </ref>. One of the strategies proposed to improve the performance of the merge phase is to have many processors organized into a tree.
Reference: [6] <author> Hennessy, J.L., and Patterson, D.A., </author> <title> Computer Architecture: a Quantitative Approach, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 1990 (pp. 557-559). </pages>
Reference: [7] <author> Iyer B.R., Ricard R. G., and Varman P.J., </author> <title> Percentile Finding Algorithm for Multiple Sorted Runs, </title> <booktitle> Proc. of the 15th Intl. Conf. on Very Large Data Bases, </booktitle> <address> Amsterdam, </address> <year> 1989, </year> <pages> pp. 135-144. </pages>
Reference-contexts: Another idea for parallelizing the merge phase is to split each sorted run into range-disjoint partitions, and have each processor merge data from a separate partition. Combining the outputs of all the processors gives the final results. Quinn [12], Iyer et al. <ref> [7] </ref> and Varman et al. [15] investigated algorithms for partitioning. Aggarwal and Vitter [1] showed that mergesort is an optimal external sorting method (up to a constant factor) in the total number of I/O operations required. They also studied the use of P disks to obtain I/O concurrency.
Reference: [8] <author> Knuth, D.E., </author> <booktitle> The Art of Computer Programming: </booktitle> <volume> Volume 3, </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, Ma, </address> <year> 1973. </year>
Reference-contexts: Files are often maintained sorted on a key attribute in order to facilitate searching and processing. External sorting refers to sorting more data than can be held in memory at one time. Mergesort <ref> [8] </ref> is the algorithm most commonly used for external sorting. Mergesort consists two phases: run formation and merging. In the run formation phase, the data to be sorted is divided into smaller sorted sets, called runs. Replacement selection [8] is the most popular run formation method because the runs produced are <p> Mergesort <ref> [8] </ref> is the algorithm most commonly used for external sorting. Mergesort consists two phases: run formation and merging. In the run formation phase, the data to be sorted is divided into smaller sorted sets, called runs. Replacement selection [8] is the most popular run formation method because the runs produced are expected to be twice the size of memory. The runs are written to external storage as they are produced. <p> There is a vast literature on internal sorting, but less on external sorting. Early studies of external sorting focused on using tape as the secondary storage device. Knuth <ref> [8] </ref> provides extensive coverage of the fundamentals of sorting. Kwan and Baer [9] studied the I/O performance of k-way mergesort. Their disk model assumes that seek time is proportional to the seek distance and rotational latency of half a revolution is charged to every disk access. <p> The merge process requires at least one buffer per run. If no extra buffers are used, I/O cannot be overlapped at all with processing. Reading is stopped until some buffer becomes empty and merging has to wait until more data has been brought in. Knuth <ref> [8] </ref> suggests the forecasting technique for reading. Forecasting uses one extra buffer. When one block from each run resides in memory, the last key of each block can be be extracted and the keys compared to determine which block will be emptied first.
Reference: [9] <author> Kwan, S.C., Baer, J.L., </author> <title> The I/O Performance of Multiway Mergesort and Tag Sort, </title> <journal> IEEE Trans. Comput., </journal> <note> C-34 Special Issue on Sorting, </note> <year> 1985, </year> <pages> pp. 383-387. </pages>
Reference-contexts: There is a vast literature on internal sorting, but less on external sorting. Early studies of external sorting focused on using tape as the secondary storage device. Knuth [8] provides extensive coverage of the fundamentals of sorting. Kwan and Baer <ref> [9] </ref> studied the I/O performance of k-way mergesort. Their disk model assumes that seek time is proportional to the seek distance and rotational latency of half a revolution is charged to every disk access. The asymptotic I/O complexity of mergesort is studied using this model.
Reference: [10] <author> Nodine, M.H., Vitter, J.S, </author> <title> Greed Sort: An Optimal External Sorting Algorithm for Multiple Disks, </title> <institution> Departmentof Computer Science, Brown University, </institution> <type> Technical Report CS-90-04, </type> <month> February </month> <year> 1990. </year>
Reference-contexts: Vitter and Shriver [17] extended the analysis to the case where the P blocks in a parallel operation are stored on different disks. Under this model, an optimal deterministic algorithm is presented in <ref> [10] </ref>.
Reference: [11] <author> Pai, </author> <title> V.S., Varman P.J. Prefetching with Multiple Disks for External Mergesort: Simulation and Analysis, </title> <booktitle> Eighth International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1992, </year> <pages> pp. 273-282. </pages>
Reference-contexts: While the last pass only adds a constant factor to the number of I/O operations, it transfers the whole set of data in and out of main memory again. A more practical approach is to use more buffer space to hold the data blocks in memory. Pai and Varman <ref> [11] </ref> used a Markov chain to model prefetching in a multiple-disk environment. Only the merge phase is studied. Their study is mostly analytical, and the number of I/O operations is the only cost measured. <p> There are several ways to distribute the data blocks over a set of unsynchronized disks. One alternative is to keep the data blocks from each run together and distribute the runs among the disks. This layout is used in <ref> [11] </ref>. In the merge phase, data blocks from different runs can be read in parallel. In addition, there are fewer runs on each disk so fewer seeks are needed. However, there is no parallelism in the write phase.
Reference: [12] <author> Quinn, M.J., </author> <title> Parallel Sorting Algorithms for Tightly Coupled Multiprocessors, Parallel Computing, </title> <publisher> North-Holland, </publisher> <year> 1988, </year> <pages> pp. 349-357. </pages>
Reference-contexts: Another idea for parallelizing the merge phase is to split each sorted run into range-disjoint partitions, and have each processor merge data from a separate partition. Combining the outputs of all the processors gives the final results. Quinn <ref> [12] </ref>, Iyer et al. [7] and Varman et al. [15] investigated algorithms for partitioning. Aggarwal and Vitter [1] showed that mergesort is an optimal external sorting method (up to a constant factor) in the total number of I/O operations required.
Reference: [13] <author> Salzberg, B., </author> <title> Merging Sorted Runs Using Large Main Memory, </title> <journal> Acta Informatica, </journal> <volume> 27, </volume> <pages> 1989 pp. 195-215. </pages>
Reference-contexts: This can be viewed as a first approximation of an extent based file system. Furthermore, we assume that the sort can be completed in a single merge pass. Given today's main memory sizes and the use of disks, it is hardly ever necessary to use multiple merge passes <ref> [13] </ref>. The rest of the paper is organized as follows. The next section gives a brief summary of previous work of direct relevance to this study. In Section 3, the traditional mergesort algorithm is analyzed. Areas of inefficiency are identified, and several ideas for improvement are outlined. <p> His analysis favours small buffers, because the speedup obtained by increasing the buffer size is not enough to compensate for the additional space requirement. Both Kwan and Baer, and Verkamo assumed that the block transfer time is much smaller than the rotational delay and seek time. Salzberg <ref> [13] </ref> pointed out that the availability of large main memory makes this assumption obsolete. She gave an example where the buffer size is 1Mb. This makes the disk overhead only 7.5% of the transfer time, even if the average seek time and rotational latency is charged to every disk access. <p> The extra buffer is then used for reading the next block from that run while the CPU is working on the data already in main memory. The forecasting technique cannot perfectly overlap processing and read time. Salzberg <ref> [13] </ref> strongly advocates using two input buffers for each run to achieve better overlap of processing and reading. First one block from each run is brought into memory. Then the merge process is started and the second block from each run is read at the same time. <p> can wait until several blocks have been written and use the memory freed up. 5 Comparison of Different Strategies When evaluating the I/O performance of mergesort, many researchers simply estimate the average time for a disk access and use this as the cost for each disk access (see e.g. [16] <ref> [13] </ref>). This type of analysis gives rather crude estimates. In reality, the I/O time depends on where data blocks are located on disk and in what order they are read.
Reference: [14] <author> Salzberg, B., Tsukerman, A., Gray, J., Stewart, M., Uren, S., Vaughan, B,., FastSort: </author> <title> A Distributed Single-Input Single-Output External Sort, </title> <booktitle> Proc. of the 1990 ACM SIGMOD Intl. Conf. on Management of Data, </booktitle> <month> May, </month> <year> 1990, </year> <pages> pp. 94-101. </pages>
Reference-contexts: Beck et al. [4] implemented this idea on a number of backend processors. Due to hardware limitations, very small buffers (1Kb) were used. They also studied different layout strategies. However, their findings are heavily influenced by the small buffer size. Salzberg et al. <ref> [14] </ref> also studied this idea applied to a network of loosely-coupled processors where each processor has local disks and a large amount of main memory. They recommend using enough memory and processors so that only one merge pass is needed at each processor.
Reference: [15] <author> Varman, P.J., Scheufler, S.D., Iyer, B.R., Ricard, </author> <title> G.R., Merging Multiple lists on Hierarchical-Memory Multiprocessors, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12, </volume> <year> 1991, </year> <pages> pp. 171-177. </pages>
Reference-contexts: Another idea for parallelizing the merge phase is to split each sorted run into range-disjoint partitions, and have each processor merge data from a separate partition. Combining the outputs of all the processors gives the final results. Quinn [12], Iyer et al. [7] and Varman et al. <ref> [15] </ref> investigated algorithms for partitioning. Aggarwal and Vitter [1] showed that mergesort is an optimal external sorting method (up to a constant factor) in the total number of I/O operations required. They also studied the use of P disks to obtain I/O concurrency.
Reference: [16] <author> Verkamo, </author> <title> A.I., Perforance Comparison of Distributive and Mergesort as External Sorting Algorithms, </title> <journal> Journal of Systems and Software, </journal> <volume> 10, </volume> <year> 1989, </year> <pages> pp. 187-200. 19 </pages>
Reference-contexts: The asymptotic I/O complexity of mergesort is studied using this model. They assume that seek time and rotational latency together dominate the transfer time. Their conclusion is that minimizing the number of merge passes by selecting k as large as possible does not always give the best performance. Verkamo <ref> [16] </ref> compared the performance of mergesort and distributive sorting. He also pointed out that the internal sort phase and the external merge phase need not use the same amount of memory. A space-time integral was used as a measure of performance. <p> we can wait until several blocks have been written and use the memory freed up. 5 Comparison of Different Strategies When evaluating the I/O performance of mergesort, many researchers simply estimate the average time for a disk access and use this as the cost for each disk access (see e.g. <ref> [16] </ref> [13]). This type of analysis gives rather crude estimates. In reality, the I/O time depends on where data blocks are located on disk and in what order they are read.
Reference: [17] <author> Vitter, J.S., Shriver, </author> <title> E.A., Optimal Disk I/O with Parallel Block Transfer Symposium on the Theory of Computing, </title> <month> May, </month> <pages> 1990 pp. 159-169 </pages>
Reference-contexts: They also studied the use of P disks to obtain I/O concurrency. However, they made the unrealistic assumption that any set of P blocks can be fetched in one parallel operation. Vitter and Shriver <ref> [17] </ref> extended the analysis to the case where the P blocks in a parallel operation are stored on different disks. Under this model, an optimal deterministic algorithm is presented in [10].
Reference: [18] <author> Zheng, </author> <title> L.Q., Speeding up External Mergesort, </title> <type> Master's thesis, </type> <institution> University of Waterloo, </institution> <year> 1992 </year>
References-found: 18

