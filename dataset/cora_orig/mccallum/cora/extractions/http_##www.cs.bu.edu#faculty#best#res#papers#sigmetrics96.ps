URL: http://www.cs.bu.edu/faculty/best/res/papers/sigmetrics96.ps
Refering-URL: http://www.cs.wustl.edu/~jxh/research/related.html
Root-URL: 
Email: fcrovella,bestg@cs.bu.edu  
Title: Self-Similarity in World Wide Web Traffic Evidence and Possible Causes  
Author: Mark E. Crovella and Azer Bestavros 
Date: May 1996  
Address: Systems,Philadelphia, PA,  Boston, ma 02215  
Affiliation: of Computer  Computer Science Department Boston University  
Note: In Proc. of the 1996 ACM SIGMETRICS Intl. Conference on Measurement and Modeling  
Abstract: Recently the notion of self-similarity has been shown to apply to wide-area and local-area network traffic. In this paper we examine the mechanisms that give rise to the self-similarity of network traffic. We present a hypothesized explanation for the possible self-similarity of traffic by using a particular subset of wide area traffic: traffic due to the World Wide Web (WWW). Using an extensive set of traces of actual user executions of NCSA Mosaic, reflecting over half a million requests for WWW documents, we examine the dependence structure of WWW traffic. While our measurements are not conclusive, we show evidence that WWW traffic exhibits behavior that is consistent with self-similar traffic models. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user "think time", and the superimposition of many such transfers in a local area network. To do this we rely on empirically measured distributions both from our traces and from data independently collected at over thirty WWW sites. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Martin F. Arlitt and Carey L. Williamson. </author> <title> Web server work load characterization: The search for invariants. </title> <booktitle> In Proceedings of the 1996 SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Previous wide-area traffic studies have studied FTP, TELNET, NNTP, and SMTP traffic [19, 20]. Our data complements those studies by providing a view of WWW (HTTP) traffic at a "stub" network. In addition, our measurements of Web file sizes are in general agreement with those reported in <ref> [1] </ref>. Since WWW traffic accounts for more than 25% of the traffic on the Internet and is currently growing more rapidly than any other traffic type [12], understanding the nature of WWW traffic is important and is expected to increase in importance.
Reference: [2] <author> Jan Beran. </author> <title> Statistics for Long-Memory Processes. Monographs on Statistics and Applied Probability. </title> <publisher> Chapman and Hall, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: In addition, using our measurements of user inter-request times, we explore reasons for the heavy-tailed distribution of quiet times needed for self-similarity. 2 Background 2.1 Definition of Self-Similarity For detailed discussion of self-similarity in timeseries data and the accompanying statistical tests, see <ref> [2, 27] </ref>. Our discussion in this subsection and the next closely follows those sources. <p> As H ! 1, the degree of self-similarity increases. Thus the fundamental test for self-similarity of a series reduces to the question of whether H is significantly different from 1=2. In this paper we use four methods to test for self-similarity. These methods are described fully in <ref> [2] </ref> and are the same methods described and used in [16]. A summary of the relative accuracy of these methods on synthetic datasets is presented in [24]. The first method, the variance-time plot, relies on the slowly decaying variance of a self-similar series. <p> The two forms that are most commonly used are fractional Gaussian noise (FGN) with parameter 1=2 &lt; H &lt; 1, and Fractional ARIMA (p; d; q) with 0 &lt; d &lt; 1=2 (for details see <ref> [2, 4] </ref>). These two models differ in their assumptions about the short-range dependences in the datasets; FGN assumes no short-range dependence while Fractional ARIMA can assume a fixed degree of short-range dependence.
Reference: [3] <author> T. Berners-Lee, L. Masinter, and M.McCahill. </author> <title> Uniform re source locators. </title> <type> RFC 1738, </type> <month> December </month> <year> 1994. </year>
Reference-contexts: A complete description of our data collection methods and the format of the log files is given in [8]; here we only give a high-level summary. We modified Mosaic to record the Uniform Resource Locator (URL) <ref> [3] </ref> of each file accessed by the Mosaic user, as well as the time the file was accessed and the time required to transfer the file from its server (if necessary).
Reference: [4] <author> Peter J. Brockwell and Richard A. Davis. </author> <title> Time Series: Theory and Methods. </title> <booktitle> Springer Series in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> second edition, </address> <year> 1991. </year>
Reference-contexts: The two forms that are most commonly used are fractional Gaussian noise (FGN) with parameter 1=2 &lt; H &lt; 1, and Fractional ARIMA (p; d; q) with 0 &lt; d &lt; 1=2 (for details see <ref> [2, 4] </ref>). These two models differ in their assumptions about the short-range dependences in the datasets; FGN assumes no short-range dependence while Fractional ARIMA can assume a fixed degree of short-range dependence.
Reference: [5] <author> Lara D. Catledge and James E. Pitkow. </author> <title> Characterizing brows ing strategies in the World-Wide Web. </title> <booktitle> In Proceedings of the Third WWW Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Previous measurement studies of the Web have focused on reference patterns established based on logs of proxies [10, 23], or servers [21]. The authors in <ref> [5] </ref> captured client traces, but they concentrated on events at the user interface level in order to study browser and page design. In contrast, our goal in data collection was to acquire a complete picture of the reference behavior and timing of user accesses to the WWW.
Reference: [6] <institution> Netscape Communications Corp. </institution> <note> Netscape Navigator software. Available from http://www.netscape.com. </note>
Reference-contexts: At the time of our study (January and February 1995) Mosaic was the WWW browser preferred by nearly all users at our site. Hence our data consists of nearly all of the WWW traffic at our site. Since the time of our study, the preferred browser has become Netscape <ref> [6] </ref>, which is not available in source form. As a result, capturing an equivalent set of WWW user traces at the current time would be significantly more difficult. The data captured consists of the sequence of WWW file requests performed during each session.
Reference: [7] <author> Mark E. Crovella and Azer Bestavros. </author> <title> Explaining world wide web traffic self-similarity. </title> <type> Technical Report TR-95-015 (Re-vised), </type> <institution> Boston University Department of Computer Science, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: Finally, we consider the potential effects of caching in general, and Mosaic caching in particular. To evaluate the potential effects of caching in general, we used our traces to measure the relationship between the number of times any particular document is accessed and the size of the document. In <ref> [7] </ref>, we showed that there is an inverse correlation between file size and file reuse. This relationship suggests that systems that perform caching on WWW objects will tend to increase the tail weight of the data traffic resulting from misses in the cache as compared to the traffic without caching. <p> In <ref> [7] </ref>, we present analyses similar to those in this paper showing that OFF times exhibit two regimes. The important regime is determined by user behavior and appears to exhibit heavy-tailed characteristics with ff approximately 1.5.
Reference: [8] <author> Carlos R. Cunha, Azer Bestavros, and Mark E. Crovella. </author> <title> Char acteristics of WWW client-based traces. </title> <type> Technical Report BU-CS-95-010, </type> <institution> Boston University Computer Science Department, </institution> <year> 1995. </year>
Reference-contexts: As a result, we collected a large dataset of client-based traces. A full description of our traces (which are available for anonymous FTP) is given in <ref> [8] </ref>. Previous wide-area traffic studies have studied FTP, TELNET, NNTP, and SMTP traffic [19, 20]. Our data complements those studies by providing a view of WWW (HTTP) traffic at a "stub" network. In addition, our measurements of Web file sizes are in general agreement with those reported in [1]. <p> The browser we used was Mosaic, since its source was publicly available and permission has been granted for using and modifying the code for research purposes. A complete description of our data collection methods and the format of the log files is given in <ref> [8] </ref>; here we only give a high-level summary. We modified Mosaic to record the Uniform Resource Locator (URL) [3] of each file accessed by the Mosaic user, as well as the time the file was accessed and the time required to transfer the file from its server (if necessary). <p> In fact, in other work <ref> [8] </ref> we show that the rule known as Zipf's Law (degree of popularity is exactly inversely proportional to rank of popularity) applies 3 This conclusion is supported by recent work by Taqqu, which shows that the value of Hurst parameter H is determined by whichever distribution is heavier-tailed.[25] quite strongly to
Reference: [9] <institution> National Center for Supercomputing Applications. </institution> <note> Mosaic soft ware. Available at ftp://ftp.ncsa.uiuc.edu/Mosaic. </note>
Reference-contexts: To study the traffic patterns of the WWW we collected reference data reflecting actual WWW use at our site. We instrumented NCSA Mosaic <ref> [9] </ref> to capture user access patterns to the Web.
Reference: [10] <author> Steven Glassman. </author> <title> A Caching Relay for the World Wide Web. </title> <booktitle> In First International Conference on the World-Wide Web, CERN, </booktitle> <address> Geneva (Switzerland), May 1994. </address> <publisher> Elsevier Science. </publisher>
Reference-contexts: Previous measurement studies of the Web have focused on reference patterns established based on logs of proxies <ref> [10, 23] </ref>, or servers [21]. The authors in [5] captured client traces, but they concentrated on events at the user interface level in order to study browser and page design.
Reference: [11] <author> B. M. Hill. </author> <title> A simple general approach to inference about the tail of a distribution. </title> <journal> The Annals of Statistics, </journal> <volume> 3 </volume> <pages> 1163-1174, </pages> <year> 1975. </year>
Reference-contexts: In all our ff estimates for file sizes we use = 1000 meaning that we consider tails to be the portions of the distributions for files of 1,000 bytes or greater. An alternative approach to estimating tail weight, used in [28], is the Hill estimator <ref> [11] </ref>. The Hill estimator does not give a single estimate of ff, but can be used to gauge the general range of ffs that are reasonable.
Reference: [12] <institution> Merit Network Inc. NSF Network statistics. </institution> <note> Available at ftp: //nis.nsf.net/statistics/nsfnet/, </note> <month> December </month> <year> 1994. </year>
Reference-contexts: In addition, our measurements of Web file sizes are in general agreement with those reported in [1]. Since WWW traffic accounts for more than 25% of the traffic on the Internet and is currently growing more rapidly than any other traffic type <ref> [12] </ref>, understanding the nature of WWW traffic is important and is expected to increase in importance. The benchmark study of self-similarity in network traffic is [14, 16], and our study uses many of the same methods used therein.
Reference: [13] <author> Gordon Irlam. </author> <title> Unix file size survey | 1993. </title> <note> Available at http://www.base.com/gordoni/ufs93.html, September 1994. </note>
Reference-contexts: A number of questions are raised by this study. First, the generalization from Web traffic to aggregated wide-area traffic is not obvious. While other authors have noted the heavy-tailed distribution of Unix files <ref> [13] </ref> and of FTP transfers [20], extending our approach to wide-area traffic in general is difficult because of the many sources of traffic and determiners of traffic demand. A second question concerns the amount of demand required to observe self-similarity in a traffic series.
Reference: [14] <author> W. Leland, M. Taqqu, W. Willinger, and D. Wilson. </author> <title> On the self-similar nature of Ethernet traffic. </title> <booktitle> In Proceedings of SIG-COMM '93, </booktitle> <pages> pages 183-193, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The benchmark study of self-similarity in network traffic is <ref> [14, 16] </ref>, and our study uses many of the same methods used therein. However, the goal of that study was to demonstrate the self-similarity of network traffic; to do that, many large datasets taken from a multi-year span were used. <p> While these four hours are self-similar, many less-busy hours in our logs do not show self-similar characteristics. We feel that this is only the result of the traffic demand present in our logs, which is much lower than that used in <ref> [14, 16] </ref>; this belief is supported by the findings in that study, which showed that the intensity of self-similarity increases as the aggregate traffic level increases. Our work is most similar in intent to [28].
Reference: [15] <author> W. E. Leland and D. V. Wilson. </author> <title> High time-resolution mea surement and analysis of LAN traffic: Implications for LAN interconnection. </title> <booktitle> In Proceeedings of IEEE Infocomm '91, </booktitle> <pages> pages 1360-1366, </pages> <address> Bal Harbour, FL, </address> <year> 1991. </year>
Reference-contexts: Surprisingly (given the counterintuitive aspects of long-range dependence) the self-similarity of Ethernet network traffic has been rigorously established [16]. The importance of long-range dependence in network traffic is beginning to be observed in studies such as <ref> [15] </ref>, which show that packet loss and delay behavior is radically different in simulations using real traffic data rather than traditional network models. However, the reasons behind network traffic self-similarity have not been clearly identified.
Reference: [16] <author> W.E. Leland, M.S. Taqqu, W. Willinger, and D.V. Wilson. </author> <title> On the self-similar nature of Ethernet traffic (extended version). </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2 </volume> <pages> 1-15, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Understanding the nature of network traffic is critical in order to properly design and implement computer networks and network services like the World Wide Web. Recent examinations of LAN traffic <ref> [16] </ref> and wide area network traffic [20] have challenged the commonly assumed models for network traffic, e.g., the Poisson distribution. <p> Since a self-similar process has observable bursts on all time scales, it exhibits long-range dependence; values at any instant are typically correlated with all future values. Surprisingly (given the counterintuitive aspects of long-range dependence) the self-similarity of Ethernet network traffic has been rigorously established <ref> [16] </ref>. The importance of long-range dependence in network traffic is beginning to be observed in studies such as [15], which show that packet loss and delay behavior is radically different in simulations using real traffic data rather than traditional network models. <p> To bridge the gap between studying network traffic on one hand and high-level system characteristics on the other, we make use of two essential tools. First, to explain self-similar network traffic in terms of individual transmission lengths, we employ the mechanism introduced in [17] and described in <ref> [16] </ref>. Those papers point out that self-similar traffic can be constructed by multiplexing a large number of ON/OFF sources that have ON and OFF period lengths that are heavy-tailed, as defined in Section 2.2. <p> The paper takes two parts. First, we consider the possibility of self-similarity of Web traffic for the busiest hours we measured. To do so we use analyses very similar to those performed in <ref> [16] </ref>. These analyses support the notion that Web traffic may show self-similar characteristics, at least when demand is high enough. This result in itself has implications for designers of systems that attempt to improve performance characteristics of the WWW. <p> Thus the fundamental test for self-similarity of a series reduces to the question of whether H is significantly different from 1=2. In this paper we use four methods to test for self-similarity. These methods are described fully in [2] and are the same methods described and used in <ref> [16] </ref>. A summary of the relative accuracy of these methods on synthetic datasets is presented in [24]. The first method, the variance-time plot, relies on the slowly decaying variance of a self-similar series. <p> The benchmark study of self-similarity in network traffic is <ref> [14, 16] </ref>, and our study uses many of the same methods used therein. However, the goal of that study was to demonstrate the self-similarity of network traffic; to do that, many large datasets taken from a multi-year span were used. <p> While these four hours are self-similar, many less-busy hours in our logs do not show self-similar characteristics. We feel that this is only the result of the traffic demand present in our logs, which is much lower than that used in <ref> [14, 16] </ref>; this belief is supported by the findings in that study, which showed that the intensity of self-similarity increases as the aggregate traffic level increases. Our work is most similar in intent to [28]. <p> The estimates of H given by these plots are in the range 0.7 to 0.8, consistent with the values for a lightly loaded network measured in <ref> [16] </ref>. Moving from the busier hours to the less-busy hours, the estimates of H seem to decline somewhat, and the variance in the estimate of H increases, which are also conclusions consistent with previous research. <p> This section provides a possible explanation, based on measured characteristics of the Web. 5.1 Superimposing Heavy-Tailed Renewal Processes Our starting point is the method of constructing self-similar processes described by Mandelbrot [17] and Taqqu and Levy [26] and summarized in <ref> [16] </ref>. A self-similar process may be constructed by superimposing many simple renewal reward processes, in which the rewards are restricted to the values 0 and 1, and in which the inter-renewal times are heavy-tailed.
Reference: [17] <author> Benoit B. Mandelbrot. </author> <title> Long-run linearity, locally Gaussian processes, H-spectra and infinite variances. </title> <journal> Intern. Econom. Rev., </journal> <volume> 10 </volume> <pages> 82-113, </pages> <year> 1969. </year>
Reference-contexts: To bridge the gap between studying network traffic on one hand and high-level system characteristics on the other, we make use of two essential tools. First, to explain self-similar network traffic in terms of individual transmission lengths, we employ the mechanism introduced in <ref> [17] </ref> and described in [16]. Those papers point out that self-similar traffic can be constructed by multiplexing a large number of ON/OFF sources that have ON and OFF period lengths that are heavy-tailed, as defined in Section 2.2. <p> This section provides a possible explanation, based on measured characteristics of the Web. 5.1 Superimposing Heavy-Tailed Renewal Processes Our starting point is the method of constructing self-similar processes described by Mandelbrot <ref> [17] </ref> and Taqqu and Levy [26] and summarized in [16]. A self-similar process may be constructed by superimposing many simple renewal reward processes, in which the rewards are restricted to the values 0 and 1, and in which the inter-renewal times are heavy-tailed.
Reference: [18] <author> Benoit B. Mandelbrot. </author> <title> The Fractal Geometry of Nature. </title> <editor> W. H. </editor> <publisher> Freedman and Co., </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: The heavy-tailed distribution of file sizes we have observed seems similar in spirit to Pareto distributions noted in the social sciences, such as the distribution of lengths of books on library shelves, and the distribution of word lengths in sample texts (for a discussion of these effects, see <ref> [18] </ref> and citations therein).
Reference: [19] <author> Vern Paxson. </author> <title> Empirically-derived analytic models of wide-area TCP connections. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2(4) </volume> <pages> 316-336, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: used the Hill estimator to check that the estimates of ff obtained using the LLCD method were within range; in all cases they were. 2.2.1 Testing for Infinite Variance There is evidence that, over their entire range, many of the distributions we study may be well characterized using lognormal distributions <ref> [19] </ref>. However, lognormal distributions do not have infinite variance, and hence are not heavy-tailed. In our work, we are not concerned with distributions over their entire range|only their tails. As a result we don't use goodness-of-fit tests to determine whether Pareto or lognormal distributions are better at describing our data. <p> As a result, we collected a large dataset of client-based traces. A full description of our traces (which are available for anonymous FTP) is given in [8]. Previous wide-area traffic studies have studied FTP, TELNET, NNTP, and SMTP traffic <ref> [19, 20] </ref>. Our data complements those studies by providing a view of WWW (HTTP) traffic at a "stub" network. In addition, our measurements of Web file sizes are in general agreement with those reported in [1].
Reference: [20] <author> Vern Paxson and Sally Floyd. </author> <title> Wide-area traffic: The failure of poisson modeling. </title> <booktitle> In Proceedings of SIGCOMM '94, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction Understanding the nature of network traffic is critical in order to properly design and implement computer networks and network services like the World Wide Web. Recent examinations of LAN traffic [16] and wide area network traffic <ref> [20] </ref> have challenged the commonly assumed models for network traffic, e.g., the Poisson distribution. Were traffic to follow a Poisson or Markovian arrival process, it would have a characteristic burst length which would tend to be smoothed by averaging over a long enough time scale. <p> As a result, we collected a large dataset of client-based traces. A full description of our traces (which are available for anonymous FTP) is given in [8]. Previous wide-area traffic studies have studied FTP, TELNET, NNTP, and SMTP traffic <ref> [19, 20] </ref>. Our data complements those studies by providing a view of WWW (HTTP) traffic at a "stub" network. In addition, our measurements of Web file sizes are in general agreement with those reported in [1]. <p> Interestingly, the authors in <ref> [20] </ref> found that the upper tail of the distribution of data bytes in FTP bursts was well fit to a Pareto distribution with 0:9 ff 1:1. <p> A number of questions are raised by this study. First, the generalization from Web traffic to aggregated wide-area traffic is not obvious. While other authors have noted the heavy-tailed distribution of Unix files [13] and of FTP transfers <ref> [20] </ref>, extending our approach to wide-area traffic in general is difficult because of the many sources of traffic and determiners of traffic demand. A second question concerns the amount of demand required to observe self-similarity in a traffic series.
Reference: [21] <author> James E. Pitkow and Margaret M. Recker. </author> <title> A Simple Yet Ro bust Caching Algorithm Based on Dynamic Access Patterns. </title> <booktitle> In Electronic Proc. of the 2nd WWW Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Previous measurement studies of the Web have focused on reference patterns established based on logs of proxies [10, 23], or servers <ref> [21] </ref>. The authors in [5] captured client traces, but they concentrated on events at the user interface level in order to study browser and page design. <p> The heavy-tailed distribution of user think times also seems to be a feature of human information processing (e.g., <ref> [21] </ref>).
Reference: [22] <institution> Regents of the University of California. </institution> <note> www-stat 1.0 software. Available from http://www.ics.uci.edu/WebSoft/wwwstat/. </note>
Reference-contexts: For comparison purposes we surveyed 32 Web servers scattered throughout North America. These servers were chosen because they provided a usage report based on www-stat 1.0 <ref> [22] </ref>. These usage reports provide information sufficient to determine the distribution of file sizes on the server (for files accessed during the reporting period). In each case we obtained the most recent usage reports (as of July 1995), for an entire month if possible.
Reference: [23] <author> Jeff Sedayao. </author> <title> "Mosaic Will Kill My Network!" Studying Net work Traffic Patterns of Mosaic Use. </title> <booktitle> In Electronic Proceedings of the Second World Wide Web Conference '94: Mosaic and the Web, </booktitle> <address> Chicago, Illinois, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Previous measurement studies of the Web have focused on reference patterns established based on logs of proxies <ref> [10, 23] </ref>, or servers [21]. The authors in [5] captured client traces, but they concentrated on events at the user interface level in order to study browser and page design.
Reference: [24] <author> M. S. Taqqu, V. Teverovsky, and W. Willinger. </author> <title> Estimators for long-range dependence: an empirical study, 1995. </title> <type> Preprint. </type>
Reference-contexts: In this paper we use four methods to test for self-similarity. These methods are described fully in [2] and are the same methods described and used in [16]. A summary of the relative accuracy of these methods on synthetic datasets is presented in <ref> [24] </ref>. The first method, the variance-time plot, relies on the slowly decaying variance of a self-similar series.
Reference: [25] <author> Murad Taqqu. </author> <type> Personal communication. </type>
Reference: [26] <author> Murad S. Taqqu and Joshua B. Levy. </author> <title> Using renewal processes to generate long-range dependence and high variability. </title> <editor> In Ernst Eberlein and Murad S. Taqqu, editors, </editor> <booktitle> Dependence in Probability and Statistics, </booktitle> <pages> pages 73-90. </pages> <publisher> Birkhauser, </publisher> <year> 1986. </year>
Reference-contexts: This section provides a possible explanation, based on measured characteristics of the Web. 5.1 Superimposing Heavy-Tailed Renewal Processes Our starting point is the method of constructing self-similar processes described by Mandelbrot [17] and Taqqu and Levy <ref> [26] </ref> and summarized in [16]. A self-similar process may be constructed by superimposing many simple renewal reward processes, in which the rewards are restricted to the values 0 and 1, and in which the inter-renewal times are heavy-tailed.
Reference: [27] <author> Walter Willinger, Murad S. Taqqu, Will E. Leland, and Daniel V. Wilson. </author> <title> Self-similarity in high-speed packet traffic: Analysis and modeling of Ethernet traffic measurements. </title> <journal> Statistical Science, </journal> <volume> 10(1) </volume> <pages> 67-85, </pages> <year> 1995. </year>
Reference-contexts: In addition, using our measurements of user inter-request times, we explore reasons for the heavy-tailed distribution of quiet times needed for self-similarity. 2 Background 2.1 Definition of Self-Similarity For detailed discussion of self-similarity in timeseries data and the accompanying statistical tests, see <ref> [2, 27] </ref>. Our discussion in this subsection and the next closely follows those sources.
Reference: [28] <author> Walter Willinger, Murad S. Taqqu, Robert Sherman, and Daniel V. Wilson. </author> <title> Self-similarity through high-variability: Statistical analysis of Ethernet LAN traffic at the source level. </title> <booktitle> In Proceedings of SIGCOMM '95, </booktitle> <pages> pages 100-113, </pages> <address> Boston, MA, </address> <year> 1995. </year>
Reference-contexts: In all our ff estimates for file sizes we use = 1000 meaning that we consider tails to be the portions of the distributions for files of 1,000 bytes or greater. An alternative approach to estimating tail weight, used in <ref> [28] </ref>, is the Hill estimator [11]. The Hill estimator does not give a single estimate of ff, but can be used to gauge the general range of ffs that are reasonable. <p> Our work is most similar in intent to <ref> [28] </ref>. That paper looked at network traffic at the packet level, identified the flows between individual source/destination pairs, and showed that transmission and idle times for those flows were heavy-tailed. In contrast, our paper is based on data collected at the application level rather than the network level. <p> As a result we are able to examine the relationship between transmission times and file sizes, and are able to assess the effects of caching and user preference on these distributions. These observations allow us to build on the conclusions presented in <ref> [28] </ref> by showing that the heavy-tailed nature of transmission and idle times is not primarily a result of network protocols or user preference, but rather stems from more basic properties of information storage and processing: both file sizes and user "think times" are themselves strongly heavy-tailed. 4 Examining Web Traffic Self-Similarity
References-found: 28

