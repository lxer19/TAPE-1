URL: ftp://psyche.mit.edu/pub/jordan/recur.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/jordan.html
Root-URL: 
Email: ftommi,jordang@psyche.mit.edu  
Title: Recursive algorithms for approximating probabilities in graphical models  
Author: Tommi S. Jaakkola and Michael I. Jordan 
Address: Cambridge, MA 02139  
Affiliation: Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  
Abstract: MIT Computational Cognitive Science Technical Report 9604 Abstract We develop a recursive node-elimination formalism for efficiently approximating large probabilistic networks. No constraints are set on the network topologies. Yet the formalism can be straightforwardly integrated with exact methods whenever they are/become applicable. The approximations we use are controlled: they maintain consistently upper and lower bounds on the desired quantities at all times. We show that Boltzmann machines, sigmoid belief networks, or any combination (i.e., chain graphs) can be handled within the same framework. The accuracy of the methods is verified exper imentally.
Abstract-found: 1
Intro-found: 1
Reference: <author> P. Dayan, G. Hinton, R. Neal, and R. </author> <title> Zemel (1995). The Helmholtz machine. </title> <booktitle> Neural Computation 7: </booktitle> <pages> 889-904. </pages>
Reference: <author> S. L. </author> <title> Lauritzen (1996). Graphical Models. </title> <publisher> Oxford: Ox ford University Press. </publisher>
Reference-contexts: 1 Introduction Graphical models <ref> (see, e.g., Lauritzen 1996) </ref> provide a medium for rigorously embedding domain knowledge into network models. <p> The feasibility of performing this computation determines the ability to make inferences or to learn on the basis of observations. The standard framework for carrying out this computation consists of exact probabilistic methods <ref> (Lauritzen 1996) </ref>. Such methods are nevertheless restricted to fairly small or sparsely connected networks and the use of approximate techniques is likely to be the rule for highly interconnected graphs of the kind studied in the neural network literature. <p> In this paper, we develop a recursive node-elimination formalism that meets all three objectives for a powerful class of networks known as chain graphs <ref> (see, e.g., Lauritzen 1996) </ref>; Boltzmann machines and sigmoid belief networks are included as special cases. We start by deriving the recursive formalism for Boltzmann machines. The results are then generalized to sigmoid belief networks and chain graphs. 2 Boltzmann machines We begin by considering Boltzmann machines with binary (0/1) variables.
Reference: <author> T. Jaakkola and M. </author> <title> Jordan (1996). Computing upper and lower bounds on likelihoods in intractable networks. 5 This follows from the linearity of the propagation rules for the biases, and the fact that the emerging prefactors are either linear or quadratic in the exponent. </title> <booktitle> To appear in Proceedings of the twelfth Conference on Uncertainty in Artificial Intelligence. </booktitle>
Reference: <author> R. Neal. </author> <title> Connectionist learning of belief networks (1992). </title> <booktitle> Artificial Intelligence 56: </booktitle> <pages> 71-113. </pages>
Reference-contexts: These goals have been addressed in the literature with varying degrees of success. For inference and learning in Boltzmann machines, for example, classical mean field approximations (Peterson & Anderson, 1987) address only the first goal. In the case of sigmoid belief networks <ref> (Neal 1992) </ref>, partial solutions have been provided to the first two goals (Dayan et al. 1995; Saul et al. 1996; Jaakkola & Jordan 1996).
Reference: <author> C. Peterson and J. R. </author> <title> Anderson (1987). A mean field theory learning algorithm for neural networks. </title> <booktitle> Complex Systems 1: </booktitle> <pages> 995-1019. </pages>
Reference-contexts: These goals have been addressed in the literature with varying degrees of success. For inference and learning in Boltzmann machines, for example, classical mean field approximations <ref> (Peterson & Anderson, 1987) </ref> address only the first goal. In the case of sigmoid belief networks (Neal 1992), partial solutions have been provided to the first two goals (Dayan et al. 1995; Saul et al. 1996; Jaakkola & Jordan 1996).
Reference: <author> L. K. Saul, T. Jaakkola, and M. I. </author> <title> Jordan (1996). Mean field theory for sigmoid belief networks. </title> <type> JAIR 4: </type> <pages> 61-76. </pages>
Reference-contexts: In the case of sigmoid belief networks (Neal 1992), partial solutions have been provided to the first two goals (Dayan et al. 1995; Saul et al. 1996; Jaakkola & Jordan 1996). The goal of integrating approximations with exact techniques has been introduced in the context of Boltzmann machines <ref> (Saul & Jordan 1996) </ref> but nevertheless leaving the solution to our second goal unattained.
Reference: <author> L. Saul and M. </author> <title> Jordan (1996). Exploiting tractable substructures in intractable networks. </title> <booktitle> To appear in Advances of Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: In the case of sigmoid belief networks (Neal 1992), partial solutions have been provided to the first two goals (Dayan et al. 1995; Saul et al. 1996; Jaakkola & Jordan 1996). The goal of integrating approximations with exact techniques has been introduced in the context of Boltzmann machines <ref> (Saul & Jordan 1996) </ref> but nevertheless leaving the solution to our second goal unattained.
References-found: 7

