URL: http://paris.lcs.mit.edu/ftpdir/papers/lfp94.ps
Refering-URL: http://www.psrg.lcs.mit.edu/publications.html
Root-URL: 
Email: freistad, giffordg@lcs.mit.edu  
Title: Static Dependent Costs for Estimating Execution Time  
Author: Brian Reistad David K. Gifford 
Address: Cambridge, Massachusetts 02139.  
Affiliation: Laboratory for Computer Science, Massachusetts Institute of Technology,  
Abstract: We present the first system for estimating and using data-dependent expression execution times in a language with first-class procedures and imperative constructs. The presence of first-class procedures and imperative constructs makes cost estimation a global problem that can benefit from type information. We estimate expression costs with the aid of an algebraic type reconstruction system that assigns every procedure a type that includes a static dependent cost. A static dependent cost describes the execution time of a procedure in terms of its inputs. In particular, a procedure's static dependent cost can depend on the size of input data structures and the cost of input first-class procedures. Our cost system produces symbolic cost expressions that contain free variables describing the size and cost of the procedure's inputs. At run-time, a cost estimate is dynamically computed from the statically determined cost expression and run-time cost and size information. We present experimental results that validate our cost system on three compilers and architectures. We experimentally demonstrate the utility of cost estimates in making dynamic parallelization decisions. In our experience, dynamic parallelization meets or exceeds the parallel performance of any fixed number of processors. 
Abstract-found: 1
Intro-found: 1
Reference: [ADGHSS85] <author> Applegate, J.F., Douglas, M.R., Gursel, Y., Hunter, P., Seitz, C., and Sussman, G.J. </author> <title> A Digital Orrery. </title> <booktitle> IEEE Computer, </booktitle> <month> September </month> <year> 1985. </year>
Reference-contexts: A large number of interesting programs can be written using only first-class procedures and a complete set of data parallel op erators [B78]. The example programs in this paper were all written without general recursion, including Sussman's n-body simulation <ref> [ADGHSS85] </ref>. Our cost system does not currently provide estimates for user-defined recursive procedures, but still provides cost estimates for non-recursive subexpressions. Predicting costs for recursive procedures requires solving recurrence equations which is not always possible. <p> We ran experiments for the game of life on a five by five grid. N-body simulation We translated Sussman's code for the n-body problem <ref> [ADGHSS85] </ref> as found in [M87] to FX/SDC. This code simulates the movement of the solar system by using differential systems.
Reference: [A90] <author> Apt, K.R. </author> <title> Logic Programming. Handbook of TCS, Vol B, Formal Models and Semantics, </title> <editor> Jan Van Leeuwen (editor), </editor> <publisher> Elsevier, </publisher> <year> 1990, </year> <pages> 491-574. </pages>
Reference-contexts: However, long does not provide us with any useful information, so we would like a minimal assignment to C vi that satisfies the constraints. This is referred to as the least pre-fixpoint 8 of the constraint equations <ref> [A90] </ref>. The least pre-fixpoint is the smallest solution of a set of inequalities. Because the operators sum, prod, and max are monotonically increasing and continuous, the least pre-fixpoint can be calculated with the least fixpoint. The least fixpoint is the smallest solution to a set of equalities.
Reference: [B78] <author> Backus, J. </author> <title> Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs. </title> <journal> CACM, </journal> <volume> 21(8), </volume> <year> 1978, </year> <pages> 613-641. </pages>
Reference-contexts: Our static dependent cost system contains dependent costs for a complete set of data parallel operators. A large number of interesting programs can be written using only first-class procedures and a complete set of data parallel op erators <ref> [B78] </ref>. The example programs in this paper were all written without general recursion, including Sussman's n-body simulation [ADGHSS85]. Our cost system does not currently provide estimates for user-defined recursive procedures, but still provides cost estimates for non-recursive subexpressions.
Reference: [CBF91] <author> Chatterjee, S., Blelloch, G.E., and Fisher, A.L. </author> <title> Size and Access Inference for Data-Parallel Programs. </title> <booktitle> PLDI 1991, </booktitle> <pages> 130-144. 13 </pages>
Reference-contexts: Interval/Range Analysis Range analysis is relevant because our system includes estimates of data structure sizes that allow us to describe dependent costs. Chatterjee et al. <ref> [CBF91] </ref> analyze a data parallel program graph to discover which vectors have the same run-time sizes. They observe that their algorithm is similar to type inference.
Reference: [C82] <author> Cohen, J. </author> <title> Computer-Assisted Microanalysis of Pro--grams. </title> <journal> CACM, </journal> <volume> 25(10), </volume> <year> 1982, </year> <pages> 724-733. </pages>
Reference-contexts: A micro analysis system is a cost system in which costs are expressed using constants that describe the costs of common, elementary operations. Micro analysis was first presented by Knuth [K68] and more recently by Wegbreit [W75] and Cohen <ref> [C82] </ref>.
Reference: [D92] <author> Dornic, V. </author> <title> Analyse de Complexite des Programmes: Verification et Inference. </title> <type> Ph.D. Thesis, </type> <institution> Ecole des Mines, Paris, France, CRI/A/212, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Cost systems Cost systems are an extension of effect systems to analyze program execution time, but to date they have not captured dependence on data structure size. Dornic et al. [DJG92] propose a cost system that parallels an effect system but it requires explicit typing. Dornic <ref> [D92] </ref> presents the first cost reconstruction algorithm and includes a notion of subcosting, but he does not describe how to handle polymorphism. Dornic [D93] presents a refinement that labels recursive calls, thus identifying the sources of recursion. <p> This subcosting flexibility allows us to report the larger of two latent costs when two procedures are constrained to have the same type. These two rules provide the same functionality as Dornic's subcosting rule <ref> [D92] </ref>. The var rule must incorporate the same subcosting and subsizing flexibility as the num and lambda rules because the identifier may be bound to a natural number or a procedure. This flexibility is expressed by the subtyping relation shown in Figure 6.
Reference: [D93] <author> Dornic, V. </author> <title> Ordering Times. </title> <institution> Yale University, Research Report YALEU/DCS/RR-956, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Dornic et al. [DJG92] propose a cost system that parallels an effect system but it requires explicit typing. Dornic [D92] presents the first cost reconstruction algorithm and includes a notion of subcosting, but he does not describe how to handle polymorphism. Dornic <ref> [D93] </ref> presents a refinement that labels recursive calls, thus identifying the sources of recursion. These systems cannot provide cost estimates for the examples given in this paper because they are not powerful enough to handle size dependencies and thus cannot describe any form of iteration.
Reference: [DJG92] <author> Dornic, V., Jouvelot, P., and Gifford, D.K. </author> <title> Polymorphic Time Systems for Estimating Program Complexity. </title> <journal> LoPLaS, </journal> <volume> 1(1), </volume> <year> 1992, </year> <pages> 33-45. </pages>
Reference-contexts: Cost systems Cost systems are an extension of effect systems to analyze program execution time, but to date they have not captured dependence on data structure size. Dornic et al. <ref> [DJG92] </ref> propose a cost system that parallels an effect system but it requires explicit typing. Dornic [D92] presents the first cost reconstruction algorithm and includes a notion of subcosting, but he does not describe how to handle polymorphism.
Reference: [GJSO92] <author> Gifford, D.K., Jouvelot, P., Sheldon, M.A., and O'Toole, J.W. </author> <booktitle> Report on the FX-91 Programming Language. </booktitle> <address> MIT/LCS/TR-531, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: for determining the value ranges of variables in the context of loops. 3 Cost System Semantics In this section we define our language and introduce our cost system. 3.1 Language Definition The experimental work described in this paper has been carried out in a subset of the FX programming language <ref> [GJSO92] </ref>, called FX/SDC. FX/SDC is statically typed with first-class procedures and mutation. FX/SDC has been used to write a number of programs, including matrix multiplication, the game of life, and n-body simulation. The syntax of our language is shown in Figure 2.
Reference: [G88] <author> Goldberg, B. </author> <title> Multiprocessor Execution of Functional Programs. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(5), </volume> <year> 1988, </year> <pages> 425-473. </pages>
Reference-contexts: With reliable static estimates an optimizing compiler can focus its attention on the most important portion of a program and analyze which expressions might be profitably evaluated in parallel <ref> [G86, SH86, G88, MKH90] </ref>. We have developed a simple dynamic paralleliza-tion system which uses cost estimates to make parallelization decisions based on their profitability. This research supported by DARPA/ONR Grant. No. DABT63-92-c-0012 To appear in the ACM Conference on Lisp and Functional Programming, June 1994, Orlando, Florida.
Reference: [G86] <author> Gray, </author> <title> S.L. Using Futures to Exploit Parallelism in Lisp. M.S. </title> <type> Thesis, </type> <institution> MIT, </institution> <month> February </month> <year> 1986. </year>
Reference-contexts: With reliable static estimates an optimizing compiler can focus its attention on the most important portion of a program and analyze which expressions might be profitably evaluated in parallel <ref> [G86, SH86, G88, MKH90] </ref>. We have developed a simple dynamic paralleliza-tion system which uses cost estimates to make parallelization decisions based on their profitability. This research supported by DARPA/ONR Grant. No. DABT63-92-c-0012 To appear in the ACM Conference on Lisp and Functional Programming, June 1994, Orlando, Florida. <p> However, a static cost system provides information about what expressions can be profitably evaluated in parallel. It is only worthwhile to evaluate two expressions in parallel if the time saved is greater than the cost of setting up the parallel computation. Gray <ref> [G86] </ref> introduced a system for inserting futures that estimates costs based on a local, syntactic method. Our cost system provides static cost estimates that can be used to make parallelization decisions at compile time.
Reference: [GSO92] <author> Grundman, D., Stata, R., and O'Toole, J. FX/DLX A Pedagogic Compiler. MIT/LCS/TR-538, </author> <month> March </month> <year> 1992. </year>
Reference: [HG88] <author> Hammel, R.T., and Gifford, D.K. </author> <title> FX-87 Performance Measurements: Dataflow Implementation. </title> <address> MIT/LCS/TR-421, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: Thus, the measurements in this section do not include the cost of garbage collection. 6 Dynamic Parallelization Efforts in automatic parallelization have been primarily concerned with identifying expressions that can be safely executed in parallel <ref> [HG88, JG89, TJ93, HL92] </ref>. However, a static cost system provides information about what expressions can be profitably evaluated in parallel. It is only worthwhile to evaluate two expressions in parallel if the time saved is greater than the cost of setting up the parallel computation.
Reference: [H77] <author> Harrison, </author> <title> W.H. Compiler Analysis of the Value Ranges of Variables. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(3), </volume> <year> 1977, </year> <pages> 243-250. </pages>
Reference-contexts: Chatterjee et al. [CBF91] analyze a data parallel program graph to discover which vectors have the same run-time sizes. They observe that their algorithm is similar to type inference. Harrison <ref> [H77] </ref> presents a mechanism for determining the value ranges of variables in the context of loops. 3 Cost System Semantics In this section we define our language and introduce our cost system. 3.1 Language Definition The experimental work described in this paper has been carried out in a subset of the <p> Consider the constraint set fC v1 5; C v2 (sub 10 C v1 )g. Minimizing C v1 makes C v2 larger and vice versa. An alternative approach could possibly provide better size information by using interval arithmetic <ref> [H77] </ref>. One could allow a subtraction operator and compute a reduced constraint set, but what should be done with the reduced constraint set remains an open issue. 4.3 Correctness Issues The reconstruction algorithm is sound if the type and cost it computes are a valid solution to the static semantics.
Reference: [HP90] <author> Hennessy, J.L., and Patterson, D.A. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: As such, it emphasizes readability over performance optimizations. It compiles FX to Hennessy and Patterson's DLX architecture <ref> [HP90] </ref>. We measured the actual number of DLX instructions executed. * The Mul-T compiler compiles a parallel version of T to the Alewife machine. Our experiments were run with ASIM, a cycle-by-cycle simulator for the Alewife machine [L92].
Reference: [HL92] <author> Huelsbergen, L., and Larus, J. </author> <title> Dynamic Program Parallelization. </title> <booktitle> LFP 1992, </booktitle> <pages> 311-323. </pages>
Reference-contexts: Thus, the measurements in this section do not include the cost of garbage collection. 6 Dynamic Parallelization Efforts in automatic parallelization have been primarily concerned with identifying expressions that can be safely executed in parallel <ref> [HG88, JG89, TJ93, HL92] </ref>. However, a static cost system provides information about what expressions can be profitably evaluated in parallel. It is only worthwhile to evaluate two expressions in parallel if the time saved is greater than the cost of setting up the parallel computation. <p> Thus, once type and cost reconstruction is complete, the program can be annotated with dynamic information where appropriate. In our simple dynamic parallelization system, this annotation was performed by hand. There is no fundamental obstacle to automating this annotation; a similar mechanism has been used with effect systems <ref> [TJ93, HL92] </ref>. We use maximum cost estimates for parallelization decisions even though they cannot guarantee speed up. If the maximum cost is much larger than the actual cost then we may parallelize when it is not profitable.
Reference: [HLA94] <author> Huelsbergen, L., Larus, J., and Aiken, A. </author> <title> Using the Run-Time Sizes of Data Structures to Guide Parallel-Thread Creation. </title> <booktitle> LFP 1994. </booktitle>
Reference-contexts: They use sizes and costs in a similar manner to our system in describing costs for data parallel operators; however, they do not deal with first-class procedures that may have dependent costs. Huelsbergen et al. <ref> [HLA94] </ref> present an automatic paral-lelization system that statically estimates costs and makes dynamic parallelization decisions. They use abstract evaluation to compute a lower bound on the cost of evaluating an expression for various sizes of an input list.
Reference: [JG89] <author> Jouvelot, P., and Gifford, D.K. </author> <title> Parallel Functional Programming: The FX Project. Parallel and Distributed Algorithms, </title> <editor> M. Cosnard et al. (editors), </editor> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <year> 1989, </year> <pages> 257-267. </pages>
Reference-contexts: Thus, the measurements in this section do not include the cost of garbage collection. 6 Dynamic Parallelization Efforts in automatic parallelization have been primarily concerned with identifying expressions that can be safely executed in parallel <ref> [HG88, JG89, TJ93, HL92] </ref>. However, a static cost system provides information about what expressions can be profitably evaluated in parallel. It is only worthwhile to evaluate two expressions in parallel if the time saved is greater than the cost of setting up the parallel computation.
Reference: [JG91] <author> Jouvelot, P., and Gifford, D.K. </author> <title> Algebraic Reconstruction of Types and Effects. </title> <booktitle> POPL 1991, </booktitle> <pages> 303-310. </pages>
Reference-contexts: Effect systems Effect systems originated the idea of annotating procedure types with static information about how a program computes and form the basis for cost systems. Lucassen and Gifford [LG88] first proposed effect systems to analyze side effects. Jouvelot and Gifford <ref> [JG91] </ref> present an algebraic reconstruction algorithm to infer effect descriptions and provide let-polymorphism using algebraic type schemes. Talpin and Jouvelot [TJ92] extend reconstruction to regions describing memory locations and show how to include the notion of subeffecting. <p> The type schemes for the primitive operators must be converted to algebraic type schemes to insure only size and cost variables appear in types. An algebraic type scheme is a pair of a type and constraint set that are abstracted over a set of description variables <ref> [JG91] </ref>. The reconstruction algorithm directly implements the static semantics. For compound expressions, R is applied to the subexpressions and the results are combined appropriately.
Reference: [K68] <author> Knuth, D.E. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol. 1: </volume> <booktitle> Fundamental Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1968. </year>
Reference-contexts: A cost system estimates the cost of a program in terms of a desired metric such as time. A micro analysis system is a cost system in which costs are expressed using constants that describe the costs of common, elementary operations. Micro analysis was first presented by Knuth <ref> [K68] </ref> and more recently by Wegbreit [W75] and Cohen [C82].
Reference: [L88] <author> Le Metayer, D. </author> <title> ACE: An Automatic Complexity Evaluator. </title> <journal> ToPLaS, </journal> <volume> 10(2), </volume> <year> 1988, </year> <pages> 248-266. </pages>
Reference-contexts: Predicting costs for recursive procedures requires solving recurrence equations which is not always possible. This problem is not addressed as it is beyond the scope of this paper and has been the subject of previous work <ref> [W75, L88, R89, S90] </ref>. Our cost system could potentially be augmented with a pre-defined database of recursion equations and their closed forms. <p> Complexity Analysis Automatic complexity analysis attempts to provide closed form costs by analyzing recurrence equations, but has been developed only for languages without first-class procedures and mutation. Weg-breit [W75] presents one of the earliest automatic complexity analysis tools METRIC to analyze simple Lisp programs. Le Metayer <ref> [L88] </ref> presents the ACE system for analyzing FP programs by rewriting to a time-complexity function and matching against a database. Rosendahl [R89] presents a similar system for a first-order subset of Lisp based on abstract interpretation.
Reference: [L92] <author> Lim, B. </author> <title> Instructions for Obtaining and Installing ASIM. </title> <journal> MIT/LCS, </journal> <note> Alewife Systems Memo #30, </note> <month> September </month> <year> 1992. </year>
Reference-contexts: It compiles FX to Hennessy and Patterson's DLX architecture [HP90]. We measured the actual number of DLX instructions executed. * The Mul-T compiler compiles a parallel version of T to the Alewife machine. Our experiments were run with ASIM, a cycle-by-cycle simulator for the Alewife machine <ref> [L92] </ref>. We measured the actual number of cycles executed in a configuration of one processor. * We used the SML/NJ compiler (version 0.93) on a Sparc IPX to run experiments after a simple syntactic translation from FX/SDC to ML. We measured actual execution time with the SML/NJ profile tool [SML/NJ93].
Reference: [LG88] <author> Lucassen, J.M., and Gifford, D.K. </author> <title> Polymorphic Effect Systems. </title> <booktitle> POPL 1988, </booktitle> <pages> 47-57. </pages>
Reference-contexts: Static analysis can be done without choosing "typical" input data that is required to gather profile statistics. Effect systems Effect systems originated the idea of annotating procedure types with static information about how a program computes and form the basis for cost systems. Lucassen and Gifford <ref> [LG88] </ref> first proposed effect systems to analyze side effects. Jouvelot and Gifford [JG91] present an algebraic reconstruction algorithm to infer effect descriptions and provide let-polymorphism using algebraic type schemes. Talpin and Jouvelot [TJ92] extend reconstruction to regions describing memory locations and show how to include the notion of subeffecting.
Reference: [M87] <author> Miller, J.S. MultiScheme: </author> <title> A Parallel Processing System Based on MIT Scheme. </title> <type> Ph.D. Thesis, </type> <institution> MIT/LCS/TR-402, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: We ran experiments for the game of life on a five by five grid. N-body simulation We translated Sussman's code for the n-body problem [ADGHSS85] as found in <ref> [M87] </ref> to FX/SDC. This code simulates the movement of the solar system by using differential systems.
Reference: [MKH90] <author> Mohr, E., Kranz, D.A., and Halstead, </author> <title> R.H. Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs. </title> <booktitle> LFP 1990, </booktitle> <pages> 197-185. </pages>
Reference-contexts: With reliable static estimates an optimizing compiler can focus its attention on the most important portion of a program and analyze which expressions might be profitably evaluated in parallel <ref> [G86, SH86, G88, MKH90] </ref>. We have developed a simple dynamic paralleliza-tion system which uses cost estimates to make parallelization decisions based on their profitability. This research supported by DARPA/ONR Grant. No. DABT63-92-c-0012 To appear in the ACM Conference on Lisp and Functional Programming, June 1994, Orlando, Florida.
Reference: [MT92] <author> Morrisett, J.G., and Tolmach, A. </author> <title> A Portable Multiprocessor Interface for Standard ML of New Jersey. </title> <institution> Carnegie Mellon University, CMU-CS-92-155, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Minimum cost estimates could ensure speed up, but may overlook opportunities for parallelism if the minimum estimate is too low. We built a simple dynamic parallelization system on an SGI IRIX computer with four processors using SML/NJ with a multiprocessor interface <ref> [MT92] </ref>. Our system exploits data parallelism by performing vector map operations in parallel. Since there are a limited number of processors, the vector is broken into segments and each processor performs the vector map on a given segment.
Reference: [R65] <author> Robinson, J.A. </author> <title> A Machine Oriented Logic Based on the Resolution Principle. </title> <journal> JACM, </journal> <volume> 12(1), </volume> <year> 1965, </year> <pages> 23-41. </pages>
Reference-contexts: ), (numof N 0 v )) ! [N v = N 0 ((listof T N v ), (listof T 0 N 0 v )) return [N v =N 0 v ]S else fail 4.1 Unification Algorithm The unification algorithm in Figure 11 is straightforward and in the spirit of Robinson <ref> [R65] </ref>. The unification algorithm works on types: U : (Type fi Type) ! Substitution Unification of procedure types not only requires unifying the input and return types, but also unifying their latent costs. Unification of data structure types such as natural numbers and lists requires unification of the size estimates.
Reference: [R89] <author> Rosendahl, M. </author> <title> Automatic Complexity Analysis. </title> <booktitle> Proceedings of the Fourth International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <year> 1989, </year> <pages> 144-156. </pages>
Reference-contexts: Predicting costs for recursive procedures requires solving recurrence equations which is not always possible. This problem is not addressed as it is beyond the scope of this paper and has been the subject of previous work <ref> [W75, L88, R89, S90] </ref>. Our cost system could potentially be augmented with a pre-defined database of recursion equations and their closed forms. <p> Weg-breit [W75] presents one of the earliest automatic complexity analysis tools METRIC to analyze simple Lisp programs. Le Metayer [L88] presents the ACE system for analyzing FP programs by rewriting to a time-complexity function and matching against a database. Rosendahl <ref> [R89] </ref> presents a similar system for a first-order subset of Lisp based on abstract interpretation. Sands [S88, S90] presents a mechanism to produce time-complexity expressions for a language with first-class procedures (but not mutation) to extend Le Metayer's ACE system.
Reference: [S88] <author> Sands, D. </author> <title> Complexity Analysis for Higher Order Languages. </title> <institution> Imperial College, London, </institution> <note> Research Report DOC 88/14, </note> <month> October </month> <year> 1988. </year>
Reference-contexts: Le Metayer [L88] presents the ACE system for analyzing FP programs by rewriting to a time-complexity function and matching against a database. Rosendahl [R89] presents a similar system for a first-order subset of Lisp based on abstract interpretation. Sands <ref> [S88, S90] </ref> presents a mechanism to produce time-complexity expressions for a language with first-class procedures (but not mutation) to extend Le Metayer's ACE system. It seems that his approach still exposes first-class procedures to the ACE system which would require a powerful deduction system to solve recurrences containing first-class procedures.
Reference: [S90] <author> Sands, D. </author> <title> Calculi for Time Analysis of Functional Programs. </title> <type> Ph.D. Thesis, </type> <institution> University of London, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Predicting costs for recursive procedures requires solving recurrence equations which is not always possible. This problem is not addressed as it is beyond the scope of this paper and has been the subject of previous work <ref> [W75, L88, R89, S90] </ref>. Our cost system could potentially be augmented with a pre-defined database of recursion equations and their closed forms. <p> Le Metayer [L88] presents the ACE system for analyzing FP programs by rewriting to a time-complexity function and matching against a database. Rosendahl [R89] presents a similar system for a first-order subset of Lisp based on abstract interpretation. Sands <ref> [S88, S90] </ref> presents a mechanism to produce time-complexity expressions for a language with first-class procedures (but not mutation) to extend Le Metayer's ACE system. It seems that his approach still exposes first-class procedures to the ACE system which would require a powerful deduction system to solve recurrences containing first-class procedures. <p> The cost of the application includes the overhead of calling the procedure, the cost of the subexpres-sions, and the latent cost of the operator. We have not distinguished between primitives and general procedures as has been done in other cost analysis systems such as <ref> [W75, S90] </ref>. To be conservative, we must assume that every application incurs the overhead of general procedure call. The static semantics includes some flexibility in deducing size and cost descriptions.
Reference: [SH86] <author> Sarkar, V., and Hennessy, J. </author> <title> Compile-Time Partitioning and Scheduling of Parallel Programs. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <year> 1986, </year> <pages> 17-26. </pages>
Reference-contexts: With reliable static estimates an optimizing compiler can focus its attention on the most important portion of a program and analyze which expressions might be profitably evaluated in parallel <ref> [G86, SH86, G88, MKH90] </ref>. We have developed a simple dynamic paralleliza-tion system which uses cost estimates to make parallelization decisions based on their profitability. This research supported by DARPA/ONR Grant. No. DABT63-92-c-0012 To appear in the ACM Conference on Lisp and Functional Programming, June 1994, Orlando, Florida. <p> The general approach is to run a program once, gather statistics about where time is spent, and feed this cost information back into the compiler to re-compile the program. Profile data has been used for various optimization efforts including partitioning and scheduling parallel programs <ref> [SH86] </ref>. Unfortunately, the profile data from one run is not always a good predictor of subsequent runs with different input data [W91].
Reference: [SC93] <author> Skillicorn, D.B., and Cai, W. </author> <title> A Cost Calculus for Parallel Functional Programming. </title> <institution> Queen's University, Kingston, Canada, ISSN-0836-0227-93-348, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: These systems cannot provide cost estimates for the examples given in this paper because they are not powerful enough to handle size dependencies and thus cannot describe any form of iteration. Cost Estimation Skillicorn and Cai <ref> [SC93] </ref> present a cost calculus for a parallel functional programming language that can be used in a program development system.
Reference: [SML/NJ93] <institution> Standard ML of New Jersey Reference Manual (Version 0.93). </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: We measured the actual number of cycles executed in a configuration of one processor. * We used the SML/NJ compiler (version 0.93) on a Sparc IPX to run experiments after a simple syntactic translation from FX/SDC to ML. We measured actual execution time with the SML/NJ profile tool <ref> [SML/NJ93] </ref>. We have implemented the reconstruction algorithm discussed in the previous section. Our implementation computes the cost of program expressions in terms of symbolic constants that describe the cost of basic language components. We experimentally determined values for these constants in the above systems.
Reference: [T93] <author> Talpin, J.P. </author> <title> Aspects Theoriques et Pratiques de l'Inference de Type et d'Effets. </title> <type> Ph.D. Thesis, </type> <institution> Paris University VI, </institution> <month> May </month> <year> 1993. </year>
Reference: [TJ92] <author> Talpin, J., and Jouvelot, P. </author> <title> Polymorphic Type, Region and Effect Inference. </title> <journal> Journal of Functional Programming, </journal> <volume> 2(3), </volume> <year> 1992, </year> <pages> 245-271. </pages>
Reference-contexts: Lucassen and Gifford [LG88] first proposed effect systems to analyze side effects. Jouvelot and Gifford [JG91] present an algebraic reconstruction algorithm to infer effect descriptions and provide let-polymorphism using algebraic type schemes. Talpin and Jouvelot <ref> [TJ92] </ref> extend reconstruction to regions describing memory locations and show how to include the notion of subeffecting. Cost systems Cost systems are an extension of effect systems to analyze program execution time, but to date they have not captured dependence on data structure size.
Reference: [TJ93] <author> Talpin, J., and Jouvelot, P. </author> <title> Compiling FX on the CM-2. </title> <booktitle> Proceedings of the Third Workshop on Static Analysis, LNCS 724, </booktitle> <month> September </month> <year> 1993, </year> <pages> 87-98. </pages>
Reference-contexts: Thus, the measurements in this section do not include the cost of garbage collection. 6 Dynamic Parallelization Efforts in automatic parallelization have been primarily concerned with identifying expressions that can be safely executed in parallel <ref> [HG88, JG89, TJ93, HL92] </ref>. However, a static cost system provides information about what expressions can be profitably evaluated in parallel. It is only worthwhile to evaluate two expressions in parallel if the time saved is greater than the cost of setting up the parallel computation. <p> Thus, once type and cost reconstruction is complete, the program can be annotated with dynamic information where appropriate. In our simple dynamic parallelization system, this annotation was performed by hand. There is no fundamental obstacle to automating this annotation; a similar mechanism has been used with effect systems <ref> [TJ93, HL92] </ref>. We use maximum cost estimates for parallelization decisions even though they cannot guarantee speed up. If the maximum cost is much larger than the actual cost then we may parallelize when it is not profitable.
Reference: [T87] <author> Tofte, M. </author> <title> Operational Semantics and Polymorphic Type Inference. </title> <type> Ph.D. Thesis, </type> <institution> University of Edin-burgh, </institution> <year> 1987. </year>
Reference-contexts: Let-polymorphism is provided for non-expansive <ref> [T87] </ref> expressions by the use of type schemes. (The rule for expansive expressions is straightforward and omitted.) Previous effect reconstruction systems have used substitution to provide let-polymorphism, but substituting the let-bound expression may artificially increase the cost estimate of the body. <p> Thus we assume that the program is explicitly annotated with type skeletons that provide information about the bound variables in lambda and rec expressions. Type skeletons are types without cost or size annotations. In our implementation, type skeletons are computed by a reconstruction algorithm similar to Tofte's <ref> [T87] </ref>. The type skeletons are converted to annotated types by the newtype algorithm (Figure 10) which inserts fresh cost and size variables. Thus the type of the variable will be known for all references and lift-type can be correctly applied.
Reference: [W91] <author> Wall, D. </author> <title> Predicting Program Behavior Using Real or Estimated Profiles. </title> <booktitle> PLDI 1991, </booktitle> <pages> 59-70. </pages>
Reference-contexts: Profile data has been used for various optimization efforts including partitioning and scheduling parallel programs [SH86]. Unfortunately, the profile data from one run is not always a good predictor of subsequent runs with different input data <ref> [W91] </ref>. The static cost estimates produced by our system do not suffer from this disadvantage because our system produces cost estimates that depend on the size of the input data and thus generalize to previously unseen data sizes.
Reference: [W75] <author> Wegbreit, B. </author> <title> Mechanical Program Analysis. </title> <journal> CACM, </journal> <volume> 18(9), </volume> <year> 1975, </year> <pages> 528-539. </pages>
Reference-contexts: A micro analysis system is a cost system in which costs are expressed using constants that describe the costs of common, elementary operations. Micro analysis was first presented by Knuth [K68] and more recently by Wegbreit <ref> [W75] </ref> and Cohen [C82]. <p> Predicting costs for recursive procedures requires solving recurrence equations which is not always possible. This problem is not addressed as it is beyond the scope of this paper and has been the subject of previous work <ref> [W75, L88, R89, S90] </ref>. Our cost system could potentially be augmented with a pre-defined database of recursion equations and their closed forms. <p> This implies that iterators such as map are not primitives or their cost would be greatly underestimated. Complexity Analysis Automatic complexity analysis attempts to provide closed form costs by analyzing recurrence equations, but has been developed only for languages without first-class procedures and mutation. Weg-breit <ref> [W75] </ref> presents one of the earliest automatic complexity analysis tools METRIC to analyze simple Lisp programs. Le Metayer [L88] presents the ACE system for analyzing FP programs by rewriting to a time-complexity function and matching against a database. <p> The cost of the application includes the overhead of calling the procedure, the cost of the subexpres-sions, and the latent cost of the operator. We have not distinguished between primitives and general procedures as has been done in other cost analysis systems such as <ref> [W75, S90] </ref>. To be conservative, we must assume that every application incurs the overhead of general procedure call. The static semantics includes some flexibility in deducing size and cost descriptions.
References-found: 39

