URL: http://www.cs.colostate.edu/~ftppub/TechReports/1994/tr-111.ps.Z
Refering-URL: http://www.cs.colostate.edu/~ftppub/
Root-URL: 
Title: Classification of EEG Signals Using a Sparse Polynomial Builder  
Affiliation: Department of Computer Science  Colorado State University  
Abstract: Edward S. Orosz and Charles W. Anderson Technical Report CS-94-111 April 27, 1994 
Abstract-found: 1
Intro-found: 1
Reference: [Jasper 58] <author> Jasper H. </author> <booktitle> (1958) The Ten Twenty Electrode System of the International Federation. Electroencephalography and Clinical Neurophysiology. </booktitle>
Reference-contexts: One such effort used Kohonen nets to classify the signals [Shiao-Lin, et al. 93]. They recorded new data, but the tasks and features used were based on Keirn's work. 2.1 Data Measurements were taken from six sensors located on the scalp. The standard "10-20 system" <ref> [Jasper 58] </ref> of electrode placement was used (see figure 1). Locations selected were C3, C4, P3, P4, O1 and O2. Data was collected for ten seconds. The sampling rate was 250 samples/sec resulting in 2500 floating point numbers in the approximate range 50:0 to +50:0 microvolts per task.
Reference: [Keirn 88] <author> Keirn, Z. A. </author> <title> (1988) Alternative Modes of Communication Between Man and Machine Master's Thesis, </title> <institution> Purdue University. </institution>
Reference-contexts: In pursuit of a new machine interface, Keirn <ref> [Keirn 88] </ref> and Aunon [Keirn and Aunon 90] attempted to determine the mental task a subject was performing by examining and processing the recorded electroencephalogram (EEG) signals. Classifiers were built to output the corresponding task using only the inputed signal. <p> Following that summary is a description of the modification made to the initial algorithm in order to improve accuracy on the EEG data and and a discussion of the results obtained with the new algorithm. The paper concludes with comments and observations. 2 PREVIOUS WORK In Keirn's, <ref> [Keirn 88] </ref>, study subjects' EEG were recorded while they performed the following mental tasks: Baseline: The subjects were told to think of "nothing in particular". Letter Composing: The subject composed a letter to a friend or relative without vocalizing. <p> For the second trial, they were told to keep their eyes closed. Thus the data was divided into "eyes closed" and "eyes open" sets. The spectral density curve for each sensor was calculated by the Burg and Wiener-Khinchine methods <ref> [Keirn 88] </ref>. The curve shows the energy at a particular frequency (see figure 3). For each of the six channels, the area under the curve was computed at four common frequency bands of the brain delta ( 0-3 Hz), theta ( 4-7 Hz), alpha (8-13 Hz), and beta (14-20 Hz). <p> The order of the electrodes in each set is the same: O1, O2, P3, P4, C3 and C4. 5 6 Subject 1 3 4 5 6 Average 92% 86.3% 95% 84.7% 91.8% Table 1: Classification accuracy on task pairs <ref> [Keirn 88] </ref> 2.2 Keirn's Results In general, Keirn achieved high classification rates, even 100%, for several subjects and task pairs. Table 1 gives the average classification Keirn achieved (using 20 different task pairs) with the Wiener-Khinchine method for feature creation. <p> Table 1 gives the average classification Keirn achieved (using 20 different task pairs) with the Wiener-Khinchine method for feature creation. For the task pair used in this report, the feature Keirn used was the ratio with p3 and p4 (alpha band) <ref> [Keirn 88] </ref>. However, Keirn stated that for most task pairs, there were "several" other feature combinations yielding the same accuracy. No particular set stood out. <p> The ranking of the features was done on a per subject basis as follows: For each n training sets, the first three features created were selected. The percentage of times a feature was selected over all the training sets denotes its ranking. The nomenclature for naming the features from <ref> [Keirn 88] </ref> is used. The power values are denoted by the band followed by the lead. For example, ffO1, is the area under electrode O1 at the alpha (ff) frequency band. The other bands use the corresponding Greek symbol, fi for beta, ffi for delta and for theta.
Reference: [Keirn and Aunon 90] <author> Keirn, Z. A. and Aunon, J. I. </author> <title> (1990) A new mode of communication between Man and his surroundings. </title> <journal> IEEE Trans., </journal> <volume> BME-37, </volume> <pages> 1209-1214. </pages>
Reference-contexts: In pursuit of a new machine interface, Keirn [Keirn 88] and Aunon <ref> [Keirn and Aunon 90] </ref> attempted to determine the mental task a subject was performing by examining and processing the recorded electroencephalogram (EEG) signals. Classifiers were built to output the corresponding task using only the inputed signal.
Reference: [Sanger 91] <author> Sanger T. D. </author> <title> (1991) Basis-function trees as a generalization of local variable selections methods for function approximation. </title> <editor> In D. S. Touretzky, (ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA.. </address>
Reference-contexts: Instead, features are rated and the top two most "useful" ones are used to form the product. The rating method developed by Sutton and Matheus was a modification of the method from Sanger <ref> [Sanger 91] </ref> which is based on the ability to predict the squared error. The reasoning behind the rating method is explained here in an intuitive rather than formal way, along the lines of [Sutton and Matheus 91].
Reference: [Sanger, et al. 92] <author> Sanger, T. D., Sutton, R. S. and Matheus, C. J. </author> <title> (1992) Iterative Construction of Sparse Polynomial Approximations. </title> <editor> In Moody, Hanson and Lippmann (ed.s), </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Each classifier was trained and tested on data from all sessions of one subject. For this project, the tasks focused on were "math" and "letter" (eyes closed). 4.2 Sutton's Algorithm Sutton and Matheus' algorithm <ref> [Sutton and Matheus 91, Sanger, et al. 92] </ref> for sparse polynomial construction was used on the EEG features in order to find the features that could be used in a linear discriminant classifier. The algorithm was designed to construct polynomials in the presence of irrelevant inputs.
Reference: [Shiao-Lin, et al. 93] <author> Lin S., Tsai Y. and Liou C. </author> <title> (1993) Conscious mental tasks and their EEG signals. Medical. </title> & <journal> Biological Engineering & Computing, </journal> <volume> 31, </volume> <pages> 421-425. </pages>
Reference-contexts: An external stimulus, such as a flashing light or an audible noise, etc., triggers the signal. Attempts like Keirn's to classify a signal that has been internally created solely from the subject's mental processing are much rarer. One such effort used Kohonen nets to classify the signals <ref> [Shiao-Lin, et al. 93] </ref>. They recorded new data, but the tasks and features used were based on Keirn's work. 2.1 Data Measurements were taken from six sensors located on the scalp. The standard "10-20 system" [Jasper 58] of electrode placement was used (see figure 1).
Reference: [Sutton and Matheus 91] <author> Sutton, R. S. and Matheus, C. J. </author> <title> (1991) Learning Polynomial functions by feature Construction. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning. </booktitle> <address> Chicago, Illinois, </address> <month> June 27-29. 22 </month>
Reference-contexts: Finally, features had to be picked by "hand", a laborious and time consuming task. Features are propertied of the data which are believe to signified and identified the item The next section summarizes Keirn's work. Presented next is the data taken from Keirn's experiment, the feature selection algorithm <ref> [Sutton and Matheus 91] </ref> used, and the loom method employed to determine classification accuracy. The accuracy achieved for each subject is summarized and compared to Keirn's results. <p> The Sutton and Matheus algorithm was chosen as a novel and untried procedure of feature selection <ref> [Sutton and Matheus 91] </ref>. Feature selection is the process of extracting from the data the information necessary to perform the classification. The information, the features, is used to determine the classification. The problem is to find a set small enough to train the classifier, without throwing away needed information. <p> Each classifier was trained and tested on data from all sessions of one subject. For this project, the tasks focused on were "math" and "letter" (eyes closed). 4.2 Sutton's Algorithm Sutton and Matheus' algorithm <ref> [Sutton and Matheus 91, Sanger, et al. 92] </ref> for sparse polynomial construction was used on the EEG features in order to find the features that could be used in a linear discriminant classifier. The algorithm was designed to construct polynomials in the presence of irrelevant inputs. <p> X; x and Xsquared in the description are 2-dimensional arrays. x [fl][i] is the i th column of x. Y; y and SquaredError are 1-dimensional arrays. Y [i] is the i th element of y. The following is a summary of the algorithm from <ref> [Sutton and Matheus 91] </ref>. This description corrects errors and typos made in their paper, plus adds a few comments. <p> The product can use just a single feature (i.e. x 1 x 1 ). If the new feature is already in the poly nomial, the next highest pair is used. The joint method makes m "joints" (potential new 1 This step was omitted in the <ref> [Sutton and Matheus 91] </ref> paper. 9 features) as the product of two existing features. m is usually set to the number of initial features (n). The joints are selected based on the m highest products of the two constituent features' potentials. <p> The rating method developed by Sutton and Matheus was a modification of the method from Sanger [Sanger 91] which is based on the ability to predict the squared error. The reasoning behind the rating method is explained here in an intuitive rather than formal way, along the lines of <ref> [Sutton and Matheus 91] </ref>. Viewing the polynomial as a single layered network which has been trained, some of the outputs (y) are predicted with a small error, others with a large error. <p> The approach proposed by Sanger (1991) measures the ability of a feature to predict the squared error by calculating the correlation between the squared error and the square of the feature's value. The motivation for <ref> [Sutton and Matheus 91] </ref> method is that the correlation of x i and x i x j is similar, resulting in the same terms being added. The algorithm becomes "stuck". The modification done by [Sutton and Matheus 91] was to give the rating more of a competive quality that discourages new <p> The motivation for <ref> [Sutton and Matheus 91] </ref> method is that the correlation of x i and x i x j is similar, resulting in the same terms being added. The algorithm becomes "stuck". The modification done by [Sutton and Matheus 91] was to give the rating more of a competive quality that discourages new features that do not improve the prediction squared 10 error. The competion is done by a regression of the squared error over the squared features (Regess (e 2 jx 2 )). <p> The smaller number of features would cause less over fitting in the data. This would result in better classification and fewer irrelevant terms produced. This was in contrast to the "pruning" method suggested by Sutton and Matheus <ref> [Sutton and Matheus 91] </ref>. Although not detailed, the pruning method would presumably involve using the potentials to determine unneeded features. As before, the polynomial would start with all the features. At the end of each cycle, an other ranking of the features would be made. <p> The information available depends on the correlations linear regression can find. If the regression can not come up with the right features, it is essentially taking blind "stabs" at the solution. One main difference from the examples in <ref> [Sutton and Matheus 91] </ref> and the EEG data was that the potentials of the individual features did not decrease when it was paired to make a compound feature. <p> Since the results were actually slightly higher than the add method, we can conclude that with the EEG data the method failed to extract and use any information from the data. However, the random method fails to find the solution for the <ref> [Sutton and Matheus 91] </ref> examples. The add method works 90% of the time on those problems. The goal of automatic feature creation, while interesting theoretically, is of less practical importance for this problem than initially believed.
References-found: 7

