URL: http://http.cs.berkeley.edu/~asah/papers/other/to-read/fauve-ds-5.ps.gz
Refering-URL: http://http.cs.berkeley.edu/~asah/papers/other/to-read/
Root-URL: http://www.cs.berkeley.edu
Title: Achieving Incremental Consistency among Autonomous Replicated Databases  
Author: Stefano Ceri Maurice A.W. Houtsma Arthur M. Keller Pierangela Samarati 
Address: Italy.  
Note: This work was performed in the context of the Fauve-project and started while some of the authors were visiting Stanford, and partially supported by NSF grant IRI-9007753 Stefano Ceri is partially supported by the LOGIDATA+ project  The research of Maurice Houtsma has been made possible by a fellowship of the Royal Netherlands Academy of Arts and Sciences. Arthur Keller is partially supported by the Center for Integrated Systems at Stanford University. -Pierangela Samarati was partially supported by a scholarship from the Rotary Foundation.  
Affiliation: Politecnico di Milano  University of Twente  Stanford University  Universita' di Milano  of CNR  
Abstract: In this paper, we present methods for supporting autonomous updates in replicated databases. Autonomous updates are of particular importance to applications that cannot tolerate the delay and vulnerability due to synchronous update methods (2PC). We separate the notion of replication consistency, meaning that all copies have the same value and reflect the same update transactions, from behavior consistency, meaning that transaction execution reflects all integrity constraints. The method proposed in this paper supports independent updates during network partitioning, and achieves a consistent final database state on recovery of partitions that reflects all actions that were executed during network partitioning. To this purpose, we describe a reconciliation procedure that applies all actions to each updated data item in the order in which they were originally performed, possibly independently; therefore, reconciliation may require the undo and redo of actions. We formally define the properties that need to hold for our approach to work, and we prove that our reconciliation procedure respects these properties. Our approach is incremental, as it can be applied to any sequence of partition-ings and recoveries; reconciliation occurs whenever possible or at the user's desire. However, we trade consistent behavior for update availability: in general, there is no guarantee that the execution will reflect all global consistency constraints. Localization techniques for constraints can be used to support consistent behavior for 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Agrawal and A. El Abbadi, </author> <title> "The tree quorum protocol: an efficient approach for managing replicated data," </title> <booktitle> in Proc. 16th Int. Conf. on VLDB, </booktitle> <address> Brisbane, </address> <month> Aug. </month> <year> 1990, </year> <pages> pp. 243-254. </pages>
Reference-contexts: Even some banking applications do not need atomic updates of replicas. Therefore, several protocols have been developed for updating replicated data without the requirement of atomic and synchronous update to each and every replica <ref> [1, 8, 11, 13, 16, 17, 20, 21] </ref>. Some of these protocols work by transforming global constraints on the data into local constraints that should hold on the replicas; each replica may then independently be updated as long as its local constraints are satisfied [5].
Reference: [2] <author> R. Alonso, D. Barbara. H. Garcia Molina, S. Abad, "Quasi-copies: </author> <title> efficient data sharing for information retrieval systems," </title> <booktitle> Proc. of the Int. Conf. on Extending Data Base Technology, </booktitle> <address> EDBT'88. </address>
Reference: [3] <author> P.M.G. Apers and G. Wiederhold, </author> <title> "Transaction classification to survive a network partition," </title> <type> Technical report STAN-CS-85-1053, </type> <institution> Stanford University, </institution> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: From our classification [7], we noticed that most protocols do not deal successfully with network partition. In case of network partitions, either updates are accepted on a subset of the sites, or transactions that are known not to lead to inconsistency are the only ones allowed to run <ref> [3] </ref>. In this paper, we develop a strategy for achieving incremental consistency by allowing updates on arbitrary sites during network partitioning; updates may take place on replicas as if the system were in a normal mode of operation.
Reference: [4] <author> D. Barbara and H. Garcia-Molina, </author> <title> "The case for controlled inconsistency in replicated data," </title> <booktitle> Proc. of the Workshop on Management of Replicated Data, </booktitle> <address> Houston, TX, </address> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Although many commercial distributed databases support atomic updates through two-phase commit, they have intrinsic disadvantages such as cost, delay, and reduced availability [10]. Moreover, many applications do not require atomic updates <ref> [4] </ref>. For instance, in airline reservation systems it is often unacceptable that a replica be unavailable while another replica is being updated, or during a network partitioning.
Reference: [5] <author> D. Barbara and H. Garcia-Molina, </author> <title> The demarcation protocol: a technique for maintaining arithmetic constraints in distributed database systems, </title> <institution> CS-TR-320-91, Princeton University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Some of these protocols work by transforming global constraints on the data into local constraints that should hold on the replicas; each replica may then independently be updated as long as its local constraints are satisfied <ref> [5] </ref>. Other protocols recover from violations of global constraints by means of compensating actions [12, 22]. From our classification [7], we noticed that most protocols do not deal successfully with network partition. <p> by communication software, we are currently working on protocols that integrate the detection of splitting and merging of nodes within the normal behavior of transactions, by making use of specific messages; see Section 6. global constraints may be strictly enforced, as guaranteed for specific replica updates by the Demarcation Protocol <ref> [5] </ref>; however, truly independent behavior is not achieved anymore. For simplicity, we assume that the ROWA (read one, write all) protocol is used to maintain consistency within a group of sites.
Reference: [6] <author> P.A. Bernstein, V. Hadzilacos, N. Goodman, </author> <title> Concurrency Control and Recovery in Database Systems, </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: In such systems it is unacceptable that transactions be blocked when some of the sites fail or become unreachable; however, propagation of updates among the participating sites frequently leads to such blocking. Many strategies have been developed for propagating updates in replicated databases <ref> [6] </ref>. An overview of this topic has been given by us in [7], and for reasons of brevity we will not dwell on this topic here but refer the interested reader to [7].
Reference: [7] <author> S. Ceri, M.A.W. Houtsma, A.M. Keller, and P. Samarati, </author> <title> "A Classification of Update Methods for Replicated Databases," </title> <institution> STAN-CS-91-1932, Stanford University, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Many strategies have been developed for propagating updates in replicated databases [6]. An overview of this topic has been given by us in <ref> [7] </ref>, and for reasons of brevity we will not dwell on this topic here but refer the interested reader to [7]. Basically, we can state that atomic updates in replicated databases form a major obstacle to the spreading of distributed database applications. <p> Many strategies have been developed for propagating updates in replicated databases [6]. An overview of this topic has been given by us in <ref> [7] </ref>, and for reasons of brevity we will not dwell on this topic here but refer the interested reader to [7]. Basically, we can state that atomic updates in replicated databases form a major obstacle to the spreading of distributed database applications. Although many commercial distributed databases support atomic updates through two-phase commit, they have intrinsic disadvantages such as cost, delay, and reduced availability [10]. <p> Other protocols recover from violations of global constraints by means of compensating actions [12, 22]. From our classification <ref> [7] </ref>, we noticed that most protocols do not deal successfully with network partition. In case of network partitions, either updates are accepted on a subset of the sites, or transactions that are known not to lead to inconsistency are the only ones allowed to run [3].
Reference: [8] <author> S. Ceri, M.A.W. Houtsma, A.M. Keller, and P. Samarati, </author> <title> "The case for independent updates," </title> <booktitle> in Proc. 2nd Workshop on the Management of Replicated Data, </booktitle> <address> Monterey, CA, </address> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Even some banking applications do not need atomic updates of replicas. Therefore, several protocols have been developed for updating replicated data without the requirement of atomic and synchronous update to each and every replica <ref> [1, 8, 11, 13, 16, 17, 20, 21] </ref>. Some of these protocols work by transforming global constraints on the data into local constraints that should hold on the replicas; each replica may then independently be updated as long as its local constraints are satisfied [5].
Reference: [9] <author> S. Ceri, M.A.W. Houtsma, A.M. Keller, and P. Samarati, </author> <title> "Independent updates and incremental consistency in replicated databases," </title> <note> in preparation. </note>
Reference-contexts: However, the algorithm may easily be extended to allow for reconciliation of multiple partitions. The algorithm we present is a centralized one: one of the sites behaves as a coordinator, determines the new status and communicates it to the other sites. In <ref> [9] </ref> we describe several other reconciliation algorithms that are more distributed in nature. <p> The work reported here gives the foundation to a family of methods for independent updates that we are currently investigating. Other aspects of this research are reported in <ref> [9] </ref>. Several variations of the reconciliation algorithm described in the paper are possible; each of them leads to a different amount of distributed processing during the reconciliation phase and is justified by some applications.
Reference: [10] <author> S. Ceri and G. Pelegatti, </author> <title> Distributed database systems, </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Basically, we can state that atomic updates in replicated databases form a major obstacle to the spreading of distributed database applications. Although many commercial distributed databases support atomic updates through two-phase commit, they have intrinsic disadvantages such as cost, delay, and reduced availability <ref> [10] </ref>. Moreover, many applications do not require atomic updates [4]. For instance, in airline reservation systems it is often unacceptable that a replica be unavailable while another replica is being updated, or during a network partitioning.
Reference: [11] <author> A. El Abbadi, D. Skeen, F. Christian, </author> <title> "An efficient fault-tolerant protocol for replicated data management," </title> <booktitle> Proc. 4th ACM SIGACT-SIGMOD Symp. on Principles of Database Systems, </booktitle> <address> Portland, OR, </address> <month> March </month> <year> 1985, </year> <pages> pp. 215-228. </pages>
Reference-contexts: Even some banking applications do not need atomic updates of replicas. Therefore, several protocols have been developed for updating replicated data without the requirement of atomic and synchronous update to each and every replica <ref> [1, 8, 11, 13, 16, 17, 20, 21] </ref>. Some of these protocols work by transforming global constraints on the data into local constraints that should hold on the replicas; each replica may then independently be updated as long as its local constraints are satisfied [5].
Reference: [12] <author> H. Garcia-Molina and K. Salem, "Sagas," </author> <booktitle> Proc. ACM SIGMOD'87, </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: Other protocols recover from violations of global constraints by means of compensating actions <ref> [12, 22] </ref>. From our classification [7], we noticed that most protocols do not deal successfully with network partition.
Reference: [13] <author> D.K. Gifford, </author> <title> "Weighted voting for replicated data," </title> <booktitle> Proc. 7th ACM-SIGOPS Symp. on Operating Systems Principles, </booktitle> <address> Pacific Grove, CA, </address> <month> Dec. </month> <year> 1979, </year> <pages> pp. 150-159. </pages>
Reference-contexts: Even some banking applications do not need atomic updates of replicas. Therefore, several protocols have been developed for updating replicated data without the requirement of atomic and synchronous update to each and every replica <ref> [1, 8, 11, 13, 16, 17, 20, 21] </ref>. Some of these protocols work by transforming global constraints on the data into local constraints that should hold on the replicas; each replica may then independently be updated as long as its local constraints are satisfied [5].
Reference: [14] <author> J.N. Gray and M. Anderton, </author> <title> "Distributed computer systems: four case studies", </title> <journal> Proc. of the IEEE, </journal> <volume> Vol. 75, No. 5, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: In distributed database systems, the availability of replicas at each site increases read-availability and thus greatly improves read-performance. Many distributed databases are indeed created as a federation of autonomous and possibly heterogeneous databases, where only a portion of data need to be shared <ref> [14] </ref>. In such systems it is unacceptable that transactions be blocked when some of the sites fail or become unreachable; however, propagation of updates among the participating sites frequently leads to such blocking. Many strategies have been developed for propagating updates in replicated databases [6]. <p> As another example, consider a replicated inventory control system, where part descriptions are stored at the various sites of a company producing or selling those parts <ref> [14] </ref>; this kind of application does not necessarily require immediate propagation of updates, but is inherently discrete and batch-processing oriented. Even some banking applications do not need atomic updates of replicas.
Reference: [15] <author> B. Kahler and O. Risnes, </author> <title> "Extending logging for database snapshot refresh," </title> <booktitle> in Proc. 13th Int. Conf. on Very Large Data Bases, </booktitle> <address> Brighton, England, </address> <year> 1987, </year> <pages> pp. 389-398. </pages>
Reference-contexts: The use of the history log for propagation of updates was suggested in <ref> [15] </ref>, The paper is organised as followed. In Section 2 we extensively discuss our model of a replicated database and its behavior; this includes discussing partitions, transactions, reconciliation, and their formal properties. In Section 3 we discuss the normal execution of transactions.
Reference: [16] <author> N. Krishnakumar and A.J. Bernstein, </author> <title> "Bounded ignorance in replicated systems," </title> <booktitle> in Proc. </booktitle> <address> ACM-PODS'91, Denver, CO, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Even some banking applications do not need atomic updates of replicas. Therefore, several protocols have been developed for updating replicated data without the requirement of atomic and synchronous update to each and every replica <ref> [1, 8, 11, 13, 16, 17, 20, 21] </ref>. Some of these protocols work by transforming global constraints on the data into local constraints that should hold on the replicas; each replica may then independently be updated as long as its local constraints are satisfied [5].
Reference: [17] <author> A. Kumar and A. Segev, </author> <title> "Optimizing voting-type algorithms for replicated data," </title> <booktitle> in Advances in Database Technology-EDBT'88, </booktitle> <editor> J.W. Schmidt, S. Ceri, and M. Mis-sikoff (Eds.), </editor> <volume> LNCS 303, </volume> <year> 1988, </year> <pages> pp. 428-442. </pages>
Reference-contexts: Even some banking applications do not need atomic updates of replicas. Therefore, several protocols have been developed for updating replicated data without the requirement of atomic and synchronous update to each and every replica <ref> [1, 8, 11, 13, 16, 17, 20, 21] </ref>. Some of these protocols work by transforming global constraints on the data into local constraints that should hold on the replicas; each replica may then independently be updated as long as its local constraints are satisfied [5].
Reference: [18] <author> L. Lamport, </author> <title> "Time, clocks, and ordering of events in a distributed system", </title> <journal> CACM, </journal> <volume> Vol. 21, No.7, </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: We assume the existence of a Lamport-style timestamping mechanism that allows us to produce a global ordering of actions executed at different sites, reflecting all the observable precedences between actions <ref> [18] </ref>. 2.1 Model of partitions Initially, all sites are connected and have the same database state. During operation, failures may occur in the system and one or more sites may become disconnected.
Reference: [19] <author> D.S. Parker, et al., </author> <title> "Detection of mutual inconsistency in distributed systems," </title> <journal> IEEE T-SE, </journal> <month> May </month> <year> 1983. </year>
Reference-contexts: More precisely, given the reception vector R s for site s, the entry R s [p] denotes the the time of execution of the last action at site p of which site s is informed. (The idea of using a vector for detecting inconsistency among sites, was proposed before in <ref> [19] </ref>.) Reconciliation is viewed as any action w.r.t. the reception vector, and will therefore be recorded as such in the reception vector. When an action is executed in a partition, all participating sites will update the entries for the participating sites in their reception vector with the time of execution.
Reference: [20] <author> C. Pu and A. Leff, "Epsilon-Serializability," </author> <type> Technical report No. </type> <institution> CUCS-054-90, Columbia University, </institution> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: Even some banking applications do not need atomic updates of replicas. Therefore, several protocols have been developed for updating replicated data without the requirement of atomic and synchronous update to each and every replica <ref> [1, 8, 11, 13, 16, 17, 20, 21] </ref>. Some of these protocols work by transforming global constraints on the data into local constraints that should hold on the replicas; each replica may then independently be updated as long as its local constraints are satisfied [5].
Reference: [21] <author> C. Pu and A. Leff, </author> <title> "Replica control in distributed systems: an asynchronous approach,", </title> <booktitle> Proc. ACM SIGMOD'91, </booktitle> <address> Denver, CO, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Even some banking applications do not need atomic updates of replicas. Therefore, several protocols have been developed for updating replicated data without the requirement of atomic and synchronous update to each and every replica <ref> [1, 8, 11, 13, 16, 17, 20, 21] </ref>. Some of these protocols work by transforming global constraints on the data into local constraints that should hold on the replicas; each replica may then independently be updated as long as its local constraints are satisfied [5].
Reference: [22] <author> A. Reuter and H. W achter, </author> <title> "The contract model," </title> <journal> IEEE Database Engineering bulletin Vol. </journal> <volume> 14, No. 1, </volume> <month> March </month> <year> 1991. </year>
Reference-contexts: Other protocols recover from violations of global constraints by means of compensating actions <ref> [12, 22] </ref>. From our classification [7], we noticed that most protocols do not deal successfully with network partition.
Reference: [23] <author> S.K. Sarin, C.W. Kaufman, and J.E. </author> <title> Somers "Using history information to process delayed database updates," </title> <booktitle> Proc. 12th Int. Conf. on Very Large Data Bases, </booktitle> <address> Kyoto, Japan, </address> <month> Aug. </month> <year> 1986, </year> <pages> pp. 71-78. </pages>
Reference-contexts: The mechanisms presented in this paper require that applications be action-based (see Section 2.2), and that all sites maintain a complete copy of the history log (which contains the actions that have led to the current database state) 1 . Similar assumptions are made in <ref> [23] </ref>, which uses timestamp-based concurrency control and proposes to immediately apply updates to replicated data in their arrival order, yet possibly restoring inconsistencies when such arrivals violate the timestamp ordering of transactions. The mechanism of [23] does not consider network partitions; it achieves consistency by undoing and re-executing updates which are <p> Similar assumptions are made in <ref> [23] </ref>, which uses timestamp-based concurrency control and proposes to immediately apply updates to replicated data in their arrival order, yet possibly restoring inconsistencies when such arrivals violate the timestamp ordering of transactions. The mechanism of [23] does not consider network partitions; it achieves consistency by undoing and re-executing updates which are out-of-order, and saves some of these operations at the cost of restoring additional information, such as read/write sets for update transactions.
References-found: 23

