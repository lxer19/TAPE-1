URL: http://www.cs.washington.edu/homes/neal/work.ps
Refering-URL: http://www.cs.washington.edu/homes/neal/pr-pub.html
Root-URL: 
Email: neal@cs.washington.edu, etzioni@cs.washington.edu  
Phone: (206) 616-1849 FAX: (206) 543-2969  
Title: Insights from Machine Learning for Plan Recognition  
Author: Neal Lesh and Oren Etzioni 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington,  
Abstract: This paper explores the benefits of adapting techniques from inductive concept learning to plan recognition. A powerful notion in concept learning is characterizing inductive systems by their bias, i.e. the implicit assumptions which justify the conclusions an inductive system produces. We present a spectrum of possible biases for plan recognition. We evaluate these biases based on how accurately they predict how people achieve goals in Unix. We also adapt algorithms for maintaining version spaces to produce a goal recognizer that runs in time sublin-ear in the number of potential goals. We show a factor of 5 to 10 speedup, on data collected in Unix, over a more straightforward approach which enumerates every potential goal.
Abstract-found: 1
Intro-found: 1
Reference: [ Chapman, 1987 ] <author> D. Chapman. </author> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelligence, </journal> <volume> 32(3) </volume> <pages> 333-377, </pages> <year> 1987. </year>
Reference-contexts: What if the actor does take Mulberry Street to get to the dentist? If the actors under observation act supoptimally, then B fl will often produce false conclusions. What are the alternative plan biases to B fl ? The B j= bias, based on the Modal Truth Criterion <ref> [ Chapman, 1987 ] </ref> , states that the actor's plan must achieve the actor's goal. The B j= bias is extremely weak because it allows unlimited irrelevant actions into the actor's plan. Suppose P is a plan to make breakfast.
Reference: [ Charniak and Goldman, 1991 ] <author> E. Charniak and R. Gold-man. </author> <title> A probablistic model of plan recognition. </title> <booktitle> In Proc. 9th Nat. Conf. on A.I., </booktitle> <volume> volume 1, </volume> <pages> pages 160-5, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: As we have shown, an unbiased recognizer could not make any predictions about the unobserved actions. Thus, every plan recognition system must have a bias. Many current plan recognizers take, as input, a plan hierarchy (or set of prior probabilities <ref> [ Charniak and Goldman, 1991 ] </ref> ) which describes the actor's typical behavior in some domain. General purpose recognition engines produce the actor's plan and goals given the plan hierarchy and the observed actions. The bias of these systems is the input plan hierarchy (or probabilities).
Reference: [ Etzioni et al., 1992 ] <author> O. Etzioni, S. Hanks, D. Weld, D. Draper, N. Lesh, and M. Williamson. </author> <title> An Approach to Planning with Incomplete Information. </title> <booktitle> In Proc. 3rd Int. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: We give our subjects English task descriptions, e.g. "Compress all files of size greater than 100,000 bytes in the /bin directory", and then record the Unix commands they execute. We process this raw data into actions and goals in UWL, a language for representing goals, actions, and plans <ref> [ Etzioni et al., 1992 ] </ref> . 1 Our preliminary investigation suggests, as expected, that the B fl bias (that people execute optimal plans) is too strong. Our subjects often executed clearly suboptimal sequences of actions to solve the goals we gave them.
Reference: [ Kautz, 1987 ] <author> H. Kautz. </author> <title> A Formal Theory Of Plan Recognition. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <year> 1987. </year>
Reference-contexts: For example, an inductive system might generalize from the statements "Nixon lied" and "Reagan lied" to produce statements such as "male Republican presidents lie" or "politicians lie". A more conservative conclusion, warranted by deduction, is simply that "Rea-gan and Nixon lied". Plan recognition (e.g. <ref> [ Kautz, 1987; Pollack, 1990 ] </ref> ) is the task of inferring actors' plans and goals given a partial view of their behavior.
Reference: [ Lesh and Etzioni, 1995 ] <author> Neal Lesh and Oren Etzioni. </author> <title> A sound and fast goal recognizer. </title> <booktitle> In Proc. 15th Int. Joint Conf. on A.I., </booktitle> <year> 1995. </year>
Reference-contexts: We intend to empirically determine a reliable value for k max . 4 Version spaces Our objective is to build a goal recognizer that performs well in large, real domains. In <ref> [ Lesh and Etzioni, 1995 ] </ref> , we describe a mechanism which can be used to quickly decide whether a single candidate goal is consistent with the observed actions. <p> In other words, 3 These terms are carefully defined in <ref> [ Lesh and Etzioni, 1995 ] </ref> . the version space is the set of goal schemas which might describe the actor's goal, given the observations.
Reference: [ Mitchell, 1980 ] <author> Tom M. Mitchell. </author> <title> The need for biases in learning generalizations. </title> <type> Technical Report CBM-TR-117, </type> <institution> Rutgers University, </institution> <year> 1980. </year>
Reference-contexts: The Machine Learning community has produced many successful techniques and algorithms for concept learning. In this paper, we show how some of these techniques can be applied to improve our understanding of, and mechanisms for, plan recognition, including: * Characterizing inductive inference systems by their biases <ref> [ Mitchell, 1980 ] </ref> . Bias is an inductive system's basis for preferring one generalization over another. Bias affords a non-procedural means for describing which conclusions an inductive inference machine is licensed to draw from the observations.
Reference: [ Mitchell, 1982 ] <author> T. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: 1 Introduction This paper explores the benefits of adapting techniques from inductive concept learning to plan recognition. Although plan recognition is not obviously a learning problem, the process of generalization underlies both learning and recognition. Concept learning (e.g. <ref> [ Mitchell, 1982; Shavlik and Dietterich, 1990 ] </ref> ) is the task of producing a hypothesis which describes, or fits, a large set of data given only a small portion of that data. <p> We use this data to evaluate recognition systems, both in terms of accuracy and speed. Below, we discuss lessons learned from preliminary analysis of our data. * Efficient techniques for managing large sets of hy pothesis. We adapt the Candidate Elimination algorithm <ref> [ Mitchell, 1982 ] </ref> to build a goal recognizer that runs in time sublinear in the number of potential goals. By maintaining the boundaries of a version space of candidate goal schemas, we compute recognition on large sets of goal schemas without explicitly enumerating each individual goal schema. <p> S set convergence occurs when the S set contains exactly one goal. We ran our algorithms, coded in Lisp, on a SPARC-10. Each number is the average over data gathered from two to four subjects. to how the Candidate Elimination algorithm <ref> [ Mitchell, 1982 ] </ref> updates its S and G sets with a positive example. To use this code, we must be able to determine if a given goal is consistent, determine whether one goal is more specific than another goal, and produce the minimal generalizations of a given goal.
Reference: [ Pollack, 1990 ] <author> M. Pollack. </author> <title> Plans as Complex Mental Attitudes, </title> <address> pages 77-101. </address> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: For example, an inductive system might generalize from the statements "Nixon lied" and "Reagan lied" to produce statements such as "male Republican presidents lie" or "politicians lie". A more conservative conclusion, warranted by deduction, is simply that "Rea-gan and Nixon lied". Plan recognition (e.g. <ref> [ Kautz, 1987; Pollack, 1990 ] </ref> ) is the task of inferring actors' plans and goals given a partial view of their behavior.
Reference: [ Shavlik and Dietterich, 1990 ] <author> J. Shavlik and T. Dietterich, </author> <title> editors. </title> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction This paper explores the benefits of adapting techniques from inductive concept learning to plan recognition. Although plan recognition is not obviously a learning problem, the process of generalization underlies both learning and recognition. Concept learning (e.g. <ref> [ Mitchell, 1982; Shavlik and Dietterich, 1990 ] </ref> ) is the task of producing a hypothesis which describes, or fits, a large set of data given only a small portion of that data.
Reference: [ Weida and Litman, 1992 ] <author> R. Weida and D. Litman. </author> <title> Terminological Reasoning with Constraint Networks and an Application to Plan Recognition. </title> <booktitle> In Proc. 3rd Int. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Remove all non-minimal goals from S n This closely the resembles the definition of generality of plans in <ref> [ Weida and Litman, 1992 ] </ref> . Our contribution is that BOCE exploits this partial answer both to produce an answer to question Q 3 and to maintain the version space without enumerating every goal in L.
References-found: 10

