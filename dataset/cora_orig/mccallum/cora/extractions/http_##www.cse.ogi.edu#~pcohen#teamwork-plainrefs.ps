URL: http://www.cse.ogi.edu/~pcohen/teamwork-plainrefs.ps
Refering-URL: http://www.cse.ogi.edu/~pcohen/online.html
Root-URL: http://www.cse.ogi.edu
Title: Teamwork  
Author: Philip R. Cohen Hector J. Levesque 
Note: This research was supported by a grant from the National Aeronautics and Space Administration to Stanford University (subcontracted to SRI International) for work on "Intelligent Communicating Agents," by a contract from ATR International to SRI International, and by a gift from the System Development Foundation. The second author was supported in part by a grant from the Natural Sciences and Engineering Research Council of Canada. This is a slightly revised version of a paper that appears in No^us 25/4, 1991. Fellow of the Canadian Institute for Advanced Research.  
Address: Toronto  
Affiliation: Artificial Intelligence Center and Center for the Study of Language and Information SRI International and  Dept. of Computer Science University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. S. Atiyah. </author> <title> An Introduction to the Law of Contract. </title> <publisher> Oxford University Press, Oxford, </publisher> <editor> U. K., </editor> <year> 1989. </year>
Reference-contexts: First, an interesting extension of our analysis would be to describe how the properties that result from the adoption of joint commitments and intentions compare with those inherent in the formulation of valid contracts <ref> [1] </ref>. We suspect that informal versions of many of the properties of contracts can be found in our notion of a joint commitment, especially in cases where there can be disagreement about the final contractual outcome. Historically, contracts (in British contract law) were regarded as formalized agreements.
Reference: [2] <author> M. Bratman. </author> <title> Intentions, Plans, and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <year> 1987. </year>
Reference-contexts: How do joint intentions to perform complex actions lead to appropriate intentions to perform the pieces? Assuming that an agent will only intend to do her own actions, what is her attitude towards the others' share? The functional role of joint intentions: Bratman <ref> [2] </ref> has argued that in the case of individuals, intentions play certain functional roles: they pose problems for agents, which can be solved by 3 means-end analysis; they rule out the adoption of intentions that conflict with existing ones; they dispose agents to monitor their attempts to achieve them; and, barring <p> Although agents can commit to other's actions, they do not intend them, as we will see shortly. 4.2 Individual Intention We adopt Bratman's <ref> [2] </ref> methodological concern for treating the future-directed properties of intention as primary, and the intention-in-action properties as secondary, contra Searle [20, 21]. <p> In our earlier paper [5], we also show how this analysis of intention satisfies Bratman's <ref> [2, 3] </ref> functional roles for intentions and solves his "package deal" problem, by not requiring agents also to intend the known side-effects of their intended actions, despite our possible-world account of belief and goal. <p> Rather, this property would only hold if it were necessary for the execution of the action or if the agents agreed to perform their actions in a more deliberate way, such as in a joint stepwise fashion. However, joint intentions do form a "screen of admissibility" <ref> [2] </ref>, analogous to those of individual commitments, because joint commitments and, hence, intentions must be consistent with the individual commitments.
Reference: [3] <author> M. Bratman. </author> <title> What is intention? In P. </title> <editor> R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: In our earlier paper [5], we also show how this analysis of intention satisfies Bratman's <ref> [2, 3] </ref> functional roles for intentions and solves his "package deal" problem, by not requiring agents also to intend the known side-effects of their intended actions, despite our possible-world account of belief and goal.
Reference: [4] <author> H. H. Clark and D. Wilkes-Gibbs. </author> <title> Referring as a collaborative process. </title> <journal> Cognition, </journal> <volume> 22 </volume> <pages> 1-39, </pages> <year> 1986. </year>
Reference-contexts: From our perspective, the signals of understanding and requests for them, which are so pervasive in ongoing discourse <ref> [4, 16, 19] </ref>, would thus be predictable as the means to attain the states of mutual belief that discharge this joint intention [8, 7].
Reference: [5] <author> P. R. Cohen and H. J. Levesque. </author> <title> Intention is choice with commitment. </title> <journal> Artificial Intelligence, </journal> <volume> 42(3), </volume> <year> 1990. </year>
Reference-contexts: We attempt to guard against specifications that are too weak, in that they would fail to guarantee intuitively appropriate outcomes, as well as specifications that are too strong, in that they would place unreasonable demands on agents. In our previous work <ref> [5] </ref>, we have presented a belief-goal-commitment model of the mental states of individuals in which intentions are specified not as primitive mental features, but as internal 2 commitments to perform an action while in a certain mental state. <p> Moreover, it is this divergence among the agents that makes communication necessary. Whereas the model of individual intention in our earlier work <ref> [5, 6] </ref> was sufficient to show how communicative acts were defined in terms of beliefs and intentions, and could be used to achieve various goals, it did so only from the perspective of each individual agent, by constraining the rational balance that agents maintain among their own beliefs, goals, commitments, intentions, <p> To see this in detail, we first briefly describe our analysis of individual commitment and intention, and then discuss the joint case. 4 Individual Commitment and Intention Our formal account of individual and joint commitments and intentions <ref> [5, 15] </ref> is given in terms of beliefs, mutual beliefs, goals, and events. In this paper, we will not present the formal language, but simply describe its features in general terms. <p> In our earlier paper <ref> [5] </ref>, we also show how this analysis of intention satisfies Bratman's [2, 3] functional roles for intentions and solves his "package deal" problem, by not requiring agents also to intend the known side-effects of their intended actions, despite our possible-world account of belief and goal. <p> All of these properties follow immediately from the fact that joint commitments entail individual commitments, and that these must be mutually consistent. For more specific analyses of the kinds of consistency predicted by our analysis of individual commitment and intention, see our more comprehensive paper <ref> [5] </ref>. In addition, as in the individual case, a group will monitor the success or failure of the joint effort, and, in particular, with joint stepwise execution, it will monitor the intermediate results as well. <p> a discrete synchronized way, but there is no reason not to generalize the notion to a continuous asynchronous mode, modeled perhaps by a function from the real numbers to the set of event types occurring at that point. 2 This definition differs slightly from that presented in our earlier work <ref> [5] </ref>, but that difference is immaterial here. 3 Of course, the agent may still intend to achieve p again if she is committed to doing so herself. 4 More accurately, we should say here that her goal is making it mutually believed that p had been true, in case p can
Reference: [6] <author> P. R. Cohen and H. J. Levesque. </author> <title> Rational interaction as the basis for communication. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Moreover, it is this divergence among the agents that makes communication necessary. Whereas the model of individual intention in our earlier work <ref> [5, 6] </ref> was sufficient to show how communicative acts were defined in terms of beliefs and intentions, and could be used to achieve various goals, it did so only from the perspective of each individual agent, by constraining the rational balance that agents maintain among their own beliefs, goals, commitments, intentions, <p> For the purposes of this paper, the two concepts have been treated as one. Future work will examine how speech acts of various kinds might be used to create agreements and, hence, joint commitments. Currently, our theory of speech acts <ref> [6] </ref> argues that the intended effect of a request is to get the addressee to form an individual commitment to do the requested action relative to the speaker's desire that he do so.
Reference: [7] <author> P. R. Cohen and H. J. Levesque. </author> <booktitle> Confirmations and joint action. In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney, Australia, August 1991. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: As for the communication criterion, by showing that agents who have adopted a joint intention commit themselves to attaining mutual belief about the status of that intention, we derive commitments that may lead to communication. For example, in other of our papers on dialogues about a task <ref> [7, 8] </ref>, we have analyzed how joint intentions to engage in the task lead to the discourse goals that underlie various speech acts. 8 Comparison with Other Analyses of Joint Intention Numerous analyses of concepts similar to joint intention have been given. <p> From our perspective, the signals of understanding and requests for them, which are so pervasive in ongoing discourse [4, 16, 19], would thus be predictable as the means to attain the states of mutual belief that discharge this joint intention <ref> [8, 7] </ref>. More generally, if such an account of dialogue were successful, it might then be possible to formalize cooperative conversation in a way that leads to the derivation of Gricean maxims. Finally, let us return to one of our original motivations, designing agents that can work together in groups.
Reference: [8] <author> P. R. Cohen, H. J. Levesque, J. Nunes, and S. L. Oviatt. </author> <title> Task-oriented dialogue as a consequence of joint activity. </title> <booktitle> In Proceedings of the Pacific Rim International Conference on Artificial Intelligence, </booktitle> <address> Nagoya, Japan, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: As for the communication criterion, by showing that agents who have adopted a joint intention commit themselves to attaining mutual belief about the status of that intention, we derive commitments that may lead to communication. For example, in other of our papers on dialogues about a task <ref> [7, 8] </ref>, we have analyzed how joint intentions to engage in the task lead to the discourse goals that underlie various speech acts. 8 Comparison with Other Analyses of Joint Intention Numerous analyses of concepts similar to joint intention have been given. <p> From our perspective, the signals of understanding and requests for them, which are so pervasive in ongoing discourse [4, 16, 19], would thus be predictable as the means to attain the states of mutual belief that discharge this joint intention <ref> [8, 7] </ref>. More generally, if such an account of dialogue were successful, it might then be possible to formalize cooperative conversation in a way that leads to the derivation of Gricean maxims. Finally, let us return to one of our original motivations, designing agents that can work together in groups.
Reference: [9] <author> A. I. Goldman. </author> <title> A Theory of Human Action. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year>
Reference-contexts: He defines a mutual intention to be each agent's having the intention to do her part and there being a mutual assumption that each agent has such intentions. Grosz and Sidner [10] propose a concept of shared plans, using Pollack's [17] analysis of plans and Goldman's <ref> [9] </ref> analysis of action. In their model, two agents have a shared plan if those agents mutually know that each agent intends to do her own part to achieve the jointly done action, and that each agent will do her part if and only if the other agent does likewise.
Reference: [10] <author> B. Grosz and C. Sidner. </author> <title> Plans for discourse. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: He defines a mutual intention to be each agent's having the intention to do her part and there being a mutual assumption that each agent has such intentions. Grosz and Sidner <ref> [10] </ref> propose a concept of shared plans, using Pollack's [17] analysis of plans and Goldman's [9] analysis of action.
Reference: [11] <author> J. Y. Halpern and Y. O. Moses. </author> <title> Knowledge and common knowledge in a distributed environment. </title> <booktitle> In Proceedings of the 3rd ACM Conference on Principles of Distributed Computing, </booktitle> <address> New York City, New York, </address> <year> 1984. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: to achieve p again if she is committed to doing so herself. 4 More accurately, we should say here that her goal is making it mutually believed that p had been true, in case p can become false again. 5 For readers familiar with the results in distributed systems theory <ref> [11] </ref> in which it is shown that mutual knowledge is impossible to obtain for computers by simply passing messages, we point out that those results do not hold for mutual beliefs acquired by default, nor for agents that can be co-present or communicate instantly. 6 Actually, agents do have the option
Reference: [12] <author> D. Harel. </author> <title> First-Order Dynamic Logic. </title> <publisher> Springer-Verlag, </publisher> <address> New York City, New York, </address> <year> 1979. </year>
Reference-contexts: These dynamic logic primitives are sufficient to form a significant class of complex actions, such as the "if-then-else" and "while-loops" familiar from computer science <ref> [12] </ref>. In all cases, the agents of the action in question are taken to be the set of agents of any of the primitive events that constitute the performance of the action.
Reference: [13] <author> J. R. Hobbs. </author> <title> Artificial intelligence and collective intentionality: Comments on Searle and on Grosz and Sidner. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: However, this introduces additional complexity, since one must be careful not to consider times when p was true before the adoption of the goal. 10 Whether Searle's example also counters Grosz and Sidner's analysis, as claimed by Hobbs <ref> [13] </ref>, is arguable.
Reference: [14] <author> H. J. Levesque. </author> <title> A logic of implicit and explicit belief. </title> <booktitle> In Proceedings of the National Conference of the American Association for Artificial Intelligence, </booktitle> <address> Austin, Texas, </address> <year> 1984. </year>
Reference-contexts: Analogous to the individual case, we assume that groups of agents correctly remember what their past mutual beliefs were. This account of the attitudes suffers from the usual possible-world problem of logical omniscience (see <ref> [14] </ref>, for example), but we will ignore that difficulty here. Moreover, we will take knowledge simply (and simplistically) to be true belief, and mutual knowledge to be true mutual belief.
Reference: [15] <author> H. J. Levesque, P. R. Cohen, and J. Nunes. </author> <title> On acting together. </title> <booktitle> In Proceedings of AAAI-90, </booktitle> <address> San Mateo, California, July 1990. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher> <pages> 20 </pages>
Reference-contexts: To see this in detail, we first briefly describe our analysis of individual commitment and intention, and then discuss the joint case. 4 Individual Commitment and Intention Our formal account of individual and joint commitments and intentions <ref> [5, 15] </ref> is given in terms of beliefs, mutual beliefs, goals, and events. In this paper, we will not present the formal language, but simply describe its features in general terms. <p> This property is captured by the following theorem, taken from our earlier work <ref> [15] </ref>. <p> status of p. 7 The normality conditions referred to here are merely that once the agent comes to a belief about the final status of the goal, she does not change her mind before arriving at a mutual belief with the others. 8 A more precise version of this definition <ref> [15] </ref> also requires that they mutually know when they started. 9 Another way to obtain a similar result might be to change the definition of persistent goal to say that an agent can drop her goal that p if she comes to believe that p has been made true, rather than
Reference: [16] <author> S. L. Oviatt and P. R. Cohen. </author> <title> Discourse structure and performance efficiency in interactive and noninteractive spoken modalities. </title> <booktitle> Computer Speech and Language, </booktitle> <year> 1991, </year> <note> in press. </note>
Reference-contexts: From our perspective, the signals of understanding and requests for them, which are so pervasive in ongoing discourse <ref> [4, 16, 19] </ref>, would thus be predictable as the means to attain the states of mutual belief that discharge this joint intention [8, 7].
Reference: [17] <author> M. E. Pollack. </author> <title> Plans as complex mental attitudes. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: He defines a mutual intention to be each agent's having the intention to do her part and there being a mutual assumption that each agent has such intentions. Grosz and Sidner [10] propose a concept of shared plans, using Pollack's <ref> [17] </ref> analysis of plans and Goldman's [9] analysis of action.
Reference: [18] <author> R. </author> <title> Power. Mutual intention. </title> <journal> Journal for the Theory of Social Behavior, </journal> <volume> 14(1) </volume> <pages> 85-102, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: Tuomela and Miller [22] propose a conceptual analysis of an individual agent's "we-intending" a group action. Essentially, that agent must intend to do her part of the action and believe it is mutually believed that the other members of the team will do their parts as well. Power <ref> [18] </ref> is perhaps the earliest researcher within the artificial intelligence research community to be concerned with modeling joint activity. He defines a mutual intention to be each agent's having the intention to do her part and there being a mutual assumption that each agent has such intentions.
Reference: [19] <author> E. A. Schegloff. </author> <title> Discourse as an interactional achievement: Some uses of unh-huh and other things that come between sentences. </title> <editor> In D. Tannen, editor, </editor> <title> Analyzing discourse: Text and talk. </title> <booktitle> Georgetown University Roundtable on Languages and Linguistics, </booktitle> <publisher> Georgetown University Press, </publisher> <address> Washington, D.C., </address> <year> 1981. </year>
Reference-contexts: From our perspective, the signals of understanding and requests for them, which are so pervasive in ongoing discourse <ref> [4, 16, 19] </ref>, would thus be predictable as the means to attain the states of mutual belief that discharge this joint intention [8, 7].
Reference: [20] <author> J. R. Searle. Intentionality: </author> <title> An Essay in the Philosophy of Mind. </title> <publisher> Cambridge University Press, </publisher> <address> New York, New York, </address> <year> 1983. </year>
Reference-contexts: Although agents can commit to other's actions, they do not intend them, as we will see shortly. 4.2 Individual Intention We adopt Bratman's [2] methodological concern for treating the future-directed properties of intention as primary, and the intention-in-action properties as secondary, contra Searle <ref> [20, 21] </ref>. By doing so, we avoid the notoriously difficult issue of how an intention self-referentially causes an agent to act, as discussed in [20], although many of those properties are captured by our account. <p> By doing so, we avoid the notoriously difficult issue of how an intention self-referentially causes an agent to act, as discussed in <ref> [20] </ref>, although many of those properties are captured by our account. Rather, we are concerned with how adopting an intention constrains the agents' adoption of other mental states. <p> Thus, it is crucial to our understanding that joint intention be regarded as a future-directed joint commitment. Although Searle's examples are motivated by cases of future-directed collective intention, Searle's analysis extends only his notion of intention-in-action <ref> [20] </ref> to the collective case.
Reference: [21] <editor> J. R. Searle. Collective intentionality. In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Although agents can commit to other's actions, they do not intend them, as we will see shortly. 4.2 Individual Intention We adopt Bratman's [2] methodological concern for treating the future-directed properties of intention as primary, and the intention-in-action properties as secondary, contra Searle <ref> [20, 21] </ref>. By doing so, we avoid the notoriously difficult issue of how an intention self-referentially causes an agent to act, as discussed in [20], although many of those properties are captured by our account. <p> Hence, such analyses are dissolved by doubt. Finally, because there is no requirement to start or terminate joint actions with mutual belief, these analyses make no predictions for communication. Searle <ref> [21] </ref> provides a different argument against approaches such as these, claiming that collective intentions are not reducible to individual intentions, even when supplemented with mutual beliefs.
Reference: [22] <author> R. Tuomela and K. Miller. </author> <title> We-intentions. </title> <journal> Philosophical Studies, </journal> <volume> 53 </volume> <pages> 367-389, </pages> <year> 1988. </year> <month> 21 </month>
Reference-contexts: Tuomela and Miller <ref> [22] </ref> propose a conceptual analysis of an individual agent's "we-intending" a group action. Essentially, that agent must intend to do her part of the action and believe it is mutually believed that the other members of the team will do their parts as well.
References-found: 22

