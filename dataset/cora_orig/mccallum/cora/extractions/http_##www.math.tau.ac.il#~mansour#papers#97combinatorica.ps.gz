URL: http://www.math.tau.ac.il/~mansour/papers/97combinatorica.ps.gz
Refering-URL: 
Root-URL: 
Title: On Construction of k-wise Independent Random Variables  
Author: Howard Karloff Yishay Mansour 
Keyword: 0-1 probability spaces.  
Abstract: A 0-1 probability space is a probability space (; 2 ; P ), where the sample space f0; 1g n for some n. A probability space is k-wise independent if, when Y i is defined to be the ith coordinate of the random n-vector, then any subset of k of the Y i 's is (mutually) independent, and it is said to be a probability space for p 1 ; p 2 ; :::; p n if P [Y i = 1] = p i . We study constructions of k-wise independent 0-1 probability spaces in which the p i 's are arbitrary. It was known that for any p 1 ; p 2 ; :::; p n , a k-wise independent probability space of size m(n; k) = k + k1 + k2 + + 0 always exists. We prove that for some p 1 ; p 2 ; :::; p n 2 [0; 1], m(n; k) is a lower bound on the size of any k-wise independent 0-1 probability space. For each fixed k, we prove that every k-wise independent 0-1 probability space when each p i = k=n has size (n k ). For a very large degree of independence|k = bffnc, for ff &gt; 1=2|and all p i = 1=2, we prove a lower bound on the size of 2 n (1 1 2ff ). We also give explicit constructions of k-wise independent 
Abstract-found: 1
Intro-found: 1
Reference: [ABI86] <author> N. Alon, L. Babai, and A. Itai, </author> <title> "A Fast and Simple Randomized Parallel Algorithm for the Maximal Independent Set Problem," </title> <booktitle> Journal of Algorithms 7 (1986), </booktitle> <pages> pages 567-583. </pages>
Reference-contexts: A great deal of research in theoretical computer science recently has dealt with the problem of constructing small probability spaces satisfying certain independence constraints, most commonly k-wise independence <ref> [Joffe74, Lub86, ABI86, Lub88] </ref>. <p> Aside from its interest as a fundamental issue, such small probability spaces can often be used for derandomizing randomized algorithms <ref> [ABI86, Lub86, KM93, Sch92, BR89, MNN89] </ref>: if one has a randomized algorithm which needs to be run on a k-wise independent 0-1 probability space, then it can be derandomized by running it deterministically on all the points in such a space (in parallel, if possible). <p> We start with lower bounds. Define m (n; k) = n k 1 + n 0 : For constant k, m (n; k) is fi (n k ). The best general lower bound is from <ref> [ABI86, CGHFRS85] </ref>; cf. [AS92]. Theorem 1. [ABI86, CGHFRS85] If H = (; 2 ; P ) is a k-wise independent 0-1 probability space for p 1 ; p 2 ; :::; p n and all p i 2 (0; 1), then jj m (n; bk=2c). <p> We start with lower bounds. Define m (n; k) = n k 1 + n 0 : For constant k, m (n; k) is fi (n k ). The best general lower bound is from <ref> [ABI86, CGHFRS85] </ref>; cf. [AS92]. Theorem 1. [ABI86, CGHFRS85] If H = (; 2 ; P ) is a k-wise independent 0-1 probability space for p 1 ; p 2 ; :::; p n and all p i 2 (0; 1), then jj m (n; bk=2c). In fact, the theorem of [ABI86, CGHFRS85] is actually stronger, since it <p> <ref> [ABI86, CGHFRS85] </ref>; cf. [AS92]. Theorem 1. [ABI86, CGHFRS85] If H = (; 2 ; P ) is a k-wise independent 0-1 probability space for p 1 ; p 2 ; :::; p n and all p i 2 (0; 1), then jj m (n; bk=2c). In fact, the theorem of [ABI86, CGHFRS85] is actually stronger, since it applies to random variables with arbitrary real range. We show that this lower bound is not always tight. We prove a lower bound of m (n; k) for certain specified probabilities: Theorem 2. <p> Note that neither Theorem 2 nor Theorem 3 can be improved by allowing the probabilities to be arbitrary, for there is a k-wise independent 0-1 probability space where all p i = 1=2 of size at most 2 (2n) bk=2c <ref> [ABI86] </ref>. How close to optimal are these theorems? It is implicit in the work of D. Koller and N.
Reference: [AS92] <author> N. Alon and J. Spencer, </author> <title> The Probabilistic Method, </title> <publisher> Wiley, </publisher> <year> 1992. </year>
Reference-contexts: We start with lower bounds. Define m (n; k) = n k 1 + n 0 : For constant k, m (n; k) is fi (n k ). The best general lower bound is from [ABI86, CGHFRS85]; cf. <ref> [AS92] </ref>. Theorem 1. [ABI86, CGHFRS85] If H = (; 2 ; P ) is a k-wise independent 0-1 probability space for p 1 ; p 2 ; :::; p n and all p i 2 (0; 1), then jj m (n; bk=2c).
Reference: [BR89] <author> B. Berger and J. Rompel, </author> <title> "Simulating (log c n)-wise Independence in NC," </title> <booktitle> Proc. 30 th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1989, </year> <pages> pages 2-7. 13 </pages>
Reference-contexts: Aside from its interest as a fundamental issue, such small probability spaces can often be used for derandomizing randomized algorithms <ref> [ABI86, Lub86, KM93, Sch92, BR89, MNN89] </ref>: if one has a randomized algorithm which needs to be run on a k-wise independent 0-1 probability space, then it can be derandomized by running it deterministically on all the points in such a space (in parallel, if possible).
Reference: [CGHFRS85] <author> B. Chor, O. Goldreich, J. H-astad, J. Friedman, S. Rudich, and R. Smolensky, </author> <title> "The Bit Extraction Problem or t-Resilient Functions," </title> <booktitle> Proc. 26 th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1985, </year> <pages> pages 396-407. </pages>
Reference-contexts: We start with lower bounds. Define m (n; k) = n k 1 + n 0 : For constant k, m (n; k) is fi (n k ). The best general lower bound is from <ref> [ABI86, CGHFRS85] </ref>; cf. [AS92]. Theorem 1. [ABI86, CGHFRS85] If H = (; 2 ; P ) is a k-wise independent 0-1 probability space for p 1 ; p 2 ; :::; p n and all p i 2 (0; 1), then jj m (n; bk=2c). <p> We start with lower bounds. Define m (n; k) = n k 1 + n 0 : For constant k, m (n; k) is fi (n k ). The best general lower bound is from <ref> [ABI86, CGHFRS85] </ref>; cf. [AS92]. Theorem 1. [ABI86, CGHFRS85] If H = (; 2 ; P ) is a k-wise independent 0-1 probability space for p 1 ; p 2 ; :::; p n and all p i 2 (0; 1), then jj m (n; bk=2c). In fact, the theorem of [ABI86, CGHFRS85] is actually stronger, since it <p> <ref> [ABI86, CGHFRS85] </ref>; cf. [AS92]. Theorem 1. [ABI86, CGHFRS85] If H = (; 2 ; P ) is a k-wise independent 0-1 probability space for p 1 ; p 2 ; :::; p n and all p i 2 (0; 1), then jj m (n; bk=2c). In fact, the theorem of [ABI86, CGHFRS85] is actually stronger, since it applies to random variables with arbitrary real range. We show that this lower bound is not always tight. We prove a lower bound of m (n; k) for certain specified probabilities: Theorem 2.
Reference: [Fri92] <author> J. Friedman, </author> <title> "On the Bit Extraction Problem," </title> <booktitle> Proc. 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1992, </year> <pages> pages 314-319. </pages>
Reference-contexts: In this case the best lower bound on size was m (n; bk=2c) (from Theorem 1). This quantity is bounded above by fi n , where fi &lt; 2 is a function only of ff. Based on the techniques of <ref> [Fri92] </ref> we improve this bound as well. 2 Theorem 5. Let 1=2 &lt; ff 1. Then the size of any bffnc-wise independent 0-1 probability space when all p i = 1=2 is at least 2 n (1 1 2ff ). <p> Suppose that there is an ff such that for all C 2 C d , ff = P v2C g (v): Then N 2 n1 (n + 2 2d)=(n + 1 d): The proof of the above theorem uses the Fourier analysis techniques of <ref> [Fri92] </ref>. Before giving the proof, we derive an interesting consequence. Corollary 13. Let H = (; 2 ; P ) be a k-wise independent 0-1 probability space where all p i = 1=2. Then jj 2 n ( 2k+2n 2k+2 ) = 2 n (1 n Proof.
Reference: [Joffe74] <author> A. Joffe, </author> <title> "On a Set of Almost Deterministic k-Independent Random Variables," </title> <booktitle> Annals of Probability 2 (1974), </booktitle> <pages> pages 161-162. </pages>
Reference-contexts: A great deal of research in theoretical computer science recently has dealt with the problem of constructing small probability spaces satisfying certain independence constraints, most commonly k-wise independence <ref> [Joffe74, Lub86, ABI86, Lub88] </ref>. <p> Now we give a construction and lemma due to Joffe <ref> [Joffe74] </ref>. Let r n be a prime. Suppose that s 1 ; s 2 ; :::; s n 2 [0; 1] satisfy s i = j i =r for all i, for some integers j i . Define a probability space, with (at most) n k points, as follows.
Reference: [KM93] <author> D. Koller and N. Megiddo, </author> <title> Constructing Small Sample Spaces Satisfying Given Constraints," </title> <booktitle> Proc. 25 th ACM Symposium on Theory of Computing, </booktitle> <year> 1993, </year> <pages> pages 268-277. </pages>
Reference-contexts: Aside from its interest as a fundamental issue, such small probability spaces can often be used for derandomizing randomized algorithms <ref> [ABI86, Lub86, KM93, Sch92, BR89, MNN89] </ref>: if one has a randomized algorithm which needs to be run on a k-wise independent 0-1 probability space, then it can be derandomized by running it deterministically on all the points in such a space (in parallel, if possible). <p> How close to optimal are these theorems? It is implicit in the work of D. Koller and N. Megiddo <ref> [KM93] </ref>, who seem to have initiated the systematic study of nonuniform spaces in theoretical computer science, that there is always a space of size at most m (n; k): Theorem 4. [KM93] Let n 1, let 1 d n, and let p 1 ; p 2 ; :::; p n 2 <p> Koller and N. Megiddo <ref> [KM93] </ref>, who seem to have initiated the systematic study of nonuniform spaces in theoretical computer science, that there is always a space of size at most m (n; k): Theorem 4. [KM93] Let n 1, let 1 d n, and let p 1 ; p 2 ; :::; p n 2 [0; 1]. Then there is a k-wise independent 0-1 probability space for p 1 ; :::; p n of size at most m (n; k). <p> Koller and N. Megiddo, whose paper <ref> [KM93] </ref>, in opening up the area of nonuniform spaces in theoretical computer science, inspired this one. We thank Shi-Chun Tsai for pointing out a typo.
Reference: [Lub86] <author> M. Luby, </author> <title> "A Simple Parallel Algorithm for the Maximal Independent Set Problem," </title> <journal> SIAM Journal on Computing 15 (1986), </journal> <pages> pages 1036-1053. </pages>
Reference-contexts: A great deal of research in theoretical computer science recently has dealt with the problem of constructing small probability spaces satisfying certain independence constraints, most commonly k-wise independence <ref> [Joffe74, Lub86, ABI86, Lub88] </ref>. <p> Aside from its interest as a fundamental issue, such small probability spaces can often be used for derandomizing randomized algorithms <ref> [ABI86, Lub86, KM93, Sch92, BR89, MNN89] </ref>: if one has a randomized algorithm which needs to be run on a k-wise independent 0-1 probability space, then it can be derandomized by running it deterministically on all the points in such a space (in parallel, if possible).
Reference: [Lub88] <author> M. Luby, </author> <title> "Removing Randomness in Parallel Computation Without a Processor Penalty," </title> <booktitle> Proc. 29 th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1988, </year> <pages> pages 162-173. </pages>
Reference-contexts: A great deal of research in theoretical computer science recently has dealt with the problem of constructing small probability spaces satisfying certain independence constraints, most commonly k-wise independence <ref> [Joffe74, Lub86, ABI86, Lub88] </ref>.
Reference: [MNN89] <author> R. Motwani, J. Naor, and J. Naor, </author> <title> "The Probabilistic Method Yields Deterministic Parallel Algorithms," </title> <booktitle> Proc. 30 th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1989, </year> <pages> pages 8-13. </pages>
Reference-contexts: Aside from its interest as a fundamental issue, such small probability spaces can often be used for derandomizing randomized algorithms <ref> [ABI86, Lub86, KM93, Sch92, BR89, MNN89] </ref>: if one has a randomized algorithm which needs to be run on a k-wise independent 0-1 probability space, then it can be derandomized by running it deterministically on all the points in such a space (in parallel, if possible).
Reference: [Sch92] <author> L. Schulman, </author> <title> "Sample Spaces Uniform on Neighborhoods," </title> <booktitle> Proc. 24 th ACM Symposium on Theory of Computing, </booktitle> <year> 1992, </year> <pages> pages 17-25. 14 </pages>
Reference-contexts: Aside from its interest as a fundamental issue, such small probability spaces can often be used for derandomizing randomized algorithms <ref> [ABI86, Lub86, KM93, Sch92, BR89, MNN89] </ref>: if one has a randomized algorithm which needs to be run on a k-wise independent 0-1 probability space, then it can be derandomized by running it deterministically on all the points in such a space (in parallel, if possible).
References-found: 11

