URL: http://www.cs.umn.edu/Research/Agassiz/Paper/dkchen.uniform.tpds95.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: dkchen@mti.sgi.com yew@cs.umn.edu  
Title: On Effective Execution of Non-Uniform DOACROSS Loops  
Author: Ding-Kai Chen Pen-Chung Yew 
Date: February 28, 1995  
Affiliation: Silicon Graphics Dept. of Computer Science Computer Systems University of Minnesota  
Note: Accepted and to appear in IEEE Trans. on Parallel and Distributed Systems.  This work was supported in part by the National Science Foundation under Grant Nos. NSF MIP89-20891, and NSF MIP93-07910, and the U.S. Department of Energy under grant No. DE-FG02-85ER25001.  
Abstract: It is extremely difficult to parallelize DOACROSS loops with non-uniform loop-carried dependences. In this paper, we present a static scheduling scheme with an accompanying synchronization strategy that can execute such DOACROSS loops effectively and efficiently. Our approach uses one of the parallelization techniques called Dependence Uniformization, which finds a small set of uniform dependence vectors to cover all possible non-uniform dependences in a DOACROSS loop. It differs from the previous schemes in that we demonstrate a better way to select the uniform dependence vectors. When used with the Static Strip Scheduling scheme, the proposed uniform dependence vectors set allows us to enforce dependences with more locality, which reduces the requirement of explicit synchronization considerably while retaining most of the parallelism. This paper describes the uniform dependence vectors selection strategy and the static strip scheduling scheme. The performance analysis and examples are also presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 63-76, </pages> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: Definition 3 A dependence is uniform if the dependence distance vector is a constant vector. That is, the distance is the same for all iterations. Otherwise, the dependence is non-uniform. Many techniques have been proposed to transform loops with loop-carried dependences to vector loops or DOALL loops <ref> [29, 3, 1] </ref>. To parallelize remaining loops with loop-carried dependences, proper data synchronization instructions have to be generated by the compiler to preserve the dependences. Synchronization for a uniform dependence is relatively easy to generate [15, 25] because the distance between the source and the sink statement is fixed.
Reference: [2] <author> G. M. </author> <title> Amdahl. Validity of the single processor approach to achieving large scale computing capability. </title> <booktitle> AFIPS Spring Joint Computer Conf., </booktitle> <volume> 30 </volume> <pages> 483-487, </pages> <month> August </month> <year> 1967. </year>
Reference-contexts: The need for synchronization can be determined either at compile-time [15, 25] for uniform (i.e., constant distance) dependences, or at run-time [30, 6] for complicated non-uniform dependences. If all these techniques fail, the DOACROSS loop must be executed serially which, according to Amdahl's law <ref> [2] </ref>, could severely degrade the performance [7]. The most common type of loop-carried dependences in engineering and scientific programs arises from references to the same array element in different loop iterations.
Reference: [3] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: Definition 3 A dependence is uniform if the dependence distance vector is a constant vector. That is, the distance is the same for all iterations. Otherwise, the dependence is non-uniform. Many techniques have been proposed to transform loops with loop-carried dependences to vector loops or DOALL loops <ref> [29, 3, 1] </ref>. To parallelize remaining loops with loop-carried dependences, proper data synchronization instructions have to be generated by the compiler to preserve the dependences. Synchronization for a uniform dependence is relatively easy to generate [15, 25] because the distance between the source and the sink statement is fixed. <p> The equations to be solved are: ae i 2 + 4i 3 = j 1 + j 2 : (3) We can use the algorithm presented in <ref> [3] </ref> to determine their integer solutions. If there is no integer solution, no synchronization is needed for this dependence. If there is exactly one solution, only the two iterations involved, ~ I p and ~ I q , need to synchronize their execution.
Reference: [4] <author> U. Banerjee. </author> <title> Unimodular transformation of double loops. </title> <booktitle> In 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: More recently, a unified framework for loop transformation, called Unimodular Transformation, has become increasingly popular <ref> [4, 28] </ref>. Given more accurate characterization on the range of each entry in the dependence distance vectors (Eq. (6)), we can determine when loop reversal, interchange, or skewing is legal.
Reference: [5] <institution> BBN Advanced Computers. Butterfly products overview, </institution> <year> 1987. </year>
Reference-contexts: Second, the strips are assigned to the processors statically so the senders and the receivers of the synchronization signals are known at compile time. This is especially useful in distributed shared memory machines such as IBM RP3 [21], BBN Butterfly <ref> [5] </ref>, and Stanford DASH [14], where a processor can access the local storage of other processors so each processor can busy-wait on the synchronization variables allocated locally but modified by remote processors. Such a technique reduces the busy-waiting traffic to the global communication network [26].
Reference: [6] <author> D.-K. Chen, J. Torrellas, and P.-C. Yew. </author> <title> An efficient algorithm for the run-time parallelization of DOACROSS loops. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pages 518-527, </pages> <month> November </month> <year> 1994. </year> <note> Also available as CSRD tech report No. 1345. </note>
Reference-contexts: If this is not possible, we can still execute the loop in parallel, provided proper synchronization is added to enforce loop-carried dependences. The need for synchronization can be determined either at compile-time [15, 25] for uniform (i.e., constant distance) dependences, or at run-time <ref> [30, 6] </ref> for complicated non-uniform dependences. If all these techniques fail, the DOACROSS loop must be executed serially which, according to Amdahl's law [2], could severely degrade the performance [7].
Reference: [7] <author> D.-K. Chen and P.-C. Yew. </author> <title> An empirical study of DOACROSS loops. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 620-632. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1991. </year> <note> Also available as CSRD tech report No. 1140. </note>
Reference-contexts: If all these techniques fail, the DOACROSS loop must be executed serially which, according to Amdahl's law [2], could severely degrade the performance <ref> [7] </ref>. The most common type of loop-carried dependences in engineering and scientific programs arises from references to the same array element in different loop iterations. In many cases, the synchronization overhead can be excessive to execute the loop in parallel, and the loop should remain serial.
Reference: [8] <author> D.-K. Chen and P.-C. Yew. </author> <title> A scheme for effective execution of irregular DOACROSS loops. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 285-292, </pages> <month> August </month> <year> 1992. </year> <note> Also available as CSRD tech report No. 1192. </note>
Reference-contexts: In our example, the maximal value of d 2 (T ) d 1 (T ) = 0:194 which results in a BDVS with a dependence cone size of 3 4 = 2:36. Example 5 (Proposed BDVS <ref> [8] </ref>) BDVS=f (1; 0); (5; 1); (2; 1)g (see Figure 6c). The BDVS in this example has the smallest dependence cone size, 0:66, in comparison with the previous ones. This is the kind of BDVS generated by our proposed scheme. <p> In addition, the index synchronization requires explicit synchronization for every iteration, which will increase proportionally to the total number of loop iterations. In one of our earlier papers <ref> [8] </ref>, we described our scheme focusing on doubly-nested loops. More recent work have led us to a new framework as described in Theorems 2 and 3. It can also easily be extended to a higher dimensional iteration space.
Reference: [9] <author> Z. Chen and W. Shang. </author> <title> On uniformization of affine dependence algorithms. </title> <booktitle> In 4th IEEE Conference on Parallel and Distributed Processing, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Because an optimal schedule is expensive to compute and we want to compare the BDVSs independently of the number of processors, we use instead the dependence cone size as the measure of choosing a good BDVS. Definition 4 (Dependence cone and dependence cone size <ref> [9] </ref>) For a dependence vector set V = f ~v 1 ; ~v 2 ; : : : ; ~v m g, the dependence cone denoted C (V ) is defined as the set C (V ) = f~v 2 R n : ~v = 1 ~v 1 + 2 ~v <p> In general, a larger dependence cone implies a smaller solution space for ~ P as shown in Figure 5a. The following theorem further shows the relation between the dependence cone size and the total execution time when a dependence cone is contained in another dependence cone. Theorem 1 <ref> [9] </ref> Let ~v 1 ; ~v 2 ; : : : ; ~v m be the vectors in BDVS 1 and ~u 1 ; ~u 2 ; : : : ; ~u k be the vectors in BDVS 2 . <p> Also, t (BDVS 1 ) t (BDVS 2 ) where t (BDVS) is the execution time of the loop using the corresponding optimal linear schedule of BDVS. Proof: See <ref> [9] </ref> for details. 2 That is, if the dependence cone for BDVS 1 is contained in BDVS 2 , then the solution space for ~ P 2 of BDVS 2 must be contained in the solution space for ~ P 1 of BDVS 1 . <p> That is, if we have (x; y) executed before (x; y + 1) because of (0; 1), then we cannot have (0; 1) because it will mean that we also can have (x; y + 1) executed before (x; y). Example 2 (Basic Method I in <ref> [9] </ref>) BDVS=f (1; 0); (0; 1); (22; 323)g : This BDVS is very similar to the previous one except for the last vector (22; 323). It is derived from lower bounds of each component of all the non-uniform dependence vectors. <p> Shaded areas are possible sink iterations of non-uniform dependences when the origin is the current iteration. 11 technique to optimize the problem, min T 2DCH fd i (T )g for i = 1; 2, where DCH is the dependence convex hull discussed in Section 2.3 <ref> [9] </ref>. Let x min and y min be the minimum of d 1 ; d 2 , respectively. <p> As a result, the dependences represented by Eq. (8) include some of the inverted real dependences. This problem can be eliminated by enumerating all possible direction vectors and combining them with the DCH to obtain the ultimate minimal values. Example 3 (Basic Method II in <ref> [9] </ref>) BDVS=f (1; 2); (1; 3); (3; 1); (0; 10)g : The validity argument of this BDVS follows from Eq. (8). <p> In one of our earlier papers [8], we described our scheme focusing on doubly-nested loops. More recent work have led us to a new framework as described in Theorems 2 and 3. It can also easily be extended to a higher dimensional iteration space. Chen and Shang <ref> [9] </ref> discussed several optimization metrics for affine dependence uniformization such as schedule lengths, BDVS cardinalities and dependence cone sizes. They also described two basic methods for generating initial BDVSs (see Example 2 and 3) and then showed how to optimize these initial BDVSs.
Reference: [10] <author> R. Cytron. </author> <title> Compile-time Scheduling and Optimization for Asychronous Machines. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1984. </year>
Reference-contexts: Many architectures and optimization compilers are aimed at efficient execution of parallel loops. One kind of DO loops, the DOALL loops, does not have loop-carried dependences, and it is relatively easy to obtain the desired speedup. On the other hand, it is much harder to parallelize DOACROSS loops <ref> [18, 19, 10, 11] </ref>, which contain loop-carried dependences. 1 They require the consideration of not only the scheduling and load-balancing issues but also the synchronization issues, which are usually much more difficult. There are several ways to deal with a DOACROSS loop.
Reference: [11] <author> R. Cytron. </author> <title> Doacross: Beyond vectorization for multiprocessors. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 836-845, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Many architectures and optimization compilers are aimed at efficient execution of parallel loops. One kind of DO loops, the DOALL loops, does not have loop-carried dependences, and it is relatively easy to obtain the desired speedup. On the other hand, it is much harder to parallelize DOACROSS loops <ref> [18, 19, 10, 11] </ref>, which contain loop-carried dependences. 1 They require the consideration of not only the scheduling and load-balancing issues but also the synchronization issues, which are usually much more difficult. There are several ways to deal with a DOACROSS loop.
Reference: [12] <author> P. Emrath, D. Padua, and P.-C. Yew. </author> <title> Cedar architecture and its software. </title> <booktitle> In Hawaii Int'l Conf. on System Science, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: Other vectors in the BDVS have to be enforced only at the boundaries of the adjacent strips with explicit synchronization using synchronization primitives such as post&wait [20], FULL/EMPTY bit [24], Cedar synchronization <ref> [12] </ref>, or by a process-based synchronization scheme as described in [25]. That is, many explicit synchronizations can be eliminated if the source and the sink of a dependence are in the same strip and are executed by the same processor.
Reference: [13] <author> L. Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Comm. ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> Feb. </month> <year> 1974. </year>
Reference-contexts: In the 2-dimensional case, the dependence cone size is in fact proportional to the angle expanded by the cone (see Figure 4). The significance of the dependence cone size can be explained by using the wavefront method <ref> [16, 13] </ref> as shown in Figure 3b where all of the iterations on each wavefront can be executed in parallel.
Reference: [14] <author> D. Lenoski, K. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Second, the strips are assigned to the processors statically so the senders and the receivers of the synchronization signals are known at compile time. This is especially useful in distributed shared memory machines such as IBM RP3 [21], BBN Butterfly [5], and Stanford DASH <ref> [14] </ref>, where a processor can access the local storage of other processors so each processor can busy-wait on the synchronization variables allocated locally but modified by remote processors. Such a technique reduces the busy-waiting traffic to the global communication network [26].
Reference: [15] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12), </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: If this is not possible, we can still execute the loop in parallel, provided proper synchronization is added to enforce loop-carried dependences. The need for synchronization can be determined either at compile-time <ref> [15, 25] </ref> for uniform (i.e., constant distance) dependences, or at run-time [30, 6] for complicated non-uniform dependences. If all these techniques fail, the DOACROSS loop must be executed serially which, according to Amdahl's law [2], could severely degrade the performance [7]. <p> To parallelize remaining loops with loop-carried dependences, proper data synchronization instructions have to be generated by the compiler to preserve the dependences. Synchronization for a uniform dependence is relatively easy to generate <ref> [15, 25] </ref> because the distance between the source and the sink statement is fixed. However, for a non-uniform dependence, it is very difficult because each iteration has varying dependence distances. In most cases, loops with nonuniform dependences must be executed serially. Consider the loop in Figure 2.
Reference: [16] <author> Y. Muroaka. </author> <title> Parallelism Exposure and Exploitation in Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, </institution> <month> Feb. </month> <year> 1971. </year>
Reference-contexts: In the 2-dimensional case, the dependence cone size is in fact proportional to the angle expanded by the cone (see Figure 4). The significance of the dependence cone size can be explained by using the wavefront method <ref> [16, 13] </ref> as shown in Figure 3b where all of the iterations on each wavefront can be executed in parallel.
Reference: [17] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Comm. of ACM, </journal> <pages> pages 1184-1201, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: There are several ways to deal with a DOACROSS loop. If possible, we might break the dependences and transform the DOACROSS loop into a DOALL loop. Many techniques, such as induction variable recognition, variable renaming, loop alignment, scalar expansion, and loop distribution can be used to break such dependences <ref> [18, 29, 17] </ref>. If this is not possible, we can still execute the loop in parallel, provided proper synchronization is added to enforce loop-carried dependences.
Reference: [18] <author> D. A. Padua. </author> <title> Multiprocessors: Discussion of Some Theoretical and Practical Problems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: Many architectures and optimization compilers are aimed at efficient execution of parallel loops. One kind of DO loops, the DOALL loops, does not have loop-carried dependences, and it is relatively easy to obtain the desired speedup. On the other hand, it is much harder to parallelize DOACROSS loops <ref> [18, 19, 10, 11] </ref>, which contain loop-carried dependences. 1 They require the consideration of not only the scheduling and load-balancing issues but also the synchronization issues, which are usually much more difficult. There are several ways to deal with a DOACROSS loop. <p> There are several ways to deal with a DOACROSS loop. If possible, we might break the dependences and transform the DOACROSS loop into a DOALL loop. Many techniques, such as induction variable recognition, variable renaming, loop alignment, scalar expansion, and loop distribution can be used to break such dependences <ref> [18, 29, 17] </ref>. If this is not possible, we can still execute the loop in parallel, provided proper synchronization is added to enforce loop-carried dependences.
Reference: [19] <author> D. A. Padua, D. J. Kuck, and D. H. Lawrie. </author> <title> High-speed multiprocessors and compilation techniques. </title> <journal> IEEE Trans. on Computers, </journal> <volume> c-29(9):763-776, </volume> <month> September </month> <year> 1980. </year>
Reference-contexts: Many architectures and optimization compilers are aimed at efficient execution of parallel loops. One kind of DO loops, the DOALL loops, does not have loop-carried dependences, and it is relatively easy to obtain the desired speedup. On the other hand, it is much harder to parallelize DOACROSS loops <ref> [18, 19, 10, 11] </ref>, which contain loop-carried dependences. 1 They require the consideration of not only the scheduling and load-balancing issues but also the synchronization issues, which are usually much more difficult. There are several ways to deal with a DOACROSS loop.
Reference: [20] <author> The Parallel Computing Forum. </author> <title> PCF Fortran: Language definition, </title> <address> 1 edition, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: The dependence vector (1; 0) can be automatically preserved by the forward execution of the outer loop in each processor. Other vectors in the BDVS have to be enforced only at the boundaries of the adjacent strips with explicit synchronization using synchronization primitives such as post&wait <ref> [20] </ref>, FULL/EMPTY bit [24], Cedar synchronization [12], or by a process-based synchronization scheme as described in [25]. That is, many explicit synchronizations can be eliminated if the source and the sink of a dependence are in the same strip and are executed by the same processor.
Reference: [21] <author> G. Pfister, W. Brantley, D. George, S. Harvey, W. Kleinfelder, K. McAuliffe, E. Melton, V. Norton, and J. Weiss. </author> <title> The IBM research parallel processor prototype (RP3): Introduction and architecture. </title> <booktitle> In 1985 Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 764-771, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Second, the strips are assigned to the processors statically so the senders and the receivers of the synchronization signals are known at compile time. This is especially useful in distributed shared memory machines such as IBM RP3 <ref> [21] </ref>, BBN Butterfly [5], and Stanford DASH [14], where a processor can access the local storage of other processors so each processor can busy-wait on the synchronization variables allocated locally but modified by remote processors. Such a technique reduces the busy-waiting traffic to the global communication network [26].
Reference: [22] <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational Geometry: An Introduction. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: are: 5 8 &gt; &gt; &gt; &gt; : 1 t 1 2t 2 + 5 t 1 1 t 2 + 3t 3 1000 9 &gt; &gt; &gt; &gt; ; The inequalities of Eq. (2) form a set of half spaces and the intersection of them is a convex polytope <ref> [22] </ref>. (See Theorem 4 in Appendix. It is called the Dependence Convex Hull or DCH.
Reference: [23] <author> Z. Shen, Z. Li, and P.-C. Yew. </author> <title> An empirical study of fortran programs for parallelizing compilers. </title> <journal> IEEE Trans. on Parallel and Distributed System, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: A recent empirical study showed that, before compiler optimizations such as forward substitution, induction variable elimination, and constant folding are applied, 66% of the array references have linear or partially linear subscript expressions <ref> [23] </ref>. It also showed that approximately 45% of the reference pairs with linear or partially linear subscript expressions have coupled subscripts; that is, the same loop index variable appears in more than one dimension. They are one of the major causes of non-uniform data dependences.
Reference: [24] <author> B. J. Smith. </author> <title> A pipelined, shared resource mimd computer. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 6-8, </pages> <month> Aug. </month> <year> 1978. </year>
Reference-contexts: The dependence vector (1; 0) can be automatically preserved by the forward execution of the outer loop in each processor. Other vectors in the BDVS have to be enforced only at the boundaries of the adjacent strips with explicit synchronization using synchronization primitives such as post&wait [20], FULL/EMPTY bit <ref> [24] </ref>, Cedar synchronization [12], or by a process-based synchronization scheme as described in [25]. That is, many explicit synchronizations can be eliminated if the source and the sink of a dependence are in the same strip and are executed by the same processor.
Reference: [25] <author> H.-M. Su and P.-C. Yew. </author> <title> On data synchronization for multiprocessors. </title> <booktitle> In Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 416-423, </pages> <month> may </month> <year> 1989. </year>
Reference-contexts: If this is not possible, we can still execute the loop in parallel, provided proper synchronization is added to enforce loop-carried dependences. The need for synchronization can be determined either at compile-time <ref> [15, 25] </ref> for uniform (i.e., constant distance) dependences, or at run-time [30, 6] for complicated non-uniform dependences. If all these techniques fail, the DOACROSS loop must be executed serially which, according to Amdahl's law [2], could severely degrade the performance [7]. <p> To parallelize remaining loops with loop-carried dependences, proper data synchronization instructions have to be generated by the compiler to preserve the dependences. Synchronization for a uniform dependence is relatively easy to generate <ref> [15, 25] </ref> because the distance between the source and the sink statement is fixed. However, for a non-uniform dependence, it is very difficult because each iteration has varying dependence distances. In most cases, loops with nonuniform dependences must be executed serially. Consider the loop in Figure 2. <p> Other vectors in the BDVS have to be enforced only at the boundaries of the adjacent strips with explicit synchronization using synchronization primitives such as post&wait [20], FULL/EMPTY bit [24], Cedar synchronization [12], or by a process-based synchronization scheme as described in <ref> [25] </ref>. That is, many explicit synchronizations can be eliminated if the source and the sink of a dependence are in the same strip and are executed by the same processor. For the example given in Figure 7, suppose that ck = 10 and the vector (5; 1) 2 BDVS . <p> For example, a scheme with 100 strips and a chunk size of 10 yields speedup of 33.6 for the same number of processors with twice as many synchronizations. 3 In fact, if we use the process-basedsynchronization scheme as described in <ref> [25] </ref>, which uses counter to inform other process the progress of the current process, we need only one counter per chunk.
Reference: [26] <author> H.-M. Su and P.-C. Yew. </author> <title> Efficient doacross execution for distributed shared memory multiprocessors. </title> <booktitle> In Supercomputing '91, </booktitle> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Such a technique reduces the busy-waiting traffic to the global communication network <ref> [26] </ref>. Also, synchronization will keep the execution of different processors more or less in lockstep which mitigates the load-balancing problem.
Reference: [27] <author> T. H. Tzen and L. Ni. </author> <title> Dependence uniformization: A loop parallelization technique. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(5), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: They are one of the major causes of non-uniform data dependences. Another major cause of non-uniform data dependences arises from array references in different loop nesting levels of a non-perfectly nested loop. In <ref> [27] </ref>, an approach is proposed to handle non-uniform dependences in DOACROSS loops. However, it does not achieve high parallelization efficiency in many cases. In this paper we propose new scheduling and synchronization strategies that can greatly improve the speedup and reduce the synchronization overhead of such techniques. <p> However, our scheme, proposed in the next section, considers both choices of source and sink statement; therefore, is not affected by this problem. 6 3 Parallelizing Non-uniform Dependence Loops For loops with non-uniform dependences, we can uniformize the dependences <ref> [27] </ref> to simplify required synchronization. The uniformization process is illustrated as in Figure 3. <p> Example 4 (S-test in <ref> [27] </ref>) BDVS=f (1; 1); (0; 1)g (see Figure 6b). This BDVS consists of only two uniform dependence vectors (1; dsl max e) and (0; 1) where sl max is an upper bound of d 2 (T ) o . <p> We can use the BDVS proposed in <ref> [27] </ref> which is shown in Example 4. An Index Synchronization scheme was also proposed in [27] to parallelize the loop nests given a BDVS similar to that of Example 4. Figure 8 illustrates the index synchronization. <p> We can use the BDVS proposed in <ref> [27] </ref> which is shown in Example 4. An Index Synchronization scheme was also proposed in [27] to parallelize the loop nests given a BDVS similar to that of Example 4. Figure 8 illustrates the index synchronization. The performance analysis for static strip scheduling and index synchronization is shown in a later section. <p> have p AE M 2 2 , and this worst case does not occur often. * (Case 3) If both of the (0; 1) and (0; 1) are in the BDVS at the beginning of the step 5 of Algorithm 1 and the compatibility problem occurs, then the index synchronization <ref> [27] </ref> as explained in the previous section has to be used. <p> Case 2 replaces (blb 1 2 c; 1) (or (bub 1 2 c; 1)) of the above BDVS with (0; 1) (or (0; 1)). Case 3 uses BDVS of Example 4, which can be generated in the step 5 of Algorithm 1, and the index synchronization <ref> [27] </ref>. Note that all BDVSs of all three cases can be used to uniformize the dependences, but they have quite different performances. <p> BDVS of case 1 can be obtained. The static strip scheduling scheme can then be applied. 28 7 Previous Work Tzen and Ni <ref> [27] </ref> proposed the first Dependence Uniformization method to parallelize doubly-nested loops with non-uniform affine dependences. They defined the notion of Dependence Slope which is the ratio of d 1 and d 2 for a dependence distance vector (d 1 ; d 2 ) in a doubly-nested loops.
Reference: [28] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <year> 1991. </year>
Reference-contexts: More recently, a unified framework for loop transformation, called Unimodular Transformation, has become increasingly popular <ref> [4, 28] </ref>. Given more accurate characterization on the range of each entry in the dependence distance vectors (Eq. (6)), we can determine when loop reversal, interchange, or skewing is legal.
Reference: [29] <author> M. J. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: There are several ways to deal with a DOACROSS loop. If possible, we might break the dependences and transform the DOACROSS loop into a DOALL loop. Many techniques, such as induction variable recognition, variable renaming, loop alignment, scalar expansion, and loop distribution can be used to break such dependences <ref> [18, 29, 17] </ref>. If this is not possible, we can still execute the loop in parallel, provided proper synchronization is added to enforce loop-carried dependences. <p> Definition 3 A dependence is uniform if the dependence distance vector is a constant vector. That is, the distance is the same for all iterations. Otherwise, the dependence is non-uniform. Many techniques have been proposed to transform loops with loop-carried dependences to vector loops or DOALL loops <ref> [29, 3, 1] </ref>. To parallelize remaining loops with loop-carried dependences, proper data synchronization instructions have to be generated by the compiler to preserve the dependences. Synchronization for a uniform dependence is relatively easy to generate [15, 25] because the distance between the source and the sink statement is fixed. <p> It is called the Dependence Distance Vector Optimization and deserves further study. If some of the entries in the dependence distance vectors Eq. (6) are zeros, they correspond to DOALL loops and can be interchanged freely with other loop nests without violating dependence relations <ref> [29] </ref>. In parallelization, it is desirable to have DOALL loops at the outer levels so that the granularity of the tasks can be increased to amortize the overhead. 27 Therefore, we should try to move all DOALL loops to the outermost levels.
Reference: [30] <author> C.-Q. Zhu and P.-C. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. on Software Enginering, </journal> <pages> pages 726-739, </pages> <month> June </month> <year> 1987. </year> <month> 31 </month>
Reference-contexts: If this is not possible, we can still execute the loop in parallel, provided proper synchronization is added to enforce loop-carried dependences. The need for synchronization can be determined either at compile-time [15, 25] for uniform (i.e., constant distance) dependences, or at run-time <ref> [30, 6] </ref> for complicated non-uniform dependences. If all these techniques fail, the DOACROSS loop must be executed serially which, according to Amdahl's law [2], could severely degrade the performance [7].
References-found: 30

