URL: http://www.cs.utoronto.ca/~sallans/csc2531/dataproj.ps.gz
Refering-URL: http://www.cs.utoronto.ca/~sallans/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: 911 589 990  
Title: Data Mining for Association Rules with Unsupervised Neural Networks  
Date: May 11, 1997  
Note: CSC 2531 Final Project Brian Sallans  
Abstract: results for Gaussian mixture models and factor analysis are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Neural networks for pattern recognition. </title> <publisher> Oxford University Press, </publisher> <address> New York. </address>
Reference-contexts: Association. Redundancy inherent in the data is used to classify or explain the underlying cause of tuples. These techniques are known as clustering and dimensionality reduction. Each of these problems has been studied in the context of AI <ref> (Bishop, 1995) </ref>. The focus has been on accuracy, efficiency, and biological plausibility. They are now being revisited from a data mining perspective. The use of neural networks for classification data-mining has been investigated (H. Lu and Liu, 1995).
Reference: <author> G. E. Hinton, M. R. and Dayan, P. </author> <year> (1995). </year> <title> Recognizing handwritten digits using mixtures of linear models. </title> <editor> In G. Tesauro, D. S. T. and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 1015-1022. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: For example, in a data set of hand-written digits, each 2 would vary slightly from other 2's, but would be completely different from any of the 4's. We can automatically classify digits by fitting 10 Gaussians to the data set <ref> (G. E. Hinton and Dayan, 1995) </ref>. Ideally, the mean of each Gaussian will describe an "average" version of one of the ten digits. The data will be 2 classified by digit. Unlike training data for supervised classification, samples are not pre-labeled by hand.
Reference: <author> Ghahramani, Z. and Hinton, G. E. </author> <year> (1996). </year> <title> The EM algorithm for mixtures of factor analyzers. </title> <type> Technical Report CRG-TR-96-1, </type> <institution> University of Toronto, Toronto ON. </institution>
Reference-contexts: 000 to 1.5k1 000 000 where k 2 [0..9], depends on zipcode hyears years house owned uniformly distributed from 1 to 30. loan total amount of loan uniformly distributed from 1 to 500 000 Table 1: Classification test data. 4 Discussion 4.1 Factor Analysis The factor analyzer used here 2 <ref> (Ghahramani and Hinton, 1996) </ref> converges via an iterative procedure called the Expectation-Maximization, or EM algorithm (Rubin and Thayer, 1982). Convergence is typically quite fast (10 steps to converge, with 100 data points).
Reference: <author> H. Lu, R. S. and Liu, H. </author> <year> (1995). </year> <title> Neurorule: A connectionist approach to data mining. </title> <booktitle> In Proceedings of the 21st VLDB Conference. </booktitle>
Reference-contexts: Each of these problems has been studied in the context of AI (Bishop, 1995). The focus has been on accuracy, efficiency, and biological plausibility. They are now being revisited from a data mining perspective. The use of neural networks for classification data-mining has been investigated <ref> (H. Lu and Liu, 1995) </ref>. An important aspect of this task is the generation 1 of classification rules. Normally, a neural network encodes its knowledge in connection weights. In order to efficiently query a large database, this knowledge must be translated into a query language. <p> The labels are generated based on the data's inherent structure. 3 Data Sets Two kinds of artificially generated data sets were used. 1 One was particularly suited to the classification task. The other was suited to association. 3.1 Classification Data This was the same data set used in <ref> (H. Lu and Liu, 1995) </ref> and in (R. Agrawal and Swami, 1993). The data is described in table 1. Notice that with the exception of commission and hvalue, the data comes from a multi-dimensional Gaussian distribution. As a result, there was very little structure inherent in the data. In (H. <p> Lu and Liu, 1995) and in (R. Agrawal and Swami, 1993). The data is described in table 1. Notice that with the exception of commission and hvalue, the data comes from a multi-dimensional Gaussian distribution. As a result, there was very little structure inherent in the data. In <ref> (H. Lu and Liu, 1995) </ref>, the class mappings were imposed on the data, by applying a function which mapped tuples to classes. The structure came from the mapping function, not from the data itself. 3.2 Association Data This data was designed to simulate a series of transactions.
Reference: <author> Hinton, G. E. and Ghahramani, Z. </author> <year> (1997). </year> <title> Generative models for discovering sparse distributed representations. </title> <journal> Phil. Trans. Roy. Soc. London B: Biol. Sci. </journal>
Reference-contexts: Factor analysis performs dimensionality reduction. Each real attribute is represented as a linear combination of the hidden attributes. Because the process is linear, it can only capture pairwise correlations among attributes <ref> (Hinton and Ghahramani, 1997) </ref>. Factor analysis tends to build global representations, where all of the hidden factors contribute to each visible attribute. 2.2 Mixture of Gaussians In some data sets, the tuples are generated by one of several distinct processes.
Reference: <author> Liu, H. and Tan, S. T. </author> <year> (1995). </year> <title> X2r: A fast rule generator. </title> <booktitle> In IEEE International Conference on Systems, Man and Cybernetics. </booktitle>
Reference-contexts: Each of these problems has been studied in the context of AI (Bishop, 1995). The focus has been on accuracy, efficiency, and biological plausibility. They are now being revisited from a data mining perspective. The use of neural networks for classification data-mining has been investigated <ref> (H. Lu and Liu, 1995) </ref>. An important aspect of this task is the generation 1 of classification rules. Normally, a neural network encodes its knowledge in connection weights. In order to efficiently query a large database, this knowledge must be translated into a query language. <p> The labels are generated based on the data's inherent structure. 3 Data Sets Two kinds of artificially generated data sets were used. 1 One was particularly suited to the classification task. The other was suited to association. 3.1 Classification Data This was the same data set used in <ref> (H. Lu and Liu, 1995) </ref> and in (R. Agrawal and Swami, 1993). The data is described in table 1. Notice that with the exception of commission and hvalue, the data comes from a multi-dimensional Gaussian distribution. As a result, there was very little structure inherent in the data. In (H. <p> Lu and Liu, 1995) and in (R. Agrawal and Swami, 1993). The data is described in table 1. Notice that with the exception of commission and hvalue, the data comes from a multi-dimensional Gaussian distribution. As a result, there was very little structure inherent in the data. In <ref> (H. Lu and Liu, 1995) </ref>, the class mappings were imposed on the data, by applying a function which mapped tuples to classes. The structure came from the mapping function, not from the data itself. 3.2 Association Data This data was designed to simulate a series of transactions. <p> The range of possible values for each attribute was divided into ten sections. For each tuple, the attributes and corresponding hidden factor activations were replaced by digits from one to ten. The final sets of rules were generated by feeding the discretized tuples and activations to X2R <ref> (Liu and Tan, 1995) </ref>, a "fast rule generator". Classification rules were generated separately for each hidden factor. The intention was to produce one set of rules that could determine whether a factor was active, independent of the other factors. Notice that this does not separate the data into disjoint sets.
Reference: <author> R. Agrawal, T. I. and Swami, A. </author> <year> (1993). </year> <title> Database mining: A performance perspective. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(6). </volume>
Reference-contexts: The other was suited to association. 3.1 Classification Data This was the same data set used in (H. Lu and Liu, 1995) and in <ref> (R. Agrawal and Swami, 1993) </ref>. The data is described in table 1. Notice that with the exception of commission and hvalue, the data comes from a multi-dimensional Gaussian distribution. As a result, there was very little structure inherent in the data. In (H.
Reference: <author> Rubin, D. B. and Thayer, D. T. </author> <year> (1982). </year> <title> EM algorithms for ML factor analysis. </title> <journal> Psychometrica, </journal> <volume> 47(1) </volume> <pages> 69-76. 10 </pages>
Reference-contexts: owned uniformly distributed from 1 to 30. loan total amount of loan uniformly distributed from 1 to 500 000 Table 1: Classification test data. 4 Discussion 4.1 Factor Analysis The factor analyzer used here 2 (Ghahramani and Hinton, 1996) converges via an iterative procedure called the Expectation-Maximization, or EM algorithm <ref> (Rubin and Thayer, 1982) </ref>. Convergence is typically quite fast (10 steps to converge, with 100 data points).
References-found: 8

