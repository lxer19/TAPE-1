URL: ftp://ftp.cs.washington.edu/tr/1997/01/UW-CSE-97-01-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: A Performance Evaluation of Cluster-based Architectures  
Keyword: shared-memory multiprocessors, clusters, protocol processors, mean value analysis.  
Abstract: This paper investigates the performance of shared-memory cluster-based architectures where each cluster is a shared-bus multiprocessor augmented with a protocol processor maintaining cache coherence across clusters. For a given number of processors, sixteen in this study, we evaluate the performance of various cluster configurations. We also consider the impact of adding a remote shared cache in each cluster. We use Mean Value Analysis to estimate the cache miss latencies of various types and the overall execution time. The service demands of shared resources are characterized in detail by examining the sub-requests issued in resolving cache misses. In addition to the architectural system parameters and the service demands on resources, the analytical model needs parameters pertinent to applications. The latter, in particular cache miss profiles, are obtained by trace-driven simulation of three benchmarks. Our results show that without remote caches the performance of cluster-based architectures is mixed. In some configurations, the negative effects of the longer latency of inter-cluster misses and of the contention on the protocol processor are too large to counter-balance the lower contention on the data buses. For two out of the three applications best results are obtained when the system has clusters of size 2 or 4. The cluster-based architectures with remote caches consistently outperform the single bus system for all 3 applications. We also exercise the model with parameters reflecting the current trend in technology making the processor relatively faster than the bus and memory. Under these new conditions, our results show a clear performance advantage for the cluster-based architectures, with or without remote caches, over single bus systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Limits on interconnection network performance. </title> <journal> IEEE Trans. on Parellel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: the contention on the network for three reasons: (1) The bandwidth provided by current network technology appears to be sufficient for the size of the systems investigated in this paper and contention is not an important factor; (2) Many models have been developed to analyze the performance of various networks <ref> [1, 13, 5] </ref>; and (3) The MVA (Mean Value Analysis) technique we use cannot cope easily with the network contention directly. If need be the results of the contention models just alluded to could be incorporated in the analysis (cf. <p> However a network model can be easily incorporated in the evaluation if the contention is not negligible. There exists a rich set of analytical models for various network topologies <ref> [1, 13, 5] </ref>. These models usually take the request rate as an input parameter and compute the network latency taking into effect the contention. This latency could be used in our model instead of the contention-free network latency to achieve a more accurate response time for inter-cluster misses.
Reference: [2] <author> C. S. Anderson. </author> <title> Improving Performance of Bus-Based Multiprocessors. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Univ. of Washington, </institution> <year> 1995. </year>
Reference-contexts: However it is not clear whether contention causes considerable delay in protocol processors or not. Cluster-based architectures where inter-cluster communication is provided by a (hierarchy of) bus and a shared cache (s) have been studied also via simulation by Nayfeh et al. [15, 16] and by Anderson <ref> [2] </ref>. Conclusions on the viability of clusters are mixed depending on the amount of memory requests and their locality. As several processors share a protocol processor, the latter is likely to become the performance bottleneck.
Reference: [3] <author> D. H. Bailey. </author> <title> FFT in external or hierarchical memory. </title> <journal> J. of Supercomputing, </journal> <volume> 4(1) </volume> <pages> 23-35, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: To that effect, we selected FFT, RADIX, and RAYTRACE from the SPLASH-2 benchmark suite [23]. The algorithms for FFT and RADIX are given in <ref> [3] </ref> and [6] respectively. RAYTRACE is an image processing program which renders a three-dimensional scene using ray tracing [18]. Table 5 summarizes some pertinent statistics about the applications: problem size, number of instructions, number of read and write references to shared data.
Reference: [4] <author> G. Bell. Multis: </author> <title> A new class of multiprocessor computers. </title> <booktitle> Science, </booktitle> <pages> pages 462-467, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: 1 Introduction Single bus shared-memory multiprocessors, or multis <ref> [4] </ref>, have enjoyed a tremendous success. However, the number of processors that can be incorporated in the system is limited since the single bus soon becomes an overcommitted resource.
Reference: [5] <author> L. N. Bhuyan, Q. Yang, and D. P. Agrawal. </author> <title> Performance of multiprocessor interconnection networks. </title> <journal> Computer, </journal> <volume> 22(2) </volume> <pages> 25-37, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: the contention on the network for three reasons: (1) The bandwidth provided by current network technology appears to be sufficient for the size of the systems investigated in this paper and contention is not an important factor; (2) Many models have been developed to analyze the performance of various networks <ref> [1, 13, 5] </ref>; and (3) The MVA (Mean Value Analysis) technique we use cannot cope easily with the network contention directly. If need be the results of the contention models just alluded to could be incorporated in the analysis (cf. <p> However a network model can be easily incorporated in the evaluation if the contention is not negligible. There exists a rich set of analytical models for various network topologies <ref> [1, 13, 5] </ref>. These models usually take the request rate as an input parameter and compute the network latency taking into effect the contention. This latency could be used in our model instead of the contention-free network latency to achieve a more accurate response time for inter-cluster misses.
Reference: [6] <author> G. E. Blelloch et al. </author> <title> A comparison of sorting algorithms for the connection machine CM-2. </title> <booktitle> In Proc. of the Symposium on Parallel Algorithms and Architecture, </booktitle> <pages> pages 3-16, </pages> <year> 1991. </year>
Reference-contexts: To that effect, we selected FFT, RADIX, and RAYTRACE from the SPLASH-2 benchmark suite [23]. The algorithms for FFT and RADIX are given in [3] and <ref> [6] </ref> respectively. RAYTRACE is an image processing program which renders a three-dimensional scene using ray tracing [18]. Table 5 summarizes some pertinent statistics about the applications: problem size, number of instructions, number of read and write references to shared data.
Reference: [7] <author> M. Galles and E. Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI challenge multiprocessor. </title> <booktitle> In Proc. of the 27th Hawaii International Conference on System Sciences. </booktitle> <volume> Vol. I: Architecture, </volume> <pages> pages 134-43, </pages> <year> 1994. </year>
Reference-contexts: Table 2 for the meanings of abbreviations). For misses involving multiple clusters, the first row shows the service demands for the local cluster; the second and third rows (if present) are the service demands for the second and the third clusters involved. * The cluster buses are split transaction <ref> [7] </ref>. The address bus and the data bus are two separated resources and can be used simultaneously for different transactions.
Reference: [8] <author> J. Heinlein, K. Gharachorloo, S. Dresser, and A. Gupta. </author> <title> Integration of message passing and shared memory in the Stanford FLASH multiprocessor. </title> <booktitle> In Proc. of 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 38-50, </pages> <year> 1994. </year>
Reference-contexts: Inter-cluster misses utilize the network and additional logic for managing the directory information and for transmitting control information and data between clusters. The current trend <ref> [8, 14, 17] </ref> is to use programmable processors, hereafter called protocol processors, rather than hardwired controllers for the management of inter-cluster transactions. <p> We will vary the number of processors in each cluster, or cluster size, from a single processor per cluster (as in FLASH <ref> [8] </ref>), to 2, 4, 8, and finally 16, i.e., a single cluster corresponding to a conventional shared-bus multiprocessor.
Reference: [9] <author> C. Holt. </author> <title> The effects of latency, occupancy, and bandwidth in distributed shared memory multiprocessors. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, Stanford Univ., </institution> <year> 1995. </year> <month> CSL-TR-95-660. </month>
Reference-contexts: Holt et al. studied, via simulation, the effects of the occupancy of the protocol processor and of the latency of the network for a Flash-like architecture where the number of processors per cluster is one <ref> [9] </ref>. They showed that the time consumed by the protocol processors did have an impact on the performance, especially with a fast network. However it is not clear whether contention causes considerable delay in protocol processors or not.
Reference: [10] <author> M. Karlsson and P. Stenstrom. </author> <title> Performance evaluation of a cluster-based multiprocessor built from ATM switches and bus-based multiprocessor servers. </title> <booktitle> In Proc. of 2nd Conf. on High-Performance Computer Architecture, </booktitle> <pages> pages 4-13, </pages> <year> 1996. </year>
Reference-contexts: If a cache or page miss blocks the computation, the compute processors may as well be used to resolve the fault. Karlsson and Stenstrom simulated such a scheme for a multiprocessor which used an ATM network to link clusters of bus-based multiprocessors <ref> [10] </ref>. The intra-cluster coherence is maintained by cache snoopy hardware while inter-cluster coherence is maintained at the page level by distributed virtual shared memory software.
Reference: [11] <author> E. D. Lazowska, J. Zahorjan, G. S. Graham, and K. C. Sevcik. </author> <title> Quantitative System Performance. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1984. </year>
Reference-contexts: bus system will be intensified at a rate faster than the contention on the protocol processor of the cluster architectures. 6 Related work The behaviors of the service centers contained in our model generally satisfy the two assumptions required by the MVA technique, i.e., routing homogeneity and service time homogeneity <ref> [11] </ref>.
Reference: [12] <author> D. Lenoski et al. </author> <title> The Standford DASH multiprocessor. </title> <journal> IEEE Transactions on Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: One way to expand the number of processors while keeping the shared-memory paradigm is to consider each multi as a cluster and to link clusters together using some interconnection network such as one (or two) mesh <ref> [12] </ref> or an SCI ring [14]. In cluster systems, memory requests are satisfied either locally, i.e., within a cluster (intra-cluster), or externally, i.e., by another cluster (inter-cluster). Intra-cluster cache coherence is enforced using a snoopy protocol while some form of directory-based coherence is used for inter-cluster transactions. <p> Detailed protocol operations are similar to those found in <ref> [12] </ref>. For our modeling purposes, we need to classify the cache misses according to their service demand on the shared resources. In Table 1 we summarize the actions needed for each type of read misses. <p> Torrellas, Hennessy and Weil also developed analytical models to investigate the impact of several architectural and application parameters in shared-memory processors based on DASH [19]. The main differences between their model and ours are that they used an open queuing network and that the inter-cluster interface was hardwired <ref> [12] </ref>. They found that contention for the bus dominated that of the cluster interface 2 . We show that this conclusion is no longer true in all cases when the design shifts from hardwired controller to protocol processor.
Reference: [13] <author> T. Lin and L. Kleinrock. </author> <title> Performance analysis of finite-buffered multistage interconnection networks with a general traffic pattern. </title> <booktitle> In Proc. of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 68-78, </pages> <year> 1991. </year>
Reference-contexts: the contention on the network for three reasons: (1) The bandwidth provided by current network technology appears to be sufficient for the size of the systems investigated in this paper and contention is not an important factor; (2) Many models have been developed to analyze the performance of various networks <ref> [1, 13, 5] </ref>; and (3) The MVA (Mean Value Analysis) technique we use cannot cope easily with the network contention directly. If need be the results of the contention models just alluded to could be incorporated in the analysis (cf. <p> However a network model can be easily incorporated in the evaluation if the contention is not negligible. There exists a rich set of analytical models for various network topologies <ref> [1, 13, 5] </ref>. These models usually take the request rate as an input parameter and compute the network latency taking into effect the contention. This latency could be used in our model instead of the contention-free network latency to achieve a more accurate response time for inter-cluster misses.
Reference: [14] <author> T. Lovett and R. Clapp. STiNG: </author> <title> A CC-NUMA computer system for the commercial marketplace. </title> <booktitle> In Proc. of 23rd International Symposium on Computer Architecture, </booktitle> <pages> pages 308-317, </pages> <year> 1996. </year>
Reference-contexts: One way to expand the number of processors while keeping the shared-memory paradigm is to consider each multi as a cluster and to link clusters together using some interconnection network such as one (or two) mesh [12] or an SCI ring <ref> [14] </ref>. In cluster systems, memory requests are satisfied either locally, i.e., within a cluster (intra-cluster), or externally, i.e., by another cluster (inter-cluster). Intra-cluster cache coherence is enforced using a snoopy protocol while some form of directory-based coherence is used for inter-cluster transactions. <p> Inter-cluster misses utilize the network and additional logic for managing the directory information and for transmitting control information and data between clusters. The current trend <ref> [8, 14, 17] </ref> is to use programmable processors, hereafter called protocol processors, rather than hardwired controllers for the management of inter-cluster transactions. <p> One variant to the base architecture that we will also study is clusters with large remote caches like those found in two recent systems <ref> [14, 17] </ref>. The remote caches are shared by all processors in the cluster and contain only remote data. <p> However, by increasing the configurations with remote caches, the performance improvements are quite noticeable. This last observation certainly justifies the design decisions in recent cluster architectures <ref> [14, 17] </ref>. The architectural parameters of Table 4 are justifiable with the current technology for processors, caches, and buses. As newer processor designs adopt more aggressive superscalar techniques the miss issuing rate will increase. In other words, the number of instruction cycles between misses decreases.
Reference: [15] <author> B. A. Nayfeh and K. Olukotun. </author> <title> Exploring the design space for a shared-cache multiprocessor. </title> <booktitle> In Proc. of 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 166-175, </pages> <year> 1994. </year>
Reference-contexts: However it is not clear whether contention causes considerable delay in protocol processors or not. Cluster-based architectures where inter-cluster communication is provided by a (hierarchy of) bus and a shared cache (s) have been studied also via simulation by Nayfeh et al. <ref> [15, 16] </ref> and by Anderson [2]. Conclusions on the viability of clusters are mixed depending on the amount of memory requests and their locality. As several processors share a protocol processor, the latter is likely to become the performance bottleneck.
Reference: [16] <author> B. A. Nayfeh, K. Olukotun, and J.P. Singh. </author> <title> The impact of shared-cache clustering in small-scale shared-memory multiprocessors. </title> <booktitle> In Proc. of 2nd Conference on High-Performance Computer Architecture, </booktitle> <pages> pages 74-84, </pages> <year> 1996. </year>
Reference-contexts: However it is not clear whether contention causes considerable delay in protocol processors or not. Cluster-based architectures where inter-cluster communication is provided by a (hierarchy of) bus and a shared cache (s) have been studied also via simulation by Nayfeh et al. <ref> [15, 16] </ref> and by Anderson [2]. Conclusions on the viability of clusters are mixed depending on the amount of memory requests and their locality. As several processors share a protocol processor, the latter is likely to become the performance bottleneck.
Reference: [17] <author> A. Nowatzyk et al. </author> <title> The S3.mp scalable shared memory multiprocessor. </title> <booktitle> In Proc. of the 1995 International Conference on Parallel Processing, </booktitle> <volume> Vol II, </volume> <pages> pages 1-10, </pages> <year> 1995. </year>
Reference-contexts: Inter-cluster misses utilize the network and additional logic for managing the directory information and for transmitting control information and data between clusters. The current trend <ref> [8, 14, 17] </ref> is to use programmable processors, hereafter called protocol processors, rather than hardwired controllers for the management of inter-cluster transactions. <p> One variant to the base architecture that we will also study is clusters with large remote caches like those found in two recent systems <ref> [14, 17] </ref>. The remote caches are shared by all processors in the cluster and contain only remote data. <p> However, by increasing the configurations with remote caches, the performance improvements are quite noticeable. This last observation certainly justifies the design decisions in recent cluster architectures <ref> [14, 17] </ref>. The architectural parameters of Table 4 are justifiable with the current technology for processors, caches, and buses. As newer processor designs adopt more aggressive superscalar techniques the miss issuing rate will increase. In other words, the number of instruction cycles between misses decreases.
Reference: [18] <author> J. P. Singh, A. Gupta, and M. Levoy. </author> <title> Parallel visualization algorithms: performance and architecture implications. </title> <journal> IEEE Computer, </journal> <volume> 27(7) </volume> <pages> 45-55, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: To that effect, we selected FFT, RADIX, and RAYTRACE from the SPLASH-2 benchmark suite [23]. The algorithms for FFT and RADIX are given in [3] and [6] respectively. RAYTRACE is an image processing program which renders a three-dimensional scene using ray tracing <ref> [18] </ref>. Table 5 summarizes some pertinent statistics about the applications: problem size, number of instructions, number of read and write references to shared data.
Reference: [19] <author> J. Torrellas, J. Hennessy, and T. Weil. </author> <title> Analysis of critical architectural and program parameters in a hierarchical shared-memory multiprocessor. </title> <booktitle> In Proc. of the 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 163-72, </pages> <year> 1990. </year>
Reference-contexts: Our model and theirs share some common elements because of the similarity in the architectures. Torrellas, Hennessy and Weil also developed analytical models to investigate the impact of several architectural and application parameters in shared-memory processors based on DASH <ref> [19] </ref>. The main differences between their model and ours are that they used an open queuing network and that the inter-cluster interface was hardwired [12]. They found that contention for the bus dominated that of the cluster interface 2 .
Reference: [20] <author> J. E. Veenstra and R. J. Fowler. MINT: </author> <title> a front end for efficient simulation of shared-memory multiprocessors. </title> <booktitle> In Proc. of the 2nd International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, </booktitle> <pages> pages 201-7, </pages> <year> 1994. </year>
Reference-contexts: To gather these statistics, we use Mint <ref> [20] </ref> as the simulation tool. Mint is a software package that emulates multi-processing execution environments and generates memory reference events which drive a memory system simulator. Figures 2, 3, and 4 show the cache miss profiles of the three applications.
Reference: [21] <author> M. Vernon, R. Jog, and G. S. Sohi. </author> <title> Performance analysis of hierarchical cache-consistent multiprocessors. </title> <booktitle> In Proc. of the International Seminar on Performance of Distributed and Parallel Systems, </booktitle> <pages> pages 111-126, </pages> <year> 1988. </year>
Reference-contexts: Vernon et al. applied the same technique to studying the performance of purely bus-based hierarchical multiprocessors <ref> [21] </ref>. Our model and theirs share some common elements because of the similarity in the architectures. Torrellas, Hennessy and Weil also developed analytical models to investigate the impact of several architectural and application parameters in shared-memory processors based on DASH [19].
Reference: [22] <author> M. K. Vernon, E. D. Lazowska, and J. Zahorjan. </author> <title> An accurate and efficient performance analysis technique for multiprocessor snooping cache-consistency protocols. </title> <booktitle> In Proc. of 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 308-315, </pages> <year> 1988. </year>
Reference-contexts: RAYTRACE Cluster size No RC With RC No RC With RC No RC With RC 16 1.00 - 1.00 - 1.00 - 4 0.71 0.57 0.66 0.50 0.54 0.43 1 0.69 0.57 0.62 0.49 0.60 0.43 Table 9: Normalized execution time for the "faster" architecture. systems is of remarkable accuracy <ref> [22] </ref>. Vernon et al. applied the same technique to studying the performance of purely bus-based hierarchical multiprocessors [21]. Our model and theirs share some common elements because of the similarity in the architectures.
Reference: [23] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proc. of 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <year> 1995. </year> <month> 20 </month>
Reference-contexts: To that effect, we selected FFT, RADIX, and RAYTRACE from the SPLASH-2 benchmark suite <ref> [23] </ref>. The algorithms for FFT and RADIX are given in [3] and [6] respectively. RAYTRACE is an image processing program which renders a three-dimensional scene using ray tracing [18]. <p> Thus, the remote caches can transform some of the inter-cluster misses caused by conflict mapping or capacity limitation in the L2 caches into intra-cluster misses. The extent of the reduction in the inter-cluster misses is determined by the proportion of conflict and capacity misses in these inter-cluster misses. From <ref> [23] </ref>, we know that when processor caches are of limited size, both RADIX and RAYTRACE encounter significant amounts of capacity misses. For these two applications, the remote caches are very effective in transforming the inter-cluster misses into intra-cluster misses.
References-found: 23

