URL: http://www.cs.colostate.edu/~vision/ps/1997/CVPR97.ps.gz
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Title: Using Multisensor Occlusion Reasoning in Object Recognition Category: Object Recognition and Pose Estimation  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. K. Aggarwal. </author> <title> MultiSensor Fusion for Automatic Scene Inter pretation. </title> <editor> In Ramesh C. Jain and Anil K. Jain, editors, </editor> <title> Analysis and Interpretation of Range Images, chapter 8. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: However, this research area is still young. Fusing information from different modalities is complicated by many fac tors, including recovery of sensor alignment and registration <ref> [1] </ref>. Our solution is to use the CAD model geometry to suggest the proper registration [7]. 3 Overview By combining basic techniques of computer graphics with those of computer vision, it is possible to build highly robust model-based object recognition algorithms which deal properly with occlusion.
Reference: [2] <author> F. Arman and J.K. Aggarwal. </author> <title> Cad-based vision: Object recog nition in cluttered range images using recognition strategies. </title> <booktitle> Image Understanding, </booktitle> <volume> 58 </volume> <pages> 33-48, </pages> <year> 1993. </year>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys [35, 16, 5] and CAD rendering [38, 18, 31]. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data <ref> [2, 4] </ref>, as well as multi-spectral imagery such as IR [34] and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10].
Reference: [3] <author> Benjamin Bell and L. F. Pau. </author> <title> Contour tracking and cor ner detection in a logic programming environment. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(9) </volume> <pages> 913-917, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Thus object recognition can be viewed as finding evidence in imagery for various model constraints. Usually model properties are chosen such that they can easily be found in the data: points [37], lines [6], curves [29], corners <ref> [3] </ref> or planes [19]. Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another.
Reference: [4] <author> P. J. Besl and R. C. Jain. </author> <title> Invariant surface characteristics for 3D object recognition from range data. </title> <journal> cvgip, </journal> <volume> 33:33 - 80, </volume> <year> 1986. </year>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys [35, 16, 5] and CAD rendering [38, 18, 31]. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data <ref> [2, 4] </ref>, as well as multi-spectral imagery such as IR [34] and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10].
Reference: [5] <author> Paul J. Besl and Ramesh C. Jain. </author> <title> Three-dimensional object recognition. </title> <journal> ACM Computing Surveys, 17(1):75 -145, </journal> <month> March </month> <year> 1985. </year>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys <ref> [35, 16, 5] </ref> and CAD rendering [38, 18, 31]. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12].
Reference: [6] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Obejct Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachuesetts at Amherst, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: When detailed CAD models of objects are available they can provide a rich source of constraint. Thus object recognition can be viewed as finding evidence in imagery for various model constraints. Usually model properties are chosen such that they can easily be found in the data: points [37], lines <ref> [6] </ref>, curves [29], corners [3] or planes [19]. Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another. <p> Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another. Traditional techniques either rely on error measures being robust to such occlusions <ref> [27, 23, 6, 44] </ref>, or associate a likelihood of finding each feature based on off-line appearance analysis [14, 45, 36, 22]. <p> For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance <ref> [28, 23, 21, 6, 10] </ref>. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed [30, 39, 43, 17, 24]. However, this research area is still young. <p> Thus the fitness term is the average gradient response for only the strongly matched edges. The omission is just the ratio of unmatched features to the total number predicted <ref> [6] </ref>. The resulting error is: E M;c (F) = fi c E fit;c (F) + (1 fi c )E om;c (F) (2) for the color, with the weighting term, fi c , determined empirically to also be 0:4.
Reference: [7] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Integrated color ccd, flir & ladar based object modeling and recognition. </title> <type> Technical report, </type> <institution> Colorado State University and Alliant Tech-systems and University of Massachusetts, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: However, this research area is still young. Fusing information from different modalities is complicated by many fac tors, including recovery of sensor alignment and registration [1]. Our solution is to use the CAD model geometry to suggest the proper registration <ref> [7] </ref>. 3 Overview By combining basic techniques of computer graphics with those of computer vision, it is possible to build highly robust model-based object recognition algorithms which deal properly with occlusion.
Reference: [8] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The data is from the Fort Carson Dataset, which contains range, IR and color imagery of military targets against natural terrain. The imagery and documentation <ref> [8] </ref> is available through our website at http://www.cs.colostate.edu/~vision. Sensor calibration data is available in [26]. The bottom row of Figure 2 shows an initial coreg-istration hypothesis for a specific CAD model. The hypothesized relationship between the model and the color image is shown as white lines in the color image.
Reference: [9] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Optimal Geo metric Model Matching Under Full 3D Perspective. </title> <booktitle> In Second CAD-Based Vision Workshop, </booktitle> <pages> pages 54 - 63. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1994. </year> <note> (Submitted to CVGIP-IU). </note>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys [35, 16, 5] and CAD rendering [38, 18, 31]. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery <ref> [15, 9, 42] </ref>, 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10].
Reference: [10] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Optimal Geomet ric Model Matching Under Full 3D Perspective. Computer Vision and Image Understanding, </title> <note> 61(3):351 - 364, 1995. (short version in IEEE Second CAD-Based Vision Workshop). </note>
Reference-contexts: For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance <ref> [28, 23, 21, 6, 10] </ref>. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed [30, 39, 43, 17, 24]. However, this research area is still young.
Reference: [11] <author> James E. Bevington. </author> <title> Laser Radar ATR Algorithms: Phase III Final Report. </title> <type> Technical report, </type> <institution> Alliant Techsystems, Inc., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: This initial hypothesis has been automatically generated by two upstream algorithms described elsewhere. The first algorithm detects vehicles based on their color [13], and the second generates a vehicle and pose estimate based on the appearance of the occluding contour in a range image <ref> [11] </ref>. We will use this example to illustrate how our iterative predict and match system is able to refine the coreg-istration estimate while detecting and accounting for the partial occlusion of the vehicle by the tree.
Reference: [12] <author> Bir Bhanu and Grinnell Jones and Joon Ahn and Ming Li and June Yi. </author> <title> Recognition of Articulated Objects in SAR Imagery. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1237-1250, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR <ref> [12] </ref>. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10]. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed [30, 39, 43, 17, 24].
Reference: [13] <author> Shashi Buluswar, Bruce A. Draper, Allen Hanson, and Ed ward Riseman. </author> <title> Non-parametric Classification of Pixels Under Varying Outdoor Illumination. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1619-1626, </pages> <address> Los Altos, CA, </address> <month> Novem-ber </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For the range data, a gray-scale rendering of the CAD model is embedded within a pseudo-colored rendering of the range image. This initial hypothesis has been automatically generated by two upstream algorithms described elsewhere. The first algorithm detects vehicles based on their color <ref> [13] </ref>, and the second generates a vehicle and pose estimate based on the appearance of the occluding contour in a range image [11].
Reference: [14] <author> Octavia I. Camps, Linda Shapiro, and Robert Haralick. </author> <title> Three Dimensional Object Recognition Systems, chapter Image Prediction for Computer Vision. In Three-Dimensional Object Recognition Systems. </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another. Traditional techniques either rely on error measures being robust to such occlusions [27, 23, 6, 44], or associate a likelihood of finding each feature based on off-line appearance analysis <ref> [14, 45, 36, 22] </ref>. In the former case, matching can be hindered by the lack of support for an occluded feature and continue to examine possibilities even when the feature will never be found.
Reference: [15] <author> Jin-Long Chen, George C. Stockman, and Kashi Rao. </author> <title> Recov ering and tracking pose of curved 3D objects from 2d images. </title> <booktitle> In Proceedings Computer Vision and Pattern Recognition, </booktitle> <pages> pages 233-239, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys [35, 16, 5] and CAD rendering [38, 18, 31]. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery <ref> [15, 9, 42] </ref>, 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10].
Reference: [16] <author> Roland T. Chin and Charles R. Dyer. </author> <title> Model-based recogni tion in robot vision. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(1) </volume> <pages> 67-108, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys <ref> [35, 16, 5] </ref> and CAD rendering [38, 18, 31]. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12].
Reference: [17] <author> R. O. Eason and R. C. Gonzalez. </author> <title> Least-Squares Fusion of Multisensory Data. </title> <editor> In Mongi A. Abidi and Rafael C. Gonzalez, editors, </editor> <booktitle> Data Fusion in Robotics and Machine Intelligence, chapter 9. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10]. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed <ref> [30, 39, 43, 17, 24] </ref>. However, this research area is still young. Fusing information from different modalities is complicated by many fac tors, including recovery of sensor alignment and registration [1].
Reference: [18] <author> James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. </author> <title> Computer Graphics: </title> <booktitle> Principles and Practice. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys [35, 16, 5] and CAD rendering <ref> [38, 18, 31] </ref>. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12].
Reference: [19] <author> P. C. Gaston and T. Lozano-Perez. </author> <title> Tactile recognition and localization using object models: The case of polyhedra on a plane. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI - 6:721 - 741, </volume> <month> May </month> <year> 1984. </year>
Reference-contexts: Thus object recognition can be viewed as finding evidence in imagery for various model constraints. Usually model properties are chosen such that they can easily be found in the data: points [37], lines [6], curves [29], corners [3] or planes <ref> [19] </ref>. Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another.
Reference: [20] <author> F. Glover. </author> <title> Tabu search part i. </title> <journal> ORSA Journal on Computing, </journal> <volume> 1(3):190 - 206, </volume> <year> 1989. </year>
Reference-contexts: In addition, the internal features of the model have been clipped to the areas where occlusion is most likely to not be present. 5 Coregistration Matching The matching algorithm uses a variant of tabu search <ref> [20] </ref>, the set of features predicted for matching, and a match error to refine the initial coregistration estimate. The search process, as outlined in Figure 3, is a simple generate and test algorithm.
Reference: [21] <author> W. Eric L. </author> <title> Grimson. Object Recognition by Computer: The Role of Geometric Constraints. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Unconstrained object recognition in outdoor scenes is a daunting task. While the human visual system is adept at recognizing a wide range of objects in such a scene, for a machine to perform reasonably well an explicit geometric model of each object being sought is usually required <ref> [23, 21] </ref>. When detailed CAD models of objects are available they can provide a rich source of constraint. Thus object recognition can be viewed as finding evidence in imagery for various model constraints. <p> For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance <ref> [28, 23, 21, 6, 10] </ref>. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed [30, 39, 43, 17, 24]. However, this research area is still young.
Reference: [22] <editor> Anthony Hoogs and Ruzena Bajcsy. </editor> <title> Model-based learning of segmentations. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <volume> volume 4, </volume> <pages> pages 494-499, </pages> <address> Vienna, </address> <month> August </month> <year> 1996. </year> <note> AIPR, IEEE. </note>
Reference-contexts: Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another. Traditional techniques either rely on error measures being robust to such occlusions [27, 23, 6, 44], or associate a likelihood of finding each feature based on off-line appearance analysis <ref> [14, 45, 36, 22] </ref>. In the former case, matching can be hindered by the lack of support for an occluded feature and continue to examine possibilities even when the feature will never be found.
Reference: [23] <author> Daniel P. Huttenlocher and Shimon Ullman. </author> <title> Recognizing Solid Objects by Alignment. </title> <booktitle> In Proc. of the DARPA Image Understanding Workshop, </booktitle> <pages> pages 1114 - 1124, </pages> <address> Cambridge, April 1988. </address> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <address> New York. </address>
Reference-contexts: 1 Introduction Unconstrained object recognition in outdoor scenes is a daunting task. While the human visual system is adept at recognizing a wide range of objects in such a scene, for a machine to perform reasonably well an explicit geometric model of each object being sought is usually required <ref> [23, 21] </ref>. When detailed CAD models of objects are available they can provide a rich source of constraint. Thus object recognition can be viewed as finding evidence in imagery for various model constraints. <p> Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another. Traditional techniques either rely on error measures being robust to such occlusions <ref> [27, 23, 6, 44] </ref>, or associate a likelihood of finding each feature based on off-line appearance analysis [14, 45, 36, 22]. <p> For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance <ref> [28, 23, 21, 6, 10] </ref>. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed [30, 39, 43, 17, 24]. However, this research area is still young.
Reference: [24] <author> Alexander Akerman III, Ronald Patton, Walter H. Delash mit, and Robert Hummel. </author> <title> Multisensor fusion using FLIR and LADAR identification. </title> <type> Technical Report NRC-TR-94-052, </type> <institution> Nichols Research Corporation, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10]. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed <ref> [30, 39, 43, 17, 24] </ref>. However, this research area is still young. Fusing information from different modalities is complicated by many fac tors, including recovery of sensor alignment and registration [1].
Reference: [25] <author> J. Ross Beveridge and Mark R. Stevens. </author> <title> CAD-based Target Identification in Range, IR and Color Imagery Using On-Line Rendering and Feature Prediction. </title> <type> Technical report, </type> <institution> Computer Science, Colorado State University, </institution> <year> 1996. </year> <note> Submitted to special issue on CAD-based vision. </note>
Reference-contexts: This paper is one of a series presenting different aspects of our work in multisensor object recognition using repeated rendering of 3D CAD models as part of an iterative predict and match cycle <ref> [40, 32, 25] </ref>. The emphasis here is on using evidence from range data to infer occlusion in range and optical images and adjusting predicted model features accordingly. A critical aspect of our work is what we call coreg-istration. <p> As can be seen, the system was able to correct for the rotation and translation errors present in the initial coregistration estimate. 6 Additional Examples We have run the matching system just described on 35 images from the Fort Carson dataset. These experiments are described in <ref> [25] </ref>. On these 35 cases, the system correctly identifies the vehicle present (out of four possible vehicles) in roughly 80 percent of the cases. While the system performed adequately on occluded targets, properly identifying about half, it often did not correctly recover the true object pose.
Reference: [26] <author> J. Ross Beveridge and Mark R. Stevens and Zhongfei Zhang and Mike Goss. </author> <title> Approximate Image Mappings Between Nearly Boresight Aligned Optical and Range Sensors. </title> <type> Technical Report CS-96-112, </type> <institution> Computer Science, Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Consider data from two co-located sensors. Through calibration, an initial estimate of the pixel-to-pixel mapping can be developed. However, for nearly boresight aligned sensors, tiny rotations of one sensor relative to another can induce several pixel translations of one image relative to the other <ref> [26] </ref>. Therefore, to perform robustly despite such minor variations in image registration, an object recognition algorithm should refine sensor alignment as well as object pose. It is the combined process of determining pose and registration that we call coregistration. <p> The data is from the Fort Carson Dataset, which contains range, IR and color imagery of military targets against natural terrain. The imagery and documentation [8] is available through our website at http://www.cs.colostate.edu/~vision. Sensor calibration data is available in <ref> [26] </ref>. The bottom row of Figure 2 shows an initial coreg-istration hypothesis for a specific CAD model. The hypothesized relationship between the model and the color image is shown as white lines in the color image.
Reference: [27] <author> David G. Lowe. </author> <title> The Viewpoint Consistency Constraint. </title> <journal> In ternational Journal of Computer Vision, </journal> <volume> 1(1):58 - 72, </volume> <year> 1987. </year>
Reference-contexts: Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another. Traditional techniques either rely on error measures being robust to such occlusions <ref> [27, 23, 6, 44] </ref>, or associate a likelihood of finding each feature based on off-line appearance analysis [14, 45, 36, 22].
Reference: [28] <author> David G. Lowe. </author> <title> Three-dimensional Object Recognition from Single Two-dimensional Images. </title> <journal> Artificial Intelligence, </journal> <volume> 31, </volume> <year> 1987. </year>
Reference-contexts: For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance <ref> [28, 23, 21, 6, 10] </ref>. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed [30, 39, 43, 17, 24]. However, this research area is still young.
Reference: [29] <author> David G. Lowe and Thomas O. Binford. </author> <title> The Recov ery of Three-Dimensional Structure from Image Curves. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(3):320 - 325, </volume> <year> 1985. </year>
Reference-contexts: Thus object recognition can be viewed as finding evidence in imagery for various model constraints. Usually model properties are chosen such that they can easily be found in the data: points [37], lines [6], curves <ref> [29] </ref>, corners [3] or planes [19]. Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another.
Reference: [30] <author> M. J. Magee, B. A. Boyter, C. H. Chien, and J. K. Aggarwal. </author> <title> Experiments in Intensity Guided Range Sensing Recognition of Three-Dimensional Objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(6):629 - 637, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10]. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed <ref> [30, 39, 43, 17, 24] </ref>. However, this research area is still young. Fusing information from different modalities is complicated by many fac tors, including recovery of sensor alignment and registration [1].
Reference: [31] <author> Martti Mantyla. </author> <title> An Introduction to Solid Modeling. </title> <publisher> Com puter Science Press, </publisher> <year> 1990. </year>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys [35, 16, 5] and CAD rendering <ref> [38, 18, 31] </ref>. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12].
Reference: [32] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Precise Matching of 3-D Target Models to Multisensor Data. </title> <journal> IEEE Transactions on Image Processing, </journal> <note> page (to appear), </note> <month> January </month> <year> 1997. </year>
Reference-contexts: This paper is one of a series presenting different aspects of our work in multisensor object recognition using repeated rendering of 3D CAD models as part of an iterative predict and match cycle <ref> [40, 32, 25] </ref>. The emphasis here is on using evidence from range data to infer occlusion in range and optical images and adjusting predicted model features accordingly. A critical aspect of our work is what we call coreg-istration. <p> Previously we have developed a CAD rendering package [41], as well as an iterative generated and test algorithm which uses the predicted features and an error function to accomplish multi-sensor object recognition <ref> [32, 40] </ref>. Here we expand on the use of rendering as a tool for coping with occlusion. To reason about occlusion, there must be information available which suggests occlusion is present. <p> Line search is used to generate perturbations of the current estimate from coarse-to-fine sampling in each dimension in order to reduce the algorithms sensitivity to step sizes. For each discrete perturbation, the match error is evaluated. This match error, formally defined in <ref> [32] </ref>, is summarized here.
Reference: [33] <author> Alan M. McIvor. </author> <title> Segmentation of 3d surface data. </title> <booktitle> In In Im age and Vision Computing New Zealand, </booktitle> <pages> pages 79-84, </pages> <address> Lower Hutt, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: For our problem, we use both 2D color imagery as well as 3D range data. Range data has the nice property of allowing the clear determination of when a certain model feature is not visible because another object is lying in front of it <ref> [33] </ref>. The use of multiple sensors facilitates the process of reasoning about why certain model features are not being found in the data, and allows an adaptive method for removing model features as they are determined to be occluded.
Reference: [34] <author> N. Nandhakumar and J. K. Aggarwal. </author> <title> Integrated analysis of thermal and visual images for scene interpretation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(4) </volume> <pages> 469-481, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR <ref> [34] </ref> and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10].
Reference: [35] <author> Arthur R. Pope. </author> <title> Model-Based Object Recognition. </title> <type> Technical report, </type> <institution> University of British Columbia, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys <ref> [35, 16, 5] </ref> and CAD rendering [38, 18, 31]. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12].
Reference: [36] <author> Arthur R. Pope. </author> <title> Learning to Recognize Objects in Images: Acquiring and Using Probablistic Models of Appearance. </title> <type> PhD thesis, </type> <institution> University of British Columbia, </institution> <year> 1995. </year>
Reference-contexts: Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another. Traditional techniques either rely on error measures being robust to such occlusions [27, 23, 6, 44], or associate a likelihood of finding each feature based on off-line appearance analysis <ref> [14, 45, 36, 22] </ref>. In the former case, matching can be hindered by the lack of support for an occluded feature and continue to examine possibilities even when the feature will never be found.
Reference: [37] <author> Sanjay Ranade and Azriel Rosenfeld. </author> <title> Point pattern matching by relaxation. </title> <journal> Pattern Recognition, </journal> <volume> 12:269 - 276, </volume> <year> 1980. </year>
Reference-contexts: When detailed CAD models of objects are available they can provide a rich source of constraint. Thus object recognition can be viewed as finding evidence in imagery for various model constraints. Usually model properties are chosen such that they can easily be found in the data: points <ref> [37] </ref>, lines [6], curves [29], corners [3] or planes [19]. Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another.
Reference: [38] <author> Scott D. Roth. </author> <title> Ray casting for modeling solids. </title> <journal> IEEE Com puter Graphics and Image Processing, </journal> <volume> 18 </volume> <pages> 109-144, </pages> <year> 1982. </year>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys [35, 16, 5] and CAD rendering <ref> [38, 18, 31] </ref>. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery [15, 9, 42], 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12].
Reference: [39] <author> A. Stentz and Y. </author> <title> Goto. </title> <booktitle> The CMU Navigational Architecture. In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 440-446, </pages> <address> Los Angeles, CA, </address> <month> February </month> <year> 1987. </year> <title> ARPA, </title> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10]. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed <ref> [30, 39, 43, 17, 24] </ref>. However, this research area is still young. Fusing information from different modalities is complicated by many fac tors, including recovery of sensor alignment and registration [1].
Reference: [40] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Interleaving 3d model feature prediction and matching to support multi-sensor object recognition. </title> <booktitle> In International Conference on Pattern Recognition, volume 13, </booktitle> <address> Austria, </address> <month> August </month> <year> 1996. </year> <journal> Internation Association of Pattern Recognition. </journal>
Reference-contexts: This paper is one of a series presenting different aspects of our work in multisensor object recognition using repeated rendering of 3D CAD models as part of an iterative predict and match cycle <ref> [40, 32, 25] </ref>. The emphasis here is on using evidence from range data to infer occlusion in range and optical images and adjusting predicted model features accordingly. A critical aspect of our work is what we call coreg-istration. <p> Previously we have developed a CAD rendering package [41], as well as an iterative generated and test algorithm which uses the predicted features and an error function to accomplish multi-sensor object recognition <ref> [32, 40] </ref>. Here we expand on the use of rendering as a tool for coping with occlusion. To reason about occlusion, there must be information available which suggests occlusion is present.
Reference: [41] <author> Mark R. Stevens, J. Ross Beveridge, and Michael E. Goss. </author> <title> Reduction of BRL/CAD Models and Their Use in Automatic Target Recognition Algorithms. </title> <booktitle> In Proceedings: BRL-CAD Symposium. </booktitle> <institution> Army Research Labs, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Previously we have developed a CAD rendering package <ref> [41] </ref>, as well as an iterative generated and test algorithm which uses the predicted features and an error function to accomplish multi-sensor object recognition [32, 40]. Here we expand on the use of rendering as a tool for coping with occlusion. <p> Because orthographic rendering is used to generate the image, the clipped endpoints can be related back to the 3D model endpoints. This allows the derivation of 3D model edges for matching to the 2D color imagery. Details of clipping have been presented elsewhere <ref> [41] </ref>. from the current coregistration estimate. Remember that these segments are actually available in the 3D coordinates of the model. Also notice that initially most of the segments to be matched lie on the front portion of the model.
Reference: [42] <author> G.D Sullivan, A.D. Worrall, and J.M Ferryman. </author> <title> Visual Object Recognition Using Deformable Models of Vehicles. </title> <booktitle> In Workshop on Context-Based Vision, </booktitle> <pages> pages 75-86, </pages> <month> june </month> <year> 1995. </year>
Reference-contexts: Hence it draws upon two literatures: object recognition for which there are many surveys [35, 16, 5] and CAD rendering [38, 18, 31]. Most CAD-based recognition systems focus upon a single sensor. For example, models have been used for matching to 2D imagery <ref> [15, 9, 42] </ref>, 3D range data [2, 4], as well as multi-spectral imagery such as IR [34] and SAR [12]. Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10].
Reference: [43] <author> C.W. Tong, S.K. Rodgers, J.P. Mills, </author> <title> and M.K. Kabrinsky. Multisensor data fusion of laser radar and forward looking in-fared for target segmentation and enhancement. </title> <editor> In R.G. Buser and F.B. Warren, editors, </editor> <title> Infared Sensors and Sensor Fusion. </title> <booktitle> SPIE, </booktitle> <year> 1987. </year>
Reference-contexts: Typically these CAD systems rely on either 3D or 2D model geometry to constrain object location and appearance [28, 23, 21, 6, 10]. The more complex task of fusing data from sensors of different modalities, for instance range and optical, has also been addressed <ref> [30, 39, 43, 17, 24] </ref>. However, this research area is still young. Fusing information from different modalities is complicated by many fac tors, including recovery of sensor alignment and registration [1].
Reference: [44] <author> William M. Wells. </author> <title> Statistical Object Recognition. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another. Traditional techniques either rely on error measures being robust to such occlusions <ref> [27, 23, 6, 44] </ref>, or associate a likelihood of finding each feature based on off-line appearance analysis [14, 45, 36, 22].
Reference: [45] <author> M.D. Wheeler and K. </author> <title> Ikeuchi. Sensor modeling, markov ran dom fields, and robust localization for recognizing partially occluded objects. </title> <journal> IUW, </journal> <volume> 93 </volume> <pages> 811-818, </pages> <year> 1993. </year>
Reference-contexts: Perhaps one of the most bedeviling complications for model-based recognition arises when one object partially occludes another. Traditional techniques either rely on error measures being robust to such occlusions [27, 23, 6, 44], or associate a likelihood of finding each feature based on off-line appearance analysis <ref> [14, 45, 36, 22] </ref>. In the former case, matching can be hindered by the lack of support for an occluded feature and continue to examine possibilities even when the feature will never be found.
References-found: 45

