URL: ftp://ftp.cs.brown.edu/pub/techreports/94/cs94-08.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-94-08.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> David H. Wolpert, </author> <title> "Stacked Generalization," </title> <type> 90. </type>
Reference-contexts: This paper reports the results of experiments in adapting an idea from stacked regression and extending it in a domain-dependent fashion to improve the performance of an existing grammar learner. Wolpert <ref> [1] </ref> lays out a very general theoretical framework for stacked regression, based on supervised learning from noise-free data. The framework is sufficiently general that it can be reified in a number of quite different learning algorithms.
Reference: [2] <author> G.E. Schulz, </author> <title> "Comparison of predicted and experimentally determined structure of adenyl kinase," </title> <booktitle> Nature 250 (1974), </booktitle> <pages> 140-142. </pages>
Reference-contexts: One of the more obvious and feasible ideas for combining generalizers is simply to average across them, and other researchers report good results from this <ref> [2] </ref>. We can do slightly better than that, by choosing optimal weights to smooth over our grammars. To be more concrete, we are suggesting generating several (probabilistic) grammars G i and smoothing them together.
Reference: [3] <author> L. E. Baum, </author> <title> "An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes," </title> <booktitle> Inequalities III (1972), </booktitle> <pages> 1-8. </pages>
Reference-contexts: To smooth across multiple grammars, we choose some positive weights i , such that P i i = 1:0 and we define the smoothed probability of a sentence s to be P (s) = i We can choose the lambda i using the forward-backward algorithm <ref> [3] </ref> and some training data reserved prior to learning the grammars. It will be noticed that this suggestion contains nothing that is domain-specific or, for that matter, original. Provided multiple generalizers (our grammars) are available, it could be applied anywhere.
Reference: [4] <author> J. Baker, </author> <title> "Trainable Grammars for Speech Recognition," in Speech Communication Papers for the 97th Meeting of the Acoustic Society of America, </title> <editor> D. Klatt and J. Wolf, ed., ASA, </editor> <year> 1982, </year> <pages> 547-550. </pages>
Reference: [5] <author> Eugene Charniak, </author> <title> Statistical Language Learning, </title> <publisher> MIT, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference: [6] <author> Glenn Carroll & Eugene Charniak, </author> <title> "Learning Probabilistic Dependency Grammars from Labelled Text," </title> <booktitle> Proceedings of the AAAI Fall Symposium (1992). </booktitle>
Reference-contexts: Our two models meet this condition, most of the time, so it is a good approximate indicator for us. 2.3 PCFG induction Stacked regression requires some existing means of induction to furnish the generalizers it needs. We discuss our induction scheme in detail elsewhere <ref> [6] </ref>. In skeleton form, there are two parts to PCFG induction: acquiring the rules, and setting their probabilities. In essence, we divide the process into corresponding phases, and handle them separately. The rule acquisition phase is error-driven.
Reference: [7] <author> W. Nelson Francis & Henry Kucera, </author> <title> Frequency Analysis of English Usage: Lexicon and Grammar, </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1982. </year> <month> 13 </month>
Reference-contexts: The training data used was a 300,000 word subset of the Brown Corpus of Tagged English <ref> [7] </ref>, a collection of articles, excerpts from books, and the like. Each word is tagged with its part of speech, e.g., noun, verb, etc. Our grammars operate on the tags alone, ignoring the words entirely.
References-found: 7

