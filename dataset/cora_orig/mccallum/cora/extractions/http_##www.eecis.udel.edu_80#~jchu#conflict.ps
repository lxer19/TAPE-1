URL: http://www.eecis.udel.edu:80/~jchu/conflict.ps
Refering-URL: http://www.eecis.udel.edu:80/~jchu/
Root-URL: http://www.cis.udel.edu
Email: carberry@udel.edu  jchu@udel.edu  lambert@pcs.cnu.edu  
Title: User-System Conflict in Task-Oriented Consultation  
Author: Sandra Carberry Jennifer Chu Lynn Lambert 
Address: 19716, U.S.A.  19716, U.S.A.  Christopher Newport University Newport News, VA 23606, U.S.A.  
Affiliation: Department of Computer Science University of Delaware Newark, DE  Department of Computer Science University of Delaware Newark, DE  Department of Physics and Computer Science  
Abstract: We have been developing a cooperative expert-consultation system in which user and consultant are jointly working to construct a plan for achieving the user's domain goal. Despite the cooperative nature of such interactions, conflict can arise due to the differing beliefs, desires, and preferences of the participants. Thus it is essential that such systems be able to detect the existence of conflict and effectively engage in a negotiation subdialogue in which the conflict is resolved. This paper describes our research on a plan-based approach to handling expert-consultation dialogues, with emphasis on recognition of conflict, managing the negotiation subdialogues that result, and identifying appropriate responses. 
Abstract-found: 1
Intro-found: 1
Reference: [AP80] <author> James F. Allen and C. Raymond Perrault. </author> <title> Analyzing Intention in Utterances. </title> <journal> Artificial Intelligence, </journal> <volume> 15:143--178, </volume> <year> 1980. </year>
Reference-contexts: The effect of an Inform act in Allen's system <ref> [AP80] </ref> was that the hearer believed the communicated proposition. In Perrault's persistence model of belief, the hearer adopts a communicated proposition unless there is evidence to the contrary, in which case the original belief persists.
Reference: [Bra90] <author> Michael Bratman. </author> <title> What is Intention? In Philip Cohen, </title> <editor> Jerry Morgan, and Martha Pollack, editors, </editor> <title> Intentions in Communication, </title> <publisher> pages 15--31. MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: U and S have different knowledge about the domain and the desirable characteristics of the domain plan; for example, S presumably has more extensive and accurate domain knowledge than U does, but U has knowledge about his particular circumstances, intentions, and preferences that are either restrictions on or potential influencers <ref> [Bra90] </ref> of the domain plan that is being constructed. U and S must communicate with one another since neither has all of the relevant information for constructing the domain plan.
Reference: [Car85] <author> Sandra Carberry. </author> <title> A Pragmatics Based Approach to Understanding Intersentential Ellipsis. </title> <booktitle> In Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics, pages 188--197, </booktitle> <address> Chicago, Illinois, </address> <year> 1985. </year>
Reference-contexts: This differentiation between the goal and effect allows our system to handle Inform actions that may not be immediately successful (i.e., when the hearer does not accept the speaker's claim). In <ref> [Car85, Car90] </ref>, we showed that in a cooperative interaction a participant must explicitly or implicitly accept a response or pursue discourse goals directed toward being able to accept the response. Thus failure to indicate a disagreement can be treated as implicit acceptance of the proposition conveyed by a response.
Reference: [Car90] <author> Sandra Carberry. </author> <title> Plan Recognition in Natural Language Dialogue. </title> <booktitle> ACL-MIT Press Series on Natural Language Processing. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: This differentiation between the goal and effect allows our system to handle Inform actions that may not be immediately successful (i.e., when the hearer does not accept the speaker's claim). In <ref> [Car85, Car90] </ref>, we showed that in a cooperative interaction a participant must explicitly or implicitly accept a response or pursue discourse goals directed toward being able to accept the response. Thus failure to indicate a disagreement can be treated as implicit acceptance of the proposition conveyed by a response.
Reference: [CGRSJ92] <author> Alison Cawsey, Julia Galliers, Steven Reece, and Karen Sparck Jones. </author> <title> Conflict and cooperation in a heterogeneous system. </title> <booktitle> In AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent Systems, </booktitle> <pages> pages 17--24, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: In our environment, the consultant is assumed to be more knowledgeable about the application domain, while the user has more knowledge about his personal preferences. Although much research effort has been devoted to conflict management, the work most closely related to ours is the information retrieval framework described in <ref> [CGRSJ92, Gal92] </ref>. Although this system models negotiations that occur when agents have inconsistent beliefs, it does not recognize conflicts unless they are directly stated in an utterance, nor does it provide general strategies for responding to a detected conflict.
Reference: [Chu93] <author> Jennifer Chu. </author> <title> Responding to User Queries in a Collaborative Environment. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 280--282, </pages> <year> 1993. </year> <note> Student Session. </note>
Reference-contexts: In this paper we briefly describe our tripartite model of user intention [LC91], and then discuss how our system detects several kinds of conflict, how it models the negotiation dialogues that ensue, and how our current work is addressing the problem of generating appropriate responses to resolve the conflict <ref> [LC92a, LC92b, Chu93] </ref>. 2 Modeling Intention In task-oriented consultation dialogues, one agent (U) is seeking help from the other agent (the consultant or S) in constructing a plan for achieving a domain goal (such as making investments or obtaining a university degree). <p> In this way the system can detect conflicts between the system and the user that are only implicitly conveyed by the user's utterances <ref> [Chu93] </ref>. 3.2.1 Detecting an Implicit Conflict Depending upon the preceding context, a query such as Who is teaching CS689? might implicitly suggest conflict as a result of the inferred actions that must be added to the domain plan in order to relate the query to the preceding dialogue. <p> Although the query itself could be answered and the dialogue continued, we argue that, in a collaborative environment, the agents should convey their disagreements as early as possible to avoid developing a plan that will later be discarded <ref> [Chu93] </ref>. In order to model and respond to such disagreements, we separate the dialogue model into an existing model, which consists of information agreed upon by both agents, and the proposed additions, which includes information that is open for negotiation.
Reference: [EC92] <author> Rhonda M. Eller and Sandra Carberry. </author> <title> A Meta-Rule Approach To Flexible Plan Recognition in Dialogue. User Modeling and User Adapted Interaction, </title> <address> 2(1-2):27--54, </address> <year> 1992. </year>
Reference-contexts: Consider a situation in which U then says (10) U: I want to satisfy my seminar course requirement. (11) Who's teaching CS689? modified to accommodate the separation of the existing and proposed dialogue models and augmented with Eller's <ref> [EC92] </ref> relaxation algorithm to recognize ill-formed plans.
Reference: [Gal92] <author> Julia Rose Galliers. </author> <title> Autonomous belief revision and communication. </title> <editor> In P. Gardenfors, editor, </editor> <title> Belief Revision, </title> <booktitle> Cambridge tracts in theoretical computer science. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1992. </year>
Reference-contexts: In our environment, the consultant is assumed to be more knowledgeable about the application domain, while the user has more knowledge about his personal preferences. Although much research effort has been devoted to conflict management, the work most closely related to ours is the information retrieval framework described in <ref> [CGRSJ92, Gal92] </ref>. Although this system models negotiations that occur when agents have inconsistent beliefs, it does not recognize conflicts unless they are directly stated in an utterance, nor does it provide general strategies for responding to a detected conflict.
Reference: [GS90] <author> Barbara Grosz and Candace Sidner. </author> <title> Plans for Discourse. </title> <editor> In P. Cohen, J. Morgan, and M. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Such interactions involve collaborative planning <ref> [GS90] </ref> since the two participants are 1 This material is based upon work supported by the National Science Foundation under Grant No. IRI-9122026. jointly working to construct U's domain plan.
Reference: [JWW84] <author> Aravind Joshi, Bonnie Webber, and Ralph Weischedel. </author> <title> Living Up To Expectations: Computing Expert Responses. </title> <booktitle> In Proceedings of the Fourth National Conference on Artificial Intelligence, pages 169--175, </booktitle> <address> Austin, Texas, </address> <year> 1984. </year>
Reference-contexts: Although this system models negotiations that occur when agents have inconsistent beliefs, it does not recognize conflicts unless they are directly stated in an utterance, nor does it provide general strategies for responding to a detected conflict. In addition, several other researchers <ref> [JWW84, Pol86, vB87] </ref> have addressed generating cooperative responses and responding to plan-based misconceptions, but did not capture these within an overall collaborative system that must negotiate proposals with the user. Our plan-based approach, on the other hand, provides a general mechanism for reasoning about and responding to conflict.
Reference: [LC91] <author> Lynn Lambert and Sandra Carberry. </author> <title> A Tripartite Plan-based Model of Dialogue. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the ACL, pages 47--54, </booktitle> <address> Berkeley, CA, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Our research focuses on recognizing and resolving both conflicts that are explicitly conveyed and conflicts that must be inferred from the actions that are proposed. In this paper we briefly describe our tripartite model of user intention <ref> [LC91] </ref>, and then discuss how our system detects several kinds of conflict, how it models the negotiation dialogues that ensue, and how our current work is addressing the problem of generating appropriate responses to resolve the conflict [LC92a, LC92b, Chu93]. 2 Modeling Intention In task-oriented consultation dialogues, one agent (U) is <p> Our project's earlier work resulted in a tripartite model of dialogue that captures intentions on three levels: domain, problem-solving, and discourse <ref> [LC91] </ref>. The domain level represents the system's beliefs about the user's plan for achieving some goal in the application domain. The problem-solving level encodes the system's beliefs about how the agents are going about constructing the user's domain plan.
Reference: [LC92a] <author> Lynn Lambert and Sandra Carberry. </author> <title> Modeling Negotiation Subdialogues. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the ACL, pages 193--200, </booktitle> <address> Newark, DE, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In this paper we briefly describe our tripartite model of user intention [LC91], and then discuss how our system detects several kinds of conflict, how it models the negotiation dialogues that ensue, and how our current work is addressing the problem of generating appropriate responses to resolve the conflict <ref> [LC92a, LC92b, Chu93] </ref>. 2 Modeling Intention In task-oriented consultation dialogues, one agent (U) is seeking help from the other agent (the consultant or S) in constructing a plan for achieving a domain goal (such as making investments or obtaining a university degree). <p> This algorithm takes into account contextual knowledge which suggests expectations based on the structure of the preceding dialogue, world knowledge which provides evidence for specific discourse acts, and linguistic knowledge which suggests certain generic discourse acts, a speaker's beliefs, and the strength of those beliefs. In <ref> [LC92a] </ref>, we present a plan-based strategy that uses this algorithm along with a multi-strength belief model to identify the structure of negotiation subdialogues, including recognizing both implicit acceptance of communicated propositions and negotiation subdialogues embedded within other negotiation subdialogues. Consider the following dialogue: (3) S: Dr.
Reference: [LC92b] <author> Lynn Lambert and Sandra Carberry. </author> <title> Using Linguistic, World, and Contextual Knowledge in a Plan Recognition Model of Dialogue. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 310--316, </booktitle> <address> Nantes, France, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: In this paper we briefly describe our tripartite model of user intention [LC91], and then discuss how our system detects several kinds of conflict, how it models the negotiation dialogues that ensue, and how our current work is addressing the problem of generating appropriate responses to resolve the conflict <ref> [LC92a, LC92b, Chu93] </ref>. 2 Modeling Intention In task-oriented consultation dialogues, one agent (U) is seeking help from the other agent (the consultant or S) in constructing a plan for achieving a domain goal (such as making investments or obtaining a university degree). <p> In other words, since U's utterance is interpreted as beginning a new Obtain-Info-Ref act unrelated to any of the actions at the discourse level of the current dialogue model, it closes those discourse actions and implicitly conveys their success. In <ref> [LC92b] </ref>, we provide an algorithm for recognizing expressions of doubt at a communicated proposition.
Reference: [Pol86] <author> Martha Pollack. </author> <title> A Model of Plan Inference that Distinguishes Between the Beliefs of Actors and Observers. </title> <booktitle> In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, pages 207--214, </booktitle> <address> New York, New York, </address> <year> 1986. </year>
Reference-contexts: It encodes information such as the preconditions for executing the action, the effects of the action, the subactions comprising the body of the action, etc. We adopt the term recipe following Pollack <ref> [Pol86] </ref>. <p> Although this system models negotiations that occur when agents have inconsistent beliefs, it does not recognize conflicts unless they are directly stated in an utterance, nor does it provide general strategies for responding to a detected conflict. In addition, several other researchers <ref> [JWW84, Pol86, vB87] </ref> have addressed generating cooperative responses and responding to plan-based misconceptions, but did not capture these within an overall collaborative system that must negotiate proposals with the user. Our plan-based approach, on the other hand, provides a general mechanism for reasoning about and responding to conflict.
Reference: [vB87] <author> Peter van Beek. </author> <title> A model for Generating Better Explanations. </title> <booktitle> In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, pages 215--220, </booktitle> <address> Stanford, California, </address> <year> 1987. </year>
Reference-contexts: Although this system models negotiations that occur when agents have inconsistent beliefs, it does not recognize conflicts unless they are directly stated in an utterance, nor does it provide general strategies for responding to a detected conflict. In addition, several other researchers <ref> [JWW84, Pol86, vB87] </ref> have addressed generating cooperative responses and responding to plan-based misconceptions, but did not capture these within an overall collaborative system that must negotiate proposals with the user. Our plan-based approach, on the other hand, provides a general mechanism for reasoning about and responding to conflict.
References-found: 15

