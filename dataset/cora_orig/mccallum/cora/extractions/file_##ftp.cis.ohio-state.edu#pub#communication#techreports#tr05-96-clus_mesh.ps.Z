URL: file://ftp.cis.ohio-state.edu/pub/communication/techreports/tr05-96-clus_mesh.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~panda/cluster_pub.html
Root-URL: 
Title: Designing Processor-cluster Based Systems: Interplay Between Cluster Organizations and Collective Communication Algorithms  
Author: Debashis Basak and Dhabaleswar K. Panda 
Abstract: Technical Report OSU-CISRC-01/96-TR05 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Asthana, H. Jagdish, and B. Mathews. </author> <title> Impact of Advanced VLSI packaging on the design of a large parallel computer. </title> <booktitle> In Proc. of the Int. Conf. on Parallel Processing, </booktitle> <month> Aug </month> <year> 1989. </year>
Reference-contexts: 1 Introduction With advancements in VLSI and packaging technologies it has become cost-effective to integrate multiple processing elements into a chip or a board <ref> [1] </ref>. This is leading to the development of parallel systems using such processor-clusters as building blocks instead of single processors, thus allowing a modular and hierarchical approach to building large systems. Prominent examples of processor-cluster based systems are the Stanford DASH [8], Intel Paragon [11], and Cray-T3D [5].
Reference: [2] <author> D. Basak and D. K. Panda. </author> <title> Alleviating Consumption Channel Bottleneck in Wormhole-Routed k-ary n-cube Systems. </title> <type> Technical Report OSU-CISRC-9/95-TR36, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <year> 1995. </year>
Reference-contexts: However, with advancements in VLSI, larger clusters are expected to become common. A cluster is connected to its inter-cluster network router through a cluster interface. The interface offers a number of channels to and from the network router. These channels (referred to as injection/consumption channels <ref> [2] </ref>) are used to inject/consume messages to/from other clusters. We use i to denote the number of injection or consumption channels per cluster. <p> When a message is injected, the memory needs to be read at a rate matching the network channel speed. Otherwise, the network channels can remain under-utilized leading to a fall in network performance <ref> [2] </ref>. In this paper we assume memory bandwidth per processor to support only one message injection, that is, a single-port model.
Reference: [3] <author> D. Basak and D. K. Panda. </author> <title> Designing Clustered Multiprocessor Systems under Packaging and Technological Advancements. </title> <type> Technical Report OSU-CISRC-11/95-TR51, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <year> 1995. </year>
Reference-contexts: In recent years there has been a lot of research related to designing processor-clustered based systems. However, most of these research have emphasized on packaging aspects <ref> [3, 14, 10] </ref>. These results are geared towards deriving optimal inter-cluster topologies under packaging and pinout constraints. While deriving such optimal topologies researchers have primarily emphasized on the load on network bisection and evaluated different topologies with respect to average message latency and throughput. <p> These channels (referred to as injection/consumption channels [2]) are used to inject/consume messages to/from other clusters. We use i to denote the number of injection or consumption channels per cluster. Typically, most systems have i = 1-2, this value being limited by the maximum available router pinout <ref> [3] </ref>. 3 Processor-cluster organizations Processor-cluster based architectures have been motivated by the convenience of packaging multiple processors into a single cluster [3, 14]. Details about the topology inside the cluster, the memory organization, impact of this organization on system performance etc. have not been addressed. <p> Typically, most systems have i = 1-2, this value being limited by the maximum available router pinout [3]. 3 Processor-cluster organizations Processor-cluster based architectures have been motivated by the convenience of packaging multiple processors into a single cluster <ref> [3, 14] </ref>. Details about the topology inside the cluster, the memory organization, impact of this organization on system performance etc. have not been addressed.
Reference: [4] <author> D. Basak and D. K. Panda. </author> <title> Benefits of Processor Clustering in Designing Large Parallel Systems: When and How? In Proc. </title> <booktitle> of the Int. Parallel Processing Symposium, </booktitle> <month> Apr </month> <year> 1996. </year>
Reference-contexts: However, the performance of a real system depends on several other factors like locality of reference, communication overheads, and performance of frequently-used communication patterns. In our recent work <ref> [4] </ref> we have considered communication overheads and have demonstrated how and when processor-clustered systems provide benefits leading to cost-effective system designs. However, in all the above works the impact of intra-cluster organization on the performance of processor-clustered systems have been ignored.
Reference: [5] <author> Cray Reasearch Inc. </author> <title> Cray T3D System Architecture Overview, </title> <year> 1993. </year>
Reference-contexts: This is leading to the development of parallel systems using such processor-clusters as building blocks instead of single processors, thus allowing a modular and hierarchical approach to building large systems. Prominent examples of processor-cluster based systems are the Stanford DASH [8], Intel Paragon [11], and Cray-T3D <ref> [5] </ref>. Typically, the interconnections connecting the processor-clusters of these systems are scalable meshes, tori, or multistage networks. This is referred to as the inter-cluster network. The interconnection inside a processor cluster is referred to as the intra-cluster topology. <p> In the recent past many parallel multiprocessor systems have been developed using such processor-clusters e.g. the CRAY T3D <ref> [5] </ref>, Intel Paragon [11], and the Stanford DASH [8]. Most of these systems are two-level architectures as shown in Figure 1. The processor-clusters are interconnected through a scalable inter-cluster network, e.g. 3D torus, 2D mesh. <p> In the following subsections, we analyze the communication delay to transfer m words to a processor's own memory from the memory of another processor in a) a different cluster and b) the same cluster. This kind of operation is similar to the basic shmem get primitive <ref> [5] </ref> available on the Cray T3D system. The shmem get transfer across clusters can be achieved only through inter-processor message communication. <p> Wormhole 3 routing being distance insensitive [6], our expression for cost is independent of a distance factor. Typical value of t s in modern systems ranges from 200-1000 network cycles, while value of t p ranges from 10 40 cycles/word <ref> [5] </ref>. The inter-cluster communication cost is independent of the cluster organization and is assumed to be the same in all cluster organizations discussed below. The time for a shmem get operation from a memory inside the same cluster depends on the cluster organization.
Reference: [6] <author> W. J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(6), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: We assume the currently popular wormhole mechanism for routing in the inter-cluster. Wormhole 3 routing being distance insensitive <ref> [6] </ref>, our expression for cost is independent of a distance factor. Typical value of t s in modern systems ranges from 200-1000 network cycles, while value of t p ranges from 10 40 cycles/word [5].
Reference: [7] <author> A. Erlichson et al. </author> <title> The benefits of clustering in shared address space mutliprocessors: An application-driven investigation. </title> <type> Technical Report CSL-TR-94-632, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: In order to support shared memory programming it will be beneficial to allow shared memory between processors inside a cluster. Recently it has been shown that DASH like architecture providing common directory access for processors within a cluster may lead to performance degradation <ref> [7] </ref>. However, it is not clear whether distributed memory systems need to provide shared-memory access between sibling processors (processors within a cluster)? From a naive point of view such sharing does not seem to be beneficial due to the programming model.
Reference: [8] <author> D. Lenoski et al. </author> <title> The Stanford DASH Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <year> 1990. </year>
Reference-contexts: This is leading to the development of parallel systems using such processor-clusters as building blocks instead of single processors, thus allowing a modular and hierarchical approach to building large systems. Prominent examples of processor-cluster based systems are the Stanford DASH <ref> [8] </ref>, Intel Paragon [11], and Cray-T3D [5]. Typically, the interconnections connecting the processor-clusters of these systems are scalable meshes, tori, or multistage networks. This is referred to as the inter-cluster network. The interconnection inside a processor cluster is referred to as the intra-cluster topology. <p> In the recent past many parallel multiprocessor systems have been developed using such processor-clusters e.g. the CRAY T3D [5], Intel Paragon [11], and the Stanford DASH <ref> [8] </ref>. Most of these systems are two-level architectures as shown in Figure 1. The processor-clusters are interconnected through a scalable inter-cluster network, e.g. 3D torus, 2D mesh.
Reference: [9] <author> C. T. Ho and S. L. Johnsson. </author> <title> Distributed Routing Algorithms for Broadcasting and Personalized Communication in Hypercubes. </title> <booktitle> In Proc. International Conference on Parallel processing, </booktitle> <pages> pages 640-648, </pages> <year> 1986. </year>
Reference-contexts: With such pinout technologies i = 2, 4 injection/consumption channels will be commonly available in clustered systems. The Cray T3D currently supports i = 2 from each processor-cluster. In literature <ref> [9] </ref> injection channels have often been referred to as communication ports. In this discussion although we have multiple injection channel per cluster, we allow a processor to be injecting at most one message at a time. Thus, from a single processor's perspective ours is not a multi-port model.
Reference: [10] <author> W. Hsu and P. C. Yew. </author> <title> The Impact of Wiring Constraints on Hierarchical Network Performance. </title> <booktitle> In Proc. of the Int. Parallel Processing Symposium, </booktitle> <month> Mar </month> <year> 1992. </year>
Reference-contexts: In recent years there has been a lot of research related to designing processor-clustered based systems. However, most of these research have emphasized on packaging aspects <ref> [3, 14, 10] </ref>. These results are geared towards deriving optimal inter-cluster topologies under packaging and pinout constraints. While deriving such optimal topologies researchers have primarily emphasized on the load on network bisection and evaluated different topologies with respect to average message latency and throughput.
Reference: [11] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <booktitle> 1991. </booktitle> <pages> 18 </pages>
Reference-contexts: This is leading to the development of parallel systems using such processor-clusters as building blocks instead of single processors, thus allowing a modular and hierarchical approach to building large systems. Prominent examples of processor-cluster based systems are the Stanford DASH [8], Intel Paragon <ref> [11] </ref>, and Cray-T3D [5]. Typically, the interconnections connecting the processor-clusters of these systems are scalable meshes, tori, or multistage networks. This is referred to as the inter-cluster network. The interconnection inside a processor cluster is referred to as the intra-cluster topology. <p> In the recent past many parallel multiprocessor systems have been developed using such processor-clusters e.g. the CRAY T3D [5], Intel Paragon <ref> [11] </ref>, and the Stanford DASH [8]. Most of these systems are two-level architectures as shown in Figure 1. The processor-clusters are interconnected through a scalable inter-cluster network, e.g. 3D torus, 2D mesh.
Reference: [12] <author> P. K. McKinley, H. Xu, A.-H. Esfahanian, and L. M. Ni. </author> <title> Unicast-based Multicast Communi--cation in Wormhole-routed Net works. </title> <journal> IEEETPDS, </journal> <volume> 5(12) </volume> <pages> 1252-1265, </pages> <month> Dec </month> <year> 1994. </year>
Reference-contexts: However, the ideas presented in 6 this paper can be applied, with minor modifications to other collective communication operations as such as gather, reduction, and barrier synchronization. 4 Evaluation of different processor-cluster organizations The U mesh algorithm has been proposed in <ref> [12] </ref> as an optimal algorithm to achieve multicasts and broadcasts in non-clustered mesh interconnected systems. To perform a multicast/broadcast to (N 1) destinations the algorithm orders the N 1 destinations and source into a dimension-ordered chain and achieves the operation in dlog 2 N e contention-free steps. <p> In this section we demonstrate that with such differential communication costs, existing algorithms designed to be optimal under the assumption of flat communication cost may not remain optimal in clustered systems. We analyze the U Mesh <ref> [12] </ref> algorithm for multicasting/broadcasting in terms of the required number of inter-cluster and intra-cluster steps. <p> A broadcast in a system with N processors is achieved in dlog 2 N e steps. The input to the U mesh algorithm is a dimension-ordered chain <ref> [12] </ref> of all the processors. Let us analyze the application of the U mesh algorithm for achieving a broadcast in a system with 9 clusters, C0 C9, each having c = 4 processors, as shown in Fig. 5.
Reference: [13] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> Mar </month> <year> 1994. </year>
Reference-contexts: We study the interplay between intra-cluster organizations and the performance of collective communication operations, which are frequently used operations on distributed memory systems <ref> [13] </ref>. We carry out our study in four phases. During the first phase we perform experiments on Cray T3D to study the impact of processor clustering on communication between remote processors and sibling processors.
Reference: [14] <author> M. T. Raghunath and A. Ranade. </author> <title> Designing interconnection networks for multi-level packaging. </title> <booktitle> In Proc. of the Supercomputing, </booktitle> <pages> pages 772-781, </pages> <year> 1993. </year> <month> 19 </month>
Reference-contexts: In recent years there has been a lot of research related to designing processor-clustered based systems. However, most of these research have emphasized on packaging aspects <ref> [3, 14, 10] </ref>. These results are geared towards deriving optimal inter-cluster topologies under packaging and pinout constraints. While deriving such optimal topologies researchers have primarily emphasized on the load on network bisection and evaluated different topologies with respect to average message latency and throughput. <p> Typically, most systems have i = 1-2, this value being limited by the maximum available router pinout [3]. 3 Processor-cluster organizations Processor-cluster based architectures have been motivated by the convenience of packaging multiple processors into a single cluster <ref> [3, 14] </ref>. Details about the topology inside the cluster, the memory organization, impact of this organization on system performance etc. have not been addressed.
References-found: 14

