URL: http://ferdowsi.eecs.berkeley.edu/~emile/publications/fairTcp.ps
Refering-URL: http://ferdowsi.eecs.berkeley.edu/~emile/pub.html
Root-URL: http://www.cs.berkeley.edu
Title: Improving Fairness of TCP Congestion Avoidance CS268 term project  
Author: Tom Henderson and Emile Sahouria 
Date: May 16, 1997  
Abstract: The congestion avoidance algorithm used by TCP results in unfair bandwidth allocation when multiple connections with different round trip times (RTTs) share a bottleneck link. In this project we use simulation to explore possible improvements to network fairness by experimenting with different window adjustment algorithms which allow connections with long RTTs to become more aggressive. We find that, with simple modifications to the algorithm, we are able to reduce but not eliminate the biases inherent in the protocol. We also find that more aggressive TCP connections tend to obtain more bandwidth from the flows that are already using the most bandwidth, thereby improving fairness. Finally, we observe that TCPs which are too aggressive can end up hurting their own throughput relative to the original algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [BP95] <author> L. Brakmo and L. Peterson. </author> <title> Tcp vegas: End to end congestion avoidance on a global internet. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 13(8) </volume> <pages> 1465-1480, </pages> <month> 10 </month> <year> 1995. </year>
Reference-contexts: In short, we assume an environment similar to the present day Internet, with the possible addition of RED queueing strategies and the latest in TCP improvements (SACK [MMFR96] and large window enhancements [JBB92]). TCP Ve-gas <ref> [BP95] </ref> is also not considered. We do not suggest this is an optimal environment, just a practical one. Following a review of related work, we begin with our operating definition of fairness and a brief analysis and heuristic insights. <p> In practice, the variable srtt in TCP implementation is a smoothed RTT estimate, but it only has the granularity of 500ms. One previously proposed solution is to adopt the TCP Vegas proposal <ref> [BP95] </ref> and put accurate system clock values into the TCP timestamp field. The second approach would be to maintain an averaged value of the instantaneous value of srtt. We experimented with this approach while collecting our TCP trace data from actual connections.
Reference: [BSS + 97] <author> H. Balakrishnan, S. Seshan, M. Stemm, V. Padmanabhan, and R. Katz. </author> <title> Tcp behavior of a busy web server analysis and improvements. </title> <type> UC Berkeley CS Technical Report, </type> <year> 1997. </year>
Reference-contexts: One advantage to this approach is that it overcomes small offered windows by the receiver. This technique, however, has not been widely accepted by the research community as an elegant solution to the problem, and necessarily incurs the overhead of multiplexing the multiple connections. Recent results <ref> [BSS + 97] </ref> in which congestion window state is shared across the multiple connections are promising, however. Finally, we note that we assumed the availability of fine grained timestamps in order to get an accurate measurement of RTT. <p> Improvements to the implementation of the aggressive window build algorithm may be one solution. Other changes to the retransmission mechanism, such as those suggested by <ref> [BSS + 97] </ref>, are another possibility. * We need to validate our simulation results with actual experiments in the network.
Reference: [CJ89] <author> D. Chiu and R. Jain. </author> <title> Analysis of the increase and decrease algorithms for congestion avoidance in computer networks. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 17 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: The "constant rate" and "linear in RTT" adjustment algorithms explored in this paper are derived from this work, although no results are given. The basis for the additive increase, multiplicative decrease policy in TCP is work relating to DECnet done by Jain and colleagues (e.g., <ref> [CJ89] </ref>). While connections with different round trip times are not considered in this paper, the authors show that such a policy will converge to fairness. <p> Since only the bottleneck links of certain topologies will be considered in this work, attention is restricted to characterizations of single links. The definition of fairness in <ref> [CJ89] </ref> is used in evaluating the fairness of flow control schemes in this paper.
Reference: [DKS90] <author> A. Demers, S. Keshav, and S. Shenker. </author> <title> Analysis and simulation of a fair queueing algorithm. Internetworking: </title> <journal> Research and Experience, </journal> <volume> 1(1) </volume> <pages> 3-26, </pages> <year> 1990. </year>
Reference-contexts: We focus on steady state connection performance; start up issues such as the proposed "4K slow start" [Flo97a] are not considered herein. We also do not assume the implementation of fair scheduling algorithms (e.g., fair queuing <ref> [DKS90] </ref>) which can isolate flows or classes of flows from one another. In short, we assume an environment similar to the present day Internet, with the possible addition of RED queueing strategies and the latest in TCP improvements (SACK [MMFR96] and large window enhancements [JBB92]).
Reference: [FJ92] <author> S. Floyd and V. Jacobson. </author> <title> On traffic phase effects in packet switched gateways. </title> <journal> Inter-networking: Research and Experience, </journal> <volume> 3(3) </volume> <pages> 115-156, </pages> <month> 9 </month> <year> 1992. </year>
Reference-contexts: Window modifications are then proposed, and tested via simulation. Implementation issues are discussed next, after which we conclude. 2 Related work Several researchers have observed the bias in TCP against connections with long round trip times. Floyd has treated the problem in the most detail. In <ref> [FJ92] </ref>, the authors discuss a constant rate window adjustment algorithm similar to the one which we explore. They observe that RED gateways 2 and Reno-style enhancements to TCP are insufficient to correct the bias. <p> prevent excessive congestive losses, leading to fewer coarse timeouts, and SACK outperforms Reno in every case by eliminating many coarse timeouts as well. 6 Alternatives One question raised by Floyd is how to pick the proper value for the constant in both the constant rate and linear in RTT strategies <ref> [FJ92] </ref>. We chose to experiment with a range of values. We determined the limits of the values from actual measurements of TCP file transfers from our host to hosts around the world.
Reference: [FJ93] <author> S. Floyd and V. Jacobson. </author> <title> Random early detection gateways for congestion avoidance. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 3(3) </volume> <pages> 115-156, </pages> <month> 9 </month> <year> 1993. </year>
Reference-contexts: One connection has a relatively short RTT of 10ms. The RTTs of the other connection are varied between the discrete values of 100ms, 200ms, 300ms, and 600ms. Two different queue management schemes were examined: DropTail (a.k.a. FIFO queueing), and Random Early Detection ("RED") <ref> [FJ93] </ref>. Two different TCP's were tested: NewReno [Hoe96] and SACK [MMFR96]. The bottleneck queue size was varied from 4 packets to 24 packets. <p> The fact that RED gateways do not attempt to equalize the bandwidth of competing flows through the gateway was discussed in <ref> [FJ93] </ref>. 5.2 Test configuration 2 testnet2. In the figure, the upper lines again correspond to RED queues, and the performance of NewReno and SACK is comparable. In general, the two short delay connections obtained roughly 50% of the bandwidth, and the background web traffic consumed 20% of the traffic.
Reference: [Flo91] <author> S. Floyd. </author> <title> Connections with multiple congested gateways in packet-switched networks, part 1: One-way traffic. </title> <journal> ACM Computer Communications Review, </journal> <volume> 21(5) </volume> <pages> 30-47, </pages> <month> 10 </month> <year> 1991. </year>
Reference-contexts: Floyd has treated the problem in the most detail. In [FJ92], the authors discuss a constant rate window adjustment algorithm similar to the one which we explore. They observe that RED gateways 2 and Reno-style enhancements to TCP are insufficient to correct the bias. In <ref> [Flo91] </ref>, the performance of a constant rate increase algorithm is evaluated via simulation and heuristic analysis for long round trip times which traverse multiple gateways. <p> For long delay connections, the propagation delay is the dominant component. For short delay connections, the other delay components can dominate. However, for short delay connections, a suitably high "floor" value for the maximum window will suffice. Floyd <ref> [Flo91] </ref> developed a fairly general characterization of window increase algorithms that facilitates the discussion of fairness. A key assumption is that a number of packets approximately equal to the send window size is sent every round trip time. <p> The first allocation is fair according to several metrics, and the second lies in a range defined by several metrics as described in <ref> [Flo91] </ref>. The window increase policy c i = c fl r 2 i is termed the constant rate increase policy, since it can roughly be interpreted as causing the rate of packet transmission to increase itself at a constant 4 rate. A continuous time analogy may clarify this concept.
Reference: [Flo97a] <author> S. Floyd. </author> <title> Message to e2e-interest group mailing list, </title> <note> february 7, 1997. e2e-interest mailing list, </note> <year> 1997. </year>
Reference-contexts: We do not assume any explicit feedback of congestion provided by the network (other than dropping packets). We focus on steady state connection performance; start up issues such as the proposed "4K slow start" <ref> [Flo97a] </ref> are not considered herein. We also do not assume the implementation of fair scheduling algorithms (e.g., fair queuing [DKS90]) which can isolate flows or classes of flows from one another.
Reference: [Flo97b] <author> S. Floyd. </author> <title> A proposed modification to tcp's window increase algorithm. </title> <type> Unpublished draft, </type> <year> 1997. </year>
Reference-contexts: The author explores the performance when all connections in the simulation topology employ the modified algorithm, and shows that the performance of the constant rate algorithm meets at least one accepted measure of fairness, while the performance of standard TCP clearly does not. Finally, in <ref> [Flo97b] </ref>, the issues surrounding alternative window increase algorithms are explored in more detail. The "constant rate" and "linear in RTT" adjustment algorithms explored in this paper are derived from this work, although no results are given.
Reference: [Hoe96] <author> J. Hoe. </author> <title> Improving the start-up behavior of a congestion control scheme for tcp. </title> <booktitle> In Proceedings of ACM Sigcomm '96 Conference [sig96], </booktitle> <pages> pages 270-280. </pages>
Reference-contexts: The RTTs of the other connection are varied between the discrete values of 100ms, 200ms, 300ms, and 600ms. Two different queue management schemes were examined: DropTail (a.k.a. FIFO queueing), and Random Early Detection ("RED") [FJ93]. Two different TCP's were tested: NewReno <ref> [Hoe96] </ref> and SACK [MMFR96]. The bottleneck queue size was varied from 4 packets to 24 packets. <p> Some other notable configuration details are * In each case, the foreground TCP connections are persistent FTP connections which always have a packet to send. This is analogous to a large file transfer. The NewReno TCP code was discussed in <ref> [Hoe96] </ref>. NewReno sources have been paired with TCP sinks which produce delayed acknowledgments.
Reference: [Jaf81a] <author> K. Jaffe. </author> <title> Bottleneck flow control. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 29(7) </volume> <pages> 954-962, </pages> <month> 7 </month> <year> 1981. </year>
Reference-contexts: Other work which predates that of Floyd includes the work by Mankin on Random Drop congestion control, which also identifies the bias against long delay connections [Man90]. Jaffe provides early analytical work showing that no decentralizable algorithm maximizes power in a multiple user system [Jaf81b]. In an earlier paper <ref> [Jaf81a] </ref>, a decentralizable algorithm for fairness was described, but convergence could not be achieved in the presence of new users of the system. This work is indirectly related to the problem at hand, but the results and the assumptions made do not particularly apply.
Reference: [Jaf81b] <author> K. Jaffe. </author> <title> Flow control power is nondecentralizable. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 29(9) </volume> <pages> 1301-1306, </pages> <month> 9 </month> <year> 1981. </year>
Reference-contexts: Other work which predates that of Floyd includes the work by Mankin on Random Drop congestion control, which also identifies the bias against long delay connections [Man90]. Jaffe provides early analytical work showing that no decentralizable algorithm maximizes power in a multiple user system <ref> [Jaf81b] </ref>. In an earlier paper [Jaf81a], a decentralizable algorithm for fairness was described, but convergence could not be achieved in the presence of new users of the system. This work is indirectly related to the problem at hand, but the results and the assumptions made do not particularly apply.
Reference: [JBB92] <author> V. Jacobson, R. Braden, and D. </author> <title> Borman. Tcp extensions for high performance. Internet RFC 1323, </title> <year> 1992. </year>
Reference-contexts: In short, we assume an environment similar to the present day Internet, with the possible addition of RED queueing strategies and the latest in TCP improvements (SACK [MMFR96] and large window enhancements <ref> [JBB92] </ref>). TCP Ve-gas [BP95] is also not considered. We do not suggest this is an optimal environment, just a practical one. Following a review of related work, we begin with our operating definition of fairness and a brief analysis and heuristic insights.
Reference: [LM97] <author> T. Lakshman and U. Madhow. </author> <title> The performance of tcp/ip for networks with high bandwidth-delay products and random loss. to appear, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <year> 1997. </year> <month> 19 </month>
Reference-contexts: This work is indirectly related to the problem at hand, but the results and the assumptions made do not particularly apply. Finally, we recently became aware of a new paper which explicitly studies the performance of TCP/IP in networks with high bandwidth-delay products <ref> [LM97] </ref>. The authors observe that TCP is "grossly unfair" towards connections with higher round-trip delays, and suggest that an alternate dynamic window mechanism is a high priority for future research, although they do not endorse any new mechanism. <p> However, with large queues, the utilization approaches 100%. We believe this is due to two factors. First, with large queues, the queueing delay becomes large (approximately 100ms for a full queue of depth 20), so the relative disparity between the RTTs of the two connections is lessened <ref> [LM97] </ref>. Second, larger queues are better able to absorb bursts of packets and prevent the occurrance of coarse timeouts, which cause the connection to idle for long periods of time. 8 * In general, RED queues perform better in terms of utilization than do DropTail queues.
Reference: [Man90] <author> A. Mankin. </author> <title> Random drop congestion control. </title> <booktitle> In Proceedings of ACM Sigcomm '90 Conference, </booktitle> <volume> volume 20, </volume> <pages> pages 1-7, </pages> <year> 1990. </year>
Reference-contexts: Other work which predates that of Floyd includes the work by Mankin on Random Drop congestion control, which also identifies the bias against long delay connections <ref> [Man90] </ref>. Jaffe provides early analytical work showing that no decentralizable algorithm maximizes power in a multiple user system [Jaf81b]. In an earlier paper [Jaf81a], a decentralizable algorithm for fairness was described, but convergence could not be achieved in the presence of new users of the system.
Reference: [Mat97] <author> M. Mathis. </author> <title> Message to e2e-interest group mailing list, </title> <note> february 7, 1997. e2e-interest mailing list, </note> <year> 1997. </year>
Reference-contexts: FIFO queueing), and Random Early Detection ("RED") [FJ93]. Two different TCP's were tested: NewReno [Hoe96] and SACK [MMFR96]. The bottleneck queue size was varied from 4 packets to 24 packets. Since a general rule for the size of buffers in current routers is 100ms at full line rate <ref> [Mat97] </ref> (which would be about 15 packets in our case), the queue size of 4 packets is small (and perhaps corresponds to bottlenecks shared by a large number of competing connections) while 24 packets is larger than current implementations.
Reference: [MM96] <author> M. Mathis and J. Mahdavi. </author> <title> Forward acknowledgment: Refining tcp congestion control. </title> <booktitle> In Proceedings of ACM Sigcomm '96 Conference [sig96], </booktitle> <pages> pages 281-291. </pages>
Reference-contexts: This is analogous to a large file transfer. The NewReno TCP code was discussed in [Hoe96]. NewReno sources have been paired with TCP sinks which produce delayed acknowledgments. The SACK TCP code is based on the Internet RFC [MMFR96] with FACK congestion control modifications as suggested in <ref> [MM96] </ref>, and the sink for these connections does not implement delayed acknowledgments. * In all of our simulations, we ran a number of simulated WWW clients and servers in order to provide a background traffic flow of approximately 20% of the bottleneck link in the direction in which we were measuring.
Reference: [MMFR96] <author> M. Mathis, J. Mahdavi, S. Floyd, and A. Romanow. </author> <title> Tcp selective acknowledgment options. Internet RFC 2018, </title> <year> 1996. </year>
Reference-contexts: In short, we assume an environment similar to the present day Internet, with the possible addition of RED queueing strategies and the latest in TCP improvements (SACK <ref> [MMFR96] </ref> and large window enhancements [JBB92]). TCP Ve-gas [BP95] is also not considered. We do not suggest this is an optimal environment, just a practical one. Following a review of related work, we begin with our operating definition of fairness and a brief analysis and heuristic insights. <p> The RTTs of the other connection are varied between the discrete values of 100ms, 200ms, 300ms, and 600ms. Two different queue management schemes were examined: DropTail (a.k.a. FIFO queueing), and Random Early Detection ("RED") [FJ93]. Two different TCP's were tested: NewReno [Hoe96] and SACK <ref> [MMFR96] </ref>. The bottleneck queue size was varied from 4 packets to 24 packets. <p> This is analogous to a large file transfer. The NewReno TCP code was discussed in [Hoe96]. NewReno sources have been paired with TCP sinks which produce delayed acknowledgments. The SACK TCP code is based on the Internet RFC <ref> [MMFR96] </ref> with FACK congestion control modifications as suggested in [MM96], and the sink for these connections does not implement delayed acknowledgments. * In all of our simulations, we ran a number of simulated WWW clients and servers in order to provide a background traffic flow of approximately 20% of the bottleneck
Reference: [Pos81] <author> J. Postel. </author> <title> Transmission control protocol. Internet RFC 793, </title> <year> 1981. </year>
Reference-contexts: 1 Introduction The Transmission Control Protocol (TCP) <ref> [Pos81] </ref> is widely used in the Internet today. At the heart of TCP is its dynamic congestion avoidance and flow control mechanism, the performance of which depends heavily on the round trip delay of the connection.
Reference: [sig96] <institution> Proceedings of ACM Sigcomm '96 Conference, volume 26, </institution> <year> 1996. </year> <month> 20 </month>
References-found: 20

