URL: http://www.umiacs.umd.edu/users/resnik/pubs/left-corner.ps.gz
Refering-URL: http://www.umiacs.umd.edu/users/resnik/pubs.html
Root-URL: 
Email: resnik@linc.cis.upenn.edu  
Title: Left-corner Parsing and Psychological Plausibility  
Author: Philip Resnik 
Address: Philadelphia, PA 19104, USA  
Affiliation: Department of Computer and Information Science University of Pennsylvania,  
Abstract: It is well known that even extremely limited center-embedding causes people to have difficulty in comprehension, but that left- and right-branching constructions produce no such effect. If the difficulty in comprehension is taken to be a result of processing load, as is widely assumed, then measuring the processing load induced by a parsing strategy on these constructions may help determine its plausibility as a psychological model. On this basis, it has been argued [AJ91, JL83] that by identifying processing load with space utilization, we can rule out both top-down and bottom-up parsing as viable candidates for the human sentence processing mechanism, and that left-corner parsing represents a plausible alternative. Examining their arguments in detail, we find difficulties with each presentation. In this paper we revise the argument and validate its central claim. In so doing, we discover that the key distinction between the parsing methods is not the form of prediction (top-down vs. bottom-up vs. left-corner), but rather the ability to instantiate the operation of composition. 
Abstract-found: 1
Intro-found: 1
Reference: [AJ91] <author> Steven Abney and Mark Johnson. </author> <title> Memory requirements and local ambiguities for parsing strategies. </title> <journal> Journal of Psycholinguistic Research, </journal> <volume> 20(3) </volume> <pages> 233-250, </pages> <year> 1991. </year>
Reference-contexts: Johnson-Laird [JL83] observes that neither the top-down nor the bottom-up methods of constructing a parse tree fit the facts of (1), and proposes instead the less-well-known alternative of left-corner parsing. Abney and Johnson <ref> [AJ91] </ref> discuss a somewhat more general version of Johnson-Laird's argument, introducing the abstract notion of a parsing strategy in order to characterize what is meant by bottom-up, top-down, and left-corner parsing. <p> In particular, we show that the psychological plausibility argument hinges on the operation of composition and not left-corner prediction per se. 2 Comparing Strategies 2.1 Summary of the Argument For expository purposes, we begin with the discussion in <ref> [AJ91] </ref>. <p> Top-down filtering restricts the non-deterministic choices made by the parser, but does not affect the bottom-up construction of the parse tree along a single computation path. remedies the first difficulty we found in <ref> [AJ91] </ref>: the formal specification of each parsing algorithm permits us to express space utilization uniformly in terms of the automaton's stack. The top-down and bottom-up automata behave exactly as we would expect. <p> Hence it is not surprising that the automaton fails to support Johnson-Laird's argument: far from being bounded, the stack of such automaton can grow without bound as the depth of right-branching increases. 4 Arc-eager Enumeration as Composition 4.1 An Easy Fix... To summarize thus far, <ref> [AJ91] </ref> and [JL83] present two forms of the same argument, but each presentation suffers from a central shortcoming. Abney and Johnson, discussing parsing strategies rather than parsers, fail to characterize top-down, bottom-up, and left-corner parsing in a way that permits a fair comparison of space utilization. <p> Thus we have succeeded in presenting a complete version of the argument in <ref> [AJ91] </ref> and [JL83] in the sense that 1. top-down, bottom-up, and left-corner parsing are characterized in a formally precise way, 2. the characterizations are abstract, in the sense that the logic of the algorithms (in the form of nondeterministic push-down automata) is separated from their control (namely the control of how <p> B in A ! ff B) and the bottom-up view of that constituent (e.g. B in B ! fifl) so that they may later be identified. Unlike top-down and bottom-up parsers, a left-corner parser meets this criterion. By presenting a complete version of the argument in <ref> [AJ91] </ref> and [JL83], we have essentially re-discovered proposals made by Pulman [Pul85, Pul86] and Thomp-son et al. [TDL91]. Both propose parsers with left-corner prediction and a composition operation added. <p> Both <ref> [AJ91] </ref> and [JL83] make the same basic claim, namely that top-down and bottom-up parsing lead to incorrect predictions of asymmetry in human processing | predictions that can be avoided by utilizing a left-corner strategy. We have demonstrated difficulties with both of their formulations and presented a more precise account. <p> This has the unfortunate consequence of tying the argument to context-free grammars, losing the attractive formalism-independent quality evoked in <ref> [AJ91] </ref>. Since context-free grammars are no longer generally considered likely models for natural language in the general case [Shi85], one wonders how the discussion here might be extended to parsing within more powerful grammatical frameworks.
Reference: [ASU86] <author> Alfred Aho, Ravi Sethi, and Jeffrey Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1986. </year>
Reference-contexts: parsing 1 This observation is by no means language specific, though in SOV languages it is embedding on objects, not subjects, that causes difficulty. strategy to be "a way of enumerating the nodes and arcs of parse trees." This is, in fact, a generalization of the concept of a traversal <ref> [ASU86] </ref>. A top-down strategy is one in which each node is enumerated before any of its descendants are; a bottom-up strategy is one in which all the descendants of a node are enumerated before it is. <p> Johnson-Laird's presentation, though it encounters a difficulty of its own, turns out to complement Abney and Johnson's and to make clear how to solve the difficulties in both. Following the standard description in the compilers literature (see, e.g., <ref> [ASU86] </ref>), Johnson-Laird adopts the definition of a top-down parser as one that operates by recursive descent: it begins with the start symbol of the grammar and successively rewrites the leftmost nonterminal until it reaches a terminal symbol or symbols that can be matched against the input.
Reference: [DJK + 82] <author> A. DeRoeck, R. Johnson, M. King, M. Ros-ner, G. Sampson, and N. Varile. </author> <title> A myth about centre-embedding. </title> <journal> Lingua, </journal> <volume> 58 </volume> <pages> 327-340, </pages> <year> 1982. </year>
Reference-contexts: the embedded clause is complete and the corresponding verb is finally encountered. 1 Alternative accounts have been proposed, most sharing the premise that the parser's capacity for recursion is limited by bounds on storage. (See, for example, [Kim73] and [MI64]; for opposing views and other pointers to the literature see <ref> [DJK + 82] </ref>.) The distinction between center-embedding and left/right-branching has important implications for those who wish to construct psychologically plausible models of parsing.
Reference: [JL83] <author> Philip N. Johnson-Laird. </author> <title> Mental Models. </title> <publisher> Har-vard University Press, </publisher> <year> 1983. </year>
Reference-contexts: Johnson-Laird <ref> [JL83] </ref> observes that neither the top-down nor the bottom-up methods of constructing a parse tree fit the facts of (1), and proposes instead the less-well-known alternative of left-corner parsing. <p> The question of exactly what "eager enumeration" does would seem to merit further attention. We shall give it that attention shortly, in Section 4. 3 Comparing Automata Abney and Johnson's argument is largely an independent account quite similar to one made earlier in <ref> [JL83] </ref>. Here we present a brief summary of the argument as presented there. Johnson-Laird's presentation, though it encounters a difficulty of its own, turns out to complement Abney and Johnson's and to make clear how to solve the difficulties in both. <p> Hence it is not surprising that the automaton fails to support Johnson-Laird's argument: far from being bounded, the stack of such automaton can grow without bound as the depth of right-branching increases. 4 Arc-eager Enumeration as Composition 4.1 An Easy Fix... To summarize thus far, [AJ91] and <ref> [JL83] </ref> present two forms of the same argument, but each presentation suffers from a central shortcoming. Abney and Johnson, discussing parsing strategies rather than parsers, fail to characterize top-down, bottom-up, and left-corner parsing in a way that permits a fair comparison of space utilization. <p> Thus we have succeeded in presenting a complete version of the argument in [AJ91] and <ref> [JL83] </ref> in the sense that 1. top-down, bottom-up, and left-corner parsing are characterized in a formally precise way, 2. the characterizations are abstract, in the sense that the logic of the algorithms (in the form of nondeterministic push-down automata) is separated from their control (namely the control of how the automata's <p> B in A ! ff B) and the bottom-up view of that constituent (e.g. B in B ! fifl) so that they may later be identified. Unlike top-down and bottom-up parsers, a left-corner parser meets this criterion. By presenting a complete version of the argument in [AJ91] and <ref> [JL83] </ref>, we have essentially re-discovered proposals made by Pulman [Pul85, Pul86] and Thomp-son et al. [TDL91]. Both propose parsers with left-corner prediction and a composition operation added. Pulman motivates his parser's design on grounds of psychological plausibility, though he does not present a complete version of the argument discussed here. <p> Both [AJ91] and <ref> [JL83] </ref> make the same basic claim, namely that top-down and bottom-up parsing lead to incorrect predictions of asymmetry in human processing | predictions that can be avoided by utilizing a left-corner strategy. We have demonstrated difficulties with both of their formulations and presented a more precise account.
Reference: [JVSW88] <author> A. K. Joshi, K. Vijay-Shanker, and D. J. Weir. </author> <title> The convergence of mildly context-sensitive grammatical formalisms. </title> <editor> In P. Sells and T. Wasow, editors, </editor> <booktitle> Processing of Linguistic Structure. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: Since one attractive feature of CCG is its inherent left-to-right, word-by-word in-crementality, it is perhaps not surprising to find that parsers of CCG tend naturally to meet the criteria for psychological plausibility discussed here. CCG is one instance of a general class known as the mildly context-sensitive grammar formalisms <ref> [JVSW88] </ref>. We are currently investigating a generalization of the argument presented here to other formalisms within that class. Acknowledgements This research was supported by the following grants: ARO DAAL 03-89-C-0031, DARPA N00014-90-J-1863, NSF IRI 90-16592, and Ben Franklin 91S.3078C-1.
Reference: [Kim73] <author> J. Kimball. </author> <title> Seven principles of surface-structure parsing in natural language. </title> <journal> Cognition, </journal> <volume> 2 </volume> <pages> 15-47, </pages> <year> 1973. </year>
Reference-contexts: the head noun phrase of each subject be stored until the processing of the embedded clause is complete and the corresponding verb is finally encountered. 1 Alternative accounts have been proposed, most sharing the premise that the parser's capacity for recursion is limited by bounds on storage. (See, for example, <ref> [Kim73] </ref> and [MI64]; for opposing views and other pointers to the literature see [DJK + 82].) The distinction between center-embedding and left/right-branching has important implications for those who wish to construct psychologically plausible models of parsing.
Reference: [LP81] <author> Harry Lewis and Christos Papadimitriou. </author> <title> Elements of the Theory of Computation. </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: parser begins with S on top of the stack, and a string has been successfully recognized if the stack is empty and the input exhausted. 5 The analysis being straightforward, we omit the details here; for a complete discussion of the construction of PDAs for top-down and bottom-up parsing, see <ref> [LP81, x3.6] </ref>. from the bottom-up view Rules 1-3 simply introduce lexical items onto the stack as they are scanned.
Reference: [MC63] <author> George Miller and Noam Chomsky. </author> <title> Finitary models of language users. </title> <editor> In R. Luce, R. Bush, and E. Galanter, editors, </editor> <booktitle> Handbook of Mathematical Psychology, </booktitle> <volume> Volume 2. </volume> <publisher> John Wiley, </publisher> <year> 1963. </year>
Reference-contexts: 1 Introduction One of our most robust observations about language | dating back at least to the seminal work of Miller and Chomsky <ref> [MC63] </ref> | is that right- and left-branching constructions such as (1a) and (1b) seem to cause no particular difficulty in processing, but that multiply center-embedded constructions such as (1c) are difficult to understand. (1) a. [[[John's] brother's] cat] despises rats. b.
Reference: [MI64] <author> G. A. Miller and S. Isard. </author> <title> Free recall of self-embedded English sentences. </title> <journal> Information and Control, </journal> <volume> 7:292|303, </volume> <year> 1964. </year>
Reference-contexts: noun phrase of each subject be stored until the processing of the embedded clause is complete and the corresponding verb is finally encountered. 1 Alternative accounts have been proposed, most sharing the premise that the parser's capacity for recursion is limited by bounds on storage. (See, for example, [Kim73] and <ref> [MI64] </ref>; for opposing views and other pointers to the literature see [DJK + 82].) The distinction between center-embedding and left/right-branching has important implications for those who wish to construct psychologically plausible models of parsing.
Reference: [PS87] <author> Fernando C. N. Pereira and Stuart M. Shieber. </author> <title> Prolog and Natural Language Analysis. Center for the Study of Language and Information, </title> <year> 1987. </year>
Reference-contexts: He then provides a more formal characterization of the various parsers by expressing each as a push-down automaton (PDA). Such a characterization immediately 4 Although top-down filtering can be added (see, e.g., <ref> [PS87, p. 182] </ref>), Schabes (personal communication) points out that left-corner parsing with top-down filtering is essentially the same as LR parsing.
Reference: [Pul85] <author> Stephen Pulman. </author> <title> A parser that doesn't. </title> <booktitle> In Proceedings of the 2nd European ACL, </booktitle> <pages> pages 128-135, </pages> <year> 1985. </year>
Reference-contexts: B in B ! fifl) so that they may later be identified. Unlike top-down and bottom-up parsers, a left-corner parser meets this criterion. By presenting a complete version of the argument in [AJ91] and [JL83], we have essentially re-discovered proposals made by Pulman <ref> [Pul85, Pul86] </ref> and Thomp-son et al. [TDL91]. Both propose parsers with left-corner prediction and a composition operation added. Pulman motivates his parser's design on grounds of psychological plausibility, though he does not present a complete version of the argument discussed here.
Reference: [Pul86] <author> Stephen Pulman. </author> <title> Grammars, parsers, and memory limitations. </title> <booktitle> Language and Cognitive Processes, </booktitle> <volume> 1(3) </volume> <pages> 197-225, </pages> <year> 1986. </year>
Reference-contexts: B in B ! fifl) so that they may later be identified. Unlike top-down and bottom-up parsers, a left-corner parser meets this criterion. By presenting a complete version of the argument in [AJ91] and [JL83], we have essentially re-discovered proposals made by Pulman <ref> [Pul85, Pul86] </ref> and Thomp-son et al. [TDL91]. Both propose parsers with left-corner prediction and a composition operation added. Pulman motivates his parser's design on grounds of psychological plausibility, though he does not present a complete version of the argument discussed here.
Reference: [Shi85] <author> S. M. Shieber. </author> <title> Evidence against the context-freeness of natural language. </title> <journal> Linguistics and Philosophy, </journal> <volume> 8 </volume> <pages> 333-343, </pages> <year> 1985. </year>
Reference-contexts: This has the unfortunate consequence of tying the argument to context-free grammars, losing the attractive formalism-independent quality evoked in [AJ91]. Since context-free grammars are no longer generally considered likely models for natural language in the general case <ref> [Shi85] </ref>, one wonders how the discussion here might be extended to parsing within more powerful grammatical frameworks. It is interesting to note the relationship between the style of left-corner parsing described here and one such framework, combinatory categorial grammar (CCG) [Ste90].
Reference: [Ste90] <author> Mark Steedman. </author> <title> Gapping as constituent coordination. </title> <journal> Linguistics and Philosophy, </journal> <volume> 13 </volume> <pages> 207-263, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: It is interesting to note the relationship between the style of left-corner parsing described here and one such framework, combinatory categorial grammar (CCG) <ref> [Ste90] </ref>. Composition is an integral part of CCG, as is the notion of type-raising, which resembles left-corner prediction. 7 The operation of a left-corner parser with composition can fairly be described as being in the style of CCG, but retaining the context-free base.
Reference: [TDL91] <author> H. Thompson, M. Dixon, and J. Lamping. </author> <title> Compose-reduce parsing. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the ACL, </booktitle> <pages> pages 87-97, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: B in B ! fifl) so that they may later be identified. Unlike top-down and bottom-up parsers, a left-corner parser meets this criterion. By presenting a complete version of the argument in [AJ91] and [JL83], we have essentially re-discovered proposals made by Pulman [Pul85, Pul86] and Thomp-son et al. <ref> [TDL91] </ref>. Both propose parsers with left-corner prediction and a composition operation added. Pulman motivates his parser's design on grounds of psychological plausibility, though he does not present a complete version of the argument discussed here. Thompson et al. are motivated by issues in parallel parsing.
Reference: [Woo70] <author> William A. Woods. </author> <title> Transition network grammars for natural language analysis. </title> <journal> Communications of the ACM, </journal> <volume> 13(10) </volume> <pages> 591-606, </pages> <month> October </month> <year> 1970. </year>
Reference-contexts: det1, 3 I am grateful to Stuart Shieber for this observation. and result are registers, the leftward arrow ( ) indicates an assignment statement, the pop arc transmits control (and the contents of the result register) to the calling subnetwork, and the asterisk (*) represents the value so transmitted (cf. <ref> [Woo70] </ref>). So, for instance, action I4 constructs an NP dominating the structure in the det1 register on the left, and, on the right, the noun structure received on return from a push to the N subnetwork.
References-found: 16

