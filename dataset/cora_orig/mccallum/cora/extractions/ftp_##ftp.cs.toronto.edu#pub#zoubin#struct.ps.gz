URL: ftp://ftp.cs.toronto.edu/pub/zoubin/struct.ps.gz
Refering-URL: http://www.cs.utoronto.ca/~zoubin/
Root-URL: 
Email: Email: zoubin@cs.toronto.edu  
Title: On Structured Variational Approximations  
Author: Zoubin Ghahramani 
Address: 6 King's College Road Toronto, Canada M5S 3H5  
Affiliation: Department of Computer Science University of Toronto  
Pubnum: Technical Report CRG-TR-97-1  
Abstract: The problem of approximating a probability distribution occurs frequently in many areas of applied mathematics, including statistics, communication theory, machine learning, and the theoretical analysis of complex systems such as neural networks. Saul and Jordan (1996) have recently proposed a powerful method for efficiently approximating probability distributions known as structured variational approximations. In structured variational approximations, exact algorithms for probability computation on tractable substructures are combined with variational methods to handle the interactions between the substructures which make the system as a whole intractable. In this note, I present a mathematical result which can simplify the derivation of struc tured variational approximations in the exponential family of distributions.
Abstract-found: 1
Intro-found: 1
Reference: <author> Cooper, G. F. </author> <year> (1990). </year> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <journal> Artificial Intelligence, 42(2-3):393-405. </journal>
Reference-contexts: These independences can be exploited to derive recursive algorithms for inferring the conditional probabilities of any set of variables given any other set of variables. However, for general belief networks with arbitrary connectivity and nonlinear interactions, the problem of exact inference is computationally intractable <ref> (Cooper, 1990) </ref>. Therefore, in practice this intractability must be circumvented by making use of approximate algorithms for inference.
Reference: <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: First, S 1 may be the variable of interest in an inference problem, for example, for medical diagnosis, which justifies marginalizing over the other hidden variables. Second, to estimate the parameters of the belief network using the Expectation-Maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref> it is necessary to compute conditional probabilities of subsets of the hidden variables given the observed variables.
Reference: <author> Neal, R. M. </author> <year> (1993). </year> <title> Probabilistic inference using Markov chain monte carlo methods. </title> <institution> University of Toronto, </institution> <note> Technical Report CRG-TR-93-1. </note>
Reference-contexts: First, it involves averages with respect to the tractable, Q, distribution. Second, minimizing this form of the KL-divergence corresponds to maximizing a lower bound on the log likelihood, a sensible criterion for a learning algorithm <ref> (Neal and Hinton, 1993) </ref>. The minimum of the KL-divergence is obtained by taking the partial derivatives of KL (QkP ) with respect to the elements of fl, which generally results in a set of fixed-point equations which can be solved iteratively.
Reference: <author> Neal, R. M. and Hinton, G. E. </author> <year> (1993). </year> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <note> Submitted. </note>
Reference-contexts: First, it involves averages with respect to the tractable, Q, distribution. Second, minimizing this form of the KL-divergence corresponds to maximizing a lower bound on the log likelihood, a sensible criterion for a learning algorithm <ref> (Neal and Hinton, 1993) </ref>. The minimum of the KL-divergence is obtained by taking the partial derivatives of KL (QkP ) with respect to the elements of fl, which generally results in a set of fixed-point equations which can be solved iteratively.
Reference: <author> Parisi, G. </author> <year> (1988). </year> <title> Statistical Field Theory. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA. </address>
Reference: <author> Saul, L. and Jordan, M. I. </author> <year> (1996). </year> <title> Exploiting tractable substructures in Intractable networks. </title> <editor> In Touretzky, D., Mozer, M., and Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: A structured variational approximation is simply an approximation in which the hidden variables are not completely factorized, but rather they are related in a structured manner <ref> (Saul and Jordan, 1996) </ref>. The belief network corresponding to this approximation would therefore contain some edges between the hidden variables.
Reference: <author> Smyth, P., Heckerman, D., and Jordan, M. I. </author> <year> (1997). </year> <title> Probabilistic independence networks for hidden Markov probability models. </title> <journal> Neural Computation, </journal> <volume> 9 </volume> <pages> 227-269. 6 </pages>
Reference-contexts: A belief network representing this approximating distribution is shown in Figure 1c, which can be recognized as the belief network corresponding to a hidden Markov model <ref> (Smyth et al., 1997) </ref>. However, it is a curious hidden Markov model in which a single observed variable has been replicated N times and placed at all of the visible (shaded) nodes.
References-found: 7

