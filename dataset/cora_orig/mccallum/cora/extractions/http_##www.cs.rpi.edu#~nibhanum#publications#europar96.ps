URL: http://www.cs.rpi.edu/~nibhanum/publications/europar96.ps
Refering-URL: http://www.cs.rpi.edu/~nibhanum/arsdir/vbsp_refs.html
Root-URL: http://www.cs.rpi.edu
Title: Adaptive Parallelism in the Bulk-Synchronous Parallel Model  
Author: Mohan V. Nibhanupudi and Boleslaw K. Szymanski 
Keyword: BSP Model, Networks of Workstations, Adaptive Parallel Computing  
Address: Troy, NY, USA 12180-3590  
Affiliation: Department of Computer Science Rensselaer Polytechnic Institute  
Abstract: The Bulk-Synchronous Parallel (BSP) model is a universal abstraction of parallel computation that can be used to design portable parallel software. Advances in processor architecture and network communication enable clusters of workstations to be used as parallel computers. This paper focuses on using the idle computing power of a network of workstations to run parallel programs. The transient nature of the processors causes straightforward execution of synchronous BSP programs to perform poorly in such an environment. In this paper, we propose a scheme, based on the eager replication of state data and lazy replication of processes, that allows BSP programs to run efficiently on transient processors. The scheme is integrated into the Oxford BSP library. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> C. K. Birdsall and A. B. Langdon. </author> <title> Plasma Physics via Computer Simulation. </title> <booktitle> The Adam Hilger Series on Plasma Physics. </booktitle> <address> Adam Hilger, New York, </address> <year> 1991. </year>
Reference-contexts: The plasma Particle In Cell simulation model <ref> [1] </ref> integrates in time the trajectories of millions of charged particles in their self-consistent electromagnetic fields. Particles can be located anywhere in the spatial domain; however, the field quantities are calculated on a fixed grid.
Reference: 2. <author> A. Bricker, M. Litzkow, and M. Livny. </author> <title> Condor Technical Summary. </title> <type> Technical Report CS-TR-92-1069, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> Jan </month> <year> 1992. </year>
Reference-contexts: Another complication is the unpredictability of machine load, i.e. each currently free workstation can suddenly become unavailable for a long time. There have been several systems that attempt to make use of idle time on workstations. Typically (see <ref> [2] </ref>), such systems avoid degrading the performance for the owner of the machine by requiring that the background computation be ? This work was partially supported by NSF Grants CCR-9216053 and CCR-9527151. The content does not necessarily reflect the position or policy of the U.S. <p> As mentioned in section 3, our prototype requires that a superstep making use of replication contain computation only. Further, we have currently implemented a simple scheme to choose a host machine to which the new process will migrate using the migration scheme of Condor <ref> [2] </ref> for this purpose. Apart from the use as a proof of concept, this implementation will help us in understanding the issues that arise when the replication scheme is available in a BSP library and in refining the set of needed primitives.
Reference: 3. <author> Pankaj Jalote. </author> <title> Fault Tolerance in Distributed Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey 07632, </address> <year> 1994. </year>
Reference-contexts: Our approach to adaptive parallelism is based on viewing the unavailability of a host machine as a transient failure; the effect of the transient failure is to delay the parallel computation. Our approach deals with transient failures through data replication <ref> [3] </ref> of computation state and time redundancy 2 [8] of computations. In distributed systems, data objects are replicated on multiple nodes to increase the availability of the data and thereby increase the resiliency to failures. Time redundancy is used to deal with the effects of temporary failures.
Reference: 4. <author> L. Kleinrock and W.Korfhage. </author> <title> Collecting Unused Processing Capacity: An Analysis of Transient Distributed Systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(5) </volume> <pages> 535-546, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Government. suspended when owner's activity is detected. The background computation is resumed when the owner's activity ends and the processor is idle. Since these processors are available for use only when they are idle and not available at other times, they are referred to as "transient" processors <ref> [4] </ref>. Consequently, we treat a change of the workstation status from free to unavailable as a transient failure. Another assumption of our approach is that the frequency of synchronization of parallel computation is high compared with the average available/nonavailable times of a workstation. <p> In each superstep, the processors perform some local computation and initiate communication to other processors. All the processors synchronize at the end of each superstep. Consequently, if a participating processor becomes unavailable while executing a superstep, the progress of the entire program will be stalled. In <ref> [4] </ref>, Kleinrock et. al. found the probability density of a program's finishing time on multiple processors in which duration of available and nonavailable periods are independent and identically distributed random variables from a general distribution assuming long duration programs (that execute for several available periods) composed of many independent tasks.
Reference: 5. <author> W F McColl. </author> <title> BSP Programming. </title> <editor> In G Blelloch, M Chandy, and S Jagannathan, editors, </editor> <booktitle> Proc. DIMACS Workshop on Specification of Parallel Algorithms, </booktitle> <address> Prince-ton, May 94. </address> <publisher> American Mathematical Society. </publisher>
Reference-contexts: Hence a superstep involving both computation and communication can always be split into a computation superstep and a communication superstep. We intend to relax this restriction in the future versions. The BSP approach provides a very general framework <ref> [5] </ref> and most applications can be represented as BSP computations. Our scheme for adaptive parallelism in the BSP approach is based on replicating the computation state and repeating the computation step in the event of failure of the component process.
Reference: 6. <author> Richard Miller. </author> <title> A Library for Bulk-synchronous Parallel Programming. </title> <booktitle> In British Computer Society Parallel Processing Specialist Group workshop on General Purpose Parallel Computing, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Another assumption of our approach is that the frequency of synchronization of parallel computation is high compared with the average available/nonavailable times of a workstation. In this paper, we propose extensions to the Oxford BSP Library <ref> [6, 7] </ref> that allow for transparent recovery from transient processor failures in a network of workstations using dynamic replication of computation state, lazy process replication and process migration. <p> is data replication, main cost of which is the communication needed to send the state of each subcomputation to peer processes. 2.2 Extensions to the Oxford BSP Library The above described scheme of using lazy replication and migration to support adaptive parallelism has been integrated into the Oxford BSP library <ref> [6, 7] </ref>. It implements a simplified version of the Bulk-Synchronous Parallel model [12] introduced by Leslie Valiant. It is simple, yet robust and was successfully used by us for implementing plasma simulation on a network of workstations. The extensions to the Oxford BSP library are introduced in two layers.
Reference: 7. <author> Richard Miller and Joy Reed. </author> <title> The Oxford BSP Library Users' Guide, version 1.0. </title> <type> Technical report, </type> <institution> Oxford Parallel, </institution> <year> 1993. </year>
Reference-contexts: Another assumption of our approach is that the frequency of synchronization of parallel computation is high compared with the average available/nonavailable times of a workstation. In this paper, we propose extensions to the Oxford BSP Library <ref> [6, 7] </ref> that allow for transparent recovery from transient processor failures in a network of workstations using dynamic replication of computation state, lazy process replication and process migration. <p> is data replication, main cost of which is the communication needed to send the state of each subcomputation to peer processes. 2.2 Extensions to the Oxford BSP Library The above described scheme of using lazy replication and migration to support adaptive parallelism has been integrated into the Oxford BSP library <ref> [6, 7] </ref>. It implements a simplified version of the Bulk-Synchronous Parallel model [12] introduced by Leslie Valiant. It is simple, yet robust and was successfully used by us for implementing plasma simulation on a network of workstations. The extensions to the Oxford BSP library are introduced in two layers.
Reference: 8. <author> Sape Mullender. </author> <title> Distributed Systems. </title> <publisher> ACM Press Frontier Series. ACM Press, </publisher> <address> New York, 2nd edition, </address> <year> 1993. </year>
Reference-contexts: Our approach to adaptive parallelism is based on viewing the unavailability of a host machine as a transient failure; the effect of the transient failure is to delay the parallel computation. Our approach deals with transient failures through data replication [3] of computation state and time redundancy 2 <ref> [8] </ref> of computations. In distributed systems, data objects are replicated on multiple nodes to increase the availability of the data and thereby increase the resiliency to failures. Time redundancy is used to deal with the effects of temporary failures.
Reference: 9. <author> M. V. Nibhanupudi, C. D. Norton, and B. K. Szymanski. </author> <title> Plasma Simulation On Networks Of Workstations Using The Bulk-Synchronous Parallel Model. </title> <booktitle> In Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA'95), </booktitle> <pages> pages 13-22, </pages> <address> Athens, Georgia, Novem-ber 3-4, </address> <year> 1995. </year>
Reference-contexts: In [10], Nibhanupudi and Szymanski analyze the execution time of a BSP program with relatively short supersteps on a network of transient processors for the special case of exponentially distributed available and nonavailable times. Many scientific applications such as the plasma simulation <ref> [9] </ref> that require frequent communication and synchronization among the component processes belong to this category. The results of the analysis in [10] are summarized below. <p> Since the failed computations are repeated by another process on a possibly different processor, we require that the computation superstep not include operations with side effects such as output, memory allocation, etc. 4 Plasma Simulation We have implemented the lazy replication scheme in the plasma simulation <ref> [9] </ref>. The plasma Particle In Cell simulation model [1] integrates in time the trajectories of millions of charged particles in their self-consistent electromagnetic fields. Particles can be located anywhere in the spatial domain; however, the field quantities are calculated on a fixed grid. <p> Particles can be located anywhere in the spatial domain; however, the field quantities are calculated on a fixed grid. In the General Concurrent Particle in Cell (GCPIC) Algorithm [11], both the particles and the field are partitioned among the processors. In the replicated grid version of the plasma simulation <ref> [9] </ref>, the simulation space is replicated on each of the processors. The particles are evenly distributed among processors in the primary decomposition, which makes advancing particle positions in space and computing their velocities efficient.
Reference: 10. <author> M. V. Nibhanupudi and B. K. Szymanski. </author> <title> Efficiency Of Parallel Computation Replication On A Network Of Transient Processors. </title> <note> Submitted to Eighth IEEE Symposium on Parallel and Distributed Processing to be held in October 1996. </note>
Reference-contexts: The impact of transient failures on tightly synchronized parallel programs with relatively small amount of computation between synchronizations is much more severe. In <ref> [10] </ref>, Nibhanupudi and Szymanski analyze the execution time of a BSP program with relatively short supersteps on a network of transient processors for the special case of exponentially distributed available and nonavailable times. <p> Many scientific applications such as the plasma simulation [9] that require frequent communication and synchronization among the component processes belong to this category. The results of the analysis in <ref> [10] </ref> are summarized below. Consider a BSP computation with the average inter-synchronization times measured on a single dedicated workstation equal to T o t a (and also T o t n ).
Reference: 11. <author> C. D. Norton, B. K. Szymanski, and V. K. Decyk. </author> <title> Object Oriented Parallel Computation for Plasma PIC Simulation. </title> <journal> Communications of the ACM, </journal> <volume> 38(10), </volume> <month> Octo-ber </month> <year> 1995. </year>
Reference-contexts: Particles can be located anywhere in the spatial domain; however, the field quantities are calculated on a fixed grid. In the General Concurrent Particle in Cell (GCPIC) Algorithm <ref> [11] </ref>, both the particles and the field are partitioned among the processors. In the replicated grid version of the plasma simulation [9], the simulation space is replicated on each of the processors.
Reference: 12. <author> Leslie G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: 1 Introduction The Bulk-Synchronous Parallel (BSP) model <ref> [12] </ref> is a universal abstraction of parallel computation which can be used to design portable parallel software. Advances in micro processor architecture and data communication allow distributed memory machines and networked workstations to deliver high performance for parallel applications. <p> It implements a simplified version of the Bulk-Synchronous Parallel model <ref> [12] </ref> introduced by Leslie Valiant. It is simple, yet robust and was successfully used by us for implementing plasma simulation on a network of workstations. The extensions to the Oxford BSP library are introduced in two layers. The first layer implements our lazy replication and migration scheme.
References-found: 12

