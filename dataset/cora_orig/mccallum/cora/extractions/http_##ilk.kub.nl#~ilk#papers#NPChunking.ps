URL: http://ilk.kub.nl/~ilk/papers/NPChunking.ps
Refering-URL: http://ilk.kub.nl/~zavrel/chunktest.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: veenstra@kub.nl  
Title: Fast NP Chunking Using Memory-Based Learning Techniques  
Author: Jorn Veenstra Sabine Buchholz 
Keyword: Submission Type: regular paper Topic Areas: robust parsing, NP chunking, memory-based learning  
Note: Author of Record: Jorn Veenstra Under consideration for other conferences (specify)? no  
Date: 20th April 1998  
Address: PO-box 90153, 5000 LE Tilburg the Netherlands  
Affiliation: Computational Linguistics at Tilburg University  
Abstract: In this paper we discuss the application of Memory-Based Learning (MBL) to fast NP chunking. We first discuss the application of a fast decision tree variant of MBL (IGTree) on the dataset described in (Ramshaw and Marcus, 1995), which consists of roughly 50,000 test and 200,000 train items. In a second series of experiments we used an architecture of two cascaded IGTrees. In the second level of this cascaded classifier we added context predictions as extra features so that incorrect predictions from the first level can be corrected, yielding a 97.2% generalisation accuracy with training and testing times in the order of seconds to minutes. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Steven Abney. </author> <year> 1991. </year> <title> Parsing by chunks. In Principle-Based Parsing. </title> <publisher> Kluwer Academic Publishers, Dordrecht. </publisher>
Reference: <author> D. W. Aha. </author> <year> 1991. </year> <title> Incremental constructive induction: an instance-based approach. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 117-121, </pages> <address> Evanston, ILL. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: = i=1 Where X and Y are the patterns to be compared; the distance between two patterns in the ith value is given by: ffi (x i ; y i ) = 0 if x i = y i 1 otherwise (2) This algorithm is often referred to as IB1 <ref> (Aha, 1991) </ref>.
Reference: <author> E. Brill. </author> <year> 1993. </year> <title> A Corpus-Based Approach to Language Learning. </title> <type> Dissertation, </type> <institution> Department of Computer and Information Science, University of Pennsylvania. </institution>
Reference-contexts: R&M used a transformation based tagger <ref> (Brill, 1993) </ref> to assign POS tags to their train and test data. We first used the same Brill tags for one experiment with IGTree. <p> We tested IB1-IG for the one-level classification, without the IOB prediction as extra features (IB1-IG1), and with the IOB predictions as extra features (IB1-IG2). The results are shown in Table 6. 4 Transformation based learning Ramshaw and Marcus (1995) used transformation-based learning <ref> (Brill, 1993) </ref> as a classifier on their R&M dataset. Transformation based learning (for a more detailed description see (Brill, 1993; Ramshaw and Marcus, 1995)) can be roughly described as follows: Start with a training corpus annotated with the correct classes and a large set of rule templates.
Reference: <author> Claire Cardie. </author> <year> 1996. </year> <title> Automatic feature set selection for case-based learning of linguistic knowledge. </title> <booktitle> In Proc. of Conference on Empirical Methods in NLP. </booktitle> <institution> University of Pennsylvania. </institution>
Reference: <author> S. Chandler. </author> <year> 1992. </year> <title> Are rules and modules really necessary for explaining language? Journal of Psycholinguistic research, </title> <booktitle> 22(6) </booktitle> <pages> 593-606. </pages>
Reference: <author> M.J. Collins. </author> <year> 1996. </year> <title> A new statistical parser based on bigram lexical dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics. </booktitle>
Reference: <author> S. Cost and S. Salzberg. </author> <year> 1993. </year> <title> A weighted nearest neighbour algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78. </pages>
Reference: <author> Walter Daelemans and Antal van den Bosch. </author> <year> 1992. </year> <title> Generalisation performance of backpropagation learning on a syllabification task. </title>
Reference: <editor> In M. F. J. Drossaers and A. Nijholt, editors, </editor> <booktitle> Proc. of TWLT3: Connectionism and Natural Language Processing, </booktitle> <pages> pages 27-37, </pages> <institution> Enschede. Twente University. </institution>
Reference: <author> W. Daelemans and A. Van den Bosch. </author> <year> 1996. </year> <title> Language-independent data-oriented grapheme-to-phoneme conversion. </title> <editor> In J. P. </editor> <publisher> H. </publisher>
Reference-contexts: We first used the same Brill tags for one experiment with IGTree. Since one of our aims is to construct an NP chunker which can take untagged sentences as input and "NP chunked" sentences as output we also used the Memory-Based Tagger (MBT) <ref> (Daelemans et al., 1996) </ref> to tag the datasets. Results The results in Table 2 show the accuracy in predicting the IOB tags, using the one-level IGTree1 classifier, with tags assigned by a transformation based (Brill) tag-ger and by a Memory-Based tagger (MBT).
Reference: <author> Van Santen, R. W. Sproat, J. P. Olive, and J. Hirschberg, </author> <title> editors, </title> <booktitle> Progress in Speech Processing, </booktitle> <pages> pages 77-89. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. </author> <year> 1996. </year> <title> MBT: A memory-based part of speech tagger generator. </title> <editor> In E. Ejerhed and I.Dagan, editors, </editor> <booktitle> Proc. of Fourth Workshop on Very Large Corpora, </booktitle> <pages> pages 14-27. </pages> <publisher> ACL SIGDAT. </publisher>
Reference: <author> W. Daelemans, A. Van den Bosch, and A. Wei-jters. </author> <year> 1997. </year> <title> igtree: using trees for compression and classification in lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 407-423. </pages>
Reference-contexts: Again, this compression does not affect the generalisation performance of IGTree. For a more detailed discussion on IGTrees, see <ref> (Daelemans et al., 1997) </ref>. 3 Experiments We carried out two series of experiments. In the first we used IGTree for the NP chunking task.
Reference: <author> W. Daelemans, A. Van den Bosch, J. Zavrel, J. Veenstra, S. Buchholz, and G. J. Busser. </author> <year> 1998a. </year> <title> Rapid development of NLP modules with Memory-Based Learning. </title> <booktitle> In Proceedings of ELSNET in Wonderland, </booktitle> <month> March, </month> <year> 1998, </year> <pages> pages 105-113. ELSNET. </pages>
Reference-contexts: 1 Introduction Language technology applications often require fast and accurate modules that can be developed and adapted rapidly <ref> (Daelemans et al., 1998a) </ref>. In this paper we concentrate on a fast implementation of an NP chunking module. In chunking, a sentence is divided into nonoverlapping segments based on a relatively crude analysis; for an early paper on chunking, see (Abney, 1991).
Reference: <author> W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. </author> <year> 1998b. </year> <title> TiMBL: Tilburg Memory Based Learner, version 1.0, reference manual. </title> <type> Technical Report ILK-9803, </type> <institution> ILK, Tilburg University. </institution>
Reference-contexts: In the TiMBL <ref> (Daelemans et al., 1998b) </ref> implementation of MBL, IB1 is extended in the following way: When a case is associated with more than one class in the training set (i.e. the case is ambiguous), the distribution of cases over the different classes is kept and the most frequently occurring class is selected
Reference: <author> B. L. Derwing and R. Skousen. </author> <year> 1989. </year> <title> Real time morphology: Symbolic rules or analogical networks. </title> <journal> Berkeley Linguistic Society, </journal> <volume> 15 </volume> <pages> 48-62. </pages> <editor> 7 S. Federici and V. Pirelli. </editor> <year> 1996. </year> <title> Analogy, compu-tation and linguistic theory. In New Methods in Language Processing. </title> <publisher> UCL Press, London. </publisher>
Reference: <author> E. B. Hunt, J. Marin, and P. J. Stone. </author> <year> 1966. </year> <title> Experiments in induction. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> J. Kolodner. </author> <year> 1993. </year> <title> Case-based reasoning. </title> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> P. Langley and S. Sage. </author> <year> 1994. </year> <title> Oblivious decision trees and abstract cases. </title> <editor> In D. W. Aha, editor, </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01), </booktitle> <address> Menlo Park, CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> M. Marcus, B. Santorini, and M.A. Marcinkiewicz. </author> <year> 1993. </year> <title> Building a large annotated corpus of english: The penn treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330. </pages>
Reference: <author> Hwee Tou Ng and Hian Beng Lee. </author> <year> 1996. </year> <title> In-tergrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. </title> <booktitle> In Proc. of 34th meeting of the Assio-ciation for Computational Linguistics. </booktitle>
Reference: <author> J.R. Quinlan. </author> <year> 1993. </year> <title> c4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> L.A. Ramshaw and M.P. Marcus. </author> <year> 1995. </year> <title> Text chunking using transformation-based learning. </title> <booktitle> In Proc. of third workshop on very large corpora, </booktitle> <pages> pages 82-94, </pages> <month> June. </month>
Reference-contexts: In parsing, NP chunking can be seen as one step in a cascade of classification tasks. One way to see NP chunking as a classification task is to describe it as a tagging problem, as first defined by <ref> (Ramshaw and Marcus, 1995) </ref>. <p> In Section 3.3 we discuss the application of the usually more accurate but slower classifier IB1-IG to NP Chunking. In Section 4 we discuss the work 1 TiMBL is an MBL software package, available for research purposes via the URL: http://ilk.kub.nl/ 1 by <ref> (Ramshaw and Marcus, 1995) </ref>, who ap-plied transformation based learning to NP chunking. 2 Memory-Based Learning Memory-Based Learning (MBL) is a form of supervised, inductive learning from examples. Examples or cases are represented as vectors of feature values with an associated class label. <p> Brill's tagger made use of 26 such templates, the NP chunker used 100 templates consisting of combinations of words, POS tags and IOB tags of the focus and its surrounding words. This large number of templates makes the transformation based NP chunker difficult to train. As <ref> (Ramshaw and Marcus, 1995, p.89) </ref> put it: "The large increase in the number of rule templates [. . . ] pushed the training process against the available limits in terms of both time and space". It took days rather than hours to train the transformation based system.
Reference: <author> A. Ratnaparkhi. </author> <year> 1997. </year> <title> A linear observed time statistical parser based on maximum entropy models. </title> <type> Technical Report cmp-lg/9706014, </type> <note> Computation and Language, http://xxx.lanl.gov/list/cmp-lg/, June. </note>
Reference: <author> R. Scha. </author> <year> 1992. </year> <title> Virtual Grammars and Creative Algorithms. </title> <journal> Gramma/TTT Tijdschrift voor Taalkunde, </journal> <volume> 1 </volume> <pages> 57-77. </pages>
Reference: <author> R. Skousen. </author> <year> 1989. </year> <title> Analogical modeling of language. </title> <publisher> Kluwer Academic Publishers, Dor-drecht. </publisher>
Reference: <author> C. Stanfill and D. Waltz. </author> <year> 1986. </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the acm, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <month> December. </month>
Reference: <author> A. Van den Bosch. </author> <year> 1997. </year> <title> Learning to pronounce written words: A study in inductive language learning. </title> <type> Ph.D. thesis, </type> <institution> Universiteit Maastricht. forthcoming. </institution> <month> 8 </month>
Reference-contexts: The advantage of using the predicted classes instead of the original target classes is that this way we can add information about the classifier to the training set; typical errors for the classifier can be taken into account in the second level of processing, see <ref> (Van den Bosch, 1997) </ref> for a discussion of this issue in the context of MBL. The first level classifier is used to add the IOB features to the test set.
References-found: 28

