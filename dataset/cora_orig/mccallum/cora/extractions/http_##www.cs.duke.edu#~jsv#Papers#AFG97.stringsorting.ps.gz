URL: http://www.cs.duke.edu/~jsv/Papers/AFG97.stringsorting.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node20.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: On Sorting Strings in External Memory (extended Abstract)  
Author: Lars Arge Paolo Ferragina Roberto Grossi Jeffrey Scott Vitter 
Keyword: B log M=B  
Address: Pisa, Pisa, Italy.  Italy.  Durham, NC  
Affiliation: Dipartimento di Informatica, Universita di  of Italy. Dipartimento di Sistemi e Informatica, Universita di Firenze, Firenze,  University of Aarhus, Denmark. Department of Computer Science, Duke University,  
Note: N  Supported in part by MURST  Part of this work was done while visiting BRICS,  Supported in part by the U.S. Army Research Office under grants DAAH0493G0076 and DAAH049610013 and by the National Science Foundation under grant CCR9522047.  
Email: Email: ferragin@di.unipi.it.  Email: grossi@dsi2.dsi.unifi.it.  Email: jsv@cs.duke.edu.  
Phone: 277080129, USA.  
Date: 1  
Abstract: In this paper we address for the first time the I/O complexity of the problem of sorting strings in external memory, which is a fundamental component of many large-scale text applications. In the standard unit-cost RAM comparison model, the complexity of sorting K strings of total length N is fi(K log 2 K +N ). By analogy, in the external memory (or I/O) model, where the internal memory has size M and the block transfer size is B, it would be natural to guess that the I/O complexity of sorting strings is fi( K B + N but the known algorithms do not come even close to achieving this bound. Our results show, somewhat counterintu-itively, that the I/O complexity of string sorting depends upon the length of the strings relative to the block size. We first consider a simple comparison I/O model, where one is not allowed to break the strings into their characters, and we show that the I/O complexity of string sorting in this model is fi( N 1 B ), where N 1 is the total length of all strings shorter than B and K 2 is the number of strings longer than B. We then consider two more general I/O comparison models in which string breaking is allowed. We obtain improved algorithms and in several cases lower bounds that match their I/O bounds. Finally, we develop more practical algorithms without assuming the comparison model. fl Department of Computer Science, Duke University, Durham, NC 277080129, USA. Email: large@cs.duke.edu. Supported in part by the U.S. Army Research Office under grant DAAH049610013 and by the ESPRIT Long Term Research Programme under project 20244 (ALCOMIT). Part of this work was done while at BRICS, Dept. of Computer Science, University of Aarhus, Denmark, and while visiting Universita di Firenze. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Adler. </author> <title> New coding techniques for improved bandwidth utilization. </title> <booktitle> In Proc. IEEE Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 173182, </pages> <year> 1996. </year>
Reference-contexts: A natural question to ask is whether compression and coding techniques can be used to get better algorithms when, for example, bit operations are allowed on the characters, akin to the recent result on a type of matrix transposition <ref> [1] </ref>. The practical algorithms in Section 4 already exploit some of these techniques. Another natural question to ask is how to sort strings optimally using D disks. As discussed, the practical algorithms in Section 4 can achieve optimal (linear) speedup in the D-disk model in many real situations.
Reference: [2] <author> A. Aggarwal and J. S. Vitter. </author> <title> The Input/Output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9):11161127, </volume> <year> 1988. </year>
Reference-contexts: To amortize (or hide) disk latency, each input/output operation (or I/O) transfers a large block of contiguous data. We use the standard model of I/O complex ity <ref> [2, 46] </ref> and define the following parameters: K = # of strings to sort; N = total # of characters in the K strings; M = # of characters fitting in internal memory; B = # of characters per disk block (or track); where M &lt; N and 1 B M=2. <p> sequence can be obtained with O (K 2 + N 2 =B) = O (N 2 =B) extra I/Os by moving the strings into their final position one at a time. 1.2 Previous Results in I/O-efficient Compu tation Early work on I/O algorithms concentrated on sorting and permutation related problems <ref> [2, 9, 18, 41, 40, 46] </ref>. Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46]. <p> Work has also been done on matrix algebra and related problems arising in scientific computation <ref> [2, 45, 46] </ref>. More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation [6, 7, 16] and string matching [11, 17, 20, 22, 23]. <p> More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation [6, 7, 16] and string matching [11, 17, 20, 22, 23]. Aggarwal and Vitter <ref> [2] </ref> proved that the number of I/Os needed to sort N indivisible elements is ( N B log M=B B ) in the comparison I/O model (where the order between two elements can be inferred only by their comparison or by transitivity). 1 They also proved that rearranging a set of <p> The result shows that sorting short strings is as difficult as sorting their individual characters, while sorting long strings is as difficult as sorting their first B characters. The lower bound for small strings in Theorem 1 is proved by extending the technique in <ref> [2] </ref> and considering the special case where all K 1 small strings have the same length N 1 =K 1 . The lower bound for the long strings then follows by considering the K 2 small strings obtained by looking at their first B characters. <p> This shows that breaking up long strings in internal memory is provably helpful in external string sorting. We again prove the lower bound by a generalization of the technique used in <ref> [2] </ref>. We obtain the upper bound by means of a novel combination of the SB-tree data structure [20] and the buffer tree technique [6]. This allows us to get a fi (M ) SB-tree node fanout rather than a fi (B) fanout. <p> In Section 2.1 we consider the upper bound and in Section 2.2 the lower bound. 2.1 Upper Bounds We use variants of external merge sort to sort both short and long strings. For the small strings the algorithm is almost identical to the standard single-character external merge sort algorithm <ref> [2] </ref>: We first produce fi (N 1 =M ) sorted runs using O (N 1 =B) I/O by repeatedly loading as many strings as can fit in internal memory, sorting them, and writing them back to disk. <p> Given the input parameters K 1 and N 1 , we consider a special instance in which all short strings have the same length N 1 =K 1 (hence, we have X = BK 1 =N 1 strings per block), and apply a modified version of the argument used in <ref> [2] </ref>. Initially there are K 1 ! possibilities for the correct ordering of the K 1 equal-length short strings; every input operation and the string comparisons done after it decrease this number. <p> First note that from a permutation point of view, Models A and B are equivalent because strings are moved in and out of internal memory in their entirety. Thus a permutation bound in Model A also holds in Model B. The permutation lower bound proved in <ref> [2] </ref> shows that there exists a permutation of N 0 indivisible elements requiring (minfN 0 ; N 0 B 0 log M 0 =B 0 N 0 B 0 g) I/Os on a machine with block size B 0 and memory size M 0 , respectively.
Reference: [3] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: 1 Introduction The problem of lexicographically sorting a set of strings in the internal memory of computers has received much attention in the past (see e.g. <ref> [3, 34] </ref>). This problem is the most general formulation of sorting because it comprises integer sorting (i.e., strings of length one), multikey sorting (i.e., equal-length strings) and variable-length key sorting (i.e., arbitrarily long strings).
Reference: [4] <author> A. Andersson, T. Hagerup, S. Nilsson, and R. Raman. </author> <title> Sorting in linear time? In Proc. </title> <booktitle> ACM Symp. on Theory of Computation, </booktitle> <pages> pages 427436, </pages> <year> 1995. </year>
Reference-contexts: Very recently several authors have considered the importance of this problem from different points of view: multikey sorting (e.g. [11, 14, 39]), integer sorting and its variants (e.g. <ref> [4, 5, 25, 43] </ref>), and parallel string sorting (e.g. [29, 30, 32]), just to cite a few. Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers.
Reference: [5] <author> A. Andersson and S. Nilsson. </author> <title> A new efficient radix sort. </title> <booktitle> In Proc. IEEE Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 714 721, </pages> <year> 1994. </year>
Reference-contexts: Very recently several authors have considered the importance of this problem from different points of view: multikey sorting (e.g. [11, 14, 39]), integer sorting and its variants (e.g. <ref> [4, 5, 25, 43] </ref>), and parallel string sorting (e.g. [29, 30, 32]), just to cite a few. Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers.
Reference: [6] <author> L. Arge. </author> <title> The buffer tree: A new technique for optimal I/O-algorithms. </title> <booktitle> In Proc. Workshop on Algorithms and Data Structures, </booktitle> <volume> LNCS 955, </volume> <pages> pages 334345, </pages> <year> 1995. </year> <note> A complete version appears as BRICS technical report RS-96-28, </note> <institution> University of Aarhus. </institution>
Reference-contexts: Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46]. More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry <ref> [6, 10, 28] </ref>, graph theoretic computation [6, 7, 16] and string matching [11, 17, 20, 22, 23]. <p> Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46]. More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation <ref> [6, 7, 16] </ref> and string matching [11, 17, 20, 22, 23]. <p> If we use the standard external memory search tree, the B-tree [12, 19], we get an O (N log B N )-I/O insertion sort. This algorithm's I/O efficiency is a multiplicative factor of B log B (M=B) larger than optimal. In <ref> [6, 8] </ref> a so-called buffer technique for lazy updates in external data structures is developed, and using this technique the buffer tree is designed. Using this structure in the external insertion sort yields an optimal sorting algorithm. <p> We again prove the lower bound by a generalization of the technique used in [2]. We obtain the upper bound by means of a novel combination of the SB-tree data structure [20] and the buffer tree technique <ref> [6] </ref>. This allows us to get a fi (M ) SB-tree node fanout rather than a fi (B) fanout. Our algorithm is based upon a type of insertion sort with a new batched insertion procedure for SB-trees. <p> Our solution for long strings will be based upon the SB-tree and the buffer tree data structures which we briefly review below (we refer the reader to <ref> [6, 8, 20] </ref> for more details). We then describe our improved algorithm which is a combination of these two data structures. The Buffer Tree. <p> In <ref> [6] </ref> it is thus shown that the total number of I/Os used to insert R elements in a buffer tree, including I/Os used for rebalancing, is O ( R B log M=B B ). The SB-Tree. <p> The only I/Os we have not accounted for so far are the ones used to rebalance the buffered SB-tree structure. Following the argument in <ref> [6] </ref>, it can be argued that inserting K 2 strings results in fi (K 2 =M ) splits in total. Since one split can be performed in O (M ) I/Os, the rebalancing cost adds up to O (K 2 ).
Reference: [7] <author> L. Arge. </author> <title> The I/O-complexity of ordered binary-decision diagram manipulation. </title> <booktitle> In Proc. Int. Symp. on Algorithms and Computation, </booktitle> <volume> LNCS 1004, </volume> <pages> pages 8291, </pages> <year> 1995. </year> <note> A complete version appears as BRICS technical report RS-96-29, </note> <institution> University of Aarhus. </institution>
Reference-contexts: Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46]. More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation <ref> [6, 7, 16] </ref> and string matching [11, 17, 20, 22, 23].
Reference: [8] <author> L. Arge. </author> <title> Efficient External-Memory Data Structures and Applications. </title> <type> PhD thesis, </type> <institution> University of Aarhus, </institution> <month> Febru-ary/August </month> <year> 1996. </year>
Reference-contexts: If we use the standard external memory search tree, the B-tree [12, 19], we get an O (N log B N )-I/O insertion sort. This algorithm's I/O efficiency is a multiplicative factor of B log B (M=B) larger than optimal. In <ref> [6, 8] </ref> a so-called buffer technique for lazy updates in external data structures is developed, and using this technique the buffer tree is designed. Using this structure in the external insertion sort yields an optimal sorting algorithm. <p> Our solution for long strings will be based upon the SB-tree and the buffer tree data structures which we briefly review below (we refer the reader to <ref> [6, 8, 20] </ref> for more details). We then describe our improved algorithm which is a combination of these two data structures. The Buffer Tree.
Reference: [9] <author> L. Arge, M. Knudsen, and K. Larsen. </author> <title> A general lower bound on the I/O-complexity of comparison-based algorithms. </title> <booktitle> In Proc. Workshop on Algorithms and Data Structures, </booktitle> <volume> LNCS 709, </volume> <pages> pages 8394, </pages> <year> 1993. </year>
Reference-contexts: sequence can be obtained with O (K 2 + N 2 =B) = O (N 2 =B) extra I/Os by moving the strings into their final position one at a time. 1.2 Previous Results in I/O-efficient Compu tation Early work on I/O algorithms concentrated on sorting and permutation related problems <ref> [2, 9, 18, 41, 40, 46] </ref>. Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46].
Reference: [10] <author> L. Arge, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory algorithms for processing line segments in geographic information systems. </title> <booktitle> In Proc. Annual European Symposium on Algorithms, </booktitle> <volume> LNCS 979, </volume> <pages> pages 295310, </pages> <year> 1995. </year> <note> A complete version (to appear in special issue of Algorithmica) appears as BRICS technical report RS-96-12, </note> <institution> University of Aarhus. </institution>
Reference-contexts: Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46]. More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry <ref> [6, 10, 28] </ref>, graph theoretic computation [6, 7, 16] and string matching [11, 17, 20, 22, 23].
Reference: [11] <author> R. A. Baeza-Yates and G. H. Gonnet. </author> <title> Handbook of Algorithms and Data Structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Very recently several authors have considered the importance of this problem from different points of view: multikey sorting (e.g. <ref> [11, 14, 39] </ref>), integer sorting and its variants (e.g. [4, 5, 25, 43]), and parallel string sorting (e.g. [29, 30, 32]), just to cite a few. Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers. <p> More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation [6, 7, 16] and string matching <ref> [11, 17, 20, 22, 23] </ref>.
Reference: [12] <author> R. Bayer and E. McCreight. </author> <title> Organization and maintenance of large ordered indexes. </title> <journal> Acta Informatica, </journal> <volume> 1:173189, </volume> <year> 1972. </year>
Reference-contexts: Aggarwal and Vitter also developed optimal sorting algorithms based upon merge and distribution sort. In internal memory a balanced search tree can be used in the context of insertion sort to sort optimally. If we use the standard external memory search tree, the B-tree <ref> [12, 19] </ref>, we get an O (N log B N )-I/O insertion sort. This algorithm's I/O efficiency is a multiplicative factor of B log B (M=B) larger than optimal. <p> The SB-Tree. From a high level point of view an SB-tree storing a set of R strings is a B-tree <ref> [12, 19] </ref> built on the set of pointers to the strings: The pointers are stored in the leaves of the tree (fi (B) pointers per leaf) and are ordered according to the lexicographical order of the strings they point to.
Reference: [13] <author> R. Bayer and K. Unterauer. </author> <title> Prefix B-trees. </title> <journal> ACM Trans. Database Syst., </journal> <volume> 2(1):1126, </volume> <year> 1977. </year>
Reference-contexts: Using this structure in the external insertion sort yields an optimal sorting algorithm. As far as the general string sorting problem is concerned, there are a number of data structures like prefix B-trees <ref> [13] </ref>, SB-trees [20], compacted tries [38], suffix trees [17, 36], and suffix arrays [35] that can be used to sort arbitrarily long strings in external memory.
Reference: [14] <author> J. L. Bentley and R. Sedgewick. </author> <title> Fast algorithms for sorting and searching strings. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 360369, </pages> <year> 1996. </year>
Reference-contexts: Very recently several authors have considered the importance of this problem from different points of view: multikey sorting (e.g. <ref> [11, 14, 39] </ref>), integer sorting and its variants (e.g. [4, 5, 25, 43]), and parallel string sorting (e.g. [29, 30, 32]), just to cite a few. Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers. <p> This sorting algorithm can be converted into an optimal fi (K log 2 K + N )-time algorithm for the internal comparison model by fixing B to a constant [21]. Very recently, Bentley and Sedgwick <ref> [14] </ref> emphasized the practical importance of string sorting and presented a version of quicksort that also achieves the optimal fi (K log 2 K + N ) comparison bound. <p> Model B: We relax the indivisibility assumption of Model A by allowing strings to be divided into single characters in internal memory only. Model C: We waive the indivisibility assumption completely and allow division of strings in both internal and external memory. The algorithm by Bentley and Sedgwick <ref> [14] </ref> can easily be modified to work in Model A; the SB-tree algorithm [20] requires the power of Model C. <p> Since we can keep C in internal memory (by means of a ternary search tree <ref> [14] </ref>), we can sort C's strings by one single scan of the whole input. This clearly gives the final rank of C's strings.
Reference: [15] <author> Peter M. Chen, Edward K. Lee, Garth A. Gibson, Randy H. Katz, and David A. Patterson. </author> <title> RAID: high-performance, reliable secondary storage. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(2):145 185, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: An increasingly popular approach to increase the throughput of I/O systems is to use a number D of disks in parallel, such that one can read (or write) D blocks in one I/O provided that they come from D distinct disks <ref> [15, 46] </ref>. An important property of our three practical algorithms is that they are all able to take full advantage of multiple disks; that is, they obtain linear speedup with respect to the number of available disks.
Reference: [16] <author> Y.-J. Chiang, M. T. Goodrich, E. F. Grove, R. Tamassia, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory graph algorithms. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 139149, </pages> <year> 1995. </year>
Reference-contexts: Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46]. More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation <ref> [6, 7, 16] </ref> and string matching [11, 17, 20, 22, 23].
Reference: [17] <author> D. R. Clark and J. I. Munro. </author> <title> Efficient suffix trees on secondary storage. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 383391, </pages> <year> 1996. </year>
Reference-contexts: More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation [6, 7, 16] and string matching <ref> [11, 17, 20, 22, 23] </ref>. <p> Using this structure in the external insertion sort yields an optimal sorting algorithm. As far as the general string sorting problem is concerned, there are a number of data structures like prefix B-trees [13], SB-trees [20], compacted tries [38], suffix trees <ref> [17, 36] </ref>, and suffix arrays [35] that can be used to sort arbitrarily long strings in external memory.
Reference: [18] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wis-niewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Dept. of Computer Science, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: sequence can be obtained with O (K 2 + N 2 =B) = O (N 2 =B) extra I/Os by moving the strings into their final position one at a time. 1.2 Previous Results in I/O-efficient Compu tation Early work on I/O algorithms concentrated on sorting and permutation related problems <ref> [2, 9, 18, 41, 40, 46] </ref>. Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46].
Reference: [19] <author> D. Cormer. </author> <title> The ubiquitous B-tree. </title> <journal> ACM Computing Surveys, </journal> <volume> 11(2):121137, </volume> <year> 1979. </year>
Reference-contexts: Aggarwal and Vitter also developed optimal sorting algorithms based upon merge and distribution sort. In internal memory a balanced search tree can be used in the context of insertion sort to sort optimally. If we use the standard external memory search tree, the B-tree <ref> [12, 19] </ref>, we get an O (N log B N )-I/O insertion sort. This algorithm's I/O efficiency is a multiplicative factor of B log B (M=B) larger than optimal. <p> The SB-Tree. From a high level point of view an SB-tree storing a set of R strings is a B-tree <ref> [12, 19] </ref> built on the set of pointers to the strings: The pointers are stored in the leaves of the tree (fi (B) pointers per leaf) and are ordered according to the lexicographical order of the strings they point to.
Reference: [20] <author> P. Ferragina and R. Grossi. </author> <title> A fully-dynamic data structure for external substring search. </title> <booktitle> In Proc. ACM Symp. on Theory of Computation, </booktitle> <pages> pages 693702, </pages> <year> 1995. </year>
Reference-contexts: More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation [6, 7, 16] and string matching <ref> [11, 17, 20, 22, 23] </ref>. <p> Using this structure in the external insertion sort yields an optimal sorting algorithm. As far as the general string sorting problem is concerned, there are a number of data structures like prefix B-trees [13], SB-trees <ref> [20] </ref>, compacted tries [38], suffix trees [17, 36], and suffix arrays [35] that can be used to sort arbitrarily long strings in external memory. <p> Model C: We waive the indivisibility assumption completely and allow division of strings in both internal and external memory. The algorithm by Bentley and Sedgwick [14] can easily be modified to work in Model A; the SB-tree algorithm <ref> [20] </ref> requires the power of Model C. One of our results is counterintuitive in the sense that we prove that the conjectured fi ( K B log M=B B + N B ) I/O bound is too low. <p> This shows that breaking up long strings in internal memory is provably helpful in external string sorting. We again prove the lower bound by a generalization of the technique used in [2]. We obtain the upper bound by means of a novel combination of the SB-tree data structure <ref> [20] </ref> and the buffer tree technique [6]. This allows us to get a fi (M ) SB-tree node fanout rather than a fi (B) fanout. Our algorithm is based upon a type of insertion sort with a new batched insertion procedure for SB-trees. <p> For the narrow range B=log M=B B &lt; N 1 =K 1 &lt; B, in which there are very few short strings per block, our bounds are only sometimes tight; the full characterization will appear in the full paper. Model C. Although the original SB-tree data structure <ref> [20] </ref> fits in Model C (it keeps a variant of tries in some blocks of the external memory and thus it divides strings into charac-ters), it does not improve the bounds that we can immediately derive from Model B (Theorem 2 and 3). <p> Our solution for long strings will be based upon the SB-tree and the buffer tree data structures which we briefly review below (we refer the reader to <ref> [6, 8, 20] </ref> for more details). We then describe our improved algorithm which is a combination of these two data structures. The Buffer Tree. <p> Consequently, BT fits in one block. This is one of the important properties used in <ref> [20] </ref>, where it is shown how the blind tries can be used to efficiently guide the search for the lexicographical position of a string P among the strings (i.e., pointers) stored in the leaves. <p> It is worth noting that the insertion procedure developed in <ref> [20] </ref> needs to be modified to work in the setting described above. We describe here the new batched insertion procedure, leaving a lot of details to the full paper: When a buffer B contains fi (M ) elements, we perform a buffer emptying process on node as follows. <p> In the latter case, f is chosen to be one of the leaves descend ing from the last traversed node. Note that f stores one of the strings in S that share the longest common prefix with P <ref> [20] </ref>. The downward traversal of BT cannot be executed without I/Os because P is stored in external memory. <p> In <ref> [20] </ref>, it has been shown that one of the strings, say Y , adjacent to P 's position in S shares lcp (P; W (f )) characters with P (i.e., lcp (P; W (f )) = lcp (P; Y )).
Reference: [21] <author> P. Ferragina and R. Grossi. </author> <title> An external-memory indexing data structure with applications. Full version of STOC'95 paper A fully-Dynamic data structure for external substring search, </title> <year> 1996. </year>
Reference-contexts: This sorting algorithm can be converted into an optimal fi (K log 2 K + N )-time algorithm for the internal comparison model by fixing B to a constant <ref> [21] </ref>. Very recently, Bentley and Sedgwick [14] emphasized the practical importance of string sorting and presented a version of quicksort that also achieves the optimal fi (K log 2 K + N ) comparison bound.
Reference: [22] <author> P. Ferragina and R. Grossi. </author> <title> Fast string searching in secondary storage: Theoretical developments and experimental results. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 373382, </pages> <year> 1996. </year>
Reference-contexts: More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation [6, 7, 16] and string matching <ref> [11, 17, 20, 22, 23] </ref>.
Reference: [23] <author> P. Ferragina and F. Luccio. </author> <title> On the parallel dictionary matching problem: New results with applications. </title> <booktitle> In Proc. Annual European Symposium on Algorithms, </booktitle> <volume> LNCS 1136, </volume> <pages> pages 261275, </pages> <year> 1996. </year>
Reference-contexts: More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation [6, 7, 16] and string matching <ref> [11, 17, 20, 22, 23] </ref>.
Reference: [24] <author> E. A. Fox, R. M. Akscyn, R. K. Furuta, and J. J. Leggett. </author> <title> Special issue: Digital libraries: Introduction. </title> <journal> Communications of the ACM, </journal> <volume> 38(4), </volume> <month> April </month> <year> 1995. </year>
Reference-contexts: Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers. Examples include textual databases [26], digital libraries <ref> [24] </ref>, and relational databases [37]. In these cases, the data need to be stored on external storage devices like disks or CD ROMs, which are roughly one million times slower than internal caches in terms of access time (or latency).
Reference: [25] <author> M.L. Fredman and D.E. Willard. </author> <title> Surpassing the information theoretic bound with fusion trees. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 47:424436, </volume> <year> 1993. </year>
Reference-contexts: Very recently several authors have considered the importance of this problem from different points of view: multikey sorting (e.g. [11, 14, 39]), integer sorting and its variants (e.g. <ref> [4, 5, 25, 43] </ref>), and parallel string sorting (e.g. [29, 30, 32]), just to cite a few. Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers.
Reference: [26] <author> K. A. Frenkel. </author> <title> The human genome project and informatics. </title> <journal> Communications of the ACM, </journal> <volume> 34:4151, </volume> <year> 1991. </year>
Reference-contexts: Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers. Examples include textual databases <ref> [26] </ref>, digital libraries [24], and relational databases [37]. In these cases, the data need to be stored on external storage devices like disks or CD ROMs, which are roughly one million times slower than internal caches in terms of access time (or latency).
Reference: [27] <author> G. A. Gibson, J. S. Vitter, and J. Wilkes. </author> <title> Report of the working group on storage I/O issues in large-scale computing. </title> <journal> ACM Computing Surveys, </journal> <volume> 28(4), </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: This disparity in latency has given rise in many applications to an input/output (or I/O) bottleneck, in which the time spent on moving data between internal and external memory dominates the overall execution time <ref> [27] </ref>. I/O bottlenecks are increasing in significance since the relative speed of disks versus internal memories is decreasing, especially with more and more use of parallel computers [42].
Reference: [28] <author> M. T. Goodrich, J.-J. Tsay, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory computational geometry. </title> <booktitle> In Proc. IEEE Symp. on Foundations of Comp. Sci., </booktitle> <pages> pages 714723, </pages> <year> 1993. </year>
Reference-contexts: Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46]. More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry <ref> [6, 10, 28] </ref>, graph theoretic computation [6, 7, 16] and string matching [11, 17, 20, 22, 23].
Reference: [29] <author> T. Hagerup. </author> <title> Optimal parallel string algorithms: Merging, sorting and computing the minimum. </title> <booktitle> In Proc. ACM Symp. on Theory of Computation, </booktitle> <pages> pages 382391, </pages> <year> 1994. </year>
Reference-contexts: Very recently several authors have considered the importance of this problem from different points of view: multikey sorting (e.g. [11, 14, 39]), integer sorting and its variants (e.g. [4, 5, 25, 43]), and parallel string sorting (e.g. <ref> [29, 30, 32] </ref>), just to cite a few. Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers. Examples include textual databases [26], digital libraries [24], and relational databases [37].
Reference: [30] <author> T. Hagerup and O. Petersson. </author> <title> Merging and sorting strings in parallel. </title> <booktitle> In Proc. International Symp. on Mathematical Foundations of Computer Science, </booktitle> <pages> pages 298306, </pages> <year> 1992. </year>
Reference-contexts: Very recently several authors have considered the importance of this problem from different points of view: multikey sorting (e.g. [11, 14, 39]), integer sorting and its variants (e.g. [4, 5, 25, 43]), and parallel string sorting (e.g. <ref> [29, 30, 32] </ref>), just to cite a few. Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers. Examples include textual databases [26], digital libraries [24], and relational databases [37].
Reference: [31] <author> S. Huddleston and K. Mehlhorn. </author> <title> A new data structure for representing sorted lists. </title> <journal> Acta Informatica, </journal> <volume> 17:157184, </volume> <year> 1982. </year>
Reference-contexts: We then describe our improved algorithm which is a combination of these two data structures. The Buffer Tree. The basic buffer tree on R integer keys is a B-tree (or rather an (a; b)-tree <ref> [31] </ref>) with fanout fi (M=B) and with blocks of elements in the leaves; thus the tree has height O (log M=B R B ). A buffer of size fi (M=B) blocks is assigned to each internal node and the operations on the structure are done in a lazy manner.
Reference: [32] <author> J. F. JaJa, K. W. Ryu, and U. Vishkin. </author> <title> Sorting strings and constructing digital search trees in parallel. </title> <booktitle> Theoretical Computer Science, </booktitle> <address> 154(2):225245, </address> <year> 1996. </year>
Reference-contexts: Very recently several authors have considered the importance of this problem from different points of view: multikey sorting (e.g. [11, 14, 39]), integer sorting and its variants (e.g. [4, 5, 25, 43]), and parallel string sorting (e.g. <ref> [29, 30, 32] </ref>), just to cite a few. Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers. Examples include textual databases [26], digital libraries [24], and relational databases [37].
Reference: [33] <author> R. Karp, R. Miller, and A. Rosenberg. </author> <title> Rapid identification of repeated patterns in strings, arrays and trees. </title> <booktitle> In Proc. ACM Symp. on Theory of Computation, </booktitle> <pages> pages 125136, </pages> <year> 1992. </year>
Reference-contexts: These algorithms exploit the limited size of the alphabet from which the characters are chosen in practice. By compressing the input strings in three different ways and using a variant of the doubling technique <ref> [33] </ref>, we have obtained three different algorithms. <p> the lower bound by substi-tuting N 0 = K 1 , B 0 = X , and M 0 = M B X into the above bound. 4 Practical Algorithms The basic tool we use to design practical sorting algorithms is an external memory version of the Karp-Miller-Rosenberg labeling technique <ref> [33] </ref>, also called the doubling algorithm, which uses O ( N DB log M=B B ) I/Os. We defer the description of the external doubling algorithm to the full paper.
Reference: [34] <author> D. Knuth. </author> <title> The Art of Computer Programming, Vol. 3 Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction The problem of lexicographically sorting a set of strings in the internal memory of computers has received much attention in the past (see e.g. <ref> [3, 34] </ref>). This problem is the most general formulation of sorting because it comprises integer sorting (i.e., strings of length one), multikey sorting (i.e., equal-length strings) and variable-length key sorting (i.e., arbitrarily long strings). <p> We then insert the string following X r in S r into H . We call this sequence of operations extract min. The H data structure, which is always stored in internal memory, is a lazy and slightly modified version of a compacted search trie <ref> [34, 38] </ref> storing X 1 , . . . , X M=B in lexicographical order. <p> In order to allow I/O-efficient search and update operations, each internal node contains the so-called blind trie data structure BT , which is built on the set of strings S pointed to by the pointers in . The blind trie is a variant of the compacted trie <ref> [34, 38] </ref> where each node u is labeled with the length len (u) of the string W (u) spelled out by the path from the root to u, just like in the H structure in Section 2.1.
Reference: [35] <author> U. Manber and G. Myers. </author> <title> Suffix arrays: A new method for on-line string searches. </title> <journal> SIAM Journal of Computing, </journal> <volume> 25(5):935948, </volume> <year> 1993. </year>
Reference-contexts: Using this structure in the external insertion sort yields an optimal sorting algorithm. As far as the general string sorting problem is concerned, there are a number of data structures like prefix B-trees [13], SB-trees [20], compacted tries [38], suffix trees [17, 36], and suffix arrays <ref> [35] </ref> that can be used to sort arbitrarily long strings in external memory. Among them, the SB-tree is the most I/O-efficient in the worst case; it can be employed to sort a set of strings in O (K log B K + N B ) I/Os.
Reference: [36] <author> E. M. McCreight. </author> <title> A space-economical suffix tree construction algorithm. </title> <journal> Journal of the ACM, </journal> <volume> 23(2):262272, </volume> <year> 1976. </year>
Reference-contexts: Using this structure in the external insertion sort yields an optimal sorting algorithm. As far as the general string sorting problem is concerned, there are a number of data structures like prefix B-trees [13], SB-trees [20], compacted tries [38], suffix trees <ref> [17, 36] </ref>, and suffix arrays [35] that can be used to sort arbitrarily long strings in external memory.
Reference: [37] <author> T. H. Merrett. </author> <title> Why sort-merge gives the best implementation of the natural join. </title> <journal> ACM SIGMOD Record, </journal> <volume> 13(2), </volume> <year> 1983. </year>
Reference-contexts: Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers. Examples include textual databases [26], digital libraries [24], and relational databases <ref> [37] </ref>. In these cases, the data need to be stored on external storage devices like disks or CD ROMs, which are roughly one million times slower than internal caches in terms of access time (or latency).
Reference: [38] <author> D. R. Morrison. PATRICIA: </author> <title> Practical algorithm to retrieve information coded in alphanumeric. </title> <journal> Journal of the ACM, </journal> <volume> 15:514534, </volume> <year> 1968. </year>
Reference-contexts: Using this structure in the external insertion sort yields an optimal sorting algorithm. As far as the general string sorting problem is concerned, there are a number of data structures like prefix B-trees [13], SB-trees [20], compacted tries <ref> [38] </ref>, suffix trees [17, 36], and suffix arrays [35] that can be used to sort arbitrarily long strings in external memory. <p> We then insert the string following X r in S r into H . We call this sequence of operations extract min. The H data structure, which is always stored in internal memory, is a lazy and slightly modified version of a compacted search trie <ref> [34, 38] </ref> storing X 1 , . . . , X M=B in lexicographical order. <p> In order to allow I/O-efficient search and update operations, each internal node contains the so-called blind trie data structure BT , which is built on the set of strings S pointed to by the pointers in . The blind trie is a variant of the compacted trie <ref> [34, 38] </ref> where each node u is labeled with the length len (u) of the string W (u) spelled out by the path from the root to u, just like in the H structure in Section 2.1.
Reference: [39] <author> J. I. Munro and V. Raman. </author> <title> Sorting multisets and vectors in-place. </title> <booktitle> In Proc. Workshop on Algorithms and Data Structures, </booktitle> <volume> LNCS 519, </volume> <pages> pages 473479, </pages> <year> 1991. </year>
Reference-contexts: Very recently several authors have considered the importance of this problem from different points of view: multikey sorting (e.g. <ref> [11, 14, 39] </ref>), integer sorting and its variants (e.g. [4, 5, 25, 43]), and parallel string sorting (e.g. [29, 30, 32]), just to cite a few. Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers.
Reference: [40] <author> M. H. Nodine and J. S. Vitter. </author> <title> Deterministic distribution sort in shared and distributed memory multiprocessors. </title> <booktitle> In Proc. ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 120129, </pages> <year> 1993. </year>
Reference-contexts: sequence can be obtained with O (K 2 + N 2 =B) = O (N 2 =B) extra I/Os by moving the strings into their final position one at a time. 1.2 Previous Results in I/O-efficient Compu tation Early work on I/O algorithms concentrated on sorting and permutation related problems <ref> [2, 9, 18, 41, 40, 46] </ref>. Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46].
Reference: [41] <author> M. H. Nodine and J. S. Vitter. </author> <title> Greed sort: An optimal sorting algorithm for multiple disks. </title> <journal> Journal of the ACM, </journal> <pages> pages 919 933, </pages> <year> 1995. </year>
Reference-contexts: sequence can be obtained with O (K 2 + N 2 =B) = O (N 2 =B) extra I/Os by moving the strings into their final position one at a time. 1.2 Previous Results in I/O-efficient Compu tation Early work on I/O algorithms concentrated on sorting and permutation related problems <ref> [2, 9, 18, 41, 40, 46] </ref>. Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46].
Reference: [42] <author> Yale N. Patt. </author> <title> The I/O subsystema candidate for improvement. Guest Editor's Introduction in IEEE Computer, </title> <address> 27(3):1516, </address> <year> 1994. </year>
Reference-contexts: I/O bottlenecks are increasing in significance since the relative speed of disks versus internal memories is decreasing, especially with more and more use of parallel computers <ref> [42] </ref>. The performance of currently used algorithms for sorting strings can seriously degrade when the space occupied by the strings is larger than the available internal memory.
Reference: [43] <author> M. </author> <title> Thorup. Randomized sorting in o(n log log n) time and linear space using addition, shift, and bit-wise boolean operations. </title> <booktitle> In Proc. ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 352359, </pages> <year> 1997. </year>
Reference-contexts: Very recently several authors have considered the importance of this problem from different points of view: multikey sorting (e.g. [11, 14, 39]), integer sorting and its variants (e.g. <ref> [4, 5, 25, 43] </ref>), and parallel string sorting (e.g. [29, 30, 32]), just to cite a few. Today's applications, however, tend to manipulate textual data sets of amazingly large size that do not fit into the internal memory of computers.
Reference: [44] <author> D. E. Vengroff and J. S. Vitter. </author> <title> Supporting I/O-efficient scientific computation in TPIE. </title> <booktitle> In Proc. IEEE Symp. on Parallel and Distributed Computing, </booktitle> <year> 1995. </year> <note> Appears also as Duke University Dept. of Computer Science technical report CS-1995-18. </note>
Reference-contexts: Even though disk striping does not in theory achieve asymptotic optimality when D is large, it is often the method of choice in practice <ref> [44] </ref>. 2 Model AIndivisible Strings In this section we prove Theorem 1 in two parts. In Section 2.1 we consider the upper bound and in Section 2.2 the lower bound. 2.1 Upper Bounds We use variants of external merge sort to sort both short and long strings.
Reference: [45] <author> D. E. Vengroff and J. S. Vitter. </author> <title> I/O-efficient computation: The TPIE approach. </title> <booktitle> In Proceedings of the Goddard Conference on Mass Storage Systems and Technologies, NASA Conference Publication 3340, </booktitle> <volume> Volume II, </volume> <pages> pages 553570, </pages> <address> College Park, MD, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: Work has also been done on matrix algebra and related problems arising in scientific computation <ref> [2, 45, 46] </ref>. More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation [6, 7, 16] and string matching [11, 17, 20, 22, 23].
Reference: [46] <author> J. S. Vitter and E. A. M. Shriver. </author> <title> Algorithms for parallel memory, I: Two-level memories. </title> <journal> Algorithmica, </journal> <volume> 12(23):110 147, </volume> <year> 1994. </year>
Reference-contexts: To amortize (or hide) disk latency, each input/output operation (or I/O) transfers a large block of contiguous data. We use the standard model of I/O complex ity <ref> [2, 46] </ref> and define the following parameters: K = # of strings to sort; N = total # of characters in the K strings; M = # of characters fitting in internal memory; B = # of characters per disk block (or track); where M &lt; N and 1 B M=2. <p> sequence can be obtained with O (K 2 + N 2 =B) = O (N 2 =B) extra I/Os by moving the strings into their final position one at a time. 1.2 Previous Results in I/O-efficient Compu tation Early work on I/O algorithms concentrated on sorting and permutation related problems <ref> [2, 9, 18, 41, 40, 46] </ref>. Work has also been done on matrix algebra and related problems arising in scientific computation [2, 45, 46]. <p> Work has also been done on matrix algebra and related problems arising in scientific computation <ref> [2, 45, 46] </ref>. More recently, researchers have designed I/O algorithms for a number of problems in different areas, such as in computational geometry [6, 10, 28], graph theoretic computation [6, 7, 16] and string matching [11, 17, 20, 22, 23]. <p> An increasingly popular approach to increase the throughput of I/O systems is to use a number D of disks in parallel, such that one can read (or write) D blocks in one I/O provided that they come from D distinct disks <ref> [15, 46] </ref>. An important property of our three practical algorithms is that they are all able to take full advantage of multiple disks; that is, they obtain linear speedup with respect to the number of available disks. <p> In contrast, we can make our theoretical algorithms work on D disks but without obtaining a linear speedup. To do so we use the disk striping technique <ref> [46] </ref>, which in terms of performance has the effect of using a single large disk with block size B 0 = DB.
References-found: 46

