URL: http://www.cs.columbia.edu/~simonb/papers/ls_cvpr.ps.gz
Refering-URL: http://www.cs.columbia.edu/~simonb/pub.html
Root-URL: http://www.cs.columbia.edu
Title: A Layered Approach to Stereo Reconstruction  
Author: Simon Baker Richard Szeliski and P. Anandan 
Affiliation: Department of Computer Science Microsoft Research Columbia University Microsoft Corporation  
Date: June 1998.  
Address: Santa Barbara, CA,  New York, NY 10027 Redmond, WA 98052  
Note: Appeared in the 1998 Conference on Computer Vision and Pattern Recognition,  
Abstract: We propose a framework for extracting structure from stereo which represents the scene as a collection of approximately planar layers. Each layer consists of an explicit 3D plane equation, a colored image with per-pixel opacity (a sprite), and a per-pixel depth offset relative to the plane. Initial estimates of the layers are recovered using techniques taken from parametric motion estimation. These initial estimates are then refined using a re-synthesis algorithm which takes into account both occlusions and mixed pixels. Reasoning about such effects allows the recovery of depth and color information with high accuracy, even in partially occluded regions. Another important benefit of our framework is that the output consists of a collection of approximately planar regions, a representation which is far more appropriate than a dense depth map for many applications such as rendering and video parsing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> PAMI, </journal> <volume> 17(4):384401, </volume> <year> 1985. </year>
Reference-contexts: Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences <ref> [1, 34, 12, 9, 15, 14, 24, 5, 36, 35] </ref>.
Reference: [2] <author> S. Ayer, P. Schroeter, and J. Bigun. </author> <title> Segmentation of moving objects by robust parameter estimation over multiple frames. </title> <booktitle> In 3rd ECCV, </booktitle> <pages> pages 316327, </pages> <year> 1994. </year>
Reference-contexts: layers remain which accurately model the scene [34, 24, 5]. * Iteratively apply dominant motion extraction [15, 24], at each step applying the algorithm to the residual re gions of the previous step. * Perform a color segmentation in each image, match the segments, and use as the initial assignment <ref> [2] </ref>. * Apply a simple stereo algorithm to get an approximate depth map, and then fit planes to the depth map. * Get a human to initialize the layers. (In many applications, such as model acquisition [10] and video parsing [24], the goal is a semi-automatic algorithm and limited user input <p> Other alternatives include, (1) performing a color segmentation of each input image and only smoothing within each segment in a similar manner to <ref> [2] </ref>, and (2) smoothing P kl less in the direction of the intensity gradient since strong gradients often coincide with depth discontinuities and hence layer boundaries. 2.
Reference: [3] <author> J. R. Bergen, P. J. Burt, R. Hingorani, and S. Peleg. </author> <title> A three-frame algorithm for estimating two-component image motion. </title> <journal> PAMI, </journal> <volume> 14(9):886896, </volume> <year> 1992. </year>
Reference-contexts: One fact which has not been exploited is that, when simultaneously imaged by several cameras, each of the layers implicitly lies on a fixed plane in the 3D world. Another omission is the proper treatment of transparency. With a few exceptions (e.g. <ref> [27, 3, 18] </ref>), the decomposition of an image into layers that are partially transparent has not been attempted. In contrast, scene modeling using multiple partially transparent layers is common in the graphics community [22, 6].
Reference: [4] <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In 2nd ECCV, </booktitle> <pages> pages 237252, </pages> <year> 1992. </year>
Reference-contexts: There are a number of functions which can be used to measure the degree of consistency between the warped images, including least squares <ref> [4] </ref> and robust measures [9, 24]. In both cases, the goal is the same: find the plane equation vector n l which maximizes the degree of consistency. <p> Typically, this extremum is found using some form of gradient decent, such as the Gauss-Newton method, and the optimization is performed in a hierarchical (i.e. pyramid based) fashion to avoid local extrema <ref> [4] </ref>. To apply this standard approach [31], we simply need to de rive the Jacobian of the image warp H l 1k with respect to the parameters of n l .
Reference: [5] <author> M.J. Black and A.D. Jepson. </author> <title> Estimating optical flow in segmented images using variable-order parametric models with local deformations. </title> <journal> PAMI, </journal> <volume> 18(10):972986, </volume> <year> 1996. </year>
Reference-contexts: Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences <ref> [1, 34, 12, 9, 15, 14, 24, 5, 36, 35] </ref>. <p> A number of approaches have been proposed in the parametric motion literature: * Randomly initialize a large number of small layers, which grow and merge until a small number of layers remain which accurately model the scene <ref> [34, 24, 5] </ref>. * Iteratively apply dominant motion extraction [15, 24], at each step applying the algorithm to the residual re gions of the previous step. * Perform a color segmentation in each image, match the segments, and use as the initial assignment [2]. * Apply a simple stereo algorithm to
Reference: [6] <author> J.F. Blinn. Jim Blinn's corner: Compositing, </author> <title> part 1: </title> <journal> Theory. IEEE Computer Graphics and Applications, </journal> <volume> 14(5):8387, </volume> <month> September </month> <year> 1994. </year>
Reference-contexts: Another omission is the proper treatment of transparency. With a few exceptions (e.g. [27, 3, 18]), the decomposition of an image into layers that are partially transparent has not been attempted. In contrast, scene modeling using multiple partially transparent layers is common in the graphics community <ref> [22, 6] </ref>. In this paper, we present a framework for reconstructing a scene as a collection of approximately planar layers. Each of the layers has an explicit 3D plane equation and is recovered as a sprite, i.e. a colored image with per-pixel opacity (transparency) [22, 6, 32, 20]. <p> In this paper, we present a framework for reconstructing a scene as a collection of approximately planar layers. Each of the layers has an explicit 3D plane equation and is recovered as a sprite, i.e. a colored image with per-pixel opacity (transparency) <ref> [22, 6, 32, 20] </ref>. To model a wider range of scenes, a per-pixel depth offset relative to the plane is also added. Recovery of the layers begins with the iteration of several steps based on techniques developed for parametric motion estimation, image registration, and mosaicing. <p> Following <ref> [6] </ref>, we denote a layer sprite with pre-multiplied opacities by: L l (u l ) = (ff l r l ; ff l g l ; ff l b l ; ff l ) (1) where r l = r l (u l ) is the red band, g l = <p> we propose a measure of how well the layers re-synthesize the input images, and show how the re-synthesis error can be minimized to refine the estimates of the layer sprites. 3.1 The Image Formation Process We formulate the generative (forward) model of the image formation process using image compositing operations <ref> [6] </ref>, i.e. by painting the sprites one over another in a back-to-front order. <p> The basic operator used to overlay the sprites is the over operator: F fi B F + (1 ff F )B; (18) where F and B are the foreground and background sprites, and ff F is the opacity of the foreground <ref> [22, 6] </ref>. This definition of the over operator assumes pre-multiplied opacities, as in Equation (1). The generative model consists of the following two steps: 1. <p> This yields the un-warped sprite: U kl = (W l Note that the opacities should be warped along with the color values <ref> [6] </ref>. 4 We may want to smooth P kl even more, e.g. using an isotropic smoother such as a Gaussian.
Reference: [7] <author> M.-C. Chiang and T.E. Boult. </author> <title> Local blur estimation and super-resolution. </title> <booktitle> In CVPR '97, </booktitle> <pages> pages 821826, </pages> <year> 1997. </year>
Reference-contexts: Part of the cause is non-planarity in the scene (which is modeled in Section 2.4), but image noise and resampling error also contribute. One simple method of compensating for this effect is to deghost the sprites [28]. Another solution is to use image enhancement techniques such as <ref> [16, 21, 7] </ref>, which can even be used to obtain super-resolution sprites. 2.4 Estimation of the Residual Depth In general, the scene will not be piecewise planar. To model any non-planarity, we allow the point u l on the plane n T l x = 0 to be displaced slightly.
Reference: [8] <author> R.T. Collins. </author> <title> A space-sweep approach to true multi-image matching. </title> <booktitle> In CVPR '96, </booktitle> <pages> pages 358363, </pages> <year> 1996. </year>
Reference-contexts: Most existing algorithms work well when matching feature points or highly textured regions, but perform poorly around occlusion boundaries and in untextured regions. A common element of many recent attempts to solve these problems is explicit modeling of the 3D volume of the scene <ref> [37, 13, 8, 25, 26, 30] </ref>. The scene volume is discretized, often in terms of equal increments of disparity, rather than into equally sized voxels. The goal is then to find the voxels which lie on the surfaces of the objects in the scene. <p> The goal is then to find the voxels which lie on the surfaces of the objects in the scene. The major benefits of such approaches include, the equal and efficient treatment of a large number of images <ref> [8] </ref>, the possibility of modeling occlusions [13], and the detection of mixed pixels at occlusion boundaries to obtain sub-pixel accuracy [30]. Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. <p> We could warp each masked image onto H ffi H l 1k ffi M kl (u 1 ) M kl (HH l 1k u 1 ). The addition of the homography H can be used to remove the dependence on one distinguished image, as advocated by Collins <ref> [8] </ref>. 2 A suitable choice for Q l would be one of the camera matrices P k , in which case Equation (8) reduces to Equation (6).
Reference: [9] <author> T. Darrell and A.P. Pentland. </author> <title> Cooperative robust estimation using layers of support. </title> <journal> PAMI, </journal> <volume> 17(5):474487, </volume> <year> 1995. </year>
Reference-contexts: Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences <ref> [1, 34, 12, 9, 15, 14, 24, 5, 36, 35] </ref>. <p> There are a number of functions which can be used to measure the degree of consistency between the warped images, including least squares [4] and robust measures <ref> [9, 24] </ref>. In both cases, the goal is the same: find the plane equation vector n l which maximizes the degree of consistency.
Reference: [10] <author> P.E. Debevec, C.J. Taylor, and J. Malik. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach. </title> <booktitle> In SIGGRAPH '96, </booktitle> <pages> pages 11 20, </pages> <year> 1996. </year>
Reference-contexts: In this respect, the framework is similar to the plane + parallax work of [19, 23, 29], the model-based stereo work of <ref> [10] </ref>, and the parametric motion + residual optical flow of [12]. * The output (a collection of approximately planar regions) is more suitable than a discrete collection of voxels for many applications, including, rendering [10] and video parsing [15, 24]. 434 We assume that the scene can be represented by L <p> similar to the plane + parallax work of [19, 23, 29], the model-based stereo work of <ref> [10] </ref>, and the parametric motion + residual optical flow of [12]. * The output (a collection of approximately planar regions) is more suitable than a discrete collection of voxels for many applications, including, rendering [10] and video parsing [15, 24]. 434 We assume that the scene can be represented by L sprite images L l on planes n T l x = 0 with depth offsets Z l . <p> color segmentation in each image, match the segments, and use as the initial assignment [2]. * Apply a simple stereo algorithm to get an approximate depth map, and then fit planes to the depth map. * Get a human to initialize the layers. (In many applications, such as model acquisition <ref> [10] </ref> and video parsing [24], the goal is a semi-automatic algorithm and limited user input is acceptable.) In this paper, we assume a human has initialized the layers. <p> Doing so essentially solves a simpler (or what Debevec et. al <ref> [10] </ref> term a model-based) stereo problem. To compute the residual depth map, we initially set Z l (u l ) to be the value (in a range close to zero) which minimizes the variance of (H l k ; t kl ; Z l ) ffi M kl across k.
Reference: [11] <author> O.D. Faugeras. </author> <title> Three-Dimensional Computer Vision: A Geometric Viewpoint. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: The motion of each layer is determined by the values of the parameters. An important transformation is the 8parameter homog-raphy (collineation), because it describes the motion of a rigid planar patch as either it or the camera moves <ref> [11] </ref>. While existing techniques have been successful in detecting multiple independent motions, layer extraction for scene modeling has not been fully developed. <p> If x is a 3D world coordinate of a point and u k is the image of x in camera P k , we have: u k = P k x (2) where equality is in the 2D projective space P 2 <ref> [11] </ref>. <p> So, we can now map this point onto its image in another camera P k 0 : u k 0 = P k 0 (n T l P fl kk 0 u k (6) where H l kk 0 is a homography (collineation of P 2 <ref> [11] </ref>). Equation (6) describes the mapping between the two images which would hold if the pixels were all images of points on the plane n T l x = 0.
Reference: [12] <author> S. Hsu, P. Anandan, and S. Peleg. </author> <title> Accurate computation of optical flow by using layered motion representations. </title> <booktitle> In ICPR '94, </booktitle> <pages> pages 743746, </pages> <year> 1994. </year>
Reference-contexts: Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences <ref> [1, 34, 12, 9, 15, 14, 24, 5, 36, 35] </ref>. <p> In this respect, the framework is similar to the plane + parallax work of [19, 23, 29], the model-based stereo work of [10], and the parametric motion + residual optical flow of <ref> [12] </ref>. * The output (a collection of approximately planar regions) is more suitable than a discrete collection of voxels for many applications, including, rendering [10] and video parsing [15, 24]. 434 We assume that the scene can be represented by L sprite images L l on planes n T l x
Reference: [13] <author> S.S. Intille and A.F. Bobick. </author> <title> Disparity-space images and large occlusion stereo. </title> <booktitle> In 2nd ECCV, </booktitle> <year> 1994. </year>
Reference-contexts: Most existing algorithms work well when matching feature points or highly textured regions, but perform poorly around occlusion boundaries and in untextured regions. A common element of many recent attempts to solve these problems is explicit modeling of the 3D volume of the scene <ref> [37, 13, 8, 25, 26, 30] </ref>. The scene volume is discretized, often in terms of equal increments of disparity, rather than into equally sized voxels. The goal is then to find the voxels which lie on the surfaces of the objects in the scene. <p> The goal is then to find the voxels which lie on the surfaces of the objects in the scene. The major benefits of such approaches include, the equal and efficient treatment of a large number of images [8], the possibility of modeling occlusions <ref> [13] </ref>, and the detection of mixed pixels at occlusion boundaries to obtain sub-pixel accuracy [30]. Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts.
Reference: [14] <author> M. Irani and P. Anandan. </author> <title> A unified approach to moving object detection in 2D and 3D scenes. </title> <booktitle> In 12th ICPR, </booktitle> <pages> pages 712717, </pages> <year> 1996. </year>
Reference-contexts: Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences <ref> [1, 34, 12, 9, 15, 14, 24, 5, 36, 35] </ref>.
Reference: [15] <author> M. Irani, P. Anandan, and S. Hsu. </author> <title> Mosiac based representations of video sequences and their applications. </title> <booktitle> In 5th ICCV, </booktitle> <pages> pages 605611, </pages> <year> 1995. </year>
Reference-contexts: Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences <ref> [1, 34, 12, 9, 15, 14, 24, 5, 36, 35] </ref>. <p> + parallax work of [19, 23, 29], the model-based stereo work of [10], and the parametric motion + residual optical flow of [12]. * The output (a collection of approximately planar regions) is more suitable than a discrete collection of voxels for many applications, including, rendering [10] and video parsing <ref> [15, 24] </ref>. 434 We assume that the scene can be represented by L sprite images L l on planes n T l x = 0 with depth offsets Z l . <p> A number of approaches have been proposed in the parametric motion literature: * Randomly initialize a large number of small layers, which grow and merge until a small number of layers remain which accurately model the scene [34, 24, 5]. * Iteratively apply dominant motion extraction <ref> [15, 24] </ref>, at each step applying the algorithm to the residual re gions of the previous step. * Perform a color segmentation in each image, match the segments, and use as the initial assignment [2]. * Apply a simple stereo algorithm to get an approximate depth map, and then fit planes <p> Both blending and resampling tend to increase blur, so, even if the pixel assignment was perfect, these images may well differ substantially. 437 Locally estimated variants of the residual normal flow have been used by Irani and coworkers <ref> [16, 17, 15] </ref>. A final possibility would be to compute the optical flow between W l k ffi ~ M kl and ~ L l . Then a decreasing function of the magnitude of the flow could be used for P kl .
Reference: [16] <author> M. Irani and S. Peleg. </author> <title> Image sequence enhancement using multiple motions analysis. </title> <booktitle> In CVPR '92, </booktitle> <pages> pages 216221, </pages> <year> 1992. </year>
Reference-contexts: Part of the cause is non-planarity in the scene (which is modeled in Section 2.4), but image noise and resampling error also contribute. One simple method of compensating for this effect is to deghost the sprites [28]. Another solution is to use image enhancement techniques such as <ref> [16, 21, 7] </ref>, which can even be used to obtain super-resolution sprites. 2.4 Estimation of the Residual Depth In general, the scene will not be piecewise planar. To model any non-planarity, we allow the point u l on the plane n T l x = 0 to be displaced slightly. <p> Both blending and resampling tend to increase blur, so, even if the pixel assignment was perfect, these images may well differ substantially. 437 Locally estimated variants of the residual normal flow have been used by Irani and coworkers <ref> [16, 17, 15] </ref>. A final possibility would be to compute the optical flow between W l k ffi ~ M kl and ~ L l . Then a decreasing function of the magnitude of the flow could be used for P kl .
Reference: [17] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Detecting and tracking multiple moving objects using temporal integration. </title> <booktitle> In 2nd ECCV, </booktitle> <pages> pages 282287, </pages> <year> 1992. </year>
Reference-contexts: Both blending and resampling tend to increase blur, so, even if the pixel assignment was perfect, these images may well differ substantially. 437 Locally estimated variants of the residual normal flow have been used by Irani and coworkers <ref> [16, 17, 15] </ref>. A final possibility would be to compute the optical flow between W l k ffi ~ M kl and ~ L l . Then a decreasing function of the magnitude of the flow could be used for P kl .
Reference: [18] <author> S.X. Ju, M.J. Black, and A.D. Jepson. </author> <title> Skin and bones: Multi-layer, locally affine, optical flow and regularization with transparency. </title> <booktitle> In CVPR '96, </booktitle> <pages> pages 307314, </pages> <year> 1996. </year>
Reference-contexts: One fact which has not been exploited is that, when simultaneously imaged by several cameras, each of the layers implicitly lies on a fixed plane in the 3D world. Another omission is the proper treatment of transparency. With a few exceptions (e.g. <ref> [27, 3, 18] </ref>), the decomposition of an image into layers that are partially transparent has not been attempted. In contrast, scene modeling using multiple partially transparent layers is common in the graphics community [22, 6].
Reference: [19] <author> R. Kumar, P. Anandan, and K. Hanna. </author> <title> Direct recovery of shape from multiple views: A parallax based approach. </title> <booktitle> In 12th ICPR, </booktitle> <pages> pages 685688, </pages> <year> 1994. </year>
Reference-contexts: In addition, it offers a number of other advantages: * The combination of the global model (the plane) with the local correction to it (the per-pixel depth offset) results in very robust performance. In this respect, the framework is similar to the plane + parallax work of <ref> [19, 23, 29] </ref>, the model-based stereo work of [10], and the parametric motion + residual optical flow of [12]. * The output (a collection of approximately planar regions) is more suitable than a discrete collection of voxels for many applications, including, rendering [10] and video parsing [15, 24]. 434 We assume <p> The distance it is displaced is denoted by Z l (u l ); as measured in the direction normal to the plane. In this case, the homographic warps used in the previous section are not applicable, but using a similar argument, it is possible to show (see also <ref> [19, 23] </ref>) that: u k = H l where H l k = P k (n T l Q fl l is the planar ho-mography of Section 2.3, t kl = P k q l is the epipole, and it is assumed that the vector n l = (n x ;
Reference: [20] <author> J. Lengyel and J. Snyder. </author> <title> Rendering with coherent layers. </title> <booktitle> In SIGGRAPH '97, </booktitle> <pages> pages 233242, </pages> <year> 1997. </year>
Reference-contexts: In this paper, we present a framework for reconstructing a scene as a collection of approximately planar layers. Each of the layers has an explicit 3D plane equation and is recovered as a sprite, i.e. a colored image with per-pixel opacity (transparency) <ref> [22, 6, 32, 20] </ref>. To model a wider range of scenes, a per-pixel depth offset relative to the plane is also added. Recovery of the layers begins with the iteration of several steps based on techniques developed for parametric motion estimation, image registration, and mosaicing.
Reference: [21] <author> S. Mann and R.W. </author> <title> Picard. Virtual bellows: Constructing high quality stills from video. </title> <booktitle> In 1st ICIP, </booktitle> <pages> pages 363367, </pages> <year> 1994. </year>
Reference-contexts: Part of the cause is non-planarity in the scene (which is modeled in Section 2.4), but image noise and resampling error also contribute. One simple method of compensating for this effect is to deghost the sprites [28]. Another solution is to use image enhancement techniques such as <ref> [16, 21, 7] </ref>, which can even be used to obtain super-resolution sprites. 2.4 Estimation of the Residual Depth In general, the scene will not be piecewise planar. To model any non-planarity, we allow the point u l on the plane n T l x = 0 to be displaced slightly.
Reference: [22] <author> T. Porter and T. Duff. </author> <title> Compositing digital images. </title> <booktitle> SIG-GRAPH '84, </booktitle> <pages> pages 253259, </pages> <year> 1984. </year>
Reference-contexts: Another omission is the proper treatment of transparency. With a few exceptions (e.g. [27, 3, 18]), the decomposition of an image into layers that are partially transparent has not been attempted. In contrast, scene modeling using multiple partially transparent layers is common in the graphics community <ref> [22, 6] </ref>. In this paper, we present a framework for reconstructing a scene as a collection of approximately planar layers. Each of the layers has an explicit 3D plane equation and is recovered as a sprite, i.e. a colored image with per-pixel opacity (transparency) [22, 6, 32, 20]. <p> In this paper, we present a framework for reconstructing a scene as a collection of approximately planar layers. Each of the layers has an explicit 3D plane equation and is recovered as a sprite, i.e. a colored image with per-pixel opacity (transparency) <ref> [22, 6, 32, 20] </ref>. To model a wider range of scenes, a per-pixel depth offset relative to the plane is also added. Recovery of the layers begins with the iteration of several steps based on techniques developed for parametric motion estimation, image registration, and mosaicing. <p> The basic operator used to overlay the sprites is the over operator: F fi B F + (1 ff F )B; (18) where F and B are the foreground and background sprites, and ff F is the opacity of the foreground <ref> [22, 6] </ref>. This definition of the over operator assumes pre-multiplied opacities, as in Equation (1). The generative model consists of the following two steps: 1.
Reference: [23] <author> H. S. Sawhney. </author> <title> 3D geometry from planar parallax. </title> <booktitle> In CVPR '94, </booktitle> <pages> pages 929934, </pages> <year> 1994. </year>
Reference-contexts: In addition, it offers a number of other advantages: * The combination of the global model (the plane) with the local correction to it (the per-pixel depth offset) results in very robust performance. In this respect, the framework is similar to the plane + parallax work of <ref> [19, 23, 29] </ref>, the model-based stereo work of [10], and the parametric motion + residual optical flow of [12]. * The output (a collection of approximately planar regions) is more suitable than a discrete collection of voxels for many applications, including, rendering [10] and video parsing [15, 24]. 434 We assume <p> The distance it is displaced is denoted by Z l (u l ); as measured in the direction normal to the plane. In this case, the homographic warps used in the previous section are not applicable, but using a similar argument, it is possible to show (see also <ref> [19, 23] </ref>) that: u k = H l where H l k = P k (n T l Q fl l is the planar ho-mography of Section 2.3, t kl = P k q l is the epipole, and it is assumed that the vector n l = (n x ;
Reference: [24] <author> H.S. Sawhney and S. Ayer. </author> <title> Compact representations of videos through dominant and multiple motion estimation. </title> <journal> PAMI, </journal> <volume> 18(8):814830, </volume> <year> 1996. </year>
Reference-contexts: Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences <ref> [1, 34, 12, 9, 15, 14, 24, 5, 36, 35] </ref>. <p> + parallax work of [19, 23, 29], the model-based stereo work of [10], and the parametric motion + residual optical flow of [12]. * The output (a collection of approximately planar regions) is more suitable than a discrete collection of voxels for many applications, including, rendering [10] and video parsing <ref> [15, 24] </ref>. 434 We assume that the scene can be represented by L sprite images L l on planes n T l x = 0 with depth offsets Z l . <p> A number of approaches have been proposed in the parametric motion literature: * Randomly initialize a large number of small layers, which grow and merge until a small number of layers remain which accurately model the scene <ref> [34, 24, 5] </ref>. * Iteratively apply dominant motion extraction [15, 24], at each step applying the algorithm to the residual re gions of the previous step. * Perform a color segmentation in each image, match the segments, and use as the initial assignment [2]. * Apply a simple stereo algorithm to <p> A number of approaches have been proposed in the parametric motion literature: * Randomly initialize a large number of small layers, which grow and merge until a small number of layers remain which accurately model the scene [34, 24, 5]. * Iteratively apply dominant motion extraction <ref> [15, 24] </ref>, at each step applying the algorithm to the residual re gions of the previous step. * Perform a color segmentation in each image, match the segments, and use as the initial assignment [2]. * Apply a simple stereo algorithm to get an approximate depth map, and then fit planes <p> image, match the segments, and use as the initial assignment [2]. * Apply a simple stereo algorithm to get an approximate depth map, and then fit planes to the depth map. * Get a human to initialize the layers. (In many applications, such as model acquisition [10] and video parsing <ref> [24] </ref>, the goal is a semi-automatic algorithm and limited user input is acceptable.) In this paper, we assume a human has initialized the layers. <p> There are a number of functions which can be used to measure the degree of consistency between the warped images, including least squares [4] and robust measures <ref> [9, 24] </ref>. In both cases, the goal is the same: find the plane equation vector n l which maximizes the degree of consistency. <p> There are a number of ways of defining P kl . Perhaps the simplest is the residual intensity difference <ref> [24] </ref>: P kl = k ~ L l W l Another is the magnitude of the residual normal flow: P kl = k ffi ~ M kl k : (15) 3 Alternatively, we could compare the input images I k with the layer sprite images warped back onto image coordinates (W
Reference: [25] <author> D. Scharstein and R. Szeliski. </author> <title> Stereo matching with nonlinear diffusion. </title> <booktitle> In CVPR '96, </booktitle> <pages> pages 343350, </pages> <year> 1996. </year>
Reference-contexts: Most existing algorithms work well when matching feature points or highly textured regions, but perform poorly around occlusion boundaries and in untextured regions. A common element of many recent attempts to solve these problems is explicit modeling of the 3D volume of the scene <ref> [37, 13, 8, 25, 26, 30] </ref>. The scene volume is discretized, often in terms of equal increments of disparity, rather than into equally sized voxels. The goal is then to find the voxels which lie on the surfaces of the objects in the scene.
Reference: [26] <author> S.M. Seitz and C.M. Dyer. </author> <title> Photorealistic scene reconstr-cution by space coloring. </title> <booktitle> In CVPR '97, </booktitle> <pages> pages 10671073, </pages> <year> 1997. </year>
Reference-contexts: Most existing algorithms work well when matching feature points or highly textured regions, but perform poorly around occlusion boundaries and in untextured regions. A common element of many recent attempts to solve these problems is explicit modeling of the 3D volume of the scene <ref> [37, 13, 8, 25, 26, 30] </ref>. The scene volume is discretized, often in terms of equal increments of disparity, rather than into equally sized voxels. The goal is then to find the voxels which lie on the surfaces of the objects in the scene.
Reference: [27] <author> M. Shizawa and K. Mase. </author> <title> A unified computational theory of motion transparency and motion boundaries based on eigenenergy analysis. </title> <booktitle> In CVPR '91, </booktitle> <pages> pages 289295, </pages> <year> 1991. </year>
Reference-contexts: One fact which has not been exploited is that, when simultaneously imaged by several cameras, each of the layers implicitly lies on a fixed plane in the 3D world. Another omission is the proper treatment of transparency. With a few exceptions (e.g. <ref> [27, 3, 18] </ref>), the decomposition of an image into layers that are partially transparent has not been attempted. In contrast, scene modeling using multiple partially transparent layers is common in the graphics community [22, 6].
Reference: [28] <author> H.-Y. Shum and R. Szeliski. </author> <title> Construction and refinement of panoramic mosaics with global and local alignment. </title> <booktitle> In 6th ICCV, </booktitle> <year> 1998. </year>
Reference-contexts: There are a number of ways in which blending could be performed. One simple method would be to take the mean of the color values. A refinement would be to use a feathering algorithm such as <ref> [28] </ref>, where the average is weighted by the distance of each pixel from the nearest invisible pixel (i.e. ff = 0) in M kl . Alternatively, robust techniques could be used to estimate L l . The simplest such example is the median operator, but more sophisticated alternatives exist. <p> Part of the cause is non-planarity in the scene (which is modeled in Section 2.4), but image noise and resampling error also contribute. One simple method of compensating for this effect is to deghost the sprites <ref> [28] </ref>. Another solution is to use image enhancement techniques such as [16, 21, 7], which can even be used to obtain super-resolution sprites. 2.4 Estimation of the Residual Depth In general, the scene will not be piecewise planar. <p> The weights should depend on the distance of the pixel from the closest pixel for which B kl = 1; in a similar manner to the feathering algorithm of <ref> [28] </ref>. Given ~ L l , our approach to pixel assignment is as follows.
Reference: [29] <author> R. Szeliski and J. Coughlan. </author> <title> Hierarchical spline-based image registration. </title> <booktitle> In CVPR '94, </booktitle> <pages> pages 194201, </pages> <year> 1994. </year>
Reference-contexts: In addition, it offers a number of other advantages: * The combination of the global model (the plane) with the local correction to it (the per-pixel depth offset) results in very robust performance. In this respect, the framework is similar to the plane + parallax work of <ref> [19, 23, 29] </ref>, the model-based stereo work of [10], and the parametric motion + residual optical flow of [12]. * The output (a collection of approximately planar regions) is more suitable than a discrete collection of voxels for many applications, including, rendering [10] and video parsing [15, 24]. 434 We assume
Reference: [30] <author> R. Szeliski and P. Golland. </author> <title> Stereo matching with transparency and matting. </title> <booktitle> In 6th ICCV, </booktitle> <year> 1998. </year>
Reference-contexts: Most existing algorithms work well when matching feature points or highly textured regions, but perform poorly around occlusion boundaries and in untextured regions. A common element of many recent attempts to solve these problems is explicit modeling of the 3D volume of the scene <ref> [37, 13, 8, 25, 26, 30] </ref>. The scene volume is discretized, often in terms of equal increments of disparity, rather than into equally sized voxels. The goal is then to find the voxels which lie on the surfaces of the objects in the scene. <p> The major benefits of such approaches include, the equal and efficient treatment of a large number of images [8], the possibility of modeling occlusions [13], and the detection of mixed pixels at occlusion boundaries to obtain sub-pixel accuracy <ref> [30] </ref>. Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences [1, 34, 12, 9, 15, 14, 24, 5, 36, 35]. <p> Recovery of the layers begins with the iteration of several steps based on techniques developed for parametric motion estimation, image registration, and mosaicing. The resulting layer estimates are then refined using a resynthesis step which takes into account both occlusions and mixed pixels in a similar manner to <ref> [30] </ref>. Our layered approach to stereo shares many of the advantages of the aforementioned volumetric techniques. In addition, it offers a number of other advantages: * The combination of the global model (the plane) with the local correction to it (the per-pixel depth offset) results in very robust performance. <p> If we have solved the stereo reconstruction problem, and neglecting re sampling issues, S k should match the input I k . This last step can be re-written as three simpler steps: 2a. Compute the visibility of each un-warped sprite <ref> [30] </ref>: V kl = V k (l1) (1 ff k (l1) ) = l 0 =1 where ff kl is the alpha channel of U kl , and V k1 = 1. 2b. Compute the masked images, M kl = V kl U kl . 2c. <p> Therefore, we refine the layer estimates by minimizing the prediction error: C = k u k using a gradient descent algorithm. (In order to further constrain the space of possible solutions, we can add smoothness constraints on the colors and opacities <ref> [30] </ref>.) Rather than trying to optimize over all of the parameters (L l , n l , and Z l ) simultaneously, we only adjust the sprite colors and opacities in L l , and then re-run the previous motion estimation steps to adjust n l and Z l (see Figure <p> The derivatives of the cost function C with respect to the colors and opacities in L l (u l ) can be computed using the chain rule <ref> [30] </ref>. In more detail, the visibility map V kl mediates the interaction between the un-warped sprite U kl and the synthesized image S k , and is itself a function of the opacities in the un-warped sprites U kl . <p> This dependence can either be exploited directly using the chain rule to propagate gradients, or alternatively the derivatives of C with respect to U kl can be warped back into the reference frame of L l <ref> [30] </ref>. 438 (a) (b) (c) (g) (h) (d) and (e) the six layer sprites; (f) depth map for planar sprites (darker denotes closer); front layer before (g) and after (h) residual depth estimation. (a) (b) (c) depth map (darker denotes closer); (d) and (e) the five layer sprites; (f) residual depth
Reference: [31] <author> R. Szeliski and H.-Y. Shum. </author> <title> Creating full view panoramic image mosaics and texture-mapped models. </title> <booktitle> In SIG-GRAPH '97, </booktitle> <pages> pages 251258, </pages> <year> 1997. </year>
Reference-contexts: Typically, this extremum is found using some form of gradient decent, such as the Gauss-Newton method, and the optimization is performed in a hierarchical (i.e. pyramid based) fashion to avoid local extrema [4]. To apply this standard approach <ref> [31] </ref>, we simply need to de rive the Jacobian of the image warp H l 1k with respect to the parameters of n l . <p> To initialize our algorithm, we first decided how many layers were required, and then performed a rough assignment of pixels to layers by hand. Various automated techniques for performing this initial labeling are described in Section 2.1. Next, the automatic hierarchical parametric motion estimation algorithm described in <ref> [31] </ref> was used to find the 8-parameter homographies between the layers and estimate the layer sprites. (For the experiments presented in this paper, we set Q l = P 1 , i.e. we reconstructed the sprites in the coordinate system of the first camera.) Using the computed homographies, we found the
Reference: [32] <author> J. Torborg and J.T. Kajiya. Talisman: </author> <title> Commodity realtime 3D graphics for the PC. </title> <booktitle> In SIGGRAPH '96, </booktitle> <pages> pages 353363, </pages> <year> 1996. </year>
Reference-contexts: In this paper, we present a framework for reconstructing a scene as a collection of approximately planar layers. Each of the layers has an explicit 3D plane equation and is recovered as a sprite, i.e. a colored image with per-pixel opacity (transparency) <ref> [22, 6, 32, 20] </ref>. To model a wider range of scenes, a per-pixel depth offset relative to the plane is also added. Recovery of the layers begins with the iteration of several steps based on techniques developed for parametric motion estimation, image registration, and mosaicing.
Reference: [33] <author> T. Vieville, C. Zeller, and L. Robert. </author> <title> Using collineations to compute motion and structure in an uncalibrated image sequence. </title> <address> IJCV, 20(3):213242, </address> <year> 1996. </year>
Reference-contexts: layer sprites. (For the experiments presented in this paper, we set Q l = P 1 , i.e. we reconstructed the sprites in the coordinate system of the first camera.) Using the computed homographies, we found the best plane estimate for each layer using a Euclidean structure from motion algorithm <ref> [33] </ref>. The results of applying these steps to the MPEG flower garden sequence are shown in Figure 3. Figures 3 (a) and (b) show the first and last image in the subsequence we used (the first nine even images). Figure 3 (c) shows the initial pixel labeling into seven layers.
Reference: [34] <author> J.Y.A. Wang and E.H. Adelson. </author> <title> Layered representation for motion analysis. </title> <booktitle> In CVPR '93, </booktitle> <pages> pages 361366, </pages> <year> 1993. </year>
Reference-contexts: Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences <ref> [1, 34, 12, 9, 15, 14, 24, 5, 36, 35] </ref>. <p> A number of approaches have been proposed in the parametric motion literature: * Randomly initialize a large number of small layers, which grow and merge until a small number of layers remain which accurately model the scene <ref> [34, 24, 5] </ref>. * Iteratively apply dominant motion extraction [15, 24], at each step applying the algorithm to the residual re gions of the previous step. * Perform a color segmentation in each image, match the segments, and use as the initial assignment [2]. * Apply a simple stereo algorithm to
Reference: [35] <author> Y. Weiss. </author> <title> Smoothness in layers: Motion segmentation using nonparametric mixture estimation. </title> <booktitle> In CVPR '97, </booktitle> <pages> pages 520526, </pages> <year> 1997. </year>
Reference-contexts: Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences <ref> [1, 34, 12, 9, 15, 14, 24, 5, 36, 35] </ref>.
Reference: [36] <author> Y. Weiss and E.H. Adelson. </author> <title> A unified mixture framework for motion segmentation: Incorporating spatial coherence and estimating the number of models. </title> <booktitle> In CVPR '96, </booktitle> <pages> pages 321326, </pages> <year> 1996. </year>
Reference-contexts: Unfortunately, discretizing space volumetrically introduces a huge number of degrees of freedom, and leads to sampling and aliasing artifacts. Another active area of research is the detection of parametric motions within image sequences <ref> [1, 34, 12, 9, 15, 14, 24, 5, 36, 35] </ref>.
Reference: [37] <author> Y. Yang, A. Yuille, and J. Lu. </author> <title> Local, global, and multilevel stereo matching. </title> <booktitle> In CVPR '93, </booktitle> <pages> pages 274279, </pages> <year> 1993. </year> <month> 441 </month>
Reference-contexts: Most existing algorithms work well when matching feature points or highly textured regions, but perform poorly around occlusion boundaries and in untextured regions. A common element of many recent attempts to solve these problems is explicit modeling of the 3D volume of the scene <ref> [37, 13, 8, 25, 26, 30] </ref>. The scene volume is discretized, often in terms of equal increments of disparity, rather than into equally sized voxels. The goal is then to find the voxels which lie on the surfaces of the objects in the scene.
References-found: 37

