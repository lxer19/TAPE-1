URL: http://polaris.cs.uiuc.edu/reports/1125.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: LARGE GRAIN PARALLEL SPARSE SYSTEM SOLVER  
Author: BY BRET ANDREW MARSOLF 
Degree: B.S.C.S., Rose-Hulman Institute of Technology, 1987 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1994 Urbana, Illinois  
Affiliation: University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Alaghband. </author> <title> A parallel pivoting algorithm on a shared memory multiprocessor. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, Volume3: Applications and Algorithms, </booktitle> <volume> volume 3, </volume> <pages> pages 177-180, </pages> <year> 1988. </year>
Reference-contexts: This strategy provided a means of controlling fillin at the processor level. One other approach to parallel sparse solvers has been the work with parallel pivots <ref> [1, 3] </ref>. These algorithms try to find a set of pivots that can be applied in parallel. As with multifrontal codes, this type of an algorithm does not provide the large grain parallelism to make use of the clusters on Cedar and so it was not used.
Reference: [2] <author> C. C. Ashcraft, R. G. Grimes, J. G. Lewis, B. W. Peyton, and H. D. Simon. </author> <title> Progress in sparse matrix methods for large linear systems on vector supercomputers. </title> <journal> The International Journal of Supercomputing Applications, </journal> <volume> 1(4) </volume> <pages> 10-30, </pages> <month> Winter </month> <year> 1987. </year>
Reference-contexts: This approach if organized correctly can provide large grain parallelism to 4 make use of the clusters on Cedar but it assumes a symmetric structure of the matrix and the pivot sequence is constrained. Previous work on performance improvements of direct sparse solvers on vector supercomputers <ref> [2] </ref> indicates that vectorization can sometimes be used to improve the performance on a vector processor. Therefore, applying vectorization at the processor level within a multi-level algorithm should provide a similar improvement.
Reference: [3] <author> T. Davis. </author> <title> A parallel algorithm for sparse unsymmetric LU factorization. </title> <type> Technical Report CSRD Report No. 907, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1989. </year> <type> PhD. thesis. </type>
Reference-contexts: This strategy provided a means of controlling fillin at the processor level. One other approach to parallel sparse solvers has been the work with parallel pivots <ref> [1, 3] </ref>. These algorithms try to find a set of pivots that can be applied in parallel. As with multifrontal codes, this type of an algorithm does not provide the large grain parallelism to make use of the clusters on Cedar and so it was not used.
Reference: [4] <author> I. S. Duff. </author> <title> Algorithm 575. permutations for a zero-free diagonal. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(3) </volume> <pages> 387-390, </pages> <month> September </month> <year> 1981. </year>
Reference: [5] <author> I. S. Duff. </author> <title> On algorithms for obtaining a maximum transversal. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(3) </volume> <pages> 315-330, </pages> <month> September </month> <year> 1981. </year>
Reference: [6] <author> I. S. Duff. </author> <title> Parallel implementation of multifrontal schemes. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 193-204, </pages> <year> 1986. </year>
Reference-contexts: Some of the methods used for determining partitions and separator sets include one-way dissection and the use of level sets [10, 11]. Another approach that has been tried for parallel sparse system solvers is the multifrontal scheme <ref> [6] </ref>. A multifrontal scheme constructs an elimination tree to organize the parallel work. A node in the tree represents a certain computation, which may include handling the information from the node's children and performing some pivot eliminations.
Reference: [7] <author> Iain S. Duff, Albert M. Erisman, and John K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: The initial phase of the ordering attempts to finds a transversal (an unsymmetric permutation to make all the elements along the diagonal nonzero). If such a permutation is not possible then the matrix is structurally singular <ref> [7] </ref>. The second phase of the ordering applies Tarjan's Algorithm to determine the strongly connected components within the adjacency graph of the matrix. These strongly connected components determine blocks within the matrix. The third phase of the algorithm, H1, is a modified version of Tarjan's algorithm.
Reference: [8] <author> A. George. </author> <title> Nested dissection of a regular finite element mesh. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 10(2) </volume> <pages> 345-363, </pages> <month> April </month> <year> 1973. </year>
Reference-contexts: The dissection algorithm has been used with finite element problems and can be implemented as a one-way algorithm [9] or as a nested algorithm <ref> [8, 11] </ref>. Given the purposes of the H2 algorithm to reduce blocks to below a maximum size, the nested dissection technique was chosen to be implemented.
Reference: [9] <author> A. George. </author> <title> An automatic one-way dissection algorithm for irregular finite element problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17(6) </volume> <pages> 740-751, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: The dissection algorithm has been used with finite element problems and can be implemented as a one-way algorithm <ref> [9] </ref> or as a nested algorithm [8, 11]. Given the purposes of the H2 algorithm to reduce blocks to below a maximum size, the nested dissection technique was chosen to be implemented.
Reference: [10] <author> A. George and J. W. H. Liu. </author> <title> Algorithms for matrix partitioning and the number solution of finite-element systems. </title> <journal> SIAM J. Numerical Analysis, </journal> <volume> 15 </volume> <pages> 297-327, </pages> <year> 1978. </year>
Reference-contexts: Some of the methods used for determining partitions and separator sets include one-way dissection and the use of level sets <ref> [10, 11] </ref>. Another approach that has been tried for parallel sparse system solvers is the multifrontal scheme [6]. A multifrontal scheme constructs an elimination tree to organize the parallel work.
Reference: [11] <author> A. George and J. W. H. Liu. </author> <title> An automatic nested dissection algorithm for irregular finite-element problems. </title> <journal> SIAM J. Numerical Analysis, </journal> <volume> 15 </volume> <pages> 1053-1069, </pages> <year> 1978. </year>
Reference-contexts: Some of the methods used for determining partitions and separator sets include one-way dissection and the use of level sets <ref> [10, 11] </ref>. Another approach that has been tried for parallel sparse system solvers is the multifrontal scheme [6]. A multifrontal scheme constructs an elimination tree to organize the parallel work. <p> The dissection algorithm has been used with finite element problems and can be implemented as a one-way algorithm [9] or as a nested algorithm <ref> [8, 11] </ref>. Given the purposes of the H2 algorithm to reduce blocks to below a maximum size, the nested dissection technique was chosen to be implemented.
Reference: [12] <author> A. George and J. W. H. Liu. </author> <title> An implementation of a pseudoperipheral node finder. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(3) </volume> <pages> 284-295, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: Given the purposes of the H2 algorithm to reduce blocks to below a maximum size, the nested dissection technique was chosen to be implemented. In order to improve the ordering, techniques can be used to improve the choice of the starting nodes <ref> [12, 13] </ref> or techniques can be used to reduce the separator set after it has been selected by exploiting the unsymmetric nature of the matrix. The H2 algorithm has implemented methods to reduce the selected separator set by considering the direction assigned to the edges of the connectivity graph.
Reference: [13] <author> N. E. Gibbs, W. B. Poole, and P. K. Stockmeyer. </author> <title> An algorithm for reducing the bandwidth and profile of a sparse matrix. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 13 </volume> <pages> 236-250, </pages> <year> 1976. </year> <month> 79 </month>
Reference-contexts: Given the purposes of the H2 algorithm to reduce blocks to below a maximum size, the nested dissection technique was chosen to be implemented. In order to improve the ordering, techniques can be used to improve the choice of the starting nodes <ref> [12, 13] </ref> or techniques can be used to reduce the separator set after it has been selected by exploiting the unsymmetric nature of the matrix. The H2 algorithm has implemented methods to reduce the selected separator set by considering the direction assigned to the edges of the connectivity graph.
Reference: [14] <author> J. E. Hopcroft and R. M. Karp. </author> <title> An n 5 2 algorithm for maximum matchings in bipartite graphs. </title> <journal> SIAM Journal on Computing, </journal> <volume> 2(4) </volume> <pages> 225-231, </pages> <month> December </month> <year> 1973. </year>
Reference-contexts: This permutation is a matching between the columns and the diagonals and could be found using many different algorithms. Algorithms for finding set representation [17] or solutions to the assignment problem [16] could be used to find the transversal. Another algorithm involves finding maximal matchings in bipartite graphs <ref> [14] </ref>. The algorithm chosen for the transversal follows the work of Iain Duff [5][4] with the Harwell MA28 package. The algorithm uses a depth first search of the matrix to determine a series of column interchanges, for the transversal.
Reference: [15] <author> D. Kuck, E. Davidson, D. Lawrie, and A. Sameh. </author> <title> Parallel supercomputing today and the Cedar approach. </title> <journal> Science, </journal> <volume> 231 </volume> <pages> 967-974, </pages> <year> 1986. </year>
Reference-contexts: The Cedar architecture is a cluster-based multivector processor <ref> [15] </ref> [20]. It comprises a small number of clusters, presently four, connected to a shared global memory via a two stage omega network. A diagram of the architecture is in Figure 1.1. Each cluster has a small number of vector processors ( 8) which share a large hierarchical cluster memory.
Reference: [16] <author> H. W. Kuhn. </author> <title> The hungarian method for the assignment problem. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 2(1) </volume> <pages> 83-97, </pages> <month> March </month> <year> 1955. </year>
Reference-contexts: This permutation is a matching between the columns and the diagonals and could be found using many different algorithms. Algorithms for finding set representation [17] or solutions to the assignment problem <ref> [16] </ref> could be used to find the transversal. Another algorithm involves finding maximal matchings in bipartite graphs [14]. The algorithm chosen for the transversal follows the work of Iain Duff [5][4] with the Harwell MA28 package.
Reference: [17] <author> Jr. M. Hall. </author> <title> An algorithm for distinct representatives. </title> <journal> The American Mathematical Monthly, </journal> <volume> 63(10) </volume> <pages> 716-717, </pages> <month> December </month> <year> 1956. </year>
Reference-contexts: This permutation is a matching between the columns and the diagonals and could be found using many different algorithms. Algorithms for finding set representation <ref> [17] </ref> or solutions to the assignment problem [16] could be used to find the transversal. Another algorithm involves finding maximal matchings in bipartite graphs [14]. The algorithm chosen for the transversal follows the work of Iain Duff [5][4] with the Harwell MA28 package.
Reference: [18] <author> H. M. Markowitz. </author> <title> The elimination form of the inverse and its application to linear programming. </title> <journal> Management Science, </journal> <volume> 3 </volume> <pages> 255-269, </pages> <month> April </month> <year> 1957. </year>
Reference-contexts: An important part of any sparse solver is the algorithm for controlling the amount of fillin that is generated. Work done with other sparse matrix packages, such as MA28, used a simple strategy proposed by Markowitz <ref> [18] </ref>.
Reference: [19] <author> X. Wang. </author> <title> private communication, </title> <month> April </month> <year> 1991. </year>
Reference-contexts: The nodes could be sorted into increasing or decreasing order by the number of edges from each node or by the Markowitz count for each node. Further work in this area has already shown that the H1 ordering can be improved through the special treatment of dense rows <ref> [19] </ref>. Another possibility is to examine the parameters being used by the ordering and determine if a better set of parameters will produce better results.
Reference: [20] <author> P. Yew. </author> <title> Architecture of the Cedar parallel supercomputer. </title> <type> Technical Report CSRD Report No. 609, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1986. </year> <month> 80 </month>
Reference-contexts: The Cedar architecture is a cluster-based multivector processor [15] <ref> [20] </ref>. It comprises a small number of clusters, presently four, connected to a shared global memory via a two stage omega network. A diagram of the architecture is in Figure 1.1. Each cluster has a small number of vector processors ( 8) which share a large hierarchical cluster memory.
References-found: 20

