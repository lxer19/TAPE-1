URL: ftp://ftp.cs.brown.edu/pub/techreports/94/cs94-44.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-94-44.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [ AC87 ] <author> P. E. Agre and D. Chapman. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 268-272, </pages> <year> 1987. </year>
Reference-contexts: Purely reactive systems do no on-line reasoning about the effects of actions; any implicit or explicit projections are done off-line. Examples of purely reactive systems include Brooks' subsumption architecture [ Bro85 ] and Agre and Chapman's video-game-playing Pengi system <ref> [ AC87 ] </ref> . Another type of reactive system is a universal plan, a concept introduced by Schoppers [ Sch87 ] . Schoppers' approach requires a specification of the domain in terms of a deterministic process, and assumes that the agent can accurately observe the current state at all times.
Reference: [ Asp49 ] <author> A. A. </author> <title> Aspin. </title> _ <journal> Biometrika, </journal> <volume> 36(290), </volume> <year> 1949. </year>
Reference-contexts: Figure 5.13 shows the 70 principal RN1 experiments described earlier. For each one, the performances of 90 over each of the three domain parameters Plexus and those of RTDP were compared using a Welch modified two-sample t-test <ref> [ Asp49 ] </ref> , at the 0.05 significance level. Those for which RTDP performed significantly better are marked with a star; those for which Plexus performed significantly better are marked with a ; the three domains for which neither was significantly better than the other have been omitted.
Reference: [ Bas92 ] <author> Kenneth Basye. </author> <title> A framework for map construction. </title> <type> Technical report, </type> <institution> Brown University Department of Computer Science, Providence, RI, </institution> <year> 1992. </year>
Reference-contexts: B.3.2 Robot Navigation (RN2) Motivation This type of domain, which I refer to as RN2, model a mobile robot moving around a static office-building environment. The design of the domains was based on experience with a Real World Interface mobile robot base equipped with sonars as its only sensor <ref> [ KBD91, DCK90, Bas92 ] </ref> . One of the main problems with navigation using mobile robots is the speed with which small errors in dead reckoning accumulate, due to irregularities in the floor surface, wheel slippage, or misalignment of the wheels.
Reference: [ BBS93 ] <author> Andrew G. Barto, Steven J. Bradtke, and Satinder P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <note> Submitted to AIJ Special Issue on Computational Theories of Interaction and Agency, </note> <year> 1993. </year>
Reference-contexts: Although many AI researchers propose strategies for attacking real-time planning problems (e.g., [ DB90 ] , <ref> [ BBS93 ] </ref> , [ Sut90 ] ), there has been very little comparative work, and few attempts to define the range of problems for which these techniques are most effective. 1.1 Example: traffic control In order to make the discussion that follows more concrete, consider the following example of a <p> There is a growing interest in techniques that learn a model of the environment while using an approximate model (see, e.g., <ref> [ BBS93 ] </ref> , [ SPS94 ] ); some of these techniques are applicable to the system described here. * Constant-time actions The time taken to make a transition from one state to another is assumed to be fixed and independent both of the original state and the action taken. <p> It gives this policy to the agent, and thereafter does nothing. It is called conservative because it does not commit to anything until it is sure it has the optimal policy. 61 * RTDP RTDP is an implementation of the real-time dynamic programming system suggested by Barto et al <ref> [ BBS93 ] </ref> ; it uses a modified version of value iteration combined with an iterative-deepening search to approximate the values of states (the expected discounted reward starting from each state), and uses a greedy policy (choosing the action that maximizes the expected value). * Recover, replan Recover and replan are <p> In this section I compare the Plexus planner (plexus), a planner using real-time dynamic programming based on Barto et al <ref> [ BBS93 ] </ref> (rtdp), the expected performance of a pre-computed optimal policy (optimal), and the expected performance of a planner that executes random actions while it computes an optimal policy, and then executes the optimal policy (cautious). first graph, domains were grouped by width, and mean performances plotted within each group. <p> The agent thus uses an anytime algorithm: it finds approximate solutions early, and continually refines them as long as time is available. I have shown that in most cases Plexus out-performs the other planners with which I compared it: RTDP <ref> [ BBS93 ] </ref> , recover and replan (two search-based planners). In certain types of problems, the search-based planners perform better; these problems are typically the least difficult (i.e., the ones with the best performance under an optimal policy). <p> B.2 Agents and worlds I have implemented a number of different agents and worlds in order to conduct the experiments described in this dissertation; they are described below. B.2.1 RTDP RTDP implements the real-time dynamic programming algorithm presented by Barto, Bradtke and Singh in <ref> [ BBS93 ] </ref> . This agent requires a world model, as defined below. I present some comparisons of the performance of RTDP and the Plexus planner in Section 5. <p> B.3.3 Racetrack (RA) Motivation The racetrack domains are based on a game called Race Track described by Martin Gardner [ Gar73 ] , and adapted to the Markov decision problem formalism by Barto, Bradtke and Singh <ref> [ BBS93 ] </ref> . The problem models a race car moving on a track discretized into a grid; a state of the world encodes the location of the car and its velocity vector. The agent controls the race car's acceleration, not its speed directly.
Reference: [ BC89 ] <author> D. P. Bertsekas and D. A. Castanon. </author> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34(6) </volume> <pages> 589-598, </pages> <year> 1989. </year>
Reference-contexts: Researchers in the field of operations research field have studied aggregation techniques in which states are clustered according to estimates of their value, rather than by state variable. Bertsekas and Casta~non <ref> [ BC89 ] </ref> present one method that is applicable to very large state spaces. Kushner and Kleinman [ KK71 ] present a method based on linear programming that does aggregation on dis-cretized versions of continuous spaces. <p> A natural extension to the set of phases for Plexus would implement this combination. Other interesting improvements to value iteration and policy iteration algorithms have been presented by, e.g., Dearden and Boutil-lier [ BD94 ] , Bertsekas and Casta~non <ref> [ BC89 ] </ref> , Kushner and Kleinman [ KK71 ] . 3.6.1 Description Value iteration is a technique for determining an optimal policy for a set of states. We start with arbitrary values associated with each state, and an arbitrary policy .
Reference: [ BD94 ] <author> Craig. Boutilier and Richard Dearden. </author> <title> Using abstractions for decision theoretic planning with time constraints. </title> <booktitle> In Proceedings AAAI-94. AAAI, </booktitle> <year> 1994. </year>
Reference-contexts: Several different approaches have been taken. Dearden and Boutillier <ref> [ BD94 ] </ref> consider problems in which the states are represented as vectors of state variables. They cluster states by ignoring certain state variables, and then determine an optimal policy on the decision problem involving the clustered states. <p> A natural extension to the set of phases for Plexus would implement this combination. Other interesting improvements to value iteration and policy iteration algorithms have been presented by, e.g., Dearden and Boutil-lier <ref> [ BD94 ] </ref> , Bertsekas and Casta~non [ BC89 ] , Kushner and Kleinman [ KK71 ] . 3.6.1 Description Value iteration is a technique for determining an optimal policy for a set of states. We start with arbitrary values associated with each state, and an arbitrary policy .
Reference: [ Bel57 ] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: For com parison, the values for the optimal policy over the entire state space are given in to the goal, the value is 1=(1 fl). 46 47 3.6 Value Iteration Value iteration <ref> [ Bel57 ] </ref> is one of the three standard techniques for computing an optimal policy for a Markov decision problem, linear programming and policy iteration (described in Section 3.5) being the others.
Reference: [ Ber76 ] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming and Stochastic Control. </title> <publisher> Academic Press, </publisher> <year> 1976. </year>
Reference-contexts: system moves through the state space over time, or because different goals are posited at different times), there are too many possible conditions to consider them all at once, and some degree of reactivity is required. 2 Note that some authors, e.g., Kemeny and Snell [ KS60 ] , Bertsekas <ref> [ Ber76 ] </ref> , use the term ergodic set for this concept. <p> Problems in which the system is halted after reaching one of a set of states (first-passage problems), however, are amenable to the techniques presented here, as shown in Section 3.11.1. Finite horizon problems have been extensively studied in the stochastic control literature (see e.g., <ref> [ Der70, Ber76, HS82 ] </ref> ), and in principle the techniques described in this literature could be applied to the planning system described here. <p> The procedure is defined in Section 3.5.2. Similar proofs of this theorem can be found in many standard textbooks on Markov processes, e.g., Derman [ Der70 ] , Bertsekas <ref> [ Ber76 ] </ref> . I present a proof here for completeness. A.1.1 Policy iteration over entire state space Theorem 1 If is a (complete) policy, the policy iteration procedure terminates in a finite number of steps, with an optimal policy under the infinite horizon discounted value definition of optimality.
Reference: [ Bro85 ] <author> Rodney Brooks. </author> <title> A Robust Layered Control System for a Mobile Robot. </title> <institution> Massachusetts Institute of Technology Artificial Intelligence Laboratory, MITAIL, </institution> <month> September </month> <year> 1985. </year>
Reference-contexts: The approaches mentioned above are generally not applicable in this type of domain, as they cannot guarantee timely responses. Purely reactive systems do no on-line reasoning about the effects of actions; any implicit or explicit projections are done off-line. Examples of purely reactive systems include Brooks' subsumption architecture <ref> [ Bro85 ] </ref> and Agre and Chapman's video-game-playing Pengi system [ AC87 ] . Another type of reactive system is a universal plan, a concept introduced by Schoppers [ Sch87 ] .
Reference: [ CKL94a ] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: This assumption is unrealistic in many domains, and is made here for simplicity with the hope that future work will allow it to be relaxed. Recent work by Cassandra, Kaelbling and Littman discusses techniques for dealing with this case; see, e.g., <ref> [ CKL94a ] </ref> , [ CKL94b ] . * Infinite horizon The accumulated reward is to be maximized over an infinite horizon. That is, I do not address problems in which the system will be halted after a fixed number of steps. <p> Their approach is more general than ours, in the sense that it does not assume complete observability of state. It is basically equivalent to the partially observable Markov decision process (POMDP) formalism, which has been used for AI planning problems by Cassandra, Kaelbling and Littman <ref> [ CKL94a ] </ref> ; the main difference is the compact STRIPS-like representation of the problem that C-BURIDAN uses, whereas the classical POMDP representation can be 19 prohibitively large.
Reference: [ CKL94b ] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Algorithms for partially observable markov decision processes. </title> <type> Technical Report Forthcoming, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1994. </year>
Reference-contexts: This assumption is unrealistic in many domains, and is made here for simplicity with the hope that future work will allow it to be relaxed. Recent work by Cassandra, Kaelbling and Littman discusses techniques for dealing with this case; see, e.g., [ CKL94a ] , <ref> [ CKL94b ] </ref> . * Infinite horizon The accumulated reward is to be maximized over an infinite horizon. That is, I do not address problems in which the system will be halted after a fixed number of steps.
Reference: [ DB88 ] <author> Thomas Dean and Mark Boddy. </author> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings AAAI-88, </booktitle> <pages> pages 49-54. </pages> <publisher> AAAI, </publisher> <year> 1988. </year>
Reference-contexts: RTA? is a modification of the heuristic search algorithm A? to do intelligent back-tracking, and to 21 provide anytime solutions | successive approximations that improve over time (the term anytime is due to Dean and Boddy <ref> [ DB88 ] </ref> . Under a few reasonable assumptions, RTA? is guaranteed to eventually find a solution to the problem (a sequence of state-action pairs that leads to a goal state).
Reference: [ DB90 ] <author> Mark Drummond and John Bresina. </author> <title> Anytime synthetic projection: Maximizing the probability of goal satisfaction. </title> <booktitle> In Proceedings AAAI-90, </booktitle> <pages> pages 138-144. </pages> <publisher> AAAI, </publisher> <year> 1990. </year>
Reference-contexts: Although many AI researchers propose strategies for attacking real-time planning problems (e.g., <ref> [ DB90 ] </ref> , [ BBS93 ] , [ Sut90 ] ), there has been very little comparative work, and few attempts to define the range of problems for which these techniques are most effective. 1.1 Example: traffic control In order to make the discussion that follows more concrete, consider the <p> As with Plexus, CIRCA requires complete state observability. An additional assumption made in CIRCA that is difficult to justify is that given a state, the best action can be chosen with no lookahead. This is used to avoid searching the space of all possible action sequences. Drummond and Bresina <ref> [ DB90 ] </ref> present a technique called anytime synthetic projection that is similar in many respects to the ideas behind Plexus; the original idea for Plexus' strengthening came from Drummond and Bresina's work. 2.3.6 Abstraction There is a growing recognition of the need for abstractions to reduce the size of the <p> The three are grouped together under the umbrella of strengthening for technical reasons: the computations required by the algorithms are most efficiently combined. Adding states from the accessible fringe is similar to the "robustification" described by Drummond and Bresina <ref> [ DB90 ] </ref> (In earlier papers describing Plexus we called this phase "robustify"; the name was changed for aesthetic reasons.) 3.4.2 Algorithm We wish to estimate the probability that the system will be in the various possible states when the policy currently being used is replaced by a new policy (the
Reference: [ DCK90 ] <author> Thomas Dean, Theodore Camus, and Jak Kirman. </author> <title> Sequential decision making for active perception. </title> <booktitle> In Proceedings of the DARPA Image Understanding Workshop, </booktitle> <pages> pages 889-894. DARPA, </pages> <year> 1990. </year>
Reference-contexts: B.3.2 Robot Navigation (RN2) Motivation This type of domain, which I refer to as RN2, model a mobile robot moving around a static office-building environment. The design of the domains was based on experience with a Real World Interface mobile robot base equipped with sonars as its only sensor <ref> [ KBD91, DCK90, Bas92 ] </ref> . One of the main problems with navigation using mobile robots is the speed with which small errors in dead reckoning accumulate, due to irregularities in the floor surface, wheel slippage, or misalignment of the wheels.
Reference: [ Der70 ] <author> Cyrus Derman. </author> <title> Finite State Markovian Decisian Processes. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: Problems in which the system is halted after reaching one of a set of states (first-passage problems), however, are amenable to the techniques presented here, as shown in Section 3.11.1. Finite horizon problems have been extensively studied in the stochastic control literature (see e.g., <ref> [ Der70, Ber76, HS82 ] </ref> ), and in principle the techniques described in this literature could be applied to the planning system described here. <p> The procedure is defined in Section 3.5.2. Similar proofs of this theorem can be found in many standard textbooks on Markov processes, e.g., Derman <ref> [ Der70 ] </ref> , Bertsekas [ Ber76 ] . I present a proof here for completeness. <p> For Markov decision problems with a finite number of states and a finite number of actions, using the discounted infinite horizon optimality criterion, there exists an optimal policy that is deterministic and stationary (independent of the past history of the system). 137 For a proof of this, see, e.g., Derman <ref> [ Der70 ] </ref> . Since the domain-task formulation of the problem is equivalent to the MDP formulation (see Section 2.1), there exists an optimal policy for any domain-task problem.
Reference: [ DHW94 ] <author> Denise Draper, Steve Hanks, and Daniel Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning, </booktitle> <year> 1994. </year>
Reference-contexts: This is clearly undesirable; if low-probability unpleasant (resulting in large negative rewards) events are modeled, the agent will behave overly cautiously; if the events are not modeled, the model is inaccurate in an important way. 2.3.3 Probabilistic planning Draper, Hanks and Weld <ref> [ DHW94 ] </ref> present an algorithm for probabilistic planning called C-BURIDAN, based on the BURIDAN algorithm by Kushmerick et al. [ KHW94 ] . Their approach is more general than ours, in the sense that it does not assume complete observability of state.
Reference: [ DKKN93a ] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Deliberation scheduling for time-critical sequential decision making. </title> <booktitle> In Eighth Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1993. </year>
Reference-contexts: There are considerable differences between the version of this algorithm used in the original version of Plexus (described in [ DKKN94 ] , <ref> [ DKKN93a ] </ref> , and the algorithm described here. A detailed list of differences is given in Section 3.10. <p> The full fringe parameter was turned off also, since it tended to expand parts of the envelope that were too distant to be of great use. 3.10 Differences from original version There are significant differences between Plexus as described here and the versions described in earlier papers referring to it <ref> [ DKKN93a, DKKN93b, DKKN94 ] </ref> . 54 3.10.1 Strengthen In the original implementation of Plexus, the strengthen phase was given a single parameter indicating the number of states to add to the envelope of the policy.
Reference: [ DKKN93b ] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In AAAI-93. AAAI, </booktitle> <year> 1993. </year>
Reference-contexts: The first version of Plexus (see <ref> [ DKKN93b, DKKN94 ] </ref> ) was the outcome of work done jointly with Tom Dean, Leslie Kaelbling and Ann Nicholson. <p> We have used this flexibility to create planners that do deliberation scheduling | reasoning about the duration of the planning cycle itself <ref> [ DKKN93b ] </ref> . This makes it possible to trade off the advantages of producing a better policy against the disadvantage of executing the current, poorer, policy. <p> This cycle continues until the system reaches a termination state. 32 I have chosen to restrict my attention to phase-cycle planners. The main reason for this is that the deliberation scheduling techniques described in Dean et al <ref> [ DKKN93b ] </ref> require large amounts of off-line computation for each domain, making a survey of a large number of different domains infeasible. <p> The full fringe parameter was turned off also, since it tended to expand parts of the envelope that were too distant to be of great use. 3.10 Differences from original version There are significant differences between Plexus as described here and the versions described in earlier papers referring to it <ref> [ DKKN93a, DKKN93b, DKKN94 ] </ref> . 54 3.10.1 Strengthen In the original implementation of Plexus, the strengthen phase was given a single parameter indicating the number of states to add to the envelope of the policy. <p> moment, no flow-control is provided; this would be a useful addition, since conditioning phase execution on results obtained during previous phases would be a simple step towards the more general deliberation-scheduling 153 Figure B.2: Possible outcomes of an action in the RN1 domain planners described in earlier Plexus papers (e.g., <ref> [ DKKN93b ] </ref> ). The phases currently implemented are: strengthen, prune, explore, policy iteration and value iteration, described in Chapter 3. B.3 Domains B.3.1 Simple Robot Navigation (RN1) The RN1 class of domains is designed for testing agents; they model a robot moving in an uncluttered area.
Reference: [ DKKN94 ] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <note> in press, </note> <year> 1994. </year>
Reference-contexts: The first version of Plexus (see <ref> [ DKKN93b, DKKN94 ] </ref> ) was the outcome of work done jointly with Tom Dean, Leslie Kaelbling and Ann Nicholson. <p> We assume a uniform probability over the actions other than the one specified by the policy, and look for outcomes that have higher probability than a specified threshold. There are considerable differences between the version of this algorithm used in the original version of Plexus (described in <ref> [ DKKN94 ] </ref> , [ DKKN93a ] , and the algorithm described here. A detailed list of differences is given in Section 3.10. <p> The full fringe parameter was turned off also, since it tended to expand parts of the envelope that were too distant to be of great use. 3.10 Differences from original version There are significant differences between Plexus as described here and the versions described in earlier papers referring to it <ref> [ DKKN93a, DKKN93b, DKKN94 ] </ref> . 54 3.10.1 Strengthen In the original implementation of Plexus, the strengthen phase was given a single parameter indicating the number of states to add to the envelope of the policy. <p> For comparison, I will present the optimal performance (the expected discounted sum of rewards for an optimal policy) and the performance of the planner called "cautious" in previous work <ref> [ DKKN94 ] </ref> ; it first computes the optimal policy using policy iteration and then executes that policy.
Reference: [ Gar73 ] <author> Martin Gardner. </author> <title> Mathematical games. </title> <publisher> Scientific American, </publisher> <address> (228:108), </address> <month> January </month> <year> 1973. </year>
Reference-contexts: This is therefore an approximation of the true performance measure, but in practice it is an accurate approximation. B.3.3 Racetrack (RA) Motivation The racetrack domains are based on a game called Race Track described by Martin Gardner <ref> [ Gar73 ] </ref> , and adapted to the Markov decision problem formalism by Barto, Bradtke and Singh [ BBS93 ] . The problem models a race car moving on a track discretized into a grid; a state of the world encodes the location of the car and its velocity vector.
Reference: [ Gre93 ] <author> William H. Greene. </author> <title> Econometric analysis. </title> <publisher> Macmillan, </publisher> <address> New York, New York, </address> <year> 1993. </year>
Reference-contexts: Various measures of this accuracy have been proposed (see, e.g., <ref> [ Gre93 ] </ref> ). One is the root mean-squared error, defined by s X (y i ^y i ) 2 ; where the y i are the actual measurements of the response, and ^y i are the predicted responses.
Reference: [ Heg94 ] <author> Matthias Heger. </author> <title> Consideration of risk in reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 105-111, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The operations research community has long been aware that expected reward is often not the best criterion to use. Sometimes it is more desirable to have a guaranteed lower bound on accumulated reward, and sometimes one would like to incorporate a notion of risk-aversion or risk-preference. Heger <ref> [ Heg94 ] </ref> discusses the various options, and provides an algorithm similar to Q-learning [ Wat89 ] that uses worst-case total discounted costs. I consider only the risk-neutral case, where the expected accumulated discounted reward is optimized.
Reference: [ How66 ] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <year> 1966. </year>
Reference-contexts: Others, e.g., Karlin and Taylor, [ KT74 ] ) use ergodic to mean aperiodic and recurrent, so to avoid confusion I follow Howard <ref> [ How66 ] </ref> in using the term recurrent. 15 The current implementation of Plexus imposes some restrictions on the planning problems it operates on. Some of the restrictions are a consequence of the theory underlying the approach, and relaxing them would require substantial modifications to the techniques I describe below. <p> The result of strengthening is shown in Figure 3.11. The actions are not shown here, as they are dependent on the previous values of the states. 39 3.5 Policy Iteration 3.5.1 Description The policy iteration phase uses a variation of Howard's policy iteration algorithm <ref> [ How66 ] </ref> to improve a policy. First, I will present the algorithm in the case where the envelope is the entire state space; the optimization phase then creates an optimal complete policy. <p> This fact is not self-evident; a proof is given in Appendix A, as one of the steps in the proof that policy iteration converges to an optimal policy in a finite number of steps. The algorithm is very similar to the standard policy iteration algorithm due to Howard <ref> [ How66 ] </ref> ; it differs in that it may make several changes to the policy before recomputing the values of the states.
Reference: [ HS82 ] <author> Daniel P. Heyman and Matthew J. Sobel. </author> <title> Stochastic Models in Operations Research (Volume II). </title> <publisher> McGraw-Hill, </publisher> <year> 1982. </year> <month> 164 </month>
Reference-contexts: Problems in which the system is halted after reaching one of a set of states (first-passage problems), however, are amenable to the techniques presented here, as shown in Section 3.11.1. Finite horizon problems have been extensively studied in the stochastic control literature (see e.g., <ref> [ Der70, Ber76, HS82 ] </ref> ), and in principle the techniques described in this literature could be applied to the planning system described here.
Reference: [ JF92 ] <author> A. Jalali and M. J. Ferguson. </author> <title> Computationally efficient algorithms for on-line optimization of markov decision processes. </title> <journal> Automatica, </journal> <volume> 28(1) </volume> <pages> 107-118, </pages> <year> 1992. </year>
Reference-contexts: Broadly speaking, value iteration is 22 inefficient because it updates all states in the domain equally often; policy itera-tion is inefficient both because it requires solving a set of S linear equations for each state updated, and because it only updates one state at a time. Jalali and Ferguson <ref> [ JF92 ] </ref> give an asynchronous variant of value iteration that updates the states with larger values more frequently than those with lower values (using the average-reward measure of optimality).
Reference: [ JS88 ] <author> Mark Jerrum and Alistair Sinclair. </author> <title> Conductance and the rapid mixing property for markov chains: the approximation of the permanent resolved. </title> ?, <year> 1988. </year>
Reference-contexts: The n-step controllability of the domain is the average of the n-step controllabilities of the states in the domain. * Conductance, merging conductance Jerrum and Sinclair <ref> [ JS88 ] </ref> and Mihail ( [ Mih89 ] ) discuss related concepts for Markov chains, called the conductance and merging conductance; however, these do not have obvious analogs in the general Markov decision problem case, and are very expensive to compute. 73 Chapter 5 Experimental results In this chapter I
Reference: [ KBD91 ] <author> Jak Kirman, Kenneth Basye, and Thomas Dean. </author> <title> Sensor abstractions for control of navigation. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 2812-2817, </pages> <year> 1991. </year>
Reference-contexts: B.3.2 Robot Navigation (RN2) Motivation This type of domain, which I refer to as RN2, model a mobile robot moving around a static office-building environment. The design of the domains was based on experience with a Real World Interface mobile robot base equipped with sonars as its only sensor <ref> [ KBD91, DCK90, Bas92 ] </ref> . One of the main problems with navigation using mobile robots is the speed with which small errors in dead reckoning accumulate, due to irregularities in the floor surface, wheel slippage, or misalignment of the wheels.
Reference: [ KHW94 ] <author> Nicholas Kushmerick, Steve Hanks, and Daniel Weld. </author> <title> An algorithm for probabilistic planning. </title> <booktitle> In Proceedings AAAI-94. AAAI, </booktitle> <year> 1994. </year>
Reference-contexts: events are modeled, the agent will behave overly cautiously; if the events are not modeled, the model is inaccurate in an important way. 2.3.3 Probabilistic planning Draper, Hanks and Weld [ DHW94 ] present an algorithm for probabilistic planning called C-BURIDAN, based on the BURIDAN algorithm by Kushmerick et al. <ref> [ KHW94 ] </ref> . Their approach is more general than ours, in the sense that it does not assume complete observability of state.
Reference: [ KK71 ] <author> H. J. Kushner and A. J. Kleinman. </author> <title> Mathematical programming and the control of markov chains. </title> <journal> International Journal of Control, </journal> <volume> 13(5) </volume> <pages> 801-820, </pages> <year> 1971. </year>
Reference-contexts: Researchers in the field of operations research field have studied aggregation techniques in which states are clustered according to estimates of their value, rather than by state variable. Bertsekas and Casta~non [ BC89 ] present one method that is applicable to very large state spaces. Kushner and Kleinman <ref> [ KK71 ] </ref> present a method based on linear programming that does aggregation on dis-cretized versions of continuous spaces. <p> A natural extension to the set of phases for Plexus would implement this combination. Other interesting improvements to value iteration and policy iteration algorithms have been presented by, e.g., Dearden and Boutil-lier [ BD94 ] , Bertsekas and Casta~non [ BC89 ] , Kushner and Kleinman <ref> [ KK71 ] </ref> . 3.6.1 Description Value iteration is a technique for determining an optimal policy for a set of states. We start with arbitrary values associated with each state, and an arbitrary policy .
Reference: [ Kno91 ] <author> Craig A. Knoblock. </author> <title> Search reduction in hierarchical problem solving. </title> <booktitle> In Proceedings AAAI-91, </booktitle> <pages> pages 686-691. </pages> <publisher> AAAI, </publisher> <year> 1991. </year>
Reference-contexts: Kushner and Kleinman [ KK71 ] present a method based on linear programming that does aggregation on dis-cretized versions of continuous spaces. Other techniques for abstraction in deterministic domains have been studied by Lin and Dean [ LD94 ] and Knoblock <ref> [ Kno91 ] </ref> . 2.3.7 Learning Several groups in the learning community have addressed the problem of finding good policies in stochastic domains, using a model of the problem (usually as a Markov Decision Process).
Reference: [ Kor87 ] <author> Richard E. Korf. </author> <title> Real-time heuristic search: </title> <booktitle> First results. </booktitle> <pages> pages 133-138, </pages> <year> 1987. </year>
Reference-contexts: In some cases the model is assumed to be known, in others it is learned on-line. Korf <ref> [ Kor87 ] </ref> presented an algorithm called RTA? (Real-Time A?) to do concurrent planning and execution in deterministic domains.
Reference: [ KS60 ] <author> J. G. Kemeny and J. L. Snell. </author> <title> Finite Markov Chains. </title> <address> D. </address> <publisher> Van Nostrand, </publisher> <address> New York, </address> <year> 1960. </year>
Reference-contexts: change over time (because the system moves through the state space over time, or because different goals are posited at different times), there are too many possible conditions to consider them all at once, and some degree of reactivity is required. 2 Note that some authors, e.g., Kemeny and Snell <ref> [ KS60 ] </ref> , Bertsekas [ Ber76 ] , use the term ergodic set for this concept.
Reference: [ KS76 ] <author> John G. Kemeny and J. Laurie Snell. </author> <title> Finite Markov Chains. </title> <publisher> Springer-Verlag, </publisher> <year> 1976. </year>
Reference-contexts: An absorbing Markov chain is a Markov chain in which every recurrent set consists of a single absorbing state. Absorbing chains have useful computational properties that I will exploit throughout this thesis. For more extensive definitions of Markov chains and associated notions, see, e.g., Kemeny and Snell <ref> [ KS76 ] </ref> . 2.2 Restrictions The approach taken by Plexus is not intended to be applicable to all planning problems; it is specifically designed for problems in which uncertainty of the effects of actions, or uncertainty about external events, play a major role in the evolution of the environment.
Reference: [ KT74 ] <author> Samuel Karlin and Howard M. Taylor. </author> <title> A First Course in Stochastic Processes. </title> <publisher> Academic Press, </publisher> <address> Sand Diego, California, </address> <note> second edition edition, </note> <year> 1974. </year>
Reference-contexts: Others, e.g., Karlin and Taylor, <ref> [ KT74 ] </ref> ) use ergodic to mean aperiodic and recurrent, so to avoid confusion I follow Howard [ How66 ] in using the term recurrent. 15 The current implementation of Plexus imposes some restrictions on the planning problems it operates on.
Reference: [ LD94 ] <author> Shieu-Hong Lin and Thomas Dean. </author> <title> Exploiting locality in temporal reasoning. </title> <editor> In E. Sandewall and C. Backstrom, editors, </editor> <booktitle> Current Trends in AI Planning, </booktitle> <address> Amsterdam, 1994. </address> <publisher> IOS Press. </publisher>
Reference-contexts: Kushner and Kleinman [ KK71 ] present a method based on linear programming that does aggregation on dis-cretized versions of continuous spaces. Other techniques for abstraction in deterministic domains have been studied by Lin and Dean <ref> [ LD94 ] </ref> and Knoblock [ Kno91 ] . 2.3.7 Learning Several groups in the learning community have addressed the problem of finding good policies in stochastic domains, using a model of the problem (usually as a Markov Decision Process).
Reference: [ LDW89 ] <author> John J. Leonard and Hugh F. Durrant-Whyte. </author> <title> Active sensor control for mobile robotics. </title> <type> Technical Report OUEL-1756/89, </type> <institution> Oxford University Robotics Research Group, </institution> <year> 1989. </year>
Reference-contexts: One of the main problems with navigation using mobile robots is the speed with which small errors in dead reckoning accumulate, due to irregularities in the floor surface, wheel slippage, or misalignment of the wheels. Analysis of the sonar readings in an office building <ref> [ LDW89 ] </ref> is more reliable when the sonars are known to be roughly perpendicular to a flat surface, since deviations from the normal will lead to specular reflections, giving wildly incorrect results.
Reference: [ MA93 ] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> Memory-based reinforcement learning: Efficient computation with prioritized sweeping. </title> <booktitle> In Advances in Neural Information Processing 5, </booktitle> <address> San Mateo, California, 1993. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 165 </pages>
Reference-contexts: The performance of Dyna degrades with large state spaces, since it examines all states. Peng and Williams [ PW93 ] suggest techniques for improving the performance of Dyna, for example by ascribing priorities to the nodes on the search boundary. Moore and Atkeson <ref> [ MA93 ] </ref> present a similar technique called prioritized sweeping. Much of the work in the area of learning Markov decision processes is based on Watkins' Q-learning [ Wat89 ] . Q-learning is a process for updating estimates of the values of each state-action pair as transitions are made. <p> This agent requires a world model, as defined below. I present some comparisons of the performance of RTDP and the Plexus planner in Section 5. My implementation of RTDP is very simple, and its performance could be greatly improved by techniques such as prioritized sweeping <ref> [ MA93 ] </ref> . RTDP starts by assigning an optimistic value to each state. In all the cases I discuss, rewards are negative, so we ascribe a value of 0 to every state. Then RTDP goes into a cycle of trials.
Reference: [ MDS93a ] <author> David Musliner, Eduund Durfee, and Kang Shin. </author> <title> Circa: A cooperative intelligent real-time control architecture. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23(6), </volume> <year> 1993. </year>
Reference-contexts: Sanborn [ San88 ] uses an approach he calls dynamic reaction. He assumes that the domain is not fully known in advance, and uses monitors to detect changes in the environment that affect the current plan, replanning if necessary. CIRCA (Musliner, Durfee, Shin) <ref> [ MDS93a, MDS93b ] </ref> is a real-time system connected to a planner in a manner similar to Plexus. However, CIRCA does not explicitly deal with the uncertainty in the model; it assumes the worst-case 20 behaviour whenever there is a doubt.
Reference: [ MDS93b ] <author> David Musliner, Eduund Durfee, and Kang Shin. </author> <title> World modeling for the dynamic construction of real-time control plans. </title> <note> submitted to AIJ, </note> <year> 1993. </year>
Reference-contexts: Sanborn [ San88 ] uses an approach he calls dynamic reaction. He assumes that the domain is not fully known in advance, and uses monitors to detect changes in the environment that affect the current plan, replanning if necessary. CIRCA (Musliner, Durfee, Shin) <ref> [ MDS93a, MDS93b ] </ref> is a real-time system connected to a planner in a manner similar to Plexus. However, CIRCA does not explicitly deal with the uncertainty in the model; it assumes the worst-case 20 behaviour whenever there is a doubt.
Reference: [ Mih89 ] <author> Milena Mihail. </author> <title> Conductance and convergence of markov chains | a combinatorial treatment of expanders. </title> <booktitle> Proceedings of the 31st ACM Symposium on Foundations of Computer Science, </booktitle> <pages> pages 526-531, </pages> <year> 1989. </year>
Reference-contexts: The n-step controllability of the domain is the average of the n-step controllabilities of the states in the domain. * Conductance, merging conductance Jerrum and Sinclair [ JS88 ] and Mihail ( <ref> [ Mih89 ] </ref> ) discuss related concepts for Markov chains, called the conductance and merging conductance; however, these do not have obvious analogs in the general Markov decision problem case, and are very expensive to compute. 73 Chapter 5 Experimental results In this chapter I present experimental results of the performance
Reference: [ NK94 ] <author> Ann Nicholson and Leslie Pack Kaelbling. </author> <title> Toward approximate planning in very large stochastic domains. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning, </booktitle> <address> Stanford, California, </address> <year> 1994. </year>
Reference-contexts: They cluster states by ignoring certain state variables, and then determine an optimal policy on the decision problem involving the clustered states. As time permits, they cluster states into smaller and smaller groups, giving a progressively more accurate approximation. Nicholson and Kaelbling <ref> [ NK94 ] </ref> use a method similar to that of Dearden and Boutillier; they form approximate models by ignoring state variables, and provide techniques for automatically determining which variables should be ignored.
Reference: [ Put94 ] <author> M. L. Puterman. </author> <title> Markov Decision Processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This allows problems to be specified without discounting, yet distinguishes between policies that have the same value asymptotically, based on whether they accumulate reward quickly or slowly. There are a number of other optimality criteria; Puterman <ref> [ Put94 ] </ref> gives a good overview of them. 2.3.8 Policy improvement There has been a great deal of work on various forms of policy improvement for Markov decision processes, both in operations research and in machine learning. <p> As with policy iteration, the procedure alternates between value computation and policy improvement, but value computation is not performed exactly. A sequence of value iteration steps are performed, giving a closer estimate to the values of the states, but the sequence is halted before complete convergence. Puterman <ref> [ Put94 ] </ref> proves that modified policy iteration converges strictly faster than value iteration, and gives empirical evidence that, in practice, it is significantly faster than either value iteration or policy iteration. 23 Chapter 3 Plexus This chapter describes the Plexus planning and execution system. <p> The conditions under which value iteration performs better than policy iteration, or vice versa, are understood theoretically only in very limited circumstances; empirically, combinations of value iteration and policy iteration perform better than either basic technique. Puterman <ref> [ Put94 ] </ref> has shown that a particular combination, called modified policy iteration, will converge more quickly than simple value iteration. A natural extension to the set of phases for Plexus would implement this combination.
Reference: [ PW93 ] <author> Jing Peng and Ronald J. Williams. </author> <title> Efficient learning and planning within the dyna framework. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1(4) </volume> <pages> 437-454, </pages> <year> 1993. </year>
Reference-contexts: The performance of Dyna degrades with large state spaces, since it examines all states. Peng and Williams <ref> [ PW93 ] </ref> suggest techniques for improving the performance of Dyna, for example by ascribing priorities to the nodes on the search boundary. Moore and Atkeson [ MA93 ] present a similar technique called prioritized sweeping.
Reference: [ R 84 ] <author> Alfred Renyi. </author> <title> A Diary on Information Theory. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, New York, </address> <year> 1984. </year>
Reference-contexts: To show the order of magnitude of the sizes of the different problems, here is a log-scale plot of some of the example problems I discuss. 63 64 4.3.3 Entropy The entropy (see, for example, Renyi <ref> [ R 84 ] </ref> ) of a random variable x ranging over a discrete state space S is defined as H (x) = s2S Entropy is a characterization of the uncertainty in a probability distribution.
Reference: [ Ros70 ] <author> Sheldon M. Ross. </author> <title> Applied Probability Models with Optimization Applications. </title> <publisher> Holden-Day, </publisher> <year> 1970. </year>
Reference-contexts: Relaxing this assumption would require modeling the problem as a semi-Markov decision process. There is some literature on this subject (see, e.g., <ref> [ Ros70 ] </ref> ), but it is beyond the scope of this dissertation.
Reference: [ San88 ] <author> James C. Sanborn. </author> <title> A model of reaction for planning in dynamic environments. </title> <year> 1988. </year>
Reference-contexts: The assumptions that are relaxed are: complete ob-servability, deterministic outcomes and unlimited reasoning time. Sanborn <ref> [ San88 ] </ref> uses an approach he calls dynamic reaction. He assumes that the domain is not fully known in advance, and uses monitors to detect changes in the environment that affect the current plan, replanning if necessary.
Reference: [ Sch87 ] <author> Marcel J. Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <year> 1987. </year>
Reference-contexts: Examples of purely reactive systems include Brooks' subsumption architecture [ Bro85 ] and Agre and Chapman's video-game-playing Pengi system [ AC87 ] . Another type of reactive system is a universal plan, a concept introduced by Schoppers <ref> [ Sch87 ] </ref> . Schoppers' approach requires a specification of the domain in terms of a deterministic process, and assumes that the agent can accurately observe the current state at all times.
Reference: [ Sch93 ] <author> Anton Schwartz. </author> <title> A reinforcement learning method for maximizing undis-counted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference in Machine Learning, </booktitle> <pages> pages 298-305, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The most interesting aspect of Q-learning is that it simultaneously learns the values associated with state-action pairs and converges to an optimal policy, without ever explicitly formulating a model of the process. Schwartz <ref> [ Sch93 ] </ref> presents an extension of Q-learning that uses a more sophisticated optimality criterion than the usual infinite-horizon discounted criterion.
Reference: [ Sci93 ] <institution> Statistical Sciences. </institution> <note> S-Plus Programmer's manual, Version 3.2. </note> <institution> StatSci, a division of MathSoft Inc., </institution> <year> 1993. </year>
Reference-contexts: For the most part we will be interested in parametric models, since, empirically, non-parametric models tend to over-fit our data. 5.1.1 Model notation When a precise description of a statistical linear model is necessary, I will follow the notation of the S language <ref> [ Sci93 ] </ref> . The basic form of a simple linear model is response ~ predictor : This means that response is a linear function of predictor.
Reference: [ SPS94 ] <author> Michael I. Jordan Satinder P. Singh, Tommi Jaakkola. </author> <title> Model-free reinforcement learning for non-markovian decision problems. </title> <year> 1994. </year>
Reference-contexts: There is a growing interest in techniques that learn a model of the environment while using an approximate model (see, e.g., [ BBS93 ] , <ref> [ SPS94 ] </ref> ); some of these techniques are applicable to the system described here. * Constant-time actions The time taken to make a transition from one state to another is assumed to be fixed and independent both of the original state and the action taken.
Reference: [ Sut90 ] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of Machine Learning, </booktitle> <pages> pages 216-224, </pages> <year> 1990. </year>
Reference-contexts: Although many AI researchers propose strategies for attacking real-time planning problems (e.g., [ DB90 ] , [ BBS93 ] , <ref> [ Sut90 ] </ref> ), there has been very little comparative work, and few attempts to define the range of problems for which these techniques are most effective. 1.1 Example: traffic control In order to make the discussion that follows more concrete, consider the following example of a planning problem: The Transportation <p> Korf also presents an algorithm called LRTA? (Learning RTA?), a modification of RTA? for the case when multiple tasks are solved in the same domain. Sutton <ref> [ Sut90 ] </ref> presented a class of architectures called Dyna, for solving real-time planning problems. Like Plexus, the Dyna architectures consist of a separate executive component and planning component.
Reference: [ The61 ] <author> H. Theil. </author> <title> Economic Forecasts and Policy. </title> <publisher> North-Holland, </publisher> <year> 1961. </year> <month> 166 </month>
Reference-contexts: Another measure is the mean absolute error, defined by M AE = i Both of these suffer from a dependence on the scale of the response. A third measure, suggested by Theil <ref> [ The61 ] </ref> , is called the U-statistic, and is defined by U = u t i (y i ^y i ) 2 i y 2 : In each case, the smaller the value, the better the predictive accuracy.
Reference: [ Wat89 ] <author> C. J. C. H Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cam--bridge University, </institution> <year> 1989. </year>
Reference-contexts: Sometimes it is more desirable to have a guaranteed lower bound on accumulated reward, and sometimes one would like to incorporate a notion of risk-aversion or risk-preference. Heger [ Heg94 ] discusses the various options, and provides an algorithm similar to Q-learning <ref> [ Wat89 ] </ref> that uses worst-case total discounted costs. I consider only the risk-neutral case, where the expected accumulated discounted reward is optimized. Expectation of future rewards is discounted by an amount increasing geometrically with the temporal distance from the present. <p> Moore and Atkeson [ MA93 ] present a similar technique called prioritized sweeping. Much of the work in the area of learning Markov decision processes is based on Watkins' Q-learning <ref> [ Wat89 ] </ref> . Q-learning is a process for updating estimates of the values of each state-action pair as transitions are made. <p> This function is called Q because it is the basic quantity used by Watkins in his Q-learning algorithm <ref> [ Wat89 ] </ref> . 41 3.5.3 Policy Iteration with E 6= S In general, we are interested in computing an optimal policy over a subset of the state space. Recall that an agent has a default policy d , and a current partial policy with envelope E.
Reference: [ WB93 ] <author> Ronald J. Williams and Leemon C. III Baird. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report Technical Report NU-CCS-93-13, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: One is to perform value iteration until the absolute value of the difference between the value functions at successive iterations is less than some constant (for all components). The constant may be chosen so as to ensure the optimality of the resulting policy (see, e.g., <ref> [ WB93 ] </ref> ). A second alternative is to periodically perform one round of policy iteration; if this does not change any actions, then the policy is optimal.
Reference: [ Wei73 ] <author> Alan J. Weir. </author> <title> Lebesgue Integration and Measure. </title> <publisher> Cambridge University Press, </publisher> <year> 1973. </year>
Reference-contexts: This can be made smaller than an arbitrary * by suitable choice of N . This sequence is defined on the reals, which form a complete metric space, and Cauchy sequences in complete metric spaces converge (see, e.g., <ref> [ Wei73 ] </ref> ). Thus each element of ff n Q n converges, so EP = lim EP n is unique.
Reference: [ Wil85 ] <author> David E. Wilkins. </author> <title> Recovering from execution errors in sipe. </title> <journal> Computational Intelligence, </journal> <volume> 1(1), </volume> <month> February </month> <year> 1985. </year> <month> 167 </month>
Reference-contexts: In most of the domains I describe here, open-loop controllers have hopelessly bad performance. 2.3.2 Deterministic planning with plan recovery Some AI systems, such as SIPE <ref> [ Wil85 ] </ref> use a deterministic model of the environment in formulating a plan, and then monitor the execution of the plan to detect failures in the actions. This approach is effective when there is a deterministic model that is generally accurate.
References-found: 56

