URL: ftp://ftp.cse.cuhk.edu.hk/pub/techreports/95/tr-95-10.ps.gz
Refering-URL: ftp://ftp.cs.cuhk.hk/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Locally Connected Recurrent Networks  
Abstract: Lai-Wan CHAN and Evan Fung-Yu YOUNG Computer Science Department, The Chinese University of Hong Kong New Territories, Hong Kong Email : lwchan@cs.cuhk.hk Technical Report : CS-TR-95-10 Abstract The fully connected recurrent network (FRN) using the on-line training method, Real Time Recurrent Learning (RTRL), is computationally expensive. It has a computational complexity of O(N 4 ) and storage complexity of O(N 3 ), where N is the number of non-input units. We have devised a locally connected recurrent model which has a much lower complexity in both computational time and storage space. The ring-structure recurrent network (RRN), the simplest kind of the locally connected has the corresponding complexity of O(mn+np) and O(np) respectively, where p, n and m are the number of input, hidden and output units respectively. We compare the performance between RRN and FRN in sequence recognition and time series prediction. We tested the networks' ability in temporal memorizing power and time warpping ability in the sequence recognition task. In the time series prediction task, we used both networks to train and predict three series; a periodic series with white noise, a deterministic chaotic series and the sunspots data. Both tasks show that RRN needs a much shorter training time and the performance of RRN is comparable to that of FRN.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Chakraborty, K. Mehrotra, C. K. Mohan, and S. Ranka. </author> <title> Forecasting the behaviour of multivariate time series using neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 961-970, </pages> <year> 1992. </year>
Reference-contexts: Time series prediction is to forecast the future values of a series based on the history of that series. Predicting the future is important in a variety of fields and the non-linear signal processing of neural networks has been a new and promising approach for this purpose <ref> [8, 10, 3, 2, 1] </ref>. Recurrent networks have an advantage over feedforward nets in this problem as the recurrent links in the network can bring the appropriate previous values back to itself in order to forecast the next one.
Reference: [2] <author> C. de Groot and D. Wurtz. </author> <title> Analysis of univariate time series with connectionist nets: a case study of two classical examples. </title> <journal> Neurocomputing, </journal> <volume> 3 </volume> <pages> 177-192, </pages> <year> 1991. </year>
Reference-contexts: Time series prediction is to forecast the future values of a series based on the history of that series. Predicting the future is important in a variety of fields and the non-linear signal processing of neural networks has been a new and promising approach for this purpose <ref> [8, 10, 3, 2, 1] </ref>. Recurrent networks have an advantage over feedforward nets in this problem as the recurrent links in the network can bring the appropriate previous values back to itself in order to forecast the next one.
Reference: [3] <author> J. B. Elsner. </author> <title> Predicting time series using a neural network as a method of distinguishing chaos from noise. </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> 25 </volume> <pages> 843-850, </pages> <year> 1992. </year>
Reference-contexts: Time series prediction is to forecast the future values of a series based on the history of that series. Predicting the future is important in a variety of fields and the non-linear signal processing of neural networks has been a new and promising approach for this purpose <ref> [8, 10, 3, 2, 1] </ref>. Recurrent networks have an advantage over feedforward nets in this problem as the recurrent links in the network can bring the appropriate previous values back to itself in order to forecast the next one.
Reference: [4] <author> R. E. Rumelhard, G. E. Hinton, and R. J. Williams. </author> <title> Learning lnternal representation by error backpropagation. Parallel Distributed Processing: </title> <journal> Explorations in Microstructure of Cognition, </journal> <volume> 1, </volume> <year> 1986. </year>
Reference-contexts: Many researchers are working on this problem in the past decades and there are some exciting results in the training procedures. Back Propagation through time (BPTT) <ref> [4] </ref> is an efficient learning algorithm but it cannot be run on-line and is impractical for tasks with input signals of unknown lengths. The Real Time Recurrent Learning (RTRL) [9] is a powerful on-line learning algorithm but it is extremely inefficient and is also non-local in space.
Reference: [5] <author> J. H. Schmidhuber. </author> <title> A local learning algorithm for dynamic feedforward and recurrent networks. </title> <type> Report FKI-124-90, </type> <year> 1990. </year>
Reference-contexts: The Real Time Recurrent Learning (RTRL) [9] is a powerful on-line learning algorithm but it is extremely inefficient and is also non-local in space. There are also some other variants of RTRL [12, 7]. Alternatively, the non-gradient descent methods 1 like NBB <ref> [5] </ref> and TD [6] have many desirable features but they do not guarantee convergence. The challenge of devising new learning algorithms for recurrent models is thus an attractive, interesting and useful task.
Reference: [6] <author> J. H. Schmidhuber. </author> <title> Temporal-difference-driven learning in recurrent networks. </title> <booktitle> Parallel Processing Neural Systems and Computers, </booktitle> <volume> 15 </volume> <pages> 626-629, </pages> <year> 1990. </year>
Reference-contexts: The Real Time Recurrent Learning (RTRL) [9] is a powerful on-line learning algorithm but it is extremely inefficient and is also non-local in space. There are also some other variants of RTRL [12, 7]. Alternatively, the non-gradient descent methods 1 like NBB [5] and TD <ref> [6] </ref> have many desirable features but they do not guarantee convergence. The challenge of devising new learning algorithms for recurrent models is thus an attractive, interesting and useful task.
Reference: [7] <author> J. H. Schmidhuber. </author> <title> A fixed size storage O[n 3 ] time complexity learning algorithm for fully recurrent continually running network. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 243-248, </pages> <year> 1992. </year>
Reference-contexts: The Real Time Recurrent Learning (RTRL) [9] is a powerful on-line learning algorithm but it is extremely inefficient and is also non-local in space. There are also some other variants of RTRL <ref> [12, 7] </ref>. Alternatively, the non-gradient descent methods 1 like NBB [5] and TD [6] have many desirable features but they do not guarantee convergence. The challenge of devising new learning algorithms for recurrent models is thus an attractive, interesting and useful task.
Reference: [8] <author> A. S. Weigend, B. A. Huberman, and D. E. Rumelhart. </author> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-209, </pages> <year> 1990. </year>
Reference-contexts: Time series prediction is to forecast the future values of a series based on the history of that series. Predicting the future is important in a variety of fields and the non-linear signal processing of neural networks has been a new and promising approach for this purpose <ref> [8, 10, 3, 2, 1] </ref>. Recurrent networks have an advantage over feedforward nets in this problem as the recurrent links in the network can bring the appropriate previous values back to itself in order to forecast the next one.
Reference: [9] <author> R. J. Williams and D. Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 270-280, </pages> <year> 1989. </year>
Reference-contexts: Back Propagation through time (BPTT) [4] is an efficient learning algorithm but it cannot be run on-line and is impractical for tasks with input signals of unknown lengths. The Real Time Recurrent Learning (RTRL) <ref> [9] </ref> is a powerful on-line learning algorithm but it is extremely inefficient and is also non-local in space. There are also some other variants of RTRL [12, 7]. Alternatively, the non-gradient descent methods 1 like NBB [5] and TD [6] have many desirable features but they do not guarantee convergence. <p> Section 4 describes the simplest kind of the locally connected recurrent models called the ring-structured recurrent network (RRN). Section 5 and 6 compare RRN with the fully recurrent network (FRN) <ref> [9] </ref> in the tasks of sequence recognition and time series prediction respectively in terms of their performance in both training and testing. 2 Locally Connected Recurrent Networks 2.1 Network Topology The model is made up of three layers of units, input layer, hidden layer and output layer. <p> In a locally connected recurrent network, q and r are constants which are much smaller than n, so the computational complexity of the above algorithm is greatly improved in comparison with the O (N 4 ) of a FRN, the fully recurrent network derived by Williams and Zipser <ref> [9] </ref>, where N is equal to the number of non-input units fl (i.e. N = m+n). <p> Unfortunately, there is still no satisfactory algorithm for training recurrent models. The FRN using Real Time Recurrent Learning Rule (RTRL) by Williams and Zipser <ref> [9] </ref> is powerful as it performs exact gradient descent for a fully recurrent network in an on-line manner.
Reference: [10] <author> F. S. Wong. </author> <title> Time series forecasting using backpropagation neural networks. </title> <type> Technical Report NU-CCS-90-9, </type> <year> 1990. </year>
Reference-contexts: Time series prediction is to forecast the future values of a series based on the history of that series. Predicting the future is important in a variety of fields and the non-linear signal processing of neural networks has been a new and promising approach for this purpose <ref> [8, 10, 3, 2, 1] </ref>. Recurrent networks have an advantage over feedforward nets in this problem as the recurrent links in the network can bring the appropriate previous values back to itself in order to forecast the next one.
Reference: [11] <author> E.F.Y. Young and L.W. Chan. </author> <title> Parallel implementation of partially connected recurrent network. </title> <booktitle> In IEEE Conference on Neural Networks 1994, Orlando, volume IV, </booktitle> <pages> pages 2058-2063, </pages> <year> 1994. </year>
Reference-contexts: This new on-line learning rule has a much lower computational complexity O (mn+np) and storage complexity O (np) than FRN where m, n and p are the number of input units, hidden units and output units respectively. This algorithm has much flexibility for implementation on parallel architectures <ref> [11] </ref>. The algorithm is local in space if the parameter t in the model is fixed to one and each processing element in this case needs only to communicate with the directly connected processing elements.
Reference: [12] <author> D. Zipser. </author> <title> Subgrouping reduces complexity and speeds up learning in recurrent net-works. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <pages> pages 638-641, </pages> <year> 1990. </year> <month> 25 </month>
Reference-contexts: The Real Time Recurrent Learning (RTRL) [9] is a powerful on-line learning algorithm but it is extremely inefficient and is also non-local in space. There are also some other variants of RTRL <ref> [12, 7] </ref>. Alternatively, the non-gradient descent methods 1 like NBB [5] and TD [6] have many desirable features but they do not guarantee convergence. The challenge of devising new learning algorithms for recurrent models is thus an attractive, interesting and useful task.
References-found: 12

