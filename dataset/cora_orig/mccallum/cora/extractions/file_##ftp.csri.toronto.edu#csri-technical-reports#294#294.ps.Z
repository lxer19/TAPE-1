URL: file://ftp.csri.toronto.edu/csri-technical-reports/294/294.ps.Z
Refering-URL: http://www.eecg.toronto.edu/~okrieg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Optimizing IPC Performance for Shared-Memory Multiprocessors  
Author: Benjamin Gamsa, Orran Krieger, and Michael Stumm 
Address: Toronto, Canada  
Affiliation: Computer Systems Research Institute University of Toronto  
Pubnum: M5S 1A1  
Abstract: Technical Report CSRI-294 January 1994 The Computer Systems Research Institute (CSRI) is an interdisciplinary group formed to conduct research and development relevant to computer systems and their application. It is an Institute within the Faculty of Applied Science and Engineering, and the Faculty of Arts and Science, at the University of Toronto, and is supported in part by the Natural Sciences and Engineering Research Council of Canada. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mike Accetta, Robert Baron, David Golub, Richard Rashid, Avadis Tevanian, and Michael Young. </author> <title> Mach: A new kernel foundation for UNIX development. </title> <booktitle> In Proceedings of the 1993 Summer Usenix Conference, </booktitle> <pages> pages 93-112. </pages> <publisher> Usenix, </publisher> <year> 1986. </year>
Reference-contexts: experiment illustrates the dramatic impact any locks in the IPC path might have. 6 4 Design and Implementation Issues This section discusses some of the issues that arose during the design and implementation stage of the PPC facility. 4.1 Separation of Authentication and Naming As opposed to systems like Mach <ref> [1] </ref> or Spring [11] that use capabilities both for naming and for providing security, we specifically chose to separate the two issues. <p> This is in contrast to Mach <ref> [1] </ref> or Spring [11] where requests are directed to the object rather than to the server that implements it.
Reference: [2] <author> Brian N. Bershad. </author> <title> The increasing irrelevance of IPC performance for microkernel-based operating systems. </title> <booktitle> In Proceedings of the Usenix Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 205-212, </pages> <address> Seattle, 1992. </address> <publisher> Usenix. </publisher>
Reference-contexts: To date, the majority of the research on performance conscious IPC has been done on uniprocessor systems. Excellent results have been reported for these systems, to the point where Bershad argues that the IPC overhead has become largely irrelevant <ref> [2] </ref>. <p> For example, Liedtke reports requiring 60 secs on a 20 MHz 386 system and 10 secs on a 50MHz 486 system for a null, round-trip RPC [13]; a recent version of Mach requires 57 secs on a 25 MHz MIPS R3000 and 95 secs on a 16 MHz MIPS R2000 <ref> [2, 10] </ref>; and QNX requires 76 secs on a 33MHz 486 [12].
Reference: [3] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> Lightweight remote procedure call. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 102-13, </pages> <year> 1989. </year>
Reference-contexts: The PPC model is thus a key component to enabling locality and concurrency within servers. Although PPCs are most naturally implemented by having the invoking process cross directly into the server's address space <ref> [3, 6, 9] </ref>, our implementation uses separate worker processes in the server to service client calls. Worker processes are created dynamically as needed and (re)initialized to the server's call 2 handling code on each call, affecting an upcall directly into the service routine. <p> A possible compromise solution would collect servers that trust each other into groups and only share stacks between servers in the same group. Probably the most important prior work in performance conscious multiprocessor IPCs is Bershad's Light-Weight RPC (LRPC) facility <ref> [3] </ref>. On the surface LRPC appears similar to our own. For example, it uses the same PPC model as its basic abstraction, and it is designed to minimize the use of shared data and locks.
Reference: [4] <author> Brian N. Bershad, Richard P. Draves, and Alessandro Forsin. </author> <title> Microbenchmarks to evaluate system performance. </title> <booktitle> In Proceedings of the Third Workshop on Workstation Operating Systems(WWOS-3), </booktitle> <year> 1992. </year>
Reference-contexts: Hence, the NUMAness is not addressed in this section. To measure the cost of individual PPC operations, we used a microsecond timer (with 10 cycle access overhead), thus eliminating some of the problems associated with microbenchmarking <ref> [4] </ref>. Finer detail was obtained using a detailed description of the architecture, low-level measurements, and direct inspection of the compiler generated assembly code. Figure 2 depicts the breakdown of the time to performance PPC operations under a variety of conditions.
Reference: [5] <author> D. L. Black. </author> <title> Scheduling support for concurrency and parallelism in the Mach operating system. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <year> 1990. </year>
Reference-contexts: These implementations apply a common set of techniques to achieve good performance: i) registers are used to directly pass data across address spaces, circumventing the need to use slow memory [8]; ii) the generalities of the scheduling subsystem are avoided with hand-off scheduling techniques <ref> [5, 8] </ref>; iii) code and data is organized to minimize the number of cache misses and TLB faults; and iv) architectural and machine-specific features are exploited or avoided depending on whether they help or hinder performance.
Reference: [6] <author> Jeffrey S. Chase, Henry M. Levy, Michael J. Feeley, and Edward D. Lazowska. </author> <title> Sharing and protection in a single address space operating system. </title> <type> Technical Report TR-93-04-02, </type> <institution> Department of Computer Science and Engineering, Unversity of Washington, </institution> <address> Seattle, WA 98195, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: The PPC model is thus a key component to enabling locality and concurrency within servers. Although PPCs are most naturally implemented by having the invoking process cross directly into the server's address space <ref> [3, 6, 9] </ref>, our implementation uses separate worker processes in the server to service client calls. Worker processes are created dynamically as needed and (re)initialized to the server's call 2 handling code on each call, affecting an upcall directly into the service routine.
Reference: [7] <author> D. R. Cheriton. </author> <title> The V Kernel: A Software Base for Distributed Systems. </title> <journal> IEEE Software, </journal> <volume> 1, </volume> <month> April </month> <year> 1984. </year>
Reference-contexts: We provide a mechanism borrowed from the V system <ref> [7] </ref> where a caller may give permission to the server to read and write selected portions of its address space. <p> Many systems initiate processor-specific code execution through remote interrupts, for example, for TLB shootdown. 9 Finally, it is also possible for the server to acquire, internally, a larger stack if required (using whatever stack management strategy is appropriate for the server). 4.5.5 Naming We use an approach similar to V <ref> [7] </ref> or L3 [13] where a client makes a request to a server and identifies in its arguments the object to which the request is directed.
Reference: [8] <author> David R. Cheriton. </author> <title> An experiment using registers for fast message-based interprocess communication. </title> <journal> Operating System Review, </journal> (4):12-20, 1984. 
Reference-contexts: These implementations apply a common set of techniques to achieve good performance: i) registers are used to directly pass data across address spaces, circumventing the need to use slow memory <ref> [8] </ref>; ii) the generalities of the scheduling subsystem are avoided with hand-off scheduling techniques [5, 8]; iii) code and data is organized to minimize the number of cache misses and TLB faults; and iv) architectural and machine-specific features are exploited or avoided depending on whether they help or hinder performance. <p> These implementations apply a common set of techniques to achieve good performance: i) registers are used to directly pass data across address spaces, circumventing the need to use slow memory [8]; ii) the generalities of the scheduling subsystem are avoided with hand-off scheduling techniques <ref> [5, 8] </ref>; iii) code and data is organized to minimize the number of cache misses and TLB faults; and iv) architectural and machine-specific features are exploited or avoided depending on whether they help or hinder performance.
Reference: [9] <author> Partha Dasgupta, Richard J. Leblanc, Jr., and William F. Appelbe. </author> <title> The clouds distributed operating systems: Functional description, implementation details and related work. </title> <booktitle> In The 8th International Conference on Distributed Computer Systems, </booktitle> <pages> pages 2-9, </pages> <address> S. Jose CA (USA), </address> <month> June </month> <year> 1988. </year> <journal> (IEEE). </journal> <volume> 11 </volume>
Reference-contexts: The PPC model is thus a key component to enabling locality and concurrency within servers. Although PPCs are most naturally implemented by having the invoking process cross directly into the server's address space <ref> [3, 6, 9] </ref>, our implementation uses separate worker processes in the server to service client calls. Worker processes are created dynamically as needed and (re)initialized to the server's call 2 handling code on each call, affecting an upcall directly into the service routine.
Reference: [10] <author> Richard P. Draves, Brian N. Bershad, Richard F. Rashid, and Randall W. Dean. </author> <title> Using continuations to implement thread management and communication in operating systems. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 122-36. </pages> <institution> Association for Computing Machinery SIGOPS, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: For example, Liedtke reports requiring 60 secs on a 20 MHz 386 system and 10 secs on a 50MHz 486 system for a null, round-trip RPC [13]; a recent version of Mach requires 57 secs on a 25 MHz MIPS R3000 and 95 secs on a 16 MHz MIPS R2000 <ref> [2, 10] </ref>; and QNX requires 76 secs on a 33MHz 486 [12].
Reference: [11] <author> G. Hamilton and P. Kougiouris. </author> <title> The spring nucleus: A microkernel for objects. </title> <booktitle> In Proceedings of the 1993 Summer Usenix Conference. Usenix, </booktitle> <year> 1993. </year>
Reference-contexts: The key benefits of our approach all result from the fact that resources needed to handle a PPC are 2 Similar factors were coincidentally noted in Spring <ref> [11] </ref>. 3 If a server supports multiple services, there is one pool per service. 3 accessed exclusively by the local processor. By using only local resources during a call, remote memory accesses are eliminated. More importantly, since there is no sharing of data, cache coherence traffic is also eliminated. <p> dramatic impact any locks in the IPC path might have. 6 4 Design and Implementation Issues This section discusses some of the issues that arose during the design and implementation stage of the PPC facility. 4.1 Separation of Authentication and Naming As opposed to systems like Mach [1] or Spring <ref> [11] </ref> that use capabilities both for naming and for providing security, we specifically chose to separate the two issues. <p> This is in contrast to Mach [1] or Spring <ref> [11] </ref> where requests are directed to the object rather than to the server that implements it.
Reference: [12] <author> Dan Hildebrand. </author> <title> Architectural overview of QNX. </title> <booktitle> In Proceedings of the Usenix Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 113-126, </pages> <address> Seattle, 1992. </address> <publisher> Usenix. </publisher>
Reference-contexts: 386 system and 10 secs on a 50MHz 486 system for a null, round-trip RPC [13]; a recent version of Mach requires 57 secs on a 25 MHz MIPS R3000 and 95 secs on a 16 MHz MIPS R2000 [2, 10]; and QNX requires 76 secs on a 33MHz 486 <ref> [12] </ref>.
Reference: [13] <author> J. Liedtke. </author> <title> Improving IPC by kernel design. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 175-187, </pages> <year> 1993. </year>
Reference-contexts: Excellent results have been reported for these systems, to the point where Bershad argues that the IPC overhead has become largely irrelevant [2]. For example, Liedtke reports requiring 60 secs on a 20 MHz 386 system and 10 secs on a 50MHz 486 system for a null, round-trip RPC <ref> [13] </ref>; a recent version of Mach requires 57 secs on a 25 MHz MIPS R3000 and 95 secs on a 16 MHz MIPS R2000 [2, 10]; and QNX requires 76 secs on a 33MHz 486 [12]. <p> processor-specific code execution through remote interrupts, for example, for TLB shootdown. 9 Finally, it is also possible for the server to acquire, internally, a larger stack if required (using whatever stack management strategy is appropriate for the server). 4.5.5 Naming We use an approach similar to V [7] or L3 <ref> [13] </ref> where a client makes a request to a server and identifies in its arguments the object to which the request is directed. This is in contrast to Mach [1] or Spring [11] where requests are directed to the object rather than to the server that implements it.
Reference: [14] <author> Michael Stumm, Ron Unrau, and Orran Krieger. </author> <title> Designing a Scalable Operating System for Shared Memory Multiprocessors. </title> <booktitle> In USENIX Workshop on Micro-kernels and Other Kernel Architectures, </booktitle> <pages> pages 285-303, </pages> <address> Seattle, Wa., </address> <month> April </month> <year> 1992. </year>
Reference-contexts: This approach would be prohibitive in today's systems with the high cost of cache misses and invalidations. 3 Performance The platform used for our implementation is the Hurricane operating system <ref> [14, 16] </ref> running on the Hector shared memory multiprocessor [17]. These experiments were performed on a fully configured but otherwise idle 16 processor system. This prototype system uses Motorola 88100/88200 processors running at 16.67 MHz, with 16KB data and instruction caches and a 16 byte line size. <p> We believe that the overhead of our facility is close to the minimum achievable on our platform. We have incorporated this facility into the Hurricane operating system <ref> [14, 16] </ref>, and adapted most of the servers to use it.
Reference: [15] <author> Charles P. Thacker, Lawrence C. Stewart, and Edwin H. Satterthwaite, Jr. Firefly: </author> <title> A Multiprocessor Workstation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 909-920, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: It is also interesting to observe how the recent changes in technology lead to design tradeoffs far different from what they used to be. The Firefly multiprocessor <ref> [15] </ref> on which Bershad's IPC work was developed has a smaller ratio of processor to memory speed, has caches that are no faster than main memory (but are used to reduce bus traffic), and uses an updating cache consistency protocol.
Reference: [16] <author> Ron Unrau, Michael Stumm, and Orran Krieger. </author> <title> Hierarchical Clustering: A Structure for Scalable Multiprocessor Operating System Design. </title> <type> Technical report, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> 93. </month>
Reference-contexts: This approach would be prohibitive in today's systems with the high cost of cache misses and invalidations. 3 Performance The platform used for our implementation is the Hurricane operating system <ref> [14, 16] </ref> running on the Hector shared memory multiprocessor [17]. These experiments were performed on a fully configured but otherwise idle 16 processor system. This prototype system uses Motorola 88100/88200 processors running at 16.67 MHz, with 16KB data and instruction caches and a 16 byte line size. <p> We believe that the overhead of our facility is close to the minimum achievable on our platform. We have incorporated this facility into the Hurricane operating system <ref> [14, 16] </ref>, and adapted most of the servers to use it.
Reference: [17] <author> Zvonko G. Vranesic, Michael Stumm, Ron White, and David Lewis. </author> <title> The Hector Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(1), </volume> <month> January </month> <year> 1991. </year> <month> 12 </month>
Reference-contexts: This approach would be prohibitive in today's systems with the high cost of cache misses and invalidations. 3 Performance The platform used for our implementation is the Hurricane operating system [14, 16] running on the Hector shared memory multiprocessor <ref> [17] </ref>. These experiments were performed on a fully configured but otherwise idle 16 processor system. This prototype system uses Motorola 88100/88200 processors running at 16.67 MHz, with 16KB data and instruction caches and a 16 byte line size. Each processor has a local portion of the globally accessible memory. <p> Although many of the design decisions were influenced by our particular hardware base, a M88000-based, non-cache-coherent multiprocessor <ref> [17] </ref>, we argue that similar design decisions would apply to most current multiprocessors.
References-found: 17

