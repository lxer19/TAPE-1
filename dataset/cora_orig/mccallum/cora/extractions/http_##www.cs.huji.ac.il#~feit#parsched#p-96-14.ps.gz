URL: http://www.cs.huji.ac.il/~feit/parsched/p-96-14.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched96.html
Root-URL: http://www.cs.huji.ac.il
Phone: 2  
Title: Dynamic Partitioning in Different Distributed-Memory Environments  
Author: Nayeem Islam Andreas Prodromidis Mark S. Squillante 
Address: Yorktown Heights NY 10598, USA  New York NY 10027, USA  
Affiliation: 1 IBM T. J. Watson Research Center,  Columbia University,  
Abstract: In this paper we present a detailed analysis of dynamic partitioning in different distributed-memory parallel environments based on experimental and analytical methods. We develop an experimental testbed for the IBM SP2 and a network of workstations, and we apply a general analytic model of dynamic partitioning. This experimental and analytical framework is then used to explore a number of fundamental performance issues and tradeoffs concerning dynamic partitioning in different distributed-memory computing environments. Our results demonstrate and quantify how the performance benefits of dynamic partitioning are heavily dependent upon several system variables, including workload characteristics, system architecture, and system load.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G. R. Andrews. </author> <title> Paradigms for process interaction in distributed programs. </title> <journal> ACM Computing Surveys, </journal> <volume> 23(1) </volume> <pages> 49-90, </pages> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: We present one such approach. We assume that each application can use all of the nodes allocated to it during its lifetime. We also assume that these applications can be decomposed into the structure depicted in Fig. 1. This structure has been variously called bag-of-tasks <ref> [1, 10] </ref>, master-slave parallelism [25] and task-queue model [15, 5]. Each application consists of a coordinator process along with a set of worker processes as shown in Fig. 1. When an application starts it spawns a set of worker processes and the logically centralized coordinator. <p> The system then starts the new application's processes on that node. The parallel programming model we have chosen is a popular one, and it is easy to make adaptive. It is possible to structure a large class of applications in this manner, including Adaptive Quadrature <ref> [1] </ref> (a method for performing numerical integration), AtEarth [3] (a simulation of the flight of neutrinos from the sun towards the earth), DNA parallel sequence generation programs [3] and Computational Fluid Dynamics applications [26, 27]. Furthermore, many Linda programs are inherently structured in this manner [4, 3]. The Applications.
Reference: 2. <author> S. Asmussen, O. Nerman, and M. Olsson. </author> <title> Fitting phase type distributions via the EM algorithm. </title> <type> Technical Report 1994:23, </type> <institution> Department of Mathematics, Chalmers University of Technology, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Just as important, however, is the fact that any real distribution can in principle be represented arbitrarily close by a phase-type distribution, and a number of algorithms have been developed for fitting phase-type distributions to empirical data <ref> [2, 12, 20] </ref>. As our measurements confirm, this results in an extremely accurate modeling analysis of dynamic partitioning in real parallel systems. This DEP model is solved for relative system loads b U in the range 0.02 to 0.98 in increments of 0.02.
Reference: 3. <author> Carriero, Freeman, Gelernter, and Kaminsky. </author> <title> Adaptive Parallelism in Piranha. </title> <journal> IEEE Computer, </journal> <volume> 28(1) </volume> <pages> 40-49, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: The parallel programming model we have chosen is a popular one, and it is easy to make adaptive. It is possible to structure a large class of applications in this manner, including Adaptive Quadrature [1] (a method for performing numerical integration), AtEarth <ref> [3] </ref> (a simulation of the flight of neutrinos from the sun towards the earth), DNA parallel sequence generation programs [3] and Computational Fluid Dynamics applications [26, 27]. Furthermore, many Linda programs are inherently structured in this manner [4, 3]. The Applications. <p> It is possible to structure a large class of applications in this manner, including Adaptive Quadrature [1] (a method for performing numerical integration), AtEarth <ref> [3] </ref> (a simulation of the flight of neutrinos from the sun towards the earth), DNA parallel sequence generation programs [3] and Computational Fluid Dynamics applications [26, 27]. Furthermore, many Linda programs are inherently structured in this manner [4, 3]. The Applications. We examine the performance of two applications in this paper: Adaptive Quadrature and AtEarth. AtEarth. AtEarth simulates the flight of neutrinos from the sun toward the earth. <p> Furthermore, many Linda programs are inherently structured in this manner <ref> [4, 3] </ref>. The Applications. We examine the performance of two applications in this paper: Adaptive Quadrature and AtEarth. AtEarth. AtEarth simulates the flight of neutrinos from the sun toward the earth.
Reference: 4. <author> N. Carriero and D. Gelernter. </author> <title> Linda in Context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: Furthermore, many Linda programs are inherently structured in this manner <ref> [4, 3] </ref>. The Applications. We examine the performance of two applications in this paper: Adaptive Quadrature and AtEarth. AtEarth. AtEarth simulates the flight of neutrinos from the sun toward the earth.
Reference: 5. <author> R. Chandra, A. Gupta, and J. Henessey. </author> <title> COOL: A Language for parallel programming. </title> <booktitle> In Proceedings of the Second Workshop on Programming Languages, and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: We assume that each application can use all of the nodes allocated to it during its lifetime. We also assume that these applications can be decomposed into the structure depicted in Fig. 1. This structure has been variously called bag-of-tasks [1, 10], master-slave parallelism [25] and task-queue model <ref> [15, 5] </ref>. Each application consists of a coordinator process along with a set of worker processes as shown in Fig. 1. When an application starts it spawns a set of worker processes and the logically centralized coordinator. Each worker process is given a set of tasks to work on.
Reference: 6. <author> C. S. Chang. </author> <title> Smoothing point processes as a means to increase throughput. </title> <type> Technical Report RC 16866, </type> <institution> IBM Research Division, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: In addition to reducing the number of reconfigurations, this 17 approach tends to reduce the performance effects of job arrival variability by effectively smoothing the arrival process <ref> [6] </ref>. There is a fundamental tradeoff between these two dynamic partitioning approaches. Folding provides the advantage of immediately responding to workload changes, but it reduces the repartitioning overhead by interrupting fewer jobs to yield somewhat less equitable node allocations.
Reference: 7. <author> S.-H. Chiang, R. K. Mansharamani, and M. K. Vernon. </author> <title> Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 33-44, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Adaptive partitioning policies, where the number of nodes allocated to a job is determined when jobs enter and leave based on the current system state, have also been considered in a number of research studies <ref> [21, 38, 16, 26, 27, 32, 7, 30] </ref>. This approach tends to outperform its static counterparts by adapting partition sizes to the current load.
Reference: 8. <author> E. G. Coffman, Jr. and L. Kleinrock. </author> <title> Computer scheduling methods and their countermeasures. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <volume> volume 32, </volume> <pages> pages 11-21, </pages> <month> April </month> <year> 1968. </year>
Reference-contexts: It is for exactly these reasons that an adaptive partitioning strategy should be used for jobs with relatively small processing demands [26, 27]. Such information can be successfully given by users [18, 19] provided that countermeasures are taken by the system <ref> [8] </ref>, it can be estimated with performance tools and run-time systems [11], and/or determined via standard methods such as multi-level feedback queues.
Reference: 9. <author> K. Dussa, B. Carlson, L. Dowdy, and K.-H. Park. </author> <title> Dynamic partitioning in transputer environments. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 203-213, </pages> <year> 1990. </year>
Reference-contexts: These potential problems are alleviated under dynamic partitioning, where the size of the partition allocated to a job can be modified during its execution, at the expense of increased overhead <ref> [37, 9, 21, 38, 17, 23, 26, 27, 34] </ref>. The relative runtime costs of a dynamic partitioning policy are heavily dependent upon the parallel architecture and application workload. In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. <p> In more distributed parallel environments, however, the overheads of a dynamic partitioning policy can be significant due to factors such as data/job migration, node preemption/coordination and, in some cases, reconfiguration of the application <ref> [9, 26, 27, 31] </ref>. There are several fundamental issues that must be considered in order to effectively exploit dynamic partitioning in distributed computing environments.
Reference: 10. <author> J. D. L. Eager and J. Zahorjan. </author> <title> Chores:Enhanced Run-Time support for shared-memory parallel computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: We present one such approach. We assume that each application can use all of the nodes allocated to it during its lifetime. We also assume that these applications can be decomposed into the structure depicted in Fig. 1. This structure has been variously called bag-of-tasks <ref> [1, 10] </ref>, master-slave parallelism [25] and task-queue model [15, 5]. Each application consists of a coordinator process along with a set of worker processes as shown in Fig. 1. When an application starts it spawns a set of worker processes and the logically centralized coordinator.
Reference: 11. <author> K. Ekanadham, V. K. Naik, and M. S. Squillante. </author> <title> PET: A parallel performance estimation tool. </title> <booktitle> In Proceedings Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: Such information can be successfully given by users [18, 19] provided that countermeasures are taken by the system [8], it can be estimated with performance tools and run-time systems <ref> [11] </ref>, and/or determined via standard methods such as multi-level feedback queues. We should point out that the scalloped shape of the response time ratio curves for both workloads are the result of the response time behavior of the best static partitioning policy.
Reference: 12. <author> M. J. Faddy. </author> <title> Fitting structured phase-type distributions. </title> <type> Technical report, </type> <institution> Department of Mathematics, University of Queensland, Australia, </institution> <month> April </month> <year> 1994. </year> <title> To appear, Applied Stochastic Models and Data Analysis. </title>
Reference-contexts: Just as important, however, is the fact that any real distribution can in principle be represented arbitrarily close by a phase-type distribution, and a number of algorithms have been developed for fitting phase-type distributions to empirical data <ref> [2, 12, 20] </ref>. As our measurements confirm, this results in an extremely accurate modeling analysis of dynamic partitioning in real parallel systems. This DEP model is solved for relative system loads b U in the range 0.02 to 0.98 in increments of 0.02.
Reference: 13. <author> D. G. Feitelson and B. Nitzberg. </author> <title> Job characteristics of a production parallel scientific workload on the NASA Ames iPSC/860. In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.). </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: the appropriate data structures so that it can communicate with the workers and the individual processes can communicate with each other. 3.3 Workloads Current and expected workloads for large-scale parallel computing environments consist of a mixture of applications with very different resource requirements, often resulting in a highly variable workload <ref> [26, 27, 13, 18, 19] </ref>. We therefore use a simple probability distribution to control and vary different mixtures of instances of the two applications discussed in Section 3.2, where an instance is determined by both the application and its input data set. <p> While 10 most previous parallel scheduling studies have assumed a Poisson arrival process (i.e., exponential interarrival times), recent measurements of real scientific and engineering workloads demonstrate that the job interarrival times in such high-performance computing environments tend to be significantly more variable <ref> [13, 18, 19] </ref>. We therefore consider hyperexponential interarrival times that statistically match the workload measurements presented in [13, 18, 19], and we compare these results with those obtained under the exponential interarrival assumptions of previous work. <p> a Poisson arrival process (i.e., exponential interarrival times), recent measurements of real scientific and engineering workloads demonstrate that the job interarrival times in such high-performance computing environments tend to be significantly more variable <ref> [13, 18, 19] </ref>. We therefore consider hyperexponential interarrival times that statistically match the workload measurements presented in [13, 18, 19], and we compare these results with those obtained under the exponential interarrival assumptions of previous work.
Reference: 14. <author> L. L. Fong, A. S. Gopal, N. Islam, A. Prodromidis, and M. S. Squillante. </author> <title> Extensible resource management for cluster computing. </title> <type> Technical report, </type> <institution> IBM Research Division, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: The application then reconfigures itself based on the new set of nodes available to it. The DS implements both DEP and SP policies, the performance characteristics of which are examined in this paper. We refer the interested reader to <ref> [14] </ref> for more details on the architecture of our distributed scheduler. 3.2 Parallel Application Structure Many parallel applications are written such that the number of nodes allocated to them can only be set when they start.
Reference: 15. <author> R. Gabriel. </author> <title> Queue based multiprocessing lisp. </title> <booktitle> In ACM Symposium on Lips and Functional Programming, </booktitle> <pages> pages 25-43, </pages> <year> 1984. </year>
Reference-contexts: We assume that each application can use all of the nodes allocated to it during its lifetime. We also assume that these applications can be decomposed into the structure depicted in Fig. 1. This structure has been variously called bag-of-tasks [1, 10], master-slave parallelism [25] and task-queue model <ref> [15, 5] </ref>. Each application consists of a coordinator process along with a set of worker processes as shown in Fig. 1. When an application starts it spawns a set of worker processes and the logically centralized coordinator. Each worker process is given a set of tasks to work on.
Reference: 16. <author> D. Ghosal, G. Serazzi, and S. K. Tripathi. </author> <title> The processor working set and its use in scheduling multiprocessor systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17 </volume> <pages> 443-453, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Adaptive partitioning policies, where the number of nodes allocated to a job is determined when jobs enter and leave based on the current system state, have also been considered in a number of research studies <ref> [21, 38, 16, 26, 27, 32, 7, 30] </ref>. This approach tends to outperform its static counterparts by adapting partition sizes to the current load.
Reference: 17. <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: These potential problems are alleviated under dynamic partitioning, where the size of the partition allocated to a job can be modified during its execution, at the expense of increased overhead <ref> [37, 9, 21, 38, 17, 23, 26, 27, 34] </ref>. The relative runtime costs of a dynamic partitioning policy are heavily dependent upon the parallel architecture and application workload. In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. <p> In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. A number of research studies have made this quite clear, showing that dynamic partitioning outperforms all other space-sharing strategies in such environments <ref> [37, 21, 38, 17, 23] </ref>. In more distributed parallel environments, however, the overheads of a dynamic partitioning policy can be significant due to factors such as data/job migration, node preemption/coordination and, in some cases, reconfiguration of the application [9, 26, 27, 31].
Reference: 18. <author> S. G. Hotovy. </author> <title> Workload evolution on the Cornell Theory Center IBM SP2. </title> <booktitle> In Proceedings of the 2nd Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: the appropriate data structures so that it can communicate with the workers and the individual processes can communicate with each other. 3.3 Workloads Current and expected workloads for large-scale parallel computing environments consist of a mixture of applications with very different resource requirements, often resulting in a highly variable workload <ref> [26, 27, 13, 18, 19] </ref>. We therefore use a simple probability distribution to control and vary different mixtures of instances of the two applications discussed in Section 3.2, where an instance is determined by both the application and its input data set. <p> While 10 most previous parallel scheduling studies have assumed a Poisson arrival process (i.e., exponential interarrival times), recent measurements of real scientific and engineering workloads demonstrate that the job interarrival times in such high-performance computing environments tend to be significantly more variable <ref> [13, 18, 19] </ref>. We therefore consider hyperexponential interarrival times that statistically match the workload measurements presented in [13, 18, 19], and we compare these results with those obtained under the exponential interarrival assumptions of previous work. <p> a Poisson arrival process (i.e., exponential interarrival times), recent measurements of real scientific and engineering workloads demonstrate that the job interarrival times in such high-performance computing environments tend to be significantly more variable <ref> [13, 18, 19] </ref>. We therefore consider hyperexponential interarrival times that statistically match the workload measurements presented in [13, 18, 19], and we compare these results with those obtained under the exponential interarrival assumptions of previous work. <p> It is for exactly these reasons that an adaptive partitioning strategy should be used for jobs with relatively small processing demands [26, 27]. Such information can be successfully given by users <ref> [18, 19] </ref> provided that countermeasures are taken by the system [8], it can be estimated with performance tools and run-time systems [11], and/or determined via standard methods such as multi-level feedback queues.
Reference: 19. <author> S. G. Hotovy, D. J. Schneider, and T. O'Donnell. </author> <title> Analysis of the early workload on the Cornell Theory Center IBM SP2. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1996. </year> <note> Poster. </note>
Reference-contexts: the appropriate data structures so that it can communicate with the workers and the individual processes can communicate with each other. 3.3 Workloads Current and expected workloads for large-scale parallel computing environments consist of a mixture of applications with very different resource requirements, often resulting in a highly variable workload <ref> [26, 27, 13, 18, 19] </ref>. We therefore use a simple probability distribution to control and vary different mixtures of instances of the two applications discussed in Section 3.2, where an instance is determined by both the application and its input data set. <p> While 10 most previous parallel scheduling studies have assumed a Poisson arrival process (i.e., exponential interarrival times), recent measurements of real scientific and engineering workloads demonstrate that the job interarrival times in such high-performance computing environments tend to be significantly more variable <ref> [13, 18, 19] </ref>. We therefore consider hyperexponential interarrival times that statistically match the workload measurements presented in [13, 18, 19], and we compare these results with those obtained under the exponential interarrival assumptions of previous work. <p> a Poisson arrival process (i.e., exponential interarrival times), recent measurements of real scientific and engineering workloads demonstrate that the job interarrival times in such high-performance computing environments tend to be significantly more variable <ref> [13, 18, 19] </ref>. We therefore consider hyperexponential interarrival times that statistically match the workload measurements presented in [13, 18, 19], and we compare these results with those obtained under the exponential interarrival assumptions of previous work. <p> It is for exactly these reasons that an adaptive partitioning strategy should be used for jobs with relatively small processing demands [26, 27]. Such information can be successfully given by users <ref> [18, 19] </ref> provided that countermeasures are taken by the system [8], it can be estimated with performance tools and run-time systems [11], and/or determined via standard methods such as multi-level feedback queues.
Reference: 20. <author> A. Lang. </author> <title> Parameter estimation for phase-type distributions, part I: Fundamentals and existing methods. </title> <type> Technical Report 159, </type> <institution> Department of Statistics, Oregon State University, </institution> <year> 1994. </year>
Reference-contexts: Just as important, however, is the fact that any real distribution can in principle be represented arbitrarily close by a phase-type distribution, and a number of algorithms have been developed for fitting phase-type distributions to empirical data <ref> [2, 12, 20] </ref>. As our measurements confirm, this results in an extremely accurate modeling analysis of dynamic partitioning in real parallel systems. This DEP model is solved for relative system loads b U in the range 0.02 to 0.98 in increments of 0.02.
Reference: 21. <author> S. T. Leutenegger and M. K. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Adaptive partitioning policies, where the number of nodes allocated to a job is determined when jobs enter and leave based on the current system state, have also been considered in a number of research studies <ref> [21, 38, 16, 26, 27, 32, 7, 30] </ref>. This approach tends to outperform its static counterparts by adapting partition sizes to the current load. <p> These potential problems are alleviated under dynamic partitioning, where the size of the partition allocated to a job can be modified during its execution, at the expense of increased overhead <ref> [37, 9, 21, 38, 17, 23, 26, 27, 34] </ref>. The relative runtime costs of a dynamic partitioning policy are heavily dependent upon the parallel architecture and application workload. In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. <p> In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. A number of research studies have made this quite clear, showing that dynamic partitioning outperforms all other space-sharing strategies in such environments <ref> [37, 21, 38, 17, 23] </ref>. In more distributed parallel environments, however, the overheads of a dynamic partitioning policy can be significant due to factors such as data/job migration, node preemption/coordination and, in some cases, reconfiguration of the application [9, 26, 27, 31].
Reference: 22. <author> R. K. Mansharamani and M. K. Vernon. </author> <title> Properties of the EQS parallel processor allocation policy. </title> <type> Technical Report 1192, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: The only overhead incurred by each job is the cost to set up the job for execution on the S nodes allocated to it. Our decision to consider equal-sized node partitions is motivated by the results of several studies (e.g., <ref> [30, 22] </ref>) showing that adaptive/static strategies in which the system is divided into equal-sized partitions outperform other adaptive/static policies when job service time requirements are not used in scheduling decisions.
Reference: 23. <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A dynamic processor allocation policy for multiprogrammed shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: These potential problems are alleviated under dynamic partitioning, where the size of the partition allocated to a job can be modified during its execution, at the expense of increased overhead <ref> [37, 9, 21, 38, 17, 23, 26, 27, 34] </ref>. The relative runtime costs of a dynamic partitioning policy are heavily dependent upon the parallel architecture and application workload. In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. <p> In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. A number of research studies have made this quite clear, showing that dynamic partitioning outperforms all other space-sharing strategies in such environments <ref> [37, 21, 38, 17, 23] </ref>. In more distributed parallel environments, however, the overheads of a dynamic partitioning policy can be significant due to factors such as data/job migration, node preemption/coordination and, in some cases, reconfiguration of the application [9, 26, 27, 31].
Reference: 24. <author> C. McCann and J. Zahorjan. </author> <title> Processor allocation policies for message-passing parallel computers. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 19-32, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: We have been exploring several variants of dynamic partitioning to address these issues. One strategy for decreasing the overheads associated with dynamic equi-partitioning is to use the folding approach found in <ref> [24] </ref>, which reduces the number reconfigurations performed under the greedy dynamic policy, at the expense of a less equitable allocation of the nodes among the competing jobs.
Reference: 25. <author> J. Mohan. </author> <title> Performance of Parallel Programs: Models and Analyses. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> July </month> <year> 1984. </year>
Reference-contexts: We present one such approach. We assume that each application can use all of the nodes allocated to it during its lifetime. We also assume that these applications can be decomposed into the structure depicted in Fig. 1. This structure has been variously called bag-of-tasks [1, 10], master-slave parallelism <ref> [25] </ref> and task-queue model [15, 5]. Each application consists of a coordinator process along with a set of worker processes as shown in Fig. 1. When an application starts it spawns a set of worker processes and the logically centralized coordinator.
Reference: 26. <author> V. K. Naik, S. K. Setia, and M. S. Squillante. </author> <title> Performance analysis of job scheduling policies in parallel supercomputing environments. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 824-833, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This is due in part to its low system overhead and its simplicity from both the system and application viewpoints. The static scheduling approach, however, can lead to relatively low system throughputs and resource utilizations under nonuniform workloads <ref> [33, 26, 27, 30, 34] </ref>, which can be common in scientific and engineering computing environments. <p> Adaptive partitioning policies, where the number of nodes allocated to a job is determined when jobs enter and leave based on the current system state, have also been considered in a number of research studies <ref> [21, 38, 16, 26, 27, 32, 7, 30] </ref>. This approach tends to outperform its static counterparts by adapting partition sizes to the current load. <p> These potential problems are alleviated under dynamic partitioning, where the size of the partition allocated to a job can be modified during its execution, at the expense of increased overhead <ref> [37, 9, 21, 38, 17, 23, 26, 27, 34] </ref>. The relative runtime costs of a dynamic partitioning policy are heavily dependent upon the parallel architecture and application workload. In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. <p> In more distributed parallel environments, however, the overheads of a dynamic partitioning policy can be significant due to factors such as data/job migration, node preemption/coordination and, in some cases, reconfiguration of the application <ref> [9, 26, 27, 31] </ref>. There are several fundamental issues that must be considered in order to effectively exploit dynamic partitioning in distributed computing environments. <p> To complement and extend previous studies of repartitioning overheads for certain distributed-memory environments <ref> [26, 27, 31] </ref>, we conduct a detailed measurement-based analysis of dynamic partitioning overheads in computing environments based on the IBM SP2 and a network of workstations. <p> A number of research studies, under different workload assumptions, have also shown that adaptive partitioning yields steady-state performance comparable to that of the best static partitioning policy for a given system load <ref> [26, 27, 32] </ref>. <p> It is possible to structure a large class of applications in this manner, including Adaptive Quadrature [1] (a method for performing numerical integration), AtEarth [3] (a simulation of the flight of neutrinos from the sun towards the earth), DNA parallel sequence generation programs [3] and Computational Fluid Dynamics applications <ref> [26, 27] </ref>. Furthermore, many Linda programs are inherently structured in this manner [4, 3]. The Applications. We examine the performance of two applications in this paper: Adaptive Quadrature and AtEarth. AtEarth. AtEarth simulates the flight of neutrinos from the sun toward the earth. <p> the appropriate data structures so that it can communicate with the workers and the individual processes can communicate with each other. 3.3 Workloads Current and expected workloads for large-scale parallel computing environments consist of a mixture of applications with very different resource requirements, often resulting in a highly variable workload <ref> [26, 27, 13, 18, 19] </ref>. We therefore use a simple probability distribution to control and vary different mixtures of instances of the two applications discussed in Section 3.2, where an instance is determined by both the application and its input data set. <p> It is for exactly these reasons that an adaptive partitioning strategy should be used for jobs with relatively small processing demands <ref> [26, 27] </ref>. Such information can be successfully given by users [18, 19] provided that countermeasures are taken by the system [8], it can be estimated with performance tools and run-time systems [11], and/or determined via standard methods such as multi-level feedback queues.
Reference: 27. <author> V. K. Naik, S. K. Setia, and M. S. Squillante. </author> <title> Scheduling of large scientific applications on distributed memory multiprocessor systems. </title> <booktitle> In Proceedings Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 913-922, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: This is due in part to its low system overhead and its simplicity from both the system and application viewpoints. The static scheduling approach, however, can lead to relatively low system throughputs and resource utilizations under nonuniform workloads <ref> [33, 26, 27, 30, 34] </ref>, which can be common in scientific and engineering computing environments. <p> Adaptive partitioning policies, where the number of nodes allocated to a job is determined when jobs enter and leave based on the current system state, have also been considered in a number of research studies <ref> [21, 38, 16, 26, 27, 32, 7, 30] </ref>. This approach tends to outperform its static counterparts by adapting partition sizes to the current load. <p> These potential problems are alleviated under dynamic partitioning, where the size of the partition allocated to a job can be modified during its execution, at the expense of increased overhead <ref> [37, 9, 21, 38, 17, 23, 26, 27, 34] </ref>. The relative runtime costs of a dynamic partitioning policy are heavily dependent upon the parallel architecture and application workload. In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. <p> In more distributed parallel environments, however, the overheads of a dynamic partitioning policy can be significant due to factors such as data/job migration, node preemption/coordination and, in some cases, reconfiguration of the application <ref> [9, 26, 27, 31] </ref>. There are several fundamental issues that must be considered in order to effectively exploit dynamic partitioning in distributed computing environments. <p> To complement and extend previous studies of repartitioning overheads for certain distributed-memory environments <ref> [26, 27, 31] </ref>, we conduct a detailed measurement-based analysis of dynamic partitioning overheads in computing environments based on the IBM SP2 and a network of workstations. <p> A number of research studies, under different workload assumptions, have also shown that adaptive partitioning yields steady-state performance comparable to that of the best static partitioning policy for a given system load <ref> [26, 27, 32] </ref>. <p> It is possible to structure a large class of applications in this manner, including Adaptive Quadrature [1] (a method for performing numerical integration), AtEarth [3] (a simulation of the flight of neutrinos from the sun towards the earth), DNA parallel sequence generation programs [3] and Computational Fluid Dynamics applications <ref> [26, 27] </ref>. Furthermore, many Linda programs are inherently structured in this manner [4, 3]. The Applications. We examine the performance of two applications in this paper: Adaptive Quadrature and AtEarth. AtEarth. AtEarth simulates the flight of neutrinos from the sun toward the earth. <p> the appropriate data structures so that it can communicate with the workers and the individual processes can communicate with each other. 3.3 Workloads Current and expected workloads for large-scale parallel computing environments consist of a mixture of applications with very different resource requirements, often resulting in a highly variable workload <ref> [26, 27, 13, 18, 19] </ref>. We therefore use a simple probability distribution to control and vary different mixtures of instances of the two applications discussed in Section 3.2, where an instance is determined by both the application and its input data set. <p> It is for exactly these reasons that an adaptive partitioning strategy should be used for jobs with relatively small processing demands <ref> [26, 27] </ref>. Such information can be successfully given by users [18, 19] provided that countermeasures are taken by the system [8], it can be estimated with performance tools and run-time systems [11], and/or determined via standard methods such as multi-level feedback queues.
Reference: 28. <author> M. F. Neuts. </author> <title> Matrix-Geometric Solutions in Stochastic Models: An Algorithmic Approach. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1981. </year>
Reference-contexts: The use of phase-type distributions <ref> [28] </ref> for our model parameters is motivated in part by their important mathematical properties, which can be exploited to obtain a tractable analytic model while capturing the fundamental aspects of dynamic partitioning.
Reference: 29. <author> J. D. Padhye and L. W. Dowdy. </author> <title> Preemptive versus non-preemptive processor allocation policies for message passing parallel computers: An empirical comparison. </title> <booktitle> In Proceedings of the 2nd Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: The latter result is in contrast to those of Padhye and Dowdy <ref> [29] </ref> which show that folding generally outperforms equi-partitioning in a distributed-memory environment under a workload based on scientific matrix computation programs. <p> The differences between the results of these two studies are primarily due to the differences in the respective workloads, where the workload used in our experiments consists of applications with larger execution times than those studied in <ref> [29] </ref>. This further highlights the fundamental tradeoff between the two above dynamic partitioning approaches for reducing the repartitioning overheads in distributed-memory environments. We are continuing to examine these and related scheduling issues in distributed-memory parallel systems. 18
Reference: 30. <author> E. Rosti, E. Smirni, L. W. Dowdy, G. Serazzi, and B. M. Carlson. </author> <title> Robust partitioning policies of multiprocessor systems. Performance Evaluation, </title> <booktitle> 19 </booktitle> <pages> 141-165, </pages> <year> 1994. </year>
Reference-contexts: This is due in part to its low system overhead and its simplicity from both the system and application viewpoints. The static scheduling approach, however, can lead to relatively low system throughputs and resource utilizations under nonuniform workloads <ref> [33, 26, 27, 30, 34] </ref>, which can be common in scientific and engineering computing environments. <p> Adaptive partitioning policies, where the number of nodes allocated to a job is determined when jobs enter and leave based on the current system state, have also been considered in a number of research studies <ref> [21, 38, 16, 26, 27, 32, 7, 30] </ref>. This approach tends to outperform its static counterparts by adapting partition sizes to the current load. <p> The only overhead incurred by each job is the cost to set up the job for execution on the S nodes allocated to it. Our decision to consider equal-sized node partitions is motivated by the results of several studies (e.g., <ref> [30, 22] </ref>) showing that adaptive/static strategies in which the system is divided into equal-sized partitions outperform other adaptive/static policies when job service time requirements are not used in scheduling decisions.
Reference: 31. <author> S. K. Setia. </author> <title> Scheduling on Multiprogrammed, Distributed Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Maryland, College Park, MD, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: In more distributed parallel environments, however, the overheads of a dynamic partitioning policy can be significant due to factors such as data/job migration, node preemption/coordination and, in some cases, reconfiguration of the application <ref> [9, 26, 27, 31] </ref>. There are several fundamental issues that must be considered in order to effectively exploit dynamic partitioning in distributed computing environments. <p> To complement and extend previous studies of repartitioning overheads for certain distributed-memory environments <ref> [26, 27, 31] </ref>, we conduct a detailed measurement-based analysis of dynamic partitioning overheads in computing environments based on the IBM SP2 and a network of workstations.
Reference: 32. <author> S. K. Setia and S. K. Tripathi. </author> <title> A comparative analysis of static processor partitioning policies for parallel computers. </title> <booktitle> In Proceedings of MASCOTS '93, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Adaptive partitioning policies, where the number of nodes allocated to a job is determined when jobs enter and leave based on the current system state, have also been considered in a number of research studies <ref> [21, 38, 16, 26, 27, 32, 7, 30] </ref>. This approach tends to outperform its static counterparts by adapting partition sizes to the current load. <p> A number of research studies, under different workload assumptions, have also shown that adaptive partitioning yields steady-state performance comparable to that of the best static partitioning policy for a given system load <ref> [26, 27, 32] </ref>.
Reference: 33. <author> K. C. Sevcik. </author> <title> Characterizations of parallelism in applications and their use in scheduling. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: This is due in part to its low system overhead and its simplicity from both the system and application viewpoints. The static scheduling approach, however, can lead to relatively low system throughputs and resource utilizations under nonuniform workloads <ref> [33, 26, 27, 30, 34] </ref>, which can be common in scientific and engineering computing environments.
Reference: 34. <author> K. C. Sevcik. </author> <title> Application scheduling and processor allocation in multiprogrammed parallel processing systems. Performance Evaluation, </title> <booktitle> 19 </booktitle> <pages> 107-140, </pages> <year> 1994. </year>
Reference-contexts: This is due in part to its low system overhead and its simplicity from both the system and application viewpoints. The static scheduling approach, however, can lead to relatively low system throughputs and resource utilizations under nonuniform workloads <ref> [33, 26, 27, 30, 34] </ref>, which can be common in scientific and engineering computing environments. <p> These potential problems are alleviated under dynamic partitioning, where the size of the partition allocated to a job can be modified during its execution, at the expense of increased overhead <ref> [37, 9, 21, 38, 17, 23, 26, 27, 34] </ref>. The relative runtime costs of a dynamic partitioning policy are heavily dependent upon the parallel architecture and application workload. In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs.
Reference: 35. <author> M. S. Squillante. </author> <title> Analysis of dynamic partitioning in parallel systems. </title> <type> Technical Report RC 19950, </type> <institution> IBM Research Division, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: The technical details of our models and their solutions are beyond the scope of this paper. We refer the interested reader to <ref> [35] </ref> for derivations of an exact solution of each model, including expressions for performance measures of interest. <p> The technical details of our models and their solutions are beyond the scope of this paper. We refer the interested reader to [35] for derivations of an exact solution of each model, including expressions for performance measures of interest. Additional details on the models can be found in <ref> [35, 36] </ref>. 4.1 Dynamic Partitioning We model a parallel computer system consisting of P identical nodes that are scheduled according to the (basic) DEP policy defined in Section 2.1. <p> The exact details of the node allocation decisions made by the scheduler in each case, as well as the overheads of making these decisions and of reconfiguring the applications involved, are reflected in the model parameter distributions and the analysis of the corresponding stochastic process <ref> [35] </ref>. In this manner we can model various types of dynamic partitioning strategies, although our focus in this paper is on DEP. The interarrival times of jobs are modeled as independent and identically distributed (i.i.d.) random variables with a phase-type probability distribution A () and mean interarrival time 1=.
Reference: 36. <author> M. S. Squillante. </author> <title> On the benefits and limitations of dynamic partitioning in parallel computer systems. In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.). </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: The technical details of our models and their solutions are beyond the scope of this paper. We refer the interested reader to [35] for derivations of an exact solution of each model, including expressions for performance measures of interest. Additional details on the models can be found in <ref> [35, 36] </ref>. 4.1 Dynamic Partitioning We model a parallel computer system consisting of P identical nodes that are scheduled according to the (basic) DEP policy defined in Section 2.1.
Reference: 37. <author> A. Tucker and A. Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: These potential problems are alleviated under dynamic partitioning, where the size of the partition allocated to a job can be modified during its execution, at the expense of increased overhead <ref> [37, 9, 21, 38, 17, 23, 26, 27, 34] </ref>. The relative runtime costs of a dynamic partitioning policy are heavily dependent upon the parallel architecture and application workload. In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. <p> In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. A number of research studies have made this quite clear, showing that dynamic partitioning outperforms all other space-sharing strategies in such environments <ref> [37, 21, 38, 17, 23] </ref>. In more distributed parallel environments, however, the overheads of a dynamic partitioning policy can be significant due to factors such as data/job migration, node preemption/coordination and, in some cases, reconfiguration of the application [9, 26, 27, 31].
Reference: 38. <author> J. Zahorjan and C. McCann. </author> <title> Processor scheduling in shared memory multiprocessors. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 214-225, </pages> <month> May </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: Adaptive partitioning policies, where the number of nodes allocated to a job is determined when jobs enter and leave based on the current system state, have also been considered in a number of research studies <ref> [21, 38, 16, 26, 27, 32, 7, 30] </ref>. This approach tends to outperform its static counterparts by adapting partition sizes to the current load. <p> These potential problems are alleviated under dynamic partitioning, where the size of the partition allocated to a job can be modified during its execution, at the expense of increased overhead <ref> [37, 9, 21, 38, 17, 23, 26, 27, 34] </ref>. The relative runtime costs of a dynamic partitioning policy are heavily dependent upon the parallel architecture and application workload. In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. <p> In uniform-access, shared-memory systems, these overheads tend to be relatively small and thus the benefits of dynamic partitioning outweigh its associated costs. A number of research studies have made this quite clear, showing that dynamic partitioning outperforms all other space-sharing strategies in such environments <ref> [37, 21, 38, 17, 23] </ref>. In more distributed parallel environments, however, the overheads of a dynamic partitioning policy can be significant due to factors such as data/job migration, node preemption/coordination and, in some cases, reconfiguration of the application [9, 26, 27, 31].
References-found: 38

