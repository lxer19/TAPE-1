URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-98-27.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.cs.virginia.edu
Email: flindahl,chapin,nfb5z,grimshawg@cs.virginia.edu  
Title: Experiences with Legion on the Centurion Cluster  
Author: Greg Lindahl Steve J. Chapin Norman Beekwilder Andrew Grimshaw 
Keyword: cluster computing, heterogeneous computing  
Note: DRAFT  This work was funded in part by NSF grant CDA9724552, ONR grant N00014-98-1-0454, Northrup-Grumman contract 9729373-00, and DOE contracts DEFG02-96ER25290, SANDIA #LD-9391, and D45900016-3C.  
Address: Charlottesville, VA 22903-2442  
Affiliation: The Legion Project Department of Computer Science University of Virginia  
Abstract: The Legion Project spends most of its time building metacomputing environments and applications, but we are also actively building a clustered system, which we have named Centurion. Centurion is currently a cluster of 64 Alpha PCs, networked with low-latency gigabit networking hardware from Myricom, joined by a dozen x86-architecture PCs. We plan on doubling the size of the cluster in the near future. This shared-nothing, heterogeneous environment is an ideal fit for the Legion metacomputing system, which provides services such as naming, scheduling, heterogeneous communications, and parallel I/O. Centurion is roughly the size of "small" HPC systems in use today at supercomputing centers, giving us the opportunity to compare price and performance with traditional systems, and investigate the hardware and software challenges of building large clusters with commodity hardware. This paper will cover our experiences with Centurion and Legion. We begin with an overview of the Legion metacomputing system. We will then discuss the costs and hidden costs of assembling a cluster. We then describe the performance of Centurion, using microbenchmarks of the communications system, standard parallel application benchmarks, and user applications. User applications running on the system come from a variety of scientific disciplines, and range from traditional MPI codes taken straight from supercomputers to more novel applications using "bag of tasks" and macro-dataflow formalisms. Within this range of applications, we will discuss our successes and failures with hiding network latency, load balancing, and most importantly, usability of the system by researchers without extensive training in parallel computing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, D. E. Culler, D. A. Patterson, </author> <title> and the NOW Team. A case for networks of workstations: Now. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: This has advantages in that there is less risk|free operating system support is better for Intel platforms than for others, and well-understood technologies such as Ethernet provide no surprises in installation, configuration, or use. The Networks of Workstation (NOW) <ref> [1] </ref> project at UC Berkeley was focused more on system-level research rather than supporting scientific computing applications. Sandia National Labs has the Computational Plant project, which is constructing multicomputers from DEC Alpha nodes.
Reference: [2] <author> A. Basu, V. Buch, W. Vogels, and T. von Eicken. U-net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <month> December 3-6 </month> <year> 1995. </year>
Reference-contexts: These microbenchmarks are important so that we can properly set our expectations for application performance. 5.1 Myrinet performance Myrinet offers 2 different interfaces to the system; the usual "networking" interface via the kernel running protocols such as TCP and UDP, and a user-level interface similar to U-Net <ref> [2] </ref> or the VIA standard [4]. Gigabit networking requires a low-overhead interface in order to get gigabit performance; there have been many demonstrations of 700 to 900 gigabit sustained transfers using Myrinet with various user-level interfaces.
Reference: [3] <author> N. Beekwilder and A. Grimshaw. </author> <title> Parallelization of an axially symmetric steady flow program. </title> <type> Technical Report CS-98-10, </type> <institution> Department of Computer Science, University of Virginia, </institution> <month> May </month> <year> 1998. </year>
Reference-contexts: We attribute this to a lack of sufficient cache when only a few processors were used. The data represent a variety of spatial decompositions; if the number of rows is &gt; 1, this indicates a 2-dimensional decomposition. Full details are available in <ref> [3] </ref>. 7 Concluding Remarks In this paper, we have described some of our experiences building the Centurion cluster, and porting and tuning applications to our run-time system using the cluster. Our experiences in building the cluster demonstrate that a high-performance multicomputer can be constructed from inexpensive commodity parts.
Reference: [4] <author> Intel Corporation. </author> <title> The virtual interface architecture. </title> <address> http://www.intel.com/procs/SERVERS/isv/vi/vi2/index.htm. </address>
Reference-contexts: are important so that we can properly set our expectations for application performance. 5.1 Myrinet performance Myrinet offers 2 different interfaces to the system; the usual "networking" interface via the kernel running protocols such as TCP and UDP, and a user-level interface similar to U-Net [2] or the VIA standard <ref> [4] </ref>. Gigabit networking requires a low-overhead interface in order to get gigabit performance; there have been many demonstrations of 700 to 900 gigabit sustained transfers using Myrinet with various user-level interfaces.
Reference: [5] <author> A. S. Grimshaw, Wm. A. Wulf, </author> <title> and the Legion Team. The legion vision of a worldwide virtual computer. </title> <journal> Communications of the ACM, </journal> <volume> 40(1), </volume> <month> January </month> <year> 1997. </year> <pages> 12 13 </pages>
Reference-contexts: The Legion project at the University of Virginia is developing such software <ref> [5] </ref>. Legion addresses issues such as parallelism, fault tolerance, security, autonomy, heterogeneity, resource management, and access transparency in a multi-language environment. As part of this effort, we are building our own clustered system, called Centurion. <p> The Legion design encompasses ten basic objectives: site autonomy, support for heterogeneity, extensibility, ease-of-use, parallel processing to achieve performance, fault tolerance, scalability, security, multi-language support, and global naming. These objectives are described in greater depth in Grimshaw et al. <ref> [5] </ref>. Many of the features supplied by Legion are superfluous in the environment under discussion|support for heterogeneity, parallel processing, and ease-of-use are of primary interest to us here. The other features are important in the context of connecting clusters from remote sites.
Reference: [6] <author> S. Pakin, V. Karamcheti, and A. Chien. </author> <title> Fast messages: Efficient, portable communica-tion for workstation clusters and MPPs. </title> <journal> IEEE Concurrency, </journal> <volume> 5(2), </volume> <month> April-June </month> <year> 1997. </year>
Reference-contexts: Note that the latency numbers are lower than Myrinet; we attribute this to high overhead in the Myrinet kernel driver, because we know that user-level Myrinet protocols can achieve sub-10 usec latencies <ref> [6] </ref>. The performance of our cluster using the cheap semi-switched 100 megabit Ethernet topology described earlier is hard to quantify.
Reference: [7] <author> T. Sterling. </author> <title> Low cost (m2cots) cluster system design and development (derived from the beowulf experience). </title> <address> http://www.cacr.caltech.edu/ tron/presenta.htm. </address> <month> 14 </month>
Reference-contexts: Phase II will provide us with a rich environment for operating systems research, as well. 4 Related Work There are three other projects which we believe are of primary importance when discussing cluster work such as Centurion. First, the Beowulf project <ref> [7] </ref> at NASA is one of the best-known "pile of PC" cluster projects. However, Beowulf systems aim for the lowest-price point rather than the best price-performance tradeoff.
References-found: 7

