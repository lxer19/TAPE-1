URL: ftp://ftp.cs.rochester.edu/pub/u/rao/papers/ijcai95.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/rao/papers.html
Root-URL: 
Email: frao,danag@cs.rochester.edu  
Title: Natural Basis Functions and Topographic Memory for Face Recognition  
Author: Rajesh P.N. Rao and Dana H. Ballard 
Date: 10-17, 1995.  
Note: Appeared in Proc. International Joint Conf. on Artificial Intelligence (IJCAI), pp.  
Address: Rochester, NY 14627, USA  
Affiliation: Department of Computer Science University of Rochester  
Abstract: Recent work regarding the statistics of natural images has revealed that the dominant eigenvectors of arbitrary natural images closely approximate various oriented derivative-of-Gaussian functions; these functions have also been shown to provide the best fit to the receptive field profiles of cells in the primate striate cortex. We propose a scheme for expression-invariant face recognition that employs a fixed set of these "natural" basis functions to generate multiscale iconic representations of human faces. Using a fixed set of basis functions obviates the need for recomputing eigenvectors (a step that was necessary in some previous approaches employing principal component analysis (PCA) for recognition) while at the same time retaining the redundancy-reducing properties of PCA. A face is represented by a set of iconic representations automatically extracted from an input image. The description thus obtained is stored in a topographically-organized sparse distributed memory that is based on a model of human long-term memory first proposed by Kanerva. We describe experimental results for an implementation of the method on a pipeline image processor that is capable of achieving near real-time recognition by exploiting the processor's frame-rate convolution capability for indexing purposes.
Abstract-found: 1
Intro-found: 1
Reference: [ Ballard and Rao, 1994 ] <author> Dana H. Ballard and Ra-jesh P.N. Rao. </author> <title> Seeing behind occlusions. </title> <booktitle> In Proc. of ECCV, </booktitle> <pages> pages 274-285, </pages> <year> 1994. </year>
Reference-contexts: The results seem to indicate that modest occlusions (as in images 2, 3, and 4) can be handled but larger occlusions (such as in 5) may require other strategies such as the one suggested in <ref> [ Ballard and Rao, 1994 ] </ref> . The results also motivate the need for using more than one point per face in order to be able to compensate for partial occlusions near specific facial locations.
Reference: [ Beymer, 1993 ] <author> David J. Beymer. </author> <title> Face recognition under varying pose. </title> <type> Technical Report (A. I. Memo) 1461, </type> <institution> M.I.T., </institution> <month> December </month> <year> 1993. </year> <title> subjects in the test database shown here for an arbitrary expression. (b) An example of a correctly recognized face (on the right) and the six images of the person used in the training set (on the left). (c) One of the 4 cases (out of 60 test cases) where the method failed. (d) Recognition rate plotted as a function of the number of points used per face. </title>
Reference: [ Bledsoe, 1966 ] <author> W. W. Bledsoe. </author> <title> Man-machine facial recognition. </title> <type> Technical Report PRI:22, </type> <institution> Panoramic Research Inc., </institution> <address> Palo Alto, CA, </address> <year> 1966. </year>
Reference-contexts: Early schemes for face recognition utilized geometrical representations; prominent features such as eyes, nose, mouth, and chin were detected and geometrical models of faces given by feature vectors whose dimensions, for instance, denoted the relative positions of the facial features were used for the purposes of recognition <ref> [ Bledsoe, 1966; Kanade, 1973 ] </ref> . Recently, researchers have reported successful results using photometric representations i.e. representations that are computed directly from the intensity values of the input image.
Reference: [ Buhmann et al., 1990 ] <author> Joachim M. Buhmann, Martin Lades, and Christoph v.d. Malsburg. </author> <title> Size and distortion invariant object recognition by hierarchical graph matching. </title> <booktitle> In Proc. IEEE IJCNN, San Diego (Vol. II), </booktitle> <pages> pages 411-416, </pages> <year> 1990. </year>
Reference-contexts: Recently, researchers have reported successful results using photometric representations i.e. representations that are computed directly from the intensity values of the input image. Some prominent examples include face representations based on biologically-motivated Gabor filter "jets" <ref> [ Buhmann et al., 1990 ] </ref> , randomly placed zeroth-order Gaussian kernels [ Edelman et al., 1992 ] , isodensity maps [ Nakamura et al., 1991 ] , and principal component analysis (PCA) [ Turk and Pentland, 1991; Pentland et al., 1994 ] .
Reference: [ Coombs, 1992 ] <author> David J. Coombs. </author> <title> Real-Time Gaze Holding in Binocular Robot Vision. </title> <type> PhD thesis, </type> <institution> University of Rochester Computer Science Dept., </institution> <year> 1992. </year> <note> Available as Technical Report 415. </note>
Reference-contexts: In our current implementation, we apply the following simple strategy after the approximate boundary of the face is determined (by using, for instance, stereo and a technique such as zero disparity filtering <ref> [ Coombs, 1992 ] </ref> ): A face is represented by response vectors from the centroid of the face and each of the points lying on the intersections of radial lines with concentric circles of exponentially increasing radii centered on the centroid. illustrates the operation of the modified sparse distributed memory model <p> The pitch of the two-eye platform is controlled by a single servo motor while separate motors control each camera's pan angle, thereby providing independent vergence control. This allows strategies for figure-ground segmentation of faces using, for instance, zero disparity filtering <ref> [ Coombs, 1992 ] </ref> ; once a face has been approximately segmented from the background, the concentric circular template of points can be centered on the centroid of the face.
Reference: [ Edelman et al., 1992 ] <author> S. Edelman, D. Reisfeld, and Y. Yeshurun. </author> <title> Learning to recognize faces from examples. </title> <booktitle> In Proc. of ECCV, </booktitle> <pages> pages 787-791, </pages> <year> 1992. </year>
Reference-contexts: Recently, researchers have reported successful results using photometric representations i.e. representations that are computed directly from the intensity values of the input image. Some prominent examples include face representations based on biologically-motivated Gabor filter "jets" [ Buhmann et al., 1990 ] , randomly placed zeroth-order Gaussian kernels <ref> [ Edelman et al., 1992 ] </ref> , isodensity maps [ Nakamura et al., 1991 ] , and principal component analysis (PCA) [ Turk and Pentland, 1991; Pentland et al., 1994 ] .
Reference: [ Freeman and Adelson, 1991 ] <author> William T. Freeman and Edward H. Adelson. </author> <title> The design and use of steerable filters. </title> <journal> IEEE PAMI, </journal> <volume> 13(9) </volume> <pages> 891-906, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The exact number and type of Gaussian derivative basis functions used is motivated by the need to make the representations invariant to rotations in the image plane. This invariance can be achieved by exploiting the property of steerability <ref> [ Freeman and Adelson, 1991 ] </ref> and using a minimal basis set of two first-order directional derivatives at 0 ffi and 90 ffi , three second-order derivatives at 0 ffi , 60 ffi and 120 ffi , and four third-order derivatives oriented at 0 ffi , 45 ffi , 90 ffi
Reference: [ Gabor, 1946 ] <author> D. </author> <title> Gabor. </title> <journal> Theory of communication. J IEE, </journal> <volume> 93 </volume> <pages> 429-459, </pages> <year> 1946. </year>
Reference-contexts: to maximize signal-to-noise ratio and yield much sharper correlation peaks than traditional raw image cross-correlation techniques [ Kumar et al., 1982 ] ; (b) they form the class of real-valued functions that simultaneously minimize the product of the standard deviation of the spatial position sensitivity and spatial frequency sensitivity ( <ref> [ Gabor, 1946 ] </ref> p. 441) 1 ; and (c) they are endorsed by neurobiological studies [ Young, 1985 ] which show that the different order derivative-of-Gaussian functions provide the best fit to primate cortical receptive field profiles among the different functions suggested in the literature. 1 The class of complex-valued <p> neurobiological studies [ Young, 1985 ] which show that the different order derivative-of-Gaussian functions provide the best fit to primate cortical receptive field profiles among the different functions suggested in the literature. 1 The class of complex-valued functions that minimize this conjoint localization metric are the well-known Gabor elementary functions <ref> [ Gabor, 1946 ] </ref> . 3 Iconic Representations of Faces The iconic representations used in our recognition scheme are based on the natural basis functions mentioned in the previous section.
Reference: [ Hancock et al., 1992 ] <author> Peter J.B. Hancock, Roland J. Baddeley, and Leslie S. Smith. </author> <title> The principal components of natural images. </title> <journal> Network, </journal> <volume> 3 </volume> <pages> 61-70, </pages> <year> 1992. </year>
Reference: [ Kanade, 1973 ] <author> Takeo Kanade. </author> <title> Picture processing by computer complex and recognition of human faces. </title> <type> Technical report, Technical Report, </type> <institution> Department of Information Science, Kyoto University, </institution> <year> 1973. </year>
Reference-contexts: Early schemes for face recognition utilized geometrical representations; prominent features such as eyes, nose, mouth, and chin were detected and geometrical models of faces given by feature vectors whose dimensions, for instance, denoted the relative positions of the facial features were used for the purposes of recognition <ref> [ Bledsoe, 1966; Kanade, 1973 ] </ref> . Recently, researchers have reported successful results using photometric representations i.e. representations that are computed directly from the intensity values of the input image.
Reference: [ Kanerva, 1988 ] <author> Pentti Kanerva. </author> <title> Sparse Distributed Memory. </title> <address> Cambridge, MA: </address> <publisher> Bradford Books, </publisher> <year> 1988. </year>
Reference: [ Kanerva, 1993 ] <author> Pentti Kanerva. </author> <title> Sparse distributed memory and related models. </title> <editor> In Mohamad H. Has-soun, editor, </editor> <booktitle> Associative Neural Memories, </booktitle> <pages> pages 50-76. </pages> <address> New York: </address> <publisher> Oxford University Press, </publisher> <year> 1993. </year>
Reference-contexts: The statistically reconstructed data vector should closely resemble the original data vector (or some linear combination of the stored vectors in the case of interpolation between stored facial expressions) provided the capacity of the SDM <ref> [ Kanerva, 1993 ] </ref> has not been exceeded. The intuitive reason for this is as follows: When storing ~ d using ~r, each of the selected locations receives one copy of the data. <p> This biases the sum vector in the direction of ~ d and hence, ~ d is output with a high probability (see <ref> [ Kanerva, 1993 ] </ref> for a more rigorous argument based on a signal-to-noise ratio analysis). 4.2 Topographical Organization of Memory Topology of the model points, as given by the concentric circular template (Figure 2), is preserved by using separate SDMs for storing vectors from each of the sparse number of locations
Reference: [ Keeler, 1988 ] <author> James D. Keeler. </author> <title> Comparison between Kanerva's SDM and Hopfield-type neural networks. </title> <journal> Cognitive Science, </journal> <volume> 12 </volume> <pages> 299-329, </pages> <year> 1988. </year>
Reference-contexts: Therefore, if addresses are picked randomly, a large number of locations will never be activated while a number of locations will be selected so often that their contents will resemble noise. The solution is to pick addresses according to the distribution of the data <ref> [ Keeler, 1988 ] </ref> . In our case, we simply use an initial subset of the training response vectors as the addresses. <p> This corresponds to a form of generalized Hebbian learning as pointed out in <ref> [ Keeler, 1988 ] </ref> . Note that this is different from a conventional memory where addresses are required to exactly match for selection and previous contents are overwritten with new data.
Reference: [ Kumar et al., 1982 ] <author> V.K. Kumar, D. Casasent, and H. Murakami. </author> <title> Principal-component imagery for statistical pattern recognition correlators. </title> <journal> Optical Engineering, </journal> <volume> 21(1) </volume> <pages> 43-47, </pages> <year> 1982. </year>
Reference-contexts: In addition, correlation filters generated via PCA have been shown to maximize signal-to-noise ratio and yield much sharper correlation peaks than traditional raw image cross-correlation techniques <ref> [ Kumar et al., 1982 ] </ref> ; (b) they form the class of real-valued functions that simultaneously minimize the product of the standard deviation of the spatial position sensitivity and spatial frequency sensitivity ( [ Gabor, 1946 ] p. 441) 1 ; and (c) they are endorsed by neurobiological studies [
Reference: [ Murase and Nayar, 1995 ] <author> Hiroshi Murase and Shree K. Nayar. </author> <title> Visual learning and recognition of 3D objects from appearance. </title> <journal> IJCV, </journal> <volume> 14 </volume> <pages> 5-24, </pages> <year> 1995. </year>
Reference-contexts: This paper explores the use of an iconic representation of human faces that exploits the dimensionality-reducing properties of PCA. However, unlike previous approaches employing PCA for recognition <ref> [ Turk and Pentland, 1991; Murase and Nayar, 1995 ] </ref> , our approach uses a fixed set of basis functions that are learned during an initial "development" phase; the costly and time consuming step of having to recompute basis functions when new faces (or other objects) are encountered is thereby avoided. <p> by the dominant eigenvec-tors of a set of training images and achieve recognition by finding the manifold that is closest to the projection of an input image in the eigenspace formed by all objects. 2.2 Unsupervised Learning of Basis Functions The methods of [ Turk and Pentland, 1991 ] and <ref> [ Murase and Nayar, 1995 ] </ref> both require recomputation of the eigenvectors when new faces/objects are encountered.
Reference: [ Nakamura et al., 1991 ] <author> O. Nakamura, S. Mathur, and T. Minami. </author> <title> Identification of human faces based on isodensity maps. </title> <journal> Pattern Recognition, </journal> <volume> 24(3) </volume> <pages> 263-272, </pages> <year> 1991. </year>
Reference-contexts: Some prominent examples include face representations based on biologically-motivated Gabor filter "jets" [ Buhmann et al., 1990 ] , randomly placed zeroth-order Gaussian kernels [ Edelman et al., 1992 ] , isodensity maps <ref> [ Nakamura et al., 1991 ] </ref> , and principal component analysis (PCA) [ Turk and Pentland, 1991; Pentland et al., 1994 ] . This paper explores the use of an iconic representation of human faces that exploits the dimensionality-reducing properties of PCA.
Reference: [ Pentland et al., 1994 ] <author> Alex Pentland, Baback Moghad-dam, and Thad Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Conference on Computer Vision and Pattern Recognition, </booktitle> <year> 1994. </year>
Reference-contexts: Some prominent examples include face representations based on biologically-motivated Gabor filter "jets" [ Buhmann et al., 1990 ] , randomly placed zeroth-order Gaussian kernels [ Edelman et al., 1992 ] , isodensity maps [ Nakamura et al., 1991 ] , and principal component analysis (PCA) <ref> [ Turk and Pentland, 1991; Pentland et al., 1994 ] </ref> . This paper explores the use of an iconic representation of human faces that exploits the dimensionality-reducing properties of PCA. <p> This view-based approach is similar in spirit to the one used by Beymer [ 1993 ] (see also <ref> [ Pentland et al., 1994 ] </ref> ). 3.2 Representing Faces The response vectors described in the previous section serve as iconic descriptions of individual image regions.
Reference: [ Poggio and Girosi, 1990 ] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <journal> Proc. IEEE, </journal> <volume> 78 </volume> <pages> 1481-1497, </pages> <year> 1990. </year>
Reference-contexts: been filled, the address space can be allowed to self-organize using the well-known competitive Hebbian learning rule: given a new input vector ~r, the closest addresses ~ A k are adapted according to ~ A 0 where &gt; 0 is a gain term and g is a radial basis function <ref> [ Poggio and Girosi, 1990 ] </ref> that weights the second summand according to the distance between ~r and ~ A k . However, for the experiments in this paper, we did not employ the above self-organization rule.
Reference: [ Rao and Ballard, 1995a ] <author> Rajesh P.N. Rao and Dana H. Ballard. </author> <title> An active vision architecture based on iconic representations. </title> <type> Technical Report 548, </type> <institution> Department of Computer Science, University of Rochester, </institution> <year> 1995. </year>
Reference-contexts: We have previously shown these iconic feature vectors to be useful for active vision <ref> [ Rao and Ballard, 1995a ] </ref> , visuomotor learning [ Rao and Ballard, 1995b ] , and general object indexing [ Rao and Ballard, 1995c ] . Here, we show that such a representation may be used for the difficult problem of expression-invariant face recognition as well. <p> The use of multiple scales increases the perspicuity of the representation and allows interpolation strategies for scale invariance (see <ref> [ Rao and Ballard, 1995a ] </ref> for more details). <p> Indexing into the SDMs using convolutions for distance computations further optimizes the recognition process. Storing upto 33 2 See <ref> [ Rao and Ballard, 1995a ] </ref> for more details. facial occlusions on the feature vectors for two different facial points (marked by `+') are shown. vectors for a face may seem extravagant but note that this choice still results in considerable savings over the alternative of pixel-wise storage of images (33
Reference: [ Rao and Ballard, 1995b ] <author> Rajesh P.N. Rao and Dana H. Ballard. </author> <title> Learning saccadic eye movements using mul-tiscale spatial filters. </title> <editor> In G. Tesauro, D.S. Touretzky, and T.K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1995. </year> <note> (To appear). </note>
Reference-contexts: We have previously shown these iconic feature vectors to be useful for active vision [ Rao and Ballard, 1995a ] , visuomotor learning <ref> [ Rao and Ballard, 1995b ] </ref> , and general object indexing [ Rao and Ballard, 1995c ] . Here, we show that such a representation may be used for the difficult problem of expression-invariant face recognition as well.
Reference: [ Rao and Ballard, 1995c ] <author> Rajesh P.N. Rao and Dana H. Ballard. </author> <title> Object indexing using an iconic sparse distributed memory. </title> <booktitle> In Proceedings of the International Conference on Computer Vision (ICCV), </booktitle> <year> 1995. </year> <note> (To appear). </note>
Reference-contexts: We have previously shown these iconic feature vectors to be useful for active vision [ Rao and Ballard, 1995a ] , visuomotor learning [ Rao and Ballard, 1995b ] , and general object indexing <ref> [ Rao and Ballard, 1995c ] </ref> . Here, we show that such a representation may be used for the difficult problem of expression-invariant face recognition as well. A face is represented by a collection of iconic feature vectors automatically extracted from specific locations in the input image (Section 3). <p> In addition, the high-dimensionality of the vectors makes them remarkably robust to noise due to the orthogonality inherent in high-dimensional spaces: given any vector, most of the other vectors in the space tend to be relatively uncorrelated with the given vector <ref> [ Rao and Ballard, 1995c ] </ref> . The iconic representations can be made invariant to rotations in the image plane (for a fixed scale) by exploiting the property of steerability [ Freeman and Adel-son, 1991 ] . <p> Note that this normalization procedure does not apply to rotations in 3D. Modest rotations in depth can be handled as noise by the representation but larger 3D rotations require the use of responses from multiple views as we have shown in <ref> [ Rao and Ballard, 1995c ] </ref> . <p> The final output of the memory is obtained from the cumulative sum vector over the SDMs for the different facial locations. This arrangement offers at least two advantages over using a single SDM for storing vectors as proposed earlier in <ref> [ Rao and Ballard, 1995c ] </ref> : (a) the crosstalk between response vectors from different locations on a face is eliminated, and (b) a given response vector from a facial location needs to compared to only the model vectors in the SDM for that location, thereby speeding up the recognition pro
Reference: [ Sanger, 1989 ] <author> Terence David Sanger. </author> <title> Optimal unsupervised learning in a single-layer linear feedforward neural network. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 459-473, </pages> <year> 1989. </year>
Reference: [ Turk and Pentland, 1991 ] <author> Matthew Turk and Alex Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: Some prominent examples include face representations based on biologically-motivated Gabor filter "jets" [ Buhmann et al., 1990 ] , randomly placed zeroth-order Gaussian kernels [ Edelman et al., 1992 ] , isodensity maps [ Nakamura et al., 1991 ] , and principal component analysis (PCA) <ref> [ Turk and Pentland, 1991; Pentland et al., 1994 ] </ref> . This paper explores the use of an iconic representation of human faces that exploits the dimensionality-reducing properties of PCA. <p> This paper explores the use of an iconic representation of human faces that exploits the dimensionality-reducing properties of PCA. However, unlike previous approaches employing PCA for recognition <ref> [ Turk and Pentland, 1991; Murase and Nayar, 1995 ] </ref> , our approach uses a fixed set of basis functions that are learned during an initial "development" phase; the costly and time consuming step of having to recompute basis functions when new faces (or other objects) are encountered is thereby avoided. <p> manifolds in the low-dimensional sub-space ("eigenspace") formed by the dominant eigenvec-tors of a set of training images and achieve recognition by finding the manifold that is closest to the projection of an input image in the eigenspace formed by all objects. 2.2 Unsupervised Learning of Basis Functions The methods of <ref> [ Turk and Pentland, 1991 ] </ref> and [ Murase and Nayar, 1995 ] both require recomputation of the eigenvectors when new faces/objects are encountered.
Reference: [ Young, 1985 ] <author> R.A. Young. </author> <title> The Gaussian derivative theory of spatial vision: Analysis of cortical cell receptive field line-weighting profiles. </title> <journal> General Motors Research Publication GMR-4920, </journal> <year> 1985. </year>
Reference-contexts: [ Kumar et al., 1982 ] ; (b) they form the class of real-valued functions that simultaneously minimize the product of the standard deviation of the spatial position sensitivity and spatial frequency sensitivity ( [ Gabor, 1946 ] p. 441) 1 ; and (c) they are endorsed by neurobiological studies <ref> [ Young, 1985 ] </ref> which show that the different order derivative-of-Gaussian functions provide the best fit to primate cortical receptive field profiles among the different functions suggested in the literature. 1 The class of complex-valued functions that minimize this conjoint localization metric are the well-known Gabor elementary functions [ Gabor, 1946
References-found: 24

