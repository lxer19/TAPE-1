URL: http://www.cs.umn.edu/Users/dept/users/kumar/mlevel_serial.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs  
Author: George Karypis and Vipin Kumar 
Keyword: Graph Partitioning, Multilevel Partitioning Methods, Spectral Partitioning Methods, Fill Reducing Ordering, Kernighan-Lin Heuristic, Parallel Sparse Matrix Algorithms.  
Address: Minneapolis, MN 55455  10:49am  
Affiliation: University of Minnesota, Department of Computer Science AHPCRC  at  
Note: To appear in SIAM Journal on Scientific Computing A  This work was supported by Army Research Office contract DA/DAAH04-95-1-0538 NSF grant CCR-9423082, and by Army High Performance Computing Research Center under the auspices of the Department of the Army, Army Research Laboratory cooperative agreement number DAAH04-95-2-0003/contract number DAAH04-95-C-0008. Access to computing facilities was provided by AHPCRC, Minnesota Supercomputer Institute, Cray Research Inc, and by the Pittsburgh Supercomputing Center. Related papers are available via WWW at  
Email: fkarypis, kumarg@cs.umn.edu  
Web: URL: http://www.cs.umn.edu/karypis  
Date: Last updated on January 23, 1997  
Abstract: The algorithms described in this paper are implemented by the `METIS: Unstructured Graph Partitioning and Sparse Matrix Ordering System'. METIS is available on WWW at URL: http://www.cs.umn.edu/karypis/metis/metis.html Abstract Recently, a number of researchers have investigated a class of graph partitioning algorithms that reduce the size of the graph by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph [4, 26]. From the early work it was clear that multilevel techniques held great promise; however, it was not known if they can be made to consistently produce high quality partitions for graphs arising in a wide range of application domains. We investigate the effectiveness of many different choices for all three phases: coarsening, partition of the coarsest graph, and refinement. In particular, we present a new coarsening heuristic (called heavy-edge heuristic) for which the size of the partition of the coarse graph is within a small factor of the size of the final partition obtained after multilevel refinement. We also present a much faster variation of the Kernighan-Lin algorithm for refining during uncoarsening. We test our scheme on a large number of graphs arising in various domains including finite element methods, linear programming, VLSI, and transportation. Our experiments show that our scheme produces partitions that are consistently better than those produced by spectral partitioning schemes in substantially smaller time. Also, when our scheme is used to compute fill reducing orderings for sparse matrices, it produces orderings that have substantially smaller fill than the widely used multiple minimum degree algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Stephen T. Barnard and Horst Simon. </author> <title> A parallel implementation of multilevel recursive spectral bisection for application to adaptive unstructured meshes. </title> <booktitle> In Proceedings of the seventh SIAM conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 627-632, </pages> <year> 1995. </year>
Reference-contexts: However, as discussed in Section 6.4, the runtime of SND is substantially higher than that of MLND. Also, SND cannot be par-allelized any better than MLND <ref> [29, 1] </ref>; therefore, it will always be slower than MLND. 8 Characterization of Different Graph Partitioning Schemes Due to the importance of the problem, a large number of graph partitioning schemes have been developed.
Reference: [2] <author> Stephen T. Barnard and Horst D. Simon. </author> <title> A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems. </title> <booktitle> In Proceedings of the sixth SIAM conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 711-718, </pages> <year> 1993. </year>
Reference-contexts: However, these methods are very expensive since they require the computation of the eigenvector corresponding to the second smallest eigenvalue (Fiedler vector). Execution time of the spectral methods can be reduced if computation of the Fiedler vector is done by using a multilevel algorithm <ref> [2] </ref>. This multilevel spectral bisection algorithm (MSB) usually manages to speed up the spectral partitioning methods by an order of magnitude without any loss in the quality of the edge-cut. However, even MSB can take a large amount of time. <p> A partition of G m can be obtained using various algorithms such as (a) spectral bisection <ref> [47, 46, 2, 24] </ref>, (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods [31, 3, 11, 12, 17, 5, 33, 21]. <p> These algorithms are described in the next sections. We choose not to use geometric bisection algorithms, since the coordinate information was not available for most of the test graphs. 4.1 Spectral Bisection (SB) In the spectral bisection algorithm, the spectral information is used to partition the graph <ref> [47, 2, 26] </ref>. <p> We believe that the BKL (*,1) refinement policy strikes a good balance between small edge-cut and fast execution. 6.4 Comparison of Our Multilevel Scheme with Other Partitioning Schemes The multilevel spectral bisection (MSB) <ref> [2] </ref> has been shown to be an effective method for partitioning unstructured problems in a variety of applications. The MSB algorithm coarsens the graph down to a few hundred vertices using random matching. It partitions the coarse graph using spectral bisection and obtains the Fiedler vector of the coarser graph. <p> For the sake of simplicity, we have chosen to represent each property in terms of a small discrete scale. In absence of extensive data, we could not have done any better anyway. In Table 9 we show three different variations of spectral partitioning <ref> [47, 46, 26, 2] </ref>, the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition [38, 25], and two variants of geometric partitioning [37, 36, 16].
Reference: [3] <author> Earl R. Barnes. </author> <title> An algorithm for partitioning the nodes of a graph. </title> <journal> SIAM J. Algebraic Discrete Methods, </journal> <volume> 3(4) </volume> <pages> 541-550, </pages> <month> December </month> <year> 1984. </year>
Reference-contexts: A partition of G m can be obtained using various algorithms such as (a) spectral bisection [47, 46, 2, 24], (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods <ref> [31, 3, 11, 12, 17, 5, 33, 21] </ref>. Since the size of the coarser graph G m is small (i.e., jV m j &lt; 100), this step takes a small amount of time. We implemented four different algorithms for partitioning the coarse graph. The first algorithm uses the spectral bisection.
Reference: [4] <author> T. Bui and C. Jones. </author> <title> A heuristic for reducing fill in sparse matrix factorization. </title> <booktitle> In 6th SIAM Conf. Parallel Processing for Scientific Computing, </booktitle> <pages> pages 445-452, </pages> <year> 1993. </year>
Reference-contexts: Another class of graph partitioning algorithms reduces the size of the graph (i.e., coarsen the graph) by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph. These are called multilevel graph partitioning schemes <ref> [4, 7, 19, 20, 26, 10, 43] </ref>. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed [4, 26, 7, 20, 10] that further refine the partition during the uncoarsening phase. <p> These are called multilevel graph partitioning schemes [4, 7, 19, 20, 26, 10, 43]. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed <ref> [4, 26, 7, 20, 10] </ref> that further refine the partition during the uncoarsening phase. These schemes tend to give good partitions at a reasonable cost. <p> Recently, a number of multilevel algorithms have been proposed [4, 26, 7, 20, 10] that further refine the partition during the uncoarsening phase. These schemes tend to give good partitions at a reasonable cost. Bui and Jones <ref> [4] </ref> use random maximal matching to successively coarsen the graph down to a few hundred vertices; they partition the smallest graph and then uncoarsen the graph level by level, applying Kernighan-Lin to refine the partition. <p> During the uncoarsening phase the light lines indicate projected partitions, and dark lines indicate partitions that were produced after refinement. Two main approaches have been proposed for obtaining coarser graphs. The first approach is based on finding a random matching and collapsing the matched vertices into a multinode <ref> [4, 26] </ref>, while the second approach is based on creating multinodes that are made of groups of vertices that are highly connected [7, 19, 20, 10]. The later approach is suited for graphs arising in VLSI applications, since these graphs have highly connected components. <p> In the remaining sections we describe four ways that we used to select maximal matchings for coarsening. Random Matching (RM) A maximal matching can be generated efficiently using a randomized algorithm. In our experiments we used a randomized algorithm similar to that described in <ref> [4, 26] </ref>. The random maximal matching algorithm is the following. The vertices are visited in random order. If a vertex u has not been matched yet, then we randomly select one of its unmatched adjacent vertices. <p> It may seem that the light-edge matching does not perform any useful transformation during coarsening. However, the average degree of G iC1 produced by LEM is significantly higher than that of G i . This is important for certain partitioning heuristics such a Kernighan-Lin <ref> [4] </ref>, because they produce good partitions in small amount of time for graphs with high average degree. To compute a matching with minimal weight we only need to slightly modify the algorithm for computing the maximal-weight matching in Section 3. <p> One such algorithm is by Fiduccia and Mattheyses [9] that reduces the complexity to O.jE j/, by using appropriate data structures. The Kernighan-Lin algorithm finds locally optimal partitions when it starts with a good initial partition and when the average degree of the graph is large <ref> [4] </ref>. If no good initial partition is known, the KL algorithm is repeated with different randomly selected initial partitions, and the one that yields the smallest edge-cut is selected. Requiring multiple runs can be expensive, especially if the graph is large.
Reference: [5] <author> T. N. Bui, S. Chaudhuri, F. T. Leighton, and M. Sipser. </author> <title> Graph bisection algorithms with good average case behavior. </title> <journal> Combi-natorica, </journal> <volume> 7(2) </volume> <pages> 171-191, </pages> <year> 1987. </year>
Reference-contexts: A partition of G m can be obtained using various algorithms such as (a) spectral bisection [47, 46, 2, 24], (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods <ref> [31, 3, 11, 12, 17, 5, 33, 21] </ref>. Since the size of the coarser graph G m is small (i.e., jV m j &lt; 100), this step takes a small amount of time. We implemented four different algorithms for partitioning the coarse graph. The first algorithm uses the spectral bisection.
Reference: [6] <author> Tony F. Chan, John R. Gilbert, and Shang-Hua Teng. </author> <title> Geometric spectral partitioning. </title> <type> Technical report, </type> <note> Xerox PARC Tech. Report., 1994. Available at ftp://parcftp.xerox.com/pub/gilbert/index.html. </note>
Reference-contexts: Geometric graph partitioning algorithms are applicable only if coordinates are available for the vertices of the graph. In many problem areas (e.g., linear programming, VLSI), there is no geometry associated with the graph. Recently, an algorithm has been proposed to compute coordinates for graph vertices <ref> [6] </ref> by using spectral methods. But these methods are much more expensive and dominate the overall time taken by the graph partitioning algorithm.
Reference: [7] <author> Chung-Kuan Cheng and Yen-Chuen A. Wei. </author> <title> An improved two-way partitioning algorithm with stable performance. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> 10(12) </volume> <pages> 1502-1511, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Another class of graph partitioning algorithms reduces the size of the graph (i.e., coarsen the graph) by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph. These are called multilevel graph partitioning schemes <ref> [4, 7, 19, 20, 26, 10, 43] </ref>. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed [4, 26, 7, 20, 10] that further refine the partition during the uncoarsening phase. <p> These are called multilevel graph partitioning schemes [4, 7, 19, 20, 26, 10, 43]. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed <ref> [4, 26, 7, 20, 10] </ref> that further refine the partition during the uncoarsening phase. These schemes tend to give good partitions at a reasonable cost. <p> The first approach is based on finding a random matching and collapsing the matched vertices into a multinode [4, 26], while the second approach is based on creating multinodes that are made of groups of vertices that are highly connected <ref> [7, 19, 20, 10] </ref>. The later approach is suited for graphs arising in VLSI applications, since these graphs have highly connected components. <p> So, by creating multinodes that contain these subgraphs, we make it easier for the partitioning algorithm to find a good bisection. Note that this scheme tries to approximate the graph coarsening schemes that are based on finding highly connected components <ref> [7, 19, 20, 10] </ref>. As in the previous schemes for computing the matching, we compute the heavy clique matching using a randomized algorithm.
Reference: [8] <author> Pedro Diniz, Steve Plimpton, Bruce Hendrickson, and Robert Leland. </author> <title> Parallel algorithms for dynamically partitioning unstructured grids. </title> <booktitle> In Proceedings of the seventh SIAM conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 615-620, </pages> <year> 1995. </year>
Reference-contexts: This can happen if the graph arises from an adaptively refined mesh. Schemes that rely on coordinate information do not seem to have this limitation, and in principle it appears that these schemes can be parallelized quite effectively. However, all available parallel formulation of these schemes <ref> [23, 8] </ref> obtained no better speedup than obtained for the multilevel scheme in [29]. 9 Conclusion and Direction for Future Research Our experiments with multilevel schemes have shown that they work quite well for many different types of coarsening, initial partition, and refinement schemes.
Reference: [9] <author> C. M. Fiduccia and R. M. Mattheyses. </author> <title> A linear time heuristic for improving network partitions. </title> <booktitle> In In Proceedings 19th IEEE Design Automation Conference, </booktitle> <pages> pages 175-181, </pages> <year> 1982. </year>
Reference-contexts: Each iteration of the KL algorithm described in [31] takes O.jE j log jE j/ time. Several improvements to the original KL algorithm have been developed. One such algorithm is by Fiduccia and Mattheyses <ref> [9] </ref> that reduces the complexity to O.jE j/, by using appropriate data structures. The Kernighan-Lin algorithm finds locally optimal partitions when it starts with a good initial partition and when the average degree of the graph is large [4]. <p> ten different runs to find a good partition. 2 Coordinates for the vertices of the successive coarser graphs can be constructed by taking the midpoint of the coordinates of the combined vertices. 8 Our implementation of the Kernighan-Lin algorithm is based on the algorithm described by Fiduccia and Matthey--ses 3 <ref> [9] </ref>, with certain modifications that significantly reduce the run time. Suppose P is the initial partition of the vertices of G D .V ; E /. <p> After moving v, v is marked so it will not be considered again in the same iteration, and the gains of the vertices adjacent to v are updated to reflect the change in the partition. The original KL algorithm <ref> [9] </ref>, continues moving vertices between the partitions, until all the vertices have been moved. However, in our implementation, the KL algorithm terminates when the edge-cut does not decrease after x vertex moves. <p> In our experiments, we found that GGGP takes somewhat less time than GGP for partitioning the coarse graph 3 The algorithm described by Fiduccia and Mattheyses (FM) <ref> [9] </ref>, is slightly different than that originally developed by Kernighan and Lin (KL) [31].
Reference: [10] <author> J. Garbers, H. J. Promel, and A. Steger. </author> <title> Finding clusters in VLSI circuits. </title> <booktitle> In Proceedings of IEEE International Conference on Computer Aided Design, </booktitle> <pages> pages 520-523, </pages> <year> 1990. </year>
Reference-contexts: Another class of graph partitioning algorithms reduces the size of the graph (i.e., coarsen the graph) by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph. These are called multilevel graph partitioning schemes <ref> [4, 7, 19, 20, 26, 10, 43] </ref>. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed [4, 26, 7, 20, 10] that further refine the partition during the uncoarsening phase. <p> These are called multilevel graph partitioning schemes [4, 7, 19, 20, 26, 10, 43]. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed <ref> [4, 26, 7, 20, 10] </ref> that further refine the partition during the uncoarsening phase. These schemes tend to give good partitions at a reasonable cost. <p> The first approach is based on finding a random matching and collapsing the matched vertices into a multinode [4, 26], while the second approach is based on creating multinodes that are made of groups of vertices that are highly connected <ref> [7, 19, 20, 10] </ref>. The later approach is suited for graphs arising in VLSI applications, since these graphs have highly connected components. <p> So, by creating multinodes that contain these subgraphs, we make it easier for the partitioning algorithm to find a good bisection. Note that this scheme tries to approximate the graph coarsening schemes that are based on finding highly connected components <ref> [7, 19, 20, 10] </ref>. As in the previous schemes for computing the matching, we compute the heavy clique matching using a randomized algorithm.
Reference: [11] <author> A. George. </author> <title> Nested dissection of a regular finite-element mesh. </title> <journal> SIAM Journal on Numerical Ananlysis, </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference-contexts: A partition of G m can be obtained using various algorithms such as (a) spectral bisection [47, 46, 2, 24], (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods <ref> [31, 3, 11, 12, 17, 5, 33, 21] </ref>. Since the size of the coarser graph G m is small (i.e., jV m j &lt; 100), this step takes a small amount of time. We implemented four different algorithms for partitioning the coarse graph. The first algorithm uses the spectral bisection. <p> In absence of extensive data, we could not have done any better anyway. In Table 9 we show three different variations of spectral partitioning [47, 46, 26, 2], the multilevel partitioning described in this paper, the levelized nested dissection <ref> [11] </ref>, the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition [38, 25], and two variants of geometric partitioning [37, 36, 16]. Table 9: Characteristics of various graph partitioning algorithms. For each graph partitioning algorithm, Table 9 shows a number of characteristics.
Reference: [12] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: If parallel direct methods are used to solve a sparse system of equations, then a graph partitioning algorithm can be used to compute a fill reducing ordering that lead to high degree of concurrency in the factorization phase <ref> [32, 12] </ref>. The multiple minimum degree ordering used almost exclusively in serial direct methods is not suitable for parallel direct methods, as it provides very little concurrency in the parallel factorization phase. The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. <p> Thus, in order to minimize the communication overhead, we need to obtain a p-way partition of G A , and then distribute the rows of A according to this partition. Another important application of recursive bisection is to find a fill reducing ordering for sparse matrix factorization <ref> [12, 32, 22] </ref>. These algorithms are generally referred to as nested dissection ordering algorithms. Nested dissection recursively splits a graph into almost equal halves by selecting a vertex separator until the desired number of partitions are obtained. <p> Thus, the problem of performing a k-way partition can be solved by performing a sequence of 2-way partitions or bisections. Even though this scheme does not necessarily lead to optimal partition, it is used extensively due to its simplicity <ref> [12, 22] </ref>. 2.1 Multilevel Graph Bisection The graph G can be bisected using a multilevel algorithm. The basic structure of a multilevel algorithm is very simple. <p> A partition of G m can be obtained using various algorithms such as (a) spectral bisection [47, 46, 2, 24], (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods <ref> [31, 3, 11, 12, 17, 5, 33, 21] </ref>. Since the size of the coarser graph G m is small (i.e., jV m j &lt; 100), this step takes a small amount of time. We implemented four different algorithms for partitioning the coarse graph. The first algorithm uses the spectral bisection. <p> KL algorithm is described in Appendix A.3. 4.3 Graph Growing Algorithm (GGP) Another simple way of bisecting the graph is to start from a vertex and grow a region around it in a breath-first fashion, until half of the vertices have been included (or half of the total vertex weight) <ref> [12, 17, 39] </ref>. The quality of the graph growing algorithm is sensitive to the choice of a vertex from which to start growing the graph, and different starting vertices yield different edge-cuts. To partially solve this problem, we randomly select 10 vertices and we grow 10 different regions. <p> However, another, even more important, advantage of MLND over MMD, is that it produces orderings that exhibit significantly more concurrency than MMD. The elimination trees produced by MMD (a) exhibit little concurrency (long and slender), and (b) are unbalanced so that subtree-to-subcube mappings lead to significant load imbalances <ref> [32, 12, 18] </ref>. On the other hand, orderings based on nested dissection produce orderings that have both more concur-rency and better balance [30, 22].
Reference: [13] <author> A. George and J. W.-H. Liu. </author> <title> The evolution of the minimum degree ordering algorithm. </title> <journal> SIAM Review, </journal> <volume> 31(1) </volume> <pages> 1-19, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: On a parallel computer, a fill reducing ordering, besides minimizing the operation count, should also increase the degree of concurrency that can be exploited during factorization. In general, nested dissection based orderings exhibit more concurrency during factorization than minimum degree orderings <ref> [13, 35] </ref>. The minimum degree [13] ordering heuristic is the most widely used fill reducing algorithm that is used to order sparse matrices for factorization on serial computers. The minimum degree algorithm has been found to produce very good orderings. <p> On a parallel computer, a fill reducing ordering, besides minimizing the operation count, should also increase the degree of concurrency that can be exploited during factorization. In general, nested dissection based orderings exhibit more concurrency during factorization than minimum degree orderings [13, 35]. The minimum degree <ref> [13] </ref> ordering heuristic is the most widely used fill reducing algorithm that is used to order sparse matrices for factorization on serial computers. The minimum degree algorithm has been found to produce very good orderings.
Reference: [14] <author> Madhurima Ghose and Edward Rothberg. </author> <title> A parallel implementation of the multiple minimum degree ordering heuristic. </title> <type> Technical report, </type> <institution> Old Dominion University, </institution> <address> Norfolk, VA, </address> <year> 1994. </year>
Reference-contexts: The MMD algorithm is usually two to three times faster than MLND for ordering the matrices in Table 1. However, efforts to parallelize the MMD algorithm have had no success <ref> [14] </ref>.
Reference: [15] <author> J. R. Gilbert and E. Zmijewski. </author> <title> A parallel graph partitioning algorithm for a message-passing multiprocessor. </title> <journal> International Journal of Parallel Programming, </journal> (16):498-513, 1987. 
Reference-contexts: The coarsening phase of these methods is relatively easy to parallelize [29], but the Kernighan-Lin heuristic used in the refinement phase is very difficult to parallelize <ref> [15] </ref>. Since both the coarsening phase and the refinement phase with the Kernighan-Lin heuristic take roughly the same amount of time, the overall run-time of the multilevel scheme of [26] cannot be reduced significantly. Our new faster methods for refinement reduce this bottleneck substantially. <p> Schemes that require multiple trials are inherently parallel, as different trials can be done on different processors. In contrast, a single trial of Kernighan-Lin is very difficult to parallelize <ref> [15] </ref>, and appears inherently serial in nature. Multilevel schemes that do not rely upon Kernighan-Lin [29] and the spectral bisection scheme are moderately parallel in nature.
Reference: [16] <author> John R. Gilbert, Gary L. Miller, and Shang-Hua Teng. </author> <title> Geometric mesh partitioning: Implementation and experiments. </title> <booktitle> In Proceedings of International Parallel Processing Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: However, due to the randomized nature of these algorithms, multiple trials are often required (5 to 50) to obtain solutions that are comparable in quality to spectral methods. Multiple trials do increase the time <ref> [16] </ref>, but the overall runtime is still substantially lower than the time required by the spectral methods. Geometric graph partitioning algorithms are applicable only if coordinates are available for the vertices of the graph. In many problem areas (e.g., linear programming, VLSI), there is no geometry associated with the graph. <p> we show three different variations of spectral partitioning [47, 46, 26, 2], the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition [38, 25], and two variants of geometric partitioning <ref> [37, 36, 16] </ref>. Table 9: Characteristics of various graph partitioning algorithms. For each graph partitioning algorithm, Table 9 shows a number of characteristics. The first column shows the number of trials that are often performed for each partitioning algorithm. <p> The quality of geometric partitioning with Kernighan-Lin refinement is also equally good, when around 50 or more trials are performed 5 . The quality of 5 This conclusion is an extrapolation of the results presented in <ref> [16] </ref> where it was shown that the geometric partitioning with 30 trials (default 22 the other schemes is worse than the above three by various degrees. Note that for both Kernighan-Lin partitioning and geometric partitioning the quality improves as the number of trials increases.
Reference: [17] <author> T. Goehring and Y. Saad. </author> <title> Heuristic algorithms for automatic graph partitioning. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1994. </year>
Reference-contexts: A partition of G m can be obtained using various algorithms such as (a) spectral bisection [47, 46, 2, 24], (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods <ref> [31, 3, 11, 12, 17, 5, 33, 21] </ref>. Since the size of the coarser graph G m is small (i.e., jV m j &lt; 100), this step takes a small amount of time. We implemented four different algorithms for partitioning the coarse graph. The first algorithm uses the spectral bisection. <p> KL algorithm is described in Appendix A.3. 4.3 Graph Growing Algorithm (GGP) Another simple way of bisecting the graph is to start from a vertex and grow a region around it in a breath-first fashion, until half of the vertices have been included (or half of the total vertex weight) <ref> [12, 17, 39] </ref>. The quality of the graph growing algorithm is sensitive to the choice of a vertex from which to start growing the graph, and different starting vertices yield different edge-cuts. To partially solve this problem, we randomly select 10 vertices and we grow 10 different regions.
Reference: [18] <author> Anshul Gupta, George Karypis, and Vipin Kumar. </author> <title> Highly scalable parallel algorithms for sparse matrix factorization. </title> <type> Technical Report 94-63, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Computing. Available on WWW at URL http://www.cs.umn.edu/karypis/papers/sparse-cholesky.ps. </note>
Reference-contexts: In particular, in parallel direct solvers, the time for computing ordering using MSB can be several orders of magnitude higher than the time taken by the parallel factorization algorithm, and thus ordering time can dominate the overall time to solve the problem <ref> [18] </ref>. Another class of graph partitioning techniques uses the geometric information of the graph to find a good partition. Geometric partitioning algorithms [23, 48, 37, 36, 38] tend to be fast but often yield partitions that are worse than those obtained by spectral methods. <p> Even though multilevel algorithms are quite fast compared to spectral methods, they can still be the bottleneck if the sparse system of equations is being solved in parallel <ref> [32, 18] </ref>. The coarsening phase of these methods is relatively easy to parallelize [29], but the Kernighan-Lin heuristic used in the refinement phase is very difficult to parallelize [15]. <p> However, another, even more important, advantage of MLND over MMD, is that it produces orderings that exhibit significantly more concurrency than MMD. The elimination trees produced by MMD (a) exhibit little concurrency (long and slender), and (b) are unbalanced so that subtree-to-subcube mappings lead to significant load imbalances <ref> [32, 12, 18] </ref>. On the other hand, orderings based on nested dissection produce orderings that have both more concur-rency and better balance [30, 22].
Reference: [19] <author> Lars Hagen and Andrew Kahng. </author> <title> Fast spectral methods for ratio cut partitioning and clustering. </title> <booktitle> In Proceedings of IEEE International Conference on Computer Aided Design, </booktitle> <pages> pages 10-13, </pages> <year> 1991. </year> <month> 24 </month>
Reference-contexts: Another class of graph partitioning algorithms reduces the size of the graph (i.e., coarsen the graph) by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph. These are called multilevel graph partitioning schemes <ref> [4, 7, 19, 20, 26, 10, 43] </ref>. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed [4, 26, 7, 20, 10] that further refine the partition during the uncoarsening phase. <p> The first approach is based on finding a random matching and collapsing the matched vertices into a multinode [4, 26], while the second approach is based on creating multinodes that are made of groups of vertices that are highly connected <ref> [7, 19, 20, 10] </ref>. The later approach is suited for graphs arising in VLSI applications, since these graphs have highly connected components. <p> So, by creating multinodes that contain these subgraphs, we make it easier for the partitioning algorithm to find a good bisection. Note that this scheme tries to approximate the graph coarsening schemes that are based on finding highly connected components <ref> [7, 19, 20, 10] </ref>. As in the previous schemes for computing the matching, we compute the heavy clique matching using a randomized algorithm.
Reference: [20] <author> Lars Hagen and Andrew Kahng. </author> <title> A new approach to effective circuit clustering. </title> <booktitle> In Proceedings of IEEE International Con--ference on Computer Aided Design, </booktitle> <pages> pages 422-427, </pages> <year> 1992. </year>
Reference-contexts: Another class of graph partitioning algorithms reduces the size of the graph (i.e., coarsen the graph) by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph. These are called multilevel graph partitioning schemes <ref> [4, 7, 19, 20, 26, 10, 43] </ref>. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed [4, 26, 7, 20, 10] that further refine the partition during the uncoarsening phase. <p> These are called multilevel graph partitioning schemes [4, 7, 19, 20, 26, 10, 43]. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed <ref> [4, 26, 7, 20, 10] </ref> that further refine the partition during the uncoarsening phase. These schemes tend to give good partitions at a reasonable cost. <p> The first approach is based on finding a random matching and collapsing the matched vertices into a multinode [4, 26], while the second approach is based on creating multinodes that are made of groups of vertices that are highly connected <ref> [7, 19, 20, 10] </ref>. The later approach is suited for graphs arising in VLSI applications, since these graphs have highly connected components. <p> So, by creating multinodes that contain these subgraphs, we make it easier for the partitioning algorithm to find a good bisection. Note that this scheme tries to approximate the graph coarsening schemes that are based on finding highly connected components <ref> [7, 19, 20, 10] </ref>. As in the previous schemes for computing the matching, we compute the heavy clique matching using a randomized algorithm.
Reference: [21] <author> S.W. Hammond. </author> <title> Mapping Unstructured Grid Problems to Massively Parallel Computers. </title> <type> PhD thesis, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, New York, </address> <year> 1992. </year>
Reference-contexts: A partition of G m can be obtained using various algorithms such as (a) spectral bisection [47, 46, 2, 24], (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods <ref> [31, 3, 11, 12, 17, 5, 33, 21] </ref>. Since the size of the coarser graph G m is small (i.e., jV m j &lt; 100), this step takes a small amount of time. We implemented four different algorithms for partitioning the coarse graph. The first algorithm uses the spectral bisection.
Reference: [22] <author> M. T. Heath, E. G.-Y. Ng, and Barry W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: Thus, in order to minimize the communication overhead, we need to obtain a p-way partition of G A , and then distribute the rows of A according to this partition. Another important application of recursive bisection is to find a fill reducing ordering for sparse matrix factorization <ref> [12, 32, 22] </ref>. These algorithms are generally referred to as nested dissection ordering algorithms. Nested dissection recursively splits a graph into almost equal halves by selecting a vertex separator until the desired number of partitions are obtained. <p> Thus, the problem of performing a k-way partition can be solved by performing a sequence of 2-way partitions or bisections. Even though this scheme does not necessarily lead to optimal partition, it is used extensively due to its simplicity <ref> [12, 22] </ref>. 2.1 Multilevel Graph Bisection The graph G can be bisected using a multilevel algorithm. The basic structure of a multilevel algorithm is very simple. <p> The elimination trees produced by MMD (a) exhibit little concurrency (long and slender), and (b) are unbalanced so that subtree-to-subcube mappings lead to significant load imbalances [32, 12, 18]. On the other hand, orderings based on nested dissection produce orderings that have both more concur-rency and better balance <ref> [30, 22] </ref>. Therefore, when the factorization is performed in parallel, the better utilization of the processors can cause the ratio of the run time of parallel factorization algorithms running ordered using MMD and that using MLND to be substantially higher than the ratio of their respective operation counts.
Reference: [23] <author> M. T. Heath and Padma Raghavan. </author> <title> A Cartesian parallel nested dissection algorithm. </title> <journal> SIAM Journal of Matrix Analysis and Applications, </journal> <volume> 16(1) </volume> <pages> 235-253, </pages> <year> 1995. </year>
Reference-contexts: Another class of graph partitioning techniques uses the geometric information of the graph to find a good partition. Geometric partitioning algorithms <ref> [23, 48, 37, 36, 38] </ref> tend to be fast but often yield partitions that are worse than those obtained by spectral methods. Among the most prominent of these schemes is the algorithm described in [37, 36]. <p> In Table 9 we show three different variations of spectral partitioning [47, 46, 26, 2], the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) <ref> [23] </ref>, two variations of the inertial partition [38, 25], and two variants of geometric partitioning [37, 36, 16]. Table 9: Characteristics of various graph partitioning algorithms. For each graph partitioning algorithm, Table 9 shows a number of characteristics. <p> This can happen if the graph arises from an adaptively refined mesh. Schemes that rely on coordinate information do not seem to have this limitation, and in principle it appears that these schemes can be parallelized quite effectively. However, all available parallel formulation of these schemes <ref> [23, 8] </ref> obtained no better speedup than obtained for the multilevel scheme in [29]. 9 Conclusion and Direction for Future Research Our experiments with multilevel schemes have shown that they work quite well for many different types of coarsening, initial partition, and refinement schemes.
Reference: [24] <author> Bruce Hendrickson and Robert Leland. </author> <title> An improved spectral graph partitioning algorithm for mapping parallel computations. </title> <type> Technical Report SAND92-1460, </type> <institution> Sandia National Laboratories, </institution> <year> 1992. </year>
Reference-contexts: The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. Spectral partitioning methods are known to produce good partitions for a wide class of problems, and they are used quite extensively <ref> [47, 46, 24] </ref>. However, these methods are very expensive since they require the computation of the eigenvector corresponding to the second smallest eigenvalue (Fiedler vector). Execution time of the spectral methods can be reduced if computation of the Fiedler vector is done by using a multilevel algorithm [2]. <p> A partition of G m can be obtained using various algorithms such as (a) spectral bisection <ref> [47, 46, 2, 24] </ref>, (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods [31, 3, 11, 12, 17, 5, 33, 21].
Reference: [25] <author> Bruce Hendrickson and Robert Leland. </author> <title> The chaco user's guide, version 1.0. </title> <type> Technical Report SAND93-2339, </type> <institution> Sandia National Laboratories, </institution> <year> 1993. </year>
Reference-contexts: For matrix A an n-vertex graph G A , can be constructed such that each row of the matrix corresponds to a vertex, 1 We used the MSB algorithm in the Chaco <ref> [25] </ref> graph partitioning package to obtain the timings for MSB. 3 and if row i has a nonzero entry in column j (i 6D j ), then there is an edge between vertex i and vertex j . <p> Note that MSB is a significantly different scheme than the multilevel scheme that uses spectral bisection to partition the graph at the coarsest level. We used the MSB algorithm in the Chaco <ref> [25] </ref> graph partitioning package to produce partitions for some of the matrices in Table 1 and compared the results against the partitions produced by our multilevel algorithm that uses HEM during coarsening phase, GGGP during 15 the cut-size of our multilevel algorithm to that of the MSB algorithm is plotted for <p> The graph partitioning package Chaco implements its own multilevel graph partitioning algorithm that is modeled after the algorithm by Hendrickson and Leland <ref> [26, 25] </ref>. This algorithm, which we refer to as Chaco-ML, uses random matching during coarsening, spectral bisection for partitioning the coarse graph, and Kernighan-Lin refinement every other coarsening level during the uncoarsening phase. Figure 5 shows the relative performance of our multilevel algorithms compared to Chaco-ML. <p> In Table 9 we show three different variations of spectral partitioning [47, 46, 26, 2], the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition <ref> [38, 25] </ref>, and two variants of geometric partitioning [37, 36, 16]. Table 9: Characteristics of various graph partitioning algorithms. For each graph partitioning algorithm, Table 9 shows a number of characteristics. The first column shows the number of trials that are often performed for each partitioning algorithm.
Reference: [26] <author> Bruce Hendrickson and Robert Leland. </author> <title> A multilevel algorithm for partitioning graphs. </title> <type> Technical Report SAND93-1301, </type> <institution> Sandia National Laboratories, </institution> <year> 1993. </year>
Reference-contexts: Another class of graph partitioning algorithms reduces the size of the graph (i.e., coarsen the graph) by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph. These are called multilevel graph partitioning schemes <ref> [4, 7, 19, 20, 26, 10, 43] </ref>. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed [4, 26, 7, 20, 10] that further refine the partition during the uncoarsening phase. <p> These are called multilevel graph partitioning schemes [4, 7, 19, 20, 26, 10, 43]. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed <ref> [4, 26, 7, 20, 10] </ref> that further refine the partition during the uncoarsening phase. These schemes tend to give good partitions at a reasonable cost. <p> Bui and Jones [4] use random maximal matching to successively coarsen the graph down to a few hundred vertices; they partition the smallest graph and then uncoarsen the graph level by level, applying Kernighan-Lin to refine the partition. Hendrickson and Leland <ref> [26] </ref> enhance this approach by using edge and vertex weights to capture the collapsing of the vertex and edges. In particular, this latter work showed that multilevel schemes can provide better partitions than spectral methods at lower cost for a variety of finite element problems. <p> We also present a new variation 2 of the Kernighan-Lin algorithm for refining the partition during the uncoarsening phase that is much faster than the Kernighan-Lin refinement used in <ref> [26] </ref>. We test our scheme on a large number of graphs arising in various domains including finite element methods, linear programming, VLSI, and transportation. <p> Our experiments show that our scheme consistently produces partitions that are better than those produced by spectral partitioning schemes in substantially smaller timer (10 to 35 times faster than multilevel spectral bisection 1 . Compared with the multilevel scheme of <ref> [26] </ref>, our scheme is about two to seven times faster, and is consistently better in terms of cut size. Much of the improvement in run time comes from our faster refinement heuristic, and the improvement in quality is due to the heavy-edge heuristic used during coarsening. <p> Since both the coarsening phase and the refinement phase with the Kernighan-Lin heuristic take roughly the same amount of time, the overall run-time of the multilevel scheme of <ref> [26] </ref> cannot be reduced significantly. Our new faster methods for refinement reduce this bottleneck substantially. In fact our parallel implementation [29] of this multilevel partitioning is able to get a speedup of as much as 56 on a 128-processor Cray T3D for moderate size problems. <p> During the uncoarsening phase the light lines indicate projected partitions, and dark lines indicate partitions that were produced after refinement. Two main approaches have been proposed for obtaining coarser graphs. The first approach is based on finding a random matching and collapsing the matched vertices into a multinode <ref> [4, 26] </ref>, while the second approach is based on creating multinodes that are made of groups of vertices that are highly connected [7, 19, 20, 10]. The later approach is suited for graphs arising in VLSI applications, since these graphs have highly connected components. <p> In the remaining sections we describe four ways that we used to select maximal matchings for coarsening. Random Matching (RM) A maximal matching can be generated efficiently using a randomized algorithm. In our experiments we used a randomized algorithm similar to that described in <ref> [4, 26] </ref>. The random maximal matching algorithm is the following. The vertices are visited in random order. If a vertex u has not been matched yet, then we randomly select one of its unmatched adjacent vertices. <p> These algorithms are described in the next sections. We choose not to use geometric bisection algorithms, since the coordinate information was not available for most of the test graphs. 4.1 Spectral Bisection (SB) In the spectral bisection algorithm, the spectral information is used to partition the graph <ref> [47, 2, 26] </ref>. <p> The graph partitioning package Chaco implements its own multilevel graph partitioning algorithm that is modeled after the algorithm by Hendrickson and Leland <ref> [26, 25] </ref>. This algorithm, which we refer to as Chaco-ML, uses random matching during coarsening, spectral bisection for partitioning the coarse graph, and Kernighan-Lin refinement every other coarsening level during the uncoarsening phase. Figure 5 shows the relative performance of our multilevel algorithms compared to Chaco-ML. <p> For the sake of simplicity, we have chosen to represent each property in terms of a small discrete scale. In absence of extensive data, we could not have done any better anyway. In Table 9 we show three different variations of spectral partitioning <ref> [47, 46, 26, 2] </ref>, the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition [38, 25], and two variants of geometric partitioning [37, 36, 16].
Reference: [27] <author> G. Karypis and V. Kumar. </author> <title> Analysis of multilevel graph partitioning. </title> <type> Technical Report TR 95-037, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year> <note> Also available on WWW at URL http://www.cs.umn.edu/karypis/papers/mlevel analysis.ps. A short version appears in Supercomputing 95. </note>
Reference-contexts: Coarsening a graph using matchings preserves many properties of the original graph. If G 0 is (maximal) planar, the G i is also (maximal) planar [34]. This property is used to show that the multilevel algorithm produces partitions that 5 are provably good for planar graphs <ref> [27] </ref>. <p> Hence, by selecting a maximal matching M i whose edges have a large weight, we can decrease the edge-weight of the coarser graph by a greater amount. As the analysis in <ref> [27] </ref> shows, since the coarser graph has smaller edge-weight, it also has a smaller edge-cut. Finding a maximal matching that contains edges with large weight is the idea behind the heavy-edge matching. <p> The global view of multilevel graph partitioning is among the highest of that of the other schemes. This is because the multilevel graph partitioning captures global graph structure at two different levels. First, it captures global structure through the process of coarsening <ref> [27] </ref>, and second, it captures global structure during the initial graph partitioning by performing multiple trials. The sixth column of Table 9 shows the relative time required by different graph partitioning schemes. CND, inertial, and geometric partitioning with one trial require relatively small amount of time.
Reference: [28] <author> G. Karypis and V. Kumar. </author> <title> Multilevel graph partition and sparse matrix ordering. </title> <booktitle> In Intl. Conf. on Parallel Processing, </booktitle> <year> 1995. </year> <note> Available on WWW at URL http://www.cs.umn.edu/karypis/papers/mlevel serial.ps. </note>
Reference-contexts: Section 7 compares the quality of the orderings produced by multilevel nested dissection to those produced by multiple minimum degree and spectral nested dissection. Section 9 provides a summary of the various results. A short version of this paper appears in <ref> [28] </ref>. 2 Graph Partitioning The k-way graph partitioning problem is defined as follows: Given a graph G D .V ; E / with jV j D n, partition V into k subsets, V 1 ; V 2 ; : : : ; V k such that V i " V j
Reference: [29] <author> G. Karypis and V. Kumar. </author> <title> A parallel algorithms for multilevel graph partitioning and sparse matrix ordering. </title> <type> Technical Report TR 95-036, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year> <note> Also available on WWW at URL http://www.cs.umn.edu/karypis/papers/mlevel parallel.ps. A short version appears in Intl. Parallel Processing Symposium 1996. </note>
Reference-contexts: Even though multilevel algorithms are quite fast compared to spectral methods, they can still be the bottleneck if the sparse system of equations is being solved in parallel [32, 18]. The coarsening phase of these methods is relatively easy to parallelize <ref> [29] </ref>, but the Kernighan-Lin heuristic used in the refinement phase is very difficult to parallelize [15]. Since both the coarsening phase and the refinement phase with the Kernighan-Lin heuristic take roughly the same amount of time, the overall run-time of the multilevel scheme of [26] cannot be reduced significantly. <p> Since both the coarsening phase and the refinement phase with the Kernighan-Lin heuristic take roughly the same amount of time, the overall run-time of the multilevel scheme of [26] cannot be reduced significantly. Our new faster methods for refinement reduce this bottleneck substantially. In fact our parallel implementation <ref> [29] </ref> of this multilevel partitioning is able to get a speedup of as much as 56 on a 128-processor Cray T3D for moderate size problems. The remainder of the paper is organized as follows. Section 2 defines the graph partitioning problem and describes the basic ideas of multilevel graph partitioning. <p> Furthermore, for HEM and HCM, as the problem size increases UTime becomes an even smaller fraction of CTime. As discussed in the introduction, this is of particular importance when the parallel formulation of the multilevel algorithm is considered <ref> [29] </ref>. As the experiments show, HEM is an excellent matching scheme that results in good initial partitions, and requires the smallest overall run time. <p> On the other hand, the MLND algorithm is amenable to parallelization. In <ref> [29] </ref> we present a parallel formulation of our MLND algorithm that achieves a speedup of as much as 57 on 128-processor Cray T3D (over the serial algorithm running on a single T3D processor) for some graphs. Spectral nested dissection (SND) [45] can be used for ordering matrices for parallel factorization. <p> However, as discussed in Section 6.4, the runtime of SND is substantially higher than that of MLND. Also, SND cannot be par-allelized any better than MLND <ref> [29, 1] </ref>; therefore, it will always be slower than MLND. 8 Characterization of Different Graph Partitioning Schemes Due to the importance of the problem, a large number of graph partitioning schemes have been developed. <p> Schemes that require multiple trials are inherently parallel, as different trials can be done on different processors. In contrast, a single trial of Kernighan-Lin is very difficult to parallelize [15], and appears inherently serial in nature. Multilevel schemes that do not rely upon Kernighan-Lin <ref> [29] </ref> and the spectral bisection scheme are moderately parallel in nature. As discussed in [29], the asymptotic speedup for these schemes is bounded by O. p O. p/ speedup can be obtained in these schemes only if the graph is nearly well partitioned among processors. <p> In contrast, a single trial of Kernighan-Lin is very difficult to parallelize [15], and appears inherently serial in nature. Multilevel schemes that do not rely upon Kernighan-Lin <ref> [29] </ref> and the spectral bisection scheme are moderately parallel in nature. As discussed in [29], the asymptotic speedup for these schemes is bounded by O. p O. p/ speedup can be obtained in these schemes only if the graph is nearly well partitioned among processors. This can happen if the graph arises from an adaptively refined mesh. <p> Schemes that rely on coordinate information do not seem to have this limitation, and in principle it appears that these schemes can be parallelized quite effectively. However, all available parallel formulation of these schemes [23, 8] obtained no better speedup than obtained for the multilevel scheme in <ref> [29] </ref>. 9 Conclusion and Direction for Future Research Our experiments with multilevel schemes have shown that they work quite well for many different types of coarsening, initial partition, and refinement schemes. <p> The reason is that this combination requires very little time for refinement, which is the most serial part of the algorithm. The coarsening phase is relatively much easier to parallelize <ref> [29] </ref>.
Reference: [30] <author> George Karypis, Anshul Gupta, and Vipin Kumar. </author> <title> A parallel formulation of interior point algorithms. </title> <booktitle> In Supercomputing 94, </booktitle> <year> 1994. </year> <note> Available on WWW at URL http://www.cs.umn.edu/karypis/papers/interior-point.ps. </note>
Reference-contexts: The elimination trees produced by MMD (a) exhibit little concurrency (long and slender), and (b) are unbalanced so that subtree-to-subcube mappings lead to significant load imbalances [32, 12, 18]. On the other hand, orderings based on nested dissection produce orderings that have both more concur-rency and better balance <ref> [30, 22] </ref>. Therefore, when the factorization is performed in parallel, the better utilization of the processors can cause the ratio of the run time of parallel factorization algorithms running ordered using MMD and that using MLND to be substantially higher than the ratio of their respective operation counts.
Reference: [31] <author> B. W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> The Bell System Technical Journal, </journal> <year> 1970. </year>
Reference-contexts: A partition of G m can be obtained using various algorithms such as (a) spectral bisection [47, 46, 2, 24], (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods <ref> [31, 3, 11, 12, 17, 5, 33, 21] </ref>. Since the size of the coarser graph G m is small (i.e., jV m j &lt; 100), this step takes a small amount of time. We implemented four different algorithms for partitioning the coarse graph. The first algorithm uses the spectral bisection. <p> This algorithm is iterative and the number of iterations required depends on the desired accuracy. In our experiments, we set the accuracy to 10 2 and the maximum number of iterations to 100. 4.2 Kernighan-Lin Algorithm (KL) The Kernighan-Lin algorithm <ref> [31] </ref> is iterative in nature. It starts with an initial bipartition of the graph. In each iteration it searches for a subset of vertices, from each part of the graph such that swapping them leads to a partition with smaller edge-cut. <p> The algorithm continues by repeating the entire process. If it cannot find two such subsets, then the algorithm terminates, since the partition is at a local minimum and no further improvement can be made by the KL algorithm. Each iteration of the KL algorithm described in <ref> [31] </ref> takes O.jE j log jE j/ time. Several improvements to the original KL algorithm have been developed. One such algorithm is by Fiduccia and Mattheyses [9] that reduces the complexity to O.jE j/, by using appropriate data structures. <p> In our experiments, we found that GGGP takes somewhat less time than GGP for partitioning the coarse graph 3 The algorithm described by Fiduccia and Mattheyses (FM) [9], is slightly different than that originally developed by Kernighan and Lin (KL) <ref> [31] </ref>. <p> In absence of extensive data, we could not have done any better anyway. In Table 9 we show three different variations of spectral partitioning [47, 46, 26, 2], the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition <ref> [31] </ref>, the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition [38, 25], and two variants of geometric partitioning [37, 36, 16]. Table 9: Characteristics of various graph partitioning algorithms. For each graph partitioning algorithm, Table 9 shows a number of characteristics.
Reference: [32] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin/Cummings Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: A key step in each iteration of these methods is the multiplication of a sparse matrix and a (dense) vector. A good partition of the graph corresponding to matrix A can significantly reduce the amount of communication in parallel sparse matrix-vector multiplication <ref> [32] </ref>. If parallel direct methods are used to solve a sparse system of equations, then a graph partitioning algorithm can be used to compute a fill reducing ordering that lead to high degree of concurrency in the factorization phase [32, 12]. <p> If parallel direct methods are used to solve a sparse system of equations, then a graph partitioning algorithm can be used to compute a fill reducing ordering that lead to high degree of concurrency in the factorization phase <ref> [32, 12] </ref>. The multiple minimum degree ordering used almost exclusively in serial direct methods is not suitable for parallel direct methods, as it provides very little concurrency in the parallel factorization phase. The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. <p> Even though multilevel algorithms are quite fast compared to spectral methods, they can still be the bottleneck if the sparse system of equations is being solved in parallel <ref> [32, 18] </ref>. The coarsening phase of these methods is relatively easy to parallelize [29], but the Kernighan-Lin heuristic used in the refinement phase is very difficult to parallelize [15]. <p> One such example is the sparse-matrix vector multiplication y D Ax . Matrix A nfin and vector x is usually partitioned along rows, with each of the p processors receiving n= p rows of A, and the corresponding n= p elements of x <ref> [32] </ref>. <p> As discussed in <ref> [32] </ref>, any edges connecting vertices from two different partitions lead to communication for retrieving the value of vector x that is not local but is needed to perform the dot-product. <p> Thus, in order to minimize the communication overhead, we need to obtain a p-way partition of G A , and then distribute the rows of A according to this partition. Another important application of recursive bisection is to find a fill reducing ordering for sparse matrix factorization <ref> [12, 32, 22] </ref>. These algorithms are generally referred to as nested dissection ordering algorithms. Nested dissection recursively splits a graph into almost equal halves by selecting a vertex separator until the desired number of partitions are obtained. <p> However, another, even more important, advantage of MLND over MMD, is that it produces orderings that exhibit significantly more concurrency than MMD. The elimination trees produced by MMD (a) exhibit little concurrency (long and slender), and (b) are unbalanced so that subtree-to-subcube mappings lead to significant load imbalances <ref> [32, 12, 18] </ref>. On the other hand, orderings based on nested dissection produce orderings that have both more concur-rency and better balance [30, 22].
Reference: [33] <author> Tom Leighton and Satish Rao. </author> <title> An approximate max-flow min-cut theorem for uniform multicommodity flow problems with applications to approximation algorithms. </title> <booktitle> In 29th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 422-431, </pages> <year> 1988. </year>
Reference-contexts: A partition of G m can be obtained using various algorithms such as (a) spectral bisection [47, 46, 2, 24], (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods <ref> [31, 3, 11, 12, 17, 5, 33, 21] </ref>. Since the size of the coarser graph G m is small (i.e., jV m j &lt; 100), this step takes a small amount of time. We implemented four different algorithms for partitioning the coarse graph. The first algorithm uses the spectral bisection.
Reference: [34] <author> R. J. Lipton and R. E. Tarjan. </author> <title> A separator theorem for planar graphs. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 36 </volume> <pages> 177-189, </pages> <year> 1979. </year>
Reference-contexts: Coarsening a graph using matchings preserves many properties of the original graph. If G 0 is (maximal) planar, the G i is also (maximal) planar <ref> [34] </ref>. This property is used to show that the multilevel algorithm produces partitions that 5 are provably good for planar graphs [27].
Reference: [35] <author> J. W.-H. Liu. </author> <title> Modification of the minimum degree algorithm by multiple elimination. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 11 </volume> <pages> 141-153, </pages> <year> 1985. </year>
Reference-contexts: We also used our graph partitioning scheme to compute fill reducing orderings for sparse matrices. Surprisingly, our scheme substantially outperforms the multiple minimum degree algorithm <ref> [35] </ref>, which is the most commonly used method for computing fill reducing orderings of a sparse matrix. Even though multilevel algorithms are quite fast compared to spectral methods, they can still be the bottleneck if the sparse system of equations is being solved in parallel [32, 18]. <p> On a parallel computer, a fill reducing ordering, besides minimizing the operation count, should also increase the degree of concurrency that can be exploited during factorization. In general, nested dissection based orderings exhibit more concurrency during factorization than minimum degree orderings <ref> [13, 35] </ref>. The minimum degree [13] ordering heuristic is the most widely used fill reducing algorithm that is used to order sparse matrices for factorization on serial computers. The minimum degree algorithm has been found to produce very good orderings. <p> The minimum degree [13] ordering heuristic is the most widely used fill reducing algorithm that is used to order sparse matrices for factorization on serial computers. The minimum degree algorithm has been found to produce very good orderings. The multiple minimum degree algorithm <ref> [35] </ref> is the most widely used variant of minimum degree due to its very fast runtime. The quality of the orderings produced by our multilevel nested dissection algorithm (MLND) compared to that of MMD is shown in Table 8 and Figure 7.
Reference: [36] <author> Gary L. Miller, Shang-Hua Teng, W. Thurston, and Stephen A. Vavasis. </author> <title> Automatic mesh partitioning. </title> <editor> In A. George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Sparse Matrix Computations: Graph Theory Issues and Algorithms. (An IMA Workshop Volume). </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Another class of graph partitioning techniques uses the geometric information of the graph to find a good partition. Geometric partitioning algorithms <ref> [23, 48, 37, 36, 38] </ref> tend to be fast but often yield partitions that are worse than those obtained by spectral methods. Among the most prominent of these schemes is the algorithm described in [37, 36]. <p> Geometric partitioning algorithms [23, 48, 37, 36, 38] tend to be fast but often yield partitions that are worse than those obtained by spectral methods. Among the most prominent of these schemes is the algorithm described in <ref> [37, 36] </ref>. This algorithm produces partitions that are provably within the bounds that exist for some special classes of graphs (that includes graphs arising in finite element applications). <p> A partition of G m can be obtained using various algorithms such as (a) spectral bisection [47, 46, 2, 24], (b) geometric bisection <ref> [37, 36] </ref> (if coordinates are available 2 ), and (c) combinatorial methods [31, 3, 11, 12, 17, 5, 33, 21]. Since the size of the coarser graph G m is small (i.e., jV m j &lt; 100), this step takes a small amount of time. <p> we show three different variations of spectral partitioning [47, 46, 26, 2], the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition [38, 25], and two variants of geometric partitioning <ref> [37, 36, 16] </ref>. Table 9: Characteristics of various graph partitioning algorithms. For each graph partitioning algorithm, Table 9 shows a number of characteristics. The first column shows the number of trials that are often performed for each partitioning algorithm.
Reference: [37] <author> Gary L. Miller, Shang-Hua Teng, and Stephen A. Vavasis. </author> <title> A unified geometric approach to graph separators. </title> <booktitle> In Proceedings of 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 538-547, </pages> <year> 1991. </year>
Reference-contexts: Another class of graph partitioning techniques uses the geometric information of the graph to find a good partition. Geometric partitioning algorithms <ref> [23, 48, 37, 36, 38] </ref> tend to be fast but often yield partitions that are worse than those obtained by spectral methods. Among the most prominent of these schemes is the algorithm described in [37, 36]. <p> Geometric partitioning algorithms [23, 48, 37, 36, 38] tend to be fast but often yield partitions that are worse than those obtained by spectral methods. Among the most prominent of these schemes is the algorithm described in <ref> [37, 36] </ref>. This algorithm produces partitions that are provably within the bounds that exist for some special classes of graphs (that includes graphs arising in finite element applications). <p> A partition of G m can be obtained using various algorithms such as (a) spectral bisection [47, 46, 2, 24], (b) geometric bisection <ref> [37, 36] </ref> (if coordinates are available 2 ), and (c) combinatorial methods [31, 3, 11, 12, 17, 5, 33, 21]. Since the size of the coarser graph G m is small (i.e., jV m j &lt; 100), this step takes a small amount of time. <p> we show three different variations of spectral partitioning [47, 46, 26, 2], the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition [38, 25], and two variants of geometric partitioning <ref> [37, 36, 16] </ref>. Table 9: Characteristics of various graph partitioning algorithms. For each graph partitioning algorithm, Table 9 shows a number of characteristics. The first column shows the number of trials that are often performed for each partitioning algorithm.
Reference: [38] <author> B. Nour-Omid, A. Raefsky, and G. Lyzenga. </author> <title> Solving finite element equations on concurrent computers. </title> <editor> In A. K. Noor, editor, </editor> <publisher> American Soc. Mech. Eng, </publisher> <pages> pages 291-307, </pages> <year> 1986. </year>
Reference-contexts: Another class of graph partitioning techniques uses the geometric information of the graph to find a good partition. Geometric partitioning algorithms <ref> [23, 48, 37, 36, 38] </ref> tend to be fast but often yield partitions that are worse than those obtained by spectral methods. Among the most prominent of these schemes is the algorithm described in [37, 36]. <p> In Table 9 we show three different variations of spectral partitioning [47, 46, 26, 2], the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition <ref> [38, 25] </ref>, and two variants of geometric partitioning [37, 36, 16]. Table 9: Characteristics of various graph partitioning algorithms. For each graph partitioning algorithm, Table 9 shows a number of characteristics. The first column shows the number of trials that are often performed for each partitioning algorithm.
Reference: [39] <author> Jr. P. Ciarlet and F. Lamour. </author> <title> On the validity of a front-oriented approach to partitioning large sparse graphs with a connectivity constraint. </title> <type> Technical Report 94-37, </type> <institution> Computer Science Department, UCLA, </institution> <address> Los Angeles, CA, </address> <year> 1994. </year>
Reference-contexts: KL algorithm is described in Appendix A.3. 4.3 Graph Growing Algorithm (GGP) Another simple way of bisecting the graph is to start from a vertex and grow a region around it in a breath-first fashion, until half of the vertices have been included (or half of the total vertex weight) <ref> [12, 17, 39] </ref>. The quality of the graph growing algorithm is sensitive to the choice of a vertex from which to start growing the graph, and different starting vertices yield different edge-cuts. To partially solve this problem, we randomly select 10 vertices and we grow 10 different regions.
Reference: [40] <author> C. C. Paige and M. A. Saunders. </author> <title> Solution to sparse indefinite systems of linear equations. </title> <journal> SIAM Journal on Numerical Ananl-ysis, </journal> <volume> 12 </volume> <pages> 617-629, </pages> <year> 1974. </year>
Reference-contexts: It partitions the coarse graph using spectral bisection and obtains the Fiedler vector of the coarser graph. During uncoarsening, it obtains an approximate Fiedler vector of the next level fine graph by interpolating the Fiedler vector of the coarser graph, and computes a more accurate Fiedler vector using SYMMLQ <ref> [40] </ref>. By using this multilevel approach, the MSB algorithm is able to compute the Fiedler vector of the graph in much less time than that taken by the original spectral bisection algorithm.
Reference: [41] <author> Christos H. Papadimitriou and Kenneth Steiglitz. </author> <title> Combinatorial Optimization. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: Note that depending on how matchings are computed, the number of edges belonging to the maximal matching may be different. The maximal matching that has the maximum number of edges is called maximum matching. However, because the complexity of computing a maximum matching <ref> [41] </ref> is in general higher than that of computing a maximal matching, the latter are preferred. Coarsening a graph using matchings preserves many properties of the original graph. If G 0 is (maximal) planar, the G i is also (maximal) planar [34]. <p> Both A and B are ordered by recursively applying nested dissection ordering. In our multilevel nested dissection algorithm (MLND) a vertex separator is computed from an edge separator by finding the minimum vertex cover <ref> [41, 44] </ref>. The minimum vertex cover has been found to produce very small vertex separators. The overall quality of a fill reducing ordering depends on whether or not the matrix is factored on a serial or parallel 19 algorithm.
Reference: [42] <author> B. N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: Since we are interested in bisections of equal size, the value of r is chosen as the weighted median of the values of y i . The eigenvector y is computed using the Lanczos algorithm <ref> [42] </ref>. This algorithm is iterative and the number of iterations required depends on the desired accuracy. In our experiments, we set the accuracy to 10 2 and the maximum number of iterations to 100. 4.2 Kernighan-Lin Algorithm (KL) The Kernighan-Lin algorithm [31] is iterative in nature.
Reference: [43] <author> R. Ponnusamy, N. Mansour, A. Choudhary, and G. C. Fox. </author> <title> Graph contraction and physical optimization methods: a quality-cost tradeoff for mapping data on parallel computers. </title> <booktitle> In International Conference of Supercomputing, </booktitle> <year> 1993. </year> <month> 25 </month>
Reference-contexts: Another class of graph partitioning algorithms reduces the size of the graph (i.e., coarsen the graph) by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph. These are called multilevel graph partitioning schemes <ref> [4, 7, 19, 20, 26, 10, 43] </ref>. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality [43]. Recently, a number of multilevel algorithms have been proposed [4, 26, 7, 20, 10] that further refine the partition during the uncoarsening phase. <p> These are called multilevel graph partitioning schemes [4, 7, 19, 20, 26, 10, 43]. Some researchers investigated multilevel schemes primarily to decrease the partitioning time, at the cost of somewhat worse partition quality <ref> [43] </ref>. Recently, a number of multilevel algorithms have been proposed [4, 26, 7, 20, 10] that further refine the partition during the uncoarsening phase. These schemes tend to give good partitions at a reasonable cost.
Reference: [44] <author> A. Pothen and C-J. Fan. </author> <title> Computing the block triangular form of a sparse matrix. </title> <journal> ACM Transactions on Mathematical Software, </journal> <year> 1990. </year>
Reference-contexts: Both A and B are ordered by recursively applying nested dissection ordering. In our multilevel nested dissection algorithm (MLND) a vertex separator is computed from an edge separator by finding the minimum vertex cover <ref> [41, 44] </ref>. The minimum vertex cover has been found to produce very small vertex separators. The overall quality of a fill reducing ordering depends on whether or not the matrix is factored on a serial or parallel 19 algorithm.
Reference: [45] <author> Alex Pothen, H. D. Simon, and Lie Wang. </author> <title> Spectral nested dissection. </title> <type> Technical Report 92-01, </type> <institution> Computer Science Department, Pennsylvania State University, University Park, </institution> <address> PA, </address> <year> 1992. </year>
Reference-contexts: In [29] we present a parallel formulation of our MLND algorithm that achieves a speedup of as much as 57 on 128-processor Cray T3D (over the serial algorithm running on a single T3D processor) for some graphs. Spectral nested dissection (SND) <ref> [45] </ref> can be used for ordering matrices for parallel factorization. The SND algorithm is based on the spectral graph partitioning algorithm described in Section 4.1. We have implemented the SND algorithm described in [45]. <p> Spectral nested dissection (SND) <ref> [45] </ref> can be used for ordering matrices for parallel factorization. The SND algorithm is based on the spectral graph partitioning algorithm described in Section 4.1. We have implemented the SND algorithm described in [45]. As in the case of MLND, the minimum vertex cover algorithm was used to compute a vertex separator from the edge separator. The quality of the orderings produced by our multilevel nested dissection algorithm compared to that of the spectral nested dissection algorithm is also shown in Figure 7.
Reference: [46] <author> Alex Pothen, H. D. Simon, Lie Wang, and Stephen T. Bernard. </author> <title> Towards a fast implementation of spectral nested dissection. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <pages> pages 42-51, </pages> <year> 1992. </year>
Reference-contexts: The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. Spectral partitioning methods are known to produce good partitions for a wide class of problems, and they are used quite extensively <ref> [47, 46, 24] </ref>. However, these methods are very expensive since they require the computation of the eigenvector corresponding to the second smallest eigenvalue (Fiedler vector). Execution time of the spectral methods can be reduced if computation of the Fiedler vector is done by using a multilevel algorithm [2]. <p> A partition of G m can be obtained using various algorithms such as (a) spectral bisection <ref> [47, 46, 2, 24] </ref>, (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods [31, 3, 11, 12, 17, 5, 33, 21]. <p> For the sake of simplicity, we have chosen to represent each property in terms of a small discrete scale. In absence of extensive data, we could not have done any better anyway. In Table 9 we show three different variations of spectral partitioning <ref> [47, 46, 26, 2] </ref>, the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition [38, 25], and two variants of geometric partitioning [37, 36, 16].
Reference: [47] <author> Alex Pothen, Horst D. Simon, and Kang-Pu Liou. </author> <title> Partitioning sparse matrices with eigenvectors of graphs. </title> <journal> SIAM Journal of Matrix Analysis and Applications, </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: The graph partitioning problem is NP-complete. However, many algorithms have been developed that find a reasonably good partition. Spectral partitioning methods are known to produce good partitions for a wide class of problems, and they are used quite extensively <ref> [47, 46, 24] </ref>. However, these methods are very expensive since they require the computation of the eigenvector corresponding to the second smallest eigenvalue (Fiedler vector). Execution time of the spectral methods can be reduced if computation of the Fiedler vector is done by using a multilevel algorithm [2]. <p> A partition of G m can be obtained using various algorithms such as (a) spectral bisection <ref> [47, 46, 2, 24] </ref>, (b) geometric bisection [37, 36] (if coordinates are available 2 ), and (c) combinatorial methods [31, 3, 11, 12, 17, 5, 33, 21]. <p> These algorithms are described in the next sections. We choose not to use geometric bisection algorithms, since the coordinate information was not available for most of the test graphs. 4.1 Spectral Bisection (SB) In the spectral bisection algorithm, the spectral information is used to partition the graph <ref> [47, 2, 26] </ref>. <p> For the sake of simplicity, we have chosen to represent each property in terms of a small discrete scale. In absence of extensive data, we could not have done any better anyway. In Table 9 we show three different variations of spectral partitioning <ref> [47, 46, 26, 2] </ref>, the multilevel partitioning described in this paper, the levelized nested dissection [11], the Kernighan-Lin partition [31], the coordinate nested dissec 21 tion (CND) [23], two variations of the inertial partition [38, 25], and two variants of geometric partitioning [37, 36, 16].
Reference: [48] <author> P. Raghavan. </author> <title> Line and plane separators. </title> <type> Technical Report UIUCDCS-R-93-1794, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <address> IL 61801, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Another class of graph partitioning techniques uses the geometric information of the graph to find a good partition. Geometric partitioning algorithms <ref> [23, 48, 37, 36, 38] </ref> tend to be fast but often yield partitions that are worse than those obtained by spectral methods. Among the most prominent of these schemes is the algorithm described in [37, 36].
References-found: 48

