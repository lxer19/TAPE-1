URL: http://www.isi.edu/teamcore/tambe/papers/99/bdi-panel.ps
Refering-URL: http://www.isi.edu/teamcore/tambe/agent.html
Root-URL: http://www.isi.edu
Email: georgeff@aaii.oz.au  pell@ptolemy.arc.nasa.gov  pollack@cs.pitt.edu  tambe@isi.edu  M.J.Wooldridge@qmw.ac.uk  
Title: The Belief-Desire-Intention Model of Agency  
Author: Michael Georgeff Barney Pell Martha Pollack Milind Tambe Michael Wooldridge fi 
Address: 6, 171 La Trobe St Melbourne, Australia 3000  Moffett Field, CA 94035-1000, USA  Pittsburgh, Pittsburgh, PA 15260, USA  4676 Admiralty Way, Marina del Rey, CA 90292, USA  London, London E1 4NS, United Kingdom  
Affiliation: Australian AI Institute, Level  RIACS, NASA Ames Research Center  Department of Computer Science/Intelligent Systems Program University of  Computer Science Department/ISI, University of Southern California  fi Department of Electronic Engineering, Queen Mary and Westfield College University of  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> M. E. Bratman. </author> <title> Intentions, Plans, and Practical Reason. </title> <publisher> Harvard University Press: </publisher> <address> Cam-bridge, MA, </address> <year> 1987. </year>
Reference-contexts: There are several reasons for its success, but perhaps the most compelling are that the BDI model combines a respectable philosophical model of human practical reasoning, (originally developed by Michael Bratman <ref> [1] </ref>), a number of implementations (in the IRMA architecture [2] and the various PRS-like systems currently available [7]), several successful applications (including the now-famous fault diagnosis system for the space shuttle, as well as factory process control systems and business process management [8]), and finally, an elegant abstract logical semantics, which <p> Let's call these Belief-Desire-Intention (BDI) models. Particular BDI models that center on claims originally propounded by Bratman <ref> [1] </ref> about the role of intentions in focusing practical reasoning. Specifically, Bratman argued that rational agents will tend to focus their practical reasoning on the intentions they have already adopted, and will tend to bypass full consideration of options that conflict with those intentions.
Reference: 2. <author> M. E. Bratman, D. J. Israel, and M. E. Pollack. </author> <title> Plans and resource-bounded practical reasoning. </title> <booktitle> Computational Intelligence, </booktitle> <address> 4:349355, </address> <year> 1988. </year>
Reference-contexts: There are several reasons for its success, but perhaps the most compelling are that the BDI model combines a respectable philosophical model of human practical reasoning, (originally developed by Michael Bratman [1]), a number of implementations (in the IRMA architecture <ref> [2] </ref> and the various PRS-like systems currently available [7]), several successful applications (including the now-famous fault diagnosis system for the space shuttle, as well as factory process control systems and business process management [8]), and finally, an elegant abstract logical semantics, which have been taken up and elaborated upon widely within <p> Let's call this Bratman's Claim, and let's call computational models that embody this claim IRMA models (for the Intelli gent Resource-Bounded Machine Architecture described in <ref> [2] </ref>). The Procedural Reasoning System (PRS) [7, 6], a programming environment for developing complex applications that execute in dynamic environments and can best be specified using BDI concepts. <p> But there are other reasoning tasks that all IRMA agents must perform as well. For example, they must deliberate about alternatives that are either compatible with their existing plans or have triggered an override <ref> [2] </ref>); recently, John Horty and I have been developing mechanisms for weighing alternatives in the context of existing plans [10]. Another example is hinted at in my earlier comments: all IRMA agents need to be able to perform means-end reasoning.
Reference: 3. <editor> M. Carrier and P. K. Machamer, editors. Mindscapes: </editor> <booktitle> Philosophy, Science, and the Mind. </booktitle> <publisher> University of Pittsburgh Press, </publisher> <address> Pittsburgh, PA, </address> <year> 1997. </year>
Reference-contexts: The debate over this question has raged in the philosophical literature; see, e.g., Carrier and Machamer <ref> [3, Chap. 1-3] </ref>. 2 Bratman actually came at things the other way round.
Reference: 4. <author> E. Ephrati, M. E. Pollack, and S. </author> <title> Ur. Deriving multi-agent coordination through filtering strategies. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 679687, </pages> <address> Montreal, Quebec, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: To my knowledge, this question has not been investigated yet. In addressing questions like these, we need to focus, at least for now, on the development of computationally sound mechanisms: algorithms and heuristics that we 3 However, it might contribute to them; see, e.g., Ephrati et al. <ref> [4] </ref> for some preliminary work on using the intention-commitment strategy in multi-agent settings to increase cooperation. can employ in building IRMA agents (perhaps using PRS!) Formal underpinnings can, and if at all possible, should accompany these mechanisms, but unless they underpin specific algorithms and heuristics they seem unlikely to have much
Reference: 5. <author> I. A. Ferguson. </author> <title> Integrated control and coordinated behaviour: A case for agent models. </title> <editor> In M. Wooldridge and N. R. Jennings, editors, </editor> <booktitle> Intelligent Agents: Theories, Architectures, and Languages (LNAI Volume 890), pages 203218. </booktitle> <publisher> Springer-Verlag: </publisher> <address> Berlin, Germany, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: Moreover, the basic BDI model gives no architectural consideration to explicitly multi-agent aspects of behaviour. More recent architectures, (such as InteRRaP [13] and TouringMachines <ref> [5] </ref>) do explicitly provide for such behaviours at the architectural level.
Reference: 6. <author> M. P. Georgeff and F. F. Ingrand. </author> <title> Decision-making in an embedded reasoning system. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <pages> pages 972978, </pages> <address> Detroit, MI, </address> <year> 1989. </year>
Reference-contexts: Let's call this Bratman's Claim, and let's call computational models that embody this claim IRMA models (for the Intelli gent Resource-Bounded Machine Architecture described in [2]). The Procedural Reasoning System (PRS) <ref> [7, 6] </ref>, a programming environment for developing complex applications that execute in dynamic environments and can best be specified using BDI concepts.
Reference: 7. <author> M. P. Georgeff and A. L. Lansky. </author> <title> Reactive reasoning and planning. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI-87), </booktitle> <pages> pages 677682, </pages> <address> Seattle, WA, </address> <year> 1987. </year>
Reference-contexts: There are several reasons for its success, but perhaps the most compelling are that the BDI model combines a respectable philosophical model of human practical reasoning, (originally developed by Michael Bratman [1]), a number of implementations (in the IRMA architecture [2] and the various PRS-like systems currently available <ref> [7] </ref>), several successful applications (including the now-famous fault diagnosis system for the space shuttle, as well as factory process control systems and business process management [8]), and finally, an elegant abstract logical semantics, which have been taken up and elaborated upon widely within the agent research community [14, 16]. <p> Let's call this Bratman's Claim, and let's call computational models that embody this claim IRMA models (for the Intelli gent Resource-Bounded Machine Architecture described in [2]). The Procedural Reasoning System (PRS) <ref> [7, 6] </ref>, a programming environment for developing complex applications that execute in dynamic environments and can best be specified using BDI concepts.
Reference: 8. <author> M. P. Georgeff and A. S. Rao. </author> <title> A profile of the Australian AI Institute. </title> <journal> IEEE Expert, </journal> <volume> 11(6):8992, </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: human practical reasoning, (originally developed by Michael Bratman [1]), a number of implementations (in the IRMA architecture [2] and the various PRS-like systems currently available [7]), several successful applications (including the now-famous fault diagnosis system for the space shuttle, as well as factory process control systems and business process management <ref> [8] </ref>), and finally, an elegant abstract logical semantics, which have been taken up and elaborated upon widely within the agent research community [14, 16].
Reference: 9. <author> B. Grosz and S. Kraus. </author> <title> Collaborative plans for group activities. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93), </booktitle> <pages> pages 367 373, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: In my own work, I have attempted to bridge this gap, roughly based on the mapping defined above. For instance, Cohen and Levesque's research on joint intentions [12], and Grosz and Kraus's work on SHAREDPLANS <ref> [9] </ref> has significantly influenced the STEAM system for teamwork, which I have developed in Soar. However, this is just one such attempt. This panel discussion was an excellent step to attempt to bridge this gap in general.
Reference: 10. <author> J. F. Horty and M. E Pollack. </author> <title> Option evaluation in context. </title> <booktitle> In Proceedings of the Seventh Conference on Theoretical Aspects of Rationality and Knowledge (TARK-98), </booktitle> <year> 1998. </year>
Reference-contexts: For example, they must deliberate about alternatives that are either compatible with their existing plans or have triggered an override [2]); recently, John Horty and I have been developing mechanisms for weighing alternatives in the context of existing plans <ref> [10] </ref>. Another example is hinted at in my earlier comments: all IRMA agents need to be able to perform means-end reasoning. But unlike standard means-end reasoning in AI (plan generation), an IRMA agent must do this reasoning taking account its existing plans.
Reference: 11. <author> D. Kinny and M. Georgeff. </author> <title> Commitment and effectiveness of situated agents. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI-91), </booktitle> <pages> pages 8288, </pages> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference-contexts: Which is the right approach? move around a grid collecting points <ref> [11] </ref>. As the world (grid) is dynamic, the points change value and come and go as the robot moves and plans thus a plan is never good for long.
Reference: 12. <author> H. J. Levesque, P. R. Cohen, and J. H. T. Nunes. </author> <title> On acting together. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI-90), </booktitle> <pages> pages 9499, </pages> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: The danger here is that both could end up reinventing each others' work in different disguises. In my own work, I have attempted to bridge this gap, roughly based on the mapping defined above. For instance, Cohen and Levesque's research on joint intentions <ref> [12] </ref>, and Grosz and Kraus's work on SHAREDPLANS [9] has significantly influenced the STEAM system for teamwork, which I have developed in Soar. However, this is just one such attempt. This panel discussion was an excellent step to attempt to bridge this gap in general.
Reference: 13. <author> J. P. Muller. </author> <title> The Design of Intelligent Agents (LNAI Volume 1177). </title> <publisher> Springer-Verlag: </publisher> <address> Berlin, Germany, </address> <year> 1997. </year>
Reference-contexts: In particular, the basic BDI model appears to be inappropriate for building systems that must learn and adapt their behaviour and such systems are becoming increasingly important. Moreover, the basic BDI model gives no architectural consideration to explicitly multi-agent aspects of behaviour. More recent architectures, (such as InteRRaP <ref> [13] </ref> and TouringMachines [5]) do explicitly provide for such behaviours at the architectural level.
Reference: 14. <author> A. S. Rao and M. Georgeff. </author> <title> Decision procedures of BDI logics. </title> <journal> Journal of Logic and Computation, </journal> <volume> 8(3):293344, </volume> <year> 1998. </year>
Reference-contexts: systems currently available [7]), several successful applications (including the now-famous fault diagnosis system for the space shuttle, as well as factory process control systems and business process management [8]), and finally, an elegant abstract logical semantics, which have been taken up and elaborated upon widely within the agent research community <ref> [14, 16] </ref>. However, it could be argued that the BDI model is now becoming somewhat dated: the principles of the architecture were established in the mid-1980s, and have remained essentially unchanged since then.
Reference: 15. <author> S. Russell and D. Subramanian. </author> <title> Provably bounded-optimal agents. </title> <journal> Journal of AI Research, </journal> <volume> 2:575609, </volume> <year> 1995. </year>
Reference-contexts: Furthermore, the focus of agent research (and AI in general) has shifted significantly since the BDI model was originally developed. New advances in understanding (such as Russell and Subramanian's model of bounded-optimal agents <ref> [15] </ref>) have led to radical changes in how the agents community (and more generally, the artificial intelligence community) views its enterprise.
Reference: 16. <author> K. Schild. </author> <title> On the relationship between BDI logics and standard logics of concurrency. </title> <editor> In J. P. Muller, M. P. Singh, and A. S. Rao, editors, </editor> <booktitle> Intelligent Agents V Proceedings of the Fifth International Workshop on Agent Theories, Architectures, and Languages (ATAL-98), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1999. </year> <note> In this volume. </note>
Reference-contexts: systems currently available [7]), several successful applications (including the now-famous fault diagnosis system for the space shuttle, as well as factory process control systems and business process management [8]), and finally, an elegant abstract logical semantics, which have been taken up and elaborated upon widely within the agent research community <ref> [14, 16] </ref>. However, it could be argued that the BDI model is now becoming somewhat dated: the principles of the architecture were established in the mid-1980s, and have remained essentially unchanged since then.
Reference: 17. <author> H. A. Simon. </author> <title> A behavioral model of rational choice. </title> <journal> Quarterly Journal of Economics, </journal> <volume> 69:99118, </volume> <year> 1995. </year>
Reference-contexts: It has been generally accepted for many years that agents cannot possibly perform optimizations over the space of all possible courses of action <ref> [17] </ref>. Bratman's Claim is aimed precisely at helping reduce that space to make the required reasoning feasible. The second question concerns the development of techniques to enable IRMA agents to learn and to interact socially.
Reference: 18. <author> Q. Yang. </author> <title> Intelligent Planning: A Decomposition and Abstraction Based Approach. </title> <publisher> Springer-Verlag: </publisher> <address> New York, </address> <year> 1997. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Another example is hinted at in my earlier comments: all IRMA agents need to be able to perform means-end reasoning. But unlike standard means-end reasoning in AI (plan generation), an IRMA agent must do this reasoning taking account its existing plans. Work on plan merging, notably that of Yang <ref> [18] </ref>, may be relevant here. One final example: IRMA agents must be capable of committing to partial plans. If they were required always to form complete plans, they would over-commit, and filter out too many subsequent options as incompatible.
References-found: 18

