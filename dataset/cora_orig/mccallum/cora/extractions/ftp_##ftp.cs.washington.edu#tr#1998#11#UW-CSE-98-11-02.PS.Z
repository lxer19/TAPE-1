URL: ftp://ftp.cs.washington.edu/tr/1998/11/UW-CSE-98-11-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-date.html
Root-URL: http://www.cs.washington.edu
Title: Array Language Support for Wavefront and Pipelined Computations  
Author: Bradford L. Chamberlain E Christopher Lewis Lawrence Snyder 
Date: October 29, 1998  
Address: Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Array languages such as Fortran 90, High Performance Fortran and ZPL are convenient vehicles for expressing data parallel computation. Unfortunately, array language semantics prohibit the natural expression of wavefront and pipelined computations, characterized by a sequential propagation of computed values across one or more dimensions of the problem space. As a result, programmers scalarize (i.e., use loop nests and scalar indexing instead of array operations) wavefront computations, sacrificing the benefits of the array language. We propose an extension to array languages that provides support for wavefront computation without scalarization and with minimal impact on the language. Our extension is particularly valuable in that it identifies parallelism to both the programmer and compiler just as conventional array operations do. In this paper we motivate the problem, introduce our language extension, describe its implementation in the ZPL data parallel array language compiler, and experimentally evaluate the parallel performance improvement due to its optimization for parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jeanne C. Adams, Walter S. Brainerd, Jeanne T. Martin, Brian T. Smith, and Jerrold L. Wagener. </author> <title> Fortran 90 Handbook. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Array languages such as Fortran 90 <ref> [1] </ref>, High Performance Fortran (HPF) [6] and ZPL [12] have achieved success in expressing and exploiting data parallelism. They are distinguished from scalar languages by their support of operations on arrays as primitive entities, frequently obviating the need for explicit looping and element-wise indexing.
Reference: [2] <author> Bradford L. Chamberlain, Sung-Eun Choi, E Christopher Lewis, Calvin Lin, Lawrence Snyder, and W. Derrick Weathersby. </author> <title> ZPL's WYSIWYG performance model. </title> <booktitle> In Third International Workshop on High-Level Parallel Programming Models and Supportive Environments, </booktitle> <pages> pages 5061. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> March </month> <year> 1998. </year>
Reference-contexts: ZPL aligns interacting arrays so that interprocessor communication is apparent to the programmer and the compiler from the parallel array operators, such as the shift operator <ref> [2] </ref>. Because only the shift operator on primed arrays may appear in scan blocks, we need only describe its implementation here. Recall that shifted primed arrays imply that there is a wavefront moving across the arrays in a scan block.
Reference: [3] <author> Bradford L. Chamberlain, Sung-Eun Choi, E Christopher Lewis, Lawrence Snyder, W. Derrick Weathersby, and Calvin Lin. </author> <title> The case for high-level parallel programming in ZPL. </title> <journal> IEEE Computational Science and Engineering, </journal> <volume> 5(3):7685, </volume> <month> JulySeptember </month> <year> 1998. </year>
Reference-contexts: We have demonstrated that our ZPL compiler is competitive with hand-coded C with MPI <ref> [3] </ref>, and it generally outperforms HPF [9]. The compiler is publicly available [14] for most modern parallel and sequential platforms, including the Cray T3E, IBM SP2, SGI PowerChallenge, SGI Origin, and networked UNIX workstations using MPI and PVM.
Reference: [4] <author> Bradford L. Chamberlain, E Christopher Lewis, Calvin Lin, and Lawrence Snyder. </author> <title> Regions: An abstraction for expressing array computation. </title> <type> Technical Report UW-CSE-98-10-02, </type> <institution> University of Washington, Department of Computer Science and Engineering, </institution> <month> October </month> <year> 1998. </year>
Reference-contexts: It supports all the usual scalar data types (e.g., integer, float, char), operators (e.g., math, logical, bit-wise), and control structures (e.g., for, while, function calls). As an array language, it also offers array data types and operators. ZPL is distinguished from other array languages by its use of regions <ref> [4] </ref>. A region represents an index set, and may precede a statement, specifying the extent of the array references within its dynamic scope. By factoring the indices that are to be computed on into the region, the use of regions eliminates the need to index arrays.
Reference: [5] <author> W. Crowley, C. P. Hendrickson, and T. I. Luby. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID-17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <year> 1978. </year>
Reference-contexts: We conduct experiments on the Cray T3E and the SGI PowerChallenge using the Tomcatv and Simple <ref> [5] </ref> benchmarks. For each experiment, we consider each program as a whole, and we consider two components of each that contain a single wavefront computation. Our extensions drastically improve the performance of the two wavefront portions of code in each benchmark, which results in significant overall performance enhancement.
Reference: [6] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Langauge Specification, </title> <note> Version 2.0. </note> <month> January </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Array languages such as Fortran 90 [1], High Performance Fortran (HPF) <ref> [6] </ref> and ZPL [12] have achieved success in expressing and exploiting data parallelism. They are distinguished from scalar languages by their support of operations on arrays as primitive entities, frequently obviating the need for explicit looping and element-wise indexing.
Reference: [7] <author> F. Thomas Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures, chapter 1.2. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1992. </year>
Reference-contexts: Associativity Detection. In the event that scan block computations are associative, more efficient parallel prefix implementations exist <ref> [7] </ref>. As an optimization, the compiler could recognize this and use the more efficient implementation. Broadcast Detection. Similarly, scan blocks can be used to copy values across an array.
Reference: [8] <author> E Christopher Lewis, Calvin Lin, and Lawrence Snyder. </author> <title> The implementation and evaluation of fusion and contraction in array languages. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 5059, </pages> <month> June </month> <year> 1998. </year>
Reference-contexts: The use of regions improves code clarity and compactness. Though the scalar variable r is promoted to an array in the array codes, we have previously demonstrated compiler techniques by which this overhead may be eliminated via array contraction <ref> [8] </ref>. 2.2 Wavefront Computation in ZPL Array language semantics dictate that the right-hand side of an array statement is evaluated before the result is assigned to the left-hand side. As a result, the compiler will not generate a loop that carries a true data dependence. <p> Notice that the j-loop has to iterate from high to low indices in order to preserve the loop carried anti-dependence from the statement to itself. We have developed unconstrained distance vectors to represent these array-level data dependences <ref> [8] </ref>. Unconstrained distance vectors are analogous to conventional distance vectors [13] except that they are independent of the iteration space. This is essential in an array language compiler because analyses are performed before the array statements have been converted to loop nests, thus before an ordered iteration space exists. <p> We have previously developed an algorithm that computes a legal loop structure given a list of the unconstrained distance vectors associated with the data dependences between the statements that the loop is to contain <ref> [8] </ref>. In the event that the unconstrained distance vectors do not uniquely determine a loop structure, cache issues are considered in order to improve spatial locality. <p> These temporary arrays will be subsequently eliminated via array contraction if possible <ref> [8] </ref>. Though all the statements in a scan block are implemented by a single loop nest, we cannot simply calculate unconstrained distance vectors as described above, because primed references have a different meaning. For example, consider the scan block in Figure 2 (b). <p> The ZPL compiler fuses statements in an effort to enable the contraction of arrays to scalars <ref> [8] </ref>. Normally, the compiler does not fuse statements if the fusion introduces a loop carried flow dependence, for this would prohibit parallelism in one or more dimensions.
Reference: [9] <author> C. Lin, L. Snyder, R. Anderson, B. Chamberlain, S. Choi, G. Forman, E. Lewis, and W. D. Weathersby. </author> <title> ZPL vs. HPF: A comparison of performance and programming style. </title> <type> Technical Report 951105, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: We have demonstrated that our ZPL compiler is competitive with hand-coded C with MPI [3], and it generally outperforms HPF <ref> [9] </ref>. The compiler is publicly available [14] for most modern parallel and sequential platforms, including the Cray T3E, IBM SP2, SGI PowerChallenge, SGI Origin, and networked UNIX workstations using MPI and PVM.
Reference: [10] <author> Naomi H. Naik, Vijay K. Naik, and Michel Nicoules. </author> <title> Parallelization of a class of implicit finite difference schemes in computational fluid dynamics. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 5(1):150, </volume> <year> 1993. </year>
Reference-contexts: They can then immediately begin computing slices of their portions of the scan block. By the time they are finished with a slice, the next bit of data has been passed on by the preceding processor. Parallel implementation of seemingly sequential wavefront computations is not a new idea <ref> [10, 11] </ref>, but providing direct array language support for it is. The compiler performs this optimization by generating a loop to iterate over slices of a processor's portion of a scan block.
Reference: [11] <author> Ton A. Ngo. </author> <title> The Role of Performance Models in Parallel Programming and Languages. </title> <type> PhD thesis, </type> <institution> University of Washington, Department of Computer Science and Engineering, </institution> <year> 1997. </year>
Reference-contexts: They can then immediately begin computing slices of their portions of the scan block. By the time they are finished with a slice, the next bit of data has been passed on by the preceding processor. Parallel implementation of seemingly sequential wavefront computations is not a new idea <ref> [10, 11] </ref>, but providing direct array language support for it is. The compiler performs this optimization by generating a loop to iterate over slices of a processor's portion of a scan block.
Reference: [12] <author> Lawrence Snyder. </author> <title> The ZPL Programmer's Guide. </title> <publisher> MIT Press (in pressavailable at ftp://ftp.cs.washington.edu/pub/orca/docs/zpl guide.ps), </publisher> <year> 1998. </year>
Reference-contexts: 1 Introduction Array languages such as Fortran 90 [1], High Performance Fortran (HPF) [6] and ZPL <ref> [12] </ref> have achieved success in expressing and exploiting data parallelism. They are distinguished from scalar languages by their support of operations on arrays as primitive entities, frequently obviating the need for explicit looping and element-wise indexing. <p> Performance data is presented in Section 5, and future work and conclusions are given in the final section. 2 Array Language Support for Wavefront Computation This section describes array language support for wavefront computation in the context of the ZPL parallel array language <ref> [12] </ref>. We have demonstrated that our ZPL compiler is competitive with hand-coded C with MPI [3], and it generally outperforms HPF [9]. <p> The first section below gives a very brief summary of the ZPL language, only describing the features of the language immediately relevant to this paper. Detailed coverage of the language may be found elsewhere <ref> [12] </ref>. After that, we introduce the prime operator and scan blocks as a means of supporting wavefront computations in ZPL. 2.1 Brief ZPL Language Summary ZPL is a data parallel array programming language.
Reference: [13] <author> Michael Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1996. </year>
Reference-contexts: Notice that the j-loop has to iterate from high to low indices in order to preserve the loop carried anti-dependence from the statement to itself. We have developed unconstrained distance vectors to represent these array-level data dependences [8]. Unconstrained distance vectors are analogous to conventional distance vectors <ref> [13] </ref> except that they are independent of the iteration space. This is essential in an array language compiler because analyses are performed before the array statements have been converted to loop nests, thus before an ordered iteration space exists.
Reference: [14] <institution> ZPL Project. ZPL project homepage. </institution> <note> http:/www.cs.washington.edu/research/zpl. 13 </note>
Reference-contexts: We have demonstrated that our ZPL compiler is competitive with hand-coded C with MPI [3], and it generally outperforms HPF [9]. The compiler is publicly available <ref> [14] </ref> for most modern parallel and sequential platforms, including the Cray T3E, IBM SP2, SGI PowerChallenge, SGI Origin, and networked UNIX workstations using MPI and PVM. The language is in active use by scientists in fields such as astronomy, civil engineering, biological statistics, mathematics, oceanography, and theoretical physics.
References-found: 14

