URL: http://drl.cs.uiuc.edu/pubs/Hughes-sigmet95.ps
Refering-URL: http://drl.cs.uiuc.edu/prototyping/
Root-URL: http://www.cs.uiuc.edu
Email: Email: fhughes,winslettg@cs.uiuc.edu  
Title: PEDCAD: A Framework for Performance Evaluation of Object Database Applications  
Author: Eric Hughes Marianne Winslett 
Note: Presented at ACM SIGMETRICS '95 (poster session)  
Address: 1304 West Springfield Avenue Urbana, IL 61801 USA  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract: This paper describes the problem of performance evaluation of object database applications. We give criteria and design goals for a solution to this problem. A framework, called PEDCAD, is described which generates implementations from high-level descriptions of applications and assists in the design of performance tests. PEDCAD is shown to meet the criteria presented. Results are given for the use of PEDCAD in the electronic CAD domain. We evaluate the performance of a benchmark implementation to validate the framework. PEDCAD is also used to improve the performance of a research CAD application. We discuss the management of performance tests in complex application environments and describe the automation of PEDCAD. 
Abstract-found: 1
Intro-found: 1
Reference: [AL90] <author> Thomas E. Anderson and Edward D. Lazowska. Quartz: </author> <title> A Tool for Tuning Parallel Program Performance. </title> <booktitle> In Proc. SIGMETRICS, </booktitle> <pages> pages 115-125. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: However, no such model is available for ODBMS applications, and it would be difficult to develop such a model for even one ODBMS. Another example is Quartz, which is a profiling facility for tuning parallel programs <ref> [AL90] </ref>. Quartz identifies the code segments of a program which consume the most resources. Anderson and Lazowska also include a discussion of the goals for the design of Quartz [AL90], many of which have been adapted to our work. <p> Another example is Quartz, which is a profiling facility for tuning parallel programs <ref> [AL90] </ref>. Quartz identifies the code segments of a program which consume the most resources. Anderson and Lazowska also include a discussion of the goals for the design of Quartz [AL90], many of which have been adapted to our work. Smith and Williams have surveyed the inclusion of performance estimation in CASE tools 2 [SW93]. <p> Customized benchmarks require that the application writer learn to use the ODBMS, which precludes quick results necessary for efficient evaluation. Third, the application writer should be able to correlate an effect in a performance result with the code that causes the effect <ref> [AL90] </ref>. Published benchmark results typically include a justification for interesting results. The application writer should be able to use a performance evaluation framework to develop such justifications, as needed. <p> Customized benchmarks are not necessarily amenable to experimentation, which can make it difficult for the application writer to isolate the cause of a performance effect. Next, the application writer should control which performance measurements are made and how those measurements are used to evaluate an application <ref> [AL90] </ref>. It may not be sufficient to collect all data that may be of interest, because this technique may affect the performance of the application and may generate more data than can be analyzed. The measurements of interest are specific to the application being evaluated. <p> The measurements of interest are specific to the application being evaluated. Standard benchmarks offer no opportunity for controlled evaluation. Finally, the results should be as correct as the input to the framework <ref> [AL90] </ref>. For example, performance tests based on an abstract representation of an application should yield results 3 which roughly approximate the performance of the application. More detailed tests should yield more accurate results.
Reference: [And90] <author> T. Anderson. </author> <title> The HyperModel Benchmark. </title> <booktitle> In Proc. </booktitle> <address> EDBT, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: The HyperModel benchmark <ref> [And90] </ref> represents the algorithms and data structures of hypertext applications. The ACOB benchmark [DFMV90] compares alternative client-server process architectures. The OO1 benchmark [CS92] models typical engineering applications. OO7 [CDN93] is a more comprehensive model of engineering applications.
Reference: [Cat93] <editor> Rick Cattell, editor. </editor> <title> The Object Database Standard: ODMG-93. </title> <publisher> Morgan-Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: PEDCAD uses a language very similar to ODMG-93, an emerging standard ODBMS interface <ref> [Cat93] </ref>. Here we give only a brief presentation of the description language. A schema of an application description is a collection of class definitions for potentially persistent data, including methods, extents, relationships, and indexes. <p> For example, a dictionary can be defined as a set of mappings, where a mapping consists of a key and a value, which can be retrieved by specifying its key <ref> [Cat93] </ref>. If the target ODBMS does not support dictionaries directly, PEDCAD can implement a dictionary in an ODBMS application as a set of instances of a mapping class generated by the tool. <p> Experiments have been run to validate the technique and to show that the framework can be applied to real CAD applications. PEDCAD meets the generality criterion. For example, it could be applied to any ODBMS or relational DBMS which supports ODMG-93 <ref> [Cat93] </ref>. If a file system or object management system were to support a similar interface, then our framework could be used to evaluate the performance of applications using the system.
Reference: [CDN93] <author> Michael Carey, David DeWitt, and Jeffrey Naughton. </author> <title> The OO7 Benchmark. </title> <booktitle> In Proc. SIGMOD, </booktitle> <pages> pages 12-21. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: The HyperModel benchmark [And90] represents the algorithms and data structures of hypertext applications. The ACOB benchmark [DFMV90] compares alternative client-server process architectures. The OO1 benchmark [CS92] models typical engineering applications. OO7 <ref> [CDN93] </ref> is a more comprehensive model of engineering applications. These benchmarks have been very useful for debugging and understanding performance problems of ODBMSes. We refer to the above works as standard benchmarks, because they are precise tests which are not adapted to the user's application. <p> These techniques are beyond the scope of this paper. 4 Experiments In this section, we describe the use of the PEDCAD framework to study the performance of two test applications, OO7 <ref> [CDN93] </ref> and ViDEO [GFHH93]. OO7 is an ODBMS benchmark which 8 models engineering applications. We use OO7 because optimized code is available for comparison with PEDCAD's generated code. <p> The server and client use Sun OS 4.1.3. The performance tests were run at times when the server and client systems were not heavily utilized. The effects of consecutive runs were isolated by scanning a 42 MB file forwards and backwards, as done for the OO7 benchmark <ref> [CDN93] </ref>. <p> We determine whether one collaboration has significantly better performance than another by estimating the probability that the performance results for the collaborations are drawn from distributions with different means. 4.1 OO7 Experiments The OO7 application consists of several programs, one of which is trav1, a traversal of a design object <ref> [CDN93] </ref>. We developed a description of OO7 in our ODMG-like language, starting from the optimized code for the application, to determine whether the generated variants could match the performance of the optimized code. <p> A performance test is then created which compares executions of the traversal variants on databases which conform to the Small-3 database specification in the benchmark (approximately 4 MB) <ref> [CDN93] </ref>. The performance goal is to minimize the elapsed time of the traversal. Figure 4 (a) gives the results for the test, including the generated hash table (GH), generated set (GS), optimized code hash table (OH) and optimized code set (OS) variants. <p> The user can also choose to study only the most interesting part of an application by describing and testing that part. We demonstrated the correctness of part of the framework with the OO7 experiments <ref> [CDN93] </ref>. In addition, the framework allows the application writer to use the newest ODBMS release running in the right application environment with the right background workload. These aspects help the application writer make the correct evaluation of the performance of the application.
Reference: [CFPS92] <author> Rosemary Candlin, Peter Fisk, Joe Phillips, and Neil Skilling. </author> <title> Studying the Performance Properties of Concurrent Programs by Simulation Experiments on Synthetic Programs. </title> <booktitle> In Proc. SIGMETRICS, </booktitle> <pages> pages 239-240. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: In addition to work on ODBMS performance evaluation, there are related efforts in performance evaluation of complex applications. Candlin and others have studied the generation of synthetic programs from a parameterization of the behavior of parallel programs <ref> [CFPS92] </ref>. This approach works well if a detailed model of the application environment is available. However, no such model is available for ODBMS applications, and it would be difficult to develop such a model for even one ODBMS.
Reference: [CS92] <author> Rick Cattell and J. Skeen. </author> <title> Object Operations Benchmark. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 17(1), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: The HyperModel benchmark [And90] represents the algorithms and data structures of hypertext applications. The ACOB benchmark [DFMV90] compares alternative client-server process architectures. The OO1 benchmark <ref> [CS92] </ref> models typical engineering applications. OO7 [CDN93] is a more comprehensive model of engineering applications. These benchmarks have been very useful for debugging and understanding performance problems of ODBMSes.
Reference: [DFMV90] <author> David DeWitt, Philippe Futtersack, David Maier, and Fernando Velez. </author> <title> A Study of Three Alternative Workstation-Server Architectures for Object-Oriented Database Systems. </title> <booktitle> In Proc. VLDB, </booktitle> <month> August </month> <year> 1990. </year> <month> 13 </month>
Reference-contexts: The HyperModel benchmark [And90] represents the algorithms and data structures of hypertext applications. The ACOB benchmark <ref> [DFMV90] </ref> compares alternative client-server process architectures. The OO1 benchmark [CS92] models typical engineering applications. OO7 [CDN93] is a more comprehensive model of engineering applications. These benchmarks have been very useful for debugging and understanding performance problems of ODBMSes.
Reference: [GFHH93] <author> Eric Golin, Annette Feng, Linus Huang, and Eric Hughes. </author> <title> A Visual Design Environ--ment. </title> <booktitle> In Proc. ICCAD, </booktitle> <pages> pages 364-367. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: These techniques are beyond the scope of this paper. 4 Experiments In this section, we describe the use of the PEDCAD framework to study the performance of two test applications, OO7 [CDN93] and ViDEO <ref> [GFHH93] </ref>. OO7 is an ODBMS benchmark which 8 models engineering applications. We use OO7 because optimized code is available for comparison with PEDCAD's generated code. <p> In contrast, the estimator reports an 83% probability that GS and OP are significantly different and less than a 70% probability that GE and OS are significantly different. 4.2 ViDEO Experiments The ViDEO application consists of about 30 programs <ref> [GFHH93] </ref>, four of which have been described in PEDCAD for performance evaluation. The partial application description was used to generate a variant which uses sets for a group of collections which must support ordered iteration.
Reference: [HW95] <author> Eric Hughes and Marianne Winslett. </author> <title> The Index Suggestion Problem for ODBMSes. </title> <note> Submitted for publication, </note> <year> 1995. </year>
Reference-contexts: Most ODBMS application writers will be domain experts, but not necessarily ODBMS experts. We have developed a tool which records operations on collections during a training run of the application and then suggests indexes for the application based on cost estimation metrics <ref> [HW95] </ref>. The final aspect of assisting variant generation is keeping track of existing variants and what database constructs they use. In the course of a detailed evaluation of an application, many variants may be created which have a mix of good and bad performance aspects.
Reference: [LC92] <author> Yingsha Liao and Donald Cohen. </author> <title> A Specificational Approach to High Level Program Monitoring and Measuring. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(11) </volume> <pages> 969-978, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: However, different ODBMSes have different performance models, and existing ODBMS performance models do not represent the complete interface. Finally, Liao and Cohen have described an approach to performance evaluation of programs in which the application writer specifies the measurements of interest in a high-level language <ref> [LC92] </ref>. A CASE tool then adds code to the application to collect data and performs the necessary calculations to extract the desired measurements from the data collected. We use a simpler approach for inserting measurement code in applications. <p> Program descriptions consist of functions and method implementations, both of which are composed of program statements. Program statements can be divided into database operations and ordinary C++ statements. In this work, we focus on the database operations, as other techniques can be applied to the ordinary C++ statements <ref> [LC92] </ref>. An example application description fragment is given in Figure 2. Figure 2 (a) shows the declaration for class Module with 4 extent (named modules) and a user-defined method, t1, which returns an integer. Class Module also defines three attributes, two of which are relationships with inverse attributes.
Reference: [Obj92] <editor> ObjectStore Release 2.0 User Guide. </editor> <title> Object Design, </title> <publisher> Inc., </publisher> <year> 1992. </year>
Reference-contexts: We use ViDEO to show that PEDCAD can be used to improve the performance of real applications. The experiments described below use the C++ interface of the ObjectStore Release 2.0 ODBMS <ref> [Obj92] </ref>. The database server is a Sparc Station 10 with 32 MB RAM, 64 MB swap on a Sun 400 MB hard drive, and with the database files on a Fujitsu 700 MB hard drive. <p> The set database is larger after T3 for two reasons: (1) the lists are slightly more compact than the sets, and (2) the set variant requires the indexed attribute to be marked as indexable in the schema, which causes objects with that attribute to be slightly larger <ref> [Obj92] </ref>. The set database becomes much larger than the list database when the indexes are added in T4. The results of this experiment allowed us to make a significant improvement in the performance of ViDEO by using sorted lists instead of indexed sets.
Reference: [Ont92] <editor> Ontos DB 2.2 Developer's Guide. ONTOS, </editor> <publisher> Inc., </publisher> <year> 1992. </year>
Reference-contexts: PEDCAD's variant generation tool does not replace debugging tools which are tailored to the interface of the ODBMS. For example, in the Ontos ODBMS, pointers to persistent objects do not necessarily remain valid across transaction boundaries <ref> [Ont92] </ref>. Thus, it is often incorrect to follow such a pointer within a transaction other than the transaction in which the pointer is set. The variant generator detect these errors due to the undecidability of program behavior. <p> For the OO7 experiments, a statistically significant performance advantage for a generated code variant always corresponds to an advantage for the associated optimized code variant. Some variants were generated for the Ontos Release 2.2 ODBMS <ref> [Ont92] </ref> to demonstrate the ability to generate variants for different ODBMSes. In addition, we have run 10 ViDEO experiments covering clustering, reference objects, collection optimization, composite objects, and versions. The experiments were designed by the authors, who are not naive users of the ODBMS.
Reference: [RAN + 93] <author> D. Reed, R. Aydt, R. Noe, P. Roth, K. Shields, B. Schwartz, and L. Tavera. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <editor> In Anthony Sjkellum, editor, </editor> <booktitle> Proceedings of the Scalable Parallel Libraries Conference. IEEE Computer Society, </booktitle> <year> 1993. </year>
Reference-contexts: The prototype also generates scripts which execute performance tests and which can be customized by the application writer. One remaining aspect of performance test management is result presentation. Many techniques for performance result presentation have been published and can be applied within our framework <ref> [RAN + 93] </ref>. These techniques are beyond the scope of this paper. 4 Experiments In this section, we describe the use of the PEDCAD framework to study the performance of two test applications, OO7 [CDN93] and ViDEO [GFHH93]. OO7 is an ODBMS benchmark which 8 models engineering applications.
Reference: [Sch94] <author> Hartmut Schreiber. </author> <title> JUSTITIA A Generic Benchmark for the OODBMS Selection. </title> <booktitle> In Proc. International Conference on Data and Knowledge Systems for Manufacturing and Engineering, </booktitle> <year> 1994. </year>
Reference-contexts: We refer to the above works as standard benchmarks, because they are precise tests which are not adapted to the user's application. Some more recent efforts have produced parameterized benchmarks, which can be tailored to represent an application. One example is JUSTITIA <ref> [Sch94] </ref>, a benchmark program which is run by the application writer and which uses parameter values to control the benchmark. To use a parameterized benchmark, the application writer must find a set of parameter values which accurately represent the application.
Reference: [SW93] <author> Connie U. Smith and Lloyd G. Williams. </author> <title> Software Performance Engineering: A Case Study Including Performance Comparison with Design Alternatives. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(7) </volume> <pages> 720-741, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Anderson and Lazowska also include a discussion of the goals for the design of Quartz [AL90], many of which have been adapted to our work. Smith and Williams have surveyed the inclusion of performance estimation in CASE tools 2 <ref> [SW93] </ref>. They note that some CASE tools can estimate performance during application design by estimating the cost of abstract components and by simulating the behavior of the design. The simulation can be used to guide the refinement of the application to run as fast as possible.
Reference: [TN92] <author> Manolis Tsangaris and Jeffrey Naughton. </author> <title> On the Performance of Object-Clustering Techniques. </title> <booktitle> In Proc. SIGMOD, </booktitle> <pages> pages 144-153. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: In the course of this work, we have identified some interesting areas for future effort. First, it is important to assist the application writer in generating variants which use complex ODBMS features. Two interesting examples are clustering and indexing. Algorithms exist which automatically cluster objects in a database <ref> [TN92] </ref> and we suspect that these algorithms can be adapted to suggest clustering policies for ODBMS applications. ODBMSes typically give the user some degree of control over these features. In our future work we will investigate techniques for suggesting indexes for an ODBMS application.
Reference: [VHD88] <editor> IEEE Standard 1076-1987: </editor> <title> IEEE Standard VHDL Language Reference Manual. </title> <institution> IEEE Standards Board, </institution> <month> March </month> <year> 1988. </year> <month> 14 </month>
Reference-contexts: The optimized code variants used in the experiments include only the gendb and trav1 programs from OO7, which generate and traverse a parts database. ViDEO is a research CAD tool which supports modeling and simulation of electronics circuits using the VHDL hardware description language <ref> [VHD88] </ref>. We use ViDEO to show that PEDCAD can be used to improve the performance of real applications. The experiments described below use the C++ interface of the ObjectStore Release 2.0 ODBMS [Obj92].
References-found: 17

