URL: ftp://grilled.cs.wisc.edu/naim/hpcn94.ps.gz
Refering-URL: http://www.cs.wisc.edu/~naim/publications.html
Root-URL: 
Email: Email: on92r@ecs.soton.ac.uk  
Title: Do-Loop-Surface: An Abstract Performance Data Visualization  
Author: Oscar Nam and A.J.G. Hey 
Keyword: Matrix Multiply parallel algorithm.  
Address: Mountbatten Building, Lab. 4037  Southampton, Southampton S09 5NH, UK  
Affiliation: Department of Electronics and Computer Science  University of  
Abstract: Performance is a critical issue in current massively parallel processors. However, to get adequate performance is not an easy task and performance tools are required in order to help the programmer to understand the behaviour of a parallel program. In recent years, a wide variety of tools have been developed for this purpose including tools for monitoring and evaluating performance and visualization tools. However, these tools do not provide an abstract representation of performance. Massively parallel processors could produce a huge amount of performance data and sophisticated methods for representing and displaying this data are required. The Do-Loop-Surface (DLS) display is an abstract representation of the performance of a particular do-loop in a program and it is implemented using AVS, a Data Visualization Tool. In this paper, this new representation is presented and used in order to improve the performance of a 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Cliff Addison, James Allwright, Norman Binsted, Nigel Bishop, Bryan Carpenter, Peter Dalloz, David Gee, Vladimir Getov, Tony Hey, Roger Hockney, Max Lemke, John Merlin, Mark Pinches, Chris Scott, and Ivan Wolton. </author> <title> The Genesis Distributed Memory Benchmarks-I. Methodology and General Relativity benchmark with results for the SUPRENUM Computer. For submission to Concurrency: </title> <journal> Practice and Experience, </journal> <month> March </month> <year> 1992. </year>
Reference: 2. <institution> Advanced Visual Systems Inc. </institution> <note> AVS User's Guide, Release 4, </note> <month> May </month> <year> 1992. </year>
Reference-contexts: A new and evocative vocabulary that explains the particular features of the display is also incorporated (e.g. mountains, valleys, hilly, flat). This surface is displayed using AVS <ref> [2] </ref> (Application Visualization System), which allows rotation of the figure in several ways, as well as zooming when necessary and some other interesting transformations. The main goals of a DLS are: To provide an abstract representation of the performance of the program. <p> the Do-Loop-Surface (DLS) representation of the performance of a particular do-loop in a program has been presented as an alternative to provide an abstract and scalable representation of performance (at least of the performance of a particular do-loop or kernel in a program), using an existing Data Visualization Tool (AVS <ref> [2] </ref>) for this purpose.
Reference: 3. <author> G. Almasi, B. Alpern, C. Berman, Larry Carter, and D. Hale. </author> <title> A Case-study in performance programming: Seismic Migration. </title> <booktitle> In 2nd Symposium on High Performance Computing, </booktitle> <pages> pages 195-206. </pages> <publisher> North Holland, </publisher> <month> October </month> <year> 1991. </year>
Reference: 4. <author> Arndt Bode and Peter Braun. </author> <title> Monitoring and visualisation in topsys. </title> <booktitle> In Workshop on Performance Measurement and Visualization of Parallel Systems, </booktitle> <address> Moravany, Czecho-Slovakia, </address> <month> October </month> <year> 1992. </year>
Reference: 5. <author> Larry Carter. </author> <title> Private Electronic Mail Communication. </title> <month> January </month> <year> 1993. </year>
Reference: 6. <author> Alva Couch. </author> <title> Categories and Context in Scalable Execution Visualization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 195-204, </pages> <year> 1993. </year>
Reference: 7. <author> Joan Francioni, L. Albright, and J. Jackson. </author> <title> Debugging parallel programs using sound. </title> <journal> In SIGPLAN Notices, </journal> <volume> volume 26, </volume> <pages> pages 68-75, </pages> <year> 1991. </year>
Reference: 8. <author> Joan Francioni, L. Albright, and J. Jackson. </author> <title> The sounds of parallel programs. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference. IEEE Computer Society, </booktitle> <year> 1991. </year>
Reference: 9. <author> Joan Francioni and Diane Rover. </author> <title> Visual-Aural representations of performance for a scalable application program. </title> <booktitle> In Scalable High Performance Computing Conference, SHPCC-92, </booktitle> <pages> pages 433-440. </pages> <publisher> IEEE Computer Society, </publisher> <month> April </month> <year> 1992. </year>
Reference: 10. <author> M. Heath and J. Etheridge. </author> <title> Visualizing the performance of parallel programs. </title> <journal> IEEE Software, </journal> <pages> pages 29-39, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: This technique has been widely used and interesting research efforts can be found in the literature. NO FILE: dls.sync.ps Fig. 1. Matrix Multiply: Do-Loop-Surface (Synchronous communication). NO FILE: dls.async.ps Fig. 2. Matrix Multiply: Do-Loop-Surface (Asynchronous communication). Popular views like space-time/Feynman diagram (ParaGraph <ref> [10] </ref>) or event timelines (Express Etool [21]) often provide some insight about program beha-viour. Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views [24].
Reference: 11. <author> Michael Heath. </author> <title> Recent Developments and Case Studies in Performance Visualization using ParaGraph. </title> <booktitle> In Workshop on Performance Measurement and Visualization of Parallel Systems, </booktitle> <address> Moravany, Czecho-Slovakia, </address> <month> October </month> <year> 1992. </year>
Reference: 12. <author> R. Hempel. </author> <title> The ANL/GMD Macros (PARMACS) in FORTRAN for Portable Parallel Programming using the Message Passing Programming Model. User's Guide and Reference Manual. </title> <publisher> Pallas GmbH, </publisher> <year> 1991. </year>
Reference-contexts: The hardware platform consist of a 64 T800 Parsys Supernode and the program was written in C using PARMACS <ref> [12] </ref> (communication library). 3.1 Results for a 100x100 matrix on 32 processors The DLS of figure 1 represents a matrix multiplication algorithm, for two matrices of dimension 100x100, on 32 processors. The values correspond to the main do-loop of the algorithm.
Reference: 13. <institution> Institut fur Informatik, Technische Universitat Munchen. </institution> <note> TOPSYS User's Overview, Version 1.0, </note> <month> December </month> <year> 1990. </year>
Reference: 14. <author> T. LeBlanc, J. Mellor-Crummey, and R. Fowler. </author> <title> Analyzing parallel program executions using multiple views. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 203-217, </pages> <year> 1990. </year>
Reference: 15. <author> Allen Malony and Gregory Wilson. </author> <title> Future directions in parallel performance environments. </title> <booktitle> In Workshop on Performance Measurement and Visualization of Parallel Systems, </booktitle> <address> Moravany, Czecho-Slovakia, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Performance visualization is the use of graphical display techniques for the analysis of performance data in order to improve understanding of complex performance phenomena <ref> [15] </ref>. Performance visualization systems for parallel programs have been helpful in the past and they are commonly used in order to improve parallel program performance.
Reference: 16. <author> John Merlin. </author> <title> HPF Visualization Tools Proposal. </title> <type> Technical report, </type> <institution> Southampton University, </institution> <month> April </month> <year> 1993. </year>
Reference: 17. <author> Barton Miller. </author> <title> What to draw? when to draw? An essay on parallel program visualization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(2) </volume> <pages> 265-269, </pages> <year> 1993. </year>
Reference: 18. <author> Bernd Mohr. </author> <title> SIMPLE: A Performance Evaluation Tool Environment for Parallel and Distributed Systems. </title> <booktitle> In EDMCC2, </booktitle> <address> Munich, </address> <month> April </month> <year> 1991. </year>
Reference: 19. <author> Oscar Nam and Alejandro Teruel. Consideraciones sobre el Paralelismo en SIM-PAR (FASE I). </author> <type> Technical Report IT-1991-001, </type> <institution> Department of Computer Science& Information Technology, Universidad Simon Bolvar, Caracas-Venezuela, </institution> <year> 1991. </year>
Reference: 20. <author> Pallas, </author> <title> GmbH. PA-Tools, Performance Analysis Tools, </title> <year> 1991. </year>
Reference: 21. <institution> Parasoft Corporation. ParaSoft Express. </institution> <note> User's Guide, </note> <year> 1990. </year>
Reference-contexts: This technique has been widely used and interesting research efforts can be found in the literature. NO FILE: dls.sync.ps Fig. 1. Matrix Multiply: Do-Loop-Surface (Synchronous communication). NO FILE: dls.async.ps Fig. 2. Matrix Multiply: Do-Loop-Surface (Asynchronous communication). Popular views like space-time/Feynman diagram (ParaGraph [10]) or event timelines (Express Etool <ref> [21] </ref>) often provide some insight about program beha-viour. Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views [24].
Reference: 22. <author> Daniel Reed, Ruth Aydt, Tara Madhyastha, Roger Noe, Keith Shields, and Brad-ley Schwartz. </author> <title> The PABLO performance analysis environment. </title> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference: 23. <author> Diane Rover and Charles Wright. </author> <title> Visualizing the Performance of SPMD and Data-Parallel Programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 129-146, </pages> <year> 1993. </year>
Reference: 24. <author> Sekhar Sarukkai, Doug Kimelman, and Larry Rudolph. </author> <title> A methodology for visualizing performance of loosely synchronous programs. </title> <booktitle> In Scalable High Performance Computing Conference, SHPCC-92, </booktitle> <pages> pages 424-432. </pages> <publisher> IEEE Computer Society, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: Matrix Multiply: Do-Loop-Surface (Asynchronous communication). Popular views like space-time/Feynman diagram (ParaGraph [10]) or event timelines (Express Etool [21]) often provide some insight about program beha-viour. Unfortunately, for these tools to be truly useful in the domain of large scale parallel machines they must include abstract high level views <ref> [24] </ref>.
Reference: 25. <author> Margaret Simmons and Rebecca Koskela. </author> <title> Performance Instrumentation and Visualization. </title> <publisher> ACM Press, Frontier Series, </publisher> <year> 1990. </year>
Reference: 26. <author> Brian Wylie, Michael Norman, and Lyndon Clarke. </author> <title> High Performance Fortran: A Perspective. </title> <institution> The University of Edinburgh, EPCC-TN92-05.04, </institution> <month> May </month> <year> 1992. </year> <title> This article was processed using the LT E X macro package with LLNCS style </title>
References-found: 26

