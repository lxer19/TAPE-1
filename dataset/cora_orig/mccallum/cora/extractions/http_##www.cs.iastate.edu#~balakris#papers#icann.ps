URL: http://www.cs.iastate.edu/~balakris/papers/icann.ps
Refering-URL: http://www.cs.iastate.edu/~balakris/publications.html
Root-URL: http://www.cs.iastate.edu
Title: Improving Convergence of Back-Propagation by Handling Flat-Spots in the Output Layer  
Author: Karthik Balakrishnan and Vasant Honavar 
Address: Iowa 50011, USA  
Affiliation: Department of Computer Science Iowa State University, Ames  
Abstract: Back-propagation (BP) [9, 5] is one of the most widely used procedures for training multi-layer artificial neural networks with sigmoid units. Though successful in a number of applications, its convergence to a set of desired weights can be excruciatingly slow. Several modifications have been proposed for improving the learning speed [2, 4, 8, 1, 6]. The phenomenon of flat-spots is known to play a significant role in the slow convergence of BP [2]. The formulation of the BP Learning rule prevents the network from learning effectively in the presence of flat-spots. In this paper we propose a new approach to minimize the error such that flat-spots occurring in the output layer are appropriately handled, thereby permitting the network to learn even in the presence of flat-spots. The improvement provided by the technique is demonstrated on a number of standard benchmark data-sets. More importantly, the speedup in learning is obtained with little or no increase in the computational requirements of each iteration. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Becker, S. and LeCun, Y. </author> <title> The Feasibility of Applying Numerical Optimization Techniques to Back-Propagation. </title> <booktitle> Proceedings, 1988 Connectionist Models Summer School. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction Back-propagation is quite popular as a training algorithm for multi-layer networks. However, its slow convergence makes it inappropriate for learning real-world tasks. Several authors have explored modifications of BP which result in faster learning <ref> [2, 4, 8, 1, 6] </ref>. Most of these modifications are either extremely heuristic or else make use of sophisticated optimization techniques. In this paper, we suggest a modification that is motivated by the theory behind the occurrence of flat-spots, and that is also simple and intuitively appealing.
Reference: [2] <author> Fahlman, S. </author> <title> E . Faster-learning variations of backpropagation : An empirical study. </title> <editor> In D. Touretzky, G. E. Hinton, and T. J Sejnowski (Eds), </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers , 38-51, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction Back-propagation is quite popular as a training algorithm for multi-layer networks. However, its slow convergence makes it inappropriate for learning real-world tasks. Several authors have explored modifications of BP which result in faster learning <ref> [2, 4, 8, 1, 6] </ref>. Most of these modifications are either extremely heuristic or else make use of sophisticated optimization techniques. In this paper, we suggest a modification that is motivated by the theory behind the occurrence of flat-spots, and that is also simple and intuitively appealing. <p> In this paper, we suggest a modification that is motivated by the theory behind the occurrence of flat-spots, and that is also simple and intuitively appealing. The points where the derivative of the sigmoid activation function approaches zero are commonly referred to as flat-spots <ref> [2] </ref>. In the conventional BP framework flat-spots are undesirable as they prevent weight updates, leading to slower learning. <p> input error in the equations for ffi i transforms the equations to : ffi i = ( (target i out i ) f 0 if i is an output unit ( j w ij ffi j ):f 0 if i is a hidden unit Considering the weight update equations (ref <ref> [2] </ref>) it is easy to notice that the weights feeding into the output layer are updated whenever there is an error in the output and that the characteristics of the sigmoid do not interfere with the necessity for weight updates. <p> This has to be handled properly. We define a unit to be in the active range if its output is greater than a predefined value of (1 MARGIN) and less than MARGIN, a technique used by Fahlman <ref> [2] </ref>. In our experiments, MARGIN was set at 0.9. If a unit is in the active range, the weight update rules use the ffi i equation just derived, otherwise weight updates are made using the ffi i expression with its denominator replaced by a constant- 0.09.
Reference: [3] <author> Honavar, V., and Uhr, L. </author> <title> Generative Learning Structures for Generalized Connectionist Networks. </title> <journal> In Information Sciences,( Special Issue on Neural Networks and Artificial Intelligence). </journal> <note> In press. </note>
Reference: [4] <author> Parker, D. B. </author> <title> Optimal Algorithms for Adaptive Networks: Second Order Back Propagation, Second Order Direct Propagation, and Second Order Hebbian Learning. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pp 593-600. </pages> <address> San Diego, CA, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Back-propagation is quite popular as a training algorithm for multi-layer networks. However, its slow convergence makes it inappropriate for learning real-world tasks. Several authors have explored modifications of BP which result in faster learning <ref> [2, 4, 8, 1, 6] </ref>. Most of these modifications are either extremely heuristic or else make use of sophisticated optimization techniques. In this paper, we suggest a modification that is motivated by the theory behind the occurrence of flat-spots, and that is also simple and intuitively appealing.
Reference: [5] <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <title> Learning Internal Representations by Error-Propagation (ICS Report 8506), </title> <institution> Institute for Cognitive Science, </institution> <address> UCSD, La Jolla, CA, </address> <year> 1985. </year>
Reference-contexts: = p i (target p p where target p i refers to the Target output of the ith output unit, for the pth pattern, and out i refers to the Obtained output of the ith unit, for the pth pattern The weight-update equations which minimize E can be derived (ref <ref> [5] </ref> for details of the derivation). The sigmoid activation function has the property that its derivative approaches zero when the output of the corresponding unit approaches `0' or `1', the well-known phenomenon of flat-spots.
Reference: [6] <author> Samad, T. </author> <title> Back Propagation With Expected Source Values. </title> <booktitle> In Neural Networks, </booktitle> <address> Vol.4,pp.615-618, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Back-propagation is quite popular as a training algorithm for multi-layer networks. However, its slow convergence makes it inappropriate for learning real-world tasks. Several authors have explored modifications of BP which result in faster learning <ref> [2, 4, 8, 1, 6] </ref>. Most of these modifications are either extremely heuristic or else make use of sophisticated optimization techniques. In this paper, we suggest a modification that is motivated by the theory behind the occurrence of flat-spots, and that is also simple and intuitively appealing.
Reference: [7] <author> Shavlik, J. W., Mooney, R. J. and Towell, G. G. </author> <title> Symbolic and Neural Learning Algorithms : An Experimental Comparison. </title> <type> Technical Report #955, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, Madison, WI, </institution> <year> 1990. </year>
Reference-contexts: The simulation conditions were as close to the original reports as possible. The use of Iris data-set was inspired by the work of Yang and Honavar [10]. The soybean data-set was motivated by the work of Shavlik et al. <ref> [7] </ref>. 4.1 Results * XOR Problem : This task was simulated using BP and the modified method. The best results for both methods are tabulated. The convergence criterion was set at 0.01 mean squared error. For each parameter setting, ten simulation runs were made with random initial weight settings.
Reference: [8] <author> Watrous, R. L. </author> <title> Learning Algorithms for Connectionist Networks : Applied Gradient Methods for Non-Linear Optimization. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pp 619-627. </pages> <address> San Diego, CA, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Back-propagation is quite popular as a training algorithm for multi-layer networks. However, its slow convergence makes it inappropriate for learning real-world tasks. Several authors have explored modifications of BP which result in faster learning <ref> [2, 4, 8, 1, 6] </ref>. Most of these modifications are either extremely heuristic or else make use of sophisticated optimization techniques. In this paper, we suggest a modification that is motivated by the theory behind the occurrence of flat-spots, and that is also simple and intuitively appealing.
Reference: [9] <author> Werbos, P. J. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis , Harvard University, </type> <year> 1974. </year>
Reference: [10] <author> Yang, J and Honavar, V. </author> <title> Experiments with the Cascade-Correlation Algorithm. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks. </booktitle> <address> Singapore, </address> <year> 1991. </year>
Reference-contexts: We also report results on some Real-world data sets for which BP simulation results are available in the literature. The simulation conditions were as close to the original reports as possible. The use of Iris data-set was inspired by the work of Yang and Honavar <ref> [10] </ref>. The soybean data-set was motivated by the work of Shavlik et al. [7]. 4.1 Results * XOR Problem : This task was simulated using BP and the modified method. The best results for both methods are tabulated. The convergence criterion was set at 0.01 mean squared error.
References-found: 10

