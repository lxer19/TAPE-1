URL: http://www.cs.ucsd.edu/~calder/papers/JplVersion.ps.Z
Refering-URL: http://www.cs.ucsd.edu/~calder/papers.html
Root-URL: http://www.cs.ucsd.edu
Email: fcalder,grunwald,zorng@cs.colorado.edu  
Title: Programs  
Author: Brad Calder, Dirk Grunwald, and Benjamin Zorn 
Keyword: Quantifying Behavioral Differences  
Date: March 28, 1995  
Address: Campus Box 430  Boulder, CO 80309-0430 USA  
Affiliation: Department of Computer Science  University of Colorado  
Note: Between C and C++  
Abstract: This paper appeared in the Journal of Programming Languages, Vol 2, Num 4, 1994. Abstract Improving the performance of C programs has been a topic of great interest for many years. Both hardware technology and compiler optimization research has been applied in an effort to make C programs execute faster. In many application domains, the C++ language is replacing C as the programming language of choice. In this paper, we measure the empirical behavior of a group of significant C and C++ programs and attempt to identify and quantify behavioral differences between them. Our goal is to investigate whether optimization technology that has been successful for C programs will also be successful in C++ programs. We furthermore identify behavioral characteristics of C++ programs that suggest optimizations that should be applied in those programs. Our results show that C++ programs exhibit behavior that is significantly different than C programs. These results should be of interest to compiler writers and architecture designers who are designing systems to execute object-oriented programs.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David R. Ditzel and H. R. McLellan. </author> <title> Register allocation for free: The C stack machine. </title> <booktitle> In Proceedings of the Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 48-56, </pages> <address> Palo Alto, CA, </address> <month> March </month> <year> 1982. </year>
Reference-contexts: 1 Introduction The design of computer architecture is typically driven by the needs of various programs and programming languages. A significant amount of research, both in compiler optimization and in architecture design, has been conducted with the specific goal of improving the performance of existing conventional programs <ref> [1, 2] </ref>. Early studies of program behavior [3, 4, 5, 6] have guided architectural design, and the importance of measurement and simulation has permeated architectural design philosophy [7]. In particular, the IBM 801, Berkeley, and Stanford RISC projects were all guided by studies of C and FORTRAN programs [8].
Reference: [2] <author> Andrew Tannenbaum. </author> <title> Implications of structured programming for machine architecture. </title> <journal> Communications of the ACM, </journal> <volume> 21(3) </volume> <pages> 237-246, </pages> <month> March </month> <year> 1978. </year>
Reference-contexts: 1 Introduction The design of computer architecture is typically driven by the needs of various programs and programming languages. A significant amount of research, both in compiler optimization and in architecture design, has been conducted with the specific goal of improving the performance of existing conventional programs <ref> [1, 2] </ref>. Early studies of program behavior [3, 4, 5, 6] have guided architectural design, and the importance of measurement and simulation has permeated architectural design philosophy [7]. In particular, the IBM 801, Berkeley, and Stanford RISC projects were all guided by studies of C and FORTRAN programs [8].
Reference: [3] <author> G. Alexander and D. Wortman. </author> <title> Static and dynamic characteristics of XPL programs. </title> <journal> IEEE Computer, </journal> <volume> 8(11) </volume> <pages> 41-46, </pages> <month> November </month> <year> 1975. </year>
Reference-contexts: A significant amount of research, both in compiler optimization and in architecture design, has been conducted with the specific goal of improving the performance of existing conventional programs [1, 2]. Early studies of program behavior <ref> [3, 4, 5, 6] </ref> have guided architectural design, and the importance of measurement and simulation has permeated architectural design philosophy [7]. In particular, the IBM 801, Berkeley, and Stanford RISC projects were all guided by studies of C and FORTRAN programs [8]. <p> We mention these measurements because our measurements include the SPECint92 programs, and in some cases can be compared directly with these previous results. 2 While other work in this area has concentrated on the behavior of programs written in a single language <ref> [3, 5] </ref>, our focus is on comparing the relative behavior of programs written in C and C++, two closely related languages.
Reference: [4] <author> John Cocke and Peter Markstein. </author> <title> Measurement of program improvement algorithms. </title> <booktitle> In Information Processing 80, </booktitle> <pages> pages 221-228. </pages> <publisher> IFIP, North-Holland Publishers, </publisher> <year> 1980. </year>
Reference-contexts: A significant amount of research, both in compiler optimization and in architecture design, has been conducted with the specific goal of improving the performance of existing conventional programs [1, 2]. Early studies of program behavior <ref> [3, 4, 5, 6] </ref> have guided architectural design, and the importance of measurement and simulation has permeated architectural design philosophy [7]. In particular, the IBM 801, Berkeley, and Stanford RISC projects were all guided by studies of C and FORTRAN programs [8].
Reference: [5] <author> J. Elshoff. </author> <title> A numerical profile of commerical PL/1 programs. </title> <journal> Software Practice and Experience, </journal> <volume> 6 </volume> <pages> 505-525, </pages> <year> 1976. </year>
Reference-contexts: A significant amount of research, both in compiler optimization and in architecture design, has been conducted with the specific goal of improving the performance of existing conventional programs [1, 2]. Early studies of program behavior <ref> [3, 4, 5, 6] </ref> have guided architectural design, and the importance of measurement and simulation has permeated architectural design philosophy [7]. In particular, the IBM 801, Berkeley, and Stanford RISC projects were all guided by studies of C and FORTRAN programs [8]. <p> We mention these measurements because our measurements include the SPECint92 programs, and in some cases can be compared directly with these previous results. 2 While other work in this area has concentrated on the behavior of programs written in a single language <ref> [3, 5] </ref>, our focus is on comparing the relative behavior of programs written in C and C++, two closely related languages.
Reference: [6] <author> Donald E. Knuth. </author> <title> An empirical study of FORTRAN programs. </title> <journal> Software, Practice and Experience, </journal> <volume> 1 </volume> <pages> 105-133, </pages> <year> 1971. </year>
Reference-contexts: A significant amount of research, both in compiler optimization and in architecture design, has been conducted with the specific goal of improving the performance of existing conventional programs [1, 2]. Early studies of program behavior <ref> [3, 4, 5, 6] </ref> have guided architectural design, and the importance of measurement and simulation has permeated architectural design philosophy [7]. In particular, the IBM 801, Berkeley, and Stanford RISC projects were all guided by studies of C and FORTRAN programs [8]. <p> Knuth measured both static and dynamic behavior of a large collection of Fortran programs <ref> [6] </ref>. Among other things, he concluded that programmers had poor intuition about what parts of their programs were the most time-consuming, and that execution profiles would significantly help programmers improve the performance of their programs.
Reference: [7] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Early studies of program behavior [3, 4, 5, 6] have guided architectural design, and the importance of measurement and simulation has permeated architectural design philosophy <ref> [7] </ref>. In particular, the IBM 801, Berkeley, and Stanford RISC projects were all guided by studies of C and FORTRAN programs [8]. More recent studies have used the SPEC program suite. <p> All of these analyses were conducted to give architecture designers insight into how to improve the next-generation architecture. This approach to architecture design has become so familiar that the method can now be found described in the popular textbook, Computer Architecture: A Quantitative Approach <ref> [7] </ref>. More recently, published measurements of this kind have concentrated on the SPEC benchmark programs (e.g., [17]). Because the SPEC benchmarks are widely used to compare system performance of workstations, compiler writers and architects study these and similar programs in detail.
Reference: [8] <author> Manolis G. H. Katevenis. </author> <title> Reduced Instruction Set Computer Architecture for VLSI. ACM Doctoral Dissertation Award Series. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: Early studies of program behavior [3, 4, 5, 6] have guided architectural design, and the importance of measurement and simulation has permeated architectural design philosophy [7]. In particular, the IBM 801, Berkeley, and Stanford RISC projects were all guided by studies of C and FORTRAN programs <ref> [8] </ref>. More recent studies have used the SPEC program suite. This set of programs, widely used to benchmark new hardware platforms and compiler implementations, consists of a mixture of C and FORTRAN programs [9, 10].
Reference: [9] <institution> Standard Performance Evaluation Corporation, Fairfax, VA. </institution> <note> SPEC CFP92 Technical Manual, release v1.1 edition, </note> <year> 1992. </year>
Reference-contexts: More recent studies have used the SPEC program suite. This set of programs, widely used to benchmark new hardware platforms and compiler implementations, consists of a mixture of C and FORTRAN programs <ref> [9, 10] </ref>. More recently, object-oriented programming, and specifically the language C++, has become widely used and is replacing procedural languages such as C in a number of application areas including user-interfaces, data structure libraries, scientific computing [11], and operating systems [12]. <p> In 1988, Weicker updated the Dhrystone benchmark, creating version 2.0, which is the version of Dhrystone that we measure. Since 1984, measurements of a number of aspects of program behavior have appeared. The SPEC benchmark suite <ref> [9, 10] </ref>, used in recent years to compare the performance of new computer architectures, has been investigated both in terms of instruction set usage [17, 18] and cache locality [19, 20].
Reference: [10] <institution> Standard Performance Evaluation Corporation, Fairfax, VA. </institution> <note> SPEC CINT92 Technical Manual, release v1.1 edition, </note> <year> 1992. </year>
Reference-contexts: More recent studies have used the SPEC program suite. This set of programs, widely used to benchmark new hardware platforms and compiler implementations, consists of a mixture of C and FORTRAN programs <ref> [9, 10] </ref>. More recently, object-oriented programming, and specifically the language C++, has become widely used and is replacing procedural languages such as C in a number of application areas including user-interfaces, data structure libraries, scientific computing [11], and operating systems [12]. <p> In 1988, Weicker updated the Dhrystone benchmark, creating version 2.0, which is the version of Dhrystone that we measure. Since 1984, measurements of a number of aspects of program behavior have appeared. The SPEC benchmark suite <ref> [9, 10] </ref>, used in recent years to compare the performance of new computer architectures, has been investigated both in terms of instruction set usage [17, 18] and cache locality [19, 20].
Reference: [11] <author> Jack Dongarra, Roldan Pozo, and David Walker. </author> <title> LAPACK++: A design overview of object-oriented extensions for high performance linear algebra. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 162-171. </pages> <publisher> IEEE Press, </publisher> <address> Washington D.C., </address> <month> November </month> <year> 1993. </year>
Reference-contexts: More recently, object-oriented programming, and specifically the language C++, has become widely used and is replacing procedural languages such as C in a number of application areas including user-interfaces, data structure libraries, scientific computing <ref> [11] </ref>, and operating systems [12]. While the C++ language is a superset of C, additional features provided in the language support a programming style that is very different from that of programming in C. <p> The C++ language was designed to be very efficient, introducing additional costs only when specific features (such as dynamic dispatch) are used. For example, careful design has produced a numerical library that can be used from C++ that is as efficient as a related FORTRAN library <ref> [11] </ref>, and efficient operating systems have been written in C++ [12]. In many cases, C++ programmers eschew `expensive' features, such as dynamic dispatch to achieve this efficiency.
Reference: [12] <author> R. H. Campbell, V. Russo, and G. M. Johnston. </author> <title> The Design of a Multiprocessor Operating System. </title> <booktitle> In Proc. USENIX C++ Workshop, </booktitle> <pages> pages 109-125, </pages> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: More recently, object-oriented programming, and specifically the language C++, has become widely used and is replacing procedural languages such as C in a number of application areas including user-interfaces, data structure libraries, scientific computing [11], and operating systems <ref> [12] </ref>. While the C++ language is a superset of C, additional features provided in the language support a programming style that is very different from that of programming in C. <p> For example, careful design has produced a numerical library that can be used from C++ that is as efficient as a related FORTRAN library [11], and efficient operating systems have been written in C++ <ref> [12] </ref>. In many cases, C++ programmers eschew `expensive' features, such as dynamic dispatch to achieve this efficiency.
Reference: [13] <author> Brad Calder and Dirk Grunwald. </author> <title> Reducing indirect function call overhead in C++ programs. </title> <booktitle> In 1994 ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 397-408, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Hardware designers and compiler writers may use our results to construct a new generation of systems that execute C++ programs more efficiently. We have conducted some of this research in related publications <ref> [13, 14, 15] </ref>, but a detailed discussion is beyond the scope of this paper. Section 2 describes related work in the area of program behavior measurement. Section 3 describes the tools we used to collect our measurements and the programs that we measured. <p> Conditional branch prediction 15 has been studied by a number of researchers; Lilja [31], McFarling and Hennessy [32] and Smith [33] present good surveys. Kaeli and Emma [34] showed that a hardware return stack effectively predicted the destination of procedure returns. In related work, we <ref> [13] </ref> have shown that indirect functions can be effectively predicted for C++ programs. <p> The results in Table 9 indicate that handling indirect calls, procedure calls, and returns properly will be important for C++ programs. Because indirect calls are quite frequent in C++, we and others are examining methods to reduce this overhead <ref> [13, 35] </ref>. The C++ language was designed to be very efficient, introducing additional costs only when specific features (such as dynamic dispatch) are used. <p> Therefore, inlining C++ procedures may result in less code expansion and there may be more promising candidates for inlining. The most relevant work in this area has been done for the Self language [45, 46]. In related work, we found considerable opportunity for similar optimizations for C++ programs <ref> [13] </ref>. In particular, we found that profile-directed multi-version procedure inlining may perform very well. Here, dynamic type checks and inlined procedures would expand the most frequently executed methods; some of these runtime checks may be eliminated by compile time type analysis [35]. <p> These results imply that different branch prediction architectures are needed for C and C++ programs in order to achieve a high prediction accuracy for the different languages. The implications of these differences are considered in other publications <ref> [13, 14, 15] </ref>, and a detailed discussion is beyond the scope of this paper. As described in x4.4, direct procedure calls and returns are easy to predict, while indirect procedure calls and conditional branches are more difficult to predict. <p> In part, this arises from the programming style; C programmers tend to pass myriad parameters and specialize behavior using those parameters. By comparison, object-oriented C++ programmers tend to use subroutine calls to handle common behavior, and those procedure calls are more predictable. As mentioned, other work <ref> [13] </ref> attempts to reduce the number of indirect function calls by conditionally inlining indirect function calls. However, the benefits of substituting conditional branches for indirect function calls is very dependent on the underlying architecture. Most current architectures have support for conditional branch prediction, but not for indirect function calls. <p> However, a direct function call (or inlined function expansion) would be possible, since any call to foo must call X::foo. In related work <ref> [13] </ref>, we found that 31% of all indirect function calls in similar C++ programs could be eliminated with this simple link-time optimization. Clearly, these optimizations do not address all programs or systems. Analyzing dynamically loaded programs or programs imported from shared libraries that may change from execution-to-execution, is problematic.
Reference: [14] <author> Brad Calder and Dirk Grunwald. </author> <title> Reducing branch costs via branch alignment. </title> <booktitle> In 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 242-251, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Hardware designers and compiler writers may use our results to construct a new generation of systems that execute C++ programs more efficiently. We have conducted some of this research in related publications <ref> [13, 14, 15] </ref>, but a detailed discussion is beyond the scope of this paper. Section 2 describes related work in the area of program behavior measurement. Section 3 describes the tools we used to collect our measurements and the programs that we measured. <p> If function calls can increase cache misses, the behavior of C functions can lead to fewer conflicts. Table 7 also shows that C++ can possibly benefit more than C programs from basic block reordering and procedure layout algorithms to eliminate cache conflicts. These algorithms <ref> [14, 40, 41] </ref> have been shown to efficiently reduce the number of instruction cache misses and improve branch prediction. 4.9.2 Data Cache The data (D) cache miss rate is a measure of the locality of reference of a program's access to data in the stack, static data segment, and heap. <p> These results imply that different branch prediction architectures are needed for C and C++ programs in order to achieve a high prediction accuracy for the different languages. The implications of these differences are considered in other publications <ref> [13, 14, 15] </ref>, and a detailed discussion is beyond the scope of this paper. As described in x4.4, direct procedure calls and returns are easy to predict, while indirect procedure calls and conditional branches are more difficult to predict.
Reference: [15] <author> Brad Calder and Dirk Grunwald. </author> <title> Fast and accurate instruction fetch and branch prediction. </title> <booktitle> In 21st Annual International Symposium of Computer Architecture, </booktitle> <pages> pages 2-11, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Hardware designers and compiler writers may use our results to construct a new generation of systems that execute C++ programs more efficiently. We have conducted some of this research in related publications <ref> [13, 14, 15] </ref>, but a detailed discussion is beyond the scope of this paper. Section 2 describes related work in the area of program behavior measurement. Section 3 describes the tools we used to collect our measurements and the programs that we measured. <p> These results imply that different branch prediction architectures are needed for C and C++ programs in order to achieve a high prediction accuracy for the different languages. The implications of these differences are considered in other publications <ref> [13, 14, 15] </ref>, and a detailed discussion is beyond the scope of this paper. As described in x4.4, direct procedure calls and returns are easy to predict, while indirect procedure calls and conditional branches are more difficult to predict.
Reference: [16] <author> Reinhold P. Weicker. Dhrystone: </author> <title> A synthetic systems programming benchmark. </title> <journal> Communications of the ACM, </journal> <volume> 27(10) </volume> <pages> 1013-1030, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Much of the work in the area of program behavior measurement prior to 1984 is summarized in Wiecker's paper describing the Dhrystone benchmark <ref> [16] </ref>. Weicker used these results to create a small benchmark program, Dhrystone, that was intended to simulate the average systems program behavior reported in previous work. In 1988, Weicker updated the Dhrystone benchmark, creating version 2.0, which is the version of Dhrystone that we measure.
Reference: [17] <author> R.F. Cmelik, S.I. Kong, D.R. Ditzel, and E.J. Kelly. </author> <title> An analysis of MIPS and SPARC instruction set utilization on the SPEC benchmarks. </title> <booktitle> In 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 290-302, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Since 1984, measurements of a number of aspects of program behavior have appeared. The SPEC benchmark suite [9, 10], used in recent years to compare the performance of new computer architectures, has been investigated both in terms of instruction set usage <ref> [17, 18] </ref> and cache locality [19, 20]. <p> This approach to architecture design has become so familiar that the method can now be found described in the popular textbook, Computer Architecture: A Quantitative Approach [7]. More recently, published measurements of this kind have concentrated on the SPEC benchmark programs (e.g., <ref> [17] </ref>). Because the SPEC benchmarks are widely used to compare system performance of workstations, compiler writers and architects study these and similar programs in detail. Modifying compilers or architectures using information from the SPEC suite will lead to good SPEC performance, but may not improve C++ programs. <p> The C programs include all of those present in the SPECint92 benchmark suite. This large program suite precluded investigating additional programs, such as the C and FORTRAN programs in the SPECfp92 benchmark suite; other studies provide such comparisons <ref> [17, 25] </ref>. The function of these programs and the input datasets we used are described in Tables 1 and 2.
Reference: [18] <author> C.B. Hall and K. O'Brien. </author> <title> Performance characteristics of architectural features of the IBM RISC System/6000. </title> <booktitle> In 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 303-309, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Since 1984, measurements of a number of aspects of program behavior have appeared. The SPEC benchmark suite [9, 10], used in recent years to compare the performance of new computer architectures, has been investigated both in terms of instruction set usage <ref> [17, 18] </ref> and cache locality [19, 20].
Reference: [19] <author> Dionisios N. Pnevmatikatos and Mark D. Hill. </author> <title> Cache performance of the integer SPEC benchmarks on a RISC. </title> <journal> Computer Architecture News, </journal> <volume> 18(2) </volume> <pages> 53-69, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Since 1984, measurements of a number of aspects of program behavior have appeared. The SPEC benchmark suite [9, 10], used in recent years to compare the performance of new computer architectures, has been investigated both in terms of instruction set usage [17, 18] and cache locality <ref> [19, 20] </ref>.
Reference: [20] <author> J. D. Gee, M. D. Hill, D. N. Pnevmatikatos, and A. J. Smith. </author> <title> Cache performance of the SPEC92 benchmark suite. </title> <journal> IEEE Micro, </journal> <volume> 13(4) </volume> <pages> 17-27, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Since 1984, measurements of a number of aspects of program behavior have appeared. The SPEC benchmark suite [9, 10], used in recent years to compare the performance of new computer architectures, has been investigated both in terms of instruction set usage [17, 18] and cache locality <ref> [19, 20] </ref>.
Reference: [21] <author> Douglas Clark and Henry M. Levy. </author> <title> Measurements and analysis of instruction set use in the VAX-11/780. </title> <booktitle> In The Ninth Annual Symposium on Computer Architecture, </booktitle> <pages> pages 9-17, </pages> <address> Austin, TX, </address> <month> April </month> <year> 1982. </year>
Reference-contexts: This practice is commonly used by architecture designers to understand how the features of the hardware are being used. Clark and Levi report on instruction set use in the VAX-11/780 <ref> [21] </ref> and conclude that different programs use different parts of the large VAX instruction set. Weicek investigates how six compilers use the VAX-11 instruction set [22].
Reference: [22] <author> Cheryl A. Wiecek. </author> <title> A case study of VAX-11 instruction set usage for compiler execution. </title> <booktitle> In Proceedings of the Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 177-184, </pages> <address> Palo Alto, CA, </address> <year> 1982. </year>
Reference-contexts: Clark and Levi report on instruction set use in the VAX-11/780 [21] and conclude that different programs use different parts of the large VAX instruction set. Weicek investigates how six compilers use the VAX-11 instruction set <ref> [22] </ref>. Similarly, Sweet reports on the static instruction set usage of the Mesa instruction set [23], while McDaniel described the dynamic instruction set usage in Mesa [24] programs. All of these analyses were conducted to give architecture designers insight into how to improve the next-generation architecture.
Reference: [23] <author> Richard E. Sweet and Jr. James G. Sandman. </author> <title> Static analysis of the Mesa instruction set. </title> <booktitle> In Proceedings of the Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 158-166, </pages> <address> Palo Alto, CA, </address> <month> March </month> <year> 1982. </year> <month> 32 </month>
Reference-contexts: Weicek investigates how six compilers use the VAX-11 instruction set [22]. Similarly, Sweet reports on the static instruction set usage of the Mesa instruction set <ref> [23] </ref>, while McDaniel described the dynamic instruction set usage in Mesa [24] programs. All of these analyses were conducted to give architecture designers insight into how to improve the next-generation architecture.
Reference: [24] <author> Gene McDaniel. </author> <title> An analysis of a Mesa instruction set using dynamic instruction frequencies. </title> <booktitle> In Proceedings of the Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 167-176, </pages> <address> Palo Alto, CA, </address> <month> March </month> <year> 1982. </year>
Reference-contexts: Weicek investigates how six compilers use the VAX-11 instruction set [22]. Similarly, Sweet reports on the static instruction set usage of the Mesa instruction set [23], while McDaniel described the dynamic instruction set usage in Mesa <ref> [24] </ref> programs. All of these analyses were conducted to give architecture designers insight into how to improve the next-generation architecture. This approach to architecture design has become so familiar that the method can now be found described in the popular textbook, Computer Architecture: A Quantitative Approach [7].
Reference: [25] <author> Mark D. Hill. </author> <title> Aspects of Cache Memory and Instruction Buffer Performance. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, Berkeley, </institution> <address> CA, </address> <month> November </month> <year> 1987. </year> <note> Also appears as tech report UCB/CSD 87/381. </note>
Reference-contexts: The C programs include all of those present in the SPECint92 benchmark suite. This large program suite precluded investigating additional programs, such as the C and FORTRAN programs in the SPECfp92 benchmark suite; other studies provide such comparisons <ref> [17, 25] </ref>. The function of these programs and the input datasets we used are described in Tables 1 and 2. <p> This comparison demonstrated little difference between the two compilers. To measure cache performance, we have constructed a direct-mapped cache simulator and compared its output to that of Tycho, a widely used, all-associativity cache simulator <ref> [25, 30] </ref> to determine its correctness. Due to the length of this study, we chose to simulate and give results for only direct-mapped caches. 3.3 Explanation of the Data Presented In the next section, results of measurements of the programs are presented in a uniform way.
Reference: [26] <author> Mark A. Linton, John M. Vlissides, and Paul R. Calder. </author> <title> Composing user interfaces with InterViews. </title> <journal> IEEE Computer, </journal> <volume> 22(2) </volume> <pages> 8-22, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Of the other C++ programs, we should note that DOC and IDRAW are both interactive X-windows programs implemented using the InterViews class library <ref> [26] </ref>. InterViews is a large class library that has evolved and changed with the evolution of C++ itself. We note that Mark Linton, the designer of InterViews, has been a very active user of C++ for many years and has been instrumental in shaping the design of the language.
Reference: [27] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 196-205, </pages> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Many of the measurements presented are dynamic, or collected from the execution of the programs. We used the ATOM tool <ref> [27] </ref> to collect program information. ATOM instruments the binary executables of programs to measure the behavior of programs executing on the DEC Alpha architecture. In earlier versions of this work, we used the the QPT trace generator [28, 29], which allowed us to measure programs on the MIPS instruction architecture.
Reference: [28] <author> James R. Larus and Thomas Ball. </author> <title> Rewriting executable files to measure program behavior. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 24(7) </volume> <pages> 197-218, </pages> <year> 1994. </year>
Reference-contexts: We used the ATOM tool [27] to collect program information. ATOM instruments the binary executables of programs to measure the behavior of programs executing on the DEC Alpha architecture. In earlier versions of this work, we used the the QPT trace generator <ref> [28, 29] </ref>, which allowed us to measure programs on the MIPS instruction architecture. With ATOM or QPT, we are able to identify all the function calls, basic block transitions, instruction fetches, data loads and stores, and other operations that occur during program execution.
Reference: [29] <author> James R. Larus. </author> <title> Efficient program tracing. </title> <journal> IEEE Computer, </journal> <volume> 26(5) </volume> <pages> 52-61, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We used the ATOM tool [27] to collect program information. ATOM instruments the binary executables of programs to measure the behavior of programs executing on the DEC Alpha architecture. In earlier versions of this work, we used the the QPT trace generator <ref> [28, 29] </ref>, which allowed us to measure programs on the MIPS instruction architecture. With ATOM or QPT, we are able to identify all the function calls, basic block transitions, instruction fetches, data loads and stores, and other operations that occur during program execution.
Reference: [30] <author> Mark D. Hill and Alan Jay Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(12):1612-1630, </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: This comparison demonstrated little difference between the two compilers. To measure cache performance, we have constructed a direct-mapped cache simulator and compared its output to that of Tycho, a widely used, all-associativity cache simulator <ref> [25, 30] </ref> to determine its correctness. Due to the length of this study, we chose to simulate and give results for only direct-mapped caches. 3.3 Explanation of the Data Presented In the next section, results of measurements of the programs are presented in a uniform way.
Reference: [31] <author> David J. Lilja. </author> <title> Reducing the branch penalty in pipelined processors. </title> <journal> IEEE Computer, </journal> <volume> 21(7) </volume> <pages> 47-55, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Mispredicting the direction of a conditional branch or the destination of an indirect call or return can stall the processor for many cycles on modern architectures. Conditional branch prediction 15 has been studied by a number of researchers; Lilja <ref> [31] </ref>, McFarling and Hennessy [32] and Smith [33] present good surveys. Kaeli and Emma [34] showed that a hardware return stack effectively predicted the destination of procedure returns. In related work, we [13] have shown that indirect functions can be effectively predicted for C++ programs.
Reference: [32] <author> Scott McFarling and John Hennessy. </author> <title> Reducing the cost of branches. </title> <booktitle> In 13th Annual International Symposium of Computer Architecture, </booktitle> <pages> pages 396-403. </pages> <publisher> ACM, </publisher> <year> 1986. </year>
Reference-contexts: Mispredicting the direction of a conditional branch or the destination of an indirect call or return can stall the processor for many cycles on modern architectures. Conditional branch prediction 15 has been studied by a number of researchers; Lilja [31], McFarling and Hennessy <ref> [32] </ref> and Smith [33] present good surveys. Kaeli and Emma [34] showed that a hardware return stack effectively predicted the destination of procedure returns. In related work, we [13] have shown that indirect functions can be effectively predicted for C++ programs.
Reference: [33] <author> J. E. Smith. </author> <title> A study of branch prediction strategies. </title> <booktitle> In 8th Annual International Symposium of Computer Architecture, </booktitle> <pages> pages 135-147. </pages> <publisher> ACM, </publisher> <year> 1981. </year>
Reference-contexts: Mispredicting the direction of a conditional branch or the destination of an indirect call or return can stall the processor for many cycles on modern architectures. Conditional branch prediction 15 has been studied by a number of researchers; Lilja [31], McFarling and Hennessy [32] and Smith <ref> [33] </ref> present good surveys. Kaeli and Emma [34] showed that a hardware return stack effectively predicted the destination of procedure returns. In related work, we [13] have shown that indirect functions can be effectively predicted for C++ programs.
Reference: [34] <author> David R. Kaeli and Philip G. Emma. </author> <title> Branch history table prediction of moving target branches due to subroutine returns. </title> <booktitle> In 18th Annual International Symposium of Computer Architecture, </booktitle> <pages> pages 34-42. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: Conditional branch prediction 15 has been studied by a number of researchers; Lilja [31], McFarling and Hennessy [32] and Smith [33] present good surveys. Kaeli and Emma <ref> [34] </ref> showed that a hardware return stack effectively predicted the destination of procedure returns. In related work, we [13] have shown that indirect functions can be effectively predicted for C++ programs.
Reference: [35] <author> Hemant D. Pande and Barbera G. Ryder. </author> <title> Static type determination for C++. </title> <type> Technical Report LCSR-TR-197, </type> <institution> Rutgers Univ., </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: The results in Table 9 indicate that handling indirect calls, procedure calls, and returns properly will be important for C++ programs. Because indirect calls are quite frequent in C++, we and others are examining methods to reduce this overhead <ref> [13, 35] </ref>. The C++ language was designed to be very efficient, introducing additional costs only when specific features (such as dynamic dispatch) are used. <p> This combination of characteristics poses several challenges and opportunities for compiler writers and architects. First, procedure inlining may be more promising, but more difficult to accomplish, due to the larger number of indirect function calls and the difficultly of interprocedural data flow analysis in C++ <ref> [35] </ref>. Other studies [42, 43, 44] have shown that procedure inlining is problematic; it does not always improve program performance. However, one attribute of procedure inlining is that it removes procedure calling overhead; this overhead is obviously a larger percentage of small procedures. <p> In particular, we found that profile-directed multi-version procedure inlining may perform very well. Here, dynamic type checks and inlined procedures would expand the most frequently executed methods; some of these runtime checks may be eliminated by compile time type analysis <ref> [35] </ref>. Procedure inlining reduces calling overhead and should also reduce the number of load and store operations. 28 In x4.4 we see that C++ programs actually execute slightly more branches than C programs.
Reference: [36] <author> Dirk Grunwald and Benjamin Zorn. </author> <title> CUSTOMALLOC: Efficient synthesized memory allocators. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 23(8) </volume> <pages> 851-869, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Table 16 indicates the number of allocations and deallocations, the mean of the size of objects allocated, and the frequency of dynamic storage allocation in the test programs. In previous work <ref> [36] </ref>, we showed that memory allocation is a time consuming operation that is easy to optimize, resulting in 5-15% performance improvements. <p> In x4.8, we found that C++ programs are more likely than C programs to dynamically allocate many small objects on the heap. This behavior implies that improvements to memory allocation, such as customizing the memory allocator for the application <ref> [36] </ref>, will be more effective for C++ programs. The negligible difference in data cache performance shown in x4.9.2 implies that specific C++ optimizations for data cache locality may not be necessary.
Reference: [37] <author> Benjamin Zorn and Dirk Grunwald. </author> <title> Evaluating models of memory allocation. </title> <journal> ACM Transactions on Modelling of Computer Systems, </journal> <volume> 4(1) </volume> <pages> 107-117, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The table shows the absolute number of allocations (calls to malloc) and deallocations (calls to free) performed by each test program, as well as the average number of instructions executed per object and byte allocated; these metrics are useful when modeling memory allocation algorithms <ref> [37] </ref>. The table also shows the mean object size for each of the test programs. The mean object size is computed by measuring the total bytes allocated by the program 23 and dividing by the number of allocations.
Reference: [38] <author> David Detlefs, Al Dosser, and Benjamin Zorn. </author> <title> Memory allocation costs in large C and C++ programs. </title> <journal> Software Practice and Experience, </journal> <volume> 24(6) </volume> <pages> 527-542, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: For example, the InterViews library (used by DOC and IDRAW) provides automatic reference counting for all InterViews objects. Conservative garbage collection algorithms have also been shown to be effective for C and C++ programs <ref> [38, 39] </ref>. One interesting question that arises is why there is so much more heap allocation in C++ programs. There are undoubtedly many reasons, but we summarize some possibilities here 2 .
Reference: [39] <author> Benjamin Zorn. </author> <title> The measured cost of conservative garbage collection. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 23(7) </volume> <pages> 733-756, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: For example, the InterViews library (used by DOC and IDRAW) provides automatic reference counting for all InterViews objects. Conservative garbage collection algorithms have also been shown to be effective for C and C++ programs <ref> [38, 39] </ref>. One interesting question that arises is why there is so much more heap allocation in C++ programs. There are undoubtedly many reasons, but we summarize some possibilities here 2 .
Reference: [40] <author> Wen mei W. Hwu and Pohua P. Chang. </author> <title> Achieving high instruction cache performance with an optimizing compiler. </title> <booktitle> In 16th Annual Annual International Symposium on Computer Architecture, SIGARCH Newsletter, </booktitle> <pages> pages 242-251. </pages> <publisher> ACM, ACM, </publisher> <year> 1989. </year>
Reference-contexts: If function calls can increase cache misses, the behavior of C functions can lead to fewer conflicts. Table 7 also shows that C++ can possibly benefit more than C programs from basic block reordering and procedure layout algorithms to eliminate cache conflicts. These algorithms <ref> [14, 40, 41] </ref> have been shown to efficiently reduce the number of instruction cache misses and improve branch prediction. 4.9.2 Data Cache The data (D) cache miss rate is a measure of the locality of reference of a program's access to data in the stack, static data segment, and heap. <p> The negligible difference in data cache performance shown in x4.9.2 implies that specific C++ optimizations for data cache locality may not be necessary. By comparison, optimizations for instruction caches <ref> [40, 41, 50] </ref> and possibly virtual memory systems [51, 52, 53, 54] will be more important for C++ programs than for C programs. Our data also indicates that link-time optimizations, such as those proposed by Wall [55] and others will become more important.
Reference: [41] <author> Karl Pettis and Robert C. Hansen. </author> <title> Profile guided code positioning. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 16-27. </pages> <publisher> ACM, ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: If function calls can increase cache misses, the behavior of C functions can lead to fewer conflicts. Table 7 also shows that C++ can possibly benefit more than C programs from basic block reordering and procedure layout algorithms to eliminate cache conflicts. These algorithms <ref> [14, 40, 41] </ref> have been shown to efficiently reduce the number of instruction cache misses and improve branch prediction. 4.9.2 Data Cache The data (D) cache miss rate is a measure of the locality of reference of a program's access to data in the stack, static data segment, and heap. <p> The negligible difference in data cache performance shown in x4.9.2 implies that specific C++ optimizations for data cache locality may not be necessary. By comparison, optimizations for instruction caches <ref> [40, 41, 50] </ref> and possibly virtual memory systems [51, 52, 53, 54] will be more important for C++ programs than for C programs. Our data also indicates that link-time optimizations, such as those proposed by Wall [55] and others will become more important.
Reference: [42] <author> K. D. Cooper, M. W. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: This combination of characteristics poses several challenges and opportunities for compiler writers and architects. First, procedure inlining may be more promising, but more difficult to accomplish, due to the larger number of indirect function calls and the difficultly of interprocedural data flow analysis in C++ [35]. Other studies <ref> [42, 43, 44] </ref> have shown that procedure inlining is problematic; it does not always improve program performance. However, one attribute of procedure inlining is that it removes procedure calling overhead; this overhead is obviously a larger percentage of small procedures.
Reference: [43] <author> K. D. Cooper, M. W. Hall, and L. Torczon. </author> <title> Unexpected side effects of inline substitution: a case study. </title> <journal> Letters on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 22-32, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: This combination of characteristics poses several challenges and opportunities for compiler writers and architects. First, procedure inlining may be more promising, but more difficult to accomplish, due to the larger number of indirect function calls and the difficultly of interprocedural data flow analysis in C++ [35]. Other studies <ref> [42, 43, 44] </ref> have shown that procedure inlining is problematic; it does not always improve program performance. However, one attribute of procedure inlining is that it removes procedure calling overhead; this overhead is obviously a larger percentage of small procedures.
Reference: [44] <author> Jack W. Davidson and Anne M. Holler. </author> <title> Subprogram inlining: A study of its effects on program execution time. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(2) </volume> <pages> 89-102, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This combination of characteristics poses several challenges and opportunities for compiler writers and architects. First, procedure inlining may be more promising, but more difficult to accomplish, due to the larger number of indirect function calls and the difficultly of interprocedural data flow analysis in C++ [35]. Other studies <ref> [42, 43, 44] </ref> have shown that procedure inlining is problematic; it does not always improve program performance. However, one attribute of procedure inlining is that it removes procedure calling overhead; this overhead is obviously a larger percentage of small procedures.
Reference: [45] <author> Craig Chambers and David Ungar. </author> <title> Customization: Optimizing compiler technology for Self, a dynamically-typed object-oriented programming language. </title> <booktitle> In Proceedings of the SIGPLAN'89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 146-160, </pages> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Therefore, inlining C++ procedures may result in less code expansion and there may be more promising candidates for inlining. The most relevant work in this area has been done for the Self language <ref> [45, 46] </ref>. In related work, we found considerable opportunity for similar optimizations for C++ programs [13]. In particular, we found that profile-directed multi-version procedure inlining may perform very well. <p> By comparison, these optimizations 29 will be essential for object-oriented languages. Not surprisingly, highly-optimizing compilers for object-oriented languages typically perform those optimizations when the full program is available <ref> [45, 46] </ref>. Ideally, these optimizations will reduce the propensity for programmers using this emerging technology to micro-optimize their existing applications.
Reference: [46] <author> Urs Holzle, Craig Chambers, and David Unger. </author> <title> Optimizating dynamically-typed object-oriented languages with polymorphic inlined caches. </title> <booktitle> In ECCOP '91 Proc., </booktitle> <pages> pages 21-38. </pages> <publisher> Springer-Verlag, </publisher> <month> July </month> <year> 1991. </year> <month> 33 </month>
Reference-contexts: Therefore, inlining C++ procedures may result in less code expansion and there may be more promising candidates for inlining. The most relevant work in this area has been done for the Self language <ref> [45, 46] </ref>. In related work, we found considerable opportunity for similar optimizations for C++ programs [13]. In particular, we found that profile-directed multi-version procedure inlining may perform very well. <p> By comparison, these optimizations 29 will be essential for object-oriented languages. Not surprisingly, highly-optimizing compilers for object-oriented languages typically perform those optimizations when the full program is available <ref> [45, 46] </ref>. Ideally, these optimizations will reduce the propensity for programmers using this emerging technology to micro-optimize their existing applications.
Reference: [47] <author> B. Ramakrishna Rau and Joseph A. Fisher. </author> <title> Instruction-level parallel processing: History, overview and perspective. </title> <journal> Journal of Supercomputing, </journal> <volume> 7 </volume> <pages> 9-50, </pages> <year> 1993. </year>
Reference-contexts: Most current architectures have support for conditional branch prediction, but not for indirect function calls. Despite the presence of fewer conditional branches, C++ programs may not benefit more than C programs from architectures offering instruction level parallelism <ref> [47] </ref>. These architectures schedule several instructions concurrently. In VLIW architectures, the compiler performs the scheduling [48, 47], while in superscalar architectures, the compiler and architecture cooperate to schedule parallel instructions [49]. <p> Despite the presence of fewer conditional branches, C++ programs may not benefit more than C programs from architectures offering instruction level parallelism [47]. These architectures schedule several instructions concurrently. In VLIW architectures, the compiler performs the scheduling <ref> [48, 47] </ref>, while in superscalar architectures, the compiler and architecture cooperate to schedule parallel instructions [49]. To take advantage of VLIW or superscalar techniques in C++ programs, compilers must be able to schedule instructions, registers and other resources across basic blocks and even procedure calls.
Reference: [48] <author> Joseph A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(7):478-490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Despite the presence of fewer conditional branches, C++ programs may not benefit more than C programs from architectures offering instruction level parallelism [47]. These architectures schedule several instructions concurrently. In VLIW architectures, the compiler performs the scheduling <ref> [48, 47] </ref>, while in superscalar architectures, the compiler and architecture cooperate to schedule parallel instructions [49]. To take advantage of VLIW or superscalar techniques in C++ programs, compilers must be able to schedule instructions, registers and other resources across basic blocks and even procedure calls.
Reference: [49] <author> M. S. Lam M. D. Smith, M. Horowitz. </author> <title> Efficient superscalar performance through boosting. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 248-259, </pages> <address> Boston, Mass., </address> <month> October </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: These architectures schedule several instructions concurrently. In VLIW architectures, the compiler performs the scheduling [48, 47], while in superscalar architectures, the compiler and architecture cooperate to schedule parallel instructions <ref> [49] </ref>. To take advantage of VLIW or superscalar techniques in C++ programs, compilers must be able to schedule instructions, registers and other resources across basic blocks and even procedure calls.
Reference: [50] <author> Scott McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> In Proceedings of the 3rd Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191. </pages> <publisher> ACM, </publisher> <year> 1988. </year>
Reference-contexts: The negligible difference in data cache performance shown in x4.9.2 implies that specific C++ optimizations for data cache locality may not be necessary. By comparison, optimizations for instruction caches <ref> [40, 41, 50] </ref> and possibly virtual memory systems [51, 52, 53, 54] will be more important for C++ programs than for C programs. Our data also indicates that link-time optimizations, such as those proposed by Wall [55] and others will become more important.
Reference: [51] <author> W. A. Abu-Sufah, D. J. Kuck, and D. H. Lawrie. </author> <title> On the performance enhancement of paging systems through program analysis and transformation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(5):341-356, </volume> <month> May </month> <year> 1981. </year>
Reference-contexts: The negligible difference in data cache performance shown in x4.9.2 implies that specific C++ optimizations for data cache locality may not be necessary. By comparison, optimizations for instruction caches [40, 41, 50] and possibly virtual memory systems <ref> [51, 52, 53, 54] </ref> will be more important for C++ programs than for C programs. Our data also indicates that link-time optimizations, such as those proposed by Wall [55] and others will become more important.
Reference: [52] <author> Domenico Ferrari. </author> <title> Improving locality by critical working sets. </title> <journal> Communications of the ACM, </journal> <volume> 17(11) </volume> <pages> 614-620, </pages> <year> 1974. </year>
Reference-contexts: The negligible difference in data cache performance shown in x4.9.2 implies that specific C++ optimizations for data cache locality may not be necessary. By comparison, optimizations for instruction caches [40, 41, 50] and possibly virtual memory systems <ref> [51, 52, 53, 54] </ref> will be more important for C++ programs than for C programs. Our data also indicates that link-time optimizations, such as those proposed by Wall [55] and others will become more important.
Reference: [53] <author> S. J. </author> <title> Hartley. Compile-time program restructing in multiprogrammed virtual memory systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(11) </volume> <pages> 1640-1644, </pages> <year> 1988. </year>
Reference-contexts: The negligible difference in data cache performance shown in x4.9.2 implies that specific C++ optimizations for data cache locality may not be necessary. By comparison, optimizations for instruction caches [40, 41, 50] and possibly virtual memory systems <ref> [51, 52, 53, 54] </ref> will be more important for C++ programs than for C programs. Our data also indicates that link-time optimizations, such as those proposed by Wall [55] and others will become more important.
Reference: [54] <author> D. Hatfield and J. Gerald. </author> <title> Program restructing for virtual memory. </title> <journal> IBM Systems Journal, </journal> <volume> 10(3) </volume> <pages> 168-192, </pages> <year> 1971. </year>
Reference-contexts: The negligible difference in data cache performance shown in x4.9.2 implies that specific C++ optimizations for data cache locality may not be necessary. By comparison, optimizations for instruction caches [40, 41, 50] and possibly virtual memory systems <ref> [51, 52, 53, 54] </ref> will be more important for C++ programs than for C programs. Our data also indicates that link-time optimizations, such as those proposed by Wall [55] and others will become more important.
Reference: [55] <author> David W. Wall. </author> <title> Register windows vs. register allocation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 23, </volume> <pages> pages 67-78, </pages> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: By comparison, optimizations for instruction caches [40, 41, 50] and possibly virtual memory systems [51, 52, 53, 54] will be more important for C++ programs than for C programs. Our data also indicates that link-time optimizations, such as those proposed by Wall <ref> [55] </ref> and others will become more important. Object-oriented languages, such as C++, allow programmers to extend the class hierarchy without affecting the functionality of previously compiled procedures. This means that a programmer could use class `X' and compile several modules using the interface of class `X'.
Reference: [56] <author> Brian W. Kernighan and Dennis M. Ritchie. </author> <title> The C Programming Language. Software Series. </title> <publisher> Prentice-Hall. Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1978. </year> <month> 34 </month>
Reference-contexts: For example, prior to the development of efficient register scheduling algorithms, a number of computer languages, such as C, provided hints to the compiler to indicate what variables should be stored in registers <ref> [56] </ref>; often, programmer intuition is incorrect.
References-found: 56

