URL: http://www.cs.purdue.edu/homes/sb/Projects/Musketeers/doc/CSD-TR-96-030.ps
Refering-URL: http://www.cs.purdue.edu/homes/sb/reports/LIST.html
Root-URL: http://www.cs.purdue.edu
Title: Dynamic Scheduling of Process Groups  
Author: Kuei Yu Wang, Dan C. Marinescu, and Octavian F. Carbunar 
Note: Work supported in part by NSF grants BIR-9301210 and MCR-9527131, by the Scalable I/O Initiative, by a grant from Intel Corporation and by CNPq Brazil  
Date: May 29, 1996  
Address: West Lafayette, IN 47907  
Affiliation: Computer Sciences Department Purdue University  
Abstract: In this paper we introduce the concept of temporal locality of communication for process groups. Empirical evidence suggests that, once a member of a process group starts to communicate with other processes in the group, it will continue to do so, while an independent process will maintain its state of isolation for some time. Other instances of inertial behavior of programs are known. Temporal and spatial locality of reference are example of inertial behavior of programs, exploited by hierarchical storage systems; once a block of information (program or data) is brought into faster storage, it is very likely that it will be referenced again within a short time frame. The temporal locality of communication can be used to schedule concurrently multiple process groups. When process groups exhibit temporal locality of communication, this information can be used to hide the latency of paging and I/O operations, to perform dynamic scheduling to reduce processor fragmentation, and to identify optimal instances of time for checkpointing of process groups. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. J. Atallah, C. Lock, D. C. Marinescu, H. J. Siegel, and T. L. Casavant. </author> <title> Models and algorithms for co-scheduling compute-intensive tasks on a network of workstations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 319-327, </pages> <year> 1992. </year>
Reference-contexts: Gang scheduling, coupled with busy waiting, is used extensively by existing MIMD systems, e.g., Paragon, CM5, SP2, Alliant FX/8 for parallel applications which exhibit fine-grain interactions. Co-scheduling is an alternative to gang scheduling <ref> [1] </ref>, [10]. In case of co-scheduling, the dynamics of synchronization requirements of the application is taken into account; only those members of the process group which need to communicate with one another are scheduled concurrently.
Reference: [2] <author> D. C. Burger, R. S. Hyder, B. P. Miller, and D. A. Wood. </author> <title> Paging tradeoffs in distributed-shared-memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: When a process blocks due to a page fault, an I/O operation, or a communication event, it does not release the control of the processor. When combined with support for demand paging, this strategy leads to wasted CPU cycles and longer execution time <ref> [2] </ref>, [13]. The paper is organized as follows. Section 2 discusses the concept of temporal locality 2 of communication and describes a simple mechanism to hide latency of page faults and I/O operations for temporarily indepentent processes.
Reference: [3] <author> S.-H. Chiang, R. K. Mansharamani, and M. K. Vernon. </author> <title> Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies. </title> <journal> Performance Evaluation Review, </journal> <volume> 22(1) </volume> <pages> 33-44, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Processor scheduling algorithms for general purpose multiprogrammed multiprocessors are not yet well understood [9]. Two classes of processor scheduling algorithms for multi-programmed parallel systems are the static and the dynamic scheduling algorithms [8], <ref> [3] </ref>, [11], [12]. Static scheduling algorithms are non-preemptive scheduling algorithms in which each application runs to completion without interruption on the set of processors initially allocated for it. All others belong to the class of dynamic scheduling.
Reference: [4] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to algorithms. </title> <publisher> McGraw-Hill Book Company, </publisher> <year> 1991. </year>
Reference-contexts: The algorithm for finding communicating sub-groups (or clusters) is similar to the algorithm in <ref> [4] </ref> for determining the connected components of an undirected graph using the disjoint-set data structure. Figure 3 adapted from [4], illustrates the pseudo code for finding connected components (clusters). 8 for each A i 2 A f do Make set (A i ) // create a set for each A i <p> The algorithm for finding communicating sub-groups (or clusters) is similar to the algorithm in <ref> [4] </ref> for determining the connected components of an undirected graph using the disjoint-set data structure. Figure 3 adapted from [4], illustrates the pseudo code for finding connected components (clusters). 8 for each A i 2 A f do Make set (A i ) // create a set for each A i g for each M [i, j] = 1 f do if Find set [i] 6= Find set [j] then <p> set (A i ) // create a set for each A i g for each M [i, j] = 1 f do if Find set [i] 6= Find set [j] then Union (i,j) // unite two disjoint sets g Using the union by rank and path compression heuristics presented in <ref> [4] </ref>, the running time for finding connected components is almost linear in the total number of operations. (Make set, Find set and Union operations.) A snapshot of the computation is illustrated in send communication relationship among components.
Reference: [5] <author> M. A. Cornea-Hasegan, D. C. Marinescu, and Z. Zhang. </author> <title> Data management for a class of iterative computations on distributed memory MIMD systems. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(3) </volume> <pages> 205-229, </pages> <year> 1994. </year>
Reference-contexts: Reference <ref> [5] </ref> describes different data management strategies for implementing a shared virtual memory. The entire data set is partitioned into Data Allocation Units, DAUs. The working set of DAU j consists of all DAUs needed to carry out the computations associated with DAU j .
Reference: [6] <author> M. A. Cornea-Hasegan, Z. Zhang, R. E. Lynch, D. C. Marinescu, A. Hadfield, J. K. Muckelbauer, S. Munshi, L. Tong, and M. G. Rossmann. </author> <title> Phase refinement and extension by means of non-crystallographic symmetry averaging using parallel computers. </title> <journal> Acta Crystallographica, </journal> <volume> D51:749-759, </volume> <year> 1995. </year>
Reference-contexts: G 2 ; : : : ; G 4 g after processing C A k of each PE A k . 4.2 The applications We discuss below two programs in the Molecular Replacement suite we have examined, envelope and fftsynth, and results gathered during their execution on a Paragon system <ref> [6] </ref>. The envelope program computes the molecular envelope of a virus. It needs as input a 3-D lattice with up to 10 9 grid points and produces a lattice of equal size as output.
Reference: [7] <author> P. J. Denning. </author> <title> The working set model for program behavior. </title> <journal> Communications of the ACM, </journal> <volume> 11(5) </volume> <pages> 323-333, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: G i = cluster i. g i = the number of elements of cluster i. Following the definition of working set model introduced by Denning <ref> [7] </ref>, we define the communication working set of process A k at time t as: C A k (t; ) = collection of processes that communicate with process A k during the time interval (t ; t) = working set parameter For a given time window [t ; t], C A
Reference: [8] <author> A. Ieumwananonthachai, A. N. Aizawa, S. R. Schwartz, B. W. Wah, and J. C. Yan. </author> <title> Intelligent mapping of communicating processes in distributed computing systems. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 512-521, </pages> <month> November </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: Processor scheduling algorithms for general purpose multiprogrammed multiprocessors are not yet well understood [9]. Two classes of processor scheduling algorithms for multi-programmed parallel systems are the static and the dynamic scheduling algorithms <ref> [8] </ref>, [3], [11], [12]. Static scheduling algorithms are non-preemptive scheduling algorithms in which each application runs to completion without interruption on the set of processors initially allocated for it. All others belong to the class of dynamic scheduling.
Reference: [9] <author> S. Majumdar, D. L. Eager, and R. B. Bunt. </author> <title> Characterization of programs for scheduling in multiprogrammed parallel systems. </title> <booktitle> Performance Evaluation 13(2) </booktitle> <pages> 109-130, </pages> <year> 1991. </year>
Reference-contexts: Thus, although multiprogramming allows better service to be provided to the users, it also complicates the processor allocation issues in multiprocessor systems. Processor scheduling algorithms for general purpose multiprogrammed multiprocessors are not yet well understood <ref> [9] </ref>. Two classes of processor scheduling algorithms for multi-programmed parallel systems are the static and the dynamic scheduling algorithms [8], [3], [11], [12]. Static scheduling algorithms are non-preemptive scheduling algorithms in which each application runs to completion without interruption on the set of processors initially allocated for it.
Reference: [10] <author> J. K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of the 3rd Intl. Conf. Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: Gang scheduling, coupled with busy waiting, is used extensively by existing MIMD systems, e.g., Paragon, CM5, SP2, Alliant FX/8 for parallel applications which exhibit fine-grain interactions. Co-scheduling is an alternative to gang scheduling [1], <ref> [10] </ref>. In case of co-scheduling, the dynamics of synchronization requirements of the application is taken into account; only those members of the process group which need to communicate with one another are scheduled concurrently.
Reference: [11] <author> K. C. Sevcik. </author> <title> Characterization of parallelism in applications and their use in scheduling. </title> <journal> Performance Evaluation Review, </journal> <volume> 17 </volume> <pages> 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Processor scheduling algorithms for general purpose multiprogrammed multiprocessors are not yet well understood [9]. Two classes of processor scheduling algorithms for multi-programmed parallel systems are the static and the dynamic scheduling algorithms [8], [3], <ref> [11] </ref>, [12]. Static scheduling algorithms are non-preemptive scheduling algorithms in which each application runs to completion without interruption on the set of processors initially allocated for it. All others belong to the class of dynamic scheduling.
Reference: [12] <author> K. C. Sevcik. </author> <title> Application scheduling and processor allocation in multipro-grammed parallel processing systems. Performance Evaluation, </title> <address> 19(2-3):107-140, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Processor scheduling algorithms for general purpose multiprogrammed multiprocessors are not yet well understood [9]. Two classes of processor scheduling algorithms for multi-programmed parallel systems are the static and the dynamic scheduling algorithms [8], [3], [11], <ref> [12] </ref>. Static scheduling algorithms are non-preemptive scheduling algorithms in which each application runs to completion without interruption on the set of processors initially allocated for it. All others belong to the class of dynamic scheduling. The static scheduling algorithms are simpler to implement and have lower overhead than dynamic algorithms.
Reference: [13] <author> K. Y. Wang and D. C. Marinescu. </author> <title> Correlation of the paging activity of individual node programs in the SPMD execution mode. </title> <booktitle> In Proceedings of the 28th Hawaii International Conference on System Sciences, HICSS'28, </booktitle> <pages> pages 61-71. </pages> <publisher> IEEE Press, </publisher> <month> January </month> <year> 1995. </year> <month> 21 </month>
Reference-contexts: When a process blocks due to a page fault, an I/O operation, or a communication event, it does not release the control of the processor. When combined with support for demand paging, this strategy leads to wasted CPU cycles and longer execution time [2], <ref> [13] </ref>. The paper is organized as follows. Section 2 discusses the concept of temporal locality 2 of communication and describes a simple mechanism to hide latency of page faults and I/O operations for temporarily indepentent processes.
References-found: 13

