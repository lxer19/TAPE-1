URL: http://web.mit.edu/dimitrib/www/td.ps
Refering-URL: http://web.mit.edu/dimitrib/www/publ.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: June 1994 T o app ear in Neural Computation A Coun terexample to T emp
Author: by Dimitri P Bertsek as 
Abstract: Sutton's TD( ) metho d aims to provide a represen tation of the cost function in an absorbing Mark ov chain with transition costs. A simple example is given where the represen tation obtained dep ends on . For = 1 the represen tation is optimal with resp ect to a least squares error criterion, but as decreases towards 0 the represen tation becomes progressiv ely worse and, in some cases, very poor. The example suggests a need to understand better the circumstances under which TD(0) and Q-learning obtain satisfactory neural net work-based compact represen tations of the cost function. A variation of TD(0) is also prop osed, which performs b etter on the example. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> REFERENCES </institution>
Reference: [BBS94] <author> Barto, A. G., Bradtk e, S. J., and Singh, S. P., </author> <title> "Learning to Act using Real-Time Dynamic Programming", </title> <journal> J. Artiflcial Intelligence, </journal> <note> 1994 (to app ear). </note>
Reference: [Bai93] <author> Baird, L. C., </author> <title> "Adv antage Up dating," </title> <type> Tech. Rep. </type> <institution> WL-TR-93-1146, Wrigh t Lab., Wrigh t-P atterson Air Force Base, OH, </institution> <year> 1993. </year>
Reference-contexts: The metho d (11) is apparen tly new, although an iteration similar to (11) and its sampled version given below [cf. Eq. (14)] have been indep enden tly developed by Baird and are brie y describ ed in Baird <ref> [Bai93] </ref>, and Harmon, Baird, and Klopf [HBK94] (this was pointed out by one of the reviewers).
Reference: [BeT89] <author> Bertsek as, D. P., and Tsitsiklis, J. </author> <title> N.,"P arallel and Distributed Computation: Numerical Metho ds," </title> <editor> Pren tice-Hall, Englew ood Clifis, N. J., </editor> <year> 1989. </year>
Reference-contexts: This metho d has satisfactory convergence beha vior, and is supp orted by classical results on sto chastic appro ximation and sto chastic gradien t metho ds (see e.g. P oljak and Tsypkin [PoT73], Kushner and Clark [KuC78], P oljak [Pol87], Bertsek as and Tsitsiklis <ref> [BeT89] </ref>), and by more recen t analyses on deterministic incremen tal gradien t metho ds by Luo [Luo91], Luo and Tseng [LuT93], and Mangasarian and Solodov [MaS93]. <p> The example also relates to one of Watkins' Q-learning metho ds [Wat89]. These metho ds have the advantage that they apply to discoun ted Mark ovian decision problems and sto chastic shortest path problems (as deflned in Bertsek as and Tsitsiklis <ref> [BeT89] </ref>, where there are multiple actions available at each state and the ob jectiv e is not just to obtain the optimal cost, but also to flnd an optimal action at each state.
Reference: [Day92] <author> Dayan, P., </author> <title> "The Con vergence of TD( ) for General ", Machine Learning, </title> <journal> Vol. </journal> <volume> 8, </volume> <pages> pp. 341-362, </pages> <year> 1992. </year>
Reference-contexts: 1, the convergence beha vior of TD ( ) is unclear, unless w contains enough parameters to mak e possible an exact represen tation of J (i) by ~ J (i; w) for all states i (a lookup table represen tation), as shown in various forms by Sutton [Sut88], Dayan <ref> [Day92] </ref>, Tsitsiklis [Tsi93], and Jaakk ola, Jordan, and Singh [JJS93]. Actually , Sutton's and Dayan's convergence results apply to the slighly more general case of linear represen tations, under a restrictiv e linear indep endence condition on the set of observ ation vectors. <p> Th us, in general, the limit obtained by TD ( ) dep ends on , as has also been shown by Dayan <ref> [Day92] </ref>. Nonetheless, there are accoun ts of good practical performance of TD ( ), even with substan tially less than 1. For example, Tesauro [Tes92] rep orts that his bac kgammon program performs better when trained with small than with high values of . 2. <p> a single sample (i k +1 = i 0 k +1 ) is used, that is, w := w + d (i k ; i k +1 ) r ~ J (i k ; w) r ~ J (i k +1 ; w) ; (15) has been discussed by Dayan <ref> [Day92] </ref>.
Reference: [JJS93] <author> Jaakk ola, T., Jordan, M. I., and Singh, S. P., </author> <title> "On the Convergence of Sto chastic Iterativ e Dynamic Programming Algorithms", </title> <institution> MIT Computational Cognitiv e Science Technical Rep ort 9307, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: is unclear, unless w contains enough parameters to mak e possible an exact represen tation of J (i) by ~ J (i; w) for all states i (a lookup table represen tation), as shown in various forms by Sutton [Sut88], Dayan [Day92], Tsitsiklis [Tsi93], and Jaakk ola, Jordan, and Singh <ref> [JJS93] </ref>. Actually , Sutton's and Dayan's convergence results apply to the slighly more general case of linear represen tations, under a restrictiv e linear indep endence condition on the set of observ ation vectors.
Reference: [HBK94] <author> Harmon, M. E., Baird, L. C., and Klopf, A. H., </author> <title> "Adv antage Up dating Applied to a Difieren tial Game," </title> <booktitle> Submitted to NIPS Conf., </booktitle> <address> Denver, Colo., </address> <year> 1994. </year>
Reference-contexts: The metho d (11) is apparen tly new, although an iteration similar to (11) and its sampled version given below [cf. Eq. (14)] have been indep enden tly developed by Baird and are brie y describ ed in Baird [Bai93], and Harmon, Baird, and Klopf <ref> [HBK94] </ref> (this was pointed out by one of the reviewers).
Reference: [KuC78] <author> Kushner, H. J., and Clark, D. S., </author> <title> "Sto chastic Appro ximation Metho ds for Constrained and Unconstrained Systems," </title> <publisher> Springer-V erlag, </publisher> <address> NY, </address> <year> 1978. </year>
Reference-contexts: This metho d has satisfactory convergence beha vior, and is supp orted by classical results on sto chastic appro ximation and sto chastic gradien t metho ds (see e.g. P oljak and Tsypkin [PoT73], Kushner and Clark <ref> [KuC78] </ref>, P oljak [Pol87], Bertsek as and Tsitsiklis [BeT89]), and by more recen t analyses on deterministic incremen tal gradien t metho ds by Luo [Luo91], Luo and Tseng [LuT93], and Mangasarian and Solodov [MaS93].
Reference: [Luo91] <author> Luo, Z. Q., </author> <title> "On the Convergence of the LMS Algorithm with Adaptiv e Learning Rate for Linear Feedforward Net works," </title> <journal> Neural Computation, </journal> <volume> Vol. 3, </volume> <year> 1991, </year> <pages> pp. 226-245. </pages>
Reference-contexts: P oljak and Tsypkin [PoT73], Kushner and Clark [KuC78], P oljak [Pol87], Bertsek as and Tsitsiklis [BeT89]), and by more recen t analyses on deterministic incremen tal gradien t metho ds by Luo <ref> [Luo91] </ref>, Luo and Tseng [LuT93], and Mangasarian and Solodov [MaS93].
Reference: [LuT93] <author> Luo, Z. Q., and Tseng, P., </author> <title> "Analysis of an Appro ximate Gradien t Pro jection Metho d with Applications to the Back Propagation Algorithm," </title> <institution> Dept. Elec. and Computer Eng., McMaster Univ., Hamilton, Ontario and Dept. of Math., Univ. </institution> <address> Washington, Seattle, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: P oljak and Tsypkin [PoT73], Kushner and Clark [KuC78], P oljak [Pol87], Bertsek as and Tsitsiklis [BeT89]), and by more recen t analyses on deterministic incremen tal gradien t metho ds by Luo [Luo91], Luo and Tseng <ref> [LuT93] </ref>, and Mangasarian and Solodov [MaS93].
Reference: [MaS93] <author> Mangasarian, O. L., and Solodov, M. V., </author> <title> "Serial and P arallel Backpropagation Convergence Via Nonmonotone P erturb ed Minimization," </title> <institution> Computer Science Dept., Computer Sciences Technical Rep ort No. 1149, Univ. of Wisconsin-Madison, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: P oljak and Tsypkin [PoT73], Kushner and Clark [KuC78], P oljak [Pol87], Bertsek as and Tsitsiklis [BeT89]), and by more recen t analyses on deterministic incremen tal gradien t metho ds by Luo [Luo91], Luo and Tseng [LuT93], and Mangasarian and Solodov <ref> [MaS93] </ref>.
Reference: [PoT73] <author> P oljak, B. T, and Tsypkin, Y. Z., </author> <title> "Pseudogradien t Adaptation and Training Algorithms," </title> <journal> Automation and Remote Control, </journal> <year> 1973, </year> <pages> pp. 45-68. </pages>
Reference-contexts: This metho d has satisfactory convergence beha vior, and is supp orted by classical results on sto chastic appro ximation and sto chastic gradien t metho ds (see e.g. P oljak and Tsypkin <ref> [PoT73] </ref>, Kushner and Clark [KuC78], P oljak [Pol87], Bertsek as and Tsitsiklis [BeT89]), and by more recen t analyses on deterministic incremen tal gradien t metho ds by Luo [Luo91], Luo and Tseng [LuT93], and Mangasarian and Solodov [MaS93].
Reference: [Pol87] <author> P oljak, B. </author> <title> T, "In tro duction to Optimization," Optimization Software Inc., </title> <address> N.Y., </address> <year> 1987. </year> <note> 10 References </note>
Reference-contexts: This metho d has satisfactory convergence beha vior, and is supp orted by classical results on sto chastic appro ximation and sto chastic gradien t metho ds (see e.g. P oljak and Tsypkin [PoT73], Kushner and Clark [KuC78], P oljak <ref> [Pol87] </ref>, Bertsek as and Tsitsiklis [BeT89]), and by more recen t analyses on deterministic incremen tal gradien t metho ds by Luo [Luo91], Luo and Tseng [LuT93], and Mangasarian and Solodov [MaS93].
Reference: [Sut88] <author> Sutton, R. S., </author> <title> "Learning to Predict by the Metho ds of Temp oral Difierences", </title> <journal> Machine Learning, </journal> <volume> Vol. 3, </volume> <pages> pp. 9-44, </pages> <year> 1988. </year>
Reference-contexts: For example ~ J (i; w) may be the output of a neural net work when the input is i and the vector of weights is w. Sutton's TD ( ) metho d <ref> [Sut88] </ref> is a gradien t-lik e algorithm for obtaining a suitable vector w after observing a large num ber of simulated tra jectories of the Mark ov chain. <p> N X g (i m ; i m +1 ) ~ J (i k ; w) r ~ J (i k ; w); so it is a gradien t iteration for minimizing the sum of squares N X m = k ! 2 It follows, as originally discussed by Sutton <ref> [Sut88] </ref>, that TD (1) can be viewed as a form of incremen tal gradien t or backpropagation metho d for minimizing over w the sum of the squared difierences of the sample costs of the states i visited by the simulation and the estimates ~ J (i; w). <p> for &lt; 1, the convergence beha vior of TD ( ) is unclear, unless w contains enough parameters to mak e possible an exact represen tation of J (i) by ~ J (i; w) for all states i (a lookup table represen tation), as shown in various forms by Sutton <ref> [Sut88] </ref>, Dayan [Day92], Tsitsiklis [Tsi93], and Jaakk ola, Jordan, and Singh [JJS93]. Actually , Sutton's and Dayan's convergence results apply to the slighly more general case of linear represen tations, under a restrictiv e linear indep endence condition on the set of observ ation vectors.
Reference: [Tes92] <author> Tesauro, G., </author> <title> "Practical Issues in Temp oral Difierence Learning", </title> <journal> Machine Learning, </journal> <volume> Vol. 8, </volume> <pages> pp. 257-277, </pages> <year> 1992. </year>
Reference-contexts: The metho d has attracted considerable atten tion, and has been used successfully in a more general setting by Tesauro <ref> [Tes92] </ref> for the training of a neural net work to play bac kgammon. See Barto, Bradtk e, and Singh [BBS93] for a nice and comprehensiv e surv ey of related issues. <p> Th us, in general, the limit obtained by TD ( ) dep ends on , as has also been shown by Dayan [Day92]. Nonetheless, there are accoun ts of good practical performance of TD ( ), even with substan tially less than 1. For example, Tesauro <ref> [Tes92] </ref> rep orts that his bac kgammon program performs better when trained with small than with high values of . 2.
Reference: [Tsi93] <editor> Tsitsiklis, J. N., "Async hronous Sto chastic Appro ximation and Q-Learning", LIDS Rep ort P-2172, </editor> <publisher> MIT, </publisher> <month> February </month> <year> 1993. </year>
Reference-contexts: convergence beha vior of TD ( ) is unclear, unless w contains enough parameters to mak e possible an exact represen tation of J (i) by ~ J (i; w) for all states i (a lookup table represen tation), as shown in various forms by Sutton [Sut88], Dayan [Day92], Tsitsiklis <ref> [Tsi93] </ref>, and Jaakk ola, Jordan, and Singh [JJS93]. Actually , Sutton's and Dayan's convergence results apply to the slighly more general case of linear represen tations, under a restrictiv e linear indep endence condition on the set of observ ation vectors. <p> Strong convergence results have been recently shown by Tsitsiklis <ref> [Tsi93] </ref> for the most commonly used Q-learning metho d in the case of a lookup table represen tation.
Reference: [Wat89] <author> Watkins, C. J. C. H., </author> <title> "Learning from Delayed Rewards," </title> <type> Ph.D. Thesis, </type> <institution> Cam bridge Univ., </institution> <address> England, </address> <year> 1989. </year> <month> 11 </month>
Reference-contexts: The example also relates to one of Watkins' Q-learning metho ds <ref> [Wat89] </ref>.
References-found: 17

