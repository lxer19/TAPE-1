URL: ftp://borneo.gmd.de/pub/as/ga/gmd_as_ga-94_07.ps
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: zhang@gmd.de  
Title: Effects of Occam's Razor in Evolving Sigma-Pi Neural Nets  
Author: Byoung-Tak Zhang 
Address: D-53757 Sankt Augustin, Germany,  
Affiliation: German National Research Center for Computer Science (GMD)  
Abstract: Several evolutionary algorithms make use of hierarchical representations of variable size rather than linear strings of fixed length. Variable complexity of the structures provides an additional representational power which may widen the application domain of evolutionary algorithms. The price for this is, however, that the search space is open-ended and solutions may grow to arbitrarily large size. In this paper we study the effects of structural complexity of the solutions on their generalization performance by analyzing the fitness landscape of sigma-pi neural networks. The analysis suggests that smaller networks achieve, on average, better generalization accuracy than larger ones, thus confirming the usefulness of Occam's razor. A simple method for implementing the Occam's razor principle is described and shown to be effective in improv ing the generalization accuracy without limiting their learning capacity.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Y. S. Abu-Mostafa: </author> <title> The Vapnik-Chervonenkis dimension: Information versus complexity in learning. </title> <booktitle> Neural Computation 1, </booktitle> <month> 312-317 </month> <year> (1989) </year>
Reference-contexts: Likewise, the generalization performance E (B N jw; A i ) of the network was measured on the test set B N of N clean examples. Each performance measure was then normalized into the interval <ref> [0; 1] </ref> by 1 E (D N jw; A i ); (13) 1 E (B N jw; A i ): (14) For each network we also measured its complexity in terms of the number of binary weights: W i = W (wjA i ) = j;k jk ; (15) Fig. 2. <p> Effect of number of units on generalization from one problem to another as well as the task at hand. However, the overall result suggests preference of small networks to larger ones if no information is available about the configuration space, confirming the principles of Occam's razor <ref> [1, 21] </ref> and minimum description length [16]. 4 Effects of Occam's Razor: Simulation Results The foregoing analysis suggests the necessity of some mechanism to constrain the network size.
Reference: 2. <author> T. Back and H.-P. Schwefel: </author> <title> An overview of evolutionary algorithms for parameter optimization. </title> <booktitle> Evolutionary Computation 1, </booktitle> <month> 1-23 </month> <year> (1993) </year>
Reference-contexts: 1 Introduction Recently, there has been increasing interest in evolutionary algorithms that use structured representations of variable length rather than linear strings of fixed size <ref> [2, 5, 12] </ref>. Parse trees or Lisp S-expressions are usually used to represent possible solutions. Since the user-defined expressions can be recursively combined, the method is particularly suited to problems in which some underlying regularity or structure must be automatically discovered.
Reference: 3. <author> R. Durbin, D. E. Rumelhart: </author> <title> Product units: A computationally powerful and biologically plausible extension to backpropagation networks. </title> <booktitle> Neural Computation 1, </booktitle> <month> 133-142 </month> <year> (1989) </year>
Reference-contexts: The representational power of this architecture can be extended by allowing pi units as well as sigma units <ref> [3] </ref>. While a sigma-unit calculates a sum of weighted inputs, a pi-unit builds a product of weighted inputs: u i = j2R (i) Here v ij is the connection weight from unit j to unit i and R (i) denotes the receptive field of unit i.
Reference: 4. <author> C. L. Giles, T. Maxwell: </author> <title> Learning, invariance, and generalization in high-order neural networks. </title> <journal> Applied Optics 26, </journal> <month> 4972-4978 </month> <year> (1987) </year>
Reference: 5. <author> D. E. Goldberg: </author> <title> Genetic Algorithms in Search, Optimization & Machine Learning. </title> <publisher> Addison Wesley, </publisher> <year> 1989 </year>
Reference-contexts: 1 Introduction Recently, there has been increasing interest in evolutionary algorithms that use structured representations of variable length rather than linear strings of fixed size <ref> [2, 5, 12] </ref>. Parse trees or Lisp S-expressions are usually used to represent possible solutions. Since the user-defined expressions can be recursively combined, the method is particularly suited to problems in which some underlying regularity or structure must be automatically discovered.
Reference: 6. <author> F. Gruau: </author> <title> Genetic synthesis of Boolean neural networks with a cell rewriting developmental process. </title> <type> Tech. Rep., </type> <institution> Laboratoire de l'Informatique du Parallelisme, </institution> <year> 1992 </year>
Reference: 7. <author> S. A. Harp, T. Samad, A. Guha: </author> <title> Towards the genetic synthesis of neural networks. </title> <editor> In: D. Schaffer (ed.): </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms (ICGA-89). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1985, </year> <pages> pp. 360-369 </pages>
Reference: 8. <author> H. Iba, T. Kurita, H. de Garis, T. Sato: </author> <title> System identification using structured genetic algorithm. </title> <editor> In: S. Forrest (ed.): </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms (ICGA-93). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993, </year> <pages> pp. 279-286 </pages>
Reference: 9. <author> H. Kargupta, R. E. Smith: </author> <title> System identification with evolving polynomial networks. </title> <editor> In: R. Belew, L. Booker (eds.): </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms (ICGA-91). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991, </year> <pages> pp. 370-376 </pages>
Reference-contexts: can construct a higher-order network of sigma-pi units: y i = f i (u i ) = f i X w ik T (k) = f i @ k 0 j k Y v kj x j A A (7) Evolutionary algorithms can be used to evolve problem-specific sigma-pi networks <ref> [9, 22] </ref>. We encode the network as a set of m trees, where m is the number of output units. Figure 1 shows an example of the genotype and its phenotype of a sigma-pi network with one output unit. Fig. 1.
Reference: 10. <author> K. E. Kinnear Jr.: </author> <title> Generality and difficulty in genetic programming: Evolving a sort. </title> <editor> In: S. Forrest (ed.): </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms (ICGA-93). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993, </year> <pages> pp. 287-294 </pages>
Reference-contexts: The simplicity and generality of these algorithms have found many interesting applications across a wide spectrum, including problems in artificial intelligence and artificial life [12]. However, the price for the additional flexibility in representation is that solutions may grow arbitrarily large. For example, Kinnear <ref> [10] </ref> observed in his sorting problem a strong tendency for the individuals in the population to grow without bound. He found almost all of the solutions were so large as to defy any human understanding of them, even after simplifying them in obvious ways. <p> Kinnear <ref> [10] </ref> also questions the existence of general connection between high evolutionary pressures against increasing size and algorithms of greater generalization ability. However, these studies did not provide a concrete relationship between the complexity of solutions and their generalization accuracy.
Reference: 11. <author> H. Kitano: </author> <title> Designing neural networks using genetic algorithms with graph generation system. </title> <journal> Complex Systems 4, </journal> <month> 461-476 </month> <year> (1990) </year>
Reference: 12. <author> J. R. Koza: </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <year> 1992 </year>
Reference-contexts: 1 Introduction Recently, there has been increasing interest in evolutionary algorithms that use structured representations of variable length rather than linear strings of fixed size <ref> [2, 5, 12] </ref>. Parse trees or Lisp S-expressions are usually used to represent possible solutions. Since the user-defined expressions can be recursively combined, the method is particularly suited to problems in which some underlying regularity or structure must be automatically discovered. <p> The simplicity and generality of these algorithms have found many interesting applications across a wide spectrum, including problems in artificial intelligence and artificial life <ref> [12] </ref>. However, the price for the additional flexibility in representation is that solutions may grow arbitrarily large. For example, Kinnear [10] observed in his sorting problem a strong tendency for the individuals in the population to grow without bound. <p> Parsimony, or simplicity of solution structures, is achievable by adding the tree size or expression length as a component of the fitness function to be minimized <ref> [12] </ref>. However, too great a penalty for size may lead to early convergence to local minima. A more general case of this multi-objective optimization problem has been discussed by Schaffer [17].
Reference: 13. <author> H. Muhlenbein: </author> <title> Evolutionary algorithms Theory and applications. </title> <editor> In: E. H. L. Aarts, J. K. Lenstra (eds.): </editor> <title> Local Search in Combinatorial Optimization. </title> <publisher> Wiley, </publisher> <year> 1993 </year>
Reference: 14. <author> H. Muhlenbein, D. Schierkamp-Voosen: </author> <title> Predictive models for the breeder genetic algorithm I: Continuous parameter optimization. </title> <booktitle> Evolutionary Computation 1, </booktitle> <month> 25-49 </month> <year> (1993) </year>
Reference: 15. <author> I. Rechenberg: </author> <title> Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. </title> <publisher> Stuttgart: </publisher> <address> Frommann-Holzboog, </address> <year> 1973 </year>
Reference: 16. <author> J. Rissanen: </author> <title> Stochastic complexity and modeling. </title> <booktitle> The Annuls of Statistics 14, </booktitle> <month> 1080-1100 </month> <year> (1986) </year>
Reference-contexts: However, the overall result suggests preference of small networks to larger ones if no information is available about the configuration space, confirming the principles of Occam's razor [1, 21] and minimum description length <ref> [16] </ref>. 4 Effects of Occam's Razor: Simulation Results The foregoing analysis suggests the necessity of some mechanism to constrain the network size. Parsimony, or simplicity of solution structures, is achievable by adding the tree size or expression length as a component of the fitness function to be minimized [12].
Reference: 17. <author> J. D. Schaffer: </author> <title> Multiple objective optimization with vector evaluated genetic algorithm. </title> <editor> In: J. D. Schaffer (ed.): </editor> <booktitle> Proceedings of the International Conference on Genetic Algorithms and Their Applications. </booktitle> <publisher> Erlbaum Associates, </publisher> <year> 1985, </year> <pages> pp. 93-100 </pages>
Reference-contexts: However, too great a penalty for size may lead to early convergence to local minima. A more general case of this multi-objective optimization problem has been discussed by Schaffer <ref> [17] </ref>. In controlling the accuracy and parsimony of neural networks, it is important that the network be able to approximate at least the training set to a specified performance level. A small network should be preferred to a greater network only if both of them achieve a comparable performance.
Reference: 18. <author> W. A. Tackett: </author> <title> Genetic programming for feature discovery and image discrimination. </title> <editor> In: S. Forrest (ed.): </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms (ICGA-93). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993, </year> <pages> pp. 303-309 </pages>
Reference-contexts: For this kind of open-ended exploration of solution space, parsimony may be an important factor not just for ease of analysis, but because of a more direct relationship to fitness. Tackett <ref> [18] </ref> empirically found, in his pattern classification application, a high degree of correlation between tree size and performance; ? Lecture Notes in Computer Science 866: Parallel Problem Solving from Nature (PPSN-94), Springer-Verlag, 1994, pp. 462-471. among the set of pretty good solution trees, those with highest performance were usually the smallest
Reference: 19. <author> D. Whitley, T. Starkweather, C. Bogart: </author> <title> Genetic algorithms and neural networks: Optimizing connections and connectivity. </title> <booktitle> Parallel Computing 14, </booktitle> <month> 347-361 </month> <year> (1990) </year>
Reference: 20. <author> B. T. Zhang, H. Muhlenbein: </author> <title> Genetic programming of minimal neural nets using Occam's razor. </title> <editor> In: S. Forrest (ed.): </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms (ICGA-93). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993, </year> <pages> pp. 342-349 </pages>
Reference: 21. <author> B. T. Zhang, H. Muhlenbein: </author> <title> Evolving optimal neural networks using genetic algorithms with Occam's razor. </title> <journal> Complex Systems (1994) </journal>
Reference-contexts: Effect of number of units on generalization from one problem to another as well as the task at hand. However, the overall result suggests preference of small networks to larger ones if no information is available about the configuration space, confirming the principles of Occam's razor <ref> [1, 21] </ref> and minimum description length [16]. 4 Effects of Occam's Razor: Simulation Results The foregoing analysis suggests the necessity of some mechanism to constrain the network size.
Reference: 22. <author> B. T. Zhang, H. Muhlenbein: </author> <title> Synthesis of sigma-pi neural networks by the breeder genetic programming. </title> <booktitle> In: Proceedings of the IEEE World Congress on Computational Intelligence (WCCI-94). </booktitle> <address> New York: </address> <publisher> IEEE Press, </publisher> <year> 1994 </year>
Reference-contexts: can construct a higher-order network of sigma-pi units: y i = f i (u i ) = f i X w ik T (k) = f i @ k 0 j k Y v kj x j A A (7) Evolutionary algorithms can be used to evolve problem-specific sigma-pi networks <ref> [9, 22] </ref>. We encode the network as a set of m trees, where m is the number of output units. Figure 1 shows an example of the genotype and its phenotype of a sigma-pi network with one output unit. Fig. 1.
References-found: 22

