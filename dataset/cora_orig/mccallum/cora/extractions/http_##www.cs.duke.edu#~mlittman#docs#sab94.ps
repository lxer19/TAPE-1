URL: http://www.cs.duke.edu/~mlittman/docs/sab94.ps
Refering-URL: http://www.cs.duke.edu/~mlittman/topics/pomdp-page.html
Root-URL: 
Email: mlittman@cs.brown.edu  
Author: Michael L. Littman 
Address: Providence, RI 02912-1910  
Affiliation: Brown University Bellcore Department of Computer Science Brown University  
Abstract: Memoryless policies: theoretical limitations and practical results Abstract One form of adaptive behavior is "goal-seeking" in which an agent acts so as to minimize the time it takes to reach a goal state. This paper presents some theoretical and empirical findings on algorithms that devise goal-seeking behaviors for "memoryless" agents who base their behavioral decisions solely on current sensations. The basic results are that (1) the general problem of finding good deterministic memoryless policies is intractable, however, (2) simple branch-and-bound heuristics can be used to find optimal memoryless policies extremely quickly for some established ex ample environments.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto, R. S. Sutton, and C. J.C.H. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical Report COINS Technical Report 89-95, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1989. </year>
Reference-contexts: The study of Markov decision processes by the operations research community (e.g., [9, 4]) and more recently by reinforcement learning researchers (e.g., <ref> [1] </ref>) has yielded deep insights and efficient algorithms for finding optimal memoryless policies in Markovian environments. It is natural to ask whether similar results can be obtained for the non-Markovian case in which an agent's sensations do not give it complete information about its current state.
Reference: [2] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: This paper is concerned with finding optimal memoryless policies. The problem of finding more general optimal policies is addressed by research on partially observable Markov decision processes <ref> [6, 2] </ref> and is even harder in some sense although the policies are typically of higher quality. We can also distinguish policies as to whether they are deterministic or probabilistic.
Reference: [3] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and intractability: A guide to the theory of NP-completeness. </title> <publisher> Freeman, </publisher> <address> San Francisco, CA., </address> <year> 1979. </year>
Reference-contexts: This section proves that finding satisficing deterministic memoryless policies is one such NP-complete problem. (For more on NP-completeness, see <ref> [3] </ref>.) 3.1 Reduction Consider the following decision problem: "Given an environment, is there some satisficing deterministic mem-oryless policy for it?" That is, is there some way to assign actions to sensations so that the goal is eventually reached from every initial state? The complete set of states, actions, and sensations, as
Reference: [4] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: In the class of Markov environments, in which sensations completely determine underlying state, it is always possible to find a deterministic memoryless policy that is optimal; no other behavioral strategy is superior. The study of Markov decision processes by the operations research community (e.g., <ref> [9, 4] </ref>) and more recently by reinforcement learning researchers (e.g., [1]) has yielded deep insights and efficient algorithms for finding optimal memoryless policies in Markovian environments. <p> The strategy adopted in this work for finding good policies is to consider small changes to known policies and to "hill-climb" to locally optimal policies. This approach, which bears some relation to Howard's policy iteration <ref> [4] </ref>, is not guaranteed to find optimal policies in non-Markovian environments but empirically often does. The algorithm takes an existing policy and considers changing the action associated with a single sensation chosen at random.
Reference: [5] <author> Michael L. Littman. </author> <title> An optimization-based categorization of reinforcement learning environments. </title> <editor> In I. H. Meyer, H. Roithlat, and S. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proc. Second International Conference on Simulation and Adaptive Behavior. </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: With 933 states, 92 distinct sensations, and 8 actions, woods7 can be considered a moderate-sized environment. Littman <ref> [5] </ref> examined the resources an agent would need to achieve optimal performance in woods7. <p> The memory-bit approach, however, is purer in that the agent can define for itself how to organize its short-term memory. For McCallum's maze, one bit of memory is sufficient to give optimal performance, making it is an h = 1-environment <ref> [5] </ref>. This means that even with additional bits of memory, no better policy can be found.
Reference: [6] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observable markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-66, </pages> <year> 1991. </year>
Reference-contexts: This paper is concerned with finding optimal memoryless policies. The problem of finding more general optimal policies is addressed by research on partially observable Markov decision processes <ref> [6, 2] </ref> and is even harder in some sense although the policies are typically of higher quality. We can also distinguish policies as to whether they are deterministic or probabilistic.
Reference: [7] <author> D. G. Luenberger. </author> <title> Introduction to Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1973. </year>
Reference-contexts: Both of these are lower bounds, but the first one is too weak a bound though easily computed and the second is perfect but too expensive. Linear programming <ref> [7] </ref> forms the basis of a sophisticated method for computing lower bounds for this problem, but this section presents a simpler and faster algorithm which seems to give bounds that are just as good.
Reference: [8] <author> R. A. McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 190-196, </pages> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This makes it impossible for the agent to select a downward action in the corner and a slightly twisty policy is the result. Nonetheless, there is no way for a memoryless policy to untwist the path without worsening performance. gridworld. 5.3 McCallum's maze McCallum <ref> [8] </ref> studied an environment for which no sat-isficing deterministic memoryless policy exists. It is depicted in Figure 5. Once again empty squares represent individual states, X's impassable walls, and O the goal state.
Reference: [9] <author> S. M. Ross. </author> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: In the class of Markov environments, in which sensations completely determine underlying state, it is always possible to find a deterministic memoryless policy that is optimal; no other behavioral strategy is superior. The study of Markov decision processes by the operations research community (e.g., <ref> [9, 4] </ref>) and more recently by reinforcement learning researchers (e.g., [1]) has yielded deep insights and efficient algorithms for finding optimal memoryless policies in Markovian environments.
Reference: [10] <author> Satinder Pal Singh, Tommi Jaakkola, and Michael I. Jordan. </author> <title> Model-free reinforcement learning for non-markovian decision problems. </title> <booktitle> In Proceedings of the Machine Learning Conference, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Note that for probabilistic policies or transitions functions, the performance measure is the total expected steps to goal. A model-free learning algorithm for this class of policies has been proposed recently <ref> [10] </ref>. 3 NP-completeness results Work in the area of NP-completeness tells us that there is a set of very natural decision problems|problems that require a yes/no response for each input|for which there is no known efficient algorithm but that if an algorithm is found for any one of them, then there
Reference: [11] <author> R. S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Machine Learning Conference, </booktitle> <year> 1990. </year>
Reference-contexts: This comparison is not entirely fair though since the Q-learning does not use a model and is only intended to work in Markovian environments. Nonetheless, the branch-and-bound algorithm guarantees optimality and runs quickly. 5.2 Sutton's gridworld A classic example from the reinforcement learning literature is Sutton's gridworld <ref> [11] </ref>, depicted in Figure 4. In the figure, empty squares represent individual states, X's impassable walls, and O the goal state.
Reference: [12] <author> C. J.C.H. Watkins. </author> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: At the time, the best known memoryless policy (Class 1 agent) took a total of 3412 steps to reach the goal from all states which is an average of 3.66 steps to goal or 0.27 goals per step. This policy was found by running the Q-learning algorithm <ref> [12] </ref> many times and keeping the best policy found. Table 1 summarizes 1000 runs of the upper bound algorithm from Section 4.1 on this environment. <p> There are a large number of related issues that deserve more attention. The algorithms in this paper are model-based in that they require detailed information about the underlying environment to work. Model-free methods, such as Q-learning <ref> [12] </ref> do not require such de 7 tailed information and are therefore more appropriate as on-line learning algorithms. These methods can also be quite effective for this problem. Some environments, such as McCallum's maze, cannot be solved by any deterministic memoryless policy.
Reference: [13] <author> Stewart W. Wilson. </author> <title> Knowledge growth in an artificial animal. </title> <booktitle> In Proceedings of the First International Conference on Genetic Algorithms and Their Applications, </booktitle> <pages> pages 16-23, </pages> <address> Hillsdale, NJ, 1985. </address> <publisher> Lawrence Erlbaum Associates. </publisher> <pages> 8 </pages>
Reference-contexts: the problem of finding optimal deterministic memory-less policies, as demonstrated by the results in the next section. 5 Empirical results This section describes three environments developed by other researchers and uses the techniques from the previous section to find optimal memoryless policies for them. 5.1 Wilson's woods7 Wilson's woods7 environment <ref> [13] </ref> consists of an 18 by 58 cell toroidal grid on which an agent can wander to any of its eight neighboring cells in a single step. Non-empty cells contain "trees" which block motion and "food" which serves as the goals.
References-found: 13

