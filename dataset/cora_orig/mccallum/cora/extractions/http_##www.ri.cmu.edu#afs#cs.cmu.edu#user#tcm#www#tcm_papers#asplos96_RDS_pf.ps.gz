URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/tcm/www/tcm_papers/asplos96_RDS_pf.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/tcm/www/Papers.html
Root-URL: 
Email: fluk,tcmg@eecg.toronto.edu  
Title: Compiler-Based Prefetching for Recursive Data Structures  
Author: Chi-Keung Luk and Todd C. Mowry 
Address: Toronto, Canada M5S 3G4  
Affiliation: Department of Computer Science Department of Electrical and Computer Engineering University of Toronto  
Abstract: Software-controlled data prefetching offers the potential for bridging the ever-increasing speed gap between the memory subsystem and today's high-performance processors. While prefetching has enjoyed considerable success in array-based numeric codes, its potential in pointer-based applications has remained largely unexplored. This paper investigates compiler-based prefetching for pointer-based applications|in particular, those containing recursive data structures. We identify the fundamental problem in prefetching pointer-based data structures and propose a guideline for devising successful prefetching schemes. Based on this guideline, we design three prefetching schemes, we automate the most widely applicable scheme (greedy prefetching) in an optimizing research compiler, and we evaluate the performance of all three schemes on a modern superscalar processor similar to the MIPS R10000. Our results demonstrate that compiler-inserted prefetching can significantly improve the execution speed of pointer-based codes|as much as 45% for the applications we study. In addition, the more sophisticated algorithms (which we currently perform by hand, but which might be implemented in future compilers) can improve performance by as much as twofold. Compared with the only other compiler-based pointer prefetching scheme in the literature, our algorithms offer substantially better performance by avoiding unnecessary overhead and hiding more latency. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> April: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The real challenge is tolerating read latency, which requires that we decouple the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for tolerating read latency are prefetching [2, 3, 15] and multithreading <ref> [1, 9, 11, 13] </ref>. Prefetching tolerates latency by anticipating what data is needed and moving it to the cache 1 ahead of time. Prefetching can be controlled either by hardware or software. <p> The compiler immediately prefetches all eight children of the current node t before bh test () is called. Although 59% of prefetches are unnecessary, the overhead remains low and there is a 4% speedup. (a) bh bh walk (..., node*t, ...) f prefetch (t!children [0]); prefetch (t!children <ref> [1] </ref>); prefetch (t!children [2]); prefetch (t!children [3]); prefetch (t!children [4]); prefetch (t!children [5]); prefetch (t!children [6]); prefetch (t!children [7]); if (bh test (p)) f for (k=0; k&lt;8; k++)f r = t!children [k]; if (r) bh walk (..., r, ...); g g else bh work (..., t, ...); g (c) health void
Reference: [2] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <year> 1991. </year>
Reference-contexts: The real challenge is tolerating read latency, which requires that we decouple the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for tolerating read latency are prefetching <ref> [2, 3, 15] </ref> and multithreading [1, 9, 11, 13]. Prefetching tolerates latency by anticipating what data is needed and moving it to the cache 1 ahead of time. Prefetching can be controlled either by hardware or software. <p> Although 59% of prefetches are unnecessary, the overhead remains low and there is a 4% speedup. (a) bh bh walk (..., node*t, ...) f prefetch (t!children [0]); prefetch (t!children [1]); prefetch (t!children <ref> [2] </ref>); prefetch (t!children [3]); prefetch (t!children [4]); prefetch (t!children [5]); prefetch (t!children [6]); prefetch (t!children [7]); if (bh test (p)) f for (k=0; k&lt;8; k++)f r = t!children [k]; if (r) bh walk (..., r, ...); g g else bh work (..., t, ...); g (c) health void waiting (Village *village, <p> In addition to non-excepting prefetches, non-excepting load instructions also appear to be quite useful for prefetching pointer-based codes, although we currently are not exploiting them aggressively in our compiler. 6 Related Work Although prefetching has been studied extensively for array-based numeric codes <ref> [2, 16] </ref>, relatively little work has been done on non-numeric applications. Chen et al. [5] used global instruction scheduling techniques to move address generation back as early as possible to hide a small cache miss latency (10 cycles), and found mixed results.
Reference: [3] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <booktitle> Software prefetch-ing. In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: While cache hierarchies are an important step toward addressing the latency problem, they are not a complete solution. To further reduce or tolerate memory latency, automatic compiler techniques such as locality optimizations [4, 21] and software-controlled prefetching <ref> [3, 16] </ref> have been proposed and evaluated in the past. While these techniques have shown considerable promise, they have been limited in scope to array-based numeric applications. <p> The real challenge is tolerating read latency, which requires that we decouple the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for tolerating read latency are prefetching <ref> [2, 3, 15] </ref> and multithreading [1, 9, 11, 13]. Prefetching tolerates latency by anticipating what data is needed and moving it to the cache 1 ahead of time. Prefetching can be controlled either by hardware or software. <p> Although 59% of prefetches are unnecessary, the overhead remains low and there is a 4% speedup. (a) bh bh walk (..., node*t, ...) f prefetch (t!children [0]); prefetch (t!children [1]); prefetch (t!children [2]); prefetch (t!children <ref> [3] </ref>); prefetch (t!children [4]); prefetch (t!children [5]); prefetch (t!children [6]); prefetch (t!children [7]); if (bh test (p)) f for (k=0; k&lt;8; k++)f r = t!children [k]; if (r) bh walk (..., r, ...); g g else bh work (..., t, ...); g (c) health void waiting (Village *village, List *list) while
Reference: [4] <author> S. Carr, K. S. McKinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 252-262, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: While cache hierarchies are an important step toward addressing the latency problem, they are not a complete solution. To further reduce or tolerate memory latency, automatic compiler techniques such as locality optimizations <ref> [4, 21] </ref> and software-controlled prefetching [3, 16] have been proposed and evaluated in the past. While these techniques have shown considerable promise, they have been limited in scope to array-based numeric applications. <p> Unfortunately, the locality optimizations developed for numeric applications (e.g., tiling, loop interchange, etc. <ref> [4, 21] </ref>) are not applicable to RDSs. Although we do explore one optimization which potentially improves spatial locality in RDSs (data linearization, as described later in Section 2.2.3), a significant number of cache misses still remain, and therefore techniques for tolerating latency are also needed. <p> Although 59% of prefetches are unnecessary, the overhead remains low and there is a 4% speedup. (a) bh bh walk (..., node*t, ...) f prefetch (t!children [0]); prefetch (t!children [1]); prefetch (t!children [2]); prefetch (t!children [3]); prefetch (t!children <ref> [4] </ref>); prefetch (t!children [5]); prefetch (t!children [6]); prefetch (t!children [7]); if (bh test (p)) f for (k=0; k&lt;8; k++)f r = t!children [k]; if (r) bh walk (..., r, ...); g g else bh work (..., t, ...); g (c) health void waiting (Village *village, List *list) while (list != NULL)
Reference: [5] <author> W. Y. Chen, S. A. Mahlke, P. P. Chang, and W. W. Hwu. </author> <title> Data access microarchitectures for superscalar processors with compiler-assisted data prefetching. </title> <booktitle> In Proceedings of Microcom-puting 24, </booktitle> <year> 1991. </year>
Reference-contexts: Although 59% of prefetches are unnecessary, the overhead remains low and there is a 4% speedup. (a) bh bh walk (..., node*t, ...) f prefetch (t!children [0]); prefetch (t!children [1]); prefetch (t!children [2]); prefetch (t!children [3]); prefetch (t!children [4]); prefetch (t!children <ref> [5] </ref>); prefetch (t!children [6]); prefetch (t!children [7]); if (bh test (p)) f for (k=0; k&lt;8; k++)f r = t!children [k]; if (r) bh walk (..., r, ...); g g else bh work (..., t, ...); g (c) health void waiting (Village *village, List *list) while (list != NULL) f prefetch (list!forward); <p> Chen et al. <ref> [5] </ref> used global instruction scheduling techniques to move address generation back as early as possible to hide a small cache miss latency (10 cycles), and found mixed results.
Reference: [6] <author> A. Deutsch. </author> <title> A storeless model of aliasing and its abstractions using finite respresentation of right-regular equivalence relations. </title> <booktitle> In Proceedings of the 1992 International Conference on Computer Languages, </booktitle> <pages> pages 2-13, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Unfortunately, despite the significant progress that has been made recently in pointer analysis techniques for heap-allocated objects <ref> [6, 8, 10] </ref>, compilers are still not sophisticated enough to differentiate these two cases automatically. In general, analyzing the addresses of heap-allocated objects is a very difficult problem for the compiler. <p> Although 59% of prefetches are unnecessary, the overhead remains low and there is a 4% speedup. (a) bh bh walk (..., node*t, ...) f prefetch (t!children [0]); prefetch (t!children [1]); prefetch (t!children [2]); prefetch (t!children [3]); prefetch (t!children [4]); prefetch (t!children [5]); prefetch (t!children <ref> [6] </ref>); prefetch (t!children [7]); if (bh test (p)) f for (k=0; k&lt;8; k++)f r = t!children [k]; if (r) bh walk (..., r, ...); g g else bh work (..., t, ...); g (c) health void waiting (Village *village, List *list) while (list != NULL) f prefetch (list!forward); i = village!hosp.free
Reference: [7] <author> M. Emami, R. Ghiya, and L. J. Hendren. </author> <title> Context-sensitive inter-procedural points-to analysis in the presence of function pointers. </title> <booktitle> In Proceedings of the ACM SIGPLAN'94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 242-256, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: This heuristic corresponds to how RDS codes are typically written. To detect recurrent pointer updates, the compiler propagates pointer values using a simplified (but less precise) version of earlier pointer analysis algorithms <ref> [7, 12] </ref>. (a) while (l) f listNode *m; ... m = l!next; l = m!next; ... (b) for (...) f listNode *n; ... ... (c) f (treeNode *t) f ... f (t!left); f (t!right); ... (d) k (treeNode tn) f ... k (*(tn.left)); k (*(tn.right)); ... traversals. l!next!next inside the while-loop. <p> Although 59% of prefetches are unnecessary, the overhead remains low and there is a 4% speedup. (a) bh bh walk (..., node*t, ...) f prefetch (t!children [0]); prefetch (t!children [1]); prefetch (t!children [2]); prefetch (t!children [3]); prefetch (t!children [4]); prefetch (t!children [5]); prefetch (t!children [6]); prefetch (t!children <ref> [7] </ref>); if (bh test (p)) f for (k=0; k&lt;8; k++)f r = t!children [k]; if (r) bh walk (..., r, ...); g g else bh work (..., t, ...); g (c) health void waiting (Village *village, List *list) while (list != NULL) f prefetch (list!forward); i = village!hosp.free personnel; p =
Reference: [8] <author> R. Ghiya and L. J. Hendren. </author> <title> Is it a Tree, a DAG, or a Cyclic Graph? A shape analysis for heap-directed pointers in C. </title> <booktitle> In Proceedings of the 23rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-15, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Unfortunately, despite the significant progress that has been made recently in pointer analysis techniques for heap-allocated objects <ref> [6, 8, 10] </ref>, compilers are still not sophisticated enough to differentiate these two cases automatically. In general, analyzing the addresses of heap-allocated objects is a very difficult problem for the compiler. <p> a record type r containing at least one pointer that points either directly or indirectly to a record type s. (Note that r and s are not restricted to be struct T f int data; struct T *left; struct T *right; g struct A f int i; struct B **kids <ref> [8] </ref>; g struct C f int j; double f; g (a) RDS type (b) RDS type (c) Not RDS type as being RDS types. the same type, since RDSs may be comprised of heterogeneous nodes.) For example, the type declarations in Figure 6 (a) and After discovering data structures with the
Reference: [9] <author> R. H. Halstead, Jr. and T. Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The real challenge is tolerating read latency, which requires that we decouple the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for tolerating read latency are prefetching [2, 3, 15] and multithreading <ref> [1, 9, 11, 13] </ref>. Prefetching tolerates latency by anticipating what data is needed and moving it to the cache 1 ahead of time. Prefetching can be controlled either by hardware or software.
Reference: [10] <author> L. J. Hendren, J. Hummel, and A. Nicolau. </author> <title> A general data dependence test for dynamic, pointer-based data structures. </title> <booktitle> In Proceedings of the SIGPLAN'94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 218-229, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Unfortunately, despite the significant progress that has been made recently in pointer analysis techniques for heap-allocated objects <ref> [6, 8, 10] </ref>, compilers are still not sophisticated enough to differentiate these two cases automatically. In general, analyzing the addresses of heap-allocated objects is a very difficult problem for the compiler.
Reference: [11] <editor> J. S. Kowalik, editor. </editor> <title> Parallel MIMD Computation : The HEP Supercomputer and Its Applications. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: The real challenge is tolerating read latency, which requires that we decouple the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for tolerating read latency are prefetching [2, 3, 15] and multithreading <ref> [1, 9, 11, 13] </ref>. Prefetching tolerates latency by anticipating what data is needed and moving it to the cache 1 ahead of time. Prefetching can be controlled either by hardware or software.
Reference: [12] <author> W. Landi, B. G. Ryder, and S. Zhang. </author> <title> Interprocedural modification side effect analysis with pointer aliasing. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 56-67, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: This heuristic corresponds to how RDS codes are typically written. To detect recurrent pointer updates, the compiler propagates pointer values using a simplified (but less precise) version of earlier pointer analysis algorithms <ref> [7, 12] </ref>. (a) while (l) f listNode *m; ... m = l!next; l = m!next; ... (b) for (...) f listNode *n; ... ... (c) f (treeNode *t) f ... f (t!left); f (t!right); ... (d) k (treeNode tn) f ... k (*(tn.left)); k (*(tn.right)); ... traversals. l!next!next inside the while-loop.
Reference: [13] <author> J. Laudon, A. Gupta, and M. Horowitz. </author> <title> Interleaving: A multi-threading technique targeting multiprocessors and workstations. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 308-318, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The real challenge is tolerating read latency, which requires that we decouple the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for tolerating read latency are prefetching [2, 3, 15] and multithreading <ref> [1, 9, 11, 13] </ref>. Prefetching tolerates latency by anticipating what data is needed and moving it to the cache 1 ahead of time. Prefetching can be controlled either by hardware or software.
Reference: [14] <author> M. H. Lipasti, W. J. Schmidt, S. R. Kunkel, and R. R. Roedi-ger. SPAID: </author> <title> Software prefetching in pointer- and call-intensive environments. </title> <booktitle> In Proceedings of the 28th Annual IEEE/ACM International Symposium on Microarchitecture, </booktitle> <year> 1995. </year>
Reference-contexts: Compared to our compiler-based approach, their scheme has two shortcomings: (i) annotations are inserted manually, and (ii) their hardware extensions are not likely to be applicable in uniprocessors. To our knowledge, the only compiler-based pointer prefetching scheme in the literature is the SPAID scheme proposed by Lipasti et al. <ref> [14] </ref>. Based on an observation that procedures are likely to dereference any pointers passed to them as arguments, SPAID inserts prefetches for the objects pointed to by these pointer arguments at the call sites. <p> To quantify the performance difference between SPAID and our greedy prefetching scheme, we implemented several versions of SPAID in our experimental framework with different numbers of prefetches inserted per call site. Our results are consistent with the conclusion in the SPAID paper <ref> [14] </ref> that the best performance is achieved by inserting only one prefetch per call site|the S bars in Figure 18 correspond to this optimal case. When a procedure has multiple pointer arguments, we select the first one pointing to any RDS to prefetch.
Reference: [15] <author> T. C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1994. </year> <note> Technical Report CSL-TR-94-626. </note>
Reference-contexts: The real challenge is tolerating read latency, which requires that we decouple the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for tolerating read latency are prefetching <ref> [2, 3, 15] </ref> and multithreading [1, 9, 11, 13]. Prefetching tolerates latency by anticipating what data is needed and moving it to the cache 1 ahead of time. Prefetching can be controlled either by hardware or software. <p> Second, a scheduling phase attempts to insert prefetches sufficiently far in advance such that latency is effectively hidden, while introducing minimal runtime overhead. For array-based applications, the compiler can use locality analysis to predict which dynamic references to prefetch, and loop splitting and software pipelining to schedule prefetches <ref> [15] </ref>. A fundamental difference between array references and pointer dereferences is the way addresses are generated. The address of an array reference A [i] can always be computed once a value of i is chosen. <p> If we are traversing the RDS inside a loop, we can accomplish this by unrolling the loop by a factor of m (similar to what is done in array-based prefetch-ing <ref> [15] </ref>). For a traversal through recursion, one could potentially keep track of the number of nodes visited between prefetches, but the overhead of doing so may be comparable to simply issuing a prefetch for every node. <p> Ideally, prefetching will not increase memory traffic, since the original memory references will simply be converted into prefetches. (In fact, previous studies have demonstrated that prefetching can actually reduce the memory traffic in a shared-memory multiprocessor through exclusive-mode hints <ref> [15] </ref>.) However, since the natural jump-pointers used by greedy prefetching may point to nodes that will not be accessed in the near future (or perhaps not at all), greedy prefetching can potentially increase the memory bandwidth demands through useless prefetches.
Reference: [16] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: While cache hierarchies are an important step toward addressing the latency problem, they are not a complete solution. To further reduce or tolerate memory latency, automatic compiler techniques such as locality optimizations [4, 21] and software-controlled prefetching <ref> [3, 16] </ref> have been proposed and evaluated in the past. While these techniques have shown considerable promise, they have been limited in scope to array-based numeric applications. <p> As a result, we may prefetch nodes unnecessarily that already reside in the cache (as discussed earlier in Section 5.1). For numeric applications, sophisticated locality analysis techniques have been combined with loop splitting techniques to isolate the dynamic iterations that should be prefetched <ref> [16] </ref>. Unfortunately, the control structures in RDS codes are less amenable to isolating dynamic node visitations, so our only option may be to eliminate static prefetch instructions altogether. This makes sense for prefetches that are almost always unnecessary (i.e. have very high hit rates). <p> In addition to non-excepting prefetches, non-excepting load instructions also appear to be quite useful for prefetching pointer-based codes, although we currently are not exploiting them aggressively in our compiler. 6 Related Work Although prefetching has been studied extensively for array-based numeric codes <ref> [2, 16] </ref>, relatively little work has been done on non-numeric applications. Chen et al. [5] used global instruction scheduling techniques to move address generation back as early as possible to hide a small cache miss latency (10 cycles), and found mixed results.
Reference: [17] <author> A. Rogers, M. Carlisle, J. Reppy, and L. Hendren. </author> <title> Supporting dynamic data structures on distributed memory machines. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 17(2), </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: Two examples of greedy prefetch scheduling are shown in Figure 8. 4 Experimental Framework To evaluate the performance of our three prefetching schemes, we performed detailed cycle-by-cycle simulations of the entire Olden benchmark suite <ref> [17] </ref> on a dynamically-scheduled, superscalar processor similar to the MIPS R10000. The Olden benchmark suite contains ten pointer-based applications written in C, which are briefly summarized in Table 1.
Reference: [18] <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: The parameters of our model are shown in Table 2. We use pixie <ref> [18] </ref> to instrument the optimized MIPS object files produced by the compiler, and pipe the resulting trace into our simulator.
Reference: [19] <author> C. J. Stephenson. </author> <title> Fast fits. </title> <booktitle> In Proceedings of the ACM 9th Symposium on Operating Systems, </booktitle> <month> October </month> <year> 1983. </year>
Reference-contexts: To minimize the impact of store stalls during the initialization of dynamically-allocated objects, we use our own memory allocator for these experiments which is similar to mallopt provided in the Irix C library <ref> [19] </ref>, but also contains built-in prefetching to avoid such store misses. This optimization alone led to dramatic improvements (greater than two-fold speedups) over using malloc for the majority of the applications|particularly the ones that frequently allocate small objects.
Reference: [20] <author> S. W. K. Tjiang and J. L. Hennessy. Sharlit: </author> <title> A tool for building optimizers. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: In the next section, we describe the implementation details of our greedy prefetching compiler pass. 3 Implementation of Greedy Prefetching Our implementation of greedy prefetching within the SUIF compiler <ref> [20] </ref> consists of an analysis phase to recognize RDS accesses, and a scheduling phase to insert prefetches. 3.1 Analysis: Recognizing RDS Accesses To recognize RDS accesses, the compiler uses both type declaration information to recognize which data objects are RDSs, and control structure information to recognize when these objects are being
Reference: [21] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: While cache hierarchies are an important step toward addressing the latency problem, they are not a complete solution. To further reduce or tolerate memory latency, automatic compiler techniques such as locality optimizations <ref> [4, 21] </ref> and software-controlled prefetching [3, 16] have been proposed and evaluated in the past. While these techniques have shown considerable promise, they have been limited in scope to array-based numeric applications. <p> Unfortunately, the locality optimizations developed for numeric applications (e.g., tiling, loop interchange, etc. <ref> [4, 21] </ref>) are not applicable to RDSs. Although we do explore one optimization which potentially improves spatial locality in RDSs (data linearization, as described later in Section 2.2.3), a significant number of cache misses still remain, and therefore techniques for tolerating latency are also needed.
Reference: [22] <author> Z. Zhang and J. Torrellas. </author> <title> Speeding up irregular applications in shared-memory multiprocessors: Memory binding and group prefetching. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 188-200, </pages> <month> June </month> <year> 1995. </year> <pages> Page 12 </pages>
Reference-contexts: In contrast, our algorithms focus only on RDS accesses, and can issue prefetches much earlier (across procedure and loop iteration boundaries) by overcoming the pointer-chasing problem. Zhang and Torrellas <ref> [22] </ref> proposed a hardware-assisted scheme for prefetching irregular applications in shared-memory multiprocessors. Under their scheme, programs are annotated to bind together groups of data (e.g., fields in a record or two records linked by a pointer), which are then prefetched under hardware control.
References-found: 22

