URL: http://www.eecs.umich.edu/Rio/papers/rioFileCache.ps
Refering-URL: http://www.eecs.umich.edu/Rio/
Root-URL: http://www.eecs.umich.edu
Abstract: One of the fundamental limits to high-perfor-mance, high-reliability file systems is memorys vulnerability to system crashes. Because memory is viewed as unsafe, systems periodically write data back to disk. The extra disk traffic lowers performance, and the delay period before data is safe lowers reliability. The goal of the Rio (RAM I/O) file cache is to make ordinary main memory safe for persistent storage by enabling memory to survive operating system crashes. Reliable memory enables a system to achieve the best of both worlds: reliability equivalent to a write-through file cache, where every write is instantly safe, and performance equivalent to a pure write-back cache, with no reliability-induced writes to disk. To achieve reliability, we protect memory during a crash and restore it during a reboot (a warm reboot). Extensive crash tests show that even without protection, warm reboot enables memory to achieve reliability close to that of a write-through file system. Adding protection makes memory even safer than a write-through file system while adding essentially no overhead. By eliminating reliability-induced disk writes, Rio performs 4-22 times as fast as a write-through file system, 2-14 times as fast as a standard Unix file system, and 1-3 times as fast as an optimized system that risks losing 30 seconds of data and metadata. 
Abstract-found: 1
Intro-found: 1
Reference: [Abbott94] <author> M. Abbott, D. Har, L. Herger, M. Kauffmann, K. Mak, J. Murdock, C. Schulz, T. B. Smith, B. Tremaine, D. Yeh, and L. Wong. </author> <title> Durable Memory RS/6000 System Design. </title> <booktitle> In Proceedings of the 1994 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 414423, </pages> <year> 1994. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures <ref> [Abbott94] </ref>. These schemes complement Rio, which protects memory from operating system crashes. 7 Conclusions We have made a case for reliable file caches: main memory that can survive operating system crashes and be as safe and permanent as disk.
Reference: [APC96] <institution> The Power Protection Handbook. </institution> <type> Technical report, </type> <note> American Power Conversion, </note> <year> 1996. </year>
Reference-contexts: Memorys vulnerability to power outages is easy to understand and fix. A $119 uninterruptible power supply can keep a system running long enough to dump memory to disk in the event of a power outage <ref> [APC96] </ref>, or one can use non-volatile memory such as Flash RAM [Wu94]. We do not consider power outages further in this paper. Memorys vulnerability to OS crashes is more challenging.
Reference: [Baker91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198212, </pages> <month> Octo-ber </month> <year> 1991. </year>
Reference-contexts: This delay is often set to 30 seconds, which risks the loss of data written within 30 seconds of a crash. Unfortunately, 1/3 to 2/3 of newly written data lives longer than 30 seconds <ref> [Baker91, Hartman93] </ref>, so a large fraction of writes must eventually be written through to disk under this policy. File systems differ in how much data is delayed. For example, BSD 4.4 only delays partially written blocks, and then only until the file is closed. <p> Reliable file caches have striking implications for future system designers: Write-backs to disk are no longer needed except when the file cache fills up. This changes the assumptions about the dominance of write traffic underlying some file system research such as LFS <ref> [Rosenblum92, Baker91] </ref>. Delaying writes to disk until the file cache fills up enables the largest possible number of files to die in memory and enables remaining files to be written out more efficiently. Thus Rio improves performance over existing delayed-write systems.
Reference: [Baker92a] <author> Mary Baker, Satoshi Asami, Etienne Deprit, John Ousterhout, and Margo Seltzer. </author> <title> NonVolatile Memory for Fast Reliable File Systems. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASP-LOS-V), </booktitle> <pages> pages 1022, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: It is hence relatively easy for many simple software errors (such as de-referencing an uninitialized pointer) to accidentally corrupt the contents of memory <ref> [Baker92a] </ref>. The main issue in protection is how to control accesses to the file cache. We want to make it unlikely that non-file-cache procedures will accidentally corrupt the file cache, essentially making the file cache a protected module within the monolithic kernel. <p> Second, since memory contains long-term data, the system should treat memory like a peripheral that can be removed from the rest of the system. If the system board fails, it should be possible to move the memory board to a different system without losing power or data <ref> [Moran90, Baker92a] </ref>. Similarly, the system should be able to be reset and rebooted without erasing the contents of memory or CPU caches containing memory data. DEC Alphas allow a reset and boot without erasing memory or the CPU caches [DEC94]; the PCs we have tested do not.
Reference: [Baker92b] <author> Mary Baker and Mark Sullivan. </author> <title> The Recovery Box: Using Fast Recovery to Provide High Availability in the UNIX Environment. </title> <booktitle> In Proceedings USENIX Summer Conference, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: We use two strategies to detect file corruption: checksums detect direct corruption, and a synthetic workload called memTest detects direct and indirect corruption. The first method to detect corruption maintains a checksum of each memory block in the file cache <ref> [Baker92b] </ref>. We update the checksum in all procedures that write the file cache; unintentional changes to file cache buffers result in an inconsistent checksum. <p> Harp designers considered using warm reboot to protect against software bugs that crash both nodes. Unfortunately, the MicroVaxs used to run Harp overwrote memory during a reboot, making warm reboot impossible [Baker94]. The Recovery Box stores system state used in recovery in a region of memory <ref> [Baker92b] </ref>. Recovery box memory is preserved across crashes and used during the reboot of file servers. Similar to results found in Rio, Baker and Sullivan expect few crashes to corrupt the contents of the Recovery Box and so rely primarily on checksums to verify that data is intact.
Reference: [Baker94] <author> Mary Louise Gray Baker. </author> <title> Fast Crash Recovery in Distributed File Systems. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Harp designers considered using warm reboot to protect against software bugs that crash both nodes. Unfortunately, the MicroVaxs used to run Harp overwrote memory during a reboot, making warm reboot impossible <ref> [Baker94] </ref>. The Recovery Box stores system state used in recovery in a region of memory [Baker92b]. Recovery box memory is preserved across crashes and used during the reboot of file servers. <p> This allows Rio to store a wider range and larger amount of data. In particular, Rio stores file data and can thus improve file system performance under normal operation. Rios protection mechanism is similar to the scheme in <ref> [Baker94] </ref> and the expose page scheme in [Sullivan91a], but Rio additionally protects against physical addresses that would otherwise bypass the TLB. Sullivan and Stonebraker measure the overhead of expose page to be 7% on a debit/credit benchmark.
Reference: [Banatre91] <author> Michel Banatre, Gilles Muller, Bruno Roch--at, and Patrick Sanchez. </author> <title> Design decisions for the FTM: a general purpose fault tolerant machine. </title> <booktitle> In Proceedings of the 1991 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 7178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Banatre, et. al. implement stable transactional memory, which protects memory contents with dual memory banks, a special memory controller, and explicit calls to allow write access to specified memory blocks <ref> [Banatre91] </ref>. In contrast, Rio makes all files in memory reliable without special-purpose hardware or replication. A variety of general-purpose hardware and software mechanisms may be used to help protect memory from soft ware faults.
Reference: [Barton90] <author> James H. Barton, Edward W. Czeck, Zary Z. Segall, and Daniel P. Siewiorek. </author> <title> Fault injection experiments using FIAT. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4):575582, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Our primary goal in designing these faults is to generate a wide variety of system crashes. Our models are derived from studies of commercial operating systems [Sullivan91b, Lee93] and from prior models used in fault-injection studies <ref> [Barton90, Kao93, Kanawati95] </ref>. The faults we inject range from low-level hardware faults such as ipping bits in memory to high-level software faults such as memory allocation errors. We classify injected faults into three categories: bit ips, low-level software faults, and high-level software faults. <p> Note that faults that leave the system running will propagate data to disk and hence not change the relative reliability between memory and disk. The first category of faults ips random bits in the kernels address space <ref> [Barton90, Kanawati95] </ref>. We target three areas of the kernels address space: the kernel text, heap, and stack. These faults are easy to inject, and they cause a 1.
Reference: [Chapin95] <author> John Chapin, Mendel Rosenblum, Scott De-vine, Tirthankar Lahiri, Dan Teodosiu, and Anoop Gupta. Hive: </author> <title> Fault Containment for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1995 Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Papers by Johnson and Wahbe suggest various hardware mechanisms to trap updates to certain memory locations [Johnson82, Wahbe92]. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor <ref> [Chapin95] </ref>. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Object code modification has been suggested as a way to provide data breakpoints [Kessler90, Wahbe92] and fault isolation between software modules [Wahbe93].
Reference: [Chen96] <author> Peter M. Chen, Wee Teck Ng, Gurushankar Rajamani, and Christopher M. Aycock. </author> <title> The Rio File Cache: Surviving Operating System Crashes. </title> <type> Technical Report CSE-TR-286-96, </type> <institution> University of Michigan, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: Even after a number of optimizations to reduce the number of checks, the performance of code patching is 20-50% slower than with our current protection method <ref> [Chen96] </ref>. Hence code patching should be used only when the processor cannot be configured to map all addresses through the TLB. Kernels that use memory-mapping to cache files must be modified to map the file read-only.
Reference: [Copeland89] <author> George Copeland, Tom Keller, Ravi Krish-namurthy, and Marc Smith. </author> <title> The Case for Safe RAM. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 327335, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: To accomplish this, we use ideas from existing protection techniques such as virtual memory [Sullivan91a] and sandboxing [Wahbe93]. At first glance, the virtual memory protection of a system seems ideally suited to protect the file cache from unauthorized stores <ref> [Copeland89, Sullivan91a] </ref>. By turning off the write-permission bits in the page table for file cache pages, the system will cause most unauthorized stores to encounter a protection violation. File cache procedures must enable the write-permission bit in the page table before writing a page and disable writes afterwards. <p> However, if memory or CPU failures becomes the most common cause of system failure, extra redundancy may need to be added to compensate for the larger number of components holding permanent data. 6 Related Work Several researchers have proposed ways to protect memory from software failures <ref> [Copeland89] </ref>, though to our knowledge none have evaluated how effectively memory withstood these failures. Many commercial I/O devices contain memory. This memory is assumed to be reliable because of the I/O interface used to access it. These devices include solid-state disks, non-volatile disk caches, and write-buffers such as Prestoserve [Moran90].
Reference: [DEC94] <institution> DEC 3000 300/400/500/600/700/800/900 AXP Models System Programmers Manual. </institution> <type> Technical report, </type> <institution> Digital Equipment Corporation, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Similarly, the system should be able to be reset and rebooted without erasing the contents of memory or CPU caches containing memory data. DEC Alphas allow a reset and boot without erasing memory or the CPU caches <ref> [DEC94] </ref>; the PCs we have tested do not. Storing permanent data both on disk and in memory makes data more vulnerable to hardware failures than simply storing data on disk. Being able to remove the memory system without losing data can reduce but not eliminate the increased vulnerability.
Reference: [DeWitt84] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation Techniques for Main Memory Database Systems. </title> <booktitle> In Proceedings of the 1984 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 18, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Memorys perceived unreliability forces a tradeoff between performance and reliability. Applications requiring high reliability, such as transaction processing, write data synchronously through to disk, but this limits throughput to that of disk. While optimizations such as logging and group commit can increase effective disk throughput <ref> [Rosenblum92, Hagmann87, DeWitt84] </ref>, disk throughput is still far slower than memory throughput. Most Unix file systems mitigate the performance lost in synchronous, reliability-induced writes by asynchronously writing data to disk. This allows a greater degree of overlap between CPU time and I/O time.
Reference: [Gait90] <author> Jason Gait. </author> <title> Phoenix: A Safe In-Memory File System. </title> <journal> Communications of the ACM, </journal> <volume> 33(1):8186, </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: Further, main memory is random-access, unlike special-purpose devices. Phoenix is the only file system we are aware of that attempts to make all permanent files reliable while in main memory <ref> [Gait90] </ref>. Phoenix keeps two versions of an in-memory file system. One of these versions is kept write-protected; the other version is unprotected and evolves from the write-protected one via copy-on-write. At periodic checkpoints, the system write-protects the unprotected version and deletes obsolete pages in the original version.
Reference: [Ganger94] <author> Gregory R. Ganger and Yale N. Patt. </author> <title> Meta-data Update Performance in File Systems. </title> <booktitle> 1994 Operating Systems Design and Implementation (OSDI), </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: It writes data asynchronously to disk when 64 KB of data has been collected, when the user writes non-sequentially, or when the update daemon ushes dirty file data (once every 30 seconds). UFS writes metadata synchronously to disk to enforce ordering constraints <ref> [Ganger94] </ref>. UFSs poor performance is due in large part to its synchronous metadata updates. To eliminate this bottleneck, we enhanced UFS to delay all data and metadata until the next time update runs; this is the optimal no-order system in [Ganger94]. <p> UFS writes metadata synchronously to disk to enforce ordering constraints <ref> [Ganger94] </ref>. UFSs poor performance is due in large part to its synchronous metadata updates. To eliminate this bottleneck, we enhanced UFS to delay all data and metadata until the next time update runs; this is the optimal no-order system in [Ganger94]. This improves performance significantly over the default UFS; however, the optimization risks losing 30 seconds of both data and metadata. We measure the behavior of two file systems that write data synchronously to disk. UFS with write-through-on-close makes data permanent upon each file close by calling fsync.
Reference: [Gray90] <author> Jim Gray. </author> <title> A Census of Tandem System Availability between 1985 and 1990. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> 39(4), </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: Being able to remove the memory system without losing data can reduce but not eliminate the increased vulnerability. Because software crashes are the dominant cause of failure today <ref> [Gray90] </ref>, we do not consider the increased vulnerability to hardware failures a serious limitation of Rio.
Reference: [Hagmann87] <author> Robert B. Hagmann. </author> <title> Reimplementing the Cedar File System Using Logging and Group Commit. </title> <booktitle> In Proceedings of the 1987 Symposium on Operating Systems Principles, </booktitle> <pages> pages 155162, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Memorys perceived unreliability forces a tradeoff between performance and reliability. Applications requiring high reliability, such as transaction processing, write data synchronously through to disk, but this limits throughput to that of disk. While optimizations such as logging and group commit can increase effective disk throughput <ref> [Rosenblum92, Hagmann87, DeWitt84] </ref>, disk throughput is still far slower than memory throughput. Most Unix file systems mitigate the performance lost in synchronous, reliability-induced writes by asynchronously writing data to disk. This allows a greater degree of overlap between CPU time and I/O time.
Reference: [Hartman93] <author> John H. Hartman and John K. Ousterhout. </author> <title> Letter to the Editor. Operating Systems Review, </title> <address> 27(1):79, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: This delay is often set to 30 seconds, which risks the loss of data written within 30 seconds of a crash. Unfortunately, 1/3 to 2/3 of newly written data lives longer than 30 seconds <ref> [Baker91, Hartman93] </ref>, so a large fraction of writes must eventually be written through to disk under this policy. File systems differ in how much data is delayed. For example, BSD 4.4 only delays partially written blocks, and then only until the file is closed.
Reference: [Hennessy90] <author> John. L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach, 2nd Edition. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year> <pages> page 493. </pages>
Reference-contexts: We do not consider power outages further in this paper. Memorys vulnerability to OS crashes is more challenging. Most people would feel nervous if their system crashed while the sole copy of important data was in memory, even if the power stayed on <ref> [Tanenbaum95, Silberschatz94, Hennessy90] </ref>. Consequently, file systems periodically write data to disk, and transaction processing applications view transactions as committed only when data is written to disk. The goal of the Rio file cache is to enable memory to survive operating system crashes without writing data to disk.
Reference: [Howard88] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):5181, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: As a final check for corruption, we keep two copies of all files that are not modified by our workload and check that the two copies are equal. These files were not corrupted in our tests. In addition to memTest, we run four copies of the Andrew benchmark <ref> [Howard88] </ref>, a general-purpose file-system workload. Andrew creates and copies a source hierarchy; examines the hierarchy using find, ls, du, grep, and wc; and compiles the source hierarchy. <p> We run three workloads, cp+rm, Sdet, and Andrew. cp+rm recursively copies then recursively removes the Digital Unix source tree (40 MB). Sdet is one of SPECs SDM benchmarks and models a multi-user software development environment [SPE91]. Andrew also models software development but is dominated by CPU-intensive compilation <ref> [Howard88] </ref>. All results represent an average of at least 5 runs. The last two rows of Table 2 show that Rios protection mechanism adds almost no performance penalty, even on very I/O intensive workloads such as cp+rm.
Reference: [Johnson82] <author> Mark Scott Johnson. </author> <title> Some Requirements for Architectural Support of Software Debugging. </title> <booktitle> In Proceedings of the 1982 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 140 148, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: In contrast, Rio makes all files in memory reliable without special-purpose hardware or replication. A variety of general-purpose hardware and software mechanisms may be used to help protect memory from soft ware faults. Papers by Johnson and Wahbe suggest various hardware mechanisms to trap updates to certain memory locations <ref> [Johnson82, Wahbe92] </ref>. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor [Chapin95]. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory.
Reference: [Kanawati95] <author> Ghani A. Kanawati, Nasser A. Kanawati, and Jacob A. Abraham. FERRARI: </author> <title> A Flexible Software-Based Fault and Error Injection System. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(2):248260, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: Our primary goal in designing these faults is to generate a wide variety of system crashes. Our models are derived from studies of commercial operating systems [Sullivan91b, Lee93] and from prior models used in fault-injection studies <ref> [Barton90, Kao93, Kanawati95] </ref>. The faults we inject range from low-level hardware faults such as ipping bits in memory to high-level software faults such as memory allocation errors. We classify injected faults into three categories: bit ips, low-level software faults, and high-level software faults. <p> Note that faults that leave the system running will propagate data to disk and hence not change the relative reliability between memory and disk. The first category of faults ips random bits in the kernels address space <ref> [Barton90, Kanawati95] </ref>. We target three areas of the kernels address space: the kernel text, heap, and stack. These faults are easy to inject, and they cause a 1.
Reference: [Kane92] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: Or the file cache procedures can create a shadow copy in memory and implement atomic writes. Unfortunately, many systems allow certain kernel accesses to bypass the virtual memory protection mechanism and directly access physical memory <ref> [Kane92, Sites92] </ref>. For example, addresses in the DEC Alpha processor with the two most significant bits equal to 10 bypass the TLB; these are called KSEG addresses. This is especially significant on the DEC Alpha because the bulk of the file cache (the UBC) is accessed using physical addresses.
Reference: [Kao93] <author> Wei-Lun Kao, Ravishankar K. Iyer, and Dong Tang. </author> <title> FINE: A Fault Injection and Monitoring Environment for Tracing the UNIX System Behavior under Faults. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(11):11051118, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: Our primary goal in designing these faults is to generate a wide variety of system crashes. Our models are derived from studies of commercial operating systems [Sullivan91b, Lee93] and from prior models used in fault-injection studies <ref> [Barton90, Kao93, Kanawati95] </ref>. The faults we inject range from low-level hardware faults such as ipping bits in memory to high-level software faults such as memory allocation errors. We classify injected faults into three categories: bit ips, low-level software faults, and high-level software faults. <p> The second category of fault changes individual instructions in the kernel text. These faults are intended to approximate the assembly-level manifestation of real C-level programming errors <ref> [Kao93] </ref>. We corrupt assignment statements by changing the source or destination register. We corrupt conditional constructs by deleting branches. We also delete random instructions (both branch and non-branch). The last and most extensive category of faults imitate specific programming errors in the kernel [Sullivan91b]. <p> The last and most extensive category of faults imitate specific programming errors in the kernel [Sullivan91b]. These are more targeted at specific programming errors than the previous fault category. We inject an initialization fault by deleting instructions responsible for initializing a variable at the start of a procedure <ref> [Kao93, Lee93] </ref>. We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register [Sullivan91b, Lee93]. <p> In addition to the standard sanity checks written by programmers, the virtual memory system implicitly checks each load/store address to make sure it is a valid address. Particularly on a 64-bit machine, most errors are first detected by issuing an illegal address <ref> [Kao93, Lee93] </ref>. Thus, even without protection, Rio stores files nearly as reliably as a write-through file system. However, some applications will require even higher levels of safety. The rightmost section of Table 1 shows the reliability of the Rio file cache with protection turned on. <p> Out of 650 crashes, we 2. We plan to trace how faults propagate to corrupt files and crash the system instead of treating the system as a black box. This is extremely challenging, however, and is beyond the scope of this paper <ref> [Kao93] </ref>. Table 1: Comparing Disk and Memory Reliability. This table shows how often each type of error corrupted data for three systems. We conducted 50 tests for each fault type for each of three systems. The disk-based system uses fsync after every write, achieving write-through reliability.
Reference: [Kessler90] <author> Peter B. Kessler. </author> <title> Fast breakpoints: </title> <booktitle> Design and implementation. In Proceedings of the 1990 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 7884, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Object code modification has been suggested as a way to provide data breakpoints <ref> [Kessler90, Wahbe92] </ref> and fault isolation between software modules [Wahbe93].
Reference: [Lee93] <author> Inhwan Lee and Ravishankar K. Iyer. </author> <title> Faults, Symptoms, and Software Fault Tolerance in the Tandem GUARDIAN Operating System. </title> <booktitle> In International Symposium on Fault-Tolerant Computing (FTCS), pages 2029, </booktitle> <year> 1993. </year>
Reference-contexts: Our primary goal in designing these faults is to generate a wide variety of system crashes. Our models are derived from studies of commercial operating systems <ref> [Sullivan91b, Lee93] </ref> and from prior models used in fault-injection studies [Barton90, Kao93, Kanawati95]. The faults we inject range from low-level hardware faults such as ipping bits in memory to high-level software faults such as memory allocation errors. <p> The last and most extensive category of faults imitate specific programming errors in the kernel [Sullivan91b]. These are more targeted at specific programming errors than the previous fault category. We inject an initialization fault by deleting instructions responsible for initializing a variable at the start of a procedure <ref> [Kao93, Lee93] </ref>. We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register [Sullivan91b, Lee93]. <p> We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register <ref> [Sullivan91b, Lee93] </ref>. We do not corrupt the stack pointer register, as this is used to access local variables instead of as a pointer variable. <p> In addition to the standard sanity checks written by programmers, the virtual memory system implicitly checks each load/store address to make sure it is a valid address. Particularly on a 64-bit machine, most errors are first detected by issuing an illegal address <ref> [Kao93, Lee93] </ref>. Thus, even without protection, Rio stores files nearly as reliably as a write-through file system. However, some applications will require even higher levels of safety. The rightmost section of Table 1 shows the reliability of the Rio file cache with protection turned on.
Reference: [Leffler89] <author> Samuel J. Leffler, Marshall Kirk McKu--sick, Michael J. Karels, and John S. Quar-terman. </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System. </title> <publisher> Addi-son-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference: [Liskov91] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. </author> <title> Replication in the Harp File System. </title> <booktitle> In Proceedings of the 1991 Symposium on Operating System Principles, </booktitle> <pages> pages 226238, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Harp protects a log of recent modifications by replicating it in volatile, battery-backed memory across several server nodes <ref> [Liskov91] </ref>. Harp designers considered using warm reboot to protect against software bugs that crash both nodes. Unfortunately, the MicroVaxs used to run Harp overwrote memory during a reboot, making warm reboot impossible [Baker94]. The Recovery Box stores system state used in recovery in a region of memory [Baker92b].
Reference: [McKusick90] <author> Marshall Kirk McKusick, Michael J. Karels, and Keith Bostic. </author> <title> A Pageable Memory Based Filesystem. </title> <booktitle> In Proceedings US-ENIX Summer Conference, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: We achieve memory performance by eliminating all reliability-induced writes to disk <ref> [McKusick90, Ohta90] </ref>. We achieve write-through reliability by protecting memory during a crash and restoring it during a reboot (a warm reboot). Extensive crash tests show that even without protection, warm reboot enables memory to achieve reliability close to that of a write-through file system. <p> Note that only UFS with write-through-on-write achieves the same reliability as Rio. The Memory File System, which is completely memory-resident and does no disk I/O, is shown to illustrate optimal performance <ref> [McKusick90] </ref>. AdvFS is a journalling file system that reduces the penalty of metadata updates by writing metadata sequentially to a log. We run three workloads, cp+rm, Sdet, and Andrew. cp+rm recursively copies then recursively removes the Digital Unix source tree (40 MB).
Reference: [Moran90] <author> J. Moran, Russel Sandberg, D. Coleman, J. Kepecs, and Bob Lyon. </author> <title> Breaking Through the NFS Performance Barrier. </title> <booktitle> In Proceedings of EUUG Spring 1990, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: Second, since memory contains long-term data, the system should treat memory like a peripheral that can be removed from the rest of the system. If the system board fails, it should be possible to move the memory board to a different system without losing power or data <ref> [Moran90, Baker92a] </ref>. Similarly, the system should be able to be reset and rebooted without erasing the contents of memory or CPU caches containing memory data. DEC Alphas allow a reset and boot without erasing memory or the CPU caches [DEC94]; the PCs we have tested do not. <p> Many commercial I/O devices contain memory. This memory is assumed to be reliable because of the I/O interface used to access it. These devices include solid-state disks, non-volatile disk caches, and write-buffers such as Prestoserve <ref> [Moran90] </ref>. While these can improve performance over disks, their performance is limited by the low bandwidth and high overhead of the I/O bus and device interface.
Reference: [Ohta90] <author> Masataka Ohta and Hiroshi Tezuka. </author> <title> A Fast /tmp File System by Delay Mount Option. </title> <booktitle> In Proceedings USENIX Summer Conference, </booktitle> <pages> pages 145150, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We achieve memory performance by eliminating all reliability-induced writes to disk <ref> [McKusick90, Ohta90] </ref>. We achieve write-through reliability by protecting memory during a crash and restoring it during a reboot (a warm reboot). Extensive crash tests show that even without protection, warm reboot enables memory to achieve reliability close to that of a write-through file system. <p> First, reliability-induced writes to disk are no longer needed, because files in memory are as permanent and safe as files on disk. Digital Unix includes tunable parameters to turn off reliability writes for the UBC. We disable buffer cache writes as in <ref> [Ohta90] </ref> by turning most bwrite and bawrite calls to bdwrite; we modify sync and fsync calls to return immediately 1 ; and we modify the panic procedure to avoid writing dirty data back to disk before a crash.
Reference: [Ousterhout85] <author> John K. Ousterhout, Herve Da Costa, </author> <note> et al. </note>
Reference-contexts: Some file systems improve performance further by delaying some writes to disk in the hopes of the data being deleted or overwritten <ref> [Ousterhout85] </ref>. This delay is often set to 30 seconds, which risks the loss of data written within 30 seconds of a crash.
References-found: 32

