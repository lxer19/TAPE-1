URL: http://www.dcs.ex.ac.uk/~jamie/icann98.ps.Z
Refering-URL: http://www.dcs.ex.ac.uk/~jamie/
Root-URL: http://www.dcs.ex.ac.uk
Email: fpclane,jamieg@dcs.exeter.ac.uk  
Title: Simple Synchrony Networks Learning to Parse Natural Language with Temporal Synchrony Variable Binding  
Author: Peter C.R. Lane and James B. Henderson 
Keyword: SSNs to parse real natural language sentences.  
Address: Road, Exeter EX4 4PT, UK  
Affiliation: Department of Computer Science, University of Exeter Prince of Wales  
Abstract: The Simple Synchrony Network (SSN) is a new connectionist architecture, incorporating the insights of Temporal Synchrony Variable Binding (TSVB) into Simple Recurrent Networks. The use of TSVB means SSNs can output representations of structures, and can learn generalisations over the constituents of these structures (as required by systematicity). This paper describes the SSN and an associated training algorithm, and demonstrates SSNs' generalisation abilities through results from training 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L Shastri and V Ajjanagadde. </author> <title> From simple associations to systematic reasoning: A connectionist representation of rules, variables, and dynamic bindings using temporal synchrony. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 16 </volume> <pages> 417-494, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Temporal Synchrony Variable Binding (TSVB) <ref> [1] </ref> extends the representational ability of a connectionist network to include entities. The original motivation behind TSVB was for a network to represent variables and so carry out symbolic reasoning [1]. Henderson [2] argues that this extension further gives connectionist networks an inherent ability to learn generalisations across entities. <p> 1 Introduction Temporal Synchrony Variable Binding (TSVB) <ref> [1] </ref> extends the representational ability of a connectionist network to include entities. The original motivation behind TSVB was for a network to represent variables and so carry out symbolic reasoning [1]. Henderson [2] argues that this extension further gives connectionist networks an inherent ability to learn generalisations across entities. This ability allows TSVB networks to learn the kinds of regularities that arise from a compositional generative grammar, which [3] uses to describe the property of systematicity. <p> This paper begins by describing the basic idea behind TSVB, which is that with pulsing units entities can be represented using the timing of pulses. The pulsing binary-threshold units of <ref> [1] </ref> provide a connectionist model of structures and symbolic reasoning, but are not suitable for training with standard algorithms such as backpropagation [4]. <p> in the Proceedings of ICANN 1998, Skovde, Sweden. y Supported by the Engineering and Physical Sciences Research Council, UK. represent structure and learn generalisations over structural constituents with results from experiments training SSNs to parse a corpus of natural language sentences. 2 Temporal Synchrony Variable Binding Temporal Synchrony Variable Binding <ref> [1] </ref> is a connectionist technique for representing entities. <p> What allows a SSN to learn effectively is the way it combines information between the separate sequences of representations computed by its SRN components. One type of interaction is when information about entities is needed to compute the overall context. In <ref> [1] </ref> this is done with logical and and or combinations across entities, but these cannot be used with backpropagation. Continuous combination operations, such as summation, do not in practice gen-eralise to greater or fewer numbers of entities, and thus cannot be used.
Reference: [2] <author> J Henderson. </author> <title> A connectionist architecture with inherent systematicity. </title> <booktitle> Proceedings of the Eighteenth Conference of the Cognitive Science Society, </booktitle> <address> La Jolla, CA, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Temporal Synchrony Variable Binding (TSVB) [1] extends the representational ability of a connectionist network to include entities. The original motivation behind TSVB was for a network to represent variables and so carry out symbolic reasoning [1]. Henderson <ref> [2] </ref> argues that this extension further gives connectionist networks an inherent ability to learn generalisations across entities. This ability allows TSVB networks to learn the kinds of regularities that arise from a compositional generative grammar, which [3] uses to describe the property of systematicity. <p> Because different times (i.e. phases) are used to represent different entities, and the same link weights are used at every time, the same learned information is applied to every entity. This argument is used in <ref> [2] </ref> to demonstrate that TSVB networks possess inherent systematicity [3]. 3 Simple Synchrony Networks Rather than starting with an existing TSVB architecture and making it compatible with backpropagation, we choose to start with an existing backpropagation architecture and add TSVB.
Reference: [3] <author> J A Fodor and Z W Pylyshyn. </author> <title> Connectionism and cognitive architecture: a critical analysis. </title> <journal> Cognition, </journal> <volume> 28 </volume> <pages> 3-71, </pages> <year> 1988. </year>
Reference-contexts: Henderson [2] argues that this extension further gives connectionist networks an inherent ability to learn generalisations across entities. This ability allows TSVB networks to learn the kinds of regularities that arise from a compositional generative grammar, which <ref> [3] </ref> uses to describe the property of systematicity. In particular, with tasks involving language, TSVB networks will generalise information learned about one syntactic constituent to other syntactic constituents. <p> Because different times (i.e. phases) are used to represent different entities, and the same link weights are used at every time, the same learned information is applied to every entity. This argument is used in [2] to demonstrate that TSVB networks possess inherent systematicity <ref> [3] </ref>. 3 Simple Synchrony Networks Rather than starting with an existing TSVB architecture and making it compatible with backpropagation, we choose to start with an existing backpropagation architecture and add TSVB. A natural choice is the Simple Recurrent Network (SRN) architecture [5]. This architecture already handles temporal sequences.
Reference: [4] <author> D E Rumelhart, G E Hinton, and R J Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, (eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1986. </year>
Reference-contexts: The pulsing binary-threshold units of [1] provide a connectionist model of structures and symbolic reasoning, but are not suitable for training with standard algorithms such as backpropagation <ref> [4] </ref>. To develop an architecture for TSVB networks that can use backpropagation, we start with the Simple Recurrent Network (SRN) architecture [5] and extend it with units that pulse. <p> This information is combined to compute the output for each entity. As for SRNs, SSNs can be trained using Backpropagation Through Time <ref> [4] </ref>. <p> SSNs are trained using the same algorithm as can be used for SRNs, Backpropagation Through Time (BPTT) <ref> [4] </ref>. BPTT works by unfolding the network into one copy per time period, and then applying standard backpropagation to the resulting feed-forward network, using weight sharing to keep the different copies of the network the same.
Reference: [5] <author> J L Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference-contexts: The pulsing binary-threshold units of [1] provide a connectionist model of structures and symbolic reasoning, but are not suitable for training with standard algorithms such as backpropagation [4]. To develop an architecture for TSVB networks that can use backpropagation, we start with the Simple Recurrent Network (SRN) architecture <ref> [5] </ref> and extend it with units that pulse. The resulting Simple Synchrony Network (SSN) architecture has two SRN components, one standard SRN that represents overall context, and one TSVB SRN that represents a set of entities. This information is combined to compute the output for each entity. <p> A natural choice is the Simple Recurrent Network (SRN) architecture <ref> [5] </ref>. This architecture already handles temporal sequences. An SRN accepts a sequence of input patterns and produces a sequence of internal and output patterns. TSVB requires such sequences both for each entity and for the overall context.
Reference: [6] <author> J Henderson and P Lane. </author> <title> A connectionist architecture for learning to parse. </title> <booktitle> Proceedings of the Association of Computational Linguistics, </booktitle> <year> 1998. </year>
Reference-contexts: The cross-validation set consisted of 4,700 words with an average 1 The Susanne corpus is sponsored by the Economic and Social Research Council (UK) with the University of Sussex as grantholder. 2 These and some related experiments are discussed in more detail in <ref> [6] </ref>. (unrestricted) sentence length of 21.6 words. The best two of these networks were then selected for testing. They each had 20 units in their gestalt and entity hidden layers, but one had 10 units in its combination layer, and the other had 20.
Reference: [7] <author> E Charniak. </author> <title> Statistical Techniques for Natural Language Parsing. </title> <journal> AI Magazine, forthcoming. </journal>
Reference-contexts: We compare our results to the current state-of-the-art for mapping word tags to labeled parse trees, which are Probabilistic Context Free Grammars <ref> [7] </ref>. The standard evaluation of performance compares the constituents output by the model (the SSN in this case) to the constituents in the corpus, to determine the percentage of the output constituents that are correct (precision), and percentage of the correct constituents that are output (recall). <p> Reported figures for both precision and recall are around 75% <ref> [7] </ref>. We trained a range of SSNs on a training set formed from sentences of length less than thirty words (13,523 words in total). Each network was trained until the sum-squared error reached a minimum, and results obtained on a cross-validation set.
References-found: 7

