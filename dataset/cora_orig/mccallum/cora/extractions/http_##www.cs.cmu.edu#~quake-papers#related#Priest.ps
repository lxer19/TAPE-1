URL: http://www.cs.cmu.edu/~quake-papers/related/Priest.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/project/quake/public/www/triangle.research.html
Root-URL: 
Title: Algorithms for Arbitrary Precision Floating Point Arithmetic  
Author: Douglas M. Priest 
Date: March 19, 1991  
Affiliation: Department of Mathematics University of California, Berkeley  
Abstract: We present techniques which may be used to perform computations of very high accuracy using only straightforward floating point arithmetic operations of limited precision, and we prove the validity of these techniques under very general hypotheses satisfied by most implementations of floating point arithmetic. To illustrate the application of these techniques, we present an algorithm which computes the intersection of a line and a line segment. The algorithm is guaranteed to correctly decide whether an intersection exists and, if so, to produce the coordinates of the intersection point accurate to full precision. Moreover, the algorithm is usually quite efficient; only in a few cases does guaranteed accuracy necessitate an expensive computation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bohlender, G., </author> <title> Floating-Point Computation of Functions with Maximum Accuracy, </title> <journal> IEEE Trans. Comput. </journal> <volume> C-26 (1977), </volume> <pages> 621-632. </pages>
Reference-contexts: Moreover, rather than simply extending the accuracy to approximately twice the working precision, as do most of the aforementioned references, our algorithms expand upon methods developed by Bohlender <ref> [1] </ref> and Kahan [9] which compute to arbitrarily high accuracy. Below, we give algorithms for exact addition and multiplication and arbitrarily accurate division of extended precision numbers using only fixed precision floating point arithmetic operations. <p> Bohlender <ref> [1] </ref> then showed that Pichat's algorithm can be used to obtain the entire expansion for y with at most n1 passes, and he added a stopping criterion for the case where only a given number of leading components of y are desired.
Reference: [2] <author> Bohlender, G., </author> <title> What Do We Need Beyond IEEE Arithmetic?, </title> <editor> in Ullrich, C. (Ed.), </editor> <title> Computer Arithmetic and Self-Validating Numerical Methods, </title> <publisher> Academic Press, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: Many other algorithms which evaluate a sum or an inner product to working precision (i.e., yielding the first component of an expansion) have been developed; see Bohlender <ref> [2] </ref> for a survey of several approaches. Unfortunately, many of these techniques rely on special features, such as an extra wide fixed point accumulator, directed roundings, or interval arithmetic, which must be implemented in a combination of hardware and low-level software to be efficient.
Reference: [3] <author> Brent, R., </author> <title> A Fortran Multiple Precision Arithmetic Package, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 4 (1978), </volume> <pages> 57-70. </pages>
Reference-contexts: He might further surmise that if he wishes to compute a result with more accuracy than such an analysis can guarantee based on the widest precision of arithmetic supported in whatever computing environment is available, then he must instead resort to a subroutine library such as Brent's MP package <ref> [3] </ref> in order to compute with higher precision arithmetic. That both conclusions are wrong follows from the existence of techniques which allow a program to compute to arbitrarily high accuracy using only fixed precision floating point arithmetic operations.
Reference: [4] <author> Dekker, T., </author> <title> A Floating-Point Technique for Extending the Available Precision, </title> <journal> Numer. Math. </journal> <volume> 18 (1971), </volume> <pages> 224-242. </pages>
Reference-contexts: Our algorithms are based on an approach pioneered by Mtller [17], Kahan [7], Dekker <ref> [4] </ref>, Pichat [19], Linnainmaa [13, 14], and several others [10, 12], but our hypotheses are slightly more general than theirs. <p> We express the cost of these algorithms in terms of the number of fixed precision operations required. Section 2 describes floating point arithmetics and defines the criteria which an arithmetic must satisfy for our results to be valid. Lemma 1 in this section generalizes results from <ref> [4, 9, 10, 13, 17, 19] </ref>. <p> This technique forms the basis of the algorithms presented in subsequent sections. As we will note below, the proof given here generalizes various special cases which are considered in <ref> [4, 9, 10, 13, 17, 19] </ref>. <p> The arithmetic's addition is properly truncating if fl (a + b) = x whenever c = x or ab &gt; 0, but fl (a + b) = y whenever c 6= x and ab &lt; 0. We have borrowed the last definition from Dekker <ref> [4] </ref>; the reader should not confuse this term, as Linnainmaa did [13], with the notion of correctly chopping, in which fl (a + b) = x always. <p> Proof: Returning to the proof of the lemma, we are assuming that r is always a t-digit number; hence the test in line 8 never succeeds. Dekker <ref> [4] </ref> and Linnainmaa [13] consider a different variation, which also appears in Pichat [19]. Again, we state a slightly more general result which follows easily from the preceding lemma. <p> We may then multiply these split components with no error, finally adding all the partial products and renormalizing to obtain a t-digit expansion. The method of splitting components was first proposed by Dekker <ref> [4] </ref>, and we shall rely on the proof given by Linnainmaa [14]. <p> We are now ready to give the multiplication algorithm. Note that we choose to split each component of the first expansion into two parts and each component of the second expansion into three parts. Dekker <ref> [4] </ref> shows that with binary correctly rounding arithmetic it suffices to split each factor into just two parts; Linnainmaa [14] gives other criteria under which splitting into two parts is sufficient. Consequently, the following algorithm can be improved in many cases.
Reference: [5] <author> Demmel, J., </author> <title> Underflow and the Reliability of Numerical Software, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 5 (1984), </volume> <pages> 887-919. </pages>
Reference-contexts: Exact calculations can easily exceed this limit after several operations.) Nevertheless, we believe that in most applications, intermediate results may be rounded or rescaled so that underflow does not pose a serious problem. Even when underflow cannot be avoided, Demmel <ref> [5] </ref> points out that the cumulative effect of the resulting errors can often be estimated by extending the error analysis to include an underflow term whose magnitude is bounded by the underflow threshhold, if underflow is gradual, or by the ratio of this threshhold to the unit roundoff otherwise.
Reference: [6] <author> Dobkin, D., and D. Silver, </author> <title> Recipes for Geometry and Numerical Analysis, </title> <booktitle> in Proc. Fourth Annual Symposium on Computational Geometry, Association for Computing Machinery, </booktitle> <address> New York, New York, </address> <year> 1988. </year>
Reference-contexts: This problem arises in many computer graphics and computer-aided design applications; numerous authors have recognized both the importance and difficulty of solving the problem accurately (see <ref> [6, 15, 16, 18] </ref> and references therein). <p> here we could exercise greater care and reuse some of the quantities already computed in version 2, so that the overall cost will be slightly less than the sum of the costs of each version alone. (In this respect, our approach is similar to that suggested by Dobkin and Silver <ref> [6] </ref>.) Moreover, the most pessimistic quantities in the cost estimates arise from the distillation procedure, which, as Kahan [9] notes, often takes far less time than we can predict. Therefore, we suggest that the preceding algorithm is not only provably accurate and robust, but usually very efficient as well. 8.
Reference: [7] <author> Kahan, W., </author> <title> Further Remarks on Reducing Truncation Errors, </title> <journal> Comm. </journal> <note> ACM 8 (1965), </note> <month> 40. </month> <title> [8] ||, A Survey of Error Analysis, </title> <editor> in Freeman, C. V. (Ed.), </editor> <booktitle> Proc. IFIP Cong. 1971, </booktitle> <volume> vol. 2, </volume> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1972. </year> <title> [9] ||, Paradoxes in Concepts of Accuracy, </title> <booktitle> lecture notes from Joint Seminar on Issues and Directions in Scientific Computation, </booktitle> <address> U. C. Berkeley, </address> <year> 1989. </year>
Reference-contexts: Our algorithms are based on an approach pioneered by Mtller [17], Kahan <ref> [7] </ref>, Dekker [4], Pichat [19], Linnainmaa [13, 14], and several others [10, 12], but our hypotheses are slightly more general than theirs.
Reference: [10] <author> Knuth, D., </author> <booktitle> The Art of Computer Programming, vol. 2 (2nd ed.), </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1981. </year>
Reference-contexts: Our algorithms are based on an approach pioneered by Mtller [17], Kahan [7], Dekker [4], Pichat [19], Linnainmaa [13, 14], and several others <ref> [10, 12] </ref>, but our hypotheses are slightly more general than theirs. Moreover, rather than simply extending the accuracy to approximately twice the working precision, as do most of the aforementioned references, our algorithms expand upon methods developed by Bohlender [1] and Kahan [9] which compute to arbitrarily high accuracy. <p> We express the cost of these algorithms in terms of the number of fixed precision operations required. Section 2 describes floating point arithmetics and defines the criteria which an arithmetic must satisfy for our results to be valid. Lemma 1 in this section generalizes results from <ref> [4, 9, 10, 13, 17, 19] </ref>. <p> This technique forms the basis of the algorithms presented in subsequent sections. As we will note below, the proof given here generalizes various special cases which are considered in <ref> [4, 9, 10, 13, 17, 19] </ref>. <p> We have borrowed the last definition from Dekker [4]; the reader should not confuse this term, as Linnainmaa did [13], with the notion of correctly chopping, in which fl (a + b) = x always. The following simplification was established by Knuth <ref> [10] </ref> for correctly rounding arithmetic; here we prove a slightly more general result which includes properly truncating arithmetics. (Mtller [17] obtained a similar but weaker result using still different hypotheses.) Corollary 1: If the arithmetic has the property that the roundoff error of a sum is always a t-digit number (as <p> In particular, if both expansions have n components, the number of basic multiply-and-add steps is O (n 2 ). Algorithms which multiply n-digit numbers in fewer than O (n log 2 n) steps are known (see Knuth <ref> [10] </ref>), but we have elected not to use them for several reasons.
Reference: [11] <author> Kulisch, U., and W. Miranker, </author> <title> Computer Arithmetic in Theory and Practice, </title> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1981. </year>
Reference-contexts: Unfortunately, many of these techniques rely on special features, such as an extra wide fixed point accumulator, directed roundings, or interval arithmetic, which must be implemented in a combination of hardware and low-level software to be efficient. Other techniques, such as one proposed by Kulisch and Miranker <ref> [11] </ref>, require direct access to the exponent and significand fields of floating point numbers, and obtaining these quantities, even when the programming environment provides a convenient way to do so, is much too time-consuming compared to the highly optimized and streamlined floating point additions and comparisons of Pichat's algorithm, at least
Reference: [12] <author> Leuprecht, H., and W. Oberaigner, </author> <title> Parallel Algorithms for the Rounding-Exact Summation of Floating-Point Numbers, </title> <booktitle> Computing 28 (1982), </booktitle> <pages> 89-104. 24 </pages>
Reference-contexts: Our algorithms are based on an approach pioneered by Mtller [17], Kahan [7], Dekker [4], Pichat [19], Linnainmaa [13, 14], and several others <ref> [10, 12] </ref>, but our hypotheses are slightly more general than theirs. Moreover, rather than simply extending the accuracy to approximately twice the working precision, as do most of the aforementioned references, our algorithms expand upon methods developed by Bohlender [1] and Kahan [9] which compute to arbitrarily high accuracy. <p> Our method, a straightforward divide-and-conquer approach using the algorithms developed above, recursively adds successive pairs of partially distilled sums in a binary tree-like reduction. A similar approach was proposed for some parallel computer architectures by Leuprecht and Oberaigner <ref> [12] </ref>. Proposition 3: Given t-digit floating point numbers x 1 ; x 2 ; : : : ; x n . Then the following algorithm carried out with faithful arithmetic computes a t-digit expansion y = i=1 y i with m n such that y = P x i .
Reference: [13] <author> Linnainmaa, S., </author> <title> Analysis of Some Known Methods of Improving the Accuracy of Floating-Point Sums, BIT 14 (1974), 167-202. [14] ||, Software for Doubled-Precision Floating-Point Computations, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 7 (1981), </volume> <pages> 272-283. </pages>
Reference-contexts: Our algorithms are based on an approach pioneered by Mtller [17], Kahan [7], Dekker [4], Pichat [19], Linnainmaa <ref> [13, 14] </ref>, and several others [10, 12], but our hypotheses are slightly more general than theirs. <p> We express the cost of these algorithms in terms of the number of fixed precision operations required. Section 2 describes floating point arithmetics and defines the criteria which an arithmetic must satisfy for our results to be valid. Lemma 1 in this section generalizes results from <ref> [4, 9, 10, 13, 17, 19] </ref>. <p> This technique forms the basis of the algorithms presented in subsequent sections. As we will note below, the proof given here generalizes various special cases which are considered in <ref> [4, 9, 10, 13, 17, 19] </ref>. <p> We have borrowed the last definition from Dekker [4]; the reader should not confuse this term, as Linnainmaa did <ref> [13] </ref>, with the notion of correctly chopping, in which fl (a + b) = x always. <p> Proof: Returning to the proof of the lemma, we are assuming that r is always a t-digit number; hence the test in line 8 never succeeds. Dekker [4] and Linnainmaa <ref> [13] </ref> consider a different variation, which also appears in Pichat [19]. Again, we state a slightly more general result which follows easily from the preceding lemma.
Reference: [15] <author> Milenkovic, V., </author> <title> Verifiable Implementations of Geometric Algorithms Using Finite Precision Arithmetic, </title> <booktitle> Artificial Intelligence 37 (1988), </booktitle> <month> 377-401. </month> <title> [16] ||, Double Precision Geometry: A General Technique for Calculating Line and Segment Intersections Using Rounded Arithmetic, </title> <booktitle> in Proc. Thirtieth Annual Symposium on Foundations of Computer Science, </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> Calif., </address> <year> 1989. </year>
Reference-contexts: This problem arises in many computer graphics and computer-aided design applications; numerous authors have recognized both the importance and difficulty of solving the problem accurately (see <ref> [6, 15, 16, 18] </ref> and references therein).
Reference: [17] <author> Mtller, O., </author> <title> Quasi Double Precision in Floating-Point Addition, </title> <booktitle> BIT 5 (1965), </booktitle> <pages> 37-50. </pages>
Reference-contexts: Our algorithms are based on an approach pioneered by Mtller <ref> [17] </ref>, Kahan [7], Dekker [4], Pichat [19], Linnainmaa [13, 14], and several others [10, 12], but our hypotheses are slightly more general than theirs. <p> We express the cost of these algorithms in terms of the number of fixed precision operations required. Section 2 describes floating point arithmetics and defines the criteria which an arithmetic must satisfy for our results to be valid. Lemma 1 in this section generalizes results from <ref> [4, 9, 10, 13, 17, 19] </ref>. <p> This technique forms the basis of the algorithms presented in subsequent sections. As we will note below, the proof given here generalizes various special cases which are considered in <ref> [4, 9, 10, 13, 17, 19] </ref>. <p> The following simplification was established by Knuth [10] for correctly rounding arithmetic; here we prove a slightly more general result which includes properly truncating arithmetics. (Mtller <ref> [17] </ref> obtained a similar but weaker result using still different hypotheses.) Corollary 1: If the arithmetic has the property that the roundoff error of a sum is always a t-digit number (as it is in correctly rounding and properly truncating arithmetics), then lines 8 and 9 may be eliminated from the
Reference: [18] <author> Ottman, T., G. Thiemt, and C. Ullrich, </author> <title> Numerical Stability of Simple Geometric Algorithms in the Plane, </title> <editor> in Btrger, E. (Ed.), </editor> <booktitle> Computation Theory and Logic, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987. </year>
Reference-contexts: This problem arises in many computer graphics and computer-aided design applications; numerous authors have recognized both the importance and difficulty of solving the problem accurately (see <ref> [6, 15, 16, 18] </ref> and references therein). <p> Unlike Milenkovic's method, our algorithm may easily be modified to allow approximate endpoint intersections to be recognized; in this way, we obtain a consistent set of calculations and no longer need to assume non-redundancy. Ottman, et al, <ref> [18] </ref> propose essentially the same approach for the 18 intersection problem and other geometric problems. In fact, they show how to com-pute intersection points to full accuracy using only single precision arithmetic and a means of computing inner products to full precision.
Reference: [19] <author> Pichat, M., </author> <title> Correction d'une Somme en Arithmetique a Virgule Flottante, </title> <journal> Nu-mer. Math. </journal> <volume> 19 (1972), </volume> <pages> 400-406. </pages>
Reference-contexts: Our algorithms are based on an approach pioneered by Mtller [17], Kahan [7], Dekker [4], Pichat <ref> [19] </ref>, Linnainmaa [13, 14], and several others [10, 12], but our hypotheses are slightly more general than theirs. <p> We express the cost of these algorithms in terms of the number of fixed precision operations required. Section 2 describes floating point arithmetics and defines the criteria which an arithmetic must satisfy for our results to be valid. Lemma 1 in this section generalizes results from <ref> [4, 9, 10, 13, 17, 19] </ref>. <p> This technique forms the basis of the algorithms presented in subsequent sections. As we will note below, the proof given here generalizes various special cases which are considered in <ref> [4, 9, 10, 13, 17, 19] </ref>. <p> Proof: Returning to the proof of the lemma, we are assuming that r is always a t-digit number; hence the test in line 8 never succeeds. Dekker [4] and Linnainmaa [13] consider a different variation, which also appears in Pichat <ref> [19] </ref>. Again, we state a slightly more general result which follows easily from the preceding lemma. <p> Kahan has coined the term "distillation" to descibe the process by which the components of y may be obtained from the x i . 11 The first algorithm to implement distillation was given by Pichat <ref> [19] </ref>, who was only interested in obtaining the first component of y.
Reference: [20] <author> Sterbenz, P., </author> <title> Floating-Point Computation, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1974. </year>
Reference-contexts: As we will note below, the proof given here generalizes various special cases which are considered in [4, 9, 10, 13, 17, 19]. In addition to the hypothesis of faithfulness, we rely heavily on the following well-known property of floating point numbers (see Sterbenz <ref> [20] </ref>): if a and b are t-digit floating point numbers such that 1=2 a=b 2 then a b is also a t-digit floating point number. In particular, if the floating point arithmetic is faithful, fl (a b) = a b exactly.
Reference: [21] <author> Wilkinson, J., </author> <title> Rounding Errors in Algebraic Processes, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1964. </year> <month> 25 </month>
Reference-contexts: 1. Introduction "How accurate is a computed result if each intermediate quantity is computed using floating point arithmetic of a given precision?" The casual reader of Wilkinson's famous treatise <ref> [21] </ref> and similar roundoff error analyses might conclude that the most one can hope to say about the accuracy of a computation carried out in fixed precision floating point arithmetic is that the computed solution is close to the exact solution of a problem close to the given problem, where the
References-found: 17

