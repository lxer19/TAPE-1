URL: http://www.cs.utah.edu/~cs686/Previous/w97/ICS96.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/w97/
Root-URL: 
Date: May 25-28, 1996.  
Address: Philadelphia, PA,  
Note: To Appear in Proc. International Conference on Supercomputing,  Abstract  
Abstract: Memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly for streaming computations such as scientific vector processing or multimedia (de)compression. Although these computations lack the temporal locality of reference that makes caches effective, they have predictable access patterns. Since most modern DRAM components support modes that make it possible to perform some access sequences faster than others, the predictability of the stream accesses makes it possible to reorder them to get better memory performance. We describe and evaluate a Stream Memory Controller system that combines compile-time detection of streams with execution-time selection of the access order and issue. The technique is practical to implement, using existing compiler technology and requiring only a modest amount of special-purpose hardware. With our prototype system, we have observed performance improvements by factors of 13 over normal caching. 
Abstract-found: 1
Intro-found: 1
Reference: [Ale93] <author> M.J. Alexander, M.W. Bailey, B.R. Childers, J.W. Davidson, and S. Jinturkar, </author> <title> Memory Bandwidth Optimizations for Wide-Bus Machines, </title> <booktitle> Proc. 26th Hawaii International Conference on Systems Sciences, </booktitle> <month> January </month> <year> 1993, </year> <pages> pages 466-475. </pages> <note> (Incorrectly published under M.A. Alexander, et al.) </note>
Reference-contexts: In contrast, our approach attempts to exploit the existing memory bandwidth as much as possible, without increasing bandwidth requirements. It is often possible to take advantage of memory component features by reordering memory accesses at compile time. For instance, the compiler optimizations of Alexander, et al., for wide-bus machines <ref> [Ale93] </ref> have the side-effect of exploiting DRAM features like fast-page mode. Moyer unrolls loops and groups accesses to each stream, so that the cost of each DRAM page-miss can be amortized over several references to the same page [Moy93].
Reference: [Ben91] <author> M.E. Benitez and J. W. Davidson, </author> <title> Code Generation for Streaming: An Access/Execute Mechanism, </title> <booktitle> Proc. Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991, </year> <pages> pages 132-141. </pages>
Reference-contexts: Initial experiments with the system have yielded performance improvements by factors of 2-13 over normal caching. 2. THE STREAM MEMORY CONTROLLER We describe our approach based on the simplified architecture of streams (as in Benitez and Davidsons streaming algorithms <ref> [Ben91] </ref>) and arrange to transmit information about them (i.e., base address, stride, length, data size, and whether the stream is being read or written) to the hardware at run-time.
Reference: [Bro95] <author> J. Brooks, </author> <title> Single PE Optimization Techniques for the Cray T3D System, </title> <booktitle> Proc. 1st European T3D Workshop, </booktitle> <month> September, </month> <year> 1995. </year>
Reference-contexts: As Jeff Brooks explains, the rates you see in [a T3D] application depend highly on how the memory is referenced <ref> [Bro95] </ref>. This variance in performance occurs because the T3Ds DRAMs can perform some access sequences faster than others.
Reference: [Cas93] <author> Epoch Users Manual 3.1, </author> <title> Cascade Design Automation, </title> <year> 1993. </year>
Reference: [Cra95] <institution> Cray T3D Massively Parallel Processing System, Cray Research, Inc., </institution> <address> http://www.cray.com/PUBLIC/product-info/ mpp/CRAY_T3D.html, </address> <year> 1995. </year>
Reference-contexts: 1. INTRODUCTION As has become painfully obvious, processor speeds are increasing much faster than memory speeds. To illustrate the current problem, consider the multiprocessor Cray T3D <ref> [Cra95] </ref>. The peak Dynamic Random Access Memory (DRAM) read bandwidth for each 150MHz DEC Alpha processor [DEC92] of this machine is 320 Mbytes/sec, or about one 64-bit word per four clock cycles.
Reference: [Dec92] <institution> Digital Technical Journal, Digital Equipment Corporation, </institution> <note> 4(4), Special Issue, 1992, http://www.digital.com/ info/DTJ/axp-toc.html. </note>
Reference-contexts: 1. INTRODUCTION As has become painfully obvious, processor speeds are increasing much faster than memory speeds. To illustrate the current problem, consider the multiprocessor Cray T3D [Cra95]. The peak Dynamic Random Access Memory (DRAM) read bandwidth for each 150MHz DEC Alpha processor <ref> [DEC92] </ref> of this machine is 320 Mbytes/sec, or about one 64-bit word per four clock cycles. Unfortunately, the actual bandwidth may be as low as 28 Mbytes/ sec in other words, the processors can perform up to 42 instructions in the time it takes to read a single DRAM location.
Reference: [Don90] <author> J. Dongarra, J. DuCroz, I. Duff, and S. Hammerling, </author> <title> A set of Level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Further details of the design, implementation, and testing of the ASIC and daughterboard can be found elsewhere [McG94,Lan95, SMC96]. 4. PERFORMANCE presented here. Daxpy, copy, scale, and swap are from the BLAS (Basic Linear Algebra Subroutines) <ref> [Don90] </ref>, and tridiag is tridiagonal gaussian elimination, the fifth Livermore Loop [McM86]. Vaxpy denotes a vector axpy operation that occurs in matrix-vector multiplication by diagonals: a vector a times a vector x plus a vector y. For our purposes, the actual computations in these loops are unimportant.
Reference: [Goo85] <author> J.R. Goodman, J. Hsieh, K. Liou, A.R. Pleszkun, P.B. Schechter, and H.C. Young, </author> <title> PIPE: A VLSI Decoupled Architecture, </title> <booktitle> Proc. 12th International Symposium n Computer Architecture, </booktitle> <month> June </month> <year> 1985, </year> <pages> pages 20-27. </pages>
Reference: [IEEE92] <author> Memory Catches Up, </author> <title> Special Report, </title> <journal> IEEE Spectrum, </journal> <volume> 29(10) </volume> <pages> 34-53, </pages> <month> October </month> <year> 1992. </year> <title> [Int91] i860 XP Microprocessor Data Book, </title> <publisher> Intel Corporation, </publisher> <year> 1991. </year>
Reference-contexts: The order of requests strongly affects the performance of other common devices that offer speed-optimizing features (nibble-mode, static column mode, or a small amount of SRAM cache on chip) or exhibit novel organizations (Ramlink and the new synchronous DRAM designs) <ref> [IEEE92] </ref>. With an advertised bandwidth of 500 Mbyte/s, Rambus DRAMS are another interesting new memory technology [Ram92]. These bus-based systems are capable of delivering a byte of data every 2ns for a block of information up to 256 bytes long.
Reference: [Jou90] <author> N.P. Jouppi, </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> Proc. 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> pages 364-373. </pages> <institution> Also Digital Equipment Corporation, Western Research Lab, </institution> <note> Technical Note TN-14, </note> <month> March </month> <year> 1990. </year>
Reference-contexts: Jouppi performed simulation studies of stream buffers used to prefetch successive cache lines on a cache miss <ref> [Jou90] </ref>, and Palacharla and Kessler investigate the use of a set of stream buffers as a replacement for secondary cache [Pal94]. Although the latter scheme generally increased the cache hit rates of the benchmarks they simulated, these improvements were achieved at the expense of increased main memory bandwidth requirements.
Reference: [Lan95] <author> T.C. Landon, R.H. Klenke, J.H. Aylor, M.H. Salinas, and S.A. McKee, </author> <title> An Approach for Optimizing Synthesized High-Speed ASICs, </title> <booktitle> Proc. of the IEEE International ASIC Conference, </booktitle> <address> Austin, TX, </address> <month> September </month> <year> 1995, </year> <pages> pages 245-248. </pages>
Reference: [Lee93] <author> K. Lee, </author> <title> The NAS860 Library Users Manual, </title> <type> NAS Technical Report RND-93-003, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Lees subroutines to mimic Cray instructions on the Intel i860XR include another purely compile-time approach: he treats the cache as a pseudo vector register by reading vector elements in blocks (using non-caching load instructions) and then writing them to a pre-allocated portion of cache <ref> [Lee93] </ref>. Meadows describes a similar scheme for the PGI i860 compiler [Mea92], and Loshin and Budge give a general description of the technique [Los92].
Reference: [Los92] <author> D. Loshin, and D. Budge, </author> <title> Breaking the Memory Bottleneck, Parts 1 & 2, </title> <booktitle> Supercomputing Review, Jan./Feb. 1992. To Appear in Proc. International Conference on Supercomputing, </booktitle> <address> Philadelphia, PA, </address> <month> May 25-28, </month> <year> 1996. </year> <month> 8 </month>
Reference-contexts: Meadows describes a similar scheme for the PGI i860 compiler [Mea92], and Loshin and Budge give a general description of the technique <ref> [Los92] </ref>. A subset of these authors measured the time to load a single vector via Moyers and Lees schemes on a node of an iPSC/ 860, observing performance improvements between about 40% to 450% over normal caching, depending on vector stride [McK95a].
Reference: [McG94] <editor> S.W. McGee, R.H. Klenke, J.H. Aylor, and A.J. Schwab, </editor> <title> Design of a Processor Bus Interface ASIC for the Stream Memory Controller, </title> <booktitle> Proc. of the IEEE International ASIC Conference, </booktitle> <address> Rochester, NY, </address> <month> September </month> <year> 1994, </year> <pages> pages 462-465. </pages> <month> [McK95a]S.A. </month> <title> McKee, Wm.A. Wulf, Access Ordering and Memory-Conscious Cache Utilization, </title> <booktitle> Proc. First International Symposium on High Performance Computer Architecture, </booktitle> <address> Raleigh, NC, </address> <month> January </month> <year> 1995, </year> <pages> pages 253-262. </pages> <month> [McK95b]S.A. </month> <title> McKee, Maximizing Memory Bandwidth for Streamed Computations, </title> <type> Ph.D. Dissertation, </type> <institution> University of Virginia, Department of Computer Science, </institution> <month> May </month> <year> 1995. </year> <note> http:// www.cs.virginia.edu/research/techrep.html. </note>
Reference: [McM86] <author> F.H. McMahon, </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, UCRL-53745, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: The processor references the next element of a stream via the memory-mapped register representing the corresponding FIFO head. This memory mapping avoids the need to modify the processors instruction set to address these registers. Figure 2 illustrates the SMC programming model for tridiagonal elimination, one of the Livermore Loops <ref> [McM86] </ref>. 3. EXPERIMENTAL IMPLEMENTATION In order to demonstrate the viability of dynamic access ordering, we have developed an experimental Stream Memory Controller system. This proof-of-concept version is implemented as a single, semi-custom VLSI integrated circuit interfaced to an Intel i860 host processor [Int91]. <p> Further details of the design, implementation, and testing of the ASIC and daughterboard can be found elsewhere [McG94,Lan95, SMC96]. 4. PERFORMANCE presented here. Daxpy, copy, scale, and swap are from the BLAS (Basic Linear Algebra Subroutines) [Don90], and tridiag is tridiagonal gaussian elimination, the fifth Livermore Loop <ref> [McM86] </ref>. Vaxpy denotes a vector axpy operation that occurs in matrix-vector multiplication by diagonals: a vector a times a vector x plus a vector y. For our purposes, the actual computations in these loops are unimportant.
Reference: [Mea92] <author> L. Meadows, S. Nakamoto, and V. Schuster, </author> <title> A Vectorizing Software Pipelining Compiler for LIW and Superscalar Architectures, </title> <booktitle> Proc. RISC92, </booktitle> <pages> pages 331-343. </pages>
Reference-contexts: Meadows describes a similar scheme for the PGI i860 compiler <ref> [Mea92] </ref>, and Loshin and Budge give a general description of the technique [Los92].
Reference: [Men93] <author> System-1076, </author> <title> Quicksim II Users Manual, </title> <institution> Mentor Graphics Corporation, </institution> <year> 1993. </year>
Reference: [Moy93] <author> S.A. Moyer, </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. Dissertation, </type> <institution> University of Virginia, Department of Computer Science Technical Report CS-93-18, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Moyer unrolls loops and groups accesses to each stream, so that the cost of each DRAM page-miss can be amortized over several references to the same page <ref> [Moy93] </ref>. Lees subroutines to mimic Cray instructions on the Intel i860XR include another purely compile-time approach: he treats the cache as a pseudo vector register by reading vector elements in blocks (using non-caching load instructions) and then writing them to a pre-allocated portion of cache [Lee93].
Reference: [Pal94] <author> S. Palacharla and R.E. Kessler, </author> <title> Evaluating Stream Buffers as a Secondary Cache Replacement, </title> <booktitle> Proc. 21st International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1994, </year> <pages> pages 24-33. </pages>
Reference-contexts: Jouppi performed simulation studies of stream buffers used to prefetch successive cache lines on a cache miss [Jou90], and Palacharla and Kessler investigate the use of a set of stream buffers as a replacement for secondary cache <ref> [Pal94] </ref>. Although the latter scheme generally increased the cache hit rates of the benchmarks they simulated, these improvements were achieved at the expense of increased main memory bandwidth requirements. In contrast, our approach attempts to exploit the existing memory bandwidth as much as possible, without increasing bandwidth requirements.
Reference: [Pal95] <author> S. Palacharla and R.E. Kessler, </author> <title> Code Restructuring to Exploit Page Mode and Read-Ahead Features of the Cray T3D, Cray Research Internal Report, </title> <month> February </month> <year> 1995. </year>
Reference-contexts: Another option is to augment the purely compile-time approach with a hardware assist. Palacharla and Kessler <ref> [Pal95] </ref> investigate code restructuring techniques to exploit fast-page mode DRAMs via a hardware read-ahead mechanism on the Cray T3D.
Reference: [Qui91] <author> R. Quinnell, </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991. </year>
Reference: [Ram92] <institution> Architectural Overview, Rambus Inc., Mountain View, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: With an advertised bandwidth of 500 Mbyte/s, Rambus DRAMS are another interesting new memory technology <ref> [Ram92] </ref>. These bus-based systems are capable of delivering a byte of data every 2ns for a block of information up to 256 bytes long.
Reference: [Smi87] <author> J.E. Smith, G.E. Dermer, </author> <title> B.D. Vanderwarn, S.D. Klinger, C.M. Roszewski, D.L. Fowler, and D.R. Scidmore, The ZS-1 Central Processor, </title> <booktitle> Proc. 2nd International Conference on Architectural Support for Programming Languages and Systems (ASPLOS-II), </booktitle> <month> Oct. </month> <year> 1987, </year> <pages> pages 199-204. </pages>
Reference: [SMC96] <institution> SMC home page, </institution> <address> http://www.cs.virginia.edu/~wm/ smc.html. </address>
Reference-contexts: More intelligent schemes are required to achieve uniformly good performance on computations involving streams with strides that do not hit all memory banks [McK95b]. Further details of the design, implementation, and testing of the ASIC and daughterboard can be found elsewhere <ref> [McG94,Lan95, SMC96] </ref>. 4. PERFORMANCE presented here. Daxpy, copy, scale, and swap are from the BLAS (Basic Linear Algebra Subroutines) [Don90], and tridiag is tridiagonal gaussian elimination, the fifth Livermore Loop [McM86].
Reference: [Wul92] <author> Wm.A. Wulf, </author> <title> Evaluation of the WM Architecture, </title> <booktitle> Proc. 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992, </year> <pages> pages 382-390. </pages>
Reference-contexts: Our confidence that the SMC could be implemented efficiently was based on the fact that similar designs have been built. For instance, the organization of the SBU is almost identical to the stream units of the WM architecture <ref> [Wul92] </ref>, and the SMC may be thought of as a special case of a decoupled access-execute architecture [Goo85,Smi87]. vaxpy vaxpy More complex stream buffers have been evaluated in other contexts.
References-found: 25

