URL: http://www-csag.cs.uiuc.edu/papers/pakin-ms.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Title: THE IMPACT OF MESSAGE TRAFFIC ON MULTICOMPUTER MEMORY HIERARCHY PERFORMANCE  
Author: BY SCOTT DOV PAKIN 
Degree: 1992 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1995 Urbana, Illinois  
Affiliation: B.S., Carnegie Mellon University,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Arvind and K. Ekanadham. </author> <title> Future scientific programming on parallel machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 460-493, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: We further hypothesize that in the future, multicomputer applications will communicate more than current multicomputer applications. This hypothesis is based on two factors: First, if fine-grained parallel programming languages such as Id <ref> [1] </ref>, Concurrent Aggregates [6], and Multilisp [20] gain more acceptance, there will be an increased reliance on communication. And second, as massively-parallel computers become commonplace, each mode has at least the potential for increased communication (i.e. there are more nodes with which to communicate). <p> bus | False True ! Write buffer gets the bus | True False ! Cache gets the bus Cache True True ! Write buffer gets the bus Write buffer True True ! Cache gets the bus end if write buffer has the bus then Output A, F , and D <ref> [0; 1] </ref> to the bus Output A, F , and D [2; 3] to the bus Output Done to the write buffer else Output A and F to the bus Output Done to the cache Input D 0 [0; 1] from the bus Input D 0 [2; 3] from the bus <p> write buffer has the bus then Output A, F , and D <ref> [0; 1] </ref> to the bus Output A, F , and D [2; 3] to the bus Output Done to the write buffer else Output A and F to the bus Output Done to the cache Input D 0 [0; 1] from the bus Input D 0 [2; 3] from the bus Output D 0 to the cache end 70 A.5 Bus repeat forever Input a (physical) address, A, read/write flag, F , data words, D, and mask, M Output A, F , D, and M to the bus interface, <p> + 1], and mask, M from the bus if F = Read then Output D [i; i + 1] to the bus end end A.7 Network Interface A.7.1 Cache-Network Simulator when data is on the bus do Input a (physical) address, A, read/write flag, F , and data words, D <ref> [0; 1] </ref> from the bus if A = A reg then if F = Write then &gt; Send data from the bus to the router dest D [0] L D [1] Input data words D [2; 3] from the bus and ignore Output dest to the router Output L to the <p> when data is on the bus do Input a (physical) address, A, read/write flag, F , and data words, D [0; 1] from the bus if A = A reg then if F = Write then &gt; Send data from the bus to the router dest D [0] L D <ref> [1] </ref> Input data words D [2; 3] from the bus and ignore Output dest to the router Output L to the router for i 0 to L 1 do Input data words, D [i mod 2; i mod 2 + 1] from the bus Output D [i mod 2] to the <p> 2] to the router Output D [i mod 2 + 1] to the router end Output Done to the router else &gt; Send data from the message FIFO to the bus for i 0 to 3 do wait until R 6= ; Dequeue R into D [i] end Output D <ref> [0; 1] </ref> to the bus Output D [2; 3] to the bus end end when data is at the router do &gt; Buffer data read from the router Output an interrupt to the CPU while data is at the router do Input data word D 0 from the router Enqueue D <p> queue R if R is full then Output Wait to the router wait until R is no longer full end Output Continue to the router end A.7.2 Primary Memory-Network Simulator when data is on the bus do Input a (physical) address, A, read/write flag, F , and data words, D <ref> [0; 1] </ref> from the bus Input data words D [2; 3] from the bus if A = A reg and F = Write then let op D [1] dest D [2] L D [3] if op = Send then &gt; Send data from memory to the router Output bus request to <p> Primary Memory-Network Simulator when data is on the bus do Input a (physical) address, A, read/write flag, F , and data words, D [0; 1] from the bus Input data words D [2; 3] from the bus if A = A reg and F = Write then let op D <ref> [1] </ref> dest D [2] L D [3] if op = Send then &gt; Send data from memory to the router Output bus request to bus interface Input Have-Bus from bus interface Output dest to the router Output L to the router for i 0 to L 1 by 32 do Output <p> from memory to the router Output bus request to bus interface Input Have-Bus from bus interface Output dest to the router Output L to the router for i 0 to L 1 by 32 do Output Read A mem + i to memory via the bus Input data words D <ref> [0; 1] </ref> from memory via the bus Input data words D [2; 3] from memory via the bus Output D [0], D [1], D [2], and D [3] to the router end Output Done to the router Output Done to the bus interface else 72 &gt; Send data to memory from <p> L to the router for i 0 to L 1 by 32 do Output Read A mem + i to memory via the bus Input data words D [0; 1] from memory via the bus Input data words D [2; 3] from memory via the bus Output D [0], D <ref> [1] </ref>, D [2], and D [3] to the router end Output Done to the router Output Done to the bus interface else 72 &gt; Send data to memory from the router Output bus request to bus interface Input Have-Bus from bus interface Dequeue dest from R Dequeue L from R for <p> Done to the bus interface else 72 &gt; Send data to memory from the router Output bus request to bus interface Input Have-Bus from bus interface Dequeue dest from R Dequeue L from R for i 0 to L 1 by 32 do Dequeue R four times|into D [0], D <ref> [1] </ref>, D [2], and D [3], respectively Output Write A mem + i D [0] D [1] to memory via the bus Output Write A mem + i + 16 D [2] D [3] to memory via the bus Output Done to the bus interface end end when data is at <p> bus request to bus interface Input Have-Bus from bus interface Dequeue dest from R Dequeue L from R for i 0 to L 1 by 32 do Dequeue R four times|into D [0], D <ref> [1] </ref>, D [2], and D [3], respectively Output Write A mem + i D [0] D [1] to memory via the bus Output Write A mem + i + 16 D [2] D [3] to memory via the bus Output Done to the bus interface end end when data is at the router do &gt; Buffer data read from the router Output an interrupt to the CPU
Reference: [2] <author> Thomas Ball and James R. Larus. </author> <title> Optimally profiling and tracing programs. </title> <type> Computer Sciences Technical Report 1031, </type> <institution> University of Wisconsin-Madison, </institution> <address> 1210 West Dayton St.; Madison, WI 53706, </address> <month> September </month> <year> 1991. </year> <note> Available from ftp://ftp.cs.wisc.edu/tech-reports/reports/91/tr1031.ps.Z. </note>
Reference-contexts: We use the same trace files for both the uniprocessor and multicomputer simulators|to do otherwise would confuse effects caused by a node architecture and artifacts of an application's implementation. Trace data was generated using QPT <ref> [2, 26] </ref>. Information about the applications is given in Table 3.2. The column labelled "Memory references" represents the number of LOADs and STOREs issued dynamically. "Working set size" was calculated by: 1. Running the memory traces through dineroIII 5 using various-sized fully-associative caches with LRU replacement 2. <p> True False ! Cache gets the bus Cache True True ! Write buffer gets the bus Write buffer True True ! Cache gets the bus end if write buffer has the bus then Output A, F , and D [0; 1] to the bus Output A, F , and D <ref> [2; 3] </ref> to the bus Output Done to the write buffer else Output A and F to the bus Output Done to the cache Input D 0 [0; 1] from the bus Input D 0 [2; 3] from the bus Output D 0 to the cache end 70 A.5 Bus repeat <p> F , and D [0; 1] to the bus Output A, F , and D <ref> [2; 3] </ref> to the bus Output Done to the write buffer else Output A and F to the bus Output Done to the cache Input D 0 [0; 1] from the bus Input D 0 [2; 3] from the bus Output D 0 to the cache end 70 A.5 Bus repeat forever Input a (physical) address, A, read/write flag, F , data words, D, and mask, M Output A, F , D, and M to the bus interface, memory controller, and network interface (cache-network) or DMA <p> bus do Input a (physical) address, A, read/write flag, F , and data words, D [0; 1] from the bus if A = A reg then if F = Write then &gt; Send data from the bus to the router dest D [0] L D [1] Input data words D <ref> [2; 3] </ref> from the bus and ignore Output dest to the router Output L to the router for i 0 to L 1 do Input data words, D [i mod 2; i mod 2 + 1] from the bus Output D [i mod 2] to the router Output D [i mod <p> mod 2 + 1] to the router end Output Done to the router else &gt; Send data from the message FIFO to the bus for i 0 to 3 do wait until R 6= ; Dequeue R into D [i] end Output D [0; 1] to the bus Output D <ref> [2; 3] </ref> to the bus end end when data is at the router do &gt; Buffer data read from the router Output an interrupt to the CPU while data is at the router do Input data word D 0 from the router Enqueue D 0 onto queue R if R is <p> to the router wait until R is no longer full end Output Continue to the router end A.7.2 Primary Memory-Network Simulator when data is on the bus do Input a (physical) address, A, read/write flag, F , and data words, D [0; 1] from the bus Input data words D <ref> [2; 3] </ref> from the bus if A = A reg and F = Write then let op D [1] dest D [2] L D [3] if op = Send then &gt; Send data from memory to the router Output bus request to bus interface Input Have-Bus from bus interface Output dest <p> when data is on the bus do Input a (physical) address, A, read/write flag, F , and data words, D [0; 1] from the bus Input data words D [2; 3] from the bus if A = A reg and F = Write then let op D [1] dest D <ref> [2] </ref> L D [3] if op = Send then &gt; Send data from memory to the router Output bus request to bus interface Input Have-Bus from bus interface Output dest to the router Output L to the router for i 0 to L 1 by 32 do Output Read A mem <p> Input Have-Bus from bus interface Output dest to the router Output L to the router for i 0 to L 1 by 32 do Output Read A mem + i to memory via the bus Input data words D [0; 1] from memory via the bus Input data words D <ref> [2; 3] </ref> from memory via the bus Output D [0], D [1], D [2], and D [3] to the router end Output Done to the router Output Done to the bus interface else 72 &gt; Send data to memory from the router Output bus request to bus interface Input Have-Bus from <p> the router for i 0 to L 1 by 32 do Output Read A mem + i to memory via the bus Input data words D [0; 1] from memory via the bus Input data words D [2; 3] from memory via the bus Output D [0], D [1], D <ref> [2] </ref>, and D [3] to the router end Output Done to the router Output Done to the bus interface else 72 &gt; Send data to memory from the router Output bus request to bus interface Input Have-Bus from bus interface Dequeue dest from R Dequeue L from R for i 0 <p> the bus interface else 72 &gt; Send data to memory from the router Output bus request to bus interface Input Have-Bus from bus interface Dequeue dest from R Dequeue L from R for i 0 to L 1 by 32 do Dequeue R four times|into D [0], D [1], D <ref> [2] </ref>, and D [3], respectively Output Write A mem + i D [0] D [1] to memory via the bus Output Write A mem + i + 16 D [2] D [3] to memory via the bus Output Done to the bus interface end end when data is at the router <p> L from R for i 0 to L 1 by 32 do Dequeue R four times|into D [0], D [1], D <ref> [2] </ref>, and D [3], respectively Output Write A mem + i D [0] D [1] to memory via the bus Output Write A mem + i + 16 D [2] D [3] to memory via the bus Output Done to the bus interface end end when data is at the router do &gt; Buffer data read from the router Output an interrupt to the CPU while data is at the router do Input data word D 0 from the router
Reference: [3] <author> Carl J. Beckmann. CARL: </author> <title> An architecture simulation language. </title> <type> Report 1066, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, 305 Talbot, 104 South Wright St.; Urbana, </institution> <address> IL 61801-2932, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Nodes are simulated at the register-transfer level, which means that we modelled the functionality of the node's components without modelling their exact implementation. 3.2.2 Simulation Tools The simulators were written using an event-driven simulation engine called Parsim [4] and a simulation language called CARL <ref> [3] </ref>. <p> True False ! Cache gets the bus Cache True True ! Write buffer gets the bus Write buffer True True ! Cache gets the bus end if write buffer has the bus then Output A, F , and D [0; 1] to the bus Output A, F , and D <ref> [2; 3] </ref> to the bus Output Done to the write buffer else Output A and F to the bus Output Done to the cache Input D 0 [0; 1] from the bus Input D 0 [2; 3] from the bus Output D 0 to the cache end 70 A.5 Bus repeat <p> F , and D [0; 1] to the bus Output A, F , and D <ref> [2; 3] </ref> to the bus Output Done to the write buffer else Output A and F to the bus Output Done to the cache Input D 0 [0; 1] from the bus Input D 0 [2; 3] from the bus Output D 0 to the cache end 70 A.5 Bus repeat forever Input a (physical) address, A, read/write flag, F , data words, D, and mask, M Output A, F , D, and M to the bus interface, memory controller, and network interface (cache-network) or DMA <p> bus do Input a (physical) address, A, read/write flag, F , and data words, D [0; 1] from the bus if A = A reg then if F = Write then &gt; Send data from the bus to the router dest D [0] L D [1] Input data words D <ref> [2; 3] </ref> from the bus and ignore Output dest to the router Output L to the router for i 0 to L 1 do Input data words, D [i mod 2; i mod 2 + 1] from the bus Output D [i mod 2] to the router Output D [i mod <p> mod 2 + 1] to the router end Output Done to the router else &gt; Send data from the message FIFO to the bus for i 0 to 3 do wait until R 6= ; Dequeue R into D [i] end Output D [0; 1] to the bus Output D <ref> [2; 3] </ref> to the bus end end when data is at the router do &gt; Buffer data read from the router Output an interrupt to the CPU while data is at the router do Input data word D 0 from the router Enqueue D 0 onto queue R if R is <p> to the router wait until R is no longer full end Output Continue to the router end A.7.2 Primary Memory-Network Simulator when data is on the bus do Input a (physical) address, A, read/write flag, F , and data words, D [0; 1] from the bus Input data words D <ref> [2; 3] </ref> from the bus if A = A reg and F = Write then let op D [1] dest D [2] L D [3] if op = Send then &gt; Send data from memory to the router Output bus request to bus interface Input Have-Bus from bus interface Output dest <p> on the bus do Input a (physical) address, A, read/write flag, F , and data words, D [0; 1] from the bus Input data words D [2; 3] from the bus if A = A reg and F = Write then let op D [1] dest D [2] L D <ref> [3] </ref> if op = Send then &gt; Send data from memory to the router Output bus request to bus interface Input Have-Bus from bus interface Output dest to the router Output L to the router for i 0 to L 1 by 32 do Output Read A mem + i to <p> Input Have-Bus from bus interface Output dest to the router Output L to the router for i 0 to L 1 by 32 do Output Read A mem + i to memory via the bus Input data words D [0; 1] from memory via the bus Input data words D <ref> [2; 3] </ref> from memory via the bus Output D [0], D [1], D [2], and D [3] to the router end Output Done to the router Output Done to the bus interface else 72 &gt; Send data to memory from the router Output bus request to bus interface Input Have-Bus from <p> i 0 to L 1 by 32 do Output Read A mem + i to memory via the bus Input data words D [0; 1] from memory via the bus Input data words D [2; 3] from memory via the bus Output D [0], D [1], D [2], and D <ref> [3] </ref> to the router end Output Done to the router Output Done to the bus interface else 72 &gt; Send data to memory from the router Output bus request to bus interface Input Have-Bus from bus interface Dequeue dest from R Dequeue L from R for i 0 to L 1 <p> else 72 &gt; Send data to memory from the router Output bus request to bus interface Input Have-Bus from bus interface Dequeue dest from R Dequeue L from R for i 0 to L 1 by 32 do Dequeue R four times|into D [0], D [1], D [2], and D <ref> [3] </ref>, respectively Output Write A mem + i D [0] D [1] to memory via the bus Output Write A mem + i + 16 D [2] D [3] to memory via the bus Output Done to the bus interface end end when data is at the router do &gt; Buffer <p> R for i 0 to L 1 by 32 do Dequeue R four times|into D [0], D [1], D [2], and D <ref> [3] </ref>, respectively Output Write A mem + i D [0] D [1] to memory via the bus Output Write A mem + i + 16 D [2] D [3] to memory via the bus Output Done to the bus interface end end when data is at the router do &gt; Buffer data read from the router Output an interrupt to the CPU while data is at the router do Input data word D 0 from the router Enqueue D
Reference: [4] <author> John Bruner. </author> <title> Parsim user interface reference manual. </title> <type> Report 1002, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, 305 Talbot, 104 South Wright St.; Urbana, </institution> <address> IL 61801-2932, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: Nodes are simulated at the register-transfer level, which means that we modelled the functionality of the node's components without modelling their exact implementation. 3.2.2 Simulation Tools The simulators were written using an event-driven simulation engine called Parsim <ref> [4] </ref> and a simulation language called CARL [3].
Reference: [5] <author> K. M. Chandy and J. Misra. </author> <title> Asynchronous distributed simulation via a sequence of parallel computations. </title> <journal> Communications of the ACM, </journal> <volume> 24(11) </volume> <pages> 198-206, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: Like fpppp, mdljsp2 is intended to be representative of quantum chemistry applications. We used the "short" input set. pthor Given the description of a digital logic circuit and its input, pthor simulates the operation of that circuit using a variant of the Chandy-Misra distributed-time algorithm <ref> [5] </ref>. As an event-driven simulator, pthor has much in common with Parsim (Section 3.2.2); both simulate arbitrarily complex functional blocks interconnected by wires. But while Parsim's clock is global, pthor's is distributed (an aggressive optimization). That 28 is, the various functional blocks do not generally observe the same simu-lated time.
Reference: [6] <author> Andrew A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: We further hypothesize that in the future, multicomputer applications will communicate more than current multicomputer applications. This hypothesis is based on two factors: First, if fine-grained parallel programming languages such as Id [1], Concurrent Aggregates <ref> [6] </ref>, and Multilisp [20] gain more acceptance, there will be an increased reliance on communication. And second, as massively-parallel computers become commonplace, each mode has at least the potential for increased communication (i.e. there are more nodes with which to communicate).
Reference: [7] <author> Lynn Choi and Andrew A. Chien. </author> <title> Integrating networks and memory hierarchies in a mul-ticomputer node architecture. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/ipps94.ps. </note>
Reference-contexts: We have already begun one such effort by starting to examine the relationship of network traffic to cache misses in the proposed DI-multicomputer <ref> [7, 8] </ref>, in which messages can be routed either directly from registers or memory, and the programmer or compiler chooses which is more appropriate on a per-message basis.
Reference: [8] <author> Lynn Choi and Andrew A. Chien. </author> <title> The design and performance evaluation of the DI-multicomputer. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 199x. Submitted for publication. </note>
Reference-contexts: We have already begun one such effort by starting to examine the relationship of network traffic to cache misses in the proposed DI-multicomputer <ref> [7, 8] </ref>, in which messages can be routed either directly from registers or memory, and the programmer or compiler chooses which is more appropriate on a per-message basis.
Reference: [9] <author> SPEC Steering Committee. </author> <title> DESCR.n files in SPEC distribution, </title> <month> January </month> <year> 1992. </year>
Reference-contexts: Running the memory traces through dineroIII 5 using various-sized fully-associative caches with LRU replacement 2. Plotting the miss rate against the cache size 3. Taking the knee in each curve as the corresponding application's working set size. Descriptions of the traced applications <ref> [9, 15, 31] </ref> follow: barnes barnes uses Barnes-Hut's hierarchical N -body algorithm to model point-masses exerting gravitational forces on each other in three dimensions. The algorithm specifies that clusters of distant point-masses are treated as a single point-mass, which significantly reduces the amount of necessary computation.
Reference: [10] <institution> Committee on Information and Communication (CIC) of the National Science and Technology Council (NSTC). High performance computing and communications: </institution> <note> Technology for the national information infrastructure (Supplement to the president's fiscal year 1995 budget). Available from http://www.hpcc.gov/blue95, 1994. </note>
Reference-contexts: Realizing how important those applications|and therefore, high-performance computing|are to all sectors, the U.S. government recently formed the Federal High Performance Computing and Communications Program (HPCC), whose charter includes "extend [ing] U.S. technological leadership in high performance computing and computer communications" <ref> [10] </ref>. Traditionally, high-performance computing meant fast uniprocessors|computers with a single, powerful processor and a high-speed memory system. However, due to physical limitations, such as the speed of light, it has become increasingly difficult to produce faster uniprocessors. <p> However, due to physical limitations, such as the speed of light, it has become increasingly difficult to produce faster uniprocessors. To achieve their goal of developing high-performance computers capable of sustaining over a trillion operations per seconds (teraops), the HPCC promotes scalable, parallel computers <ref> [10] </ref>. In addition to being a more promising technology in terms of performance, parallel computers have the additional advantage of comparatively low cost, as|unlike high-performance uniprocessors| they are generally built from commodity components. <p> Traces of more "realistic" (i.e. long-running and computationally-intensive) scientific applications would help researchers 1 Primarily SPARCstation 10s and 2s 2 Including time spent outside of the memory hierarchy 63 estimate the computation, communication, and memory demands of large applications, such as those on the scale of Grand Challenge problems <ref> [10] </ref>. It would also be beneficial to trace parallel applications (including both memory and communication) to increase the simulations' correspondence to reality, although doing so comes at the expense of control over simulation parameters. In addition, traces could be augmented with operating system or run-time system references.
Reference: [11] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year> <month> 92 </month>
Reference-contexts: In this thesis, "message rate" refers to the average number of incoming+outgoing words per processor clock. Messages are introduced with a Poisson distribution. That is, the time intervals between messages are uniformly distributed while maintaining a given message rate. Data derived from <ref> [11] </ref> (Appendix B) show that for the machines and applications examined in that paper, the message rate varies from 0.00004 to 0.00674 64-bit words per clock, with a mean of 0.00114. However, those numbers include the time spent executing non-memory instructions, while our simulations execute exclusively memory instructions. <p> However, those numbers include the time spent executing non-memory instructions, while our simulations execute exclusively memory instructions. In other words, the numbers calculated from <ref> [11] </ref> are unrealistically low for a memory-only study. Hence, we adjusted our range of message rates upward. The message rates used emphasize "interesting" regions of the data|where the curves are not flat|and were selected based on preliminary results. <p> Because the primary memory-network system uses DMA to transport messages directly from memory to the network interface, no additional cache misses are generated for message sends. 48 49 50 4.4 Misses versus Message Size Message-passing applications employ a large variety of message lengths <ref> [11] </ref>. Therefore, we investigated the effect of message size on cache performance. We expect message size to impact the cache for two reasons: First, a small number of long messages exhibits more spatial locality than a large number of short messages. <p> In contrast, the applications studied in <ref> [11] </ref> range in duration from 180-17,520 seconds. 2 Note, however, that Cypher, et. al.'s study is not simulation-based; their measurements are taken from monitors on actual multicomputers and were therefore gathered in real time.
Reference: [12] <author> William J. Dally, J. A. Stuart Fiske, John S. Keen, Richard A. Lethin, Michael D. Noakes, Peter R. Nuth, Roy E. Davison, and Gregory A. Fyler. </author> <title> The message-driven processor. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 23-39, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Because chip area is limited, fitting the CPU, network interface, router, and primary memory on a single chip implies that each component is generally less powerful than it would have been, given the entire chip. Like the Mosaic C, the J-Machine <ref> [12] </ref> is a single-chip fine-grained machine. What distinguishes the J-Machine from a memory hierarchy/network standpoint is that while messages can be sent and received using primary memory, the can additionally be sent directly from CPU registers (Figure 2.10).
Reference: [13] <institution> Digital Equipment Corporation. Alpha Architecture Handbook, </institution> <year> 1992. </year> <title> Order number EC-H1689-10. </title>
Reference-contexts: The CPU, cache, write buffer, and bus are based specifically on the DECchip 21064-AA implementation [14] of the Alpha architecture <ref> [13] </ref>. In addition to the modules shown in Figure 3.7, we simulate a network fringe (Figure 3.8). The network fringe embodies the actions of the remainder of the multicomputer (everything external to the node). <p> 8i Output W to the network interface end else &gt; Ordinary memory operations if the trace data queue 6= ; then Dequeue an address read/write pair Output to memory the address and read/write flag if read the address, read/write flag, and data if write else Output a memory barrier (see <ref> [13] </ref>) done True end end while not done A.1.2 Primary Memory-Network Simulator repeat &gt; Tell DMA to move a message from network to memory upon arrival if an interrupt occured then let L recv message length (input from the DMA controller) B base memory address at which to store the message <p> recv destination=dest to the DMA controller's registers end else &gt; Ordinary memory operations if the trace data queue 6= ; then Dequeue an address read/write pair Output to memory the address and read/write flag if read the address, read/write flag, and data if write else Output a memory barrier (see <ref> [13] </ref>) done True end end while not done A.2 Cache when a (physical) address, A, read/write flag, F , and data word, D arrive from the CPU do if A is cacheable then Split the 31-bit A into a 21 tag z -| - index 2 offset triple if C [index]
Reference: [14] <institution> Digital Equipment Corporation, Maynard, </institution> <address> MA. </address> <note> DECchip 21064-AA Microprocessor Hardware Reference Manual, 1st edition, </note> <month> October </month> <year> 1992. </year> <title> Order number EC-N0079-72. </title>
Reference-contexts: Table 1.1 lists the size of each level of the memory hierarchy, measured in kilobytes (KB), and the corresponding latency, measured in nanoseconds (ns). Data used in the table is based on that in <ref> [14, 27] </ref> (160 megahertz (MHz) DECchip 21064 CPU, maximum memory/cache sizes, bus latency included). <p> The CPU, cache, write buffer, and bus are based specifically on the DECchip 21064-AA implementation <ref> [14] </ref> of the Alpha architecture [13]. In addition to the modules shown in Figure 3.7, we simulate a network fringe (Figure 3.8). The network fringe embodies the actions of the remainder of the multicomputer (everything external to the node). <p> The write buffer module is implemented as a four-line-deep queue of cache lines. A mask bit corresponds to each word in the write buffer and specifies whether the word is valid (modified) or invalid (unmodified). Multiple writes to a location are merged in the write buffer. (See <ref> [14] </ref> for further details.) The bus module models a non-pipelined bus with 128 data and 32 address lines, similar to the 21064-AA's. The bus interface module arbitrates fairly between the cache and the write buffer for control of the bus. <p> In contrast, sequences of short messages generally do not. Because of its spatial locality, a single long message should displace fewer cache lines than an equivalent volume of short messages, and is therefore expected to be less harmful to cache performance. 6 Based on data in <ref> [14, 19] </ref> and assuming a split 32 KB cache and an average execution time of one cycle per instruction (CPI) for non-memory instructions 30 To quantify the impact of short and long messages, we simulated different mixes of two message lengths (short and long).
Reference: [15] <author> Kaivalya M. Dixit. </author> <title> CINT2.0 and CFP2.0 benchmark descriptions. </title> <journal> SPEC Newsletter, </journal> <volume> 3(4) </volume> <pages> 18-21, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Therefore, we use memory traces from several different applications. We selected application traces based on the following criteria: * scientific nature * programming language * locality * size The applications are taken from well-known benchmark suites (SPEC <ref> [15, 16] </ref> and SPLASH [31]). They are written in different languages (C and Fortran). They exhibit a range of localities. And they each make enough memory references to provide meaningful data, but not so many as to take an inordinate amount of time to simulate. <p> Running the memory traces through dineroIII 5 using various-sized fully-associative caches with LRU replacement 2. Plotting the miss rate against the cache size 3. Taking the knee in each curve as the corresponding application's working set size. Descriptions of the traced applications <ref> [9, 15, 31] </ref> follow: barnes barnes uses Barnes-Hut's hierarchical N -body algorithm to model point-masses exerting gravitational forces on each other in three dimensions. The algorithm specifies that clusters of distant point-masses are treated as a single point-mass, which significantly reduces the amount of necessary computation.
Reference: [16] <author> Kaivalya M. Dixit. </author> <title> New CPU benchmark suites from SPEC. </title> <booktitle> In Thirty-Seventh IEEE Computer Society International Conference, </booktitle> <pages> pages 305-310, </pages> <month> Spring </month> <year> 1992. </year>
Reference-contexts: Therefore, we use memory traces from several different applications. We selected application traces based on the following criteria: * scientific nature * programming language * locality * size The applications are taken from well-known benchmark suites (SPEC <ref> [15, 16] </ref> and SPLASH [31]). They are written in different languages (C and Fortran). They exhibit a range of localities. And they each make enough memory references to provide meaningful data, but not so many as to take an inordinate amount of time to simulate.
Reference: [17] <author> C. Leiserson et al. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992. </year> <note> Available from ftp://cmns.think.com/doc/Papers/net.ps.Z. </note>
Reference-contexts: The former scheme|used, for example, in the CM-5|is optimized for low-latency short messages, while the latter|used, for example, in the Paragon|is optimized for high-bandwidth long messages. Table 5.1 confirms that claim using data obtained from <ref> [17] </ref> and [35]. (The latter reference cites the performance of a Paragon running SUNMOS/PUMA, an operating system and software architecture geared towards more efficient messaging than is provided with the software bundled with the Paragon.) Clock frequencies listed are for the processor clock. Clock freq.
Reference: [18] <author> M. J. Flynn. </author> <title> Very high-speed computing systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 54(12) </volume> <pages> 1901-1909, </pages> <year> 1966. </year>
Reference-contexts: The parallel computers we consider in this thesis are scalable, which means that aggregate computational power available to an application can be increased by increasing the number of nodes. Parallel computers are generally converging to multiple instruction/multiple data (MIMD) <ref> [18] </ref> architectures. Each processor in a MIMD machine operates independently on a subcomputation and synchronizes with other processors as needed. In this thesis, we focus on one subcategory of MIMD machines, called multicomputers. Multicomput-ers are distinguished by their distributed memory and message-passing style of communication. <p> Two examples of the basic cache-network interface model are the CM-5 and the T9000. One of the design goals [29] of the CM-5 was to reap the benefits of a MIMD architecture, while still maintaining compatibility with the CM-5's single instruction, multiple data (SIMD) <ref> [18] </ref> predecessor, the CM-2. Because SIMD architectures communicate frequently, providing low-latency communication in the CM-5 was crucial for CM-2 compatibility. Therefore, the CM-5's network interface was designed to logically connect to the cache, as depicted in only to the cache.
Reference: [19] <author> Jeffrey D. Gee, Mark D. Hill, Dionisios N. Pnevmatikatos, and Alan Jay Smith. </author> <title> Cache performance of the SPEC92 benchmark suite. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 17-27, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In contrast, sequences of short messages generally do not. Because of its spatial locality, a single long message should displace fewer cache lines than an equivalent volume of short messages, and is therefore expected to be less harmful to cache performance. 6 Based on data in <ref> [14, 19] </ref> and assuming a split 32 KB cache and an average execution time of one cycle per instruction (CPI) for non-memory instructions 30 To quantify the impact of short and long messages, we simulated different mixes of two message lengths (short and long).
Reference: [20] <author> Robert H. Halstead Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: We further hypothesize that in the future, multicomputer applications will communicate more than current multicomputer applications. This hypothesis is based on two factors: First, if fine-grained parallel programming languages such as Id [1], Concurrent Aggregates [6], and Multilisp <ref> [20] </ref> gain more acceptance, there will be an increased reliance on communication. And second, as massively-parallel computers become commonplace, each mode has at least the potential for increased communication (i.e. there are more nodes with which to communicate).
Reference: [21] <author> M. Homewood and M. McLaren. </author> <title> Meiko CS-2 interconnect Elan Elite design. </title> <booktitle> In Proceedings of the IEEE Hot Interconnects Symposium. IEEE TCMM, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: In that sense, the network interface behaves like a CPU itself: It actively sends LOAD and STORE requests to primary memory. An example of the basic memory-network interface model is the CS-2. The CS-2 <ref> [21] </ref> logically connects the network interface to primary memory in an extremely straightforward manner. As Figure 2.6 indicates, the network interface and DMA controller lie on the bus connecting the processor and memory, and the router connects to the network interface.
Reference: [22] <institution> Intel Corporation. </institution> <note> iPSC/2 and iPSC/860 User's Guide, </note> <month> June </month> <year> 1990. </year> <title> Order number 311532-006. </title>
Reference-contexts: The primary exception is [33], in which Stunkel and Fuchs characterize the cache performance of an Intel iPSC/2 hypercube <ref> [22] </ref>. The iPSC/2 is a multicomputer composed of up to 128 Intel 80386 microprocessors, each equipped with a 64 KB off-chip cache and up to 16 megabytes (MB) of primary memory. The network interface connects to primary memory.
Reference: [23] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: Operating system or run-time system involvement in message construction, packetization, and protocol processing affects the cache. One way to alleviate this problem is to add a messaging processor. Two examples of systems that follow this approach are the Paragon XP/S and the EDS. The Paragon XP/S <ref> [23] </ref> system tries to minimize cache pollution by adding a second processor (with a separate on-chip cache) specifically to handle messaging operations (Figure 2.7). Thus, the "compute" processor's cache is largely unaffected by communication; only the "messaging" processor's cache suffers.
Reference: [24] <author> Hiroaki Ishihata, Takeshi Horie, Satoshi Inano, Toshiyuki Shimizu, and Sadayuki Kato. </author> <title> An architecture of highly parallel computer AP1000. </title> <booktitle> In Proceedings of the IEEE Pacific Rim Conference on Communications, Computers, and Signal Processing, </booktitle> <pages> pages 13-16, </pages> <month> May </month> <year> 1991. </year> <note> Available from ftp://fcapwide.fujitsu.co.jp/ap1000/english/rim/ rim 91.ps.Z. </note>
Reference-contexts: Two examples of architectures using a cache- and memory-network interface are the AP1000 and Alewife. The AP1000 computer shares with the CM-5 the goal of low-latency communication <ref> [24] </ref>. It, too, logically connects the network interface and the cache, but in a substantially different manner from the CM-5 (Figure 2.4). Primarily, the AP1000 logically connects the network interface to both the cache and primary memory.
Reference: [25] <author> John Kubiatowicz and Anant Agarwal. </author> <title> The anatomy of a message in the Alewife multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year> <note> Available from ftp://cag.lcs.mit.edu/pub/papers/anatomy.ps.Z. </note>
Reference-contexts: The former is increasingly difficult given increasing CPU clock rates, and the latter requires functionality not supported until recently 11 by current-day microprocessors (in the form of cache-coherence protocols, which can provide limited forms of access to the on-chip cache). The Alewife machine <ref> [25] </ref> shares the AP1000's high-level architectural design. Both systems give the network interface access to both the cache and primary memory. However, as Figure 2.5 indicates, the Alewife's network interface is additionally directly accessible by the CPU. There are thus three ways to send messages in the Alewife: 1.
Reference: [26] <author> James R. Larus and Thomas Ball. </author> <title> Rewriting executable files to measure program behavior. </title> <type> Computer Sciences Technical Report 1083, </type> <institution> University of Wisconsin-Madison, </institution> <address> 1210 West Dayton St.; Madison, WI 53706, </address> <month> March </month> <year> 1992. </year> <note> Available from ftp://ftp.cs.wisc.edu/tech-reports/reports/91/tr1083.ps.Z. 93 </note>
Reference-contexts: We use the same trace files for both the uniprocessor and multicomputer simulators|to do otherwise would confuse effects caused by a node architecture and artifacts of an application's implementation. Trace data was generated using QPT <ref> [2, 26] </ref>. Information about the applications is given in Table 3.2. The column labelled "Memory references" represents the number of LOADs and STOREs issued dynamically. "Working set size" was calculated by: 1. Running the memory traces through dineroIII 5 using various-sized fully-associative caches with LRU replacement 2.
Reference: [27] <author> Barry A. Maskas, Stephen F. Shirron, and Nicholas A. Warchol. </author> <title> Design and performance of the DEC 4000 AXP departmental server computing systems. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4) </volume> <pages> 1-18, </pages> <year> 1992. </year> <note> Available from ftp://ftp.digital.com/pub/Digital/info/DTJ/ axp-dec-4000.ps. </note>
Reference-contexts: Table 1.1 lists the size of each level of the memory hierarchy, measured in kilobytes (KB), and the corresponding latency, measured in nanoseconds (ns). Data used in the table is based on that in <ref> [14, 27] </ref> (160 megahertz (MHz) DECchip 21064 CPU, maximum memory/cache sizes, bus latency included).
Reference: [28] <author> David May, Roger Shepherd, and Peter Thompson. </author> <title> The T9000 Transputer. </title> <type> Technical report, Inmos Limited, </type> <address> 1000 Aztec West, Almondsbury, Bristol BS12 4SQ, UK, </address> <year> 1993. </year> <note> Available from ftp://ftp.inmos.co.uk/inmos/info/T9000/T9000.ps.Z. </note>
Reference-contexts: An additional drawback of the CM-5's node architecture is that the CPU is involved in message transfers. The CPU must explicitly move message data from the cache to the network interface. Thus, the CPU must postpone computation during message handling. The T9000 Transputer <ref> [28] </ref> is designed specifically for fine-grained MIMD parallelism. It implements a custom-designed CPU, network interface, and cache 1 on a single chip, reducing latency but increasing design complexity. As Figure 2.3 shows, the processor, cache, and network interface are interconnected via a crossbar instead of a (slower) bus.
Reference: [29] <author> John Palmer and Guy L. Steele Jr. </author> <title> Connection machine model CM-5 system overview. </title> <booktitle> In Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 474-483, </pages> <year> 1992. </year>
Reference-contexts: In that sense, the network interface behaves like primary memory: It passively awaits the CPU's LOAD and STORE commands, which it then responds to by returning or accepting data. Two examples of the basic cache-network interface model are the CM-5 and the T9000. One of the design goals <ref> [29] </ref> of the CM-5 was to reap the benefits of a MIMD architecture, while still maintaining compatibility with the CM-5's single instruction, multiple data (SIMD) [18] predecessor, the CM-2. Because SIMD architectures communicate frequently, providing low-latency communication in the CM-5 was crucial for CM-2 compatibility.
Reference: [30] <author> C. Seitz, N. Boden, J. Seizovic, and W. Su. </author> <title> The design of the Caltech Mosaic C multicom-puter. </title> <booktitle> In Proceedings of the University of Washington Symposium on Integrated Systems, </booktitle> <year> 1993. </year>
Reference-contexts: Specifically, memory-network interfaces require a higher level of system integration to prevent primary memory from becoming a communication bottleneck. The Mosaic C is an example of a fine-grained multicomputer. Like the CS-2, Paragon, and EDS, the Mosaic C <ref> [30] </ref> logically connects the network interface to primary memory. However, unlike those other machines, the Mosaic C is designed for fine-grained parallel applications. Fine-grained applications require low-latency communication, making the cache in the Mosaic C's block diagram (Figure 2.9) conspicuously absent.
Reference: [31] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <type> Technical report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford University, </institution> <address> CA 94305, </address> <month> April </month> <year> 1991. </year> <note> Available from ftp://mojave.stanford.edu/pub/splash/report/splash.ps. </note>
Reference-contexts: Therefore, we use memory traces from several different applications. We selected application traces based on the following criteria: * scientific nature * programming language * locality * size The applications are taken from well-known benchmark suites (SPEC [15, 16] and SPLASH <ref> [31] </ref>). They are written in different languages (C and Fortran). They exhibit a range of localities. And they each make enough memory references to provide meaningful data, but not so many as to take an inordinate amount of time to simulate. <p> Running the memory traces through dineroIII 5 using various-sized fully-associative caches with LRU replacement 2. Plotting the miss rate against the cache size 3. Taking the knee in each curve as the corresponding application's working set size. Descriptions of the traced applications <ref> [9, 15, 31] </ref> follow: barnes barnes uses Barnes-Hut's hierarchical N -body algorithm to model point-masses exerting gravitational forces on each other in three dimensions. The algorithm specifies that clusters of distant point-masses are treated as a single point-mass, which significantly reduces the amount of necessary computation.
Reference: [32] <author> A. J. Smith. </author> <title> Cache memories. </title> <journal> ACM Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: The largest, slowest, but least expensive memory|primary memory| is at the bottom of the memory hierarchy 1 , with levels of increasingly smaller, faster, but more expensive, memory|generally organized as a cache|higher up. Cache memories <ref> [32] </ref> contain a dynamically-changing subset of lower memories in the hierarchy. When the CPU makes a request (a LOAD or STORE) to a memory address, the first level of cache memory is searched. If the data is present in the cache, the cache returns or modifies the data (as appropriate).
Reference: [33] <author> Craig B. Stunkel and W. Kent Fuchs. </author> <title> An analysis of cache performance for a hypercube multicomputer. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(4) </volume> <pages> 421-432, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Using those three simulators, we studied message handling's impact on the cache under a variety of message rates, cache sizes, and message types. 1.3 Related Work While multiprocessor cache performance has been studied extensively, there have been virtually no studies of multicomputer cache performance. The primary exception is <ref> [33] </ref>, in which Stunkel and Fuchs characterize the cache performance of an Intel iPSC/2 hypercube [22]. The iPSC/2 is a multicomputer composed of up to 128 Intel 80386 microprocessors, each equipped with a 64 KB off-chip cache and up to 16 megabytes (MB) of primary memory. <p> According to <ref> [33] </ref>, given a constant problem size, the cache miss rate decreases with the number of processors until the application's working set fits in the cache, and increases from there on.
Reference: [34] <author> Gunter Watzlawik and Franz Hutner. </author> <title> A pipelined network interface for a parallel computer. </title> <type> Technical report, </type> <institution> Bull/ECRC/ICL/Siemens, ZFE ST SN 22, Siemens AG, </institution> <address> Otto-Hahn-Ring 6, 81730 Munchen, Germany, </address> <year> 1993. </year>
Reference-contexts: The disadvantage of dedicating one processor to computation and one to communication is that computation-intensive tasks are limited to using half of the available CPU power. The European Declarative System (EDS), a multicomputer produced by a consortium of Bull, ECRC, ICL, and Siemens <ref> [34] </ref>, is architecturally similar to the Paragon, although it addresses the dedicated-processor issue. The main differences between the EDS and the Paragon are: 1. The EDS does not dedicate its messaging processor to messaging; it is also allowed to compute. 2.
Reference: [35] <author> Stephen Wheat, Barney Maccabe, Rolf Riesen, David van Dresser, and Mark Stall-cup. </author> <note> Overheads from SUNMOS/PUMA presentation, September 1993. Available from http://www.ccsf.caltech.edu/paragon/sunmos/sunmos slides.ps. </note>
Reference-contexts: The former scheme|used, for example, in the CM-5|is optimized for low-latency short messages, while the latter|used, for example, in the Paragon|is optimized for high-bandwidth long messages. Table 5.1 confirms that claim using data obtained from [17] and <ref> [35] </ref>. (The latter reference cites the performance of a Paragon running SUNMOS/PUMA, an operating system and software architecture geared towards more efficient messaging than is provided with the software bundled with the Paragon.) Clock frequencies listed are for the processor clock. Clock freq.
Reference: [36] <author> Patrick Henry Winston and Berthold Klaus Paul Horn. </author> <title> LISP. </title> <publisher> Addison-Wesley, </publisher> <address> second edition, </address> <year> 1984. </year> <month> 94 </month>
Reference-contexts: When used in SPEC, the interpreter's input is an implementation of the N -queens problem (Find all arrangements of N queens on an N fiN chessboard such that no queen can take any other queen) based on that in <ref> [36, pp. 183, 367-368] </ref>.
References-found: 36

