URL: http://www.cse.ogi.edu/~hyang/paper/natGdNIPS97.ps.Z
Refering-URL: http://www.cse.ogi.edu/~hyang/paper.html
Root-URL: http://www.cse.ogi.edu
Email: hyang@cse.ogi.edu  amari@zoo.brain.riken.go.jp  
Title: NIPS*97 The Efficiency and The Robustness of Natural Gradient Descent Learning Rule  
Author: Howard Hua Yang Shun-ichi Amari 
Keyword: descent learning rule  Sub-category: dynamics of learning algorithms Category: Theory  
Note: is not only efficient but also robust.  
Address: PO Box 91000, Portland, OR 97291, USA  351-01, JAPAN  
Affiliation: Department of Computer Science Oregon Graduate Institute  Lab. for Information Representation FRP, RIKEN Wako-shi, Saitama  
Abstract: We have discovered a new scheme to represent the Fisher information matrix of a stochastic multi-layer perceptron. Based on this scheme, we have designed an algorithm to compute the inverse of the Fisher information matrix. When the input dimension n is much larger than the number of hidden neurons, the complexity of this algorithm is of order O(n 2 ) while the complexity of conventional algorithms for the same purpose is of order O(n 3 ). The inverse of the Fisher information matrix is used in the natural gradient descent algorithm to train single-layer or multi-layer perceptrons. It is confirmed by simulation that the natural gradient 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amari. </author> <title> Differential-Geometrical Methods in Statistics, </title> <booktitle> Lecture Notes in Statistics vol.28. </booktitle> <publisher> Springer, </publisher> <year> 1985. </year>
Reference: [2] <author> S. Amari. </author> <title> Natural gradient works efficiently in learning. Accepted by Neural Computation, </title> <year> 1997. </year>
Reference: [3] <author> S. Amari. </author> <title> Neural learning in structured parameter spaces natural Riemannian gradient. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> 9, </volume> <editor> ed. M. C. Mozer, M. I. Jordan and T. Petsche, </editor> <publisher> The MIT Press: </publisher> <address> Cambridge, MA., </address> <pages> pages 127-133, </pages> <year> 1997. </year>
Reference: [4] <author> C. Darken and J. Moody. </author> <title> Towards faster stochastic gradient search. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> 4, </volume> <editor> eds. Moody, Hanson, and Lippmann, </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <pages> pages 1009-1016, </pages> <year> 1992. </year>
Reference: [5] <author> G. B. Orr and T. K. Leen. </author> <title> Using curvature information for fast stochastic search. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> 9, </volume> <editor> ed. M. C. Mozer, M. I. Jordan and T. Petsche, </editor> <publisher> MIT Press: </publisher> <address> Cambridge, MA., </address> <year> 1997. </year>
Reference: [6] <author> D. Saad and S. A. Solla. </author> <title> On-line learning in soft committee machines. </title> <journal> Physical Review E, </journal> <volume> 52 </volume> <pages> 4225-4243, </pages> <year> 1995. </year>
Reference: [7] <author> H. H. Yang and S. Amari. </author> <title> Natural gradient descent for training multi-layer perceptrons. </title> <note> Submitted to IEEE Tr. on Neural Networks, </note> <year> 1997. </year>
Reference-contexts: Since is unknown, instead of using l (zjx; ) we use the following loss function: l 1 (zjx; ) = 2 We have proved in <ref> [7] </ref> that G () = 1 2 A () where A () does not depend on explicitly. <p> The main difficulty in implementing the natural gradient descent algorithm (6) is to compute A () and its inverse on-line. To overcome this difficulty, we studied the structure of the matrix A () in <ref> [7] </ref> and proposed an efficient scheme to represent this matrix. Here, we briefly describe this scheme. <p> Denote u i = w i =kw i k; i = 1; ; m; U 1 = [u 1 ; ; u m ] and [v 1 ; ; v m ] = U 1 (U T 1 U 1 ) 1 . It has been proved in <ref> [7] </ref> that those blocks in A () are divided into three classes: C 1 = fA ij ; i; j = 1; ; mg, C 2 = fA i;m+1 ; A T m+2;i ; i = 1; ; mg and C 3 = fA m+i;m+j ; i; j = 1; 2g. <p> Each block in C 3 is an m fi m matrix whose entries are also integrals with respect to N (0; R 1 ). Detail expressions for these integrals are given in <ref> [7] </ref>. The dimension of A () is (nm + 2m) fi (nm + 2m). By using the above scheme, the space for storing this large matrix is reduced from O (n 2 ) to O (n). We also gave a fast algorithm in [7] to compute A 1 (). <p> expressions for these integrals are given in <ref> [7] </ref>. The dimension of A () is (nm + 2m) fi (nm + 2m). By using the above scheme, the space for storing this large matrix is reduced from O (n 2 ) to O (n). We also gave a fast algorithm in [7] to compute A 1 (). When the input dimension n is much larger than the number of hidden neurons, the time complexity of this algorithm is O (n 2 ) while the complexity of conventional algorithms for the same purpose is O (n 3 ).
References-found: 7

