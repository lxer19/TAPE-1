URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/selforgantiming.ps.Z
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Title: Self-Organization of Spiking Neurons Using Action Potential Timing  
Author: Berthold Ruf, Michael Schmitt 
Keyword: Self-organizing map, spiking neurons, temporal coding, unsupervised learning.  
Abstract: We propose a mechanism for unsupervised learning in networks of spiking neurons which is based on the timing of single firing events. Our results show that a topology preserving behaviour quite similar to that of Kohonen's self-organizing map can be achieved using temporal coding. In contrast to previous approaches, which use rate coding, the winner among competing neurons can be determined fast and locally. Our model is a further step towards a more realistic description of unsupervised learning in biological neural systems. Furthermore, it may provide a basis for fast implementations in pulsed VLSI. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Gerstner and L. van Hemmen, </author> <title> "How to describe neuronal activity: spikes, rates or assemblies?," </title> <booktitle> in Advances in Neural Information Processing Systems. 1994, </booktitle> <volume> vol. 6, </volume> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We used the familiar rectangular grid in two dimension with the Manhattan metric for the neighbourhood relationships in this example. Obviously, an optimal topology preserving mapping minimizes the value of E MDS . In order to scale the values of E MDS into the interval <ref> [0; 1] </ref> and to make the results for different initializations comparable, we defined the relative neighbourhood distortion which is the actual value of E MDS divided by the maximum initial value of E MDS . The results for two typical simulations are shown in Figure 3.
Reference: [2] <author> T. Sejnowski, </author> <title> "Time for a new neural code?," </title> <journal> Nature, </journal> <volume> vol. 376, </volume> <pages> pp. 21 - 22, </pages> <year> 1995. </year>
Reference: [3] <author> F. Rieke, D. Warland, W. Bialek, and R. de Ruyter van Steveninck, SPIKES: </author> <title> Exploring the Neural Code, </title> <publisher> MIT-Press, </publisher> <address> Cambridge, </address> <year> 1996. </year>
Reference: [4] <author> W. Maass, </author> <title> "Networks of spiking neurons: </title> <booktitle> The third generation of neural network modells," To appear in: Neural Networks, </booktitle> <year> 1997. </year>
Reference: [5] <author> M. A. Arbib, Ed., </author> <title> The Handbook of Brain Theory and Neural Networks, </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1995. </year>
Reference: [6] <author> W. Maass, </author> <title> "Lower bounds for the computational power of networks of spiking neurons," </title> <journal> Neural Computation, </journal> <volume> vol. 8, </volume> <pages> pp. 1-40, </pages> <year> 1996. </year>
Reference: [7] <author> B. Ruf and M. Schmitt, </author> <title> "Learning temporally encoded patterns in networks of spiking neurons," </title> <journal> Neural Processing Letters, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 9-18, </pages> <year> 1997. </year>
Reference: [8] <author> W. Maass, </author> <title> "Fast sigmoidal networks via spiking neurons," </title> <journal> Neural Computation, </journal> <volume> vol. 9, </volume> <pages> pp. 279-304, </pages> <year> 1997. </year>
Reference-contexts: Our approach also may give rise to fast hardware implementations of self-organizing networks in pulse coded VLSI. As mentioned above we have assumed as in <ref> [8] </ref> that the neurons are of the leaky integrate-and-fire type, where the initial segment of the postsynaptic potentials rises linearly.
Reference: [9] <author> T. Kohonen, </author> <title> Self-Organizing Maps, </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference: [10] <author> T. Kohonen, </author> <title> "Physiological interpretation of the self-organizing map algorithm," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 6, </volume> <pages> pp. 895 - 905, </pages> <year> 1993. </year>
Reference: [11] <author> J. Sirosh and R. Miikkulainen, </author> <title> "Topographic receptive fields and patterned lateral interaction in a self-organizing model of the primary visual cortex.," </title> <journal> Neural Computation, </journal> <volume> vol. 9, </volume> <pages> pp. 577 - 594, </pages> <year> 1997. </year>
Reference: [12] <author> Y. Choe and R. Miikkulainen, </author> <title> "Self-organization and segmentation with laterally connected spiking neurons," </title> <type> Tech. Rep. AI TR 96-251, </type> <institution> Department of Computer Science, University of Texas at Austin, </institution> <year> 1996. </year>
Reference: [13] <author> G. J. Goodhill and T. J. Sejnowski, </author> <title> "A unifying objective function for topographic mappings," </title> <journal> Neural Computation, </journal> <volume> vol. 9, </volume> <pages> pp. 1291 - 1303, </pages> <year> 1997. </year>
Reference-contexts: This may be appropriate for a single mapping but it is hardly possible to compare the degree of topology preservation of two different mappings. In our second experiment we therefore used a measure for quantifying the neighbourhood preservation known as "metric multidimensional scaling" (see e.g. <ref> [13] </ref>). It is based on the objective function E MDS = i=1 j&lt;i where N is the number of input patterns and M denotes the mapping of the network, i.e. M (i) is the index of the winner neuron in the competitive layer for input s i .
Reference: [14] <author> A. Murray and L. Tarassenko, </author> <title> Analogue Neural VLSI: A Pulse Stream Approach, </title> <publisher> Chapman & Hall, </publisher> <year> 1994. </year>
Reference: [15] <author> B. Ruf and M. Schmitt, </author> <title> "Unsupervised learning in networks of spiking neurons using temporal coding," </title> <booktitle> in Proc. of the 7th International Conference on Artificial Neural Networks, </booktitle> <year> 1997, </year> <pages> pp. 361 - 366. </pages>
Reference: [16] <author> B. Ruf, </author> <title> "Computing functions with spiking neurons using temporal coding," </title> <booktitle> in Biological and Artificial Computation: From Neuro-science to Technology. Proc. of the International Work-Conference on Artificial and Natural Neural Networks, </booktitle> <editor> J. Mira, R. Moreno-Diaz, and J. Cabestany, Eds. </editor> <booktitle> 1997, vol. 1240 of Lecture Notes on Computer Science, </booktitle> <pages> pp. 265 - 272, </pages> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference-contexts: Our approach also may give rise to fast hardware implementations of self-organizing networks in pulse coded VLSI. As mentioned above we have assumed as in [8] that the neurons are of the leaky integrate-and-fire type, where the initial segment of the postsynaptic potentials rises linearly. Recent simulations <ref> [16] </ref> have shown that even when using a more detailed neural model which includes nonlinear effects and more realistic shapes (e.g. ff-functions) for the postsynaptic potentials, spiking neurons can still compute weighted sums in the way described in Section II. This indicates that the abovementioned simplifying assumptions can be dropped. 5
References-found: 16

