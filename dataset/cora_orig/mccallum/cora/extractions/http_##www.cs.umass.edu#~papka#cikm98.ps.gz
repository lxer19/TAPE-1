URL: http://www.cs.umass.edu/~papka/cikm98.ps.gz
Refering-URL: http://www.cs.umass.edu/~papka/
Root-URL: 
Email: fpapka,allang@cs.umass.edu  
Title: Document Classification using Multiword Features  
Author: Ron Papka and James Allan 
Address: Amherst, MA 01003  
Affiliation: Center for Intelligent Information Retrieval Department of Computer Science University of Massachusetts  
Abstract: We investigate the use of multiword query features to improve the effectiveness of text-retrieval systems that accept natural-language queries. A relevance feedback process is explained that expands an initial query with single and multiword features. The multiword features are modelled as a set of words appearing within windows of varying sizes. Our experimental results suggest that windows of larger span yield improvements in retrieval over windows of smaller span. This result gives rise to a query contraction process that prunes 25% of the features in an expanded query with no loss in retrieval effectiveness.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Allan, L. Ballesteros, J. Callan, W.B. Croft, and Z. Lu, </author> <title> "Recent Experiments with Inquery," </title> <booktitle> Proceedings of TREC-4, </booktitle> <pages> 49-64, </pages> <year> 1996. </year>
Reference-contexts: These negative conclusions regarding multiword features are offset by positive results that have been reported. For example, Strzalkowski and Carballo [19] describe improvements in an ad hoc retrieval using natural language phrases. Allan et al. <ref> [1] </ref> and Hawking et al. [11] report using proximity operators over sets of multiple words for improving filtering and ad hoc retrieval. Boolean features comprising multiple words have been used to improve precision by Hearst [12]. Results for other document classification problems show solutions using multiword features. <p> The integration of multiword features into the query representation implies additional support from the system's indexing processes, and requires new query language constructs for expressing concepts in natural language. One prevalent method for creating multiword features is to build system functionality that incorporates the proximity between words. Inquery <ref> [5, 1] </ref>, for example, accepts queries containing ordered and unordered-window operators. They allow the user to specify an information request more precisely by imposing a locality constraint on groups of words. <p> in which the feature appears in the collection, dl is the document's length, avg dl is the average document length in the collection, and jcj is the number of documents in the collection. 4.2 Expansion Process The query expansion process used for this work is similar to those described in <ref> [3, 1] </ref>. The process has two steps: feature selection and feature weight assignment. The primary focus of our experiments is on multiword feature selection. Our methodology for selecting expansion features begins with collecting the union of the stemmed words appearing in documents judged to be relevant and non-relevant. <p> Phrases are added at the beginning of the expansion processes using a technique described in <ref> [1] </ref>. 4 In the following experiments query feature weights are assigned using a relevance feedback methodol-ogy similar to one originally developed by Rocchio [18]. The weight we assign to added query features is based on the f f component of the comparison model described above.
Reference: [2] <author> P. F. Brown, J. Cocke, S.A. Della Pietra , V.J. Della Pietra, F. Jelinek, J.D. Lafferty, R.L. Mercer, and P.S. Roossin, </author> <title> "A Statistical Approach to Machine Translation," </title> <journal> Computational Linguistics, </journal> <volume> 16(2): </volume> <pages> 79-85, </pages> <year> 1990. </year>
Reference-contexts: There are several reported methods for extracting multiword features. N-gram models based on mutual information metrics are used to find sets of adjacent words that are likely to cooccur within sentences <ref> [2] </ref>. Part-of-speech tagging using pre-specified syntactic templates or more complex natural language parsing [9, 21] give rise to related multiple words comprising noun, verb, and prepositional phrases.
Reference: [3] <author> C. Buckley, G. Salton, J. Allan, </author> " <title> The Effect of Adding Relevance Information in a Relevance Feedback Environment," </title> <booktitle> Proceedings of SIGIR, </booktitle> <pages> 49-58, </pages> <year> 1993. </year>
Reference-contexts: Users of web search-engines, such as Excite! and Infoseek, are currently entering very few words (without weights) per query [8]; however, the data from document retrieval research suggests that expanding queries with many words yields better results <ref> [3, 23, 12] </ref>. It becomes evident from examples that using queries comprising weighted independent single words, or their boolean combinations, limits retrieval effectiveness. Consider a user interested in information about Harley Davidson Motorcycles. Documents containing the word "Davidson" without the word "Harley" become unwanted relevant candidates using weighted independent words. <p> in which the feature appears in the collection, dl is the document's length, avg dl is the average document length in the collection, and jcj is the number of documents in the collection. 4.2 Expansion Process The query expansion process used for this work is similar to those described in <ref> [3, 1] </ref>. The process has two steps: feature selection and feature weight assignment. The primary focus of our experiments is on multiword feature selection. Our methodology for selecting expansion features begins with collecting the union of the stemmed words appearing in documents judged to be relevant and non-relevant. <p> #1 21.6 6.4% after also adding top 50 #UW5 23.2 7.4% after also adding top 50 #UW20 25.1 8.2% after also adding top 50 #UW50 25.8 2.8% after also adding top 50 #BAND 27.4 6.2% Table 2: Successive precision improvements on test set realized by massive query expansion. et al. <ref> [3] </ref>, where single word features were used. In addition, these results suggest that the positive effects of massive query expansion extend to multiword features. In an effort to understand this phenomenon, we examined the query features and their cooccurrence in relevant versus non-relevant documents.
Reference: [4] <author> C. Buckley and G. Salton, </author> <title> "Optimization of Relevance Feedback Weights," </title> <booktitle> Proceedings of SIGIR, </booktitle> <pages> 351-357, </pages> <year> 1995. </year>
Reference-contexts: Our analysis has addressed the impact of feature cooccurrence, but we have yet to address the effects of feature weight assignment. We anticipate that feature weight learning approaches applied to single word features <ref> [4, 14, 15] </ref> will further improve retrieval effectiveness for queries with multiword features. We are continuing to investigate the value of phrases and multiword features for several document classification tasks.
Reference: [5] <author> J. P. Callan, W.B. Croft, </author> <title> and S.M. Harding, "The INQUERY Retrieval System," Database and Expert Systems Applications: </title> <booktitle> Proceedings of the International Conference in Valencia Spain, A.M. </booktitle> <editor> Tjoa and I. Ramos eds., </editor> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The integration of multiword features into the query representation implies additional support from the system's indexing processes, and requires new query language constructs for expressing concepts in natural language. One prevalent method for creating multiword features is to build system functionality that incorporates the proximity between words. Inquery <ref> [5, 1] </ref>, for example, accepts queries containing ordered and unordered-window operators. They allow the user to specify an information request more precisely by imposing a locality constraint on groups of words. <p> list are contained in the document. 1 A recent analysis indicated that the average Thomas query was 2.3 words as well. 3 4 Massive Query Expansion 4.1 Retrieval Representation The massive query expansion process used in the experiments that follow is based on an implementation of the Inquery retrieval system <ref> [5] </ref>.
Reference: [6] <author> C. Carrick, and C. Watters, </author> <title> "Automatic Association of New Items," </title> <booktitle> Information Processing & Management, </booktitle> <volume> 33(5) </volume> <pages> 615-632, </pages> <year> 1997. </year>
Reference-contexts: Results for other document classification problems show solutions using multiword features. Recent work by Carrick and Watters explained an application that matches news stories to photo captions using a frame-based approach focusing on proper noun phrases <ref> [6] </ref>. Riloff and Lehnert [16] also use multiword features as an integral part of a message understanding system that classifies news stories into pre-specified event frames. There are several reported methods for extracting multiword features.
Reference: [7] <author> W.B. Croft, R Cook, and D. Wilder, </author> <title> "Providing Government Information on the Internet: Experiences with THOMAS," </title> <booktitle> Proceedings of Digital Libraries Conference, </booktitle> <pages> 19-24, </pages> <year> 1995. </year>
Reference-contexts: Croft et al. tracked several thousand queries against the Thomas Congressional database and found that most queries were between 2 and 8 words 1 . In addition, they showed that internally translating the user's request into a query containing proximity operators provided better results than using the single words <ref> [7] </ref>.
Reference: [8] <author> D. </author> <title> Cutting, "Industry Panel Discussion," </title> <booktitle> SIGIR, </booktitle> <year> 1997. </year> <title> (A sample of Excite! queries from September 16, </title> <note> 1997 became available after this discussion.) </note>
Reference-contexts: The query representation for several commercial and research systems comprises words and weights of their relative importance. Users of web search-engines, such as Excite! and Infoseek, are currently entering very few words (without weights) per query <ref> [8] </ref>; however, the data from document retrieval research suggests that expanding queries with many words yields better results [3, 23, 12]. It becomes evident from examples that using queries comprising weighted independent single words, or their boolean combinations, limits retrieval effectiveness. <p> A recent sample of roughly 500,000 queries from the Excite! search engine indicates that the average 2 request is a query of 2.3 words <ref> [8] </ref>. A closer inspection of the corpus indicates that 95% of the searches do not make use of phrase query features even though the functionality for creating them is provided.
Reference: [9] <author> J.L. Fagan, </author> <title> A Comparison of Syntactic and Non-Syntactic Methods, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1987. </year>
Reference-contexts: There are several reported methods for extracting multiword features. N-gram models based on mutual information metrics are used to find sets of adjacent words that are likely to cooccur within sentences [2]. Part-of-speech tagging using pre-specified syntactic templates or more complex natural language parsing <ref> [9, 21] </ref> give rise to related multiple words comprising noun, verb, and prepositional phrases.
Reference: [10] <author> D. </author> <title> Harman, </title> <booktitle> Proceedings of Text REtrieval Conferences (TREC), </booktitle> <pages> 1993-1997. </pages>
Reference-contexts: The weights for this Rocchio function were determined to work well empirically. 5 Experiments 5.1 Data Experiments were conducted on 50 natural-language information requests used for the routing track for TREC-4 <ref> [10] </ref>. The queries are a subset of TREC topics 3-191. The short description for each topic was stemmed, and then stopwords were removed to produce the initial query. Each query was subsequently expanded in several ways using the retrieval and expansion processes described in the section above.
Reference: [11] <author> D. Hawking, and P. Thistlewaite, </author> <title> "Proximity Operators So Near And Yet So Far," </title> <booktitle> Proceedings of TREC-4, </booktitle> <pages> 131-144, </pages> <year> 1996. </year>
Reference-contexts: Much of the previous work that uses natural language phrases and other proximity constraints 1 between multiple words suggests that "the closer a set of intersecting terms, the more likely they are to indicate relevance" <ref> [11] </ref>. Our experiments indicate that in the context of massive query expansion (queries with several hundred features), retrieval effectiveness improves by using query features implemented as sets of words that appear further separated within natural language text. <p> These negative conclusions regarding multiword features are offset by positive results that have been reported. For example, Strzalkowski and Carballo [19] describe improvements in an ad hoc retrieval using natural language phrases. Allan et al. [1] and Hawking et al. <ref> [11] </ref> report using proximity operators over sets of multiple words for improving filtering and ad hoc retrieval. Boolean features comprising multiple words have been used to improve precision by Hearst [12]. Results for other document classification problems show solutions using multiword features.
Reference: [12] <author> M. A. Hearst, </author> <title> "Improving Full-Text Precision on Short Queries using Simple constraints," </title> <booktitle> Proceedings of SDAIR, </booktitle> <year> 1996. </year>
Reference-contexts: Users of web search-engines, such as Excite! and Infoseek, are currently entering very few words (without weights) per query [8]; however, the data from document retrieval research suggests that expanding queries with many words yields better results <ref> [3, 23, 12] </ref>. It becomes evident from examples that using queries comprising weighted independent single words, or their boolean combinations, limits retrieval effectiveness. Consider a user interested in information about Harley Davidson Motorcycles. Documents containing the word "Davidson" without the word "Harley" become unwanted relevant candidates using weighted independent words. <p> Allan et al. [1] and Hawking et al. [11] report using proximity operators over sets of multiple words for improving filtering and ad hoc retrieval. Boolean features comprising multiple words have been used to improve precision by Hearst <ref> [12] </ref>. Results for other document classification problems show solutions using multiword features. Recent work by Carrick and Watters explained an application that matches news stories to photo captions using a frame-based approach focusing on proper noun phrases [6].
Reference: [13] <author> D.D. Lewis, </author> <title> Representations and Learning In Information Retrieval, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer and Information Science, University of Massachussetts, </institution> <year> 1991. </year>
Reference-contexts: Previous research in document classification extends the feature space by extracting natural language phrases and more general multiword features. The utility of multiword features and their effects on retrieval has been mixed in the previous literature. Lewis <ref> [13] </ref> has documented some of the earlier work pertaining to representation and ambiguity issues arising from the use of phrases. In addition, TREC routing and filtering participants using multiword features do not appear to significantly outperform some participants that use only single word features [17]. <p> It appears that the frequency statistics for smaller windows are not as statistically useful to our retrieval model which is based on feature frequency and inverse document frequency. The insufficient occurrence of natural language phrases has been concluded as a possible cause for no improvement using near-proximity multiword features <ref> [13] </ref>. 5.3.2 Query Contraction Process Reducing the number of features used to represent a query may have a positive impact on the computation time needed to process a query as well as retrieval effectiveness.
Reference: [14] <author> D. Lewis, R. Schapire, J. Callan, and R. Papka, </author> <title> "Training Algorithms for Linear Text Classifiers," </title> <booktitle> Proceedings of SIGIR, </booktitle> <pages> 298-306, </pages> <year> 1996. </year> <month> 11 </month>
Reference-contexts: Pruning irrelevant single word features after massive query expansion has been reported using machine learning approaches by Lewis et al. <ref> [14] </ref> and Papka et al. [15]. Window size #1 #UW5 #UW20 #UW50 #1 - 7.6% 3.5% 2.6% #UW20 - 28.0% Table 4: Average percent of overlapping identical word-pairs occurring in different window sizes. <p> Our analysis has addressed the impact of feature cooccurrence, but we have yet to address the effects of feature weight assignment. We anticipate that feature weight learning approaches applied to single word features <ref> [4, 14, 15] </ref> will further improve retrieval effectiveness for queries with multiword features. We are continuing to investigate the value of phrases and multiword features for several document classification tasks.
Reference: [15] <author> R. Papka, J. Callan, and A. Barto, </author> <title> "Text-Based Information Retrieval using Exponentiated Gradient Descent," </title> <booktitle> Proceedings of Neural Information Processing Systems Conference, </booktitle> <pages> 3-9, </pages> <year> 1996. </year>
Reference-contexts: Pruning irrelevant single word features after massive query expansion has been reported using machine learning approaches by Lewis et al. [14] and Papka et al. <ref> [15] </ref>. Window size #1 #UW5 #UW20 #UW50 #1 - 7.6% 3.5% 2.6% #UW20 - 28.0% Table 4: Average percent of overlapping identical word-pairs occurring in different window sizes. <p> Our analysis has addressed the impact of feature cooccurrence, but we have yet to address the effects of feature weight assignment. We anticipate that feature weight learning approaches applied to single word features <ref> [4, 14, 15] </ref> will further improve retrieval effectiveness for queries with multiword features. We are continuing to investigate the value of phrases and multiword features for several document classification tasks.
Reference: [16] <author> E. Riloff and W. Lehnert, </author> <title> "Information Extraction as a Basis for High-Precision Text Classification," </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 12(3): </volume> <pages> 296-333, </pages> <year> 1994. </year>
Reference-contexts: Results for other document classification problems show solutions using multiword features. Recent work by Carrick and Watters explained an application that matches news stories to photo captions using a frame-based approach focusing on proper noun phrases [6]. Riloff and Lehnert <ref> [16] </ref> also use multiword features as an integral part of a message understanding system that classifies news stories into pre-specified event frames. There are several reported methods for extracting multiword features.
Reference: [17] <editor> S.E. Robertson, S. Walker, M.M. Beaulieu, M. Gatford, and A. Payne, </editor> <booktitle> "Okapi at TREC-4," Proceedings of TREC-4, </booktitle> <pages> 73-86, </pages> <year> 1996. </year>
Reference-contexts: Lewis [13] has documented some of the earlier work pertaining to representation and ambiguity issues arising from the use of phrases. In addition, TREC routing and filtering participants using multiword features do not appear to significantly outperform some participants that use only single word features <ref> [17] </ref>. These negative conclusions regarding multiword features are offset by positive results that have been reported. For example, Strzalkowski and Carballo [19] describe improvements in an ad hoc retrieval using natural language phrases.
Reference: [18] <author> J.J. Rocchio, </author> <title> "Relevance Feedback in Information Retrieval," in The Smart System - Experiments in Automatic Document Processing, </title> <address> 313-323, Englewood Cliffs, NJ: </address> <publisher> Prentice Hall Inc. </publisher> <year> 1971. </year>
Reference-contexts: Phrases are added at the beginning of the expansion processes using a technique described in [1]. 4 In the following experiments query feature weights are assigned using a relevance feedback methodol-ogy similar to one originally developed by Rocchio <ref> [18] </ref>. The weight we assign to added query features is based on the f f component of the comparison model described above.
Reference: [19] <author> T. Strzalkowski and J. Perez Carballo, </author> <booktitle> "Natural Language Information Retrieval: TREC-4 Report," Proceedings of TREC-4, </booktitle> <pages> 245-258, </pages> <year> 1996. </year>
Reference-contexts: In addition, TREC routing and filtering participants using multiword features do not appear to significantly outperform some participants that use only single word features [17]. These negative conclusions regarding multiword features are offset by positive results that have been reported. For example, Strzalkowski and Carballo <ref> [19] </ref> describe improvements in an ad hoc retrieval using natural language phrases. Allan et al. [1] and Hawking et al. [11] report using proximity operators over sets of multiple words for improving filtering and ad hoc retrieval.
Reference: [20] <author> H. Turtle and and W.B. Croft, </author> <title> "A Comparison of Text Retrieval Models," </title> <journal> Computer Journal, </journal> <volume> 35(3): </volume> <pages> 279-290, </pages> <year> 1992. </year>
Reference-contexts: Belief values are produced by Inquery's belief function, which uses a probabilistic inference network model <ref> [20] </ref>. In our implementation, the belief function is composed of a feature frequency component, f f , and an inverse document frequency component, idf .
Reference: [21] <author> E. Tzoukermann, J.L. Klavans, and C. Jacquemin, </author> <title> "Effective Use of Natural Language Processing Techniques for Automatic Conflation of Multi-Word Terms: The Role of Derivational Morphology, Part of Speech Tagging, and Shallow Parsing," </title> <booktitle> Proceedings of SIGIR, </booktitle> <pages> 148-155, </pages> <year> 1997. </year>
Reference-contexts: There are several reported methods for extracting multiword features. N-gram models based on mutual information metrics are used to find sets of adjacent words that are likely to cooccur within sentences [2]. Part-of-speech tagging using pre-specified syntactic templates or more complex natural language parsing <ref> [9, 21] </ref> give rise to related multiple words comprising noun, verb, and prepositional phrases.
Reference: [22] <author> C.J. van Rijsbergen, </author> <title> Information Retrieval, </title> <publisher> 2ed., Butterworths, </publisher> <address> Massachusetts, </address> <year> 1979. </year>
Reference: [23] <author> E. M. Voorhees, </author> <title> "Query Expansion using Lexical-Semantic Relations," </title> <booktitle> Proceedings of SIGIR, </booktitle> <pages> 311-317, </pages> <year> 1994. </year>
Reference-contexts: Users of web search-engines, such as Excite! and Infoseek, are currently entering very few words (without weights) per query [8]; however, the data from document retrieval research suggests that expanding queries with many words yields better results <ref> [3, 23, 12] </ref>. It becomes evident from examples that using queries comprising weighted independent single words, or their boolean combinations, limits retrieval effectiveness. Consider a user interested in information about Harley Davidson Motorcycles. Documents containing the word "Davidson" without the word "Harley" become unwanted relevant candidates using weighted independent words.
References-found: 23

