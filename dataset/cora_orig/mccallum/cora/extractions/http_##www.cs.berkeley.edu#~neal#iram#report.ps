URL: http://www.cs.berkeley.edu/~neal/iram/report.ps
Refering-URL: http://www.cs.berkeley.edu/~neal/iram/simIRAM.html
Root-URL: 
Email: fbowman,neal,cromerg@cs.berkeley.edu  
Title: The Performance of Applications and Operating Systems on Simple IRAM Architectures  
Author: Ngeci Bowman, Neal Cardwell and Cynthia Romer 
Address: California-Berkeley  
Affiliation: Computer Science Division, University of  
Abstract: Computer memory systems are increasingly a bottleneck limiting the performance of applications and operating systems. IRAM architectures, which integrate a CPU with DRAM main memory on a single chip, have the potential to liberate systems from this bottleneck. In order to better understand the performance tradeoffs of a variety of simple IRAM architectures we used the SimOS simulation environment to evaluate seven possible IRAM configurations and one conventional machine configuration. For each configuration, we analyzed the behavior of several standard integer and floating-point benchmark applications as well as memory and operating system microbenchmarks. Our simulation results for these benchmarks indicate that simple IRAM systems of 1998 ought to be as much as 40% faster for memory-intensive applications and nearly twice as fast for memory-intensive operating system services compared to a hypothetical conventional machine of 1998. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Galileo project. http://www.cs.wisc.edu/ galileo/. Computer Sciences Department, University of Wisconin - Madison. </institution>
Reference-contexts: Researchers affiliated with the Galileo project <ref> [1] </ref>, as well as Perl and Sites from DEC SRC [27] examined memory bandwidth requirements of microprocessors and programs, and concluded that pin bandwidth is an important determinant of performance: For the near future at least, computer design is memory design, and software design is memory design.
Reference: [2] <institution> Neomagic corporation. </institution> <address> http://www.neomagic.com/. Santa Clara, CA. </address>
Reference-contexts: Mitsubishi recently fabricated a chip targeted at personal digital assistants that combines a 32-bit RISC processor, 2KB of SRAM cache, and 2MB of DRAM [31] on a single chip. NEC has developed a chip that combines 16 Mb of DRAM with 128 1-bit processors for image-processing applications [5]. NeoMagic <ref> [2] </ref> manufactures a popular chip that features a high-performance graphics accelerator with integrated DRAM. Toshiba is also in the process of fabricating a chip that combines processor and DRAM on a single chip.
Reference: [3] <institution> PPRAM project. http://kasuga.csce.kyushu-u.ac.jp/ ppram/. Computer Science Division, Kyushu Univeristy, </institution> <address> Japan. </address>
Reference-contexts: The Execube [18], a chip with eight 16-bit processors each with 64 KB of DRAM, was targeted at massively parallel systems. Similarly, the M-machine [13] features four superscalar processors and 128 KB per processor. The PPRAM project <ref> [3] </ref> at Kyushu University in Japan plans to fabricate a chip in 1999 with four 32-bit RISC processors, each with 24KB of SRAM cache and 8 MB of DRAM [23].
Reference: [4] <institution> Spec95 published results. </institution> <address> http://www.specbench.org/ osg/cpu95/results/results.html, </address> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: Table 5 lists the total execution time of our application benchmarks for all models. For comparison, we also provide where available the execution times for a AlphaStation 500/500 with a 500 Mhz 21164 processor and 8 MB L3 cache <ref> [4] </ref>. We categorize Linpack, our SPEC'95 programs, and HS-FSYS as memory-intensive, cpu-intensive, and I/O-intensive, respectively. Figures 9 to 14 in the Appendix graphically illustrate the execution behavior of one program from each of these sets for the conventional machine model and the 500 Mhz IRAM model.
Reference: [5] <editor> AIMOTO, Y., ET AL. A 7.68 GIPS, </editor> <booktitle> 3.84 gb/s 1W parallel image-processing RAM integrating a 16 Mb DRAM and 128 processors. In Digest of Technical Papers, 1996 IEEE International SolidState Circuits Conference (San Francisco, </booktitle> <address> CA, </address> <month> February </month> <year> 1996), </year> <pages> pp. 372-373, 476. </pages>
Reference-contexts: Mitsubishi recently fabricated a chip targeted at personal digital assistants that combines a 32-bit RISC processor, 2KB of SRAM cache, and 2MB of DRAM [31] on a single chip. NEC has developed a chip that combines 16 Mb of DRAM with 128 1-bit processors for image-processing applications <ref> [5] </ref>. NeoMagic [2] manufactures a popular chip that features a high-performance graphics accelerator with integrated DRAM. Toshiba is also in the process of fabricating a chip that combines processor and DRAM on a single chip.
Reference: [6] <author> BERSHAD, B. N., DRAVES, R. P., AND FORIN, A. </author> <title> Using microbenchmarks to evaluate system performance. </title> <booktitle> In Proceedings of the Fourth Workshop on Workstation Operating Systems (1992). </booktitle>
Reference-contexts: This rapid re-use is likely to result in most relevant data being in the L1 or L2 cache of the conventional machine. It is questionable whether this caching effect is representative of real invocations of process creation services. These concerns are similar to those expressed by Bershad et al. <ref> [6] </ref>; they suggest flushing caches to measure the impact of caches on microbenchmarks. This issue has larger ramifications for IRAMs: microbench-marks will almost always underestimate the value of IRAM architectures, since the repetition in these benchmarks increases locality and artificially reduces main memory traffic.
Reference: [7] <author> BURGER, D., GOODMAN, J. R., AND KAGI, A. </author> <title> Memory bandwidth limitations of future microprocessors. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture (Philadelphia, </booktitle> <address> PA, </address> <month> May </month> <year> 1996), </year> <pages> pp. 78-89. </pages>
Reference-contexts: In order to overcome this growing processor-memory performance gap, chip manufacturers have come to rely on deep cache hierarchies; however, this stopgap solution may prove inadequate as latency-tolerating techniques such as prefetch-ing and non-blocking caches expose memory bandwidth as the overriding constraint on system performance <ref> [7] </ref>. Without new approaches to memory hierarchies, we may soon hit the memory wall, where application performance is determined solely by average memory access time [33]. The processor-memory gap has received attention from both the hardware and software communities in recent years. <p> The array sizes used by the benchmark are defined to be larger than the cache of the machine being tested, and data is not reused. We chose STREAM because Ousterhout, Burger, and others <ref> [7] </ref> [27] [24] argue that high bandwidth is essential to the performance of operating systems, commercial workloads, and scientific programs. Modified AndreaD Benchmark is a synthetic benchmark designed to uncover the latency of memory accesses for reference strings with differing amounts of locality. <p> This dramatic improvement bodes well for both bandwidth-hungry commercial applications and operating systems. 3.3.2 Memory Latency Benchmark Modified AndreaD Benchmark: Applications with large working sets, such as CAD tools and databases, are often limited by memory latency, even in the presence of large, fast caches <ref> [7] </ref>.
Reference: [8] <author> CHEN, J. B., AND BERSHAD, B. N. </author> <title> The impact of operating system structure on memory system performance. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles (1993). </booktitle>
Reference-contexts: Using similar techniques, Golbus and Gribstad [15] measured an expected slowdown of between 20% and 50% for SRAM implemented in a DRAM process. 4.2 Architecture and Operating Systems Perfor mance Chen and Bershad <ref> [8] </ref> used exhaustive tracing to compare the memory behavior of several applications running on Mach and a monolithic UNIX kernel. They concluded that Mach has significantly worse locality than a monolithic kernel, and that kernel memory references have much less locality than user code. <p> This falls out of the natural internal structure of DRAMs, and it should be exploited to satisfy multiple simultaneous memory requests. * A hardware buffer to allow large block copies without polluting the cache. Chen and Bershad <ref> [8] </ref> concluded that as much as a third of the cost of memory accesses in some workloads is due to cache interference from kernel block copies. * Hardware support to reduce the operating system overhead for cross-address space block copies.
Reference: [9] <author> CHEN, J. B., ET AL. </author> <title> The measured performance of personal computer operating systems. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating System Principles (Copper Mountain Resort, </booktitle> <publisher> CO, </publisher> <month> Dec. </month> <year> 1996), </year> <pages> pp. 299-313. </pages>
Reference-contexts: Another way to collect miss rates is to use hardware counters provided by processors to measure instruction counts, data reads and writes, cache misses, and other low-level events [10] <ref> [9] </ref> [12]. The main limitation of this approach is, again, that the statistics it generates are specific to a particular execution using a particular hardware and software architecture and implementation. 2.1.2 Complete Computer System Simulation Simulation is the principal alternative to estimating performance from cache miss rates. <p> As a result, the performance of the kernel is more sensitive to the latency of the memory system than user applications. Chen and others at Harvard used hardware counters to compare the performance of three operating systems running mi-crobenchmarks and applications on personal computers <ref> [9] </ref>. They concluded that the operating systems had dramatically different performance. Windows for Workgroups was forced by compatibility concerns to maintain segment registers and to make frequent changes in machine mode. Windows NT 3.5 suffers from a microkernel-like, objectoriented structure which places a higher load on instruction caches and TLBs.
Reference: [10] <author> CVETANOVIC, Z., AND BHANDARKAR, D. </author> <title> Performance characterization of the Alpha 21164 microprocessor using TP and SPEC workloads. </title> <booktitle> In Proceedings, Second International Symposium on High-Performance Computer Architecture (San Jose, </booktitle> <address> CA, </address> <month> February </month> <year> 1996), </year> <pages> pp. 270-280. </pages>
Reference-contexts: 1 Introduction Microprocessor performance has recently been improving by 60% per year, while memory latencies are decreasing by only 7% per year [26]. As a consequence, memory accesses now account for up to 75% of execution time in some cases <ref> [10] </ref>. In order to overcome this growing processor-memory performance gap, chip manufacturers have come to rely on deep cache hierarchies; however, this stopgap solution may prove inadequate as latency-tolerating techniques such as prefetch-ing and non-blocking caches expose memory bandwidth as the overriding constraint on system performance [7]. <p> Another way to collect miss rates is to use hardware counters provided by processors to measure instruction counts, data reads and writes, cache misses, and other low-level events <ref> [10] </ref> [9] [12]. The main limitation of this approach is, again, that the statistics it generates are specific to a particular execution using a particular hardware and software architecture and implementation. 2.1.2 Complete Computer System Simulation Simulation is the principal alternative to estimating performance from cache miss rates.
Reference: [11] <author> DUSSEAU, A. </author> <title> Memory system diagnostic program. </title> <type> Personal Communication, </type> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: Modified AndreaD Benchmark is a synthetic benchmark designed to uncover the latency of memory accesses for reference strings with differing amounts of locality. It pinpoints the characteristics of the caches and virtual memory system by striding through ever-larger arrays with ever-larger strides [16] <ref> [11] </ref>. <p> While we did not have sufficient time to port such an application to SimOS, we did examine the average memory latency of strides through large arrays using a memory system diagnostic program written by Andrea Arpaci-Dusseau <ref> [11] </ref> [16]. shows latencies for the same reference pattern on the 250 Mhz IRAM. Note that, for a data set of 1 MB or larger and a stride of 64 Bytes or larger, the deep memory hierarchy of the Ul-traSPARC workstation charges at least 330 ns for each access.
Reference: [12] <author> ENDO, Y., ET AL. </author> <title> Using latency to evaluate interactive system performance. </title> <booktitle> In Proceedings of The Second Symposium on Operating Systems Design and Implementation (Seattle, </booktitle> <address> WA, </address> <month> Oct. </month> <year> 1996), </year> <pages> pp. 185-199. </pages>
Reference-contexts: Another way to collect miss rates is to use hardware counters provided by processors to measure instruction counts, data reads and writes, cache misses, and other low-level events [10] [9] <ref> [12] </ref>. The main limitation of this approach is, again, that the statistics it generates are specific to a particular execution using a particular hardware and software architecture and implementation. 2.1.2 Complete Computer System Simulation Simulation is the principal alternative to estimating performance from cache miss rates.
Reference: [13] <editor> FILLO, M., ET AL. </editor> <booktitle> The M-Machine multicomputer. In Proceedings of the 28th Annual International Symposium on Microarchitecture (Ann Arbor, </booktitle> <address> MI, </address> <month> Nov. </month> <year> 1995), </year> <pages> pp. 146-156. </pages>
Reference-contexts: There have also been several attempts at integrating processors and memory for parallel processing. The Execube [18], a chip with eight 16-bit processors each with 64 KB of DRAM, was targeted at massively parallel systems. Similarly, the M-machine <ref> [13] </ref> features four superscalar processors and 128 KB per processor. The PPRAM project [3] at Kyushu University in Japan plans to fabricate a chip in 1999 with four 32-bit RISC processors, each with 24KB of SRAM cache and 8 MB of DRAM [23].
Reference: [14] <author> FROMM, R., ET AL. </author> <title> The energy efficiency of IRAM architectures. </title> <address> http://www.cs.berkeley.edu/ rfromm/. </address>
Reference-contexts: Since DRAM is approximately 16 to 32 times more dense than SRAM embedded on a microprocessor <ref> [14] </ref> these areas should be roughly equal. We expect that in 1998 0.25 m CMOS process technology will be available for both logic and SRAM manufacture. <p> We identified CPU clock rate, memory latency, and cache line size as the parameters of primary interest to our investigation. Estimates of the slowdown of logic manufactured in a DRAM process range from a factor of 2 to less than 10% <ref> [14] </ref> [32] [26]; we picked three likely CPU clock rates that span this IRAM Conventional Pipeline simple in-order simple in-order CPU frequency vary: 250, 333, 500 MHz 500 MHz technology 0.25 m DRAM 0.25 m logic L1 configuration 64 KB I + 64 KB D 64 KB I + 64 KB <p> Using speedup estimates, they contrasted predicted IRAM benchmark performance with published performance numbers for the Alpha 21164 and CRAY T90 [26]. Their results suggest that for logic slowdowns of up to a factor of two, an IRAM should give equivalent or superior performance for memory-intensive applications. Fromm et al. <ref> [14] </ref> estimated the performance of several SPEC 95 integer programs on alternative IRAM designs, using cache miss rates and base CPI.
Reference: [15] <author> GOLBUS, J., AND GRIBSTAD, B. </author> <title> Simulation and analysis of an ALU and SRAM in a DRAM process. </title> <address> http://www-inst.eecs.berkeley.edu/ gribstad/cs252/. </address>
Reference-contexts: Our simulations predict a speedup of 1.4. Tang and Wu [32] ran Hspice simulations of logic cells using standard logic and DRAM process parameters to calculate an expected 80% slowdown from manufacturing logic in a DRAM process. Using similar techniques, Golbus and Gribstad <ref> [15] </ref> measured an expected slowdown of between 20% and 50% for SRAM implemented in a DRAM process. 4.2 Architecture and Operating Systems Perfor mance Chen and Bershad [8] used exhaustive tracing to compare the memory behavior of several applications running on Mach and a monolithic UNIX kernel.
Reference: [16] <author> HENNESSY, J., AND PATTERSON, D. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: Modified AndreaD Benchmark is a synthetic benchmark designed to uncover the latency of memory accesses for reference strings with differing amounts of locality. It pinpoints the characteristics of the caches and virtual memory system by striding through ever-larger arrays with ever-larger strides <ref> [16] </ref> [11]. <p> While we did not have sufficient time to port such an application to SimOS, we did examine the average memory latency of strides through large arrays using a memory system diagnostic program written by Andrea Arpaci-Dusseau [11] <ref> [16] </ref>. shows latencies for the same reference pattern on the 250 Mhz IRAM. Note that, for a data set of 1 MB or larger and a stride of 64 Bytes or larger, the deep memory hierarchy of the Ul-traSPARC workstation charges at least 330 ns for each access. <p> While such latency-tolerating techniques may delay our impact with the memory wall, their efficacy is ultimately limited by pipeline depth and instruction-level parallelism <ref> [16] </ref>. 5 Future Work In the future, we hope to examine the performance of a wider array of applications. The applications we studied were primarily scientific and engineering workloads, which we would expect to have more locality than commercial applications [20].
Reference: [17] <author> HERROD, S., ET AL. </author> <title> The SimOS simulation environment. </title> <address> http://www-flash.stanford.edu/SimOS, Sept. </address> <year> 1996. </year>
Reference-contexts: The IRAM architecture has drawbacks, however, including the slowdown of SRAM caches and processor logic fabricated in a process optimized for DRAM. In order to understand the performance trade-offs of a variety of simple IRAM architectures, we used the SimOS complete computer system simulation environment [28] <ref> [17] </ref> to simulate seven possible IRAM configurations, on which we ran several integer and floating point applications as well as memory and operating system microbenchmarks. We compared the benchmark results of all seven models to the simulated performance of a hypothetical conventional machine of 1998.
Reference: [18] <author> KOGGE, P. </author> <title> Execube anew architecture for scaleable mpps. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing (Raleigh, NC, Aug. 1994), </booktitle> <volume> vol. 1, </volume> <pages> pp. 77-84. </pages>
Reference-contexts: These speedup estimates are pessimistic, because the study did not distinguish between cache misses accumulated at the user level, in the operating system, and in the kernel idle loop. There have also been several attempts at integrating processors and memory for parallel processing. The Execube <ref> [18] </ref>, a chip with eight 16-bit processors each with 64 KB of DRAM, was targeted at massively parallel systems. Similarly, the M-machine [13] features four superscalar processors and 128 KB per processor.
Reference: [19] <author> KOZYRAKIS, C., AND WANG, H. </author> <title> Evaluation and comparison of existing cache designs implemented as an IRAM. </title> <address> http://www.cs.berkeley.edu/ kozyraki/project/252/. </address>
Reference-contexts: However, they did not explain how these applications were parallelized, nor did they discuss the performance of applications that cannot be parallelized. Kozyrakis and Wang <ref> [19] </ref> estimated IRAM performance using hardware counters on Alpha 21064 and Pentium Pro architectures. Their results indicate that for a logic slowdown of less than 50%, IRAM performance exceeds that of non-IRAM configurations, and for greater logic slowdown, the clock rate is the main determinant of performance.
Reference: [20] <author> MAYNARD, A. M. G., ET AL. </author> <title> Contrasting characteristics and cache performance of technical and multiuser commercial workloads. </title> <booktitle> In Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (San Jose, </booktitle> <address> CA, </address> <month> Oct. </month> <year> 1994), </year> <pages> pp. 145-155. </pages>
Reference-contexts: He concluded that fl Ngeci Bowman, Neal Cardwell and Cynthia Romer are supported by National Science Foundation Graduate Fellowships. lack of adequate memory bandwidth is the primary factor keeping operating system performance from scaling with application performance. Maynard and others from IBM <ref> [20] </ref> demonstrated that commercial applications, such as databases and software development, have poorer instruction locality and spend more time in the operating system and executing context switches or I/O, and thus derive little benefit from large caches and deep memory hierarchies. <p> The applications we studied were primarily scientific and engineering workloads, which we would expect to have more locality than commercial applications <ref> [20] </ref>. Hence we suspect that simulating the execution of databases, CAD tools, multi-user software development, video games, multimedia applications, WWW clients and servers, and personal productivity tools will reveal more applications whose performance is tied to memory bandwidth and latency.
Reference: [21] <author> MCCALPIN, J. </author> <title> Memory bandwidth and machine bal-ance in high performance computers. </title> <journal> In IEEE Computer Society Technical Committee on Computer Architecture (TCCA) Newsletter (Dec. </journal> <year> 1995). </year>
Reference-contexts: performs JPEG image compression 134.perl Perl Interpreter: Uses Perl to perform text and data manipulation Table 3: SPEC'95 Benchmark Programs 2.4.3 Memory Bandwidth and Latency Benchmarks STREAM is a synthetic benchmark that measures the sustainable memory bandwidth of a processor system using four long vector operations, described in Table 4 <ref> [21] </ref>. The array sizes used by the benchmark are defined to be larger than the cache of the machine being tested, and data is not reused.
Reference: [22] <author> MCVOY, L., AND STAELIN, C. lmbench: </author> <title> Portable tools for performance analysis. </title> <booktitle> In The Proceedings of the USENIX 1996 Annual Technical Conference (San Diego, </booktitle> <address> CA, </address> <month> Jan. </month> <year> 1996), </year> <pages> pp. 279-294. </pages>
Reference-contexts: HSFSYS is a public-domain form-based handwriting recognition algorithm from NIST which uses eigenvector calculations to implement pattern recognition techniques. 2.4.2 Operating System Benchmarks lmbench is a microbenchmark suite that measures the latency and bandwidth of common operating system services <ref> [22] </ref>. We used it to benchmark the bandwidth of memory accesses, I/O to cached files, local inter-process communication latencies, and process creation overhead.
Reference: [23] <author> MURAKAMI, K., SHIRAKAWA, S., AND MIYAJIMA, H. </author> <title> Parallel processing RAM chip with 256Mb DRAM and quad processors. </title> <booktitle> In To appear in 1997 IEEE International Solid-State Circuits Conference (San Francisco, </booktitle> <address> CA, </address> <month> Feb. </month> <year> 1997). </year>
Reference-contexts: Main memory in 1998 conventional systems will probably be built out of fast SDRAM. In predicting an IRAM configuration for 1998, we took into account the PPRAM project's proposed 1999 fabrication <ref> [23] </ref> 1 This is a very rough approximation, as typical PCs will very likely include 64MB of main memory by 1998, however, this would necessitate modeling either 1) 1Gb IRAMs, which won't be feasible until 2002 or 2) IRAM machines with off-chip DRAM, which requires operating system support that we could <p> Similarly, the M-machine [13] features four superscalar processors and 128 KB per processor. The PPRAM project [3] at Kyushu University in Japan plans to fabricate a chip in 1999 with four 32-bit RISC processors, each with 24KB of SRAM cache and 8 MB of DRAM <ref> [23] </ref>. Their simulations, though limited to SPEC 95 integer programs, indicate that this architecture outperforms the equal-area alternatives asingle superscalar processor with main memory and four superscalar processors with cache only by factors of 2.22 and 1.41, respectively.
Reference: [24] <author> OUSTERHOUT, J. K. </author> <title> Why aren't operating systems getting faster as fast as hardware? In Proceedings of the USENIX Summer Conference (Anaheim, </title> <address> CA, </address> <month> June </month> <year> 1990). </year>
Reference-contexts: Without new approaches to memory hierarchies, we may soon hit the memory wall, where application performance is determined solely by average memory access time [33]. The processor-memory gap has received attention from both the hardware and software communities in recent years. Using performance measurements of several microbenchmarks, Ousterhout <ref> [24] </ref> investigated the growing disparity between processor and operating systems speedup. He concluded that fl Ngeci Bowman, Neal Cardwell and Cynthia Romer are supported by National Science Foundation Graduate Fellowships. lack of adequate memory bandwidth is the primary factor keeping operating system performance from scaling with application performance. <p> Modified Andrew Benchmark (MAB) is a file system benchmark that creates directories, copies a source tree into these directories, recursively stats each file in the tree, searches through each source file, then and compiles each of the 17 short C source code files <ref> [24] </ref>. We used the variant of this benchmark included with the SimOS 1.0 release [29]. We chose MAB in order to round out our study of the performance of operating systems on IRAM architectures. <p> The array sizes used by the benchmark are defined to be larger than the cache of the machine being tested, and data is not reused. We chose STREAM because Ousterhout, Burger, and others [7] [27] <ref> [24] </ref> argue that high bandwidth is essential to the performance of operating systems, commercial workloads, and scientific programs. Modified AndreaD Benchmark is a synthetic benchmark designed to uncover the latency of memory accesses for reference strings with differing amounts of locality.
Reference: [25] <author> PATTERSON, D., ANDERSON, T., AND YELICK, K. </author> <title> A case for intelligent DRAM: </title> <booktitle> IRAM. In HotChips VIII (Stan-ford, </booktitle> <address> CA, </address> <year> 1996), </year> <pages> pp. 75-93. </pages>
Reference-contexts: They concluded that the simulated system had integer performance comparable to 1995's fastest shipping Alpha workstations, with floating point performance roughly a factor of two slower than Alpha systems. The Berkeley IRAM research group has also investigated the performance tradeoffs of IRAM computer systems relative to several traditional designs <ref> [25] </ref>. Using speedup estimates, they contrasted predicted IRAM benchmark performance with published performance numbers for the Alpha 21164 and CRAY T90 [26]. Their results suggest that for logic slowdowns of up to a factor of two, an IRAM should give equivalent or superior performance for memory-intensive applications.
Reference: [26] <author> PATTERSON, D., ET AL. </author> <title> Intelligent ram (IRAM): Chips that remember and compute. </title> <booktitle> In To appear in 1997 IEEE International Solid-State Circuits Conference (San Fran-cisco, </booktitle> <address> CA, </address> <month> Feb. </month> <year> 1997). </year>
Reference-contexts: 1 Introduction Microprocessor performance has recently been improving by 60% per year, while memory latencies are decreasing by only 7% per year <ref> [26] </ref>. As a consequence, memory accesses now account for up to 75% of execution time in some cases [10]. <p> We identified CPU clock rate, memory latency, and cache line size as the parameters of primary interest to our investigation. Estimates of the slowdown of logic manufactured in a DRAM process range from a factor of 2 to less than 10% [14] [32] <ref> [26] </ref>; we picked three likely CPU clock rates that span this IRAM Conventional Pipeline simple in-order simple in-order CPU frequency vary: 250, 333, 500 MHz 500 MHz technology 0.25 m DRAM 0.25 m logic L1 configuration 64 KB I + 64 KB D 64 KB I + 64 KB D L1 <p> The Berkeley IRAM research group has also investigated the performance tradeoffs of IRAM computer systems relative to several traditional designs [25]. Using speedup estimates, they contrasted predicted IRAM benchmark performance with published performance numbers for the Alpha 21164 and CRAY T90 <ref> [26] </ref>. Their results suggest that for logic slowdowns of up to a factor of two, an IRAM should give equivalent or superior performance for memory-intensive applications. Fromm et al. [14] estimated the performance of several SPEC 95 integer programs on alternative IRAM designs, using cache miss rates and base CPI.
Reference: [27] <author> PERL, S. E., AND SITES, R. L. </author> <title> Studies of Windows NT performance using dynamic execution traces. </title> <booktitle> In Proceedings of The Second Symposium on Operating Systems Design and Implementation (Seattle, </booktitle> <address> WA, </address> <month> Oct. </month> <year> 1996), </year> <pages> pp. 169-183. </pages>
Reference-contexts: Researchers affiliated with the Galileo project [1], as well as Perl and Sites from DEC SRC <ref> [27] </ref> examined memory bandwidth requirements of microprocessors and programs, and concluded that pin bandwidth is an important determinant of performance: For the near future at least, computer design is memory design, and software design is memory design. <p> The array sizes used by the benchmark are defined to be larger than the cache of the machine being tested, and data is not reused. We chose STREAM because Ousterhout, Burger, and others [7] <ref> [27] </ref> [24] argue that high bandwidth is essential to the performance of operating systems, commercial workloads, and scientific programs. Modified AndreaD Benchmark is a synthetic benchmark designed to uncover the latency of memory accesses for reference strings with differing amounts of locality.
Reference: [28] <author> ROSENBLUM, M., ET AL. </author> <title> Complete computer system simulation: The SimOS approach. </title> <booktitle> In IEEE Parallel and Distributed Technology: Systems and Applications (Winter 1995), </booktitle> <volume> vol. 3, </volume> <pages> pp. 34-43. </pages>
Reference-contexts: The IRAM architecture has drawbacks, however, including the slowdown of SRAM caches and processor logic fabricated in a process optimized for DRAM. In order to understand the performance trade-offs of a variety of simple IRAM architectures, we used the SimOS complete computer system simulation environment <ref> [28] </ref> [17] to simulate seven possible IRAM configurations, on which we ran several integer and floating point applications as well as memory and operating system microbenchmarks. We compared the benchmark results of all seven models to the simulated performance of a hypothetical conventional machine of 1998. <p> Complete computer system simulation allows for quantitative evaluation of all aspects of an entire computer system in the presence of the complex interactions between components of modern computer systems <ref> [28] </ref>.
Reference: [29] <editor> ROSENBLUM, M., ET AL. </editor> <booktitle> The impact of architectural trends on operating system performance. In The 15th ACM Symposium on Operating Systems Principles (San Jose, </booktitle> <address> CA, </address> <month> Dec. </month> <year> 1995). </year>
Reference-contexts: These modifications were sufficient to model simple conventional and IRAM machines. 2.2.1 Simulator Validation Rosenblum and members of the SimOS team have verified that SimOS 1.0 accurately models the performance of real SGI machines <ref> [29] </ref>, so in the absence of any modifications SimOS should behave correctly. <p> We used the variant of this benchmark included with the SimOS 1.0 release <ref> [29] </ref>. We chose MAB in order to round out our study of the performance of operating systems on IRAM architectures. <p> Windows for Workgroups was forced by compatibility concerns to maintain segment registers and to make frequent changes in machine mode. Windows NT 3.5 suffers from a microkernel-like, objectoriented structure which places a higher load on instruction caches and TLBs. Rosenblum et al <ref> [29] </ref> used SimOS to investigate the impact of varying architectural parameters on operating system performance. Their results indicate that although memory stalls often account for half of kernel execution time, larger, non-blocking caches and dynamic scheduling tend to alleviate memory latency effects.
Reference: [30] <author> SAULSBURY, A., PONG, F., AND NOWATZYK, A. </author> <title> Missing the memory wall: The case for processor/memory integration. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture (Philadelphia, </booktitle> <address> PA, </address> <month> May </month> <year> 1996), </year> <pages> pp. 90-101. </pages>
Reference-contexts: We expect that in 1998 0.25 m CMOS process technology will be available for both logic and SRAM manufacture. Therefore by adding an extra 15% to the area of a 256Mb DRAM chip we obtain adequate space to include a processor and SRAM L1 cache on the IRAM chip <ref> [30] </ref>. The IRAM's CPU and L1 cache will likely be somewhat larger than those of the conventional machine because they are implemented in a process optimized for DRAM. Nonetheless, this bloat should be insignificant relative to the area occupied by the 32 MB of DRAM. <p> Toshiba is also in the process of fabricating a chip that combines processor and DRAM on a single chip. These devices demonstrate the feasibility of implementing SRAM and logic in a DRAM process, although performance analyses have not yet been published for any of them. Sun Microsystems <ref> [30] </ref> analyzed a hypothetical system composed of a simple RISC CPU integrated with 32MB of DRAM, a 16KB data cache, and an 8KB instruction cache.
Reference: [31] <author> SHIMIZU, T., ET AL. </author> <title> A multimedia 32b RISC microprocessor with 16Mb DRAM. </title> <booktitle> In Digest of Technical Papers, 1996 IEEE International SolidState Circuits Conference (San Francisco, </booktitle> <address> CA, </address> <month> Feb. </month> <year> 1996), </year> <pages> pp. 216-217, 448. </pages>
Reference-contexts: Mitsubishi recently fabricated a chip targeted at personal digital assistants that combines a 32-bit RISC processor, 2KB of SRAM cache, and 2MB of DRAM <ref> [31] </ref> on a single chip. NEC has developed a chip that combines 16 Mb of DRAM with 128 1-bit processors for image-processing applications [5]. NeoMagic [2] manufactures a popular chip that features a high-performance graphics accelerator with integrated DRAM.
Reference: [32] <author> TANG, S., AND WU, K. </author> <title> Logic in a DRAM process. </title> <address> http://www-inst.eecs.berkeley.edu/ kevinw/cs252/. </address>
Reference-contexts: We identified CPU clock rate, memory latency, and cache line size as the parameters of primary interest to our investigation. Estimates of the slowdown of logic manufactured in a DRAM process range from a factor of 2 to less than 10% [14] <ref> [32] </ref> [26]; we picked three likely CPU clock rates that span this IRAM Conventional Pipeline simple in-order simple in-order CPU frequency vary: 250, 333, 500 MHz 500 MHz technology 0.25 m DRAM 0.25 m logic L1 configuration 64 KB I + 64 KB D 64 KB I + 64 KB D <p> Their results for Lin-pack agree quite closely with ours. Applying their models and miss rates to the parameters from our 500 MHz IRAM and conventional models yields a predicted speedup of 1.45 from using an IRAM Alpha. Our simulations predict a speedup of 1.4. Tang and Wu <ref> [32] </ref> ran Hspice simulations of logic cells using standard logic and DRAM process parameters to calculate an expected 80% slowdown from manufacturing logic in a DRAM process.

References-found: 32

