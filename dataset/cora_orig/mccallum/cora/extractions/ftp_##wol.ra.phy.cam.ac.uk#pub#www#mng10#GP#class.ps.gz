URL: ftp://wol.ra.phy.cam.ac.uk/pub/www/mng10/GP/class.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Title: Variational Gaussian Process Classifiers  
Author: Mark N. Gibbs David J.C. MacKay 
Date: May 28, 1997  
Address: Cambridge CB3 0HE United Kingdom  Cambridge CB3 0HE United Kingdom  
Affiliation: Cavendish Laboratory  Cavendish Laboratory  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Barber, D., and Williams, C. K. I., </author> <title> (1996) Gaussian processes for Bayesian classification via hybrid Monte Carlo. </title> <note> Submitted to NIPS. </note>
Reference-contexts: We would also like to set the hyperparameters of the covariance function to their most probable values given the data (a Monte Carlo approach could also be used <ref> (Barber and Williams 1996) </ref>). This is not possible as we do not have an analytic expression for P (Djfi). However we can maximize Z 0 and Z 00 with respect to fi to obtain approximations to the most probable fi given the data.
Reference: <author> Gibbs, M. N., and MacKay, D. J. C., </author> <title> (1996) Efficient implementation of Gaussian processes for interpolation. </title> <note> in preparation. </note>
Reference-contexts: Unlike the regression covariance function <ref> (Gibbs and MacKay 1996) </ref> we assume a (x) to be noise free. However we do introduce a "jitter" term ffi mn J (Neal 1997) to make the matrix computations well-conditioned. We choose the magnitude of J is small in comparison to 1 .
Reference: <author> Hinton, G. E., and van Camp, D. </author> <title> (1993) Keeping neural networks simple by minimizing the description length of the weights. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pp. 5-13. </pages> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference: <author> Ichikawa, K., Bhadeshia, H. K. D. H., and MacKay, D. J. C., </author> <title> (1996) Model for hot cracking in low-alloy steel weld metals. </title> <note> In preparation. </note>
Reference-contexts: We wish to predict whether a given weld will crack by examining the dependence of cracking on 13 specific characteristics of a weld. In a previous treatment of this problem using Bayesian neural networks <ref> (Ichikawa et al. 1996) </ref> the relationship between cracking and carbon content was highlighted and compared with experimental data. We performed a similar analysis using VGCs. An initial test was performed using a training set of 77 examples and a test set of 77 examples. <p> We performed a similar analysis using VGCs. An initial test was performed using a training set of 77 examples and a test set of 77 examples. The test error rates and test log likelihoods for the VGC and the Bayesian neural network approach <ref> (Ichikawa et al. 1996) </ref> can be seen in Table 2 where the test log likelihood is defined as test log likelihood = N test X n=1 where t n is the true test set classification (either 0 or 1) and ^ t n is the prediction P (t n = 1jD).
Reference: <author> Jaakkola, T. S., and Jordan, M. I. </author> <title> (1996) Computing upper and lower bounds on likelihoods in intractable networks. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in AI . publisher? MacKay, </booktitle> <editor> D. J. C. </editor> <title> (1992a) The evidence framework applied to classification networks. </title> <booktitle> Neural Computation 4 (5): </booktitle> <pages> 698-714. </pages>
Reference-contexts: Barber and Williams (1996) have implemented classifiers based on Gaussian process priors using Laplace approximations. Neal (1997) has implemented a Monte Carlo approach to implementing a Gaussian process classifier. 1 In this paper another approach is suggested based on the methods of Jaakkola and Jordan <ref> (Jaakkola and Jordan 1996) </ref>. We obtain tractable upper and lower bounds for the unnormalized posterior density P (ftgja)P (a). These bounds are parameterized by variational parameters which are adjusted in order to obtain the tightest possible fit. <p> We can define upper and lower bounds on the sigmoid, i.e. on P (t = 1ja (x)), <ref> (Jaakkola and Jordan 1996) </ref> as follows: 2 P (t n = 1ja (x n )) Q (t n = 1ja n ; -n ) = g (-n ) exp (a n -n )=2 (-n )(a 2 n ) (8) where a n = a (x n ) and g (a n
Reference: <author> MacKay, D. J. C. </author> <title> (1992b) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 448-472. </pages>
Reference-contexts: bound approximation P Q (a (x N+1 )jx N+1 ; D; f-n g; fi) into equa tion 3 we find P (t N+1 = 1jx N+1 ; D) ' g (t (s l )a MP using the approximation for the integral of the product of a Gaussian and a sigmoid <ref> (MacKay 1992b) </ref>, Z dx g (x)Gaussian (a MP l ; s 2 l ) (23) where t (s) = 1= p We should note that, although we have been using the lower bound on P (t n = 1ja (x n )), we have not generated a lower bound on the
Reference: <author> MacKay, D. J. C. </author> <title> (1995a) Free energy minimization algorithm for decoding and cryptanalysis. </title> <note> Electronics Letters 31 (6): 446-447. 11 MacKay, </note> <author> D. J. C. </author> <title> (1995b) Probable networks and plausible predictions a review of practical Bayesian methods for supervised neural networks. Network: </title> <booktitle> Computation in Neural Systems 6: </booktitle> <pages> 469-505. </pages>
Reference: <author> Neal, R. M., </author> <title> (1995) Bayesian Learning for Neural Networks. </title> <institution> Dept. of Computer Science, Univ. of Toronto dissertation. </institution>
Reference-contexts: Most parametric models are in fact special cases of Gaussian processes, with the covariance matrix depending on the details of the choice of basis functions h (x) and the prior P (w) <ref> (Neal 1995) </ref>. Efficient methods for implementing Gaussian processes are described in Gibbs and MacKay (1996). For classification models there are two well-established approaches to Bayesian inference: Gaussian approximations centred on the posterior modes (MacKay 1992a) and Monte Carlo methods (Neal 1995). <p> choice of basis functions h (x) and the prior P (w) <ref> (Neal 1995) </ref>. Efficient methods for implementing Gaussian processes are described in Gibbs and MacKay (1996). For classification models there are two well-established approaches to Bayesian inference: Gaussian approximations centred on the posterior modes (MacKay 1992a) and Monte Carlo methods (Neal 1995). Barber and Williams (1996) have implemented classifiers based on Gaussian process priors using Laplace approximations.
Reference: <author> Neal, R. M. </author> <title> (1997) Monte carlo implementation of gaussian process models for bayesian regression and classification. </title> <type> preprint. </type>
Reference-contexts: Unlike the regression covariance function (Gibbs and MacKay 1996) we assume a (x) to be noise free. However we do introduce a "jitter" term ffi mn J <ref> (Neal 1997) </ref> to make the matrix computations well-conditioned. We choose the magnitude of J is small in comparison to 1 . The product of sigmoid functions in equation 5 generally makes the integral in equation 4 analytically intractable.
Reference: <author> Ripley, B. D. </author> <title> (1994) Flexible non-linear approaches to classification. In From Statistics to Neural Networks. Theory and Pattern Recognition Applications, </title> <editor> ed. by J. H. F. V. Cherkassky and H. Wechsler, </editor> <booktitle> number subseries F in ASI Proceedings. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Ripley, B. D. </author> <title> (1996) Pattern Recognition and Neural Networks. </title> <publisher> C.U.P. </publisher>
Reference: <author> Skilling, J. </author> <title> (1993) Bayesian numerical analysis. In Physics and Probability, </title> <editor> ed. by W. T. G. Jr. and P. Milonni. </editor> <publisher> C.U.P. </publisher>
Reference-contexts: The cost of direct methods of inversion may become prohibitive when the number of data points N is greater than ' 1000. In Gibbs and MacKay (1996) efficient methods for matrix inversion <ref> (Skilling 1993) </ref> are developed that when applied to the Gaussian process framework allow large data sets to be tackled. Another problem with the variational approach is the profileration of variational parameters when dealing with large amounts of the data.
Reference: <author> Williams, C. K. I., </author> <title> (1995) Regression with Gaussian processes. </title> <note> To appear in Annals of Mathematics and Artificial Intelligence. </note>
Reference: <author> Williams, C. K. I., and Rasmussen, C. E. </author> <title> (1996) Gaussian processes for regression. </title> <booktitle> In Advances in Neural Information Processing Systems 8 , ed. </booktitle> <editor> by D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo. </editor> <publisher> MIT Press. </publisher> <pages> 12 </pages>
Reference-contexts: We would also like to set the hyperparameters of the covariance function to their most probable values given the data (a Monte Carlo approach could also be used <ref> (Barber and Williams 1996) </ref>). This is not possible as we do not have an analytic expression for P (Djfi). However we can maximize Z 0 and Z 00 with respect to fi to obtain approximations to the most probable fi given the data.
References-found: 14

