URL: http://theory.lcs.mit.edu/~slonim/AS91.ps
Refering-URL: http://theory.lcs.mit.edu/~slonim/incomplete.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: [Ang87] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <booktitle> Information and Computation 75, </booktitle> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: To address this problem, Goldman, Kearns, and Schapire [GKS90] consider a model of persistent noise in membership queries, which is related to (but stronger than) the model we adopt here. Our model relies on the definition of a minimally adequate teacher <ref> [Ang87] </ref>, in which a learner tries to learn a target concept h fl from a known concept class H. In this (error-free) model, the learner is assisted by a teacher that answers two types of queries. <p> Another is whether we can find polynomial time algorithms in this model for other learning problems known to have polynomial time algorithms using equivalence and membership oracles. For example, deterministic finite state acceptors <ref> [Ang87] </ref>, simple deterministic languages [Ish89], read-once formulas [AHK89], - formula decision trees [Han90], switch configurations [RS90], and propositional Horn sentences [AFP90] all have such algorithms. Of these, the problem of propositional Horn sentences is the closest to the case of monotone DNF, but the basic algorithms are quite different.
Reference: [Ang88] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <booktitle> Machine Learning 2, </booktitle> <year> 1988, </year> <pages> pp. 319-342. </pages>
Reference-contexts: In this paper we examine the learnabil-ity of monotone DNF formulas over n variables. There is a known algorithm for exactly learning these formulas from a minimally adequate teacher in time polynomial in n and m, where m is the number of terms in the target formula <ref> [Ang88] </ref>. We present an algorithm for the new model that, for any failure probability p &lt; 1, produces with probability at least 1 2 s a hypothesis equivalent to the target concept in time polynomial in n, m and s. <p> Moreover, in this representation, since we have assumed that the target concept h fl is minimized, the terms of h fl are precisely the minimum positive examples of h fl . 3 USING INCOMPLETE MEMBERSHIP QUERIES A key subprocedure in the monotone DNF algorithm of <ref> [Ang88] </ref> takes a positive example v of the target concept h fl and uses membership queries to reduce v to a minimum positive example of h fl . The algorithm starts with the empty formula and uses equivalence queries to generate new positive counterexamples to reduce.
Reference: [Ang90] <author> D. Angluin. </author> <title> Negative results for equivalence queries. </title> <booktitle> Machine Learning 5, </booktitle> <year> 1990, </year> <pages> pp. 121-150. </pages>
Reference-contexts: However, the choice of counterexample may not depend on the answers to membership queries not yet made. This adversary is strong enough to generate the "worst-case" counterexamples used to provide lower bounds for equivalence queries <ref> [Ang90] </ref>, but it cannot predict the blind spots of the incomplete membership oracle. It may at first seem odd to assume that membership queries are flawed while equivalence queries remain correct. <p> However, there is no algorithm that runs in time polynomial in n and m and exactly identifies any monotone DNF formula using equivalence queries only <ref> [Ang90] </ref>. Thus, the quantification of "with high probability" is neces sary in the statement of our main result. 2 PRELIMINARIES The target concepts are monotone DNF formulas over the variables x 1 ; : : : ; x n for some positive integer n.
Reference: [AFP90] <author> D. Angluin, M. Frazier, and L. Pitt. </author> <title> Learning conjunctions of Horn clauses. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1990, </year> <pages> pp. 186-192. </pages>
Reference-contexts: For example, deterministic finite state acceptors [Ang87], simple deterministic languages [Ish89], read-once formulas [AHK89], - formula decision trees [Han90], switch configurations [RS90], and propositional Horn sentences <ref> [AFP90] </ref> all have such algorithms. Of these, the problem of propositional Horn sentences is the closest to the case of monotone DNF, but the basic algorithms are quite different.
Reference: [AHK89] <author> D. Angluin, L. Hellerstein, and M. Karpinski. </author> <title> Learning read-once formulas with queries. </title> <type> Technical Report, </type> <institution> University of California at Berkeley, </institution> <note> No. 89/528, 1989. Also issued as: </note> <institution> International Computer Science Institute Report TR-89-05099. </institution>
Reference-contexts: Another is whether we can find polynomial time algorithms in this model for other learning problems known to have polynomial time algorithms using equivalence and membership oracles. For example, deterministic finite state acceptors [Ang87], simple deterministic languages [Ish89], read-once formulas <ref> [AHK89] </ref>, - formula decision trees [Han90], switch configurations [RS90], and propositional Horn sentences [AFP90] all have such algorithms. Of these, the problem of propositional Horn sentences is the closest to the case of monotone DNF, but the basic algorithms are quite different.
Reference: [AL88] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples, </title> <booktitle> Machine Learning 2, </booktitle> <year> 1988, </year> <pages> pp. 343-370. </pages>
Reference: [GKS90] <author> S. Goldman, M. Kearns, and R. Schapire. </author> <title> Exact identification of circuits using fixed points of amplification functions. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1990, </year> <pages> pp. 193-202. </pages>
Reference-contexts: To address this problem, Goldman, Kearns, and Schapire <ref> [GKS90] </ref> consider a model of persistent noise in membership queries, which is related to (but stronger than) the model we adopt here.
Reference: [Han90] <author> T. Hancock. </author> <title> Identifying -formula decision trees with queries. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1990, </year> <pages> pp. 23-37. </pages>
Reference-contexts: Another is whether we can find polynomial time algorithms in this model for other learning problems known to have polynomial time algorithms using equivalence and membership oracles. For example, deterministic finite state acceptors [Ang87], simple deterministic languages [Ish89], read-once formulas [AHK89], - formula decision trees <ref> [Han90] </ref>, switch configurations [RS90], and propositional Horn sentences [AFP90] all have such algorithms. Of these, the problem of propositional Horn sentences is the closest to the case of monotone DNF, but the basic algorithms are quite different.
Reference: [Ish90] <author> H. Ishizaka. </author> <title> Polynomial time learnabil-ity of simple deterministic languages. </title> <booktitle> Machine Learning 5, </booktitle> <year> 1990, </year> <pages> pp. 151-164. </pages>
Reference: [Kea89] <author> M. Kearns. </author> <title> The Computational Complexity of Machine Learning. </title> <type> Doctoral dissertation, </type> <institution> Harvard University, </institution> <year> 1989. </year> <note> To be published by MIT Press. </note>
Reference: [KLPV87] <author> M. Kearns, M. Li, L. Pitt, and L. Valiant. </author> <title> On the learnability of Boolean formulae. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <publisher> ACM Press, </publisher> <year> 1987, </year> <pages> pp. 285-295. </pages>
Reference: [Lai87] <author> P. Laird. </author> <title> Learning from Good Data and Bad. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1987. </year> <title> Published as Learning from Good and Bad Data by Kluwer Academic Publishers, </title> <year> 1988. </year>
Reference: [Lit88] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. </title> <booktitle> Machine Learning 2, </booktitle> <year> 1988, </year> <pages> pp. 285-318. </pages>
Reference-contexts: (produced and classified by Nature) with the assistance of a teacher who can correctly classify some but not all of the possible examples (modelled by an incomplete membership oracle.) If we can get an efficient learning algorithm using equivalence queries and an incomplete membership oracle, then by a general transformation <ref> [Lit88] </ref> we can obtain an efficient algorithm for the prediction task in the mistake bounded model that uses an incomplete membership oracle.
Reference: [Maa91] <author> W. Maass. </author> <title> On-line learning with an oblivious environment and the power of randomization. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: concept is still possible (since an algorithm could simply perform identification by enumeration using equivalence queries.) However, in this new model we must specify the type of adversary selecting the counterexamples. (This is also an issue in the standard model when randomized learning algorithms are considered, as Maass has shown <ref> [Maa91] </ref>.) We assume that the adversary is "online." That is, the choice of a counterexample may depend on the target hypothesis and the history of the computation to the point at which the query is asked, including the hypothesis queried, all previous queries and their answers, and any previous coin-flips of
Reference: [RS90] <author> V. Raghavan and S. Schach. </author> <title> Learning switch configurations. </title> <booktitle> In Proceedings of Third Annual Workshop on Computa--tional Learning Theory, </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1990, </year> <pages> pp. 38-51. </pages>
Reference-contexts: Another is whether we can find polynomial time algorithms in this model for other learning problems known to have polynomial time algorithms using equivalence and membership oracles. For example, deterministic finite state acceptors [Ang87], simple deterministic languages [Ish89], read-once formulas [AHK89], - formula decision trees [Han90], switch configurations <ref> [RS90] </ref>, and propositional Horn sentences [AFP90] all have such algorithms. Of these, the problem of propositional Horn sentences is the closest to the case of monotone DNF, but the basic algorithms are quite different.
Reference: [Sak90] <author> Y. Sakakibara. </author> <title> On learning from queries and counterexamples in the presence of noise. </title> <note> To appear in Information Processing Letters. </note>
Reference-contexts: Some work has been done on errors in the distribution free model of learning introduced by Valiant [Val84]. This includes studies of malicious and random errors in classification and attributes [Val85,Kea89,AL88,Lai87,SV88,Slo88,Slo89]. There has been less work on errors in query models. Sakakibara <ref> [Sak90] </ref> proposes a model of noise in queries, which assumes that every time a query is asked there is some independent probability of getting the wrong answer.
Reference: [SV88] <author> G. Shackelford and D. Volper. </author> <title> Learning k-DNF with noise in the attributes, </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1988, </year> <pages> pp. 97-103. </pages>
Reference: [Slo88] <author> R. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA, </address> <year> 1988, </year> <pages> pp. 91-96. </pages>
Reference: [Slo89] <author> R. Sloan. </author> <title> Computational Learning Theory: New Models and Algorithms. Doctoral dissertation, </title> <publisher> MIT, </publisher> <year> 1989. </year>
Reference: [Val84] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <type> CACM 27, </type> <year> 1984, </year> <pages> pp. 1134-1142. </pages>
Reference-contexts: By defining a measurable tradeoff between membership and equivalence queries, our model yields some insight into the degree of additional information that membership queries provide a learning algorithm. Some work has been done on errors in the distribution free model of learning introduced by Valiant <ref> [Val84] </ref>. This includes studies of malicious and random errors in classification and attributes [Val85,Kea89,AL88,Lai87,SV88,Slo88,Slo89]. There has been less work on errors in query models.
Reference: [Val85] <author> L. Valiant. </author> <title> Learning disjunctions of conjunctions, </title> <booktitle> In Proceedings of the 9th IJCAI, </booktitle> <year> 1985, </year> <pages> pp. 560-566. </pages>
References-found: 21

