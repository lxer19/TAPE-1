URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94523-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Abstract  
Abstract: Irregularly coupled regular mesh problems arise in important application areas, e.g., fluid flow simulation and aerodynamic simulation. Such simulations are computationally intensive and inherently parallel. Unfortunately, parallel programming languages such as High Performance Fortran provide little explicit support for these problems. It is essential that these applications be automatically parallelized when the mesh configurations are generated automatically. Here we develop an algorithm that automatically determines, at compile time, distributions for a class of irregularly coupled regular mesh problems. The input for the algorithm is a well-structured modern Fortran program, a description of the regular meshs, and a description of the topological connections between the regular meshs. We illustrate the description of topological connections by extending Fortran D specifications with a statement that allows the user to specify connectivity between meshs. In addition, the user specifies the target architecture and the number of processors, P . A standard processor configuration is an n-dimensional processor mesh with at most the same number of dimensions as the target architecture and exactly P processors. The algorithm begins by finding an optimal (according to a model) mapping of array dimensions onto processor dimensions for each mesh and standard processor configuration. Next, the standard processor configuration and associated mapping that minimizes the total computation and internal (to the meshs) communication is selected. Finally, the meshs are aligned with respect to each other in order to reduce coupling communication cost in the selected mapping. This results in full distribution and alignment specifications. These specifications are used as input to a High Performance Fortran program that applies the selected mapping (distribution and alignment) for the execution of the simulation code. Excerpts from such a High Performance Fortran program are presented to illustrate style and compilation issues. Some of the advantages to this automatic approach are that it is applicable for run-time or compile-time analysis and communication generation; conforms to a uniform memory model; is applicable even when automatic verification of parallelism over meshs is not possible; is applicable for SIMD architectures; and is easily applicable to Fortran D or High Performance Fortran compilation. A sample High Performance Fortran code was used with three real-world irregularly coupled regular mesh configurations to validate this algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.E. Allwright. </author> <title> Techniques in multiblock domain decomposition and surface grid generation. </title> <editor> In S. Sengupta, J. Hauser, P.R. Eiseman, and J.F. Thompson, editors, </editor> <booktitle> Numerical Grid Generation in Computational Fluid Mechanics '88, </booktitle> <pages> pages 559-568. </pages> <publisher> Pineridge Press, </publisher> <year> 1988. </year>
Reference-contexts: They are: 1. create multiple grids that adjoin each other, or 2. create multiple grids that overlap each other. In recent years, it has become possible to automatically generate the grids used in some ICRM problems <ref> [15, 19, 14, 1, 17, 20, 16] </ref>. <p> 3 ; I 4 ; :::; I D:dimensions ) with D (I 1 + offset 1 ; I 3 + offset 3 ; I 2 + offset 2 ; I 4 + offset 4 ; :::; I D:dimensions + offset D:dimensions ) Page 18 becomes align X (wrap ((1 D:direction <ref> [1] </ref>) fl ( 1 2 ) fl D:size [1] + D:direction [1] fl (I 1 )); wrap ((1 D:direction [2]) fl ( 1 2 ) fl D:size [2] + D:direction [2] fl (I 2 )); :::; wrap ((1 D:direction [D:dimensions]) fl ( 1 2 ) fl D:size [D:dimensions] +D:direction [D:dimensions] fl <p> ) with D (I 1 + offset 1 ; I 3 + offset 3 ; I 2 + offset 2 ; I 4 + offset 4 ; :::; I D:dimensions + offset D:dimensions ) Page 18 becomes align X (wrap ((1 D:direction <ref> [1] </ref>) fl ( 1 2 ) fl D:size [1] + D:direction [1] fl (I 1 )); wrap ((1 D:direction [2]) fl ( 1 2 ) fl D:size [2] + D:direction [2] fl (I 2 )); :::; wrap ((1 D:direction [D:dimensions]) fl ( 1 2 ) fl D:size [D:dimensions] +D:direction [D:dimensions] fl (I D:dimensions ))) with D (I map 1 <p> (I 1 + offset 1 ; I 3 + offset 3 ; I 2 + offset 2 ; I 4 + offset 4 ; :::; I D:dimensions + offset D:dimensions ) Page 18 becomes align X (wrap ((1 D:direction <ref> [1] </ref>) fl ( 1 2 ) fl D:size [1] + D:direction [1] fl (I 1 )); wrap ((1 D:direction [2]) fl ( 1 2 ) fl D:size [2] + D:direction [2] fl (I 2 )); :::; wrap ((1 D:direction [D:dimensions]) fl ( 1 2 ) fl D:size [D:dimensions] +D:direction [D:dimensions] fl (I D:dimensions ))) with D (I map 1 (1) + D:elt <p> 3) align ze (wrap (i); wrap (j)) with E (i; j + 80 fl 3) Note that in HPF the alignments must be shifted as HPF does not support wrap alignment. 3.7.3 Distribution Generation Finally, we generate distributions for each decomposition D of the form distribute D (block (SP C:procs <ref> [1] </ref>); block (SP C:procs [2]); :::block (SP C:procs [D:dimensions])) where SP C:procs [i] is set to one for i &gt; SP C:dimensions. <p> Therefore D m :i = 1; D u :i = 1 D m :Poffset = 2; D u :Poffset = 2: Therefore the starting processor for the first dimension of D u (B) is: D u :start proc <ref> [1] </ref> = 1: For the second couple pair of dimensions, we again find they are mapped to the same processor dimension and proceed again with step 3. <p> D m :i = 2; D u :i = 2 D m :Poffset = 4; D u :Poffset = 4: Page 21 Therefore the starting processor for the second dimension of D u (B) is: D u :start proc <ref> [1] </ref> = 1: For the second couple pair of dimensions, we again find they are mapped to the same processor dimension and proceed again with step 3. <p> D m :i = 2; D u :i = 2 D m :Poffset = 4; D u :Poffset = 4: Therefore the starting processor for the second dimension of D u (B) is: D u :start proc <ref> [1] </ref> = 1: The process is exactly the same for dimension three as for dimension two. There are no new couplings to insert in the heap for B and we find that all decompositions are now fully mapped. This illustrates the simplest case for this algorithm. <p> Therefore, in step 2, D u = C and D m = D. For the first dimension of C we find that it is coupled with the second dimension of D and that they are both mapped to the first processor dimension. Since D:direction <ref> [1] </ref> is increasing and the stride (by default) is plus one, the C:direction [1] is set to increasing. <p> For the first dimension of C we find that it is coupled with the second dimension of D and that they are both mapped to the first processor dimension. Since D:direction <ref> [1] </ref> is increasing and the stride (by default) is plus one, the C:direction [1] is set to increasing. Therefore D m :i = 2; D u :i = 1; D m :Poffset = 1; D u :Poffset = 8: Hence the starting processor for the first dimension of D U (C) is: D u :start proc [1] = 8: For the second dimension of <p> (by default) is plus one, the C:direction <ref> [1] </ref> is set to increasing. Therefore D m :i = 2; D u :i = 1; D m :Poffset = 1; D u :Poffset = 8: Hence the starting processor for the first dimension of D U (C) is: D u :start proc [1] = 8: For the second dimension of D u , since D m :direction [2] is increasing and the stride (by default) is plus one, the D u :direction [2] is set to increasing. <p> Therefore D m :i = 1; D u :i = 2; D m :Poffset = 3; D u :Poffset = 3: Hence the starting processor for the second dimension of D u (C) is: D u :start proc <ref> [1] </ref> = 1: Decomposition C is now fully mapped and aligned. The coupling between C and E is inserted into the heap. Now we return to step 1. Deletion of the maximum cost entry from the heap yields the coupling between D and E. <p> Therefore D u :direction <ref> [1] </ref> is set to increasing. Then D m :i = 2 and k = 2 so that D m :Poffset = 8 and D u :Poffset = 8: Hence the starting processor for the first dimension of D u (E) is: D u :start proc [1] = 1: For the second <p> Therefore D u :direction <ref> [1] </ref> is set to increasing. Then D m :i = 2 and k = 2 so that D m :Poffset = 8 and D u :Poffset = 8: Hence the starting processor for the first dimension of D u (E) is: D u :start proc [1] = 1: For the second coupling entry D m :i = 2 and D u :i = : The second dimension of D u is coupled with the second dimension of D m , but they are not both mapped to the same processor dimension. Therefore D u :direction [1] <p> <ref> [1] </ref> = 1: For the second coupling entry D m :i = 2 and D u :i = : The second dimension of D u is coupled with the second dimension of D m , but they are not both mapped to the same processor dimension. Therefore D u :direction [1] is set to increasing. Then D m :i = 1 and k = 1 so that D m :Poffset = 1 and D u :Poffset = 4: Hence the starting processor for the second dimension of D u (E) is: D u :start proc [1] = 4: Decomposition D u <p> Therefore D u :direction <ref> [1] </ref> is set to increasing. Then D m :i = 1 and k = 1 so that D m :Poffset = 1 and D u :Poffset = 4: Hence the starting processor for the second dimension of D u (E) is: D u :start proc [1] = 4: Decomposition D u is now fully mapped and aligned and is added to mapped. There are no new couplings to insert in the heap for E.
Reference: [2] <author> B.E. Boyack, H. Stumpf, and J.F. Lime. </author> <title> TRAC User's Guide. </title> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, New Mexico 87545, </address> <year> 1985. </year>
Reference-contexts: This limits the use of this approach for some problems. For example, in many Nuclear Reactor Simulations <ref> [2] </ref> many of the meshs have few elements (~ 10 100) and hence cannot reasonably be distributed over many processors. Further, if enough processors are used even large meshs can not be distributed over all processors. We are currently exploring other approaches for automatic distribution of such problems. <p> + offset 3 ; I 2 + offset 2 ; I 4 + offset 4 ; :::; I D:dimensions + offset D:dimensions ) Page 18 becomes align X (wrap ((1 D:direction [1]) fl ( 1 2 ) fl D:size [1] + D:direction [1] fl (I 1 )); wrap ((1 D:direction <ref> [2] </ref>) fl ( 1 2 ) fl D:size [2] + D:direction [2] fl (I 2 )); :::; wrap ((1 D:direction [D:dimensions]) fl ( 1 2 ) fl D:size [D:dimensions] +D:direction [D:dimensions] fl (I D:dimensions ))) with D (I map 1 (1) + D:elt per proc [map 1 (1)] fl (start proc <p> 2 ; I 4 + offset 4 ; :::; I D:dimensions + offset D:dimensions ) Page 18 becomes align X (wrap ((1 D:direction [1]) fl ( 1 2 ) fl D:size [1] + D:direction [1] fl (I 1 )); wrap ((1 D:direction <ref> [2] </ref>) fl ( 1 2 ) fl D:size [2] + D:direction [2] fl (I 2 )); :::; wrap ((1 D:direction [D:dimensions]) fl ( 1 2 ) fl D:size [D:dimensions] +D:direction [D:dimensions] fl (I D:dimensions ))) with D (I map 1 (1) + D:elt per proc [map 1 (1)] fl (start proc [map 1 (1)] 1) + tof fset map <p> 4 + offset 4 ; :::; I D:dimensions + offset D:dimensions ) Page 18 becomes align X (wrap ((1 D:direction [1]) fl ( 1 2 ) fl D:size [1] + D:direction [1] fl (I 1 )); wrap ((1 D:direction <ref> [2] </ref>) fl ( 1 2 ) fl D:size [2] + D:direction [2] fl (I 2 )); :::; wrap ((1 D:direction [D:dimensions]) fl ( 1 2 ) fl D:size [D:dimensions] +D:direction [D:dimensions] fl (I D:dimensions ))) with D (I map 1 (1) + D:elt per proc [map 1 (1)] fl (start proc [map 1 (1)] 1) + tof fset map 1 (1) ; <p> (i); wrap (j)) with E (i; j + 80 fl 3) Note that in HPF the alignments must be shifted as HPF does not support wrap alignment. 3.7.3 Distribution Generation Finally, we generate distributions for each decomposition D of the form distribute D (block (SP C:procs [1]); block (SP C:procs <ref> [2] </ref>); :::block (SP C:procs [D:dimensions])) where SP C:procs [i] is set to one for i &gt; SP C:dimensions. <p> :i = 2; D u :i = 1; D m :Poffset = 1; D u :Poffset = 8: Hence the starting processor for the first dimension of D U (C) is: D u :start proc [1] = 8: For the second dimension of D u , since D m :direction <ref> [2] </ref> is increasing and the stride (by default) is plus one, the D u :direction [2] is set to increasing. <p> :Poffset = 8: Hence the starting processor for the first dimension of D U (C) is: D u :start proc [1] = 8: For the second dimension of D u , since D m :direction <ref> [2] </ref> is increasing and the stride (by default) is plus one, the D u :direction [2] is set to increasing.
Reference: [3] <author> C. Chase, K. Crowley, J. Saltz, and A. Reeves. </author> <title> Compiler and runtime support for irregularly coupled regular meshes. </title> <journal> Journal of the ACM, </journal> <month> July </month> <year> 1992. </year>
Reference-contexts: C (640,320) E (640,320) The first example problem is a multiblock CFD problem. The problem is to analyze transonic flow over 3-d configurations by solving the unsteady thin-layer Navier-Stokes equations. The problem was used by J. Saltz <ref> [3] </ref> to show the feasibility of the PARTI multiblock approach to parallelization of ICRM problems. This problem consists of two meshs, A and B, each of which are 40x40x40 with one coupled face. These meshs are illustrated in Figure 4. We refer to this as the 3D-CFD problem.
Reference: [4] <author> Louis Comtet. </author> <title> Advanced Combinatorics. </title> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <address> Boston, Mass., </address> <year> 1974. </year>
Reference-contexts: This is the same as the number of partitions of an integer, f , into p d or fewer summands <ref> [4, 13] </ref>. Consider a machine with 2 14 processors configurable in up to seven dimensions. For such a machine, there would be 105 standard processor configurations.
Reference: [5] <author> Lars-Erik Eriksson. </author> <title> Flow solution on a dual-block grid around an airplane. </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 64 </volume> <pages> 79-83, </pages> <year> 1987. </year>
Reference-contexts: Perhaps one of the best known application areas for multiple grid approachs is aerodynamics. In aerodynamic simulations, different grids are used to resolve flow in the space surrounding the fuselage, wings, foreplane, pylons, etc. <ref> [22, 5, 18] </ref>. Two examples of grids for such aerodynamic simulations are shown in Figure 2 and Figure 3. The grid descriptions for both of these examples were provided by Paul Craft who works with Dr. Brahat Soni at Mississippi State University.
Reference: [6] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Vienna Fortran or HPF, in an analogous manner. Our validation results are obtained by generating an HPF code and compiling it on Digital Equipment Corporation's HPF compiler. Page 4 2.1 Specification of Topological Connectivity We illustrate the specification of topological connectivity via an extension to Fortran D <ref> [6, 9] </ref> that describes the connections between coupled decompositions. We first introduce two problems that will be used to illustrate the algorithms presented in this paper.
Reference: [7] <author> Charles H. Koelbel, David B. Loveman, Robert S. Schreiber, Guy L. Steele Jr., and Mary E. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: The style should also make development and support more efficient in terms of programmer effort. Further, programs written in this style should achieve good performance when compiled by a good compiler. We will describe this style in terms of HPF <ref> [7] </ref>. Two of the most important features of a modern language, for the current discussion, are dynamic memory allocation and user-defined data types.
Reference: [8] <author> L.M. Liebrock and K. Kennedy. </author> <title> Automatic data distribution of large meshs in coupled grid applications. </title> <note> In Submitted to ? 1994, </note> <month> November </month> <year> 1994. </year>
Reference-contexts: The heuristic we use is to reduce the maximum coupling communication cost for each decomposition. This is done by aligning the coupled subsections of coupled dimensions of different decompositions. The distribution decision procedure for selecting a data distribution uses the computational model presented in <ref> [8] </ref>. The model provides expected runtimes based on application and machine parameters as well as the selected data distribution. Any model could be used which will predict approximate runtimes of different mappings on the target machine.
Reference: [9] <author> L.M. Liebrock and K. Kennedy. </author> <title> Automatic data distribution of large meshs in coupled grid applications. </title> <type> Technical Report 94-395, </type> <institution> Rice University, Center for Research in Parallel Computation, Houston, TX, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Vienna Fortran or HPF, in an analogous manner. Our validation results are obtained by generating an HPF code and compiling it on Digital Equipment Corporation's HPF compiler. Page 4 2.1 Specification of Topological Connectivity We illustrate the specification of topological connectivity via an extension to Fortran D <ref> [6, 9] </ref> that describes the connections between coupled decompositions. We first introduce two problems that will be used to illustrate the algorithms presented in this paper.
Reference: [10] <author> L.M. Liebrock and K. Kennedy. </author> <title> Automatic data distribution of small meshs in coupled grid applications. </title> <note> In Submitted to Concurrency Practice and Experience, </note> <year> 1994, </year> <month> November </month> <year> 1994. </year>
Reference-contexts: The stride is optional and has a default value of 1. This illustrates the type of coupling information that must be extractable from the input file for these applications. For a more general form of coupling specification see <ref> [10] </ref>. 2.2 Programming Style Recommendations Here we attempt to outline a programming style that is natural to ICRM applications such as aerodynamic simulations, but does not inhibit dependence analysis. The style should also make development and support more efficient in terms of programmer effort.
Reference: [11] <author> L.M. Liebrock and K. Kennedy. </author> <title> Modeling parallel computation. </title> <type> Technical Report 94-394, </type> <institution> Rice University, Center for Research in Parallel Computation, Houston, TX, </institution> <month> April </month> <year> 1994. </year> <pages> Page 23 </pages>
Reference-contexts: When meshs are generated automatically, automatic distribution of large-mesh ICRM problems is critical. This research provides an important first step. Related work includes development of an algorithm to map ICRM problems in which the meshs must be grouped together for mapping onto parallel processors <ref> [11] </ref>. Two examples of problems of this type are nuclear reactor simulations and electric circuit simulations. Future work will include an investigation of how to map ICRM problems in which some or all of the meshs must be mapped to a subset of the processors.
Reference: [12] <author> L.M. Liebrock and K. Kennedy. </author> <title> Parallelization of linearized application in Fortran D. </title> <booktitle> In International Parallel Processing Symposium '94, </booktitle> <pages> pages 51-60, </pages> <address> Washington, DC, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Details of code generation and sample compiler support will follow next. This section will conclude with a discussion of the limitations and advantages of this approach. For efficient parallelization of ICRM applications, we would like to distribute each mesh according to its dimensionality <ref> [12] </ref>. On the other hand, the resulting code should have uniform memory allocation. Let n be the dimensionality of the mesh with the fewest number of dimensions. Then we will distribute all of the meshs over all of the processors in an n-dimensional processor topology. <p> In the 2D-Multiblock problem the number of bytes of communication is the same in each dimension of each mesh. This makes the number of elements to be communicated be the only communication factor that influences the choice of mapping. Surface to volume effects <ref> [12] </ref> imply we want to have each communication "edge" be the same length. This leads to our second heuristic for reducing the number of evaluations of the model.
Reference: [13] <author> Ivan Niven and H.S. Zuckerman, </author> <title> editors. An Introduction to the Theory of Numbers. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <note> fourth edition, </note> <year> 1980. </year>
Reference-contexts: This is the same as the number of partitions of an integer, f , into p d or fewer summands <ref> [4, 13] </ref>. Consider a machine with 2 14 processors configurable in up to seven dimensions. For such a machine, there would be 105 standard processor configurations.
Reference: [14] <author> J.A. Shaw, J.M. Georgala, </author> <title> and N.P. Weatherill. The construction of component adaptive grids for aerodynamic geometries. </title> <editor> In S. Sengupta, J. Hauser, P.R. Eiseman, and J.F. Thompson, editors, </editor> <booktitle> Numerical Grid Generation in Computational Fluid Mechanics '88, </booktitle> <pages> pages 383-394. </pages> <publisher> Pineridge Press, </publisher> <year> 1988. </year>
Reference-contexts: They are: 1. create multiple grids that adjoin each other, or 2. create multiple grids that overlap each other. In recent years, it has become possible to automatically generate the grids used in some ICRM problems <ref> [15, 19, 14, 1, 17, 20, 16] </ref>.
Reference: [15] <author> J.A. Shaw and N.P. Weatherill. </author> <title> Automatic topology generation for multiblock grids. </title> <journal> Applied Mathematics and Computation, </journal> <volume> 52 </volume> <pages> 355-388, </pages> <year> 1992. </year>
Reference-contexts: They are: 1. create multiple grids that adjoin each other, or 2. create multiple grids that overlap each other. In recent years, it has become possible to automatically generate the grids used in some ICRM problems <ref> [15, 19, 14, 1, 17, 20, 16] </ref>. <p> They are: 1. create multiple grids that adjoin each other, or 2. create multiple grids that overlap each other. In recent years, it has become possible to automatically generate the grids used in some ICRM problems [15, 19, 14, 1, 17, 20, 16]. Shaw and Weatherill <ref> [15] </ref> provide an introduction to the problem of automatic grid generation and the difficulty encountered when generating grids for multiblock problems: The fundamental problem that is encountered in the construction of structured, body-conforming meshs for general aerodynamic configurations is that each component of the configuration has its own natural type of
Reference: [16] <author> Mark S. Shephard and Marcel K. Georges. </author> <title> Reliability of automatic 3d mesh generation. </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 101 </volume> <pages> 443-462, </pages> <year> 1992. </year>
Reference-contexts: They are: 1. create multiple grids that adjoin each other, or 2. create multiple grids that overlap each other. In recent years, it has become possible to automatically generate the grids used in some ICRM problems <ref> [15, 19, 14, 1, 17, 20, 16] </ref>.
Reference: [17] <author> Robert E. Smith and Lars-Erik Eriksson. </author> <title> Algebraic grid generation. </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 64 </volume> <pages> 285-300, </pages> <year> 1987. </year>
Reference-contexts: In one unsymmetrical calculation of the HOPE 63 model, a total of approximately 9,000,000 grid points were used in the three-dimensional grids. Fortunately, these problems are inherently parallel. Unfortunately, they are not necessarily easy to parallelize. From Smith and Eriksson <ref> [17] </ref>, we see that along with the computational complexity there is added complexity due to the need for multiple coupled grids: Generating structured grids about complex geometries that map into a single rectangular computational block is, for all practical purposes impossible. <p> They are: 1. create multiple grids that adjoin each other, or 2. create multiple grids that overlap each other. In recent years, it has become possible to automatically generate the grids used in some ICRM problems <ref> [15, 19, 14, 1, 17, 20, 16] </ref>.
Reference: [18] <author> Joseph L. Steger and John A. Benek. </author> <title> On the use of composite grid schemes in computational aerodynamics. </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 64 </volume> <pages> 301-320, </pages> <year> 1987. </year>
Reference-contexts: Perhaps one of the best known application areas for multiple grid approachs is aerodynamics. In aerodynamic simulations, different grids are used to resolve flow in the space surrounding the fuselage, wings, foreplane, pylons, etc. <ref> [22, 5, 18] </ref>. Two examples of grids for such aerodynamic simulations are shown in Figure 2 and Figure 3. The grid descriptions for both of these examples were provided by Paul Craft who works with Dr. Brahat Soni at Mississippi State University.
Reference: [19] <author> J.P. Steinbrenner, S.L. Karmen, and J.R. Chawner. </author> <title> Generation of multiple block grids for arbitrary 3-d geometries. </title> <editor> In H. Yoshihara, editor, </editor> <title> 3-Dimensional Grid Generation for Complex Configurations. </title> <address> AGARDograph 309, </address> <year> 1988. </year>
Reference-contexts: They are: 1. create multiple grids that adjoin each other, or 2. create multiple grids that overlap each other. In recent years, it has become possible to automatically generate the grids used in some ICRM problems <ref> [15, 19, 14, 1, 17, 20, 16] </ref>.
Reference: [20] <author> Joe F. Thompson. </author> <title> A general three-dimensional elliptic grid generation system on a composite block structure. </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 64 </volume> <pages> 377-411, </pages> <year> 1987. </year>
Reference-contexts: They are: 1. create multiple grids that adjoin each other, or 2. create multiple grids that overlap each other. In recent years, it has become possible to automatically generate the grids used in some ICRM problems <ref> [15, 19, 14, 1, 17, 20, 16] </ref>.
Reference: [21] <author> Michael Thune. </author> <title> A partitioning algorithm for composite grids. </title> <booktitle> Parallel Algorithms and Applications, </booktitle> <volume> 1(1) </volume> <pages> 69-81, </pages> <year> 1993. </year>
Reference-contexts: The one type of ICRM problem for which automatic distribution has been explored is multigrid. Thune has been working on automatic distribution of data structures for multigrid computations <ref> [21] </ref>. We are working to make parallelization of these applications easier for the user as well as efficient and applicable to a large class of architectures. To this end, we must develop an approach that achieves more of the computational efficiency of a regular approach.
Reference: [22] <author> Yukimitsu Yamamoto. </author> <title> Numerical simulation of hypersonic viscous flow for the design of h-ii orbiting plane (hope). </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 89 </volume> <pages> 59-72, </pages> <year> 1990. </year> <pages> Page 24 </pages>
Reference-contexts: Perhaps one of the best known application areas for multiple grid approachs is aerodynamics. In aerodynamic simulations, different grids are used to resolve flow in the space surrounding the fuselage, wings, foreplane, pylons, etc. <ref> [22, 5, 18] </ref>. Two examples of grids for such aerodynamic simulations are shown in Figure 2 and Figure 3. The grid descriptions for both of these examples were provided by Paul Craft who works with Dr. Brahat Soni at Mississippi State University. <p> Many ICRM problems require the use of the fastest computers available, even for simplified simulations. For the solution of the grand challenge simulations in these problems, it is clear that parallelization will be necessary. For example, HOPE <ref> [22] </ref> is a winged vehicle for space transportation called the H-II Orbiting Plane planned by the National Space Development Agency of Japan. To make accurate simulation of re-entry feasible, hypersonic aerodynamics and aerothermodynamic characteristics must be precisely evaluated.
References-found: 22

