URL: http://www.research.microsoft.com/~carlk/papers/ccsc.ps
Refering-URL: http://www.research.microsoft.com/~carlk/papers/ccsc.htm
Root-URL: http://www.research.microsoft.com
Email: kadie@cs.uiuc.edu  
Title: Continuous Conceptual Set Covering: Learning Robot Operators From Examples  
Author: Carl Myers Kadie 
Address: 405 N. Mathews Ave., Urbana, IL 61801  
Affiliation: Knowledge-Based Systems Group, Department of Computer Science Beckman Institute, University of Illinois,  
Abstract: Continuous Conceptual Set Covering (CCSC) is an algorithm that uses engineering knowledge to learn operator effects from training examples. The program produces an operator hypothesis that, even in noisy and nondeterministic domains, can make good quantitative predictions. An empirical evaluation in the tray-tilting domain shows that CCSC learns faster than an alternative case-based approach. The best results, however, come from integrating CCSC and the case-based approach. Figure 1. Experimental Set Up 
Abstract-found: 1
Intro-found: 1
Reference: [Bennett, 1990] <author> Scott W. Bennett. </author> <title> Reducing real-world failures of approximate explanation-based rules. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 226-234, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> June </month> <year> 1990. </year> <title> Section 3.3 contained a prediction that CCSC's conceptual expert selection method would produce hypotheses that were more accurate than those produced by greedy expert selection. This prediction was tested with repeated runs of CCSC and greedy expert selection on the raw tray data with no extra attributes. CCSC often performed significantly better than the greedy algorithm. On average, CCSC's error rate is 10% lower than the greedy method's error rate. </title>
Reference-contexts: For example, slider is the output of a program that takes the training examples as input and then uses multiple-linear regression to find the best linear relation. The Grasper system demonstrates explanation-based learning in a robot domain <ref> [Bennett, 1990] </ref>. Grasper is given an approximate domain theory. In contrast with CCSC's more empirical approach, Grasper uses explanation-based methods to help it tune scalar parameters such as the initial width of a robot's grasper.

Reference: [Kadie, 1988] <author> Carl M. Kadie. Diffy-S: </author> <title> learning robot operator schemata from examples. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 430-436, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> June </month> <year> 1988. </year>
Reference: [Kadie, 1990] <author> Carl M. Kadie. </author> <title> Conceptual set covering: improving fit-and-split algorithms. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 40-48, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> June </month> <year> 1990. </year> <title> Several learning algorithms were considered. The best algorithm was a hybrid in which CCSC used the case-based algorithm as one of its experts. This algorithm learned significantly quicker than the case-based learner and unlike, the first version of CCSC, converged toward the minimal error. </title>
Reference-contexts: A limitation of the case-based approach is that it is very sensitive to the number of attributes used to describe a case. Machine Learning offers a number of generalization techniques that are relatively insensitive to the number of attributes <ref> [Quinlan, 1986; Kadie, 1990] </ref>. Most of these techniques, however, work only on discrete concept-learning problems. The CCSC algorithm extends these generalization methods to work on continuous problems. The key extensions involve the application of background knowledge and the automatic determination of an error threshold.

Reference: [Moore, 1990] <author> Andrew W. Moore. </author> <title> Acquisition of dynamic control knowledge for a robot manipulator. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 244-252, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: An important open problem in Machine Learning is learning the effects of robot operators from examples. Previous research has provided a partial solution. Case-based approaches, for example, have been effective in some domains <ref> [Moore, 1990] </ref>. A limitation of the case-based approach is that it is very sensitive to the number of attributes used to describe a case. Machine Learning offers a number of generalization techniques that are relatively insensitive to the number of attributes [Quinlan, 1986; Kadie, 1990]. <p> If the puck then slid along the wall, it would reach point c . The case-based, or exemplar, approach to operator learning is also explored in <ref> [Moore, 1990] </ref>. Moore's system efficiently uses the same nearest-neighbor metric used in this paper. Like all case-based approaches, however, this approach is sensitive to the number of attributes and has difficulty accepting background knowledge. The experts may be fixed or they may be the output of other learning programs.
Reference: [Quinlan, 1986] <author> J. Ross Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year> <title> Despite these limitations, CCSC offers immediate benefits to those who wish to learn operator effects. It shows how background knowledge can be used to improve this type of inductive learning. It is especially useful when the dimensionality of the input space is high. </title>
Reference-contexts: A limitation of the case-based approach is that it is very sensitive to the number of attributes used to describe a case. Machine Learning offers a number of generalization techniques that are relatively insensitive to the number of attributes <ref> [Quinlan, 1986; Kadie, 1990] </ref>. Most of these techniques, however, work only on discrete concept-learning problems. The CCSC algorithm extends these generalization methods to work on continuous problems. The key extensions involve the application of background knowledge and the automatic determination of an error threshold. <p> In the experiments of section 4 the decision function D was of the form of a decision list where each decision rule was produced by a version of the ID3 program <ref> [Quinlan, 1986] </ref>. | | E ( state i , parms i ) - state i +1 &lt; cutoff Because an acceptable error cut off for one domain will not necessarily be an acceptable error cut off in another, CCSC determines the error cut off automatically.
References-found: 5

