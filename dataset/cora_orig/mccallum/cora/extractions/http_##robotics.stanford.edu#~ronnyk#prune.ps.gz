URL: http://robotics.stanford.edu/~ronnyk/prune.ps.gz
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: fjbradfor,brodleyg@ecn.purdue.edu fclayk,ronnyk,brunkg@engr.sgi.com  
Title: Pruning Decision Trees with Misclassification Costs  
Author: Jeffrey P. Bradford Clayton Kunz Ron Kohavi Cliff Brunk Carla E. Brodley 
Address: 2011 N. Shoreline Blvd. West Lafayette, IN 47907 Mountain View, CA 94043  
Affiliation: 1 School of Electrical Engineering 2 Data Mining and Visualization Purdue University Silicon Graphics, Inc.  
Note: Appears in ECML-98 as a research note  
Abstract: We describe an experimental study of pruning methods for decision tree classifiers when the goal is minimizing loss rather than error. In addition to two common methods for error minimization, CART's cost-complexity pruning and C4.5's error-based pruning, we study the extension of cost-complexity pruning to loss and one pruning variant based on the Laplace correction. We perform an empirical comparison of these methods and evaluate them with respect to loss. We found that applying the Laplace correction to estimate the probability distributions at the leaves was beneficial to all pruning methods. Unlike in error minimization, and somewhat surprisingly, performing no pruning led to results that were on par with other methods in terms of the evaluation criteria. The main advantage of pruning was in the reduction of the decision tree size, sometimes by a factor of ten. While no method dominated others on all datasets, even for the same domain different pruning mechanisms are better for different loss matrices.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bradford, J. P., Kunz, C., Kohavi, R., Brunk, C. & Brodley, C. E. </author> <year> (1998), </year> <title> Pruning decision trees with misclassification costs (long). </title> <address> http://robotics.stanford.edu/~ronnyk/prune-long.ps.gz. </address>
Reference-contexts: No single pruning algorithm dominated over all datasets in terms of loss, but more interestingly, even for a fixed domain, different pruning algorithms were better for different loss matrices. In the long version of this paper <ref> (Bradford, Kunz, Kohavi, Brunk & Brodley 1998) </ref> we showed ROC curves for different algorithms, including another pruning method. These differences, however, were not major.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: To combat this overfitting problem, the tree is pruned back with the goal of identifying the tree with the lowest error rate on previously unobserved instances, breaking ties in favor of smaller trees <ref> (Breiman, Friedman, Olshen & Stone 1984, Quinlan 1993) </ref>. Several pruning methods have been introduced in the literature, including cost-complexity pruning, reduced error pruning, pessimistic pruning, error-based pruning, penalty pruning, and MDL pruning. <p> Pruning for loss minimization can lead to different pruning behavior than does pruning for error minimization. In this paper, we investigate the behavior of several pruning algorithms. In addition to the two most common methods for error minimization, cost-complexity pruning <ref> (Breiman et al. 1984) </ref> and error-based pruning (Quinlan 1993), we study the extension of cost-complexity pruning to loss and a pruning variant based on the Laplace correction (Good 1965, Cestnik 1990). We perform an empirical comparison of these methods and evaluate them with respect to loss under two different matrices. <p> The crux of the problem is to find an honest estimate of error <ref> (Breiman et al. 1984) </ref>, which is defined as one that is not overly optimistic for a tree that was built to minimize errors in the first place. <p> Two commonly used pruning algorithms for error minimization are C4.5's error-based pruning (Quinlan 1993) and CART's cost-complexity pruning <ref> (Breiman et al. 1984) </ref>. We attempted to extend several error-based pruning to loss-based pruning. In some cases the extensions are obvious, but C4.5's error-based pruning based on confidence intervals does not extend easily.
Reference: <author> Cestnik, B. </author> <year> (1990), </year> <title> Estimating probabilities: A crucial task in machine learning, </title> <editor> in L. C. Aiello, ed., </editor> <booktitle> `Proceedings of the ninth European Conference on Artificial Intelligence', </booktitle> <pages> pp. 147-149. </pages>
Reference: <author> Draper, B. A., Brodley, C. E. & Utgoff, P. E. </author> <year> (1994), </year> <title> `Goal-directed classification using linear machine decision trees', </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 16(9), </journal> <pages> 888-893. </pages>
Reference-contexts: This result differs from error minimization, where pruning was consistently shown to help. Pruning based on loss matrices performed better than pruning based on error for frequency counts for all methods. This result (for frequency counts) has been observed previously for reduced error/cost pruning <ref> (Draper, Brodley & Utgoff 1994) </ref>. When the Laplace correction was used, pruning with loss matrices performed better than error-based pruning (eb-lc) for the 100:1 (ccp-lc, lp) but there was no significant difference for the 10:1 loss matrix. For each pruning method, applying the Laplace correction improved performance on average.
Reference: <author> Good, I. J. </author> <year> (1965), </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods, </title> <publisher> M.I.T. Press. </publisher>
Reference-contexts: In addition to the two most common methods for error minimization, cost-complexity pruning (Breiman et al. 1984) and error-based pruning (Quinlan 1993), we study the extension of cost-complexity pruning to loss and a pruning variant based on the Laplace correction <ref> (Good 1965, Cestnik 1990) </ref>. We perform an empirical comparison of these methods and evaluate them with respect to loss under two different matrices. We found that even for the same domain, different pruning mechanisms are better for different loss matrices. <p> The Laplace correction method biases the probability towards a uniform distribution. Specifically, if a node has m instances, c of which are from a given class, in a k-class problem, the probability assigned to the class is (c + 1)=(m + k) <ref> (Good 1965, Cestnik 1990) </ref>. The Laplace correction makes the distribution at the leaves more uniform and less extreme. Given a node, we can compute the expected loss using the loss matrix. The expected loss of a subtree is the sum of expected loss of the leaves.
Reference: <author> Kohavi, R., Sommerfield, D. & Dougherty, J. </author> <year> (1996), </year> <title> Data mining using MLC ++ : A machine learning library in C ++ , in `Tools with Artificial Intelligence', </title> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 234-245. </pages> <address> http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: The basic decision tree growing algorithm is implemented in MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1996) </ref> and called MC4 (MLC ++ C4.5). It is a Top-Down Decision Tree induction algorithm very similar to C4.5.
Reference: <author> Merz, C. J. & Murphy, P. M. </author> <year> (1997), </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: In our initial experiments, the Laplace correction outperformed frequency counts in all variants. Therefore, excluding the basic method of error-based-pruning, all other pruning methods were run with the Laplace correction. Ten datasets were chosen from the UCI repository <ref> (Merz & Murphy 1997) </ref>: adult (salary classification based on census bureau data), breast cancer diag nosis, chess, crx (credit), german (credit), pima diabetes, road (dirt), satellite images, shuttle, and vehicle. In choosing the datasets, we decided on the follow ing desiderata: 1.
Reference: <author> Oates, T. & Jensen, D. </author> <year> (1997), </year> <title> The effects of training set size on decision tree complexity, </title> <editor> in D. Fisher, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Fourteenth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 254-262. </pages>
Reference: <author> Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T. & Brunk, C. </author> <year> (1994), </year> <title> Reducing misclassification costs, </title> <booktitle> in `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: Pruning for loss minimization can lead to different pruning behavior than does pruning for error minimization. In this paper, we investigate the behavior of several pruning algorithms. In addition to the two most common methods for error minimization, cost-complexity pruning (Breiman et al. 1984) and error-based pruning <ref> (Quinlan 1993) </ref>, we study the extension of cost-complexity pruning to loss and a pruning variant based on the Laplace correction (Good 1965, Cestnik 1990). We perform an empirical comparison of these methods and evaluate them with respect to loss under two different matrices. <p> The resubstitution error (error rate on the training set) does not provide a suitable estimate because a leaf-node replacing a subtree will never have fewer errors on the training set than the subtree. Two commonly used pruning algorithms for error minimization are C4.5's error-based pruning <ref> (Quinlan 1993) </ref> and CART's cost-complexity pruning (Breiman et al. 1984). We attempted to extend several error-based pruning to loss-based pruning. In some cases the extensions are obvious, but C4.5's error-based pruning based on confidence intervals does not extend easily.
Reference: <author> Turney, P. </author> <year> (1997), </year> <note> Cost-sensitive learning. http://ai.iit.nrc.ca/bibliographies/cost-sensitive.html. </note>
Reference-contexts: Our objective in this paper is different than the above-mentioned studies. Instead of pruning to minimize error, we aim to study pruning algorithms with the goal of minimizing loss. In many practical applications one has a loss matrix associated with classification errors <ref> (Turney 1997) </ref>, and pruning should be performed with respect to the loss matrix. Pruning for loss minimization can lead to different pruning behavior than does pruning for error minimization. In this paper, we investigate the behavior of several pruning algorithms.
References-found: 11

