URL: http://www-wavelet.eecs.berkeley.edu/~martin/Papers/adap_xform.ps.gz
Refering-URL: http://www-wavelet.eecs.berkeley.edu/~martin/Papers/
Root-URL: 
Email: e-mail: v.goyal@ieee.org  
Phone: #1772 voice: +1 510 643 5798  fax: +1 510 642 2845  
Title: ADAPTIVE TRANSFORM CODING USING LMS-LIKE 1 Adaptive Transform Coding Using LMS-like Principal Component Tracking  
Author: GOYAL VETTERLI: Vivek K Goyal and Martin Vetterli ; Vivek K Goyal 
Keyword: EDICS Category: SP 2. Digital Signal Processing SP 2.6 Adaptive Filters SP 2.6.2 Algorithms (Gradient-Descent, RLS, Random Search, Genetic)  
Note: Corresponding author:  Permission to publish this abstract separately is granted. January 8, 1998 Submitted to IEEE Trans. Sig. Proc.  
Address: Berkeley, CA, USA Lausanne, Switzerland  211-79 Cory Hall  Berkeley, CA 94720-1772  
Affiliation: 1 Dept. of Electrical Eng. Comp. Sci. 2 Laboratoire de Communications Audiovisuelles University of California, Berkeley Ecole Polytechnique Federale de Lausanne  
Abstract: A new set of algorithms for transform adaptation in adaptive transform coding is presented. These algorithms are inspired by standard techniques in adaptive finite impulse response (FIR) Wiener filtering and demonstrate that similar algorithms with simple updates exist for tracking principal components (eigenvectors of a correlation matrix). For coding an N -dimensional source, the transform adaptation problem is posed as an unconstrained minimization over K = N (N 1)=2 parameters, and this for two possible performance measures. Performing this minimization through a gradient descent gives an algorithm analogous to LMS. Step size bounds for stability similar in form to those for LMS are proven. Linear and fixed-step random search methods are also considered. The stochastic gradient descent algorithm is simulated for both time-invariant and slowly-varying sources. A "backward-adaptive" mode, where the adaptation is based on quantized data so that the decoder and encoder can maintain the same state without side information, is also considered. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Widrow, J. M. McCool, M. G. Larimore, and C. R. Johnson, Jr., </author> <title> "Stationary and nonstationary learning characteristics of the LMS adaptive filter," </title> <journal> Proc. IEEE, </journal> <volume> vol. 64, </volume> <pages> pp. 1151-1162, </pages> <year> 1976. </year>
Reference-contexts: studies the stability and rate of convergence of (21) by analyzing f (n + 1) = f (n) 2ff (X f r dx ): This can be interpreted as ignoring the stochastic aspect of the algorithm or as looking at the mean of f and applying the so-called "independence assumption" <ref> [1] </ref>. III. Alternative Gradient Expressions The gradient expressions given in Section IV-C were intended to facilitate Theorems 1 and 3. Alternative expressions for rJ 1 and rJ 2 are given in this appendix. 10 Under some technical conditions, the minimum is unique. January 8, 1998 Submitted to IEEE Trans. Sig.
Reference: [2] <author> B. Widrow and S. D. Stearns, </author> <title> Adaptive Signal Processing, </title> <publisher> Prentice-Hall, </publisher> <address> Upper Saddle River, NJ, </address> <year> 1985. </year> <note> January 8, 1998 Submitted to IEEE Trans. Sig. Proc. </note> <author> GOYAL & VETTERLI: </author> <title> ADAPTIVE TRANSFORM CODING USING LMS-LIKE . . . 30 </title>
Reference-contexts: The most common method of performance surface search is gradient descent|which leads to the LMS algorithm [1]|but linear and fixed-step random searches <ref> [2] </ref> also fall into this class. This paper defines two meaningful performance surfaces (cost functions) for linear transform coding and analyzes various search methods for these surfaces. The result is a set of new algorithms for adaptive linear transform coding. <p> These algorithms and the random search algorithm of Section IV-B are inspired by and parallel the standard methods in adaptive FIR Wiener filtering <ref> [2] </ref>. For comparison, standard methods which are computationally attractive for computing single eigendecompositions are presented in Section IV-D. The effects of the time variation of X and estimation noise are left for subsequent sections. <p> A fixed-step random search makes no progress on an iteration where fi k + ff k is found to be worse than fi k . Another possibility is a linear random search <ref> [2] </ref>. In this case, instead of taking no step if k seems to be a step in the wrong direction, one takes a step in the opposite direction; the size of each step is proportional to the increase or decrease in J . <p> This "excess" in J increases as the step size is increased. We have not attempted to characterize this analytically. Qualitatively it is similar to the "excess" mean-square error in LMS filtering <ref> [2] </ref>. Referring to Figure 5 (a), the steady-state value of J 1 decreases monotonically as ff is decreased, but the convergence is slower. Because the source is time-invariant, there is a conceptually simple alternative to the stochastic gradient descent which provides a bound to attainable performance.
Reference: [3] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins Univ. Press, </publisher> <address> Baltimore, MD, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: to a Gaussian condition on the source and fine-quantization approximations, 1 finding an optimal transform for transform coding amounts to finding an orthonormal set of eigenvectors of a symmetric, positive semidefinite matrix; i.e., finding an optimal transform is an instance of the symmetric eigenproblem, a fundamental problem of numerical analysis <ref> [3] </ref>. Thus, in finding a method for transform adaptation we are in fact attempting to approximately solve a sequence of symmetric eigenvalue problems. <p> The idea of using performance surface search (i.e., cost function minimization) for this problem seems to be new, although the cost function which we will later call J 1 has been used in convergence analyses <ref> [3] </ref>. The algorithms we develop here are not competitive with cyclic Jacobi methods for computing a single eigendecomposition of a large matrix; however, they are potentially useful for computing eigendecompositions of a slowly varying sequence of matrices. <p> Sig. Proc. GOYAL & VETTERLI: ADAPTIVE TRANSFORM CODING USING LMS-LIKE . . . 3 are more parallelizable than cyclic Jacobi methods. In addition, further insights may come from drawing together techniques from adaptive filtering, transform coding, and numerical linear algebra. The reader is referred to <ref> [3] </ref> for a thorough treatment of the techniques for computing eigen-decompositions including the techniques specific to the common special case where the matrix is symmetric. Appendices A and B provide brief reviews of transform coding and adaptive FIR Wiener filtering, respectively. II. <p> . . . . . . . . . . . . . . . . . . . . . . . . . 3 7 7 7 7 7 7 7 7 i 0 i j 0 where =2 &lt; =2, is called a Givens (or Jacobi) rotation <ref> [3] </ref>. It can be interpreted as a counterclockwise rotation of radians in the (i; j) coordinate plane. <p> Proc. GOYAL & VETTERLI: ADAPTIVE TRANSFORM CODING USING LMS-LIKE . . . 14 would have with a slowly-varying sequence of X matrices. In this section we briefly introduce Jacobi methods, which allow this prior information to be effectively incorporated. Details on QR and Jacobi algorithms can be found in <ref> [3, x8.5] </ref>. The idea of the classical Jacobi algorithm is to at each iteration choose a Givens rotation to reduce the off-diagonal energy as much as possible.
Reference: [4] <author> M. Effros and P. A. Chou, </author> <title> "Weighted universal transform coding: Universal image compression with the Karhunen-Loeve transform," </title> <booktitle> in Proc. IEEE Int. Conf. Image Proc., 1995, </booktitle> <volume> vol. II, </volume> <pages> pp. 61-64. </pages>
Reference-contexts: Linear and fixed-step random searches are 2 If the dependence of X n on n is not mild, then it is rather hopeless to use adaptation in the traditional sense of learning source behavior based on the recent past. Better strategies might include classification <ref> [4] </ref>, [5] or other basis selection methods [6], [7]. January 8, 1998 Submitted to IEEE Trans. Sig. Proc. GOYAL & VETTERLI: ADAPTIVE TRANSFORM CODING USING LMS-LIKE . . . 4 also discussed.
Reference: [5] <author> M. Effros, </author> <title> "Fast weighted universal transform coding: Toward optimal, low complexity bases for image compression," </title> <booktitle> in Proc. IEEE Data Compression Conf., </booktitle> <editor> J. A. Storer and M. Cohn, Eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <year> 1997, </year> <pages> pp. 211-220, </pages> <publisher> IEEE Comp. Soc. Press. </publisher>
Reference-contexts: Linear and fixed-step random searches are 2 If the dependence of X n on n is not mild, then it is rather hopeless to use adaptation in the traditional sense of learning source behavior based on the recent past. Better strategies might include classification [4], <ref> [5] </ref> or other basis selection methods [6], [7]. January 8, 1998 Submitted to IEEE Trans. Sig. Proc. GOYAL & VETTERLI: ADAPTIVE TRANSFORM CODING USING LMS-LIKE . . . 4 also discussed.
Reference: [6] <author> J. Adler, B. D. Rao, and K. Kreutz-Delgado, </author> <title> "Comparison of basis selection methods," </title> <booktitle> in Proc. Asilomar Conf. on Sig., Sys. & Computers, </booktitle> <year> 1996. </year>
Reference-contexts: Better strategies might include classification [4], [5] or other basis selection methods <ref> [6] </ref>, [7]. January 8, 1998 Submitted to IEEE Trans. Sig. Proc. GOYAL & VETTERLI: ADAPTIVE TRANSFORM CODING USING LMS-LIKE . . . 4 also discussed. Section IV contains the linear algebraic computations which underlie the signal processing algorithms which are ultimately presented in Section V.
Reference: [7] <author> V. K Goyal, M. Vetterli, and N. T. Thao, </author> <title> "Quantized overcomplete expansions in R N : Analysis, synthesis, and algorithms," </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> vol. 44, no. 1, </volume> <month> Jan. </month> <year> 1998. </year>
Reference-contexts: Better strategies might include classification [4], [5] or other basis selection methods [6], <ref> [7] </ref>. January 8, 1998 Submitted to IEEE Trans. Sig. Proc. GOYAL & VETTERLI: ADAPTIVE TRANSFORM CODING USING LMS-LIKE . . . 4 also discussed. Section IV contains the linear algebraic computations which underlie the signal processing algorithms which are ultimately presented in Section V.
Reference: [8] <author> R. A. Horn and C. R. Johnson, </author> <title> Matrix Analysis, </title> <publisher> Cambridge Univ. Press, </publisher> <year> 1985. </year>
Reference-contexts: X n will not be known, but must be estimated or in some sense inferred from fx k g n k=1 . First of all, note that if X n is known, then a T n consisting of normalized eigenvectors of X n solves our problem <ref> [8] </ref>. A traditional approach would be to construct an estimate ^ X n = f (fx k g n k=1 ) for each n, and then use an "off the shelf" method to compute the eigenvectors of ^ X n . <p> ] T 2 [=2; =2) K such that T fi XT T fi is diagonal, where T fi = G 1; 1 G 2; 2 : : : G K; K : (4) Proof: Since X is symmetric, there exists an orthogonal matrix S such that SXS T is diagonal <ref> [8] </ref>. Any orthogonal matrix can be factored as S = ( e G 1;2; 1;2 where D * = diag (* 1 ; : : : ; * N ), * i = 1, i = 1; 2; : : : ; N [9].
Reference: [9] <author> T. W. Anderson, I. Olkin, and L. G. Underhill, </author> <title> "Generation of random orthogonal matrices," </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> vol. 8, no. 4, </volume> <pages> pp. 625-629, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Any orthogonal matrix can be factored as S = ( e G 1;2; 1;2 where D * = diag (* 1 ; : : : ; * N ), * i = 1, i = 1; 2; : : : ; N <ref> [9] </ref>. It is now obvious that we can take T = SD 1 * because D 1 * X (D 1 B.
Reference: [10] <author> M. Vidyasagar, </author> <title> Nonlinear Systems Analysis, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1978. </year>
Reference-contexts: A sufficient condition for local convergence is that the eigenvalues of I ffF lie in the unit circle. The fact that the local exponential stability of the original nonlinear system can be inferred from an eigenvalue condition on the linearized system follows from the continuous differentiability of F <ref> [10] </ref>. We now evaluate F . Differentiating (8) gives @ 2 J 1 = 2 i6=j Y ij (k) @ ` (k) @Y ij ! January 8, 1998 Submitted to IEEE Trans. Sig. Proc.
Reference: [11] <author> P. M. Clarkson, </author> <title> Optimal and Adaptive Signal Processing, </title> <publisher> CRC Press, </publisher> <address> Boca Raton, FL, </address> <year> 1993. </year>
Reference-contexts: Thus a quantity of fundamental interest in using the descent with respect to J 1 is the variability of ( i k j k ) 2 , which we will call the pseudo-eigenvalue spread (since it is analogous to the eigenvalue spread in LMS adaptive filtering <ref> [11] </ref>): s 1 (X) = min i;j ( i j ) 2 The corresponding quantity for the descent with respect to J 2 is s 2 (X) = ( i j ) 2 min i;j i j January 8, 1998 Submitted to IEEE Trans. Sig. Proc. <p> Autocorrelation Estimation The most obvious way to implement an adaptive transform coding system is to use a windowed correlation estimate of the form ^ X n = M k=nM+1 k : (15) If the true correlation is constant, then ^ X n is elementwise an unbiased, consistent estimator of X <ref> [11] </ref>. There will be "estimation noise" (variance in ^ X n due to having finite sample size) which decreases monotonically with M . <p> For equalization, x (n) = d (n) fl c (n), where c (n) is a channel impulse response. We consider here the case where f (n) is constrained to be a causal, L-tap FIR filter. This and other cases are discussed in detail in <ref> [11] </ref>. Finding the optimal filter is conceptually simple once we select a convenient vector notation. Let f = [f (0); f (1); : : : ; f (L 1)] T and x n = [x (n); x (n 1); : : : ; x (n L + 1)] T .
Reference: [12] <author> C. R. Johnson, Jr., </author> <title> Lectures on Adaptive Parameter Estimation, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: The estimate (15) implicitly uses a rectangular window to window the incoming data stream, so each sample vector is equally weighted. One way to more heavily weight the later sample vectors is to use a "forgetting factor" fi (as in Recursive Least Squares <ref> [12] </ref>), which is equivalent to using an exponential window: ^ X n = fi ^ X n1 + (1 fi)x n x T This scheme also reduces memory requirements. B.
Reference: [13] <author> V. K Goyal, J. Zhuang, and M. Vetterli, </author> <title> "Universal transform coding based on backward adaptation," </title> <booktitle> in Proc. IEEE Data Compression Conf., </booktitle> <editor> J. A. Storer and M. Cohn, Eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <year> 1997, </year> <pages> pp. 231-240, </pages> <publisher> IEEE Comp. Soc. Press. </publisher>
Reference-contexts: As the quantization becomes coarser, the convergence slows. Notice that with direct computation, quantization does not seem to lead to a nonzero steady-state error. This is suggestive of universal performance of the backward-adaptive scheme <ref> [13] </ref>; further discussion of this is beyond the scope of this paper. For a slowly varying source (! 1 = ! 2 = ! 3 = 0:001; see Figure 7 (b)), we again have that the performance with = 0:125 or 0.25 is indistinguishable from the performance without quantization.
Reference: [14] <author> H. P. Kramer and M. V. Mathews, </author> <title> "A linear coding for transmitting a set of correlated signals," </title> <journal> IRE Trans. Info. Theory, </journal> <volume> vol. 23, </volume> <pages> pp. 41-46, </pages> <month> Sept. </month> <year> 1956. </year>
Reference-contexts: January 8, 1998 Submitted to IEEE Trans. Sig. Proc. GOYAL & VETTERLI: ADAPTIVE TRANSFORM CODING USING LMS-LIKE . . . 25 Fig. 8. Basic transform coding system. performed at the decoder. In classical transform coding theory, as introduced in <ref> [14] </ref> and analyzed in detail in [15], the source vectors are assumed to have identical distributions and are treated independently. 5 The problem is to find the orthogonal 6 transform T and quantizer Q such that for a fixed bit rate R (bits per scalar coefficient) the distortion D = Ekx
Reference: [15] <author> J. J. Y. Huang and P. M. Schultheiss, </author> <title> "Block quantization of correlated Gaussian random variables," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 11, </volume> <pages> pp. 289-296, </pages> <month> Sept. </month> <year> 1963. </year>
Reference-contexts: January 8, 1998 Submitted to IEEE Trans. Sig. Proc. GOYAL & VETTERLI: ADAPTIVE TRANSFORM CODING USING LMS-LIKE . . . 25 Fig. 8. Basic transform coding system. performed at the decoder. In classical transform coding theory, as introduced in [14] and analyzed in detail in <ref> [15] </ref>, the source vectors are assumed to have identical distributions and are treated independently. 5 The problem is to find the orthogonal 6 transform T and quantizer Q such that for a fixed bit rate R (bits per scalar coefficient) the distortion D = Ekx n ^x n k 2 is <p> It is often said that otherwise the inverse transform will enhance quantization errors, but detailed justification is difficult. For a Gaussian signal, it was shown in <ref> [15] </ref> that T should be orthogonal and that the decoder should use T 1 . 7 We have used X in place of the usual R x in order to reduce the need for multiple subscripts. 8 This is the case when the source is Gaussian.
Reference: [16] <author> A. Gersho and R. M. Gray, </author> <title> Vector Quantization and Signal Compression, </title> <publisher> Kluwer Acad. Pub., </publisher> <address> Boston, MA, </address> <year> 1992. </year> <note> January 8, 1998 Submitted to IEEE Trans. Sig. Proc. </note>
Reference-contexts: We arrive at the optimality of the Karhunen-Loeve Transform (KLT) as follows (see <ref> [16] </ref> for more details). Because of the orthogonality of T , kx n ^x n k 2 = ky n ^y n k 2 , so the distortion in x is exactly the distortion in quantizing y. <p> An orthogonal T that minimizes (19) (and hence (18)) is one that diagonalizes X; i.e., Y = T XT T is diagonal. (A simple proof based on the arithmetic/geometric mean inequality is given in <ref> [16] </ref>.) Among such transforms, one that leaves the diagonal of Y sorted in nonincreasing order is called a Karhunen-Loeve Transform (KLT) for the source. The KLT is unique (up to a choice of signs for each row) if the eigenvalues of X are distinct. <p> However, the KLT is believed to be a good transform for transform coding in other situations as well because the principle of decorrelating as a means of reducing redundancy is generally applicable. The reader is referred to <ref> [16] </ref> for more details on transform coding. II. Brief Review of Adaptive FIR Wiener Filtering The canonical Wiener filtering 9 problem is described as follows. Let x (n) and d (n) be jointly wide-sense stationary, zero-mean, scalar random processes.
References-found: 16

