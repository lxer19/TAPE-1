URL: http://www.pdl.cs.cmu.edu/ftp/NASD/Goddard96.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/er1p/www/ErikRiedel.html
Root-URL: 
Title: Understanding Customer Dissatisfaction With Underutilized Distributed File Servers  
Author: Erik Riedel Garth Gibson 
Address: Park, MD.  5000 Forbes Avenue Pittsburgh PA 15213  5000 Forbes Avenue Pittsburgh PA 15213  
Affiliation: College  Department of Electrical and Computer Engineering Carnegie Mellon University  School of Computer Science Carnegie Mellon University  
Note: Appears in the Proceedings of the Fifth NASA Goddard Space Flight Center Conference on Mass Storage Systems and Technologies  See also the CMU web pages at  
Email: riedel@cmu.edu  garth.gibson@cs.cmu.edu  
Phone: Tel: 412-268-3056  
Date: September 17-19, 1996  
Web: http://www.cs.cmu.edu/Web/Groups/NASD  
Abstract: An important trend in the design of storage subsystems is a move toward direct network attachment. Network-attached storage offers the opportunity to offload distributed file system functionality from dedicated file server machines and execute many requests directly at the storage devices. For this strategy to lead to better performance as perceived by users, the response time of distributed operations must improve. In this paper, we analyze measurements of an Andrew File System (AFS) server that we recently upgraded in an effort to improve client performance in our laboratory. While the original servers overall utilization was only about 3%, we show how burst loads were sufficiently intense to lead to periods of poor response time significant enough to trigger customer dissatisfaction. In particular, we show how, after adjusting for network load and traffic to non-project servers, 50% of the variation in client response time was explained by variation in server CPU utilization. That is, clients saw long response times in large part because the server was often overutilized when it was used at all. Using these measures, we see that offloading file server work in a network-attached storage architecture has the potential to benefit user response time. Computational power in such a system scales directly with storage capacity, so the slowdown during burst periods should be reduced. This research is sponsored by DARPA/ITO through ARPA Order D306, and issued by Indian Head Division, Naval Surface Warfare Center, under contract N00174-96-0002. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing official policies, either expressed or implied, of any sponsoring or supporting agency, including the Defense Advanced Research Projects Agency and the United States Government. 
Abstract-found: 1
Intro-found: 1
Reference: [Spasojevic96] <author> Spasojevic, M. and Satyanarayanan, M. </author> <title> An Empirical Study of a Wide-Area Distributed File System ACM Transactions on Computer Systems . May 1996. </title>
Reference-contexts: A distributed file system, with a number of machines acting as servers and a much larger number of clients have become popular due to a number of factors, including separation of administrative concerns, sharing of data, and transparency <ref> [Spasojevic96] </ref>. Advances in other computing technologies have made possible many novel applications that are placing increasing demands on distributed storage systems. <p> This introduces some amount of noise into our data, making some variations more difficult to explain. 7 3.1. Client Caching As shown in previous work, the hit ratio for data in the local AFS cache is extremely good <ref> [Spasojevic96, Howard88] </ref>. Table 1 gives the average hourly hit ratio across the twenty clients for which we have the most complete data. <p> Request Sizes Table 3 shows the distribution of request sizes over the course of a week. As seen in previous studies, small requests dominate the mix, while most of the bytes are moved in large requests <ref> [Spasojevic96, Baker91] </ref>. 80% of reads and 65% of writes are for less than 8 kilobytes. However, for StoreData requests, more than two-thirds of the bytes are moved at the largest request size. <p> As Table 5 shows, peak loads, even at the granularity of an hour, are much higher than average loads. Moreover, the distribution of operations measured over the long term, shown on the left of Table 5 and similar to previous studies <ref> [Spasojevic96] </ref> is not preserved in these peak periods - data activity is nearly twice as common in these peaks. With customer satisfaction sensitive to response time variation, the server performance during peak loads is likely to be more important than at other times.
Reference: [Howard88] <editor> Howard, J. et. al. </editor> <booktitle> Scale and Performance in a Distributed File System ACM Transactions on Computer Systems . Volume 6, </booktitle> <volume> Number 1. </volume> <month> February </month> <year> 1988, </year> <pages> pp. 51-81. 18 </pages>
Reference-contexts: The major contribution of AFS over previous distributed file systems such as the Network File System (NFS), was the focus on scalability of server resources. The goal of AFS was to support a campus-wide network of workstations and users with a relatively small amount of file server resources <ref> [Howard88] </ref>. The primary way in which AFS addressed this goal is through the use of local disk for extensive clientside caching. Each client workstation in an AFS environment dedicates a portion of its local disk space as a cache for frequently accessed remote data. <p> This introduces some amount of noise into our data, making some variations more difficult to explain. 7 3.1. Client Caching As shown in previous work, the hit ratio for data in the local AFS cache is extremely good <ref> [Spasojevic96, Howard88] </ref>. Table 1 gives the average hourly hit ratio across the twenty clients for which we have the most complete data.
Reference: [Mummert94] <author> Mummert, L. and Satyanarayanan, M. </author> <title> Long Term Distributed File Reference Tracing: Implementation and Experience Technical Report CMU-CS-94-213 . November 1994. </title>
Reference-contexts: Analysis Tools Traces of file server activity were taken with the aid of a tracing package developed by the Coda group at Carnegie Mellon <ref> [Mummert94] </ref>. A number of trace points, including most system calls, all accesses into the buffer cache, and all disk requests, within the operating system were annotated with log entries.
Reference: [Kirk90] <author> Kirk, R. </author> <title> Statistics: An Introduction. </title> <publisher> Holt, Rinehart and Winston, Inc. </publisher> <year> 1990, </year> <pages> pp. 155-190. </pages>
Reference-contexts: We use the Pearson coefficient of determination to quantify how much of the variation in a set of measurements can be accounted for by the characteristics of underlying system factors <ref> [Kirk90] </ref>. 3. Workload Characteristics In this section, we summarize a number of basic parameters of the workload recorded in our traces.
Reference: [Ruemmler93] <author> Ruemmler, C. and Wilkes, J. </author> <booktitle> UNIX disk access patterns Proceedings of the USENIX Winter 1995 Technical Conference . January 1993, </booktitle> <pages> pp. 405-420. </pages>
Reference: [Baker91] <editor> Baker, M. et. al. </editor> <booktitle> Measurements of a Distributed File Sy stem Proceedings of the 13 th Symposium on Operating Systems Principles . October 1991. </booktitle>
Reference-contexts: Request Sizes Table 3 shows the distribution of request sizes over the course of a week. As seen in previous studies, small requests dominate the mix, while most of the bytes are moved in large requests <ref> [Spasojevic96, Baker91] </ref>. 80% of reads and 65% of writes are for less than 8 kilobytes. However, for StoreData requests, more than two-thirds of the bytes are moved at the largest request size.
Reference: [Gibson96] <author> Gibson, G. et. al. </author> <title> A Case for Network-Attached Secure Disks Technical Report CMU-CS-96-142 . June 1996. </title>
Reference-contexts: This result fits well with our prior observations that a considerable number of cycles are required to move data from a disk, through the user-level fileserver process, back into the kernel, and onto the network, and that these numbers scale with the amount of data being moved <ref> [Gibson96] </ref>. We have finally discovered the correlation we have been seeking - a faster server CPU benefits AFS users because there are bursts of CPU activity, specifically when data is being transferred, during which server load leads to poor client response times. 15 5. Network-Attached Storage 5.1. <p> As illustrated in Figure 8, this creates the opportunity for disks with sufficient intelligence to perform a significant fraction of the clients file operations without the need for intervention from the distributed file server <ref> [Gibson96] </ref>. Eliminating the server machine as a bottleneck for data transfers between storage and applications provides a significant opportunity for improving overall performance. <p> In order to achieve the desired scalability and performance, it may also be necessary to have file status and inquiry functions handled at the drives <ref> [Gibson96] </ref>. 16 This direct transfer concept is not a new one. In 1991, Randy Katz described the basic advances that make network-attached devices feasible [Katz91].
Reference: [Seagate96] <institution> Seagate Corporation Barricuda Family Product Brief (ST19171). </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: At one end of the spectrum, Network SCSI is being promoted by several vendors as a means of providing third-party transfer between clients and drives attached directly to the network <ref> [Seagate96] </ref>. All commands are processed by a server which uses the SCSI third-party transfer interface to instruct drives to transfer data directly to clients.
Reference: [NetApp96] <institution> Network Appliance Advantage http://www.netapp.com/products . July 1996. </institution>
Reference-contexts: All commands are processed by a server which uses the SCSI third-party transfer interface to instruct drives to transfer data directly to clients. At the other end of the spectrum, dedicated Network File System (NFS) or Netware servers <ref> [NetApp96, NetFrame96] </ref> are storage systems that directly implement these distributed file system protocols, backed by specially optimized hardware configurations. Network-attached storage proposes to provide an intermediate point.
Reference: [Netframe96] <institution> The ClusterServer 8500 Series http://www.netframe.com/products . July 1996. </institution>
Reference-contexts: All commands are processed by a server which uses the SCSI third-party transfer interface to instruct drives to transfer data directly to clients. At the other end of the spectrum, dedicated Network File System (NFS) or Netware servers <ref> [NetApp96, NetFrame96] </ref> are storage systems that directly implement these distributed file system protocols, backed by specially optimized hardware configurations. Network-attached storage proposes to provide an intermediate point.
Reference: [Katz91] <author> Katz, R., </author> <booktitle> High-Performance Netwo rk- and Channel-Attached Storage Proceedings of the IEEE. </booktitle> <volume> Volume 80. </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: In order to achieve the desired scalability and performance, it may also be necessary to have file status and inquiry functions handled at the drives [Gibson96]. 16 This direct transfer concept is not a new one. In 1991, Randy Katz described the basic advances that make network-attached devices feasible <ref> [Katz91] </ref>. The High Performance Storage Systems project [Watson95] is exploring these technologies in the context of large MPP and SMP systems based on the framework of the Mass Storage Systems Reference Model [Miller88].
Reference: [Watson95] <author> Watson, R. and Coyne, R. </author> <booktitle> The Parallel I/O Architecture of the High-Performance Storage System (HPSS) Fourteenth IEEE Symposium on Mass Storage Systems. </booktitle> <month> September </month> <year> 1995, </year> <pages> pp. 27-44. </pages>
Reference-contexts: In 1991, Randy Katz described the basic advances that make network-attached devices feasible [Katz91]. The High Performance Storage Systems project <ref> [Watson95] </ref> is exploring these technologies in the context of large MPP and SMP systems based on the framework of the Mass Storage Systems Reference Model [Miller88].
Reference: [Miller88] <author> Miller, S. </author> <title> A Reference Model for Mass Storage Systems Advances in Computers . Volume 27. </title> <booktitle> 1988, </booktitle> <pages> pp. 157-210. </pages>
Reference-contexts: In 1991, Randy Katz described the basic advances that make network-attached devices feasible [Katz91]. The High Performance Storage Systems project [Watson95] is exploring these technologies in the context of large MPP and SMP systems based on the framework of the Mass Storage Systems Reference Model <ref> [Miller88] </ref>. Van Meter provides a survey of current products and major research issues, including security, network protocols, and the changes in operating system paradigms necessary to efficiently support network-attached devices [Van Meter96]. Such an architecture raises several important issues.
Reference: [Van Meter96] <author> Van Meter, R. </author> <title> A Brief Survey of Current Work on Network Attached Peripherals Operating Systems Review . Volume 30, Number 1. </title> <month> January </month> <year> 1996. </year>
Reference-contexts: Van Meter provides a survey of current products and major research issues, including security, network protocols, and the changes in operating system paradigms necessary to efficiently support network-attached devices <ref> [Van Meter96] </ref>. Such an architecture raises several important issues.
References-found: 14

