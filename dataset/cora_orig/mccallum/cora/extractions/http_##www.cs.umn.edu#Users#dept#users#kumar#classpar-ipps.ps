URL: http://www.cs.umn.edu/Users/dept/users/kumar/classpar-ipps.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: vsingh@hitachi.com fhan,kumarg@cs.umn.edu  
Title: Dynamic Load Balancing of Unstructured Computations in Decision Tree Classifiers  
Author: A. Srivastava E. Han V. Kumar V. Singh 
Address: anurags@hitachi.com University of Minnesota  
Affiliation: Information Technology Lab Dept. of Computer Science Information Technology Lab Hitachi America, Ltd. Army HPC Research Center Hitachi America, Ltd.  
Abstract: One of the important problems in data mining is discovering classification models from datasets. Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Highly parallel algorithms for constructing classification decision trees are desirable for dealing with large data sets. Algorithms for building classification decision trees have a natural con-currency, but are difficult to parallelize due to the inherent dynamic nature of the computation. In this short paper, we present parallel formulations of classification decision tree learning algorithm based on induction. We describe two basic parallel formulations, Synchronous Tree Construction Approach and Partitioned Tree Construction Approach. We propose a hybrid method that employs the good features of these methods. Our experimental results of the hybrid method on an IBM SP-2 demonstrate excellent speedups. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Experimental results of the hybrid method on an IBM SP-2 demonstrate excellent speedups. Extended version of this paper is available at [15]. 2 Sequential Classification Rule Learning Al gorithms Most of the existing induction-based algorithms like C4.5 [12], CDP <ref> [1] </ref>, SLIQ [11], and SPRINT [13] use Hunt's method [12] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g.
Reference: [2] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, Monterrey, </publisher> <address> CA, </address> <year> 1984. </year>
Reference-contexts: For a continuous attribute, binary tests involving all the distinct values of the attribute are considered. Once the class distribution information of all the attributes are gathered, each attribute is evaluated in terms of either entropy [12] or Gini Index <ref> [2] </ref>. The best attribute is selected as a test for the node expansion. 3 Parallel Formulations In this section, we give two basic parallel formulations for the classification decision tree construction and a hybrid scheme that combines good features of both of these approaches.
Reference: [3] <author> M. Chen, J. Han, and P. Yu. </author> <title> Data mining: An overview from database perspective. </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> 8(6) </volume> <pages> 866-883, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Data mining <ref> [3, 5, 7, 8, 13, 14] </ref> is the efficient and possibly unsupervised discovery of interesting, useful and previously unknown patterns from databases. One of the important problems in data mining is discovering classification models from datasets.
Reference: [4] <author> D. S. D. Michie and C. Taylor. </author> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks [10], genetic algorithms [6], and decision trees [12], have been proposed. Among these classification models, decision trees are probably the most popular since they obtain reasonable accuracy <ref> [4] </ref>, they are relatively inexpensive to compute and they are easy to interpret. In the data mining domain, the data to be processed tends to be very large. Hence, it is highly desirable to design com-putationally efficient as well as scalable algorithms.
Reference: [5] <author> U. Fayyad, G. Piatetski-Shapiro, and P. Smith. </author> <title> From data mining to knowledge discovery: An overview. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 1-34. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Data mining <ref> [3, 5, 7, 8, 13, 14] </ref> is the efficient and possibly unsupervised discovery of interesting, useful and previously unknown patterns from databases. One of the important problems in data mining is discovering classification models from datasets.
Reference: [6] <author> D. E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimizations and Machine Learning. </title> <address> Morgan-Kaufman, </address> <year> 1989. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks [10], genetic algorithms <ref> [6] </ref>, and decision trees [12], have been proposed. Among these classification models, decision trees are probably the most popular since they obtain reasonable accuracy [4], they are relatively inexpensive to compute and they are easy to interpret.
Reference: [7] <author> E. Han, G. Karypis, and V. Kumar. </author> <title> Scalable parallel data mining for association rules. </title> <booktitle> In Proc. of 1997 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Tucson, Ari-zona, </address> <year> 1997. </year>
Reference-contexts: 1 Introduction Data mining <ref> [3, 5, 7, 8, 13, 14] </ref> is the efficient and possibly unsupervised discovery of interesting, useful and previously unknown patterns from databases. One of the important problems in data mining is discovering classification models from datasets.
Reference: [8] <author> E. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hypergraphs (position paper). </title> <booktitle> In Proc. of the Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <pages> pages 9-13, </pages> <address> Tucson, Arizona, </address> <year> 1997. </year> <title> with different size datasets. </title>
Reference-contexts: 1 Introduction Data mining <ref> [3, 5, 7, 8, 13, 14] </ref> is the efficient and possibly unsupervised discovery of interesting, useful and previously unknown patterns from databases. One of the important problems in data mining is discovering classification models from datasets.
Reference: [9] <author> M. Joshi, G. Karypis, and V. Kumar. ScalParC: </author> <title> A new scalable and efficient parallel classification algorithm for mining large datasets. </title> <booktitle> In Proc. of the International Parallel Processing Symposium, </booktitle> <year> 1998. </year>
Reference-contexts: We focus our presentation for discrete attributes only. Our approaches are also applicable when continuous attributes are present. Detailed discussion of handling continuous attributes is given in <ref> [9, 13, 16] </ref>. In all parallel formulations, we assume that N training cases are randomly distributed to P processors initially such that each processor has N=P cases. In Synchronous Tree Construction Approach approach, all processors construct a decision tree synchronously by sending and receiving class distribution information of local data.
Reference: [10] <author> R. Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 4(22), </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks <ref> [10] </ref>, genetic algorithms [6], and decision trees [12], have been proposed. Among these classification models, decision trees are probably the most popular since they obtain reasonable accuracy [4], they are relatively inexpensive to compute and they are easy to interpret.
Reference: [11] <author> M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology, </booktitle> <address> Avignon, France, </address> <year> 1996. </year>
Reference-contexts: Experimental results of the hybrid method on an IBM SP-2 demonstrate excellent speedups. Extended version of this paper is available at [15]. 2 Sequential Classification Rule Learning Al gorithms Most of the existing induction-based algorithms like C4.5 [12], CDP [1], SLIQ <ref> [11] </ref>, and SPRINT [13] use Hunt's method [12] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> We use binary split ting at each decision tree node and grow the tree in breadth first manner. For generating large datasets, we have used the widely used synthetic dataset proposed in the SLIQ paper <ref> [11] </ref> for all our experiments. Ten classification functions were also proposed in [11] for these datasets. We have used the function 2 dataset for our algorithms. In this dataset, there are two class labels and each record consists of 9 attributes having 3 categoric and 6 continuous attributes. <p> We use binary split ting at each decision tree node and grow the tree in breadth first manner. For generating large datasets, we have used the widely used synthetic dataset proposed in the SLIQ paper <ref> [11] </ref> for all our experiments. Ten classification functions were also proposed in [11] for these datasets. We have used the function 2 dataset for our algorithms. In this dataset, there are two class labels and each record consists of 9 attributes having 3 categoric and 6 continuous attributes. Experiments were done on an IBM SP2.
Reference: [12] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Application domains include retail target marketing, fraud detection, and design of telecommunication service plans. Several classification models like neural networks [10], genetic algorithms [6], and decision trees <ref> [12] </ref>, have been proposed. Among these classification models, decision trees are probably the most popular since they obtain reasonable accuracy [4], they are relatively inexpensive to compute and they are easy to interpret. In the data mining domain, the data to be processed tends to be very large. <p> Experimental results of the hybrid method on an IBM SP-2 demonstrate excellent speedups. Extended version of this paper is available at [15]. 2 Sequential Classification Rule Learning Al gorithms Most of the existing induction-based algorithms like C4.5 <ref> [12] </ref>, CDP [1], SLIQ [11], and SPRINT [13] use Hunt's method [12] as the basic algorithm. <p> Experimental results of the hybrid method on an IBM SP-2 demonstrate excellent speedups. Extended version of this paper is available at [15]. 2 Sequential Classification Rule Learning Al gorithms Most of the existing induction-based algorithms like C4.5 <ref> [12] </ref>, CDP [1], SLIQ [11], and SPRINT [13] use Hunt's method [12] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> For a continuous attribute, binary tests involving all the distinct values of the attribute are considered. Once the class distribution information of all the attributes are gathered, each attribute is evaluated in terms of either entropy <ref> [12] </ref> or Gini Index [2]. The best attribute is selected as a test for the node expansion. 3 Parallel Formulations In this section, we give two basic parallel formulations for the classification decision tree construction and a hybrid scheme that combines good features of both of these approaches.
Reference: [13] <author> J. Shafer, R. Agrawal, and M. Mehta. SPRINT: </author> <title> A scalable parallel classifier for data mining. </title> <booktitle> In Proc. of the 22nd VLDB Conference, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction Data mining <ref> [3, 5, 7, 8, 13, 14] </ref> is the efficient and possibly unsupervised discovery of interesting, useful and previously unknown patterns from databases. One of the important problems in data mining is discovering classification models from datasets. <p> Experimental results of the hybrid method on an IBM SP-2 demonstrate excellent speedups. Extended version of this paper is available at [15]. 2 Sequential Classification Rule Learning Al gorithms Most of the existing induction-based algorithms like C4.5 [12], CDP [1], SLIQ [11], and SPRINT <ref> [13] </ref> use Hunt's method [12] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> We focus our presentation for discrete attributes only. Our approaches are also applicable when continuous attributes are present. Detailed discussion of handling continuous attributes is given in <ref> [9, 13, 16] </ref>. In all parallel formulations, we assume that N training cases are randomly distributed to P processors initially such that each processor has N=P cases. In Synchronous Tree Construction Approach approach, all processors construct a decision tree synchronously by sending and receiving class distribution information of local data.
Reference: [14] <author> R. Srikant and R. Agrawal. </author> <title> Mining sequential patterns: Generalizations and performance improvements. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology, </booktitle> <address> Avignon, France, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Data mining <ref> [3, 5, 7, 8, 13, 14] </ref> is the efficient and possibly unsupervised discovery of interesting, useful and previously unknown patterns from databases. One of the important problems in data mining is discovering classification models from datasets.
Reference: [15] <author> A. Srivastava, E.-H. H. V. Singh, and V. Kumar. </author> <title> Parallel formulations of decision-tree classification algorithms. </title> <note> http://www.cs.umn.edu/kumar, 1998. </note>
Reference-contexts: We propose a hybrid method that employs the good features of these methods. Experimental results of the hybrid method on an IBM SP-2 demonstrate excellent speedups. Extended version of this paper is available at <ref> [15] </ref>. 2 Sequential Classification Rule Learning Al gorithms Most of the existing induction-based algorithms like C4.5 [12], CDP [1], SLIQ [11], and SPRINT [13] use Hunt's method [12] as the basic algorithm.
Reference: [16] <author> A. Srivastava, V. Singh, E.-H. Han, and V. Kumar. </author> <title> An efficient, scalable, parallel classifier for data mining. </title> <type> Technical Report TR-97-010,http://www.cs.umn.edu/kumar, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: We focus our presentation for discrete attributes only. Our approaches are also applicable when continuous attributes are present. Detailed discussion of handling continuous attributes is given in <ref> [9, 13, 16] </ref>. In all parallel formulations, we assume that N training cases are randomly distributed to P processors initially such that each processor has N=P cases. In Synchronous Tree Construction Approach approach, all processors construct a decision tree synchronously by sending and receiving class distribution information of local data. <p> For these experiments, we used the original data set with continuous attributes and used a clustering technique to dis-cretize continuous attributes at each decision tree node <ref> [16] </ref>. hybrid algorithm. Note that the parallel formulation gives almost identical performance as the serial algorithm in terms of accuracy and classification tree size [16]. The results in Figure 7 show the speedup of the hybrid approach. The results confirm that the hybrid approach is indeed very effective. <p> For these experiments, we used the original data set with continuous attributes and used a clustering technique to dis-cretize continuous attributes at each decision tree node <ref> [16] </ref>. hybrid algorithm. Note that the parallel formulation gives almost identical performance as the serial algorithm in terms of accuracy and classification tree size [16]. The results in Figure 7 show the speedup of the hybrid approach. The results confirm that the hybrid approach is indeed very effective.
References-found: 16

