URL: http://www.umiacs.umd.edu/users/vishkin/PUBLICATIONS/spaa97-sub.ps
Refering-URL: http://www.umiacs.umd.edu/users/vishkin/PUBLICATIONS/papers.html
Root-URL: 
Email: E-mail: vishkin@umiacs.umd.edu.  
Title: From Algorithm Parallelism to Instruction-Level Parallelism: An Encode-Decode Chain Using Prefix-Sum (extended abstract) general insight
Author: Uzi Vishkin designs. 
Note: Informally stated, a conclusion of this extended abstract is the  supported by NSF grant 9416890.  
Date: July 7, 1997  
Abstract: A novel comprehensive and coherent approach for the purpose of increasing instruction-level parallelism (ILP) is devised. The key new tool in our envisioned system update is the addition of a parallel prefix-sum (PS) instruction, which will have efficient implementation in hardware, to the instruction-set architecture. This addition gives for the first time a concrete way for recruiting the whole knowledge base of parallel algorithms for that purpose. The potential increase in ILP is demonstrated by experimental results for a test application. The main technical contribution is in the form of a "completeness theorem". Perhaps surprisingly, the current abstract proves that in an envisioned system which employs parallel PS functional units, a proper use of a serial programming language suffices for the following. With a moderate effort, one can program a parallel algorithm (in a serial language), so that a par-allelizing compiler (even without run-time methods!) will be able to extract the same (i.e., "complete") ILP from such serial code as from code written in a parallel language. Alternatively, rather than have the programmer produce the serial code, a precompiler could derive it from a parallel language. The most interesting idea in the proof is the reliance on the new parallel PS for circumventing collision-ambiguity in references to memory. Other new ideas in the paper include hardware-design of a prefix-sum unit and an on-line algorithm for high-bandwidth register-files. 
Abstract-found: 1
Intro-found: 1
Reference: [AG] <author> G.S. Almasi, and A. Gottlieb. </author> <title> Highly Parallel Computing, Second Edition. </title> <address> Ben-jamin/Cummings, Redwood City, California. </address> <year> 1994. </year>
Reference-contexts: Multiple prefix-sum will then be a sequence of the form: P S R 0 R 1 ... where, R 0 ; R 1 ... R k are registers and again the semantics should be obvious. 2. Prefix-sum is very similar to the Fetch-and-Add instruction; see <ref> [AG] </ref> who discuss its potential use in multi-processor systems. Using Fetch-and-Add as a multiprefix priority-concurrent-write primitive in a parallel programmer's model for a multi-processor parallel machine, was considered in [Vi83].
Reference: [Tera] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In Int. Conf. on Supercomputing, </booktitle> <pages> 1-6, </pages> <year> 1990. </year>
Reference-contexts: Note that in concept the multi-threading in this paper is much more general than vector processing. It is more similar to [TEL-95], which advocated simultaneous multi-threading, than to <ref> [Tera] </ref>, where frequent thread switching is used (but at a given clock only one thread can issue an instruction). The similarity to [TEL-95] is also limited though. In that paper each thread must be guided by completely separate control (i.e., program counter).
Reference: [BYA] <author> G. R. Beck, D. W. L. Yen and T. L. Anderson. </author> <title> "The Cydra 5 minisupercomputer: </title> <booktitle> architecture and implementation". The Journal of Supercomputing 7, </booktitle> <pages> 143-180, </pages> <year> 1993. </year>
Reference: [B90] <author> G.E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. 02142, </address> <year> 1990. </year>
Reference-contexts: Using Fetch-and-Add as a multiprefix priority-concurrent-write primitive in a parallel programmer's model for a multi-processor parallel machine, was considered in [Vi83]. See also: (i) a discussion of the Scan primitive and a demonstration of its usefulness in multi-processors in <ref> [B90] </ref>, and (ii) the multiprefix as an operator and instruction in [K-96] in the context of the SB-PRAM multiprocessor. In other words, the usefulness of multi-prefixes in compact and elegant parallel programming is well-established and the current paper relies on that.
Reference: [BCHSZ] <author> G.E. Blelloch, S. Chatterjee, J.C. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <booktitle> In Proc. 4th ACM PPOPP, </booktitle> <pages> 102-111, </pages> <year> 1993. </year>
Reference: [CSV] <author> A. Chandra, L. Stockmeyer and U. Vishkin. </author> <title> Constant size reducibility. </title> <journal> SIAM J. Computing 13, </journal> <year> 1984, </year> <pages> 423-439. </pages>
Reference: [CKPSSSSvE] <author> D.E. Culler, R.M. Karp, D. Patterson, A. Sahay, E.E. Santos, K.E. Schausner, R. Subramonian and T. von Eicken. </author> <title> LogP: A practical model of parallel computation. </title> <journal> CACM, </journal> <volume> 39(11), </volume> <year> 1996, </year> <pages> 78-85 </pages> . 
Reference: [CS] <author> D. Culler and J.P. Singh. </author> <title> Parallel Computer Architecture (alpha draft). </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <note> to appear. </note>
Reference-contexts: However, the first appendix tries to put this paper in a wider context than theory. The interesting discussion of architectural trends in the forthcoming book <ref> [CS] </ref> provides relevant background for the current paper. It is also helpful to bring verbatim the following statement: "the natural question to ask is how far will ILP go and at what point will the emphasis shift to thread level parallelism".
Reference: [DT93] <author> J. C. Dehnert and R. A. Towle. </author> <title> "Compiling for the Cydra 5". </title> <booktitle> The Journal of Supercomputing 7, </booktitle> <pages> 181-228, </pages> <year> 1993. </year>
Reference: [GMR] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> The QRQW PRAM: Accounting for contention in parallel algorithms. </title> <booktitle> In Proceedings of the Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 638-648, </pages> <year> 1994. </year>
Reference-contexts: of instructions on some current ILP-rich processors: any form of concurrent-writes (i.e., simultaneous access to the same memory location by several instructions for write purposes) results in having such concurrent-writes queued and treated serially one at a time (similar to what is modeled by QW, in the QRQW PRAM model <ref> [GMR] </ref>). So, current reality guides prohibiting concurrent-write in the modeling of ILP. Leaving other aspects of a PRAM-like model intact we proceed to defining the ILP-RAM. According to PRAM folklore this is similar to the CREW PRAM. 1.1.2.
Reference: [H] <author> J. Hastad. </author> <title> Almost optimal lower bounds for small depth circuits. </title> <booktitle> Advances in Computing Research 5, </booktitle> <year> 1989, </year> <pages> 143-170. </pages> <publisher> JAI Press, </publisher> <address> Greenwich, Conn. </address>
Reference: [HP] <author> J.L. Hennessy and D.A. Patterson. </author> <title> Computer Architecture A Qualitative Approach, Second Edition. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1996. </year>
Reference-contexts: Classic performance goals such as: (i) improving completion time of a single job, and (ii) overall throughput of a computer system, make it no surprise that on-chip parallelism, in general, and instruction-level parallelism (ILP), in particular, are getting increasing attention. For instance, a considerable fraction of the standard text <ref> [HP] </ref> is devoted to that, and each among the main computer vendors came out in the last two years with processors supporting ILP (page 358 in [HP] lists Intel P6, AMD K5, Sun UltraSPARC, DEC Alpha 21164, MIPS R10000, PowerPC 640/620 and HP 8000). <p> For instance, a considerable fraction of the standard text <ref> [HP] </ref> is devoted to that, and each among the main computer vendors came out in the last two years with processors supporting ILP (page 358 in [HP] lists Intel P6, AMD K5, Sun UltraSPARC, DEC Alpha 21164, MIPS R10000, PowerPC 640/620 and HP 8000). This text also implies that hardware capabilities will allow ILP of several hundreds by the beginning of the next decade. <p> Extant literature Parallelizing compilers for standard serial languages, such as C, are getting a lot of attention recently: (i) current and emerging "ILP-based" processors need them to fully 2 utilize their computation power <ref> [HP] </ref> (for example, writing programs in ordinary languages, such as C or C++, tops the "software wish list" for future processors in [So]), and (ii) in connection with the multi-institution umbrella research project entitled SUIF (Stanford University Intermediate Format) in the context of multiprocessors [SUIF]. <p> to study this parallelizing emulation of a RAM computation by an ILP-RAM, and the memory disambiguation determinations required, since the inductive process used in that emulation for determining the instructions that can be simultaneously issued next is similar to current typical superscalar processors (using a single program counter, as per <ref> [HP] </ref>, page 279). For the purpose of the current paper, this shows that the RAM can provide a useful platform for describing parallel algorithm. The proof of our Completeness Theorem, later in the abstract, essentially revisits the above back and forth emulations to achieve a considerable enhancement. 1.1.3. <p> It is also worth noting that using standard loop-unrolling (see, e.g. Chapter 4 in <ref> [HP] </ref>), a compiler will be able to: (i) increment the FOR loop counter (i.e., generate the required intermediate values for i), and (ii) remove branch instructions at the end of unrolled iterations, thereby removing data and control dependencies. Simplifying the problem . Getting a single operation per thread. <p> This completes the proof outline of the Completeness Theorem. Using computer architecture terminology these show how to use the parallel prefix-sum instruction in the assumed PS-RAM machine for reducing read-after-write (RAW) hazards and write-after-write (WAW) hazards, as defined in <ref> [HP] </ref>. It has already been emphasized that the above presentation aims to convey the new concept rather than attempting the most efficient worst-case implementation, or any improvement at all beyond the worst-case. For instance, the explicit use of "gatekeeper" is often not needed.
Reference: [Ja] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference-contexts: This general way of looking at parallel PRAM algorithms is represented, for example, in <ref> [Ja] </ref>; it has been first demonstrated as useful for describing parallel algorithms in [SV]. <p> This paper suggests adding this (individual and multiple) prefix-sum instruction to the instruction-set architecture. Prefix summation may be implemented by existing instruction-set architectures, using a standard balanced binary tree, as presented, for example, in <ref> [Ja] </ref>. Preferred embodyments for the theme of the current paper, however, implement prefix summation in hardware. <p> We refer the reader to books on parallel algorithms, e.g., <ref> [Ja] </ref>, to see how PARDO commands are used for code-like description of PRAM algorithms. <p> It appears that using PARDO (as described above) for expressing parallel algorithms is without loss of generality, since other forms of expressing parallelism can be efficiently emulated using PARDO commands (using time and total number of operations as the efficiency parameters). Indeed, books on parallel algorithms (again, we take <ref> [Ja] </ref>, as an example) use primarily PARDO commands for code-like description of algorithms.
Reference: [K-96] <author> C.W. Kessler. </author> <title> Quick reference guides: (i) Fork95, and (ii) SB-PRAM: Instruction set simulator system software. </title> <institution> Universitat Trier, FB IV -Informatik, D-54286 Trier, Germany, </institution> <month> may </month> <year> 1996. </year>
Reference-contexts: See also: (i) a discussion of the Scan primitive and a demonstration of its usefulness in multi-processors in [B90], and (ii) the multiprefix as an operator and instruction in <ref> [K-96] </ref> in the context of the SB-PRAM multiprocessor. In other words, the usefulness of multi-prefixes in compact and elegant parallel programming is well-established and the current paper relies on that.
Reference: [Knuth] <author> D.E. Knuth. </author> <title> The Stanford GraphBase. </title> <publisher> Addison-Wesley, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: For generating inputs, we used <ref> [Knuth] </ref> which generates random graphs. For the first code the amount of ILP was measured by a simulator due to Lam-Wilson [LW]. That simulator allows several alternative "abstract machine models" (each representing some potential power of compilation and run-time methods).
Reference: [LW] <author> M.S. Lam and R.P. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In Proc. 19th Int. Symp. on Computer Architecture, </booktitle> <pages> 46-57, </pages> <year> 1992. </year>
Reference-contexts: Another approach for "programming" an ILP-RAM is given later in this subsection in the context of mapping a RAM algorithm into an ILP-RAM algorithm. Note that empirical studies of ILP, such as <ref> [LW] </ref>, have also used work and time as efficiency measures, although sometimes implicitly. Next, we relate the ILP-RAM and the RAM. Emulating the ILP-RAM by the RAM. This emulation is known and trivial. One parallel round is emulated after the other. Consider the emulation of a single ILP-RAM round. <p> For generating inputs, we used [Knuth] which generates random graphs. For the first code the amount of ILP was measured by a simulator due to Lam-Wilson <ref> [LW] </ref>. That simulator allows several alternative "abstract machine models" (each representing some potential power of compilation and run-time methods).
Reference: [Ma] <editor> M.M. Mano. </editor> <booktitle> Digital Logic and Computer Design. </booktitle> <publisher> Prentice Hall, </publisher> <year> 1979. </year>
Reference: [MV] <author> S.M. Mueller and U. Vishkin. </author> <title> Conflict-free access to multiple single-ported register files. </title> <booktitle> in Proc. 11th IEEE-IPPS, </booktitle> <year> 1997, </year> <note> to appear. 14 </note>
Reference: [OV95] <author> R. Orni and U. Vishkin. </author> <title> Two Computer Systems Paradoxes: Serialize-to-Parallelize, and Queuing Concurrent-Writes. </title> <type> Preprint, </type> <month> September </month> <year> 1995. </year>
Reference-contexts: A recent position paper, <ref> [OV95] </ref>, criticizes adverse effects in current programming practice which block, and even undoes, potential benefits from parallel algorithmic thinking. The current paper binds parallelizing compiler ideas with suggestions for alleviating this critique. For concreteness, these ideas are presented as part of the proof of the Completeness Theorem.
Reference: [P96] <author> Y. Patt. </author> <title> How Should We Use the 100 Million Transistors Each Chip Will Have Available in the Year 2000? Title of a Distinguished Lecture, </title> <institution> Department of Computer Science, UCLA, </institution> <month> November </month> <year> 1996. </year>
Reference: [RGP] <author> B.R. Rau, </author> <title> C.D. Glaeser and R.L. Picard, "Efficient code generation for horizontal architectures: </title> <booktitle> compiler techniques and architectural support". In Proc. ISCA, </booktitle> <pages> 131-139, </pages> <year> 1982. </year>
Reference: [SV] <author> Y. Shiloach and U. Vishkin. </author> <title> An O(n 2 logn) parallel max-flow algorithm. </title> <editor> J. </editor> <booktitle> of Algorithms, 3 (1982), </booktitle> <pages> 128-146. </pages>
Reference-contexts: This general way of looking at parallel PRAM algorithms is represented, for example, in [Ja]; it has been first demonstrated as useful for describing parallel algorithms in <ref> [SV] </ref>.
Reference: [So] <author> G. Sohi. </author> <title> Multiscalar processors. </title> <note> Transparencies of a talk available from the following URL: http : ==www:cs:wisc:edu= mscalar=multiscalar talks:html. </note>
Reference-contexts: standard serial languages, such as C, are getting a lot of attention recently: (i) current and emerging "ILP-based" processors need them to fully 2 utilize their computation power [HP] (for example, writing programs in ordinary languages, such as C or C++, tops the "software wish list" for future processors in <ref> [So] </ref>), and (ii) in connection with the multi-institution umbrella research project entitled SUIF (Stanford University Intermediate Format) in the context of multiprocessors [SUIF]. A recent position paper, [OV95], criticizes adverse effects in current programming practice which block, and even undoes, potential benefits from parallel algorithmic thinking.
Reference: [SUIF] <author> SUIF Compiler System: </author> <title> Advanced Compiler Technology for Scalable Parallel Machines. </title> <note> Summary of Research in 95-96. http://suif.stanford.edu/darpa/summary96.html. </note>
Reference-contexts: fully 2 utilize their computation power [HP] (for example, writing programs in ordinary languages, such as C or C++, tops the "software wish list" for future processors in [So]), and (ii) in connection with the multi-institution umbrella research project entitled SUIF (Stanford University Intermediate Format) in the context of multiprocessors <ref> [SUIF] </ref>. A recent position paper, [OV95], criticizes adverse effects in current programming practice which block, and even undoes, potential benefits from parallel algorithmic thinking. The current paper binds parallelizing compiler ideas with suggestions for alleviating this critique.
Reference: [TEL-95] <author> D.M. Tullsen, S.J. Eggers, and H.M. Levy. </author> <title> Simultaneous multithreading: maximizing on-chip parallelism. </title> <booktitle> In Proc. 22nd Int. Symp. on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Note that in concept the multi-threading in this paper is much more general than vector processing. It is more similar to <ref> [TEL-95] </ref>, which advocated simultaneous multi-threading, than to [Tera], where frequent thread switching is used (but at a given clock only one thread can issue an instruction). The similarity to [TEL-95] is also limited though. In that paper each thread must be guided by completely separate control (i.e., program counter). <p> It is more similar to <ref> [TEL-95] </ref>, which advocated simultaneous multi-threading, than to [Tera], where frequent thread switching is used (but at a given clock only one thread can issue an instruction). The similarity to [TEL-95] is also limited though. In that paper each thread must be guided by completely separate control (i.e., program counter).
Reference: [Va90] <author> L.G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> CACM, </journal> <volume> 33(8), </volume> <year> 1990. </year>
Reference: [Vi83] <author> U. Vishkin. </author> <title> On choice of a parallel model of computation. </title> <institution> TR-61, Dept. of Computer Science, Courant Institute, </institution> <address> New York University, </address> <year> 1983. </year>
Reference-contexts: Prefix-sum is very similar to the Fetch-and-Add instruction; see [AG] who discuss its potential use in multi-processor systems. Using Fetch-and-Add as a multiprefix priority-concurrent-write primitive in a parallel programmer's model for a multi-processor parallel machine, was considered in <ref> [Vi83] </ref>. See also: (i) a discussion of the Scan primitive and a demonstration of its usefulness in multi-processors in [B90], and (ii) the multiprefix as an operator and instruction in [K-96] in the context of the SB-PRAM multiprocessor.
Reference: [Vi84] <author> U. Vishkin. </author> <title> Parallel-Design Distributed-Implementation (PDDI) General Purpose Computer. </title> <booktitle> Theoretical Computer Science 32 (1984), </booktitle> <pages> 157-172. </pages>
Reference: [Vi96a] <author> U. Vishkin. </author> <title> Can parallel algorithms enhance serial implementation? CACM, </title> <type> 39(9), </type> <year> 1996, </year> <pages> 88-91 </pages> . 
Reference: [Vi96b] <author> U. Vishkin. </author> <title> U.S. Patent Application No. </title> <address> 08/667,554, </address> <month> June </month> <year> 1996. </year> <title> U. Vishkin. Thinking in Parallel: Some Basic Data-Parallel Algorithms and Techniques. </title> <journal> Monograph, </journal> <note> in preparation. </note>
Reference-contexts: Contributions A few salient contributions are highlighted below (emphasizing new ideas). The first is a new tool (the main micro-contribution), the second is a new framework (the main macro-contribution), and the third is another application of the new tool. The first and third contributions are based on <ref> [Vi96b] </ref>. Add prefix-sum to the instruction-set architecture of a uniprocessor. While the format of a prefix-sum instruction is similar to standard uniprocessor instructions, successive prefix-sum instructions may combine into a (non-standard) multi-operand powerful parallel instruction. The following bullet points to a new idea.

References-found: 30

