URL: ftp://ftp.ics.hawaii.edu/pub/tr/ics-tr-93-17.ps.Z
Refering-URL: ftp://ftp.ics.hawaii.edu/pub/tr/INDEX.html
Root-URL: 
Title: An Instrumented Approach to Improving Software Quality through Formal Technical Review Research Paper  
Author: Philip M. Johnson 
Address: Honolulu, HI 96822  
Affiliation: Department of Information and Computer Sciences University of Hawaii  
Abstract: CSRS is an instrumented, computer-supported cooperative work environment for formal technical review. CSRS addresses problems in the practice of FTR by providing computer support for both the process and products of FTR. CSRS also addresses problems in research on FTR through instrumentation supporting fine-grained, high quality data collection and analysis. This paper describes CSRS, a computer-mediated review method called FTArm, and selected findings from their use to explore issues in formal technical review. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Frank Ackerman, Lynne S. Buchwald, and Frank H. Lewski. </author> <title> Software inspections: An effective verification process. </title> <journal> IEEE Software, </journal> <pages> pages 31-36, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Another problem with the current state of research on review is conflicting and/or anecdotal explanations of the causal factors underlying review outcomes. For example, researchers have variously attributed an FTR method's effectiveness to general group interaction [8, 23], producer-reviewer interaction [20, 22], lack of producer-reviewer interaction <ref> [1, 24] </ref>, individual effort [15], paraphrasing [9], selective test cases [1], stepwise abstraction [25], and checklists [17]. While these claims are not all mutually exclusive, they clearly reveal confusion within the community about review factors and their effectiveness. <p> For example, researchers have variously attributed an FTR method's effectiveness to general group interaction [8, 23], producer-reviewer interaction [20, 22], lack of producer-reviewer interaction [1, 24], individual effort [15], paraphrasing [9], selective test cases <ref> [1] </ref>, stepwise abstraction [25], and checklists [17]. While these claims are not all mutually exclusive, they clearly reveal confusion within the community about review factors and their effectiveness. These issues in the state of review research are not raised with the intent of denigrating the research or the researchers.
Reference: [2] <author> Lowell Jay Arthur. </author> <title> Improving Software Quality. </title> <booktitle> Wi-ley Professional Computing, </booktitle> <year> 1993. </year>
Reference-contexts: Low quality has always figured prominently in explanations for software mishaps, from the Mariner I probe destruction in 1962, to AT&T's 4EES switching circuit failure in 1992. More recently, however, low software quality has also been implicated in competitive failure on a corporate scale <ref> [2] </ref>, as well as in loss of life on a human scale [18]. Research on tools and techniques to improve software quality shows that formal technical review (FTR) provides unique and important benefits. <p> In concert with other process improvements, Fujitsu found FTR to be so effective at discovering errors that they dropped system testing from their software development procedure <ref> [2] </ref>. FTR forms an essential part of methods and models for very high quality software, such as Cleanroom Software Engineering and the SEI Capability Maturity Model. Finally, FTR displays a unique ability to improve the quality of the producer as well as the quality of the product.
Reference: [3] <author> V.R. Basili and R.W. Selby. </author> <title> Comparing the effectiveness of software testing strategies. </title> <type> Technical Report TR-1501, </type> <institution> University of Maryland at College Park, Department of Computer Science, </institution> <year> 1985. </year>
Reference-contexts: However, the current state of FTR research fails to provide clear guidance to an organization in choosing a well-suited review method. One deficiency in the literature is the lack of high quality, empirical studies comparing different review methods to each other. Past studies compare review to testing <ref> [3, 20] </ref> or compare different factors within a single review method (usually Fagan's inspection), such as the effect of the number of participants or group composition on review effectiveness [5, 19].
Reference: [4] <author> V.R. Basili, R.W. Selby, and D.H. Hutchins. </author> <title> Experimentation in software engineering. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(7):733-743, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: Research on tools and techniques to improve software quality shows that formal technical review (FTR) provides unique and important benefits. Some studies provide evidence that FTR can be more effective at discovering errors than testing, while others indicate that it can discover dif ferent classes of errors than testing <ref> [20, 4] </ref>. In concert with other process improvements, Fujitsu found FTR to be so effective at discovering errors that they dropped system testing from their software development procedure [2].
Reference: [5] <author> David B. Bisant and James R. Lyle. </author> <title> A two-person inspection method to improve programming productivity. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(10) </volume> <pages> 1294-1304, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Past studies compare review to testing [3, 20] or compare different factors within a single review method (usually Fagan's inspection), such as the effect of the number of participants or group composition on review effectiveness <ref> [5, 19] </ref>. In addition, although these latter comparative studies claim to use the same approach (Fagan code inspection), ambiguities and inconsistencies in the description of the review method indicate that this key factor was not controlled sufficiently to allow cross-study comparison.
Reference: [6] <author> L. Brothers, V. Sembugamoorthy, and M. Muller. ICICLE: </author> <title> Groupware for code inspection. </title> <booktitle> In Proceedings of the 1990 Conference on Computer Supported Cooperative Work, </booktitle> <pages> pages 169-181, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Our goal for this instrumentation is to provide a basis for empirically-based process experi mentation and improvement. Such instrumentation is a major feature distinguishing CSRS from other automated review environments such as ICICLE <ref> [6] </ref>, Scrutiny [13], or INSPEQ [17]. CSRS supports both outcome and process instrumentation. Outcome instrumentation. <p> As a result, we take issue with research claiming to "automate" Fagan code inspection, such as the research on ICICLE <ref> [6] </ref>. While this system appears to provide a useful form of computer-supported review, the introduction of substantial computer support results in a method fundamentally different from Fagan's code inspection.
Reference: [7] <author> Lionel E. Deimel. </author> <title> Scenes of Software Inspections. Video Dramatizations for the Classroom. </title> <institution> Software Engineering Institute, Carnegie Mellon University, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Although manual FTR, when properly carried out, is typically cost-effective in the long run, this is a significant qualification, since manual FTR is very difficult to properly carry out. The primary obstacles to successful practice have been documented <ref> [7, 11, 12] </ref> and include: * Insufficient preparation. A ubiquitous cause of low quality review is when one or more inadequately prepared reviewers attempt to "bluff" their way through the review process. This problem is serious enough that fairly devious remedies are presented in the literature.
Reference: [8] <author> Robert Dunn. </author> <title> Software Quality: Concepts and Plans. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Another problem with the current state of research on review is conflicting and/or anecdotal explanations of the causal factors underlying review outcomes. For example, researchers have variously attributed an FTR method's effectiveness to general group interaction <ref> [8, 23] </ref>, producer-reviewer interaction [20, 22], lack of producer-reviewer interaction [1, 24], individual effort [15], paraphrasing [9], selective test cases [1], stepwise abstraction [25], and checklists [17]. While these claims are not all mutually exclusive, they clearly reveal confusion within the community about review factors and their effectiveness.
Reference: [9] <author> Michael E. Fagan. </author> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM System Journal, </journal> <volume> 15(3) </volume> <pages> 182-211, </pages> <year> 1976. </year>
Reference-contexts: Beyond this general similarity, specific approaches to FTR exhibit wide variations in process and products, from Fagan Code Inspection <ref> [9] </ref>, to Active Design Reviews [22], to Phased Inspections [17]. Despite its importance and potential, the state of both FTR practice and research suffers from problems that hinder its adoption and effective use within organizations. <p> For example, researchers have variously attributed an FTR method's effectiveness to general group interaction [8, 23], producer-reviewer interaction [20, 22], lack of producer-reviewer interaction [1, 24], individual effort [15], paraphrasing <ref> [9] </ref>, selective test cases [1], stepwise abstraction [25], and checklists [17]. While these claims are not all mutually exclusive, they clearly reveal confusion within the community about review factors and their effectiveness.
Reference: [10] <author> Michael E. Fagan. </author> <title> Advances in software inspections. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(7):744-751, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: This has been identified as a key open problem in formal technical review <ref> [10] </ref>. We are currently investigating whether or not various measures of complexity can serve as a predictive measure of review effort for source code artifacts. Three measures have been chosen for analysis: Halstead's volumetric complexity, McCabe's cyclometric complexity, and simple lines of code.
Reference: [11] <author> D. P. Freedman and G. M. Weinberg. </author> <title> Handbook of Walkthroughs, Inspections and Technical Reviews. Little, </title> <publisher> Brown, </publisher> <year> 1990. </year>
Reference-contexts: Although manual FTR, when properly carried out, is typically cost-effective in the long run, this is a significant qualification, since manual FTR is very difficult to properly carry out. The primary obstacles to successful practice have been documented <ref> [7, 11, 12] </ref> and include: * Insufficient preparation. A ubiquitous cause of low quality review is when one or more inadequately prepared reviewers attempt to "bluff" their way through the review process. This problem is serious enough that fairly devious remedies are presented in the literature. <p> Such a method is less expensive and more objective than techniques where behavioral data (such as "Shows solidarity", "Shows Tension", and so forth) is collected by a passive observer of the review in order to provide feedback on the quality of reviewer participation <ref> [11] </ref>. Third, outcome measures can contribute to empirically-guided process improvement. <p> It is well known that precise and accurate measurement of cumulative review effort (including preparation) is notoriously difficult to obtain in typical industrial settings <ref> [11] </ref>. Moreover, no other manual or automated review method provides effort data at the grain size of individual components of a review artifact.
Reference: [12] <author> Tom Gilb and Dorothy Graham. </author> <title> Software Inspection. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Although manual FTR, when properly carried out, is typically cost-effective in the long run, this is a significant qualification, since manual FTR is very difficult to properly carry out. The primary obstacles to successful practice have been documented <ref> [7, 11, 12] </ref> and include: * Insufficient preparation. A ubiquitous cause of low quality review is when one or more inadequately prepared reviewers attempt to "bluff" their way through the review process. This problem is serious enough that fairly devious remedies are presented in the literature.
Reference: [13] <author> John Gintell, John Arnold, Michael Houde, Jacek Kruszelnicki, Roland McKen-ney, and Gerard Memmi. Scrutiny: </author> <title> A collaborative inspection and review system. </title> <booktitle> In Proceedings of the Fourth European Software Engineering Conference, </booktitle> <address> Garwisch-Partenkirchen, Germany, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Our goal for this instrumentation is to provide a basis for empirically-based process experi mentation and improvement. Such instrumentation is a major feature distinguishing CSRS from other automated review environments such as ICICLE [6], Scrutiny <ref> [13] </ref>, or INSPEQ [17]. CSRS supports both outcome and process instrumentation. Outcome instrumentation.
Reference: [14] <author> Robert L. Glass. </author> <title> Modern Programming Practices: </title>
Reference-contexts: Research also tends to agree that manual review is expensive. For example, one person-year of technical staff time is required per 20 KLOC for FTR at Bell-Northern Research, and this cost adds 15-20% new overhead onto development [24]. Boeing Computer Services found reviews to be "extremely expensive" <ref> [14] </ref>. Such admissions are usually followed by analyses demonstrating that this upstream investment is more than recouped through decreases in downstream rework costs.
References-found: 14

