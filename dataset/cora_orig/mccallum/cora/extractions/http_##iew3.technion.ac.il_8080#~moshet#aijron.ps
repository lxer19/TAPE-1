URL: http://iew3.technion.ac.il:8080/~moshet/aijron.ps
Refering-URL: http://iew3.technion.ac.il:8080/~moshet/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: brafman@cs.ubc.ca  moshet@ie.technion.ac.il  
Title: Modeling Agents as Qualitative Decision Makers  
Author: Ronen I. Brafman Moshe Tennenholtz 
Address: Vancouver, B.C. Canada V6T 1Z4  Haifa 32000, Israel  
Affiliation: Dept. of Computer Science University of British Columbia  Industrial Engineering and Management Technion  
Abstract: We investigate the semantic foundations of a method for modeling agents as entities with a mental state which was suggested by McCarthy and by Newell. Our goals are to formalize this modeling approach and its semantics, to understand the theoretical and practical issues that it raises, and to address some of them. In particular, this requires specifying the model's parameters and how these parameters are to be assigned (i.e., their grounding). We propose a basic model in which the agent is viewed as a qualitative decision maker with beliefs, preferences, and decision strategy; and we show how these components would determine the agent's behavior. We ground this model in the agent's interaction with the world, namely, in its actions. This is done by viewing model construction as a constraint satisfaction problem in which we search for a model consistent with the agent's behavior and with our general background knowledge. In addition, we investigate the conditions under which a mental state model exists, characterizing a class of goal-seeking agents that can be modeled in this manner; and we suggest two criteria for choosing between consistent models, showing conditions under which they lead to a unique choice of model.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Adams. </author> <title> The Logic of Conditionals. </title> <address> D. </address> <publisher> Reidel, </publisher> <address> Dordrecht, Netherlands, </address> <year> 1975. </year>
Reference-contexts: Even in the case of an intelligent agent, using those ideas to ascribe its beliefs requires knowing what statistical information it has, and that it is acting in accordance to the ideas of Bacchus et al. Goldsmidtz and Pearl's *-semantics <ref> [24, 1] </ref> views beliefs as qualitative representations of probabilities: One believes that birds fly if one holds that P r (F lyjBird) 1. This approach is semantically close to ours, since subjective probability can be given semantics in terms of the agent's choice of actions.
Reference: [2] <author> C. E. Alchourron, P. Gardenfors, and D. Makinson. </author> <title> On the logic of theory change: partial meet functions for contraction and revision. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 50 </volume> <pages> 510-530, </pages> <year> 1985. </year>
Reference-contexts: We wish to add global constraints on the agent's beliefs that reflect these relations between local states. That is, we would like to model an agent's belief change. There is vast literature on the issue of how an agent should change its beliefs (e.g., <ref> [2, 7, 16, 22, 28, 29] </ref>), and we will discuss its relation to our work later on. However, our modeling perspective will lead us to ask somewhat different questions. In what follows, we assume our agent does not forget. <p> Indeed, relations between belief revision and belief update, and representations using partial and total pre-orders are well known. It was shown by Grove [25] that any revision operator that satisfies the AGM postulates <ref> [2] </ref> can be represented using a ranking of the set of possible states. However, to obtain that result, additional constraints on the agent's beliefs in a revision state are needed. <p> When we observe an agent repeatedly performing the same task, starting at different actual worlds but with the same local state, we will need the additional properties used in [25, 29] to obtain a fixed ranking. However, we note that the difference between our approach and the AGM approach <ref> [2] </ref> is more fundamental. They ask the question: how should I change my beliefs? We ask: how should I model the belief change of another agent? This difference becomes clearer in the next subsection, where we examine the suitability of admissible beliefs for modeling agents.
Reference: [3] <author> J. F. Allen. </author> <title> Recognizing intentions from natrual language utterances. </title> <editor> In M. Brady and R.C. Berwick, editors, </editor> <booktitle> Computational models of discourse. </booktitle> <publisher> MIT Press, </publisher> <year> 1983. </year> <month> 43 </month>
Reference-contexts: Given the success of Levesque's approach in the more limited domain, we hope that the more general abstraction given here will serve the same role in the general study of agents. Plan recognition In plan recognition <ref> [3, 14, 27, 30, 48] </ref> one tries to infer the plans of other agents by communicating with them or observing their behavior. This modeling task closely resembles the prediction task we discussed in Section 4, and the latter can be viewed as giving a semantic account of plan recognition.
Reference: [4] <author> F. Bacchus, A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Statistical foundations for default rea-soning. </title> <booktitle> In Proc. Thirteenth International Joint Conference on AI, </booktitle> <pages> pages 563-569, </pages> <year> 1995. </year>
Reference-contexts: A model of an agent's knowledge does not tell us about this agent's actual behavior. Other works have proposed groundings for beliefs. We presented our view of the semantics of beliefs in the end of Section 2.3. Bacchus, Grove, Halpern, and Koller <ref> [4] </ref> ground the notion of belief in statistical information. Their work answers questions such as: what should we believe about the bird Tweety given that 90% of birds fly. Statistical information can be viewed as summarizing concrete observation of the world, therefore, it is grounded.
Reference: [5] <author> D.G. Bobrow. </author> <title> Artificial intelligence in perspective: </title> <journal> fifty volumes of artificial intelligence journal. Artificial Intelligence, </journal> <pages> pages 5-20, </pages> <year> 1993. </year>
Reference-contexts: The scarcity of citations on this issue in a recent survey of work on mental states within AI by Shoham and Cousins [58] attests to this fact. 1 Similarly, although Newell's paper on the Knowledge Level [45] is among the most referenced AI papers <ref> [5] </ref>, in his perspective paper [46], Newell laments the lack of work following up on these ideas by the logicist community. He mentions Levesque's [40] as the only exception. Given this situation, it is worth clarifying what we view as the four central questions in mental-level modeling.
Reference: [6] <author> C. Boutilier. </author> <title> Toward a logic for qualitative decision theory. </title> <editor> In J. Doyle, E. Sandewall, and P. Torasso, editors, </editor> <booktitle> Proc. of Fourth Intl. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 75-86, </pages> <year> 1994. </year>
Reference-contexts: Many researchers support an agent design approach in which the designer specifies an initial database of beliefs, goals, intentions, etc., which is then explicitly manipulated by the agent (e.g., <ref> [57, 52, 49, 6] </ref> and much of the work on belief revision). Grounding may not seem a crucial issue for such work because human designers are likely to find mental attitudes natural to specify. However, grounding is crucial for modeling applications. <p> deliberation under uncertainty has not escaped the attention of AI researchers (e.g., Thomason [60] incorporates some type of common-sense deliberation about conflicting goals, and Rao and Georgeff [51] incorporate expected payoff calculations for decision making), qualitative decision criteria have only recently shown up in the AI literature on mental states <ref> [6, 10] </ref>. In contrast, we do not include intentions in our model. It seems that intentions play an important role in modeling resource bounded agents: much like beliefs allow the agent to ignore certain pos 34 sible outcomes of its actions, intentions allow it to ignore certain possible actions. <p> Usually, these worlds will be truth assignments to some set of propositions deemed relevant by the modeler. This points to the various logics for representing mental states as natural tools for reasoning about these models. In particular, Boutilier's logic for qualitative decision theory <ref> [6] </ref> seems promising. It is able to deal with beliefs, preferences, and a limited set of decision criteria; and its semantics is close to ours. Similarly, algorithms for constructing mental-level models will be needed.
Reference: [7] <author> C. Boutilier and M. Goldszmidt. </author> <title> Revising by conditional beliefs. </title> <booktitle> In Proc. of AAAI-93, </booktitle> <pages> pages 648-654, </pages> <year> 1993. </year>
Reference-contexts: We wish to add global constraints on the agent's beliefs that reflect these relations between local states. That is, we would like to model an agent's belief change. There is vast literature on the issue of how an agent should change its beliefs (e.g., <ref> [2, 7, 16, 22, 28, 29] </ref>), and we will discuss its relation to our work later on. However, our modeling perspective will lead us to ask somewhat different questions. In what follows, we assume our agent does not forget.
Reference: [8] <author> R. I. Brafman. </author> <title> Qualitative Models of Information and Decision Making: Foundations and Applications. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: For example, depending on a number of modeling choices, this may be the case when the agent has no information about its past positions. We refer the reader to <ref> [8] </ref> for a more complete discussion of this issue. 13 As we remarked in Section 2, non-determinism is handled by transforming all uncertainty about the effect of actions to uncertainty about the initial state of the environment.
Reference: [9] <author> R. I. Brafman and M. Tennenholtz. </author> <title> Belief ascription. </title> <booktitle> Working notes, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: It is able to deal with beliefs, preferences, and a limited set of decision criteria; and its semantics is close to ours. Similarly, algorithms for constructing mental-level models will be needed. We have designed such algorithms, which appear in <ref> [9] </ref>, but they are based on the state space representation and are only used for conceptual understanding of this issue. In fact, a declarative implementation of the abstract models described in this paper may not be desirable.
Reference: [10] <author> R. I. Brafman and M. Tennenholtz. </author> <title> Belief ascription and mental-level modelling. </title> <editor> In J. Doyle, E. Sandewall, and P. Torasso, editors, </editor> <booktitle> Proc. of Fourth Intl. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 87-98, </pages> <year> 1994. </year>
Reference-contexts: They ask the question: how should I change my beliefs? We ask: how should I model the belief change of another agent? This difference becomes clearer in the next subsection, where we examine the suitability of admissible beliefs for modeling agents. Finally, in <ref> [10] </ref> we investigate another pattern of belief change, which we call weak admissibility. <p> deliberation under uncertainty has not escaped the attention of AI researchers (e.g., Thomason [60] incorporates some type of common-sense deliberation about conflicting goals, and Rao and Georgeff [51] incorporate expected payoff calculations for decision making), qualitative decision criteria have only recently shown up in the AI literature on mental states <ref> [6, 10] </ref>. In contrast, we do not include intentions in our model. It seems that intentions play an important role in modeling resource bounded agents: much like beliefs allow the agent to ignore certain pos 34 sible outcomes of its actions, intentions allow it to ignore certain possible actions.
Reference: [11] <author> R. I. Brafman and M. Tennenholtz. </author> <title> Towards action prediction using a mental-level model. </title> <booktitle> In Proc. Fourteenth International Joint Conference on AI, </booktitle> <year> 1995. </year>
Reference-contexts: Then, A can be ascribed a consistent decomposed mental-level model hL A ; A A ; B cur ; u cur ; i. Proof: Theorem 3 of <ref> [11] </ref> says that when an agent can be ascribed admissible beliefs and the assumptions we have made in this paper are satisfied, i.e., satisfies the sure-thing principle and is static w.r.t. u, and its runs have bounded length, then its protocol can be derived from its mental-level model by using backwards
Reference: [12] <author> R. I. Brafman and Moshe Tennenholtz. </author> <title> On the foundations of qualitative decision theory. </title> <booktitle> In Proc.of the 13th National Conference on AI (AAAI '96), </booktitle> <pages> pages 1291-1296, </pages> <year> 1996. </year>
Reference-contexts: Or alternatively, if we decide to adopt the model, what implicit assumptions are we making about the agent's behavior. We are finally in a position to provide a partial answer to this question. 19 19 Additional results of this nature appear in <ref> [12, 13] </ref>. 29 We approach this task in a manner similar to Savage's work on the foundations of subjective probability [56]. Savage's approach allows us to ascribe a probability assignment and a value function to an agent based on its choice among actions. <p> All of the decision criteria that were mentioned in this paper, i.e., maximin, minmax regret and the principle of indifference, have been studied in this context [41]. However, until recently, there were no representation theorems analogous to Savage's for these qualitative decision models. In <ref> [12, 13] </ref>, we present the first such results for the maximin and minmax regret criteria. Using the language of this paper, these results should be viewed as existence theorems for static agents employing these decision criteria.
Reference: [13] <author> R. I. Brafman and Moshe Tennenholtz. </author> <title> On the axiomatization of qualitative decision criteria. </title> <booktitle> In AAAI Spring Symposium on Qualitative Preferences in Deliberation and Practical Reasoning, </booktitle> <year> 1997. </year>
Reference-contexts: Or alternatively, if we decide to adopt the model, what implicit assumptions are we making about the agent's behavior. We are finally in a position to provide a partial answer to this question. 19 19 Additional results of this nature appear in <ref> [12, 13] </ref>. 29 We approach this task in a manner similar to Savage's work on the foundations of subjective probability [56]. Savage's approach allows us to ascribe a probability assignment and a value function to an agent based on its choice among actions. <p> All of the decision criteria that were mentioned in this paper, i.e., maximin, minmax regret and the principle of indifference, have been studied in this context [41]. However, until recently, there were no representation theorems analogous to Savage's for these qualitative decision models. In <ref> [12, 13] </ref>, we present the first such results for the maximin and minmax regret criteria. Using the language of this paper, these results should be viewed as existence theorems for static agents employing these decision criteria.
Reference: [14] <author> E. Charniak and R. P. Goldman. </author> <title> A bayesian model of plan recognition. </title> <journal> Artificial Intelligence, </journal> <volume> 64(1), </volume> <year> 1993. </year>
Reference-contexts: Given the success of Levesque's approach in the more limited domain, we hope that the more general abstraction given here will serve the same role in the general study of agents. Plan recognition In plan recognition <ref> [3, 14, 27, 30, 48] </ref> one tries to infer the plans of other agents by communicating with them or observing their behavior. This modeling task closely resembles the prediction task we discussed in Section 4, and the latter can be viewed as giving a semantic account of plan recognition.
Reference: [15] <author> A. Darwiche and J. Pearl. </author> <title> Symbolic causal networks. </title> <booktitle> In AAAI'94, Proceedings of the Twelfth National American Conference on Artificial Intelligence, </booktitle> <pages> pages 238-244, </pages> <year> 1994. </year>
Reference: [16] <author> A. del Val and Y. Shoham. </author> <title> Deriving properties of belief update from theories of action. </title> <booktitle> In Proc. Eleventh Intl. Joint Conf. on Artificial Intelligence (IJCAI '89), </booktitle> <pages> pages 584-589, </pages> <year> 1993. </year>
Reference-contexts: We wish to add global constraints on the agent's beliefs that reflect these relations between local states. That is, we would like to model an agent's belief change. There is vast literature on the issue of how an agent should change its beliefs (e.g., <ref> [2, 7, 16, 22, 28, 29] </ref>), and we will discuss its relation to our work later on. However, our modeling perspective will lead us to ask somewhat different questions. In what follows, we assume our agent does not forget.
Reference: [17] <author> T. G. Dietterich. </author> <title> Machine learning. </title> <booktitle> In Annual Review of Computer Science, </booktitle> <volume> volume 4, </volume> <pages> pages 255-306. </pages> <year> 1990. </year>
Reference-contexts: One common approach for choosing among different models is to a priori restrict or rank them. In the first case, we limit the models that we are willing to ascribe; this is similar to the restricted hypothesis space bias in machine learning <ref> [17] </ref>. In the latter case we use the ranking over models to ascribe the most normal consistent model; this is similar to the notion of preference bias [17]. 17 In particular, we could use the additional structure obtained when the set of possible worlds corresponds to models of some propositional language. <p> case, we limit the models that we are willing to ascribe; this is similar to the restricted hypothesis space bias in machine learning <ref> [17] </ref>. In the latter case we use the ranking over models to ascribe the most normal consistent model; this is similar to the notion of preference bias [17]. 17 In particular, we could use the additional structure obtained when the set of possible worlds corresponds to models of some propositional language.
Reference: [18] <author> J. Doyle. </author> <title> Reasoned Assumptions and Pareto Optimality. </title> <booktitle> In Proc. Ninth International Joint Conference on AI, </booktitle> <pages> pages 87-90, </pages> <year> 1985. </year>
Reference: [19] <author> J. Doyle. </author> <title> Constructive belief and rational representation. </title> <journal> Computational Intelligence, </journal> <volume> 5 </volume> <pages> 1-11, </pages> <year> 1989. </year>
Reference: [20] <author> R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi. </author> <title> Reasoning about Knowledge. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Model structure We propose a structure for mental-level models (Sections 2, 3) that is motivated by work in decision theory [41] and previous work on knowledge ascription <ref> [20, 54] </ref> in which the agent is described as a qualitative decision maker. This model contains three key components: beliefs, preferences, and a decision criterion. <p> In order to model the behavior described in Figure 4 adequately, a number of obvious changes must be made to our model. First, we are no longer considering single states, but sequences of states, e.g., in the above example, we care about the robot's path. Following <ref> [20] </ref>, we shall use the term run to refer to the sequence of global states of the agent/environment system starting with its initial state. Runs describe the state of the agent and the environment over time.
Reference: [21] <author> P. C. Fishburn. </author> <title> Nonlinear preference and utility theory. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1988. </year>
Reference-contexts: In order to obtain better descriptive models, various weaker representation theorems have been proposed. These theorems make weaker assumptions on the manner in which agents choose their actions and use weaker representations of beliefs in their agents models, e.g., non-additive probabilities (see <ref> [21] </ref> for more details).
Reference: [22] <author> N. Friedman and J. Y. Halpern. </author> <title> A knowledge-based framework for belief change. Part I: </title> <booktitle> Foundations. In Proc. of the Fifth Conf. on Theoretical Aspects of Reasoning About Knowledge, </booktitle> <address> San Francisco, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We wish to add global constraints on the agent's beliefs that reflect these relations between local states. That is, we would like to model an agent's belief change. There is vast literature on the issue of how an agent should change its beliefs (e.g., <ref> [2, 7, 16, 22, 28, 29] </ref>), and we will discuss its relation to our work later on. However, our modeling perspective will lead us to ask somewhat different questions. In what follows, we assume our agent does not forget. <p> Patterns of belief change similar to ours emerge in the work of other researchers (e.g., <ref> [22, 36] </ref>). Indeed, relations between belief revision and belief update, and representations using partial and total pre-orders are well known. It was shown by Grove [25] that any revision operator that satisfies the AGM postulates [2] can be represented using a ranking of the set of possible states.
Reference: [23] <author> P. Gardenfors and N. E. Sahlin, </author> <title> editors. Decision, Probability, and Utility. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: For example, based on previous consumption habits of an agent, we may be able to predict its future habits. Such information can be of considerable value to many economic 21 Though there is no consensus on this matter. See the articles in <ref> [23] </ref>. 33 agents. The basic idea is that an agent's choices in various settings reveal his/her preference among various options.
Reference: [24] <author> M. Goldszmidt and J. Pearl. </author> <title> Rank-based systems: A simple approach to belief revision, belief update and reasoning about evidence and actions. </title> <booktitle> In Principles of Knowledge Representation and Reasoning: Proc. Third Intl. Conf. (KR '92), </booktitle> <pages> pages 661-672, </pages> <year> 1992. </year>
Reference-contexts: Even in the case of an intelligent agent, using those ideas to ascribe its beliefs requires knowing what statistical information it has, and that it is acting in accordance to the ideas of Bacchus et al. Goldsmidtz and Pearl's *-semantics <ref> [24, 1] </ref> views beliefs as qualitative representations of probabilities: One believes that birds fly if one holds that P r (F lyjBird) 1. This approach is semantically close to ours, since subjective probability can be given semantics in terms of the agent's choice of actions.
Reference: [25] <author> Adam Grove. </author> <title> Two modellings for theory change. </title> <journal> Journal of Philosophical Logic, </journal> <volume> 17 </volume> <pages> 157-170, </pages> <year> 1988. </year>
Reference-contexts: Patterns of belief change similar to ours emerge in the work of other researchers (e.g., [22, 36]). Indeed, relations between belief revision and belief update, and representations using partial and total pre-orders are well known. It was shown by Grove <ref> [25] </ref> that any revision operator that satisfies the AGM postulates [2] can be represented using a ranking of the set of possible states. However, to obtain that result, additional constraints on the agent's beliefs in a revision state are needed. <p> This puts less constraints on the modeler and allows us to obtain this result. When we observe an agent repeatedly performing the same task, starting at different actual worlds but with the same local state, we will need the additional properties used in <ref> [25, 29] </ref> to obtain a fixed ranking. However, we note that the difference between our approach and the AGM approach [2] is more fundamental.
Reference: [26] <author> J. Y. Halpern and Y. Moses. </author> <title> Knowledge and common knowledge in a distributed environment. </title> <journal> J. ACM, </journal> <volume> 37(3) </volume> <pages> 549-587, </pages> <year> 1990. </year>
Reference-contexts: Many of the definitions used here are standard and should be familiar to readers acquainted with formal work on mental states and the fundamentals of decision theory. We start with a low level description of an agent, motivated by the work of Halpern and Moses <ref> [26] </ref> and of Rosenschein [54], on top of which the mental-level model is defined. To clarify our definitions, we will accompany them with a simplified version of McCarthy's thermostat example [43]. <p> Thus, P W (l) are the worlds consistent with the agent's state of information when its local state is l. Halpern and Moses <ref> [26] </ref> and Rosenschein [54] use this definition to ascribe knowledge to an agent at a world w. <p> Methods of grounding these attributes are useful in our modeling context when they show us how to model a particular agent, and when they are able to say whether an agent is implementing a particular mental-level specification. Halpern and Moses <ref> [26] </ref> and Rosenschein [54] ground the notion of knowledge in the relationship between the local state of a machine and the state of its environment. We discussed their work in Section 2.
Reference: [27] <author> M. J. Huber, E. H. Durfee, and M. P. Wellman. </author> <title> The automated mapping of plans for plan recognition. In R.L. </title> <editor> de Mantaras and D. Poole, editors, </editor> <booktitle> Uncertainty in AI, Proceedings of the Tenth Conference, </booktitle> <pages> pages 344-352, </pages> <year> 1994. </year>
Reference-contexts: Given the success of Levesque's approach in the more limited domain, we hope that the more general abstraction given here will serve the same role in the general study of agents. Plan recognition In plan recognition <ref> [3, 14, 27, 30, 48] </ref> one tries to infer the plans of other agents by communicating with them or observing their behavior. This modeling task closely resembles the prediction task we discussed in Section 4, and the latter can be viewed as giving a semantic account of plan recognition.
Reference: [28] <author> H. Katsuno and A. Mendelzon. </author> <title> On the difference between updating a knowledge base and revising it. </title> <booktitle> In Principles of Knowledge Representation and Reasoning: Proc. Second Intl. Conf. (KR '91), </booktitle> <pages> pages 387-394, </pages> <year> 1991. </year>
Reference-contexts: We wish to add global constraints on the agent's beliefs that reflect these relations between local states. That is, we would like to model an agent's belief change. There is vast literature on the issue of how an agent should change its beliefs (e.g., <ref> [2, 7, 16, 22, 28, 29] </ref>), and we will discuss its relation to our work later on. However, our modeling perspective will lead us to ask somewhat different questions. In what follows, we assume our agent does not forget.
Reference: [29] <author> H. Katsuno and A. O. Mendelzon. </author> <title> Propositional knowledge base revision and minimal change. </title> <journal> Artificial Intelligence, </journal> <volume> 52(3), </volume> <year> 1991. </year>
Reference-contexts: We wish to add global constraints on the agent's beliefs that reflect these relations between local states. That is, we would like to model an agent's belief change. There is vast literature on the issue of how an agent should change its beliefs (e.g., <ref> [2, 7, 16, 22, 28, 29] </ref>), and we will discuss its relation to our work later on. However, our modeling perspective will lead us to ask somewhat different questions. In what follows, we assume our agent does not forget. <p> This puts less constraints on the modeler and allows us to obtain this result. When we observe an agent repeatedly performing the same task, starting at different actual worlds but with the same local state, we will need the additional properties used in <ref> [25, 29] </ref> to obtain a fixed ranking. However, we note that the difference between our approach and the AGM approach [2] is more fundamental. <p> We can then introduce various assumptions about the relationship between the agent's beliefs, as stated in that language, before and after new observations are made. Various relationships appear in the literature, and a number of such methods are discussed in <ref> [29] </ref>. As an illustration, consider an agent that knows p _ r and believes p ^ q. Suppose it now learn :q. Thus, none of the worlds that were previously considered plausible are still plausible.
Reference: [30] <author> H. Kautz. </author> <title> Generalized plan recognition. </title> <booktitle> In Proc. of AAAI-86, </booktitle> <year> 1986. </year>
Reference-contexts: Given the success of Levesque's approach in the more limited domain, we hope that the more general abstraction given here will serve the same role in the general study of agents. Plan recognition In plan recognition <ref> [3, 14, 27, 30, 48] </ref> one tries to infer the plans of other agents by communicating with them or observing their behavior. This modeling task closely resembles the prediction task we discussed in Section 4, and the latter can be viewed as giving a semantic account of plan recognition. <p> Grounding is not a central issue, since human agents (presumably) have an explicit mental state. The issue of existence is not dealt with either, but Kautz's use of circumscription <ref> [30] </ref> can be viewed as addressing the issue of model choice. Given these general differences, among the work on plan recognition we find Pollack's work [48] to be the most relevant to our work. Pollack explicitly treats plans as complex mental attitudes involving beliefs and intentions of agents with goals.
Reference: [31] <author> S. Kraus and D. J. Lehmann. </author> <title> Knowledge, belief and time. </title> <journal> Theoretical Computer Science, </journal> <volume> 58 </volume> <pages> 155-174, </pages> <year> 1988. </year>
Reference-contexts: We remark that (after adding interpretations to each world) this approach yields a KD45 belief operator and a relationship between knowledge and belief that was proposed by Kraus and Lehmann in <ref> [31] </ref>. 5 To precisely conform with the definitions, we would have had to include the set of outcomes in the set of possible states and to define the effects of actions on these outcomes. 9 Preferences Beliefs really make sense as part of a fuller description of the agent's mental state,
Reference: [32] <author> D. M. Kreps. </author> <title> Notes on the Theory of Choice. </title> <publisher> Westview Press, </publisher> <address> Boulder, </address> <year> 1988. </year>
Reference-contexts: In particular, most people do not feel they perform expected utility calculations when making different choices. But here lies an important conceptual idea upon which the theory of modeling choice <ref> [32] </ref> is founded: whether or not the agent actually makes its decision using probabilistic reasoning is of no consequence. The issue is whether or not a probabilistic model employed externally has the required predictive power. That is, will this model lead to accurate predictions of observables.
Reference: [33] <author> D. M. Kreps. </author> <title> A course in microeconomic theory. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, N.J., </address> <year> 1990. </year>
Reference-contexts: For example, one property, called the general revealed preference principle stipulates that preferences are acyclic. We refer the reader to <ref> [33] </ref> for more details. The aims of revealed preference theory and of our work have considerable overlap. In both cases, an attempt is made to construct an agent model with predictive power. In both areas, the question of preference persistence arises.
Reference: [34] <author> D. M. Kreps and R. Wilson. </author> <title> Sequential equilibria. </title> <journal> Econometrica, </journal> <volume> 50(4) </volume> <pages> 863-894, </pages> <year> 1982. </year>
Reference-contexts: This task of determining the set of possible worlds is a difficult one, and we view it as part of the model framing problem. Example 4 A simple game The following tree describes a one-person decision problem based on a game that appears in <ref> [34] </ref>: A A Y N 0 x Initially the agent decides whether to choose Y or N .
Reference: [35] <author> S. Kripke. </author> <title> Semantical considerations of modal logic. </title> <journal> Zeitschrift fur Mathematische Logik und Grundlagen der Mathematik, </journal> <volume> 9 </volume> <pages> 67-96, </pages> <year> 1963. </year> <month> 45 </month>
Reference-contexts: This last point is crucial if we take seriously Newell's idea of mental state models as abstract descriptions of agents. Grounding 1 The only modeling related works there deal with plan recognition. 2 is important semantically even from the design perspective: it makes concrete the abstract Kripke semantics <ref> [35] </ref> that is often used in the literature, and it allows us to answer a central question in the theoretical analysis of agents and their design: Does program X implement mental-level specification Y ? While grounding has been discussed by some authors (see Section 8), the questions of model choice, and
Reference: [36] <author> P. Lamarre and Y. Shoham. </author> <title> Knowledge, certainty, belief and conditionalization. </title> <booktitle> In Proc. of Fourth Intl. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <year> 1994. </year>
Reference-contexts: Patterns of belief change similar to ours emerge in the work of other researchers (e.g., <ref> [22, 36] </ref>). Indeed, relations between belief revision and belief update, and representations using partial and total pre-orders are well known. It was shown by Grove [25] that any revision operator that satisfies the AGM postulates [2] can be represented using a ranking of the set of possible states.
Reference: [37] <author> D. Lehmann and M. Magidor. </author> <title> What does a conditional knowledge base entail? Artificial Intelligence, </title> <booktitle> 55 </booktitle> <pages> 1-60, </pages> <year> 1992. </year>
Reference-contexts: Indeed, in the context of non-monotonic logics such preferences are well known. In non-monotonic logics, worlds minimal in a ranking structure are often described as `most normal', and structures that are `thicker' at the bottom are often preferred because they make fewer assumptions of non-normality (see, e.g., <ref> [37] </ref>).
Reference: [38] <author> H. Levesque. </author> <title> Foundations of a functional approach to knowledge representation. </title> <journal> Artificial Intelligence, </journal> <volume> 23(2) </volume> <pages> 155-212, </pages> <year> 1984. </year>
Reference-contexts: The semantics of this sequence is not clear. Another concrete interpretation for belief is supplied by Levesque's work on making believers out of computers <ref> [38, 40] </ref>. Levesque provides a functional view of knowledge bases, treating them as abstract data types on which two operations are performed: TELL and ASK. TELL adds new information to the database, and ASK is used to query the database.
Reference: [39] <author> H. J. Levesque. </author> <title> Knowledge representation and reasoning. An. </title> <journal> Rev. Comput. Sci., </journal> <volume> 1 </volume> <pages> 255-287, </pages> <year> 1986. </year>
Reference-contexts: Levesque's functional approach enables knowledge representation researchers to treat different knowledge-representation structures in a uniform manner, abstracting away implementation details that are irrelevant to their query answering behavior. This abstract view has proven extremely fruitful for understanding central issues in knowledge representation <ref> [39] </ref>. Given the success of Levesque's approach in the more limited domain, we hope that the more general abstraction given here will serve the same role in the general study of agents.
Reference: [40] <author> H. J. Levesque. </author> <title> Making believers out of computers. </title> <journal> Artificial Intelligence, </journal> <volume> 30 </volume> <pages> 81-108, </pages> <year> 1986. </year>
Reference-contexts: The abstract nature of mental-level models is also important for theoretical analysis. It provides a uniform basis for comparing and analyzing agents, much like Levesque's Computers as believers paradigm <ref> [40] </ref> allows for abstract analysis of knowledge representation schemes. The second property, intuitiveness, is valuable in design validation since one approach to design validation is to transform low level descriptions of agents that are difficult to analyze, such as procedural programs or mechanical designs, into intuitive high level models. <p> He mentions Levesque's <ref> [40] </ref> as the only exception. Given this situation, it is worth clarifying what we view as the four central questions in mental-level modeling. <p> The semantics of this sequence is not clear. Another concrete interpretation for belief is supplied by Levesque's work on making believers out of computers <ref> [38, 40] </ref>. Levesque provides a functional view of knowledge bases, treating them as abstract data types on which two operations are performed: TELL and ASK. TELL adds new information to the database, and ASK is used to query the database.
Reference: [41] <author> R. D Luce and H. Raiffa. </author> <title> Games and Decisions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1957. </year>
Reference-contexts: Model structure We propose a structure for mental-level models (Sections 2, 3) that is motivated by work in decision theory <ref> [41] </ref> and previous work on knowledge ascription [20, 54] in which the agent is described as a qualitative decision maker. This model contains three key components: beliefs, preferences, and a decision criterion. <p> Various parts of the model we propose should be familiar to readers acquainted with decision theory, game theory, and in particular, qualitative decision making techniques (see, e.g., <ref> [41] </ref>). We examine static agents that take only one-shot actions (dynamic agents are discussed in Section 3). These agents enter one of a possible set of states, perform an action and restart. We start in Section 2.1 with a motivating example introducing the central concepts used in our model. <p> We have previously encountered the maximin criterion, which selects those tuples whose worst case outcome is maximal. 6 Another example is the principle of indifference which selects those tuples whose average outcome is maximal. 7 (For a fuller discussion of decision criteria consult <ref> [41] </ref>.) Example 3 Consider the example given in the introduction in which the following matrix was used: rainy not-rainy take umbrella 5 1 leave umbrella 4 10 We have seen that when both worlds are plausible the two plausible outcomes are (5; 1) and (4; 10). <p> The principle of insufficient reason is not closed under unions, hence the lack of unique minimal belief assignment in Example 6. However, the maximin criterion is closed under unions, as are a number of other criteria discussed in <ref> [41] </ref>. Theorem 4 Given a belief ascription problem with a decision criterion that is closed under unions, if a consistent belief assignment exists then there is a unique minimal consistent belief assignment. <p> This is often referred to as decision making under complete ignorance. All of the decision criteria that were mentioned in this paper, i.e., maximin, minmax regret and the principle of indifference, have been studied in this context <ref> [41] </ref>. However, until recently, there were no representation theorems analogous to Savage's for these qualitative decision models. In [12, 13], we present the first such results for the maximin and minmax regret criteria.
Reference: [42] <author> M. Machina. </author> <title> Dynamic consistency and non-expected utility models of choice under uncertainty. </title> <journal> Journal of Economic Literature, </journal> <volume> 27 </volume> <pages> 1622-1668, </pages> <year> 1989. </year>
Reference: [43] <author> J. McCarthy. </author> <title> Ascribing mental qualities to machines. </title> <editor> In M. Ringle, editor, </editor> <booktitle> Philosophical Perspectives in Artificial Intelligence, </booktitle> <address> Atlantic Highlands, NJ, 1979. </address> <publisher> Humanities Press. </publisher>
Reference-contexts: In this method, agents are described as if they are qualitative decision makers with a mental state consisting of mental attributes such as beliefs, knowledge, and preferences. The use of such models, which we refer to as mental-level models, was proposed by McCarthy <ref> [43] </ref> and by Newell [45], and our goals are to provide a formal semantic account of this modeling process, to understand some of the key issues it raises, and to address some of them. The ability to model agents is useful in many settings. <p> We start with a low level description of an agent, motivated by the work of Halpern and Moses [26] and of Rosenschein [54], on top of which the mental-level model is defined. To clarify our definitions, we will accompany them with a simplified version of McCarthy's thermostat example <ref> [43] </ref>. The choice of modeling a thermostat, which we normally do not view as having beliefs, stresses our view of mental states as modeling abstractions. Example 2 In [43], McCarthy shows how we often ascribe mental states to simple devices. Our goal is to formalize his informal discussion of thermostats. <p> To clarify our definitions, we will accompany them with a simplified version of McCarthy's thermostat example <ref> [43] </ref>. The choice of modeling a thermostat, which we normally do not view as having beliefs, stresses our view of mental states as modeling abstractions. Example 2 In [43], McCarthy shows how we often ascribe mental states to simple devices. Our goal is to formalize his informal discussion of thermostats. We assume that we have a thermostat in a room that controls the flow of hot water into that room's radiator. <p> McCarthy advocated this idea in <ref> [43] </ref> where he motivated this approach and suggested a number of ideas for formalizing it. Newell also advocated this idea, stressing the need for an abstract level of representation of programs and machines, which he called the knowledge level [45].
Reference: [44] <author> S. Morris. </author> <title> Revising knowledge: A hierarchical approach. </title> <booktitle> In Proc. of the Fifth Conf. on Theoretical Aspects of Reasoning About Knowledge, </booktitle> <address> San Francisco, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus, the agent considers s to be plausible, for otherwise it would not have taken it into consideration and would have not preferred a over a 0 . This view of beliefs is closely related to Savage's notion of null-states [56] and Morris's definition of belief <ref> [44] </ref>. 3 Dynamic mental-level models In the previous section, we described a model of simple static agents and the basis for its construction. In this section we consider a more complex, dynamic model of agents that can take sequences of actions interleaved with observations.
Reference: [45] <author> A. Newell. </author> <title> The knowledge level. </title> <journal> AI Magazine, </journal> <pages> pages 1-20, </pages> <year> 1980. </year>
Reference-contexts: In this method, agents are described as if they are qualitative decision makers with a mental state consisting of mental attributes such as beliefs, knowledge, and preferences. The use of such models, which we refer to as mental-level models, was proposed by McCarthy [43] and by Newell <ref> [45] </ref>, and our goals are to provide a formal semantic account of this modeling process, to understand some of the key issues it raises, and to address some of them. The ability to model agents is useful in many settings. <p> The scarcity of citations on this issue in a recent survey of work on mental states within AI by Shoham and Cousins [58] attests to this fact. 1 Similarly, although Newell's paper on the Knowledge Level <ref> [45] </ref> is among the most referenced AI papers [5], in his perspective paper [46], Newell laments the lack of work following up on these ideas by the logicist community. He mentions Levesque's [40] as the only exception. <p> McCarthy advocated this idea in [43] where he motivated this approach and suggested a number of ideas for formalizing it. Newell also advocated this idea, stressing the need for an abstract level of representation of programs and machines, which he called the knowledge level <ref> [45] </ref>. However, the tone of both papers is, in general, intuitive, informal, and motivational. In fact, in [46], Newell laments the lack of attempts to pursue this approach within the logicist community. We believe this to be the first work within AI to provide formal treatment of these ideas. <p> In fact, a declarative implementation of the abstract models described in this paper may not be desirable. Indeed, an important point behind the concept of mental-level models, one stressed by Newell <ref> [45] </ref> in his discussion of the knowledge-level, is that they provide an abstract description of a system.
Reference: [46] <author> A. Newell. </author> <title> Reflections on the knowledge level. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> pages 31-38, </pages> <year> 1993. </year>
Reference-contexts: The scarcity of citations on this issue in a recent survey of work on mental states within AI by Shoham and Cousins [58] attests to this fact. 1 Similarly, although Newell's paper on the Knowledge Level [45] is among the most referenced AI papers [5], in his perspective paper <ref> [46] </ref>, Newell laments the lack of work following up on these ideas by the logicist community. He mentions Levesque's [40] as the only exception. Given this situation, it is worth clarifying what we view as the four central questions in mental-level modeling. <p> Newell also advocated this idea, stressing the need for an abstract level of representation of programs and machines, which he called the knowledge level [45]. However, the tone of both papers is, in general, intuitive, informal, and motivational. In fact, in <ref> [46] </ref>, Newell laments the lack of attempts to pursue this approach within the logicist community. We believe this to be the first work within AI to provide formal treatment of these ideas.
Reference: [47] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1988. </year>
Reference-contexts: of reasoning another step, we realize that the modeling process itself need not employ an abstract logical language, but can simply use the ideas discussed here and in 22 Semantically, probability distributions are also defined over sets of possible worlds, yet efficient representations of probability measures employ, e.g., Bayesian nets <ref> [47] </ref>. Similar structures developed for discrete notions as well, e.g.,[15], may be of use here. 37 similar work as abstract specifications. The implemented system will perform mental-level model-ing of other agents using data-structures and algorithms that are suitable for its domain.
Reference: [48] <author> M. E. Pollack. </author> <title> Plans as complex mental attitudes. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pol-lack, editors, </editor> <booktitle> Intentions in Communication, </booktitle> <pages> pages 77-104, </pages> <address> Cambridge, Massachusetts, 1990. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Given the success of Levesque's approach in the more limited domain, we hope that the more general abstraction given here will serve the same role in the general study of agents. Plan recognition In plan recognition <ref> [3, 14, 27, 30, 48] </ref> one tries to infer the plans of other agents by communicating with them or observing their behavior. This modeling task closely resembles the prediction task we discussed in Section 4, and the latter can be viewed as giving a semantic account of plan recognition. <p> The issue of existence is not dealt with either, but Kautz's use of circumscription [30] can be viewed as addressing the issue of model choice. Given these general differences, among the work on plan recognition we find Pollack's work <ref> [48] </ref> to be the most relevant to our work. Pollack explicitly treats plans as complex mental attitudes involving beliefs and intentions of agents with goals.
Reference: [49] <author> M. E. Pollack, D. J. Israel, and M. Bratman. </author> <title> Towards an architecture for resource-bounded agents. </title> <type> Technical Report Technical Note 425, </type> <institution> SRI International, </institution> <year> 1987. </year>
Reference-contexts: Many researchers support an agent design approach in which the designer specifies an initial database of beliefs, goals, intentions, etc., which is then explicitly manipulated by the agent (e.g., <ref> [57, 52, 49, 6] </ref> and much of the work on belief revision). Grounding may not seem a crucial issue for such work because human designers are likely to find mental attitudes natural to specify. However, grounding is crucial for modeling applications. <p> Rao and Georgeff [52] define an interpreter that uses three mental components, beliefs, intentions and desires. Pollack et al. propose an abstract agent architecture based on similar mental states <ref> [49] </ref>. Rosenschein and Kaelbling [55] developed an interpreter that can implement behavior that is specified using notions such as knowledge and goals. Shoham [57] presents an agent oriented programming language based on the notions of belief and commitment.
Reference: [50] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Machine learning One driving force behind our research is the desire to use agent models for prediction in multi-agent systems. In machine learning, models are also constructed to help make predictions. For example, decision trees <ref> [50] </ref> provide a way of modeling observed instances in a manner that enables predicting the classes of future instances.
Reference: [51] <author> A. S. Rao and M. P. Georgeff. </author> <title> Deliberation and its role in the formation of intentions. </title> <booktitle> In Proc. 7 Annual Conference on Uncertainty Artificial Intelligence (UAI '91), </booktitle> <year> 1991. </year>
Reference-contexts: While the need for deliberation under uncertainty has not escaped the attention of AI researchers (e.g., Thomason [60] incorporates some type of common-sense deliberation about conflicting goals, and Rao and Georgeff <ref> [51] </ref> incorporate expected payoff calculations for decision making), qualitative decision criteria have only recently shown up in the AI literature on mental states [6, 10]. In contrast, we do not include intentions in our model.
Reference: [52] <author> A. S. Rao and M. P. Georgeff. </author> <title> An abstract architecture for rational agents. </title> <booktitle> In Principles of Knowledge Representation and Reasoning: Proc. Third Intl. Conf. (KR '92), </booktitle> <pages> pages 439-449, </pages> <year> 1992. </year>
Reference-contexts: Many researchers support an agent design approach in which the designer specifies an initial database of beliefs, goals, intentions, etc., which is then explicitly manipulated by the agent (e.g., <ref> [57, 52, 49, 6] </ref> and much of the work on belief revision). Grounding may not seem a crucial issue for such work because human designers are likely to find mental attitudes natural to specify. However, grounding is crucial for modeling applications. <p> While their aim has been to supply intuitive and well founded tools for agent specification and design, rather than agent modeling, they are clearly relevant to the question of what structure our model should have. Rao and Georgeff <ref> [52] </ref> define an interpreter that uses three mental components, beliefs, intentions and desires. Pollack et al. propose an abstract agent architecture based on similar mental states [49]. Rosenschein and Kaelbling [55] developed an interpreter that can implement behavior that is specified using notions such as knowledge and goals.
Reference: [53] <author> A. S. Rao and G. Murray. </author> <title> Multi-agent mental state recognition and its application to air--combat modeling. </title> <booktitle> In Proceedings of the 13th Int. DAI Workshop (DAI-13), </booktitle> <year> 1994. </year>
Reference-contexts: The implemented system will perform mental-level model-ing of other agents using data-structures and algorithms that are suitable for its domain. A case in point are some of the recent systems used in the air-combat modeling domain <ref> [53, 59] </ref>. The goal of workers in these areas is to provide realistic air-combat simulators. Combat pilots use the current behavior of their opponents to ascribe them with goals and intentions. Then, they use these models to predict the future behavior of their opponents.
Reference: [54] <author> S. J. Rosenschein. </author> <title> Formal theories of knowledge in AI and robotics. </title> <journal> New Generation Comp., </journal> <volume> 3 </volume> <pages> 345-357, </pages> <year> 1985. </year>
Reference-contexts: Model structure We propose a structure for mental-level models (Sections 2, 3) that is motivated by work in decision theory [41] and previous work on knowledge ascription <ref> [20, 54] </ref> in which the agent is described as a qualitative decision maker. This model contains three key components: beliefs, preferences, and a decision criterion. <p> Many of the definitions used here are standard and should be familiar to readers acquainted with formal work on mental states and the fundamentals of decision theory. We start with a low level description of an agent, motivated by the work of Halpern and Moses [26] and of Rosenschein <ref> [54] </ref>, on top of which the mental-level model is defined. To clarify our definitions, we will accompany them with a simplified version of McCarthy's thermostat example [43]. <p> Thus, P W (l) are the worlds consistent with the agent's state of information when its local state is l. Halpern and Moses [26] and Rosenschein <ref> [54] </ref> use this definition to ascribe knowledge to an agent at a world w. Roughly, they say that the agent knows some fact ' at a world w if its local state l in w is such that ' holds in all the worlds in P W (l). <p> Methods of grounding these attributes are useful in our modeling context when they show us how to model a particular agent, and when they are able to say whether an agent is implementing a particular mental-level specification. Halpern and Moses [26] and Rosenschein <ref> [54] </ref> ground the notion of knowledge in the relationship between the local state of a machine and the state of its environment. We discussed their work in Section 2.
Reference: [55] <author> S. J. Rosenschein and L. P. Kaelbling. </author> <title> A situated view of representation and control. </title> <journal> Artificial Intelligence, </journal> <volume> 73(1-2):149-174, </volume> <year> 1995. </year>
Reference-contexts: Rao and Georgeff [52] define an interpreter that uses three mental components, beliefs, intentions and desires. Pollack et al. propose an abstract agent architecture based on similar mental states [49]. Rosenschein and Kaelbling <ref> [55] </ref> developed an interpreter that can implement behavior that is specified using notions such as knowledge and goals. Shoham [57] presents an agent oriented programming language based on the notions of belief and commitment.
Reference: [56] <author> L. J. Savage. </author> <title> The Foundations of Statistics. </title> <publisher> Dover Publications, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: Thus, the agent considers s to be plausible, for otherwise it would not have taken it into consideration and would have not preferred a over a 0 . This view of beliefs is closely related to Savage's notion of null-states <ref> [56] </ref> and Morris's definition of belief [44]. 3 Dynamic mental-level models In the previous section, we described a model of simple static agents and the basis for its construction. In this section we consider a more complex, dynamic model of agents that can take sequences of actions interleaved with observations. <p> A decision criterion satisfies the sure-thing principle if v ffi w is at least as preferred as v 0 ffi w 0 whenever v is at least as preferred as v 0 and w is at least as preferred as w 0 . Intuitively the sure-thing principle <ref> [56] </ref> says the following: suppose you prefer action a over a 0 when the current (or initial) plausible worlds are w 1 ; w 2 ; w 3 , and you also prefer a over a 0 when the 22 plausible worlds are w 4 ; w 5 ; w 6 <p> We are finally in a position to provide a partial answer to this question. 19 19 Additional results of this nature appear in [12, 13]. 29 We approach this task in a manner similar to Savage's work on the foundations of subjective probability <ref> [56] </ref>. Savage's approach allows us to ascribe a probability assignment and a value function to an agent based on its choice among actions. <p> These theorems tell the modeler under what assumptions on the agent's behavior the model is applicable. Because the assumptions are on the agent's behavior which is, in principle, observable, they can be empirically tested and (in)validated. The work of Savage <ref> [56] </ref> provides what many consider to be the most important result in choice representation. Savage provides a set of assumptions about the agent's approach to action choice under which it can be modeled as if it were an expected utility maximizer.
Reference: [57] <author> Y. Shoham. </author> <title> Agent-oriented programming. </title> <journal> Artificial Intelligence, </journal> <volume> 60(1) </volume> <pages> 51-92, </pages> <year> 1993. </year>
Reference-contexts: Many researchers support an agent design approach in which the designer specifies an initial database of beliefs, goals, intentions, etc., which is then explicitly manipulated by the agent (e.g., <ref> [57, 52, 49, 6] </ref> and much of the work on belief revision). Grounding may not seem a crucial issue for such work because human designers are likely to find mental attitudes natural to specify. However, grounding is crucial for modeling applications. <p> Pollack et al. propose an abstract agent architecture based on similar mental states [49]. Rosenschein and Kaelbling [55] developed an interpreter that can implement behavior that is specified using notions such as knowledge and goals. Shoham <ref> [57] </ref> presents an agent oriented programming language based on the notions of belief and commitment. What these structures lack is the notion of a decision criterion, which embodies the agent's approach to action choice under uncertainty.
Reference: [58] <author> Y. Shoham and S. B. Cousins. </author> <title> Logics of mental attitudes in AI: a very preliminary survey. </title> <editor> In G. Lakemeyer and B. Nebel, editors, </editor> <booktitle> Foundations of knowledge representation and reasoning. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The scarcity of citations on this issue in a recent survey of work on mental states within AI by Shoham and Cousins <ref> [58] </ref> attests to this fact. 1 Similarly, although Newell's paper on the Knowledge Level [45] is among the most referenced AI papers [5], in his perspective paper [46], Newell laments the lack of work following up on these ideas by the logicist community.
Reference: [59] <author> M. Tambe. </author> <title> Recursive agent and agent-group tracking in real-time, dynamic environments. </title> <booktitle> In Proc. of the Int. Conf. on Multi-Agent Systems (ICMAS '95), </booktitle> <year> 1995. </year>
Reference-contexts: The implemented system will perform mental-level model-ing of other agents using data-structures and algorithms that are suitable for its domain. A case in point are some of the recent systems used in the air-combat modeling domain <ref> [53, 59] </ref>. The goal of workers in these areas is to provide realistic air-combat simulators. Combat pilots use the current behavior of their opponents to ascribe them with goals and intentions. Then, they use these models to predict the future behavior of their opponents. <p> However, mental-level models play the above role of abstract specification tools. And while the implemented system is guided by the semantic model, the implementation itself makes extensive use of domain specific information and domain specific heuristics. For example, Tambe's system <ref> [59] </ref> assigns heuristic values to each consistent model as a means of choosing between different models consistent with the modeler's current information. Indeed, often there are many ways to model a given behavior.
Reference: [60] <author> R. H. Thomason. </author> <title> Towards a logical theory of practical reasoning. </title> <booktitle> In AAAI Spring Symposium on Reasoning About Mental States: Formal Theories and Applications, </booktitle> <year> 1993. </year>
Reference-contexts: What these structures lack is the notion of a decision criterion, which embodies the agent's approach to action choice under uncertainty. While the need for deliberation under uncertainty has not escaped the attention of AI researchers (e.g., Thomason <ref> [60] </ref> incorporates some type of common-sense deliberation about conflicting goals, and Rao and Georgeff [51] incorporate expected payoff calculations for decision making), qualitative decision criteria have only recently shown up in the AI literature on mental states [6, 10]. In contrast, we do not include intentions in our model.
Reference: [61] <author> Gil Tidhar, </author> <month> October </month> <year> 1996. </year> <type> Personal communication. </type>
Reference-contexts: The ability to reason about the mental state of opponents, i.e., their beliefs, goals, and intentions, is important for building good simulators. Groups in Australia (AAII) and California (ISI) have incorporated such technology in their commercial systems to a limited extent <ref> [61] </ref>. 5 s1 another car is crossing and it has right of way; s2 another car is crossing and A has right of way; s3 no other car is crossing and A has right of way; s4 no other car is crossing and A does not have right of way.
Reference: [62] <author> J. von Neumann and O. Morgenstern. </author> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1944. </year> <month> 47 </month>
Reference-contexts: Under this representation, the state s 1 is at least as preferred as state s 2 iff u (s 1 ) u (s 2 ). Value functions are usually associated with the work of von Neumann and Morgenstern on utility functions <ref> [62] </ref>. However, their utility functions express more than a simple pre-order over the set of states, and we do need to incorporate all of their additional assumptions. Because this section is concerned with simple agents that take one-shot actions and restart, we can view values as a function of state.
References-found: 62

