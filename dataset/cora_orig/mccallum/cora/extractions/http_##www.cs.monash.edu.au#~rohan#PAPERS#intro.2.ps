URL: http://www.cs.monash.edu.au/~rohan/PAPERS/intro.2.ps
Refering-URL: http://www.cs.monash.edu.au/~rohan/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: (jono@cs.monash.edu.au)  (rohan@cs.monash.edu.au)  
Title: MML and Bayesianism: Similarities and Differences (Introduction to Minimum Encoding Inference  
Author: Jonathan Oliver and Rohan Baxter . JONATHAN J. OLIVER ROHAN A. BAXTER 
Address: Clayton, Victoria, 3168, AUSTRALIA  
Affiliation: Computer Science Department Monash University  
Date: 1994 Amended August 15th 1995  
Note: Part II) (C) Copyright  December  
Abstract: Tech Report 206 Department of Computer Science, Monash University, Clayton, Vic. 3168, Australia Abstract: This paper continues the introduction to minimum encoding inference given by Oliver and Hand. This series of papers were written with the objective of providing an introduction to this area for statisticians. We examine the relationship between Bayesianism and Minimum Message Length (MML) inference. We argue that MML augments Bayesian methods by providing a sound Bayesian method for point estimation which is invariant under non-linear transformations. We explore the issues of invariance of estimators under non-linear transformations, the role of the Fisher Information matrix in MML inference, and the apparent similarity between MML and the adoption of a Jeffreys' Prior. We then compare MML to an approximate method of Bayesian Model Class Selection. Despite apparent similarities in their expressions, the properties of the two approaches can be different. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Akaike. </author> <title> Factor Analysis and AIC. </title> <journal> Psychometrika, </journal> <volume> 52(3) </volume> <pages> 317-332, </pages> <year> 1987. </year>
Reference-contexts: This second criticism may appear artificial. However, there are models where symmetric multimodal posteriors exist. For example, in Factor Analysis [32], a sign ambiguity means that posteriors will always be symmetric about 0. Bayesian Factor Analysis has therefore tended to use the mode as an estimate <ref> [1, 21] </ref>. * Median. | Selecting the median of the posterior has the property that it IS invariant under nonlinear transformations.
Reference: [2] <author> A. Barron. </author> <title> Logically smooth density estimation. </title> <type> PhD thesis, </type> <institution> Dept. Elect. Eng., Stanford Univ., </institution> <month> August </month> <year> 1985. </year>
Reference-contexts: The evidence equation (34) requires us to have found a mode, ^ , of the likelihood surface. * Thirdly, the MML equation (23) is invariant under non-linear parameter transformations; the evidence equation (34) is not invariant. Barron <ref> [2] </ref> saw the coding analogy and considered substituting the observed Fisher, M (), for the expected Fisher, F ().
Reference: [3] <author> R.A. Baxter and J.J. Oliver. </author> <title> MDL and MML: Similarities and differences. </title> <type> Technical report TR 207, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <year> 1994. </year> <note> Available on the WWW from http://www.cs.monash.edu.au/ ~ jono. </note>
Reference-contexts: For details of Rissanen's approach (termed Minimum Description Length or MDL) see [23, 24, 25]. A comparison of MML and MDL can be found in Baxter and Oliver <ref> [3] </ref>. 1.1 MML and Bayesian Data Analysis Given a data set, D, there are a number of ways in which we may wish to use D. <p> in situations such as modelling the arrival of patients into a hospital with a von Mises distribution, we do not want to preclude the possibility that patients arrive at a hospital uniformly round the clock. 10 The Fisher Information matrix for a Gaussian model is derived in Baxter and Oliver <ref> [3, Page 20,21] </ref>. 11 The Fisher Information matrix for a von Mises model is derived in Wallace and Dowe [30, Page 7]. 16 5.4 Lemma 1 | Invariance of the Wallace-Freeman MML Method Lemma (given in Wallace [27, Chapter 5]): Consider a data set, D, with a likelihood f (D j
Reference: [4] <author> J.M. Bernado and A.F.M. Smith. </author> <title> Bayesian Theory. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: There is general agreement on how a posterior density may be used to achieve optimal predictions, and make optimal decisions <ref> [4, 20] </ref>. However, there is not general agreement on how (if at all) a posterior should be used to perform inference. Section 2 discusses some methods for using the posterior to give parameter estimates, and discuss problematic features of these estimation schemes. <p> ^ ) P rob ( j D) d Three types of loss functions are commonly identified: * A 0 1 loss function: l (; ^ ) / 1 P rob (B * ( ^ )) where B * ( ^ ) is a ball of radius * centered at ^ <ref> [4, Page 257] </ref>. * A quadratic loss function: l (; ^ ) / ( ^ ) 2 * A linear loss function: l (; ^ ) / j ^ j These three loss functions lead to the following estimates [14]: * 0 1 loss | Mode. | The value which maximises
Reference: [5] <author> P. Cheeseman, M. Self, J. Kelly, W. Taylor, D. Freeman, and J. Stutz. </author> <title> Bayesian classification. </title> <booktitle> In Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 607-611, </pages> <address> Saint Paul, Minnesota, </address> <year> 1988. </year>
Reference-contexts: One approach is to use Laplace's method to approximate the integral (e.g., Cheeseman et al. <ref> [5] </ref> Kass and Raftery [13] MacKay [16] and Raftery [22]). If the un-normalised posterior has a mode at ^ .
Reference: [6] <author> J.H. Conway and N.J.A Sloane. </author> <title> Sphere Packings, Lattices and Groups. </title> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: Conway and Sloane <ref> [6] </ref> give bounds on the d dimensional optimal quantizing lattice constants, d . Therefore M essLen ( & D) log (U g ()) + L () + d d U d (22) 5.0.6 Finding the Optimal AOPV To find the optimal AOPV, V , we differentiate Equation (22) w.r.t.
Reference: [7] <author> N.G. de Bruijn. </author> <title> Asymptotic Methods for Analysis. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1970. </year> <month> 21 </month>
Reference-contexts: In Appendix 3, we use Laplace's Method to achieve the following approximation to I <ref> [7] </ref>: I P rob (D j ^ ) d q (33) This approximation is equivalent to approximating the likelihood, P rob (D j ) in the vicinity of the peak, ^ , with a Gaussian whose covariance matrix is constructed to be the second derivatives of the log-likelihood at the peak.
Reference: [8] <author> N.I. Fisher. </author> <title> Statistical Analysis of Circular Data. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference: [9] <author> I.J. </author> <title> Good. Explicativity, corroboration, and the relative odds of hypotheses. In Good thinking| The Foundations of Probability and its applications. </title> <publisher> University of Minnesota Press, </publisher> <address> Minneapolis,MN, </address> <year> 1983. </year>
Reference-contexts: Consider the problem of selecting between two model classes M C 1 and M C 2 with parameter vectors 1 and 2 (we can generalize easily to more than two 13 The term Bayes Factor was first used by Good <ref> [9] </ref>, who attributes the method to Turing, in addition to and independently of Jeffreys [10]. 18 model classes).
Reference: [10] <author> H. Jeffreys. </author> <title> Some tests of significance, treated by the theory of probability. </title> <journal> Proc. Camb. Phil. Soc., </journal> <volume> 31 </volume> <pages> 203-222, </pages> <year> 1935. </year>
Reference-contexts: selecting between two model classes M C 1 and M C 2 with parameter vectors 1 and 2 (we can generalize easily to more than two 13 The term Bayes Factor was first used by Good [9], who attributes the method to Turing, in addition to and independently of Jeffreys <ref> [10] </ref>. 18 model classes).
Reference: [11] <author> H. Jeffreys. </author> <title> An invariant form for the prior probability in estimation problems. </title> <journal> Proc. of the Royal Soc. of London A, </journal> <volume> 186 </volume> <pages> 453-454, </pages> <year> 1946. </year>
Reference-contexts: We are therefore selecting the point estimate with an associated region of maximal posterior probability. 15 5.3 Differences between MML and the Adoption of a Jeffreys' Prior It may appear that the Wallace-Freeman MML approach is related to maximising the posterior when we use a Jeffreys' Prior <ref> [11, 12] </ref> of the form: h () / det (F ()) The Wallace-Freeman approach is different from the use of a Jeffreys' Prior in the following ways: * The Wallace-Freeman approach allows the use of a prior which reflects prior knowledge. * A Jeffreys' Prior is an improper prior. * The
Reference: [12] <author> H. Jeffreys. </author> <title> Theory of Probability. </title> <address> Cambridge, </address> <year> 1961. </year>
Reference-contexts: We are therefore selecting the point estimate with an associated region of maximal posterior probability. 15 5.3 Differences between MML and the Adoption of a Jeffreys' Prior It may appear that the Wallace-Freeman MML approach is related to maximising the posterior when we use a Jeffreys' Prior <ref> [11, 12] </ref> of the form: h () / det (F ()) The Wallace-Freeman approach is different from the use of a Jeffreys' Prior in the following ways: * The Wallace-Freeman approach allows the use of a prior which reflects prior knowledge. * A Jeffreys' Prior is an improper prior. * The
Reference: [13] <author> R.E. Kass and A.E. Raftery. </author> <title> Bayes Factors and model uncertainty. </title> <type> Technical Report 571, </type> <institution> Dept. of Statistics, Carnegie-Mellon University, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: For example, the set of neural nets with 1 hidden layer, and 3 input nodes, 6 hidden nodes, and 2 output nodes would constitute a model class. 6.2 Levels of Inference In some Bayesian literature, model class selection is considered a distinct task from parameter estimation <ref> [16, 13, 22] </ref>. Using MacKay's terminology [16], parameter estimation is level one inference and model class selection is level two inference. <p> One approach is to use Laplace's method to approximate the integral (e.g., Cheeseman et al. [5] Kass and Raftery <ref> [13] </ref> MacKay [16] and Raftery [22]). If the un-normalised posterior has a mode at ^ .
Reference: [14] <author> T.J. Loredo. </author> <title> From Laplace to Supernova SN 1987A: Bayesian inference in astrophysics. </title> <editor> In P.F. Fougere, editor, </editor> <booktitle> Maximum Entropy and Bayesian Methods, </booktitle> <pages> pages 81-142. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1990. </year>
Reference-contexts: ^ ) is a ball of radius * centered at ^ [4, Page 257]. * A quadratic loss function: l (; ^ ) / ( ^ ) 2 * A linear loss function: l (; ^ ) / j ^ j These three loss functions lead to the following estimates <ref> [14] </ref>: * 0 1 loss | Mode. | The value which maximises the posterior density: ^ = max (P osterior ()) * Quadratic loss | Mean. ^ = P osterior () d * Linear loss | Median. | The value of which satisfies: Z ^ P osterior () d = Z
Reference: [15] <author> D. J. C. MacKay. </author> <title> Bayesian methods for backpropagation networks. </title> <editor> In E. Domany, J. L. van Hemmen, and K. Schulten, editors, </editor> <booktitle> Models of Neural Networks III, chapter 6. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: with the MML Method Note that if we take the negative logarithm of Equation (34) then we get an expression similar to Equa tion (23): log P rob (D j M C) 2 1 log (det (M ( ^ ))) + L ( ^ ) (35) MacKay therefore concludes that <ref> [15] </ref>: "With care, therefore, one can replicate Bayesian results in MDL terms. Although some of the earliest work on complex model comparison involved the MDL framework [19], MDL has no apparent advantages over the direct probabilistic approach".
Reference: [16] <author> David J.C. MacKay. </author> <title> Bayesian Modeling and Neural Networks. </title> <type> PhD thesis, </type> <institution> Dept. of Computation and Neural Systems, CalTech, </institution> <year> 1992. </year>
Reference-contexts: For example, the set of neural nets with 1 hidden layer, and 3 input nodes, 6 hidden nodes, and 2 output nodes would constitute a model class. 6.2 Levels of Inference In some Bayesian literature, model class selection is considered a distinct task from parameter estimation <ref> [16, 13, 22] </ref>. Using MacKay's terminology [16], parameter estimation is level one inference and model class selection is level two inference. <p> Using MacKay's terminology <ref> [16] </ref>, parameter estimation is level one inference and model class selection is level two inference. <p> One approach is to use Laplace's method to approximate the integral (e.g., Cheeseman et al. [5] Kass and Raftery [13] MacKay <ref> [16] </ref> and Raftery [22]). If the un-normalised posterior has a mode at ^ .
Reference: [17] <author> R.M. Neal. </author> <title> Bayesian Learning for Neural Networks. </title> <type> PhD thesis, </type> <institution> Graduate Dept. of Computer Science, Univ. of Toronto, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: We therefore use natural logarithms throughout this paper. 3 The division of a prior density into cells and AOPVs are discussed in Section 3.1 of Oliver and Hand [18]. 4 2 Bayesian Inference Fundamentalist Bayesians reject attempts to summarise a posterior density by an estimate as being unsound (e.g., Neal <ref> [17, Chapter 1] </ref>). For them, the posterior density is a sufficient and satisfactory end result of inference. In some cases, we may be able to present the full posterior graphically. For multivariate densities, this is often infeasible in practice.
Reference: [18] <author> J.J. Oliver and D.J. </author> <title> Hand. Introduction to minimum encoding inference. </title> <type> Technical report TR 4-94, </type> <institution> Dept. of Statistics, Open University, Walton Hall, Milton Keynes, MK7 6AA, UK, </institution> <year> 1994. </year> <note> Available on the WWW from http://www.cs.monash.edu.au/ ~ jono. </note>
Reference-contexts: 1 Introduction This paper continues the introduction to minimum encoding inference given by Oliver and Hand <ref> [18] </ref>. This series of papers were written with the objective of providing an introduction for statisticians to this area. In this paper, we use the Wallace interpretation [26, 28, 29, 31, 33] of minimum encoding inference which he termed Minimum Message Length (MML). <p> We discuss how to determine optimal AOPVs in Section 4. Throughout this paper, we interpret MML in a Bayesian light. We find that MML offers Bayesianism a sound method for inference. 1 AOMs are discussed in Section 2.7 of Oliver and Hand <ref> [18] </ref>. 2 For convenience, we calculate all message lengths in nits. We therefore use natural logarithms throughout this paper. 3 The division of a prior density into cells and AOPVs are discussed in Section 3.1 of Oliver and Hand [18]. 4 2 Bayesian Inference Fundamentalist Bayesians reject attempts to summarise a <p> 1 AOMs are discussed in Section 2.7 of Oliver and Hand <ref> [18] </ref>. 2 For convenience, we calculate all message lengths in nits. We therefore use natural logarithms throughout this paper. 3 The division of a prior density into cells and AOPVs are discussed in Section 3.1 of Oliver and Hand [18]. 4 2 Bayesian Inference Fundamentalist Bayesians reject attempts to summarise a posterior density by an estimate as being unsound (e.g., Neal [17, Chapter 1]). For them, the posterior density is a sufficient and satisfactory end result of inference. <p> We find that the MML framework for determining estimates establishes a method for determining AOPVs. 4.1 The Original MML Method In <ref> [18] </ref> we used the Original MML Method (offered by Wallace and Boulton [28]) for determining the AOPVs of and for a normal distribution. We did this by: 1. <p> In Section 4.4 we consider this more general problem for a single parameter. Section 5 extends this to a vector of parameters. 7 Equation (11) is Equation (14) from Oliver and Hand <ref> [18] </ref> rewritten in nits. 10 4.4 Estimating a Single Parameter Using MML We estimate a single parameter, from some data D with prior density h (), likelihood f (D j ) and negative log-likelihood, L () = log f (D j ). <p> of the Taylor's expansion (to terms of the second power): M essLen ( & D) log A () log h () + 1 Z A () A () @L () + 2 @ 2 dx 8 An argument for this definition is given in Section 3.3.1. of Oliver and Hand <ref> [18] </ref>. 11 We note that: 1 Z A () A () L () dx = L () 1 Z A () A () x @ Therefore M essLen ( & D) log A () log h () + L () + @ 2 L () 2 A () 2 2 log <p> create a decodeable message by using AOP V = A 0 () where A 0 () is the value given by Equation (16) with the Observed Fisher Information, M (), replaced by the Expected Fisher Information, F (), 9 Code dictionaries are discussed in Section 2 of Oliver and Hand <ref> [18] </ref>. 12 We therefore replace M () = @ 2 ) F () = f (D j ) @ 2 dz Thus, we can construct a code using AOP V : A 0 () = 12 (17) Using this approximation allows us to construct a coding scheme with decodeable messages.
Reference: [19] <author> J.D. Patrick and C.S. Wallace. </author> <title> Stone circle geometries: An information theory approach. In D.C. Heggie, editor, Archeoastronomy in the Old World. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1982. </year>
Reference-contexts: Although some of the earliest work on complex model comparison involved the MDL framework <ref> [19] </ref>, MDL has no apparent advantages over the direct probabilistic approach".
Reference: [20] <author> S.J. </author> <title> Press. Bayesian Statistics. </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: There is general agreement on how a posterior density may be used to achieve optimal predictions, and make optimal decisions <ref> [4, 20] </ref>. However, there is not general agreement on how (if at all) a posterior should be used to perform inference. Section 2 discusses some methods for using the posterior to give parameter estimates, and discuss problematic features of these estimation schemes.
Reference: [21] <author> S.J. Press and K. </author> <title> Shigemasu. Bayesian inference in factor analysis. </title> <editor> In L. Gleser, M. Perlman, S.J. Press, and A. Sampson, editors, </editor> <title> Contributions to Probability and Statistics: </title> <booktitle> Essays in Honor of Ingram Olkin. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: This second criticism may appear artificial. However, there are models where symmetric multimodal posteriors exist. For example, in Factor Analysis [32], a sign ambiguity means that posteriors will always be symmetric about 0. Bayesian Factor Analysis has therefore tended to use the mode as an estimate <ref> [1, 21] </ref>. * Median. | Selecting the median of the posterior has the property that it IS invariant under nonlinear transformations.
Reference: [22] <author> A.E. Raftery. </author> <title> Approximate Bayes Factors and accounting for model uncertainty in generalized linear models. </title> <type> Technical Report 255, </type> <institution> Dept. of Statistics, University of Washington, </institution> <year> 1993. </year>
Reference-contexts: For example, the set of neural nets with 1 hidden layer, and 3 input nodes, 6 hidden nodes, and 2 output nodes would constitute a model class. 6.2 Levels of Inference In some Bayesian literature, model class selection is considered a distinct task from parameter estimation <ref> [16, 13, 22] </ref>. Using MacKay's terminology [16], parameter estimation is level one inference and model class selection is level two inference. <p> One approach is to use Laplace's method to approximate the integral (e.g., Cheeseman et al. [5] Kass and Raftery [13] MacKay [16] and Raftery <ref> [22] </ref>). If the un-normalised posterior has a mode at ^ .
Reference: [23] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11 </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: In this paper, we use the Wallace interpretation [26, 28, 29, 31, 33] of minimum encoding inference which he termed Minimum Message Length (MML). For details of Rissanen's approach (termed Minimum Description Length or MDL) see <ref> [23, 24, 25] </ref>. A comparison of MML and MDL can be found in Baxter and Oliver [3]. 1.1 MML and Bayesian Data Analysis Given a data set, D, there are a number of ways in which we may wish to use D.
Reference: [24] <author> J. Rissanen. </author> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 49 </volume> <pages> 223-239, </pages> <year> 1987. </year>
Reference-contexts: In this paper, we use the Wallace interpretation [26, 28, 29, 31, 33] of minimum encoding inference which he termed Minimum Message Length (MML). For details of Rissanen's approach (termed Minimum Description Length or MDL) see <ref> [23, 24, 25] </ref>. A comparison of MML and MDL can be found in Baxter and Oliver [3]. 1.1 MML and Bayesian Data Analysis Given a data set, D, there are a number of ways in which we may wish to use D.
Reference: [25] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: In this paper, we use the Wallace interpretation [26, 28, 29, 31, 33] of minimum encoding inference which he termed Minimum Message Length (MML). For details of Rissanen's approach (termed Minimum Description Length or MDL) see <ref> [23, 24, 25] </ref>. A comparison of MML and MDL can be found in Baxter and Oliver [3]. 1.1 MML and Bayesian Data Analysis Given a data set, D, there are a number of ways in which we may wish to use D.
Reference: [26] <author> C.S. Wallace. </author> <title> Classification by minimum-message-length inference. </title> <editor> In G. Goos and J. Hartmanis, editors, </editor> <booktitle> Advances in Computing and Information - ICCI '90, </booktitle> <pages> pages 72-81. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction This paper continues the introduction to minimum encoding inference given by Oliver and Hand [18]. This series of papers were written with the objective of providing an introduction for statisticians to this area. In this paper, we use the Wallace interpretation <ref> [26, 28, 29, 31, 33] </ref> of minimum encoding inference which he termed Minimum Message Length (MML). For details of Rissanen's approach (termed Minimum Description Length or MDL) see [23, 24, 25].
Reference: [27] <author> C.S. Wallace. </author> <note> MML book (in preparation). </note> <year> 1993. </year>
Reference-contexts: Fisher Information matrix for a Gaussian model is derived in Baxter and Oliver [3, Page 20,21]. 11 The Fisher Information matrix for a von Mises model is derived in Wallace and Dowe [30, Page 7]. 16 5.4 Lemma 1 | Invariance of the Wallace-Freeman MML Method Lemma (given in Wallace <ref> [27, Chapter 5] </ref>): Consider a data set, D, with a likelihood f (D j ) in some convenient parameterisation (of dimension d) with a prior h () defined in that parameterisation.
Reference: [28] <author> C.S. Wallace and D.M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: 1 Introduction This paper continues the introduction to minimum encoding inference given by Oliver and Hand [18]. This series of papers were written with the objective of providing an introduction for statisticians to this area. In this paper, we use the Wallace interpretation <ref> [26, 28, 29, 31, 33] </ref> of minimum encoding inference which he termed Minimum Message Length (MML). For details of Rissanen's approach (termed Minimum Description Length or MDL) see [23, 24, 25]. <p> We find that the MML framework for determining estimates establishes a method for determining AOPVs. 4.1 The Original MML Method In [18] we used the Original MML Method (offered by Wallace and Boulton <ref> [28] </ref>) for determining the AOPVs of and for a normal distribution. We did this by: 1. <p> The derivation for the "optimal" values for two variables using the successive differentiation approach <ref> [28] </ref> was an involved calculation. 4.2 A Bayesian Interpretation of the Original MML Method The Original MML Method can be interpreted as a Bayesian method for point estimation.
Reference: [29] <author> C.S. Wallace and D.M. Boulton. </author> <title> An invariant Bayes method for point estimation. </title> <journal> Classification Society Bulletin, </journal> <volume> 3(3) </volume> <pages> 11-34, </pages> <year> 1975. </year> <month> 22 </month>
Reference-contexts: 1 Introduction This paper continues the introduction to minimum encoding inference given by Oliver and Hand [18]. This series of papers were written with the objective of providing an introduction for statisticians to this area. In this paper, we use the Wallace interpretation <ref> [26, 28, 29, 31, 33] </ref> of minimum encoding inference which he termed Minimum Message Length (MML). For details of Rissanen's approach (termed Minimum Description Length or MDL) see [23, 24, 25]. <p> Taking this expectation is an essential difference between the MML method and some Bayesian methods (discussed in Section 6). This issue is discussed in detail in Section 3.3 of Wallace and Boulton <ref> [29] </ref>. 4.4.4 Approximating the Optimal Message Length Substituting the optimal AOP V from Equation (17) into Equation (15) gives us: M essLen ( & D) log s F () 12 24 Simplifying, M essLen ( & D) 1 log 12 1 log F () log h () + L () +
Reference: [30] <author> C.S. Wallace and D.L. Dowe. </author> <title> MML estimation of the von Mises concentration parameter. </title> <type> Technical report TR 193, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <year> 1993. </year>
Reference-contexts: We wish to adopt a specific prior, and investigate the effect of reparameterisations on the posterior. Wallace and Dowe <ref> [30, Page 7] </ref> found h 3 = (1 + 2 ) was an appropriate prior for a range of problems. <p> This is an expanded derivation of the Wallace-Freeman MML Method presented to the Royal Statistical Society [31] (also given in Wallace and Dowe <ref> [30] </ref>). We consider estimating a vector of parameters, from some data D with prior density h (), likelihood f (D j ) and negative log-likelihood, L () = log f (D j ). <p> not want to preclude the possibility that patients arrive at a hospital uniformly round the clock. 10 The Fisher Information matrix for a Gaussian model is derived in Baxter and Oliver [3, Page 20,21]. 11 The Fisher Information matrix for a von Mises model is derived in Wallace and Dowe <ref> [30, Page 7] </ref>. 16 5.4 Lemma 1 | Invariance of the Wallace-Freeman MML Method Lemma (given in Wallace [27, Chapter 5]): Consider a data set, D, with a likelihood f (D j ) in some convenient parameterisation (of dimension d) with a prior h () defined in that parameterisation. <p> Let us assume a prior distribution over values of : h () = 2 (1 + 2 ) 2 12 The requirement that the determinant of a Fisher Information matrix be non-zero is used here, but is not strictly necessary (for example Wallace and Freeman [32] or Wallace and Dowe <ref> [30, Page 8] </ref>). 17 Then the equivalent prior in the (mean, variance) frame is h (v) = h () J where J = @v @ = 2, and hence: h (v) = 4 (1 + v) 2 We find the ratio of prior to square root of the Fisher Information remains <p> If we apply the Wallace-Freeman MML method to this problem (as was done in Wallace and Dowe <ref> [30] </ref>), we find that the mode moving is not an obstacle.
Reference: [31] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 49 </volume> <pages> 240-252, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction This paper continues the introduction to minimum encoding inference given by Oliver and Hand [18]. This series of papers were written with the objective of providing an introduction for statisticians to this area. In this paper, we use the Wallace interpretation <ref> [26, 28, 29, 31, 33] </ref> of minimum encoding inference which he termed Minimum Message Length (MML). For details of Rissanen's approach (termed Minimum Description Length or MDL) see [23, 24, 25]. <p> We take as the point estimate for the parameter, = (; ), the midpoint of the rectangle with the maximum posterior probability. 4.3 Generalising the Original MML Method We can generalise and improve the Original MML Method (Section 4.1) by using the approach taken by Wallace and Freeman <ref> [31] </ref>. Rather than successively optimising a set of parameters in turn with a particular distribution in mind, we consider a more general problem. We assume the distribution f (:) to be a regular distribution, and attempt to optimise the parameters together. <p> An alternative suggested by Wallace and Freeman <ref> [31] </ref> is to use the "average" optimal AOPV. Given , we find each data set D has a different optimal AOPV. Thus, each has a set of optimal AOPV values. <p> This is an expanded derivation of the Wallace-Freeman MML Method presented to the Royal Statistical Society <ref> [31] </ref> (also given in Wallace and Dowe [30]). We consider estimating a vector of parameters, from some data D with prior density h (), likelihood f (D j ) and negative log-likelihood, L () = log f (D j ).
Reference: [32] <author> C.S. Wallace and P.R. Freeman. </author> <title> Single factor analysis by MML estimation. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 54 </volume> <pages> 195-209, </pages> <year> 1992. </year>
Reference-contexts: Secondly, we may find that the mean of the posterior is in a region with little probability associated with it, as shown in Figure 4 4 . This second criticism may appear artificial. However, there are models where symmetric multimodal posteriors exist. For example, in Factor Analysis <ref> [32] </ref>, a sign ambiguity means that posteriors will always be symmetric about 0. Bayesian Factor Analysis has therefore tended to use the mode as an estimate [1, 21]. * Median. | Selecting the median of the posterior has the property that it IS invariant under nonlinear transformations. <p> Let us assume a prior distribution over values of : h () = 2 (1 + 2 ) 2 12 The requirement that the determinant of a Fisher Information matrix be non-zero is used here, but is not strictly necessary (for example Wallace and Freeman <ref> [32] </ref> or Wallace and Dowe [30, Page 8]). 17 Then the equivalent prior in the (mean, variance) frame is h (v) = h () J where J = @v @ = 2, and hence: h (v) = 4 (1 + v) 2 We find the ratio of prior to square root

References-found: 32

