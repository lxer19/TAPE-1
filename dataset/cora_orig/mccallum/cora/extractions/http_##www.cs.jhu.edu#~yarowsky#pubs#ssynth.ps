URL: http://www.cs.jhu.edu/~yarowsky/pubs/ssynth.ps
Refering-URL: http://www.cs.jhu.edu/~yarowsky/pubs.html
Root-URL: http://www.cs.jhu.edu
Title: Homograph Disambiguation in Text-to-speech Synthesis  
Author: David Yarowsky 
Abstract: This chapter presents a statistical decision procedure for lexical ambiguity resolution in text-to-speech synthesis. Based on decision lists, the algorithm incorporates both local syntactic patterns and more distant collocational evidence, combining the strengths of decision trees, N-gram taggers and Bayesian classifiers. The algorithm is applied to 7 major types of ambiguity where context can be used to choose a word's pronunciation.
Abstract-found: 1
Intro-found: 1
Reference: [BFOS84] <author> L. Brieman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Monterrey CA, </address> <year> 1984. </year>
Reference: [Bri93] <author> E. Brill. </author> <title> A Corpus-Based Approach to Language Learning. </title> <type> Ph.D. Thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1993. </year>
Reference-contexts: Finding a more accurate probability estimate depends on several factors, including the size 4 The richness of this feature set is one of the key reasons for the success of this algorithm. Others who have very productively exploited a diverse feature set include [Hea91], <ref> [Bri93] </ref> and [DI94]. 5 Position markers include +1 (token to the right), 1 (token to the left), k (co-occurrence in k-token window) and -V (head verb).
Reference: [BDDM91] <author> P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. </author> <title> Word sense disambiguation using statistical methods. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 264-270, </pages> <address> Berkeley, </address> <year> 1991. </year>
Reference: [BW94] <author> R. Bruce and and J. Wiebe. </author> <title> Word-sense disambiguation using decomposable models. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 139-146, </pages> <address> Las Cruces, NM, </address> <year> 1994. </year>
Reference-contexts: In addition, these models have been greatly simplified by assuming that occurrence probabilities of content words are independent of each other, a false and potentially problematic assumption that tends to yield inflated probability values. One can attempt to model these dependencies (as in <ref> [BW94] </ref>), but data sparsity problems and computational constraints can make this difficult and costly to do. 1 Leacock et al. have pursued a similar bag-of-words strategy, using an IR-style vector space model and neural network [LTV93].
Reference: [Chu88] <author> K. W. Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143, </pages> <year> 1988. </year> <note> 174 David Yarowsky </note>
Reference: [DI94] <author> I. Dagan and A. Itai. </author> <title> Word sense disambiguation using a second language monolingual corpus. </title> <journal> Computational Linguistics, </journal> <volume> 20 </volume> <pages> 563-596, </pages> <year> 1994. </year>
Reference-contexts: Finding a more accurate probability estimate depends on several factors, including the size 4 The richness of this feature set is one of the key reasons for the success of this algorithm. Others who have very productively exploited a diverse feature set include [Hea91], [Bri93] and <ref> [DI94] </ref>. 5 Position markers include +1 (token to the right), 1 (token to the left), k (co-occurrence in k-token window) and -V (head verb).
Reference: [GCY92] <author> W. Gale, K. Church, and D. Yarowsky. </author> <title> A method for disambiguating word senses in a large corpus. </title> <journal> Computers and the Humanities, </journal> <volume> 26 </volume> <pages> 415-439, </pages> <year> 1992. </year>
Reference-contexts: Bayesian classifiers [MW64] have been used for a number of sense disambiguation tasks [GCY94], typically involving semantic ambiguity. In an effort to generalize from longer distance word associations regardless of position, an implementation proposed in <ref> [GCY92] </ref> characterizes each token of a homograph by the 100 words nearest to it, treated as an unordered bag. 1 Although such models can successfully capture topic-level differences, they lose the ability to make distinctions based on local sequence or sentence structure. <p> The 20 words employed here is a practical width for many applications. The issues involved in choosing appropriate context widths are discussed in <ref> [GCY92] </ref>. 3 Such skewed distributions are in fact quite typical. A study in [Yar93] showed that P (pronunciationjcollocation) is a very low entropy distribution. Certain types of content-word collocations seen only once in training data predicted the correct pronunciation in held-out test data with 92% accuracy. <p> Several smoothing methods have been explored in this work, including those discussed in <ref> [GCY92] </ref>. The preferred technique is to take all instances of the same raw frequency distribution (such as 2/0 or 10/1), and collectively compute a smoothed ratio that better reflects the true probability distribution.
Reference: [GCY94] <author> W. Gale, K. Church, and D. Yarowsky. </author> <title> Discrimination decisions for 100,000-dimensional spaces. </title> <editor> In A. Zampoli, N. Calzolari and M. Palmer (eds.), </editor> <booktitle> Current Issues in Computational Linguistics: In Honour of Don Walker, </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <pages> pages 429-450, </pages> <year> 1994. </year>
Reference-contexts: A more fundamental limitation, however, is the inherent myopia of these models. They cannot generally capture longer distance word associations, such as between wound and hospital, and hence are not appropriate for resolving many semantic ambiguities. Bayesian classifiers [MW64] have been used for a number of sense disambiguation tasks <ref> [GCY94] </ref>, typically involving semantic ambiguity.
Reference: [Hea91] <author> M. Hearst. </author> <title> Noun homograph disambiguation using local context in large text corpora. In Using Corpora, </title> <institution> University of Waterloo, Waterloo, </institution> <address> Ontario, </address> <year> 1991. </year>
Reference-contexts: Finding a more accurate probability estimate depends on several factors, including the size 4 The richness of this feature set is one of the key reasons for the success of this algorithm. Others who have very productively exploited a diverse feature set include <ref> [Hea91] </ref>, [Bri93] and [DI94]. 5 Position markers include +1 (token to the right), 1 (token to the left), k (co-occurrence in k-token window) and -V (head verb).
Reference: [Jel85] <author> F. Jelinek. </author> <title> Markov source modeling of text generation. In Impact of Processing Techniques on Communication, </title> <editor> J. Skwirzin-ski, ed., </editor> <address> Dordrecht, </address> <year> 1985. </year>
Reference: [LTV93] <author> C. Leacock, G. Towell and E. Voorhees. </author> <title> Corpus-based statistical sense resolution. </title> <booktitle> In Proceedings, ARPA Human Language Technology Workshop, </booktitle> <pages> pages 260-265, </pages> <address> Princeton, </address> <year> 1993. </year>
Reference-contexts: One can attempt to model these dependencies (as in [BW94]), but data sparsity problems and computational constraints can make this difficult and costly to do. 1 Leacock et al. have pursued a similar bag-of-words strategy, using an IR-style vector space model and neural network <ref> [LTV93] </ref>. Homograph Disambiguation in Text-to-speech Synthesis 161 Decision Trees [BFOS84][BDDM91] can be effective at handling complex conditional dependencies and non-independence, but often encounter severe difficulties with very large parameter spaces, such as the highly lex-icalized feature sets frequently used in homograph resolution. <p> While there are certainly other ways to combine such evidence, this approach has many advantages. In particular, precision seems to be at least as good as that achieved with Bayesian methods applied to the same evidence. This is not surprising, given the observation in <ref> [LTV93] </ref> that widely divergent sense-disambiguation algorithms tend to perform roughly the same given the same evidence.
Reference: [Mer90] <author> B. Merialdo. </author> <title> Tagging text with a probabilistic model In Proceedings of the IBM Natural Language ITL, </title> <address> Paris, France, </address> <pages> pages 161-172, </pages> <year> 1990. </year>
Reference: [MW64] <editor> F. Mosteller and D. Wallace Inference and Disputed Authorship: </editor> <publisher> The Federalist Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1964. </year>
Reference-contexts: A more fundamental limitation, however, is the inherent myopia of these models. They cannot generally capture longer distance word associations, such as between wound and hospital, and hence are not appropriate for resolving many semantic ambiguities. Bayesian classifiers <ref> [MW64] </ref> have been used for a number of sense disambiguation tasks [GCY94], typically involving semantic ambiguity.
Reference: [Riv87] <author> R. L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: Current Algorithm: The algorithm described in this paper is a hybrid approach, combining the strengths of each of these 3 general paradigms. It was proposed in [SHY92] and refined in [Yar94]. It is based on the formal model of decision lists in <ref> [Riv87] </ref>, although feature conjuncts have been restricted to a much narrower complexity, namely word and class tri-grams.
Reference: [SHY92] <author> R. Sproat, J. Hirschberg and D. Yarowsky. </author> <title> A corpus-based synthesizer. </title> <booktitle> In Proceedings, International Conference on Spoken Language Processing, </booktitle> <address> Banff, </address> <year> 1992. </year>
Reference-contexts: The AT&T TTS synthesizer <ref> [SHY92] </ref> uses Church's PARTS tagger for this purpose. A weakness of these taggers is that they are typically not sensitive to specific word associations. The standard algorithm relies on models of part-of-speech sequence, captured by probabilities of part-of-speech bigrams or trigrams, to the exclusion of lexical collocations. <p> Current Algorithm: The algorithm described in this paper is a hybrid approach, combining the strengths of each of these 3 general paradigms. It was proposed in <ref> [SHY92] </ref> and refined in [Yar94]. It is based on the formal model of decision lists in [Riv87], although feature conjuncts have been restricted to a much narrower complexity, namely word and class tri-grams.
Reference: [Yar92] <author> D. Yarowsky. </author> <title> Word-Sense disambiguation using statistical models of Roget's categories trained on large corpora. </title> <booktitle> In Proceedings, COLING-92, </booktitle> <pages> pages 454-460, </pages> <address> Nantes, </address> <year> 1992. </year>
Reference-contexts: Then label each example of the target homograph with its correct pronunciation in that context. Here this process was partially auto 162 David Yarowsky mated, using tools that included a sense disambiguation system based on semantic classes <ref> [Yar92] </ref>. For this study, the training and test data were extracted from a 400-million-word corpus collection containing news articles (AP newswire and Wall Street Journal), scientific abstracts (NSF and DOE), 85 novels, two encyclopedias and 3 years of Canadian parliamentary debates, augmented with e-mail correspondence and miscellaneous Internet dialogues.
Reference: [Yar93] <author> D. Yarowsky. </author> <title> One sense per collocation. </title> <booktitle> In Proceedings, ARPA Human Language Technology Workshop, </booktitle> <pages> pages 266-271, </pages> <address> Prince-ton, NJ, </address> <year> 1993. </year> <title> Homograph Disambiguation in Text-to-speech Synthesis 175 </title>
Reference-contexts: The 20 words employed here is a practical width for many applications. The issues involved in choosing appropriate context widths are discussed in [GCY92]. 3 Such skewed distributions are in fact quite typical. A study in <ref> [Yar93] </ref> showed that P (pronunciationjcollocation) is a very low entropy distribution. Certain types of content-word collocations seen only once in training data predicted the correct pronunciation in held-out test data with 92% accuracy. <p> Which approach is best? Using only the global proabilities does surprisingly well, and the results cited here are based on this readily replicable procedure. The reason is grounded in the strong tendency of a word to exhibit only one sense or pronunciation per collocation (discussed in Step 3 and <ref> [Yar93] </ref>). Most classifications are based on an x vs. 0 distribution, and while the magnitude of the log-likelihood ratios may decrease in the residual, they rarely change sign. <p> Of course this behavior does not hold for all classification tasks, but does seem to be characteristic of lexically-based semantic classifications. This may be explained by the previously noted observation that in most cases, and with high probability, words exhibit only one sense in a given collocation <ref> [Yar93] </ref>. Thus for this type of ambiguity resolution, there is no apparent detriment, and some apparent performance gain, from using only the single most reliable evidence in a classification. There are other advantages as well, including run-time efficiency and ease of parallelization.
Reference: [Yar94] <author> D. Yarowsky. </author> <title> Decision lists for lexical ambiguity resolution: application to accent restoration in Spanish and French. </title> <booktitle> Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 88-95, </pages> <address> Las Cruces, NM, </address> <year> 1994. </year>
Reference-contexts: Current Algorithm: The algorithm described in this paper is a hybrid approach, combining the strengths of each of these 3 general paradigms. It was proposed in [SHY92] and refined in <ref> [Yar94] </ref>. It is based on the formal model of decision lists in [Riv87], although feature conjuncts have been restricted to a much narrower complexity, namely word and class tri-grams. <p> There are several motivations for this approach. The first is that combining all available evidence rarely produces a different classification than just using the single most reliable piece of evidence, and when these differ it is as likely to hurt as to help. A study in <ref> [Yar94] </ref> based on 20 homographs showed that the two methods agreed in 98% of the test cases. Indeed, in the 2% cases of disagreement, using only the single best piece of evidence worked slightly better than combining evidence.
Reference: [Yar94b] <author> D. Yarowsky. </author> <title> A comparison of corpus-based techniques for restoring accents in Spanish and French text. </title> <booktitle> In Proceedings, 2nd Annual Workshop on Very Large Corpora, Kyoto, </booktitle> <pages> pages 19-32, </pages> <year> 1994. </year>
Reference-contexts: The algorithm is also extremely flexible|it is quite straightforward to use any new feature for which a probability distribution can be calculated. This is a considerable strength relative to other algorithms which are more constrained in their ability to handle diverse types of evidence. In a comparative study <ref> [Yar94b] </ref>, the decision list algorithm outperformed both an N-Gram tagger and Bayesian classifier primarily because it could effectively integrate a wider range of available evidence types.
References-found: 19

