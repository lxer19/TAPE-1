URL: ftp://flint.cs.yale.edu/pub/flint/publications/listrep.ps.gz
Refering-URL: http://daffy.cs.yale.edu/users/shao-zhong/papers.html
Root-URL: http://www.cs.yale.edu
Title: Unrolling Lists  
Author: Zhong Shao John H. Reppy Andrew W. Appel 
Affiliation: Princeton University AT&T Bell Laboratories Princeton University  
Abstract: Lists are ubiquitous in functional programs, thus supporting lists efficiently is a major concern to compiler writers for functional languages. Lists are normally represented as linked cons cells, with each cons cell containing a car (the data) and a cdr (the link); this is inefficient in the use of space, because 50% of the storage is used for links. Loops and recursions on lists are slow on modern machines because of the long chains of control dependences (in checking for nil) and data dependences (in fetching cdr fields). We present a data structure for "unrolled lists," where each cell has several data items (car fields) and one link (cdr). This reduces the memory used for links, and it significantly shortens the length of control-dependence and data-dependence chains in operations on lists. We further present an efficient compile-time analysis that transforms programs written for "ordinary" lists into programs on unrolled lists. The use of our new representation requires no change to existing programs. We sketch the proof of soundness of our analysis|which is based on refinement types|and present some preliminary measurements of our technique. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> William E. Aitken and John H. Reppy. </author> <title> Abstract value constructors. </title> <booktitle> In ACM SIGPLAN Workshop on ML and its Applications, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: First we describe a simple syntactic transformation that gets us partway to our goal. A simple way to implement the NUR is to make the compiler interpret the normal "::" constructor abstractly, just as Aitken and Reppy deal with their abstract value constructors <ref> [1] </ref>.
Reference: [2] <author> Andrew W. Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cam-bridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: that our algorithm cannot be applied to polymorphic languages; polymorphic expressions can be easily translated 3 Unlike dynamically typed languages such as Lisp and Scheme, there is no "setcdr" operator in ML. 4 In practice, TAIL1 is a transparent data constructor, thus does not require any extra storage to represent <ref> [7, 2] </ref>. e ::= c t j x t j fn m j e 1 e 2 j NIL j CONS (e 1 ,e 2 ) d ::= d 1 and d 2 j (x t = e) p ::= x t j NILP j CONSP (x,p 1 ) e ::= <p> They are mostly taken from the initial basis of the Standard ML of New Jersey compiler [3]. It turns out that this problem can be easily solved in the continuation-passing style (CPS) framework <ref> [23, 2] </ref>, because we can specialize the return continuation on the length parity of the result, and make it have multiple entry points also. <p> section) is that the NUR version of filter is about as fast as the OSR version, even without the specialized CPS version of our analysis. 4 Experiments We have implemented the algorithm described in Section 2 in an experimental version of the Standard ML of New Jer sey compiler (SML/NJ) <ref> [3, 2] </ref>. Because the compiler uses continuation-passing style as its intermediate language, the multiple-continuation approach described in Section 3 can be easily added (this has not been done yet).
Reference: [3] <author> Andrew W. Appel and David B. MacQueen. </author> <title> Standard ML of New Jersey. </title> <editor> In Martin Wirsing, editor, </editor> <booktitle> Third Int'l Symp. on Prog. Lang. Implementation and Logic Programming, </booktitle> <pages> pages 1-13, </pages> <address> New York, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: They are mostly taken from the initial basis of the Standard ML of New Jersey compiler <ref> [3] </ref>. It turns out that this problem can be easily solved in the continuation-passing style (CPS) framework [23, 2], because we can specialize the return continuation on the length parity of the result, and make it have multiple entry points also. <p> section) is that the NUR version of filter is about as fast as the OSR version, even without the specialized CPS version of our analysis. 4 Experiments We have implemented the algorithm described in Section 2 in an experimental version of the Standard ML of New Jer sey compiler (SML/NJ) <ref> [3, 2] </ref>. Because the compiler uses continuation-passing style as its intermediate language, the multiple-continuation approach described in Section 3 can be easily added (this has not been done yet).
Reference: [4] <author> Henry G. Baker. </author> <title> List processing in real time on a serial computer. </title> <journal> Communications of the ACM, </journal> <volume> 21(4) </volume> <pages> 280-294, </pages> <month> April </month> <year> 1978. </year>
Reference-contexts: A depth-first (or breadth-first <ref> [4] </ref>) copying garbage collector helps ensure that most lists are arranged sequentially in storage, so they can take advantage of this encoding.
Reference: [5] <author> Daniel G. Bobrow. </author> <title> A note on hash linking. </title> <journal> Communications of the ACM, </journal> <volume> 18(7) </volume> <pages> 413-415, </pages> <month> July </month> <year> 1975. </year>
Reference-contexts: Email address: jhr@research.att.com. dependent on the previous one. With modern superscalar hardware, these dependences are a serious bottleneck. In order to save on storage for links, "cdr-coding" was proposed in the 1970's <ref> [15, 13, 8, 9, 6, 5] </ref>. <p> NUR is that it does not have a good dead code detection algorithm, we believe that a more refined implementation can achieve more code sharing and produce much smaller code. 5 Related Work Cdr-coding techniques were first proposed in early 70's by a group of researchers at MIT and Xerox <ref> [15, 13, 8, 9, 6, 5] </ref>. While these schemes differ from each other on the encoding methods, they all rely on the hardware support from microcoded Lisp machines [25, 10] to alleviate the high costs incurred by the runtime encoding bits.
Reference: [6] <author> Daniel G. Bobrow and Douglas W. Clark. </author> <title> Compact encodings of list structure. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(2) </volume> <pages> 267-286, </pages> <month> October </month> <year> 1979. </year>
Reference-contexts: Email address: jhr@research.att.com. dependent on the previous one. With modern superscalar hardware, these dependences are a serious bottleneck. In order to save on storage for links, "cdr-coding" was proposed in the 1970's <ref> [15, 13, 8, 9, 6, 5] </ref>. <p> NUR is that it does not have a good dead code detection algorithm, we believe that a more refined implementation can achieve more code sharing and produce much smaller code. 5 Related Work Cdr-coding techniques were first proposed in early 70's by a group of researchers at MIT and Xerox <ref> [15, 13, 8, 9, 6, 5] </ref>. While these schemes differ from each other on the encoding methods, they all rely on the hardware support from microcoded Lisp machines [25, 10] to alleviate the high costs incurred by the runtime encoding bits.
Reference: [7] <author> Luca Cardelli. </author> <title> Compiling a functional language. </title> <booktitle> In Proc. of the 1984 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 208-217, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: that our algorithm cannot be applied to polymorphic languages; polymorphic expressions can be easily translated 3 Unlike dynamically typed languages such as Lisp and Scheme, there is no "setcdr" operator in ML. 4 In practice, TAIL1 is a transparent data constructor, thus does not require any extra storage to represent <ref> [7, 2] </ref>. e ::= c t j x t j fn m j e 1 e 2 j NIL j CONS (e 1 ,e 2 ) d ::= d 1 and d 2 j (x t = e) p ::= x t j NILP j CONSP (x,p 1 ) e ::=
Reference: [8] <author> Douglas W. Clark. </author> <title> List structure: measurements, algorithms, and encodings. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon Univ., </institution> <address> Pittsburgh, PA, </address> <month> August </month> <year> 1976. </year>
Reference-contexts: Email address: jhr@research.att.com. dependent on the previous one. With modern superscalar hardware, these dependences are a serious bottleneck. In order to save on storage for links, "cdr-coding" was proposed in the 1970's <ref> [15, 13, 8, 9, 6, 5] </ref>. <p> NUR is that it does not have a good dead code detection algorithm, we believe that a more refined implementation can achieve more code sharing and produce much smaller code. 5 Related Work Cdr-coding techniques were first proposed in early 70's by a group of researchers at MIT and Xerox <ref> [15, 13, 8, 9, 6, 5] </ref>. While these schemes differ from each other on the encoding methods, they all rely on the hardware support from microcoded Lisp machines [25, 10] to alleviate the high costs incurred by the runtime encoding bits.
Reference: [9] <author> Douglas W. Clark and C. Cordell Green. </author> <title> An empirical study of list structure in lisp. </title> <journal> Communications of the ACM, </journal> <volume> 20(2) </volume> <pages> 78-87, </pages> <month> February </month> <year> 1977. </year>
Reference-contexts: Email address: jhr@research.att.com. dependent on the previous one. With modern superscalar hardware, these dependences are a serious bottleneck. In order to save on storage for links, "cdr-coding" was proposed in the 1970's <ref> [15, 13, 8, 9, 6, 5] </ref>. <p> NUR is that it does not have a good dead code detection algorithm, we believe that a more refined implementation can achieve more code sharing and produce much smaller code. 5 Related Work Cdr-coding techniques were first proposed in early 70's by a group of researchers at MIT and Xerox <ref> [15, 13, 8, 9, 6, 5] </ref>. While these schemes differ from each other on the encoding methods, they all rely on the hardware support from microcoded Lisp machines [25, 10] to alleviate the high costs incurred by the runtime encoding bits.
Reference: [10] <author> L. P. Deutsch. </author> <title> A lisp machine with very compact programs. </title> <booktitle> In Proc. 3rd IJACI, </booktitle> <pages> pages 697-703, </pages> <year> 1973. </year>
Reference-contexts: Cdr-coding solves the space-usage problem (and in the MIT version allows random access subscripting of lists [13]), but makes the control-dependence problem even worse, as the cdr-coding tag of each car must be checked. Cdr-coding was popular on microcoded Lisp machines circa 1980 <ref> [25, 10] </ref>, but it is not an attractive solution on modern machines. Our new "compile-time cdr-coding" method works for statically typed languages such as ML. Our scheme allows a more compact runtime representation for lists, but does not require any runtime encoding at all. <p> While these schemes differ from each other on the encoding methods, they all rely on the hardware support from microcoded Lisp machines <ref> [25, 10] </ref> to alleviate the high costs incurred by the runtime encoding bits. Since modern machines tend not to offer these kinds of special hardware support, the runtime cdr-coding technique quickly became obsolete in the 1980's.
Reference: [11] <institution> Tim Freeman. Carnegie Mellon University, </institution> <type> personal communication, </type> <year> 1992. </year>
Reference-contexts: We can keep track of length parity information for most program variables at compile time, in statically-typed languages such as ML, because lists are accessed via data constructors and pattern matching only, and they are immune to side-effects. 3 We borrow the refinement type inference algorithm of Freeman and Pfenning <ref> [12, 11] </ref> by introducing a refinement of the list type: the type olist for odd-length lists and the type elist for even-length lists. <p> Most of the notations used in this paper is just a simplified version of that used by Freeman and Pfenning <ref> [12, 11] </ref>: &lt; t means that the refinement type refines the SRC type t ; 1 _ 2 denotes a refinement type that is the union of 1 and 2 . We formally define our refinement type system in the appendix. <p> The loop inside CompDec computes the fixed point of the refinement types; this is guaranteed to terminate because there are only finitely many refinement types below any given SRC type (a proof of this is given by Freeman <ref> [11, 12] </ref>). Translation of function application (i.e., e 1 e 2 ) is a simple recursive call of ExpComp on e 1 and e 2 . <p> Coercions among representations for even-length lists, odd-length lists, and lists whose length parity is unknown, are quite cheap. The refinement type system used in Section 2 is a much simplified version of Freeman and Pfenning's refinement type system <ref> [12, 11] </ref>. While the underlying framework and type inference algorithm are quite similar, our motivation is rather different. In their system, the refinement type is declared by the programmer, and the refinement type information is used to detect program errors at compile time.
Reference: [12] <author> Tim Freeman and Frank Pfenning. </author> <title> Refinement types for ML. </title> <booktitle> In Proc. ACM SIGPLAN '91 Conf. on Prog. Lang. Design and Implementation, </booktitle> <pages> pages 268-277, </pages> <address> New York, July 1991. </address> <publisher> ACM Press. </publisher>
Reference-contexts: We can keep track of length parity information for most program variables at compile time, in statically-typed languages such as ML, because lists are accessed via data constructors and pattern matching only, and they are immune to side-effects. 3 We borrow the refinement type inference algorithm of Freeman and Pfenning <ref> [12, 11] </ref> by introducing a refinement of the list type: the type olist for odd-length lists and the type elist for even-length lists. <p> Most of the notations used in this paper is just a simplified version of that used by Freeman and Pfenning <ref> [12, 11] </ref>: &lt; t means that the refinement type refines the SRC type t ; 1 _ 2 denotes a refinement type that is the union of 1 and 2 . We formally define our refinement type system in the appendix. <p> The loop inside CompDec computes the fixed point of the refinement types; this is guaranteed to terminate because there are only finitely many refinement types below any given SRC type (a proof of this is given by Freeman <ref> [11, 12] </ref>). Translation of function application (i.e., e 1 e 2 ) is a simple recursive call of ExpComp on e 1 and e 2 . <p> Coercions among representations for even-length lists, odd-length lists, and lists whose length parity is unknown, are quite cheap. The refinement type system used in Section 2 is a much simplified version of Freeman and Pfenning's refinement type system <ref> [12, 11] </ref>. While the underlying framework and type inference algorithm are quite similar, our motivation is rather different. In their system, the refinement type is declared by the programmer, and the refinement type information is used to detect program errors at compile time.
Reference: [13] <author> R. Greenblatt. </author> <title> Lisp machine progress report memo 444. </title> <type> Technical report, </type> <institution> A.I. Lab., M.I.T., </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1977. </year>
Reference-contexts: Email address: jhr@research.att.com. dependent on the previous one. With modern superscalar hardware, these dependences are a serious bottleneck. In order to save on storage for links, "cdr-coding" was proposed in the 1970's <ref> [15, 13, 8, 9, 6, 5] </ref>. <p> A depth-first (or breadth-first [4]) copying garbage collector helps ensure that most lists are arranged sequentially in storage, so they can take advantage of this encoding. Cdr-coding solves the space-usage problem (and in the MIT version allows random access subscripting of lists <ref> [13] </ref>), but makes the control-dependence problem even worse, as the cdr-coding tag of each car must be checked. Cdr-coding was popular on microcoded Lisp machines circa 1980 [25, 10], but it is not an attractive solution on modern machines. <p> NUR is that it does not have a good dead code detection algorithm, we believe that a more refined implementation can achieve more code sharing and produce much smaller code. 5 Related Work Cdr-coding techniques were first proposed in early 70's by a group of researchers at MIT and Xerox <ref> [15, 13, 8, 9, 6, 5] </ref>. While these schemes differ from each other on the encoding methods, they all rely on the hardware support from microcoded Lisp machines [25, 10] to alleviate the high costs incurred by the runtime encoding bits.
Reference: [14] <author> Cordelia V. Hall. </author> <title> Using hindley-milner type inference to optimize list representation. </title> <booktitle> In 1994 ACM Conference on Lisp and Fucntional Programming, page (to appear), </booktitle> <address> New York, June 1994. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Our static cdr-coding method, on the other hand, exploits compile-time analysis to eliminate most runtime checks; at the same time, it poses no more problem in parallel environments than does "ordinary" consing. On the side of statically typed languages, Hall <ref> [14] </ref> has presented a list compaction technique for Haskell [16]. In her scheme, lists can be represented as the old standard representation (OSR) at one place, and in an optimized representation at another place.
Reference: [15] <author> Wilfred J. Hansen. </author> <title> Compact list representation: Definition, garbage collection, and system implementation. </title> <journal> Communications of the ACM, </journal> <volume> 12(9) </volume> <pages> 499-507, </pages> <month> Sep </month> <year> 1969. </year>
Reference-contexts: Email address: jhr@research.att.com. dependent on the previous one. With modern superscalar hardware, these dependences are a serious bottleneck. In order to save on storage for links, "cdr-coding" was proposed in the 1970's <ref> [15, 13, 8, 9, 6, 5] </ref>. <p> NUR is that it does not have a good dead code detection algorithm, we believe that a more refined implementation can achieve more code sharing and produce much smaller code. 5 Related Work Cdr-coding techniques were first proposed in early 70's by a group of researchers at MIT and Xerox <ref> [15, 13, 8, 9, 6, 5] </ref>. While these schemes differ from each other on the encoding methods, they all rely on the hardware support from microcoded Lisp machines [25, 10] to alleviate the high costs incurred by the runtime encoding bits.
Reference: [16] <editor> Paul Hudak, Simon L. Peyton Jones, and Philip Wadler et al. </editor> <title> Report on the programming language Haskell a non-strict, purely functional language version 1.2. </title> <journal> SIGPLAN Notices, </journal> <volume> 21(5), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Our static cdr-coding method, on the other hand, exploits compile-time analysis to eliminate most runtime checks; at the same time, it poses no more problem in parallel environments than does "ordinary" consing. On the side of statically typed languages, Hall [14] has presented a list compaction technique for Haskell <ref> [16] </ref>. In her scheme, lists can be represented as the old standard representation (OSR) at one place, and in an optimized representation at another place.
Reference: [17] <author> Xavier Leroy. </author> <title> Unboxed objects and polymorphic typing. </title> <booktitle> In Nineteenth Annual ACM Symp. on Principles of Prog. Languages, </booktitle> <address> New York, Jan 1992. </address> <publisher> ACM Press. </publisher>
Reference-contexts: d ::= d 1 and d 2 j (x = e) p ::= x j OLISTP (p 1 ) j ELISTP (p 1 ) j TNILP j TAIL1P (x,p 1 ) j TAIL2P (x,y,p 1 ) into monomorphically-typed intermediate language by using representation analysis, a technique first proposed by Leroy <ref> [17] </ref> and Peyton Jones [21]. <p> This is exactly the form we desired in Section 1. 2.3 Correctness of the translation The type and semantic correctness of our translation can be proven using a technique similar to that of Leroy <ref> [17] </ref>. Because of space limitations, here we only sketch the proof method and state the main theorem. We use ` SRC to denote the type deduction rule for SRC, and ` TGT to denote the refinement type deduction rule for TGT. <p> The heap allocation of intermediate OLIST cons cell is still avoided because of representation analysis <ref> [17] </ref>. It is likely, however, that this transformation will only improve performance if the underlying compiler uses representation analysis [17], and is very sophisticated about closure construction and register usage. Otherwise, the extra cost of closure creations could outweigh the elimination of the cons operations. <p> The heap allocation of intermediate OLIST cons cell is still avoided because of representation analysis <ref> [17] </ref>. It is likely, however, that this transformation will only improve performance if the underlying compiler uses representation analysis [17], and is very sophisticated about closure construction and register usage. Otherwise, the extra cost of closure creations could outweigh the elimination of the cons operations. Note, however, that though there are extra costs of testing for ELIST/OLIST, there are fewer tests for nil. <p> Because the compiler uses continuation-passing style as its intermediate language, the multiple-continuation approach described in Section 3 can be easily added (this has not been done yet). The SML/NJ compiler supports representation analysis <ref> [17] </ref>, so intermediate odd-length lists are represented by unboxed records, which normally stay in registers; this makes the specialized versions (for even-length and odd-length lists) of ucons and uproj operations involve even fewer memory allocations. 4.1 Avoiding code explosion Translating from OSR to NUR involves function specialization and recursion unrolling. <p> In effect, this means that library functions must be explicitly programmed using several different representations, and programs will be improved only if they use the library functions. The idea of using special and more efficient representations for frequently used data objects is originally from Leroy <ref> [17] </ref> and Peyton Jones [21]. Both propose a type-based program transformation scheme that allows objects with monomorphic ML types to use special unboxed representations. When an unboxed object interacts with a boxed polymorphic object, appropriate coercions are inserted. But as mentioned by Leroy [17], their representation analysis techniques do not work <p> used data objects is originally from Leroy <ref> [17] </ref> and Peyton Jones [21]. Both propose a type-based program transformation scheme that allows objects with monomorphic ML types to use special unboxed representations. When an unboxed object interacts with a boxed polymorphic object, appropriate coercions are inserted. But as mentioned by Leroy [17], their representation analysis techniques do not work well with ML's recursive data types, such as the list type. This is because the coercion between the unboxed and boxed representation for lists is often rather expensive (i.e., has costs proportional to the list length).
Reference: [18] <author> Kai Li and Paul Hudak. </author> <title> A new list compaction method. </title> <journal> Software Practice and Experience, </journal> <volume> 16(2) </volume> <pages> 145-163, </pages> <month> Febru-ary </month> <year> 1986. </year>
Reference-contexts: The "static cdr-coding" technique presented in this paper is a simple compile-time method for doing list compaction. It is attractive for modern machines because it does not require any runtime encoding bits at all. Li and Hudak <ref> [18] </ref> proposed a cdr-coding scheme for list compaction under parallel environments. When several lists are being constructed simultaneously from the same heap, the non-contiguous nature of the cells being allocated might eliminate the opportunity for compaction under traditional cdr-coding techniques.
Reference: [19] <author> Robin Milner, Mads Tofte, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: This sharing can be introduced by the garbage collector to avoid complicating the compiled code, if necessary. 2 In Standard ML <ref> [19] </ref>, lists are declared as datatype 'a list = nil | :: of 'a * 'a list. To simplify the presentation, we omit the type variable 'a by considering only integer lists.
Reference: [20] <author> Laurence C. Paulson. </author> <title> ML for the Working Programmer. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: Our benchmarks include: life, the game of Life implemented using lists (written by Reade [22]); ray, a simple ray tracer (this program does not contain much list processing); quicksort, sorting a list of 20000 real numbers using the quicksort algorithm (taken from Paulson <ref> [20] </ref>); sam-sort, sorting a list of 2000 real numbers using a variation of mergesort algorithm (taken from Paulson [20]); intset, "set" operations on sets of integers implemented with sorted lists; mmap, several runs of the map function on a long list. <p> (written by Reade [22]); ray, a simple ray tracer (this program does not contain much list processing); quicksort, sorting a list of 20000 real numbers using the quicksort algorithm (taken from Paulson <ref> [20] </ref>); sam-sort, sorting a list of 2000 real numbers using a variation of mergesort algorithm (taken from Paulson [20]); intset, "set" operations on sets of integers implemented with sorted lists; mmap, several runs of the map function on a long list.
Reference: [21] <author> Simon L. Peyton Jones and John Launchbury. </author> <title> Unboxed values as first class citizens in a non-strict functional language. </title> <booktitle> In The Fifth International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 636-666, </pages> <address> New York, </address> <month> August </month> <year> 1991. </year> <note> ACM Press. </note>
Reference-contexts: and d 2 j (x = e) p ::= x j OLISTP (p 1 ) j ELISTP (p 1 ) j TNILP j TAIL1P (x,p 1 ) j TAIL2P (x,y,p 1 ) into monomorphically-typed intermediate language by using representation analysis, a technique first proposed by Leroy [17] and Peyton Jones <ref> [21] </ref>. <p> In effect, this means that library functions must be explicitly programmed using several different representations, and programs will be improved only if they use the library functions. The idea of using special and more efficient representations for frequently used data objects is originally from Leroy [17] and Peyton Jones <ref> [21] </ref>. Both propose a type-based program transformation scheme that allows objects with monomorphic ML types to use special unboxed representations. When an unboxed object interacts with a boxed polymorphic object, appropriate coercions are inserted.
Reference: [22] <author> Chris Reade. </author> <title> Elements of Functional Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: To demonstrate the savings of execution time, we have compared the performance of several benchmarks under the standard representation (OSR) and the unrolled representation (NUR). Our benchmarks include: life, the game of Life implemented using lists (written by Reade <ref> [22] </ref>); ray, a simple ray tracer (this program does not contain much list processing); quicksort, sorting a list of 20000 real numbers using the quicksort algorithm (taken from Paulson [20]); sam-sort, sorting a list of 2000 real numbers using a variation of mergesort algorithm (taken from Paulson [20]); intset, "set" operations
Reference: [23] <author> Guy L. Steele. Rabbit: </author> <title> a compiler for Scheme. </title> <type> Technical Report AI-TR-474, </type> <institution> MIT, </institution> <address> Cambridge, MA, </address> <year> 1978. </year>
Reference-contexts: They are mostly taken from the initial basis of the Standard ML of New Jersey compiler [3]. It turns out that this problem can be easily solved in the continuation-passing style (CPS) framework <ref> [23, 2] </ref>, because we can specialize the return continuation on the length parity of the result, and make it have multiple entry points also.
Reference: [24] <author> Philip Wadler. </author> <title> Views: A way for pattern matching to cohabit with data abstraction. </title> <booktitle> In Fourteenth Annual ACM Symp. on Principles of Prog. Languages, </booktitle> <pages> pages 307-313, </pages> <address> New York, Jan 1987. </address> <publisher> ACM Press. </publisher>
Reference-contexts: The reason that we use the refinement types, on the other hand, is to do compile-time program transformations and optimizations. The refinement type declaration used in our scheme is embedded in the compiler, and is completely hidden from programmers. As in Wadler's views mechanism <ref> [24] </ref>, the standard and unrolled representations of lists in our scheme can be linked together by a pair of in and out functions (e.g., the "ucons" and "uproj" function in Section 1).
Reference: [25] <author> D. Weinreb and D. Moon. </author> <title> Lisp machine manual. </title> <type> Technical report, </type> <institution> Symolics Corp., </institution> <address> Cambridge, Mass., </address> <year> 1981. </year>
Reference-contexts: Cdr-coding solves the space-usage problem (and in the MIT version allows random access subscripting of lists [13]), but makes the control-dependence problem even worse, as the cdr-coding tag of each car must be checked. Cdr-coding was popular on microcoded Lisp machines circa 1980 <ref> [25, 10] </ref>, but it is not an attractive solution on modern machines. Our new "compile-time cdr-coding" method works for statically typed languages such as ML. Our scheme allows a more compact runtime representation for lists, but does not require any runtime encoding at all. <p> While these schemes differ from each other on the encoding methods, they all rely on the hardware support from microcoded Lisp machines <ref> [25, 10] </ref> to alleviate the high costs incurred by the runtime encoding bits. Since modern machines tend not to offer these kinds of special hardware support, the runtime cdr-coding technique quickly became obsolete in the 1980's.
References-found: 25

