URL: ftp://ftp.cs.toronto.edu/pub/yiming/SIRS96.ps.Z
Refering-URL: http://www.cs.toronto.edu/~yiming/Research.html
Root-URL: http://www.cs.toronto.edu
Email: yiming@vis.toronto.edu tsotsos@vis.toronto.edu  
Title: Sensor Planning in 3D Object Search  
Author: Yiming Ye and John K. Tsotsos 
Address: Toronto, Ontario, Canada M5S 1A4  
Affiliation: Department of Computer Science University of Toronto  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> R. </author> <title> Bajcsy. Active perception vs. passive perception. </title> <booktitle> In Third IEEE Workshop on Vision, </booktitle> <address> USA, </address> <year> 1985. </year>
Reference-contexts: The initial probability distribution is denoted as p [0] (c 1 ); p [0] (c 2 ); : : : ; p [0] (c n ); p [0] (c o ). After the application of the operation f 1 , the distribution is denoted by p <ref> [1] </ref> (c 1 ); p [1] (c 2 ); : : :; p [1] (c n ); p [1] (c o ). <p> After the application of the operation f 1 , the distribution is denoted by p <ref> [1] </ref> (c 1 ); p [1] (c 2 ); : : :; p [1] (c n ); p [1] (c o ). <p> After the application of the operation f 1 , the distribution is denoted by p <ref> [1] </ref> (c 1 ); p [1] (c 2 ); : : :; p [1] (c n ); p [1] (c o ). <p> After the application of the operation f 1 , the distribution is denoted by p <ref> [1] </ref> (c 1 ); p [1] (c 2 ); : : :; p [1] (c n ); p [1] (c o ). Generally, after the application of the operation f i , the distribution is denoted by p [i] (c 1 ); p [i] (c 2 ); : : : ; p [i] (c n ); p [i] (c o ), where 1 i q.
Reference: 2. <author> Connel. </author> <title> An Artificial Creature. </title> <type> PhD thesis, </type> <institution> AI Lab, MIT, </institution> <year> 1989. </year>
Reference-contexts: Garvey [3] proposes the idea of indirect search for the target. Wixson et al. [8] present a mathematical model of search efficiency and analyze the efficiency of indirect search and conclude that indirect search can improve efficiency in many situations. Connell et al. <ref> [2] </ref> construct a robot that roams an area searching for and collecting soda cans.
Reference: 3. <author> T. D. Garvey. </author> <title> Perceptual strategies for purposive vision. </title> <type> Technical Report Technical Note 117, </type> <institution> SRI International, </institution> <year> 1976. </year>
Reference-contexts: Although sensor planning for object search is very important if a robot is to interact intelligently and effectively with its environment, it is interesting to note that there is little research on this subject within the computer vision community. Garvey <ref> [3] </ref> proposes the idea of indirect search for the target. Wixson et al. [8] present a mathematical model of search efficiency and analyze the efficiency of indirect search and conclude that indirect search can improve efficiency in many situations.
Reference: 4. <author> P. Jasiobedzki, M. Jenkin, E. Milios, B. Down, and J. Tsotsos. </author> <title> Laser eye anew 3d sensor for active vision. In Intelligent Robotics and Computer Vision: Sensor Fusion VI. </title> <booktitle> Proceedings of SPIE. </booktitle> <volume> vol. </volume> <year> 2059, </year> <pages> pages 316-321, </pages> <address> Boston, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: The searcher model is based on the ARK robot [5], which is a mobile platform equipped with a special sensor: the Laser Eye <ref> [4] </ref>. The Laser Eye is mounted on a robotic head with pan and tilt capabilities. It consists of a camera with controllable focal length (zoom), a laser range-finder and a mirror. The mirror is used to ensure collinearity of effective optical axes of the camera lens and the range finder. <p> The probability of detecting the target by this allocation is: P [F] = P (f 1 ) + [1 P (f 1 )]P (f 2 ) + : : : + f i=1 The total cost for applying this allocation is (following <ref> [4] </ref>): T [F] = P k Suppose K is the total time that can be allowed in the search, then the task of sensor planning for object search can be defined as finding an allocation F ae O , which satisfies T (F) K and maximizes P [F]. 3 Theoretical Analysis
Reference: 5. <author> S. Nickerson, M. Jenkin, E. Milios, B. Down, P. Jasiobedzki, A. Jepson, D. Terzopoulos, J. Tsotsos, D. Wilkes, N. Bains, and K. Tran. </author> <title> Ark: Autonomous navigation of a mobile robot in a known environment. </title> <booktitle> In Intelligent Autonomous Systems-3, </booktitle> <pages> pages 288-296, </pages> <address> Pennsylvania, USA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Finally, experimental results are presented. 2 Problem Formulation It is important to examine different aspects of object search individually, to study their relationship and to integrate them into a whole search system. The searcher model is based on the ARK robot <ref> [5] </ref>, which is a mobile platform equipped with a special sensor: the Laser Eye [4]. The Laser Eye is mounted on a robotic head with pan and tilt capabilities. It consists of a camera with controllable focal length (zoom), a laser range-finder and a mirror.
Reference: 6. <author> J. Tsotsos. </author> <title> Analyzing vision at the complexity level. </title> <journal> The behavioral and brain science, </journal> <volume> 13 </volume> <pages> 423-469, </pages> <year> 1990. </year>
Reference: 7. <author> D. Wilkes and J. Tsotsos. </author> <title> Active object recognition. </title> <booktitle> In Proceedings of Computer Vision and Pattern Recognition, </booktitle> <pages> pages 136-141, </pages> <address> Illinois, USA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: For example, the problem of object recognition may be less difficult when the image is intelligently grabbed by a controlled camera and the image analysis is interpreted within specific contexts. Research shows that the object recognition problem can be simpler if the camera is intelligently controlled <ref> [7] </ref>. Of course, the simplicity obtained in the image analysis phase is at the cost of the complexity encountered in the camera controlling phase. Much work has been done on image analysis. It is now time to study the problems related to the control of the camera.
Reference: 8. <author> L. E. Wixson. </author> <title> Gaze Selection for Visual Search. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <year> 1994. </year>
Reference-contexts: Garvey [3] proposes the idea of indirect search for the target. Wixson et al. <ref> [8] </ref> present a mathematical model of search efficiency and analyze the efficiency of indirect search and conclude that indirect search can improve efficiency in many situations. Connell et al. [2] construct a robot that roams an area searching for and collecting soda cans.
Reference: 9. <author> Y. Ye. </author> <title> Sensor planning in 3D object search. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto, Toronto, </institution> <address> Ontario, Canada M5S 1A4, </address> <year> 1996. </year>
Reference-contexts: We first list some properties of the sensor planning task. Proofs are omitted (refer to <ref> [9] </ref> for detail). Suppose = S n 1 ; f fl m g. For any operation f 2 O , we define its influence range as (f ) = fc j b (c; f ) 6= 0g. <p> Normally, we only need to record the detection function values of &lt; w 0 ; h 0 &gt;. The detection function values of other angle sizes can be found by the following transformation (for detail, see <ref> [9] </ref>): l 0 = l tan ( w 2 ) 2 )tan ( h 0 ; 0 = arctan [tan () tan ( w 0 tan ( w ]; ffi 0 = arctan [tan (ffi) tan ( w 0 tan ( w ] In general, when we need to find b <p> At this point, we must consider the inaccuracy of the robot movement. We have proposed a strategy to incorporate the inaccuracy of the robot movement into the robot's knowledge about the environment and the target distribution (see <ref> [9] </ref> for detail). 7 Experimental Results Y G P2 A C D (d) (e) (f) (g) Fig. 2.: The real experiment performed in our Lab with the ARK robot. Experiments are performed in our lab using the ARK robot (Figure 2 (a)).
Reference: 10. <author> Y. Ye and J. K. Tsotsos. </author> <title> The detection function in object search. </title> <booktitle> In Proceedings of the fourth international conference for young computer scientist, </booktitle> <pages> pages 868-873, </pages> <address> Beijing, </address> <year> 1995. </year> <title> This article was processed using the T E X macro package with SIRS96 style </title>
References-found: 10

