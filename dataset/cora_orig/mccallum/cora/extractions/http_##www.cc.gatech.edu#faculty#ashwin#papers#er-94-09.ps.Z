URL: http://www.cc.gatech.edu/faculty/ashwin/papers/er-94-09.ps.Z
Refering-URL: http://www.cs.gatech.edu/faculty/ashwin/ABSTRACTS-summary.html
Root-URL: 
Title: Abstract  
Abstract: Self-selection of input examples on the basis of performance failure is a powerful bias for learning systems. The definition of what constitutes a learning bias, however, has been typically restricted to bias provided by the input language, hypothesis language, and preference criteria between competing concept hypotheses. But if bias is taken in the broader context as any basis that provides a preference for one concept change over another, then the paradigm of failure-driven processing indeed provides a bias. Bias is exhibited by the selection of examples from an input stream that are examples of failure; successful performance is filtered out. We show that the degrees of freedom are less in failure-driven learning than in success-driven learning and that learning is facilitated because of this constraint. We also broaden the definition of failure, provide a novel taxonomy of failure causes, and illustrate the interaction of both in a multistrategy learning system called Meta-AQUA. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Birnbaum, L., Collins, G., Freed, M., & Krulwich, B. </author> <year> (1990). </year> <title> Model-based diagnosis of planning failures. </title> <booktitle> In Proc. of the 8th Nat. Conf. of Artificial Intelligence (pp. </booktitle> <pages> 318-323). </pages>
Reference: <author> Collins, G., Birnbaum, L., Krulwich, B., & Freed, M. </author> <year> (1993). </year> <title> The role of self-models in learning to plan (pp. 83-116). In Foundations of knowledge acquisition: </title> <booktitle> Machine learning. </booktitle> <address> Boston: </address> <note> Kluwer. (Also available as Tech. Rep. #24, </note> <institution> Institute of the Learning Sciences, Northwestern University, </institution> <address> Evanston, IL, </address> <month> April, </month> <year> 1992). </year>
Reference: <author> Cox, M. T. </author> <year> (1994). </year> <title> Machines that forget: Learning from retrieval failure of mis-indexed explanations. </title> <booktitle> In Proc. of the 16th Annual Conf. of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> LEA. </publisher>
Reference-contexts: Hills dale, NJ: Lawrence Erlbaum. knowledge, but knowledge of processing strategies (e.g., operators, reasoning schemas, or reasoning components 2 and goal structure. Additionally, if the indexing problem is taken seriously <ref> (see Cox, 1994) </ref>, then selection of domain knowledge, processing strategies and goals can be faulty because of the reasoners applicability information (hence the three columns concerning selection). <p> We have produced an implementation of a multistrategy learning system called Meta-AQUA <ref> (Ram & Cox, 1994) </ref> that learns strictly from failure. <p> the cause of its misunderstanding; (2) decide what to learn - form a set of learning goals to change its knowledge so that such a misunderstanding is not repeated on similar stories; and then (3) strategy selection - choose or construct a learning method by which it achieves these goals <ref> (Ram & Cox, 1994) </ref>. Meta-AQUAs background knowledge (BK) consists of frame representations based on conceptual dependency theory (Schank, 1975) and explanation pattern (XP) theory (Schank, 1986; Ram, 1991; 1993) augmented with a dynamic memory (Schank, 1982). <p> The explanation is in its memory, but it does not have the proper index with which to retrieve the explanation. In effect, it has forgotten the explanation. Given a bias for failure, this demonstrates that forgetting represents an interesting opportunity to learn <ref> (Cox, 1994) </ref> and a novel interpretation of failure (failure of the memory system instead of the inference system).
Reference: <author> Cox, M. T., & Ram, A. </author> <year> (1994). </year> <title> Choosing Learning Strategies to Achieve Learning Goals (pp. 12-21). </title> <editor> In M. desJardins & A. Ram (Eds.), </editor> <booktitle> Proc. of the 1994 AAAI Spring Symposium on Goal-Driven Learning, </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Hills dale, NJ: Lawrence Erlbaum. knowledge, but knowledge of processing strategies (e.g., operators, reasoning schemas, or reasoning components 2 and goal structure. Additionally, if the indexing problem is taken seriously <ref> (see Cox, 1994) </ref>, then selection of domain knowledge, processing strategies and goals can be faulty because of the reasoners applicability information (hence the three columns concerning selection). <p> We have produced an implementation of a multistrategy learning system called Meta-AQUA <ref> (Ram & Cox, 1994) </ref> that learns strictly from failure. <p> the cause of its misunderstanding; (2) decide what to learn - form a set of learning goals to change its knowledge so that such a misunderstanding is not repeated on similar stories; and then (3) strategy selection - choose or construct a learning method by which it achieves these goals <ref> (Ram & Cox, 1994) </ref>. Meta-AQUAs background knowledge (BK) consists of frame representations based on conceptual dependency theory (Schank, 1975) and explanation pattern (XP) theory (Schank, 1986; Ram, 1991; 1993) augmented with a dynamic memory (Schank, 1982). <p> The explanation is in its memory, but it does not have the proper index with which to retrieve the explanation. In effect, it has forgotten the explanation. Given a bias for failure, this demonstrates that forgetting represents an interesting opportunity to learn <ref> (Cox, 1994) </ref> and a novel interpretation of failure (failure of the memory system instead of the inference system).
Reference: <author> DeJong, G., & Mooney, R. </author> <year> (1986). </year> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2), </volume> <pages> 145-176. </pages> <editor> desJardins, M. </editor> <year> (1992). </year> <title> Goal-directed learning: A decision-theoretic model for deciding what to learn next. </title> <booktitle> In Proc. of the ML-92 Workshop on Machine Discovery (pp. </booktitle> <pages> 147-151). </pages>
Reference: <author> Hammond, K. J. </author> <year> (1989). </year> <title> Case-based planning: Viewing planning as a memory task, </title> <booktitle> vol. 1 of Perspectives in artificial intelligence. </booktitle> <address> San Diego: </address> <publisher> Academic Press. </publisher>
Reference: <author> Kolodner, J. L. </author> <year> (1987). </year> <title> Capitalizing on failure through case-based inference. </title> <booktitle> In Proc. of the 9th Annual Conf. of the Cog. Science Society (pp. </booktitle> <pages> 715-726). </pages> <address> Hillsdale, NJ: </address> <publisher> LEA. </publisher>
Reference: <author> Markovitch, S., & Scott, P. D. </author> <year> (1993). </year> <title> Information filtering: Selection mechanisms in learning systems. </title> <journal> Machine Learning, </journal> <volume> 10 (2), </volume> <pages> 113-151. </pages>
Reference: <author> Minton, S. </author> <year> (1990). </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> Art. Intel., </booktitle> <volume> 42, </volume> <pages> 363-392. </pages>
Reference-contexts: The Degrees of Freedom in Learning Many systems invest too much computational overhead in evaluating examples that, in ordinary performance situations, provide few or no useful opportunities to learn. For example, PRODIGY <ref> (Minton, 1990) </ref> has no input bias. As a enly recognizes an example of an enemy target, then it must be overly general; and at the same time, if the classifier of enemy planes rejects the same example, then the concept must be overly specialized.
Reference: <author> Mitchell, T. M. </author> <year> (1990). </year> <title> The need for biases in learning generalizations. </title> <editor> In J.W. Shavlik and T.G. Dietterich (Eds.), </editor> <booktitle> Readings in machine learning (pp. </booktitle> <pages> 184-191). </pages> <address> San Mateo, CA: </address> <note> Morgan Kaufmann. (Originally published in 1980.) </note> <author> Mitchell, T. M., Keller, R., & Kedar-Cabelli, S. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <pages> pp. 47-80. </pages>
Reference: <author> Mitchell, T. M., Utgoff, P., & Banerji, R. </author> <title> (1983) Learning by experimentation: Acquiring and refining problem-solving heuristics. </title> <editor> In R. Michalski, J. Carbonell, & T. Mitchell (Eds.), </editor> <booktitle> Machine leaning: An artificial intelligence approach (pp. </booktitle> <pages> 163-190). </pages> <address> San Mateo, CA: </address> <publisher> M. Kaufmann. </publisher>
Reference-contexts: Likewise, in the Vincennes scenario, even though failure may facilitate learning, life-critical tasks require that the performance system not choose a course that results in failed examples. The approach of LEX <ref> (Mitchell, Utgoff, & Banerji, 1983) </ref>, which generates learning examples on the basis of their expected utility, irrespective of any inductive policy, is unacceptable. The crew of the Vincennes strove for hits and correct rejec-a.
Reference: <author> Newell, A. </author> <title> (1990) Unified theories of cognition. </title> <address> Cambridge, MA: </address> <publisher> Harvard Univ. Press. </publisher>
Reference: <author> Owens, C. </author> <year> (1991). </year> <title> A Functional Taxonomy of Abstract Plan Failures, </title> <booktitle> In Proc. of the 13th Annual Conf. of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> LEA. </publisher>
Reference: <author> Provost, F. J., & Buchanan, B. G. </author> <year> (1992). </year> <title> Inductive policy. </title> <booktitle> In Proc. of the 10th Nat. Conf. on Artificial Intelligence (pp. </booktitle> <pages> 255-261). </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Failure-driven input bias is limited, however. Although failure may constrain learning, some systems may not be able to use this fact because a particular inductive policy (the strategy used to make bias choices based on the underlying assumptions of the domain) may inuence a learning system toward certain results <ref> (Provost & Buchanan, 1992) </ref>. Provost and Buchanan show that inductive policies can bias a learner toward speed of acquisition rather than accuracy (when time is a limited resource, for example) or toward accuracy instead of speed (when safety is a high priority).
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: For failure to occur, the learning system must be associated with some performance task. In the simplest case, the task may be attribute prediction or classification. For example, when a decision tree misclassifies an instance, ID3 <ref> (Quinlan, 1986) </ref> uses the instance for learning.
Reference: <author> Ram, A. </author> <year> (1991). </year> <title> A theory of questions and question asking. </title>
Reference: <author> J. </author> <booktitle> of the Learning Sciences, </booktitle> <volume> 1(3&4), </volume> <pages> 273-318. </pages>
Reference: <author> Ram, A. </author> <year> (1993). </year> <title> Indexing, elaboration and refinement: Incremental learning of explanatory cases. </title> <journal> Machine Learning, </journal> <volume> 10(3), </volume> <pages> 201-248. </pages>
Reference: <author> Ram, A., & Cox, M. T. </author> <year> (1994). </year> <title> Introspective reasoning using meta-explanations for multistrategy learning. </title> <editor> In R. Michal-ski & G. Tecuci (Eds.), </editor> <booktitle> Machine learning: A multistrategy approach IV (pp. </booktitle> <pages> 349-377). </pages> <address> San Mateo,CA:M. </address> <publisher> Kaufmann. </publisher>
Reference-contexts: We have produced an implementation of a multistrategy learning system called Meta-AQUA <ref> (Ram & Cox, 1994) </ref> that learns strictly from failure. <p> the cause of its misunderstanding; (2) decide what to learn - form a set of learning goals to change its knowledge so that such a misunderstanding is not repeated on similar stories; and then (3) strategy selection - choose or construct a learning method by which it achieves these goals <ref> (Ram & Cox, 1994) </ref>. Meta-AQUAs background knowledge (BK) consists of frame representations based on conceptual dependency theory (Schank, 1975) and explanation pattern (XP) theory (Schank, 1986; Ram, 1991; 1993) augmented with a dynamic memory (Schank, 1982).
Reference: <author> Schank, R. C. </author> <year> (1975). </year> <title> Conceptual information processing. </title> <publisher> Amsterdam: North-Holland. </publisher>
Reference-contexts: Meta-AQUAs background knowledge (BK) consists of frame representations based on conceptual dependency theory <ref> (Schank, 1975) </ref> and explanation pattern (XP) theory (Schank, 1986; Ram, 1991; 1993) augmented with a dynamic memory (Schank, 1982). The BK includes general facts about dogs and sniffing, including the explanation that dogs bark when threatened, but it has no knowledge of police dogs.
Reference: <author> Schank, R. C. </author> <year> (1982). </year> <title> Dynamic memory: A theory of reminding and learning in computers and people. </title> <address> Cambridge, MA: </address> <publisher> Cambridge Univ. Press. </publisher>
Reference-contexts: Meta-AQUAs background knowledge (BK) consists of frame representations based on conceptual dependency theory (Schank, 1975) and explanation pattern (XP) theory (Schank, 1986; Ram, 1991; 1993) augmented with a dynamic memory <ref> (Schank, 1982) </ref>. The BK includes general facts about dogs and sniffing, including the explanation that dogs bark when threatened, but it has no knowledge of police dogs. The BK also contains cases of gun smuggling, but the system has not experienced drug interdiction.
Reference: <author> Schank, R. C. </author> <year> (1986). </year> <title> Explanation patterns: Understanding mechanically and creatively. </title> <address> Hillsdale, NJ: </address> <publisher> LEA Sussman, </publisher> <editor> G. J. </editor> <year> (1975). </year> <title> A computer model of skill acquisition. </title> <address> New York: </address> <publisher> American Elsevier. </publisher>
Reference: <author> Thagard, P. </author> <year> (1992). </year> <title> Adversarial problem solving: Modeling an opponent using explanatory coherence. </title> <journal> Cognitive Science, </journal> <volume> 16 (1), </volume> <pages> 123-149. </pages>
Reference: <author> VanLehn, K., Jones, R. M., and Chi, M. T. H. </author> <year> (1992). </year> <title> A model of the self-explanation effect. </title> <journal> J. of the Learning Sciences, </journal> <volume> 2 (1), </volume> <pages> 1-60. </pages>
References-found: 24

