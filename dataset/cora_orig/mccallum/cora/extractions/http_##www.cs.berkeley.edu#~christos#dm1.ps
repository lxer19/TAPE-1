URL: http://www.cs.berkeley.edu/~christos/dm1.ps
Refering-URL: http://www.cs.berkeley.edu/~christos/
Root-URL: 
Email: Email: kleinber@cs.cornell.edu.  christos@cs.berkeley.edu  pragh@almaden.ibm.com  
Title: A Microeconomic View of Data Mining  
Author: Jon Kleinberg Christos Papadimitriou Prabhakar Raghavan 
Note: Supported in part by an Alfred P. Sloan Research Fellowship and by NSF Faculty Early Career Development Award CCR-9701399.  
Address: Ithaca NY 14853.  Soda Hall, UC Berkeley, CA 94720.  650 Harry Road, San Jose CA 95120.  
Affiliation: Department of Computer Science, Cornell University,  Computer Science Division,  IBM Almaden Research Center,  
Abstract: We present a rigorous framework, based on optimization, for evaluating data mining operations such as associations and clustering, in terms of their utility in decision-making. This framework leads quickly to some interesting computational problems related to sensitivity analysis, segmentation and the theory of games. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, A. Swami. </author> <title> "Mining association rules between sets of items in a large database." </title> <booktitle> 1993 SIGMOD, </booktitle> <pages> pp. 207-216, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Data mining is about extracting interesting patterns from raw data. There is some agreement in the literature on what qualifies as a "pattern" (association rules and correlations <ref> [1, 2, 3, 5, 6, 12, 20, 21] </ref> as well as clustering of the data points [9], are some common classes of patterns sought), but only disjointed discussion of what "interesting" means. <p> Automatically focusing on the "interesting" patterns has received very limited formal treatment. Patterns are often deemed "interesting" on the basis of their confidence and support <ref> [1] </ref>, information content [19], and unexpectedness [14, 18]. <p> If @ 2 f j @y k @y ` 6= 0 for some k and `, then we say that f j is nonlinear. Assume that all attributes of the relation C are real numbers in the range <ref> [0; 1] </ref>, and that f j depends on two attributes, call them k j and ` j (extending to more general situations is straightforward, but would make our notation and development less concrete and more cumbersome).
Reference: [2] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, A. I. Verkamo. </author> <title> "Fast discovery of association rules." </title> <booktitle> Advances in Knowledge discovery and data mining, </booktitle> <pages> pp. 307-328, </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Data mining is about extracting interesting patterns from raw data. There is some agreement in the literature on what qualifies as a "pattern" (association rules and correlations <ref> [1, 2, 3, 5, 6, 12, 20, 21] </ref> as well as clustering of the data points [9], are some common classes of patterns sought), but only disjointed discussion of what "interesting" means.
Reference: [3] <author> R. Agrawal, R. Srikant. </author> <title> Fast algorithms for mining association rules. </title> <booktitle> Proc. 20th VLDB Conference, </booktitle> <pages> 487-499, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Data mining is about extracting interesting patterns from raw data. There is some agreement in the literature on what qualifies as a "pattern" (association rules and correlations <ref> [1, 2, 3, 5, 6, 12, 20, 21] </ref> as well as clustering of the data points [9], are some common classes of patterns sought), but only disjointed discussion of what "interesting" means.
Reference: [4] <author> R. Aumann, S. Hart, </author> <title> editors. Handbook of Game Theory, volume I, </title> <publisher> Elsevier, </publisher> <year> 1992. </year>
Reference-contexts: Such games have been studied and analyzed in tremendous depth in the area of game theory <ref> [4] </ref>. To add a data mining twist to the situation, suppose that two corporations I and II each have a fixed set of m and n marketing strategies, respectively, for attracting consumers.
Reference: [5] <author> S. Brin, R. Motwani, J.D. Ullman, S. Tsur. </author> <title> "Dynamic itemset counting and implication rules for market basket data". </title> <booktitle> Proc. ACM SIGMOD, </booktitle> <year> 1997. </year>
Reference-contexts: 1 Introduction Data mining is about extracting interesting patterns from raw data. There is some agreement in the literature on what qualifies as a "pattern" (association rules and correlations <ref> [1, 2, 3, 5, 6, 12, 20, 21] </ref> as well as clustering of the data points [9], are some common classes of patterns sought), but only disjointed discussion of what "interesting" means.
Reference: [6] <author> S. Brin, R. Motwani, C. Silverstein. </author> <title> "Beyond Market Baskets: Generalizing Association Rules to Correlations." </title> <booktitle> Proc. ACM SIGMOD, </booktitle> <year> 1997. </year>
Reference-contexts: 1 Introduction Data mining is about extracting interesting patterns from raw data. There is some agreement in the literature on what qualifies as a "pattern" (association rules and correlations <ref> [1, 2, 3, 5, 6, 12, 20, 21] </ref> as well as clustering of the data points [9], are some common classes of patterns sought), but only disjointed discussion of what "interesting" means.
Reference: [7] <author> M. Avriel. </author> <title> Nonlinear Programming: Analysis and Methods. </title> <publisher> Prentice-Hall, </publisher> <year> 1976. </year>
Reference-contexts: Such optimization problems are the object of study in mathematical programming and microeconomics. 2 The feasible region D and the objective f (x) are both comparably complex components of the problem |and classical optimization theory often treats them in a unified way via Lagrange multipliers and penalty functions <ref> [7] </ref>. <p> Extensions to non-linear inequalities and objectives are possible, with the Kuhn-Tucker conditions <ref> [7] </ref> replacing the sensitivity analysis below.
Reference: [8] <author> M. J. Berry, G. Linoff. </author> <title> Data Mining Techniques. </title> <publisher> John-Wiley, </publisher> <year> 1997. </year>
Reference-contexts: view there is a major difference between the two: We assume that the feasible region D is basically endogenous to the enterprise, while the objective function f (x) is a function that reflects the enterprise's interaction with a multitude of other agents in the market (customers, suppliers, 1 To quote <ref> [8] </ref>, "merely finding the patterns is not enough.
Reference: [9] <author> M. S. Chen, J. Han, P. S. Yu. </author> <title> "Data mining: An overview from a database perspective." </title> <journal> IEEE Trans. on Knowledge and Data Eng., </journal> <volume> 8, 6, </volume> <pages> pp. 866-884, </pages> <year> 1996. </year> <month> 13 </month>
Reference-contexts: 1 Introduction Data mining is about extracting interesting patterns from raw data. There is some agreement in the literature on what qualifies as a "pattern" (association rules and correlations [1, 2, 3, 5, 6, 12, 20, 21] as well as clustering of the data points <ref> [9] </ref>, are some common classes of patterns sought), but only disjointed discussion of what "interesting" means. Most work on data mining studies how patterns are to be extracted automatically, presumably for subsequent human evaluation of the extent in which they are interesting.
Reference: [10] <author> G. B. </author> <title> Dantzig Linear programming and Extensions. </title> <publisher> Princeton Univ. Press, </publisher> <year> 1963. </year>
Reference-contexts: In this section we give an extensive example suggesting that many common data mining activities can be fruitfully analyzed within this framework. To fix ideas, we shall consider the case in which the optimization problem facing the enterprise is a linear program <ref> [10, 16] </ref>, that is, we have an m fi n matrix A (the constraint matrix, m &lt; n), an m-vector b (the resource bounds), and an n-row vector c (the objective function coefficients), and we seek to max c x: (1) The columns of A |the components of x| are called <p> The important question for us is the following: Under what changes in the c j 's will the chosen level of activities continue to be optimal? The theory of sensitivity analysis of linear programming <ref> [10, 16] </ref> answers this question precisely: If the c j 's are changed to new values c 0 j , then the optimal solution remains the same if and only if the condition c 0 c 0 B X 0 is preserved, where X = B 1 A is the simplex
Reference: [11] <author> C. Derman. </author> <title> Finite State Markov Decision Processes. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: within the sensitivity analysis framework of Section 4? Are there interesting and realistic segmentation problems that are tractable, or for which fast and effective heuristics can be developed? How does one model temporal issues within this framework (we can view problem faced by the enterprise as a Markov Decision Process <ref> [11] </ref>), and what insights result? More importantly, what new ideas are needed in order to apply our framework in realistic situations, in which the precise nature of the enterprise's decision problem is murky, and the data on customers incomplete, unreliable, and only very implicitly containing information on the parameters of interest
Reference: [12] <author> D. Gunopoulos, R. Khardon, H. Mannila, H. Toivonen. </author> <title> "Data mining, hypergraph transversals, </title> <booktitle> and machine learning". Proc. 1997 PODS, </booktitle> <pages> pp. 209-217, </pages> <year> 1997. </year>
Reference-contexts: 1 Introduction Data mining is about extracting interesting patterns from raw data. There is some agreement in the literature on what qualifies as a "pattern" (association rules and correlations <ref> [1, 2, 3, 5, 6, 12, 20, 21] </ref> as well as clustering of the data points [9], are some common classes of patterns sought), but only disjointed discussion of what "interesting" means. <p> We wish to data mine this relation with an eye towards discovering correlations that will allow us to jointly promote items. Analyzing correlations between columns over the whole table is a central current problem in data mining (see, for example, <ref> [12] </ref>). However, in this example we focus on a more subtle issue: Correlations in horizontal partitions of the table (i.e., restrictions of the relation, subsets of the rows).
Reference: [13] <author> J. Kleinberg, C. H. Papadimitriou, P. Raghavan. </author> <title> "Segmentation problems," </title> <booktitle> Proc. ACM STOC, </booktitle> <year> 1998. </year>
Reference-contexts: Building on the classical setting of game theory, we develop a notion of segmented matrix games to model this setting. This quickly leads to a number of novel (and largely unsolved) issues in computational game theory. In related work <ref> [13] </ref> in the area of discrete algorithms and complexity theory 4 , we have studied approximation algorithms for some of the most basic segmentation problems that arise from our framework. <p> This leads to interesting connections with classical problems of combinatorial optimization | such as facility location and the maximization of submodular functions | and to settings in which one can concretely analyze the power of methods such as random sampling and greedy iterative-improvement algorithms. We refer the reader to <ref> [13] </ref> for further details. 2 Three examples We pointed out above that aggregation is especially unsatisfactory and inaccurate when the cost function g (x; y i ) is nonlinear in y i . The next two anecdote-based examples illustrate certain interesting and common kinds of nonlinearities. <p> We further explore segmentation, and the computational complexity of the novel problems that it suggests in Section 3, and from the standpoint of approximability in <ref> [13] </ref>. Example 3: Beer and Diapers, Revisited. 7 Let us now formulate a more elaborate data-mining situation related to that of Example 1. Suppose that a retailer has a database of past transactions over a period of time and over many outlets, involving the sales of several items.
Reference: [14] <author> B. Liu and W. Hsu. </author> <title> "Post-analysis of learned rules." </title> <booktitle> Proc. 1996 AAAI, </booktitle> <pages> pp. 828-834, </pages> <year> 1996. </year>
Reference-contexts: Automatically focusing on the "interesting" patterns has received very limited formal treatment. Patterns are often deemed "interesting" on the basis of their confidence and support [1], information content [19], and unexpectedness <ref> [14, 18] </ref>.
Reference: [15] <author> B.M. Masand and G. Piatetsky-Shapiro. </author> <title> "A comparison of approaches for maximizing business payoff of prediction models". </title> <booktitle> Proc. </booktitle> <pages> KDD 195-201, </pages> <year> 1996. </year>
Reference-contexts: Patterns are often deemed "interesting" on the basis of their confidence and support [1], information content [19], and unexpectedness [14, 18]. The more promising concept of actionability |the ability of the pattern to suggest concrete and profitable action by the decision-makers <ref> [15, 17, 18] </ref>, and on the sound of it very close to our concerns in this paper| has not been defined rigorously or elaborated on in the data mining literature. We want to develop a theory of the value of extracted patterns.
Reference: [16] <author> C. H. Papadimitriou, K. Steiglitz. </author> <title> Combinatorial Optimization: Algorithms and Complexity (second edition). </title> <publisher> Dover, </publisher> <year> 1997. </year>
Reference-contexts: In this section we give an extensive example suggesting that many common data mining activities can be fruitfully analyzed within this framework. To fix ideas, we shall consider the case in which the optimization problem facing the enterprise is a linear program <ref> [10, 16] </ref>, that is, we have an m fi n matrix A (the constraint matrix, m &lt; n), an m-vector b (the resource bounds), and an n-row vector c (the objective function coefficients), and we seek to max c x: (1) The columns of A |the components of x| are called <p> The important question for us is the following: Under what changes in the c j 's will the chosen level of activities continue to be optimal? The theory of sensitivity analysis of linear programming <ref> [10, 16] </ref> answers this question precisely: If the c j 's are changed to new values c 0 j , then the optimal solution remains the same if and only if the condition c 0 c 0 B X 0 is preserved, where X = B 1 A is the simplex
Reference: [17] <author> G. Piatetsky-Shapiro, C. J. Matheus. </author> <title> "The interestingness of deviations." </title> <booktitle> Proc. KDD 1994, </booktitle> <pages> 25-36, </pages> <year> 1994. </year>
Reference-contexts: Patterns are often deemed "interesting" on the basis of their confidence and support [1], information content [19], and unexpectedness [14, 18]. The more promising concept of actionability |the ability of the pattern to suggest concrete and profitable action by the decision-makers <ref> [15, 17, 18] </ref>, and on the sound of it very close to our concerns in this paper| has not been defined rigorously or elaborated on in the data mining literature. We want to develop a theory of the value of extracted patterns.
Reference: [18] <author> A. Silberschatz and A. Tuzhilin. </author> <title> "What makes patterns interesting in knowledge discovery systems ." IEEE Trans. </title> <journal> on Knowledge and Data Eng., </journal> <volume> 8, 6, </volume> <year> 1996. </year>
Reference-contexts: Automatically focusing on the "interesting" patterns has received very limited formal treatment. Patterns are often deemed "interesting" on the basis of their confidence and support [1], information content [19], and unexpectedness <ref> [14, 18] </ref>. <p> Patterns are often deemed "interesting" on the basis of their confidence and support [1], information content [19], and unexpectedness [14, 18]. The more promising concept of actionability |the ability of the pattern to suggest concrete and profitable action by the decision-makers <ref> [15, 17, 18] </ref>, and on the sound of it very close to our concerns in this paper| has not been defined rigorously or elaborated on in the data mining literature. We want to develop a theory of the value of extracted patterns.
Reference: [19] <author> P. Smyth, R. M. Goodman. </author> <title> "Rule induction using information theory." </title> <booktitle> Proc. KDD, </booktitle> <pages> 159-176, </pages> <year> 1991. </year>
Reference-contexts: Automatically focusing on the "interesting" patterns has received very limited formal treatment. Patterns are often deemed "interesting" on the basis of their confidence and support [1], information content <ref> [19] </ref>, and unexpectedness [14, 18].
Reference: [20] <author> R. Srikant and R. Agrawal. </author> <title> Mining generalized association rules. </title> <booktitle> Proc. VLDB Conference, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction Data mining is about extracting interesting patterns from raw data. There is some agreement in the literature on what qualifies as a "pattern" (association rules and correlations <ref> [1, 2, 3, 5, 6, 12, 20, 21] </ref> as well as clustering of the data points [9], are some common classes of patterns sought), but only disjointed discussion of what "interesting" means.
Reference: [21] <author> H. Toivonen. </author> <title> Sampling large databases for finding association rules. </title> <booktitle> Proc. VLDB Conference, </booktitle> <year> 1996. </year> <month> 14 </month>
Reference-contexts: 1 Introduction Data mining is about extracting interesting patterns from raw data. There is some agreement in the literature on what qualifies as a "pattern" (association rules and correlations <ref> [1, 2, 3, 5, 6, 12, 20, 21] </ref> as well as clustering of the data points [9], are some common classes of patterns sought), but only disjointed discussion of what "interesting" means.
References-found: 21

