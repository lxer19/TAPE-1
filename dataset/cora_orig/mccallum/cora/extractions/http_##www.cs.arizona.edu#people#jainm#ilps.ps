URL: http://www.cs.arizona.edu/people/jainm/ilps.ps
Refering-URL: http://www.cs.arizona.edu/people/jainm/resume.html
Root-URL: http://www.cs.arizona.edu
Email: fdebray, jainmg@cs.arizona.edu  
Title: A Simple Program Transformation for Parallelism  
Author: Saumya Debray Mudita Jain 
Address: Tucson, AZ 85721, U.S.A.  
Affiliation: Department of Computer Science University of Arizona  
Abstract: Most of the research, to date, on optimizing program transformations for declarative languages has focused on sequential execution strategies. In this paper, we consider a class of commonly encountered computations whose "natural" specification is essentially sequential, and show how algebraic properties of the operators involved can be used to transform them into divide-and-conquer programs that are considerably more efficient, both in theory and in practice, on parallel machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Arsac and Y. Kodratoff, </author> <title> "Some Techniques for Recursion Removal from Recursive Functions", </title> <journal> ACM TOPLAS vol. </journal> <volume> 4 no. 2, </volume> <month> Apr. </month> <year> 1982, </year> <pages> pp. 295-322. </pages>
Reference-contexts: Nevertheless, most of the research on transformation of programs in high level languages has focused, to date, on execution strategies that are|explicitly or implicitly|sequential (see, for example, <ref> [1, 2, 4, 18] </ref>). As a result, it is not at all certain that the application of such techniques leads to programs that run faster on parallel implementations: performance improvements, if any, are purely incidental. In this paper, we focus on transforming declarative programs for efficient parallel execution.
Reference: [2] <author> R. M. Burstall and J. Darlington, </author> <title> "A Transformation System for Developing Recursive Programs", </title> <journal> JACM vol. </journal> <volume> 24 no. 1, </volume> <pages> pp. 44-67, </pages> <month> Jan. </month> <year> 1977. </year>
Reference-contexts: Nevertheless, most of the research on transformation of programs in high level languages has focused, to date, on execution strategies that are|explicitly or implicitly|sequential (see, for example, <ref> [1, 2, 4, 18] </ref>). As a result, it is not at all certain that the application of such techniques leads to programs that run faster on parallel implementations: performance improvements, if any, are purely incidental. In this paper, we focus on transforming declarative programs for efficient parallel execution.
Reference: [3] <author> V. J. Bush and J. R. Gurd, </author> <title> "Transforming Recursive Programs for Execution on Parallel Machines", </title> <booktitle> Proc. Functional Programming Languages and Computer Architecture, </booktitle> <address> Nancy, France, </address> <month> Sept. </month> <year> 1985, </year> <pages> pp. 350-367. </pages>
Reference-contexts: The literature on parallel prefix algorithms also does not usually consider issues such as granularity control, multi-way divide-and-conquer, and adaptive partitioning. The program transformation work that is probably the closest to ours is that of Bush and Gurd <ref> [3] </ref>. These authors define a transformation scheme for FP programs that is similar in many ways to the basic transformation described in Section 2.
Reference: [4] <author> S. K. Debray, </author> <title> "Optimizing Almost-Tail-Recursive Prolog Programs", </title> <booktitle> Proc. Functional Programming Languages and Computer Architecture, </booktitle> <address> Nancy, France, </address> <month> Sept. </month> <year> 1985. </year>
Reference-contexts: Nevertheless, most of the research on transformation of programs in high level languages has focused, to date, on execution strategies that are|explicitly or implicitly|sequential (see, for example, <ref> [1, 2, 4, 18] </ref>). As a result, it is not at all certain that the application of such techniques leads to programs that run faster on parallel implementations: performance improvements, if any, are purely incidental. In this paper, we focus on transforming declarative programs for efficient parallel execution. <p> It is easy to show that this computation can be expressed using a procedure p defined as follows (to keep the program simple, the accumulator has not been made explicit here: it can easily be transformed to a tail-recursive procedure that manipulates an accumulator explicitly: see, for example, <ref> [4] </ref>).
Reference: [5] <author> S. K. Debray, N. Lin and M. Hermenegildo, </author> <title> "Task Granularity Analysis in Logic Programs," </title> <booktitle> Proc. ACM SIGPLAN'90 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1990, </year> <pages> pp. 174-188. </pages>
Reference-contexts: Thus, there is a need to balance the amount of parallelism exploited against the overheads incurred in doing this. The question of task granularity control has been investigated by a number of researchers (see, e.g., <ref> [5, 16] </ref>). The transformation described above is easily amenable to granularity control. <p> (x; m; n; y) : q seq 1 (e m1 (x); n m; "; y): q seq 1 (x; n; w; y) : n &gt; 0 [] q seq 1 (e (x); n 1; d (x) w; y): One of the observations from the the granularity control experiments reported in <ref> [5] </ref> is that in general, keeping track of argument sizes for dynamic granularity control can incur additional runtime overheads that can, in some cases, swamp the performance gains resulting from granularity control and lead to a slowdown in the execution speed of the program.
Reference: [6] <author> S. Decorte, D. De Schreye, and M. Fabris, </author> <title> "Automatic Inference of Norms: a Missing Link in Automatic Termination Proofs", </title> <booktitle> Proc. 1993 International Symposium on Logic Programming, </booktitle> <address> Vancouver, B.C., </address> <month> Nov. </month> <year> 1993, </year> <pages> pp. 420-436. </pages>
Reference-contexts: Techniques for automatically inferring the norm appropriate for a particular procedure, based on the types of its input arguments, have been investigated by Decorte et al. <ref> [6] </ref>. <p> our purposes, we assume that input and output arguments for procedures have been identified via mode analysis; that the types for these arguments have been determined; and that the appropriate norms for the input arguments of procedures have been given, either via programmer annotations, or through dataflow analysis as in <ref> [6] </ref>. Given a procedure p in a program P , let in type (p) denote the type of the input arguments of p.
Reference: [7] <author> T. J. Dekker, </author> <title> "Finding a Zero by means of Successive Linear Interpolation", in Constructive Aspects of the Fundamental Theorem of Algebra, </title> <editor> eds. B. Dejon and P. Henrici, </editor> <publisher> Wiley-Interscience, </publisher> <address> London, </address> <year> 1969. </year>
Reference-contexts: This program uses a nontrivial, and better-behaved, modification of the Newton-Raphson method very similar to one described by Dekker <ref> [7] </ref>. 5 Performance While the divide-and-conquer programs resulting from our transformation may uncover more parallelism than the original program, it is not immediately obvious whether they are, in practice, "better" than either the original programs, or the programs one might get using existing parallelizing compilers such as that of &- Prolog
Reference: [8] <author> M. J. Fernandez, M. Carro, and M. Hermenegildo, </author> <title> "IDeal Resource Allocation (IDRA): A Technique for Computing Accurate Ideal Speedups in Parallel Logic Languages", </title> <type> Technical Report FIM26.3/AI/92, </type> <institution> Computer Science Faculty, Technical University of Madrid, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: The numbers reported were obtained by first running each program under &-Prolog [9] on a Sparcstation-1, and then using the IDRA tool <ref> [8] </ref> to compute ideal speedups under &-Prolog for more than one processor. In each case, the tail-recursive program was entirely sequential and showed no speedups.
Reference: [9] <author> M. Hermenegildo and K. Greene, </author> <title> "The &-Prolog System: Exploiting Independent And-Parallelism", </title> <journal> New Generation Computing vol. </journal> <volume> 9 nos. </volume> <pages> 3-4, </pages> <year> 1991, </year> <pages> pp. 233-257. </pages>
Reference-contexts: 5 Performance While the divide-and-conquer programs resulting from our transformation may uncover more parallelism than the original program, it is not immediately obvious whether they are, in practice, "better" than either the original programs, or the programs one might get using existing parallelizing compilers such as that of &- Prolog <ref> [9] </ref>. <p> Three versions of each program were tested: a straightforward tail-recursive program (TR), the parallel program obtained using the &-Prolog compiler on the tail-recursive program (&-P), and the divide-and-conquer program obtained using our transformation (D&C). The numbers reported were obtained by first running each program under &-Prolog <ref> [9] </ref> on a Sparcstation-1, and then using the IDRA tool [8] to compute ideal speedups under &-Prolog for more than one processor. In each case, the tail-recursive program was entirely sequential and showed no speedups.
Reference: [10] <author> F. T. Leighton, </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes, </title> <publisher> Morgan Kaufman, </publisher> <year> 1992. </year>
Reference-contexts: to multiply two 64 fi 64 matrices produced so many parallel tasks that the trace file generated was over 20 MB in size, took over 3 hours to read in, and required too much resources to process.) 6 Related Work Conceptually, our ideas are closely related to parallel prefix computation <ref> [10] </ref>. While parallel prefix algorithms are generally formulated in terms of computing prefixes, in parallel, of some given string, we address the slightly different problem of starting with an encoding of a family of strings (the original program) and producing an encoding of a family of trees (the transformed program).
Reference: [11] <author> U. Manber, </author> <title> Introduction to Algorithms: A Creative Approach, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Such a search procedure is called an "interpolation search," and is very efficient for inputs where the values are more or less uniformly distributed <ref> [11] </ref>. Using adaptive partitioning, as discussed in Section 3.1, it is trivial to transform the binary search program obtained above to an interpolation search. Suppose we are searching an array A for a value x. The split function for this is defined as follows (see [11]): split (A; x; m; n) <p> more or less uniformly distributed <ref> [11] </ref>. Using adaptive partitioning, as discussed in Section 3.1, it is trivial to transform the binary search program obtained above to an interpolation search. Suppose we are searching an array A for a value x. The split function for this is defined as follows (see [11]): split (A; x; m; n) = bm + (x A [m])(n m)=(A [n] A [m])c.
Reference: [12] <author> H. Millroth, </author> <title> "Reforming Compilation of Logic Programs", </title> <booktitle> Proc. 1991 International Symposium on Logic Programming, </booktitle> <address> San Diego, </address> <month> Oct. </month> <year> 1991, </year> <pages> pp. 485-499. </pages>
Reference-contexts: Finally, Bush and Gurd do not give any empirical evidence of the practicality of their transformation. Also closely related is Millroth's work on compilation of Reform Prolog <ref> [12] </ref>. The biggest difference between the two is that Millroth's work relies on low-level aspects of the Reform Prolog system, while ours is formulated as a high-level source-to-source transformation.
Reference: [13] <author> R. S. Nikhil, </author> <title> Id Language Reference Manual, Computation Structures Group Memo 284-2, </title> <institution> Lab. for Computer Science, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Accumulative computations of this kind, involving associative operators, are commonly encountered in a wide variety of contexts, to the point that some authors have proposed augmenting programming languages with constructs designed specifically to handle such computations <ref> [13, 14] </ref>.
Reference: [14] <author> K. Pingali and K. Ekanadham, </author> <title> "Accumulators: New Logic Variable Abstractions for Functional Languages", </title> <booktitle> Proc. Eighth Conference on Foundations of Software Technology and Theoretical Computer Science, </booktitle> <address> Pune, India, Dec.. </address> <year> 1988, </year> <note> pp.377-399. Springer-Verlag LNCS vol. 338. </note>
Reference-contexts: Accumulative computations of this kind, involving associative operators, are commonly encountered in a wide variety of contexts, to the point that some authors have proposed augmenting programming languages with constructs designed specifically to handle such computations <ref> [13, 14] </ref>.
Reference: [15] <author> R. Ramakrishnan, C. Beeri, and R. Krishnamurthy, </author> <title> "Optimizing Existential Datalog Queries", </title> <booktitle> Proc. Seventh ACM Symp. on Principles of Database Systems, </booktitle> <address> Austin, TX, </address> <month> March </month> <year> 1988, </year> <pages> pp. 89-102. </pages>
Reference-contexts: Substituting this back into the definiton of search/6 yields an implementation of binary search. The resulting program can be further optimized in various ways. Since the auxiliary predicate search/4 is called from exactly one place, and can be "in-lined" away. The techniques of Ramakrishnan et al. <ref> [15] </ref> can be used to detect that the third argument of search/6 is never defined or used, and can therefore be discarded.
Reference: [16] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Thus, there is a need to balance the amount of parallelism exploited against the overheads incurred in doing this. The question of task granularity control has been investigated by a number of researchers (see, e.g., <ref> [5, 16] </ref>). The transformation described above is easily amenable to granularity control. <p> be rewritten as q (x; m; n; y) : n m 1 [] q (x; m; n; y) : n m = 0 [] y = d (e m1 (x)): Now suppose that for a given program and implementation, we decide|either from user-supplied information; or from profile information, as in <ref> [16] </ref>; or using program analysis techniques, as in [5]|that it is worth creating a parallel task for this function only if the interval being processed is of size at least N .
Reference: [17] <author> L. F. Shampline and R. C. Allen, Jr., </author> <title> Numerical Computing: an Introduction, </title> <editor> W. B. </editor> <publisher> Saunders, </publisher> <year> 1973. </year>
Reference-contexts: The resulting program finds roots of equations using what is essentially the bisection method <ref> [17] </ref>. 4.2.2 Transformation to (Modified) Newton-Raphson Instead of blindly splitting the input interval at the midpoint at each recursive step, we can use adaptive partitioning. <p> The resulting split function is given by the following (see, for example, <ref> [17] </ref>), where f 0 denotes the first derivative of f: split (m; n) = n f (n)=f 0 (n). If we were to use this split function, the resulting program would use the Newton-Raphson method.
Reference: [18] <author> H. Tamaki and T. Sato, </author> <title> "Unfold/Fold Transformations of Logic Programs", </title> <booktitle> Proc. Second International Conference on Logic Programming. </booktitle> <address> Uppsala, Swe-den, </address> <year> 1984. </year>
Reference-contexts: Nevertheless, most of the research on transformation of programs in high level languages has focused, to date, on execution strategies that are|explicitly or implicitly|sequential (see, for example, <ref> [1, 2, 4, 18] </ref>). As a result, it is not at all certain that the application of such techniques leads to programs that run faster on parallel implementations: performance improvements, if any, are purely incidental. In this paper, we focus on transforming declarative programs for efficient parallel execution.
Reference: [19] <author> M. Wolfe, </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: More importantly, the reliance on Fortran-like loop-parallelization techniques, unlike our approach, leaves unaddressed the question of parallel execution of loops involving nondeterministic computations, since these do not arise in Fortran-like languages and are not considered in standard texts on Fortran implementation <ref> [19, 20] </ref>. 7 Conclusions While it is generally believed that programs written in high-level programming languages are amenable to manipulation by powerful semantics-based tools for transformation to more efficient forms on the one hand, and to parallel execution on the other, most of the work on program transformation appears to have
Reference: [20] <author> H. Zima, </author> <title> Supercompilers for Parallel and Vector Computers, </title> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: More importantly, the reliance on Fortran-like loop-parallelization techniques, unlike our approach, leaves unaddressed the question of parallel execution of loops involving nondeterministic computations, since these do not arise in Fortran-like languages and are not considered in standard texts on Fortran implementation <ref> [19, 20] </ref>. 7 Conclusions While it is generally believed that programs written in high-level programming languages are amenable to manipulation by powerful semantics-based tools for transformation to more efficient forms on the one hand, and to parallel execution on the other, most of the work on program transformation appears to have
References-found: 20

