URL: http://www.research.att.com/~lewis/papers/lewis94b.ps
Refering-URL: http://www.csi.uottawa.ca/~debruijn/irbib.html
Root-URL: 
Email: lewis@research.att.com  mnr@cs.cmu.edu  
Title: A Comparison of Two Learning Algorithms for Text Categorization  
Author: David D. Lewisy and Marc Ringuettez 
Date: April 11-13, 1994, pp. 81-93.  
Note: Appeared (with same pagination) in Third Annual Symposium on Document Analysis and  
Address: Murray Hill, NJ 07974;  Pittsburgh, PA 15213;  Las Vegas, NV,  
Affiliation: yAT&T Bell Laboratories;  zSchool of Computer Science; Carnegie Mellon University;  Information Retrieval,  
Abstract: Algorithms for training Bayesian independence classifiers and decision trees were tested on two text categorization data sets. Both algorithms allow adjustable tradeoffs between recall and precision and have similar classification effectiveness. The decision tree method, while slower, produces a classifier that is easier to understand and in one case revealed an unsuspected chronological variation in the category definitions. Maximum effectiveness is reached for both algorithms when the initial set of features is pruned using collection frequency and mutual information. This supports previous suggestions that the stepwise feature selection in decision tree algorithms can be aided by prefiltering the feature set. 
Abstract-found: 1
Intro-found: 1
Reference: [AD91] <author> Almuallim, Hus-sein and Thomas G. Dietterich. </author> <title> Learning with many irrelevant features. </title> <booktitle> AAAI-91, </booktitle> <pages> pages 547-552, </pages> <year> 1991. </year>
Reference-contexts: This is in agreement with Almuallin and Dietterich's results suggesting that the decision tree learning algorithms ID3 and FRINGE are not effective at minimizing number of features used in the face of many irrelevant features <ref> [AD91] </ref>. The success of our relatively simple pre-filtering strategy suggests that more research on feature selection would be profitable, even for relatively well-understood learning algorithms.
Reference: [BFL + 88] <author> Biebricher, Peter, Norbert Fuhr, Gerhard Lustig, Michael Schwantner, and Ger-hard Knorz. </author> <title> The automatic indexing system AIR/PHYS| from research to application. </title> <booktitle> In Eleventh International Conference on Research & Development in Information Retrieval, </booktitle> <pages> pages 333|342, </pages> <year> 1988. </year>
Reference-contexts: A primary application of text categorization is to assign subject categories to documents to support information retrieval, or to aid human indexers in assigning such categories <ref> [BFL + 88, HW90] </ref>. Text categorization components are also seeing increasing use in natural language processing systems for data extraction [Lew92c]. Categorization may be used to filter out documents or parts of documents that are unlikely to contain extractable data, without incurring the cost of more expensive natural language processing. <p> The second strategy is to use existing bodies of manually categorized text in constructing categorizers by inductive learning. A wide variety of learning approaches have been used, including Bayesian classification [Mar61], decision trees [CFAT91], factor analysis [BB63], fuzzy sets [COL83], linear regression <ref> [BFL + 88] </ref>, and nearest neighbor approaches [CMSW92]. Learning-based systems have been found to be cheaper and faster to build, as well as more accurate in some applications [CMSW92]. Text categorization applications nevertheless provide many challenges for machine learning.
Reference: [BB63] <author> Borko, Harold and Myrna Ber-nick. </author> <title> Automatic document classification. </title> <journal> Journal of the Association for Computing Machinery, </journal> <pages> pages 151-161, </pages> <year> 1963. </year>
Reference-contexts: The second strategy is to use existing bodies of manually categorized text in constructing categorizers by inductive learning. A wide variety of learning approaches have been used, including Bayesian classification [Mar61], decision trees [CFAT91], factor analysis <ref> [BB63] </ref>, fuzzy sets [COL83], linear regression [BFL + 88], and nearest neighbor approaches [CMSW92]. Learning-based systems have been found to be cheaper and faster to build, as well as more accurate in some applications [CMSW92]. Text categorization applications nevertheless provide many challenges for machine learning.
Reference: [Bun90] <author> Buntine, Wray. </author> <title> A theory of learning classification rules. </title> <type> PhD thesis, </type> <institution> School of Computing Science, University of Technology, </institution> <address> Sydney, </address> <month> February </month> <year> 1990. </year>
Reference-contexts: If X% of the 83 training documents are in category C j , we place the kX% of the test documents with the highest estimates of P (C j = 1jD) in that category. 3.2 DT-min10 Our second approach was a decision tree learning algorithm implemented using the IND package <ref> [Bun90] </ref>. A decision tree was constructed for each category using the recursive partitioning algorithm with information gain splitting rule.
Reference: [Bun91] <author> Buntine, Wray. </author> <title> Introduction to IND and recursive partitioning. </title> <type> Technical Report. </type> <institution> RIACS/NASA Ames Research Center, </institution> <month> September </month> <year> 1991. </year>
Reference: [COL83] <author> Cerny, Barbara A., Anna Okse-niuk, and J. Dennis Lawrence. </author> <title> A fuzzy measure of agreement between machine and manual assignment of documents to subject categories. </title> <booktitle> In Proceedings of the 46th ASIS Annual Meeting, </booktitle> <pages> page 265, </pages> <year> 1983. </year>
Reference-contexts: The second strategy is to use existing bodies of manually categorized text in constructing categorizers by inductive learning. A wide variety of learning approaches have been used, including Bayesian classification [Mar61], decision trees [CFAT91], factor analysis [BB63], fuzzy sets <ref> [COL83] </ref>, linear regression [BFL + 88], and nearest neighbor approaches [CMSW92]. Learning-based systems have been found to be cheaper and faster to build, as well as more accurate in some applications [CMSW92]. Text categorization applications nevertheless provide many challenges for machine learning.
Reference: [Cla85] <author> Clancey, William J. </author> <title> Heuristic classification. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 289-350, </pages> <year> 1985. </year>
Reference-contexts: Other systems take advantage of multi-word phrases, positional or linguistic structure, or other information. There have been two main approaches to the construction of text categorization systems. First, a number of systems [VS87, Har88a, HW90] have embodied approaches similar to those used in expert systems for classification or diagnosis <ref> [Cla85] </ref>. Knowledge engineers define one or more layers of intermediate conclusions between the input evidence (words and other textual features) and the output categories and write rules for mapping from one layer to another, and for confirming or removing conclusions.
Reference: [Cle91] <author> Cleverdon, Cyril W. </author> <title> The significance of the Cranfield tests of index languages. </title> <booktitle> In Fourteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 3-12, </pages> <year> 1991. </year>
Reference: [CFAT91] <author> Crawford, Stuart L., Robert M. Fung, Lee A. Appelbaum, and Richard M. Tong. </author> <title> Classification trees for information retrieval. </title> <booktitle> In Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 245-249, </pages> <year> 1991. </year>
Reference-contexts: The second strategy is to use existing bodies of manually categorized text in constructing categorizers by inductive learning. A wide variety of learning approaches have been used, including Bayesian classification [Mar61], decision trees <ref> [CFAT91] </ref>, factor analysis [BB63], fuzzy sets [COL83], linear regression [BFL + 88], and nearest neighbor approaches [CMSW92]. Learning-based systems have been found to be cheaper and faster to build, as well as more accurate in some applications [CMSW92]. Text categorization applications nevertheless provide many challenges for machine learning.
Reference: [CMSW92] <author> Creecy, Robert M., Brij M. Masand, Stephen J. Smith, and David L. Waltz. </author> <title> Trading MIPS 91 and Memory for Knowledge En--gineering. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 48-63, </pages> <year> 1992. </year>
Reference-contexts: The second strategy is to use existing bodies of manually categorized text in constructing categorizers by inductive learning. A wide variety of learning approaches have been used, including Bayesian classification [Mar61], decision trees [CFAT91], factor analysis [BB63], fuzzy sets [COL83], linear regression [BFL + 88], and nearest neighbor approaches <ref> [CMSW92] </ref>. Learning-based systems have been found to be cheaper and faster to build, as well as more accurate in some applications [CMSW92]. Text categorization applications nevertheless provide many challenges for machine learning. <p> variety of learning approaches have been used, including Bayesian classification [Mar61], decision trees [CFAT91], factor analysis [BB63], fuzzy sets [COL83], linear regression [BFL + 88], and nearest neighbor approaches <ref> [CMSW92] </ref>. Learning-based systems have been found to be cheaper and faster to build, as well as more accurate in some applications [CMSW92]. Text categorization applications nevertheless provide many challenges for machine learning. Feature sets are huge| on the order of tens of thousands of features when words are used, or even more if multi-word phrases are allowed.
Reference: [DH73] <author> Duda, Richard O. and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: (5:414 fi stake + 0:826) Given accurate estimates of P (C j = 1jD), the optimal categorization strategy, under the assumption of equal costs for all errors, is to set a threshold p and assign to document D all C j for which P (C j = 1jD) &gt;= p <ref> [DH73] </ref>. This strategy is not necessarily optimal when there are errors in the probability estimates, due to limited samples or the violation of our independence assumptions. We have experimented with a number of alternative thresholding strategies, and found the best to be proportional assignment. <p> For PropBayes, performance peaks at around 10 features for the Reuters task and 15 features for the MUC-3 task (Figure 4). With more features performance starts to decline. One cause of this phenomenon is overfitting or "the curse of dimensionality" <ref> [DH73] </ref>. As an increasing number of parameters are estimated from a fixed amount of data, we induce a model of the noise as well as the true relationships in the training set.
Reference: [Fis87] <author> Fisher, Douglas H. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: We plan to investigate making available to our standard learning algorithms features corresponding to time of publication, day, date of the week, month, and so on. More generally, we plan to investigate incremental learning algorithms that are designed to track concept drift <ref> [Fis87] </ref> and to see how the idea of cyclical changes in concept definition might be used. Raw performance is not the only characteristic of interest in a learning algorithm for text categorization. For applications it is likely that manual editing or tuning of induced concept descriptions will be desirable.
Reference: [FHL + 91] <author> Fuhr, Norbert, Stephan Hart-mann, Gerhard Lustig, Michael Schwantner, Kon-stadinos Tzeras, and Gerhard Knorz. </author> <title> AIR/X|a rule-based multistage indexing system for large subject fields. </title> <booktitle> In RIAO 91 Conference Proceedings: Intelligent Text and Image Handling, </booktitle> <pages> pages 606-623, </pages> <year> 1991. </year>
Reference-contexts: For instance, the operational AIR/X system uses both rule-based and statistical techniques to achieve a microaveraged breakeven point of approximately 0:65 in indexing a physics database <ref> [FHL + 91] </ref>. 0 .4 .8 0 .2 .4 .6 .8 1 Precision Recall Reuters PropBayes, 10f DT-min10, 90f .2 .6 1 Precision Recall MUC-3 PropBayes, 15f DT-min10, 4f similar for each data set, but vary considerably between the data sets.
Reference: [GC90] <author> Gale, William A. and Ken-neth W. Church. </author> <title> Poor estimates of context are worse than none. </title> <booktitle> In Speech and Natural Language Workshop, </booktitle> <pages> pages 283-287, </pages> <address> San Mateo, CA: </address> <publisher> Mor-gan Kaufmann, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: To a certain extent, this problem is inherent in the simplicity of the decision tree model, in which each estimated probability corresponds to a disjoint set of training instances. However, more sophisticated treatment of estimation from small samples might help <ref> [GC90] </ref>. The second thing we notice is that performance seems to approach an asymptote on the Reuters data as the number of features increases. This suggests we have avoided overfitting in the range of feature set sizes tested.
Reference: [Har88a] <author> Hardt, S. L. </author> <title> On recognizing planned deception. </title> <booktitle> In AAAI-88 Workshop on Plan Recognition, </booktitle> <year> 1988. </year>
Reference-contexts: Other systems take advantage of multi-word phrases, positional or linguistic structure, or other information. There have been two main approaches to the construction of text categorization systems. First, a number of systems <ref> [VS87, Har88a, HW90] </ref> have embodied approaches similar to those used in expert systems for classification or diagnosis [Cla85].
Reference: [HW90] <author> Hayes, Philip J. and Steven P. Weinstein. CONSTRUE/TIS: </author> <title> a system for content-based indexing of a database of news stories. </title> <booktitle> In Second Annual Conference on Innovative Applications of Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: A primary application of text categorization is to assign subject categories to documents to support information retrieval, or to aid human indexers in assigning such categories <ref> [BFL + 88, HW90] </ref>. Text categorization components are also seeing increasing use in natural language processing systems for data extraction [Lew92c]. Categorization may be used to filter out documents or parts of documents that are unlikely to contain extractable data, without incurring the cost of more expensive natural language processing. <p> Other systems take advantage of multi-word phrases, positional or linguistic structure, or other information. There have been two main approaches to the construction of text categorization systems. First, a number of systems <ref> [VS87, Har88a, HW90] </ref> have embodied approaches similar to those used in expert systems for classification or diagnosis [Cla85]. <p> The CONSTRUE rule-based text categorization system achieves a microaveraged breakeven of around 0:90 on a different, and possibly easier, testset drawn from the Reuters data <ref> [HW90] </ref>. This level of performance, the result of a 9.5 person-year effort, is an admirable target for learning based systems to shoot for. Comparison with published results on MUC-3 are difficult, since we attempted only a part of the complex MUC-3 task.
Reference: [Lew91a] <author> Lewis, David D. </author> <title> Data extraction as text categorization: An experiment with the MUC-3 corpus. </title> <booktitle> In Proceedings of the Third Message Understanding Evaluation and Conference, </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kauf-mann, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: We used the other 1,200 MUC-3 training documents (encoded by 16 different MUC-3 sites) as our categoriza 85 tion training documents. The goal of this text/training split was to put the most accurately encoded documents in the test set. Details of the Reuters [Lew92, Lew92b] and MUC-3 <ref> [Lew91a] </ref> datasets are available elsewhere. 4.2 Evaluation Measures The performance measures used were recall (number of categories correctly assigned divided by the total number of categories that should be assigned) and precision (number of categories correctly assigned divided by total number of categories assigned) [van79]. <p> Comparison with published results on MUC-3 are difficult, since we attempted only a part of the complex MUC-3 task. However, in earlier experiments using the official MUC-3 testset and scoring, Prop-Bayes achieved performance toward but within the low end of official MUC-3 scores <ref> [Lew91a] </ref>. 6 Feature Set Size We experimented with varying the number of features chosen by information gain for each category to study the effect of feature set size on performance.
Reference: [Lew91b] <author> Lewis, David D. </author> <title> Evaluating text categorization. </title> <booktitle> In Proceedings of Speech and Natural Language Workshop, </booktitle> <pages> pages 312-318, </pages> <month> February, </month> <year> 1991. </year>
Reference-contexts: For a set of k categories and d documents a total of n = kd categorization decisions are made. Given those kd decisions, several ways of computing average performance are available <ref> [Lew91b] </ref>. We used mi-croaveraging, which considers all kd decisions as a single group and computes recall and precision for the group as whole. Both of our categorization algorithms include an adjustable parameter controlling the algorithm's willingness to assign categories to documents.
Reference: [Lew92] <author> Lewis, David D. </author> <title> Representation and learning in information retrieval. </title> <type> PhD thesis, </type> <institution> Computer Science Dept., Univ. of Mas-sachusetts at Amherst, </institution> <month> Febru-ary </month> <year> 1992. </year> <type> Technical Report 91-93. </type>
Reference-contexts: Feature sets are huge| on the order of tens of thousands of features when words are used, or even more if multi-word phrases are allowed. Natural language features exhibit a number of properties, including synonymy, ambiguity, and skewed distributions, that interfere with forming classification functions <ref> [Lew92] </ref>. Proper categorization may depend on subtle distinctions in emphasis. A human indexer assigned the story in Figure 1 to the GOLD and SILVER categories, but not to the DLR (U.S. dollar) or OIL categories, as these concepts were apparently not considered central. <p> explore variations in the size of the feature set given each algorithm, and was also a practical necessity given the size of the full feature sets (see Section 4.1). 3.1 PropBayes We describe our use of Bayesian probabilistic methods for categorization only briefly, since full details have been presented elsewhere <ref> [Lew92, Lew92b] </ref>. For brevity, we refer to this method as PropBayes in the paper. <p> We used the other 1,200 MUC-3 training documents (encoded by 16 different MUC-3 sites) as our categoriza 85 tion training documents. The goal of this text/training split was to put the most accurately encoded documents in the test set. Details of the Reuters <ref> [Lew92, Lew92b] </ref> and MUC-3 [Lew91a] datasets are available elsewhere. 4.2 Evaluation Measures The performance measures used were recall (number of categories correctly assigned divided by the total number of categories that should be assigned) and precision (number of categories correctly assigned divided by total number of categories assigned) [van79].
Reference: [Lew92b] <author> Lewis, David D. </author> <title> An evaluation of phrasal and clustered representations on a text categorization task. </title> <booktitle> In Fifteenth Annual International Conference on Research & Development in Information Retrieval, </booktitle> <pages> pages 37-50, </pages> <year> 1992. </year>
Reference-contexts: explore variations in the size of the feature set given each algorithm, and was also a practical necessity given the size of the full feature sets (see Section 4.1). 3.1 PropBayes We describe our use of Bayesian probabilistic methods for categorization only briefly, since full details have been presented elsewhere <ref> [Lew92, Lew92b] </ref>. For brevity, we refer to this method as PropBayes in the paper. <p> We used the other 1,200 MUC-3 training documents (encoded by 16 different MUC-3 sites) as our categoriza 85 tion training documents. The goal of this text/training split was to put the most accurately encoded documents in the test set. Details of the Reuters <ref> [Lew92, Lew92b] </ref> and MUC-3 [Lew91a] datasets are available elsewhere. 4.2 Evaluation Measures The performance measures used were recall (number of categories correctly assigned divided by the total number of categories that should be assigned) and precision (number of categories correctly assigned divided by total number of categories assigned) [van79].
Reference: [Lew92c] <author> Lewis, David D. and Richard M. Tong. </author> <title> Text Filtering in MUC-3 and MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <pages> pages 51-66, </pages> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: A primary application of text categorization is to assign subject categories to documents to support information retrieval, or to aid human indexers in assigning such categories [BFL + 88, HW90]. Text categorization components are also seeing increasing use in natural language processing systems for data extraction <ref> [Lew92c] </ref>. Categorization may be used to filter out documents or parts of documents that are unlikely to contain extractable data, without incurring the cost of more expensive natural language processing. They also can be used to route texts to category-specific processing mechanisms, and even to generate fillers for some fields.
Reference: [Mar61] <author> Maron, M. E. </author> <title> Automatic indexing: An experimental inquiry. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 8 </volume> <pages> 404-417, </pages> <year> 1961. </year>
Reference-contexts: The second strategy is to use existing bodies of manually categorized text in constructing categorizers by inductive learning. A wide variety of learning approaches have been used, including Bayesian classification <ref> [Mar61] </ref>, decision trees [CFAT91], factor analysis [BB63], fuzzy sets [COL83], linear regression [BFL + 88], and nearest neighbor approaches [CMSW92]. Learning-based systems have been found to be cheaper and faster to build, as well as more accurate in some applications [CMSW92].
Reference: [Qui86a] <author> Quinlan, J. R. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Before either algorithm was used, a preliminary filtering of the features for each data set was done. All features were ranked for each category using the information gain measure <ref> [Qui86a] </ref> and the top features were given to each algorithm.
Reference: [Sun91] <editor> Sundheim, Beth M., ed. </editor> <booktitle> Proceedings of the Third Message 92 Understanding Evaluation and Conference. </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> Los Altos, CA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Capitalization was ignored and a standard list of stop words (mostly grammatical function words) were removed. The second data set consisted of 1,500 documents from the U.S. Foreign Broad cast Information Service (FBIS) that had previously been used in the MUC-3 evaluation of natural language processing systems <ref> [Sun91] </ref>. The documents are mostly translated from Spanish, and include newspaper stories, transcripts of broadcasts, communiques, and other material. Documents were represented by a set of 8,876 binary features corresponding to English words occurring in 2 or more training documents. The MUC-3 text was all capitalized.
Reference: [Utg86] <author> Utgoff, Paul E. </author> <title> Shift of bias for inductive concept learning. </title> <editor> In Michalski, Ryszard S., Car-bonell, Jaime G., and Mitchell, Tom M., editors, </editor> <booktitle> Machine Learning. An Artificial Intelligence Approach. </booktitle> <volume> Volume II, </volume> <pages> pages 107-148. </pages> <publisher> Morgan Kauf-mann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year> <editor> [van79] van Rijsbergen, C. J. </editor> <booktitle> Information Retrieval. </booktitle> <publisher> Butterworths, </publisher> <address> London, </address> <note> second edition, </note> <year> 1979. </year>
Reference-contexts: It also means that DT-min10 that can induce any discrete probability distribution over the feature space, and unlike PropBayes will converge to the optimal categorizer (for a given feature set) in the limit of an infinite well-distributed training set. If we consider the tendencies or bias <ref> [Utg86] </ref> of the algorithms instead of asymptotic behavior, we see that DT-min10 most succintly represents distributions where conditional probabilities are uniform except as specifiable by conjunctions of a small number of features.
Reference: [VS87] <author> Vleduts-Stokolov, Natasha. </author> <title> Concept recognition in an automatic text-processing system for the life sciences. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 38 </volume> <pages> 269-287, </pages> <year> 1987. </year>
Reference-contexts: Other systems take advantage of multi-word phrases, positional or linguistic structure, or other information. There have been two main approaches to the construction of text categorization systems. First, a number of systems <ref> [VS87, Har88a, HW90] </ref> have embodied approaches similar to those used in expert systems for classification or diagnosis [Cla85].
References-found: 26

