URL: http://s2k-ftp.cs.berkeley.edu:8000/postgres/papers/S2K-94-49.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/mariposa/papers.html
Root-URL: 
Title: AN ECONOMIC PARADIGM FOR QUERY PROCESSING AND DATA MIGRATION IN MARIPOSA  
Author: Michael Stonebraker, Robert Devine, Marcel Kornacker, Witold Litwin, Avi Pfeffer, Adam Sah, and Carl Staelin 
Address: Berkeley, California 94720  
Affiliation: Computer Science Div., Dept. of EECS University of California  
Abstract: In this paper we explore query execution and storage management issues for Mariposa, a distributed data base system under construction at Berkeley. Because of the extreme complexity of both issues, we have adopted an underlying economic paradigm for both problems. Hence, queries receive a budget which they spend to obtain their answers, and each processing site attempts to maximize income by buying and selling storage objects and processing queries for locally stored objects. This paper presents the protocols which underlie this economic system. 
Abstract-found: 1
Intro-found: 1
Reference: [BERN81] <author> Bernstein, P. A., Goodman, N., Wong, E., Reeve, C. L. and Rothnie, J. </author> <title> Query Processing in a System for Distributed Databases (SDD-1), </title> <journal> ACM Trans. on Database Sys. </journal> <volume> 6(4), </volume> <month> Dec. </month> <year> 1981. </year>
Reference-contexts: Box 10490, Palo Alto, CA 94303. This research was sponsored by the National Science Foundation under grant IRI-9107455, the Defense Advanced Research Projects Agency under contract DABT63-92-C-0007, and the Army Research Office under grant DAAL03-91-G-0183. (2) Support data mobility. Previous distributed database systems (e.g., <ref> [WILL81, BERN81, LITW82, STON86] </ref>) and distributed storage managers (e.g., [HOWA88]) have all assumed that each storage object had a fixed home to which it is returned upon system quiescence.
Reference: [BERN83] <author> Bernstein, P. and Goodman, N., </author> <title> The Failure and Recovery Problem for Replicated Databases, </title> <booktitle> Proc. 1983 ACM Symp. on Principles of Distributed Computing, </booktitle> <address> Montreal, Quebec, </address> <month> Aug. </month> <year> 1983. </year>
Reference-contexts: As a result, the caching of objects in client memory yields transient copies of storage objects. Alternately, traditional distributed database systems implemented (or at least specified) support for permanent copies of database relations <ref> [WILL81, BERN83, ELAB85] </ref>. Our goal in Mariposa is to support both transient and permanent copies of storage fragments within a single framework. (7) Support autonomous site decisions. In a very large network, it is unreasonable to assume that any central entity has control over policy decisions at the local sites.
Reference: [CATT92] <author> Cattell, R. G. G. and Skeen, J., </author> <title> Object Operations Benchmark, </title> <journal> ACM Trans. on Database Sys. </journal> <volume> 17(1), </volume> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Alternately, distributed file systems and object-oriented database systems move the data a storage block at a time from a server to a client. As such, they implement a move the data to the query processing scenario. If there is high locality of reference (as in <ref> [CATT92] </ref>) then the latter policy is appropriate because the movement cost can be amortized over sev eral subsequent interactions. On the other hand, sending the query to the data is appropriate when low locality is observed.
Reference: [CHER89] <author> Cheriton, D. </author> <title> and Mann T.P., Decentralizing a Global Naming Service for Improved Performance and Fault Tolerance, </title> <journal> ACM Trans. on Comp. Sys. </journal> <volume> 7(2), </volume> <month> May </month> <year> 1989. </year>
Reference-contexts: Like other objects, a name context can also be named. In addition, like data fragments, it can be migrated between name servers and there can be multiple copies residing on different servers for better load balancing and availability. This scheme differs from another proposed decentralized name service <ref> [CHER89] </ref> that avoided a centralized name authority by relying upon each type of server to manage their own names without relying on a dedicated name service. 18 5.2. Name Resolution A name must be resolved to discover which object is bound to the name.
Reference: [COPE88] <author> Copeland, G., Alexander, W., Boughter, E. and Keller, T., </author> <title> Data Placement in Bubba, </title> <booktitle> Proc. 1988 ACM-SIGMOD Conf. on Management of Data, </booktitle> <address> Chicago, IL, </address> <month> Jun. </month> <year> 1988. </year>
Reference-contexts: Clearly, if there are too few fragments in a class, then parallel execution of Mariposa queries will be hindered. On the other hand, if there are too many fragments, then the overhead of dealing with all the fragments will increase and response time will suffer, as noted in <ref> [COPE88] </ref>. The algorithms for splitting and coalescing fragments must strike the correct balance between these two effects. 16 One strategy is to simply let market economics determine the sizes of fragments. Consider a frag-ment, F , which has high revenue.
Reference: [DOZI92] <author> Dozier, J., </author> <title> How Sequoia 2000 Addresses Issues in Data and Information Systems for Global Change, </title> <type> Sequoia 2000 Technical Report 92/14, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Moreover, in certain areas it alleviates common disadvantages of previous distributed storage systems. The goals of Mariposa are eight-fold: (1) Support a very large number of sites. Mariposa must be capable of dealing with several hundred sites (logical hosts) in a co-operating environment. For example, the Sequoia 2000 project <ref> [STON91, DOZI92] </ref> has around 200 sites with varying data storage needs and capabilities, mostly on the desktops of participating scientists. Other distributed databases may be substantially larger. For example, a group of cooperating retailers might want to share sales data.
Reference: [ELAB85] <author> El Abbadi, A. et al., </author> <title> An Efficient, Fault-Tolerant Protocol for Replicated Data Management, </title> <booktitle> Proc. ACM SIGMOD-SIGACT Symp. on Principles of Data Base Systems, </booktitle> <month> Apr. </month> <year> 1985. </year>
Reference-contexts: As a result, the caching of objects in client memory yields transient copies of storage objects. Alternately, traditional distributed database systems implemented (or at least specified) support for permanent copies of database relations <ref> [WILL81, BERN83, ELAB85] </ref>. Our goal in Mariposa is to support both transient and permanent copies of storage fragments within a single framework. (7) Support autonomous site decisions. In a very large network, it is unreasonable to assume that any central entity has control over policy decisions at the local sites.
Reference: [EPST78] <author> Epstein, R. S., Stonebraker, M. and Wong, E., </author> <title> Distributed Query Processing in Relational Database Systems, </title> <booktitle> Proc. 1978 ACM-SIGMOD Conf. on Management of Data, </booktitle> <address> Austin, TX, </address> <month> May </month> <year> 1978. </year>
Reference-contexts: Traditional distributed database systems operate by moving the query from a client site to the site where the object resides, and then moving the result of the query back to the client <ref> [EPST78, LOHM86] </ref>. (Temporary copies of an object may be created and moved during query processing, but only the database administrator can change where an object resides.) This implements a move the query to the data processing scenario.
Reference: [FERG93] <author> Ferguson, D., Nikolaou, C. and Yemini, Y., </author> <title> An Economy for Managing Replicated Data in Autonomous Decentralized Systems, </title> <booktitle> Proc. Int. Symp. on Autonomous Decentralized Sys. </booktitle> <address> (ISADS 93), Kaw asaki, Japan, </address> <year> 1993. </year>
Reference-contexts: Lastly, each processing site makes storage decisions to buy and sell fragments and copies of fragments, based on optimizing the revenue it collects. Our model is similar to <ref> [FERG93, WALD92, MALO88] </ref> which take similar economic approaches to other computer resource allocation problems. In the next section, we describe the three kinds of entities in our economic system. <p> Hence, the buyer can prefetch fragments which it expects will be profitable in the future. A possible improvement is to lease copies instead of selling them to the sites, as outlined in <ref> [FERG93] </ref>. The initial lease price is established along the same lines as a fixed offer price. The difference is that the lease is only valid for a particular period of time, the lease period, after which the lease contract has to be renewed. <p> His optimality results are clearly invalidated by the possible exclusion of optimal bidders, suggesting the importance of high-quality name service, to ensure that the winning bidders are usually solicited for bids. A model similar to ours is proposed in <ref> [FERG93] </ref>, where fragments can be moved and replicated between the nodes of a network of computers, although they are not allowed to be split or coalesced. Transactions, consisting of simple read/write requests for fragments, are given a budget when entering the system.
Reference: [HONG91] <author> Hong, W. and Stonebraker, M., </author> <title> Optimization of Parallel Query Execution Plans in XPRS, </title> <booktitle> Proc. 1st Int. Conf. on Parallel and Distributed Info. Sys., </booktitle> <address> Miami Beach, FL, </address> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: Locally defined rules may affect how the subqueries are assigned to sites. Decomposing query plans in the manner just described greatly reduces optimizer complexity. Signs that the resulting plans may not be significantly suboptimal appear in <ref> [HONG91] </ref>, where a similar decomposition is studied. Decomposing the plan before distributing it also makes it easier to assign portions of the budget to subqueries. 5 Servers. Server sites provide a processor with varying amounts of persistent storage.
Reference: [HOWA88] <author> Howard, J. H., Kazar, M. L. Menees, S. G., Nichols, D. A., Satyanarayanan, M., Side--botham, R. N. and West, M. J., </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM Trans. on Comp. Sys. </journal> <volume> 6(1), </volume> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: This research was sponsored by the National Science Foundation under grant IRI-9107455, the Defense Advanced Research Projects Agency under contract DABT63-92-C-0007, and the Army Research Office under grant DAAL03-91-G-0183. (2) Support data mobility. Previous distributed database systems (e.g., [WILL81, BERN81, LITW82, STON86]) and distributed storage managers (e.g., <ref> [HOWA88] </ref>) have all assumed that each storage object had a fixed home to which it is returned upon system quiescence. Changing the home of an an object is a heavyweight operation that entails, for example, destroying and recreating all the indexes for that object.
Reference: [HUBE88] <author> Huberman, B. A. (ed.), </author> <title> The Ecology of Computation, </title> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: The quality of service is then a measurement of the metadata's rate of update rather than the name server's rate of update. 6. RELATED WORK So far there are only a few systems documented in the literature which incorporate microeconomic approaches to deal with resource sharing problems. <ref> [HUBE88] </ref> contains a collection of articles that cover the underlying principles and explore the behavior of those systems. [MILL88] uses the term Agoric Systems for software systems deploying market mechanisms for resource allocation among independent objects. The data-type agents proposed in that article are comparable to our brokers.
Reference: [KURO89] <author> Kurose, J. and Simha, R., </author> <title> A Microeconomic Approach to Optimal Resource Allocation in Distributed Computer Systems, </title> <journal> IEEE Trans. on Computers 38(5), </journal> <month> May </month> <year> 1989. </year>
Reference-contexts: As an extension, agents have a reputation and their services are brokered by an agent-selection agent. This is analogous to the notion of a quality-of-service of name servers, that also offer their services to brokers. <ref> [KURO89] </ref> present a solution to the file allocation problem that makes use of microeconomic principles, but is based on a cooperative, not competitive environment. The agents in this economy exchange fragments in order to minimize the cumulative system-wide access costs for all incoming requests.
Reference: [LAMP86] <author> Lampson, B., </author> <title> Designing a Global Name Service, </title> <booktitle> Proc. ACM Symp. on Principles of Distributed Computing, </booktitle> <address> Calgary, Canada, </address> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: All full names are globally unique within the name space however the policy for selecting names is locally defined. So as not to constrain the later growth of the name space from the amalgamation of other name spaces, a non-fixed-root name space as suggested in <ref> [LAMP86] </ref> can be used to support upwards growth beyond the current root. Finally, a name context is a set of names that are affiliated.
Reference: [LITW82] <author> Litwin, W. et al., </author> <title> SIRIUS System for Distributed Data Management, in Distributed Databases, </title> <editor> H. J. Schneider (ed.), </editor> <publisher> North-Holland, </publisher> <year> 1982. </year>
Reference-contexts: Box 10490, Palo Alto, CA 94303. This research was sponsored by the National Science Foundation under grant IRI-9107455, the Defense Advanced Research Projects Agency under contract DABT63-92-C-0007, and the Army Research Office under grant DAAL03-91-G-0183. (2) Support data mobility. Previous distributed database systems (e.g., <ref> [WILL81, BERN81, LITW82, STON86] </ref>) and distributed storage managers (e.g., [HOWA88]) have all assumed that each storage object had a fixed home to which it is returned upon system quiescence.
Reference: [LOHM86] <author> Mackert, L. F. and Lohman, G. M., </author> <title> R* Optimizer Validation and Performance Evaluation for Local Queries, </title> <booktitle> Proc. 1986 ACM-SIGMOD Conf. on Management of Data, </booktitle> <address> Washing-ton, DC, </address> <month> May </month> <year> 1986. </year>
Reference-contexts: Traditional distributed database systems operate by moving the query from a client site to the site where the object resides, and then moving the result of the query back to the client <ref> [EPST78, LOHM86] </ref>. (Temporary copies of an object may be created and moved during query processing, but only the database administrator can change where an object resides.) This implements a move the query to the data processing scenario. <p> Moreover, it must convert site-independent resource usage numbers into ones specific to its site through a weighting function, as in <ref> [LOHM86] </ref>. In addition, it must assume that it would have successfully bid on the same set of queries as appeared in the revenue history. Since it will be faster or slower than the site from which the revenue history was collected, it must adjust the revenue collected for each query.
Reference: [MALO88] <author> Malone, T. W., Fikes, R. E., Grant, K. R. and Howard, M. T., </author> <title> Enterprise: A Market-like Task Scheduler for Distributed Computing Environments, in The Ecology of Computation, B.A. </title> <editor> Huberman (ed.), </editor> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: Lastly, each processing site makes storage decisions to buy and sell fragments and copies of fragments, based on optimizing the revenue it collects. Our model is similar to <ref> [FERG93, WALD92, MALO88] </ref> which take similar economic approaches to other computer resource allocation problems. In the next section, we describe the three kinds of entities in our economic system. <p> This is achieved by having the sites voluntarily cede fragments or portions thereof to other sites if it lowers access costs. In this model, all sites cooperate to achieve a global optimum instead of competing for resources to selfishly maximize their own utility. <ref> [MALO88] </ref> describes the implementation of a process migration facility for a pool of workstations connected through a LAN. In this system, a client broadcasts a request for bids that includes a task description.
Reference: [MILL88] <author> Miller, M. S. and Drexler, K. E., </author> <title> Markets and Computation: Agoric Open Systems, in The Ecology of Computation, B.A. </title> <editor> Huberman (ed.), </editor> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: RELATED WORK So far there are only a few systems documented in the literature which incorporate microeconomic approaches to deal with resource sharing problems. [HUBE88] contains a collection of articles that cover the underlying principles and explore the behavior of those systems. <ref> [MILL88] </ref> uses the term Agoric Systems for software systems deploying market mechanisms for resource allocation among independent objects. The data-type agents proposed in that article are comparable to our brokers. They mediate between consumer and supplier objects, helping to find the currently best price and supplier for a needed service.
Reference: [SELI79] <author> Selinger, P. G., Astrahan, M. M., Chamberlin, D. D., Lorie, R. A. and Price, T. G., </author> <title> Access Path Selection in a Relational Database Management System, </title> <booktitle> Proc. 1979 ACM-SIGMOD Conf. on Management of Data, </booktitle> <address> Boston, MA, </address> <month> June </month> <year> 1979. </year>
Reference-contexts: After successful parsing, the broker prepares a query execution plan. This is a two-step process. First, a conventional query optimizer along the lines of <ref> [SELI79] </ref> generates a single site query execution plan by assuming that all the fragments are merged together and reside at a single server site.
Reference: [STON86] <author> Stonebraker, M., </author> <title> The Design and Implementation of Distributed INGRES, in The INGRES Papers, </title> <editor> M. Stonebraker (ed.), </editor> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Box 10490, Palo Alto, CA 94303. This research was sponsored by the National Science Foundation under grant IRI-9107455, the Defense Advanced Research Projects Agency under contract DABT63-92-C-0007, and the Army Research Office under grant DAAL03-91-G-0183. (2) Support data mobility. Previous distributed database systems (e.g., <ref> [WILL81, BERN81, LITW82, STON86] </ref>) and distributed storage managers (e.g., [HOWA88]) have all assumed that each storage object had a fixed home to which it is returned upon system quiescence.
Reference: [STON91a] <author> Stonebraker, M. and Kemnitz, G., </author> <title> The POSTGRES Next-Generation Database Management System, </title> <journal> Comm. of the ACM 34(10), </journal> <month> Oct. </month> <year> 1991. </year>
Reference: [STON91b] <author> Stonebraker, M., </author> <title> An Overview of the Sequoia 2000 Project, </title> <type> Sequoia 2000 Technical Report 91/5, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> Dec. </month> <year> 1991. </year>
Reference: [STON94] <author> Stonebraker, M., Aoki, P. M., Devine, R., Litwin, W. and Olson, M., Mariposa: </author> <title> A New Architecture for Distributed Data, </title> <booktitle> Proc. 10th Int. Conf. on Data Engineering, </booktitle> <address> Houston, TX, </address> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: 1. INTRODUCTION In <ref> [STON94] </ref> we presented the design of a new distributed database and storage system, called Mariposa. This system combines the best features of traditional distributed database systems, object-oriented DBMSs, tertiary memory file systems and distributed file systems. Moreover, in certain areas it alleviates common disadvantages of previous distributed storage systems. <p> This parallelizes the single site plan produced from the first step. The details of this fragmentation process are described in <ref> [STON94] </ref>.
Reference: [WALD92] <author> Waldspurger, C. A., Hogg, T., Huberman, B., Kephart, J. and Stornetta, S., Spawn: </author> <title> A Distributed Computational Ecology, </title> <journal> IEEE Trans. on Software Engineering 18(2), </journal> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: Lastly, each processing site makes storage decisions to buy and sell fragments and copies of fragments, based on optimizing the revenue it collects. Our model is similar to <ref> [FERG93, WALD92, MALO88] </ref> which take similar economic approaches to other computer resource allocation problems. In the next section, we describe the three kinds of entities in our economic system. <p> No prices are charged for processing services and there is no provision for a shortcut to the bidding process by mechanisms like posting server characteristics or advertisements of servers. Another distributed process scheduling system is presented in <ref> [WALD92] </ref>. Here, CPU time on remote machines is auctioned off by the processing sites and applications hand in bids for time slices. This is is contrast to our system, where processing sites make bids for servicing requests.
Reference: [WELL93] <author> Wellman, M. P. </author> <title> A Market-Oriented Programming Environment and Its Applications to Distributed Multicommodity Flow Problems, </title> <journal> Journal of AI Research 1(1), </journal> <month> Aug. </month> <year> 1993. </year> <month> 23 </month>
Reference-contexts: The managers are responsible for funding their workers and divide the available funds between them in an application-specific way. To adjust the degree of parallelism to the 20 availability of idle CPUs, the manager changes the funding of individual workers. Wellman offers a simulation of multicommodity ow in <ref> [WELL93] </ref> that is quite close to our bidding model, but with a bid resolution model that convergies with multiple rounds of messages. His clearinghouses violate our constraint against single points of failure; hence, Mariposa name service can be though of as clearinghouses with only a partial list of possible suppliers.
Reference: [WILL81] <author> Williams, R., Daniels, D., Haas, L., Lapis, G., Lindsay, B., Ng, P., Obermarck, R., Selinger, P., Walker, A., Wilms, P. and Yost, R., </author> <title> R*: An Overview of the Architecture, </title> <institution> IBM Research Report RJ3325, IBM Research Laboratory, </institution> <address> San Jose, CA, </address> <month> Dec. </month> <year> 1981. </year> <month> 24 </month>
Reference-contexts: Box 10490, Palo Alto, CA 94303. This research was sponsored by the National Science Foundation under grant IRI-9107455, the Defense Advanced Research Projects Agency under contract DABT63-92-C-0007, and the Army Research Office under grant DAAL03-91-G-0183. (2) Support data mobility. Previous distributed database systems (e.g., <ref> [WILL81, BERN81, LITW82, STON86] </ref>) and distributed storage managers (e.g., [HOWA88]) have all assumed that each storage object had a fixed home to which it is returned upon system quiescence. <p> As a result, the caching of objects in client memory yields transient copies of storage objects. Alternately, traditional distributed database systems implemented (or at least specified) support for permanent copies of database relations <ref> [WILL81, BERN83, ELAB85] </ref>. Our goal in Mariposa is to support both transient and permanent copies of storage fragments within a single framework. (7) Support autonomous site decisions. In a very large network, it is unreasonable to assume that any central entity has control over policy decisions at the local sites. <p> The advantage of a name context is that names do not have to be globally registered nor are the names tied to a physical resources to make them unique such as birth site as in <ref> [WILL81] </ref>. Like other objects, a name context can also be named. In addition, like data fragments, it can be migrated between name servers and there can be multiple copies residing on different servers for better load balancing and availability.
References-found: 26

