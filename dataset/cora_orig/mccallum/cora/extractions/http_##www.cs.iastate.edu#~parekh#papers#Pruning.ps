URL: http://www.cs.iastate.edu/~parekh/papers/Pruning.ps
Refering-URL: http://www.cs.iastate.edu/~yang/publications.html
Root-URL: 
Email: fparekhjyangjhonavarg@cs.iastate.edu  
Title: Pruning Strategies for the MTiling Constructive Learning Algorithm  
Author: Rajesh Parekh, Jihoon Yang, and Vasant Honavar 
Address: Ames IA 50011. U.S.A.  
Affiliation: Artificial Intelligence Research Group Department of Computer Science Iowa State University  
Abstract: We present a framework for incorporating pruning strategies in the MTiling constructive neural network learning algorithm. Pruning involves elimination of redundant elements (connection weights or neurons) from a network and is of considerable practical interest. We describe three elementary sensitivity based strategies for pruning neurons. Experimental results demonstrate a moderate to significant reduction in the network size without compromising the network's generalization performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Burgess. </author> <title> A constructive algorithm that converges for real-valued input patterns. </title> <journal> International Journal of Neural Systems, </journal> <volume> 5(1):5966, </volume> <year> 1994. </year>
Reference-contexts: Several constructive learning algorithms have been proposed in the literature Tower, Pyramid [4], Tiling [9], Upstart [2], Perceptron Cascade <ref> [1] </ref>, and Sequential Learning [8].
Reference: [2] <author> M. Frean. </author> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <booktitle> Neural Computation, </booktitle> <address> 2:198209, </address> <year> 1990. </year>
Reference-contexts: Several constructive learning algorithms have been proposed in the literature Tower, Pyramid [4], Tiling [9], Upstart <ref> [2] </ref>, Perceptron Cascade [1], and Sequential Learning [8].
Reference: [3] <author> M. Frean. </author> <title> A thermal perceptron learning rule. </title> <booktitle> Neural Computation, </booktitle> <address> 4:946957, </address> <year> 1992. </year>
Reference-contexts: Algorithms such as the Pocket algorithm with ratchet modification [4], the Thermal perceptron algorithm <ref> [3] </ref>, and the Barycentric correction procedure [12] are commonly used for training individual TLUs (or groups of TLUs) in constructive algorithms. We denote such a suitable TLU training algorithm by A. An exhaustive search through the space of neural network architectures is computationally infeasible.
Reference: [4] <author> S. Gallant. </author> <title> Perceptron based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2):179191, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: 1. Introduction Constructive neural network learning algorithms offer an interesting paradigm for incremental construction of near minimal architectures for pattern classification problems <ref> [4, 5, 6, 7] </ref>. Traditional algorithms for multi-layer feed forward neural network training (such as the backpropagation [14]) search for an appropriate weight setting using a gradient descent based error minimization in an a-priori fixed network architecture. <p> Several constructive learning algorithms have been proposed in the literature Tower, Pyramid <ref> [4] </ref>, Tiling [9], Upstart [2], Perceptron Cascade [1], and Sequential Learning [8]. <p> Algorithms such as the Pocket algorithm with ratchet modification <ref> [4] </ref>, the Thermal perceptron algorithm [3], and the Barycentric correction procedure [12] are commonly used for training individual TLUs (or groups of TLUs) in constructive algorithms. We denote such a suitable TLU training algorithm by A. An exhaustive search through the space of neural network architectures is computationally infeasible.
Reference: [5] <author> S. Gallant. </author> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: 1. Introduction Constructive neural network learning algorithms offer an interesting paradigm for incremental construction of near minimal architectures for pattern classification problems <ref> [4, 5, 6, 7] </ref>. Traditional algorithms for multi-layer feed forward neural network training (such as the backpropagation [14]) search for an appropriate weight setting using a gradient descent based error minimization in an a-priori fixed network architecture.
Reference: [6] <author> V. Honavar. </author> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1990. </year>
Reference-contexts: 1. Introduction Constructive neural network learning algorithms offer an interesting paradigm for incremental construction of near minimal architectures for pattern classification problems <ref> [4, 5, 6, 7] </ref>. Traditional algorithms for multi-layer feed forward neural network training (such as the backpropagation [14]) search for an appropriate weight setting using a gradient descent based error minimization in an a-priori fixed network architecture.
Reference: [7] <author> V. Honavar and L. Uhr. </author> <title> Generative learning structures and processes for connectionist networks. </title> <journal> Information Sciences, </journal> <volume> 70:75108, </volume> <year> 1993. </year>
Reference-contexts: 1. Introduction Constructive neural network learning algorithms offer an interesting paradigm for incremental construction of near minimal architectures for pattern classification problems <ref> [4, 5, 6, 7] </ref>. Traditional algorithms for multi-layer feed forward neural network training (such as the backpropagation [14]) search for an appropriate weight setting using a gradient descent based error minimization in an a-priori fixed network architecture.
Reference: [8] <author> M. Marchand, M. Golea, and P. Rujan. </author> <title> A convergence theorem for sequential learning in two-layer perceptrons. </title> <journal> Euro-physics Letters, </journal> <volume> 11(6):487492, </volume> <year> 1990. </year>
Reference-contexts: Several constructive learning algorithms have been proposed in the literature Tower, Pyramid [4], Tiling [9], Upstart [2], Perceptron Cascade [1], and Sequential Learning <ref> [8] </ref>.
Reference: [9] <author> M. Mezard and J. Nadal. </author> <title> Learning feed-forward networks: The tiling algorithm. </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> 22:21912203, </volume> <year> 1989. </year>
Reference-contexts: Several constructive learning algorithms have been proposed in the literature Tower, Pyramid [4], Tiling <ref> [9] </ref>, Upstart [2], Perceptron Cascade [1], and Sequential Learning [8]. <p> Section 3 describes pruning strategies for eliminating unwanted neurons from a MTiling network. Section 4 presents the results of experiments. Finally, section 5 concludes with an analysis of the experiments with pruning and suggestions for future research. 2. The MTiling Algorithm The 2-category Tiling algorithm <ref> [9] </ref> constructs a strictly layered network of TLUs. The bottom-most layer receives inputs from each of the N input neurons. The neurons in each subsequent layer receive inputs from the neurons in the layer immediately below itself. <p> The faithfulness criterion states that no two training examples belonging to different target classes should produce identical outputs at any given layer. Faithfulness is a necessary condition for convergence in strictly layered networks <ref> [9] </ref>. The Tiling algorithm is guaranteed to converge to zero classification errors (under certain assumptions) for finite, noncontradictory datasets where the pattern attributes are bipolar valued (i.e., 1 or 1 only).
Reference: [10] <author> R. G. Parekh, J. Yang, and V. G. Honavar. </author> <title> Constructive neural network learning algorithms for multi-category classification. </title> <type> Technical Report ISU-CS-TR95-15a, </type> <institution> Department of Computer Science, Iowa State University, </institution> <year> 1995. </year>
Reference-contexts: These differences in design choices result in constructive learning algorithms with different representational and inductive biases. Provably correct and practical extensions of these algorithms to handle real-valued pattern attributes and multiple output categories are studied in <ref> [10, 11, 15] </ref>. <p> Pruning can be performed either after the entire network is trained or can be integrated into the training process itself. In this paper we study the application of pruning techniques to MTiling , an extension of the Tiling algorithm to handle real-valued pattern attributes and multiple output classes <ref> [10, 11, 15] </ref>. The remainder of this paper is organized as follows: Section 2 outlines the MTiling constructive neural network learning algorithm. Section 3 describes pruning strategies for eliminating unwanted neurons from a MTiling network. Section 4 presents the results of experiments.
Reference: [11] <author> R. G. Parekh, J. Yang, and V. G. Honavar. </author> <title> Constructive neural network learning algorithms for multi-category real-valued pattern classification. </title> <type> Technical Report ISU-CS-TR97-06, </type> <institution> Department of Computer Science, Iowa State University, </institution> <year> 1997. </year>
Reference-contexts: These differences in design choices result in constructive learning algorithms with different representational and inductive biases. Provably correct and practical extensions of these algorithms to handle real-valued pattern attributes and multiple output categories are studied in <ref> [10, 11, 15] </ref>. <p> Pruning can be performed either after the entire network is trained or can be integrated into the training process itself. In this paper we study the application of pruning techniques to MTiling , an extension of the Tiling algorithm to handle real-valued pattern attributes and multiple output classes <ref> [10, 11, 15] </ref>. The remainder of this paper is organized as follows: Section 2 outlines the MTiling constructive neural network learning algorithm. Section 3 describes pruning strategies for eliminating unwanted neurons from a MTiling network. Section 4 presents the results of experiments. <p> It is then demonstrated that with each additional layer the number of classification errors is reduced by at least one. Since the number of training patterns is finite, the total number of classification errors made by the MTiling network will eventually become zero. The reader is referred to <ref> [11] </ref> for the detailed convergence proof. 3. Pruning Strategies An excellent survey of neural network pruning strategies appears in [13]. Reed outlines two types of pruning techniques sensitivity calculations and penalty terms for feed-forward neural networks trained using the backpropagation algorithm.
Reference: [12] <author> H. Poulard. </author> <title> Barycentric correction procedure: A fast method of learning threshold units. </title> <booktitle> In Proceedings of WCNN'95, </booktitle> <address> July 17-21, Washington D.C., </address> <booktitle> volume 1, </booktitle> <pages> pages 710713, </pages> <year> 1995. </year>
Reference-contexts: Algorithms such as the Pocket algorithm with ratchet modification [4], the Thermal perceptron algorithm [3], and the Barycentric correction procedure <ref> [12] </ref> are commonly used for training individual TLUs (or groups of TLUs) in constructive algorithms. We denote such a suitable TLU training algorithm by A. An exhaustive search through the space of neural network architectures is computationally infeasible.
Reference: [13] <author> R. Reed. </author> <title> Pruning algorithms a survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5):740747, </volume> <year> 1993. </year>
Reference-contexts: Since the number of training patterns is finite, the total number of classification errors made by the MTiling network will eventually become zero. The reader is referred to [11] for the detailed convergence proof. 3. Pruning Strategies An excellent survey of neural network pruning strategies appears in <ref> [13] </ref>. Reed outlines two types of pruning techniques sensitivity calculations and penalty terms for feed-forward neural networks trained using the backpropagation algorithm. The former investigates the sensitivity of the error function to the removal of a network element. Elements with the least sensitivity are pruned.
Reference: [14] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. In Parallel Distributed Processing: Explorations into the Microstruc-ture of Cognition, </title> <booktitle> volume 1 (Foundations). </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: 1. Introduction Constructive neural network learning algorithms offer an interesting paradigm for incremental construction of near minimal architectures for pattern classification problems [4, 5, 6, 7]. Traditional algorithms for multi-layer feed forward neural network training (such as the backpropagation <ref> [14] </ref>) search for an appropriate weight setting using a gradient descent based error minimization in an a-priori fixed network architecture. Constructive algorithms grow the network incrementally by training one or more threshold logic units (TLUs or neurons) and integrating these trained neurons into the existing network architecture.
Reference: [15] <author> J. Yang, R. G. Parekh, and V. G. Honavar. </author> <title> Mtiling a constructive neural network learning algorithm for multi-category pattern classification. </title> <booktitle> In Proceedings of the World Congress on Neural Networks'96, </booktitle> <pages> pages 182187, </pages> <address> San Diego, </address> <year> 1996. </year>
Reference-contexts: These differences in design choices result in constructive learning algorithms with different representational and inductive biases. Provably correct and practical extensions of these algorithms to handle real-valued pattern attributes and multiple output categories are studied in <ref> [10, 11, 15] </ref>. <p> Pruning can be performed either after the entire network is trained or can be integrated into the training process itself. In this paper we study the application of pruning techniques to MTiling , an extension of the Tiling algorithm to handle real-valued pattern attributes and multiple output classes <ref> [10, 11, 15] </ref>. The remainder of this paper is organized as follows: Section 2 outlines the MTiling constructive neural network learning algorithm. Section 3 describes pruning strategies for eliminating unwanted neurons from a MTiling network. Section 4 presents the results of experiments.
References-found: 15

