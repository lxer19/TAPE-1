URL: http://www.cs.colostate.edu/~vision/ps/1997/CVIP97.ps.gz
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: Precise Matching of 3-D Target Models to Multisensor Data  
Author: Mark R. Stevens and J. Ross Beveridge 
Note: This work was sponsored by the Defense Advanced Research Projects Agency (DARPA) Image Understanding Program under grants DAAH04-93-G- 422 and DAAH04-95-1-0447, monitored by the U. S. Army Research Office, and the National Science Foundation under grants CDA-9422007 and IRI- 9503366  
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523-1873  
Date: January 20, 1997  
Affiliation: Computer Science  Computer Science Department Colorado State University  
Pubnum: Technical Report  Technical Report CS-96-122  
Abstract: y This paper appears in the IEEE Transactions on Image Processing, January 1997. c fl 1996 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. z This material is presented electronically to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors and by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. K. Aggarwal. </author> <title> Multisensor Fusion for Automatic Scene Interpretation. </title> <editor> In Ramesh C. Jain and Anil K. Jain, editors, </editor> <title> Analysis and Interpretation of Range Images, chapter 8. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Many researchers have contributed to the following areas that our work addresses: pose-determination, feature prediction, match evaluation and optimization. Related work for each topic is cited within the sections describing our contributions. On the general topic of sensor fusion, Aggarwal <ref> [1] </ref> nicely summarizes past work and notes that typically sensor fusion has emphasized single modality sensors, with comparatively little work on different 2 Non-stationary here indicates that the quality of a state in the search space may depend upon the path taken to arrive at that state. <p> A weighting term is created to scale the edge strength at each pixel, and to obtain the edge strength for a single line. The cumulative strength, ^ G, for the line segment k is normalized to lie in the range <ref> [0; 1] </ref>: ^ G (k) = i=X a j=Y a Gradient (i; j) w (i; j) X b X Y b X w (i; j) where Gradient (i; j) is the target edge strength discussed earlier (normalized to [0; 1]) for pixel (i; j), and w (i; j) is a weighting <p> for the line segment k is normalized to lie in the range <ref> [0; 1] </ref>: ^ G (k) = i=X a j=Y a Gradient (i; j) w (i; j) X b X Y b X w (i; j) where Gradient (i; j) is the target edge strength discussed earlier (normalized to [0; 1]) for pixel (i; j), and w (i; j) is a weighting term proportional to the distance of the pixel from the true line. The weight w (i; j) is 0 for pixels lying outside the radius of 1:5 pixels from the line segment. <p> Because ^ G (k) is in the range <ref> [0; 1] </ref>, the error term remains normalized to the same range. An annealing schedule is then used as the matching algorithm executes so that over time the system looks for lines with stronger and stronger gradient support. <p> Similar to the optical threshold, the t value is also initially set to a large value and then "cooled" over time. The total fitness for the range sensor is then summed over the matched points and normalized to lie in the range <ref> [0; 1] </ref>. Normalization takes account of the number of matched points, p, and the maximum allowable distance t : E fit;R (F ) = i2 p t 5.3 Omission Error for All Sensors Omission accounts for weak responses in optical and unmatched points in range.
Reference: [2] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Object Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachusetts at Amherst, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Robust optimization to find locally optimal multisensor matches. The match evaluation function is non-- differentiable and, because new target features are predicted during search, non-stationary 2 . Traditional gradient descent methods are not applicable for minimizing such a function. A new form of local search <ref> [25, 27, 36, 2] </ref>, more specifically a variant on Tabu Search [15], has been developed for this task. While our work focuses specifically on the RSTA multisensor ATR problem, many of the advances described in this paper can be adapted and applied to other model-based object recognition problems. <p> A detailed explanation of why sensor translation may be used to compensate for small changes in pointing angles appears in [23]. 1.2 Related Work Model-based object recognition work has long emphasized the importance of aligning 3D object models to features extracted from sensed imagery <ref> [7, 30, 18, 22, 2] </ref>. While model-based approaches to Automatic Target Recognition have become much more common [14], direct incorporation of alignment into the recognition process is rare [6]. <p> These linear features, in turn, may be matched to linear features of stored object models [29, 22, 18, 5]. These bottom-up feature extraction algorithms are prone to error [13, 3], and often produce extraneous line segments, fragmented segments, and sometimes over-grouped segments. Our past work in other problem domains <ref> [2] </ref> demonstrated that local search, coupled with sound and efficient tests of global alignment, could overcome significant amounts of fragmentation, over-grouping and clutter. <p> The parameter ff introduces a non-linear bias which essentially reduces the penalty for small amounts of omission while increasing the penalty for large amounts of omission. A detailed explanation of this relationship may be found in <ref> [2] </ref>.
Reference: [3] <author> J. Ross Beveridge, Joey Griffith, Ralf R. Kohler, Allen R. Hanson, and Edward M. Riseman. </author> <title> Segmenting images using localized histograms and region merging. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(3):311 - 347, </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: Local edges [34, 20] may be grouped into larger features such as straight line segments [9, 31]. These linear features, in turn, may be matched to linear features of stored object models [29, 22, 18, 5]. These bottom-up feature extraction algorithms are prone to error <ref> [13, 3] </ref>, and often produce extraneous line segments, fragmented segments, and sometimes over-grouped segments. Our past work in other problem domains [2] demonstrated that local search, coupled with sound and efficient tests of global alignment, could overcome significant amounts of fragmentation, over-grouping and clutter.
Reference: [4] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: While Aggarwal [32] and others [39] have examples of successful mixed-modality fusion, this is still a young research area. 2 The Fort Carson Range, IR and Color Dataset. Our algorithms are tested on multisensor images collected at Fort Carson <ref> [4] </ref> in 1993. The entire collection contains over 30 range, IR and color image triples which are publicly available through our web site 3 . The range data was acquired using a LADAR built by Rathyeon and owned by Alliant TechSystems in Minnesota.
Reference: [5] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Optimal Geometric Model Matching Under Full 3D Perspective. Computer Vision and Image Understanding, </title> <note> 61(3):351 - 364, 1995. (short version in IEEE Second CAD-Based Vision Workshop). </note>
Reference-contexts: Traditional methods for locating objects in optical imagery typically use edge detection algorithms. Local edges [34, 20] may be grouped into larger features such as straight line segments [9, 31]. These linear features, in turn, may be matched to linear features of stored object models <ref> [29, 22, 18, 5] </ref>. These bottom-up feature extraction algorithms are prone to error [13, 3], and often produce extraneous line segments, fragmented segments, and sometimes over-grouped segments.
Reference: [6] <author> James E. Bevington. </author> <title> Laser Radar ATR Algorithms: Phase III Final Report. </title> <type> Technical report, </type> <institution> Alliant Techsystems, Inc., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: While model-based approaches to Automatic Target Recognition have become much more common [14], direct incorporation of alignment into the recognition process is rare <ref> [6] </ref>. The work reported here is the first such attempt of which we are aware for the case of multiple, heterogeneous, ground-looking sensors. Many researchers have contributed to the following areas that our work addresses: pose-determination, feature prediction, match evaluation and optimization. <p> The detection information is then passed to a target type and pose hypothesis phase which generates a list of possible target types and orientations. This hypothesis generation algorithm uses boundary template matching in the range imagery <ref> [6] </ref>. Finally, for each hypothesized target, multisensor matching uses an iterative improvement optimization scheme to develop a best match between target features and the multisensor imagery. phases of the ATR process. <p> the ROI of interest provided by the detection phase and Figure 5b shows the associated likelihood map. 3.2 Target Type and Pose Hypothesis Generation Once the ROIs are determined, they are fed into the template matching algorithm which compares stored templates of the different CAD models against the range data <ref> [6] </ref>. The center of the ROI is converted to a range pixel, and the templates are then applied about that point. A score measuring the percentage of probes matched is used to rank each template according to how well it fits the data.
Reference: [7] <author> R. C. Bolles and R. A. Cain. </author> <title> Recognizing and Locating Partially Visible Objects: The Local-Feature-Focus Method. </title> <journal> International Journal of Robotics Research, </journal> <volume> 1(3):57 - 82, </volume> <year> 1982. </year>
Reference-contexts: A detailed explanation of why sensor translation may be used to compensate for small changes in pointing angles appears in [23]. 1.2 Related Work Model-based object recognition work has long emphasized the importance of aligning 3D object models to features extracted from sensed imagery <ref> [7, 30, 18, 22, 2] </ref>. While model-based approaches to Automatic Target Recognition have become much more common [14], direct incorporation of alignment into the recognition process is rare [6].
Reference: [8] <author> Shashi Buluswar, Bruce A. Draper, Allen Hanson, and Edward Riseman. </author> <title> Non-parametric Classification of Pixels Under Varying Outdoor Illumination. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 16191626, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This queuing is provided by two upstream processes: 1) target detection and 2) target type and pose hypothesis generation. Detection uses color imagery to predict targets based upon the color characteristics of the camouflaged vehicles <ref> [8] </ref>. The detection information is then passed to a target type and pose hypothesis phase which generates a list of possible target types and orientations. This hypothesis generation algorithm uses boundary template matching in the range imagery [6]. <p> That said, in order to better understand the functioning of the complete system, all three components are briefly summarized. 3.1 Target Detection The detection algorithm was developed at the University of Massachusetts <ref> [8] </ref>. Using training imagery, it learns to discriminate between color values produced by camouflaged vehicles and values produced by background terrain. 4 Actual Range is 50 meters. However range-to-targets are unusually short to accommodate the short operating range of the older LADAR.
Reference: [9] <author> J. B. Burns, A. R. Hanson, and E. M. Riseman. </author> <title> Extracting straight lines. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-8(4):425 - 456, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: The fitness terms are formed by examining the estimated gradient magnitude underlying each predicted model line. Traditional methods for locating objects in optical imagery typically use edge detection algorithms. Local edges [34, 20] may be grouped into larger features such as straight line segments <ref> [9, 31] </ref>. These linear features, in turn, may be matched to linear features of stored object models [29, 22, 18, 5]. These bottom-up feature extraction algorithms are prone to error [13, 3], and often produce extraneous line segments, fragmented segments, and sometimes over-grouped segments. <p> Our past work in other problem domains [2] demonstrated that local search, coupled with sound and efficient tests of global alignment, could overcome significant amounts of fragmentation, over-grouping and clutter. However, in this domain we find the features produced by the Burns algorithm <ref> [9] </ref> are of such poor quality that a top-down rather than a bottom-up approach is preferable. Bottom-up feature extraction is hindered by low resolution, highly textured backgrounds and targets, and finally by similar colors appearing in both camouflage and background.
Reference: [10] <author> Jin-Long Chen and George C. Stockman. </author> <title> Determining pose of 3d objects with curved surfaces. </title> <type> Technical Report CPS-93-40, </type> <institution> Michigan State University, </institution> <year> 1994. </year>
Reference-contexts: Hoogs has noted that many factors can enter into such predictions, including geometric, temporal, functional, and radiometric factors [21]. Our feature prediction utilizes simple radiometric and temporal context information in order to predict the internal structure 7 likely to be visible in the optical imagery. Like others <ref> [11, 10] </ref>, we have found these additional features to greatly aid the matching process. 4.1.1 Silhouette Lines To determine which parts of the CAD model produce the silhouette, a unique color is first assigned to each existing face.
Reference: [11] <author> Jin-Long Chen, George C. Stockman, and Kashi Rao. </author> <title> Recovering and tracking pose of curved 3d objects from 2d images. </title> <booktitle> In Proceedings Computer Vision and Pattern Recognition, </booktitle> <pages> pages 233-239, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Hoogs has noted that many factors can enter into such predictions, including geometric, temporal, functional, and radiometric factors [21]. Our feature prediction utilizes simple radiometric and temporal context information in order to predict the internal structure 7 likely to be visible in the optical imagery. Like others <ref> [11, 10] </ref>, we have found these additional features to greatly aid the matching process. 4.1.1 Silhouette Lines To determine which parts of the CAD model produce the silhouette, a unique color is first assigned to each existing face.
Reference: [12] <author> C. H. Chien and J. K. Aggarwal. </author> <title> Shape recognition from single silhouettes. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 481-490, </pages> <year> 1987. </year>
Reference-contexts: Many systems have been developed to recognize 3-D objects based on their projected 2D silhouettes [45, 28, 46]. More rare are works using 3-D edges directly <ref> [12] </ref>, and then the goal is usually to link 2D image features to 3-D model features. Our method approaches the problem from the other direction: our goal is to work backward from a 2D edge produced in real-time using rendering hardware to predict the original 3-D feature inducing that edge.
Reference: [13] <author> James J. Clark. </author> <title> Authenticating Edges Produced by Zero-Crossing Algorithms. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-11(1):43-57, </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: Local edges [34, 20] may be grouped into larger features such as straight line segments [9, 31]. These linear features, in turn, may be matched to linear features of stored object models [29, 22, 18, 5]. These bottom-up feature extraction algorithms are prone to error <ref> [13, 3] </ref>, and often produce extraneous line segments, fragmented segments, and sometimes over-grouped segments. Our past work in other problem domains [2] demonstrated that local search, coupled with sound and efficient tests of global alignment, could overcome significant amounts of fragmentation, over-grouping and clutter.
Reference: [14] <author> Richard L. Delanoy, Jacques G. Verly, and Dan E. Dudgeon. </author> <title> Machine Intelligent Automatic Recognition of Critical Mobile Targets in Laser Radar Imagery. </title> <journal> The Lincoln Laboratory Journal, </journal> <volume> 6(1) </volume> <pages> 161-186, </pages> <month> Spring </month> <year> 1993. </year>
Reference-contexts: While model-based approaches to Automatic Target Recognition have become much more common <ref> [14] </ref>, direct incorporation of alignment into the recognition process is rare [6]. The work reported here is the first such attempt of which we are aware for the case of multiple, heterogeneous, ground-looking sensors.
Reference: [15] <author> F. Glover. </author> <title> Tabu search part i. </title> <journal> ORSA Journal on Computing, </journal> <volume> 1(3):190 - 206, </volume> <year> 1989. </year>
Reference-contexts: The match evaluation function is non-- differentiable and, because new target features are predicted during search, non-stationary 2 . Traditional gradient descent methods are not applicable for minimizing such a function. A new form of local search [25, 27, 36, 2], more specifically a variant on Tabu Search <ref> [15] </ref>, has been developed for this task. While our work focuses specifically on the RSTA multisensor ATR problem, many of the advances described in this paper can be adapted and applied to other model-based object recognition problems. <p> One consequence of feature regeneration is that the match error landscape about the current estimate can change. At times, this change in the landscape causes a move back to the previous state to appear most attractive. In keeping with the underlying concept of Tabu Search <ref> [15] </ref>, our search algorithm keeps a modest history of past states. This history is used to prevent cycling back to previously visited parts of the search space. While the regeneration of features does complicate the search process, it is critical to the success of our approach.
Reference: [16] <author> M. E. Goss, J. R. Beveridge, M. Stevens, and A. Fuegi. </author> <title> Three-dimensional visualization environment for multisensor data analysis, interpretation, and model-based object recognition. </title> <booktitle> In IS&T/SPIE Symposium on Electronic Imaging: Science & Technology, </booktitle> <pages> pages 283 - 291, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: From this elevation, the viewer looks slightly down upon the range points and can thus begin to discern some of the 3-D structure in the data. This 3-D range rendering is produced by our interactive 3-D visualization system <ref> [17, 16, 42] </ref>. While modestly useful for still images, the induced 3-D effect becomes more dramatic as the viewpoint changes in the interactive visualization environment. 3 http://www.cs.colostate.edu/~vision 3 a. LADAR Image c. Color (720x480) e. FLIR (256x256) b. (180:0; 5:0) d. Color Cropped f. FLIR Cropped a. LADAR Image c.
Reference: [17] <author> Michael E. Goss, J. Ross Beveridge, Mark Stevens, and Aaron Fuegi. </author> <title> Visualization and Verification of Automatic Target Recognition Results Using Combined Range and Optical Imagery. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 491 - 494, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: From this elevation, the viewer looks slightly down upon the range points and can thus begin to discern some of the 3-D structure in the data. This 3-D range rendering is produced by our interactive 3-D visualization system <ref> [17, 16, 42] </ref>. While modestly useful for still images, the induced 3-D effect becomes more dramatic as the viewpoint changes in the interactive visualization environment. 3 http://www.cs.colostate.edu/~vision 3 a. LADAR Image c. Color (720x480) e. FLIR (256x256) b. (180:0; 5:0) d. Color Cropped f. FLIR Cropped a. LADAR Image c.
Reference: [18] <author> W. Eric L. Grimson and Daniel P. Huttenlocher. </author> <title> On the Verification of Hypothesized Matches in Model-Based Recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(12):1201 - 1213, </volume> <month> December </month> <year> 1991. </year>
Reference-contexts: A detailed explanation of why sensor translation may be used to compensate for small changes in pointing angles appears in [23]. 1.2 Related Work Model-based object recognition work has long emphasized the importance of aligning 3D object models to features extracted from sensed imagery <ref> [7, 30, 18, 22, 2] </ref>. While model-based approaches to Automatic Target Recognition have become much more common [14], direct incorporation of alignment into the recognition process is rare [6]. <p> Traditional methods for locating objects in optical imagery typically use edge detection algorithms. Local edges [34, 20] may be grouped into larger features such as straight line segments [9, 31]. These linear features, in turn, may be matched to linear features of stored object models <ref> [29, 22, 18, 5] </ref>. These bottom-up feature extraction algorithms are prone to error [13, 3], and often produce extraneous line segments, fragmented segments, and sometimes over-grouped segments.
Reference: [19] <author> Martial Hebert. </author> <title> Presentation of the Mobility Group, </title> <booktitle> UGV Demo II, Killeen, Texas. Future Recommendations of the Stereo Group, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: However, this a risky and limiting assumption. The stereo group within the Unmanned Ground Vehicle Program has considerable experience with sensors operating on mobile platforms. They have reported that minor day-to-day alignment variations arise due to slight shifts in relative sensor pointing angles <ref> [19] </ref>. Presumably this is because bouncing around on rough terrain shifts slightly the geometry of the sensor platform. Similar misalignment problems can be expected with other types of co-located sensors.
Reference: [20] <author> Ellen C. Hildreth. </author> <title> The Detection of Intensity Changes by Computer and Biological Vision Systems. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 22 </volume> <pages> 1-27, </pages> <year> 1983. </year>
Reference-contexts: E fit;C represents the color fitness, and E fit;I the IR fitness. The fitness terms are formed by examining the estimated gradient magnitude underlying each predicted model line. Traditional methods for locating objects in optical imagery typically use edge detection algorithms. Local edges <ref> [34, 20] </ref> may be grouped into larger features such as straight line segments [9, 31]. These linear features, in turn, may be matched to linear features of stored object models [29, 22, 18, 5].
Reference: [21] <author> Anthony Hoogs and Douglas Hackett. </author> <title> Model-supported exploitation as a framework for image understanding. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 265-268. ARPA, </pages> <month> nov </month> <year> 1994. </year>
Reference-contexts: Using internal detail, as well as silhouette information, has proven essential. Generally, what is desired is a mechanism for predicting what features are most likely to be measurable. Hoogs has noted that many factors can enter into such predictions, including geometric, temporal, functional, and radiometric factors <ref> [21] </ref>. Our feature prediction utilizes simple radiometric and temporal context information in order to predict the internal structure 7 likely to be visible in the optical imagery.
Reference: [22] <author> Daniel P. Huttenlocher and Shimon Ullman. </author> <title> Recognizing Solid Objects by Alignment with an Image. </title> <journal> International Journal of Computer Vision, </journal> <volume> 5(2):195 - 212, </volume> <month> November </month> <year> 1990. </year>
Reference-contexts: A detailed explanation of why sensor translation may be used to compensate for small changes in pointing angles appears in [23]. 1.2 Related Work Model-based object recognition work has long emphasized the importance of aligning 3D object models to features extracted from sensed imagery <ref> [7, 30, 18, 22, 2] </ref>. While model-based approaches to Automatic Target Recognition have become much more common [14], direct incorporation of alignment into the recognition process is rare [6]. <p> Traditional methods for locating objects in optical imagery typically use edge detection algorithms. Local edges [34, 20] may be grouped into larger features such as straight line segments [9, 31]. These linear features, in turn, may be matched to linear features of stored object models <ref> [29, 22, 18, 5] </ref>. These bottom-up feature extraction algorithms are prone to error [13, 3], and often produce extraneous line segments, fragmented segments, and sometimes over-grouped segments.
Reference: [23] <author> J. Ross Beveridge and Bruce A. Draper and Kris Siejko. </author> <title> Progress on Target and Terrain Recognition Research at Colorado State University. </title> <booktitle> In Proceedings: Image Understanding Workshop, page (to appear), </booktitle> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: In the ideal case of perfect boresight alignment, the mapping may be expressed as a 2D affine transformation between image coordinate systems. For nearly boresight aligned sensors viewing distant objects, the 2D affine mapping is still a good approximation <ref> [23] </ref>. Presuming that sensors are firmly affixed to a single solid platform, a calibration step can recover the affine mapping between image coordinate systems for different sensors. It might be assumed that once calibrated, the problem of image registration between sensors is solved for all time. <p> The specific geometric constraints which we use to accomplish this are presented in Section 6.1. A detailed explanation of why sensor translation may be used to compensate for small changes in pointing angles appears in <ref> [23] </ref>. 1.2 Related Work Model-based object recognition work has long emphasized the importance of aligning 3D object models to features extracted from sensed imagery [7, 30, 18, 22, 2]. <p> The IR imagery was acquired using a 3 to 5 micron Amber FLIR. The three sensors were co-located to simulate three nearly boresight aligned sensors operating from a single pan-tilt platform. Additional information about sensor calibration and issues pertaining to alignment may be found in <ref> [23] </ref>. a. LADAR Image c. Color (720x480) e. FLIR (256x256) b. (180:0; 5:0) d. Color Cropped f. FLIR Cropped Example triples of range, IR and color imagery are presented in Figures 1, 2 and 3.
Reference: [24] <author> J. Ross Beveridge and Mark R. Stevens and Zhongfei Zhang and Mike Goss. </author> <title> Approximate Image Mappings Between Nearly Boresight Aligned Optical and Range Sensors. </title> <type> Technical Report CS-96-112, </type> <institution> Computer Science, Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Projecting the 3-D edges into the imagery is possible because both the intrinsic sensor parameters and the approximate pose of the target are known. The parameters for the color sensor have been determined off-line using calibration targets <ref> [24] </ref> 10 a. Gradient Magnitude b. Detection Mask c. Range Mask d. Gradient Result used in top-down feature matching. while the IR parameters have been derived from the manufacturer's specifications and interactive adjustment using our visualization tool [42].
Reference: [25] <author> B. W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> Bell Systems Tech. Journal, </journal> <volume> 49:291 - 307, </volume> <year> 1972. </year>
Reference-contexts: Robust optimization to find locally optimal multisensor matches. The match evaluation function is non-- differentiable and, because new target features are predicted during search, non-stationary 2 . Traditional gradient descent methods are not applicable for minimizing such a function. A new form of local search <ref> [25, 27, 36, 2] </ref>, more specifically a variant on Tabu Search [15], has been developed for this task. While our work focuses specifically on the RSTA multisensor ATR problem, many of the advances described in this paper can be adapted and applied to other model-based object recognition problems.
Reference: [26] <author> J.J. Koenderink. </author> <title> What does occluding contour tell us about solid shape? Perception, </title> <booktitle> 13 </booktitle> <pages> 321-330, </pages> <year> 1984. </year>
Reference-contexts: The model shape and contours are loosely based on those developed by Verly [44], who has analyzed the model shape in relation to LADAR data. 4.1 Predicting 3-D Line Segments Which Induce Observable Edges The silhouette of an object is a valuable recognition cue when dealing with two-dimensional optical imagery <ref> [33, 26] </ref>. Many systems have been developed to recognize 3-D objects based on their projected 2D silhouettes [45, 28, 46]. More rare are works using 3-D edges directly [12], and then the goal is usually to link 2D image features to 3-D model features.
Reference: [27] <author> S. Lin and B. Kernighan. </author> <title> An effective heuristic algorithm for the traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 21:498 - 516, </volume> <year> 1973. </year>
Reference-contexts: Robust optimization to find locally optimal multisensor matches. The match evaluation function is non-- differentiable and, because new target features are predicted during search, non-stationary 2 . Traditional gradient descent methods are not applicable for minimizing such a function. A new form of local search <ref> [25, 27, 36, 2] </ref>, more specifically a variant on Tabu Search [15], has been developed for this task. While our work focuses specifically on the RSTA multisensor ATR problem, many of the advances described in this paper can be adapted and applied to other model-based object recognition problems.
Reference: [28] <author> Cheng-Hsiung Liu and We-Hsiang Tsai. </author> <title> 3d curved object recognition from multiple 2d camera views. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 50 </volume> <pages> 177-187, </pages> <year> 1990. </year>
Reference-contexts: Many systems have been developed to recognize 3-D objects based on their projected 2D silhouettes <ref> [45, 28, 46] </ref>. More rare are works using 3-D edges directly [12], and then the goal is usually to link 2D image features to 3-D model features.
Reference: [29] <author> David G. Lowe. </author> <title> Perceptual Organization and Visual Recognition. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1985. </year>
Reference-contexts: Traditional methods for locating objects in optical imagery typically use edge detection algorithms. Local edges [34, 20] may be grouped into larger features such as straight line segments [9, 31]. These linear features, in turn, may be matched to linear features of stored object models <ref> [29, 22, 18, 5] </ref>. These bottom-up feature extraction algorithms are prone to error [13, 3], and often produce extraneous line segments, fragmented segments, and sometimes over-grouped segments.
Reference: [30] <author> David G. Lowe. </author> <title> Fitting Parameterized Three-Dimensional Models to Images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(5):441 - 450, </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: A detailed explanation of why sensor translation may be used to compensate for small changes in pointing angles appears in [23]. 1.2 Related Work Model-based object recognition work has long emphasized the importance of aligning 3D object models to features extracted from sensed imagery <ref> [7, 30, 18, 22, 2] </ref>. While model-based approaches to Automatic Target Recognition have become much more common [14], direct incorporation of alignment into the recognition process is rare [6].
Reference: [31] <author> David G. Lowe and T. O. Binford. </author> <title> The Perceptual Organization of Visual Images: Segmentation as a Basis for Recognition. </title> <booktitle> In Proceedings Image Understanding Workshop, Stanford, </booktitle> <pages> pages 203 - 209, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: The fitness terms are formed by examining the estimated gradient magnitude underlying each predicted model line. Traditional methods for locating objects in optical imagery typically use edge detection algorithms. Local edges [34, 20] may be grouped into larger features such as straight line segments <ref> [9, 31] </ref>. These linear features, in turn, may be matched to linear features of stored object models [29, 22, 18, 5]. These bottom-up feature extraction algorithms are prone to error [13, 3], and often produce extraneous line segments, fragmented segments, and sometimes over-grouped segments.
Reference: [32] <author> M. J. Magee, B. A. Boyter, C. H. Chien, and J. K. Aggarwal. </author> <title> Experiments in Intensity Guided Range Sensing Recognition of Three-Dimensional Objects. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(6):629 - 637, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: He goes on to state that relating data from different modalities is more difficult, in part because of issues of sensor alignment and registration. While Aggarwal <ref> [32] </ref> and others [39] have examples of successful mixed-modality fusion, this is still a young research area. 2 The Fort Carson Range, IR and Color Dataset. Our algorithms are tested on multisensor images collected at Fort Carson [4] in 1993.
Reference: [33] <author> David Marr. </author> <title> Analysis of occluding contour. </title> <journal> Proceedings of the Royal Society of London, </journal> <volume> B197:441-475, </volume> <year> 1977. </year>
Reference-contexts: The model shape and contours are loosely based on those developed by Verly [44], who has analyzed the model shape in relation to LADAR data. 4.1 Predicting 3-D Line Segments Which Induce Observable Edges The silhouette of an object is a valuable recognition cue when dealing with two-dimensional optical imagery <ref> [33, 26] </ref>. Many systems have been developed to recognize 3-D objects based on their projected 2D silhouettes [45, 28, 46]. More rare are works using 3-D edges directly [12], and then the goal is usually to link 2D image features to 3-D model features.
Reference: [34] <author> David Marr and Ellen C. Hildreth. </author> <title> Theory of Edge Detection. </title> <journal> Proceedings of the Royal Society of London, </journal> <volume> B207:187-217, </volume> <year> 1980. </year>
Reference-contexts: E fit;C represents the color fitness, and E fit;I the IR fitness. The fitness terms are formed by examining the estimated gradient magnitude underlying each predicted model line. Traditional methods for locating objects in optical imagery typically use edge detection algorithms. Local edges <ref> [34, 20] </ref> may be grouped into larger features such as straight line segments [9, 31]. These linear features, in turn, may be matched to linear features of stored object models [29, 22, 18, 5].
Reference: [35] <author> G.W. Paltridge and C.M.R Platt. </author> <title> Radiative Processes in Meteorology and Climatology. </title> <publisher> Elsevier Scientific Publishing Company, </publisher> <year> 1976. </year>
Reference-contexts: The sun is modeled as an area light source, and the vector to the sun is calculated using a long/lat estimate, time of day, date, and compass orientation <ref> [35] </ref>. All of this information is available for our current data set. Once the vector is determined, it provides the direction to the sun for the entire scene, and can be used to predict the internal model edges.
Reference: [36] <author> Christos H. Papadimitriou and Kenneth Steiglitz. </author> <title> Combinatorial Optimization: Algorithms and Complexity, chapter Local Search, </title> <booktitle> pages 454 - 480. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: Robust optimization to find locally optimal multisensor matches. The match evaluation function is non-- differentiable and, because new target features are predicted during search, non-stationary 2 . Traditional gradient descent methods are not applicable for minimizing such a function. A new form of local search <ref> [25, 27, 36, 2] </ref>, more specifically a variant on Tabu Search [15], has been developed for this task. While our work focuses specifically on the RSTA multisensor ATR problem, many of the advances described in this paper can be adapted and applied to other model-based object recognition problems.
Reference: [37] <author> Juan Pineda. </author> <title> A Parallel Algorithm for Polygon Rasterization. </title> <booktitle> In Proceedings of Siggraph '88, </booktitle> <pages> pages 17-20, </pages> <year> 1988. </year>
Reference-contexts: The second step is to measure the overall target edge strength under the line segment. A commonly used graphics anti-aliasing technique, known as Pineda Arithmetic <ref> [37] </ref>, is used to determine with subpixel accuracy where the projected line crosses the sensor image. A weighting term is created to scale the edge strength at each pixel, and to obtain the edge strength for a single line.
Reference: [38] <author> W. Brent Seales and Charles R. Dyer. </author> <title> Modeling the Rim Appearance. </title> <booktitle> In Proceedings of the 3rd International Conference on Computer Vision, </booktitle> <pages> pages 698-701, </pages> <year> 1992. </year> <month> 22 </month>
Reference-contexts: Thus, if the background color appears in a pixel's eight-connected neighborhood, the associated face lies on the silhouette. Subsequent search determines which specific face boundaries (edges) generate the silhouette. An edge is a possible silhouette edge if only one of the two bounding faces is visible <ref> [38] </ref>. This step may leave some edges which are actually internal as hypothesized silhouette edges, and it also does not deal with self-occlusion. A clipping algorithm is then used to discover and discard those edges and portions of edges which are not part of the silhouette.
Reference: [39] <author> A. Stentz and Y. </author> <title> Goto. </title> <booktitle> The CMU Navigational Architecture. In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 440-446, </pages> <address> Los Angeles, CA, </address> <month> February </month> <year> 1987. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: He goes on to state that relating data from different modalities is more difficult, in part because of issues of sensor alignment and registration. While Aggarwal [32] and others <ref> [39] </ref> have examples of successful mixed-modality fusion, this is still a young research area. 2 The Fort Carson Range, IR and Color Dataset. Our algorithms are tested on multisensor images collected at Fort Carson [4] in 1993.
Reference: [40] <author> Mark R. Stevens. </author> <title> Obtaining 3D Silhouettes and Sampled Surfaces from Solid Models for use in Computer Vision. </title> <type> Master's thesis, </type> <institution> Colorado State University, Fort Collins, Colorado, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Highly detailed models of the vehicles in our Fort Carson dataset exist in the CAD model format known as BRL/CAD [43]. Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed <ref> [41, 40] </ref>. From these simpler models, features to be used in the matching process are obtained. Currently, we have models for the M113 APC, the M60 tank and a pickup truck.
Reference: [41] <author> Mark R. Stevens, J. Ross Beveridge, and Michael E. Goss. </author> <title> Reduction of BRL/CAD Models and Their Use in Automatic Target Recognition Algorithms. </title> <booktitle> In Proceedings: BRL-CAD Symposium. </booktitle> <institution> Army Research Labs, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Highly detailed models of the vehicles in our Fort Carson dataset exist in the CAD model format known as BRL/CAD [43]. Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed <ref> [41, 40] </ref>. From these simpler models, features to be used in the matching process are obtained. Currently, we have models for the M113 APC, the M60 tank and a pickup truck.
Reference: [42] <author> Mark R. Stevens, J. Ross Beveridge, and Mike Goss. </author> <title> Visualization of multi-sensor model-based object recog-nition. </title> <journal> IEEE Transactions on Visualization and Computer Graphics, </journal> <note> (submitted). </note>
Reference-contexts: From this elevation, the viewer looks slightly down upon the range points and can thus begin to discern some of the 3-D structure in the data. This 3-D range rendering is produced by our interactive 3-D visualization system <ref> [17, 16, 42] </ref>. While modestly useful for still images, the induced 3-D effect becomes more dramatic as the viewpoint changes in the interactive visualization environment. 3 http://www.cs.colostate.edu/~vision 3 a. LADAR Image c. Color (720x480) e. FLIR (256x256) b. (180:0; 5:0) d. Color Cropped f. FLIR Cropped a. LADAR Image c. <p> Gradient Magnitude b. Detection Mask c. Range Mask d. Gradient Result used in top-down feature matching. while the IR parameters have been derived from the manufacturer's specifications and interactive adjustment using our visualization tool <ref> [42] </ref>. The second step is to measure the overall target edge strength under the line segment. A commonly used graphics anti-aliasing technique, known as Pineda Arithmetic [37], is used to determine with subpixel accuracy where the projected line crosses the sensor image. <p> For each triple, a ground truth estimate was manually determined. The ground truth established the correct target rotation and translation as well as the alignment between sensors. This ground truth estimate was establish by hand using our visualization system <ref> [42] </ref>. Due to the coarse sampling of the range data, and the few number of pixels on target in the optical imagery, we expect there to be a slight amount of error in the ground truth 5 .
Reference: [43] <author> U. S. </author> <note> Army Ballistic Research Laboratory. BRL-CAD User's Manual, release 4.0 edition, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: Therefore, features representing internal detail as a function of lighting angle are also utilized. Highly detailed models of the vehicles in our Fort Carson dataset exist in the CAD model format known as BRL/CAD <ref> [43] </ref>. Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed [41, 40]. From these simpler models, features to be used in the matching process are obtained.
Reference: [44] <author> Jacques G. Verly, Dan E. Dudgeon, and Richard T. Lacoss. </author> <title> Model-Based Automatic Target Recognition System for the UGV/RSTA Ladar: Status at Demo C. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 549-583. ARPA, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: From these simpler models, features to be used in the matching process are obtained. Currently, we have models for the M113 APC, the M60 tank and a pickup truck. The model shape and contours are loosely based on those developed by Verly <ref> [44] </ref>, who has analyzed the model shape in relation to LADAR data. 4.1 Predicting 3-D Line Segments Which Induce Observable Edges The silhouette of an object is a valuable recognition cue when dealing with two-dimensional optical imagery [33, 26].
Reference: [45] <author> T.P. Wallace and P.A. Wintz. </author> <title> An efficient three-dimensional aircraft recognition algorithm using normalized Fourier descriptors. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 13 </volume> <pages> 99-126, </pages> <year> 1980. </year>
Reference-contexts: Many systems have been developed to recognize 3-D objects based on their projected 2D silhouettes <ref> [45, 28, 46] </ref>. More rare are works using 3-D edges directly [12], and then the goal is usually to link 2D image features to 3-D model features.
Reference: [46] <author> Y. F. Wang, M. J. Magee, and J. K. Aggarwal. </author> <title> Matching three-dimensional objects using silhouettes. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 513-518, </pages> <year> 1984. </year> <month> 23 </month>
Reference-contexts: Many systems have been developed to recognize 3-D objects based on their projected 2D silhouettes <ref> [45, 28, 46] </ref>. More rare are works using 3-D edges directly [12], and then the goal is usually to link 2D image features to 3-D model features.
References-found: 46

