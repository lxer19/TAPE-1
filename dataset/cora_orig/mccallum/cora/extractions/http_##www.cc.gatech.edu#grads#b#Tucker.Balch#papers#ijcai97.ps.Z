URL: http://www.cc.gatech.edu/grads/b/Tucker.Balch/papers/ijcai97.ps.Z
Refering-URL: http://www.cc.gatech.edu/grads/b/Tucker.Balch/papers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: tucker@cc.gatech.edu  
Title: Integrating RL and Behavior-based Control for Soccer  
Author: Tucker Balch 
Address: Atlanta, Georgia 30332-0280  
Affiliation: Mobile Robot Laboratory College of Computing Georgia Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [ Arkin and Balch, 1997 ] <author> R.C. Arkin and T.R. Balch. Aura: </author> <title> principles and practice in review. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <note> in press, </note> <year> 1997. </year>
Reference-contexts: A simple robot soccer strategy is used to to illustrate the utility of the system. Motor schemas are the reactive component of Arkin's Autonomous Robot Architecture (AuRA) <ref> [ Arkin and Balch, 1997 ] </ref> . AuRA's design integrates deliberative planning at a top level with behavior-based motor control at the bottom. The lower levels, concerned with executing the reactive behaviors are incorporated in this research. <p> In the example figure an entire field is shown, but this is only for visualization purposes. Problems with local minima, maxima, and cyclic behavior which are endemic to many potential fields strategies are handled by several methods including: the injection of noise into the system <ref> [ Arkin and Balch, 1997 ] </ref> ; resorting to high-level planning; repulsion from previously visited locales [ Balch and Arkin, 1993 ] ; continuous adaptation [ Clark et al., 1992 ] ; and other learning strategies. <p> Clay's primitive, the motor schema, provides a rich repertoire for behavioral design <ref> [ Arkin and Balch, 1997 ] </ref> . Motor schemas take full advantage of continuous sensor values and can generate an infinite range of actuator output; most other approaches integrating reinforcement learning and behavior-based control only select from a discrete list of actions.
Reference: [ Arkin and MacKenzie, 1994 ] <author> R.C. </author> <title> Arkin and D.C. MacKen-zie. Temporal coordination of perceptual algorithms for mobile robot navigation. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(3) </volume> <pages> 276-286, </pages> <year> 1994. </year>
Reference-contexts: The steps in the sequence are separate behavioral states. Perceptual events that cause transitions from one behavioral state to another are called perceptual triggers. The resulting task-solving strategy can be represented as a Finite State Automaton (FSA). The technique is referred to as temporal sequencing <ref> [ Arkin and MacKenzie, 1994 ] </ref> . schema, pulling the robot to a location on the right. The center diagram shows an avoid-obstacles field, repelling the robot from two sensed obstacles. On the right, the two schemas are summed, resulting in a complete behavior for reaching the goal.
Reference: [ Balch and Arkin, 1993 ] <author> T. </author> <title> Balch and R.C. Arkin. Avoiding the past: a simple but effective strategy for reactive navigation. </title> <booktitle> In IEEE Conference on Robotics and Automation, </booktitle> <pages> pages 678-685. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1993. </year> <institution> Atlanta, Georgia. </institution>
Reference-contexts: Problems with local minima, maxima, and cyclic behavior which are endemic to many potential fields strategies are handled by several methods including: the injection of noise into the system [ Arkin and Balch, 1997 ] ; resorting to high-level planning; repulsion from previously visited locales <ref> [ Balch and Arkin, 1993 ] </ref> ; continuous adaptation [ Clark et al., 1992 ] ; and other learning strategies.
Reference: [ Clark et al., 1992 ] <author> R.J. Clark, R.C. Arkin, and A. Ram. </author> <title> Learning momentum: On-line performance enhancement for reactive systems. </title> <booktitle> In IEEE Conf. on Robotics and Automation, </booktitle> <pages> pages 111-116. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1992. </year> <institution> Nice, France. </institution>
Reference-contexts: maxima, and cyclic behavior which are endemic to many potential fields strategies are handled by several methods including: the injection of noise into the system [ Arkin and Balch, 1997 ] ; resorting to high-level planning; repulsion from previously visited locales [ Balch and Arkin, 1993 ] ; continuous adaptation <ref> [ Clark et al., 1992 ] </ref> ; and other learning strategies. Schema-based robot control has been demonstrated to provide robust navigation in complex and dynamic worlds. 1.1 Temporal Sequencing As illustrated above for navigation, motor schemas may be grouped to form more complex, emergent behaviors.
Reference: [ Connolly and Grupen, 1993 ] <author> C. Connolly and R. Grupen. </author> <title> On the applications of harmonic functions to robotics. </title> <journal> Journal of Robotic Systems, </journal> <volume> 10(7) </volume> <pages> 931-936, </pages> <year> 1993. </year>
Reference-contexts: Each motor vector is multiplied by the associated gain value and the results are summed and normalized. The resultant vector is sent to the robot hardware for execution. An example of this process is illustrated in Figure 1. The approach bears a strong resemblance to potential field methods (e.g. <ref> [ Connolly and Grupen, 1993 ] </ref> ), but with an important difference: the entire field is never computed, only the robot's reaction to its current perception of the world at its present location. In the example figure an entire field is shown, but this is only for visualization purposes.
Reference: [ Kitano et al., 1997 ] <author> H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, and E. Osawa. </author> <title> Robocup: The robot world cup initiative. </title> <booktitle> In Proc. Autonomous Agents 97. ACM, </booktitle> <year> 1997. </year> <institution> Marina Del Rey, California. </institution>
Reference-contexts: The coordination modules activate specific assemblages based on the presently perceived situation. Learning occurs as the robot selects assemblages and samples a reinforcement signal over time. Clay was used by Geor-gia Tech in the configuration of a soccer team for the RoboCup-97 simulator competition <ref> [ Kitano et al., 1997 ] </ref> . A simple robot soccer strategy is used to to illustrate the utility of the system. Motor schemas are the reactive component of Arkin's Autonomous Robot Architecture (AuRA) [ Arkin and Balch, 1997 ] .
Reference: [ Watkins and Dayan, 1992 ] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Technical note: Q learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Where s the state or situation and a is a possible action. If the function is properly computed, an agent can act optimally simply by looking up the best-valued action for any situation. The problem is to find the Q (s; a) that provides an optimal policy. Watkins <ref> [ Watkins and Dayan, 1992 ] </ref> has developed an algorithm for determining Q (s; a) that converges to optimal under certain conditions. Q-learning is integrated by the addition of a new coordination operator, CoordinateLearner. Coordi-nateLearner is "plug compatible" with Coordinate-Selection but it learns which subordinate assemblage to activate.
References-found: 7

