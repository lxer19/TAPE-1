URL: http://www.cs.wisc.edu/~samit/vldb96.ps
Refering-URL: http://www.cs.wisc.edu/~samit/samit.html
Root-URL: 
Email: famit,pmd,naughton,karthikg@cs.wisc.edu  
Title: Storage Estimation for Multidimensional Aggregates in the Presence of Hierarchies  
Author: Amit Shukla Prasad M. Deshpande Jeffrey F. Naughton Karthikeyan Ramasamy 
Address: Wisconsin Madison  
Affiliation: Computer Sciences Department University of  
Abstract: To speed up multidimensional data analysis, database systems frequently precompute aggregates on some subsets of dimensions and their corresponding hierarchies. This improves query response time. However, the decision of what and how much to precompute is a difficult one. It is further complicated by the fact that precomputation in the presence of hierarchies can result in an unintuitively large increase in the amount of storage required by the database. Hence, it is interesting and useful to estimate the storage blowup that will result from a proposed set of precomputations without actually computing them. We propose three strategies for this problem: one based on sampling, one based on mathematical approximation, and one based on probabilistic counting. We investigate the accuracy of these algorithms in estimating the blowup for different data distributions and database schemas. The algorithm based upon probabilistic counting is particularly attractive, since it estimates the storage blowup to within provable error bounds while performing only a single scan of the data. fl Work supported by an IBM CAS Fellowship, NSF grant IRI-9157357, and a grant from IBM under the University Partnership Program. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 22nd VLDB Conference Mumbai (Bombay), India, 1996 
Abstract-found: 1
Intro-found: 1
Reference: [AAD+96] <author> S. Agarwal, R. Agrawal, P.M. Deshpande, A. Gupta, J.F. Naughton, R. Ramakrish-nan, S. Sarawagi. </author> <title> On the Computation of Multidimensional Aggregates. </title> <booktitle> Proc. of the 22nd Int. VLDB Conf., </booktitle> <year> 1996. </year>
Reference-contexts: This comes at the cost of a complete scan of the base data table; however, even this scan is much cheaper than actually computing the cube, which in general requires multiple scans and sorts of the input table <ref> [AAD+96] </ref>. Table 4: The number of distinct elements in each of the dimensions. The total number of tuples in the base data = 60,000 [Schema 1] Dimension num. Dimension Hierarchy 1 2 1 10,000 500 - Table 5: The number of distinct elements in each of the dimensions.
Reference: [AGS95] <author> R. Agrawal, A. Gupta, S. Sarawagi. </author> <title> Modeling Multidimensional Databases. </title> <institution> IBM Research Report, IBM Almaden Research Center, </institution> <address> San Jose, California, </address> <year> 1995. </year>
Reference-contexts: Finally, suppose we have an additional table Stores (StoreId, Region) which captures for each store to which region it belongs. This data set can be viewed conceptually as a two-dimensional array with hierarchies on the dimensions, as shown in Figure 1 (a). 1 This example first appeared in <ref> [AGS95] </ref> (a) (b) Database DB 1 (c) Database DB 2 S i 's represent Stores and P i 's represent Products. Stores S1 - S5 are in California, and so roll up into the region California, while S6 - S10 are in Wisconsin, and roll up into the region Wisconsin.
Reference: [CCS93] <author> E.F. Codd, </author> <title> S.B. Codd, C.T. Salley. Providing OLAP (On-Line Analytical Processing) to User-Analysts: An IT Mandate, </title> <publisher> E.F. Codd and Associates, </publisher> <year> 1993. </year> <note> Available from http://www.arborsoft.com/papers/intro.html. </note>
Reference: [Fel57] <author> W. Feller. </author> <title> An Introduction to Probability Theory and Its Applications, Vol. I, </title> <publisher> John Wiley and Sons, </publisher> <pages> pp 241, </pages> <year> 1957. </year>
Reference-contexts: Feller <ref> [Fel57] </ref>: If r elements are chosen uniformly and at random from a set of n elements, the ex pected number of distinct elements obtained is n n (1 1=n) r . This can be used to quickly find the upper bound on the size of the cube as follows.
Reference: [FM85] <author> P. Flajolet, G.N. Martin. </author> <title> Probabilistic Counting Algorithms for Database Applications, </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 31(2): </volume> <pages> 182-209, </pages> <year> 1985. </year>
Reference-contexts: We use this idea to construct an algorithm that estimates the size of the cube based on the following probabilistic algorithm which counts the number of distinct elements in a multi-set. 2.3.1 The Probabilistic Counting Algorithm Flajolet and Martin <ref> [FM85] </ref> propose a probabilistic algorithm that counts the number of distinct elements in a multi-set. It makes the estimate after a single pass through the database, and using only a fixed amount of additional storage. We present a description of their algorithm below.
Reference: [GBLP96] <author> J. Gray, A. Bosworth, A. Layman, H. Pira-hesh. </author> <title> Data Cube: A Relational Aggregation Operator Generalizing Group-By, </title> <booktitle> Cross-Tab, and Sub-Totals, Proc. of the 12th Int. Conf. on Data Engg., </booktitle> <pages> pp 152-159, </pages> <year> 1996. </year>
Reference-contexts: Furthermore, once we have described how to estimate this full precomputation the extension to precomputation of a subset is trivial. A useful way to describe the full precomputation problem is to use the framework proposed by Gray et al. <ref> [GBLP96] </ref>: the cube operator. The cube operator is the n-dimensional generalization of the SQL group by operator. The cube on n attributes computes the group by aggregates for each possible subset of these dimensions. In our example, this is: (), (ProductId), (StoreId), (ProductId, StoreId). <p> The cube as defined by <ref> [GBLP96] </ref>, will be referred to as a cube without hierarchies.
Reference: [HNSS93] <author> P.J. Haas, J.F. Naughton, S. Seshadri, </author> <title> A.N. Swami. Selectivity and Cost Estimation for Joins Based on Random Sampling. </title> <institution> IBM Research Report RJ9577, IBM Almaden Research Center, </institution> <address> San Jose, California, </address> <year> 1993. </year>
Reference: [HNSS95] <author> P.J. Haas, J.F. Naughton, S. Seshadri, L. </author> <title> Stokes. Sampling-Based Estimation of the Number of Distinct Values of an Attribute, </title> <booktitle> Proc. of the 21st Int. VLDB Conf., </booktitle> <pages> 311-322, </pages> <year> 1995. </year>
Reference: [HRU96] <author> V. Harinarayanan, A. Rajaraman, J.D. Ull-man. </author> <title> Implementing Data Cubes Efficiently, </title> <booktitle> Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> 205-227, </pages> <year> 1996. </year>
Reference: [KT95] <author> Kenan Technologies. </author> <title> An Introduction to Multidimensional Database Technology, </title> <note> Available from http://www.kenan.com/ </note>
Reference: [Str95] <author> MicroStrategy Inc. </author> <title> The Case for Relational OLAP, </title> <note> A white paper available from http://www.strategy.com/ </note>
Reference: [Zipf49] <author> G.K. Zipf. </author> <title> Human Behavior and the Principle of Least Effort, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1949. </year>
Reference-contexts: Dimension 0 has 1000 distinct values, and its hierarchies have 200 and 50 values respectively, while dimension 1 has 10,000 distinct values, and its hierarchy has 500 values. The database is a combination of distinct values of all dimensions. A Zipfian distribution <ref> [Zipf49] </ref> was used to generate the database from the distinct values of each dimension. A Zipf value of 0 means that the data is uniformly distributed. By increasing Zipf, we increase the skew in the distribution of distinct values in the database.
References-found: 12

