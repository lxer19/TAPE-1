URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR98739-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A Rigorous Framework for Optimization of Expensive Functions by Surrogates  
Author: Andrew J. Booker J. E. Dennis, Jr. Paul D. Frank David B. Serafini Virginia Torczon Michael W. Trosset k k 
Keyword: Key Words: Approximation concepts, surrogate optimization, response surfaces, pattern search methods, derivative-free optimization, design and analysis of computer experiments (DACE), computational engineering.  
Address: P. O. Box 1892, Houston TX 77005.  Box 3707, M/S 7L-68, Seattle, WA 98124.  P. O. Box 8795, Williamsburg, VA 23187.  Tucson, AZ 85721.  
Affiliation: Department of Computational and Applied Mathematics Center for Research on Parallel Computation, Rice University,  Mathematics Engineering Analysis, Boeing Shared Service Group, Applied Research and Technology,  Department of Computer Science, College of William Mary,  Department of Mathematics, University of Arizona,  
Date: April 23, 1998  
Abstract: The goal of the research reported on here is to develop rigorous optimization algorithms to apply to some engineering design problems for which direct application of traditional optimization approaches is not practical. This paper presents and analyzes a framework for generating and managing a sequence of surrogate objective functions to obtain convergence to a minimizer of an expensive objective function subject to simple constraints. The approach is widely applicable because it does not require, or even approximate, derivatives of the objective. Numerical results are presented for a 31-variable helicopter rotor design example and for a standard optimization test example. This is a brief description of a portion of the Boeing/IBM/Rice University collaboration, whose purpose is to develop effective numerical methods for managing the use of approximation concepts in design optimization. fl Research was supported by DOE FG03-93ER25178, CRPC CCR-9120008, AFOSR-F49620-95-1-0210, NASA Contract No. NAS1-19480 while Virginia Torczon was in residence at the Institute for Computer Applications in Science and Engineering (CASE), NASA Langley Research Center, The Boeing Company, and the REDI Foundation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anonymous. </author> <year> 1972: </year> <title> A new algorithm for optimization. </title> <journal> Mathematical Programming. </journal> <volume> 3, </volume> <pages> 124-128. </pages>
Reference-contexts: This algorithm is just an instantiation of <ref> [1] </ref>. Given x 0 2 M 0 , x 0 x fl , M 0 a mesh on B fx : a x bg : For k = 0; 1; , do 1. Choose x k+1 to minimize f (x) for x 2 M k . 2. <p> Write this as M k+1 = M k =2. For this simple algorithm, one easily can prove a powerful result: Theorem 4.1.1. If f is continuous on the feasible region B, then every limit point of fx k g is a global minimizer of f on B. Proof: See <ref> [1] </ref> Of course, the problem with this algorithm is that the additional number of f evaluations required at each iteration grows exponentially as we refine the mesh. This is absurd even for the surrogate. <p> We are confident that the fault does not lie with the quasi-Newton implementation, which is used widely and successfully. The next figure presents the results for the Hartman 6 dimensional test problem. The global minimum value is -3.322. We took B = <ref> [0; 1] </ref> n . All runs started from the center of B. The two MMF results for were for initial models constructed using 16 points.
Reference: [2] <author> Barthelemy, Jean-Francois M. and Haftka, Raphael T.. </author> <year> 1993: </year> <title> Approximation concepts for optimum structural design a review. </title> <journal> Structural Optimization. </journal> <volume> 5, </volume> <pages> 129-144. </pages>
Reference-contexts: Furthermore, there is a standard engineering practice <ref> [2] </ref> for such problems: 1.
Reference: [3] <author> Besag, J.E. </author> <year> 1977: </year> <title> Efficiency of pseudolikelihood estimation for simple Gaussian fields. </title> <journal> Biometrika. </journal> <pages> 64(3) 616-618. </pages>
Reference: [4] <author> Booker, A. J. </author> <year> 1994: </year> <title> DOE for computer output. </title> <type> Technical Report BCSTECH-94-052, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, WA. </address>
Reference-contexts: This problem has been studied extensively in the recent literature on the design and analysis of computer experiments (DACE), surveys of which include Sacks, Welch, Mitchell and Wynn [38], Booker <ref> [4] </ref> Koehler and Owen [28]. We seek designs that are "space-filling" (for lack of a better term), i.e. that will allow us to sample the behavior of the objective function throughout the feasible region.
Reference: [5] <author> Booker, A. J.; Conn, A. R.; Dennis, J. E. Jr; Frank, Paul D.; Trosset, Michael W. and Torczon, Virginia. </author> <year> 1995: </year> <title> Global modeling for optimization: Boeing/IBM/Rice collaborative project 1995 final report. </title> <type> Technical Report ISSTECH-95-032, </type> <institution> Boeing Information Support Services, Research and Technology, </institution> <address> Box 3707, M/S 7L-68, Seattle, Washington 98124. </address> <month> 20 </month>
Reference-contexts: In these searches, one might wish to include in T k points chosen because they are indicated by some measure to be points at which the surrogate is likely to be a poor predictor of f . See <ref> [5, 49] </ref>. It is reasonable that one might want to recalibrate the surrogate using such points, and then search again for surrogate decrease before resorting to a poll step. We will discuss this more in the next section. <p> The numbers reported for GA are the total number of values requested. It performs as advertised nice initial decrease and then leveling out. It makes little progress after 1500 evaluations and terminates at 3300 evaluations. * BLGS: This is an intuitively appealing method of Booker and Frank <ref> [5] </ref> in which several truth values are computed at each iteration. The number of expensive evaluations allocated to each iteration is fixed, in this case at 50.
Reference: [6] <author> Booker, A. J.; Dennis, J. E. Jr; Frank, Paul D.; Serafini, D. B.; and Torczon, Virginia. </author> <year> 1997: </year> <title> Optimization using surrogate objectives on a helicopter test example. </title> <type> Technical Report SSGTECH-97-027, </type> <institution> Boeing Shared Services Group, Applied Research & Technology. </institution> <note> Also available as Rice U. CAAM Tech. Report 97-31. to appear in Optimal Design and Control; edited by J. </note> <author> Borggaard, J. Burns, E. Cliff and S. Schreck, Birkhauser, </author> <year> 1998. </year>
Reference-contexts: We also plan to investigate frameworks based on the ideas of Section 5. The first test case is the helicopter rotor design example described in Section 2, which furnished major motivation for this research. We will present a graph from <ref> [6] </ref> that summarizes all the results we know for various methods applied to this example. The excellent results we obtain, as well as those obtained by the DFO code of our IBM collaborators, make us optimistic about other applications to real engineering design problems.
Reference: [7] <author> Box, G. E. P. and Draper, N. R.. </author> <year> 1987: </year> <title> Empirical Model Building and Response Surfaces. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: [8] <author> Box, G. E. P.; Hunter, W. G. and Hunter, J. S.. </author> <year> 1978: </year> <institution> Statistics for Experimenters. </institution> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: [9] <author> Buck, R.J.; Mitchell, T.; Morris, M.; Sacks, J.; Welch, W.J. and Wynn, H.P.. </author> <year> 1992: </year> <title> Screening, predicting, and computer experiments. </title> <journal> Technometrics. </journal> <volume> 34(1), </volume> <pages> 15-25. </pages>
Reference: [10] <author> Burgee, S. L.; Giunta, A. A.; Balabanov, V.; Grossman, B.; Mason,W. H.; Narducci, R.; Haftka, R. T. and Watson, L. T.. </author> <year> 1996: </year> <title> A coarse grained parallel variable-complexity multidisciplinary optimization paradigm. </title> <booktitle> Intl. J. Supercomputing Applications and High Performance Computing. </booktitle> <volume> 10, </volume> <pages> 269-299. </pages>
Reference-contexts: One aspect of this approach, called "variable complexity modeling," has been systematically developed at the Multidisciplinary Analysis and Design Center (MAD) at Virginia Tech <ref> [10, 25] </ref>. Although the one-shot approach is easily implemented (subject to being able to construct the surrogate), difficulties arise when trying to use it repeatedly as part of an iterative procedure. We believe that [19] gives the first rigorous attempt to extend the surrogate approach to a practical iteration.
Reference: [11] <author> Conn, A. R.; Scheinberg, K. and Toint, Ph. L.. </author> <year> 1996: </year> <title> On the convergence of derivative-free methods for unconstrained optimization. </title> <type> Technical Report 96/10, </type> <institution> Department of Mathematics, Faculte Universitaires, </institution> <address> B-5000 Namur, Belgium. </address>
Reference-contexts: This is a problem for all derivative-free methods. A poll step can be expensive. If it is unsuccessful, then it may require up to 2n truth evaluations [31]. In fact, our plan is that we will switch to the derivative-free optimization algorithm (DFO) <ref> [11, 12] </ref> after we encounter the first unsuccessful complete poll step. <p> The number of function evaluations given are the converged points for the initial model and the total of all the simulation calls requested by the MMF. * DFO: This is an interesting method from <ref> [11, 12] </ref>, which we plan to combine with the MMF as we will discuss later. Here, we present results from all the runs we were provided since the results were so variable.
Reference: [12] <author> Conn, A. R. and Toint, Ph. L.. </author> <year> 1995: </year> <title> An algorithm using quadratic interpolation for unconstrained derivative free optimization. </title> <editor> In Gianni Di Pillo and Franco Giannessi, editors, </editor> <title> Nonlinear Optimization and Applications. </title> <publisher> Plenum Publishing. </publisher>
Reference-contexts: This is a problem for all derivative-free methods. A poll step can be expensive. If it is unsuccessful, then it may require up to 2n truth evaluations [31]. In fact, our plan is that we will switch to the derivative-free optimization algorithm (DFO) <ref> [11, 12] </ref> after we encounter the first unsuccessful complete poll step. <p> The number of function evaluations given are the converged points for the initial model and the total of all the simulation calls requested by the MMF. * DFO: This is an interesting method from <ref> [11, 12] </ref>, which we plan to combine with the MMF as we will discuss later. Here, we present results from all the runs we were provided since the results were so variable.
Reference: [13] <author> Cox, D. D. and John, S.. </author> <year> 1996: </year> <title> SDO: A statistical method for global optimization. </title> <editor> In Alexandrov, N. and Hussaini, M. Y., editors, </editor> <booktitle> Multidisciplinary Design Optimization: State of the Art. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia. </address>
Reference-contexts: The balanced search method selects a portion of the total of the trial set T k based on each measure. Ideally, one would measure the surrogate function values and MSEs at each point on a fine grid in design space and select the best candidates, as in <ref> [13] </ref>. Unfortunately, in high-dimensional design spaces it is impossible to consider even a crude grid formed by splitting each dimension in two; hence, the balanced search algorithm described below considers each member of a "dense cloud" of (say 5000) trial sites.
Reference: [14] <author> Currin, C.; Mitchell, T.; Morris, M. and Ylvisaker, D.. </author> <year> 1991: </year> <title> Bayesian prediction of deterministic functions, with applications to the design and analysis of computer experiments. </title> <journal> Journal of the American Statistical Association. </journal> <volume> 86(416), </volume> <pages> 953-963. </pages>
Reference: [15] <author> Currin, C.; Mitchell, T.; Morris, M. and Ylvisaker, D.. </author> <year> 1988: </year> <title> A Bayesian approach to the design and analysis of computer experiments. </title> <type> Technical Report ORNL-6498, </type> <institution> Oak Ridge National Laboratory. </institution>
Reference-contexts: Although MLE has been criticized in the spatial statistics literature, e.g. by Ripley [36], it has been defended by others as a crude form of cross-validation <ref> [24, 15] </ref>.
Reference: [16] <author> Davis, C. </author> <year> 1954: </year> <title> Theory of positive linear dependence. </title> <journal> American Journal of Mathematics. </journal> <volume> 448-474. 21 [17] deBoor, </volume> <editor> C. and Ron, A.. </editor> <year> 1992: </year> <title> Computational aspects of polynomial interpolation in several variables. </title> <journal> Mathematics of Computation. </journal> <volume> 58(198), </volume> <pages> 705-727. </pages>
Reference-contexts: The situation with which we are concerned, optimization over a rectangular region, requires a form of the algorithm that uses 2n vectors gotten from the mesh directions (both positive and negative orientations) from any mesh point. See [30]. This set of 2n vectors must form a maximal positive basis <ref> [16] </ref>. A positive basis is a set of vectors whose nonnegative linear combinations span &lt; n .
Reference: [18] <author> Dennis, J. E. Jr and Torczon, Virginia. </author> <year> 1991: </year> <title> Direct search methods on parallel machines. </title> <journal> SIAM J. Optimization. </journal> <volume> 1(4), </volume> <pages> 448-474. </pages>
Reference-contexts: If we discount the expense of evaluating f (x), then direct search methods [46, 50, 48] avoid many of the difficulties that we have identified. Indeed, we will show that parallel direct search (PDS) <ref> [18, 45] </ref> can solve such a problem. Our last two assumptions explain why we allow the value of f (x) to be infinite. Formally, we assign f (x) = 1 when f (x) is not available. <p> An amazing fact is that DFO requested function values at points for which only 3% failed to return a value. The numbers reported for DFO are the total number of values requested. * PDS: This is Torczon's implementation [45] of the parallel direct search method of <ref> [18] </ref>. The numbers reported for PDS are the total number of values requested. We used a smaller template than the default template on the distributed code.
Reference: [19] <author> Dennis, J. E. Jr and Torczon, Virginia.1997: </author> <title> Managing approximation models in optimization. </title> <editor> In Natalia Alexandrov and M.Y. Hussaini, editors, </editor> <booktitle> Multidisciplinary Design Optimization: State-of-the-Art, </booktitle> <pages> pages 330-347. </pages> <publisher> SIAM, </publisher> <address> Philadelphia. </address> <note> Also available as Rice U. CAAM Tech. Report 95-19. </note>
Reference-contexts: Although the one-shot approach is easily implemented (subject to being able to construct the surrogate), difficulties arise when trying to use it repeatedly as part of an iterative procedure. We believe that <ref> [19] </ref> gives the first rigorous attempt to extend the surrogate approach to a practical iteration. Our purpose here is to present a specific, simplified methodology based on the original ideas in [19]. We will emphasize an approach in which surrogates are constructed by interpolating known values of f . <p> We believe that <ref> [19] </ref> gives the first rigorous attempt to extend the surrogate approach to a practical iteration. Our purpose here is to present a specific, simplified methodology based on the original ideas in [19]. We will emphasize an approach in which surrogates are constructed by interpolating known values of f .
Reference: [20] <author> Dennis, J. E. Jr. and Walker, H. F.. </author> <year> 1984: </year> <title> Inaccuracy in quasi-Newton methods: Local Improvement Theorems. </title> <journal> Mathematical Programming Study. </journal> <volume> 22, </volume> <pages> 70-85. </pages>
Reference-contexts: We hope that, as automatic differentiation technology advances, actual derivatives can be used rather than finite-difference approximations. However, the last assumption is again relevant because quasi-Newton methods are affected by function inaccuracies <ref> [20] </ref>. If we discount the expense of evaluating f (x), then direct search methods [46, 50, 48] avoid many of the difficulties that we have identified. Indeed, we will show that parallel direct search (PDS) [18, 45] can solve such a problem.
Reference: [21] <author> Dixon, L. C. W. and Szego G. P., editors. </author> <year> 1978: </year> <title> Towards Global Optimization 2. </title> <publisher> North-Holland Pub. Co. </publisher>
Reference-contexts: Another major reason for including these results here is that we have such a complete suite of results for the best implementations of other methods, for which we do not have code. The second example is a standard global optimization test problem, the 6 variable Hart-man problem <ref> [21] </ref>. We use this problem to show that the model management framework can work with both DACE and polynomials. Indeed, different modeling packages is the only difference between the two run histories presented below. The initial models are built using exactly the same 16 points.
Reference: [22] <author> Efron, B. and Stein, C.. </author> <year> 1981: </year> <title> The jacknife estimate of variance. </title> <journal> The Annals of Statistics. </journal> <volume> 9(3), </volume> <pages> 586-596. </pages>
Reference-contexts: Similarly, it was suggested in [27] that one might cross-validate the MSEs to assess their predictive capabilities. Finally, we have found that performing a functional analysis of variance <ref> [22, 34, 38] </ref> on the surrogate function s is a useful way of identifying lower-dimensional subspaces in which most of the variation in s resides.
Reference: [23] <author> Frank, P. D. </author> <year> 1995: </year> <title> Global modeling for optimization. </title> <journal> SIAG/OPT Views-and-News. </journal> <volume> 7, </volume> <pages> 9-12. </pages>
Reference-contexts: MAGS was intended for situations in which only a very small number of function evaluations are permitted. Because it approximates the objective function over the entire feasible region, local refinement of the surrogates proceeds slowly. A "zoom-in" method for local refinement was proposed in <ref> [23] </ref>. This strategy uses the existing surrogate to determine an interesting subregion of the design space for further exploration. Additional function values are obtained in the subregion and a new surrogate is formed.
Reference: [24] <author> Geisser, S. and Eddy, W. F.. </author> <year> 1979: </year> <title> A predictive approach to model selection. </title> <journal> Journal of the American Statistical Association. </journal> <volume> 74, </volume> <pages> 153-160. </pages>
Reference-contexts: Although MLE has been criticized in the spatial statistics literature, e.g. by Ripley [36], it has been defended by others as a crude form of cross-validation <ref> [24, 15] </ref>.
Reference: [25] <author> Giunta, Anthony A.. </author> <year> 1997: </year> <title> Aircraft Multidisciplinary Optimization using Design of Experiments Theory and Response Surface Modeling Methods. </title> <type> PhD thesis, </type> <institution> Virginia Tech. </institution> <note> Available as MAD 97-05-01, </note> <month> May </month> <year> 1997, </year> <institution> Department of Aerospace and Ocean Engineering, Virginia Tech, 215 Randolph Hall, Blacksburg, </institution> <address> VA 24061. </address>
Reference-contexts: One aspect of this approach, called "variable complexity modeling," has been systematically developed at the Multidisciplinary Analysis and Design Center (MAD) at Virginia Tech <ref> [10, 25] </ref>. Although the one-shot approach is easily implemented (subject to being able to construct the surrogate), difficulties arise when trying to use it repeatedly as part of an iterative procedure. We believe that [19] gives the first rigorous attempt to extend the surrogate approach to a practical iteration.
Reference: [26] <author> Johnson, M. E.; Moore, L. M. and Ylvisaker, D.. </author> <year> 1990: </year> <title> Minimax and maximin distance designs. </title> <journal> Journal of Statistical Planning and Inference. </journal> <volume> 26, </volume> <pages> 131-148. </pages>
Reference: [27] <author> Jones, D.R.; Schonlau, M.; and Welch, W.J.. </author> <year> 1997: </year> <title> A data analytic approach to Bayesian global optimization. </title> <booktitle> In Proceedings of the ASA. </booktitle>
Reference-contexts: Once a surrogate function s has been constructed, one can use it to predict values of f and also to bound the errors in such predictions. The latter is accomplished by calculating mean squared error (MSE) under the assumption of a stationary Gaussian process. It has been argued in <ref> [38, 27] </ref> that this is a reasonable framework in which to bound future prediction errors, particularly if one can assess the plausibility of the assumption of a stationary Gaussian process. <p> One also might predict future prediction errors by examining the cross-validation residuals. These error estimates are obtained at each observation by kriging (with the original MLE parameters) the other observations and predicting the designated observation. Similarly, it was suggested in <ref> [27] </ref> that one might cross-validate the MSEs to assess their predictive capabilities. Finally, we have found that performing a functional analysis of variance [22, 34, 38] on the surrogate function s is a useful way of identifying lower-dimensional subspaces in which most of the variation in s resides.
Reference: [28] <author> Koehler, J. R. and Owen, A. B.. </author> <year> 1996: </year> <title> Computer experiments. </title> <booktitle> In Handbook of Statistics, </booktitle> <volume> Volume 13, </volume> <pages> pages 261-308, </pages> <publisher> Elsevier Science, New York. </publisher> <editor> S. Ghosh and C. R. Rao, </editor> <publisher> editors. </publisher>
Reference-contexts: This problem has been studied extensively in the recent literature on the design and analysis of computer experiments (DACE), surveys of which include Sacks, Welch, Mitchell and Wynn [38], Booker [4] Koehler and Owen <ref> [28] </ref>. We seek designs that are "space-filling" (for lack of a better term), i.e. that will allow us to sample the behavior of the objective function throughout the feasible region. We want to avoid designs that are tied to a narrow class of approximating functions, e.g. linear or quadratic functions.
Reference: [29] <author> Levine, David. </author> <year> 1996: </year> <title> Users guide to the PGAPack parallel genetic algorithm library. </title> <type> Technical Report ANL-95/18, </type> <institution> Argonne National Laboratory. </institution> <note> Available from URL ftp://info.mcs.anl.gov/pub/pgapack/pgapack.tar.Z. 22 </note>
Reference-contexts: We used a smaller template than the default template on the distributed code. As advertised, it steadily descends to a quite good objective value, slowing down after about 1500 evaluations, but continuing to descend out to 5500 function evaluations. * GA: This is an algorithm from PGAPack <ref> [29] </ref>. We used the parameter settings suggested to us for this problem by its author David Levine of the Boeing Company. The numbers reported for GA are the total number of values requested. It performs as advertised nice initial decrease and then leveling out.
Reference: [30] <author> Lewis, Robert Michael and Torczon, Virginia. </author> <year> 1996: </year> <title> Pattern search algorithms for bound constrained minimization. </title> <type> Technical Report 96-20, </type> <institution> ICASE, NASA Langley Research Center, </institution> <address> Hampton, VA 23681-0001. </address> <note> To appear in SIAM Journal on Optimization. </note>
Reference-contexts: However, both of these topics are too important to explore fully in this paper without obscuring the basic points about the framework. Our framework exploits the convergence analysis for general pattern search methods given by <ref> [47, 30, 31] </ref> in a novel way that allows great flexibility in the heuristics one can employ. The reader interested in details of the convergence analysis for our framework should consult [39]. <p> The definition of pattern search algorithms is due to Torczon [47]. The situation with which we are concerned, optimization over a rectangular region, requires a form of the algorithm that uses 2n vectors gotten from the mesh directions (both positive and negative orientations) from any mesh point. See <ref> [30] </ref>. This set of 2n vectors must form a maximal positive basis [16]. A positive basis is a set of vectors whose nonnegative linear combinations span &lt; n . <p> unconstrained problems, a minimal positive basis is sufficient to guarantee convergence [31]; however, for problems over a rectangular feasible region, a maximal positive basis is necessary to guarantee that it is possible to move along the boundaries of the feasible region and thus prevent premature convergence to a nonstationary point <ref> [30] </ref>. <p> Obviously, the choice matters for practical performance of the algorithm, but even the most bone-headed choice is guaranteed to converge, under certain mild restrictions, by the following theorem of Lewis and Torczon <ref> [30] </ref>. Theorem 4.1.2 . <p> We can allow this flexibility because a powerful and crucial practical aspect of the theory developed by Torczon [47] and extended by Lewis and Torczon <ref> [30, 31, 32] </ref> is that it is not necessary for a pattern search algorithm to find the best next iterate on the current mesh, or the best decrease in X k any step that produces decrease will do.
Reference: [31] <author> Lewis, Robert Michael and Torczon, Virginia. </author> <year> 1996: </year> <title> Rank ordering and positive bases in pattern search algorithms. </title> <type> Technical Report 96-71, </type> <institution> ICASE, NASA Langley Research Center, </institution> <address> Hampton, VA 23681-0001. </address> <note> Submitted to Mathematical Programming. </note>
Reference-contexts: However, both of these topics are too important to explore fully in this paper without obscuring the basic points about the framework. Our framework exploits the convergence analysis for general pattern search methods given by <ref> [47, 30, 31] </ref> in a novel way that allows great flexibility in the heuristics one can employ. The reader interested in details of the convergence analysis for our framework should consult [39]. <p> A minimal positive basis can be constructed with no fewer than n+1 vectors; a maximal positive basis requires no more than 2n vectors. For unconstrained problems, a minimal positive basis is sufficient to guarantee convergence <ref> [31] </ref>; however, for problems over a rectangular feasible region, a maximal positive basis is necessary to guarantee that it is possible to move along the boundaries of the feasible region and thus prevent premature convergence to a nonstationary point [30]. <p> We can allow this flexibility because a powerful and crucial practical aspect of the theory developed by Torczon [47] and extended by Lewis and Torczon <ref> [30, 31, 32] </ref> is that it is not necessary for a pattern search algorithm to find the best next iterate on the current mesh, or the best decrease in X k any step that produces decrease will do. <p> This is a problem for all derivative-free methods. A poll step can be expensive. If it is unsuccessful, then it may require up to 2n truth evaluations <ref> [31] </ref>. In fact, our plan is that we will switch to the derivative-free optimization algorithm (DFO) [11, 12] after we encounter the first unsuccessful complete poll step.
Reference: [32] <author> Lewis, Robert Michael and Torczon, Virginia. </author> <year> 1997: </year> <title> Pattern search methods for linearly constrained minimization. </title> <note> To appear as an ICASE technical report. Submitted to SIAM Journal on Optimization. </note>
Reference-contexts: Recent work <ref> [32] </ref> shows that it is possible to construct adaptive pattern search algorithms that identify only those constraints that are either binding or "almost" binding at the current iterate so that the number of vectors needed at any given iteration can vary between n + 1 and 2n, inclusively. <p> We can allow this flexibility because a powerful and crucial practical aspect of the theory developed by Torczon [47] and extended by Lewis and Torczon <ref> [30, 31, 32] </ref> is that it is not necessary for a pattern search algorithm to find the best next iterate on the current mesh, or the best decrease in X k any step that produces decrease will do.
Reference: [33] <author> McKay, M. D.; Conover, W. J. and Beckman, R. J. </author> <title> 1979 A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. </title> <journal> Technometrics. </journal> <volume> 21(2), </volume> <pages> 239-245. </pages>
Reference-contexts: We want to be able to generate designs somewhat automatically, and we would like to be able to generate designs for irregular (nonrectangular) feasible regions. We have opted for designs that are used in quasi-Monte Carlo integration: Latin hypercube sampling (LHS) <ref> [33, 42] </ref>, orthogonal arrays (OA) [34] and OA-based LHS [43]. In LHS, each of the n variables is chosen from d equally spaced values. The OAs we use are space-filling in the following sense: The variables in the experimental design are assigned from l distinct values.
Reference: [34] <author> Owen, A. B.. </author> <year> 1992: </year> <title> Orthogonal arrays for computer experiments, integration and visualization. </title> <journal> Statistica Sinica. </journal> <volume> 2, </volume> <pages> 439-452. </pages>
Reference-contexts: We want to be able to generate designs somewhat automatically, and we would like to be able to generate designs for irregular (nonrectangular) feasible regions. We have opted for designs that are used in quasi-Monte Carlo integration: Latin hypercube sampling (LHS) [33, 42], orthogonal arrays (OA) <ref> [34] </ref> and OA-based LHS [43]. In LHS, each of the n variables is chosen from d equally spaced values. The OAs we use are space-filling in the following sense: The variables in the experimental design are assigned from l distinct values. <p> Similarly, it was suggested in [27] that one might cross-validate the MSEs to assess their predictive capabilities. Finally, we have found that performing a functional analysis of variance <ref> [22, 34, 38] </ref> on the surrogate function s is a useful way of identifying lower-dimensional subspaces in which most of the variation in s resides.
Reference: [35] <author> Rinnooy Kan, A.H.G. and Timmer, G.T.. </author> <year> 1984: </year> <title> A stochastic approach to global optimization. </title> <booktitle> In Numerical Optimization. </booktitle> <pages> pages 245-262, </pages> <address> Philadelphia. </address> <note> P.T. Boggs, R.H. Byrd and R.B. Schnabel eds. </note>
Reference-contexts: To obtain MLEs of the correlation parameters, we have attempted global optimization of the (log) likelihood function via an implementation of the algorithm in <ref> [35] </ref>. One technical difficulty with kriging should be noted. Kriging calculations require inversion of the matrix of estimated correlations between function values at the design sites.
Reference: [36] <author> Ripley, B.D. </author> . <year> 1988: </year> <title> Statistical Inference for Spatial Processes. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: Although MLE has been criticized in the spatial statistics literature, e.g. by Ripley <ref> [36] </ref>, it has been defended by others as a crude form of cross-validation [24, 15].
Reference: [37] <author> Sacks, J.; Schiller, S. B. and Welch, W. J. </author> <year> 1989: </year> <title> Designs for computer experiments. </title> <journal> Technometrics. </journal> <volume> 31(1), </volume> <pages> 41-47. </pages>
Reference: [38] <author> Sacks, J.; Welch, W. J.; Mitchell, T. J. and Wynn, H. P.. </author> <year> 1989: </year> <title> Design and analysis of computer experiments. </title> <journal> Statistical Science. </journal> <volume> 4(4), </volume> <pages> 409-435. </pages>
Reference-contexts: This problem has been studied extensively in the recent literature on the design and analysis of computer experiments (DACE), surveys of which include Sacks, Welch, Mitchell and Wynn <ref> [38] </ref>, Booker [4] Koehler and Owen [28]. We seek designs that are "space-filling" (for lack of a better term), i.e. that will allow us to sample the behavior of the objective function throughout the feasible region. <p> Although MLE has been criticized in the spatial statistics literature, e.g. by Ripley [36], it has been defended by others as a crude form of cross-validation [24, 15]. Our experience to date has been similar to that reported in <ref> [38] </ref>: "crude MLE's lead to useful prediction. . . " Assuming that the covariances in question are a constant unknown variance times unknown correlations of a specified form, there exist closed-form expressions for the MLEs of the mean and variance parameters. <p> Once a surrogate function s has been constructed, one can use it to predict values of f and also to bound the errors in such predictions. The latter is accomplished by calculating mean squared error (MSE) under the assumption of a stationary Gaussian process. It has been argued in <ref> [38, 27] </ref> that this is a reasonable framework in which to bound future prediction errors, particularly if one can assess the plausibility of the assumption of a stationary Gaussian process. <p> Similarly, it was suggested in [27] that one might cross-validate the MSEs to assess their predictive capabilities. Finally, we have found that performing a functional analysis of variance <ref> [22, 34, 38] </ref> on the surrogate function s is a useful way of identifying lower-dimensional subspaces in which most of the variation in s resides.
Reference: [39] <author> Serafini, D. B.. </author> <year> 1998: </year> <title> A Framework for Managing Models in Nonlinear Optimization of Computationally Expensive Functions. </title> <type> Ph.D. Thesis, </type> <institution> Rice University. </institution>
Reference-contexts: It is common practice not to evaluate the objective at points that violate bound constraints since infeasibility is straightforward to detect. However, it is common practice to evaluate the objective at points that violate (say) nonlinear equality constraints. Our software <ref> [39] </ref> is designed so that it can be applied regardless of the smoothness (i.e., the differentiability) of f ; however, in discussing the convergence analysis given in [39], we will need to assume some smoothness. 3 Problems of exactly the type that we have described arise in disparate ways in engi-neering <p> However, it is common practice to evaluate the objective at points that violate (say) nonlinear equality constraints. Our software <ref> [39] </ref> is designed so that it can be applied regardless of the smoothness (i.e., the differentiability) of f ; however, in discussing the convergence analysis given in [39], we will need to assume some smoothness. 3 Problems of exactly the type that we have described arise in disparate ways in engi-neering design and in control of manufacturing processes. Furthermore, there is a standard engineering practice [2] for such problems: 1. <p> Our framework exploits the convergence analysis for general pattern search methods given by [47, 30, 31] in a novel way that allows great flexibility in the heuristics one can employ. The reader interested in details of the convergence analysis for our framework should consult <ref> [39] </ref>. To illustrate the power of these results, we do not require any specific accuracy or form in the class of functions from which we choose our surrogate objective. We can deal adaptively with surrogates of any type, with no a priori information as to their relative virtue. <p> We will emphasize kriging approximations here, but in the numerical results we give an example to show that the framework described here can work with polynomial interpolants. Another example using polynomial approximations is analyzed throughly in <ref> [39] </ref>. The specific choice of a surrogate approximation is not material to the discussion in this section, but it certainly will affect the efficiency of the resulting algorithm.
Reference: [40] <author> Shewry, M.C. and Wynn, H.P.. </author> <year> 1987: </year> <title> Maximum entropy sampling. </title> <journal> Journal of Applied Statistics. </journal> <volume> 14(2), </volume> <pages> 165-170. </pages>
Reference: [41] <author> Shultz, L., et al.. </author> <year> 1994: </year> <title> Interdisciplinary analysis for advanced rotors approach, capabilities and status. </title> <booktitle> AHS Aeromechanics Specialists Conference, </booktitle> <month> January 19-21, </month> <year> 1994. </year>
Reference-contexts: The examples considered here have between 10 variables and 56 variables. As described below the objective function is a weighted sum of various harmonics of forces and moments. The analysis code used is Tech01 <ref> [41] </ref>. Tech01 is an interdisciplinary analysis code. The disciplines include dynamic structures, aerodynamics, wake modeling, and controls. The run time for a Tech01 fixed-wake analysis is roughly 20 minutes on a mid-level workstation.
Reference: [42] <author> Stein, M.. </author> <year> 1987: </year> <title> Large sample properties of simulations using latin hypercube sampling. </title> <journal> Technometrics. </journal> <volume> 29(2), </volume> <pages> 143-151. </pages>
Reference-contexts: We want to be able to generate designs somewhat automatically, and we would like to be able to generate designs for irregular (nonrectangular) feasible regions. We have opted for designs that are used in quasi-Monte Carlo integration: Latin hypercube sampling (LHS) <ref> [33, 42] </ref>, orthogonal arrays (OA) [34] and OA-based LHS [43]. In LHS, each of the n variables is chosen from d equally spaced values. The OAs we use are space-filling in the following sense: The variables in the experimental design are assigned from l distinct values.
Reference: [43] <author> Tang, B.. </author> <year> 1993: </year> <title> Orthogonal array-based latin hypercubes. </title> <journal> Journal American Statistical Association. </journal> <volume> 88(424), </volume> <pages> 1392-1397. 23 </pages>
Reference-contexts: We have opted for designs that are used in quasi-Monte Carlo integration: Latin hypercube sampling (LHS) [33, 42], orthogonal arrays (OA) [34] and OA-based LHS <ref> [43] </ref>. In LHS, each of the n variables is chosen from d equally spaced values. The OAs we use are space-filling in the following sense: The variables in the experimental design are assigned from l distinct values.
Reference: [44] <author> Torczon, Virginia. </author> <year> 1991: </year> <title> On the convergence of the multidirectional search algorithm. </title> <journal> SIAM J. Optimization. </journal> <volume> 1(1), </volume> <pages> 123-145. </pages>
Reference: [45] <author> Torczon, </author> <title> Virginia 1992: PDS: Direct search methods for unconstrained optimization on either sequential or parallel machines. </title> <type> Technical Report 92-9, </type> <institution> Department of Mathematical Sciences, Rice University, Houston, TX 77251-1892. </institution> <note> Submitted to ACM Transactions on Mathematical Software. </note>
Reference-contexts: If we discount the expense of evaluating f (x), then direct search methods [46, 50, 48] avoid many of the difficulties that we have identified. Indeed, we will show that parallel direct search (PDS) <ref> [18, 45] </ref> can solve such a problem. Our last two assumptions explain why we allow the value of f (x) to be infinite. Formally, we assign f (x) = 1 when f (x) is not available. <p> An amazing fact is that DFO requested function values at points for which only 3% failed to return a value. The numbers reported for DFO are the total number of values requested. * PDS: This is Torczon's implementation <ref> [45] </ref> of the parallel direct search method of [18]. The numbers reported for PDS are the total number of values requested. We used a smaller template than the default template on the distributed code.
Reference: [46] <author> Torczon, Virginia. </author> <year> 1995: </year> <title> Pattern search methods for nonlinear optimization SIAG/OPT Views and News 6, </title> <type> 7-11, </type> <month> Spring </month> <year> 1995. </year>
Reference-contexts: We hope that, as automatic differentiation technology advances, actual derivatives can be used rather than finite-difference approximations. However, the last assumption is again relevant because quasi-Newton methods are affected by function inaccuracies [20]. If we discount the expense of evaluating f (x), then direct search methods <ref> [46, 50, 48] </ref> avoid many of the difficulties that we have identified. Indeed, we will show that parallel direct search (PDS) [18, 45] can solve such a problem. Our last two assumptions explain why we allow the value of f (x) to be infinite.
Reference: [47] <author> Torczon, Virginia. </author> <year> 1997: </year> <title> On the convergence of pattern search algorithms. </title> <journal> SIAM J. Optimization. </journal> <volume> 7(1), </volume> <pages> 1-25. </pages>
Reference-contexts: However, both of these topics are too important to explore fully in this paper without obscuring the basic points about the framework. Our framework exploits the convergence analysis for general pattern search methods given by <ref> [47, 30, 31] </ref> in a novel way that allows great flexibility in the heuristics one can employ. The reader interested in details of the convergence analysis for our framework should consult [39]. <p> Pattern search methods allow the user to choose how much global flavor to retain in the algorithm, but the cost is that under appropriate assumptions, convergence is guaranteed to only a stationary point of the objective. The definition of pattern search algorithms is due to Torczon <ref> [47] </ref>. The situation with which we are concerned, optimization over a rectangular region, requires a form of the algorithm that uses 2n vectors gotten from the mesh directions (both positive and negative orientations) from any mesh point. See [30]. <p> We can allow this flexibility because a powerful and crucial practical aspect of the theory developed by Torczon <ref> [47] </ref> and extended by Lewis and Torczon [30, 31, 32] is that it is not necessary for a pattern search algorithm to find the best next iterate on the current mesh, or the best decrease in X k any step that produces decrease will do.
Reference: [48] <author> Torczon, Virginia and Trosset, Michael W.. </author> <year> 1997: </year> <title> From evolutionary operation to parallel direct search: pattern search algorithms for numerical optimization. </title> <note> To appear in Computing Science and Statistics. 29. </note>
Reference-contexts: We hope that, as automatic differentiation technology advances, actual derivatives can be used rather than finite-difference approximations. However, the last assumption is again relevant because quasi-Newton methods are affected by function inaccuracies [20]. If we discount the expense of evaluating f (x), then direct search methods <ref> [46, 50, 48] </ref> avoid many of the difficulties that we have identified. Indeed, we will show that parallel direct search (PDS) [18, 45] can solve such a problem. Our last two assumptions explain why we allow the value of f (x) to be infinite.
Reference: [49] <author> Trosset, Michael W. and Torczon, Virginia. </author> <year> 1997: </year> <title> Numerical optimization using computer experiments. </title> <type> ICASE Report No. 97-38, </type> <institution> Institute for Computer Applications in Science and Engineering, NASA Langley Research Center, Hampton, VA. </institution>
Reference-contexts: In these searches, one might wish to include in T k points chosen because they are indicated by some measure to be points at which the surrogate is likely to be a poor predictor of f . See <ref> [5, 49] </ref>. It is reasonable that one might want to recalibrate the surrogate using such points, and then search again for surrogate decrease before resorting to a poll step. We will discuss this more in the next section. <p> A simple implementation of this strategy, in which a finite-difference quasi-Newton method was used to find a local minimizer of the current DACE (see Section 3) approximation, is the model-assisted grid search (MAGS) described in <ref> [49] </ref>. MAGS was intended for situations in which only a very small number of function evaluations are permitted. Because it approximates the objective function over the entire feasible region, local refinement of the surrogates proceeds slowly. A "zoom-in" method for local refinement was proposed in [23].
Reference: [50] <author> Trosset, Michael W.. </author> <year> 1997: </year> <title> I know it when I see it: toward a definition of direct search methods. </title> <journal> SIAG/OPT Views and News. </journal> <volume> 9, </volume> <pages> 7-10, </pages> <month> Fall </month> <year> 1997. </year>
Reference-contexts: We hope that, as automatic differentiation technology advances, actual derivatives can be used rather than finite-difference approximations. However, the last assumption is again relevant because quasi-Newton methods are affected by function inaccuracies [20]. If we discount the expense of evaluating f (x), then direct search methods <ref> [46, 50, 48] </ref> avoid many of the difficulties that we have identified. Indeed, we will show that parallel direct search (PDS) [18, 45] can solve such a problem. Our last two assumptions explain why we allow the value of f (x) to be infinite.
Reference: [51] <author> Vecchia, A.V. </author> . <year> 1998: </year> <title> Estimation and model verification for continuous spatial process. </title> <journal> J. Royal Stat. Soc. Ser. B. </journal> <volume> 50, </volume> <pages> 297-312. </pages>
Reference: [52] <author> Watson, G.S. </author> . <year> 1984: </year> <title> Smoothing and interpolation by kriging and with splines. </title> <journal> Mathematical Geology. </journal> <volume> 16, </volume> <pages> 601-615. </pages>
Reference-contexts: For some choices of covari 7 ance function kriging is equivalent to spline interpolation, a correspondence that has been discussed in the geostatistics literature by Watson <ref> [52] </ref> and others. It is quite common in the statistics literature to motivate kriging by assuming that f is a realization of a stationary Gaussian spatial process.
Reference: [53] <author> Welch, W.J. </author> . <year> 1983: </year> <title> A mean squared error criterion for the design of experiments. </title> <journal> Biometrika. </journal> <volume> 70(1), </volume> <pages> 205-213. 24 </pages>
References-found: 52

