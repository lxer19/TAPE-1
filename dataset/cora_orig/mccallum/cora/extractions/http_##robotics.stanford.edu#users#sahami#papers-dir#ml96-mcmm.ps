URL: http://robotics.stanford.edu/users/sahami/papers-dir/ml96-mcmm.ps
Refering-URL: http://robotics.stanford.edu/users/sahami/papers.html
Root-URL: 
Email: sahami@cs.stanford.edu  hearst@parc.xerox.com  saund@parc.xerox.com  
Title: Applying the Multiple Cause Mixture Model to Text Categorization  
Author: Mehran Marti Hearst Eric Saund 
Date: 1996.  
Note: In Lorenza Saitta, ed., Machine Learning: Proc. of the Thirteenth International Conference, Morgan Kaufmann,  
Address: Gates Building 1A  Stanford, CA 94305-9010  3333 Coyote Hill Road Palo Alto, CA 94304  3333 Coyote Hill Road Palo Alto, CA 94304  
Affiliation: Sahami  Computer Science Department Stanford University  Xerox Palo Alto Research Center  Xerox Palo Alto Research Center  
Abstract: This paper introduces the use of the Multiple Cause Mixture Model to automatic text category assignment. Although much research has been done on text categorization, this algorithm is novel in that is unsupervised, that is, does not require pre-labeled training examples, and it can assign multiple category labels to documents. In this paper we present very preliminary results of the application of this model to a standard test collection, evaluating it in supervised mode in order to facilitate comparison with other methods, and showing initial results of its use in unsuper vised mode.
Abstract-found: 1
Intro-found: 1
Reference: <author> Apte, C.; Damerau, F.; and Weiss, S. M. </author> <year> 1994. </year> <title> Automated learning of decision rules for text categorization. </title> <journal> Transactions of Office Information Systems 12(3). </journal> <note> Special Issue on Text Categorization. </note>
Reference-contexts: Most algorithms assign only one label per document, or else treat the classification task as a sequence of dichotomous decisions where, for a given document, a binary yes-or-no decision is made for each category label in turn <ref> (Apte, Damerau, & Weiss 1994) </ref>. <p> For example, (Yang & Chute 1994), when comparing binary vs. weighted representations of terms, finds very little difference in the results for the two methods. Also, <ref> (Apte, Damerau, & Weiss 1994) </ref> do not make use of term weight information at all, but nevertheless achieve strong categorization results. Another assumption of the model is the independence between components of the input vector (i.e., words). <p> This approach uses Memory Based Reasoning (Stanfill & Waltz 1986), in which a highly parallel machine is used to compare a new text against all previously seen training texts in order to determine which are most similar. <ref> (Apte, Damerau, & Weiss 1994) </ref> use a large training set to learn rules (expressed in first order logic) about which combinations of terms must be present in a document in order to classify correctly in all cases. <p> The first dataset (DS1) is comprised of 983 documents from the Reuters collection, split 70%/30% into training and testing sets, respectively. These documents 1 This collection can be obtained by anonymous ftp from /pub/reuters1 on ciir-ftp.cs.umass.edu. According to <ref> (Apte, Damerau, & Weiss 1994) </ref>, free distribution for research purposes has been granted by Reuters and Carnegie Group. Arrangements for access were made by David Lewis. represent 9 labels: gold, silver, copper, coffee, soybean, livestock, treasury bill, gross national product, and yen, which we number 1 through 9, respectively. <p> For the MCMM evaluation of Table 1, the mean precision is 0.79 and mean recall is 0.77. Studies of symbolic rule induction methods using the Reuters collection give comparable precision and recall results to those given for the MCMM. For example, <ref> (Apte, Damerau, & Weiss 1994) </ref> measure precision and recall at four different points using several different system configurations, with the closest to the above score being a precision of 0.83 and recall of 0.77.
Reference: <author> Croft, W. B., and Turtle, H. R. </author> <year> 1992. </year> <title> Text retrieval and inference. </title> <editor> In Jacobs, P. S., ed., </editor> <booktitle> Text-Based Intelligent Systems: Current Research and Practice in Information Extraction and Retrieval. </booktitle> <publisher> Lawrence Erl-baum Associates. </publisher> <pages> 127-156. </pages>
Reference-contexts: The MCMM also differs from recent Bayesian network (Pearl 1988) approaches to text categorization. First, the direction of causality is from the underlying hidden causes (topics) to observed document features (similar to the approach of (Fung & DelFavero 1995), but unlike that of <ref> (Croft & Turtle 1992) </ref>). Second, while Bayesian networks may employ the same Noisy Or mixing function as the MCMM, they are to be viewed in strict probabilistic terms. The MCMM refrains from committing to a formal probabilistic interpretation and its associated tools and constraints.
Reference: <author> Deerwester, S.; Dumais, S. T.; Furnas, G. W.; Lan-dauer, T. K.; and Harshman, R. </author> <year> 1990. </year> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science 41(6) </journal> <pages> 391-407. </pages>
Reference-contexts: Another unsupervised method is that of latent semantic indexing (LSI), usually used in document comparison tasks <ref> (Deerwester et al. 1990) </ref>, (Dumais & Nielsen 1992). LSI does not require pre-encoded knowledge or pre-labeled training data; word co-occurrences are counted and recorded in a matrix which is then reduced via singular value decomposition.
Reference: <author> Duda, R., and Hart, P. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley. </publisher>
Reference-contexts: When presented with word-vector data whose distribution reflects the underlying multiple cause assumption, the Multiple Cause Mixture Model can be trained in unsupervised fashion to tease apart the various constituent word-cluster subspaces. The learning algorithm is analgous to the EM algorithm for training the standard single cause Mixture Model <ref> (Duda & Hart 1973) </ref>. <p> For purposes of comparison, we also ran a Naive-Bayesian classifier <ref> (Duda & Hart 1973) </ref> on DS1 and report these results in Table 3. Here we see that although the MCMM was not originally intended for supervised classification, it gives respectable performance according to the precision and recall metrics.
Reference: <author> Dumais, S. T., and Nielsen, J. </author> <year> 1992. </year> <title> Automating the assignment of submitted manuscripts to reviewers. </title> <booktitle> In Proceedings of the 15th Annual International ACM/SIGIR Conference, </booktitle> <pages> 233-244. </pages>
Reference-contexts: Another unsupervised method is that of latent semantic indexing (LSI), usually used in document comparison tasks (Deerwester et al. 1990), <ref> (Dumais & Nielsen 1992) </ref>. LSI does not require pre-encoded knowledge or pre-labeled training data; word co-occurrences are counted and recorded in a matrix which is then reduced via singular value decomposition. Documents that are near one another in this space can be said to belong to the same category.
Reference: <author> Fung, R., and DelFavero, B. </author> <year> 1995. </year> <title> Applying bayesian networks to information retrieval. </title> <journal> Communications of the ACM 38(3) </journal> <pages> 42-48. </pages>
Reference-contexts: Cluster units may also be only partially active, analogous to fuzzy set membership. The MCMM also differs from recent Bayesian network (Pearl 1988) approaches to text categorization. First, the direction of causality is from the underlying hidden causes (topics) to observed document features (similar to the approach of <ref> (Fung & DelFavero 1995) </ref>, but unlike that of (Croft & Turtle 1992)). Second, while Bayesian networks may employ the same Noisy Or mixing function as the MCMM, they are to be viewed in strict probabilistic terms. <p> Furthermore, our formulation places no constraints on the combinations of topics that can occur <ref> (Fung & DelFavero 1995) </ref>. 2.2 Model Assumptions Several assumptions made by the current version of the MCMM are worth noting, especially in the context of text categorization. Foremost, our model assumes that all input data is binary.
Reference: <author> Fung, R. M.; Crawford, S. L.; Appelbaum, L. A.; and Tong, R. M. </author> <year> 1990. </year> <title> An architecture for probabilistic concept-based information retrieval. </title> <booktitle> In Proceedings of the 13th International ACM/SIGIR Conference, </booktitle> <pages> 455-467. </pages>
Reference-contexts: As mentioned above, this work included experiments with variations in term weighting schemes. (Jacobs & Rau 1990), (Hayes 1992), <ref> (Fung et al. 1990) </ref> and (McCune et al. 1985) all require the system designer to hand-code extensive information about what terms in what combinations indicate which categories and the systems only work in limited domains. (Riloff & Lehnart 1994) also requires hand-coded knowledge which is incorporated into a parser; by training
Reference: <author> Hayes, P. J. </author> <year> 1992. </year> <title> Intelligent high-volume text processing using shallow, domain-specific techniques. </title>
Reference-contexts: As mentioned above, this work included experiments with variations in term weighting schemes. (Jacobs & Rau 1990), <ref> (Hayes 1992) </ref>, (Fung et al. 1990) and (McCune et al. 1985) all require the system designer to hand-code extensive information about what terms in what combinations indicate which categories and the systems only work in limited domains. (Riloff & Lehnart 1994) also requires hand-coded knowledge which is incorporated into a parser;
Reference: <editor> In Jacobs, P. S., ed., </editor> <booktitle> Text-Based Intelligent Systems: Current Research and Practice in Information Extraction and Retrieval. </booktitle> <publisher> Lawrence Erlbaum Associates. </publisher> <pages> 227-242. </pages>
Reference: <author> Hearst, M. A. </author> <year> 1994. </year> <title> Using categories to provide context for full-text retrieval results. </title> <booktitle> In Proceedings of RIAO '94; Intelligent Multimedia Information Retrieval Systems and Management, </booktitle> <pages> 115-130. </pages>
Reference-contexts: By contrast, the MCMM attempts to place documents into multiple categories when appropriate. <ref> (Hearst 1994) </ref> discusses why multiple category assignment is important for information access. 2 Topical Clustering We intend to capture the topics inherent in a text corpus by finding coherent clusters of lexical entries (words) within a high-dimensional space.
Reference: <author> Jacobs, P., and Rau, L. </author> <year> 1990. </year> <title> SCISOR: Extracting information from On-Line News. </title> <journal> Communications of the ACM 33(11) </journal> <pages> 88-97. </pages>
Reference-contexts: Their algorithm achieves impressive results on the Reuters collection (see below). (Yang & Chute 1994) learns associations between words in documents and category labels, learning a transformation from one to the other using singular value decomposition. As mentioned above, this work included experiments with variations in term weighting schemes. <ref> (Jacobs & Rau 1990) </ref>, (Hayes 1992), (Fung et al. 1990) and (McCune et al. 1985) all require the system designer to hand-code extensive information about what terms in what combinations indicate which categories and the systems only work in limited domains. (Riloff & Lehnart 1994) also requires hand-coded knowledge which is
Reference: <author> Koller, D., and Sahami, M. </author> <year> 1996. </year> <title> Toward optimal feature selection. </title> <editor> In Saitta, L., ed., </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: To help address the problem, we are currently pursuing a wide variety of methods for dimensionality reduction <ref> (Koller & Sa hami 1996) </ref> as we hope to integrate this algorithm in a real-time information access architecture in the future. Acknowledgements This work was supported by NSF/ARPA/NASA un der Stanford's Digital Libraries grant.
Reference: <author> Krishnaiah, P. R., and Kanal, L. N. </author> <year> 1982. </year> <title> Classification, Pattern Recognition, and Reduction in Dimensionality. </title> <publisher> Amsterdam: North Holland. </publisher>
Reference-contexts: This is where we believe the real strength of the MCMM lies. We ran the MCMM in unsupervised mode on the DS2 training set to see what clusters it would find without being given any label information. For comparison, we also ran K-Means clustering <ref> (Krishnaiah & Kanal 1982) </ref> on the DS2 training set. K was set to 4 to match the number of clusters found by the MCMM.
Reference: <author> Lewis, D. D., and Gale, W. A. </author> <year> 1994. </year> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of the 17th Annual International ACM/SIGIR Conference, </booktitle> <pages> 3-11. </pages>
Reference-contexts: most existing algorithms are supervised; that is, they require a large training set of pre-labeled documents. (Although there has fl This work was done while the first author was at the Xerox Palo Alto Research Center. been interesting work on how to intelligently reduce the amount of training data required <ref> (Lewis & Gale 1994) </ref>.) In this paper we describe an algorithm that can automatically induce multiple category structure underlying an unlabeled document corpus. Following an unsupervised learning phase, newly presented documents can be evaluated according to the multiple category descriptors learned.
Reference: <author> Li, Z., and D'Ambrosio, B. </author> <year> 1994. </year> <title> Efficient inference in bayes nets as a combinatorial optimization problem. </title> <journal> Int'l, Journal of Approximate Reasoning 11(1) </journal> <pages> 55-81. </pages>
Reference: <author> Masand, B.; Linoff, G.; and Waltz, D. </author> <year> 1992. </year> <title> Classifying news stories using memory based reasoning. </title> <booktitle> In Proceedings of ACM/SIGIR, </booktitle> <pages> 59-65. </pages>
Reference-contexts: This subsection briefly describes some of this work. The work of <ref> (Masand, Linoff, & Waltz 1992) </ref> is quite successful given an extremely large training set of 80,000 hand-labeled instances chosen from 350 codes (in 6 categories).
Reference: <author> McCune, B.; Tong, R.; Dean, J.; and Shapiro, D. </author> <year> 1985. </year> <title> Rubric: A system for rule-based information retrieval. </title> <journal> IEEE Transactions on Software Engineering 11(9). </journal>
Reference-contexts: As mentioned above, this work included experiments with variations in term weighting schemes. (Jacobs & Rau 1990), (Hayes 1992), (Fung et al. 1990) and <ref> (McCune et al. 1985) </ref> all require the system designer to hand-code extensive information about what terms in what combinations indicate which categories and the systems only work in limited domains. (Riloff & Lehnart 1994) also requires hand-coded knowledge which is incorporated into a parser; by training on a small number of
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In general many different options are available for the mixing function r j = r j (m; c). For the present purposes we choose for the mixing function, soft disjunction, also known as Noisy Or <ref> (Pearl 1988) </ref>: r j = 1 k Thus 0 c j;k 1 and 0 r j 1. <p> Thus multiple cluster units may be fully active, reflecting belief that a combination of several topics best accounts for the observed word vector. Cluster units may also be only partially active, analogous to fuzzy set membership. The MCMM also differs from recent Bayesian network <ref> (Pearl 1988) </ref> approaches to text categorization. First, the direction of causality is from the underlying hidden causes (topics) to observed document features (similar to the approach of (Fung & DelFavero 1995), but unlike that of (Croft & Turtle 1992)).
Reference: <author> Riloff, E., and Lehnart, W. </author> <year> 1994. </year> <title> Information extraction as a basis for high-precision text classification. </title> <journal> Transactions of Office Information Systems 12(3). </journal> <note> Special Issue on Text Categorization. </note>
Reference-contexts: work included experiments with variations in term weighting schemes. (Jacobs & Rau 1990), (Hayes 1992), (Fung et al. 1990) and (McCune et al. 1985) all require the system designer to hand-code extensive information about what terms in what combinations indicate which categories and the systems only work in limited domains. <ref> (Riloff & Lehnart 1994) </ref> also requires hand-coded knowledge which is incorporated into a parser; by training on a small number of pre-labeled texts, the terms that are important for the recognition of a particular predetermined category are identified; this work is also domain-sensitive. 2.4 Related Work in Unsupervised Categorization The MCMM
Reference: <author> Rumelhart, D. E.; Hinton, G. E.; and Williams, R. J. </author> <year> 1986. </year> <title> Learning Internal Representations by Error Propagation. </title> <publisher> MIT Press. </publisher> <address> chapter 8. </address>
Reference-contexts: It is also possible to employ the MCMM as a supervised learning algorithm where the activity vectors m i are provided by a teacher so the algorithm only needs to optimize the appropriate cluster centroids, c. Note that contrary to conventional neural network models <ref> (Rumelhart, Hinton, & Williams 1986) </ref>, there exists no direct "feedforward" computation of cluster layer activities from observed data. Also, unlike the conventional single cause mixture model, the activities m are not constrainted to sum to unity.
Reference: <author> Saund, E. </author> <year> 1995. </year> <title> A multiple cause mixture model for unsupervised learning. </title> <booktitle> Neural Computation 7 </booktitle> <pages> 51-71. </pages>
Reference-contexts: Following an unsupervised learning phase, newly presented documents can be evaluated according to the multiple category descriptors learned. This algorithm makes use of the Multiple Cause Mixture Model <ref> (Saund 1995) </ref>, which has been shown to have strong results when applied to various test data sets including a small-scale mock-up of the text categorization application. In this paper we describe the application of the Multiple Cause Mixture Model (MCMM) to the automatic discovery of document categories. <p> A mathematical model reflecting this structure is provided by the Multiple Cause Mixture Model <ref> (Saund 1995) </ref>. Figure 1 illustrates. Activity m in cluster-layer topic nodes flows top-down to "cause" activity in nodes r, which reflect predictions of how likely individual words are to appear in the document. <p> The learning algorithm is analgous to the EM algorithm for training the standard single cause Mixture Model (Duda & Hart 1973). Although the MCMM is presented in detail elsewhere <ref> (Saund 1995) </ref>, we give a brief overview here for completeness and present the assumptions of the model focusing on text categorization. 2.1 The Multiple Cause Mixture Model The Multiple Cause Mixture Model (MCMM) shares with other cluster style models the core idea that regularities in high dimensional data may be captured
Reference: <author> Stanfill, C., and Waltz, D. </author> <year> 1986. </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM 29(12) </journal> <pages> 1213-1228. </pages>
Reference-contexts: This subsection briefly describes some of this work. The work of (Masand, Linoff, & Waltz 1992) is quite successful given an extremely large training set of 80,000 hand-labeled instances chosen from 350 codes (in 6 categories). This approach uses Memory Based Reasoning <ref> (Stanfill & Waltz 1986) </ref>, in which a highly parallel machine is used to compare a new text against all previously seen training texts in order to determine which are most similar. (Apte, Damerau, & Weiss 1994) use a large training set to learn rules (expressed in first order logic) about which
Reference: <author> Willett, P. </author> <year> 1988. </year> <title> Recent trends in hierarchical document clustering: A critical review. </title> <booktitle> Information Processing and Management 24(5) </booktitle> <pages> 577-597. </pages> <month> Yahoo! </month> <year> 1995. </year> <title> On-line guide for the internet. </title> <address> http://www.yahoo.com/. </address>
Reference: <author> Yang, Y., and Chute, C. G. </author> <year> 1994. </year> <title> An example-based mapping method for text categorization and retrieval. </title> <journal> Transactions of Office Information Systems 12(3). </journal> <note> Special Issue on Text Categorization. </note>
Reference-contexts: Since longer documents will tend to have larger r values, this serves to eliminate spurious word appearances from the document vector. It should be noted, however, that some other categorization research hints that simply using unprocessed word occurrence in document vectors may not dramatically alter results. For example, <ref> (Yang & Chute 1994) </ref>, when comparing binary vs. weighted representations of terms, finds very little difference in the results for the two methods. Also, (Apte, Damerau, & Weiss 1994) do not make use of term weight information at all, but nevertheless achieve strong categorization results. <p> This is not a greedy algorithm, and so in principle must try an exponential number of combinations of words, but in practice they use local computations in order to approximate the optimal results. Their algorithm achieves impressive results on the Reuters collection (see below). <ref> (Yang & Chute 1994) </ref> learns associations between words in documents and category labels, learning a transformation from one to the other using singular value decomposition.
References-found: 24

