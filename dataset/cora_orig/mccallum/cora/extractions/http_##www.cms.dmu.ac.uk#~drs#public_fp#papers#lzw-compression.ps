URL: http://www.cms.dmu.ac.uk/~drs/public_fp/papers/lzw-compression.ps
Refering-URL: http://www.cms.dmu.ac.uk/~drs/public_fp/papers/
Root-URL: 
Title: LZW Text Compression in Haskell  
Author: Paul Sanders Colin Runciman 
Address: Martlesham Heath, Ipswich  Heslington, York.  
Affiliation: Applications Research Division, BT Laboratories  Department of Computer Science, University of York  
Abstract: Functional programming is largely untested in the industrial environment. This paper summarises the results of a study into the suitability of Haskell in the area of text compression, an area with definite commercial interest. Our program initially performs very poorly in comparison with a version written in C. Experiments reveal the cause of this to be the large disparity in the relative speed of I/O and bit-level operations and also a space leak inherent in the Haskell definition.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J Hughes. </author> <title> Why functional programming matters. </title> <journal> The Computer Journal, </journal> <volume> 32(2) </volume> <pages> 98-107, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Claims for the advantages of functional programming languages abound <ref> [1] </ref> but industrial take-up of the paradigm has been almost non-existent. Two of the main reasons for this are: * lack of proof that the advantages apply to large-scale developments; * lack of industry-strength compilers and environments, particularly for high speed computation.
Reference: [2] <editor> P Hudak, S Peyton-Jones, and P Wadler (Editors). </editor> <title> Report on the pro gramming language Haskell. Version 1.2. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 27(5), </volume> <year> 1992. </year>
Reference-contexts: It is therefore important to choose an application where speed is of interest so that it can be judged whether or not these criticisms are still valid. A version of the compress program has been implemented in Haskell <ref> [2] </ref> on a Sun workstation. 2 LZW Compression There are many algorithms for data compression [3]. LZW-compression [4] is a classic being found in an advanced variant on most Unix systems as the program compress.
Reference: [3] <author> T Bell, I Witten, and J Cleary. </author> <title> Modelling for text compression. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(4), </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: A version of the compress program has been implemented in Haskell [2] on a Sun workstation. 2 LZW Compression There are many algorithms for data compression <ref> [3] </ref>. LZW-compression [4] is a classic being found in an advanced variant on most Unix systems as the program compress. The algorithm works by maintaining a table of strings and associated code values which grows as the input is processed. <p> is not directly comparable; we will refer to it in the tables however to provide a reference for the order of performance we should be aiming at.) The performance results have been obtained by running each program on three of the benchmark files from the Calgary Text Compression Corpus 1 <ref> [3] </ref>: bib (a bibliography unusual English, 111Kb), paper1 (troff source usual English, 53Kb) and geo (non-ascii data file, 101Kb). 3.1.1 Performance Speed Table 2 shows average CPU times recorded for running the programs on the benchmarks, together with the slowdown factor compared with the straightforward C version (LZWC) in each case.
Reference: [4] <author> T Welch. </author> <title> A technique for high-performance data compression. </title> <journal> Computer, </journal> <volume> 17(6) </volume> <pages> 8-19, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: A version of the compress program has been implemented in Haskell [2] on a Sun workstation. 2 LZW Compression There are many algorithms for data compression [3]. LZW-compression <ref> [4] </ref> is a classic being found in an advanced variant on most Unix systems as the program compress. The algorithm works by maintaining a table of strings and associated code values which grows as the input is processed. <p> For example: 4; 11 ) 000000000110000000001011 ) 0; 96; 11 One problem remains what to do when the number of codes exceeds what can be represented in the chosen word size ? Welch suggests <ref> [4] </ref> that entries are no longer added to the table. Our word size of 12 bits means that we will stop adding entries when we reach 4096 elements. 3 A Haskell Implementation of LZW We shall describe only the encoding function.
Reference: [5] <author> D Knuth. </author> <title> Sorting and Searching, </title> <booktitle> The Art of Computer Programming, </booktitle> <volume> vol ume 3. </volume> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1973. </year>
Reference-contexts: In a language with side-effects we might use an array and hash function to provide fast indexing into the table. However, there is no such constant-time access structure available in current implementations of Haskell. The best we can do is use a logarithmic-time structure such as the trie <ref> [5] </ref>: &gt; data PrefixTree a b &gt; = PTNil | &gt; PT a b (PrefixTree a b) (PrefixTree a b) (PrefixTree a b) Each element of the trie contains the prefix character, its code, a trie containing the possible extensions of that character and the two tries comprising the left and
Reference: [6] <editor> S L Peyton Jones. </editor> <booktitle> The Implementation of Functional Programming. Prentice-Hall International, </booktitle> <year> 1987. </year>
Reference-contexts: Moreover, it is usually impossible to find out File LZWC LZWH Slowdown Factor bib 1.34 6.36 4.7 paper1 0.7 3.82 5.5 Table 4: Average times to perform the bit-level operations exactly how much store is being used or what is responsible for it. Chapter 23 of <ref> [6] </ref> gives some good examples of this. Recently, Runciman and Wakeling [7] have devised a scheme whereby the heap usage of a Haskell program may be profiled.
Reference: [7] <author> C Runciman and D Wakeling. </author> <title> Heap profiling of lazy functional programs. </title> <type> Technical Report YCS172, </type> <institution> University of York Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Chapter 23 of [6] gives some good examples of this. Recently, Runciman and Wakeling <ref> [7] </ref> have devised a scheme whereby the heap usage of a Haskell program may be profiled. They have extended the Haskell compiler with a profiler which allows the programmer to produce a graph of the producers of heap cells or of the constructors being produced.
Reference: [8] <author> D Sleator and R Tarjan. </author> <title> Self-adjusting binary search trees. </title> <journal> Journal of the ACM, </journal> <volume> 32(3) </volume> <pages> 652-686, </pages> <year> 1985. </year>
Reference-contexts: There is still scope for improvement in the Haskell program. A tree structure which adapts itself to the pattern of usage might prove beneficial. Trees such as the splay tree <ref> [8] </ref> and the weighted path length tree [9] which modify themselves on read and writes could improve access times to the table. Further, once the table is full it is used in a read-only manner.
Reference: [9] <author> G Argo. </author> <title> Weighting without waiting: the weighted path length tree. </title> <journal> Com puter Journal, </journal> <year> 1991. </year>
Reference-contexts: There is still scope for improvement in the Haskell program. A tree structure which adapts itself to the pattern of usage might prove beneficial. Trees such as the splay tree [8] and the weighted path length tree <ref> [9] </ref> which modify themselves on read and writes could improve access times to the table. Further, once the table is full it is used in a read-only manner.
References-found: 9

