URL: http://www.lsi.upc.es/dept/techreps/ps/R98-1.ps.gz
Refering-URL: http://www.lsi.upc.es/dept/techreps/1998.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Optimal Sampling Strategies in Quicksort and  
Author: Quickselect Conrado Martnez and Salvador Roura 
Keyword: ysis of quickselect, a general-purpose selection algorithm closely related  
Date: January 7, 1998  
Abstract: It is well known that the performance of quicksort can be substantially improved by selecting the median of a sample of three elements as the pivot of each partitioning stage. This variant is easily generalized to samples of size s = 2k + 1. For large samples the partitions are better as the median of the sample makes a more accurate estimate of the median of the array to be sorted, but the amount of additional comparisons and exchanges to find the median of the sample also increases. We show that the optimal sample size to minimize the average total cost of quicksort (which includes both comparisons and exchanges) is s = a n ). We also give a closed expression for the constant factor a, which depends on the median-finding algorithm and the costs of elementary comparisons and exchanges. The result above holds in most situations, unless the cost of an exchange exceeds by far the cost of a comparison. In that particular case, it is better to select not the median of the samples, but the (p + 1) th element. The value of p can be precisely determined as a function of the ratio between the cost of an exchange and the cost of a comparison. We also show how to apply the same ideas and techniques to the anal to quicksort, and get similar results to those for quicksort. 1 Introduction n + o(
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.H. Anderson and R. Brown. </author> <title> Combinatorial aspects of C.A.R. Hoare's Find algorithm. </title> <journal> Australasian Journal of Combinatorics, </journal> <volume> 5 </volume> <pages> 109-119, </pages> <year> 1992. </year>
Reference-contexts: Quite recently, it has been shown that quickselect with median-of-three yields significant savings over the standard quickselect algorithm <ref> [1, 9] </ref>. Ideally, using large samples helps, but there are two problems. The first one is easily exemplified. Assume that we take s = 1001. This is a large sample size, indeed. <p> Equations (4), (6) and (11), we have q ~ (s; p) = 0 u (z) (1 + z (1 z)~) dz 0 u (z) (z ln z (1 z) ln (1 z)) dz We will use the following fact: Let f (z), g (z) be positive functions over the interval <ref> [0; 1] </ref>. Let z fl be the location of a minimum of f (z)=g (z), and assume 14 q 5 (s; p) that 0 &lt; z fl &lt; 1 and g (z) 6= 0 for 0 &lt; z &lt; 1. <p> Then, as f (z) g (z)f (z fl )=g (z fl ) and u (z) is also positive in the interval <ref> [0; 1] </ref> we have R 1 R 1 0 u (z) [g (z)f (z fl )=g (z fl )] dz 0 u (z)g (z) dz f (z fl ) : Now taking f (z) = 1 + z (1 z)~, g (z) = z ln z (1 z) ln (1 z),
Reference: [2] <author> J.L. Bentley and M.D. McIlroy. </author> <title> Engineering a sort function. </title> <journal> Software| Practice and Experience, </journal> <volume> 23 </volume> <pages> 1249-1265, </pages> <year> 1993. </year>
Reference-contexts: In this variant of quicksort, we select a pivot in each recursive stage by taking a sample of three elements and using the median of the sample as the pivot. The idea is that it is more likely that no subarray is degenerate after the partitioning <ref> [2, 15, 20] </ref>. This variant is easily generalized to samples of size s = 2k + 1 elements, so that the (k + 1) th element in the sample is selected as the pivot. <p> Finally, notice that for any ~ there is always an interval <ref> [ 1 ; 2 ] </ref> with 0 1 &lt; 2 1 such that q ~ ( ) &gt; q ~ ( fl (~)) and u (z) &gt; 0 for every in ( 1 ; 2 ). Thus q ~ (s; p) &gt; q ~ ( fl ).
Reference: [3] <author> M. Blum, R.W. Floyd, V.R. Pratt, R.L. Rivest, and R. Tarjan. </author> <title> Time bounds for selection. </title> <journal> J. Computer and System Sciences, </journal> <volume> 7 </volume> <pages> 448-461, </pages> <year> 1973. </year>
Reference-contexts: For example, the expected number of comparisons to select the median with standard quickselect is 2 (1+ln 2)s+o (s). Recall <ref> [3, 4] </ref> that the expected number of comparisons to select the (p + 1) th out of s elements is bounded by below by 2 fi fi 1 s + 1 fi fi s + o (s); and hence if p = b (s 1)=2c) then fi 1:5 u c .
Reference: [4] <author> R.W. Floyd and R.L. Rivest. </author> <title> Expected time bounds for selection. </title> <journal> Communications of the ACM, </journal> <volume> 18 </volume> <pages> 165-173, </pages> <year> 1975. </year>
Reference-contexts: For example, the expected number of comparisons to select the median with standard quickselect is 2 (1+ln 2)s+o (s). Recall <ref> [3, 4] </ref> that the expected number of comparisons to select the (p + 1) th out of s elements is bounded by below by 2 fi fi 1 s + 1 fi fi s + o (s); and hence if p = b (s 1)=2c) then fi 1:5 u c .
Reference: [5] <author> G.H. Gonnet and R. Baeza-Yates. </author> <title> Handbook of Algorithms and Data Structures In Pascal and C. </title> <publisher> Addison-Wesley, </publisher> <address> 2nd edition, </address> <year> 1991. </year>
Reference-contexts: Both algorithms are based upon the divide-and-conquer principle and operate using similar ideas. (A brief, but complete description is given in Section 2. Excellent sources for background information and further references on quicksort and quickselect and their analysis include <ref> [5, 10, 15, 16, 18, 17, 19] </ref>.) Contrary to other divide-and-conquer algorithms, quicksort and quickselect are not guaranteed to divide the problem into subproblems of approximately the same size.
Reference: [6] <author> R.L. Graham, D.E. Knuth, and O. Patashnik. </author> <title> Concrete Mathematics. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference: [7] <author> C.A.R. Hoare. </author> <title> Find (Algorithm 65). </title> <journal> Communications of the ACM, </journal> <volume> 4 </volume> <pages> 321-322, </pages> <year> 1961. </year>
Reference-contexts: 1 Introduction Early in the sixties, C.A.R. Hoare devised two efficient algorithms, quicksort and quickselect (also known as Hoare's Find algorithm and as one-sided quick-sort ), for internal sorting and selection, respectively, both of great theoretical and practical importance <ref> [7, 8] </ref>.
Reference: [8] <author> C.A.R. Hoare. </author> <title> Quicksort. </title> <journal> Computer Journal, </journal> <volume> 5 </volume> <pages> 10-15, </pages> <year> 1962. </year>
Reference-contexts: 1 Introduction Early in the sixties, C.A.R. Hoare devised two efficient algorithms, quicksort and quickselect (also known as Hoare's Find algorithm and as one-sided quick-sort ), for internal sorting and selection, respectively, both of great theoretical and practical importance <ref> [7, 8] </ref>.
Reference: [9] <author> P. Kirschenhofer, H. Prodinger, and C. Martnez. </author> <title> Analysis of Hoare's Find algorithm with median-of-three partition. Random Structures & Algorithms, </title> <booktitle> 10 </booktitle> <pages> 143-156, </pages> <year> 1997. </year>
Reference-contexts: Quite recently, it has been shown that quickselect with median-of-three yields significant savings over the standard quickselect algorithm <ref> [1, 9] </ref>. Ideally, using large samples helps, but there are two problems. The first one is easily exemplified. Assume that we take s = 1001. This is a large sample size, indeed.
Reference: [10] <author> D.E. Knuth. </author> <title> The Art of Computer Programming: Sorting and Searching, volume 3. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Both algorithms are based upon the divide-and-conquer principle and operate using similar ideas. (A brief, but complete description is given in Section 2. Excellent sources for background information and further references on quicksort and quickselect and their analysis include <ref> [5, 10, 15, 16, 18, 17, 19] </ref>.) Contrary to other divide-and-conquer algorithms, quicksort and quickselect are not guaranteed to divide the problem into subproblems of approximately the same size.
Reference: [11] <author> H. Mahmoud, R. Modarres, and R. Smythe. </author> <title> Analysis of quickselect: An algorithm for order statistics. </title> <journal> RAIRO, Theoretical Informatics & Applications, </journal> <volume> 29 </volume> <pages> 255-276, </pages> <year> 1995. </year>
Reference-contexts: This analysis is somewhat more complicated than the analysis of its expected performance, but feasible. Recall that the variance of the number of comparisons made by standard quickselect is fi (n 2 ) <ref> [11] </ref>. Thus, the expected cost and the standard deviation are of the same order of magnitude, and this implies that there is a low but non-negligible probability that the number of comparisons actually made is much larger than the expected number.
Reference: [12] <author> C.C. McGeoch and J.D. Tygar. </author> <title> Optimal sampling strategies for quicksort. Random Structures & Algorithms, </title> <booktitle> 7 </booktitle> <pages> 287-300, </pages> <year> 1995. </year>
Reference-contexts: The time spent in finding the median of the samples shows up in larger lower order terms of the average performance, growing with the sample size, so that they cannot be disregarded unless n is impractically large. As far as we know, McGeoch and Tygar <ref> [12] </ref> were the first authors that analyzed the performance of quicksort with sampling when the size of the samples is not fixed, but grows as a function of n, the size of the (sub)array to be sorted. <p> We also find the constant factor of the main term in s fl . Our results state that p sampling is optimal for quicksort with respect to comparisons, if we assume that the selected pivots are the medians of the samples. In <ref> [12] </ref> it was conjectured that the optimal sample size w.r.t. the number of comparisons in quicksort is fi (n 0:56 ), by curve-fitting of the exact optimal sample sizes for various values of n. These exact values were found by a dynamic-programming algorithm. Our results disprove this conjecture. <p> This provides a further check for the conclusions of Section 4, and is consistent with the observation that the optimal samples in that situation have constant size. for the number of comparisons. The data comes from <ref> [12] </ref>.
Reference: [13] <author> S. Roura. </author> <title> Divide-and-Conquer Algorithms and Data Structures. </title> <type> PhD thesis, </type> <institution> Dept. Llenguatges i Sistemes Informatics, U. Politecnica de Catalunya, </institution> <month> October </month> <year> 1997. </year> <month> 33 </month>
Reference-contexts: The basic tools for our analysis are the continuous master theorem for divide-and-conquer recurrences (CMT, for short) and several related results <ref> [13] </ref> (see also [14]). The CMT provides almost automatically all the results for quickselect and quicksort with fixed-sized sampling | some of which were already known after Van Emden's paper. <p> This analysis is almost straightforward using the CMT <ref> [13] </ref>. <p> : (2) It is not difficult to show that ! (s;p) (z) = p! q! z p+1 (1 z) q + z q+1 (1 z) p 8 is the shape function in this case, and then we can compute the limiting const--entropy H (s; p) associated to this problem (in <ref> [13] </ref>, the const-entropy is denoted H n and its limit by H (1) ). <p> As the toll function is fi (1), the limiting const-entropy is now H (s; p) = 1'(0) = 0, and we are led to compute the limiting log-entropy (denoted H (ln) in <ref> [13] </ref>), which is 0 (s; p) = (c + 1) 0 with b = c = 0, as the toll function is fi . <p> Thus, the expected cost and the standard deviation are of the same order of magnitude, and this implies that there is a low but non-negligible probability that the number of comparisons actually made is much larger than the expected number. In <ref> [13] </ref> it is shown that, under some mild assumptions, it is fairly easy to systematically obtain recurrences for E fi n , the second moment of the random variable X n denoting the cost of a one-branch or a two-branch divide-and-conquer algorithm.
Reference: [14] <author> S. Roura. </author> <title> An improved master theorem for divide-and-conquer recurrences. </title> <editor> In P. Degano, R. Gorrieri, and A. Marchetti-Spaccamela, editors, </editor> <booktitle> Proc. of the 24th Int. Colloquium on Automata, Languages and Programming (ICALP'97), volume 1256 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer, </publisher> <year> 1997. </year>
Reference-contexts: The basic tools for our analysis are the continuous master theorem for divide-and-conquer recurrences (CMT, for short) and several related results [13] (see also <ref> [14] </ref>). The CMT provides almost automatically all the results for quickselect and quicksort with fixed-sized sampling | some of which were already known after Van Emden's paper.
Reference: [15] <author> R. Sedgewick. </author> <title> The analysis of quicksort programs. </title> <journal> Acta Informatica, </journal> <volume> 7 </volume> <pages> 327-355, </pages> <year> 1976. </year>
Reference-contexts: Both algorithms are based upon the divide-and-conquer principle and operate using similar ideas. (A brief, but complete description is given in Section 2. Excellent sources for background information and further references on quicksort and quickselect and their analysis include <ref> [5, 10, 15, 16, 18, 17, 19] </ref>.) Contrary to other divide-and-conquer algorithms, quicksort and quickselect are not guaranteed to divide the problem into subproblems of approximately the same size. <p> In this variant of quicksort, we select a pivot in each recursive stage by taking a sample of three elements and using the median of the sample as the pivot. The idea is that it is more likely that no subarray is degenerate after the partitioning <ref> [2, 15, 20] </ref>. This variant is easily generalized to samples of size s = 2k + 1 elements, so that the (k + 1) th element in the sample is selected as the pivot.
Reference: [16] <author> R. Sedgewick. </author> <title> Implementing quicksort programs. </title> <journal> Communications of the ACM, </journal> <volume> 21 </volume> <pages> 847-856, </pages> <year> 1978. </year>
Reference-contexts: Both algorithms are based upon the divide-and-conquer principle and operate using similar ideas. (A brief, but complete description is given in Section 2. Excellent sources for background information and further references on quicksort and quickselect and their analysis include <ref> [5, 10, 15, 16, 18, 17, 19] </ref>.) Contrary to other divide-and-conquer algorithms, quicksort and quickselect are not guaranteed to divide the problem into subproblems of approximately the same size.
Reference: [17] <author> R. Sedgewick. </author> <title> Quicksort. </title> <publisher> Garland, </publisher> <year> 1978. </year>
Reference-contexts: Both algorithms are based upon the divide-and-conquer principle and operate using similar ideas. (A brief, but complete description is given in Section 2. Excellent sources for background information and further references on quicksort and quickselect and their analysis include <ref> [5, 10, 15, 16, 18, 17, 19] </ref>.) Contrary to other divide-and-conquer algorithms, quicksort and quickselect are not guaranteed to divide the problem into subproblems of approximately the same size. <p> We will find that, if only comparisons are considered, the best choice is to select the median, a result already proved by Sedgewick <ref> [17] </ref> for quicksort. But if both comparisons and exchanges are taken into account, the best choice for p will depend, in the case of quicksort, on the ratio between the cost of a single exchange and the cost of a single comparison. <p> However, it seems that, apart from brute-force solutions, no such selection mechanism exists for general s. (In <ref> [17] </ref>, Sedgewick thoroughly discusses the benefits that the partition procedure is randomness-preserving |the selection of the pivot included as a part of that procedure|, and the dangers of partitioning strategies that do not have that property.) Let S (s; p) denote the average total cost of the algorithm that selects the <p> As with quickselect, q ~ (2k+1; k) is a strictly decreasing function of k, and hence we should get samples as large as possible and select the medians of the samples as pivots. (This conclusion, in fact an even stronger statement, was already proved by Sedgewick <ref> [17] </ref>.
Reference: [18] <author> R. Sedgewick. </author> <title> Algorithms in C, volume 1. </title> <publisher> Addison-Wesley, 3rd edition, </publisher> <year> 1997. </year>
Reference-contexts: Both algorithms are based upon the divide-and-conquer principle and operate using similar ideas. (A brief, but complete description is given in Section 2. Excellent sources for background information and further references on quicksort and quickselect and their analysis include <ref> [5, 10, 15, 16, 18, 17, 19] </ref>.) Contrary to other divide-and-conquer algorithms, quicksort and quickselect are not guaranteed to divide the problem into subproblems of approximately the same size.
Reference: [19] <author> R. Sedgewick and Ph. Flajolet. </author> <title> An Introduction to the Analysis of Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: Both algorithms are based upon the divide-and-conquer principle and operate using similar ideas. (A brief, but complete description is given in Section 2. Excellent sources for background information and further references on quicksort and quickselect and their analysis include <ref> [5, 10, 15, 16, 18, 17, 19] </ref>.) Contrary to other divide-and-conquer algorithms, quicksort and quickselect are not guaranteed to divide the problem into subproblems of approximately the same size.
Reference: [20] <author> R.C. </author> <title> Singleton. Algorithm 347: An efficient algorithm for sorting with minimal storage. </title> <journal> Communications of the ACM, </journal> <volume> 12 </volume> <pages> 185-187, </pages> <year> 1969. </year>
Reference-contexts: In this variant of quicksort, we select a pivot in each recursive stage by taking a sample of three elements and using the median of the sample as the pivot. The idea is that it is more likely that no subarray is degenerate after the partitioning <ref> [2, 15, 20] </ref>. This variant is easily generalized to samples of size s = 2k + 1 elements, so that the (k + 1) th element in the sample is selected as the pivot.
Reference: [21] <author> M.H. van Emden. </author> <title> Increasing the efficiency of quicksort. </title> <journal> Communications of the ACM, </journal> <volume> 13 </volume> <pages> 563-567, </pages> <year> 1970. </year>
Reference-contexts: This variant is easily generalized to samples of size s = 2k + 1 elements, so that the (k + 1) th element in the sample is selected as the pivot. Van Emden <ref> [21] </ref> analyzed this generalized variant of quicksort using wise information-theoretic arguments, showing that the average number of comparisons made to sort an array of size n is q (k) n ln n + o (n log n), where the coefficient q (k) steadily decreases with k, from q (0) = 2 <p> For any positive integers p and q, if s = p + q + 1 then VE (s; p) = H s+1 s + 1 The name of this function follows after Van Emden, who first used it for the particular case where p = q <ref> [21] </ref>. <p> have Q n = q ~ (s; p) n ln n + o (n log n); (10) where q ~ (s; p) = (p+1)(q+1) VE (s; p) Setting s = 2k + 1, p = q = k and ~ = 0, we recover the original result of Van Emden <ref> [21] </ref>. It is possible to get more information about Q n by subtracting the main order term from Q n and setting up a recurrence for the remaining part, i.e., for R n = Q n q ~ (s; p) nH n .
References-found: 21

