URL: http://www.cs.utah.edu/~cs686/Previous/w97/CS-94-10.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/w97/
Root-URL: 
Title: Access Order and Memory-Conscious Cache Utilization  
Author: Sally A. McKee and Wm. A. Wulf 
Abstract: Computer Science Report No. CS-94-10 March 1, 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baer, J.-L., and Chen, T.-F., </author> <title> An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty, </title> <booktitle> Supercomputing 91, </booktitle> <month> November, </month> <year> 1991. </year>
Reference-contexts: Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches <ref> [1, 6, 34] </ref>. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> system. a) stride 1 b) stride 2 c) stride 3 d) stride 4 Appeared in 1st Symposium on High Performance Computer Architecture, Raleigh, NC, January 1995 9 Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [1, 4, 11, 16, 27, 34] </ref>. These methods can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [2] <author> Benitez, M.E., and Davidson, J.W., </author> <title> Code Generation for Streaming: An Access/Execute Mechanism, </title> <address> ASPLOS-IV, </address> <month> April, </month> <year> 1991. </year>
Reference-contexts: The first, naive ordering, provides a basis for comparing the performance improvements of the other schemes. None of the techniques requires heroic compiler technology: the compiler need only detect streams, as in Benitez and Davidsons algorithm <ref> [2] </ref>. Dynamic access ordering requires a small amount of special-purpose hardware [25], and both static and dynamic access ordering depend on the availability of non-caching load instructions. Although rare, these instructions are available in some commercial processors, such as the Convex C-1 [37] and Intel i860 [15].
Reference: [3] <author> Budnik, P., and Kuck, D., </author> <title> The Organization and Use of Parallel Memories, </title> <journal> IEEE Trans. Comput., </journal> <volume> 20, 12, </volume> <year> 1971. </year>
Reference-contexts: Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations [12, 13], unusual memory systems <ref> [3, 10, 33, 36] </ref>, and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Our results support these conclusions. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published <ref> [3, 10, 12, 13, 33] </ref>; these, too, are complementary to the techniques for improving bandwidth that we analyze here. Moyer [28] and Lee [19] investigate the oating point and memory performance of the i860XR. Our examples for this architecture agree largely with their findings. 5.
Reference: [4] <author> Callahan, D., et.al., </author> <title> Software Prefetching, </title> <address> ASPLOS-IV, </address> <month> April, </month> <year> 1991. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching <ref> [4, 16, 27] </ref> and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. <p> system. a) stride 1 b) stride 2 c) stride 3 d) stride 4 Appeared in 1st Symposium on High Performance Computer Architecture, Raleigh, NC, January 1995 9 Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [1, 4, 11, 16, 27, 34] </ref>. These methods can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [5] <author> Carr, S., Kennedy, K., </author> <title> Blocking Linear Algebra Codes for Memory Hierarchies, </title> <booktitle> Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1989. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling <ref> [5, 8, 9, 18, 32, 38] </ref>, to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [8, 9, 5, 32, 38, 18, 35] </ref>. These studies assume a uniform memory access cost, thus they dont address minimizing the time to load vector data into cache. These techniques will also deliver better performance when integrated with methods to make more efficient use of memory resources.
Reference: [6] <author> Chen, T.-F., and Baer, J.-L., </author> <title> Reducing Memory Latency via Non-blocking and Prefetching Caches, </title> <type> TR UW-CSE-92-06-03, </type> <institution> Univ. of Washington, </institution> <month> July, </month> <year> 1992. </year>
Reference-contexts: Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches <ref> [1, 6, 34] </ref>. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes.
Reference: [7] <institution> Alpha Architecture Handbook, Digital Equipment Corp., </institution> <year> 1992. </year>
Reference-contexts: Although rare, these instructions are available in some commercial processors, such as the Convex C-1 [37] and Intel i860 [15]. Other architectures, such as the DEC Alpha <ref> [7] </ref>, provide a means of specifying some portions of memory as 1. These devices behave as if implemented with a single on-chip cache line, or page. A memory access falling outside the address range of the current DRAM page forces a new page to be accessed. <p> Even fewer instructions are required on architectures that prefetch larger blocks, such as the Alpha with its 512-byte FETCH <ref> [7] </ref>.
Reference: [8] <author> Gallivan, K., et.al., </author> <title> The Impact of Hierarchical Memory Systems on Linear Algebra Algorithm Design, </title> <type> TR UIUCSRD 625, </type> <institution> Univ. of Illinois, </institution> <year> 1987. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling <ref> [5, 8, 9, 18, 32, 38] </ref>, to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Note that the effectiveness of naive ordering decreases rapidly as vector stride increases. 2.2 Block prefetching Blocking or tiling changes a computation so that subblocks of data are repeatedly manipulated <ref> [8, 9, 18, 32, 38] </ref>. <p> Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [8, 9, 5, 32, 38, 18, 35] </ref>. These studies assume a uniform memory access cost, thus they dont address minimizing the time to load vector data into cache. These techniques will also deliver better performance when integrated with methods to make more efficient use of memory resources.
Reference: [9] <author> Gannon, D., and Jalby, W., </author> <title> The Inuence of Memory Hierarchy on Algorithm Organization: Programming FFTs on a Vector Multiprocessor, in The Characteristics of Parallel Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling <ref> [5, 8, 9, 18, 32, 38] </ref>, to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Note that the effectiveness of naive ordering decreases rapidly as vector stride increases. 2.2 Block prefetching Blocking or tiling changes a computation so that subblocks of data are repeatedly manipulated <ref> [8, 9, 18, 32, 38] </ref>. <p> For instance, in parallelizing a Fast Fourier Transform, Gannon and Jalby use copying to generate the transpose of a matrix, T prefetch t miss m 1 ( ) t hit + b t hit giving both row-wise and column-wise array accesses the same locality of reference <ref> [9] </ref>. Lam et. al. investigate blocking in conjunction with copying in order to eliminate self-interference, or cache misses caused by more than one element of a given vector mapping to the same location [18]. <p> Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [8, 9, 5, 32, 38, 18, 35] </ref>. These studies assume a uniform memory access cost, thus they dont address minimizing the time to load vector data into cache. These techniques will also deliver better performance when integrated with methods to make more efficient use of memory resources.
Reference: [10] <author> Gao, </author> <title> Q.S., The Chinese Remainder Theorem and the Prime Memory System, </title> <booktitle> 20th ISCA, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations [12, 13], unusual memory systems <ref> [3, 10, 33, 36] </ref>, and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Our results support these conclusions. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published <ref> [3, 10, 12, 13, 33] </ref>; these, too, are complementary to the techniques for improving bandwidth that we analyze here. Moyer [28] and Lee [19] investigate the oating point and memory performance of the i860XR. Our examples for this architecture agree largely with their findings. 5.
Reference: [11] <author> Gupta, A., et.al., </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques, </title> <booktitle> 18th ISCA, </booktitle> <month> May, </month> <year> 1991. </year>
Reference-contexts: system. a) stride 1 b) stride 2 c) stride 3 d) stride 4 Appeared in 1st Symposium on High Performance Computer Architecture, Raleigh, NC, January 1995 9 Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [1, 4, 11, 16, 27, 34] </ref>. These methods can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [12] <author> Harper, D. T., Jump, J., </author> <title> Vector Access Performance in Parallel Memories Using a Skewed Storage Scheme, </title> <journal> IEEE Trans. Comput., </journal> <volume> 36, 12, </volume> <year> 1987. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations <ref> [12, 13] </ref>, unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Our results support these conclusions. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published <ref> [3, 10, 12, 13, 33] </ref>; these, too, are complementary to the techniques for improving bandwidth that we analyze here. Moyer [28] and Lee [19] investigate the oating point and memory performance of the i860XR. Our examples for this architecture agree largely with their findings. 5.
Reference: [13] <author> Harper, D. T., </author> <title> Address Transformation to Increase Memory Performance, </title> <booktitle> 1989 International Conference on Supercomputing. </booktitle>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations <ref> [12, 13] </ref>, unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Our results support these conclusions. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published <ref> [3, 10, 12, 13, 33] </ref>; these, too, are complementary to the techniques for improving bandwidth that we analyze here. Moyer [28] and Lee [19] investigate the oating point and memory performance of the i860XR. Our examples for this architecture agree largely with their findings. 5.
Reference: [14] <author> High-speed DRAMs, </author> <title> Special Report, </title> <journal> IEEE Spectrum, </journal> <volume> vol. 29, no. 10, </volume> <month> October, </month> <year> 1992. </year> <title> [15] i860 XP Microprocessor Data Book, </title> <institution> Intel Corp., </institution> <year> 1991. </year>
Reference-contexts: For instance, nearly all current DRAMs implement a form of page-mode operation [30]. Other common devices offer similar features (nibble-mode, static column mode, or a small amount of SRAM cache on chip) or exhibit novel organizations (such as Rambus [31], Ramlink, and the new synchronous DRAM designs <ref> [14] </ref>). The order of requests strongly affects the performance of all these components. A comprehensive, successful solution to the memory bandwidth problem must exploit the richness of the full memory hierarchy, both its architecture and its component characteristics.
Reference: [16] <author> Klaiber, A., et. al., </author> <title> An Architecture for Software-Controlled Data Prefetching, </title> <booktitle> 18th ISCA, </booktitle> <month> May, </month> <year> 1991. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching <ref> [4, 16, 27] </ref> and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. <p> system. a) stride 1 b) stride 2 c) stride 3 d) stride 4 Appeared in 1st Symposium on High Performance Computer Architecture, Raleigh, NC, January 1995 9 Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [1, 4, 11, 16, 27, 34] </ref>. These methods can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [17] <author> Laird, M., </author> <title> A Comparison of Three Current Superscalar Designs, Computer Architecture News, </title> <address> 20:3, </address> <month> June, </month> <year> 1992. </year>
Reference-contexts: This allows us to amortize the overhead of the page miss over as many accesses as there are registers available to hold data. The Intel i960MM has a local register cache with 240 entries that could be used to store vector elements for this scheme <ref> [17] </ref>, but most processors have far fewer registers at their disposal. Assuming for double-word vectors would probably be optimistic for most computations and current architectures.
Reference: [18] <author> Lam, M., et.al., </author> <title> The Cache Performance and Optimizations of Blocked Algorithms, </title> <publisher> ASPLOS-IV. </publisher>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling <ref> [5, 8, 9, 18, 32, 38] </ref>, to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Note that the effectiveness of naive ordering decreases rapidly as vector stride increases. 2.2 Block prefetching Blocking or tiling changes a computation so that subblocks of data are repeatedly manipulated <ref> [8, 9, 18, 32, 38] </ref>. <p> Determining optimal block size in the presence of cache conicts may be difficult, but algorithms to address this problem have been presented elsewhere <ref> [18, 35] </ref>. The ideas presented here can be incorporated into those algorithms to yield even better memory performance. Ideally, we would like to use non-blocking loads. <p> Lam et. al. investigate blocking in conjunction with copying in order to eliminate self-interference, or cache misses caused by more than one element of a given vector mapping to the same location <ref> [18] </ref>. This optimization also reduces TLB misses and increases the number of data elements that will fit in cache when the vector being copied is of non-unit stride. Copying essentially attempts to explicitly manage the cache as a fast, local memory. <p> Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [8, 9, 5, 32, 38, 18, 35] </ref>. These studies assume a uniform memory access cost, thus they dont address minimizing the time to load vector data into cache. These techniques will also deliver better performance when integrated with methods to make more efficient use of memory resources. <p> Lam et. al. develop a model of data conicts and demonstrate that the amount of cache interference is highly dependent on block size and vector stride, with large variations in performance for matrices of different sizes <ref> [18] </ref>. For best results, block size for a computation must be tailored to matrix size and cache parameters, and efficient blocked access patterns tend to use only a relatively small portion of the cache. This may limit the applicability of cache-based access ordering techniques discussed here. <p> Thus these reports neither develop a general performance model nor present measured timing results specific to this optimization. Copying incurs an overhead cost proportional to the amount of data being copied, but the benefits often outweigh the cost <ref> [18] </ref>, and Temam et. al. present a compile-time technique for determining when copying is advantageous [35]. Using caching loads to create the copy can cause subtle problems with self-interference, though. As new data from the original vector is loaded, it may evict cache lines holding previously copied data.
Reference: [19] <author> Lee, K. </author> <title> On the Floating Point Performance of the i860 Microprocessor, </title> <type> NAS TR RNR-90-019, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> July, </month> <year> 1992. </year>
Reference-contexts: Our results support these conclusions. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published [3, 10, 12, 13, 33]; these, too, are complementary to the techniques for improving bandwidth that we analyze here. Moyer [28] and Lee <ref> [19] </ref> investigate the oating point and memory performance of the i860XR. Our examples for this architecture agree largely with their findings. 5. Conclusions As processors become faster, memory bandwidth is rapidly becoming the performance bottleneck in the application of high performance microprocessors to vector-like algorithms.
Reference: [20] <author> Lee, K. </author> <title> The NAS860 Library Users Manual, </title> <type> NAS TR RND-93-003, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March, </month> <year> 1993. </year>
Reference-contexts: This may limit the applicability of cache-based access ordering techniques discussed here. Block-size limitations can be circumvented by providing a separate buffer space for vector operands. Lee develops a subroutine library, called NASPACK, to mimic Cray instructions on the Intel i860XR <ref> [20] </ref>. Included are routines for streaming vector elements into cache. Data is read in blocks via non-caching load instructions, and is written into pre-allocated local memory.
Reference: [21] <author> Loshin, D., and Budge, D., </author> <title> Breaking the Memory Bottleneck, Parts 1 & 2, </title> <booktitle> Supercomputing Review, </booktitle> <address> January/February, </address> <year> 1992. </year>
Reference-contexts: Data is read in blocks via non-caching load instructions, and is written into pre-allocated local memory. Meadows, et. al., describe a similar scheme used by the PGI i860 compiler [22], and a Loshin and Budge give a general description of the technique <ref> [21] </ref>. The Loshin and Budge article is intended only to introduce the concept of Memory Hierarchy Management (MHM) by the compiler, and the NASA and PGI studies address streaming in conjunction with other operations.
Reference: [22] <author> Meadows, L., et.al., </author> <title> A Vectorizing Software Pipelining Compiler for LIW and Superscalar Architectures, </title> <publisher> RISC92. </publisher>
Reference-contexts: Included are routines for streaming vector elements into cache. Data is read in blocks via non-caching load instructions, and is written into pre-allocated local memory. Meadows, et. al., describe a similar scheme used by the PGI i860 compiler <ref> [22] </ref>, and a Loshin and Budge give a general description of the technique [21]. The Loshin and Budge article is intended only to introduce the concept of Memory Hierarchy Management (MHM) by the compiler, and the NASA and PGI studies address streaming in conjunction with other operations.
Reference: [23] <author> McKee, S.A, </author> <title> Hardware Support for Dynamic Access Ordering: Performance of Some Design Options, </title> <institution> Univ. of Virginia, Department of Computer Science, TR CS-93-08, </institution> <month> August, </month> <year> 1993. </year>
Reference-contexts: A system that reorders accesses at runtime and provides separate buffer space can reap the benefits of access ordering without these disadvantages, at the expense of adding a relatively small amount of special-purpose hardware. One such scheme is depicted in Figure 1 <ref> [23, 25] </ref>. In this organization, memory is interfaced to the processor through a controller (or Memory Scheduling Unit) that includes logic to issue memory requests and logic to determine the order of requests during streaming computations. <p> The stream buffers are implemented logically as a set of FIFOs, with each stream assigned to one FIFO. Detailed performance models and simulation results for this organization are presented elsewhere <ref> [23, 24, 25] </ref>. What follows is an approximate model to determine memory performance for a single vector of a computation. Accurate prediction requires knowledge of the entire computation, since performance for each stream depends on the nature and number of other streams.
Reference: [24] <author> McKee, S.A., et.al., </author> <title> Experimental Implementation of Dynamic Access Ordering, </title> <address> HICSS-27, Maui, HI, </address> <month> January, </month> <year> 1994. </year>
Reference-contexts: The stream buffers are implemented logically as a set of FIFOs, with each stream assigned to one FIFO. Detailed performance models and simulation results for this organization are presented elsewhere <ref> [23, 24, 25] </ref>. What follows is an approximate model to determine memory performance for a single vector of a computation. Accurate prediction requires knowledge of the entire computation, since performance for each stream depends on the nature and number of other streams.
Reference: [25] <author> McKee, S.A., et.al., </author> <title> Increasing Memory Bandwidth for Vector Computations, </title> <booktitle> Lecture Notes in Computer Science 782 (PLSA, </booktitle> <address> Zurich, Switzerland, March 1994), </address> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The first, naive ordering, provides a basis for comparing the performance improvements of the other schemes. None of the techniques requires heroic compiler technology: the compiler need only detect streams, as in Benitez and Davidsons algorithm [2]. Dynamic access ordering requires a small amount of special-purpose hardware <ref> [25] </ref>, and both static and dynamic access ordering depend on the availability of non-caching load instructions. Although rare, these instructions are available in some commercial processors, such as the Convex C-1 [37] and Intel i860 [15]. <p> A system that reorders accesses at runtime and provides separate buffer space can reap the benefits of access ordering without these disadvantages, at the expense of adding a relatively small amount of special-purpose hardware. One such scheme is depicted in Figure 1 <ref> [23, 25] </ref>. In this organization, memory is interfaced to the processor through a controller (or Memory Scheduling Unit) that includes logic to issue memory requests and logic to determine the order of requests during streaming computations. <p> The stream buffers are implemented logically as a set of FIFOs, with each stream assigned to one FIFO. Detailed performance models and simulation results for this organization are presented elsewhere <ref> [23, 24, 25] </ref>. What follows is an approximate model to determine memory performance for a single vector of a computation. Accurate prediction requires knowledge of the entire computation, since performance for each stream depends on the nature and number of other streams.
Reference: [26] <author> McMahon, F.H., </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, UCRL-53745, </institution> <month> December, </month> <year> 1986. </year>
Reference-contexts: The dynamic access ordering results given here are for a computation involving three vector operands (such as the first and fifth Livermore Loops, hydro fragment and tri-diagonal elimination <ref> [26] </ref>). Average cycles per element will be slightly lower for computations involving fewer vectors and slightly higher for computations requiring more. Note that for dynamic access ordering, block size corresponds to FIFO depth. fetch a unit stride vector using each of our schemes.
Reference: [27] <author> Mowry, </author> <title> T.C., et.al., Design and Evaluation of a Compiler Algorithm for Prefetching, </title> <type> ASPLOS-V, </type> <month> September, </month> <year> 1992. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching <ref> [4, 16, 27] </ref> and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. <p> system. a) stride 1 b) stride 2 c) stride 3 d) stride 4 Appeared in 1st Symposium on High Performance Computer Architecture, Raleigh, NC, January 1995 9 Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [1, 4, 11, 16, 27, 34] </ref>. These methods can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [28] <author> Moyer, S.A., </author> <title> Performance of the iPSC/860 Node Architecture, </title> <institution> Univ. of Virginia, IPC-TR-91-007, </institution> <year> 1991. </year>
Reference-contexts: The i860XR cache controller prevents us from implementing block-prefetching as described in Section 2.2. On this processor, each successive cache-line fill incurs a seven cycle delay <ref> [28] </ref>. This is long enough for the memory controller to transition to its idle state, causing the next memory access to take the same time decreasing page-hit costs as a DRAM page-miss, regardless of whether or not it lies in the same page as the previous access. <p> Our results support these conclusions. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published [3, 10, 12, 13, 33]; these, too, are complementary to the techniques for improving bandwidth that we analyze here. Moyer <ref> [28] </ref> and Lee [19] investigate the oating point and memory performance of the i860XR. Our examples for this architecture agree largely with their findings. 5. Conclusions As processors become faster, memory bandwidth is rapidly becoming the performance bottleneck in the application of high performance microprocessors to vector-like algorithms.
Reference: [29] <author> Moyer, S.A., </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, Univ. of Virginia, TR CS-93-18, </institution> <month> April, </month> <year> 1993. </year>
Reference-contexts: possible to overlap the writes to cache with non-caching loads, in which case t cwr drops out of the equation. 2.4 Static access ordering Moyer introduces static access ordering to maximize bandwidth for non-caching register loads, and derives compile-time access-ordering algorithms relative to a precise analytic model of memory systems <ref> [29] </ref>. This approach unrolls loops and orders non-caching memory operations to exploit architectural and device features of the target memory system.
Reference: [30] <author> Quinnell, R., </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991. </year>
Reference-contexts: Memory components are commonly assumed to require about the same time to access any random location, but this no longer applies to modern memory devices. For instance, nearly all current DRAMs implement a form of page-mode operation <ref> [30] </ref>. Other common devices offer similar features (nibble-mode, static column mode, or a small amount of SRAM cache on chip) or exhibit novel organizations (such as Rambus [31], Ramlink, and the new synchronous DRAM designs [14]). The order of requests strongly affects the performance of all these components.
Reference: [31] <institution> Architectural Overview, Rambus Inc., </institution> <year> 1992. </year>
Reference-contexts: For instance, nearly all current DRAMs implement a form of page-mode operation [30]. Other common devices offer similar features (nibble-mode, static column mode, or a small amount of SRAM cache on chip) or exhibit novel organizations (such as Rambus <ref> [31] </ref>, Ramlink, and the new synchronous DRAM designs [14]). The order of requests strongly affects the performance of all these components. A comprehensive, successful solution to the memory bandwidth problem must exploit the richness of the full memory hierarchy, both its architecture and its component characteristics.
Reference: [32] <author> Porterfield, </author> <title> A.K., Software Methods for Improvement of Cache Performance on Supercomputer Applications, </title> <type> Ph.D. Thesis, </type> <institution> Rice Univ., </institution> <month> May, </month> <year> 1989. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling <ref> [5, 8, 9, 18, 32, 38] </ref>, to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Note that the effectiveness of naive ordering decreases rapidly as vector stride increases. 2.2 Block prefetching Blocking or tiling changes a computation so that subblocks of data are repeatedly manipulated <ref> [8, 9, 18, 32, 38] </ref>. <p> Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [8, 9, 5, 32, 38, 18, 35] </ref>. These studies assume a uniform memory access cost, thus they dont address minimizing the time to load vector data into cache. These techniques will also deliver better performance when integrated with methods to make more efficient use of memory resources.
Reference: [33] <author> Rau, B. R., </author> <title> Pseudo-Randomly Interleaved Memory, </title> <booktitle> 18th ISCA, </booktitle> <month> May, </month> <year> 1991. </year>
Reference-contexts: Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations [12, 13], unusual memory systems <ref> [3, 10, 33, 36] </ref>, and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Our results support these conclusions. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published <ref> [3, 10, 12, 13, 33] </ref>; these, too, are complementary to the techniques for improving bandwidth that we analyze here. Moyer [28] and Lee [19] investigate the oating point and memory performance of the i860XR. Our examples for this architecture agree largely with their findings. 5.
Reference: [34] <author> Sohi, G., and Franklin, M., </author> <title> High Bandwidth Memory Systems for Superscalar Processors, </title> <address> ASPLOS-IV, </address> <month> April, </month> <year> 1991. </year>
Reference-contexts: Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches <ref> [1, 6, 34] </ref>. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> system. a) stride 1 b) stride 2 c) stride 3 d) stride 4 Appeared in 1st Symposium on High Performance Computer Architecture, Raleigh, NC, January 1995 9 Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [1, 4, 11, 16, 27, 34] </ref>. These methods can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [35] <author> Temam, O., et.al., </author> <title> To Copy or Not to Copy: A Compile-Time Technique for Assessing When Data Copying Should Be Used to Eliminate Cache Conicts, </title> <address> Supercomputing93, </address> <month> December, </month> <year> 1993. </year>
Reference-contexts: Determining optimal block size in the presence of cache conicts may be difficult, but algorithms to address this problem have been presented elsewhere <ref> [18, 35] </ref>. The ideas presented here can be incorporated into those algorithms to yield even better memory performance. Ideally, we would like to use non-blocking loads. <p> Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [8, 9, 5, 32, 38, 18, 35] </ref>. These studies assume a uniform memory access cost, thus they dont address minimizing the time to load vector data into cache. These techniques will also deliver better performance when integrated with methods to make more efficient use of memory resources. <p> Copying incurs an overhead cost proportional to the amount of data being copied, but the benefits often outweigh the cost [18], and Temam et. al. present a compile-time technique for determining when copying is advantageous <ref> [35] </ref>. Using caching loads to create the copy can cause subtle problems with self-interference, though. As new data from the original vector is loaded, it may evict cache lines holding previously copied data. Explicitly managing the cache becomes easier when a cache bypass mechanism is available.
Reference: [36] <author> Valero, M., et. al., </author> <title> Increasing the Number of Strides for Conict-Free Vector Access, </title> <booktitle> 19th ISCA, </booktitle> <month> May, </month> <year> 1992. </year>
Reference-contexts: Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling [5, 8, 9, 18, 32, 38], to address transformations [12, 13], unusual memory systems <ref> [3, 10, 33, 36] </ref>, and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes.
Reference: [37] <author> Wallach, S., </author> <title> The CONVEX C-1 64-bit Supercomputer, </title> <booktitle> Compcon Spring 85, </booktitle> <month> February, </month> <year> 1985. </year>
Reference-contexts: Dynamic access ordering requires a small amount of special-purpose hardware [25], and both static and dynamic access ordering depend on the availability of non-caching load instructions. Although rare, these instructions are available in some commercial processors, such as the Convex C-1 <ref> [37] </ref> and Intel i860 [15]. Other architectures, such as the DEC Alpha [7], provide a means of specifying some portions of memory as 1. These devices behave as if implemented with a single on-chip cache line, or page.
Reference: [38] <author> Wolfe, M., </author> <title> More Iteration Space Tiling, </title> <booktitle> Supercomputing 89, </booktitle> <year> 1989. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds, thus memory bandwidth is rapidly becoming the limiting performance factor for many applications, particularly scientific computations. Proposed solutions range from software prefetching [4, 16, 27] and iteration space tiling <ref> [5, 8, 9, 18, 32, 38] </ref>, to address transformations [12, 13], unusual memory systems [3, 10, 33, 36], and prefetching or non-blocking caches [1, 6, 34]. Here we take one technique, access ordering, and examine it in depth by analyzing the performance of five different access-ordering schemes. <p> Note that the effectiveness of naive ordering decreases rapidly as vector stride increases. 2.2 Block prefetching Blocking or tiling changes a computation so that subblocks of data are repeatedly manipulated <ref> [8, 9, 18, 32, 38] </ref>. <p> Such techniques are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [8, 9, 5, 32, 38, 18, 35] </ref>. These studies assume a uniform memory access cost, thus they dont address minimizing the time to load vector data into cache. These techniques will also deliver better performance when integrated with methods to make more efficient use of memory resources.
References-found: 37

