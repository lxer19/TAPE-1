URL: http://www.cs.virginia.edu/~cyberia/papers/EuDL97.ps
Refering-URL: http://www.cs.virginia.edu/~alp4g/publications.html
Root-URL: http://www.cs.virginia.edu
Email: E-mail: ffrench|alp4g|jlpg@cs.virginia.edu, eschulma@nrao.edu  
Phone: 1  2  
Title: Automating the Construction of Authority Files in Digital Libraries: A Case Study  
Author: James C. French Allison L. Powell Eric Schulman and John L. Pfaltz 
Address: Charlottesville, Virginia 22903 USA  520 Edgemont Road Charlottesville, Virginia 22903-2475 USA  
Affiliation: Department of Computer Science University of Virginia  National Radio Astronomy Observatory  
Abstract: The issue of quality control has become increasingly important as more online databases are integrated into digital libraries. This can have a dramatic effect on the search effectiveness of an online system. Authority work, the need to discover and reconcile variant forms of strings in bibliographic entries, will become more difficult. Spelling variants, misspellings, translation and transliteration differences all increase the difficulty of retrieving information. This paper is a case study of our efforts to automate the creation of an authority file for authors' institutional affiliations in the Astrophysics Data System. The techniques surveyed here for the detection and categorization of variant forms have broader applicability and may be used to help automate authority work for other bibliographic fields.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> H. A. </author> <title> Abt. </title> <journal> Institutional Productivities. Publications of the Astronomical Society of the Pacific, </journal> <volume> 105 </volume> <pages> 794-798, </pages> <year> 1993. </year>
Reference-contexts: The astronomy community collects statistics about publication in that field, tracking changes in measures such as paper length, general productivity and institutional productivity. Traditionally, such statistics have been gathered by hand on a necessarily small subset of available documents (e.g. <ref> [1, 16] </ref>). We have been collaborating with astronomers interested in automatically gathering this information using the ADS database as the data source [11, 12]. Automatically gathering statistics about these electronic documents has allowed a much larger fraction of documents to be considered | it has also presented new challenges.
Reference: 2. <author> A. Accomazzi, G. Eichhorn, M. J. Kurtz, C. S. Grant, and S. S. Murray. </author> <title> The ADS Article Service Data Holdings and Access Method. </title> <editor> In G. Hunt and H. Payne, editors, </editor> <booktitle> Astronomical Data Analysis Software and Systems VI, volume 125 of A.S.P. Conference Series, </booktitle> <pages> pages 357-360, </pages> <year> 1997. </year>
Reference: 3. <author> L. Auld. </author> <title> Authority Control: An Eighty-Year Review. Library Resources & Technical Services, </title> <booktitle> 26 </booktitle> <pages> 319-330, </pages> <year> 1982. </year>
Reference-contexts: In recent years there has also been an increasing emphasis on data quality in online databases [10, 14]. One aspect of improving data quality is detecting variant names for unique entities in the database. This is called authority work <ref> [3] </ref> and results in the creation of authority files that maintain the correspondence between all of the allowable forms for strings in a particular bibliographic field, ??? This work supported in part by Dept. of Energy grant no. <p> More concretely, we are trying to automate this authority control mechanism to achieve a transparent facility having the characteristics described by Auld <ref> [3] </ref>. A bibliographic record, together with all variant forms of each associated heading, would be entered into the system. The computer would establish linkages between the preferred forms of headings and the bibliographic record. <p> The authority control mechanism would be invisible so far as the user was concerned.[3, p. 327]. While this is not a new problem <ref> [3] </ref>, it is increasingly important due to the proliferation of online databases and the need to provide effective access to these resources. The age of digital libraries is dawning rapidly and will demand efficient solutions as more and more disparate online resources are aggregated into cohesive collections.
Reference: 4. <author> C. L. Borgman and S. L. Siegfried. </author> <title> Getty's Synoname and its Cousins: A Survey of Applications of Personal Name-Matching Algorithms. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 43(7) </volume> <pages> 459-476, </pages> <year> 1992. </year>
Reference-contexts: Others have considered similar problems with variant forms in bibliographic fields, for example, author names [13] and titles [18]. Borgman <ref> [4] </ref> surveys many other name-matching algorithms. This data cleanup effort will make it possible for the ADS to provide services that are currently infeasible.
Reference: 5. <author> J. R. Davis. </author> <title> Creating a Networked Computer Science Technical Report Library. </title> <journal> D-Lib Magazine, </journal> <month> Sept. </month> <year> 1995. </year>
Reference: 6. <author> J. C. French, A. L. Powell, and E. Schulman. </author> <title> Applications of Approximate Word Matching in Information Retrieval. </title> <booktitle> In 6th International Conference on Information and Knowledge Management (CIKM'97), </booktitle> <address> Las Vegas, Nevada, </address> <month> 10-14 November </month> <year> 1997. </year> <note> (to appear). </note>
Reference-contexts: Using the alternate representations of the affiliation string is helpful, but it does not allow fine control over the types of errors allowed. In a companion paper <ref> [6] </ref>, we describe a new approach | approximate word matching | which finds a minimum distance matching between the words in string1 and string2. We have performed experiments on a small subset of the collection using this new approach. The results are reported in [6]. <p> In a companion paper <ref> [6] </ref>, we describe a new approach | approximate word matching | which finds a minimum distance matching between the words in string1 and string2. We have performed experiments on a small subset of the collection using this new approach. The results are reported in [6]. Preliminary experiments using the full collection show promise to reduce the number of clusters further without introducing further error. More extensive experiments are forthcoming. 4 Recommendations We have shown that when used together, the approaches outlined in this paper are a useful component of semi-automatic authority file generation. <p> However, it is apparent from Table 4 that we have reached a point of diminishing returns using our current methodology. The next step is to perform comparisons between strings on a word-by-word basis as described in <ref> [6] </ref>. This presents a number of interesting questions, the most important of which is determining an appropriate distance measure. We propose that it should contain components of total edit distance and individual edit distances between words. Note that individual between-word edit distances must be below some threshold. <p> The methods are effective and efficient enough to be used in production environments. In the study reported here, we extracted 20,168 unique affiliation strings from our sample of 146,000 records. Preliminary results from the application of approximate word matching <ref> [6] </ref> show that we are able to further reduce these to 8,928 strings by using extremely conservative thresholds. Although we do not yet have a quantitative measure of the misclassification rate, our qualitative assessment is that the results are excellent.
Reference: 7. <author> P. A. V. Hall and G. R. Dowling. </author> <title> Approximate String Matching. </title> <journal> Computing Surveys, </journal> <volume> 12(4) </volume> <pages> 381-402, </pages> <month> Dec. </month> <year> 1980. </year>
Reference-contexts: We chose to use edit distance as a domain independent way to measure the difference between two strings. The edit distance [9, 17] is the number of insertions, deletions, transpositions and substitutions required to turn string1 into string2. Edit distance has traditionally been used in approximate string matching <ref> [7] </ref>, spelling error detection and correction [8], and more recently has been shown to be more effective than Soundex for phonetic string matching [19]. We used the edit distance algorithm presented by Hall and Dowl-ing [7] as implemented by Zobel and Dart [19]. <p> Edit distance has traditionally been used in approximate string matching <ref> [7] </ref>, spelling error detection and correction [8], and more recently has been shown to be more effective than Soundex for phonetic string matching [19]. We used the edit distance algorithm presented by Hall and Dowl-ing [7] as implemented by Zobel and Dart [19]. For each round of experiments, we computed an edit distance cost matrix that contained the distances between all affiliation strings in the current set. These distances were used to form affiliation clusters based on a fixed or variable edit distance threshold.
Reference: 8. <author> K. Kukich. </author> <title> Techniques for Automatically Correcting Words in Text. </title> <journal> Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-440, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: The edit distance [9, 17] is the number of insertions, deletions, transpositions and substitutions required to turn string1 into string2. Edit distance has traditionally been used in approximate string matching [7], spelling error detection and correction <ref> [8] </ref>, and more recently has been shown to be more effective than Soundex for phonetic string matching [19]. We used the edit distance algorithm presented by Hall and Dowl-ing [7] as implemented by Zobel and Dart [19].
Reference: 9. <author> R. Lowrance and R. A. Wagner. </author> <title> An Extension of the String-to-String Correction Problem. </title> <journal> Journal of the ACM, </journal> <volume> 22(2) </volume> <pages> 177-183, </pages> <month> Apr. </month> <year> 1975. </year>
Reference-contexts: While it would be theoretically possible to handle these variants lexically, this would be a time-consuming operation for large sets of variants. We chose to use edit distance as a domain independent way to measure the difference between two strings. The edit distance <ref> [9, 17] </ref> is the number of insertions, deletions, transpositions and substitutions required to turn string1 into string2.
Reference: 10. <author> E. T. O'Neill and D. Vizine-Goetz. </author> <title> Quality Control in Online Databases. </title> <journal> Annual Review of Information Science and Technology, </journal> <volume> 23 </volume> <pages> 125-156, </pages> <year> 1988. </year>
Reference-contexts: There will be increasing reliance on automated techniques to aid information providers as they seek to reach this goal. In recent years there has also been an increasing emphasis on data quality in online databases <ref> [10, 14] </ref>. One aspect of improving data quality is detecting variant names for unique entities in the database.
Reference: 11. <author> E. Schulman, J. C. French, A. L. Powell, S. S. Murray, G. Eichhorn, and M. J. Kurtz. </author> <title> The Sociology of Astronomical Publication Using ADS and ADAMS. </title> <editor> In G. Hunt and H. Payne, editors, </editor> <booktitle> Astronomical Data Analysis Software and Systems VI, volume 125 of A.S.P. Conference Series, </booktitle> <pages> pages 361-364, </pages> <year> 1997. </year>
Reference-contexts: Traditionally, such statistics have been gathered by hand on a necessarily small subset of available documents (e.g. [1, 16]). We have been collaborating with astronomers interested in automatically gathering this information using the ADS database as the data source <ref> [11, 12] </ref>. Automatically gathering statistics about these electronic documents has allowed a much larger fraction of documents to be considered | it has also presented new challenges.
Reference: 12. <author> E. Schulman, A. L. Powell, J. C. French, G. Eichhorn, M. J. Kurtz, and S. S. Mur-ray. </author> <title> Using the ADS Database to Study Trends in Astronomical Publication. </title> <journal> Bulletin of the American Astronomical Society, </journal> <volume> 28(4):1281, </volume> <year> 1996. </year>
Reference-contexts: Traditionally, such statistics have been gathered by hand on a necessarily small subset of available documents (e.g. [1, 16]). We have been collaborating with astronomers interested in automatically gathering this information using the ADS database as the data source <ref> [11, 12] </ref>. Automatically gathering statistics about these electronic documents has allowed a much larger fraction of documents to be considered | it has also presented new challenges.
Reference: 13. <author> S. L. Siegfried and J. Bernstein. Synoname: </author> <title> The Getty's New Approach to Pattern Matching for Personal Names. </title> <journal> Computers and the Humanities, </journal> <volume> 25(4) </volume> <pages> 211-226, </pages> <year> 1991. </year>
Reference-contexts: Others have considered similar problems with variant forms in bibliographic fields, for example, author names <ref> [13] </ref> and titles [18]. Borgman [4] surveys many other name-matching algorithms. This data cleanup effort will make it possible for the ADS to provide services that are currently infeasible.
Reference: 14. <author> D. M. Strong, Y. W. Lee, and R. Y. Wang. </author> <title> Data Quality in Context. </title> <journal> Communi--cations of the ACM, </journal> <volume> 40(5) </volume> <pages> 103-110, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: There will be increasing reliance on automated techniques to aid information providers as they seek to reach this goal. In recent years there has also been an increasing emphasis on data quality in online databases <ref> [10, 14] </ref>. One aspect of improving data quality is detecting variant names for unique entities in the database.
Reference: 15. <author> A. G. Taylor. </author> <title> Authority Files in Online Catalogs: An Investigation of Their Value. </title> <journal> Cataloging & Classification Quarterly, </journal> <volume> 4(3) </volume> <pages> 1-17, </pages> <year> 1984. </year>
Reference-contexts: In this paper we look at techniques to aid in detecting variant forms of strings in bibliographic databases. Taylor <ref> [15] </ref> elucidates two principles of authority control. The first is that all variants of a name will be brought together under a single form so that once users find that form, they will be confident that they have located everything relating to the name.
Reference: 16. <author> V. Trimble. </author> <title> Postwar growth in the length of astronomical and other scientific papers. </title> <journal> Publications of the Astronomical Society of the Pacific, </journal> <volume> 96 </volume> <pages> 1007-1016, </pages> <year> 1984. </year>
Reference-contexts: The astronomy community collects statistics about publication in that field, tracking changes in measures such as paper length, general productivity and institutional productivity. Traditionally, such statistics have been gathered by hand on a necessarily small subset of available documents (e.g. <ref> [1, 16] </ref>). We have been collaborating with astronomers interested in automatically gathering this information using the ADS database as the data source [11, 12]. Automatically gathering statistics about these electronic documents has allowed a much larger fraction of documents to be considered | it has also presented new challenges.
Reference: 17. <author> R. A. Wagner and M. J. Fischer. </author> <title> The String-to-String Correction Problem. </title> <journal> Journal of the ACM, </journal> <volume> 21(1) </volume> <pages> 168-173, </pages> <month> Jan. </month> <year> 1974. </year>
Reference-contexts: While it would be theoretically possible to handle these variants lexically, this would be a time-consuming operation for large sets of variants. We chose to use edit distance as a domain independent way to measure the difference between two strings. The edit distance <ref> [9, 17] </ref> is the number of insertions, deletions, transpositions and substitutions required to turn string1 into string2.
Reference: 18. <author> M. E. Williams and L. Lannom. </author> <title> Lack of Standardization of the Journal Title Data Element in Databases. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 32(3) </volume> <pages> 229-233, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Others have considered similar problems with variant forms in bibliographic fields, for example, author names [13] and titles <ref> [18] </ref>. Borgman [4] surveys many other name-matching algorithms. This data cleanup effort will make it possible for the ADS to provide services that are currently infeasible.
Reference: 19. <author> J. Zobel and P. </author> <title> Dart. Phonetic String Matching: Lessons from Information Retrieval. </title> <booktitle> In Proc. 19th Inter. Conf. on Research and Development in Information Retrieval (SIGIR'96), </booktitle> <pages> pages 166-172, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Edit distance has traditionally been used in approximate string matching [7], spelling error detection and correction [8], and more recently has been shown to be more effective than Soundex for phonetic string matching <ref> [19] </ref>. We used the edit distance algorithm presented by Hall and Dowl-ing [7] as implemented by Zobel and Dart [19]. For each round of experiments, we computed an edit distance cost matrix that contained the distances between all affiliation strings in the current set. <p> has traditionally been used in approximate string matching [7], spelling error detection and correction [8], and more recently has been shown to be more effective than Soundex for phonetic string matching <ref> [19] </ref>. We used the edit distance algorithm presented by Hall and Dowl-ing [7] as implemented by Zobel and Dart [19]. For each round of experiments, we computed an edit distance cost matrix that contained the distances between all affiliation strings in the current set. These distances were used to form affiliation clusters based on a fixed or variable edit distance threshold.
References-found: 19

