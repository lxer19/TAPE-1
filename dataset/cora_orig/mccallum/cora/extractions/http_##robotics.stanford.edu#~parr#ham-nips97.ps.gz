URL: http://robotics.stanford.edu/~parr/ham-nips97.ps.gz
Refering-URL: http://robotics.stanford.edu/~parr/
Root-URL: http://www.cs.stanford.edu
Email: fparr,russellg@cs.berkeley.edu  
Title: Reinforcement Learning with Hierarchies of Machines  
Author: Ronald Parr and Stuart Russell 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division, UC  
Abstract: We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and behavior-based or teleo-reactive approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Benson and N. Nilsson. </author> <title> Reacting, planning and learning in an autonomous agent. </title> <editor> In K. Furukawa, D. Michie, and S. Muggleton, editors, </editor> <booktitle> Machine Intelligence 14. </booktitle> <publisher> Oxford University Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: To review, an MDP is a 4-tuple, (S; A; T; R) where S is a set of states, A is a set of actions, T is a transition model mapping S fi A fi S into probabilities in <ref> [0; 1] </ref>, and R is a reward function mapping S fi A fi S into real-valued rewards. Algorithms for solving MDPs can return a policy that maps from S to A, a real-valued value function V on states, or a real-valued Q-function on state-action pairs. <p> The design of hierarchically organized, layered controllers was popularized by Brooks [4]. His designs use a somewhat different means of passing control, but our analysis and theorems apply equally well to his machine description language. The teleo-reactive agent designs of Benson and Nilsson <ref> [1] </ref> are even closer to our HAM language. Both of these approaches assume that the agent is completely specified, albeit self-modifiable.
Reference: [2] <author> D. C. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: Moreover, state aggregation may be hard to apply effectively in many cases. Dean and Lin [8] and Bertsekas and Tsitsiklis <ref> [2] </ref>, showed that some MDPs are loosely coupled and hence amenable to divide-and-conquer algorithms. A machine-like language was used in [13] to partition an MDP into decoupled subproblems. In problems that are amenable to decoupling, this could approaches could be used in combinated with HAMs. <p> We believe that if desired, subgoal information can be incorporated into the HAM structure, unifying subgoal-based approaches with the HAM approach. Moreover, the HAM structure provides a natural decomposition of the HAM-induced model, making it amenable to the divide-and-conquer approaches of [8] and <ref> [2] </ref>. There are opportunities for generalization across all levels of the HAM paradigm. Value function approximation can be used for the HAM induced model and inductive learning methods can be used to produce HAMs or to generalize their effects upon different regions of the state space.
Reference: [3] <author> S. J. Bradtke and M. O. Duff. </author> <title> Reinforcement learning methods for continuous-time Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7: Proc. of the 1994 Conference, </booktitle> <address> Denver, Colorado, December 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Subgoals seem natural in some domains, but they may require a significant amount of outside knowledge about the domain and establishing the relationship between the value of subgoals with respect to the overall problem can be difficult. Bradtke and Duff <ref> [3] </ref> proposed an RL algorithm for SMDPs. Sutton [19] proposes temporal abstractions, which concatenate sequences of state transitions together to permit reasoning about temporally extended events, and which can thereby form a behavioral hierarchy as in [14] and [15].
Reference: [4] <author> R. A. Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 2, </volume> <year> 1986. </year>
Reference-contexts: Lin's somewhat informal scheme [12] also allows agents to treat entire policies as single actions. These approaches can be emcompassed within our framework by encoding the events or behaviors as machines. The design of hierarchically organized, layered controllers was popularized by Brooks <ref> [4] </ref>. His designs use a somewhat different means of passing control, but our analysis and theorems apply equally well to his machine description language. The teleo-reactive agent designs of Benson and Nilsson [1] are even closer to our HAM language.
Reference: [5] <author> K. W. Currie and A. Tate. O-Plan: </author> <title> the Open Planning Architecture. </title> <journal> Artificial Intelligence, </journal> <volume> 52(1), </volume> <month> November </month> <year> 1991. </year>
Reference-contexts: Although it can yield suboptimal policies, top-down hierarchical control often reduces the complexity of decision making from exponential to linear in the size of the problem. For example, hierarchical task network (HTN) planners can generate solutions containing tens of thousands of steps <ref> [5] </ref>, whereas flat planners can manage only tens of steps. HTN planners are successful because they use a plan library that describes the decomposition of high-level activities into lower-level activities.
Reference: [6] <author> P. Dayan and G. E. Hinton. </author> <title> Feudal reinforcement learning. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Neural Information Processing Systems 5, </booktitle> <address> San Mateo, California, 1993. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: A machine-like language was used in [13] to partition an MDP into decoupled subproblems. In problems that are amenable to decoupling, this could approaches could be used in combinated with HAMs. Dayan and Hinton <ref> [6] </ref> have proposed feudal RL which specifies an explicit subgoal structure, with fixed values for each subgoal achieved, in order to achieve a hierarchical decomposition of the state space. Dietterich extends and generalizes this approach in [9].
Reference: [7] <author> T. Dean, R. Givan, and S. Leach. </author> <title> Model reduction techniques for computing approximately optimal solutions for markov decision processes. </title> <booktitle> In Proc. of the Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Providence, Rhode Island, August 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Even after 20,000,000 iterations, Q-learning did not do as well as HAMQ-learning. 2 2 Speedup techniques such as eligibility traces could be applied to get better Q-learning results; such methods apply equally well to HAMQ-learning. 6 Related work State aggregation (see, e.g., [18] and <ref> [7] </ref>) clusters similar states together and assigns them the same value, effectively reducing the state space. This is orthogonal to our approach and could be combined with HAMs.
Reference: [8] <author> T. Dean and S.-H. Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proc. of the Fourteenth Int. Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada, August 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Moreover, state aggregation may be hard to apply effectively in many cases. Dean and Lin <ref> [8] </ref> and Bertsekas and Tsitsiklis [2], showed that some MDPs are loosely coupled and hence amenable to divide-and-conquer algorithms. A machine-like language was used in [13] to partition an MDP into decoupled subproblems. In problems that are amenable to decoupling, this could approaches could be used in combinated with HAMs. <p> We believe that if desired, subgoal information can be incorporated into the HAM structure, unifying subgoal-based approaches with the HAM approach. Moreover, the HAM structure provides a natural decomposition of the HAM-induced model, making it amenable to the divide-and-conquer approaches of <ref> [8] </ref> and [2]. There are opportunities for generalization across all levels of the HAM paradigm. Value function approximation can be used for the HAM induced model and inductive learning methods can be used to produce HAMs or to generalize their effects upon different regions of the state space.
Reference: [9] <author> Thomas G. Dietterich. </author> <title> Hierarchical reinforcement learning with the MAXQ value function decomposition. </title> <type> Technical report, </type> <institution> Department of Computer Science, Oregon State University, Corvallis, Oregon, </institution> <year> 1997. </year>
Reference-contexts: Dayan and Hinton [6] have proposed feudal RL which specifies an explicit subgoal structure, with fixed values for each subgoal achieved, in order to achieve a hierarchical decomposition of the state space. Dietterich extends and generalizes this approach in <ref> [9] </ref>. Singh has investigated a number of approaches to subgoal based decomposition in reinforcement learning (e.g. [17] and [16]).
Reference: [10] <author> Y.-J. Hsu. </author> <title> Synthesizing efficient agents from partial programs. </title> <booktitle> In Methodologies for Intelligent Systems: 6th Int. Symposium, ISMIS '91, Proc., </booktitle> <address> Charlotte, North Carolina, </address> <month> October </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: The teleo-reactive agent designs of Benson and Nilsson [1] are even closer to our HAM language. Both of these approaches assume that the agent is completely specified, albeit self-modifiable. The idea of partial behavior descriptions can be traced at least to Hsu's partial programs <ref> [10] </ref>, which were used with a deterministic logical planner. 7 Conclusions and future work We have presented HAMs as a principled means of constraining the set of policies that are considered for a Markov decision process and we have demonstrated the efficacy of this approach in a simple example for both
Reference: [11] <author> T. Jaakkola, M.I. Jordan, and S.P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6), </volume> <year> 1994. </year>
Reference-contexts: Proof sketch We note that the expected reinforcement signal in HAMQ-learning is the same as the expected reinforcement signal that would be received if the agent were acting directly in the transformed model of Theorem 1 above. Thus, Theorem 1 of <ref> [11] </ref> can be applied to prove the convergence of the HAMQ-learning agent, provided that we enforce suitable constraints on the exploration strategy and the update parameter decay rate. We ran some experiments to measure the performance of HAMQ-learning on our sample problem.
Reference: [12] <author> L.-J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania, </institution> <year> 1993. </year>
Reference-contexts: Bradtke and Duff [3] proposed an RL algorithm for SMDPs. Sutton [19] proposes temporal abstractions, which concatenate sequences of state transitions together to permit reasoning about temporally extended events, and which can thereby form a behavioral hierarchy as in [14] and [15]. Lin's somewhat informal scheme <ref> [12] </ref> also allows agents to treat entire policies as single actions. These approaches can be emcompassed within our framework by encoding the events or behaviors as machines. The design of hierarchically organized, layered controllers was popularized by Brooks [4].
Reference: [13] <author> Shieu-Hong Lin. </author> <title> Exploiting Structure for Planning and Control. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1997. </year>
Reference-contexts: Moreover, state aggregation may be hard to apply effectively in many cases. Dean and Lin [8] and Bertsekas and Tsitsiklis [2], showed that some MDPs are loosely coupled and hence amenable to divide-and-conquer algorithms. A machine-like language was used in <ref> [13] </ref> to partition an MDP into decoupled subproblems. In problems that are amenable to decoupling, this could approaches could be used in combinated with HAMs.
Reference: [14] <author> A. McGovern, R. S. Sutton, and A. H. Fagg. </author> <title> Roles of macro-actions in accelerating reinforcement learning. </title> <booktitle> In 1997 Grace Hopper Celebration of Women in Computing, </booktitle> <year> 1997. </year>
Reference-contexts: Bradtke and Duff [3] proposed an RL algorithm for SMDPs. Sutton [19] proposes temporal abstractions, which concatenate sequences of state transitions together to permit reasoning about temporally extended events, and which can thereby form a behavioral hierarchy as in <ref> [14] </ref> and [15]. Lin's somewhat informal scheme [12] also allows agents to treat entire policies as single actions. These approaches can be emcompassed within our framework by encoding the events or behaviors as machines. The design of hierarchically organized, layered controllers was popularized by Brooks [4].
Reference: [15] <author> D. Precup and R. S. Sutton. </author> <title> Multi-time models for temporally abstract planning. </title> <booktitle> In This Volume. </booktitle>
Reference-contexts: Bradtke and Duff [3] proposed an RL algorithm for SMDPs. Sutton [19] proposes temporal abstractions, which concatenate sequences of state transitions together to permit reasoning about temporally extended events, and which can thereby form a behavioral hierarchy as in [14] and <ref> [15] </ref>. Lin's somewhat informal scheme [12] also allows agents to treat entire policies as single actions. These approaches can be emcompassed within our framework by encoding the events or behaviors as machines. The design of hierarchically organized, layered controllers was popularized by Brooks [4].
Reference: [16] <author> S. P. Singh. </author> <title> Scaling reinforcement learning algorithms by learning variable temporal resolution models. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> Aberdeen, July 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Dietterich extends and generalizes this approach in [9]. Singh has investigated a number of approaches to subgoal based decomposition in reinforcement learning (e.g. [17] and <ref> [16] </ref>). Subgoals seem natural in some domains, but they may require a significant amount of outside knowledge about the domain and establishing the relationship between the value of subgoals with respect to the overall problem can be difficult. Bradtke and Duff [3] proposed an RL algorithm for SMDPs.
Reference: [17] <author> S. P. Singh. </author> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8(3), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Dietterich extends and generalizes this approach in [9]. Singh has investigated a number of approaches to subgoal based decomposition in reinforcement learning (e.g. <ref> [17] </ref> and [16]). Subgoals seem natural in some domains, but they may require a significant amount of outside knowledge about the domain and establishing the relationship between the value of subgoals with respect to the overall problem can be difficult. Bradtke and Duff [3] proposed an RL algorithm for SMDPs.
Reference: [18] <author> S. P. Singh, T. Jaakola, and M. I. Jordan. </author> <title> Reinforcement learning with soft state aggregation. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Neural Information Processing Systems 7, </booktitle> <address> Cambridge, Massachusetts, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Even after 20,000,000 iterations, Q-learning did not do as well as HAMQ-learning. 2 2 Speedup techniques such as eligibility traces could be applied to get better Q-learning results; such methods apply equally well to HAMQ-learning. 6 Related work State aggregation (see, e.g., <ref> [18] </ref> and [7]) clusters similar states together and assigns them the same value, effectively reducing the state space. This is orthogonal to our approach and could be combined with HAMs.
Reference: [19] <author> R. S. Sutton. </author> <title> Temporal abstraction in reinforcement learning. </title> <booktitle> In Proc. of the Twelfth Int. Conference on Machine Learning, </booktitle> <address> Tahoe City, CA, July 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Subgoals seem natural in some domains, but they may require a significant amount of outside knowledge about the domain and establishing the relationship between the value of subgoals with respect to the overall problem can be difficult. Bradtke and Duff [3] proposed an RL algorithm for SMDPs. Sutton <ref> [19] </ref> proposes temporal abstractions, which concatenate sequences of state transitions together to permit reasoning about temporally extended events, and which can thereby form a behavioral hierarchy as in [14] and [15]. Lin's somewhat informal scheme [12] also allows agents to treat entire policies as single actions.
References-found: 19

