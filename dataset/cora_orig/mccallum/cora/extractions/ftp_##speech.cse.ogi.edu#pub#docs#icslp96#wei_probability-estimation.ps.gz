URL: ftp://speech.cse.ogi.edu/pub/docs/icslp96/wei_probability-estimation.ps.gz
Refering-URL: http://www.cse.ogi.edu/CSLU/publications/publications.html
Root-URL: http://www.cse.ogi.edu
Title: Improved Probability Estimation with Neural Network Models  for Spoken Language Understanding  
Author: Wei Wei, Etienne Barnard and Mark Fanty 
Address: 20000 N.W.Walker Road, Portland, OR 97291-1000  
Affiliation: Center  Oregon Graduate Institute of Science and Technology  
Abstract: Neural network classifiers can provide outputs that estimate Bayesian posterior probabilities under the assumptions that an infinite amount of training data are available, the network is sufficiently complex and the training can reach the global minimum. In practice, however, the number of training tokens is limited and may not accurately reflect the prior class probabilities and true likelihood distributions. Additionally, computational constraints place a limit on the complexity of the network. Consequently, practical networks often fall far short of being ideal estimators. We address this problem and propose a new method of improved probability estimation by combining neural network models with empirical probability estimation methods. We use a histogram-based estimation method to remap the network outputs to match the data and thereby improve the accuracy of the probability estimates. Our current experiments on the OGI Census Year corpus resulted in a 20.6% reduction in recognition errors at the utterance level. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> E. Barnard, R. Cole, M. Fanty, and P. Ver-meulen. </author> <title> Real-world speech recognition with neural networks. </title> <booktitle> Proceedings of the International Symposium on Aerospace/Defense Sensing & Control and Dual-Use Photonics, </booktitle> <month> Apr. 17-21 </month> <year> 1995. </year> <type> invited paper, </type> <address> Orlando, FL. </address>
Reference-contexts: Experimental Setup Our experimental task is recognition of the OGI Census Year corpus. This corpus was collected and created by CSLU, as part of a study to determine the feasibility of using an automated spoken questionnaire to collect information for the Year 2000 United States Census <ref> [1] </ref>. The goal of the study was to develop and evaluate a telephone questionnaire that automatically captures and recognizes the following information: (1) full name, (2) sex, (3) birth date, (4) marital status, (5) Hispanic origin, and (6) race. The OGI Census Year corpus comes from the birth-date database.
Reference: 2. <author> C. M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: Therefore applying extra training using those patterns that fail to generate outputs close to the desired values will be counter productive, because it alters the distributions and makes the network less likely to generate the correct Bayesian posterior probabilities <ref> [2] </ref>. Unlike conventional Bayesian classifiers, which derive the Bayesian posterior probabilities from likelihood indirectly by Bayes Rule, neural network classifiers can provide outputs that estimate Bayesian posterior probabilities directly.
Reference: 3. <author> L. Gillick and S. J. Cox. </author> <title> Some statistical issues in the comparison of speech recognition algorithms. </title> <booktitle> Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 532-535, </pages> <year> 1989. </year>
Reference-contexts: As shown in Table 2, this optimal probability estimator (ff = 2=3) resulted in a 20:6% reduction in recognition errors at the utterance level. Using McNemar's test <ref> [3] </ref>, the significance level of this improvement is 2.2%, which means that the reduction in the error rate is statistically significant.
Reference: 4. <author> R. M. Neal. </author> <title> Bayesian Learning for Neural Networks. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <year> 1995. </year>
Reference-contexts: We use neural network outputs as the first estimates of Bayesian posterior probabilities. Neural networks are flexible in modeling regularities in the data, but an over-flexible network can be misled by stray correlations within the data into "discovering" improbable structure <ref> [4] </ref>. Our empirical probability estimation based on histograms is to remap the network outputs to match the data and thereby improve the accuracy of the probability estimation.
Reference: 5. <author> M. D. Richard and R. P. Lippmann. </author> <title> Neural network classifiers estimate bayesian a posteriori probabilities. </title> <journal> Neural Computation, </journal> (3):461-483, 1991. 
Reference-contexts: Unless the value of a "correct" network output is greater than 0:5, the classification decisions are often considered incorrect and it is thought that more training is still required. However, this might not always be true, because the output values of the neural networks often approximate Bayesian posterior probabilities <ref> [5] </ref>. If density functions of classes overlap, the network outputs need not be close to 0 or 1: they can have values ranging from 0.0 to 1.0. <p> The latter is however true only under the assumptions that (1)an infinite amount of training data is available, (2) the network is sufficiently complex and (3) the training error can reach the global minimum <ref> [5] </ref>. In practice, however, these assumptions are not satisfied and therefore neural network outputs may fail to approximate Bayesian posterior probabilities well. 2. <p> Our experimental results (shown in Section 4) confirm that the network outputs do not always estimate probabilities accurately. Earlier simulation results <ref> [5] </ref> also demonstrate that the neural network classifiers provide outputs that estimate Bayesian posterior probabilities, with the estimation accuracy influenced by the network complexity, the number of training data, and the degree to which training data reflect true likelihood distributions and the prior class probabilities.
References-found: 5

