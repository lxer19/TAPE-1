URL: http://www.cs.ualberta.ca/~greiner/PAPERS/pao.ps
Refering-URL: http://www.cs.ualberta.ca/~greiner/PAPERS/
Root-URL: 
Title: Probably Approximately Optimal Satisficing Strategies  
Author: Russell Greiner Pekka Orponen 
Date: January 30, 1995  
Address: 755 College Road, East Princeton, NJ 08540  Helsinki, P. O. Box 26 FIN-00014 Helsinki, Finland  
Affiliation: Siemens Corporate Research  Department of Computer Science University of  
Abstract: A satisficing search problem consists of a set of probabilistic experiments to be performed in some order, seeking a satisfying configuration of successes and failures. The expected cost of the search depends both on the success probabilities of the individual experiments, and on the search strategy, which specifies the order in which the experiments are to be performed. A strategy that minimizes the expected cost is optimal. Earlier work has provided "optimizing functions" that compute optimal strategies for certain classes of search problems from the success probabilities of the individual experiments. We extend those results by providing a general model of such strategies, and an algorithm pao that identifies an approximately optimal strategy when the probability values are not known. The algorithm first estimates the relevant probabilities from a number of trials of each undetermined experiment, and then uses these estimates, and the proper optimizing function, to identify a strategy whose cost is, with high probability, close to optimal. We also show that if the search problem can be formulated as an and-or tree, then the pao algorithm can also "learn while doing", i.e. gather the necessary statistics while performing the search.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, J. H. Spencer, and P. Erdos. </author> <title> The Probabilistic Method. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: To state this more precisely, we define: Definition 4 (Expected Cost of a Strategy) Let fi be a strategy for the decision structure G = hW; F; R; ci, and p : W ! <ref> [0; 1] </ref> a distribution function that maps each experiment to its success probability. The (expected) cost of strategy fi relative to the distribution p, denoted C p (fi), is defined as the sum of the cost of each path in the strategy, weighted by its probability, i.e. <p> Definition 5 (Optimizing Functions) An optimizing function for a class of decision structures D DS is a function OSS that maps any decision structure G = hW; F; R; ci 2 D, together with a distribution p 2 <ref> [0; 1] </ref> W , to a strategy in SS (G) whose cost is minimal. That is, W For brevity, we often denote by fi p the optimal strategy OSS (G; p) provided by the optimizing function for a given distribution p. <p> OSS can produce an optimal strategy even if j^p i p i j = 1). Definition 7 Let G = hW; F; R; ci 2 DS be a decision structure, and p: W ! <ref> [0; 1] </ref> a distribution function that maps each experiment to its success probability. <p> probabilistic polynomial time algorithms with one-way error, cf. [12].) Then there is no probabilistic poly nomial time algorithm, and consequently no deterministic polynomial time algorithm that, 14 given a decision structure G = hW; F; R; ci, an experiment e 2 W , a distribution func-tion p : W ! <ref> [0; 1] </ref>, and parameters *, ffi &gt; 0, can estimate the value (e) to within * with probability at least 1 ffi. 4 Conclusion The results presented in this paper have been motivated by, and extend, various other lines of research. <p> Then 8e i 2 W: Pr D (e i ) fi (e i ) fi jp i ^p i j 2n n Proof: We use Hoeffding's Inequality, which is a simple form of Chernoff bounds <ref> [4, 1] </ref>: Let fX i g be a set of independent, identically-distributed random Bernoulli variables, whose 19 common mean is . Let S (M) = 1 M i=1 X i be the sample mean after taking M samples. <p> Then there is no probabilistic polynomial time algorithm that, given a decision structure G = hW; F; R; ci, an experiment e 2 W , a distribution function p : W ! <ref> [0; 1] </ref>, and parameters *, ffi &gt; 0, can estimate the value (e) to within * with probability at least 1 ffi. Proof: Assume to the contrary that such an algorithm exists for some fixed values of *; ffi &gt; 0; say * = 1=3 = ffi.
Reference: [2] <author> J. A. Barnett. </author> <title> How much is control knowledge worth?: A primitive example. </title> <journal> Artificial Intelligence, </journal> <volume> 22 </volume> <pages> 77-89, </pages> <year> 1984. </year>
Reference-contexts: The underlying objective of finding a provably good search strategy comes from the work on optimal satisficing search strategies <ref> [6, 21, 2, 18, 22] </ref>. Each of these earlier papers considered some specifically defined class of search structures and, moreover, required the user to supply precise success probability values for the experiments. Our work extends this body of research in three ways.
Reference: [3] <author> D. A. Berry and B. Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1985. </year>
Reference-contexts: We are not considering ways of modifying the strategy gradually over time to become incrementally better; but see [9]. Also, this issue differs from the "Exploration-Exploitation" tradeoff discussed in the context of the Bandit problem (cf. <ref> [3, 17] </ref>) as we are not concerned with minimizing the cumulative cost of the learning and performance systems together, over an infinite sequence of samples. 10 Fortunately, there is a way around this problem.
Reference: [4] <author> Herman Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sums of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23 </volume> <pages> 493-507, </pages> <year> 1952. </year>
Reference-contexts: Then 8e i 2 W: Pr D (e i ) fi (e i ) fi jp i ^p i j 2n n Proof: We use Hoeffding's Inequality, which is a simple form of Chernoff bounds <ref> [4, 1] </ref>: Let fX i g be a set of independent, identically-distributed random Bernoulli variables, whose 19 common mean is . Let S (M) = 1 M i=1 X i be the sample mean after taking M samples.
Reference: [5] <author> Gerald DeJong and Raymond Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-76, </pages> <year> 1986. </year>
Reference-contexts: Our approach also resembles the work on speed-up learning (including both "explanation-based learning" <ref> [15, 5, 14] </ref> and "chunking" [13]), as it uses previous solutions to suggest a way of improving the speed of a performance system.
Reference: [6] <author> M. R. Garey. </author> <title> Optimal task sequencing with precedence constraints. </title> <journal> Discrete Mathematics, </journal> <volume> 4, </volume> <year> 1973. </year>
Reference-contexts: Earlier research on this decision making model has produced a number of "optimizing functions" that each identify a strategy optimal for a specific testing situation, given the success probability values of the relevant experiments <ref> [6, 21, 18, 22, 7] </ref>. A limitation of these techniques, however, is that the probability values are in practice typically not known a priori . <p> Other examples of such problems include, e.g., performing a sequence of tests to decide whether a product specimen is satisfactory <ref> [6] </ref>, screening employment candidates for a position [6], competing for prizes at a quiz show [6], mining for gold buried in treasure chests [21], and performing inference in simple expert systems [22, 10]. <p> Other examples of such problems include, e.g., performing a sequence of tests to decide whether a product specimen is satisfactory <ref> [6] </ref>, screening employment candidates for a position [6], competing for prizes at a quiz show [6], mining for gold buried in treasure chests [21], and performing inference in simple expert systems [22, 10]. In general, such tasks may involve searching through general "decision structures", which can involve an arbitrary number of experiments, constrained by various precedence constraints. <p> Other examples of such problems include, e.g., performing a sequence of tests to decide whether a product specimen is satisfactory <ref> [6] </ref>, screening employment candidates for a position [6], competing for prizes at a quiz show [6], mining for gold buried in treasure chests [21], and performing inference in simple expert systems [22, 10]. In general, such tasks may involve searching through general "decision structures", which can involve an arbitrary number of experiments, constrained by various precedence constraints. <p> Of course, exhaustive search is in general impractical, and if we are dealing with decision structures with concise encodings, such as and-or trees, the optimal strategies may not even have polynomial-size representations. Nevertheless, optimal strategies can be determined in polynomial time in many interesting special cases. Garey <ref> [6] </ref> provided an algorithm for finding the optimal search strategy when the constraints can be represented as a regular "or-tree" (i.e. no conjunctive subgoals and no multiple predecessors are allowed; cf. also [22]). <p> The underlying objective of finding a provably good search strategy comes from the work on optimal satisficing search strategies <ref> [6, 21, 2, 18, 22] </ref>. Each of these earlier papers considered some specifically defined class of search structures and, moreover, required the user to supply precise success probability values for the experiments. Our work extends this body of research in three ways.
Reference: [7] <author> Dan Geiger and Jeffrey A. Barnett. </author> <title> Optimal satisficing tree searches. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <year> 1991. </year>
Reference-contexts: Earlier research on this decision making model has produced a number of "optimizing functions" that each identify a strategy optimal for a specific testing situation, given the success probability values of the relevant experiments <ref> [6, 21, 18, 22, 7] </ref>. A limitation of these techniques, however, is that the probability values are in practice typically not known a priori .
Reference: [8] <author> Russell Greiner. </author> <title> Finding the optimal derivation strategy in a redundant knowledge base. </title> <journal> Artificial Intelligence, </journal> <volume> 50(1) </volume> <pages> 95-116, </pages> <year> 1991. </year>
Reference-contexts: Simon and Kadane [21] later extended this algorithm to deal with directed acyclic graphs in the special case where success at any intermediate node implies global success. (In dag's where global success requires reaching a specified goal node, the problem is NP-hard <ref> [8] </ref>.) It is currently not known whether optimal strategies can be found in polynomial time for and-or trees.
Reference: [9] <author> Russell Greiner and Igor Jurisica. </author> <title> A statistical approach to solving the EBL utility problem. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <address> San Jose, </address> <year> 1992. </year>
Reference-contexts: We are not considering ways of modifying the strategy gradually over time to become incrementally better; but see <ref> [9] </ref>.
Reference: [10] <author> Russell Greiner and Pekka Orponen. </author> <title> Probably approximately optimal derivation strategies. </title> <editor> In J.A. Allen, R. Fikes, and E. Sandewall, editors, </editor> <booktitle> Proceedings of KR-91, </booktitle> <address> San Mateo, CA, April 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The authors thank Dale Schuurmans, Tom Hancock and the anonymous referees for their useful comments on earlier versions of this paper. Preliminary versions of parts of the work have appeared in the conference reports [19] and <ref> [10] </ref>. 1 test and conclude the patient has hepatitis if that test is positive. If not, he would then examine the patient's liver, and conclude his diagnosis based on the result of that biopsy. <p> examples of such problems include, e.g., performing a sequence of tests to decide whether a product specimen is satisfactory [6], screening employment candidates for a position [6], competing for prizes at a quiz show [6], mining for gold buried in treasure chests [21], and performing inference in simple expert systems <ref> [22, 10] </ref>. In general, such tasks may involve searching through general "decision structures", which can involve an arbitrary number of experiments, constrained by various precedence constraints.
Reference: [11] <author> Russell Greiner and Pekka Orponen. </author> <title> Probably approximately optimal satisficing strategies. </title> <type> Technical report, </type> <institution> Siemens Corporate Research, </institution> <year> 1993. </year>
Reference-contexts: When dealing with certain search structures, notably and-or trees, the pao algorithm can "learn while doing", i.e. gather the necessary statistics while solving relevant performance tasks. An extended version of this paper, available as a technical report <ref> [11] </ref>, discusses several variants and applications of the basic algorithm presented here. 2 Framework 2.1 Decision Structures The doctor's task presented in Section 1 is a simple example of a satisficing search problem (term due to Simon and Kadane [21]), as his goal is to find a single satisfactory configuration of <p> In general, we may want the cost to depend also on whether e, and/or various prior experiments, have been successful. There are also situations which require yet more complicated ways of computing the incremental cost of performing a particular experiment; see the extended paper <ref> [11] </ref>. To accommodate these extensions, we define a more general class of "decision structures". <p> In fact, one can use the same GS algorithm whenever it is possible to identify each experiment e with a strategy fi e for which (e; fi e ) = (e). This is not always straightforward. The extended paper <ref> [11] </ref> includes an algorithm that uses dynamic programming techniques to sequentially estimate the probabilities of each "layer" of certain types of decision structures. Unfortunately, the following general result shows that the computational complexity of any such algorithm is likely to be exponential in the number of experiments.
Reference: [12] <author> D. S. Johnson. </author> <title> A catalog of complexity classes. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume A: Algorithms and Complexity, </booktitle> <pages> pages 67-186. </pages> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: Unfortunately, the following general result shows that the computational complexity of any such algorithm is likely to be exponential in the number of experiments. Theorem 2 Assume RP 6= NP. (RP is the class of problems solvable by probabilistic polynomial time algorithms with one-way error, cf. <ref> [12] </ref>.) Then there is no probabilistic poly nomial time algorithm, and consequently no deterministic polynomial time algorithm that, 14 given a decision structure G = hW; F; R; ci, an experiment e 2 W , a distribution func-tion p : W ! [0; 1], and parameters *, ffi &gt; 0, can
Reference: [13] <author> John E. Laird, Paul S. Rosenbloom, and Allan Newell. </author> <title> Universal Subgoaling and Chunking: The Automatic Generation and Learning of Goal Hierarchies. </title> <publisher> Kluwer Academic Press, </publisher> <address> Hingham, MA, </address> <year> 1986. </year> <month> 23 </month>
Reference-contexts: Our approach also resembles the work on speed-up learning (including both "explanation-based learning" [15, 5, 14] and "chunking" <ref> [13] </ref>), as it uses previous solutions to suggest a way of improving the speed of a performance system.
Reference: [14] <author> Steven Minton, Jaime Carbonell, C.A. Knoblock, D.R. Kuokka, Oren Etzioni, and Y. Gil. </author> <title> Explanation-based learning: A problem solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40(1-3):63-119, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: Our approach also resembles the work on speed-up learning (including both "explanation-based learning" <ref> [15, 5, 14] </ref> and "chunking" [13]), as it uses previous solutions to suggest a way of improving the speed of a performance system.
Reference: [15] <author> Thomas M. Mitchell, Richard M. Keller, and Smadar T. Kedar-Cabelli. </author> <title> Example-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Our approach also resembles the work on speed-up learning (including both "explanation-based learning" <ref> [15, 5, 14] </ref> and "chunking" [13]), as it uses previous solutions to suggest a way of improving the speed of a performance system.
Reference: [16] <author> Thomas M. Mitchell, Sridhar Mahadevan, and Louis I. Steinberg. </author> <title> LEAP: A learning apprentice for VLSI design. </title> <booktitle> In Proceedings of IJCAI-85, </booktitle> <pages> pages 573-80, </pages> <address> Los Angeles, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: After gathering enough information, the learner would compute the approximately optimal strategy fi pao , instruct the doctor to use this fi pao strategy, and terminate itself. 1 We view this as a "learning while doing" protocol <ref> [16] </ref>, as the overall system is performing useful work during the learning phase (here, examining patients). From now on, we assume our oracle O, when queried, provides only an unlabeled sample (e.g., a patient ), rather than the full labelings of that sample, L ().
Reference: [17] <author> Kumpati S. Narendra and Mandayam A. L. Thathachar. </author> <title> Learning automata: an introduction. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1989. </year>
Reference-contexts: We are not considering ways of modifying the strategy gradually over time to become incrementally better; but see [9]. Also, this issue differs from the "Exploration-Exploitation" tradeoff discussed in the context of the Bandit problem (cf. <ref> [3, 17] </ref>) as we are not concerned with minimizing the cumulative cost of the learning and performance systems together, over an infinite sequence of samples. 10 Fortunately, there is a way around this problem.
Reference: [18] <author> K. S. Natarajan. </author> <title> Optimizing depth-first search of AND-OR trees. </title> <type> Technical report, </type> <institution> Research report RC-11842, IBM T. J. Watson Research Center, </institution> <month> January </month> <year> 1986. </year>
Reference-contexts: Earlier research on this decision making model has produced a number of "optimizing functions" that each identify a strategy optimal for a specific testing situation, given the success probability values of the relevant experiments <ref> [6, 21, 18, 22, 7] </ref>. A limitation of these techniques, however, is that the probability values are in practice typically not known a priori . <p> Some partial results on this question exist: for instance, Natarajan <ref> [18] </ref> presents an efficient algorithm for finding optimal "depth-first" search strategies in this case, and Smith [22] provides an algorithm for finding optimal "serial strategies". <p> The underlying objective of finding a provably good search strategy comes from the work on optimal satisficing search strategies <ref> [6, 21, 2, 18, 22] </ref>. Each of these earlier papers considered some specifically defined class of search structures and, moreover, required the user to supply precise success probability values for the experiments. Our work extends this body of research in three ways.
Reference: [19] <author> Pekka Orponen and Russell Greiner. </author> <title> On the sample complexity of finding good search strategies. </title> <booktitle> In Proceedings of COLT-90, </booktitle> <pages> pages 352-58, </pages> <address> Rochester, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: The authors thank Dale Schuurmans, Tom Hancock and the anonymous referees for their useful comments on earlier versions of this paper. Preliminary versions of parts of the work have appeared in the conference reports <ref> [19] </ref> and [10]. 1 test and conclude the patient has hepatitis if that test is positive. If not, he would then examine the patient's liver, and conclude his diagnosis based on the result of that biopsy.
Reference: [20] <author> S. Sahni. </author> <title> Computationally related problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 3(4) </volume> <pages> 262-279, </pages> <year> 1974. </year>
Reference-contexts: In the more general case of and-or dag's, the problem is NP-hard even when all the success probabilities are 1 <ref> [20] </ref>. 7 3 The pao Algorithm Each of the abovementioned optimization algorithms assumes that the precise success probabilities of the experiments are known, which of course is not the case in most real-life situations.
Reference: [21] <author> H. A. Simon and J. B. Kadane. </author> <title> Optimal problem-solving search: All-or-none solutions. </title> <journal> Artificial Intelligence, </journal> <volume> 6 </volume> <pages> 235-247, </pages> <year> 1975. </year>
Reference-contexts: Earlier research on this decision making model has produced a number of "optimizing functions" that each identify a strategy optimal for a specific testing situation, given the success probability values of the relevant experiments <ref> [6, 21, 18, 22, 7] </ref>. A limitation of these techniques, however, is that the probability values are in practice typically not known a priori . <p> An extended version of this paper, available as a technical report [11], discusses several variants and applications of the basic algorithm presented here. 2 Framework 2.1 Decision Structures The doctor's task presented in Section 1 is a simple example of a satisficing search problem (term due to Simon and Kadane <ref> [21] </ref>), as his goal is to find a single satisfactory configuration of events: in this case, an informative combination of test results. <p> Other examples of such problems include, e.g., performing a sequence of tests to decide whether a product specimen is satisfactory [6], screening employment candidates for a position [6], competing for prizes at a quiz show [6], mining for gold buried in treasure chests <ref> [21] </ref>, and performing inference in simple expert systems [22, 10]. In general, such tasks may involve searching through general "decision structures", which can involve an arbitrary number of experiments, constrained by various precedence constraints. <p> Garey [6] provided an algorithm for finding the optimal search strategy when the constraints can be represented as a regular "or-tree" (i.e. no conjunctive subgoals and no multiple predecessors are allowed; cf. also [22]). Simon and Kadane <ref> [21] </ref> later extended this algorithm to deal with directed acyclic graphs in the special case where success at any intermediate node implies global success. (In dag's where global success requires reaching a specified goal node, the problem is NP-hard [8].) It is currently not known whether optimal strategies can be found <p> The underlying objective of finding a provably good search strategy comes from the work on optimal satisficing search strategies <ref> [6, 21, 2, 18, 22] </ref>. Each of these earlier papers considered some specifically defined class of search structures and, moreover, required the user to supply precise success probability values for the experiments. Our work extends this body of research in three ways.
Reference: [22] <author> David E. Smith. </author> <title> Controlling backward inference. </title> <journal> Artificial Intelligence, </journal> <volume> 39(2) </volume> <pages> 145-208, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Earlier research on this decision making model has produced a number of "optimizing functions" that each identify a strategy optimal for a specific testing situation, given the success probability values of the relevant experiments <ref> [6, 21, 18, 22, 7] </ref>. A limitation of these techniques, however, is that the probability values are in practice typically not known a priori . <p> examples of such problems include, e.g., performing a sequence of tests to decide whether a product specimen is satisfactory [6], screening employment candidates for a position [6], competing for prizes at a quiz show [6], mining for gold buried in treasure chests [21], and performing inference in simple expert systems <ref> [22, 10] </ref>. In general, such tasks may involve searching through general "decision structures", which can involve an arbitrary number of experiments, constrained by various precedence constraints. <p> Nevertheless, optimal strategies can be determined in polynomial time in many interesting special cases. Garey [6] provided an algorithm for finding the optimal search strategy when the constraints can be represented as a regular "or-tree" (i.e. no conjunctive subgoals and no multiple predecessors are allowed; cf. also <ref> [22] </ref>). <p> Some partial results on this question exist: for instance, Natarajan [18] presents an efficient algorithm for finding optimal "depth-first" search strategies in this case, and Smith <ref> [22] </ref> provides an algorithm for finding optimal "serial strategies". <p> The underlying objective of finding a provably good search strategy comes from the work on optimal satisficing search strategies <ref> [6, 21, 2, 18, 22] </ref>. Each of these earlier papers considered some specifically defined class of search structures and, moreover, required the user to supply precise success probability values for the experiments. Our work extends this body of research in three ways.
Reference: [23] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-42, </pages> <year> 1984. </year>
Reference-contexts: Finally, this work derives many of its mathematical methods, as well as its title, from the field of "probably approximately correct learning" <ref> [23] </ref>. We hope to have enriched this field by providing an application of the PAC framework outside of its traditional setting of concept learning. 15 A Proofs This appendix contains the proofs of the results mentioned in the body of the paper.
References-found: 23

