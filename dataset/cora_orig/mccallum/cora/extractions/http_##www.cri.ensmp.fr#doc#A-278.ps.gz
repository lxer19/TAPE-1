URL: http://www.cri.ensmp.fr/doc/A-278.ps.gz
Refering-URL: http://www.cri.ensmp.fr/rapports.html
Root-URL: 
Title: A Linear Algebra Framework for Static HPF Code Distribution  
Author: Corinne Ancourt, Fabien Coelho, Fran~cois Irigoin, Ronan Keryell Fabien Coelho. 
Address: 35, rue Saint-Honore, F-77305 Fontainebleau cedex, France.  
Affiliation: Centre de Recherche en Informatique, Ecole Nationale Superieure des Mines de Paris,  
Note: Corresponding author:  ftp:  
Pubnum: Technical report A-278-CRI  
Email: anonymous@ftp.cri.ensmp.fr  
Phone: Phone: +33 1 64 69 47 08. Fax: 33 1 64 69 47 09.  
Date: November 24, 1995  
Web: URL: http://www.cri.ensmp.fr/pips,  
Abstract: High Performance Fortran (hpf) was developed to support data parallel programming for simd and mimd machines with distributed memory. The programmer is provided a familiar uniform logical address space and specifies the data distribution by directives. The compiler then exploits these directives to allocate arrays in the local memories, to assign computations to elementary processors and to migrate data between processors when required. We show here that linear algebra is a powerful framework to encode Hpf directives and to synthesize distributed code with space-efficient array allocation, tight loop bounds and vectorized communications for INDEPENDENT loops. The generated code includes traditional optimizations such as guard elimination, message vectorization and aggregation, overlap analysis... The systematic use of an affine framework makes it possible to prove the compilation scheme correct. fl An early version of this paper was presented at the Fourth International Workshop on Compilers for Parallel Computers held in Delft, the Netherlands, December 1993. y fancourt,coelho,irigoin,keryellg@cri.ensmp.fr, http://www.cri.ensmp.fr/~f...g 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Many partial optimization techniques are integrated in our direct synthesis approach: message vectorization, and aggregation [47], overlap analysis [38]. A new storage management scheme is also proposed. Moreover other optimizations techniques may be applied to the generated code such as vectorization [87], loop invariant code motion <ref> [1] </ref> and software pipelining [37, 84]. This technique uses algorithms, directly or indirectly, that may be costly, such as Fourier elimination or the simplex algorithm, which have exponential worst-case behaviors. They are used for array region analysis, in the set manipulations and in the code generation for polyhedron scanning.
Reference: [2] <author> Saman P. Amarasinghe and Monica S. Lam. </author> <title> Communication Optimization and Code Generation for Distributed Memory Machines. </title> <booktitle> In ACM SIGPLAN International Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Using (13) the constraints on u can be written K (x 0 (n; p) + P 01 H 0 u) k 0 (n), that is K 0 u k 0 0 (n; p), where K 0 = KP 0a H 0 and k 0 Algorithms presented in [4] or others <ref> [35, 30, 53, 2, 24, 23, 62, 59, 54, 86] </ref> can be used to generate the loop nest enumerating the local iterations. When S is of rank jaj, optimal code is generated because no projections are required. <p> Stichnoth uses the dual method for array allocation as in [26], that is blocks are first compressed, and the cycle number is used as a second argument. In [3, 5] polyhedron-based techniques are presented to generate transfer code for machines with a distributed memory. In <ref> [2, 82] </ref> advanced analyses are used as an input to a code generation phase for distributed memory machines. Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. <p> In [2, 82] advanced analyses are used as an input to a code generation phase for distributed memory machines. Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques <ref> [49, 4, 53, 2, 62, 59, 54, 86] </ref>, and second, parametric integer programming based methods [35, 30, 24, 23]. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest.
Reference: [3] <author> Corinne Ancourt. </author> <title> Generation automatique de codes de transfert pour multipro-cesseurs a memoires locales. </title> <type> PhD thesis, </type> <institution> Universite Paris VI, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Arrays are densely allocated as in [25] and the initial order is preserved but no formulae are given. Stichnoth uses the dual method for array allocation as in [26], that is blocks are first compressed, and the cycle number is used as a second argument. In <ref> [3, 5] </ref> polyhedron-based techniques are presented to generate transfer code for machines with a distributed memory. In [2, 82] advanced analyses are used as an input to a code generation phase for distributed memory machines. Polyhedron scanning techniques are used for generating the code.
Reference: [4] <author> Corinne Ancourt and Fran~cois Irigoin. </author> <title> Scanning Polyhedra with DO Loops. </title> <booktitle> In Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: If regions are not precise enough because convex hulls are used to summarize multiple references, it is possible to use additional parameters to exactly express a set of references with regular holes <ref> [4] </ref>. This might be useful for red-black sor. <p> Send and Receive are polyhedral sets and algorithms in <ref> [4] </ref> can be used. If the last component of u is allocated contiguously, vector messages can be generated. 3.5 Output SPMD code The generic output spmd code, parametric on the local processor p, is shown in Figure 10. U represents nay local array generated according to the dependence graph. <p> Using (13) the constraints on u can be written K (x 0 (n; p) + P 01 H 0 u) k 0 (n), that is K 0 u k 0 0 (n; p), where K 0 = KP 0a H 0 and k 0 Algorithms presented in <ref> [4] </ref> or others [35, 30, 53, 2, 24, 23, 62, 59, 54, 86] can be used to generate the loop nest enumerating the local iterations. When S is of rank jaj, optimal code is generated because no projections are required. <p> Experiments are needed to find out the best approach. Note that an extra-loop is generated. Y diagonal can be enumerated with only two loops and three are generated. This is due to the use of an imprecise projection algorithm but does not endanger correctness <ref> [4] </ref>. Further work is needed in this area. Integer divide One implementation of the integer divide is finally shown. The divider is assumed strictly positive, as is the case in all call sites. It necessary because Fortran remainder is not positive for negative numbers. <p> In [2, 82] advanced analyses are used as an input to a code generation phase for distributed memory machines. Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques <ref> [49, 4, 53, 2, 62, 59, 54, 86] </ref>, and second, parametric integer programming based methods [35, 30, 24, 23]. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest.
Reference: [5] <author> Corinne Ancourt and Fran~cois Irigoin. </author> <title> Automatic code distribution. </title> <booktitle> In Third Workshop on Compilers for Parallel Computers, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: When Y (or X, or Z,...) is referenced many times in the input loop or in the input program, these references must be clustered according to their connexion in the dependence graph <ref> [5] </ref>. Input dependencies are taken into account as well as usual ones (flow-, anti- and output-dependencies). <p> Arrays are densely allocated as in [25] and the initial order is preserved but no formulae are given. Stichnoth uses the dual method for array allocation as in [26], that is blocks are first compressed, and the cycle number is used as a second argument. In <ref> [3, 5] </ref> polyhedron-based techniques are presented to generate transfer code for machines with a distributed memory. In [2, 82] advanced analyses are used as an input to a code generation phase for distributed memory machines. Polyhedron scanning techniques are used for generating the code.
Reference: [6] <author> Fran~coise Andre, Olivier Cheron, and Jean-Louis Pazat. </author> <title> Compiling sequential programs for distributed memory parallel computers with Pandore II. </title> <type> Technical report, </type> <institution> IRISA, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C <ref> [8, 63, 6, 60, 7, 61] </ref> or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. Each instruction is guarded by a condition which is only true for processors that must execute it.
Reference: [7] <author> Fran~coise Andre, Marc Le Fur, Yves Maheo, and Jean-Louis Pazat. </author> <title> Parallelization of a Wave Propagation Application using a Data Parallel Compiler. </title> <note> Publication interne 868, IRISA, </note> <month> October </month> <year> 1994. </year>
Reference-contexts: Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C <ref> [8, 63, 6, 60, 7, 61] </ref> or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. Each instruction is guarded by a condition which is only true for processors that must execute it.
Reference: [8] <author> Fran~coise Andre, Jean-Louis Pazat, and Henry Thomas. </author> <title> Pandore: A system to manage data distribution. </title> <note> Publication Interne 519, IRISA, </note> <month> February </month> <year> 1990. </year>
Reference-contexts: Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C <ref> [8, 63, 6, 60, 7, 61] </ref> or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. Each instruction is guarded by a condition which is only true for processors that must execute it.
Reference: [9] <author> Beatrice Apvrille. </author> <title> Calcul de regions de tableaux exactes. </title> <booktitle> In Rencontres Franco-phones du Parallelisme, </booktitle> <pages> pages 65-68, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The definition of View is thus altered to take into account array regions. These regions are the result of a precise program analysis which is presented in <ref> [80, 50, 9, 11, 10, 33, 32, 31] </ref>. An array region is a set of array elements described by equalities and inequalities defining a convex polyhedron. This polyhedron may be parameterized by program variables. Each array dimension is described by a variable.
Reference: [10] <author> Beatrice Apvrille-Creusillet. </author> <title> Regions exactes et privatisation de tableaux (exact array region analysis and array privatization). </title> <type> Master's thesis, </type> <institution> Universite Paris VI, France, </institution> <month> September </month> <year> 1994. </year> <note> Available via http://www.cri.ensmp.fr/~creusil. </note>
Reference-contexts: The definition of View is thus altered to take into account array regions. These regions are the result of a precise program analysis which is presented in <ref> [80, 50, 9, 11, 10, 33, 32, 31] </ref>. An array region is a set of array elements described by equalities and inequalities defining a convex polyhedron. This polyhedron may be parameterized by program variables. Each array dimension is described by a variable.
Reference: [11] <author> Beatrice Apvrille-Creusillet. </author> <title> Calcul de regions de tableaux exactes. </title> <journal> TSI, Numero special RenPar'6, </journal> <month> mai </month> <year> 1995. </year>
Reference-contexts: The definition of View is thus altered to take into account array regions. These regions are the result of a precise program analysis which is presented in <ref> [80, 50, 9, 11, 10, 33, 32, 31] </ref>. An array region is a set of array elements described by equalities and inequalities defining a convex polyhedron. This polyhedron may be parameterized by program variables. Each array dimension is described by a variable.
Reference: [12] <author> Florin Balasa, Frank H. M. Fransen, Francky V. M. Catthoor, and Hugo J. De Man. </author> <title> Transformation on nested loops with modulo indexing to affine recurrences. </title> <journal> Parallel Processing Letters, </journal> <volume> 4(3) </volume> <pages> 271-280, </pages> <month> March </month> <year> 1994. </year> <title> 38 Ancourt et al., A Linear Algebra Framework: </title> : : 
Reference-contexts: Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques [49, 4, 53, 2, 62, 59, 54, 86], and second, parametric integer programming based methods [35, 30, 24, 23]. In <ref> [12] </ref>, a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest. First, variables are added to explicit the modulo computation, then the Hermite computations are used to regenerate simply new loop bounds.
Reference: [13] <author> V. Balasundaram and Ken Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In ACM SIGPLAN International Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Moreover these systems tend to be composed of independent subsystems on a dimension per dimension basis, resulting in a more efficient practical behavior. Secondly efficient and highly tuned versions of such algorithms are available, for instance in the Omega library. Thirdly, potentially less precise but faster program analysis <ref> [21, 13, 45] </ref> can also be used in place of the region analysis. Polyhedron-based techniques are already implemented in hpfc, our prototype Hpf compiler [27] to deal with I/O communications in a host/nodes model [28] and also to deal with dynamic remappings [29] (realign and redistribute directives).
Reference: [14] <author> Prithviraj Banerjee, John A. Chandy, Manish Gupta, Eugene W. Hodges IV, John G. Holm, Antonio Lain, Daniel J. Palermo, Shankar Ramaswamy, and Ernesto Su. </author> <title> An Overview of the PARADIGME Ccompiler for Distributed-Memory Multicomputers. </title> <journal> IEEE Computer, </journal> <volume> 28(10), </volume> <month> October </month> <year> 1995. </year>
Reference-contexts: The initial definition of the new language, Hpf, was frozen in May 1993, and corrections were added in November 1994 [36]. Prototype compilers incorporating some Hpf features are available <ref> [18, 19, 26, 81, 88, 14] </ref>. Commercial compilers from APR [64, 65], DEC [71, 17], IBM [42] and PGI [34, 68] are also being developed or are already available. These compilers implement part or all of the Hpf Subset, which only allows static distribution of data and prohibits dynamic redistributions.
Reference: [15] <author> Siegfried Benkner. </author> <title> Handling Block-Cyclic Distributed Arrays in Vienna Fortran. </title> <type> Technical Report 9, </type> <institution> Institute for Software Technology and Parallel Systems, University of Vienna, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Each dimension is independent of the others as was assumed in Section 4.2. In Paalvast et al. [72, 83] a technique based on the resolution of the associated Diophantine equations is presented. Row- and column-wise allocation and addressing schemes are discussed. Benkner et al. <ref> [16, 15] </ref> present similar techniques. Chatterjee et al. [25] developed a finite state machine approach to enumerate local elements. No memory space is wasted and local array elements are ordered by Fortran lexicographic order exactly like user array elements.
Reference: [16] <author> Siegfried Benkner, Peter Brezany, and Hans Zima. </author> <title> Processing Array Statements and Procedure Interfaces in the Prepare High Performance Fortran Compiler. </title> <booktitle> In 5th International Conference on Compiler Construction, </booktitle> <address> April 1994. </address> <publisher> Springer-Verlag LNCS vol. </publisher> <pages> 786, pages 324-338. </pages>
Reference-contexts: Each dimension is independent of the others as was assumed in Section 4.2. In Paalvast et al. [72, 83] a technique based on the resolution of the associated Diophantine equations is presented. Row- and column-wise allocation and addressing schemes are discussed. Benkner et al. <ref> [16, 15] </ref> present similar techniques. Chatterjee et al. [25] developed a finite state machine approach to enumerate local elements. No memory space is wasted and local array elements are ordered by Fortran lexicographic order exactly like user array elements.
Reference: [17] <author> John A. Bircsak, M. Regina Bolduc, Jill Ann Diewald, Israel Gale, Jonathan Harris, Neil W. Johnson, Shin Lee, C. Alexander Nelson, and Carl D. Offner. </author> <title> Compiling High Performance Fortran for Distributed-Memory Systems. </title> <type> Report, </type> <institution> Digital Equipment Corp., </institution> <month> October </month> <year> 1995. </year> <note> To be published in the Digital Technical Journal. </note>
Reference-contexts: The initial definition of the new language, Hpf, was frozen in May 1993, and corrections were added in November 1994 [36]. Prototype compilers incorporating some Hpf features are available [18, 19, 26, 81, 88, 14]. Commercial compilers from APR [64, 65], DEC <ref> [71, 17] </ref>, IBM [42] and PGI [34, 68] are also being developed or are already available. These compilers implement part or all of the Hpf Subset, which only allows static distribution of data and prohibits dynamic redistributions.
Reference: [18] <author> Thomas Brandes. </author> <title> Efficient Data-Parallel Programming without Explicit Message Passing for Distributed Memory Multiprocessors. </title> <type> Internal Report AHR-92 4, </type> <institution> High Performance Computing Center, German National Research Institute for Computer Science, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: The initial definition of the new language, Hpf, was frozen in May 1993, and corrections were added in November 1994 [36]. Prototype compilers incorporating some Hpf features are available <ref> [18, 19, 26, 81, 88, 14] </ref>. Commercial compilers from APR [64, 65], DEC [71, 17], IBM [42] and PGI [34, 68] are also being developed or are already available. These compilers implement part or all of the Hpf Subset, which only allows static distribution of data and prohibits dynamic redistributions. <p> Techniques and prototypes have been developed based on Fortran <ref> [38, 39, 47, 18, 69, 88, 19, 20] </ref>, C [8, 63, 6, 60, 7, 61] or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74].
Reference: [19] <author> Thomas Brandes. </author> <title> Adaptor: A compilation system for data parallel fortran programs. </title> <type> Technical report, </type> <institution> High Performance Computing Center, German National Research Institute for Computer Science, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: The initial definition of the new language, Hpf, was frozen in May 1993, and corrections were added in November 1994 [36]. Prototype compilers incorporating some Hpf features are available <ref> [18, 19, 26, 81, 88, 14] </ref>. Commercial compilers from APR [64, 65], DEC [71, 17], IBM [42] and PGI [34, 68] are also being developed or are already available. These compilers implement part or all of the Hpf Subset, which only allows static distribution of data and prohibits dynamic redistributions. <p> Techniques and prototypes have been developed based on Fortran <ref> [38, 39, 47, 18, 69, 88, 19, 20] </ref>, C [8, 63, 6, 60, 7, 61] or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74].
Reference: [20] <author> Thomas Brandes. </author> <title> Evaluation of High Performance Fortran on some Real Applications. In High-Performance Computing and Networking, </title> <publisher> Springer-Verlag LNCS 797, </publisher> <pages> pages 417-422, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Techniques and prototypes have been developed based on Fortran <ref> [38, 39, 47, 18, 69, 88, 19, 20] </ref>, C [8, 63, 6, 60, 7, 61] or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74].
Reference: [21] <author> D. Callahan and Ken Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <journal> Jounal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference-contexts: Moreover these systems tend to be composed of independent subsystems on a dimension per dimension basis, resulting in a more efficient practical behavior. Secondly efficient and highly tuned versions of such algorithms are available, for instance in the Omega library. Thirdly, potentially less precise but faster program analysis <ref> [21, 13, 45] </ref> can also be used in place of the region analysis. Polyhedron-based techniques are already implemented in hpfc, our prototype Hpf compiler [27] to deal with I/O communications in a host/nodes model [28] and also to deal with dynamic remappings [29] (realign and redistribute directives).
Reference: [22] <author> David Callahan and Ken Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <year> 1988. </year>
Reference-contexts: Nevertheless, if we can prove the dividend is positive, we can use the Fortran division. 34 Ancourt et al., A Linear Algebra Framework: : : 6 Related work Techniques to generate distributed code from sequential or parallel code using a uniform memory space have been extensively studied since 1988 <ref> [22, 70, 89] </ref>. Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C [8, 63, 6, 60, 7, 61] or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. <p> Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C [8, 63, 6, 60, 7, 61] or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution <ref> [22, 70, 74] </ref>. Each instruction is guarded by a condition which is only true for processors that must execute it. <p> This rewriting scheme is easy to implement [26] but very inefficient at run-time because guards, tests, sends and receives are pure overhead. Moreover every processor has to execute the whole control flow of the program, and even for parallel loop, communications may sequentialize the program at run-time <ref> [22] </ref>. Many optimization techniques have been introduced to handle specific cases. Gerndt introduced overlap analysis in [38] for block distributions.
Reference: [23] <author> Zbigniew Chamski. </author> <title> Fast and efficient generation of loop bounds. </title> <note> Research Report 2095, INRIA, </note> <month> October </month> <year> 1993. </year>
Reference-contexts: Using (13) the constraints on u can be written K (x 0 (n; p) + P 01 H 0 u) k 0 (n), that is K 0 u k 0 0 (n; p), where K 0 = KP 0a H 0 and k 0 Algorithms presented in [4] or others <ref> [35, 30, 53, 2, 24, 23, 62, 59, 54, 86] </ref> can be used to generate the loop nest enumerating the local iterations. When S is of rank jaj, optimal code is generated because no projections are required. <p> Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques [49, 4, 53, 2, 62, 59, 54, 86], and second, parametric integer programming based methods <ref> [35, 30, 24, 23] </ref>. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest. First, variables are added to explicit the modulo computation, then the Hermite computations are used to regenerate simply new loop bounds.
Reference: [24] <author> Zbigniew Chamski. </author> <title> Nested loop sequences: Towards efficient loop structures in automatic parallelization. </title> <note> Research Report 2094, INRIA, </note> <month> October </month> <year> 1993. </year> <booktitle> In Proceedings of the 27th Annual Hawaii Int. Conf. on System Sciences, </booktitle> <year> 1994, </year> <note> p. 14-22. Submitted to Scientific Programming 39 </note>
Reference-contexts: Using (13) the constraints on u can be written K (x 0 (n; p) + P 01 H 0 u) k 0 (n), that is K 0 u k 0 0 (n; p), where K 0 = KP 0a H 0 and k 0 Algorithms presented in [4] or others <ref> [35, 30, 53, 2, 24, 23, 62, 59, 54, 86] </ref> can be used to generate the loop nest enumerating the local iterations. When S is of rank jaj, optimal code is generated because no projections are required. <p> Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques [49, 4, 53, 2, 62, 59, 54, 86], and second, parametric integer programming based methods <ref> [35, 30, 24, 23] </ref>. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest. First, variables are added to explicit the modulo computation, then the Hermite computations are used to regenerate simply new loop bounds.
Reference: [25] <author> Siddhartha Chatterjee, John R. Gilbert, Fred J. E. Long, Robert Schreiber, and Shang-Hua Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <journal> Jounal of Parallel and Distributed Computing, </journal> <volume> 26(1) </volume> <pages> 72-84, </pages> <month> April </month> <year> 1995. </year> <note> Also appeared in PPoPP'93. </note>
Reference-contexts: used to obtain a triangular enumeration, and from (3) the independent parallelism of the loop which allows any enumeration order. 4.2 Symbolic resolution The previous method can be applied in a symbolic way, if the dimensions are not coupled and thus can be dealt with independently, as array sections in <ref> [25, 43, 78] </ref>. <p> The geometric intuition of the packing scheme for the <ref> [25] </ref> example is shown in Figure 12. The basic idea is to remove the regular holes due to the alignment stride by allocating the dense u space on each processor. The "ffi" and "*" are just used to support the geometrical intuition of the change of frame. <p> Constraints may also be simplified, for instance if the concerned elements just match a cycle. Moreover, it is possible to generate the loop nest directly on u 0 , when u is not used in the loop body. For the main example in <ref> [25] </ref>, such transformations produce the code shown in Figure 15. In the general resolution (Section 4.1) the cycle variables c were put after the local offsets `. The induced inner loop nest is then on c. <p> This is an extension of the example in <ref> [25] </ref> showing that allocation of Hpf arrays may be non-trivial. The reference to X in the first loop requires an allocation of X 0 and the computation of new loop bounds. <p> In Paalvast et al. [72, 83] a technique based on the resolution of the associated Diophantine equations is presented. Row- and column-wise allocation and addressing schemes are discussed. Benkner et al. [16, 15] present similar techniques. Chatterjee et al. <ref> [25] </ref> developed a finite state machine approach to enumerate local elements. No memory space is wasted and local array elements are ordered by Fortran lexicographic order exactly like user array elements. <p> Submitted to Scientific Programming 35 They use array sections but compute some of the coefficients at run-time. Gupta et al. solve the block distribution case and use processor virtualization to handle cyclic distributions. Arrays are densely allocated as in <ref> [25] </ref> and the initial order is preserved but no formulae are given. Stichnoth uses the dual method for array allocation as in [26], that is blocks are first compressed, and the cycle number is used as a second argument.
Reference: [26] <author> Fabien Coelho. </author> <title> Etude de la Compilation du High Performance Fortran. </title> <type> Master's thesis, </type> <institution> Universite Paris VI, </institution> <month> September </month> <year> 1993. </year> <institution> Rapport de DEA Systemes Informa-tiques. TR EMP E/178/CRI. </institution>
Reference-contexts: The initial definition of the new language, Hpf, was frozen in May 1993, and corrections were added in November 1994 [36]. Prototype compilers incorporating some Hpf features are available <ref> [18, 19, 26, 81, 88, 14] </ref>. Commercial compilers from APR [64, 65], DEC [71, 17], IBM [42] and PGI [34, 68] are also being developed or are already available. These compilers implement part or all of the Hpf Subset, which only allows static distribution of data and prohibits dynamic redistributions. <p> Each memory address is checked before it is referenced to decide whether the address is local and the reference is executed, whether it is remote, and a receive is executed, or whether it is remotely accessed and a send is executed. This rewriting scheme is easy to implement <ref> [26] </ref> but very inefficient at run-time because guards, tests, sends and receives are pure overhead. Moreover every processor has to execute the whole control flow of the program, and even for parallel loop, communications may sequentialize the program at run-time [22]. <p> Gupta et al. solve the block distribution case and use processor virtualization to handle cyclic distributions. Arrays are densely allocated as in [25] and the initial order is preserved but no formulae are given. Stichnoth uses the dual method for array allocation as in <ref> [26] </ref>, that is blocks are first compressed, and the cycle number is used as a second argument. In [3, 5] polyhedron-based techniques are presented to generate transfer code for machines with a distributed memory.
Reference: [27] <author> Fabien Coelho. </author> <title> Experiments with HPF Compilation for a Network of Workstations. In High-Performance Computing and Networking, </title> <publisher> Springer-Verlag LNCS 797, </publisher> <pages> pages 423-428, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Secondly efficient and highly tuned versions of such algorithms are available, for instance in the Omega library. Thirdly, potentially less precise but faster program analysis [21, 13, 45] can also be used in place of the region analysis. Polyhedron-based techniques are already implemented in hpfc, our prototype Hpf compiler <ref> [27] </ref> to deal with I/O communications in a host/nodes model [28] and also to deal with dynamic remappings [29] (realign and redistribute directives). For instance, the code generation times for arbitrary remappings are in 0.1-2s range.
Reference: [28] <author> Fabien Coelho. </author> <title> Compilation of I/O Communications for HPF. </title> <booktitle> In 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 102-109, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Thirdly, potentially less precise but faster program analysis [21, 13, 45] can also be used in place of the region analysis. Polyhedron-based techniques are already implemented in hpfc, our prototype Hpf compiler [27] to deal with I/O communications in a host/nodes model <ref> [28] </ref> and also to deal with dynamic remappings [29] (realign and redistribute directives). For instance, the code generation times for arbitrary remappings are in 0.1-2s range.
Reference: [29] <author> Fabien Coelho and Corinne Ancourt. </author> <title> Optimal Compilation of HPF Remappings. </title> <type> Technical Report A 277, </type> <institution> CRI, Ecole des mines de Paris, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: An additional constraint should be added to restrict the sets of communications to needed ones. Different techniques can be used to address this issue: (1) replication allows broadcasts and/or load-balance, what is simply translated into linear constraints as described in <ref> [29] </ref>. (2) The affectation of owners to viewers can also be optimized in order to reduce the distance between communicating processors. <p> Polyhedron-based techniques are already implemented in hpfc, our prototype Hpf compiler [27] to deal with I/O communications in a host/nodes model [28] and also to deal with dynamic remappings <ref> [29] </ref> (realign and redistribute directives). For instance, the code generation times for arbitrary remappings are in 0.1-2s range. Future work includes the implementation of our scheme in hpfc, experiments, extensions to optimize sequential loops, to overlap communication and computation, and to handle indirections.
Reference: [30] <author> Jean-Fran~cois Collard, Paul Feautrier, and Tanguy Risset. </author> <title> Construction of DO loops from Systems of Affine Constraints. LIP RR93 15, </title> <address> ENS-Lyon, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Using (13) the constraints on u can be written K (x 0 (n; p) + P 01 H 0 u) k 0 (n), that is K 0 u k 0 0 (n; p), where K 0 = KP 0a H 0 and k 0 Algorithms presented in [4] or others <ref> [35, 30, 53, 2, 24, 23, 62, 59, 54, 86] </ref> can be used to generate the loop nest enumerating the local iterations. When S is of rank jaj, optimal code is generated because no projections are required. <p> Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques [49, 4, 53, 2, 62, 59, 54, 86], and second, parametric integer programming based methods <ref> [35, 30, 24, 23] </ref>. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest. First, variables are added to explicit the modulo computation, then the Hermite computations are used to regenerate simply new loop bounds.
Reference: [31] <editor> Beatrice Creusillet. Analyse de flot de donnees : Regions de tableaux IN et OUT. In Rencontres Francophones du Parallelisme, </editor> <month> mai-juin </month> <year> 1995. </year>
Reference-contexts: The definition of View is thus altered to take into account array regions. These regions are the result of a precise program analysis which is presented in <ref> [80, 50, 9, 11, 10, 33, 32, 31] </ref>. An array region is a set of array elements described by equalities and inequalities defining a convex polyhedron. This polyhedron may be parameterized by program variables. Each array dimension is described by a variable.
Reference: [32] <author> Beatrice Creusillet. </author> <title> IN and OUT array region analyses. </title> <booktitle> In Workshop on Compilers for Parallel Computers, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The definition of View is thus altered to take into account array regions. These regions are the result of a precise program analysis which is presented in <ref> [80, 50, 9, 11, 10, 33, 32, 31] </ref>. An array region is a set of array elements described by equalities and inequalities defining a convex polyhedron. This polyhedron may be parameterized by program variables. Each array dimension is described by a variable.
Reference: [33] <author> Beatrice Creusillet and Fran~cois Irigoin. </author> <title> Interprocedural Array Region Analyses. </title> <booktitle> In Language and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: The definition of View is thus altered to take into account array regions. These regions are the result of a precise program analysis which is presented in <ref> [80, 50, 9, 11, 10, 33, 32, 31] </ref>. An array region is a set of array elements described by equalities and inequalities defining a convex polyhedron. This polyhedron may be parameterized by program variables. Each array dimension is described by a variable.
Reference: [34] <author> Leonardo Dagum, Larry Meadows, and Douglas Miles. </author> <title> Data Parallel Direct Simulation Monte Carlo in High Performance Fortran. </title> <booktitle> Scientific Programming, </booktitle> <address> xx(xx):xx, </address> <month> June </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: The initial definition of the new language, Hpf, was frozen in May 1993, and corrections were added in November 1994 [36]. Prototype compilers incorporating some Hpf features are available [18, 19, 26, 81, 88, 14]. Commercial compilers from APR [64, 65], DEC [71, 17], IBM [42] and PGI <ref> [34, 68] </ref> are also being developed or are already available. These compilers implement part or all of the Hpf Subset, which only allows static distribution of data and prohibits dynamic redistributions.
Reference: [35] <author> Paul Feautrier. </author> <title> Parametric integer programming. </title> <journal> RAIRO Recherche Opera-tionnelle, </journal> <volume> 22 </volume> <pages> 243-268, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: A combined cost function might even be better by taking advantage of the Manhattan distance to minimize the number of hops and of the lexicographical minimum to insure uniqueness. These problems can be cast as linear parametric problems and solved <ref> [35] </ref>. When no replication occurs, elementary data communications implied by Send Y and Receive Y can be parametrically enumerated in basis (p; u), where u is a basis for Y 0 , the local part of Y, the allocation and addressing of which are discussed in Section 4.3. <p> Using (13) the constraints on u can be written K (x 0 (n; p) + P 01 H 0 u) k 0 (n), that is K 0 u k 0 0 (n; p), where K 0 = KP 0a H 0 and k 0 Algorithms presented in [4] or others <ref> [35, 30, 53, 2, 24, 23, 62, 59, 54, 86] </ref> can be used to generate the loop nest enumerating the local iterations. When S is of rank jaj, optimal code is generated because no projections are required. <p> Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques [49, 4, 53, 2, 62, 59, 54, 86], and second, parametric integer programming based methods <ref> [35, 30, 24, 23] </ref>. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest. First, variables are added to explicit the modulo computation, then the Hermite computations are used to regenerate simply new loop bounds.
Reference: [36] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <institution> Rice University, Houston, Texas, </institution> <month> November </month> <year> 1994. </year> <note> Version 1.1. </note>
Reference-contexts: The initial definition of the new language, Hpf, was frozen in May 1993, and corrections were added in November 1994 <ref> [36] </ref>. Prototype compilers incorporating some Hpf features are available [18, 19, 26, 81, 88, 14]. Commercial compilers from APR [64, 65], DEC [71, 17], IBM [42] and PGI [34, 68] are also being developed or are already available. <p> The relations between the global programmer space and the local processor spaces can also be used to translate sequential loops with a run-time resolution mechanism or with some optimizations. The reader is assumed knowledgeable in Hpf directives <ref> [36] </ref> and optimization techniques for Hpf [38, 81]. The paper is organized as follow. Section 2 shows how Hpf directives can be expressed as affine constraints and normalized to simplify the compilation process and its description. <p> Alignments are specified dimension-wise with integer affine expressions as template subscript expressions. Each array index can be used at most once in a template subscript expression in any given alignment, and each subscript expression cannot contain more than one index <ref> [36] </ref>. <p> The extents (n in BLOCK (n) or CYCLIC (n)) are stored in a diagonal matrix, C. P is a square matrix with the size of the processor dimensions on the diagonal. Such a distribution is not linear according to its definition <ref> [36] </ref> but may be written as a linear relation between the processor coordinate p, the template coordinate t and two additional variables, ` and c: t = CP c + Cp + ` (3) Vector ` represents the offset within one block in one processor and vector c represents the number
Reference: [37] <author> Franco Gasperoni and Uwe Scheiegelshohn. </author> <title> Scheduling loop on parallel processors: A simple algorithm with close to optimum performance. </title> <note> Lecture Note, INRIA, </note> <year> 1992. </year>
Reference-contexts: A new storage management scheme is also proposed. Moreover other optimizations techniques may be applied to the generated code such as vectorization [87], loop invariant code motion [1] and software pipelining <ref> [37, 84] </ref>. This technique uses algorithms, directly or indirectly, that may be costly, such as Fourier elimination or the simplex algorithm, which have exponential worst-case behaviors. They are used for array region analysis, in the set manipulations and in the code generation for polyhedron scanning.
Reference: [38] <author> Hans Michael Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> University of Vienna, </institution> <year> 1989. </year>
Reference-contexts: Manufacturers and research laboratories, led by Digital and Rice University, decided in 1991 to shift part of the burden onto compilers by providing the programmer a uniform address space to allocate objects and a (mainly) implicit way to express parallelism. Numerous research projects <ref> [38, 47, 81] </ref> and a few commercial products had shown that this goal could be achieved and the High Performance Fortran Forum was set up to select the most useful functionalities and to standardize the syntax. <p> This compilation scheme directly generates optimized code which includes techniques such as guard elimination <ref> [38] </ref>, message vectorization and aggregation [47, 81]. Submitted to Scientific Programming 3 ! non-independent ! A in the rhs may ! induce RW dependences... <p> INDEPENDENT (j,i) do j=1, m A (i,j) = TMP (i,j) enddo enddo 4 Ancourt et al., A Linear Algebra Framework: : : It is compatible with overlap analysis <ref> [38] </ref>. There are no restrictions neither on the kind of distribution (general cyclic distributions are handled), nor on the rank of array references (the dimension of the referenced space: for instance rank of A (i,i) is 1). <p> The relations between the global programmer space and the local processor spaces can also be used to translate sequential loops with a run-time resolution mechanism or with some optimizations. The reader is assumed knowledgeable in Hpf directives [36] and optimization techniques for Hpf <ref> [38, 81] </ref>. The paper is organized as follow. Section 2 shows how Hpf directives can be expressed as affine constraints and normalized to simplify the compilation process and its description. <p> This optimization is known as overlap analysis <ref> [38] </ref>. Once remote values are copied into the overlapping Y 0 , all elements of View Y (p) can be accessed uniformly in Y 0 with no overhead. <p> Thus the local cache and/or prefetch mechanisms, if any, are efficiently used. The packing scheme is also compatible with overlap analysis techniques <ref> [38] </ref>. Local array declarations are extended to provide space for border elements that are owned by neighbor processors, and to simplify accesses to non-local elements. <p> Techniques and prototypes have been developed based on Fortran <ref> [38, 39, 47, 18, 69, 88, 19, 20] </ref>, C [8, 63, 6, 60, 7, 61] or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. <p> Moreover every processor has to execute the whole control flow of the program, and even for parallel loop, communications may sequentialize the program at run-time [22]. Many optimization techniques have been introduced to handle specific cases. Gerndt introduced overlap analysis in <ref> [38] </ref> for block distributions. <p> Such a scheme could reuse HPF distribution to map HPF processors on physical processors. Many partial optimization techniques are integrated in our direct synthesis approach: message vectorization, and aggregation [47], overlap analysis <ref> [38] </ref>. A new storage management scheme is also proposed. Moreover other optimizations techniques may be applied to the generated code such as vectorization [87], loop invariant code motion [1] and software pipelining [37, 84].
Reference: [39] <author> Hans Michael Gerndt and Hans Peter Zima. </author> <title> Optimizing Communication in Superb. </title> <booktitle> In CONPAR90, </booktitle> <pages> pages 300-311, </pages> <year> 1990. </year>
Reference-contexts: Techniques and prototypes have been developed based on Fortran <ref> [38, 39, 47, 18, 69, 88, 19, 20] </ref>, C [8, 63, 6, 60, 7, 61] or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74].
Reference: [40] <author> Philippe Granger. </author> <title> Analyses Semantiques de Congruence. </title> <type> PhD thesis, </type> <institution> Ecole Poly-technique, </institution> <month> July </month> <year> 1991. </year> <title> 40 Ancourt et al., A Linear Algebra Framework: </title> : : 
Reference-contexts: Linearity of access to temporary elements is preserved. This scheme is correct if and only if a unique location y 0 is associated to each y. Further insight on this problem, the minimal covering of a set by interval congruences, can be found in <ref> [40, 67] </ref>. 4.6 Data movements The relationships between the bases and frames defined in the previous sections are shown in Figure 17. Three areas are distinguished. The top one contains user level bases for iterations, i, and array elements, a X , a Y ,...
Reference: [41] <author> Torbjorn Granlund and Peter L. Montgomery. </author> <title> Division by invariant integers using multiplication. </title> <booktitle> In ACM SIGPLAN International Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 61-72, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Different choices can be made for each dimension. The access function requires an integer division for the reduced memory allocation. Techniques have been suggested to handle divisions by invariant integers efficiently <ref> [41] </ref> that could help reduce this cost. Also, because contiguity is preserved, only one division per column is required to compute the base location.
Reference: [42] <author> Manish Gupta, Sam Midkiff, Edith Schonberg, Ven Seshadri, David Shields, Ko-Yang Wang, Wai-Mee Ching, and Ton Ngo. </author> <title> An HPF Compiler for the IBM SP2. </title> <booktitle> In Workshop on Compilers for Parallel Computers, Malaga, </booktitle> <pages> pages 22-39, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The initial definition of the new language, Hpf, was frozen in May 1993, and corrections were added in November 1994 [36]. Prototype compilers incorporating some Hpf features are available [18, 19, 26, 81, 88, 14]. Commercial compilers from APR [64, 65], DEC [71, 17], IBM <ref> [42] </ref> and PGI [34, 68] are also being developed or are already available. These compilers implement part or all of the Hpf Subset, which only allows static distribution of data and prohibits dynamic redistributions.
Reference: [43] <author> S. K. S. Gupta, S. D. Kaushik, S. Mufti, S. Sharma, C.-H. Huang, and P. Sadayap-pan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages II-301-II-305, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: used to obtain a triangular enumeration, and from (3) the independent parallelism of the loop which allows any enumeration order. 4.2 Symbolic resolution The previous method can be applied in a symbolic way, if the dimensions are not coupled and thus can be dealt with independently, as array sections in <ref> [25, 43, 78] </ref>. <p> Also multi-dimensional cases need many transition maps to be handled. Papers by Stichnoth et al. [78, 77] on the one hand and Gupta et al. <ref> [43, 44, 52] </ref> on the other hand present two similar methods based on closed forms for this problem. Submitted to Scientific Programming 35 They use array sections but compute some of the coefficients at run-time.
Reference: [44] <author> S.K.S. Gupta, S. D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <type> Technical Report 19, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <year> 1994. </year>
Reference-contexts: Also multi-dimensional cases need many transition maps to be handled. Papers by Stichnoth et al. [78, 77] on the one hand and Gupta et al. <ref> [43, 44, 52] </ref> on the other hand present two similar methods based on closed forms for this problem. Submitted to Scientific Programming 35 They use array sections but compute some of the coefficients at run-time.
Reference: [45] <author> Paul Havlak and Ken Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Moreover these systems tend to be composed of independent subsystems on a dimension per dimension basis, resulting in a more efficient practical behavior. Secondly efficient and highly tuned versions of such algorithms are available, for instance in the Omega library. Thirdly, potentially less precise but faster program analysis <ref> [21, 13, 45] </ref> can also be used in place of the region analysis. Polyhedron-based techniques are already implemented in hpfc, our prototype Hpf compiler [27] to deal with I/O communications in a host/nodes model [28] and also to deal with dynamic remappings [29] (realign and redistribute directives).
Reference: [46] <author> Seema Hiranandani, Ken Kennedy, John Mellor-Crummey, and Ajay Sethi. </author> <title> Compilation techniques for block-cyclic distributions. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Note that the code generated in Figure 11 may be used to compute the fsm. In fact the lower iteration of the innermost loop is computed by the algorithm that builds the fsm. Kennedy et al. <ref> [55, 56, 48, 46] </ref> and others [79] have suggested improvements to this technique, essentially to compute faster at run-time the automaton transition map. Also multi-dimensional cases need many transition maps to be handled.
Reference: [47] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD Distributed-Memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Manufacturers and research laboratories, led by Digital and Rice University, decided in 1991 to shift part of the burden onto compilers by providing the programmer a uniform address space to allocate objects and a (mainly) implicit way to express parallelism. Numerous research projects <ref> [38, 47, 81] </ref> and a few commercial products had shown that this goal could be achieved and the High Performance Fortran Forum was set up to select the most useful functionalities and to standardize the syntax. <p> This compilation scheme directly generates optimized code which includes techniques such as guard elimination [38], message vectorization and aggregation <ref> [47, 81] </ref>. Submitted to Scientific Programming 3 ! non-independent ! A in the rhs may ! induce RW dependences... <p> Techniques and prototypes have been developed based on Fortran <ref> [38, 39, 47, 18, 69, 88, 19, 20] </ref>, C [8, 63, 6, 60, 7, 61] or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. <p> Our scheme can also be extended to cope with processor virtualization if the virtu-alization scheme is expressed with affine constraints. Such a scheme could reuse HPF distribution to map HPF processors on physical processors. Many partial optimization techniques are integrated in our direct synthesis approach: message vectorization, and aggregation <ref> [47] </ref>, overlap analysis [38]. A new storage management scheme is also proposed. Moreover other optimizations techniques may be applied to the generated code such as vectorization [87], loop invariant code motion [1] and software pipelining [37, 84].
Reference: [48] <author> Semma Hirannandani, Ken Kennedy, John Mellor-Crummey, and Ajay Sethi. </author> <title> Advanced Compilation Techniques for Fortran D. </title> <type> CRPC-TR 93338, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Note that the code generated in Figure 11 may be used to compute the fsm. In fact the lower iteration of the innermost loop is computed by the algorithm that builds the fsm. Kennedy et al. <ref> [55, 56, 48, 46] </ref> and others [79] have suggested improvements to this technique, essentially to compute faster at run-time the automaton transition map. Also multi-dimensional cases need many transition maps to be handled.
Reference: [49] <author> Fran~cois Irigoin. </author> <title> Code generation for the hyperplane method and for loop interchange. </title> <institution> ENSMP-CAI-88 E102/CAI/I, CRI, Ecole des mines de Paris, </institution> <month> October </month> <year> 1988. </year>
Reference-contexts: In [2, 82] advanced analyses are used as an input to a code generation phase for distributed memory machines. Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques <ref> [49, 4, 53, 2, 62, 59, 54, 86] </ref>, and second, parametric integer programming based methods [35, 30, 24, 23]. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest.
Reference: [50] <author> Fran~cois Irigoin. </author> <title> Interprocedural analyses for programming environment. </title> <editor> In Jack J. Dongara and Bernard Tourancheau, editors, </editor> <booktitle> Environments and Tools for Parallel Scientific Computing, </booktitle> <pages> pages 333-350, </pages> <address> Saint-Hilaire-du-Touvet, September 1992. </address> <publisher> North-Holland, </publisher> <address> Amsterdam, NSF-CNRS. </address>
Reference-contexts: The definition of View is thus altered to take into account array regions. These regions are the result of a precise program analysis which is presented in <ref> [80, 50, 9, 11, 10, 33, 32, 31] </ref>. An array region is a set of array elements described by equalities and inequalities defining a convex polyhedron. This polyhedron may be parameterized by program variables. Each array dimension is described by a variable.
Reference: [51] <author> Fran~cois Irigoin, Pierre Jouvelot, and Remi Triolet. </author> <title> Semantical interprocedural parallelization: An overview of the PIPS project. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: They are used for array region analysis, in the set manipulations and in the code generation for polyhedron scanning. However our experience with such algorithms is that they remain practical for our purpose: Polyhedron-based techniques are widely implemented in the PIPS framework <ref> [51] </ref> where hpfc, our prototype Hpf compiler, is developped. Firstly, for a given loop nest, the number of equalities and inequalities is quite low, typically a dozen or less.
Reference: [52] <author> S. D. Kaushik, C.-H. Huang, and P. Sadayappan. </author> <title> Compiling Array Statements for Efficient Execution on Distributed-Memory Machines: Two-level Mappings. </title> <booktitle> In Language and Compilers for Parallel Computing, </booktitle> <pages> pages 14.1-14.15, </pages> <month> August </month> <year> 1995. </year> <note> Submitted to Scientific Programming 41 </note>
Reference-contexts: Also multi-dimensional cases need many transition maps to be handled. Papers by Stichnoth et al. [78, 77] on the one hand and Gupta et al. <ref> [43, 44, 52] </ref> on the other hand present two similar methods based on closed forms for this problem. Submitted to Scientific Programming 35 They use array sections but compute some of the coefficients at run-time.
Reference: [53] <author> Wayne Kelly and William Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> UMIACS-TR-93 134, </type> <institution> Institute for Advanced Computer Studies, University of Maryland, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Using (13) the constraints on u can be written K (x 0 (n; p) + P 01 H 0 u) k 0 (n), that is K 0 u k 0 0 (n; p), where K 0 = KP 0a H 0 and k 0 Algorithms presented in [4] or others <ref> [35, 30, 53, 2, 24, 23, 62, 59, 54, 86] </ref> can be used to generate the loop nest enumerating the local iterations. When S is of rank jaj, optimal code is generated because no projections are required. <p> In [2, 82] advanced analyses are used as an input to a code generation phase for distributed memory machines. Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques <ref> [49, 4, 53, 2, 62, 59, 54, 86] </ref>, and second, parametric integer programming based methods [35, 30, 24, 23]. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest.
Reference: [54] <author> Wayne Kelly, William Pugh, and Evan Rosser. </author> <title> Code generation for multiple mappings. </title> <booktitle> In 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 332-341, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Using (13) the constraints on u can be written K (x 0 (n; p) + P 01 H 0 u) k 0 (n), that is K 0 u k 0 0 (n; p), where K 0 = KP 0a H 0 and k 0 Algorithms presented in [4] or others <ref> [35, 30, 53, 2, 24, 23, 62, 59, 54, 86] </ref> can be used to generate the loop nest enumerating the local iterations. When S is of rank jaj, optimal code is generated because no projections are required. <p> In [2, 82] advanced analyses are used as an input to a code generation phase for distributed memory machines. Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques <ref> [49, 4, 53, 2, 62, 59, 54, 86] </ref>, and second, parametric integer programming based methods [35, 30, 24, 23]. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest.
Reference: [55] <author> Ken Kennedy, Nenad Nedeljkovic, and Ajay Sethi. </author> <title> A Linear Time Algorithm for Computing the Memory Access Sequence in Data-Parallel Programs. </title> <booktitle> In Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1995. </year> <journal> Sigplan Notices, </journal> <volume> 30(8) </volume> <pages> 102-111. </pages>
Reference-contexts: Note that the code generated in Figure 11 may be used to compute the fsm. In fact the lower iteration of the innermost loop is computed by the algorithm that builds the fsm. Kennedy et al. <ref> [55, 56, 48, 46] </ref> and others [79] have suggested improvements to this technique, essentially to compute faster at run-time the automaton transition map. Also multi-dimensional cases need many transition maps to be handled.
Reference: [56] <author> Ken Kennedy, Nenad Nedeljkovic, and Ajay Sethi. </author> <title> Efficient address generation for block-cyclic distributions. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <pages> pages 180-184, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Note that the code generated in Figure 11 may be used to compute the fsm. In fact the lower iteration of the innermost loop is computed by the algorithm that builds the fsm. Kennedy et al. <ref> [55, 56, 48, 46] </ref> and others [79] have suggested improvements to this technique, essentially to compute faster at run-time the automaton transition map. Also multi-dimensional cases need many transition maps to be handled.
Reference: [57] <author> Charles Koelbel and Piyush Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The parallel loop on U can be exchanged with the parallel inner loop on p 0 included in the forall. This other ordering makes message aggregation possible. This code presented in Figure 10 is quite different from the one suggested in <ref> [58, 57] </ref>. <p> Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C [8, 63, 6, 60, 7, 61] or others languages <ref> [74, 75, 58, 66, 57] </ref>. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. Each instruction is guarded by a condition which is only true for processors that must execute it.
Reference: [58] <author> Charles Koelbel, Piyush Mehrotra, and John Van Rosendale. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <type> Technical Report ASD 915, </type> <institution> Purdue University, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: The parallel loop on U can be exchanged with the parallel inner loop on p 0 included in the forall. This other ordering makes message aggregation possible. This code presented in Figure 10 is quite different from the one suggested in <ref> [58, 57] </ref>. <p> Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C [8, 63, 6, 60, 7, 61] or others languages <ref> [74, 75, 58, 66, 57] </ref>. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. Each instruction is guarded by a condition which is only true for processors that must execute it.
Reference: [59] <author> Marc Le Fur. </author> <title> Scanning Parametrized Polyhedron using Fourier-Motzkin Elimination. </title> <note> Publication interne 858, IRISA, </note> <month> September </month> <year> 1994. </year>
Reference-contexts: Using (13) the constraints on u can be written K (x 0 (n; p) + P 01 H 0 u) k 0 (n), that is K 0 u k 0 0 (n; p), where K 0 = KP 0a H 0 and k 0 Algorithms presented in [4] or others <ref> [35, 30, 53, 2, 24, 23, 62, 59, 54, 86] </ref> can be used to generate the loop nest enumerating the local iterations. When S is of rank jaj, optimal code is generated because no projections are required. <p> In [2, 82] advanced analyses are used as an input to a code generation phase for distributed memory machines. Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques <ref> [49, 4, 53, 2, 62, 59, 54, 86] </ref>, and second, parametric integer programming based methods [35, 30, 24, 23]. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest.
Reference: [60] <author> Marc Le Fur, Jean-Louis Pazat, and Fran~coise Andre. </author> <title> Commutative loop nests distribution. </title> <booktitle> In Workshop on Compilers for Parallel Computers, Delft, </booktitle> <pages> pages 345-350, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C <ref> [8, 63, 6, 60, 7, 61] </ref> or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. Each instruction is guarded by a condition which is only true for processors that must execute it.
Reference: [61] <author> Marc Le Fur, Jean-Louis Pazat, and Fran~coise Andre. </author> <title> An Array Partitioning Analysis for Parallel Loop Distribution. </title> <booktitle> In Euro-Par'95, </booktitle> <address> Stockholm, Sweden, </address> <pages> pages 351-364, </pages> <address> August 1995. </address> <publisher> Springer Verlag, LNCS 966. </publisher>
Reference-contexts: Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C <ref> [8, 63, 6, 60, 7, 61] </ref> or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. Each instruction is guarded by a condition which is only true for processors that must execute it.
Reference: [62] <author> Herve Le Verge, Vincent Van Dongen, and Doran K. Wilde. </author> <title> Loop nest synthesis using the polyhedral library. </title> <note> Publication Interne 830, IRISA, </note> <month> May </month> <year> 1994. </year>
Reference-contexts: Using (13) the constraints on u can be written K (x 0 (n; p) + P 01 H 0 u) k 0 (n), that is K 0 u k 0 0 (n; p), where K 0 = KP 0a H 0 and k 0 Algorithms presented in [4] or others <ref> [35, 30, 53, 2, 24, 23, 62, 59, 54, 86] </ref> can be used to generate the loop nest enumerating the local iterations. When S is of rank jaj, optimal code is generated because no projections are required. <p> In [2, 82] advanced analyses are used as an input to a code generation phase for distributed memory machines. Polyhedron scanning techniques are used for generating the code. Two family of techniques have been suggested for that purpose. First, Fourier elimination based techniques <ref> [49, 4, 53, 2, 62, 59, 54, 86] </ref>, and second, parametric integer programming based methods [35, 30, 24, 23]. In [12], a two-fold Hermite transformation is also used to remove modulo indexing from a loop nest.
Reference: [63] <author> Oded Lempel, Shlomit S. Pinter, and Eli Turiel. </author> <title> Parallelizing a C dialect for Distributed Memory MIMD machines. </title> <booktitle> In Language and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C <ref> [8, 63, 6, 60, 7, 61] </ref> or others languages [74, 75, 58, 66, 57]. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. Each instruction is guarded by a condition which is only true for processors that must execute it.
Reference: [64] <author> John M. Levesque. </author> <title> FORGE 90 and High Performance Fortran. Applied Parallel Research, </title> <publisher> Inc., </publisher> <year> 1992. </year> <note> xHPF77 presentation. </note>
Reference-contexts: The initial definition of the new language, Hpf, was frozen in May 1993, and corrections were added in November 1994 [36]. Prototype compilers incorporating some Hpf features are available [18, 19, 26, 81, 88, 14]. Commercial compilers from APR <ref> [64, 65] </ref>, DEC [71, 17], IBM [42] and PGI [34, 68] are also being developed or are already available. These compilers implement part or all of the Hpf Subset, which only allows static distribution of data and prohibits dynamic redistributions.
Reference: [65] <author> John M. Levesque. </author> <title> Applied Parallel Research's xHPF system. </title> <booktitle> IEEE Parallel and Distributed Technologies, </booktitle> <pages> page 71, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: The initial definition of the new language, Hpf, was frozen in May 1993, and corrections were added in November 1994 [36]. Prototype compilers incorporating some Hpf features are available [18, 19, 26, 81, 88, 14]. Commercial compilers from APR <ref> [64, 65] </ref>, DEC [71, 17], IBM [42] and PGI [34, 68] are also being developed or are already available. These compilers implement part or all of the Hpf Subset, which only allows static distribution of data and prohibits dynamic redistributions.
Reference: [66] <author> J. Li and Marina Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Techniques and prototypes have been developed based on Fortran [38, 39, 47, 18, 69, 88, 19, 20], C [8, 63, 6, 60, 7, 61] or others languages <ref> [74, 75, 58, 66, 57] </ref>. The most obvious, most general and safest technique is called run-time resolution [22, 70, 74]. Each instruction is guarded by a condition which is only true for processors that must execute it.
References-found: 66

