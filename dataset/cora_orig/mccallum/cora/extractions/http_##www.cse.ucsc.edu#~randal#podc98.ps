URL: http://www.cse.ucsc.edu/~randal/podc98.ps
Refering-URL: http://www.cse.ucsc.edu/~randal/pubs.html
Root-URL: http://www.cse.ucsc.edu
Email: randal@almaden.ibm.com darrell@cs.ucsc.edu  
Title: In-Place Reconstruction of Delta Compressed Files  
Author: Randal C. Burns Darrell D. E. Long 
Address: 650 Harry Rd., San Jose, CA 95120 University of California, Santa Cruz, CA 95064  
Affiliation: IBM Almaden Research Center Department of Computer Science  
Abstract: We present an algorithm for modifying delta compressed files so that the compressed versions may be reconstructed without scratch space. This allows network clients with limited resources to efficiently update software by retrieving delta compressed versions over a network. Delta compression for binary files, compactly encoding a version of data with only the changed bytes from a previous version, may be used to efficiently distribute software over low bandwidth channels, such as the Internet. Traditional methods for rebuilding these delta files require memory or storage space on the target machine for both the old and new version of the file to be reconstructed. With the advent of network computing and Internet-enabled devices, many of these network attached target machines have limited additional scratch space. We present an algorithm for modifying a delta compressed version file so that it may rebuild the new file version in the space that the current version occupies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ajtai, R. Burns, R. Fagin, D. Long, and L. Stockmeyer. </author> <title> Compactly encoding arbitrary inputs with differential compression. </title> <journal> IBM Research: </journal> <note> In Preparation, </note> <year> 1998. </year>
Reference-contexts: In particular, heavy Internet traffic y The work of this author was performed while a Visiting Scientist at the IBM Almaden Research Center. results in high latency and low bandwidth to web-enabled clients and prevents the timely delivery of software. Differential or delta compression <ref> [5, 1] </ref>, compactly encoding a new version of a file using only the changed bytes from a previous version, can be used to reduce the size of the file to be transmitted and consequently the time to perform software update. <p> Recent advances in differencing algorithms have produced efficient algorithms that detect matching strings between versions at an arbitrarily fine granularity without alignment restrictions <ref> [1, 5] </ref>. These differencing al gorithms trade an experimentally verified small amount of compression in order to run using time linear in the length of the input files. <p> Previous studies have indicated that delta compression algorithms can compact data for the transmission of software <ref> [1] </ref>. Delta compression algorithms compatible with in-place reconstruction compress a large body of distributed software by a factor of 4 to 10 and reduce the amount of time required to transmit these files over low bandwidth channels accordingly. <p> Except for the codewords the algorithms use, they are identical, finding the same matching strings in the input files. We adopt the format of the add and copy codewords directly from a standard differential compression algorithms <ref> [11, 1] </ref>. While this decision eases implementation, the code-words are poorly suited to in-place reconstruction. The encoding scheme uses only a single byte to encode the length of add commands and therefore generates many short add commands. <p> Since this improvement is significantly less than the encoding overhead for write offsets (1.9%), we feel that the locally minimum policy performs extremely well in practice. The delta compression algorithm used to create delta files <ref> [1] </ref> before permutation operates using time O (L V + L R ) and space in O (1) for reference file R of size L R and version file V of size L V .
Reference: [2] <author> G. Banga, F. Douglis, and M. Rabinovich. </author> <title> Optimistic deltas for WWW latency reduction. </title> <booktitle> In Proceedings of the 1998 Usenix Technical Conference, </booktitle> <year> 1998. </year>
Reference-contexts: The improved algorithms allow large files without known structure to be efficiently differenced and permits the application of delta compression to backup and restore [4], file system replication, and software distribution. Recently, applications distributing HTTP objects using delta files have emerged <ref> [10, 2] </ref>. This permits web servers to both reduce the amount of data to be transmitted to a client and reduce the latency associated with loading web pages.
Reference: [3] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concur-rency Control and Recovery in Database Systems. </title> <publisher> Addison Wesley Publishing Co., </publisher> <year> 1987. </year>
Reference-contexts: Assume that we attempt to read from an offset that has already been written. This will result in an incorrect reconstruction since the reference file data we are attempting to read have been overwritten. This is termed a write before read (WR) conflict <ref> [3] </ref>.
Reference: [4] <author> R. C. Burns and D. D. E. </author> <title> Long. Efficient distributed backup with delta compression. </title> <booktitle> In Proceedings of the 1997 I/O in Parallel and Distributed Systems (IOPADS'97), 17 November 1997, </booktitle> <address> San Jose, CA, USA, </address> <month> November </month> <year> 1997. </year>
Reference-contexts: The improved algorithms allow large files without known structure to be efficiently differenced and permits the application of delta compression to backup and restore <ref> [4] </ref>, file system replication, and software distribution. Recently, applications distributing HTTP objects using delta files have emerged [10, 2]. This permits web servers to both reduce the amount of data to be transmitted to a client and reduce the latency associated with loading web pages.
Reference: [5] <author> R. C. Burns and D. D. E. </author> <title> Long. A linear time, constant space differencing algorithm. </title> <booktitle> In Proceedings of the 1997 International Performance, Computing and Communications Conference (IPCCC'97), </booktitle> <month> Feb. </month> <pages> 5-7, </pages> <address> Tempe/Phoenix, Arizona, USA, </address> <month> February </month> <year> 1997. </year>
Reference-contexts: In particular, heavy Internet traffic y The work of this author was performed while a Visiting Scientist at the IBM Almaden Research Center. results in high latency and low bandwidth to web-enabled clients and prevents the timely delivery of software. Differential or delta compression <ref> [5, 1] </ref>, compactly encoding a new version of a file using only the changed bytes from a previous version, can be used to reduce the size of the file to be transmitted and consequently the time to perform software update. <p> Recent advances in differencing algorithms have produced efficient algorithms that detect matching strings between versions at an arbitrarily fine granularity without alignment restrictions <ref> [1, 5] </ref>. These differencing al gorithms trade an experimentally verified small amount of compression in order to run using time linear in the length of the input files.
Reference: [6] <author> S. P. De Jong. </author> <title> Combining of changes to a source file. </title> <journal> IBM Technical Disclosure Bulletin, </journal> <volume> 15(4):11861188, </volume> <month> September </month> <year> 1972. </year>
Reference-contexts: The first applications of delta compression found changed lines in text data for analyzing the recent modifications to files <ref> [6] </ref>. Considering data as lines of text fails to encode minimum sized delta files, as it does not examine data at a minimum granularity and only finds matching data that are aligned at the beginning of a new line.
Reference: [7] <author> R. M. Karp. </author> <title> Reducibility among combinatorial problems. </title> <editor> In R. E. Miller and J. W. Thatcher, editors, </editor> <booktitle> Complexity of Computer Computations, </booktitle> <pages> pages 85104. </pages> <publisher> Plenum Press, </publisher> <year> 1972. </year>
Reference-contexts: This problem is NP-hard. The related decision problem can be shown NP-complete by reduction from Karp's well known problem <ref> [7] </ref> of determining if there exists a set of vertices and their adjacent edges to remove in a general digraph that make that digraph acyclic and the set has fewer members than some fixed target.
Reference: [8] <author> D. E. Knuth. </author> <title> Fundamental Algorithms, </title> <booktitle> volume 1 of The Art of Computer Programming. </booktitle> <publisher> AddisonWesley Publishing Co., </publisher> <year> 1968. </year>
Reference-contexts: As a total topological ordering is only possible on acyclic digraphs and the digraphs we construct on delta files may contain cycles, we enhance a standard topological sort to break cycles and output a total topological order on a subgraph. Common implementations of topological sort can detect cycles <ref> [8] </ref>. Upon detecting a cycle, our modified topological sort breaks the cycle by removing a vertex. When completing this enhanced sort, the result consists of a digraph containing a subset of all vertices in topological order and a set of vertices that were removed. <p> The total execution time is O (jCj log jCj + jEj + jAj). This result relies upon a topological sort algorithm that runs in time O (jV j + jEj) <ref> [8] </ref>. Additionally, we have assumed that cycles found in the digraph will be broken with the constant time policy (O (1)). The algorithm uses space O (jEj + jCj + jAj) to store the data associated with each set. The algorithm accepts as input a delta file of length n.
Reference: [9] <author> W. Miller and E. W. Myers. </author> <title> A file comparison program. </title> <journal> Software Practice and Experience, </journal> <volume> 15(11):10251040, </volume> <month> Novem-ber </month> <year> 1985. </year>
Reference-contexts: Efforts to generalize delta compression to data that are not aligned and to minimize the granularity of the smallest change resulted in algorithms for compressing data at the granularity of a byte. Early algorithms were based upon either dynamic programming <ref> [9] </ref> or the greedy method [11] and performed this task using time quadratic in the length of the input files. Recent advances in differencing algorithms have produced efficient algorithms that detect matching strings between versions at an arbitrarily fine granularity without alignment restrictions [1, 5].
Reference: [10] <author> J. C. Mogul, F. Douglis, A. Feldman, and B. Krishnamurthy. </author> <title> Potential benefits of delta encoding and data compression for HTTP. In Proceedings of ACM SIGCOMM '97, </title> <month> September </month> <year> 1997. </year>
Reference-contexts: The improved algorithms allow large files without known structure to be efficiently differenced and permits the application of delta compression to backup and restore [4], file system replication, and software distribution. Recently, applications distributing HTTP objects using delta files have emerged <ref> [10, 2] </ref>. This permits web servers to both reduce the amount of data to be transmitted to a client and reduce the latency associated with loading web pages.
Reference: [11] <author> C. Reichenberger. </author> <title> Delta storage for arbitrary non-text files. </title> <booktitle> In Proceedings of the 3rd International Workshop on Software Configuration Management, </booktitle> <address> Trondheim, Norway, </address> <month> 12-14 June </month> <year> 1991, </year> <pages> pages 144152. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: Efforts to generalize delta compression to data that are not aligned and to minimize the granularity of the smallest change resulted in algorithms for compressing data at the granularity of a byte. Early algorithms were based upon either dynamic programming [9] or the greedy method <ref> [11] </ref> and performed this task using time quadratic in the length of the input files. Recent advances in differencing algorithms have produced efficient algorithms that detect matching strings between versions at an arbitrarily fine granularity without alignment restrictions [1, 5]. <p> Except for the codewords the algorithms use, they are identical, finding the same matching strings in the input files. We adopt the format of the add and copy codewords directly from a standard differential compression algorithms <ref> [11, 1] </ref>. While this decision eases implementation, the code-words are poorly suited to in-place reconstruction. The encoding scheme uses only a single byte to encode the length of add commands and therefore generates many short add commands.
Reference: [12] <author> M. J. Rochkind. </author> <title> The source code control system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(4):364370, </volume> <month> Decem-ber </month> <year> 1975. </year>
Reference-contexts: Even though the general problem of detecting and encoding version to version modifications was well defined, delta compression applications continued to rely on the alignment of data, as in database records [13], and the grouping of data into block or line granularity, as in source code control systems <ref> [12, 15] </ref>, to simplify the combinatorial task of finding the common and different strings between files. Efforts to generalize delta compression to data that are not aligned and to minimize the granularity of the smallest change resulted in algorithms for compressing data at the granularity of a byte.
Reference: [13] <author> D. G. Severance and G. M. Lohman. </author> <title> Differential files: Their application to the maintenance of large databases. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 1(2):256267, </volume> <month> September </month> <year> 1976. </year>
Reference-contexts: Even though the general problem of detecting and encoding version to version modifications was well defined, delta compression applications continued to rely on the alignment of data, as in database records <ref> [13] </ref>, and the grouping of data into block or line granularity, as in source code control systems [12, 15], to simplify the combinatorial task of finding the common and different strings between files.
Reference: [14] <author> W. F. Tichy. </author> <title> The string-to-string correction problem with block move. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4), </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: The problem of compactly representing the changes between version of data was formalized as string-to-string correction with block move <ref> [14] </ref> detecting maximally matching regions of a file at an arbitrarily fine granularity without alignment.
Reference: [15] <author> W. F. Tichy. </author> <title> RCS A system for version control. </title> <journal> Software Practice and Experience, </journal> <volume> 15(7):637654, </volume> <month> July </month> <year> 1985. </year>
Reference-contexts: Even though the general problem of detecting and encoding version to version modifications was well defined, delta compression applications continued to rely on the alignment of data, as in database records [13], and the grouping of data into block or line granularity, as in source code control systems <ref> [12, 15] </ref>, to simplify the combinatorial task of finding the common and different strings between files. Efforts to generalize delta compression to data that are not aligned and to minimize the granularity of the smallest change resulted in algorithms for compressing data at the granularity of a byte.
References-found: 15

