URL: ftp://gaia.cs.umass.edu/pub/Nahu97:Cache.ps.Z
Refering-URL: http://www.cs.umass.edu/~nahum/home.html
Root-URL: 
Email: fnahum,yates,kurose,towsleyg@cs.umass.edu  
Title: Cache Behavior of Network Protocols  
Author: Erich Nahum, David Yates, Jim Kurose, and Don Towsley 
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: In this paper we present a performance study of memory reference behavior in network protocol processing, using an Internet-based protocol stack implemented in the x-kernel running in user space on a MIPS R4400-based Silicon Graphics machine. We use the protocols to drive a validated execution-driven architectural simulator of our machine. We characterize the behavior of network protocol processing, deriving statistics such as cache miss rates and percentage of time spent waiting for memory. We also determine how sensitive protocol processing is to the architectural environment, varying factors such as cache size and associativity, and predict performance on future machines. We show that network protocol cache behavior varies widely, with miss rates ranging from 0 to 28 percent, depending on the scenario. We find instruction cache behavior has the greatest effect on protocol latency under most cases, and that cold cache behavior is very different from warm cache behavior. We demonstrate the upper bounds on performance that can be expected by improving memory behavior, and the impact of features such as associativity and larger cache sizes. In particular, we find that TCP is more sensitive to cache behavior than UDP, gaining larger benefits from improved associativity and bigger caches. We predict that network protocols will scale well with CPU speeds in the future. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jean-Loup Baer and Wen-Hann Wang. </author> <title> On the inclusion property for multi-level cache hierarchies. </title> <booktitle> In Proceedings 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 73-80, </pages> <address> Honolulu Hawaii, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Our SGI machine also has a 1 MB second-level direct-mapped onboard unified cache with a line size of 128 bytes. The simulator captures the cost of the important performance characteristics of the SGI platform. It supports multiple levels of cache hierarchy, including the inclusion property for multi-level caches <ref> [1] </ref>, and models the aspects of the MIPS R4400 processor that have a statistically significant impact on performance, such as branch delays and load delay pipeline interlocks. It does not, however, capture translation lookaside bufffer (TLB) behavior 1 .
Reference: [2] <author> David Banks and Michael Prudence. </author> <title> A high-performance network architecture for a PA-RISC workstation. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 191-202, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: It is interesting to note that despite the initial cold state of the caches, miss rates are still under 25 percent. 3.3 Instructions vs. Data Much of the literature on network protocol performance has focused on reducing the number of copies, since touching the data is expensive <ref> [2, 11, 13, 15] </ref>. However, this work has not made explicit how much of this cost is due to data references as opposed to instruction references. <p> Our work, in contrast, separates the benefits of branch prediction from instruction reordering, and shows that the latter has at least as much of an effect as the former. Much research has been done supporting high-speed network interfaces, both in the kernel and in user space <ref> [2, 6, 11, 13, 14, 15, 26] </ref>. A common theme throughout this body of work is the desire to reduce the number of data copies as much as possible, as naive network protocol implementations can copy packet data as much as five times.
Reference: [3] <author> Robert C. Bedichek. Talisman: </author> <title> Fast and accurate multicomputer simulation. </title> <booktitle> In Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 14-24, </pages> <address> Ottawa, Canada, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Note the average error is under 5 percent, with the worst case error being about 15 percent. We are 4 aware of only a very few pieces of work that use trace-driven or execution-driven simulation that actually validate their simulators <ref> [3, 8, 12] </ref>. Our accuracy is comparable to theirs.
Reference: [4] <author> Mats Bjorkman and Per Gunningberg. </author> <title> Locking effects in multiprocessor implementations of protocols. </title> <booktitle> In ACM SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 74-83, </pages> <address> San Francisco, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This approach is similar to those taken in <ref> [4, 16, 34] </ref>. In addition to emulating the actual hardware drivers, the in-memory drivers also simulate the behavior of a peer entity that would be at the remote end of a connection.
Reference: [5] <author> Trevor Blackwell. </author> <title> Speeding up protocols for small messages. </title> <booktitle> In ACM SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <address> Stanford, CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: In this paper, we have advocated techniques that improve instruction cache behavior. Mosberger et al. [27] and Blackwell <ref> [5] </ref> provide two examples of how this can be done. Mosberger et al. examine several compiler-related approaches to improving protocol latency. Using a combination of their techniques (outlining, cloning, and path-inlining), they show up to a 40 percent reduction in protocol processing times. Blackwell [5] also identifies instruction cache behavior as <p> Mosberger et al. [27] and Blackwell <ref> [5] </ref> provide two examples of how this can be done. Mosberger et al. examine several compiler-related approaches to improving protocol latency. Using a combination of their techniques (outlining, cloning, and path-inlining), they show up to a 40 percent reduction in protocol processing times. Blackwell [5] also identifies instruction cache behavior as an important performance factor using traces of NetBSD. <p> In this section we outline their results and, as appropriate, relate their findings to ours. Blackwell <ref> [5] </ref> also identifies instruction cache behavior as an important performance factor using traces of NetBSD on an Alpha.
Reference: [6] <author> Matthias A. Blumrich, Cezary Dubnicki, Edward W. Felton, Kai Li, and Malena R. Mesarina. </author> <title> Virtual-memory mapped interfaces. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 21-28, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Our work, in contrast, separates the benefits of branch prediction from instruction reordering, and shows that the latter has at least as much of an effect as the former. Much research has been done supporting high-speed network interfaces, both in the kernel and in user space <ref> [2, 6, 11, 13, 14, 15, 26] </ref>. A common theme throughout this body of work is the desire to reduce the number of data copies as much as possible, as naive network protocol implementations can copy packet data as much as five times.
Reference: [7] <author> D. Borman, R. Braden, and V. Jacobson. </author> <title> TCP extensions for high performance. Request for Comments (Proposed Standard) RFC 1323, </title> <institution> Internet Engineering Task Force, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: In addition, the code has some BSD 4.4 fixes, but none of the RFC1323 extensions <ref> [7] </ref>. Checksumming has been identified as a potential performance issue in TCP/UDP implementations. Certain network interfaces, such as SGI's FDDI boards, have hardware support for calculating checksums that effectively eliminate the checksum performance overhead. However, not all devices have this hardware support.
Reference: [8] <author> Brad Calder, Dirk Grunwald, and Joel Emer. </author> <title> A system level perspective on branch architecture performance. </title> <booktitle> In Proceedings of the 28th Annual IEEE/ACM International Symposium on Microarchitecture, </booktitle> <pages> pages 199-206, </pages> <address> Ann Arbor, MI, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: Note the average error is under 5 percent, with the worst case error being about 15 percent. We are 4 aware of only a very few pieces of work that use trace-driven or execution-driven simulation that actually validate their simulators <ref> [3, 8, 12] </ref>. Our accuracy is comparable to theirs.
Reference: [9] <author> Hsiao-Keng Jerry Chu. </author> <title> Zero copy TCP in Solaris. </title> <booktitle> In Proceedings of the Winter USENIX Technical Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: A common theme throughout this body of work is the desire to reduce the number of data copies as much as possible, as naive network protocol implementations can copy packet data as much as five times. As a consequence, single-copy and even zero-copy protocol stacks have been demonstrated <ref> [9, 28] </ref>. These pieces of work focus on `reducing work' done during protocol processing, 10 namely reducing the number of instructions executed. Our protocol stacks emulate zero-copy stacks.
Reference: [10] <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6) </volume> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year> <month> 11 </month>
Reference-contexts: He proposes a technique for improving processing times for small messages, by processing batches of packets at each layer so as to maximize instruction cache behavior, and evaluates this technique via a simulation model of protocol processing. Clark et al. <ref> [10] </ref> provide an analysis of TCP processing overheads on an Intel i386 architecture circa 1988. Their analysis focuses on protocol-related processing, and does not address OS issues such as buffering and copying data.
Reference: [11] <author> Chris Dalton, Greg Watson, David Banks, Costas Clamvokis, Aled Edwards, and John Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <volume> 11(2) </volume> <pages> 36-43, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: It is interesting to note that despite the initial cold state of the caches, miss rates are still under 25 percent. 3.3 Instructions vs. Data Much of the literature on network protocol performance has focused on reducing the number of copies, since touching the data is expensive <ref> [2, 11, 13, 15] </ref>. However, this work has not made explicit how much of this cost is due to data references as opposed to instruction references. <p> Our work, in contrast, separates the benefits of branch prediction from instruction reordering, and shows that the latter has at least as much of an effect as the former. Much research has been done supporting high-speed network interfaces, both in the kernel and in user space <ref> [2, 6, 11, 13, 14, 15, 26] </ref>. A common theme throughout this body of work is the desire to reduce the number of data copies as much as possible, as naive network protocol implementations can copy packet data as much as five times.
Reference: [12] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Memory-system performance of programs with intensive heap allocation. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 244-273, </pages> <year> 1995. </year>
Reference-contexts: Note the average error is under 5 percent, with the worst case error being about 15 percent. We are 4 aware of only a very few pieces of work that use trace-driven or execution-driven simulation that actually validate their simulators <ref> [3, 8, 12] </ref>. Our accuracy is comparable to theirs.
Reference: [13] <author> Peter Druschel, Larry Peterson, and Bruce Davie. </author> <title> Experiences with a high-speed network adaptor: A software perspective. </title> <booktitle> In ACM SIG-COMM Symposium on Communications Architectures and Protocols, </booktitle> <address> London, England, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: It is interesting to note that despite the initial cold state of the caches, miss rates are still under 25 percent. 3.3 Instructions vs. Data Much of the literature on network protocol performance has focused on reducing the number of copies, since touching the data is expensive <ref> [2, 11, 13, 15] </ref>. However, this work has not made explicit how much of this cost is due to data references as opposed to instruction references. <p> Our work, in contrast, separates the benefits of branch prediction from instruction reordering, and shows that the latter has at least as much of an effect as the former. Much research has been done supporting high-speed network interfaces, both in the kernel and in user space <ref> [2, 6, 11, 13, 14, 15, 26] </ref>. A common theme throughout this body of work is the desire to reduce the number of data copies as much as possible, as naive network protocol implementations can copy packet data as much as five times.
Reference: [14] <author> Peter Druschel and Larry L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 189-202, </pages> <address> Asheville, NC, </address> <month> Dec </month> <year> 1993. </year>
Reference-contexts: Our work, in contrast, separates the benefits of branch prediction from instruction reordering, and shows that the latter has at least as much of an effect as the former. Much research has been done supporting high-speed network interfaces, both in the kernel and in user space <ref> [2, 6, 11, 13, 14, 15, 26] </ref>. A common theme throughout this body of work is the desire to reduce the number of data copies as much as possible, as naive network protocol implementations can copy packet data as much as five times.
Reference: [15] <author> Aled Edwards and Steve Muir. </author> <title> Experiences implementing a high-performance TCP in user space. </title> <booktitle> In ACM SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 196-205, </pages> <address> Cam-bridge, MA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: It is interesting to note that despite the initial cold state of the caches, miss rates are still under 25 percent. 3.3 Instructions vs. Data Much of the literature on network protocol performance has focused on reducing the number of copies, since touching the data is expensive <ref> [2, 11, 13, 15] </ref>. However, this work has not made explicit how much of this cost is due to data references as opposed to instruction references. <p> Our work, in contrast, separates the benefits of branch prediction from instruction reordering, and shows that the latter has at least as much of an effect as the former. Much research has been done supporting high-speed network interfaces, both in the kernel and in user space <ref> [2, 6, 11, 13, 14, 15, 26] </ref>. A common theme throughout this body of work is the desire to reduce the number of data copies as much as possible, as naive network protocol implementations can copy packet data as much as five times.
Reference: [16] <author> Murray W. Goldberg, Gerald W. Neufeld, and Mabo R. Ito. </author> <title> A parallel approach to OSI connection-oriented protocols. </title> <booktitle> Third IFIP WG6.1/WG6.4 International Workshop on Protocols for High-Speed Networks, </booktitle> <pages> pages 219-232, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This approach is similar to those taken in <ref> [4, 16, 34] </ref>. In addition to emulating the actual hardware drivers, the in-memory drivers also simulate the behavior of a peer entity that would be at the remote end of a connection.
Reference: [17] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach (2nd Edition). </title> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction Cache behavior is a central issue in contemporary computer system performance. The large gap between CPU and memory speeds is well-known, and is expected to continue for the forseeable future <ref> [17] </ref>. Cache memories are used to bridge this gap, and multiple levels of cache memories are typical in contemporary systems. Many fl This research supported in part by NSF under grant NCR-9206908, and by ARPA under contract F19628-92-C-0089.
Reference: [18] <author> Mark D. Hill. </author> <title> A case for direct mapped caches. </title> <journal> IEEE Computer, </journal> <volume> 21(12) </volume> <pages> 24-40, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: In contrast, TLBs and virtual memory systems are usually fully associative. Cache memories have historically been direct mapped because adding associativity has tended to increase the critical path length and thus increase cycle time <ref> [18] </ref>. While many RISC machines today have direct-mapped on-chip caches, emerging machines, such as the MIPS R10000, are starting to have 2 way set-associative on-chip caches. It is thus useful to assess the impact of improved associativity on network protocol performance.
Reference: [19] <author> Mark D. Hill and Alan J. Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1612-1630, </pages> <month> De-cember </month> <year> 1989. </year>
Reference-contexts: For example, it allows us to estimate how much of memory time is due to conflicts in the cache rather than capacity problems <ref> [19] </ref>. In these experiments, for simplicity, all caches in the system have the same associativity, e.g., an experiment marked with 2 indicates that the instruction cache, the data cache, and the level 2 unified cache all have 2-way set associativity.
Reference: [20] <author> Norman C. Hutchinson and Larry L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: We have constructed a simulator for our MIPS R4400-based Silicon Graphics machines, and taken great effort to validate our simulator, i.e., to ensure that it models the performance costs of our platform accurately. We use the simulator to analyze a suite of Internet-based protocol stacks implemented in the x-kernel <ref> [20] </ref>, which we ported to user space on our SGI machine. We characterize the behavior of network protocol processing, deriving statistics such as cache miss rates, instruction use, and percentage of time spent waiting for memory. <p> We now present the protocols and test environment that we use. Validation results are given in Section 2.3. 2.2 Network Protocol Workload The network protocol stacks we consider in this paper are implemented in the x-kernel <ref> [20] </ref>, an environment for quickly developing efficient network protocol software. Unfortunately, we did not have access to the source code of the IRIX operating system that runs on our Silicon Graphics machines. Our stack is thus a user-space implementation of the x-kernel that we ported to the SGI platform.
Reference: [21] <author> Van Jacobson. </author> <title> Efficient protocol implementation. </title> <booktitle> In ACM SIGCOMM 1990 Tutorial Notes, </booktitle> <address> Philadelphia, PA, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: In addition to adding header prediction, this involved updating the congestion control and timer mechanisms, as well as reordering code in the send side to test for the most frequent scenarios first <ref> [21] </ref> 2 . In addition, the code has some BSD 4.4 fixes, but none of the RFC1323 extensions [7]. Checksumming has been identified as a potential performance issue in TCP/UDP implementations.
Reference: [22] <author> Van Jacobson. </author> <title> A high performance TCP/IP implementation. In NRI Gigabit TCP Workshop, </title> <address> Reston, VA, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: We have also focused on protocol-related issues, but on a contemporary RISC architecture, and have quantified the instruction usage. We have examined both instruction and data references, measured cache miss rates for both, and have explored the range of cache behavior. Jacobson <ref> [22] </ref> presents a high-performance TCP implementation that tries to minimize data memory references. He shows that by combining the packet checksum with the data copy, the checksum incurs little additional overhead since it is hidden in the memory latency of the copy.
Reference: [23] <author> Jonathan Kay and Joseph Pasquale. </author> <title> Measurement, analysis, and improvement of UDP/IP throughput for the DECStation 5000. </title> <booktitle> In USENIX Winter 1993 Technical Conference, </booktitle> <pages> pages 249-258, </pages> <address> San Diego, CA, </address> <year> 1993. </year>
Reference-contexts: To capture both scenarios, we run experiments with checksumming on and off, to emulate checksums being calculated in software and hardware, respectively. For our software checksum experiments, the checksum code we use is the fastest available portable algorithm that we are aware of, which is from UCSD <ref> [23] </ref>. Since our platform runs in user space, accessing the FDDI adaptor involves crossing the IRIX socket layer and the user/kernel boundary, which is prohibitively expensive.
Reference: [24] <author> S. J. Leffler, M.K. McKusick, M.J. Karels, and J.S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: FDDI is the Fiber Distributed Data Interface, a 100 Mbit fiber-optic token-ring based LAN protocol. Our TCP implementation is based upon the x-kernel's adaptation of the Berkeley Tahoe release, which we also updated to be compliant with the BSD Net/2 <ref> [24] </ref> software. In addition to adding header prediction, this involved updating the congestion control and timer mechanisms, as well as reordering code in the send side to test for the most frequent scenarios first [21] 2 .
Reference: [25] <author> Larry McVoy and Carl Staelin. LMBENCH: </author> <title> Portable tools for performance analysis. </title> <booktitle> In USENIX Technical Conference of UNIX and Advanced Computing Systems, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: We used the memory striding benchmarks from LMBench <ref> [25] </ref> to measure the cache hit and miss latencies for all three levels of the memory hierarchy: L1, L2, and main memory. Table 1 lists the cycle times to read and write the caches on the 100MHz SGI Challenge.
Reference: [26] <author> Ron Minnich, Dan Burns, and Frank Hady. </author> <title> The memory-integrated network interface. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 11-20, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Our work, in contrast, separates the benefits of branch prediction from instruction reordering, and shows that the latter has at least as much of an effect as the former. Much research has been done supporting high-speed network interfaces, both in the kernel and in user space <ref> [2, 6, 11, 13, 14, 15, 26] </ref>. A common theme throughout this body of work is the desire to reduce the number of data copies as much as possible, as naive network protocol implementations can copy packet data as much as five times.
Reference: [27] <author> David Mosberger, Larry L. Peterson, Patrick G. Bridges, and Sean O'- Malley. </author> <title> Analysis of techniquesto improve protocol processing latency. </title> <booktitle> In ACM SIGCOMM Symposium on CommunicationsArchitectures and Protocols, </booktitle> <address> Stanford, CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: In this paper, we have advocated techniques that improve instruction cache behavior. Mosberger et al. <ref> [27] </ref> and Blackwell [5] provide two examples of how this can be done. Mosberger et al. examine several compiler-related approaches to improving protocol latency. Using a combination of their techniques (outlining, cloning, and path-inlining), they show up to a 40 percent reduction in protocol processing times. <p> We have measured the cache miss rates of protocol stacks of a zero-copy protocol stack on a contemporary RISC-based machine with and without the checksum. Mosberger et al. <ref> [27] </ref> examine several compiler-related approaches to improving protocol latency. They present an updated study of protocol processing on a DEC Alpha, including a detailed analysis of instruction cache effectiveness.
Reference: [28] <author> B.J. Murphy, S. Zeadally, and C.J. Adams. </author> <title> An analysis of process and memory models to support high-speed networking in a UNIX environment. </title> <booktitle> In Proceedings of the Winter USENIX Technical Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: A common theme throughout this body of work is the desire to reduce the number of data copies as much as possible, as naive network protocol implementations can copy packet data as much as five times. As a consequence, single-copy and even zero-copy protocol stacks have been demonstrated <ref> [9, 28] </ref>. These pieces of work focus on `reducing work' done during protocol processing, 10 namely reducing the number of instructions executed. Our protocol stacks emulate zero-copy stacks.
Reference: [29] <author> Erich M. </author> <title> Nahum. Validating an architectural simulator. </title> <type> Technical Report 96-40, </type> <institution> Department of Computer Science, University of Mass-achusetts at Amherst, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: We are 4 aware of only a very few pieces of work that use trace-driven or execution-driven simulation that actually validate their simulators [3, 8, 12]. Our accuracy is comparable to theirs. More details about the construction and validation of the simulator can be found in <ref> [29] </ref>. 3 Characterization and Analysis In this section, we present our characterization and analysis of memory reference behavior of network protocols under a number of different conditions. 3.1 Baseline Memory Analysis We begin by determining the contribution to packet latency that is due to waiting for memory.
Reference: [30] <author> Erich M. Nahum, David J. Yates, James F. Kurose, and Don Towsley. </author> <title> Performance issues in parallelized network protocols. </title> <booktitle> In First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 125-137, </pages> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Our stack is thus a user-space implementation of the x-kernel that we ported to the SGI platform. The code is the uniprocessor base for two different multiprocessor versions of the x-kernel <ref> [30, 39] </ref>. The protocols we examine are from the core TCP/IP suite, those used in typical Internet scenarios. The execution paths we study are those that would be seen along the common case or fast path during data transfer of an application. <p> Normally, in a user-space implementation of the x-kernel, a simulated device driver is configured below the media access control layer (in this case, 2 We use 32 bits for the flow-control windows; see <ref> [30] </ref> for more details. FDDI). The simulated driver uses the socket interface to emulate a network device, crossing the user-kernel boundary on every packet.
Reference: [31] <author> Karl Pettis and Robert C. Hansen. </author> <title> Profile guided code positioning. </title> <booktitle> In ACM SIGPLAN `90 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 16-27, </pages> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: CORD is a binary re-writing tool that uses profile-guided code positioning <ref> [31] </ref> to reorganize executables for better instruction cache behavior. An original executable is run through Pixie [36] to determine its run time behavior and profile which procedures are used most frequently. CORD uses this information to re-link the executable so that procedures used most frequently are grouped together.
Reference: [32] <author> Mendel Rosenblum, Edouard Bugnion, Stephen A. Herrod, Emmett Witchell, and Anoop Gupta. </author> <title> The impact of computer architecture on operating system performance. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Canyon, CO, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: They present an updated study of protocol processing on a DEC Alpha, including a detailed analysis of instruction cache effectiveness. Using a combination of their techniques (outlining, cloning, and path-inlining), they show up to a 40 percent reduction in protocol processing times. Rosenblum et al. <ref> [32] </ref> present an execution-driven simulator that executes both application and operating system code. They evaluate scientific, engineering, and software development workloads on their simulator.
Reference: [33] <author> James D. Salehi, James F. Kurose, and Don Towsley. </author> <title> The effectiveness of affinity-based scheduling in multiprocessor network protocol processing. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 4(4) </volume> <pages> 516-530, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: In general, we observe a factor of 5 to 6 increase in latency between experiments with hot caches and those with cold caches. Our experiments using UDP exhibit an increase by a factor of 6, which is even more drastic than the increase measured by Salehi et al. <ref> [33] </ref>, who observed a slowdown by a factor of 4 when coercing cold-cache behavior with UDP without checksumming. <p> Although we cannot evaluate some of the more advanced architectural features that they do, our conclusions about our workload on future architectures agree with theirs, due to the increased cache sizes and associativities that are predicted for these machines. Salehi et al. <ref> [33] </ref> examine scheduling for parallelized network protocol processing via a simulation model parameterized by measurements of a UDP/IP protocol stack on a shared-memory multiprocessor. They find that scheduling for cache affinity can reduce protocol processing latency and improve throughput.
Reference: [34] <author> Douglas C. Schmidt and Tatsuya Suda. </author> <title> Measuring the performance of parallel message-based process architectures. </title> <booktitle> In Proceedings of the Conference on Computer Communications (IEEE Infocom), </booktitle> <address> Boston, MA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: The drivers emulate a high-speed FDDI interface, and support the FDDI maximum transmission unit (MTU) of slightly over 4K bytes. This approach is similar to those taken in <ref> [4, 16, 34] </ref>. In addition to emulating the actual hardware drivers, the in-memory drivers also simulate the behavior of a peer entity that would be at the remote end of a connection.
Reference: [35] <institution> Silicon Graphics Inc. </institution> <note> Cord manual page, IRIX 5.3. </note>
Reference-contexts: Off 2.24 1.42 1.42 HOT UDP Send Cksum On 1.30 1.10 1.10 COLD TCP Send Cksum Off 12.87 16.97 23.93 COLD TCP Send Cksum On 6.49 8.29 11.41 COLD UDP Send Cksum Off 16.72 22.62 32.28 COLD UDP Send Cksum On 4.86 6.23 8.48 Table 10: Machine CPIs using CORD <ref> [35] </ref>. CORD is a binary re-writing tool that uses profile-guided code positioning [31] to reorganize executables for better instruction cache behavior. An original executable is run through Pixie [36] to determine its run time behavior and profile which procedures are used most frequently.
Reference: [36] <author> Michael D. Smith. </author> <title> Tracing with Pixie. </title> <type> Technical report, </type> <institution> Center for Integrated Systems, Stanford University, Stanford, </institution> <address> CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: CORD is a binary re-writing tool that uses profile-guided code positioning [31] to reorganize executables for better instruction cache behavior. An original executable is run through Pixie <ref> [36] </ref> to determine its run time behavior and profile which procedures are used most frequently. CORD uses this information to re-link the executable so that procedures used most frequently are grouped together. This heuristic approach attempts to minimize the likelihood that hot procedures will conflict in the instruction cache.
Reference: [37] <author> Steven E. Speer, Rajiv Kumar, and Craig Partridge. </author> <title> Improving UNIX kernel performance using profile based optimization. </title> <booktitle> In Proceedings of the Winter 1994 USENIX Conference, </booktitle> <pages> pages 181-188, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Rather than using a model of protocol behavior, we use real protocols to drive a validated execution-driven simulator. We examine both TCP and UDP, determine instruction and memory costs, and vary architectural dimensions to determine sensitivity. Speer et al. <ref> [37] </ref> describe profile-based optimization (PBO), which uses profiles of previous executions of a program to determine how to reorganize code to reduce branch costs and, to a lesser extent, reduce cache misses.
Reference: [38] <author> Jack E. Veenstra and Robert J. Fowler. MINT: </author> <title> A front end for efficient simulation of shared-memory multiprocessors. </title> <booktitle> In Proceedings 2nd International Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of Computer and Telecommunication Systems (MASCOTS), Durham, NC, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: We use this simulator to understand the performance costs of our network protocol stacks, and to guide us in identifying and reducing bottlenecks. The primary goal of the simulator has been to accurately model CPU and memory costs for the SGI architecture. Our architectural simulator is built using MINT <ref> [38] </ref>, a toolkit for implementing multiprocessor memory reference simulators. MINT interprets a compiled binary directly and executes it, albeit much more slowly than if the binary was run on the native machine. This process is called direct execution.
Reference: [39] <author> David J. Yates, Erich M. Nahum, James F. Kurose, and Don Towsley. </author> <title> Networking support for large scale multiprocessor servers. </title> <booktitle> In Proceedings of the ACM Sigmetrics Conference on Measurementand Modeling of Computer Systems, </booktitle> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year> <month> 12 </month>
Reference-contexts: Our stack is thus a user-space implementation of the x-kernel that we ported to the SGI platform. The code is the uniprocessor base for two different multiprocessor versions of the x-kernel <ref> [30, 39] </ref>. The protocols we examine are from the core TCP/IP suite, those used in typical Internet scenarios. The execution paths we study are those that would be seen along the common case or fast path during data transfer of an application.
References-found: 39

