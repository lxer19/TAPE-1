URL: http://ftp.eecs.umich.edu/people/neuhoff/balamesh_thesis.ps
Refering-URL: http://ftp.eecs.umich.edu/people/neuhoff/
Root-URL: http://www.eecs.umich.edu
Title: BLOCK-CONSTRAINED METHODS OF FIXED-RATE ENTROPY-CODED QUANTIZATION  
Author: by Ahmed Said Abdullah Balamesh 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Doctoral Committee: Professor David L. Neuhoff, Chair Assistant Professor John T. Coffey Professor Robert L. Smith Associate Professor Wayne E. Stark  
Date: 1993  
Affiliation: (Electrical Engineering: Systems) in The University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 170 171 BIBLIOGRAPHY </institution>
Reference: [1] <author> R. Laroia and N. Farvardin, </author> <title> "A structured fixed-rate vector quantizer derived from a variable-length encoded scalar quantizer," </title> <booktitle> Proc. of the Twenty-Fourth Annual Conference on Information Sciences and Systems, </booktitle> <pages> pp. 796-801, </pages> <address> Prince-ton, NJ, </address> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: However, such schemes degrade the performance of the system for future samples of the source if the past ones were bad. This means that the wrong samples get the "punishment". A new fixed-rate method that resembles entropy-coded, scalar quantization has recently been proposed by Laroia and Farvardin <ref> [1] </ref>. With this approach, given a block of source symbols, dynamic programming is used to find a sequence of quantization levels that can be encoded with a fixed number of bits, using a kind of 1 2 lexicographic indexing. <p> For example, to get within 2 dB of the rate-distortion limit for an IID Gaussian source, a rate-3, tree-structured VQ requires about 160,000 storage locations. Thus, there is considerable need for low-complexity methods. Block-constrained methods such as those presented in <ref> [1] </ref> and here have complexity that increases slowly with rate. Thus, they are well suited to coding in the moderate-to-high rate range. This thesis is oriented towards achieving the performance of scalar quantization with entropy coding, but with fixed-rate, low-complexity schemes. <p> CHAPTER III BLOCK-CONSTRAINED QUANTIZATION : AN INTRODUCTION 3.1 Introduction In this chapter, we introduce a class of quantizers that we call block-constrained quantizers (BCQ) which represent a method of converting prefix-encoded, scalar quantizers into fixed-rate quantizers. We relate our approach to that of Laroia and Farvardin <ref> [1] </ref> and present both approaches in a unified framework. We also relate them to permutation codes [9, 13]. Also, we discuss the binary encoding, the quantization process and the optimization of such quantizers. A brief discussion of the asymptotic behavior of such quantizers is given. <p> This method is actually just one of a family of methods that we call block-constrained, fixed-rate, entropy-coded quantization (or block-constrained quantization (BCQ) for short) that were inspired by the original pioneering work of Laroia and Farvardin <ref> [1] </ref>. We now give the details of the specific block-constrained method described above. <p> As shown above, this loss is inherited by any pe-BCQ. Of course, this redundancy can be reduced by applying a prefix code to blocks of quantizer levels. However, this will increase the complexity of the BCQ. As we will explain later, Laroia and Farvardin <ref> [1] </ref> reduce this redundancy by 27 bits per source sampler, SNR, dB =0.54, m=20D Naive System (optimized) Naive System n=50 100 900 (q,l,r)d sq _ _ source. 28 assigning non-integer lengths to the quantization levels. <p> We will discuss this in more detail in the next section. 3.3 General Block-Constrained Quantizers In this section, we describe a more general class of block-constrained quantizers motivated by the case explained above and by the work of Laroia and Farvardin <ref> [1] </ref>. <p> The quantization process proceeds as before via (3.1). If l consists of rational (or rationalizable) lengths with a relatively small denominator, then the quantization 29 can be implemented efficiently using dynamic programming as proposed in <ref> [1] </ref> and explained in Section 3.4. If l consists of arbitrary real numbers, the implementation of (3.1) can be complex; however, low-complexity, approximations are possible. Such methods will be considered in Chapter IV. There are many ways to encode the codevectors into bits. <p> In this case the minimization can be implemented using dynamic programming as in <ref> [1] </ref>. For completeness and for future reference, we describe it here. Assume that a source sequence x is given. Let y fl denote a minimizing sequence of levels and D fl denote the resulting distortion. The minimization in (3.1) is simplified by the introduction of the notion of state. <p> Optimizing q given l; n and L Given l; one can optimize q using the method described in <ref> [1, 2] </ref>. This is, actually, a generalized LBG algorithm for trellises [22, 23], which guarantees monotonic decrease in distortion in every iteration. We describe this algorithm very briefly. Given a training sequence fx 1 ; x 2 ; : : : ; x K g of n-dimensional source vectors. <p> Also, the optimized q; l found for high dimensions are good initial choices for smaller dimensions. For BCQ's using other encoding schemes similar considerations may still hold, as in the case of lf-BCQ <ref> [1, 2] </ref>. 39 Table 3.1: Rough complexity comparisons for lf-BCQ's and pe-BCQ's. lf-BCQ pe-BCQ Trellis Depth n n Number of States 1:5bnr nr Trellis Branching Factor m m Search Operations per Branch 2 2 Arithmetic Operations per Source Sample 3bmnr 2mnr Storage Locations 1:5bn 2 r n 2 r Binary Arithmetic <p> The richness of such schemes necessitates deeper understanding. We leave this to future studies. 3.8 Conclusion In this chapter, we have introduced a quite general class of fixed-rate quantizers that we called block-constrained quantizers, motivated by the work of Laroia and Farvardin <ref> [1] </ref>. We proposed methods to reduce the binary encoding complexities of such quantizers. We have discussed the relationship between block-constrained quantizers, prefix-encoded quantizers, entropy-constrained quantizers and permutation codes. Node-varying BCQ's have been proposed as a means of reducing the code-space loss of pe-BCQ's. The results show significant gains. <p> Before we describe the encoding and decoding algorithm, we need the following facts about the binary expansions of numbers in [0; 1): Lemma 5.3.2 Let a 2 <ref> [0; 1) and b 2 [0; 1] </ref> be such that a &lt; b and let L = d log 2 (b a)e: 1. The smallest l such that a + 2 l b is given by L: 72 2. <p> Before we describe the encoding and decoding algorithm, we need the following facts about the binary expansions of numbers in [0; 1): Lemma 5.3.2 Let a 2 [0; 1) and b 2 <ref> [0; 1] </ref> be such that a &lt; b and let L = d log 2 (b a)e: 1. The smallest l such that a + 2 l b is given by L: 72 2. <p> More details are given in the next section. 6.8.4 Asymptotic OPTA for lf-BCQ As we recall from Subsection 3.3.1, lf-BCQ is a block-encoded BCQ with rational lengths. This restriction of lengths was proposed by Laroia and Farvardin <ref> [1] </ref> to simplify the BCQ search and the block binary encoding. For a rational l 2 QQ m ; where QQ is the set of rational numbers, we define the denominator of l as the least-common denominator (lcd) of its components.
Reference: [2] <author> R. Laroia and N. Farvardin, </author> <title> "A structured fixed-rate vector quantizer derived from a variable-length encoded scalar quantizer," </title> <journal> IEEE Trans. Inform. Theory, </journal> <note> to appear. </note>
Reference-contexts: Its OPTA is ffi be 4 n;m;q;l;L:R be d bcq (q; l; n; L): 5 In the notation of Chapter VI, c (r; l) = h 1 l (r): 6 See Lemma 6.4.3 and Appendix G. 30 Laroia and Farvardin <ref> [2] </ref> have shown 7 that ffi be sq (r): (3.8) Therefore, be-BCQ is, asymptotically, at least as good as optimum ECSQ. <p> Optimizing q given l; n and L Given l; one can optimize q using the method described in <ref> [1, 2] </ref>. This is, actually, a generalized LBG algorithm for trellises [22, 23], which guarantees monotonic decrease in distortion in every iteration. We describe this algorithm very briefly. Given a training sequence fx 1 ; x 2 ; : : : ; x K g of n-dimensional source vectors. <p> Thus, y k is no longer guaranteed to belong to C bcq (q; ~ l; n; L). Nevertheless, this algorithm can be tried. In many cases, it results in a reduction in distortion. Another less intuitive algorithm has been proposed by Laroia and Farvardin in <ref> [2] </ref>, which they argue is asymptotically optimum in n: 3. Choice of initial q and l Initially, q and l can be obtained using a good PESQ design algorithm and a Huffman design for the lengths for the case of pe-BCQ. <p> Also, the optimized q; l found for high dimensions are good initial choices for smaller dimensions. For BCQ's using other encoding schemes similar considerations may still hold, as in the case of lf-BCQ <ref> [1, 2] </ref>. 39 Table 3.1: Rough complexity comparisons for lf-BCQ's and pe-BCQ's. lf-BCQ pe-BCQ Trellis Depth n n Number of States 1:5bnr nr Trellis Branching Factor m m Search Operations per Branch 2 2 Arithmetic Operations per Source Sample 3bmnr 2mnr Storage Locations 1:5bn 2 r n 2 r Binary Arithmetic <p> The signal-to-noise ratio of lf-BCQ with dimension 32 and b = 4, as reported in <ref> [2] </ref>, is also included, 40 Table 3.2: SNR, in dB, for pe-BCQ and several other methods. <p> We see from the table that with n = 192, the signal-to-noise ratio of pe-BCQ is less than that of PESQ by about .1 to :6 dB. As explained earlier, entropy-constrained quantization has higher SNR than PESQ, typically two or three 10 These are quoted from <ref> [2] </ref>. 41 tenths of a dB. (An exception is the Laplacian source at rate 1.5.) The decrease in rate that would result by replacing the prefix encoding with block encoding is shown in parentheses in Table 3.2. <p> For these reasons, we feel that the effect of m needs to be extensively investigated and, thus, we did not try to investigate it here. We leave it for future research. The reader is referred to <ref> [2] </ref> for some discussion of the effect of m on be-BCQ's. 3.7 A Node-Varying, Block-Constrained Quantizer In this section, we propose a method to reduce the code-space loss of pe-BCQ and, thereby, improve its performance. The basic idea is introduced by means of an example. <p> The above result is a more general version of a result given by Laroia and Farvardin <ref> [2] </ref>. <p> Laroia and Farvardin conjectured <ref> [2] </ref> that ffi be;b sq (r; m) for all n: However, we will give a counterexample to their conjecture for the case of a discrete source.
Reference: [3] <author> R. Laroia and N. Farvardin, </author> <title> "Extension of of the fixed-rate structured vector quantizer to vector sources," </title> <booktitle> Proc. of the Twenty-Fifth Annual Conference on Information Sciences and Systems, </booktitle> <pages> pp. 576-581, </pages> <publisher> Johns Hopkins, </publisher> <address> Baltimore, MD, </address> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: For memoryless sources and moderate-to-high rates, such performance is better than that achievable by practical VQ's. For sources with memory, VQ may work better, but it is believed that the methods described here can be extended to exploit source correlation, as for example in <ref> [3] </ref>. In this case, future work may find these methods to be competitive with the best fixed-rate quantization techniques. Thesis Organization Chapter II gives a brief introduction to the basic concepts of data compression and scalar quantization with entropy coding.
Reference: [4] <author> C. E. Shannon, </author> <title> "Coding theorems for a discrete source with a fidelity criterion," </title> <journal> IRE Nat. Conv. Rec., </journal> <volume> Part 4, </volume> <pages> pp. 142-163, </pages> <year> 1959. </year>
Reference-contexts: Shannon's 8 source coding theorem <ref> [4] </ref> and its generalizations show that D (r) gives the minimum distortion that can be theoretically achieved by any finite-delay data compression system having rate less than or equal r operating on the given source (see [5-7]).
Reference: [5] <author> R. G. Gallager, </author> <title> Information Theory and Reliable Communication. </title> <address> New York: </address> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1968. </year>
Reference-contexts: By the above lemma and Lemma 4A.2 of <ref> [5] </ref>, we get the following theorem: Theorem 6.7.2 For a stationary source, lim n!1 d bcq (q; l; n; nr) exists and is equal to inf n d bcq (q; l; n; nr): Similarly, lim n!1 d bcq (q; l; n; nr 1) exists and is equal to inf n d
Reference: [6] <author> T. Berger, </author> <title> Rate Distortion Theory: A Mathematical Basis for Data Compression. </title> <address> Engelwood Cliffs, NJ:Prentice-Hall, </address> <publisher> Inc., </publisher> <year> 1971. </year>
Reference: [7] <author> R. M. Gray, </author> <title> Source Coding Theory. </title> <publisher> Boston:Kluwer Academic, </publisher> <year> 1990. </year>
Reference: [8] <author> D. A. Huffman, </author> <title> "A method for the construction of minimum-redundancy codes," </title> <journal> Proc. IRE, </journal> <volume> vol. 40, </volume> <pages> pp. 1098-1101, </pages> <month> Sep. </month> <year> 1952. </year>
Reference-contexts: The rate of the prefix code is given by the average length of its codewords, i.e. P j p j l j ; where p j is the probability with which the j-th codeword is used. Given a set of probabilities fp j g; Huffman <ref> [8] </ref> gave an algorithm for designing a prefix code with minimum average 11 1 01 110 1111 1 1 1 1 length. The rate of such prefix codes is no less than, and within 1:0 bit per sample of, the entropy of the given set of probabilities.
Reference: [9] <author> D. Slepian, </author> <title> "Permutation modulation," </title> <journal> Proc. IEEE, </journal> <volume> vol. 53, </volume> <pages> pp. 228-236, </pages> <month> Mar. </month> <year> 1965. </year>
Reference-contexts: We relate our approach to that of Laroia and Farvardin [1] and present both approaches in a unified framework. We also relate them to permutation codes <ref> [9, 13] </ref>. Also, we discuss the binary encoding, the quantization process and the optimization of such quantizers. A brief discussion of the asymptotic behavior of such quantizers is given. More detailed analysis is left to Chapter VI.
Reference: [10] <author> T. J. Goblick, Jr. and J. L. Holsinger, </author> <title> "Analog source digitization: A comparison of theory and practice," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-13, </volume> <pages> pp. 323-326, </pages> <month> Apr. </month> <year> 1967. </year>
Reference-contexts: The first discovery was that of Goblick and Holsinger <ref> [10] </ref> who numerically demonstrated that, for large values of r; uniform quantizers with output entropy r have distortion about 1:5 dB larger than D (r).
Reference: [11] <author> H. Gish and J. N. Pierce, </author> <title> "Asymptotically efficient quantizing," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-14, </volume> <pages> pp. 676-683, </pages> <month> Sep. </month> <year> 1968. </year> <month> 172 </month>
Reference-contexts: The first discovery was that of Goblick and Holsinger [10] who numerically demonstrated that, for large values of r; uniform quantizers with output entropy r have distortion about 1:5 dB larger than D (r). Subsequently, Gish and Pierce <ref> [11] </ref> proved the optimality of uniform scalar quantizers for sufficiently smooth source densities and asymptotically large 2 OPTA stands for the optimum performance theoretically achievable. 13 rates r: They also proved that lim r!1 (ffi ent sq (r)=D (r)) = e=6 (i.e. an increase of 1:53 dB).
Reference: [12] <author> R. C. Wood, </author> <title> "On optimum quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-15, </volume> <pages> pp. 248-252, </pages> <month> Mar. </month> <year> 1969. </year>
Reference-contexts: Algorithms for designing quantizers with minimum distortion subject to a constraint on their entropy have been proposed in <ref> [12, 14, 15, 18, 21] </ref>. Such quantizers are called entropy-constrained, scalar quantizers (ECSQ). <p> Wood <ref> [12] </ref> proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see [12, 14, 17, 18]). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of [18], for a Gaussian source. <p> Wood [12] proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see <ref> [12, 14, 17, 18] </ref>). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of [18], for a Gaussian source.
Reference: [13] <author> T. Berger, F. Jelinek and J. K. Wolf, </author> <title> "Permutation codes for sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-18, </volume> <pages> pp. 160-169, </pages> <month> Jan. </month> <year> 1972. </year>
Reference-contexts: Since the goal of the thesis is to achieve the performance of ECSQ with fixed-rate schemes, we mention here a scheme that is known to achieve this goal, namely permutation codes, for later reference <ref> [13] </ref>. Permutation Codes Let q 1 &lt; q 2 &lt; &lt; q m and let n 1 ; n 2 ; : : : ; n m be positive integers such that n 1 + n 2 + + n m = n. <p> We relate our approach to that of Laroia and Farvardin [1] and present both approaches in a unified framework. We also relate them to permutation codes <ref> [9, 13] </ref>. Also, we discuss the binary encoding, the quantization process and the optimization of such quantizers. A brief discussion of the asymptotic behavior of such quantizers is given. More detailed analysis is left to Chapter VI. <p> Finally, there is a finite-dimensional loss when n is not large enough for the law of large numbers to entirely dictate performance. 33 3.3.3 Relationship to Permutation Codes Permutation codes are fixed-rate, block codes that have been shown to achieve ffi ent sq (r) <ref> [13, 14, 17] </ref>. Therefore, it is interesting and important to compare them to BCQ. Specifically, C bcq (q; l; n; L) can be viewed as a union of permutation code-books. <p> : : ; m; m X n j = n; j=1 Then, the block-constrained quantizer codebook is given by C bcq (q; l; n; L) = n2N (l;n;L) Moreover, for any n; fy 2 (q) n : T (y) = ng is a the codebook of a variant-I permutation code <ref> [13] </ref>. Thus, a BCQ codebook is the union of permutation codebooks. It is interesting to note that the lengths l and L determine which types are included and which are not. Thus, careful choice of l and L can be used to construct codes with special features. <p> Permutation codes do this. However, if n is not sufficiently high, then it is better to distribute the codewords over a spherical shell whose thickness increases with rate. This explains the observation in <ref> [13] </ref> that for a given dimension n; the distortion of permutation codes diverges from ffi ent sq (r) as r increases.
Reference: [14] <author> T. Berger, </author> <title> "Optimum quantizers and permutation codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-18, </volume> <pages> pp. 759-765, </pages> <month> Nov. </month> <year> 1972. </year>
Reference-contexts: Algorithms for designing quantizers with minimum distortion subject to a constraint on their entropy have been proposed in <ref> [12, 14, 15, 18, 21] </ref>. Such quantizers are called entropy-constrained, scalar quantizers (ECSQ). <p> Wood [12] proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see <ref> [12, 14, 17, 18] </ref>). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of [18], for a Gaussian source. <p> Berger <ref> [14, 17] </ref> showed that permutation codes achieve, asymptotically in n; the performance of ECSQ and that for sufficiently large n permutation codes cannot do better than ECSQ. <p> Finally, there is a finite-dimensional loss when n is not large enough for the law of large numbers to entirely dictate performance. 33 3.3.3 Relationship to Permutation Codes Permutation codes are fixed-rate, block codes that have been shown to achieve ffi ent sq (r) <ref> [13, 14, 17] </ref>. Therefore, it is interesting and important to compare them to BCQ. Specifically, C bcq (q; l; n; L) can be viewed as a union of permutation code-books.
Reference: [15] <author> A. N. Netravali and R. Saigal, </author> <title> "Optimum quantizer design using a fixed-point algorithm," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 55, </volume> <pages> pp. 1423-1435, </pages> <month> Nov. </month> <year> 1976. </year>
Reference-contexts: Algorithms for designing quantizers with minimum distortion subject to a constraint on their entropy have been proposed in <ref> [12, 14, 15, 18, 21] </ref>. Such quantizers are called entropy-constrained, scalar quantizers (ECSQ).
Reference: [16] <author> P. Noll and R. Zelinski, </author> <title> "Bounds on quantizer performance in the low bit-rate region," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. COM-26, </volume> <pages> pp. 300-305, </pages> <month> Feb. </month> <year> 1978. </year>
Reference: [17] <author> T. Berger, </author> <title> "Minimum entropy quantizers and permutation codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-28, </volume> <pages> pp. 149-157, </pages> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: Wood [12] proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see <ref> [12, 14, 17, 18] </ref>). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of [18], for a Gaussian source. <p> Berger <ref> [14, 17] </ref> showed that permutation codes achieve, asymptotically in n; the performance of ECSQ and that for sufficiently large n permutation codes cannot do better than ECSQ. <p> Finally, there is a finite-dimensional loss when n is not large enough for the law of large numbers to entirely dictate performance. 33 3.3.3 Relationship to Permutation Codes Permutation codes are fixed-rate, block codes that have been shown to achieve ffi ent sq (r) <ref> [13, 14, 17] </ref>. Therefore, it is interesting and important to compare them to BCQ. Specifically, C bcq (q; l; n; L) can be viewed as a union of permutation code-books.
Reference: [18] <author> N. Farvardin and J. W. Modestino, </author> <title> "Optimum quantizer performance for a class of non-Gaussian memoryless sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-30, </volume> <pages> pp. 485-497, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: Algorithms for designing quantizers with minimum distortion subject to a constraint on their entropy have been proposed in <ref> [12, 14, 15, 18, 21] </ref>. Such quantizers are called entropy-constrained, scalar quantizers (ECSQ). <p> Wood [12] proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see <ref> [12, 14, 17, 18] </ref>). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of [18], for a Gaussian source. <p> Wood [12] proved similar results. Moreover, it was demonstrated by several authors that even for moderate rates uniform quantizers are generally nearly optimum (see [12, 14, 17, 18]). sq (r) expressed as signal-to-noise ratio vs. entropy, computed via Algorithm 2 of <ref> [18] </ref>, for a Gaussian source. <p> Choice of initial q and l Initially, q and l can be obtained using a good PESQ design algorithm and a Huffman design for the lengths for the case of pe-BCQ. The method of Chou, et al., [21] (which is a generalization of Algorithm 2 in <ref> [18] </ref>) is quite a good choice. We tried both the training-sequence and density versions of such. For high rates, we found that a uniform scalar quantizer with a Huffman code is a sufficiently good initial choice. <p> Finally, we comment on the effect of m on the performance of BCQ's. One is tempted to believe that the performance of a BCQ must improve as m increases. However, the subtle behavior of the number of levels in entropy-constrained quantiz-ers <ref> [18, 20] </ref>, as well as prefix-encoded quantizers, makes the issue less trivial. Moreover, the effects of m on the code-space loss is not at all clear.
Reference: [19] <author> N. Farvardin and J. W. Modestino, </author> <title> "Adaptive buffer-instrumented entropy-coded quantizer performance for memoryless sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-32, </volume> <pages> pp. 9-22, </pages> <month> Jan. </month> <year> 1986. </year>
Reference-contexts: The major disadvantage of entropy-coded quantizers is their variable output rate, which requires buffering and causes synchronization problems. Buffer-instrumented strategies have been used <ref> [19] </ref> to minimize such problems. However, such schemes degrade the performance of the system for future samples of the source if the past ones were bad. This means that the wrong samples get the "punishment".
Reference: [20] <author> J. C. Kieffer, T. M. Jahns and V. A. Obuljen, </author> <title> "New results on optimal entropy-constrained quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-34, </volume> <pages> pp. 1250-1258, </pages> <month> Sep. </month> <year> 1988. </year>
Reference-contexts: Finally, we comment on the effect of m on the performance of BCQ's. One is tempted to believe that the performance of a BCQ must improve as m increases. However, the subtle behavior of the number of levels in entropy-constrained quantiz-ers <ref> [18, 20] </ref>, as well as prefix-encoded quantizers, makes the issue less trivial. Moreover, the effects of m on the code-space loss is not at all clear.
Reference: [21] <author> P. A. Chou, T. Lookabaugh and R. M. Gray, </author> <title> "Entropy-constrained vector quantization," </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> vol. ASSP-37, </volume> <pages> pp. 31-42, </pages> <month> Jan </month> <year> 1989. </year>
Reference-contexts: Algorithms for designing quantizers with minimum distortion subject to a constraint on their entropy have been proposed in <ref> [12, 14, 15, 18, 21] </ref>. Such quantizers are called entropy-constrained, scalar quantizers (ECSQ). <p> Thus, the comparison in Figure 3.2 is unfair to the naive system. To make a fairer comparison, we ran the naive system with a variety of threshold sets, and for each r we plot in Figure 3.3 the best signal-to-noise ratio attained. Motivated by the work in <ref> [21] </ref>, for a range of 0; we considered sets of thresholds that minimized the functional (x q j ) 2 + l j over j. The same figure also shows the performance of BCQ as well as its asymptotic distortion limit ffi sq (q; l; r). <p> Choice of initial q and l Initially, q and l can be obtained using a good PESQ design algorithm and a Huffman design for the lengths for the case of pe-BCQ. The method of Chou, et al., <ref> [21] </ref> (which is a generalization of Algorithm 2 in [18]) is quite a good choice. We tried both the training-sequence and density versions of such. For high rates, we found that a uniform scalar quantizer with a Huffman code is a sufficiently good initial choice.
Reference: [22] <author> Y. Linde, A. Buzo and R. M. Gray, </author> <title> "An algorithm for vector quantizer design," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. COM-28, </volume> <pages> pp. 84-95, </pages> <month> Jan </month> <year> 1980. </year>
Reference-contexts: Optimizing q given l; n and L Given l; one can optimize q using the method described in [1, 2]. This is, actually, a generalized LBG algorithm for trellises <ref> [22, 23] </ref>, which guarantees monotonic decrease in distortion in every iteration. We describe this algorithm very briefly. Given a training sequence fx 1 ; x 2 ; : : : ; x K g of n-dimensional source vectors.
Reference: [23] <author> L. C. Stewart, R. M. Gray and Y. Linde, </author> <title> "The design of trellis waveform coders," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. COM-30, </volume> <pages> pp. 702-710, </pages> <month> Apr. </month> <year> 1982. </year>
Reference-contexts: Optimizing q given l; n and L Given l; one can optimize q using the method described in [1, 2]. This is, actually, a generalized LBG algorithm for trellises <ref> [22, 23] </ref>, which guarantees monotonic decrease in distortion in every iteration. We describe this algorithm very briefly. Given a training sequence fx 1 ; x 2 ; : : : ; x K g of n-dimensional source vectors.
Reference: [24] <author> M. S. Bazaraa and C. M. Shetty, </author> <title> Nonlinear Programming: Theory and Algorithms. </title> <address> New York: </address> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1979. </year>
Reference-contexts: formulation of entropy-constrained, scalar quantization and by the work of Chou, et al.[21], we consider scalar quantizer thresholds that minimize the quantity (x q j ) 2 + l j for different values of the Lagrange multiplier : To further motivate this collection of scalar quantizers, consider the Lagrangian relaxation <ref> [24] </ref> of (3.1): Minimize n X (x i y i ) 2 + l (y i ); (4.2) subject to y i 2 fq 1 ; q 2 ; : : : ; q m g for all i; where 0 is a Lagrange multiplier. <p> we briefly discuss the properties of the function h l : It is defined as the maximum entropy of any probability distribution (p 1 ; p 2 ; : : : ; p m ) with P m j=1 p j l j R: Using the strong lagrangian duality theorem <ref> [24, Theorem 6.2.4] </ref>, we get for R &gt; l min ; ff0 @ ffR + log j=1 1 Using this result, we can get an explicit expression for h l as given by the following lemma. <p> By Chernoff's bound, Pr i=1 ) = exp (n (su n ln M (s))); for all s 0; which implies Pr i=1 ) s0 = exp (nI (u n )) Since I ( l) = I 0 ( l) = 0; then by the second-order Taylor's theorem (e.g. <ref> [24, p. 504] </ref>), there exists fl n ; l &lt; fl n &lt; u n ; such that I (u n ) = 2 or v u I 00 (fl n ) v u I 00 (fl n ) p + l 2 ln (1=ff)K 1 p + l = p
Reference: [25] <author> W. H. Press, et al., </author> <title> Numerical Recipes: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cam-bridge University Press, </publisher> <year> 1986. </year>
Reference-contexts: We did not spend any effort in optimizing the strategy of varying : Methods from nonlinear programming can also be used to optimize the strategy, e.g. bisection and related methods (see <ref> [25] </ref>). Also, it should be noted that in the above method, iterations continue until L () = L or the maximum number of iterations is exceeded. In many cases, no improvement is obtained after a certain number of iterations.
Reference: [26] <author> P. Zador, </author> <title> "Asymptotic quantization error of continuous signals and the quantization dimension," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-28, </volume> <pages> pp. 139-149, </pages> <month> Mar. </month> <year> 1982. </year> <month> 173 </month>
Reference-contexts: Thus, for small rates, VQ and TSVQ are preferred to BCQ. On the other hand, for moderate-to-high rates (3 or higher), the complexities of VQ and TSVQ become very large compared to BCQ of the same distortion. To substantiate this claim, we use the Zador-Gersho lower bound <ref> [26, 27] </ref> for VQ distortion with "spherical" quantization cells assuming an IID Gaussian source and MSE distortion. For this bound to be within 2 dB (actually 2:01 dB) of the rate-distortion limit, a dimension of at least 4 is needed.
Reference: [27] <author> A. Gersho, </author> <title> "Asymptotically optimal block quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-25, </volume> <pages> pp. 373-380, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Thus, for small rates, VQ and TSVQ are preferred to BCQ. On the other hand, for moderate-to-high rates (3 or higher), the complexities of VQ and TSVQ become very large compared to BCQ of the same distortion. To substantiate this claim, we use the Zador-Gersho lower bound <ref> [26, 27] </ref> for VQ distortion with "spherical" quantization cells assuming an IID Gaussian source and MSE distortion. For this bound to be within 2 dB (actually 2:01 dB) of the rate-distortion limit, a dimension of at least 4 is needed.
Reference: [28] <author> N. Moayeri, </author> <title> Private Communication. </title>
Reference-contexts: A rate-3, 5-dimensional, VQ requires (in the worst case) at least 65,536 floating-point operations per source sample and 163,840 storage locations, not taking the binary encoding into consideration. Moreover, results on TSVQ <ref> [28, 29] </ref> show that a rate-3; 4-dimensional TSVQ is about 3 dB away from the rate-distortion limit. Thus, it is safe to assume that a dimension of at least 7 is needed for a TSVQ to perform within 2 dB.
Reference: [29] <author> D. L. Neuhoff, </author> <title> Private Communication. </title>
Reference-contexts: A rate-3, 5-dimensional, VQ requires (in the worst case) at least 65,536 floating-point operations per source sample and 163,840 storage locations, not taking the binary encoding into consideration. Moreover, results on TSVQ <ref> [28, 29] </ref> show that a rate-3; 4-dimensional TSVQ is about 3 dB away from the rate-distortion limit. Thus, it is safe to assume that a dimension of at least 7 is needed for a TSVQ to perform within 2 dB.
Reference: [30] <author> N. M. Abramson, </author> <title> Information Theory and Coding. </title> <address> New York:McGraw-Hill, </address> <year> 1963. </year>
Reference-contexts: The receiver knows the size of the source block n: This scenario is of a block-to-variable nature. 70 The basic idea of arithmetic coding, as presented here, is originally due to Elias as reported in <ref> [30] </ref>. To a good extent, we will follow the notation of Jones [36]. 5.3.1 Ideal (Arbitrary-Precision) Arithmetic Coding Here, we will explain the basic idea of arithmetic coding in a block-to-variable scenario.
Reference: [31] <author> J. J. Rissanen, </author> <title> "Generalized Kraft inequality and arithmetic coding," </title> <journal> IBM J. Res. Develop., </journal> <volume> vol. 20, </volume> <pages> pp. 198-203, </pages> <month> May </month> <year> 1976. </year>
Reference-contexts: This can be cumbersome, on the one hand, and limits the size of the source string that can be practically handled, on the other. The first successful solutions to this problem are due to Rissanen <ref> [31] </ref> and Pasco [32]. Subsequently, more efficient solutions were proposed by Rubin [34], Jones [36] and Witten, et al. [38].
Reference: [32] <author> R. C. </author> <title> Pasco, Source Coding Algorithms for Fast Data Compression. </title> <publisher> Ph. </publisher> <address> D. </address> <institution> dissertation, Dept. Elec. Eng., Stanford Univ., </institution> <address> CA, </address> <year> 1976. </year>
Reference-contexts: Also, we will discuss practical implementation of arithmetic coding. In particular, we will discuss the implementations due to Jones [36] and Pasco <ref> [32] </ref>. We will show how Pasco's algorithm can be implemented using fixed-point arithmetic and demonstrate the advantage of Pasco's implementation regarding the ease of estimating the number of bits produced due to a given source sequence. <p> This can be cumbersome, on the one hand, and limits the size of the source string that can be practically handled, on the other. The first successful solutions to this problem are due to Rissanen [31] and Pasco <ref> [32] </ref>. Subsequently, more efficient solutions were proposed by Rubin [34], Jones [36] and Witten, et al. [38]. <p> The reason is that we assume that the decoder knows the number of bits transmitted. Pasco assumes that the decoder will continue to receive bits that do not belong to the sequence being decoded and thus one more bit might be needed to ensure correct decoding (see <ref> [32] </ref> for more details).
Reference: [33] <author> J. J. Rissanen and G. G. Langdon, Jr., </author> <title> "Arithmetic coding," </title> <journal> IBM J. Res. Develop., </journal> <volume> vol. 23, </volume> <pages> pp. 149-162, </pages> <month> Mar. </month> <year> 1979. </year>
Reference: [34] <author> F. Rubin, </author> <title> "Arithmetic stream coding using fixed precision registers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-25, </volume> <pages> pp. 672-675, </pages> <month> Nov. </month> <year> 1979. </year>
Reference-contexts: This can be cumbersome, on the one hand, and limits the size of the source string that can be practically handled, on the other. The first successful solutions to this problem are due to Rissanen [31] and Pasco [32]. Subsequently, more efficient solutions were proposed by Rubin <ref> [34] </ref>, Jones [36] and Witten, et al. [38].
Reference: [35] <author> J. J. Rissanen, </author> <title> "Arithmetic coding as number representation," </title> <journal> Acta Polyt. Scan-dinavica Math., </journal> <volume> vol. 34, </volume> <pages> pp. 44-51, </pages> <month> Dec. </month> <year> 1979. </year>
Reference: [36] <author> C. B. Jones, </author> <title> "An efficient coding system for long source sequences," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-27, </volume> <pages> pp. 280-291, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Also, we will discuss practical implementation of arithmetic coding. In particular, we will discuss the implementations due to Jones <ref> [36] </ref> and Pasco [32]. We will show how Pasco's algorithm can be implemented using fixed-point arithmetic and demonstrate the advantage of Pasco's implementation regarding the ease of estimating the number of bits produced due to a given source sequence. <p> The receiver knows the size of the source block n: This scenario is of a block-to-variable nature. 70 The basic idea of arithmetic coding, as presented here, is originally due to Elias as reported in [30]. To a good extent, we will follow the notation of Jones <ref> [36] </ref>. 5.3.1 Ideal (Arbitrary-Precision) Arithmetic Coding Here, we will explain the basic idea of arithmetic coding in a block-to-variable scenario. <p> This can be cumbersome, on the one hand, and limits the size of the source string that can be practically handled, on the other. The first successful solutions to this problem are due to Rissanen [31] and Pasco [32]. Subsequently, more efficient solutions were proposed by Rubin [34], Jones <ref> [36] </ref> and Witten, et al. [38]. <p> Figures 5.3 and 5.4 show flowcharts for the encoding and decoding algorithms for Pasco's arithmetic codes. It is interesting to compare these flowcharts with those for Jones' codes (Figures 5.1 and 5.2). Figures 5.1 and 5.2 are modified from the ones given in <ref> [36] </ref> to make the comparison easier. <p> Also, let p j be the probability of level q j : We use a Pasco's arithmetic code with parameters J and w: As in <ref> [36] </ref>, we find the F j 's by 5 F j = b1=2 + 2 J P j i=1 p i c and f j = F j F j1 ; where F 0 = 0 and F m = 2 J : If any of the f j 's is
Reference: [37] <author> G. G. Langdon, </author> <title> "An introduction to arithmetic coding," </title> <journal> IBM J. Res. Develop., </journal> <volume> vol. 28, </volume> <pages> pp. 135-149, </pages> <month> Mar. </month> <year> 1984. </year>
Reference-contexts: In the latter case, the bits are transmitted as they are. This carry delay can be a problem if a long sequence of source symbols is to be encoded. In such a case, several solutions have been proposed in the literature, e.g. see <ref> [37] </ref>. In our case, n is small enough that we do not have to worry about this problem. Also, the BCQ application, for which we will use arithmetic coding, does not require sequential transmission, since it works on a block basis.
Reference: [38] <author> I. H. Witten, R. M. Neal and J. G. Cleary, </author> <title> "Arithmetic coding for data compression," </title> <journal> Comm. of ACM, </journal> <volume> Vol. 30, </volume> <pages> pp. 520-540, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: The first successful solutions to this problem are due to Rissanen [31] and Pasco [32]. Subsequently, more efficient solutions were proposed by Rubin [34], Jones [36] and Witten, et al. <ref> [38] </ref>.
Reference: [39] <author> A. C. Popat, </author> <title> Scalar Quantization with Arithmetic Coding. M.S. </title> <type> Thesis, </type> <institution> MIT, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: Then, the F j 's are obtained from the p j 's 5 It might be a better idea to use the algorithm in <ref> [39] </ref>. We have chosen Jones' approach for simplicity. 94 as above. We have tried several strategies of whether to update the levels and/or the f j 's in every iteration. For the design, we used b100,000=nc n-dimensional training vectors, i.e. approximately 100,000 training source samples have been used.
Reference: [40] <author> W. </author> <title> Rudin, </title> <booktitle> Principles of Mathematical Analysis. 3rd. </booktitle> <publisher> Ed., </publisher> <address> New York:McGraw-Hill, </address> <year> 1976. </year>
Reference: [41] <author> H. L. Royden, </author> <title> Real Analysis. </title> <publisher> 3rd Ed., </publisher> <address> New York:Macmillan, </address> <year> 1988. </year>
Reference-contexts: 1: Define a n 4 = E [f n (X)]: First, we show that for any convergent subsequence a n k ; a n k ! 0 as k ! 1: Note that f n k (X) ! 0; in probability, as k ! 1: Then, by Proposition 4.18 of <ref> [41] </ref>, there exists a further subsequence f n k i (X) such that f n k i (X ) ! 0; with probability 1, as i ! 1: Now, by Theorem 4.17 of [41], a n k i = E [f n k i (X )] ! 0 as i ! <p> f n k (X) ! 0; in probability, as k ! 1: Then, by Proposition 4.18 of <ref> [41] </ref>, there exists a further subsequence f n k i (X) such that f n k i (X ) ! 0; with probability 1, as i ! 1: Now, by Theorem 4.17 of [41], a n k i = E [f n k i (X )] ! 0 as i ! 1 and since a n k is convergent, then a n k ! 0 as k ! 1: Now, it suffices to show that a n ! 0 as n ! 1: To <p> ) ! I (K;K] c (Y )g (Y ); with probability one. 155 Noting that 0 I (K;K] c (Y n )g (Y n ) g (Y n ); and that E [g (Y n )] = g d n ! g d = E [g (Y )] and using <ref> [41, Theorem 4.17] </ref>, we get Z g d n = E [I (K;K] c (Y n )g (Y n )] ! E [I (K;K] c (Y )g (Y )] = (K;K] c Lemma E.4 Let fF n g; F be probability distributions such that F n ! F uniformly and let
Reference: [42] <author> P. Billingsley, </author> <title> Probability and Measure. 2nd Ed., </title> <publisher> New York:John Wiley & Sons, Inc., </publisher> <year> 1986. </year>
Reference-contexts: Proof of Theorem 6.3.1 Part (a) or (a 0 ): Follows directly from the definitions of ^ ffi n and ffi sq by Lemmas 6.3.4 and 6.3.5. Part (b): Follows from Part (a), Fatou's lemma <ref> [42, Theorem 16.3 and Problem 16.4] </ref> and the facts that ^ ffi n 0; and ^ ffi n (q; l; n; n kxk 2 + 2q max p + q 2 where E kXk 2 + 2q max p + q 2 # max 106 Remark: Lemma 6.3.4, and hence Theorem <p> F (x)j ; where F n (x; !) is the empirical distribution function defined as F n (x; !) = n k=1 and I A is the indicator function for the set A: Then, lim n!1 D n (!) ! 0 almost surely. 2 The following proof is reproduced from <ref> [42] </ref> with the only exception that we assume that the source is ergodic and might not be IID. We observe that the proof in [42] does not actually need the IID property and that ergodicity would suffice. <p> A is the indicator function for the set A: Then, lim n!1 D n (!) ! 0 almost surely. 2 The following proof is reproduced from <ref> [42] </ref> with the only exception that we assume that the source is ergodic and might not be IID. We observe that the proof in [42] does not actually need the IID property and that ergodicity would suffice. <p> Now, it remains to show that Z g d n ! (K;K] c where g = P j f j : Since F n converges weakly to F; by Skorohod's theorem <ref> [42, Theo rem 25.6] </ref>, there exist random variables fY n g and fY g such that Y n has distribution F n ; Y has distribution F and Y n ! Y; surely. Since g is continuous, g (Y n ) ! g (Y ); surely.
Reference: [43] <author> K. P. Parthasarathy, </author> <title> Probability Measures on Metric Spaces. </title> <publisher> New York:Academic Press, </publisher> <year> 1967. </year>
Reference: [44] <author> I. Csiszar and J. Korner, </author> <title> Information Theory: Coding Theorems for Discrete Memoryless Systems. </title> <publisher> New York:Academic Press, </publisher> <year> 1981. </year> <month> 174 </month>
Reference-contexts: by the continuity of f; we have lim inf f (x n ) = lim f (x n k ) i!1 = f ( lim x n k i ) i!1 = lim f (x n k i + fl n k i ) n!1 169 Lemma I.2 (Stirling's Formula <ref> [44, Problem 1.2.2] </ref>) p 2 e 12 (n+1) n! 2n n+ 1 12n : This implies that for n 1 ; n 2 ; : : : ; n m such that n j is a non-negative integer for all j and n 1 + n 2 + + n m <p> 's 0 #(n; n) 1; ln is the natural logarithmic function, log = log 2 is the binary logarithmic function, p = (n 1 =n; n 2 =n; : : : ; n m =n) and H (p) = j=1 p j log p j : Lemma I.3 (Log-Sum Inequality <ref> [44, Lemma 3.1] </ref>) For any non-negative, real numbers, a 1 ; a 2 ; : : : ; a m ; b 1 ; b 2 ; : : : ; b m ; m X a j log b j 0 m X a j A log j=1 a j
Reference: [45] <author> J. A. Bucklew, </author> <title> Large Deviation Techniques in Decision, Simulation, and Estimation. </title> <publisher> New York:John Wiley & Sons, Inc., </publisher> <year> 1990. </year>
Reference-contexts: Define the moment generating function M (s) by M (s) = E [expfsl (Q (X 1 ))g] = m X p j exp (sl j ); where p j 4 = Prfl (Q (X 1 )) = l j g; and define the large deviation rate function I (u) by <ref> [45] </ref> 4 8 &gt; &gt; : sup s&lt;0 (su ln M (s)); u &lt; l It can be easily shown that I (u) is convex S and continuous on [l min ; l max ]: Moreover, I ( l) = 0; I 0 ( l) = 0; and I (l max
Reference: [46] <author> A. S. Balamesh and D. L. Neuhoff, </author> <title> "New methods of fixed-rate entropy-coded quantization," </title> <booktitle> Proc. of the Twenty-Sixth Annual Conference on Information Sciences and Systems, </booktitle> <pages> pp. 665-670, </pages> <address> Princeton, NJ, </address> <month> Mar. </month> <year> 1992. </year>
Reference: [47] <author> M. W. Marcellin, </author> <title> "On entropy-constrained trellis-coded quantization," </title> <booktitle> Proc. 1990 Int. Symposium on Inform. Theory and Its Applications, </booktitle> <pages> pp. 511-514, </pages> <address> Waikiki, Hawaii, </address> <month> Nov. </month> <year> 1990. </year>
Reference: [48] <author> T. R. Fischer and M. Wang, </author> <title> "Entropy-constrained trellis-coded quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-38, </volume> <pages> pp. 415-426, </pages> <month> Mar. </month> <year> 1992. </year>
References-found: 49

