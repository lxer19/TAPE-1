URL: http://www.ai.mit.edu/~mjones/real-models.ps
Refering-URL: http://www.ai.mit.edu/~mjones/mjones.html
Root-URL: 
Title: Model-Based Matching by Linear Combinations of Prototypes  
Author: Michael J. Jones and Tomaso Poggio 
Date: 1583 November 1996  139  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING  
Pubnum: A.I. Memo No.  C.B.C.L. Paper No.  
Abstract: We describe a flexible model similar to (Vetter and Poggio, 1993, 1995 and Jones and Poggio, 1995) for representing images of objects of a certain class, known a priori, such as faces, and introduce a new algorithm for matching it to a novel image and thereby perform image analysis. The flexible model is learned from example images (called prototypes) of objects of a class. In the learning phase, pixelwise correspondences between a reference prototype and each of the other prototypes are first computed and then used to obtain the shape and texture vectors corresponding to each prototype. The flexible model is then synthesized as a linear combination that spans the linear vector space determined by the prototypical shapes and textures. In this paper we introduce an effective stochastic gradient descent algorithm that automatically matches a model to a novel image by finding the parameters that minimize the error between the image generated by the model and the novel image. Several experiments demonstrate the robustness and the broad range of applicability of the matching algorithm and the underlying flexible model. Our approach can provide novel solutions to several vision tasks, including the computation of image correspondence, object verification, image synthesis and image compression. c fl Massachusetts Institute of Technology, 1996 This paper describes research done at the Artificial Intelligence Laboratory and within the Center for Biological and Computational Learning in the Department of Brain and Cognitive Sciences at the Massachusetts Institute of Technology. This research is sponsored by grants from ARPA-ONR under contract N00014-92-J-1879 and from ONR under contract N00014-93-1-0385 and by a grant from the National Science Foundation under contract ASC-9217041 (this award includes funds from ARPA provided under the HPCC program) and by a MURI grant N0014-95-1-0600. Additional support is provided by ATR Audio and Visual Perception Research Laboratories, Sumitomo Metal Industries, Kodak, Daimler-Benz and Siemens AG. Tomaso Poggio is supported by the Uncas and Helen Whitaker Chair at MIT's Whitaker College. 
Abstract-found: 1
Intro-found: 1
Reference: [ Atick et al., 1995 ] <author> Joseph J. Atick, Paul A. Griffin, and A. Norman Redlich. </author> <title> Statistical approach to shape from shading: Reconstruction of 3d face surfaces from single 2d images. </title> <note> submitted to Neural Computation, </note> <year> 1995. </year>
Reference-contexts: Our image representation, relying on shape and texture vectors obtained through pixelwise correspondence, seems to be a significant extension which allows us to incorporate texture as well as shape seamlessly into a single framework. Work which is superficially similar but is based on different representations includes <ref> [ Atick et al., 1995 ] </ref> who use 3D data taken from a Cyberware scanner and their linear combinations (without correspondence) to build a model.
Reference: [ Bergen and Hingorani, 1990 ] <author> J.R. Bergen and R. Hingorani. </author> <title> Hierarchical motion-based frame rate conversion. </title> <type> Technical report, </type> <institution> David Sarnoff Research Center, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Beymer modeled texture the same way we do as a linear combination of prototypical textures. Shape, however, was not constrained to be a linear combination of prototypical shapes and was instead estimated with an optical flow algorithm <ref> [ Bergen and Hingorani, 1990 ] </ref> which was used to map each novel image to the reference face of the model. <p> The other was from David Beymer, formerly of the MIT AI Lab [ Beymer, 1996 ] . The correspondences for the Vetter-Troje face database were computed automatically by using an algorithm which relied on an optical flow algorithm implemented by Bergen and Hingorani <ref> [ Bergen and Hingorani, 1990 ] </ref> . Of the 130 faces, 100 were successfully put in correspondence using this algorithm.
Reference: [ Besl and Jain, 1985 ] <author> Paul J. Besl and Ramesh C. Jain. </author> <title> Three-dimensional object recognition. </title> <journal> Computing Surveys, </journal> <volume> 17(1) </volume> <pages> 75-145, </pages> <year> 1985. </year>
Reference-contexts: Many approaches have been proposed. Several of them represent objects using 3D models, represented in different ways (for a review, see <ref> [ Besl and Jain, 1985 ] </ref> ). Such models are typically quite sophisticated, difficult to build and hard to use for many applications image matching in particular. A rather different approach is suggested by recent results in the science of biological vision.
Reference: [ Beymer and Poggio, 1995 ] <author> David Beymer and Tomaso Poggio. </author> <title> Face recognition from one ex ample view. </title> <journal> A.I. </journal> <volume> Memo 1536, </volume> <publisher> MIT, </publisher> <year> 1995. </year> <month> 31 </month>
Reference-contexts: Using pixelwise correspondence [ Vetter and Poggio, 1995 ] and <ref> [ Beymer and Poggio, 1995 ] </ref> showed that a good approximation of a new face image can be obtained with as few as 50 base faces, suggesting a low dimensionality for both the shape and the texture spaces. <p> Once a novel face is approximated by the linear model it is effectively in correspondence with the model and each of the prototypes. Two novel images can be therefore set in correspondence in this way. A very similar correspondence technique was successfully used by <ref> [ Beymer and Poggio, 1995 ] </ref> in a real-time vectorizing step needed for the generation of virtual views.
Reference: [ Beymer and Poggio, 1996 ] <author> David Beymer and Tomaso Poggio. </author> <title> Image representations for visual learning. </title> <journal> Science, </journal> <volume> 272 </volume> <pages> 1905-1909, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The "linear class" idea of [ Poggio and Vetter, 1992 ] and [ Vetter and Poggio, 1995 ] together with the image representation, based on pixelwise correspondence, used by [ Beymer et al., 1993 ] (see <ref> [ Beymer and Poggio, 1996 ] </ref> ) is the main motivation for our work. Poggio and Vetter introduced the idea of linear combinations of views to define and model classes of objects.
Reference: [ Beymer et al., 1993 ] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <journal> A.I. </journal> <volume> Memo 1431, </volume> <publisher> MIT, </publisher> <year> 1993. </year>
Reference-contexts: In this paper, we present the full approach and its use for gray-value images. The "linear class" idea of [ Poggio and Vetter, 1992 ] and [ Vetter and Poggio, 1995 ] together with the image representation, based on pixelwise correspondence, used by <ref> [ Beymer et al., 1993 ] </ref> (see [ Beymer and Poggio, 1996 ] ) is the main motivation for our work. Poggio and Vetter introduced the idea of linear combinations of views to define and model classes of objects. <p> Starting with <ref> [ Beymer et al., 1993 ] </ref> , we have attempted to avoid the computation of sparse features while keeping as much as possible the representation of Ullman and Basri which has at least under some conditions the algebraic structure of a vector space. <p> As another example, consider a set of prototypes for faces which includes faces with different expressions (smiling, frowning, angry, sad, etc), as in <ref> [ Beymer et al., 1993 ] </ref> . After matching a novel image of a face, the coefficients of the model can be used to determine if the input face is smiling, frowning, etc.
Reference: [ Beymer, 1995 ] <author> David Beymer. </author> <title> Vectorizing face images by interleaving shape and texture com putations. </title> <journal> A.I. </journal> <volume> Memo 1537, </volume> <publisher> MIT, </publisher> <year> 1995. </year>
Reference-contexts: In this paper we develop the approach in detail and show its performance not only on line drawings but also on gray-level images. Among papers directly related to ours, <ref> [ Beymer, 1995 ] </ref> addressed the problem of modeling human faces in order to generate "virtual" faces to be used in a face recognition system. A virtual view is a synthetically generated image of a face with a novel pose or expression.
Reference: [ Beymer, 1996 ] <author> David Beymer. </author> <title> Pose-Invariant Face Recognition Using Real and Virtual Views. </title> <type> PhD thesis, </type> <institution> Massachussetts Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: Two different databases of faces were used. One was from Thomas Vetter and Nikolaus Troje of the Max Planck Institute in Tubingen, Germany [ Troje and Bulthoff, 1995 ] . The other was from David Beymer, formerly of the MIT AI Lab <ref> [ Beymer, 1996 ] </ref> . The correspondences for the Vetter-Troje face database were computed automatically by using an algorithm which relied on an optical flow algorithm implemented by Bergen and Hingorani [ Bergen and Hingorani, 1990 ] .
Reference: [ Blake and Isard, 1994 ] <author> Andrew Blake and Michael Isard. </author> <title> 3d position, attitude and shape input using video tracking of hands and lips. </title> <booktitle> Computer Graphics Proceedings, </booktitle> <pages> pages 185-192, </pages> <year> 1994. </year>
Reference-contexts: Many other flexible models have been proposed. We mention here the model of Blake and Issard <ref> [ Blake and Isard, 1994 ] </ref> , which is similar in spirit to ours. <p> Another possible application in the image analysis domain is lip reading. A model of lips can be built from examples. This model can then be used to track lips in a sequence of images (see also <ref> [ Blake and Isard, 1994 ] </ref> ). A mapping from model parameters to phonemes can be learned to read the lips.
Reference: [ Bulthoff et al., 1995 ] <author> H. H. Bulthoff, S. Y. Edelman, and M. J. Tarr. </author> <title> How are three-dimensional objects represented in the brain? Cerebral Cortex, </title> <booktitle> 5(3) </booktitle> <pages> 247-260, </pages> <year> 1995. </year>
Reference-contexts: now convincing psychophysical and even physiological evidence suggesting that the human visual system often uses strategies that have the flavor of object representations based on 2D rather than 3D models ( [ Edelman and Bulthoff, 1990 ] ; [ Sinha, 1995 ] ; [ Logothetis et al., 1995 ] ; <ref> [ Bulthoff et al., 1995 ] </ref> ; [ Pauls et al., 1996 ] ). With this motivation, we have explored an approach in which object models are learned from several prototypical 2D images.
Reference: [ Burt and Adelson, 1983 ] <author> Peter J. Burt and Edward J. Adelson. </author> <title> The laplacian pyramid as a compact image code. </title> <journal> IEEE Transactions on Communications, </journal> <volume> COM-31(4):532-540, </volume> <year> 1983. </year>
Reference: [ Burt, 1984 ] <author> Peter J. Burt. </author> <title> The pyramid as a structure for efficient computation. </title> <booktitle> In Multi Resolution Image Processing and Analysis, </booktitle> <pages> pages 6-37. </pages> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Our matching algorithm uses a coarse-to-fine approach to improve robustness ( [ Burt and Adel-son, 1983 ] ; <ref> [ Burt, 1984 ] </ref> ) by creating a pyramid of images with each level of the pyramid containing an image that is one fourth the size of the one below.
Reference: [ Choi et al., 1991 ] <author> Chang Seok Choi, Toru Okazaki, Hiroshi Harashima, and Tsuyoshi Takebe. </author> <title> A system of analyzing and synthesizing facial images. </title> <journal> IEEE, </journal> <pages> pages 2665-2668, </pages> <year> 1991. </year>
Reference: [ Cootes and Taylor, 1992 ] <author> T.F. Cootes and C.J. Taylor. </author> <title> Active shape models - 'smart snakes'. </title> <booktitle> British Machine Vision Conference, </booktitle> <pages> pages 266-275, </pages> <year> 1992. </year>
Reference-contexts: In his study of illumination invariant recognition techniques, Hallinan ( [ Hallinan, 1995 ] ) describes deformable models of a similar general flavor. The work of Taylor and coworkers ( <ref> [ Cootes and Taylor, 1992 ] </ref> ; [ Cootes and Taylor, 1994 ] ; [ Cootes et al., 1992 ] ; [ Cootes et al., 1994 ] ; [ Cootes et al., 1993 ] ; [ Hill et al., 1992 ] ; [ Lanitis et al., 1995 ] ) on active
Reference: [ Cootes and Taylor, 1994 ] <author> T.F. Cootes and C.J. Taylor. </author> <title> Using grey-level models to improve active shape model search. </title> <booktitle> International Conference on Pattern Recognition, </booktitle> <pages> pages 63-67, </pages> <year> 1994. </year>
Reference-contexts: In his study of illumination invariant recognition techniques, Hallinan ( [ Hallinan, 1995 ] ) describes deformable models of a similar general flavor. The work of Taylor and coworkers ( [ Cootes and Taylor, 1992 ] ; <ref> [ Cootes and Taylor, 1994 ] </ref> ; [ Cootes et al., 1992 ] ; [ Cootes et al., 1994 ] ; [ Cootes et al., 1993 ] ; [ Hill et al., 1992 ] ; [ Lanitis et al., 1995 ] ) on active shape models is probably the closest to
Reference: [ Cootes et al., 1992 ] <author> T.F. Cootes, C.J. Taylor, D.H. Cooper, and J. Graham. </author> <title> Training models of shape from sets of examples. </title> <booktitle> British Machine Vision Conference, </booktitle> <pages> pages 9-18, </pages> <year> 1992. </year>
Reference-contexts: In his study of illumination invariant recognition techniques, Hallinan ( [ Hallinan, 1995 ] ) describes deformable models of a similar general flavor. The work of Taylor and coworkers ( [ Cootes and Taylor, 1992 ] ; [ Cootes and Taylor, 1994 ] ; <ref> [ Cootes et al., 1992 ] </ref> ; [ Cootes et al., 1994 ] ; [ Cootes et al., 1993 ] ; [ Hill et al., 1992 ] ; [ Lanitis et al., 1995 ] ) on active shape models is probably the closest to ours.
Reference: [ Cootes et al., 1993 ] <author> T.F. Cootes, C.J. Taylor, A. Lanitis, D.H. Cooper, and J. Graham. </author> <title> Building and using flexible models incorporating grey-level information. </title> <booktitle> In ICCV, </booktitle> <pages> pages 242-246, </pages> <address> Berlin, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The work of Taylor and coworkers ( [ Cootes and Taylor, 1992 ] ; [ Cootes and Taylor, 1994 ] ; [ Cootes et al., 1992 ] ; [ Cootes et al., 1994 ] ; <ref> [ Cootes et al., 1993 ] </ref> ; [ Hill et al., 1992 ] ; [ Lanitis et al., 1995 ] ) on active shape models is probably the closest to ours. It is based on the idea of linear combinations of prototypes to model non-rigid transformations within classes of objects.
Reference: [ Cootes et al., 1994 ] <author> T.F. Cootes, C.J. Taylor, and A. Lanitis. </author> <title> Multi-resolution search with active shape models. </title> <booktitle> International Conference on Pattern Recognition, </booktitle> <pages> pages 610-612, </pages> <year> 1994. </year>
Reference-contexts: The work of Taylor and coworkers ( [ Cootes and Taylor, 1992 ] ; [ Cootes and Taylor, 1994 ] ; [ Cootes et al., 1992 ] ; <ref> [ Cootes et al., 1994 ] </ref> ; [ Cootes et al., 1993 ] ; [ Hill et al., 1992 ] ; [ Lanitis et al., 1995 ] ) on active shape models is probably the closest to ours.
Reference: [ Edelman and Bulthoff, 1990 ] <author> Shimon Edelman and Heinrich Bulthoff. </author> <title> Viewpoint-specific rep resentations in three dimensional object recognition. </title> <journal> A.I. </journal> <volume> Memo 1239, </volume> <publisher> MIT, </publisher> <year> 1990. </year>
Reference-contexts: A rather different approach is suggested by recent results in the science of biological vision. There is now convincing psychophysical and even physiological evidence suggesting that the human visual system often uses strategies that have the flavor of object representations based on 2D rather than 3D models ( <ref> [ Edelman and Bulthoff, 1990 ] </ref> ; [ Sinha, 1995 ] ; [ Logothetis et al., 1995 ] ; [ Bulthoff et al., 1995 ] ; [ Pauls et al., 1996 ] ).
Reference: [ Ezzat, 1996 ] <author> Tony Ezzat. </author> <title> Example-based image analysis and synthesis for images of human faces. </title> <type> Master's thesis, </type> <institution> Massachussetts Institute of Technology, </institution> <year> 1996. </year> <month> 32 </month>
Reference-contexts: Shape, however, was not constrained to be a linear combination of prototypical shapes and was instead estimated with an optical flow algorithm [ Bergen and Hingorani, 1990 ] which was used to map each novel image to the reference face of the model. Ezzat ( <ref> [ Ezzat, 1996 ] </ref> ) uses the same image representation and linear combinations of vectorized images that we use in order to build a model for synthesis and analysis of novel views of a specific face. [ Vetter and Poggio, 1995 ] combined a linear model of shape and texture vectors
Reference: [ Hallinan, 1995 ] <author> Peter Winthrop Hallinan. </author> <title> A Deformable Model for the Recognition of Human Faces Under Arbitrary Illumination. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1995. </year>
Reference-contexts: In his study of illumination invariant recognition techniques, Hallinan ( <ref> [ Hallinan, 1995 ] </ref> ) describes deformable models of a similar general flavor. <p> We are currently investigating techniques 29 for handling different lighting conditions by using preprocessing filtering steps and more importantly- by adding prototypes taken under different lighting conditions ( [ Shashua, 1992a ] , <ref> [ Hallinan, 1995 ] </ref> ). In addition, there are two major directions in which we plan to extend this work. One is to decompose a model into simpler components and thus create hierarchical models built from components.
Reference: [ Hill et al., 1992 ] <author> A. Hill, T.F. Cootes, and C.J. Taylor. </author> <title> A generic system for image interpretation using flexible templates. </title> <booktitle> British Machine Vision Conference, </booktitle> <pages> pages 276-285, </pages> <year> 1992. </year>
Reference-contexts: The work of Taylor and coworkers ( [ Cootes and Taylor, 1992 ] ; [ Cootes and Taylor, 1994 ] ; [ Cootes et al., 1992 ] ; [ Cootes et al., 1994 ] ; [ Cootes et al., 1993 ] ; <ref> [ Hill et al., 1992 ] </ref> ; [ Lanitis et al., 1995 ] ) on active shape models is probably the closest to ours. It is based on the idea of linear combinations of prototypes to model non-rigid transformations within classes of objects.
Reference: [ Jones and Poggio, 1995 ] <author> Michael Jones and Tomaso Poggio. </author> <title> Model-based matching of line draw ings by linear combinations of prototypes. </title> <booktitle> In Proceedings of the Fifth International Conference on Computer Vision, </booktitle> <pages> pages 531-536, </pages> <year> 1995. </year>
Reference-contexts: But how can we use it to analyze novel images? In this paper we provide an answer in terms of a novel algorithm for matching the flexible model to a novel image (for a preliminary and partial version see <ref> [ Jones and Poggio, 1995 ] </ref> ). The paper is organized as follows. First, we discuss related work. In section 3 we explain in detail our model. Section 4 describes the matching algorithm. Section 5 shows example applications and presents experiments on the robustness of the matching algorithm. <p> Next, section 7 discusses a number of applications. Extensions and future work are considered in section 8. Section 9 concludes with a summary and discussion. 1 2 Related Work A preliminary version of this paper is <ref> [ Jones and Poggio, 1995 ] </ref> which only presented applications to line drawings. In this paper, we present the full approach and its use for gray-value images. <p> Furthermore, other object transformations (such as the changing expression of a face) can be approximated by linear transformations. In particular, they used their linear model to generate new virtual views of an object from a single (example) view. Jones and Poggio ( <ref> [ Jones and Poggio, 1995 ] </ref> ) sketched a novel approach to match linear models to novel images that can be used for several visual analysis tasks, including recognition.
Reference: [ Kirby and Sirovich, 1990 ] <author> M. Kirby and L. Sirovich. </author> <title> The application of the karhunen-loeve procedure for the characterization of human faces. </title> <journal> IEEE, </journal> <volume> 12(1) </volume> <pages> 103-108, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: For a fixed viewpoint a specific 3 class of objects with a similar 3D structure, such as faces, seems to induce a texture vector space of relatively low dimensionality as shown indirectly by the results of <ref> [ Kirby and Sirovich, 1990 ] </ref> and more directly by [ Lanitis et al., 1995 ] .
Reference: [ Lanitis et al., 1995 ] <author> A. Lanitis, C.J. Taylor, and T.F. Cootes. </author> <title> A unified approach to coding and interpreting face images. </title> <booktitle> In ICCV, </booktitle> <pages> pages 368-373, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The work of Taylor and coworkers ( [ Cootes and Taylor, 1992 ] ; [ Cootes and Taylor, 1994 ] ; [ Cootes et al., 1992 ] ; [ Cootes et al., 1994 ] ; [ Cootes et al., 1993 ] ; [ Hill et al., 1992 ] ; <ref> [ Lanitis et al., 1995 ] </ref> ) on active shape models is probably the closest to ours. It is based on the idea of linear combinations of prototypes to model non-rigid transformations within classes of objects. <p> For a fixed viewpoint a specific 3 class of objects with a similar 3D structure, such as faces, seems to induce a texture vector space of relatively low dimensionality as shown indirectly by the results of [ Kirby and Sirovich, 1990 ] and more directly by <ref> [ Lanitis et al., 1995 ] </ref> .
Reference: [ Logothetis et al., 1995 ] <author> N. Logothetis, J. Pauls, and T. Poggio. </author> <title> Shape Representation in the Inferior Temporal Cortex of Monkeys. </title> <booktitle> Current Biology, </booktitle> <volume> 5(5) </volume> <pages> 552-563, </pages> <year> 1995. </year>
Reference-contexts: There is now convincing psychophysical and even physiological evidence suggesting that the human visual system often uses strategies that have the flavor of object representations based on 2D rather than 3D models ( [ Edelman and Bulthoff, 1990 ] ; [ Sinha, 1995 ] ; <ref> [ Logothetis et al., 1995 ] </ref> ; [ Bulthoff et al., 1995 ] ; [ Pauls et al., 1996 ] ). With this motivation, we have explored an approach in which object models are learned from several prototypical 2D images.
Reference: [ Murase and Nayar, 1995 ] <author> Hiroshi Murase and Shree K. Nayar. </author> <title> Visual learning and recognition of 3-d objects from appearance. </title> <journal> International Journal of Computer Vision, </journal> <pages> pages 5-24, </pages> <year> 1995. </year>
Reference-contexts: Turk and Pentland's [ Turk and Pentland, 1991 ] and also Nayar and Murase's <ref> [ Murase and Nayar, 1995 ] </ref> use of eigenimages and therefore of a model which is a linear combination of example images is quite different from ours since the basic representation is very different.
Reference: [ Nastar et al., 1996 ] <author> Chahab Nastar, Baback Moghaddam, and Alex Pentland. </author> <title> Generalized im age matching: </title> <journal> Statistical learning of physically-based deformations. </journal> <note> In ECCV, page to appear, </note> <institution> Cambridge, UK, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: Many other flexible models have been proposed. We mention here the model of Blake and Issard [ Blake and Isard, 1994 ] , which is similar in spirit to ours. Finally, a very recent paper by Nastar, Moghaddam and Pentland <ref> [ Nastar et al., 1996 ] </ref> , which was published after the work described in this paper was completed, has intriguing similarities with our work, since it is based on the computation of correspondence between images, albeit with an algorithm different from the optical flow algorithm we use. 3 Modeling Classes
Reference: [ Pauls et al., 1996 ] <author> J. Pauls, E. Bricolo, </author> <title> and N.K. Logothetis. Physiological evidence for viewer centered representation in the monkey. </title> <editor> In S. Nayar and T. Poggio, editors, </editor> <title> Early Visual Learning. </title> <publisher> Oxford University Press, </publisher> <year> 1996. </year>
Reference-contexts: suggesting that the human visual system often uses strategies that have the flavor of object representations based on 2D rather than 3D models ( [ Edelman and Bulthoff, 1990 ] ; [ Sinha, 1995 ] ; [ Logothetis et al., 1995 ] ; [ Bulthoff et al., 1995 ] ; <ref> [ Pauls et al., 1996 ] </ref> ). With this motivation, we have explored an approach in which object models are learned from several prototypical 2D images.
Reference: [ Poggio and Beymer, 1996 ] <author> Tomaso Poggio and David Beymer. </author> <title> Learning to see. </title> <journal> IEEE Spectrum, </journal> <pages> pages 60-69, </pages> <year> 1996. </year>
Reference-contexts: For images considered as bitmaps, on the other hand, basic vector space operations like addition and linear combination are not meaningful. We have argued therefore that a better way to represent images is to associate with each image a shape vector and a texture vector (see for a review <ref> [ Poggio and Beymer, 1996 ] </ref> ). The shape vector of an example image associates to each pixel in the reference image the coordinates of the corresponding point in the example image. <p> As reviewed by <ref> [ Poggio and Beymer, 1996 ] </ref> correspondence and the resulting vector structure underlie many of the recent view-based approaches to recognition and detection either implicitly or explicitly. <p> We discuss a few of these briefly below. 26 correspondences. 27 7.1 Example-based correspondence The original application of the technique described here is to "vectorize" a novel image <ref> [ Poggio and Beymer, 1996 ] </ref> , providing dense correspondence between two images of a known class of objects. Once a novel face is approximated by the linear model it is effectively in correspondence with the model and each of the prototypes.
Reference: [ Poggio and Brunelli, 1992 ] <author> Tomaso Poggio and Roberto Brunelli. </author> <title> A novel approach to graphics. </title> <journal> A.I. </journal> <volume> Memo 1354, </volume> <publisher> MIT, </publisher> <year> 1992. </year>
Reference-contexts: Recently we have become aware of several papers dealing with various forms of the idea of linear combination of prototypical images. Choi et. al. (1991) were perhaps the first (see also <ref> [ Poggio and Brunelli, 1992 ] </ref> ) to suggest a model which represented face images with separate shape and texture components, using a 3D model to provide correspondences between example face images.
Reference: [ Poggio and Vetter, 1992 ] <author> Tomaso Poggio and Thomas Vetter. </author> <title> Recognition and structure from one 2d model view: Observations on prototypes, object classes and symmetries. </title> <journal> A.I. </journal> <volume> Memo 1347, </volume> <publisher> MIT, </publisher> <year> 1992. </year>
Reference-contexts: In this paper, we present the full approach and its use for gray-value images. The "linear class" idea of <ref> [ Poggio and Vetter, 1992 ] </ref> and [ Vetter and Poggio, 1995 ] together with the image representation, based on pixelwise correspondence, used by [ Beymer et al., 1993 ] (see [ Beymer and Poggio, 1996 ] ) is the main motivation for our work. <p> intriguing similarities with our work, since it is based on the computation of correspondence between images, albeit with an algorithm different from the optical flow algorithm we use. 3 Modeling Classes of Ob jects The work of Ullman and Basri [ Ullman and Basri, 1991 ] and Poggio and Vetter <ref> [ Poggio and Vetter, 1992 ] </ref> was based on a representation of images as vectors of the x; y positions of a small number of labeled features and left open the question of how to find them reliably and accurately. <p> As reviewed by [ Poggio and Beymer, 1996 ] correspondence and the resulting vector structure underlie many of the recent view-based approaches to recognition and detection either implicitly or explicitly. Certain special object classes (such as cuboids and symmetric objects) can be proved to be exactly linear classes (see <ref> [ Poggio and Vetter, 1992 ] </ref> ).
Reference: [ Poggio, 1990 ] <author> Tomaso Poggio. </author> <title> A theory of how the brain might work. </title> <journal> A.I. </journal> <volume> Memo 1253, </volume> <publisher> MIT, </publisher> <year> 1990. </year>
Reference-contexts: The shape vectors resulting from different orthographic views of a single 3D object (in which features are always visible) constitute a linear vector subspace of very low dimensionality spanned by just two views ( [ Ullman and Basri, 1991 ] ; see also <ref> [ Poggio, 1990 ] </ref> ).
Reference: [ Shashua, 1992a ] <author> Amnon Shashua. </author> <title> Geometry and Photometry in 3D Visual Recognition. </title> <type> PhD thesis, </type> <institution> Massachussetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: They must be made more robust for changes in imaging parameters, in particular to changes in illumination and camera properties. We are currently investigating techniques 29 for handling different lighting conditions by using preprocessing filtering steps and more importantly- by adding prototypes taken under different lighting conditions ( <ref> [ Shashua, 1992a ] </ref> , [ Hallinan, 1995 ] ). In addition, there are two major directions in which we plan to extend this work. One is to decompose a model into simpler components and thus create hierarchical models built from components.
Reference: [ Shashua, 1992b ] <author> Amnon Shashua. </author> <title> Projective structure from two uncalibrated images: Structure from motion and recognition. </title> <journal> A.I. </journal> <volume> Memo 1363, </volume> <publisher> MIT, </publisher> <year> 1992. </year> <month> 33 </month>
Reference-contexts: Poggio and Vetter introduced the idea of linear combinations of views to define and model classes of objects. They were inspired in turn by the results of [ Ullman and Basri, 1991 ] and <ref> [ Shashua, 1992b ] </ref> who showed that linear combinations of three views of a single object may be used to obtain any other views of the object (barring self-occlusion and assuming orthographic projection).
Reference: [ Sinha, 1995 ] <author> P. Sinha. </author> <title> Perceiving and recognizing 3D forms. </title> <type> PhD thesis, </type> <institution> Massachussetts Insti tute of Technology, </institution> <year> 1995. </year>
Reference-contexts: There is now convincing psychophysical and even physiological evidence suggesting that the human visual system often uses strategies that have the flavor of object representations based on 2D rather than 3D models ( [ Edelman and Bulthoff, 1990 ] ; <ref> [ Sinha, 1995 ] </ref> ; [ Logothetis et al., 1995 ] ; [ Bulthoff et al., 1995 ] ; [ Pauls et al., 1996 ] ). With this motivation, we have explored an approach in which object models are learned from several prototypical 2D images.
Reference: [ Troje and Bulthoff, 1995 ] <author> N. Troje and H.H. Bulthoff. </author> <title> Face recognition under varying pose: The role of texture and shape. </title> <journal> Vision Research, </journal> <volume> 36(12) </volume> <pages> 1761-1771, </pages> <year> 1995. </year>
Reference-contexts: The first was the class of frontal views of human faces. Two different databases of faces were used. One was from Thomas Vetter and Nikolaus Troje of the Max Planck Institute in Tubingen, Germany <ref> [ Troje and Bulthoff, 1995 ] </ref> . The other was from David Beymer, formerly of the MIT AI Lab [ Beymer, 1996 ] .
Reference: [ Turk and Pentland, 1991 ] <author> M.A. Turk and A.P Pentland. </author> <title> Face recognition using eigenfaces. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 586-591, </pages> <year> 1991. </year>
Reference-contexts: Work which is superficially similar but is based on different representations includes [ Atick et al., 1995 ] who use 3D data taken from a Cyberware scanner and their linear combinations (without correspondence) to build a model. Turk and Pentland's <ref> [ Turk and Pentland, 1991 ] </ref> and also Nayar and Murase's [ Murase and Nayar, 1995 ] use of eigenimages and therefore of a model which is a linear combination of example images is quite different from ours since the basic representation is very different. <p> To define a model, a set of example images called prototypes are given. We denote these prototypes as I 0 ; I 1 ; : : : ; I N . A naive approach might model this class of images using a linear combination of the images ( <ref> [ Turk and Pentland, 1991 ] </ref> ) as follows: I model = N X b i I i : (1) This approach does not result in good matches as shown in figure 1. The underlying reason for this method's poor performance is that the example images are not in correspondence. <p> They are significantly better than in the case without correspondences. As another example of the better match quality obtained by our image representation, consider the case of frontal views of faces. The eigenface model of <ref> [ Turk and Pentland, 1991 ] </ref> was computed from the face database #1 shown in figure 2. These faces are aligned using the center of the face. One hundred face prototypes were used to define the model and all 100 eigenvectors were used in the eigenface representation.
Reference: [ Ullman and Basri, 1991 ] <author> S. Ullman and R. Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 992-1006, </pages> <year> 1991. </year>
Reference-contexts: Poggio and Vetter introduced the idea of linear combinations of views to define and model classes of objects. They were inspired in turn by the results of <ref> [ Ullman and Basri, 1991 ] </ref> and [ Shashua, 1992b ] who showed that linear combinations of three views of a single object may be used to obtain any other views of the object (barring self-occlusion and assuming orthographic projection). <p> after the work described in this paper was completed, has intriguing similarities with our work, since it is based on the computation of correspondence between images, albeit with an algorithm different from the optical flow algorithm we use. 3 Modeling Classes of Ob jects The work of Ullman and Basri <ref> [ Ullman and Basri, 1991 ] </ref> and Poggio and Vetter [ Poggio and Vetter, 1992 ] was based on a representation of images as vectors of the x; y positions of a small number of labeled features and left open the question of how to find them reliably and accurately. <p> The shape and texture vectors form separate linear vector spaces with specific properties. The shape vectors resulting from different orthographic views of a single 3D object (in which features are always visible) constitute a linear vector subspace of very low dimensionality spanned by just two views ( <ref> [ Ullman and Basri, 1991 ] </ref> ; see also [ Poggio, 1990 ] ).
Reference: [ Vetter and Poggio, 1995 ] <author> Thomas Vetter and Tomaso Poggio. </author> <title> Linear object classes and image synthesis from a single example image. </title> <journal> A.I. </journal> <volume> Memo 1531, </volume> <publisher> MIT, </publisher> <year> 1995. </year>
Reference-contexts: In this paper, we present the full approach and its use for gray-value images. The "linear class" idea of [ Poggio and Vetter, 1992 ] and <ref> [ Vetter and Poggio, 1995 ] </ref> together with the image representation, based on pixelwise correspondence, used by [ Beymer et al., 1993 ] (see [ Beymer and Poggio, 1996 ] ) is the main motivation for our work. <p> Ezzat ( [ Ezzat, 1996 ] ) uses the same image representation and linear combinations of vectorized images that we use in order to build a model for synthesis and analysis of novel views of a specific face. <ref> [ Vetter and Poggio, 1995 ] </ref> combined a linear model of shape and texture vectors to generate virtual views across large viewpoint changes. Recently we have become aware of several papers dealing with various forms of the idea of linear combination of prototypical images. <p> Using pixelwise correspondence <ref> [ Vetter and Poggio, 1995 ] </ref> and [ Beymer and Poggio, 1995 ] showed that a good approximation of a new face image can be obtained with as few as 50 base faces, suggesting a low dimensionality for both the shape and the texture spaces.
Reference: [ Vetter et al., 1996 ] <author> Thomas Vetter, Michael Jones, and Tomaso Poggio. </author> <title> A bootstrapping algo rithm for learning linearized models of object classes. </title> <journal> A.I. </journal> <note> Memo submitted, </note> <institution> MIT, </institution> <year> 1996. </year>
Reference-contexts: Automating this process would be a great help to model building and also to give some biological plausibility to the class of flexible models introduced here. Recently, we had promising results with a bootstrapping technique for automatically finding correspondences for images in the same object class ( <ref> [ Vetter et al., 1996 ] </ref> , in preparation). The idea is first to determine the pixelwise correspondence between a reference image and just one other prototype using an automatic technique such as optical flow.
Reference: [ Viola, 1995 ] <author> Paul Viola. </author> <title> Alignment by maximization of mutual information. MIT A.I. </title> <type> Technical Report 1548, </type> <institution> MIT, </institution> <year> 1995. </year> <month> 34 </month>
Reference-contexts: The work of Viola <ref> [ Viola, 1995 ] </ref> on alignment of a model with a novel image by maximization of mutual information suggested the stochastic gradient descent algorithm we use in this paper for the matching stage. Many other flexible models have been proposed. <p> We use here the L 2 norm but other norms may also be appropriate (e.g. robust statistics). In order to minimize the error function any standard minimization algorithm could be used. We have chosen to use the stochastic gradient descent algorithm <ref> [ Viola, 1995 ] </ref> because it is fast and can avoid remaining trapped in local minima. The summation in equation 7 is over all pixels in the model image. <p> The summation in equation 7 is over all pixels in the model image. The idea of stochastic gradient descent is to randomly sample a small set of pixels from the image and only compute the gradient at those pixels. This gives an estimate for the true gradient. As Viola <ref> [ Viola, 1995 ] </ref> discusses, this estimate for the gradient will be good enough to use for optimizing the parameters if the gradient estimate is unbiased, the parameter update rate asymptotically converges to zero and the error surface is smooth.
References-found: 42

