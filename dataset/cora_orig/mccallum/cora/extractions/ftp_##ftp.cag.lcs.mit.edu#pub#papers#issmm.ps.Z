URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/issmm.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/issmm.html
Root-URL: 
Title: Latency Tolerance through Multithreading in Large-Scale Multiprocessors  
Author: Kiyoshi Kurihara David Chaiken, and Anant Agarwal 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: In large-scale distributed-memory multiprocessors, remote memory accesses suffer significant latencies. Caches help alleviate the memory latency problem by maintaining local copies of frequently used data. However, they cannot eliminate the latency caused by first-time references and invalidations needed to enforce cache coherence. Multithreaded processors tolerate such latencies by rapidly switching between threads when they encounter cache misses. This paper evaluates the effectiveness of multi-threading in Alewife, a scalable multiprocessor that is being developed at MIT. For the applications used in this study, multithreading results in a modest 20% improvement in execution time on a 64-processor machine. The impact of multithreading is expected to be far more significant in larger machines, when remote memory latency becomes a dominant term in the performance equation. fl Kiyoshi Kurihara is currently at IBM Japan, Ltd. Tokyo, Japan.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The implementation of such an architecture requires multiple register sets or some other mechanism to allow fast context switches, additional network bandwidth, support logic in the cache controller, and extra complexity in the thread scheduling mechanism. Other methods for allowing multiple outstanding requests, such as weak ordering <ref> [1, 8, 10] </ref>, incur similar implementation complexities in the cache controller. In Alewife, since the same context-switching mechanism is used for fast traps and for masking synchronization latencies, we feel the extra complexity in the processor is justified. <p> This trend indicates that the effect of mul-tithreading becomes more significant as system size increases. 5.4 Comparison to Weak Ordering In order to evaluate the contribution of multithread-ing, we compare the performance of our multi-threaded system to the performance of a system with weak ordering <ref> [1, 8, 10] </ref>. Weak ordering is an alternate method of tolerating remote memory access latency by allowing multiple outstanding transactions per processor.
Reference: [2] <author> Anant Agarwal. </author> <title> Performance Tradeoffs in Mul-tithreaded Processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1991. To appear. </note>
Reference-contexts: In Alewife, since the same context-switching mechanism is used for fast traps and for masking synchronization latencies, we feel the extra complexity in the processor is justified. See <ref> [2] </ref> for a detailed analysis of the interaction of multithreading with cache interference, network contention, context-switching overhead, and data-sharing effects. 4 The Simulation System We use ASIM, the Alewife machine simulator, to estimate the extent to which multiple contexts can 4 Statistics Trace File Post-Mortem Scheduler System Simulator overlap communication latency. <p> Table 3 shows the effect of multithreading on the average network channel utilization and the average packet transit latency. The higher bandwidth demands, evident from the increased channel utilizations, cause larger packet communication latencies. However, the speed-up results validate the prediction in <ref> [2] </ref>: as long as the network does not approach saturation, the benefits of tolerating remote access latency outweigh the increase in packet communication latency.
Reference: [3] <author> Anant Agarwal, Beng-Hong Lim, David A. Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: While previous architectures have implemented multithreading with cycle-by-cycle interleaving of instructions from different processes [11, 12, 16, 21] (termed fine multithreading), we use the same name for systems that interleave blocks of instructions from different processes as well <ref> [3, 23] </ref> (termed coarse or block multithreading). Block multithreaded processors do not force a context switch every cycle and can achieve high single-thread performance. They switch between threads only on long-latency memory requests or synchronization attempts. <p> Block multithreaded processors do not force a context switch every cycle and can achieve high single-thread performance. They switch between threads only on long-latency memory requests or synchronization attempts. We are using such a block multithreaded processor architecture <ref> [3] </ref> in the 1 Time Idle Idle Mem Req 1 Resp1 Mem Req 2 Resp2 Alewife design. Alewife is a large-scale multiprocessor being implemented at MIT; it is described in more detail in the next section. <p> Context switching on a cycle by-cycle basis reduces system performance on inherently sequential portions of an application. It is possible to provide reasonable performance on sequential code while still enjoying the merits of multithreading for highly parallel sections of applications. In Alewife's processor architecture, APRIL <ref> [3] </ref>, context switches occur only when a thread executes a memory request that must be serviced by a remote node in the multiprocessor. As long as a context's memory requests hit in the cache or can be serviced by a local memory module, the context continues to execute. <p> The trap routine saves the Program Counter (PC) and Processor Status Register (PSR), switches register sets by setting the Frame Pointer (FP) register, flushes the pipeline, and switches to the next context by setting the Frame Pointer (FP) register to point to a new register window. <ref> [3] </ref> shows that even with a low-cost implementation, a context switch can be done in about 11 cycles. By maintaining a separate PC and PSR for each context, a custom processor could switch contexts faster than our current implementation.
Reference: [4] <author> Anant Agarwal, Richard Simoni, John Hen-nessy, and Mark Horowitz. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th International Symposium on Computer Architecture, IEEE, </booktitle> <address> New York, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Alewife's cache controllers synthesize a globally shared memory address space. It uses a directory-based cache coherence scheme called the LimitLESS protocol [7], which realizes the performance of full-map directory protocols [5, 22], with the memory overhead of a limited directory protocol <ref> [4] </ref>. The directory used by the cache coherence protocol is also distributed to the processing nodes. Latency tolerance in Alewife is part of a layered approach to automatic management of multiprocessor communication locality. Several components in the Alewife system cooperate in automatic minimization of latency.
Reference: [5] <author> Lucien M. Censier and Paul Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December </month> <year> 1978. </year>
Reference-contexts: Alewife's cache controllers synthesize a globally shared memory address space. It uses a directory-based cache coherence scheme called the LimitLESS protocol [7], which realizes the performance of full-map directory protocols <ref> [5, 22] </ref>, with the memory overhead of a limited directory protocol [4]. The directory used by the cache coherence protocol is also distributed to the processing nodes. Latency tolerance in Alewife is part of a layered approach to automatic management of multiprocessor communication locality.
Reference: [6] <author> David Chaiken, Craig Fields, Kiyoshi Kuri-hara, and Anant Agarwal. </author> <title> Directory-Based Cache-Coherence in Large-Scale Multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The scheduler simulates the synchronization behavior of the processor and synthesizes synchronization references according to prespecified waiting algorithms. The implementation in this study uses software combining trees for barrier synchronizations <ref> [6] </ref>. In general, a processor issues a memory request from its task only after its previous network request is satisfied. However, the simulator can also choose to switch to a different thread after a network request to model a multithreaded processor.
Reference: [7] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), ACM, </booktitle> <month> April </month> <year> 1991. </year> <note> To appear. </note>
Reference-contexts: The shared memory is distributed to the processing nodes so that the system does not suffer from the bandwidth bottleneck of a single, monolithic memory. Alewife's cache controllers synthesize a globally shared memory address space. It uses a directory-based cache coherence scheme called the LimitLESS protocol <ref> [7] </ref>, which realizes the performance of full-map directory protocols [5, 22], with the memory overhead of a limited directory protocol [4]. The directory used by the cache coherence protocol is also distributed to the processing nodes.
Reference: [8] <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The implementation of such an architecture requires multiple register sets or some other mechanism to allow fast context switches, additional network bandwidth, support logic in the cache controller, and extra complexity in the thread scheduling mechanism. Other methods for allowing multiple outstanding requests, such as weak ordering <ref> [1, 8, 10] </ref>, incur similar implementation complexities in the cache controller. In Alewife, since the same context-switching mechanism is used for fast traps and for masking synchronization latencies, we feel the extra complexity in the processor is justified. <p> This trend indicates that the effect of mul-tithreading becomes more significant as system size increases. 5.4 Comparison to Weak Ordering In order to evaluate the contribution of multithread-ing, we compare the performance of our multi-threaded system to the performance of a system with weak ordering <ref> [1, 8, 10] </ref>. Weak ordering is an alternate method of tolerating remote memory access latency by allowing multiple outstanding transactions per processor.
Reference: [9] <author> D. A. George. </author> <title> EPEX Environment for Parallel Execution. </title> <type> Technical Report RC 13158 (58851), </type> <institution> IBM T. J. Watson Research Center, Yorktown Heights, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: The SIMPLE application simulates the hydrodynamic and thermal behavior of a fluids in two dimensions, and the Weather application forecasts the state of the atmosphere given an initial state. Both SIMPLE and Weather are written in EPEX/FORTRAN <ref> [9, 20] </ref>, a version of FORTRAN that has been extended with parallel constructs at IBM. Table 1 summarizes the characteristics of these programs. The total number of memory references for each trace is the number of memory transactions that are issued in a uniprocessor execution of the trace.
Reference: [10] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The implementation of such an architecture requires multiple register sets or some other mechanism to allow fast context switches, additional network bandwidth, support logic in the cache controller, and extra complexity in the thread scheduling mechanism. Other methods for allowing multiple outstanding requests, such as weak ordering <ref> [1, 8, 10] </ref>, incur similar implementation complexities in the cache controller. In Alewife, since the same context-switching mechanism is used for fast traps and for masking synchronization latencies, we feel the extra complexity in the processor is justified. <p> This trend indicates that the effect of mul-tithreading becomes more significant as system size increases. 5.4 Comparison to Weak Ordering In order to evaluate the contribution of multithread-ing, we compare the performance of our multi-threaded system to the performance of a system with weak ordering <ref> [1, 8, 10] </ref>. Weak ordering is an alternate method of tolerating remote memory access latency by allowing multiple outstanding transactions per processor.
Reference: [11] <author> R.H. Halstead and T. Fujita. MASA: </author> <title> A Mul-tithreaded Processor Architecture for Parallel Symbolic Computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <publisher> IEEE, </publisher> <address> New York, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: This strategy attempts to hide the latency of interprocessor communication by allowing multiple outstanding transactions per processor. While previous architectures have implemented multithreading with cycle-by-cycle interleaving of instructions from different processes <ref> [11, 12, 16, 21] </ref> (termed fine multithreading), we use the same name for systems that interleave blocks of instructions from different processes as well [3, 23] (termed coarse or block multithreading). Block multithreaded processors do not force a context switch every cycle and can achieve high single-thread performance. <p> In order to hide long memory access latency and achieve high processor utilization, the system schedules the threads in round-robin fashion. Multithreading is also used in data flow machines [15], and some Lisp-oriented architectures <ref> [11] </ref>. These multithreaded architectures switch contexts after every instruction execution.
Reference: [12] <author> W. J. Kaminsky and E. S. Davidson. </author> <title> Developing a Multiple-Instruction-Stream Single-Chip Processor. </title> <booktitle> IEEE Computer, </booktitle> <pages> 66-78, </pages> <month> December </month> <year> 1979. </year>
Reference-contexts: This strategy attempts to hide the latency of interprocessor communication by allowing multiple outstanding transactions per processor. While previous architectures have implemented multithreading with cycle-by-cycle interleaving of instructions from different processes <ref> [11, 12, 16, 21] </ref> (termed fine multithreading), we use the same name for systems that interleave blocks of instructions from different processes as well [3, 23] (termed coarse or block multithreading). Block multithreaded processors do not force a context switch every cycle and can achieve high single-thread performance.
Reference: [13] <author> Manoj Kumar and Kimming So. </author> <title> Trace Driven Simulation for Studying MIMD Parallel Computers. </title> <booktitle> In International Conference on Parallel Computing, </booktitle> <address> pages I-68 - I-72, </address> <year> 1989. </year>
Reference-contexts: The dynamic post-mortem scheduler produces a parallel trace by simulating processors executing the task segments in the trace. Figure 5 illustrates its structure. Comparable simulators have been developed by other researchers <ref> [13] </ref>. The scheduler first makes a pass through the uniprocessor trace and constructs a task trace from the synchronization mark PE1 STATE PE64 STATE PE2 STATE PE3 STATE Cache Simulator Network Simulator Address Trace File Post-Mortem Scheduler Performance Statistics ers.
Reference: [14] <author> Leslie Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multi-process Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9), </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: When transactions cause the cache coherence protocol to issue invalidation messages in order to ensure sequential consistency <ref> [14] </ref>, the remote memory access latency is especially high. Transactions that require any invalidations do not complete until all the invalidations are complete.
Reference: [15] <author> G. M. Papadopoulos and D.E. Culler. Monsoon: </author> <title> An Explicit Token-Store Architecture. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Eight threads reside in each of the HEP's processors, and each thread uses its own dedicated register set. In order to hide long memory access latency and achieve high processor utilization, the system schedules the threads in round-robin fashion. Multithreading is also used in data flow machines <ref> [15] </ref>, and some Lisp-oriented architectures [11]. These multithreaded architectures switch contexts after every instruction execution.
Reference: [16] <author> B.J. Smith. </author> <title> A Pipelined, Shared Resource MIMD Computer. </title> <booktitle> In Proceedings of the 1978 International Conference on Parallel Processing, </booktitle> <pages> pages 6-8, </pages> <year> 1978. </year> <month> 10 </month>
Reference-contexts: This strategy attempts to hide the latency of interprocessor communication by allowing multiple outstanding transactions per processor. While previous architectures have implemented multithreading with cycle-by-cycle interleaving of instructions from different processes <ref> [11, 12, 16, 21] </ref> (termed fine multithreading), we use the same name for systems that interleave blocks of instructions from different processes as well [3, 23] (termed coarse or block multithreading). Block multithreaded processors do not force a context switch every cycle and can achieve high single-thread performance.
Reference: [17] <author> B.J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. </title> <booktitle> SPIE, </booktitle> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: The type of system depicted in Figure 3 is referred to as a multithreaded architecture. The idea of multithreading in multiprocessor systems is not new. The prototypical multithreaded architecture is the HEP <ref> [17] </ref>. Eight threads reside in each of the HEP's processors, and each thread uses its own dedicated register set. In order to hide long memory access latency and achieve high processor utilization, the system schedules the threads in round-robin fashion.
Reference: [18] <author> K. So, F. Darema-Rogers, D. A. George, V. A. Norton, and G. F. Pfister. </author> <title> PSIMUL A System for Parallel Simulation of Parallel Systems. </title> <type> Technical Report RC 11674 (58502), </type> <institution> IBM T. J. Watson Research Center, Yorktown Heights, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: The single processor trace is derived from a uniprocessor execution of an application parallelized using the single-processor-multiple-data (SPMD) computational model. Single processor traces are gathered using PSIMUL <ref> [18] </ref>, a system for tracing parallel applications on IBM S/370 machines. In the SPMD model, each code section (task) in the system ends with a synchronization event, typically a barrier.
Reference: [19] <author> SPARC Architecture Manual. </author> <year> 1988. </year> <institution> SUN Mi-crosystems, Mountain View, California. </institution>
Reference-contexts: This context switching policy allows a single thread to benefit from the maximum performance of the processor. Sparcle, the initial APRIL implementation, uses the register windows of the SPARC processor <ref> [19] </ref> to implement multithreading. With a small number of hardware modifications, the register window mechanism can be used to implement both the hardware contexts and the rapid context switch needed for mul-tithreading. Sparcle dedicates one register window to each thread.
Reference: [20] <author> J.M. Stone, F. Darema-Rogers, V.A. Norton, </author> <title> and G.F. Pfister. Introduction to the VM/EPEX FORTRAN Preprocessor. </title> <type> Technical Report RC 11407 (#51329), </type> <institution> IBM T. J. Watson Research Center, Yorktown Height, </institution> <address> NY, </address> <month> September </month> <year> 1985. </year>
Reference-contexts: The SIMPLE application simulates the hydrodynamic and thermal behavior of a fluids in two dimensions, and the Weather application forecasts the state of the atmosphere given an initial state. Both SIMPLE and Weather are written in EPEX/FORTRAN <ref> [9, 20] </ref>, a version of FORTRAN that has been extended with parallel constructs at IBM. Table 1 summarizes the characteristics of these programs. The total number of memory references for each trace is the number of memory transactions that are issued in a uniprocessor execution of the trace.
Reference: [21] <author> H. Sullivan and T. R. Bashkow. </author> <title> A Large Scale, Homogeneous, Fully Distributed Parallel Machine. </title> <booktitle> In Proceedings of the 4th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 105-117, </pages> <month> March </month> <year> 1977. </year>
Reference-contexts: This strategy attempts to hide the latency of interprocessor communication by allowing multiple outstanding transactions per processor. While previous architectures have implemented multithreading with cycle-by-cycle interleaving of instructions from different processes <ref> [11, 12, 16, 21] </ref> (termed fine multithreading), we use the same name for systems that interleave blocks of instructions from different processes as well [3, 23] (termed coarse or block multithreading). Block multithreaded processors do not force a context switch every cycle and can achieve high single-thread performance.
Reference: [22] <author> C. K. Tang. </author> <title> Cache Design in the Tightly Coupled Multiprocessor System. </title> <booktitle> In AFIPS Conference Proceedings, National Computer Conference, </booktitle> <address> NY, NY, </address> <pages> pages 749-753, </pages> <month> June </month> <year> 1976. </year>
Reference-contexts: Alewife's cache controllers synthesize a globally shared memory address space. It uses a directory-based cache coherence scheme called the LimitLESS protocol [7], which realizes the performance of full-map directory protocols <ref> [5, 22] </ref>, with the memory overhead of a limited directory protocol [4]. The directory used by the cache coherence protocol is also distributed to the processing nodes. Latency tolerance in Alewife is part of a layered approach to automatic management of multiprocessor communication locality.
Reference: [23] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Exploring the Benefits of Multiple Hardware Contexts in a Multiprocessor Architecture: Preliminary Results. </title> <booktitle> In Proceedings 16th Annual International Symposium on Computer Architecture, IEEE, </booktitle> <address> New York, </address> <month> June </month> <year> 1989. </year> <month> 11 </month>
Reference-contexts: While previous architectures have implemented multithreading with cycle-by-cycle interleaving of instructions from different processes [11, 12, 16, 21] (termed fine multithreading), we use the same name for systems that interleave blocks of instructions from different processes as well <ref> [3, 23] </ref> (termed coarse or block multithreading). Block multithreaded processors do not force a context switch every cycle and can achieve high single-thread performance. They switch between threads only on long-latency memory requests or synchronization attempts.
References-found: 23

