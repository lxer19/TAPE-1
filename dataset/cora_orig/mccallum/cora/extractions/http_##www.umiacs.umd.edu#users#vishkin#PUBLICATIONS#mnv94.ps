URL: http://www.umiacs.umd.edu/users/vishkin/PUBLICATIONS/mnv94.ps
Refering-URL: http://www.umiacs.umd.edu/users/vishkin/PUBLICATIONS/papers.html
Root-URL: 
Title: Trade-offs Between Communication Throughput and Parallel Time development of a few interesting techniques which can
Author: Yishay Mansour Noam Nisan Uzi Vishkin 
Address: Jerusalem,  Park, MD 20742;  
Affiliation: Dept. of Computer Science, Hebrew University,  Academy of Sciences. University of Maryland Institute for Advanced Computer Studies, and Department of Electrical Engineering, College  and Dept. of Computer Science, Tel Aviv University, Tel Aviv,  
Note: quires the  Israel; This work was supported by BSF grant 92-00043 and by a Wolfeson award administered by the Israeli  Israel; Partially supported by NSF grants CCR-8906949 and CCR-9111348.  
Abstract: We study the effect of limited communication throughput on parallel computation in a setting where the number of processors is much smaller than the length of the input. Our model has p processors that communicate through a shared memory of size m. The input has size n, and can be read directly by all the processors. We will be primarily interested in studying cases where n p m. As a test case we study the list reversal problem. For this problem we prove a time lower bound of ( n p mp ). (A similar lower bound holds also for the problems of sorting, finding all unique elements, convolution, and universal hashing.) This result shows that limiting the communication (i.e., small m) has significant effect on parallel computation. We show an almost matching upper bound of O( n p mp log O(1) n). The upper bound re fl Dept. of Computer Science, Tel Aviv University, Tel Aviv, Israel; This research was supported in part by The Israel Science Foundation administered by The Israel Academy of Science and Humanities and by a grant of the Israelly Ministry of Science and Technology. general settings. Specifically, we show how to emulate a large shared memory on a limited shared memory efficiently. We also argue that some standard methodology for designing parallel algorithms appears to require a relatively high level of communication throughput. Our results suggest that new alternative methodologies that need a lower such level must be invented for parallel machines that enable a low level of communication throughput, since otherwise those machines will be severly handicapped as general purpose parallel machines. We cannot offer any encouraging evidence to suggest that such new methodologies are likely to be found. 
Abstract-found: 1
Intro-found: 1
Reference: [AKP91] <author> F. Abolhassan, J. Keller and W. Paul. </author> <title> On the cost-effectiveness and realization of the theoretical PRAM model, </title> <type> technical report 09/91, </type> <institution> FB Informatik, universitat des saarlandes. </institution> <year> 1991. </year>
Reference: [Abr86] <author> K. Abrahamson. </author> <title> Time-space tradeoffs for branching programs constructed with those for straight line programs. </title> <booktitle> In 27 th Annual Symposium on Foundations of Computer Science, </booktitle> <address> Toronto, Ontario, Canada, </address> <pages> pages 402-409, </pages> <month> Oc-tober </month> <year> 1986. </year>
Reference-contexts: fact our lemma regarding decision trees suffices to prove the lower bound in [Yao90]. 3.1.2 Time-Space tradeoffs for Branching Programs It is interesting to compare our decision tree lemma to similar lemmas that are used as the standard way (due to [BC82]) of proving time-space tradeoffs for branch ing programs <ref> [BC82, Bea89, Yes84, MNT90, Abr86] </ref>. It is not difficult to see that the lemmas proved in some of above-mentioned papers also suffice for our purposes.
Reference: [Aza92] <author> Y. Azar. </author> <title> Lower bounds for threshold and symmetric functions in parallel computation. </title> <journal> SIAM J. Comput., </journal> <volume> 21(2) </volume> <pages> 329-338, </pages> <year> 1992. </year>
Reference-contexts: This makes strong lower bound results more significant than strong upper bounds. More justification and discussion of this model can be found in the next section 1.2 Extant work The case where p is not so much smaller than n was studied in <ref> [VW85, LY86, Aza92] </ref>. The parameter m is called communication width in these papers.
Reference: [B74] <author> R.P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> J. ACM, </journal> <volume> 21(2): </volume> <pages> 201-208, </pages> <year> 1974. </year>
Reference-contexts: A parallel algorithm is described in terms of a sequence of steps, and for each step the set of operations to be performed (concurrently) in that step is given. Using an informal extension of a theorem by Brent <ref> [B74] </ref>, a data-parallel style algorithm which uses a total of W operations in T steps can "typically" be emulated on a PRAM with any number of p W=T processors in O (W=p) time.
Reference: [BC82] <author> A. Borodin and S. Cook. </author> <title> A time-space tradeoff for sorting on a general sequential model of computation. </title> <journal> SIAM J. Comput., </journal> <volume> 1(2) </volume> <pages> 287-297, </pages> <year> 1982. </year>
Reference-contexts: In fact our lemma regarding decision trees suffices to prove the lower bound in [Yao90]. 3.1.2 Time-Space tradeoffs for Branching Programs It is interesting to compare our decision tree lemma to similar lemmas that are used as the standard way (due to <ref> [BC82] </ref>) of proving time-space tradeoffs for branch ing programs [BC82, Bea89, Yes84, MNT90, Abr86]. It is not difficult to see that the lemmas proved in some of above-mentioned papers also suffice for our purposes. <p> fact our lemma regarding decision trees suffices to prove the lower bound in [Yao90]. 3.1.2 Time-Space tradeoffs for Branching Programs It is interesting to compare our decision tree lemma to similar lemmas that are used as the standard way (due to [BC82]) of proving time-space tradeoffs for branch ing programs <ref> [BC82, Bea89, Yes84, MNT90, Abr86] </ref>. It is not difficult to see that the lemmas proved in some of above-mentioned papers also suffice for our purposes. <p> Using one of these lemmas instead of our lemma 3.1 in the proof of theorem 2, we can obtain the same t = n= pm lower bounds for the problems of sorting <ref> [BC82] </ref>, finding all unique elements [Bea89], convolution [Yes84], or universal hashing [MNT90].
Reference: [Bea89] <author> P. </author> <title> Beam. A general sequential time space tradeoff for finding unique elements. </title> <booktitle> In Proceedings of the 21 st Annual ACM Symposium on Theory of Computing, </booktitle> <address> Seattle, Washington, </address> <pages> pages 197-203, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: fact our lemma regarding decision trees suffices to prove the lower bound in [Yao90]. 3.1.2 Time-Space tradeoffs for Branching Programs It is interesting to compare our decision tree lemma to similar lemmas that are used as the standard way (due to [BC82]) of proving time-space tradeoffs for branch ing programs <ref> [BC82, Bea89, Yes84, MNT90, Abr86] </ref>. It is not difficult to see that the lemmas proved in some of above-mentioned papers also suffice for our purposes. <p> Using one of these lemmas instead of our lemma 3.1 in the proof of theorem 2, we can obtain the same t = n= pm lower bounds for the problems of sorting [BC82], finding all unique elements <ref> [Bea89] </ref>, convolution [Yes84], or universal hashing [MNT90].
Reference: [CKP + 93] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proc. 4th ACM PPOPP, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The issue of communication throughput is reflected in various models of computation, which are designed to be closer to currently available machines such as the LogP model of <ref> [CKP + 93] </ref>. We give an additional justification for considering the range m &lt; p for the size of the read/write shared memory m. One of the most useful methodologies for describing PRAM algorithms is the data-parallel style, as suggested in [SV-82].
Reference: [Hel80] <author> M. E. Hellman. </author> <title> A cryptanalytic time-memory tradeoff. </title> <journal> IEEE Trans. Infor. Theor., </journal> <volume> 26 </volume> <pages> 401-406, </pages> <year> 1980. </year>
Reference-contexts: Similar lower bounds apply to other problems such as sorting, finding unique elements, convolution, and universal hashing. The lower bound is related to lower bounds and tradeoffs in several other models of computation, including the problem of inverting a black-box permutation studied in <ref> [Hel80, Yao90] </ref>. A discussion of some of these connections appears in a later subsection. <p> We briefly point to them here. 3.1.1 Inverting a Permutation The problem of reversing a list is quite similar to the problem of inverting a black-box permutation studied in <ref> [Hel80, Yao90] </ref>.
Reference: [HS86] <author> W.D. Hillis, and G.L. Steele. </author> <title> Data parallel algorithms. </title> <journal> Comm. ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <year> 1986. </year>
Reference-contexts: We give an additional justification for considering the range m &lt; p for the size of the read/write shared memory m. One of the most useful methodologies for describing PRAM algorithms is the data-parallel style, as suggested in [SV-82]. A similar style was dubbed "data-parallel" in <ref> [HS86] </ref> and has been used extensively in [J-92], as one example. A parallel algorithm is described in terms of a sequence of steps, and for each step the set of operations to be performed (concurrently) in that step is given.
Reference: [J-92] <author> J. JaJa. </author> <title> Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference-contexts: One of the most useful methodologies for describing PRAM algorithms is the data-parallel style, as suggested in [SV-82]. A similar style was dubbed "data-parallel" in [HS86] and has been used extensively in <ref> [J-92] </ref>, as one example. A parallel algorithm is described in terms of a sequence of steps, and for each step the set of operations to be performed (concurrently) in that step is given.
Reference: [Lei92] <author> T. Leighton. </author> <title> Methods for message routing in parallel machines. </title> <booktitle> In Proceedings of the 24 th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 77-96, </pages> <year> 1992. </year>
Reference-contexts: That paper shows that an MPC, for which m p, is strong enough to efficiently simulate, using randomization, a PRAM algorithm which is designed for a much bigger shared memory. Additional simulation results with respect to the MPC model are referenced in <ref> [Lei92] </ref>. Again, these simulations are targeted only for the case where m p.
Reference: [LY86] <author> M. Li and Y. Yesha. </author> <title> New lower bounds for parallel computation. </title> <booktitle> In Proceedings of the 18 th Annual ACM Symposium on Theory of Computing, </booktitle> <address> Berkeley, California, </address> <pages> pages 177-187, </pages> <year> 1986. </year>
Reference-contexts: This makes strong lower bound results more significant than strong upper bounds. More justification and discussion of this model can be found in the next section 1.2 Extant work The case where p is not so much smaller than n was studied in <ref> [VW85, LY86, Aza92] </ref>. The parameter m is called communication width in these papers.
Reference: [MNT90] <author> Y. Mansour, N. Nisan, and P. Tiwari. </author> <title> The computational complexity of universal hashing. </title> <booktitle> In Proceedings of the 22 nd Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1990. </year>
Reference-contexts: fact our lemma regarding decision trees suffices to prove the lower bound in [Yao90]. 3.1.2 Time-Space tradeoffs for Branching Programs It is interesting to compare our decision tree lemma to similar lemmas that are used as the standard way (due to [BC82]) of proving time-space tradeoffs for branch ing programs <ref> [BC82, Bea89, Yes84, MNT90, Abr86] </ref>. It is not difficult to see that the lemmas proved in some of above-mentioned papers also suffice for our purposes. <p> Using one of these lemmas instead of our lemma 3.1 in the proof of theorem 2, we can obtain the same t = n= pm lower bounds for the problems of sorting [BC82], finding all unique elements [Bea89], convolution [Yes84], or universal hashing <ref> [MNT90] </ref>.
Reference: [MV84] <author> K. Mehlhorn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 339-374, </pages> <year> 1984. </year>
Reference-contexts: An alternative model of shared memory, which is closer than the PRAM to what is considered technologically feasible, is the module parallel computer (MPC) as defined in <ref> [MV84] </ref>. There, the shared memory is partitioned into m memory modules and only one memory cell of each module can be accessed in a given time unit. If there are several access requests they are queued up and performed one at a time. <p> Using an informal extension of a theorem by Brent [B74], a data-parallel style algorithm which uses a total of W operations in T steps can "typically" be emulated on a PRAM with any number of p W=T processors in O (W=p) time. Incorporating the simulations of <ref> [MV84] </ref>, this emulation needs a shared memory of size m = p for achieving ~ O (W=p) time.
Reference: [SV-82] <author> Y. Shiloach and U. Vishkin. </author> <title> An O(n 2 log n) parallel max-flow algorithm. </title> <journal> J. Algorithms, </journal> <volume> 3(2) </volume> <pages> 128-146, </pages> <year> 1982. </year>
Reference-contexts: We give an additional justification for considering the range m &lt; p for the size of the read/write shared memory m. One of the most useful methodologies for describing PRAM algorithms is the data-parallel style, as suggested in <ref> [SV-82] </ref>. A similar style was dubbed "data-parallel" in [HS86] and has been used extensively in [J-92], as one example. A parallel algorithm is described in terms of a sequence of steps, and for each step the set of operations to be performed (concurrently) in that step is given.
Reference: [Val77] <author> L. Valiant. </author> <title> Graph theoretic arguments in low-level complexity, </title> <type> technical report CS 13-77, </type> <institution> University of Edinburgh, UK. </institution> <year> 1977. </year>
Reference-contexts: Then, for any * &gt; 0, f may be computed by a PRAM with p = n * processors and m = 1 common memory cells in time O (n= log log n). Proof: We use Valiant's result <ref> [Val77] </ref> stating that in a linear-size log-depth circuit for f there exist O (n= log log n) wires in the circuit such that the value of each output bit and of each one of these wires depends on at most n * of the input bits and the other wires.
Reference: [VW85] <author> U. Vishkin and A. Wigderson. </author> <title> Trade-offs between depth and width in parallel computation. </title> <journal> SIAM J. Computing, </journal> <volume> 14,2:303-314, </volume> <year> 1985. </year>
Reference-contexts: This makes strong lower bound results more significant than strong upper bounds. More justification and discussion of this model can be found in the next section 1.2 Extant work The case where p is not so much smaller than n was studied in <ref> [VW85, LY86, Aza92] </ref>. The parameter m is called communication width in these papers. <p> This tradeoff for the case p n is similar in form to the tradeoff derived in <ref> [VW85] </ref>. 3.1 Connections of lower bounds to other models The lower bound proved in this paper has interesting connections with questions regarding tradeoffs in sev eral other models of computation.
Reference: [Yao90] <author> A. Yao. </author> <title> Coherent functions and program checkers. </title> <booktitle> In Proceedings of the 22 nd Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1990. </year>
Reference-contexts: Similar lower bounds apply to other problems such as sorting, finding unique elements, convolution, and universal hashing. The lower bound is related to lower bounds and tradeoffs in several other models of computation, including the problem of inverting a black-box permutation studied in <ref> [Hel80, Yao90] </ref>. A discussion of some of these connections appears in a later subsection. <p> We briefly point to them here. 3.1.1 Inverting a Permutation The problem of reversing a list is quite similar to the problem of inverting a black-box permutation studied in <ref> [Hel80, Yao90] </ref>. <p> We briefly point to them here. 3.1.1 Inverting a Permutation The problem of reversing a list is quite similar to the problem of inverting a black-box permutation studied in [Hel80, Yao90]. In fact our lemma regarding decision trees suffices to prove the lower bound in <ref> [Yao90] </ref>. 3.1.2 Time-Space tradeoffs for Branching Programs It is interesting to compare our decision tree lemma to similar lemmas that are used as the standard way (due to [BC82]) of proving time-space tradeoffs for branch ing programs [BC82, Bea89, Yes84, MNT90, Abr86].
Reference: [Yes84] <author> Y. Yesha. </author> <title> A time-space tradeoff for matrix multiplication and the discrete Fourier transform on a general sequential random access computer. </title> <journal> J. Comp. and Syst. Sci., </journal> <volume> 29 </volume> <pages> 183-197, </pages> <year> 1984. </year>
Reference-contexts: fact our lemma regarding decision trees suffices to prove the lower bound in [Yao90]. 3.1.2 Time-Space tradeoffs for Branching Programs It is interesting to compare our decision tree lemma to similar lemmas that are used as the standard way (due to [BC82]) of proving time-space tradeoffs for branch ing programs <ref> [BC82, Bea89, Yes84, MNT90, Abr86] </ref>. It is not difficult to see that the lemmas proved in some of above-mentioned papers also suffice for our purposes. <p> Using one of these lemmas instead of our lemma 3.1 in the proof of theorem 2, we can obtain the same t = n= pm lower bounds for the problems of sorting [BC82], finding all unique elements [Bea89], convolution <ref> [Yes84] </ref>, or universal hashing [MNT90].
References-found: 19

