URL: http://www.cs.man.ac.uk/ai/Papers/magnus/noise3.ps.gz
Refering-URL: http://www.cs.man.ac.uk/~magnus/magnus.html
Root-URL: http://www.cs.man.ac.uk
Title: Noisy Fitness Evaluation in Genetic Algorithms and the Dynamics of Learning  
Author: Magnus Rattray and Jonathan Shapiro 
Note: Internet address: rattraym@cs.man.ac.uk. Internet address: jls@cs.man.ac.uk.  
Address: Oxford Road, Manchester M13 9PL, U.K.  
Affiliation: Computer Science Department, University of Manchester,  
Abstract: A theoretical model is presented which describes selection in a genetic algorithm (GA) under a stochastic fitness measure and correctly accounts for finite population effects. Although this model describes a number of selection schemes, we only consider Boltzmann selection in detail here as results for this form of selection are particularly transparent when fitness is corrupted by additive Gaussian noise. Finite population effects are shown to be of fundamental importance in this case, as the noise has no effect in the infinite population limit. In the limit of weak selection we show how the effects of any Gaussian noise can be removed by increasing the population size appropriately. The theory is tested on two closely related problems: the one-max problem corrupted by Gaussian noise and generalization in a perceptron with binary weights. The averaged dynamics can be accurately modelled for both problems using a formalism which describes the dynamics of the GA using methods from statistical mechanics. The second problem is a simple example of a learning problem and by considering this problem we show how the accurate characterization of noise in the fitness evaluation may be relevant in machine learning. The training error (negative fitness) is the number of misclassified training examples in a batch and can be considered as a noisy version of the generalization error if an independent batch is used for each evaluation. The noise is due to the finite batch size and in the limit of large problem size and weak selection we show how the effect of this noise can be removed by increasing the population size. This allows the optimal batch size to be determined, which minimizes computation time as well as the total number of training examples required. 
Abstract-found: 1
Intro-found: 1
Reference: <author> J. E. </author> <title> Baker (1987) Reducing Bias and Inefficiency in the Selection Algorithm, </title> <booktitle> Proc. of the 2nd Int. Conf. on Genetic Algorithms, </booktitle> <editor> ed J. J. Grefenstette (Hillsdale, </editor> <publisher> NJ; Lawrence Erlbaum) p 14-21. </publisher>
Reference-contexts: We will consider Roulette wheel sampling, as this provides an analytically tractable model for finite population effects. Other, less noisy forms of sampling are often preferred in practice (see, for example, <ref> (Baker, 1987) </ref>) and a challenging task would be to extend the present analysis to these cases. Under Roulette wheel sampling, N new population members are selected with replacement, with probability p a . Following the discussion in (Prugel-Bennett, 1996), this process can be divided into two stages, 1.
Reference: <author> E. B. Baum, D. Boneh and C. </author> <booktitle> Garret (1995) On Genetic Algorithms, in COLT '95: Proc. of the 8th Annual Conf. on Computational Learning Theory (New York; Assoc. </booktitle> <publisher> for Computing Machinery Inc.) </publisher> <pages> p 230-239. </pages>
Reference-contexts: It has been suggested that GAs are suitable in this domain, since they are relatively robust against the effects of noise (Fitzpatrick & Grefenstette, 1988). Indeed, GAs have recently been shown to deal better with noise than competing local search algorithms on a class of simple additive problems <ref> (Baum et al, 1995) </ref>. In (Miller & Goldberg, 1995), noise corrupted fitness was modelled in terms of its effect on the mean fitness after selection from a continuous and Gaussian distribution of fitness. <p> A perceptron with binary weights is trained to learn a teacher perceptron by training on examples produced by the teacher. This has previously been shown to be equivalent to a noisy version of one-max if a new batch of examples are presented each time the training error is calculated <ref> (Baum et al, 1995) </ref>. This problem was solved under the statistical mechanics formalism in (Rattray & Shapiro, 1996) and those results are reviewed here. <p> Typically b is of order 1= p l and this population resizing will not blow up with increases in problem size (for fixed l). This is consistent with the result in <ref> (Baum et al, 1995) </ref>, although they provide a rigorous proof for the scaling of their algorithm. Both selection strength and noise variance will change over time, and it would therefore be necessary to change the population size each generation in order to apply the above expression.
Reference: <author> T. Blickle, L. </author> <title> Thiele (1995) A Comparison of Selection Schemes used in Genetic Algorithms, </title> <institution> Computer Engineering and Communication Network Lab, Swiss Federal Institute of Technology, </institution> <address> Gloriastrasse 35, 8092 Zurich, </address> <note> Switzerland TIK-Report Nr.11 Version 2. </note>
Reference: <author> M. De la Maza, B. </author> <title> Tidor (1991) Increased Flexibility in Genetic Algorithms: The Use of Variable Boltzmann Selective Pressure to Control Propagation, </title> <booktitle> Proc. of the ORSA CSTS Conference - Computer Science and Operations Research: New Developments in their Interfaces, </booktitle> <pages> p 425-440. </pages>
Reference: <author> B. </author> <title> Derrida (1981) Random-energy Model: An Exactly Solvable Model of Disordered Systems, </title> <journal> Phys. Rev. </journal> <volume> B 24, </volume> <pages> 2613-2625. </pages>
Reference-contexts: equation (12), hln Z s i = N a=1 df a p ( f a ) dF a p (F a j f a ) ln Z s : (14) Following the discussion in (Prugel-Bennett & Shapiro, 1994) we use Derrida's trick to express the logarithm as an integral 2 <ref> (Derrida, 1981) </ref>. hln Z s i = 0 e t he tZ s i : (15) If the selection weight associated with population member a is a function of F a alone then the aver ages on the right hand side decouple from one another, he tZ s i = N
Reference: <author> W. J. </author> <title> Ewens (1979) Mathematical Population Genetics, </title> <publisher> (Berlin; Springer-Verlag). </publisher>
Reference-contexts: Equation (25b) shows how an increase in correlation results in a reduced variance, all other terms being equal. The i 6= j term in this expression is related to the linkage disequilibrium in population genetics <ref> (Ewens, 1979) </ref> and disappears after bit-simulated crossover. In this case the correlation can be deduced directly from the variance after crossover. 4.2 MUTATION Under mutation, bits are flipped throughout the population with probability p m .
Reference: <author> J. M. Fitzpatrick, J. J. </author> <title> Grefenstette (1988) Genetic Algorithms in Noisy Environments, </title> <booktitle> Machine Learning 3, </booktitle> <pages> 101-120. </pages>
Reference-contexts: In some machine learning and optimization applications there may be a tradeoff between improved fidelity in evaluating fitness and the increased computational cost this requires. It has been suggested that GAs are suitable in this domain, since they are relatively robust against the effects of noise <ref> (Fitzpatrick & Grefenstette, 1988) </ref>. Indeed, GAs have recently been shown to deal better with noise than competing local search algorithms on a class of simple additive problems (Baum et al, 1995). <p> Fitzpatrick and Grefenstette also identified the existence of such a tradeoff between population size and batch size, and they suggest that there is often an optimal choice of batch size (or measurement accuracy) <ref> (Fitzpatrick & Grefenstette, 1988) </ref>. If the population resizing in equation (47) is used, then it is possible to identify such an optimal batch size, which minimizes the computational cost of training error evaluations.
Reference: <author> D. E. </author> <title> Goldberg (1989) Genetic Algorithms in Search, Optimization and Machine Learning, </title> <address> (Reading, </address> <publisher> MA; Addison-Wesley). </publisher>
Reference: <author> D. E. Goldberg, K. Deb, J. H. </author> <title> Clark (1992) Genetic Algorithms, Noise, and the Sizing of Populations, </title> <booktitle> Complex Systems 6, </booktitle> <pages> 333-362. </pages>
Reference-contexts: Although Miller and Goldberg sized the population to account for increased finite population effects due to noise, their choice of population size was based on a conservative predictor rather than an exact result <ref> (Goldberg et al, 1992) </ref>. Their calculation of the variance for the one-max domain assumes a binomial distribution of alleles within the population and this assumption is also made in a number of other predictive models (Muhlenbein & Schlierkamp-Voosen, 1995; Srinivas & Patnaik, 1995; Thierens & Goldberg, 1995).
Reference: <author> J. H. </author> <booktitle> Holland (1975) Adaptation in Natural and Artificial Systems, </booktitle> <institution> (Ann Arbor; The University of Michigan Press). </institution>
Reference: <author> B. L. Miller, D. E. </author> <title> Goldberg (1995) Genetic Algorithms, Selection Schemes and the Varying Effects of Noise, </title> <institution> Dept. of General Engineering, University of Illinois at Urbana-Champaign, 117 Transportation Building, Urbana, IL 61801. </institution> <note> (IlliGAL Report No. 95009). </note>
Reference-contexts: Indeed, GAs have recently been shown to deal better with noise than competing local search algorithms on a class of simple additive problems (Baum et al, 1995). In <ref> (Miller & Goldberg, 1995) </ref>, noise corrupted fitness was modelled in terms of its effect on the mean fitness after selection from a continuous and Gaussian distribution of fitness. This is effectively an infinite population assumption and leads to the conclusion that proportionate selection is unaffected by noise.
Reference: <author> H. Muhlenbein, </author> <title> D Schlierkamp-Voosen (1995) Analysis of Selection, Mutation and Recombination in Genetic Algorithms, </title> <booktitle> Lecture Notes in Computer Science 899, </booktitle> <pages> 188-214. </pages>
Reference: <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, W. T. </author> <title> Vetterling (1992) Numerical Recipes in C : The Art of Scientific Computing, 2nd ed. </title> <publisher> (Cambridge; Cambridge University Press). </publisher>
Reference-contexts: In general these integrals have to be determined numerically and for the simulation results presented in this paper the integrals were computed by Gaussian quadratures <ref> (Press et al, 1992) </ref>. <p> be used to obtain values for the Lagrange multipliers, K 1 = tanh (z + xh i ) q = 1 l i=1 tanh 2 (z + xh i ): (60) Bars denote averages over the Gaussian noise which in general must be done numerically (Gauss-Hermite quadrature was used here <ref> (Press et al, 1992) </ref>).
Reference: <author> A. Prugel-Bennett, J. L. </author> <title> Shapiro (1994) An Analysis of Genetic Algorithms using Statistical Mechanics, </title> <journal> Phys. Rev. Lett. </journal> <volume> 72(9), </volume> <pages> 1305. </pages>
Reference: <author> A. Prugel-Bennett, J. L. </author> <title> Shapiro (1995) The Dynamics of a Genetic Algorithm for Simple Random Ising Systems, </title> <institution> Computer Science Dept., University of Manchester, </institution> <address> Oxford Road, Manchester M13 9PL, U.K. </address> <note> (to appear in Physica D). </note>
Reference: <author> A. </author> <title> Prugel-Bennett (1996) Modelling Evolving Populations, </title> <address> NORDITA, Blegdamsvej 17, DK-2100 Copenhagen, Denmark, </address> <note> (submitted to J. Theor. Biol.). </note>
Reference: <author> L. M. </author> <title> Rattray (1995) The Dynamics of a Genetic Algorithm under Stabilizing Selection, </title> <booktitle> Complex Systems 9(3), </booktitle> <pages> 213-234. </pages>
Reference-contexts: The second term is the natural increase in correlation as fitness increases (and entropy lowers) and is the sole contribution in the infinite population limit (these definitions differ slightly from those used in <ref> (Rattray, 1995) </ref>). 4.4.1 Natural Increase Term We estimate the conditional probability distribution for correlation given two fitness values before selection p (q ab j f a ; f b ) by assuming the alleles within the population are distributed according to the maximum entropy distribution described in the appendix. <p> )p s ( f b )p (q ab j f a ; f b ) q ab : (33) This integral can be calculated for large l by the saddle point method 4 and we find that in this limit the result depends only on the mean fitness after selection <ref> (Rattray, 1995) </ref>, q (y) = l t i + tanh (y) 1 + t i tanh (y) 2 where y is implicitly related to the mean fitness after selection through, K s l i=1 t i + tanh (y) 1 + t i tanh (y) : (34b) Here, t i is
Reference: <author> L. M. </author> <title> Rattray (1996) Modelling the Dynamics of Genetic Algorithms using Statistical Mechanics, </title> <institution> Computer Science Dept., University of Manchester, Oxford Road, Manchester M13 9PL, UK (PhD. </institution> <note> Thesis In Preperation). </note>
Reference-contexts: This has previously been shown to be equivalent to a noisy version of one-max if a new batch of examples are presented each time the training error is calculated (Baum et al, 1995). This problem was solved under the statistical mechanics formalism in <ref> (Rattray & Shapiro, 1996) </ref> and those results are reviewed here. The training error is well approximated by a Gaussian distribution whose mean is the the generalization error and whose variance increases as the batch size is reduced. <p> For fitness proportionate selection the selection weight is simply equal to the fitness. Selection weights can also be defined for ranking, tournament and truncation selection, and the general method described here can be applied to these cases <ref> (Rattray, 1996) </ref>. We will consider Boltzmann selection in greatest detail, as this provides a transparent result for Gaussian noise (De la Maza & Tidor, 1991; Prugel-Bennett & Shapiro, 1994). <p> This discussion will follow that presented in <ref> (Rattray & Shapiro, 1996) </ref> most closely. To simplify matters bit-simulated crossover is used, where the population is completely shuffled during crossover so that a child's alleles come from any population member with equal probability (Syswerda, 1993). <p> i ): The Lagrange multipliers, z and x, are chosen to enforce constraints on the mean overlap and correlation within the population before selection and h i is drawn from a Gaussian distribution with zero mean and unit variance. 4 For weak selection the large l restriction can be dropped <ref> (Rattray, 1996) </ref>. It is instructive to expand in y, which is appropriate in the weak selection limit. <p> As in the selection calculation presented in section 3.1, population members are independently averaged over a distribution with the correct cu-mulants to calculate the expectation value of this quantity. In general the expressions must be computed numerically, but the results can be expanded in 1=N for sufficiently weak selection <ref> (Rattray & Shapiro, 1996) </ref>. In this case one finds, Dq d = [1 q (2b)]r (2b) + O 1 where q (y) is defined in equation (34a) and r (b) is the characteristic function defined in equation (5). <p> A simple example of this is the case where a perceptron with binary weights is trained on patterns generated from a teacher perceptron, also with binary weights. The statistical mechanics formalism was applied to this problem in <ref> (Rattray & Shapiro, 1996) </ref> and here we review these results in order to show how this work may be of relevance to problems from machine learning. <p> This can be determined and if the size of each batch is O (l) then p (Ej f ) is well approximated by a Gaussian distribution <ref> (Rattray & Shapiro, 1996) </ref>, 1 2ps 2 2s 2 ; (42) where the mean and variance are, E g ( f ) = p s 2 ( f ) = E g ( f ) 1 ll : (43b) Here, E g ( f ) is the generalization error, which is <p> The correlation result can similarly be calculated by generalizing the noisy one-max result in section 4.4 and one finds that the results are equivalent under the same effective selection strength and noise. A more thorough discussion of these results is given in <ref> (Rattray & Shapiro, 1996) </ref>. 5.2 RESIZING THE POPULATION The noise introduced by the finite sized training set increases the magnitude of the detrimental finite population terms in selection. <p> It is also possible that the Gaussian approximation for p (Ej f ) breaks down for small l, in which case it would be necessary to expand the noise in terms of more cumulants. Results for the higher cumulants also agree with high significance, as shown in <ref> (Rattray & Shapiro, 1996) </ref>. 6 CONCLUSION A theory which describes selection on a finite population under a general stochastic fitness measure has been applied to two related problems, showing excellent predictive power. <p> The other parameters were l = 155, b s = 0:3, p m = 0:005, N = 80 and bit-simulated crossover was used. Adapted from <ref> (Rattray & Shapiro, 1996) </ref>. 0 50 100 150 0.2 0.6 1.0 Generation f max the same simulations as the results presented in figure 3. The solid lines show the theoretical predic tions and symbols are as in figure 3.
Reference: <author> L. M. Rattray , J. L. </author> <title> Shapiro (1996) The Dynamics of a Genetic Algorithm for a Simple Learning Problem, </title> <institution> Computer Science Dept., University of Manchester, </institution> <address> Oxford Road, Manchester M13 9PL, </address> <note> UK (to appear in J. </note> <institution> Phys. A). </institution>
Reference-contexts: This has previously been shown to be equivalent to a noisy version of one-max if a new batch of examples are presented each time the training error is calculated (Baum et al, 1995). This problem was solved under the statistical mechanics formalism in <ref> (Rattray & Shapiro, 1996) </ref> and those results are reviewed here. The training error is well approximated by a Gaussian distribution whose mean is the the generalization error and whose variance increases as the batch size is reduced. <p> For fitness proportionate selection the selection weight is simply equal to the fitness. Selection weights can also be defined for ranking, tournament and truncation selection, and the general method described here can be applied to these cases <ref> (Rattray, 1996) </ref>. We will consider Boltzmann selection in greatest detail, as this provides a transparent result for Gaussian noise (De la Maza & Tidor, 1991; Prugel-Bennett & Shapiro, 1994). <p> This discussion will follow that presented in <ref> (Rattray & Shapiro, 1996) </ref> most closely. To simplify matters bit-simulated crossover is used, where the population is completely shuffled during crossover so that a child's alleles come from any population member with equal probability (Syswerda, 1993). <p> i ): The Lagrange multipliers, z and x, are chosen to enforce constraints on the mean overlap and correlation within the population before selection and h i is drawn from a Gaussian distribution with zero mean and unit variance. 4 For weak selection the large l restriction can be dropped <ref> (Rattray, 1996) </ref>. It is instructive to expand in y, which is appropriate in the weak selection limit. <p> As in the selection calculation presented in section 3.1, population members are independently averaged over a distribution with the correct cu-mulants to calculate the expectation value of this quantity. In general the expressions must be computed numerically, but the results can be expanded in 1=N for sufficiently weak selection <ref> (Rattray & Shapiro, 1996) </ref>. In this case one finds, Dq d = [1 q (2b)]r (2b) + O 1 where q (y) is defined in equation (34a) and r (b) is the characteristic function defined in equation (5). <p> A simple example of this is the case where a perceptron with binary weights is trained on patterns generated from a teacher perceptron, also with binary weights. The statistical mechanics formalism was applied to this problem in <ref> (Rattray & Shapiro, 1996) </ref> and here we review these results in order to show how this work may be of relevance to problems from machine learning. <p> This can be determined and if the size of each batch is O (l) then p (Ej f ) is well approximated by a Gaussian distribution <ref> (Rattray & Shapiro, 1996) </ref>, 1 2ps 2 2s 2 ; (42) where the mean and variance are, E g ( f ) = p s 2 ( f ) = E g ( f ) 1 ll : (43b) Here, E g ( f ) is the generalization error, which is <p> The correlation result can similarly be calculated by generalizing the noisy one-max result in section 4.4 and one finds that the results are equivalent under the same effective selection strength and noise. A more thorough discussion of these results is given in <ref> (Rattray & Shapiro, 1996) </ref>. 5.2 RESIZING THE POPULATION The noise introduced by the finite sized training set increases the magnitude of the detrimental finite population terms in selection. <p> It is also possible that the Gaussian approximation for p (Ej f ) breaks down for small l, in which case it would be necessary to expand the noise in terms of more cumulants. Results for the higher cumulants also agree with high significance, as shown in <ref> (Rattray & Shapiro, 1996) </ref>. 6 CONCLUSION A theory which describes selection on a finite population under a general stochastic fitness measure has been applied to two related problems, showing excellent predictive power. <p> The other parameters were l = 155, b s = 0:3, p m = 0:005, N = 80 and bit-simulated crossover was used. Adapted from <ref> (Rattray & Shapiro, 1996) </ref>. 0 50 100 150 0.2 0.6 1.0 Generation f max the same simulations as the results presented in figure 3. The solid lines show the theoretical predic tions and symbols are as in figure 3.
Reference: <author> J. L. Shapiro, A. Prugel-Bennett, L. M. </author> <title> Rattray (1994) A Statistical Mechanical Formulation of the Dynamics of Genetic Algorithms, </title> <booktitle> Lecture Notes in Computer Science 865, </booktitle> <pages> 17-27. </pages>
Reference-contexts: directly, it is more convenient to average over the logarithm of the partition function defined in equation (12), hln Z s i = N a=1 df a p ( f a ) dF a p (F a j f a ) ln Z s : (14) Following the discussion in <ref> (Prugel-Bennett & Shapiro, 1994) </ref> we use Derrida's trick to express the logarithm as an integral 2 (Derrida, 1981). hln Z s i = 0 e t he tZ s i : (15) If the selection weight associated with population member a is a function of F a alone then the aver <p> As shown in <ref> (Prugel-Bennett & Shapiro, 1994) </ref>, one can express the logarithm of the partition function analytically for small b. This limit is accurate for sufficiently small b p K 2 + s 2 and is instructive as it shows the relevant effects of selection for each cumulant. <p> one finds, K s e (bs) 2 b 2 3e (bs) 2 K s e (bs) 2 3e (bs) 2 K s 3e (bs) 2 " 7e (bs) 2 6e (bs) 2 (K 2 ) 2 + : (22c) For zero noise (s = 0) one retrieves the result in <ref> (Prugel-Bennett & Shapiro, 1994) </ref>. As in the zero noise case, finite population effects lead to a reduced variance and a negative third cumulant 3 , related to the population's skewness, which leads to an accelerated reduction in variance under further selection.
Reference: <author> M. Srinivas, L. M. </author> <title> Patnaik (1995) Binomially Distributed Populations for Modelling GAs, </title> <booktitle> Proc. of the 5th Int. Conf. on Ganetic Algorithms, </booktitle> <editor> ed S. </editor> <address> Forrest (San Mateo, </address> <publisher> CA; Morgan Kaufmann) p 138-145. </publisher>
Reference: <author> A. Stuart, J. K. </author> <booktitle> Ord (1987) Kendall's Advanced Theory of Statistics, </booktitle> <volume> Vol 1. </volume> <booktitle> Distribution Theory, 5th ed. </booktitle> <publisher> (New York; Oxford University Press). </publisher>
Reference-contexts: The characteristic function can also be written in terms of a cumulant expansion, r (g) = exp n=1 n! : (6) It is often useful to parameterize the fitness distribution by expanding around a Gaussian distribution. In this case we choose a Gram-Charlier expansion (see, for example, <ref> (Stuart & Ord, 1987) </ref>), p ( f ) = p exp ( f K 1 ) 2 " n c n=3 n!K 2 p # where H n (x) = (1) n e x 2 =2 d n dx n e x 2 =2 are Hermite polynomials and n c is
Reference: <author> G. </author> <title> Syswerda (1993) Simulated Crossover in Genetic Algorithms, </title> <booktitle> in Foundations of Genetic Algorithms 2, </booktitle> <address> (San Mateo, </address> <publisher> CA; Morgan Kaufmann). </publisher>
Reference-contexts: The first two cumulants are the mean and variance respectively while higher cumulants describe deviations from a Gaussian distribution. The first case considered is the one-max problem corrupted by Gaussian noise. To simplify the discussion, bit-simulated crossover is used <ref> (Syswerda, 1993) </ref> and this allows the dynamics to be mod-elled by iterating only two macroscopics: the mean fitness and correlation within the population. A maximum entropy assumption is required to determine the higher cumulants before selection and to evolve the correlation under selection. <p> This discussion will follow that presented in (Rattray & Shapiro, 1996) most closely. To simplify matters bit-simulated crossover is used, where the population is completely shuffled during crossover so that a child's alleles come from any population member with equal probability <ref> (Syswerda, 1993) </ref>. This brings the population straight to the fixed point of standard uniform crossover (without selection) and allows the population to be accurately described by only two macroscopics: the mean fitness and correlation.
Reference: <author> D. Thierens, D. </author> <title> Goldberg (1995) Convergence Models of Genetic Algorithm Selection Schemes, </title> <booktitle> Parallel Problem Solving from Nature III (in Lecture Notes in Computer Science 866), </booktitle> <pages> 119-129. </pages>
Reference: <author> M. D. Vose, A. H. </author> <title> Wright (1994) Simple Genetic Algorithms with Linear Fitness, </title> <journal> Evol. Comp. </journal> <volume> 2, </volume> <pages> 347-368. </pages>
Reference-contexts: Randomly sample N population members from the infinite population to make up the new finite population. Mutation and crossover do not involve sampling and can therefore be carried out during the infinite population stage of the dynamics without any loss of generality. A similar sampling procedure is used in <ref> (Vose & Wright, 1994) </ref>, but there they follow an exact microscopic description of the population while we only consider a small number of macroscopic statistics.
References-found: 25

