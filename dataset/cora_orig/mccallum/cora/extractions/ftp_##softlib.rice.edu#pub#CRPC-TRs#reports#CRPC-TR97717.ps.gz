URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR97717.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Implementation in ScaLAPACK of Divide-and-Conquer Algorithms for Banded and Tridiagonal Linear Systems  
Author: A. Cleary J. Dongarra 
Affiliation: Department of Computer Science University of Tennessee  Department of Computer Science University of Tennessee Mathematical Sciences Section Oak Ridge National Laboratory  
Abstract: Described here are the design and implementation of a family of algorithms for a variety of classes of narrowly banded linear systems. The classes of matrices include symmetric and positive definite, nonsymmetric but diagonally dominant, and general nonsymmetric; and, all these types are addressed for both general band and tridiagonal matrices. The family of algorithms captures the general flavor of existing divide-and-conquer algorithms for banded matrices in that they have three distinct phases, the first and last of which are completely parallel, and the second of which is the parallel bottleneck. The algorithms have been modified so that they have the desirable property that they are the same mathematically as existing factorizations (Cholesky, Gaussian elimination) of suitably reordered matrices. This approach represents a departure in the nonsymmetric case from existing methods, but has the practical benefits of a smaller and more easily handled reduced system. All codes implement a block odd-even reduction for the reduced system that allows the algorithm to scale far better than existing codes that use variants of sequential solution methods for the reduced system. A cross section of results is displayed that supports the predicted performance results for the algorithms. Comparison with existing dense-type methods shows that for areas of the problem parameter space with low bandwidth and/or high number of processors, the family of algorithms described here is superior. 
Abstract-found: 1
Intro-found: 1
Reference: [ABB + 95] <author> E. Anderson, Z. Bai, C. H. Bischof, J. Demmel, J. J. Dongarra, J. Du Croz, A. Green-baum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. C. Sorensen. </author> <note> LAPACK Users' Guide. SIAM, 2nd edition, 1995. (Also available in Japanese, published by Maruzen, Tokyo, translated by Dr Oguni). </note>
Reference-contexts: We take this approach in the sequel. Figure 1 pictorially illustrates the user matrix upon input, assuming the user chooses to input the matrix in lower triangular form (analogous to banded routines in LAPACK <ref> [ABB + 95] </ref>, we provide the option of entering the matrix in either lower or upper form). Each processor stores a contiguous set of columns of the matrix, denoted by the thicker lines in the figure. We partition each processor's matrix into blocks, as shown. <p> Because the reordering allows the Cholesky factorization to begin simultaneously with each of the A i , the first computational step in processor i is the factorization A i = L i L i This is easily done via a single call to the LAPACK <ref> [ABB + 95] </ref> routine DPBTRF. The matrix resulting after this step is illustrated in Figure 6.
Reference: [AG95] <author> P. Arbenz and W. Gander. </author> <title> A survey of direct parallel algorithms for banded linear systems. </title> <type> Technical report, </type> <institution> Swiss Federal Institute of Technology, </institution> <address> Zurich, Swizterland, </address> <year> 1995. </year>
Reference-contexts: They are therefore limited to an efficiency of no greater than 25%. Nonetheless, these algorithms scale well and must be included in a parallel library. Many studies of parallel computing reported in the literature <ref> [Wri91, AG95, GGJT96, BCD + 94, Joh87, LS84] </ref> address either the general banded problem or special cases such as the tridiagonal problem. Wright [Wri91] presented an ambitious attempt at incorporating pivoting, both row and column, throughout the entire calculation. <p> His single-width separator algorithm without pivoting is similar to the work described here. However, Wright targeted relatively small parallel systems and, in particular, used a sequential method for the reduced system. Arbenz and Gander <ref> [AG95] </ref> present experimental results demonstrating that sequential solution of the reduced system seriously impacts scalability. They discuss the basic divide-and-conquer algorithms used here, but do not give the level of detail on implementation that we present. <p> An algorithm with similar performance characteristics involves gathering the reduced system on a single processor, solving it sequentially, and broadcasting the results. Again, the principal disadvantage is the lack of scalability due to the sequential solution. Arbenz and Gander <ref> [AG95] </ref> show that the best time for this algorithm occurs at P = 20, which is a disaster for scalability. Theoretically and practically they show the need for a parallel reduced system algorithm, which is what we have implemented for ScaLAPACK.
Reference: [BCC + 97] <author> S. Blackford, J. Choi, A. Cleary, E. D'Azevedo, J. Demmel, I. Dhillon, J. Dongarra, S. Hammarling, G. Henry, A. Petitet, K. Stanley, D. Walker, and R. Whaley. </author> <title> Scalapack: A linear algebra library for message-passing computers. </title> <booktitle> In Proceedings of 1997 SIAM Conference on Parallel Processing, </booktitle> <month> May </month> <year> 1997. </year> <month> 17 </month>
Reference-contexts: In this article we focus on the last two cases and indicate how the family of algorithms we present can apply to other classes of matrix. The first class, general nonsymmetric, is the subject of a forthcoming report. This article concentrates on issues related to the inclusion in ScaLAPACK <ref> [CDPW93, BCC + 97] </ref> of library-quality implementations of the algorithms discussed here. ScaLAPACK is a public-domain portable software library that provides broad functionality in linear algebra mathematical software to a wide variety of distributed-memory parallel systems.
Reference: [BCD + 94] <author> R. Brent, A. Cleary, M. Dow, M. Hegland, J. Jenkinson, Z. Leyk, M. Nakanishi, M. Os--borne, P. Price, S. Roberts, and D. </author> <title> Singleton. Implementation and performance of scalable scientific library subroutines on Fujitsu's VPP500 parallel-vector supercomputer. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <year> 1994. </year>
Reference-contexts: They are therefore limited to an efficiency of no greater than 25%. Nonetheless, these algorithms scale well and must be included in a parallel library. Many studies of parallel computing reported in the literature <ref> [Wri91, AG95, GGJT96, BCD + 94, Joh87, LS84] </ref> address either the general banded problem or special cases such as the tridiagonal problem. Wright [Wri91] presented an ambitious attempt at incorporating pivoting, both row and column, throughout the entire calculation. <p> They discuss the basic divide-and-conquer algorithms used here, but do not give the level of detail on implementation that we present. Gustavson et al. [GGJT96] tackle the wide-banded case with a systolic approach that involves an initial remapping of the data. Cleary <ref> [BCD + 94] </ref> presents an in-place wide-banded algorithm that is much closer to the ScaLAPACK-style of algorithm [CDPW93], utilizing fine-grained parallelism within BLAS-3 for parallel performance. 2 2 Divide-and-Conquer Algorithms In this article, we are concerned only with divide-and-conquer methods that are appropriate for narrowly banded matrices.
Reference: [CDPW93] <author> J. Choi, J. Dongarra, R. Pozo, and D. Walker. </author> <title> ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers. </title> <type> Technical Report 53, </type> <note> LAPACK Working Note, </note> <year> 1993. </year>
Reference-contexts: In this article we focus on the last two cases and indicate how the family of algorithms we present can apply to other classes of matrix. The first class, general nonsymmetric, is the subject of a forthcoming report. This article concentrates on issues related to the inclusion in ScaLAPACK <ref> [CDPW93, BCC + 97] </ref> of library-quality implementations of the algorithms discussed here. ScaLAPACK is a public-domain portable software library that provides broad functionality in linear algebra mathematical software to a wide variety of distributed-memory parallel systems. <p> Gustavson et al. [GGJT96] tackle the wide-banded case with a systolic approach that involves an initial remapping of the data. Cleary [BCD + 94] presents an in-place wide-banded algorithm that is much closer to the ScaLAPACK-style of algorithm <ref> [CDPW93] </ref>, utilizing fine-grained parallelism within BLAS-3 for parallel performance. 2 2 Divide-and-Conquer Algorithms In this article, we are concerned only with divide-and-conquer methods that are appropriate for narrowly banded matrices. The family of divide-and-conquer algorithms used in ScaLAPACK perform the following algebraic steps.
Reference: [Cle89] <author> A. Cleary. </author> <title> Algorithms for Solving Narrowly Banded Linear Systems on Parallel Computers by Direct Methods. </title> <type> PhD thesis, </type> <institution> The University of Virginia, Department of Applied Mathematics, </institution> <year> 1989. </year>
Reference: [DDHH84] <author> J. Dongarra, J. DuCroz, S. Hammarling, and R. Hanson. </author> <title> A proposal for an extended set of Fortran basic linear algebra subprograms. </title> <type> Technical Memo 41, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> December </month> <year> 1984. </year>
Reference-contexts: Once the matrix is in the workspace, the BLAS <ref> [DDHH84] </ref> subroutine DTRTRS is used to compute B 0 i , which is copied back into the space held by B i . Mathematically, the local matrix is given by Figure 7.
Reference: [DJ87] <author> J. Dongarra and L. Johnsson. </author> <title> Solving banded systems on a parallel processor. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 219-246, </pages> <year> 1987. </year>
Reference-contexts: 0 : This is solved in the traditional fashion by using triangular solutions: Lz = b 0 ; U x 0 = z: The final step is recovery of x from x 0 : x = P 1 x 0 : 3 The Symmetric Positive Definite Case Dongarra and Johnsson <ref> [DJ87] </ref> showed that their divide-and-conquer algorithm, when applied correctly to a symmetric positive definite matrix, can take advantage of symmetry throughout, including the solution of the reduced system. In fact, the reduced system is itself symmetric positive definite.
Reference: [GGJT96] <author> A. Gupta, F. Gustavson, M. Joshi, and S. Toledo. </author> <title> The design, implementation, and evaluation of a banded linear solver for distributed-memory parallel computers. </title> <institution> Research Report RC 20481, IBM, </institution> <month> june </month> <year> 1996. </year>
Reference-contexts: They are therefore limited to an efficiency of no greater than 25%. Nonetheless, these algorithms scale well and must be included in a parallel library. Many studies of parallel computing reported in the literature <ref> [Wri91, AG95, GGJT96, BCD + 94, Joh87, LS84] </ref> address either the general banded problem or special cases such as the tridiagonal problem. Wright [Wri91] presented an ambitious attempt at incorporating pivoting, both row and column, throughout the entire calculation. <p> Arbenz and Gander [AG95] present experimental results demonstrating that sequential solution of the reduced system seriously impacts scalability. They discuss the basic divide-and-conquer algorithms used here, but do not give the level of detail on implementation that we present. Gustavson et al. <ref> [GGJT96] </ref> tackle the wide-banded case with a systolic approach that involves an initial remapping of the data. <p> Several methods for factoring the reduced system have been proposed and implemented in the past. For small P or small fi, an efficient algorithm is to perform an all-to-all broadcast of each processor's portion of the reduced system, leaving the entire reduced system on each processor <ref> [GGJT96] </ref>. Each processor then solves this system locally. The advantage of this scheme is that there is only one communication step, albeit an expensive one whose cost grows quickly with P .
Reference: [Joh87] <author> L. Johnsson. </author> <title> Solving tridiagonal systems on ensemble architectures. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 8 </volume> <pages> 354-392, </pages> <year> 1987. </year>
Reference-contexts: They are therefore limited to an efficiency of no greater than 25%. Nonetheless, these algorithms scale well and must be included in a parallel library. Many studies of parallel computing reported in the literature <ref> [Wri91, AG95, GGJT96, BCD + 94, Joh87, LS84] </ref> address either the general banded problem or special cases such as the tridiagonal problem. Wright [Wri91] presented an ambitious attempt at incorporating pivoting, both row and column, throughout the entire calculation.
Reference: [LS84] <author> D. Lawrie and A. Sameh. </author> <title> The computation and communication complexity of a parallel banded system solver. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 10 </volume> <pages> 185-195, </pages> <year> 1984. </year>
Reference-contexts: They are therefore limited to an efficiency of no greater than 25%. Nonetheless, these algorithms scale well and must be included in a parallel library. Many studies of parallel computing reported in the literature <ref> [Wri91, AG95, GGJT96, BCD + 94, Joh87, LS84] </ref> address either the general banded problem or special cases such as the tridiagonal problem. Wright [Wri91] presented an ambitious attempt at incorporating pivoting, both row and column, throughout the entire calculation. <p> The literature has several divide-and-conquer algorithms for unsymmetric matrices (see, for example, that of Lawrie and Sameh <ref> [LS84] </ref>). However, the majority of these have an unsymmetric aspect that has two practical drawbacks: it results in unnecessarily large and complicated reduced systems, and it complicates reusing the algorithmic structure of the symmetric positive definite codes.
Reference: [Wri91] <author> S. Wright. </author> <title> Parallel algorithms for banded linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12(4) </volume> <pages> 824-843, </pages> <year> 1991. </year>
Reference-contexts: They are therefore limited to an efficiency of no greater than 25%. Nonetheless, these algorithms scale well and must be included in a parallel library. Many studies of parallel computing reported in the literature <ref> [Wri91, AG95, GGJT96, BCD + 94, Joh87, LS84] </ref> address either the general banded problem or special cases such as the tridiagonal problem. Wright [Wri91] presented an ambitious attempt at incorporating pivoting, both row and column, throughout the entire calculation. <p> Nonetheless, these algorithms scale well and must be included in a parallel library. Many studies of parallel computing reported in the literature [Wri91, AG95, GGJT96, BCD + 94, Joh87, LS84] address either the general banded problem or special cases such as the tridiagonal problem. Wright <ref> [Wri91] </ref> presented an ambitious attempt at incorporating pivoting, both row and column, throughout the entire calculation. His single-width separator algorithm without pivoting is similar to the work described here. However, Wright targeted relatively small parallel systems and, in particular, used a sequential method for the reduced system.
References-found: 12

