URL: http://www.cs.ucl.ac.uk/staff/B.Southall/papers/ECCV98.ps.gz
Refering-URL: http://www.cs.ucl.ac.uk/staff/B.Southall/pubs.html
Root-URL: http://www.cs.ucl.ac.uk
Email: B.Southall@cs.ucl.ac.uk  
Phone: 2  
Title: Model based tracking for navigation and segmentation  
Author: B Southall ; J A Marchant T Hague and B F Buxton 
Keyword: Key Words: Model-based tracking, parallel update Kalman filter, performance analysis  
Address: Wrest Park, Silsoe, Bedfordshire, MK45 4HS, UK  College London, Gower Street, London, WC1E 6BT, UK  
Affiliation: 1 Silsoe Research Institute,  Department of Computer Science, University  
Abstract: An autonomous vehicle has been developed for precision application of treatment on outdoor crops. This document details a new vision algorithm to aid navigation and crop/weed discrimination being developed for this machine. The algorithm tracks a model of the crop planting pattern through an image sequence using an extended Kalman filter. A parallel update scheme is used to provide not only navigation information for the vehicle controller but also estimates of plant position for the treatment system. The algorithm supersedes a previous Hough transform tracking technique currently used on the vehicle which provides navigation information alone, from the rows of plants. The crop planting model is introduced and the tracking system developed, along with a method for automatically starting the algorithm. In applications such as this, where the vehicle traverses unsurfaced outdoor terrain, "ground truth" data for the path taken by the vehicle is unavailable; lacking this veridical information, the algorithm's performance is evaluated with respect to human assessment and the previous row-only tracking algorithm, and found to offer improvements over the previous technique. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Y Bar-Shalom and T Fortmann. </author> <title> Tracking and Data Association. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Note that the observation model h is both a function of the state x (k), and m and n, the integer variables which index the crop grid so one state estimate generates 3 fi m max observations. The observation model is also non-linear, so an extended Kalman filter <ref> [1] </ref> is required. 3.2 The Extended Kalman Filter The extended Kalman filter allows a linearisation of the observation model about the state prediction point, and although it offers a sub-optimal solution to the estimation problem, and can converge to local minima (and therefore lose track) if poorly initialised, it offers a <p> From [8], an estimate of root mean square error has been obtained on the accuracy of the Hough transform algorithm, giving r.m.s error of 12.5 mm on t x and 1 o on . As noted in <ref> [1] </ref>, in the extended Kalman filter the matrix P is not strictly a covariance, but a measure of mean square error on the estimate ^x, so these values of offset and angular error may be used directly.
Reference: 2. <author> T Hague, J A Marchant, and N D Tillett. </author> <title> Autonomous robot navigation for precision horticulture. </title> <booktitle> In IEEE International Conference on Robotics and Automation. </booktitle> <address> Albuquerque, </address> <year> 1997. </year>
Reference-contexts: 1 Introduction An autonomous agricultural vehicle <ref> [2] </ref> has been developed at the Silsoe Research Institute to perform the task of plant scale husbandry, which aims, for example, to reduce the use of chemicals in crop protection by treating individual plants and weeds separately, with little waste chemical sprayed onto the bare earth.
Reference: 3. <author> T Hague and N D Tillett. </author> <title> Navigation and control of an autonomous horticultural robot. </title> <journal> Mechatronics, </journal> <volume> 6(2) </volume> <pages> 165-180, </pages> <year> 1996. </year>
Reference-contexts: The crop row structure is a cue used by the vehicle, in combination with non-vision sensors <ref> [3] </ref>, to navigate along the rows. A Hough transform algorithm for tracking the plant rows has been previously reported [7] [8] and is currently part of the system. <p> feature largely in further research, although results show that this relatively crude method produces satisfactory results. 3.4 Covariance Matrices Three covariance matrices are required to run the filter; a measure of process noise Q (k), which may be obtained from the Kalman filter used to estimate the vehicle motion parameters <ref> [3] </ref>, an observation noise matrix R (k; m; n), which will be discussed here, and an initial value of the estimate covariance P (0j0), which will be provided in section 4.
Reference: 4. <author> C Harris. </author> <title> Tracking with rigid models. In A Blake and A Yuille, editors, Active Vision, chapter 4. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: A second advantage of the new algorithm is that the Kalman filter provides a covariance matrix for the state estimate which can be given to the vehicle controller; the Hough transform does not produce these covariances. The use of a rigid model in tracking systems is well established, notably <ref> [4] </ref> where the rigid model in question consisted of control points on straight edge segments, thus suitable mainly for man-made objects. The problem here is extracting man-made structure imposed on a natural world.
Reference: 5. <author> R E Kalman. </author> <title> A new approach to linear filtering and prediction problems. </title> <journal> Transactions of the ASME Journal of Basic Engineering, </journal> <year> 1960. </year>
Reference-contexts: seen in a single image is 15, so m max = 5 generates a suitable number of predictions (if there are less than 15 plants in the image, then some predictions will lie outside the bounds of the image; such predictions are ignored). 3 Tracking the Pattern The Kalman filter <ref> [5] </ref> is used to provide a means of tracking the plant model through the image sequence by predicting the crop structure position (and hence the individual plant positions) and using observations of plants taken from the image to correct this prediction.
Reference: 6. <author> J J Leonard and H F Durrant-Whyte. </author> <title> Mobile robot localization by tracking geometric beacons. </title> <journal> IEEE Trans. Robotics and Automation, </journal> <volume> 7(3) </volume> <pages> 376-382, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Feature matching is achieved using the nearest-neighbour data association procedure [9], with each associ ated feature being validated prior to incorporation (see below). The incorporation itself is performed using a batch update method as used in <ref> [6] </ref> (originally appearing in [13]) as opposed to the more traditional recursive estimation procedure.
Reference: 7. <author> J A Marchant. </author> <title> Tracking of row structure in three crops using image analysis. </title> <journal> Computers and electronics in agriculture, </journal> <volume> 15(9) </volume> <pages> 161-179, </pages> <year> 1996. </year>
Reference-contexts: The crop row structure is a cue used by the vehicle, in combination with non-vision sensors [3], to navigate along the rows. A Hough transform algorithm for tracking the plant rows has been previously reported <ref> [7] </ref> [8] and is currently part of the system. The existing algorithm extracts only the direction and offset of the rows of plants whereas the new algorithm also provides an estimate of individual plant positions which can then be used to target treatment by the spray system.
Reference: 8. <author> J A Marchant and R Brivot. </author> <title> Real time tracking of plant rows using a hough transform. Real Time Imaging, </title> <booktitle> 1 </booktitle> <pages> 363-371, </pages> <year> 1995. </year>
Reference-contexts: The crop row structure is a cue used by the vehicle, in combination with non-vision sensors [3], to navigate along the rows. A Hough transform algorithm for tracking the plant rows has been previously reported [7] <ref> [8] </ref> and is currently part of the system. The existing algorithm extracts only the direction and offset of the rows of plants whereas the new algorithm also provides an estimate of individual plant positions which can then be used to target treatment by the spray system. <p> terms of a heading angle and offset of the camera (and hence the vehicle) relative to the rows, and the parameter Y , in conjunction with model parameters D 1 and D 1 can yield the position of individual plants via equations 1 and 2 (extended from those presented in <ref> [8] </ref>) below. x w = nr (1) The quantities n 2 f1; 0; 1g and m 2 f0; 1; 2; : : : ; (m max 1)g index into the 3 fi m max grid formed by the planting pattern. <p> Owing to the angle at which the camera is mounted on the vehicle, features on the ground plane are viewed on the image plane under a perspective projection. Marchant and Brivot <ref> [8] </ref> arrive at the following expressions for the 2D image plane co-ordinates (x u ; y u ) of 3D world points (x w ; y w ; 0) (t y ; t z and OE are explained below): x u = f x w sin sin OE y w cos <p> The quantity t z is distance along the optic axis between the camera optical centre and the intersection with the ground plane, and t y gives the offset (in camera co-ordinates) of the point where the optical axis intersects the world x w axis and, as in <ref> [8] </ref>, this can be set to zero. One further assumption is that the angle is small enough for the approximations cos 1 and sin to hold. <p> The sequences thus contain both crop and weeds, with some plants missing from the row structure. The images are collected using a camera sensitive to near infra-red wavelengths where contrast between plant and soil matter is enhanced <ref> [8] </ref> (the camera has a visible light blocking filter). Two methods have been used to extract plant positions from the images; an automatic method described below, and manual selection of features by a human, which is used to evaluate the performance of the tracker independently of the feature extraction mechanism. <p> The automatic feature extraction method is that used in <ref> [8] </ref>, which chain-codes areas of the infra-red image exceeding a grey-level threshold (currently, the vehicle uses dedicated hardware to perform this function). The centre of each chain code is calculated and used as a candidate feature to be matched to a prediction generated by the Kalman filter. <p> a working alternative. 4 Starting the Algorithm To start the tracking process, an initial estimate of the crop pattern position x is acquired in two stages; the initial estimates of t x and are obtained by a global search of the Hough space which is formed using the methods of <ref> [8] </ref>. The model position Y and row offsets D 1 and D 1 are then obtained from Fourier analysis of 1D image samples taken along the extracted rows. By sampling along the line of the rows in the image a profile of the grey-levels along the row is obtained. <p> Once initial values for t x ; Y and have been obtained, the state estimate ^x (0j0) may be formed, leaving only the initial state covariance P (0j0) undetermined. From <ref> [8] </ref>, an estimate of root mean square error has been obtained on the accuracy of the Hough transform algorithm, giving r.m.s error of 12.5 mm on t x and 1 o on . <p> To obtain a measure of mean square error for the estimate of row offset, the method of <ref> [8] </ref> was used; the row offset algorithm was applied to 40 images, and estimates of the central row offset were acquired. <p> Human assessment of the model position; a mouse-driven program has been designed to allow the user to place the crop pattern on each image in the sequence. Data from three different people has been collected (HUMAN 1-3). 4. The method of <ref> [8] </ref>, which produces estimates of t x and alone (HOUGH). plots the three human responses, whilst the right hand column shows the equivalent automatic results (note that there is no Y estimate available from the Hough transform algorithm).
Reference: 9. <author> B Rao. </author> <title> Data association methods for tracking systems. In A Blake and A Yuille, editors, Active Vision, chapter 6. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Feature matching is achieved using the nearest-neighbour data association procedure <ref> [9] </ref>, with each associ ated feature being validated prior to incorporation (see below). The incorporation itself is performed using a batch update method as used in [6] (originally appearing in [13]) as opposed to the more traditional recursive estimation procedure.
Reference: 10. <author> D Reynard, </author> <title> A Wildenberg, A Blake, and J A Marchant. Learning dynamics of complex motions from image sequences. </title> <editor> In B Buxton and R Cipolla, editors, </editor> <booktitle> Computer Vision - ECCV `96, Lecture Notes in Computer Science. </booktitle> <publisher> Springer, </publisher> <month> April </month> <year> 1996. </year>
Reference-contexts: The two methods mentioned make use of plant models (in [11] a cluster of chain-coded areas, in <ref> [10] </ref> a more complex shape model) both of which could potentially be distracted by large clusters of weed material. By using the information available on the planting pattern, the search for plant material is constrained, and weed patches away from the crop structure will be ignored.
Reference: 11. <author> J M Sanchiz, </author> <title> F Pla, J A Marchant, and R Brivot. Structure from motion techniques applied to crop field mapping. </title> <journal> Image and Vision Computing, </journal> <volume> 14 </volume> <pages> 353-363, </pages> <year> 1996. </year>
Reference-contexts: The two methods mentioned make use of plant models (in <ref> [11] </ref> a cluster of chain-coded areas, in [10] a more complex shape model) both of which could potentially be distracted by large clusters of weed material.
Reference: 12. <author> R Y Tsai. </author> <title> An efficient and accurate camera calibration technique for 3d machine vision. </title> <booktitle> In Proc. IEEE Computer Soc. Conf. Comp Vis Patt Recog, </booktitle> <address> Miami Beach, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: and OE are explained below): x u = f x w sin sin OE y w cos sin OE + t z y u = f x w sin sin OE y w cos sin OE + t z Equations 3 and 4 are derived from those given by Tsai <ref> [12] </ref>, with the added assumption that the camera does not pitch or roll (i.e that the camera axes (x c ; y c ; z c ) of figure 1 do not rotate about the world axes x w or y w ).
Reference: 13. <author> D Willner, C B Chang, and K P Dunn. </author> <title> Kalman filter algorithms for a multi-sensor system. </title> <booktitle> In Proc. IEEE Int. Conf. Decision and Control, </booktitle> <pages> pages 570-574, </pages> <year> 1976. </year>
Reference-contexts: Feature matching is achieved using the nearest-neighbour data association procedure [9], with each associ ated feature being validated prior to incorporation (see below). The incorporation itself is performed using a batch update method as used in [6] (originally appearing in <ref> [13] </ref>) as opposed to the more traditional recursive estimation procedure.
References-found: 13

