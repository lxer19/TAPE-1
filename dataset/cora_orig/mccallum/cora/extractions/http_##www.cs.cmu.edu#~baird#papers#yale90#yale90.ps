URL: http://www.cs.cmu.edu/~baird/papers/yale90/yale90.ps
Refering-URL: http://www.cs.cmu.edu/~baird/papers/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A MATHEMATICAL ANALYSIS OF ACTOR-CRITIC ARCHITECTURES FOR LEARNING OPTIMAL CONTROLS THROUGH INCREMENTAL DYNAMIC PROGRAMMING  
Author: Ronald J. Williams and Leemon C. Baird, III 
Address: Boston, MA 02115  
Affiliation: College of Computer Science Northeastern University  
Abstract: Combining elements of the theory of dynamic programming with features appropriate for on-line learning has led to an approach Watkins has called incremental dynamic programming. Here we adopt this incremental dynamic programming point of view and obtain some preliminary mathematical results relevant to understanding the capabilities and limitations of actor-critic learning systems. Examples of such systems are Samuel's learning checker player, Hol-land's bucket brigade algorithm, Witten's adaptive controller, and the adaptive heuristic critic algorithm of Barto, Sutton, and Anderson. Particular emphasis here is on the effect of complete asynchrony in the updating of the actor and the critic across individual states or state-action pairs. The main results are that, while convergence to optimal performance is not guaranteed in general, there are a number of situations in which such convergence is assured. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 835-846. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning and sequential decision making (COINS Technical Report 89-95). </title> <address> Amherst, MA: </address> <institution> University of Massachusetts, Department of Computer and Information Science. </institution>
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibility of general-purpose learning algorithms applied to rule-based systems. </title> <editor> In: R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.) </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume II. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Ross, S. </author> <year> 1983). </year> <title> Introduction to Stochastic Dynamic Programming. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: We let v fl denote the ideal evaluation function, or the return from any optimal policy. 1 Nevertheless, with the intent of eventually extending these results, we adhere throughout to the probabilistic formulation used in the theory of stochastic dynamic programming <ref> (Ross, 1983) </ref>. 2.3 Markov Decision Task For purposes of this paper we define a (stationary, finite) Markov decision task, to be a 5-tuple (X; A; r; f; fl) consisting of a finite state set X, finite action set A, stationary random reward function r, stationary random transition function f, and discount
Reference: <author> Samuel, A. L. </author> <year> (1957). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3, </volume> <pages> 210-229. </pages>
Reference: <editor> Reprinted in: E. A. Feigenbaum and J. Feldman (Eds.) </editor> <booktitle> (1963). Computers and Thought. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal credit assignment in reinforcement learning. </title> <type> Ph.D. Dissertation, </type> <institution> University of Massachusetts, </institution> <note> Amherst (also COINS Technical Report 84-02). </note>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> First results with DYNA, an integrated architecture for learning, planning, and reacting. </title> <booktitle> Stanford Spring Symposium on Planning. </booktitle>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Ph.D. Dissertation, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address>
Reference-contexts: 1 Introduction The purpose of this paper is to present some results in the theory of incremental dynamic programming <ref> (Watkins, 1989) </ref>, which combines elements of the theory of dynamic programming with features appropriate for on-line learning in the absence of an a priori model of the environment.
Reference: <author> Werbos, P. J. </author> <year> (1987). </year> <title> Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 17, </volume> <pages> 7-20. </pages>
Reference: <author> Witten, I. H. </author> <year> (1977). </year> <title> An adaptive optimal controller for discrete-time Markov environments. </title> <journal> Information and Control, </journal> <volume> 34, </volume> <pages> 286-295. Page 6 </pages>
References-found: 12

