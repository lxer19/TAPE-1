URL: http://www.graphics.stanford.edu/papers/light/light.ps.gz
Refering-URL: http://www.graphics.stanford.edu/papers/light/
Root-URL: http://www.cs.stanford.edu
Title: Light Field Rendering  
Author: Marc Levo and Pat Hanrahan 
Keyword: CR Categories: I.3.2 [Computer Graphics]: Picture/Image Generation Digitizing and scanning, Viewing algorithms; I.4.2 [Computer Graphics]: Compression Approximate methods Additional keywords: image-based rendering, light field, holographic stereogram, vector quantization, epipolar analysis  
Affiliation: Computer Science Department Stanford University  
Note: To appear in SIGGRAPH '96  
Abstract: A number of techniques have been proposed for ying through scenes by redisplaying previously rendered or digitized views. Techniques have also been proposed for interpolating between views by warping input images, using depth information or correspondences between multiple images. In this paper, we describe a simple and robust method for generating new views from arbitrary camera positions without depth information or feature matching, simply by combining and resampling the available images. The key to this technique lies in interpreting the input images as 2D slices of a 4D function - the light field. This function completely characterizes the ow of light through unobstructed space in a static scene with fixed illumination. We describe a sampled representation for light fields that allows for both efficient creation and display of inward and outward looking views. We hav e created light fields from large arrays of both rendered and digitized images. The latter are acquired using a video camera mounted on a computer-controlled gantry. Once a light field has been created, new views may be constructed in real time by extracting slices in appropriate directions. Since the success of the method depends on having a high sample rate, we describe a compression system that is able to compress the light fields we have generated by more than a factor of 100:1 with very little loss of fidelity. We also address the issues of antialiasing during creation, and resampling during slice extraction. 
Abstract-found: 1
Intro-found: 1
Reference: [Adelson91] <author> Adelson, E.H., Bergen, J.R., </author> <title> ``The Plenoptic Function and the Elements of Early Vision,'' In Computation Models of Visual Processing, </title> <editor> M. Landy and J.A. Movshon, eds., </editor> <publisher> MIT Press, </publisher> <address> Cam bridge, </address> <year> 1991. </year>
Reference: [Ashdown93] <author> Ashdown, I., </author> <title> ``Near-Field Photometry: A New Approach,'' </title> <journal> Journal of the Illuminating Engineering Society, </journal> <volume> Vol. 22, No. 1, </volume> <pages> Win ter, </pages> <year> 1993, </year> <pages> pp. 163-180. </pages>
Reference-contexts: The redundancy of the 5D representation is undesirable for two reasons: first, redundancy increases the size of the total dataset, and second, redundancy complicates the reconstruction of the radiance function from its samples. This reduction in dimension has been used to simplify the representation of radiance emitted by luminaires <ref> [Levin71, Ashdown93] </ref>. For the remainder of this paper we will be only concerned with 4D light fields. 1 For those familiar with Gershun's paper, he actually uses the term light field to mean the irradiance vector as a function of position. For this reason P.
Reference: [Beers96] <author> Beers, A., Agrawala, M., Chaddha, N., </author> <title> ``Rendering from Com pressed Textures.'' </title> <booktitle> In these proceedings. </booktitle>
Reference-contexts: The compression scheme we chose was a two-stage pipeline consisting of fixed-rate vector quantization followed by entropy coding (Lempel-Ziv), as shown in figure 11. Following similar motivations, Beers et al. use vector quantization to compress textures for use in rendering pipelines <ref> [Beers96] </ref>. 4.1. Vector quantization The first stage of our compression pipeline is vector quantization (VQ) [Gersho92], a lossy compression technique wherein a vector of samples is quantized to one of a number of predetermined reproduction vectors.
Reference: [Benton83] <author> Benton, S., </author> <title> ``Survey of Holographic Stereograms,'' Processing and Display of Three-Dimensional Data, </title> <booktitle> Proc. SPIE, </booktitle> <volume> Vol. 367, </volume> <year> 1983. </year>
Reference-contexts: Third, image generation involves only resampling, a simple linear process. This representation of the light field is similar to the epipo-lar volumes used in computer vision [Bolles87] and to horizontal-parallax-only holographic stereograms <ref> [Benton83] </ref>. An epipolar volume is formed from an array of images created by translating a camera in equal increments in a single direction. Such a representation has recently been used to perform view interpolation [Katayama95].
Reference: [Blinn76] <author> Blinn, J.F., Newell, </author> <title> M.E., ``Texture and Reection in Computer Generated Images,'' </title> <journal> CACM, </journal> <volume> Vol. 19, No. 10, </volume> <month> October, </month> <year> 1976, </year> <pages> pp. 542-547. </pages>
Reference-contexts: In fact, the two can be mixed together. The forerunner to these techniques is the use of environment maps to capture the incoming light in a texture map <ref> [Blinn76, Greene86] </ref>. An environment map records the incident light arriving from all directions at a point. The original use of environment maps was to efficiently approximate reections of the environment on a surface.
Reference: [Bolles87] <author> Bolles, R., Baker, H., Marimont, D., </author> <title> ``Epipolar-Plane Image Analysis: An Approach to Determining Structure from Motion,'' </title> <journal> International Journal of Computer Vision, </journal> <volume> Vol. 1, No. 1, </volume> <year> 1987, </year> <pages> pp. 7-55. </pages>
Reference-contexts: Second, no model information, such as depth values or image correspondences, is needed to extract the image values. Third, image generation involves only resampling, a simple linear process. This representation of the light field is similar to the epipo-lar volumes used in computer vision <ref> [Bolles87] </ref> and to horizontal-parallax-only holographic stereograms [Benton83]. An epipolar volume is formed from an array of images created by translating a camera in equal increments in a single direction. Such a representation has recently been used to perform view interpolation [Katayama95].
Reference: [Chen93] <author> Chen, S.E., Williams, L., </author> <title> ``View Interpolation for Image Syn thesis,'' </title> <booktitle> Proc. SIGGRAPH '93 (Anaheim, </booktitle> <address> California, </address> <month> August 1-6, </month> <year> 1993). </year> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, </booktitle> <pages> pp. 279-288. </pages>
Reference-contexts: The major limitation of rendering systems based on environment maps is that the viewpoint is fixed. One way to relax this fixed position constraint is to use view interpolation <ref> [Chen93, Greene94, Fuchs94, McMillan95a, McMillan95b, Narayanan95] </ref>. Most of these methods require a depth value for each pixel in the environment map, which is easily provided if the environment maps are synthetic images.
Reference: [Chen95] <author> Chen, </author> <title> S.E., ``QuickTime VR An Image-Based Approach to Virtual Environment Navigation,'' </title> <booktitle> Proc. </booktitle> <address> SIGGRAPH '95 (Los Ange-les, CA, </address> <month> August 6-11, </month> <year> 1995). </year> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIGGRAPH, </booktitle> <pages> pp. 29-38. </pages>
Reference-contexts: However, environment maps also may be used to quickly display any outward looking view of the environment from a fixed location but at a variable orientation. This is the basis of the Apple QuickTimeVR system <ref> [Chen95] </ref>. In this system environment maps are created at key locations in the scene. The user is able to navigate discretely from location to location, and while at each location continuously change the viewing direction. <p> The camera in such a gantry moves along a spherical surface, always pointing at the center of the sphere. Apple Computer has constructed such a gantry to acquire imagery for Quick-Time VR yarounds <ref> [Chen95] </ref>. Unfortunately, the lighting in their system is attached to the moving camera, so it is unsuitable for acquiring static light fields.
Reference: [Fuchs94] <author> Fuchs, H., Bishop, G., Arthur, K., McMillan, L., Bajcsy, R., Lee, S.W., Farid, H., Kanade, T., </author> <title> ``Virtual Space Teleconferencing Using a Sea of Cameras,'' </title> <booktitle> Proc. First International Conference on Medical Robotics and Computer Assisted Surgery, </booktitle> <year> 1994, </year> <pages> pp. 161-167. </pages>
Reference-contexts: The major limitation of rendering systems based on environment maps is that the viewpoint is fixed. One way to relax this fixed position constraint is to use view interpolation <ref> [Chen93, Greene94, Fuchs94, McMillan95a, McMillan95b, Narayanan95] </ref>. Most of these methods require a depth value for each pixel in the environment map, which is easily provided if the environment maps are synthetic images.
Reference: [Gersho92] <author> Gersho, A., Gray, </author> <title> R.M., Vector Quantization and Signal Com pression, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Following similar motivations, Beers et al. use vector quantization to compress textures for use in rendering pipelines [Beers96]. 4.1. Vector quantization The first stage of our compression pipeline is vector quantization (VQ) <ref> [Gersho92] </ref>, a lossy compression technique wherein a vector of samples is quantized to one of a number of predetermined reproduction vectors.
Reference: [Gershun36] <author> Gershun, A., </author> <title> ``The Light Field,'' </title> <address> Moscow, </address> <year> 1936. </year> <title> Translated by P. Moon and G. </title> <journal> Timoshenko in Journal of Mathematics and Physics, </journal> <volume> Vol. </volume> <pages> XVIII, </pages> <publisher> MIT, </publisher> <year> 1939, </year> <pages> pp. 51-151. </pages>
Reference-contexts: Note that our definition is equivalent to the plenoptic function introduced by Adelson and Bergen [Adel-son91]. The phrase light field was coined by A. Gershun in his classic paper describing the radiometric properties of light in a space <ref> [Gershun36] </ref>. 1 McMillan and Bishop [McMillan95b] discuss the representation of 5D light fields as a set of panoramic images at different 3D locations. However, the 5D representation may be reduced to 4D in free space (regions free of occluders).
Reference: [Gortler96] <author> Gortler, S.J., Grzeszczuk, R., Szeliski, R., Cohen, M., </author> <booktitle> ``The Lumigraph.'' In these proceedings. </booktitle>
Reference-contexts: Calibrations and alignments are verified with the aid of a Faro digitizing arm, which is accu rate to 0.3 mm. Human versus computer-controlled. An inexpensive approach to digitizing light fields is to move a handheld camera through the scene, populating the field from the resulting images <ref> [Gortler96] </ref>. This approach necessitates estimating camera pose at each frame and interpolating the light field from scattered data - two challenging problems. To simplify the situation, we chose instead to build a computer-controlled camera gantry and to digitize images on a regular grid. Spherical versus planar camera motion.
Reference: [Greene86] <author> Greene, N., </author> <title> ``Environment Mapping and Other Applications of World Projections,'' </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> Vol. 6, No. 11, </volume> <month> November, </month> <year> 1986, </year> <pages> pp. 21-29. </pages>
Reference-contexts: In fact, the two can be mixed together. The forerunner to these techniques is the use of environment maps to capture the incoming light in a texture map <ref> [Blinn76, Greene86] </ref>. An environment map records the incident light arriving from all directions at a point. The original use of environment maps was to efficiently approximate reections of the environment on a surface.
Reference: [Greene94] <author> Greene, N. and Kass, M., </author> <title> ``Approximating Visibility with Environment Maps,'' </title> <type> Apple Technical Report No. 41, </type> <month> November, </month> <year> 1994. </year>
Reference-contexts: The major limitation of rendering systems based on environment maps is that the viewpoint is fixed. One way to relax this fixed position constraint is to use view interpolation <ref> [Chen93, Greene94, Fuchs94, McMillan95a, McMillan95b, Narayanan95] </ref>. Most of these methods require a depth value for each pixel in the environment map, which is easily provided if the environment maps are synthetic images.
Reference: [Halle94] <author> Halle, M., </author> <title> ``Holographic Stereograms as Discrete Imaging Systems.'' Practical Holography, </title> <booktitle> Proc. SPIE, </booktitle> <volume> Vol. 2176, </volume> <month> February, </month> <year> 1994. </year>
Reference-contexts: A holographic stereogram is formed by exposing a piece of film to an array of images captured by a camera moving sideways. Halle has discussed how to set the camera aperture to properly acquire images for holographic stereograms <ref> [Halle94] </ref>, and that theory is applicable to this work. Gavin Miller has also recognized the potential synergy between true 3D display technologies and computer graphics algorithms [Miller95]. There are several major challenges to using the light field approach to view 3D scenes on a graphics workstation. <p> The theory behind this filtering process has been discussed in the context of holographic stere-ograms by Halle <ref> [Halle94] </ref>. Note that although prefiltering has the desired effect of antialiasing the light field, it has what at first seems like an undesirable side effect introducing blurriness due to depth of field. However, this blurriness is precisely correct for the situation. <p> Another solution is to employ a view camera in which the sensor and optical system translate in parallel planes, the former moving faster than the latter. Horizontal parallax holographic stereograms are constructed using such a camera <ref> [Halle94] </ref>. Incorporating this solution into a gantry that moves both horizontally and vertically is difficult. We instead chose to equip our camera with pan and tilt motors, enabling us to use a narrow-angle lens.
Reference: [Katayama95] <author> Katayama, A., Tanaka, K., Oshino, T., Tamura, H., </author> <title> ``Viewpoint-Dependent Stereoscopic Display Using Interpolation of Multi-viewpoint Images,'' Stereoscopic Displays and Virtual Reality Systems II, </title> <booktitle> Proc. SPIE, </booktitle> <volume> Vol. 2409, </volume> <editor> S. Fisher, J. Merritt, B. Bolas eds. </editor> <year> 1995, </year> <pages> pp. 11-20. </pages>
Reference-contexts: An epipolar volume is formed from an array of images created by translating a camera in equal increments in a single direction. Such a representation has recently been used to perform view interpolation <ref> [Katayama95] </ref>. A holographic stereogram is formed by exposing a piece of film to an array of images captured by a camera moving sideways. Halle has discussed how to set the camera aperture to properly acquire images for holographic stereograms [Halle94], and that theory is applicable to this work.
Reference: [Laveau94] <author> Laveau, S., Faugeras, </author> <title> O.D., ``3-D Scene Representation as a Collection of Images and Fundamental Matrices,'' </title> <type> INRIA Technical Report No. 2205, </type> <year> 1994. </year>
Reference-contexts: The key challenge in this warping approach is to "fill in the gaps" when previously occluded areas become visible. Another approach to interpolating between acquired images is to find corresponding points in the two <ref> [Laveau94, McMillan95b, Seitz95] </ref>. If the positions of the cameras are known, this is equivalent to finding the depth values of the corresponding points.
Reference: [Levin71] <author> Levin, R., </author> <title> ``Photometric Characteristics of Light Controlling Apparatus,'' </title> <journal> Illuminating Engineering, </journal> <volume> Vol. 66, No. 4, </volume> <year> 1971, </year> <pages> pp. 205-215. </pages>
Reference-contexts: The redundancy of the 5D representation is undesirable for two reasons: first, redundancy increases the size of the total dataset, and second, redundancy complicates the reconstruction of the radiance function from its samples. This reduction in dimension has been used to simplify the representation of radiance emitted by luminaires <ref> [Levin71, Ashdown93] </ref>. For the remainder of this paper we will be only concerned with 4D light fields. 1 For those familiar with Gershun's paper, he actually uses the term light field to mean the irradiance vector as a function of position. For this reason P.
Reference: [McMillan95a] <author> McMillan, L., Bishop, G., </author> <title> ``Head-Tracked Stereoscopic Display Using Image Warping,'' Stereoscopic Displays and Virtual Reality Systems II, </title> <booktitle> Proc. SPIE, </booktitle> <volume> Vol. 2409, </volume> <editor> S. Fisher, J. Merritt, B. Bolas eds. </editor> <year> 1995, </year> <pages> pp. 21-30. </pages>
Reference-contexts: The major limitation of rendering systems based on environment maps is that the viewpoint is fixed. One way to relax this fixed position constraint is to use view interpolation <ref> [Chen93, Greene94, Fuchs94, McMillan95a, McMillan95b, Narayanan95] </ref>. Most of these methods require a depth value for each pixel in the environment map, which is easily provided if the environment maps are synthetic images.
Reference: [McMillan95b] <author> McMillan, L., Bishop, G., </author> <title> Plenoptic Modeling: An Image-Based Rendering System, </title> <booktitle> Proc. </booktitle> <address> SIGGRAPH '95 (Los Ange-les, CA, </address> <month> August 6-11, </month> <year> 1995). </year> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIGGRAPH, </booktitle> <pages> pp. 39-46. </pages>
Reference-contexts: The major limitation of rendering systems based on environment maps is that the viewpoint is fixed. One way to relax this fixed position constraint is to use view interpolation <ref> [Chen93, Greene94, Fuchs94, McMillan95a, McMillan95b, Narayanan95] </ref>. Most of these methods require a depth value for each pixel in the environment map, which is easily provided if the environment maps are synthetic images. <p> The key challenge in this warping approach is to "fill in the gaps" when previously occluded areas become visible. Another approach to interpolating between acquired images is to find corresponding points in the two <ref> [Laveau94, McMillan95b, Seitz95] </ref>. If the positions of the cameras are known, this is equivalent to finding the depth values of the corresponding points. <p> Note that our definition is equivalent to the plenoptic function introduced by Adelson and Bergen [Adel-son91]. The phrase light field was coined by A. Gershun in his classic paper describing the radiometric properties of light in a space [Gershun36]. 1 McMillan and Bishop <ref> [McMillan95b] </ref> discuss the representation of 5D light fields as a set of panoramic images at different 3D locations. However, the 5D representation may be reduced to 4D in free space (regions free of occluders).
Reference: [Miller95] <author> Miller, G., </author> <title> ``Volumetric Hyper-Reality: A Computer Graphics Holy Grail for the 21st Century?,'' </title> <booktitle> Proc. Graphics Interface '95, </booktitle> <editor> W. Davis and P. Prusinkiewicz eds., </editor> <booktitle> Canadian Information Processing Society, </booktitle> <year> 1995, </year> <pages> pp. 56-64. </pages>
Reference-contexts: Halle has discussed how to set the camera aperture to properly acquire images for holographic stereograms [Halle94], and that theory is applicable to this work. Gavin Miller has also recognized the potential synergy between true 3D display technologies and computer graphics algorithms <ref> [Miller95] </ref>. There are several major challenges to using the light field approach to view 3D scenes on a graphics workstation. First, there is the choice of parameterization and representation of the light field. Related to this is the choice of sampling pattern for the field.
Reference: [Moon81] <author> Moon, P., Spencer, D.E., </author> <title> The Photic Field, </title> <publisher> MIT Press, </publisher> <year> 1981. </year>
Reference-contexts: For the remainder of this paper we will be only concerned with 4D light fields. 1 For those familiar with Gershun's paper, he actually uses the term light field to mean the irradiance vector as a function of position. For this reason P. Moon in a later book <ref> [Moon81] </ref> uses the term photic field to denote what we call the light field. Although restricting the validity of the representation to free space may seem like a limitation, there are two common situations where this assumption is useful. First, most geometric models are bounded.
Reference: [Narayanan95] <author> Narayanan, P.J., </author> <title> ``Virtualized Reality: Concepts and Early Results,'' </title> <booktitle> Proc. IEEE Workshop on the Representation of Visual Scenes, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: The major limitation of rendering systems based on environment maps is that the viewpoint is fixed. One way to relax this fixed position constraint is to use view interpolation <ref> [Chen93, Greene94, Fuchs94, McMillan95a, McMillan95b, Narayanan95] </ref>. Most of these methods require a depth value for each pixel in the environment map, which is easily provided if the environment maps are synthetic images.
Reference: [Nimeroff94] <author> Nimeroff, J., Simoncelli, E., Dorsey, J., </author> <title> ``Efficient Re rendering of Naturally Illuminated Scenes,'' </title> <booktitle> Proc. Fifth Eurograph ics Rendering Workshop, </booktitle> <year> 1994, </year> <pages> pp. 359-373. </pages>
Reference-contexts: Third, the illumination must be fixed. If we ignore interreections, this limitation can be addressed by augmenting light fields to include surface normals and optical properties. To handle interreections, we might try representing illumination as a superposition of basis functions <ref> [Nimeroff94] </ref>. This would correspond in our case to computing a sum of light fields each lit with a different illumination function. It is useful to compare this approach with depth-based or correspondence-based view interpolation.
Reference: [Sbert93] <author> Sbert, </author> <title> A.M., ``An Integral Geometry Based Method for Form-Factor Computation,'' </title> <journal> Computer Graphics Forum, </journal> <volume> Vol. 13, No. 3, </volume> <year> 1993, </year> <pages> pp. 409-420. </pages>
Reference-contexts: In this sense, a uniform sampling pattern is one where the number of lines in intervals between samples is constant ev erywhere. Note that the correct measure for number of lines is related to the form factor kernel <ref> [Sbert93] </ref>. The solution we propose is to parameterize lines by their intersections with two planes in arbitrary position (see figure 1). By convention, the coordinate system on the first plane is (u, v) and on the second plane is (s, t).
Reference: [Seitz95] <author> Seitz, S., Dyer, C., </author> <title> ``Physically-Valid View Synthesis by Image Interpolation,'' </title> <booktitle> Proc. IEEE Workshop on the Representation of Visual Scenes, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: The key challenge in this warping approach is to "fill in the gaps" when previously occluded areas become visible. Another approach to interpolating between acquired images is to find corresponding points in the two <ref> [Laveau94, McMillan95b, Seitz95] </ref>. If the positions of the cameras are known, this is equivalent to finding the depth values of the corresponding points.
Reference: [Williams83] <editor> Williams, L., ``Pyramidal Parametrics,'' </editor> <booktitle> Computer Graphics (Proc. Siggraph '83), </booktitle> <volume> Vol. 17, No. 3, </volume> <month> July, </month> <year> 1983, </year> <pages> pp. 1-11. </pages>
Reference-contexts: However, if the image of the light field is very small, then some form of prefiltering should be applied. This could easily be done with a 4D variation of the standard mipmapping algorithm <ref> [Williams83] </ref>. of the full 4D function. Quadralinear interpolation coupled with the proper prefiltering generates images with few aliasing artifacts. The improvement is particularly dramatic when the object or camera is moving. However, quadralinear filtering is more expensive and can sometimes be avoided.
Reference: [Ziv77] <author> Ziv, J., Lempel, A., </author> <title> ``A universal algorithm for sequential data compression,'' </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-23:337-343, </volume> <year> 1977. </year> <month> 12 </month>
Reference-contexts: Since our objects are typically rendered or photographed against a constant-color background, the array contains many tiles that occur with high probability. For the examples in this paper, we employed gzip, an implementation of Lempel-Ziv coding <ref> [Ziv77] </ref>. In this algorithm, the input stream is partitioned into nonoverlapping blocks while constructing a dictionary of blocks seen thus far. Applying gzip to our array of code indices typically gives us an additional 5:1 compression.
References-found: 28

