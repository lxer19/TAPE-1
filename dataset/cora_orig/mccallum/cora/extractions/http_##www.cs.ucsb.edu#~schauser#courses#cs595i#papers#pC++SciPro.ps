URL: http://www.cs.ucsb.edu/~schauser/courses/cs595i/papers/pC++SciPro.ps
Refering-URL: http://www.cs.ucsb.edu/~schauser/courses/cs595i/
Root-URL: http://www.cs.ucsb.edu
Title: Distributed pC++: Basic Ideas for an Object Parallel Language  
Author: Fran~cois Bodin, Peter Beckman, Dennis Gannon, Srinivas Narayana, Shelby X. Yang 
Address: 35042 Rennes, France  Bloomington, Indiana  
Affiliation: Irisa, University of Rennes Campus de Beaulieu,  Department of Computer Science Indiana University,  
Abstract: pC++ is an object-parallel extension to the C++ programming language. This paper describes the current language definition and illustrates the programming style. Examples of parallel linear algebra operations are presented and a fast poisson solver is described in complete detail. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. von Eiken, D. Culler, S. Goldstein, K. Schauser, </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation, </title> <booktitle> Proc. 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Australia, </address> <month> May </month> <year> 1992. </year> <month> 21 </month>
Reference-contexts: These coefficients correspond to the eigenvalues of tridiag-onal system <ref> [1; 4; 1] </ref> which is one slice of the finite difference operator.
Reference: [2] <author> D. Culler, T. von Eiken, </author> <title> CMAM Introduction to CM-5 Active Message commu-nication layer, man page, CMAM distribution. </title>
Reference-contexts: However there is no coherence for element copies and, at this time, no methods for updating a remote element is provided. However this has not been ruled out for the future. The current kernel is implemented on the NCSA CM5 using the CMAM communication package <ref> [2] </ref>, the Intel Paragon using NX. 4 Collection Function SPMD Execution The greatest potential hazard for programmers using pC++ lies in managing the interface between the single control thread of the main program and the "multi-threaded SPMD" functions of the collections.
Reference: [3] <author> A. Chien and W. Dally. </author> <title> Concurrent Aggregates (CA), </title> <booktitle> Proceedings of the Second ACM Sigplan Symposium on Principles & Practice of Parallel Programming, </booktitle> <address> Seat-tle, Washington, </address> <month> March, </month> <year> 1990. </year>
Reference-contexts: A major challenge for pC++ is to show that a shared address space is not essential for good performance. 2 Collections pC++ is based on a simple extension to C++ that provides parallelism via distributed aggregates <ref> [3, 6] </ref>. Distributed aggregates are structured sets of objects distributed over processors of a parallel system. Data structures such as vectors, arrays, matrices, grids, trees, dags or any other large, application specific aggregate can be described in pC++ as a collection class type.
Reference: [4] <author> R. Hockney and C. Jesshope, </author> <title> Parallel Computers. </title> <publisher> Adam Hilger Ltd., </publisher> <address> Bristol, </address> <year> 1981. </year>
Reference-contexts: At each stage all the eliminations can be done in parallel provided that the updates are done correctly. (See <ref> [4] </ref> for details.) One way to program this in pC++ is to build a special subcollection of the Dis-tributedVector to represent the tridiagonal system of equations. <p> These transforms can all be applied in parallel. The result is a decoupled system of tridiagonal equations. Each tridiagonal equation involves only the data in one row of the grid. After solving this system of n 1 equations, a final sin transform along each column completes the solution. (see <ref> [4] </ref> for the mathematical details.) The easiest way to do this is to view the array U as a distributed vector of column vectors, DistributedVector&lt; Vector &gt; U (.....); 19 The first and last steps of the parallel algorithm require a sine transforms operation on each column in parallel. <p> These coefficients correspond to the eigenvalues of tridiag-onal system <ref> [1; 4; 1] </ref> which is one slice of the finite difference operator.
Reference: [5] <author> High Performance Fortran Forum, </author> <title> Draft High Performance Fortran Language Specification, </title> <note> Version 0.4, Nov. 1992. Available from titan.cs.rice.edu by anonymous ftp. </note>
Reference: [6] <author> J. K. Lee, </author> <title> Object Oriented Parallel Programming Paradigms and Environments For Supercomputers, </title> <type> Ph.D. Thesis, </type> <institution> Indiana University, Bloomington, Indiana, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: We expect that more than half of our effort will be devoted to building new libraries and integrating existing ones into the system. pC++ is derived from an earlier experiment by Jenq Kuen Lee <ref> [6] </ref>. In its original form, pC++ proved that it was possible to build a language that provided real portability of "object-parallel" programs across a variety of shared-address-space parallel systems. <p> A major challenge for pC++ is to show that a shared address space is not essential for good performance. 2 Collections pC++ is based on a simple extension to C++ that provides parallelism via distributed aggregates <ref> [3, 6] </ref>. Distributed aggregates are structured sets of objects distributed over processors of a parallel system. Data structures such as vectors, arrays, matrices, grids, trees, dags or any other large, application specific aggregate can be described in pC++ as a collection class type.
Reference: [7] <author> J. K. Lee and D. Gannon, </author> <title> Object Oriented Parallel Programming: Experiments and Results Proceedings of Supercomputing 91 (Albuquerque, </title> <journal> Nov.), IEEE Computer Society and ACM SIGARCH, </journal> <year> 1991, </year> <pages> pp. 273-282. </pages>
Reference-contexts: The focus has been on a complete discussion of the language extensions and the associated semantics as well as providing an introduction to the standard library of linear algebra collection classes. We have not cited any performance results here. These are described in other papers <ref> [7] </ref>, [9], [11], [13] and new results for various benchmarks will be available soon. The compiler currently runs on any standard scalar unix system as well as the Thinking Machines CM-5. A version for the Intel Paragon will be available by the middle of 1993.
Reference: [8] <author> K. Li, </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors, </title> <type> Ph.D. Thesis, </type> <institution> Yale University, </institution> <month> Sept. </month> <year> 1986 </year>
Reference-contexts: In addition the local collection object contains a table that describes how non-local elements may be accessed. This table is very similar to the distributed manager used in shared virtual memory systems <ref> [8] </ref> [14]. Every element has a manager processor that is responsible for keeping track of where an element resides and every processor knows who the manager of each element is. The owner of an element is the processor that has that element in its local collection.
Reference: [9] <author> D. Gannon and J. K. Lee, </author> <title> Object Oriented Parallelism: </title> <booktitle> pC++ Ideas and Experiments Proceedings of 1991 Japan Society for Parallel Processing, </booktitle> <pages> pp. 13-23. </pages>
Reference-contexts: The focus has been on a complete discussion of the language extensions and the associated semantics as well as providing an introduction to the standard library of linear algebra collection classes. We have not cited any performance results here. These are described in other papers [7], <ref> [9] </ref>, [11], [13] and new results for various benchmarks will be available soon. The compiler currently runs on any standard scalar unix system as well as the Thinking Machines CM-5. A version for the Intel Paragon will be available by the middle of 1993.
Reference: [10] <author> D. Gannon, J. K. Lee, </author> <title> On Using Object Oriented Parallel Programming to Build Distributed Algebraic Abstractions, </title> <booktitle> Proceedings Conpar-Vap, </booktitle> <address> Lyon, </address> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: For example, this is used in the pC++ implementation of the NAS sparse CG benchmark <ref> [10] </ref>. Along these lines, other useful collections for sparse structured matrices would be DistBandedMatrix&lt; TridiagonalMatrix &gt;.
Reference: [11] <author> D. Gannon, </author> <title> Libraries and Tools for Object Parallel Programming, </title> <booktitle> Proceedings, CNRS-NSF Workshop on Environments and Tools For Parallel Scientific Computing, </booktitle> <address> Sept., 1992 Saint Hilaire du Touvet, France. </address> <note> To appear: Springer-Verlag, lectures on computer science. </note>
Reference-contexts: The focus has been on a complete discussion of the language extensions and the associated semantics as well as providing an introduction to the standard library of linear algebra collection classes. We have not cited any performance results here. These are described in other papers [7], [9], <ref> [11] </ref>, [13] and new results for various benchmarks will be available soon. The compiler currently runs on any standard scalar unix system as well as the Thinking Machines CM-5. A version for the Intel Paragon will be available by the middle of 1993.
Reference: [12] <author> F. Bodin, P. Beckman, D. Gannon, S. Narayana, S. Srinivas, Sage++: </author> <title> A Class Library for Building Fortran 90 and C++ Restructuring Tools, </title> <type> Technical Report, </type> <institution> Indiana University. </institution>
Reference-contexts: The Kernel class contains most of the machine specific details, so building new ports of the system is very easy. The compiler is written using the Sage++ compiler toolkit <ref> [12] </ref> which will also be distributed. We expect that pC++ will evolve. The current version does not allow collections of collections and there is no support for heterogenous, networked parallel systems. Because we feel that both are essential for future applications, we will move in that direction.
Reference: [13] <author> F. Bodin, D. Gannon, </author> <title> pC++ for Distributed Memory: </title> <type> CM-5 Communication Performance Experiments Technical Report, </type> <institution> Indiana University. </institution>
Reference-contexts: The reason for this two stage manager-owner scheme is to simplify dynamic collections and to provide a simple mechanism for load balancing. Our initial experiments on the CM-5 have shown that the added latency introduced by this scheme is very small (see <ref> [13] </ref>). Get_ElementPart () and Get_CopyElem () are similar. The former returns part of the elements and the latter allocates a new buffer to keep a copy the element. However there is no coherence for element copies and, at this time, no methods for updating a remote element is provided. <p> The focus has been on a complete discussion of the language extensions and the associated semantics as well as providing an introduction to the standard library of linear algebra collection classes. We have not cited any performance results here. These are described in other papers [7], [9], [11], <ref> [13] </ref> and new results for various benchmarks will be available soon. The compiler currently runs on any standard scalar unix system as well as the Thinking Machines CM-5. A version for the Intel Paragon will be available by the middle of 1993.
Reference: [14] <author> Z. Lahjomri and T. Priol, KOAN: </author> <title> a Shared Virtual Memory for the iPSC/2 hypercube CONPAR/VAP, </title> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: In addition the local collection object contains a table that describes how non-local elements may be accessed. This table is very similar to the distributed manager used in shared virtual memory systems [8] <ref> [14] </ref>. Every element has a manager processor that is responsible for keeping track of where an element resides and every processor knows who the manager of each element is. The owner of an element is the processor that has that element in its local collection.
Reference: [15] <author> A. H. Sameh, S. C. Chen, and D. J. Kuck, </author> <title> Parallel Poisson and Biharmonic Solvers, </title> <booktitle> Computing 17 (1976), </booktitle> <pages> 219-230 22 </pages>
Reference-contexts: i+1;j U i;j1 U i;j+1 = F i;j with j; i in [1:::n 1], given n = 2 k with boundary conditions U 0;i = U i;0 = U i;n = U n;i The algorithm we will use was first described in terms of parallel computation by Sameh and Kuck <ref> [15] </ref> consists of factoring the 5-point stencil, finite difference operator by first applying a sin transform to each column of the data. These transforms can all be applied in parallel. The result is a decoupled system of tridiagonal equations.
References-found: 15

