URL: http://www.cs.wisc.edu/~johannes/rainforest.ps
Refering-URL: http://www.cs.wisc.edu/logs/1997/11.14.1997.html
Root-URL: 
Email: fjohannes,raghu,vgantig@cs.wisc.edu  
Title: RainForest A Framework for Fast Decision Tree Construction of Large Datasets  
Author: Johannes Gehrke Raghu Ramakrishnan Venkatesh Ganti 
Date: 1998  
Address: New York, USA,  
Note: This research was supported by Grant 2053 from the IBM Corporation. Proceedings of the 24th VLDB Conference  
Affiliation: Department of Computer Sciences, University of Wisconsin-Madison  
Abstract: Classification of large datasets is an important data mining problem. Many classification algorithms have been proposed in the literature, but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality. In this paper, we present a unifying framework for decision tree classifiers that separates the scalability aspects of algorithms for constructing a decision tree from the central features that determine the quality of the tree. This generic algorithm is easy to instantiate with specific algorithms from the literature (including C4.5, CART, CHAID, FACT, ID3 and extensions, SLIQ, Sprint and QUEST). In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach also offers performance improvements of over a factor of five over the Sprint algorithm, the fastest scalable classification algorithm proposed previously. In contrast to Sprint, however, our generic algorithm requires a certain minimum amount of main memory, proportional to the set of distinct values in a column of the input relation. Given current main memory costs, this requirement is readily met in most if not all workloads. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. fl Supported by an IBM Corporate Fellowship
Abstract-found: 1
Intro-found: 1
Reference: [AGI + 92] <author> R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and A. Swami. </author> <title> An interval classifier for database mining applications. </title> <booktitle> In Proc. of VLDB, </booktitle> <year> 1992. </year>
Reference-contexts: Whether the tree is pruned top-down or bottom-up is an orthogonal issue. 2.2 Previous work in the database literature Agrawal et al. introduce in <ref> [AGI + 92] </ref> an interval classifier that could use database indices to efficiently retrieve portions of the classified dataset using SQL queries. However, the method does not scale to large training sets [SAM96]. Fukuda et al. [FMM96] construct decision trees with two-dimensional splitting criteria.
Reference: [AIS93] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE TKDE, </journal> <month> December </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Classification is an important data mining problem <ref> [AIS93] </ref>. The input is a database of training records; each record has several attributes. Attributes whose underlying domain is totally ordered are called ordered attributes, whereas attributes whose underlying domain is not ordered are called categorical attributes. <p> Most algorithms in the machine learning and statistics community are main memory algorithms, even though today's databases are in general much larger than main memory <ref> [AIS93] </ref>. There have been several approaches to dealing with large databases. One approach is to discretize each ordered attribute and run the algorithm on the discretized data. <p> Rather the AVC-group of r contains aggregated information that is sufficient for decision tree construction. In Section 5, we calculate example numbers for the AVC-group of the root node generated by a synthetic data generator introduced by Agrawal et al. in <ref> [AIS93] </ref> (which was designed to model real-life data). The maximum memory size for the AVC-group of the generated datasets is about 16 megabytes. <p> The largest dataset in the often used Statlog collection of training databases [MST94] contains only 57000 records, and the largest training dataset considered in [LLS97] has 4435 tuples. We therefore use the synthetic data generator introduced by Agrawal et al. in <ref> [AIS93] </ref>, henceforth referred to as Generator. The synthetic data has nine predictor attributes as shown in Table 4. Included in the generator are classification functions that assign labels to the records produced. We selected two of the functions (Function 1 and Function 7) from [AIS93] for our performance study. <p> introduced by Agrawal et al. in <ref> [AIS93] </ref>, henceforth referred to as Generator. The synthetic data has nine predictor attributes as shown in Table 4. Included in the generator are classification functions that assign labels to the records produced. We selected two of the functions (Function 1 and Function 7) from [AIS93] for our performance study. Function 1 generates relatively small decision tree whereas the trees generated by Function 7 are large. (Note that this adheres to the methodology used in the Sprint performance study [SAM96]).
Reference: [ASW87] <author> M.M. Astrahan, M. Schkolnick, and K.-Y. Whang. </author> <title> Approximating the number of unique values of an attribute without sorting. </title> <journal> Information Systems, </journal> <volume> 12(1):1115, </volume> <year> 1987. </year>
Reference: [BFOS84] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title>
Reference-contexts: Many classification models have been proposed in the literature. (For overviews of classification methods see [WK91, MST94].) Decision trees are especially attractive for a data mining environment for three reasons. First, due to their intuitive representation, they are easy to assimilate by humans <ref> [BFOS84] </ref>. Second, they can be constructed relatively fast compared to other methods [MAR96, SAM96]. Last, the accuracy of decision tree classifiers is comparable or superior to other models [LLS97, Han97]. In this paper, we restrict our attention to decision tree classifiers. <p> The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART <ref> [BFOS84] </ref>, CHAID [Mag93], FACT [LV88], ID3 and extensions [Qui79, Qui83, Qui86, CFIQ88, Fay91], SLIQ and Sprint [MAR96, MRA95, SAM96] and QUEST [LS97]) access the data using a common pattern, as described in Figure 1. <p> This limiting data structure was eliminated in [SAM96], which introduced Sprint, a scalable classifier. Sprint works for very large datasets and removes all relationships between main memory and size of the dataset. Sprint builds classification trees with binary splits using the gini index <ref> [BFOS84] </ref> to decide the splitting criterion; it controls the final quality of the decision tree through an application of the MDL principle [Ris89, MRA95]. To decide on the splitting attribute at a node n, the algorithm requires access to F (n) for each ordered attribute in sorted order. <p> The reason why the techniques used in Sprint do not straightforwardly extend to a broader range of algorithms is that the data management of Sprint is designed to enable efficient sequential access to ordered attributes in sorted order. Thus, decision tree algorithms that exhibit this access pattern (e.g., CART <ref> [BFOS84] </ref>) can be implemented with the data management of Sprint.
Reference: [BU92] <author> C.E. Brodley and P.E. Utgoff. </author> <title> Multivariate versus univariate decision trees. </title> <type> TR 8, </type> <institution> Department of Computer Science, University of Massachussetts, </institution> <year> 1992. </year>
Reference-contexts: The emphasis of the research in the machine learning and statistics community has been on improving the accuracy of classifiers. Many studies have been performed to determine which algorithm has the highest prediction accuracy <ref> [SMT91, BU92, DBP93, CM94, MST94] </ref>.
Reference: [Cat91] <author> J. Catlett. </author> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> PhD thesis, </type> <institution> University of Sydney, </institution> <year> 1991. </year>
Reference-contexts: One approach is to discretize each ordered attribute and run the algorithm on the discretized data. But all discretization methods for classification that take the class label into account when discretizing assume that the database fits into main memory [Qui93, FI93, Maa94, DKS95]. Catlett <ref> [Cat91] </ref> proposed sampling at each node of the classification tree, but considers in his studies only datasets that could fit in main memory.
Reference: [CFIQ88] <author> J.. Cheng, U.M. Fayyad, K.B. Irani, and Z. Qian. </author> <title> Improved decision trees: A generalized version of ID3. </title> <booktitle> In Proc. of Machine Learning, </booktitle> <year> 1988. </year>
Reference-contexts: The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID [Mag93], FACT [LV88], ID3 and extensions <ref> [Qui79, Qui83, Qui86, CFIQ88, Fay91] </ref>, SLIQ and Sprint [MAR96, MRA95, SAM96] and QUEST [LS97]) access the data using a common pattern, as described in Figure 1. <p> Thus, decision tree algorithms that exhibit this access pattern (e.g., CART [BFOS84]) can be implemented with the data management of Sprint. But other decision tree algorithms (e.g., ID3 [Qui86] or GID3 <ref> [CFIQ88] </ref>) that do not exhibit a sequential access pattern can not be scaled using this approach. 4 Algorithms In this section, we present algorithms for two of the three cases listed above.
Reference: [CM94] <author> S.P. Curram and J. Mingers. </author> <title> Neural networks, decision tree induction and discriminant analysis: an empirical comparison. </title> <journal> Journal of the Operational Research Society, </journal> <volume> 45:440450, </volume> <year> 1994. </year>
Reference-contexts: The emphasis of the research in the machine learning and statistics community has been on improving the accuracy of classifiers. Many studies have been performed to determine which algorithm has the highest prediction accuracy <ref> [SMT91, BU92, DBP93, CM94, MST94] </ref>.
Reference: [CS93a] <author> P. K. Chan and S. J. Stolfo. </author> <title> Experiments on mul-tistrategy learning by meta-learning. </title> <booktitle> In Proc. of CIKM, </booktitle> <year> 1993. </year>
Reference-contexts: Catlett [Cat91] proposed sampling at each node of the classification tree, but considers in his studies only datasets that could fit in main memory. Methods for partitioning the dataset such that each subset fits in main memory are considered by Chan and Stolfo <ref> [CS93a, CS93b] </ref>; although this method enables classification of large datasets their studies show that the quality of the resulting decision tree is worse than that of a classifier that was constructed taking the complete database into account at once.
Reference: [CS93b] <author> P. K. Chan and S. J. Stolfo. </author> <title> Meta-learning for mul-tistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Workshop on Multistrategy Learning, </booktitle> <year> 1993. </year>
Reference-contexts: Catlett [Cat91] proposed sampling at each node of the classification tree, but considers in his studies only datasets that could fit in main memory. Methods for partitioning the dataset such that each subset fits in main memory are considered by Chan and Stolfo <ref> [CS93a, CS93b] </ref>; although this method enables classification of large datasets their studies show that the quality of the resulting decision tree is worse than that of a classifier that was constructed taking the complete database into account at once.
Reference: [DBP93] <author> V. Corruble D.E. Brown and C.L. Pittard. </author> <title> A comparison of decision classifiers with backpropagation neural networks for multimodal classification problems. </title> <journal> Pattern Recognition, </journal> <volume> 26:953961, </volume> <year> 1993. </year>
Reference-contexts: The emphasis of the research in the machine learning and statistics community has been on improving the accuracy of classifiers. Many studies have been performed to determine which algorithm has the highest prediction accuracy <ref> [SMT91, BU92, DBP93, CM94, MST94] </ref>.
Reference: [DKS95] <author> J. Dougherty, R. Kahove, and M. Sahami. </author> <title> Supervised and unsupervised discretization of continous features. </title> <booktitle> In Proc. of Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: There have been several approaches to dealing with large databases. One approach is to discretize each ordered attribute and run the algorithm on the discretized data. But all discretization methods for classification that take the class label into account when discretizing assume that the database fits into main memory <ref> [Qui93, FI93, Maa94, DKS95] </ref>. Catlett [Cat91] proposed sampling at each node of the classification tree, but considers in his studies only datasets that could fit in main memory.
Reference: [Fay91] <author> U.M. Fayyad. </author> <title> On the induction of decision trees for multiple concept learning. </title> <type> PhD thesis, </type> <institution> EECS Department, The University of Michigan, </institution> <year> 1991. </year>
Reference-contexts: The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID [Mag93], FACT [LV88], ID3 and extensions <ref> [Qui79, Qui83, Qui86, CFIQ88, Fay91] </ref>, SLIQ and Sprint [MAR96, MRA95, SAM96] and QUEST [LS97]) access the data using a common pattern, as described in Figure 1. <p> Therefore it is not efficient for splitting algorithms that apply bottom-up pruning (except for the case that the families at the pure leaf nodes are very large and this is usually not known in advance). But for splitting algorithms that prune the tree top-down <ref> [Fay91, RS98] </ref>, this approach might be a viable solution. We included Algorithm RF-Read for completeness: it marks one end of the design spectrum in the RainForest framework and it is one of the two parents of the Algorithm RF-Hybrid described in the next section.
Reference: [FI93] <author> U.M. Fayyad and K. Irani. </author> <title> Multi-interval discretiza-tion of continous-valued attributes for classification learning. </title> <booktitle> In Proc. of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1993. </year>
Reference-contexts: There have been several approaches to dealing with large databases. One approach is to discretize each ordered attribute and run the algorithm on the discretized data. But all discretization methods for classification that take the class label into account when discretizing assume that the database fits into main memory <ref> [Qui93, FI93, Maa94, DKS95] </ref>. Catlett [Cat91] proposed sampling at each node of the classification tree, but considers in his studies only datasets that could fit in main memory.
Reference: [FMM96] <author> T. Fukuda, Y. Morimoto, and S. Morishita. </author> <title> Constructing efficient decision trees by using optimized numeric association rules. </title> <booktitle> In Proc. of VLDB, </booktitle> <year> 1996. </year>
Reference-contexts: However, the method does not scale to large training sets [SAM96]. Fukuda et al. <ref> [FMM96] </ref> construct decision trees with two-dimensional splitting criteria. Although their algorithm can produce rules with very high classification accuracy, scalability was not one of the design goals. In addition, the decision tree no longer has the intuitive representation of a tree with one-dimensional splits at each node.
Reference: [GJ79] <author> M.R. Garey and D.S. Johnson. </author> <title> Computer and Intractability, </title> <year> 1979. </year>
Reference-contexts: for now that we have estimates of the sizes of the AVC-groups of all nodes in N . (We will address the problem of size estimation of AVC-groups in Section 4.5.) According to the formulation in the preceding paragraph, the choice of M is an instance of the knapsack problem <ref> [GJ79] </ref>. <p> The goal is to find a subset of the items such that the total cost of the subset does not exceed the capacity of the knapsack while maximizing the sum of the benefits of the items in the knapsack. The knapsack problem is well-known to be NP-complete <ref> [GJ79] </ref>.
Reference: [Han97] <author> D.J. </author> <title> Hand. Construction and Assessment of Classification Rules, </title> <year> 1997. </year>
Reference-contexts: First, due to their intuitive representation, they are easy to assimilate by humans [BFOS84]. Second, they can be constructed relatively fast compared to other methods [MAR96, SAM96]. Last, the accuracy of decision tree classifiers is comparable or superior to other models <ref> [LLS97, Han97] </ref>. In this paper, we restrict our attention to decision tree classifiers. Within the area of decision tree classification, there exist a large number of algorithms to construct decision trees (also called classification trees; we will use both terms interchangeably).
Reference: [HNSS95] <author> P.J. Haas, J.F. Naughton, S. Seshadri, and L. </author> <title> Stokes. Sampling-based estimation of the number of distinct values of an attribute. </title> <booktitle> In Proc. of VLDB, </booktitle> <year> 1995. </year>
Reference: [LLS97] <author> T.-S. Lim, W.-Y. Loh, and Y.-S. Shih. </author> <title> An empirical comparison of decision trees and other classification methods. </title> <type> TR 979, </type> <institution> Department of Statistics, UW Madison, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: First, due to their intuitive representation, they are easy to assimilate by humans [BFOS84]. Second, they can be constructed relatively fast compared to other methods [MAR96, SAM96]. Last, the accuracy of decision tree classifiers is comparable or superior to other models <ref> [LLS97, Han97] </ref>. In this paper, we restrict our attention to decision tree classifiers. Within the area of decision tree classification, there exist a large number of algorithms to construct decision trees (also called classification trees; we will use both terms interchangeably). <p> an attribute ([ASW87, HNSS95]); we intend to explore their use in future research. 5 Experimental results In the machine learning and statistics literature, the two main performance measures for classification tree algorithms are: (i) The quality of the rules of the resulting tree, and (ii) The decision tree construction time <ref> [LLS97] </ref>. The generic schema described in Section 3 allows the instan-tiation of most (to our knowledge, all) classification tree algorithms from the literature without modifying the result of the algorithm. <p> The largest dataset in the often used Statlog collection of training databases [MST94] contains only 57000 records, and the largest training dataset considered in <ref> [LLS97] </ref> has 4435 tuples. We therefore use the synthetic data generator introduced by Agrawal et al. in [AIS93], henceforth referred to as Generator. The synthetic data has nine predictor attributes as shown in Table 4. Included in the generator are classification functions that assign labels to the records produced.
Reference: [LS97] <author> W.-Y. Loh and Y.-S. Shih. </author> <title> Split selection methods for classification trees. </title> <journal> Statistica Sinica, </journal> <volume> 7(4), </volume> <month> Octo-ber </month> <year> 1997. </year>
Reference-contexts: The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID [Mag93], FACT [LV88], ID3 and extensions [Qui79, Qui83, Qui86, CFIQ88, Fay91], SLIQ and Sprint [MAR96, MRA95, SAM96] and QUEST <ref> [LS97] </ref>) access the data using a common pattern, as described in Figure 1.
Reference: [LV88] <author> W.-Y. Loh and N. Vanichsetakul. </author> <title> Tree-structured classification via generalized disriminant analysis (with discussion). </title> <journal> Journal of the American Statistical Association, </journal> <volume> 83:715728, </volume> <year> 1988. </year>
Reference-contexts: The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID [Mag93], FACT <ref> [LV88] </ref>, ID3 and extensions [Qui79, Qui83, Qui86, CFIQ88, Fay91], SLIQ and Sprint [MAR96, MRA95, SAM96] and QUEST [LS97]) access the data using a common pattern, as described in Figure 1.
Reference: [Maa94] <author> W. Maass. </author> <title> Efficient agnostic pac-learning with simple hypothesis. </title> <booktitle> In Proc. of Conference on Computational Learning Theory, </booktitle> <year> 1994. </year>
Reference-contexts: There have been several approaches to dealing with large databases. One approach is to discretize each ordered attribute and run the algorithm on the discretized data. But all discretization methods for classification that take the class label into account when discretizing assume that the database fits into main memory <ref> [Qui93, FI93, Maa94, DKS95] </ref>. Catlett [Cat91] proposed sampling at each node of the classification tree, but considers in his studies only datasets that could fit in main memory.
Reference: [Mag93] <author> J. Magidson. </author> <title> The CHAID approach to segmentation modeling. </title> <booktitle> In Handbook of Marketing Research, </booktitle> <year> 1993. </year>
Reference-contexts: The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID <ref> [Mag93] </ref>, FACT [LV88], ID3 and extensions [Qui79, Qui83, Qui86, CFIQ88, Fay91], SLIQ and Sprint [MAR96, MRA95, SAM96] and QUEST [LS97]) access the data using a common pattern, as described in Figure 1.
Reference: [MAR96] <author> M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. of EDBT, </booktitle> <year> 1996. </year>
Reference-contexts: First, due to their intuitive representation, they are easy to assimilate by humans [BFOS84]. Second, they can be constructed relatively fast compared to other methods <ref> [MAR96, SAM96] </ref>. Last, the accuracy of decision tree classifiers is comparable or superior to other models [LLS97, Han97]. In this paper, we restrict our attention to decision tree classifiers. <p> The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID [Mag93], FACT [LV88], ID3 and extensions [Qui79, Qui83, Qui86, CFIQ88, Fay91], SLIQ and Sprint <ref> [MAR96, MRA95, SAM96] </ref> and QUEST [LS97]) access the data using a common pattern, as described in Figure 1. <p> In this paper, we will concentrate on the tree growth phase, since it is due to its data-intensive nature the most time-consuming part of decision tree construction <ref> [MAR96, SAM96] </ref>. Whether the tree is pruned top-down or bottom-up is an orthogonal issue. 2.2 Previous work in the database literature Agrawal et al. introduce in [AGI + 92] an interval classifier that could use database indices to efficiently retrieve portions of the classified dataset using SQL queries. <p> Although their algorithm can produce rules with very high classification accuracy, scalability was not one of the design goals. In addition, the decision tree no longer has the intuitive representation of a tree with one-dimensional splits at each node. The decision tree classifier in <ref> [MAR96] </ref>, called SLIQ, was designed for large databases but uses an in-memory data structure that grows linearly with the number of tuples in the training database. This limiting data structure was eliminated in [SAM96], which introduced Sprint, a scalable classifier. <p> Many studies have been performed to determine which algorithm has the highest prediction accuracy [SMT91, BU92, DBP93, CM94, MST94]. These studies indicate that no algorithm is uniformly most accurate over all the datasets studied. (Mehta et al. also show quality studies <ref> [MRA95, MAR96] </ref> which indicate that the accuracy of the decision tree built by Sprint is not uniformly superior.) We have therefore concentrated on developing a unifying framework that can be applied to most decision tree algorithms, and results in a scalable version of the algorithm without modifying the result. <p> Recursively, at a non-root node n, F (n) is examined and from it crit (n) is computed. (This is the well-known schema for top-down decision tree induction; for example, a specific instance of this schema for binary splits is shown in <ref> [MAR96] </ref>). This schema is shown in Figure 1.
Reference: [MRA95] <author> M. Mehta, J. Rissanen, and R. Agrawal. </author> <title> MDL-based decision tree pruning. </title> <booktitle> In Proc. of KDD, </booktitle> <year> 1995. </year>
Reference-contexts: The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID [Mag93], FACT [LV88], ID3 and extensions [Qui79, Qui83, Qui86, CFIQ88, Fay91], SLIQ and Sprint <ref> [MAR96, MRA95, SAM96] </ref> and QUEST [LS97]) access the data using a common pattern, as described in Figure 1. <p> There are two ways to control the size of a classification tree. A bottom-up pruning algorithm <ref> [MRA95] </ref> has two phases: In phase one, the growth phase, a very deep tree is constructed. In phase two, the pruning phase, this tree is cut back to avoid overfitting the training data. <p> Sprint builds classification trees with binary splits using the gini index [BFOS84] to decide the splitting criterion; it controls the final quality of the decision tree through an application of the MDL principle <ref> [Ris89, MRA95] </ref>. To decide on the splitting attribute at a node n, the algorithm requires access to F (n) for each ordered attribute in sorted order. So conceptually, for each node n of the decision tree, a sort of F (n) for each ordered attribute is required. <p> Many studies have been performed to determine which algorithm has the highest prediction accuracy [SMT91, BU92, DBP93, CM94, MST94]. These studies indicate that no algorithm is uniformly most accurate over all the datasets studied. (Mehta et al. also show quality studies <ref> [MRA95, MAR96] </ref> which indicate that the accuracy of the decision tree built by Sprint is not uniformly superior.) We have therefore concentrated on developing a unifying framework that can be applied to most decision tree algorithms, and results in a scalable version of the algorithm without modifying the result.
Reference: [MST94] <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor, editors. </editor> <title> Machine Learning, Neural and Statistical Classification, </title> <year> 1994. </year>
Reference-contexts: Classification has a wide range of applications, including scientific experiments, medical diagnosis, fraud detection, credit approval and target marketing. Many classification models have been proposed in the literature. (For overviews of classification methods see <ref> [WK91, MST94] </ref>.) Decision trees are especially attractive for a data mining environment for three reasons. First, due to their intuitive representation, they are easy to assimilate by humans [BFOS84]. Second, they can be constructed relatively fast compared to other methods [MAR96, SAM96]. <p> The emphasis of the research in the machine learning and statistics community has been on improving the accuracy of classifiers. Many studies have been performed to determine which algorithm has the highest prediction accuracy <ref> [SMT91, BU92, DBP93, CM94, MST94] </ref>. <p> The largest dataset in the often used Statlog collection of training databases <ref> [MST94] </ref> contains only 57000 records, and the largest training dataset considered in [LLS97] has 4435 tuples. We therefore use the synthetic data generator introduced by Agrawal et al. in [AIS93], henceforth referred to as Generator. The synthetic data has nine predictor attributes as shown in Table 4.
Reference: [Qui79] <author> J.R. Quinlan. </author> <title> Discovering rules by induction from large collections of examples. </title> <booktitle> In Expert Systems in the Micro Electronic Age, </booktitle> <year> 1979. </year>
Reference-contexts: The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID [Mag93], FACT [LV88], ID3 and extensions <ref> [Qui79, Qui83, Qui86, CFIQ88, Fay91] </ref>, SLIQ and Sprint [MAR96, MRA95, SAM96] and QUEST [LS97]) access the data using a common pattern, as described in Figure 1.
Reference: [Qui83] <author> J.R Quinlan. </author> <title> Learning efficient classification procedures. </title> <booktitle> In Machine Learning: An Artificial Intelligence Approach, </booktitle> <year> 1983. </year>
Reference-contexts: The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID [Mag93], FACT [LV88], ID3 and extensions <ref> [Qui79, Qui83, Qui86, CFIQ88, Fay91] </ref>, SLIQ and Sprint [MAR96, MRA95, SAM96] and QUEST [LS97]) access the data using a common pattern, as described in Figure 1.
Reference: [Qui86] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <address> 1:81106, </address> <year> 1986. </year>
Reference-contexts: The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID [Mag93], FACT [LV88], ID3 and extensions <ref> [Qui79, Qui83, Qui86, CFIQ88, Fay91] </ref>, SLIQ and Sprint [MAR96, MRA95, SAM96] and QUEST [LS97]) access the data using a common pattern, as described in Figure 1. <p> Thus, decision tree algorithms that exhibit this access pattern (e.g., CART [BFOS84]) can be implemented with the data management of Sprint. But other decision tree algorithms (e.g., ID3 <ref> [Qui86] </ref> or GID3 [CFIQ88]) that do not exhibit a sequential access pattern can not be scaled using this approach. 4 Algorithms In this section, we present algorithms for two of the three cases listed above.
Reference: [Qui93] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning, </title> <year> 1993. </year>
Reference-contexts: There have been several approaches to dealing with large databases. One approach is to discretize each ordered attribute and run the algorithm on the discretized data. But all discretization methods for classification that take the class label into account when discretizing assume that the database fits into main memory <ref> [Qui93, FI93, Maa94, DKS95] </ref>. Catlett [Cat91] proposed sampling at each node of the classification tree, but considers in his studies only datasets that could fit in main memory. <p> The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 <ref> [Qui93] </ref>, CART [BFOS84], CHAID [Mag93], FACT [LV88], ID3 and extensions [Qui79, Qui83, Qui86, CFIQ88, Fay91], SLIQ and Sprint [MAR96, MRA95, SAM96] and QUEST [LS97]) access the data using a common pattern, as described in Figure 1.
Reference: [RS98] <author> R.Rastogi and K.Shim. </author> <title> PUBLIC: A Decision Tree Classifier that Integrates Pruning and Building. </title> <booktitle> In Proc. of VLDB, </booktitle> <year> 1998. </year>
Reference-contexts: A bottom-up pruning algorithm [MRA95] has two phases: In phase one, the growth phase, a very deep tree is constructed. In phase two, the pruning phase, this tree is cut back to avoid overfitting the training data. In a top-down pruning algorithm <ref> [RS98] </ref> the two phases are interleaved: Stopping criteria are calculated during tree growth to inhibit further construction of parts of the tree when appropriate. <p> In recent work, Morimoto et al. developed algorithms for decision tree construction for categorical predictor variables with large domains [YFM + 98]; the emphasis of this work is to improve the quality of the resulting tree. Rastogi and Shim developed PUBLIC, a scalable decision tree classifier using top-down pruning <ref> [RS98] </ref>. Since pruning is an orthogonal dimension to tree growth, their techniques can be easily incorporated into our schema. 2.3 Discussion One can think of Sprint as a prix fixe all-you-can-eat meal in a world-class restaurant. <p> Therefore it is not efficient for splitting algorithms that apply bottom-up pruning (except for the case that the families at the pure leaf nodes are very large and this is usually not known in advance). But for splitting algorithms that prune the tree top-down <ref> [Fay91, RS98] </ref>, this approach might be a viable solution. We included Algorithm RF-Read for completeness: it marks one end of the design spectrum in the RainForest framework and it is one of the two parents of the Algorithm RF-Hybrid described in the next section.
Reference: [Ris89] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry, </title> <year> 1989. </year>
Reference-contexts: Sprint builds classification trees with binary splits using the gini index [BFOS84] to decide the splitting criterion; it controls the final quality of the decision tree through an application of the MDL principle <ref> [Ris89, MRA95] </ref>. To decide on the splitting attribute at a node n, the algorithm requires access to F (n) for each ordered attribute in sorted order. So conceptually, for each node n of the decision tree, a sort of F (n) for each ordered attribute is required.
Reference: [SAM96] <author> J. Shafer, R. Agrawal, and M. Mehta. SPRINT: </author> <title> A scalable parallel classifier for data mining. </title> <booktitle> In Proc. of VLDB, </booktitle> <year> 1996. </year>
Reference-contexts: First, due to their intuitive representation, they are easy to assimilate by humans [BFOS84]. Second, they can be constructed relatively fast compared to other methods <ref> [MAR96, SAM96] </ref>. Last, the accuracy of decision tree classifiers is comparable or superior to other models [LLS97, Han97]. In this paper, we restrict our attention to decision tree classifiers. <p> The main insight, based on a careful analysis of the algorithms in the literature, is that most (to our knowledge, all) algorithms (including C4.5 [Qui93], CART [BFOS84], CHAID [Mag93], FACT [LV88], ID3 and extensions [Qui79, Qui83, Qui86, CFIQ88, Fay91], SLIQ and Sprint <ref> [MAR96, MRA95, SAM96] </ref> and QUEST [LS97]) access the data using a common pattern, as described in Figure 1. <p> In this paper, we will concentrate on the tree growth phase, since it is due to its data-intensive nature the most time-consuming part of decision tree construction <ref> [MAR96, SAM96] </ref>. Whether the tree is pruned top-down or bottom-up is an orthogonal issue. 2.2 Previous work in the database literature Agrawal et al. introduce in [AGI + 92] an interval classifier that could use database indices to efficiently retrieve portions of the classified dataset using SQL queries. <p> However, the method does not scale to large training sets <ref> [SAM96] </ref>. Fukuda et al. [FMM96] construct decision trees with two-dimensional splitting criteria. Although their algorithm can produce rules with very high classification accuracy, scalability was not one of the design goals. <p> The decision tree classifier in [MAR96], called SLIQ, was designed for large databases but uses an in-memory data structure that grows linearly with the number of tuples in the training database. This limiting data structure was eliminated in <ref> [SAM96] </ref>, which introduced Sprint, a scalable classifier. Sprint works for very large datasets and removes all relationships between main memory and size of the dataset. <p> Comparing the size of the AVC-group of a node n to the attribute lists created in Sprint <ref> [SAM96] </ref> for n, the AVC-group is typically much smaller than even a single attribute list, because the AVC-set size is proportional to the number of distinct values in the columns of D, rather than to the number of records in D. <p> We selected two of the functions (Function 1 and Function 7) from [AIS93] for our performance study. Function 1 generates relatively small decision tree whereas the trees generated by Function 7 are large. (Note that this adheres to the methodology used in the Sprint performance study <ref> [SAM96] </ref>). Since the feasibility of our framework relies on the size of the initial AVC-group, we examined the sizes of the AVC-group of the training data sets generated by Generator. <p> splitting algorithm will never choose such a noisy attribute in its splitting criterion.) As can be seen in Figure 12, the RainForest family of algorithms exhibits a roughly linear scaleup with the number of attributes. 5.3 Performance comparison with Sprint In this section, we present a performance comparison with Sprint <ref> [SAM96] </ref>. We tried to make our implementation of Sprint as efficient as possible, resulting in the following two implementation improvements over the algorithm described in [SAM96]. First, we create only one attribute list for all categorical attributes together. <p> exhibits a roughly linear scaleup with the number of attributes. 5.3 Performance comparison with Sprint In this section, we present a performance comparison with Sprint <ref> [SAM96] </ref>. We tried to make our implementation of Sprint as efficient as possible, resulting in the following two implementation improvements over the algorithm described in [SAM96]. First, we create only one attribute list for all categorical attributes together.
Reference: [SMT91] <author> J.W. Shavlik, R.J. Mooney, and G.G. Towell. </author> <title> Symbolic and neural learning algorithms: an empirical comparison. </title> <booktitle> Machine Learning, </booktitle> <address> 6:111144, </address> <year> 1991. </year>
Reference-contexts: The emphasis of the research in the machine learning and statistics community has been on improving the accuracy of classifiers. Many studies have been performed to determine which algorithm has the highest prediction accuracy <ref> [SMT91, BU92, DBP93, CM94, MST94] </ref>.
Reference: [WK91] <author> S.M. Weiss and C.A. </author> <title> Kulikowski. Computer Systems that Learn: Classification and Prediction Methods from Statistics, Neural Nets, </title> <journal> Machine Learning, and Expert Systems, </journal> <year> 1991. </year>
Reference-contexts: Classification has a wide range of applications, including scientific experiments, medical diagnosis, fraud detection, credit approval and target marketing. Many classification models have been proposed in the literature. (For overviews of classification methods see <ref> [WK91, MST94] </ref>.) Decision trees are especially attractive for a data mining environment for three reasons. First, due to their intuitive representation, they are easy to assimilate by humans [BFOS84]. Second, they can be constructed relatively fast compared to other methods [MAR96, SAM96].
Reference: [YFM + 98] <author> Y.Morimoto, T.Fukuda, H.Matsuzawa, T.Tokuyama, and K.Yoda. </author> <title> Algorithms for Mining Association Rules for Binary Segmentations of Huge Categorical Databases. </title> <booktitle> In Proc. of VLDB, </booktitle> <year> 1998. </year>
Reference-contexts: Since during the hash-join each attribute list is read and distributed sequentially, the initial sort order of the attribute list is preserved. In recent work, Morimoto et al. developed algorithms for decision tree construction for categorical predictor variables with large domains <ref> [YFM + 98] </ref>; the emphasis of this work is to improve the quality of the resulting tree. Rastogi and Shim developed PUBLIC, a scalable decision tree classifier using top-down pruning [RS98].
References-found: 36

