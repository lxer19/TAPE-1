URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1996/GIT-CC-96-08.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.96.html
Root-URL: 
Email: jdixon@cc.gatech.edu calvert@cc.gatech.edu  
Title: Increasing Demultiplexing Efficiency in TCP/IP Network Servers GIT-CC-96-08  
Author: Joseph T. Dixon and Kenneth Calvert 
Affiliation: Networking and Telecommunications Group College of Computing Georgia Institute of Technology  
Abstract: This paper shows how software caches and hashing to multiple PCB (protocol control block) lists can increase demultiplexing efficiency in TCP/IP network server hosts. We implemented six algorithms and executed 200 simulations - using four server traffic traces as input - to formulate best-use caching and hashing policies for demultiplexing TCP-based http, telnet and login services and UDP-based services such as nfs. Our work, motivated by promising analytical results and general suggestions in past works, differs from its predecessors in several important ways: (1) we examine the limits of applicability of last-referenced-PCB caching and list-hashing; (2) we test the merit of popular assumptions made in existing implementations; (3) we make several concrete recommendations so that individual servers can realize potentially significant performance gains through simple, server-independent modifications. Our experimentation led to a series of findings. First, we show conclusively that, while caching alone can enhance performance, more than two cache entries can cause efficiency to diminish below peak performance. Furthermore, while our two cache entry algorithm reduces the mean number of instructions required for a PCB lookup by as much as 75% below the original TCP demultiplex algorithms performance, variability of execution cost for a caching-only scheme remains high. Next, we customize a simple combination caching and multiple list algorithm first introduced by McKenney and Dove and show how TCP PCB lookups can be reduced by more than 93%. We then verify that, in many cases, the iterative nature of most UDP-based services relegates a caching to unnecessary overhead and makes multiple list benefits uncertain at best. We demonstrate that this overhead can actually make overall demultiplexing performance even worse than that of the original algorithm. Some of its negative effect, however, can be overcome if different algorithms are used for TCP and UDP. Our introduction of this separable solution debunks the popular, longstanding convention of using the same algorithm for both TCP and UDP. These findings imply applicationspecific implementation strategies that can yield potentially significant demultiplexing performance benefits in existing and new TCP/IP network servers. 
Abstract-found: 1
Intro-found: 1
Reference: [Chan&Varg95] <author> Girish P. Chandranmenon and George Varghese, </author> <title> Trading packet Headers for Packet Processing, </title> <type> SIGCOMM 95, </type> <year> 1995. </year>
Reference-contexts: This means that the number of hash locations can limit the number of simultaneous TCP connections. Another solution (with the same implementation issues) is based on methods and mechanisms proposed by Chandranmenon and Varghese <ref> [Chan&Varg95] </ref>. They propose a technique, called source hashing, that allows direct access to various information associated with packet processing. They do not directly address demultiplexing, but their technique can be extended to provide an appropriate solution. <p> This hash-id could then be passed to the destination host via the TCP/UDP header so that both hosts connection identification information. In a client/server communication scenario, clearly hash-id collisions can occur; <ref> [Chan&Varg95] </ref> proposed various ways to address such instances. Finally, Mentat, Inc. provides an entirely different strategy [Mentat93]. In their TCP/IP implementation (which requires a UNIX SVR4 STREAMS environment), incoming packets are demultiplexed by IP rather than TCP or UDP.
Reference: [Clark89] <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen, </author> <title> An Analysis of TCP Processing Overhead, </title> <journal> IEEE Communications Magazine, </journal> <month> June, </month> <year> 1989. </year>
Reference-contexts: Two simple observations have led to implementations that improve performance. First, Clark showed that often, once a PCB (or small set of them) is referenced, it tends to be preferentially referenced in the future <ref> [Clark89] </ref>. In this case, a single cached PCB pointer can yield substantial performance gains. This pointer always directly addresses the last PCB referenced; thus, a cache hit allows referencing of the correct PCB without the costly list search. <p> A concise summary of the most relevant works follows: Clark, et. al., wrote the seminal paper on packet processing overhead <ref> [Clark89] </ref>. In their discussion of TCP demultiplexing, they first suggested that, if a pointer that directly addresses the last TCB (transmission control block) referenced is cached, the result is a substantial performance benefit. <p> He performed experiments that traced locality at the process level using fine-grained time resolution and concluded that packets to a host usually arrive for the process that most recently sent a packet, and often with little intervening delay. He also verified the observations of <ref> [Clark89] </ref> concerning processes that receive arriving packets. Kay and Pasquale measured TCP and UDP packet processing overhead for various computer architectures [Kay&Pasq93]. Although they focus on non-data touching overhead for systems that send and receive many small packets, they provide a detailed measure of packet multiplexing and demultiplexing. <p> This single cache capitalizes on the presence of packet trains (i. e., high temporal locality) in arriving traffic. Traditionally, it has proven quite effective for TCP packets <ref> [Clark89] </ref>.
Reference: [Comer94] <author> Douglas E. Comer and David L. Stevens, </author> <title> Internetworking with TCP/IP: ClientServer Programming and Applications, </title> <publisher> Prentice Hall, Inc., </publisher> <year> 1994. </year>
Reference: [Huitema95] <author> Christian Huitema, </author> <type> Multi-homed TCP - IETF Draft, </type> <institution> Network Working Group, </institution> <month> May, </month> <year> 1995. </year> <note> This is a work in progress. </note>
Reference-contexts: To this end, our next activity is to implement the best of our algorithms and compare them with other available solutions. Several novel demultiplexing solutions have recently been proposed or implemented. Interest in a strategy that all but eliminates list searches has culminated in the solution proposed by Huitema <ref> [Huitema95] </ref>. 8 In this solution, communicating TCP entities exchange multipurpose 32-bit PCB identification parameters (included in each entitys initial synchronization packet.) This parameter can, among other actions, specify a context-id, which is normally an identifier used to uniquely and directly address a PCB.
Reference: [Jain91] <author> Raj Jain, </author> <title> A Comparison of Hashing Schemes for Address Lookup in Computer Networks, </title> <journal> IEEE (?), June, </journal> <year> 1991. </year>
Reference: [Kay&Pasq93] <author> Jonathan Kay and Joseph Pasquale, </author> <title> The Importance of Non-Data Touching Processing Overheads in TCP/IP, </title> <booktitle> ACM SIGCOMM 93, </booktitle> <month> September, </month> <year> 1993. </year>
Reference-contexts: He also verified the observations of [Clark89] concerning processes that receive arriving packets. Kay and Pasquale measured TCP and UDP packet processing overhead for various computer architectures <ref> [Kay&Pasq93] </ref>. Although they focus on non-data touching overhead for systems that send and receive many small packets, they provide a detailed measure of packet multiplexing and demultiplexing. They use customized hardware and software mechanisms to measure actual processing time (in microseconds).
Reference: [Klein95] <author> Karl Kleinpaste, Peter Steenkiste, and Brian Zill, </author> <title> Software Support for Outboard Buffering and Checksumming, </title> <type> SIGCOMM 95, </type> <year> 1995. </year>
Reference-contexts: 1. Introduction As network services (such as WorldWide Web, connection services, etc.) proliferate, greater performance demands are placed on the computers that provide them. Likewise, recent efforts have identified packet processing rather than link bandwidth as the primary bottleneck in todays highspeed networks <ref> [Klein95] </ref>. Thus, network servers must process packets with optimal efficiency. TCP/IP (Transmission Control Protocol/Internet Protocol) packet processing efficiency issues are particularly important for two reasons: 1. UNIX-based computer systems abound as network servers; 2.
Reference: [McK&Dove92] <author> Paul E. McKenney and Ken F. Dove, </author> <title> Efficient Demultiplexing of Incoming TCP Packets, </title> <booktitle> ACM SIGCOMM 92, </booktitle> <month> August, </month> <year> 1992. </year>
Reference-contexts: Second, Clark suggested and McKenney and Dove showed analytically that, under certain conditions, improved performance can be achieved when PCBs are distributed over several hash chains <ref> [McK&Dove92] </ref>. The hash chain to search is determined by a hash function. We constructed tracedriven simulations that characterize both caches and hash chains as performance enhancers. These simulations provide comprehensive, quantifiable comparisons and ultimately support a set of implementation recommendations for todays high-demand TCP and UDP servers. <p> They further suggested that, if this optimization failed to be useful, a natural extension of this method is to hash to one of a set of TCB lists with the most recently used TCB sorted first. McKenney and Dove compared various PCB lookup schemes using an analytic approach <ref> [McK&Dove92] </ref>. They were interested in performance of TCP lookups on online transaction processing (OLTP) systems. Such systems characteristically have large aggregate-packet-rates, large numbers of connections, and predominantly small packets.
Reference: [Mentat93] <institution> Mentat TCP/IP Design Overview (extracted from Mentat TCP/IP Internals Manual), Mentat, Inc., </institution> <address> Los Angeles, CA., </address> <month> July, </month> <year> 1993. </year>
Reference-contexts: This hash-id could then be passed to the destination host via the TCP/UDP header so that both hosts connection identification information. In a client/server communication scenario, clearly hash-id collisions can occur; [Chan&Varg95] proposed various ways to address such instances. Finally, Mentat, Inc. provides an entirely different strategy <ref> [Mentat93] </ref>. In their TCP/IP implementation (which requires a UNIX SVR4 STREAMS environment), incoming packets are demultiplexed by IP rather than TCP or UDP.
Reference: [Mogul92] <author> Jeffrey C. Mogul, </author> <title> Network Locality at the Scale of Processes, </title> <journal> ACM Transactions on Computer Systems, </journal> <month> May, </month> <year> 1992. </year>
Reference-contexts: Their comparison results were in terms of expected number of PCBs searched. Mogul examined temporal locality of reference (i. e., referencing of PCBs) as packets depart from and arrive at a network computer <ref> [Mogul92] </ref>. He performed experiments that traced locality at the process level using fine-grained time resolution and concluded that packets to a host usually arrive for the process that most recently sent a packet, and often with little intervening delay. <p> incorporated the Reno release single-cache implementation and found that the cache had no effect on UDP demultiplexing. (most services that use UDP are iterative; hence, cache misses are extremely likely.) Further modifications to UDP packet processing showed that the use of a cache at the sender (as suggested by Mogul <ref> [Mogul92] </ref> ) provided the greatest performance improvement. They used the gprof profiling application to produce their results, which they readily admit is flawed because it assumes all calls to a function take the same amount of time. 1 3.
Reference: [Part&Pink93] <author> Craig Partridge and Stephen Pink, </author> <title> A Faster UDP, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <month> July, </month> <year> 1993. </year>
Reference-contexts: They acknowledge that their results are not consistent with previous findings; their single-user test environment only had a single connection. Partridge and Pink attempted to optimize the UDP implementation in BSD 4.3 Tahoe Release UNIX, which does not use a cache as arriving packets are demultiplexed <ref> [Part&Pink93] </ref>. <p> For this case, this experiment assumes that a fully specified PCB is added to the UDP list. For all other possible port combinations, the local host is assumed to act as a server and (since most UDP-based servers are iterative <ref> [Part&Pink93] </ref>) the packet is matched with a wildcarded PCB that already exists. These assumptions introduce possible skewing of performance measurements due to transient initial conditions. <p> However, if a single cache results in such benefits, how might additional software caches affect performance? A popular presumption asserts that there is little or no benefit (mainly due to cache management overhead) in using more than one software cache to directly reference PCBs for incoming packets (e. g., <ref> [Part&Pink93] </ref>). Contrary to this, our experiments showed that substantial performance gains over BSD 4.3-Reno performance can be realized by a second cache entry. Table 5.2 compares the mean number of instructions executed per TCP PCB lookup for the BSD 4.3-Reno algorithm and the 2-cache algorithm described in section 4.1.3.
Reference: [Stern92] <author> Hal Stern, </author> <title> Managing NFS and NIS, </title> <publisher> OReilly & Associates, Inc., </publisher> <year> 1992. </year>
Reference-contexts: Table 5.9 shows that 1024 hash chains yielded the best performance, but it was only marginally better than 64 (or certainly 128) hash chains. Choosing a larger number of hash chains has a caveat: a larger number results in a larger kernel and data structures therein. See <ref> [Stern92] </ref> for an excellent discussion of memory caching, system memory utilization, and system performance tradeoffs for SunOS and AT&T UNIX - System 5, Release 4. 5.1.9 A Combined 2-cache/multiple-hash chain solution produces marginally better results when the number of hash chains is small.
Reference: [Stevens94] <author> W. Richard Stevens, </author> <title> TCP/IP Illustrated, Volume 1 - The Protocols, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1994. </year>
Reference-contexts: When complete, the new server terminates (after a non-optional wait period) <ref> [Stevens94] </ref>. The above observations suggest that, if PCBs are preferentially referenced, the potential benefits of caching can be significant, since substantial work may be avoided.
Reference: [Wright94] <author> Gary R. Wright and W. Richard Stevens, </author> <title> TCP/IP Illustrated, </title> <booktitle> Volume 2: The Implementation, </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1995. </year>
Reference-contexts: A NULL value is returned when no PCB can be found. In BSD UNIX , SunOS , and other UNIX implementations, the functions tcp_input () and udp_input () invoke in_pcblookup () for TCP and UDP demultiplexing, respectively. See <ref> [Wright94] </ref> for an excellent discussion of this functions implementation. All the algorithms implemented for this study eventually invoke in_pcblookup () . 4.1.1 A non-caching Algorithm: Unconditional Invocation of in_pcblookup () A direct call to in_pcblookup () was the original demultiplexing solution for TCP and UDP.
References-found: 14

