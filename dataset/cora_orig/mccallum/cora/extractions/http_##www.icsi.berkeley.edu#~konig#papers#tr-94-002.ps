URL: http://www.icsi.berkeley.edu/~konig/papers/tr-94-002.ps
Refering-URL: http://www.icsi.berkeley.edu/~konig/papers/welcome.html
Root-URL: http://www.icsi.berkeley.edu
Title: "Eigenlips" for Robust Speech Recognition  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Christoph Bregler and Yochai Konig 
Date: January 1994  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-94-002  
Abstract: In this study we improve the performance of a hybrid connectionist speech recognition system by incorporating visual information about the corresponding lip movements. Specifically, we investigate the benefits of adding visual features in the presence of additive noise and crosstalk (cocktail party effect). Our study extends previous experiments by using a new visual front end, and an alternative architecture for combining the visual and acoustic information. Furthermore, we have extended our recognizer to a multi-speaker, connected letters recognizer. Our results show a significant improvement for the combined architecture (acoustic and visual information) over just the acoustic system in the presence of additive noise and crosstalk. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Benoit, </author> <title> The Intrinsic Bimodality of Speech Communication and the Synthesis of Talking Faces in "HiradaTechnika" (Journal of the Hungarian Telecommunication Association), </title> <booktitle> in 1992. </booktitle>
Reference-contexts: In part this offset is caused by different channel delays, but this "forward-articulation" is also confirmed by psychological experiments <ref> [1] </ref>. As a result we experimented with changing the temporal window from a symmetric window to an asymmetric window, i.e., the 19 frames are combined from 15 frames to the past and 3 future frames.
Reference: [2] <author> H.A. Bourlard and N. </author> <title> Morgan.Connectionist Speech Recognition, A Hybrid Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: It is invariant to steady-state spectral distortions and has been shown to improve the recog nition performance by an order of magnitude with realistic channel distortions. 4. ARCHITECTURE AND BIMODAL SENSOR FUSION When combining the visual information into our hybrid connectionist MLP/HMM speech recognition system <ref> [2] </ref> we have to decide on the level of integration of the two modalities. In our former system [3] we trained a separate TDNN for each modality and combined the viseme and phoneme hypotheses.
Reference: [3] <author> C. Bregler, H. Hild, S. Manke, and A. Waibel, </author> <title> Improving Connected Letter Recognition by Lipreading, </title> <booktitle> in Proc. Int. Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Minneapolis 1993. </address>
Reference-contexts: We simulated such situations by adding car noise and crosstalk of different ratios to clean speech. We have done similar experiments on the same database already described in <ref> [3] </ref>. In this study, however we use a new visual processing technique, a different acoustic front-end, and an alternative hybrid connectionist recognition architecture (MLP/HMM). Our experiments show that with the incorporation of the additional visual modality, we can significantly reduce the recognition error. 2. <p> Ideally the spatial segmentation should be driven by the recognition, and the dimension reduction should be invariant against spatial shifting, scaling, rotation, and lighting. In our earlier system <ref> [3] </ref> we solve the tracking problem by using a template based pyramid approach. Based on the tracking information the visual input to the TDNN classifier was the gray-level coding of the mouth area. The obvious advantage of direct gray-level coding is that we do not throw away any information. <p> ARCHITECTURE AND BIMODAL SENSOR FUSION When combining the visual information into our hybrid connectionist MLP/HMM speech recognition system [2] we have to decide on the level of integration of the two modalities. In our former system <ref> [3] </ref> we trained a separate TDNN for each modality and combined the viseme and phoneme hypotheses. This was necessary, because the visual features were with a higher dimension than the acoustic features, but it also had the flexibility of dynamically weighting the modalities differently, according to the cross-modal entropy.
Reference: [4] <author> C. Bregler and S. Omohundro, </author> <title> Surface Learning with Applications to Lip-Reading, </title> <editor> in Cowan, J.D., Tesauro, G., and Alspector, J. (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> San Fran-cisco, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1994. </year>
Reference-contexts: If we constrain the surface to be linear, it is similar to the space spanned by the first 5 principal components of the 80 dimensional contour database. In the nonlinear case we currently are using a learnable "mixture of local patches" representation which is described in <ref> [4] </ref>. (But the experiments reported here are only based on linear subspaces.) For finding and tracking the lips in new images we compute an energy term, which is the negative sum of all gray-level gradient estimates along the contour.
Reference: [5] <author> B. Dodd and R. Campbell. </author> <title> Hearing by Eye: The Psychology of Lipreading. </title> <publisher> Lawrence Erlbaum Press, </publisher> <year> 1987. </year>
Reference-contexts: In fact it is well known that human speech perception is inherently bi-modal as well <ref> [10, 5] </ref>. The idea of extending automated speech recognition to the visual modality has already been investigated for a long time. As popular non-connectionist approaches the work of Petajan, Bischoff, Bodoff, and Brooke [11], Mase and Pentland [9] should be mentioned. Just recently Goldschen [6] completed a lip reading system.
Reference: [6] <author> A.J. Goldschen. </author> <title> Continuous Automatic Speech Recognition by Lipreading. </title> <type> Ph.D. Dissertation, </type> <institution> School of Engineering and Applied Science of the George Washington University, </institution> <month> Sep 10, </month> <year> 1993. </year>
Reference-contexts: The idea of extending automated speech recognition to the visual modality has already been investigated for a long time. As popular non-connectionist approaches the work of Petajan, Bischoff, Bodoff, and Brooke [11], Mase and Pentland [9] should be mentioned. Just recently Goldschen <ref> [6] </ref> completed a lip reading system. He trained HMMs to discriminate visual information on a continuous word database. Recent connectionist systems were investigated by Yuhas, Goldstein, and Sejnowski [14], who used static images for vowel discrimination.
Reference: [7] <author> H. Hermansky, N. Morgan, A. Bayya, and P. Kohn, </author> <title> RASTA-PLP speech Analysis Technique, </title> <booktitle> in Proc. Int. Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> San Francisco 1992. </address>
Reference-contexts: With the visual information we try to substitute missing acoustic information of the speech. Another error source which breaks most state-of-the-art systems, is caused by non-speech factors like distortion of the channel itself. To be invariant against this, we use the RASTA-PLP method <ref> [7] </ref>. RASTA-PLP replaces the conventional short-term absolute spectrum by a band-pass filtered spectral estimate. It is invariant to steady-state spectral distortions and has been shown to improve the recog nition performance by an order of magnitude with realistic channel distortions. 4.
Reference: [8] <author> M. Kass, A. Witkin, and D. Terzopoulos, SNAKES: </author> <title> Active Contour Models, </title> <booktitle> in Proc. of the First Int. Conf. on Computer Vision, </booktitle> <address> London 1987. </address>
Reference-contexts: Furthermore, we leave to the TDNN the nontrivial task of finding the generalization against spatial shifting, scaling, rotation, and lighting. 2.1. ACTIVE CONTOUR MODELS Our new tracking approach is related to the so called "snakes" <ref> [8] </ref> and "deformable templates" [15]. Basically a "energy function" which measures the quality of the match between image features and a contour model is minimized.
Reference: [9] <author> K. Mase and A. Pentland. </author> <title> LIP READING: Automatic Visual Recognition of Spoken Words. </title> <booktitle> Proc. Image Understanding and Machine Vision, Optical Society of America, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: The idea of extending automated speech recognition to the visual modality has already been investigated for a long time. As popular non-connectionist approaches the work of Petajan, Bischoff, Bodoff, and Brooke [11], Mase and Pentland <ref> [9] </ref> should be mentioned. Just recently Goldschen [6] completed a lip reading system. He trained HMMs to discriminate visual information on a continuous word database. Recent connectionist systems were investigated by Yuhas, Goldstein, and Sejnowski [14], who used static images for vowel discrimination.
Reference: [10] <author> D.W. Massaro and M.M. Cohen, </author> <title> Evaluation and Integration of Visual and Auditory information in Speech Perception. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 9, </volume> <year> 1983. </year>
Reference-contexts: In fact it is well known that human speech perception is inherently bi-modal as well <ref> [10, 5] </ref>. The idea of extending automated speech recognition to the visual modality has already been investigated for a long time. As popular non-connectionist approaches the work of Petajan, Bischoff, Bodoff, and Brooke [11], Mase and Pentland [9] should be mentioned. Just recently Goldschen [6] completed a lip reading system.
Reference: [11] <author> E. Petahan, B. Bischoff, D. Bodoff, </author> <title> and N.M. Brooke. An Improved Automatic Lipreading System to enhance Speech Recognition. </title> <publisher> ACM SIGCHI, </publisher> <year> 1988. </year>
Reference-contexts: In fact it is well known that human speech perception is inherently bi-modal as well [10, 5]. The idea of extending automated speech recognition to the visual modality has already been investigated for a long time. As popular non-connectionist approaches the work of Petajan, Bischoff, Bodoff, and Brooke <ref> [11] </ref>, Mase and Pentland [9] should be mentioned. Just recently Goldschen [6] completed a lip reading system. He trained HMMs to discriminate visual information on a continuous word database. Recent connectionist systems were investigated by Yuhas, Goldstein, and Sejnowski [14], who used static images for vowel discrimination.
Reference: [12] <author> M. </author> <title> Turk and A. </title> <journal> Pentland Eigenfaces for Recognition Journal of Cognitive Neuroscience, </journal> <volume> Volume 3, Number 1, </volume> <publisher> MIT 1991. </publisher>
Reference-contexts: For that reason axis, c: Variations along the 2nd principal axis, d: Variations along the 3rd principal axis we investigate the gray-level matrix coding as well. We call this approach "Eigenlips" in regards to a similar approach from Turk and Pentland <ref> [12] </ref> for face recognition, called "Eigenfaces". the principal axis (or eigenvectors) of the lips. The first 10 principal components are enough to distinguish the full gray-level lip shapes. The contour coding is invariant against spatial shifting, rotation, scaling, and lighting.
Reference: [13] <author> G.J. Wolff, K.V. Prasad, D.G. Stork, </author> <title> and M.Hennecke Lipreading by Neural Networks: Visual Preprocessing, Learning and Sensory Integration. </title> <editor> in Cowan, J.D., Tesauro, G., and Alspector, J. (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1994. </year>
Reference-contexts: Just recently Goldschen [6] completed a lip reading system. He trained HMMs to discriminate visual information on a continuous word database. Recent connectionist systems were investigated by Yuhas, Goldstein, and Sejnowski [14], who used static images for vowel discrimination. Wolff, Prasad, Stork, and Hennecke <ref> [13] </ref> are using a modified TDNN for isolated word segments. We focus on scenarios where the acoustic modality is degraded in a way that causes state-of-the-art speech recognition systems to achieve poor recognition performance. We simulated such situations by adding car noise and crosstalk of different ratios to clean speech.
Reference: [14] <author> B.P. Yuhas, M.H. Goldstein, and T.J. Sejnowski. </author> <title> Integration of Acoustic and Visual Speech Signals using Neural Networks. </title> <journal> IEEE Communications Magazine. </journal>
Reference-contexts: Just recently Goldschen [6] completed a lip reading system. He trained HMMs to discriminate visual information on a continuous word database. Recent connectionist systems were investigated by Yuhas, Goldstein, and Sejnowski <ref> [14] </ref>, who used static images for vowel discrimination. Wolff, Prasad, Stork, and Hennecke [13] are using a modified TDNN for isolated word segments. We focus on scenarios where the acoustic modality is degraded in a way that causes state-of-the-art speech recognition systems to achieve poor recognition performance.
Reference: [15] <author> A. Yuille, </author> <title> Deformable Templates for Face Recognition, </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> Volume 3, Number 1, </volume> <publisher> MIT 1991. </publisher>
Reference-contexts: Furthermore, we leave to the TDNN the nontrivial task of finding the generalization against spatial shifting, scaling, rotation, and lighting. 2.1. ACTIVE CONTOUR MODELS Our new tracking approach is related to the so called "snakes" [8] and "deformable templates" <ref> [15] </ref>. Basically a "energy function" which measures the quality of the match between image features and a contour model is minimized.
References-found: 15

