URL: http://www.upl.cs.wisc.edu/~hamblin/ifp/papers/rpb115.ps
Refering-URL: http://www.upl.cs.wisc.edu/~hamblin/ifp/html/
Root-URL: http://www.cs.wisc.edu
Title: Parallel Algorithms for Integer Factorisation  
Author: Richard P. Brent 
Note: 1 which was ac complished using ECM.  
Address: Canberra, ACT 2601  
Affiliation: Computer Sciences Laboratory Australian National University  
Abstract: The problem of finding the prime factors of large composite numbers has always been of mathematical interest. With the advent of public key cryptosystems it is also of practical importance, because the security of some of these cryptosystems, such as the Rivest-Shamir-Adelman (RSA) system, depends on the difficulty of factoring the public keys. In recent years the best known integer factorisation algorithms have improved greatly, to the point where it is now easy to factor a 60-decimal digit number, and possible to factor numbers larger than 120 decimal digits, given the availability of enough computing power. We describe several algorithms, including the elliptic curve method (ECM), and the multiple-polynomial quadratic sieve (MPQS) algorithm, and discuss their parallel implementation. It turns out that some of the algorithms are very well suited to parallel implementation. Doubling the degree of parallelism (i.e. the amount of hardware devoted to the problem) roughly increases the size of a number which can be factored in a fixed time by 3 decimal digits. Some recent computational results are mentioned for example, the complete factorisation of the 617-decimal digit Fermat number F 11 = 2 2 11 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. P. Brent, </author> <title> "An improved Monte Carlo factorization algorithm", </title> <booktitle> BIT 20 (1980), </booktitle> <pages> 176-184. </pages>
Reference-contexts: B. The run time depends mainly on the size of f; the factor found. (We can assume that f N 1=2 .) Examples are - The trial division algorithm, which has run time O (f (log N ) 2 ). The Pollard "rho" algorithm <ref> [1, 28] </ref> which under plausible assumptions has expected run time O (f 1=2 (log N ) 2 ). <p> Because GCDs are more expensive than multiplications (mod N ), it is preferable to avoid the computation of most of the GCDs by accumulating the product Q (x 2i x i ) mod N . Also, the choice of subscripts 2i and i here is not optimal <ref> [1] </ref>. The "rho" algorithm is an improvement over trial division in that it has (conjectured) expected run time O (p 1=2 (log N ) 2 ) to find a prime factor p of N .
Reference: 2. <author> R. P. Brent, </author> <title> "Factorization of the eleventh Fermat number (preliminary report)", </title> <booktitle> AMS Abstracts 10 (1989), </booktitle> <address> 89T-11-73. </address>
Reference-contexts: For example, the factorisation c 101 = (467 41 1)=(466 1022869) = 4089568263561830388113662969166474269 p 65 was found by ECM. We recently <ref> [2] </ref> completed the factorisation of the 617-decimal digit Fermat num ber F 11 = 2 2 11 + 1. In fact F 11 = 319489 974849 167988556341760475137 3560841906445833920513 p 564 where the 21-digit and 22-digit prime factors were found using ECM, and p 564 is a 564-decimal digit prime.
Reference: 3. <author> R. P. Brent, </author> <title> Factor: an integer factorization program for the IBM PC, </title> <type> Report TR-CS-89-23, </type> <institution> Computer Sciences Laboratory, Australian National University, </institution> <month> Oct. </month> <year> 1989. </year> <note> Available from the author. </note>
Reference-contexts: Remark We take this opportunity to announce the availability of an integer factorisation program written in Turbo Pascal for the IBM PC <ref> [3] </ref>. 10
Reference: 4. <author> R. P. Brent, </author> <title> "Some integer factorization algorithms using elliptic curves", </title> <booktitle> Aus-tralian Computer Science Communications 8 (1986), </booktitle> <pages> 149-163. </pages>
Reference-contexts: The Pollard "rho" algorithm [1, 28] which under plausible assumptions has expected run time O (f 1=2 (log N ) 2 ). Lenstra's "Elliptic Curve Method" (ECM) <ref> [4, 22] </ref> which under plausible assumptions has expected run time O (exp (c (log f log log f ) 1=2 ) (log N ) 2 ); where c is a constant. <p> The cost of an extended GCD computation is about the same as that of 10 to 12 multiplications mod N (see <ref> [4, 19] </ref>). 5.2 One trial of Lenstra's algorithm A trial is the computation involving one random group G. The steps involved are - 1. Choose x 0 ; y 0 and a randomly in [0; N ). <p> in (p 2p 1=2 ; p + 2p 1=2 ), we may show that the optimal choice of m is m = p 1=ff , where ff ~ (2 ln p= ln ln p) 1=2 (5:4) The expected run time is T = p 2=ff+o (1=ff) (5:5) For details, see <ref> [4, 22] </ref>. From (5.5), we see that the exponent 2=ff should be compared with 1 (for trial division) or 1=2 (for Pollard's "rho" method). For 10 10 &lt; p &lt; 10 30 , we have ff 2 (3:2; 5:0) . <p> For 10 10 &lt; p &lt; 10 30 , we have ff 2 (3:2; 5:0) . Because of the overheads involved with ECM, a simpler algorithm such as Pollard's "rho" is preferable for finding factors of size up to about 10 10 (see Figure 1 in <ref> [4] </ref>), but for larger factors the asymptotic advantage of ECM becomes apparent. 5.4 A second phase Both the Pollard "p 1" and Lenstra elliptic curve algorithms can be speeded up by the addition of a second phase. <p> By the birthday paradox argument, there is a good chance that two points in the random walk will coincide after O (jhP ij) 1=2 steps, and when this occurs a nontrivial factor of N can usually be found. Details may be found in <ref> [4, 24] </ref>. The use of a second phase provides a significant speedup in practice, but does not change the asymptotic time bound (5.5). <p> Similar comments apply to other implementation details, such as ways of avoiding most divisions and speeding up group operations [11, 23, 24], ways of choosing good initial points [24, 37], and ways of using preconditioned polynomial evaluation <ref> [4, 26, 40] </ref>. 5.5 Parallel implementation of ECM So long as the expected number of trials is much larger than the number P of processors available, linear speedup is possible by performing P trials in parallel.
Reference: 5. <author> R. P. Brent and G. L. Cohen, </author> <title> "A new lower bound for odd perfect numbers", </title> <booktitle> Mathematics of Computation, </booktitle> <month> July </month> <year> 1989. </year>
Reference-contexts: However, in practice it is only a small fraction of the computation.) 9 7. Some recent computational results In the process of proving the non-existence of an odd perfect number less than 10 300 <ref> [5, 6] </ref>, we needed many factorisations of numbers of the form p n 1, where p and n are prime. For example, the factorisation c 101 = (467 41 1)=(466 1022869) = 4089568263561830388113662969166474269 p 65 was found by ECM.
Reference: 6. <author> R. P. Brent, G. L. Cohen and H. J. J. te Riele, </author> <title> Improved techniques for lower bounds for odd perfect numbers, </title> <note> to appear as a Technical Report, </note> <institution> Computer Sciences Laboratory, Australian National University, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: However, in practice it is only a small fraction of the computation.) 9 7. Some recent computational results In the process of proving the non-existence of an odd perfect number less than 10 300 <ref> [5, 6] </ref>, we needed many factorisations of numbers of the form p n 1, where p and n are prime. For example, the factorisation c 101 = (467 41 1)=(466 1022869) = 4089568263561830388113662969166474269 p 65 was found by ECM.
Reference: 7. <author> R. P. Brent and J. M. Pollard, </author> <title> "Factorization of the eighth Fermat number", </title> <booktitle> Mathematics of Computation 36 (1981), </booktitle> <pages> 627-630. </pages>
Reference-contexts: Thus, if we have an algorithm in class B which can find factors of size 10 22 in a reasonable time, there is a 50 percent chance that the algorithm will be able to completely factor a random number N of size about 10 100 . (See <ref> [7] </ref> for an example, and [12, 16] for the theory.) In cryptographic applications [33] the number N to be factored are not random. More likely they have been constructed with the intention of being difficult to factor. For such numbers, algorithms in class A are preferable. <p> An example of the success of a variation on the Pollard "rho" algorithm is the complete factorisation of the Fermat number F 8 = 2 2 8 + 1 by Brent and Pollard <ref> [7] </ref>. Unfortunately, parallel implementation of the "rho" algorithm does not give linear speedup.
Reference: 8. <author> J. Brillhart, D. H. Lehmer, J. L. Selfridge, B. Tuckerman and S. S. Wagstaff, Jr., </author> <title> Factorizations of b n 1; b = 2; 3; 5; 6; 7; 10; 11; 12 up to high powers, </title> <publisher> American Mathematical Society, </publisher> <address> Providence, Rhode Island, </address> <note> second edition, </note> <year> 1985. </year>
Reference-contexts: We aim for a linear speedup, i.e. S = (P ). If the speedup is linear in the number of processors P , then each processor is being used with efficiency bounded below by a positive constant. There are several recent surveys of integer factorisation algorithms <ref> [8, 9, 13, 24, 29, 32] </ref>. In this paper we concentrate on the efficient parallel implementation of the algorithms. 2. Trial division Trial division is a straightforward factorisation algorithm.
Reference: 9. <author> D. A. Buell, </author> <title> "Factoring: algorithms, computations, and computers", </title> <editor> J. </editor> <booktitle> Supercomputing 1 (1987), </booktitle> <pages> 191-216. </pages>
Reference-contexts: We aim for a linear speedup, i.e. S = (P ). If the speedup is linear in the number of processors P , then each processor is being used with efficiency bounded below by a positive constant. There are several recent surveys of integer factorisation algorithms <ref> [8, 9, 13, 24, 29, 32] </ref>. In this paper we concentrate on the efficient parallel implementation of the algorithms. 2. Trial division Trial division is a straightforward factorisation algorithm.
Reference: 10. <author> T. R. Caron and R. D. Silverman, </author> <title> "Parallel implementation of the quadratic sieve", </title> <editor> J. </editor> <booktitle> Supercomputing 1 (1988), </booktitle> <pages> 273-290. </pages>
Reference-contexts: In quadratic sieve algorithms the numbers w i are the values of one (or more) polynomials with integer coefficients. This makes it easy to factorise the w i by sieving. For details of the process, we refer to the recent papers <ref> [10, 20, 29, 30, 31, 36] </ref>. The conclusion is that the best quadratic sieve algorithms (such as the multiple polynomial quadratic sieve algorithm MPQS [29]) can, under plausible assumptions, factor a number N in time O (exp (c (log N log log N ) 1=2 )), where c ~ 1.
Reference: 11. <author> D. V. Chudnovsky and G. V. Chudnovsky, </author> <title> Sequences of numbers generated by addition in formal groups and new primality and factorization tests, </title> <institution> Dept. of Mathematics, Columbia University, </institution> <month> July </month> <year> 1985. </year>
Reference-contexts: Details may be found in [4, 24]. The use of a second phase provides a significant speedup in practice, but does not change the asymptotic time bound (5.5). Similar comments apply to other implementation details, such as ways of avoiding most divisions and speeding up group operations <ref> [11, 23, 24] </ref>, ways of choosing good initial points [24, 37], and ways of using preconditioned polynomial evaluation [4, 26, 40]. 5.5 Parallel implementation of ECM So long as the expected number of trials is much larger than the number P of processors available, linear speedup is possible by performing P <p> T 1 is the expected run time on one processor, then the expected run time on a parallel machine with P processors is T P = T 1 =P + O (T 1 ) (5:6) The bound (5.6) applies on single-instruction multiple-data (SIMD) machine if we use the Montgomery-Chudnovsky form <ref> [11, 24] </ref> by 2 = x 3 + ax 2 + x instead of the Weierstrass normal form (5.1) in order to avoid divisions. In practice, it may be difficult to perform P trials in parallel because of storage limitations.
Reference: 12. <author> K. Dickman, </author> <title> "On the frequency of numbers containing prime factors of a certain relative magnitude", Ark. </title> <journal> Mat., Astronomi och Fysik, 22A, </journal> <volume> 10 (1930), </volume> <pages> 1-14. </pages>
Reference-contexts: we have an algorithm in class B which can find factors of size 10 22 in a reasonable time, there is a 50 percent chance that the algorithm will be able to completely factor a random number N of size about 10 100 . (See [7] for an example, and <ref> [12, 16] </ref> for the theory.) In cryptographic applications [33] the number N to be factored are not random. More likely they have been constructed with the intention of being difficult to factor. For such numbers, algorithms in class A are preferable.
Reference: 13. <author> R. K. Guy, </author> <title> "How to factor a number", Congressus Numerantum XVI, </title> <booktitle> Proc. Fifth Manitoba Conference on Numerical Mathematics, </booktitle> <address> Winnipeg, </address> <year> 1976, </year> <pages> 49-89. </pages>
Reference-contexts: We aim for a linear speedup, i.e. S = (P ). If the speedup is linear in the number of processors P , then each processor is being used with efficiency bounded below by a positive constant. There are several recent surveys of integer factorisation algorithms <ref> [8, 9, 13, 24, 29, 32] </ref>. In this paper we concentrate on the efficient parallel implementation of the algorithms. 2. Trial division Trial division is a straightforward factorisation algorithm.
Reference: 14. <author> K. F. Ireland and M. Rosen, </author> <title> A Classical Introduction to Modern Number Theory, </title> <publisher> Springer-Verlag, </publisher> <year> 1982, </year> <note> Ch. 18. </note>
Reference-contexts: The identity element I in G is the "point at infinity". The geometric interpretation is straightforward. We refer the reader to <ref> [14, 17] </ref> for an introduction to the theory of elliptic curves. In Lenstra's algorithm [22] the field F is the finite field F p of p elements, where p is a prime factor of N .
Reference: 15. <author> J-R. Joly, </author> <title> "Equations et varietes algebriques sur un corps fini", </title> <booktitle> L'Enseignement Mathematique 19 (1973), </booktitle> <pages> 1-117. </pages>
Reference-contexts: Thus, allowing for the identity element, we have g = jGj 2p + 1. Although this would be sufficient for an approximate analysis of ECM, a much stronger result, the Riemann hypothesis for finite fields <ref> [15] </ref>, is known - jg p 1j &lt; 2p 1=2 : (5:3) Making the (incorrect, but close enough) assumption that g behaves like a random integer distributed uniformly in (p 2p 1=2 ; p + 2p 1=2 ), we may show that the optimal choice of m is m = p
Reference: 16. <author> D. E. Knuth, </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol. 2, </volume> <publisher> Addison Wesley, </publisher> <address> 2nd edition, </address> <year> 1982. </year>
Reference-contexts: we have an algorithm in class B which can find factors of size 10 22 in a reasonable time, there is a 50 percent chance that the algorithm will be able to completely factor a random number N of size about 10 100 . (See [7] for an example, and <ref> [12, 16] </ref> for the theory.) In cryptographic applications [33] the number N to be factored are not random. More likely they have been constructed with the intention of being difficult to factor. For such numbers, algorithms in class A are preferable. <p> With P processors we can perform up to P trials in parallel. Thus, provided P t p, a linear speedup is obtained. In Sections 3 to 6 we assume that N is composite, since in practice this is easily checked using a probabilistic primality test <ref> [16, 32] </ref> which runs in time O (log N ) 3 . It is also convenient to assume that all "small" factors of N have been removed by trial division. 3.
Reference: 17. <author> S. Lang, </author> <title> Elliptic Curves Diophantine Analysis, </title> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: The identity element I in G is the "point at infinity". The geometric interpretation is straightforward. We refer the reader to <ref> [14, 17] </ref> for an introduction to the theory of elliptic curves. In Lenstra's algorithm [22] the field F is the finite field F p of p elements, where p is a prime factor of N .
Reference: 18. <author> R. S. Lehman, </author> <title> "Factoring large integers", </title> <booktitle> Mathematics of Computation 28 (1974), </booktitle> <pages> 637-646. </pages>
Reference-contexts: Most useful factorisation algorithms fall into one of two classes - A. The run time depends mainly on the size of N; the number being factored, and is not strongly dependent on the size of the factor found. Examples are - Lehman's algorithm <ref> [18] </ref> which has a rigorous worst-case run time bound O (N 1=3 ). Shanks's SQUFOF algorithm [38], which has expected run time O (N 1=4 ). Shanks's Class Group algorithm [34, 35] which has run time O (N 1=5+* ) on the assumption of the Generalised Riemann Hypothesis.
Reference: 19. <author> D. H. Lehmer, </author> <title> "Euclid's algorithm for large numbers", </title> <journal> Amer. Math. </journal> <volume> Monthly 45 (1938), </volume> <pages> 227-233. </pages>
Reference-contexts: The cost of an extended GCD computation is about the same as that of 10 to 12 multiplications mod N (see <ref> [4, 19] </ref>). 5.2 One trial of Lenstra's algorithm A trial is the computation involving one random group G. The steps involved are - 1. Choose x 0 ; y 0 and a randomly in [0; N ).
Reference: 20. <author> A. K. Lenstra and M. S. Manasse, </author> <title> Factoring by electronic mail, </title> <type> preprint, </type> <month> 10 June </month> <year> 1989. </year>
Reference-contexts: In quadratic sieve algorithms the numbers w i are the values of one (or more) polynomials with integer coefficients. This makes it easy to factorise the w i by sieving. For details of the process, we refer to the recent papers <ref> [10, 20, 29, 30, 31, 36] </ref>. The conclusion is that the best quadratic sieve algorithms (such as the multiple polynomial quadratic sieve algorithm MPQS [29]) can, under plausible assumptions, factor a number N in time O (exp (c (log N log log N ) 1=2 )), where c ~ 1. <p> The process requires very little communication between processors. Each processor can generate relations and forward them to some central collection point. This has been demonstrated most clearly by A. K. Lenstra and M. S. Manasse <ref> [20] </ref> who distribute their program and collect relations via electronic mail. The processors are scattered around the world anyone with access to electronic mail and a C compiler can volunteer to contribute. (The final stage Gaussian elimination to combine the relations is not so easily distributed. <p> The factorisation required about 360 million multiplications mod N , which took less than 2 hours on a Fujitsu VP 100 vector processor. Using the MPQS algorithm and their worldwide distributed network <ref> [20] </ref>, Lenstra and Manasse (with many assistants, including the present author) have factorised several numbers larger than 10 100 , the largest (at the time of writing) having 106 decimal digits.
Reference: 21. <author> A. K. Lenstra and M. S. Manasse, </author> <type> personal communication, </type> <month> 28 August </month> <year> 1989. </year>
Reference-contexts: Lenstra and Manasse <ref> [21] </ref> recently announced the factorisation of the 122-decimal digit number c 122 = (7 149 + 1)=(2 3 10133), in fact c 122 = 47338433355189929279110650931837806119829008573928501623 p 66 This impressive factorisation was obtained using an unpublished algorithm, the Number Field Sieve (NFS) due to J. M. Pollard, A. K.
Reference: 22. <author> H. W. Lenstra, Jr., </author> <title> "Factoring integers with elliptic curves", </title> <journal> Ann. of Math. </journal> <volume> (2) 126 (1987), </volume> <pages> 649-673. </pages>
Reference-contexts: The Pollard "rho" algorithm [1, 28] which under plausible assumptions has expected run time O (f 1=2 (log N ) 2 ). Lenstra's "Elliptic Curve Method" (ECM) <ref> [4, 22] </ref> which under plausible assumptions has expected run time O (exp (c (log f log log f ) 1=2 ) (log N ) 2 ); where c is a constant. <p> The identity element I in G is the "point at infinity". The geometric interpretation is straightforward. We refer the reader to [14, 17] for an introduction to the theory of elliptic curves. In Lenstra's algorithm <ref> [22] </ref> the field F is the finite field F p of p elements, where p is a prime factor of N . The multiplicative group of F p , used in Pollard's "p 1" algorithm, is replaced by the group G defined by (5.1) and (5.2). <p> in (p 2p 1=2 ; p + 2p 1=2 ), we may show that the optimal choice of m is m = p 1=ff , where ff ~ (2 ln p= ln ln p) 1=2 (5:4) The expected run time is T = p 2=ff+o (1=ff) (5:5) For details, see <ref> [4, 22] </ref>. From (5.5), we see that the exponent 2=ff should be compared with 1 (for trial division) or 1=2 (for Pollard's "rho" method). For 10 10 &lt; p &lt; 10 30 , we have ff 2 (3:2; 5:0) .
Reference: 23. <author> P. L. Montgomery, </author> <title> "Modular multiplication without trial division", </title> <booktitle> Mathematics of Computation 44 (1985), </booktitle> <pages> 519-521. </pages>
Reference-contexts: Details may be found in [4, 24]. The use of a second phase provides a significant speedup in practice, but does not change the asymptotic time bound (5.5). Similar comments apply to other implementation details, such as ways of avoiding most divisions and speeding up group operations <ref> [11, 23, 24] </ref>, ways of choosing good initial points [24, 37], and ways of using preconditioned polynomial evaluation [4, 26, 40]. 5.5 Parallel implementation of ECM So long as the expected number of trials is much larger than the number P of processors available, linear speedup is possible by performing P
Reference: 24. <author> P. L. Montgomery, </author> <title> "Speeding the Pollard and elliptic curve methods of factorization", </title> <booktitle> Mathematics of Computation 48 (1987), </booktitle> <pages> 243-264. </pages>
Reference-contexts: We aim for a linear speedup, i.e. S = (P ). If the speedup is linear in the number of processors P , then each processor is being used with efficiency bounded below by a positive constant. There are several recent surveys of integer factorisation algorithms <ref> [8, 9, 13, 24, 29, 32] </ref>. In this paper we concentrate on the efficient parallel implementation of the algorithms. 2. Trial division Trial division is a straightforward factorisation algorithm. <p> The Pollard "p - 1" algorithm Pollard's "p 1" algorithm <ref> [24, 27] </ref> is based on Fermat's theorem a p1 = 1 mod p for 0 &lt; a &lt; p, p prime. Suppose that p is a prime factor of N and that E is a multiple of p 1. <p> By the birthday paradox argument, there is a good chance that two points in the random walk will coincide after O (jhP ij) 1=2 steps, and when this occurs a nontrivial factor of N can usually be found. Details may be found in <ref> [4, 24] </ref>. The use of a second phase provides a significant speedup in practice, but does not change the asymptotic time bound (5.5). <p> Details may be found in [4, 24]. The use of a second phase provides a significant speedup in practice, but does not change the asymptotic time bound (5.5). Similar comments apply to other implementation details, such as ways of avoiding most divisions and speeding up group operations <ref> [11, 23, 24] </ref>, ways of choosing good initial points [24, 37], and ways of using preconditioned polynomial evaluation [4, 26, 40]. 5.5 Parallel implementation of ECM So long as the expected number of trials is much larger than the number P of processors available, linear speedup is possible by performing P <p> The use of a second phase provides a significant speedup in practice, but does not change the asymptotic time bound (5.5). Similar comments apply to other implementation details, such as ways of avoiding most divisions and speeding up group operations [11, 23, 24], ways of choosing good initial points <ref> [24, 37] </ref>, and ways of using preconditioned polynomial evaluation [4, 26, 40]. 5.5 Parallel implementation of ECM So long as the expected number of trials is much larger than the number P of processors available, linear speedup is possible by performing P trials in parallel. <p> T 1 is the expected run time on one processor, then the expected run time on a parallel machine with P processors is T P = T 1 =P + O (T 1 ) (5:6) The bound (5.6) applies on single-instruction multiple-data (SIMD) machine if we use the Montgomery-Chudnovsky form <ref> [11, 24] </ref> by 2 = x 3 + ax 2 + x instead of the Weierstrass normal form (5.1) in order to avoid divisions. In practice, it may be difficult to perform P trials in parallel because of storage limitations.
Reference: 25. <author> M. A. Morrison and J. Brillhart, </author> <title> "A method of factorization and the factorization of F 7 ", Mathematics of Computation 29 (1975), </title> <type> 183-205. </type>
Reference-contexts: Shanks's SQUFOF algorithm [38], which has expected run time O (N 1=4 ). Shanks's Class Group algorithm [34, 35] which has run time O (N 1=5+* ) on the assumption of the Generalised Riemann Hypothesis. The Continued Fraction algorithm <ref> [25] </ref> and the Multiple Polynomial Quadratic Sieve algorithm [29], which under plausible assumptions have expected run time O (exp (c (log N log log N ) 1=2 )); where c is a constant (depending on details of the algorithm). B.
Reference: 26. <author> M. Paterson and L. Stockmeyer, </author> <title> "On the number of nonscalar multiplications necessary to evaluate polynomials", </title> <journal> SIAM J. on Computing 2 (1973), </journal> <pages> 60-66. </pages>
Reference-contexts: Similar comments apply to other implementation details, such as ways of avoiding most divisions and speeding up group operations [11, 23, 24], ways of choosing good initial points [24, 37], and ways of using preconditioned polynomial evaluation <ref> [4, 26, 40] </ref>. 5.5 Parallel implementation of ECM So long as the expected number of trials is much larger than the number P of processors available, linear speedup is possible by performing P trials in parallel.
Reference: 27. <author> J. M. Pollard, </author> <title> "Theorems in factorization and primality testing", </title> <journal> Proc. Cam-bridge Philos. Soc. </journal> <volume> 76 (1974), </volume> <pages> 521-528. </pages>
Reference-contexts: The Pollard "p - 1" algorithm Pollard's "p 1" algorithm <ref> [24, 27] </ref> is based on Fermat's theorem a p1 = 1 mod p for 0 &lt; a &lt; p, p prime. Suppose that p is a prime factor of N and that E is a multiple of p 1.
Reference: 28. <author> J. M. Pollard, </author> <title> "A Monte Carlo method for factorization", </title> <booktitle> BIT 15 (1975), </booktitle> <pages> 331-334. </pages>
Reference-contexts: B. The run time depends mainly on the size of f; the factor found. (We can assume that f N 1=2 .) Examples are - The trial division algorithm, which has run time O (f (log N ) 2 ). The Pollard "rho" algorithm <ref> [1, 28] </ref> which under plausible assumptions has expected run time O (f 1=2 (log N ) 2 ). <p> It is also convenient to assume that all "small" factors of N have been removed by trial division. 3. The Pollard "rho" algorithm Pollard's "rho" algorithm <ref> [28] </ref> uses an iteration of the form x i+1 = f (x i ) mod N; i 0; where N is the number to be factored, x 0 is a random starting value, and f is a polynomial with integer coefficients.
Reference: 29. <author> C. Pomerance, </author> <title> "Analysis and comparison of some integer factoring algorithms", in Computational Methods in Number Theory (edited by H. </title> <editor> W. Lenstra, Jr. and R. Tijdeman), </editor> <publisher> Math. Centrum Tract 154, </publisher> <address> Amsterdam, </address> <year> 1982, </year> <pages> 89-139. </pages>
Reference-contexts: Shanks's SQUFOF algorithm [38], which has expected run time O (N 1=4 ). Shanks's Class Group algorithm [34, 35] which has run time O (N 1=5+* ) on the assumption of the Generalised Riemann Hypothesis. The Continued Fraction algorithm [25] and the Multiple Polynomial Quadratic Sieve algorithm <ref> [29] </ref>, which under plausible assumptions have expected run time O (exp (c (log N log log N ) 1=2 )); where c is a constant (depending on details of the algorithm). B. <p> We aim for a linear speedup, i.e. S = (P ). If the speedup is linear in the number of processors P , then each processor is being used with efficiency bounded below by a positive constant. There are several recent surveys of integer factorisation algorithms <ref> [8, 9, 13, 24, 29, 32] </ref>. In this paper we concentrate on the efficient parallel implementation of the algorithms. 2. Trial division Trial division is a straightforward factorisation algorithm. <p> In quadratic sieve algorithms the numbers w i are the values of one (or more) polynomials with integer coefficients. This makes it easy to factorise the w i by sieving. For details of the process, we refer to the recent papers <ref> [10, 20, 29, 30, 31, 36] </ref>. The conclusion is that the best quadratic sieve algorithms (such as the multiple polynomial quadratic sieve algorithm MPQS [29]) can, under plausible assumptions, factor a number N in time O (exp (c (log N log log N ) 1=2 )), where c ~ 1. <p> This makes it easy to factorise the w i by sieving. For details of the process, we refer to the recent papers [10, 20, 29, 30, 31, 36]. The conclusion is that the best quadratic sieve algorithms (such as the multiple polynomial quadratic sieve algorithm MPQS <ref> [29] </ref>) can, under plausible assumptions, factor a number N in time O (exp (c (log N log log N ) 1=2 )), where c ~ 1.
Reference: 30. <author> C. Pomerance, J. W. Smith and R. Tuler, </author> <title> "A pipeline architecture for factoring large integers with the quadratic sieve algorithm", </title> <journal> SIAM J. on Computing 17 (1988), </journal> <pages> 387-403. </pages>
Reference-contexts: In quadratic sieve algorithms the numbers w i are the values of one (or more) polynomials with integer coefficients. This makes it easy to factorise the w i by sieving. For details of the process, we refer to the recent papers <ref> [10, 20, 29, 30, 31, 36] </ref>. The conclusion is that the best quadratic sieve algorithms (such as the multiple polynomial quadratic sieve algorithm MPQS [29]) can, under plausible assumptions, factor a number N in time O (exp (c (log N log log N ) 1=2 )), where c ~ 1.
Reference: 31. <author> H. J. J. te Riele, W. Lioen and D. Winter, </author> <title> Factoring with the quadratic sieve on large vector computers, </title> <type> Report NM-R8805, </type> <institution> Centre for Mathematics and Computer Science, </institution> <address> Amsterdam, </address> <year> 1988. </year>
Reference-contexts: In quadratic sieve algorithms the numbers w i are the values of one (or more) polynomials with integer coefficients. This makes it easy to factorise the w i by sieving. For details of the process, we refer to the recent papers <ref> [10, 20, 29, 30, 31, 36] </ref>. The conclusion is that the best quadratic sieve algorithms (such as the multiple polynomial quadratic sieve algorithm MPQS [29]) can, under plausible assumptions, factor a number N in time O (exp (c (log N log log N ) 1=2 )), where c ~ 1.
Reference: 32. <author> H. Riesel, </author> <title> Prime Numbers and Computer Methods for Factorization, </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1985. </year>
Reference-contexts: We aim for a linear speedup, i.e. S = (P ). If the speedup is linear in the number of processors P , then each processor is being used with efficiency bounded below by a positive constant. There are several recent surveys of integer factorisation algorithms <ref> [8, 9, 13, 24, 29, 32] </ref>. In this paper we concentrate on the efficient parallel implementation of the algorithms. 2. Trial division Trial division is a straightforward factorisation algorithm. <p> With P processors we can perform up to P trials in parallel. Thus, provided P t p, a linear speedup is obtained. In Sections 3 to 6 we assume that N is composite, since in practice this is easily checked using a probabilistic primality test <ref> [16, 32] </ref> which runs in time O (log N ) 3 . It is also convenient to assume that all "small" factors of N have been removed by trial division. 3.
Reference: 33. <author> R. L. Rivest, A. Shamir and L. Adelman, </author> <title> "A method for obtaining digital dig-natures and public-key cryptosystems", </title> <booktitle> Communications of the ACM 21 (1978), </booktitle> <pages> 120-126. </pages>
Reference-contexts: can find factors of size 10 22 in a reasonable time, there is a 50 percent chance that the algorithm will be able to completely factor a random number N of size about 10 100 . (See [7] for an example, and [12, 16] for the theory.) In cryptographic applications <ref> [33] </ref> the number N to be factored are not random. More likely they have been constructed with the intention of being difficult to factor. For such numbers, algorithms in class A are preferable.
Reference: 34. <author> R. J. Schoof, </author> <title> "Quadratic fields and factorization", in Studieweek Getaltheorie en Computers (edited by J. </title> <editor> van de Lune), </editor> <publisher> Math. Centrum, </publisher> <address> Amsterdam, </address> <year> 1980, </year> <pages> 165-206. </pages>
Reference-contexts: Examples are - Lehman's algorithm [18] which has a rigorous worst-case run time bound O (N 1=3 ). Shanks's SQUFOF algorithm [38], which has expected run time O (N 1=4 ). Shanks's Class Group algorithm <ref> [34, 35] </ref> which has run time O (N 1=5+* ) on the assumption of the Generalised Riemann Hypothesis.
Reference: 35. <author> D. Shanks, </author> <title> "Class number, a theory of factorization, and genera", </title> <journal> Proc. Symp. Pure Math. 20, American Math. Soc., </journal> <year> 1971, </year> <pages> 415-440. </pages>
Reference-contexts: Examples are - Lehman's algorithm [18] which has a rigorous worst-case run time bound O (N 1=3 ). Shanks's SQUFOF algorithm [38], which has expected run time O (N 1=4 ). Shanks's Class Group algorithm <ref> [34, 35] </ref> which has run time O (N 1=5+* ) on the assumption of the Generalised Riemann Hypothesis.
Reference: 36. <author> R. D. Silverman, </author> <title> "The multiple polynomial quadratic sieve", </title> <booktitle> Mathematics of Computation 48 (1987), </booktitle> <pages> 329-339. </pages>
Reference-contexts: In quadratic sieve algorithms the numbers w i are the values of one (or more) polynomials with integer coefficients. This makes it easy to factorise the w i by sieving. For details of the process, we refer to the recent papers <ref> [10, 20, 29, 30, 31, 36] </ref>. The conclusion is that the best quadratic sieve algorithms (such as the multiple polynomial quadratic sieve algorithm MPQS [29]) can, under plausible assumptions, factor a number N in time O (exp (c (log N log log N ) 1=2 )), where c ~ 1.
Reference: 37. <author> H. Suyama, </author> <type> Informal preliminary report (8), personal communication, </type> <month> October </month> <year> 1985. </year>
Reference-contexts: The use of a second phase provides a significant speedup in practice, but does not change the asymptotic time bound (5.5). Similar comments apply to other implementation details, such as ways of avoiding most divisions and speeding up group operations [11, 23, 24], ways of choosing good initial points <ref> [24, 37] </ref>, and ways of using preconditioned polynomial evaluation [4, 26, 40]. 5.5 Parallel implementation of ECM So long as the expected number of trials is much larger than the number P of processors available, linear speedup is possible by performing P trials in parallel.
Reference: 38. <author> M. Voorhoeve, </author> <title> "Factorization", in Studieweek Getaltheorie en Computers (edited by J. </title> <editor> van de Lune), </editor> <publisher> Math. Centrum, </publisher> <address> Amsterdam, </address> <year> 1980, </year> <pages> 61-68. </pages>
Reference-contexts: The run time depends mainly on the size of N; the number being factored, and is not strongly dependent on the size of the factor found. Examples are - Lehman's algorithm [18] which has a rigorous worst-case run time bound O (N 1=3 ). Shanks's SQUFOF algorithm <ref> [38] </ref>, which has expected run time O (N 1=4 ). Shanks's Class Group algorithm [34, 35] which has run time O (N 1=5+* ) on the assumption of the Generalised Riemann Hypothesis.
Reference: 39. <author> D. </author> <title> Wiedemann, "Solving sparse linear equations over finite fields", </title> <journal> IEEE Trans. Inform. Theory 32 (1986), </journal> <pages> 54-62. </pages>
Reference-contexts: Each relation (6.1) gives a row in matrix M whose columns correspond to the primes in the factor base. Once enough rows have been generated, we can use Gaussian elimination in F 2 <ref> [39] </ref> to find a linear dependency (mod 2) between a set of rows of M . Multiplying the corresponding relations now gives a relation of the form (6.1). In quadratic sieve algorithms the numbers w i are the values of one (or more) polynomials with integer coefficients.
Reference: 40. <author> S. Winograd, </author> <title> "Evaluating polynomials using rational auxiliary functions", </title> <journal> IBM Technical Disclosure Bulletin 13 (1970), </journal> <pages> 1133-1135. 12 </pages>
Reference-contexts: Similar comments apply to other implementation details, such as ways of avoiding most divisions and speeding up group operations [11, 23, 24], ways of choosing good initial points [24, 37], and ways of using preconditioned polynomial evaluation <ref> [4, 26, 40] </ref>. 5.5 Parallel implementation of ECM So long as the expected number of trials is much larger than the number P of processors available, linear speedup is possible by performing P trials in parallel.
References-found: 40

