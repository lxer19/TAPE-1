URL: http://www.ics.uci.edu/~pedrod/imlm.ps.gz
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: pedrod@ics.uci.edu  
Title: Using Partitioning to Speed Up Specific-to-General Rule Induction  
Author: Pedro Domingos 
Web: http://www.ics.uci.edu/~pedrod  
Address: Irvine, California 92717, U.S.A.  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: RISE (Domingos 1995; in press) is a rule induction algorithm that proceeds by gradually generalizing rules, starting with one rule per example. This has several advantages compared to the more common strategy of gradually specializing initially null rules, and has been shown to lead to significant accuracy gains over algorithms like C4.5RULES and CN2 in a large number of application domains. However, RISE's running time (like that of other rule induction algorithms) is quadratic in the number of examples, making it unsuitable for processing very large databases. This paper studies the use of partitioning to speed up RISE, and compares it with the well-known method of windowing. The use of partitioning in a specific-to-general induction setting creates synergies that would not be possible with a general-to-specific system. Partitioning often reduces running time and improves accuracy at the same time. In noisy conditions, the performance of windowing deteriorates rapidly, while that of partitioning remains stable. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W.; Kibler, D.; and Albert, M. K. </author> <year> 1991. </year> <title> Instance-based learning algorithms. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 37-66. </pages>
Reference-contexts: Thus RISE's behavior is in many ways similar to that of nearest-neighbor or instance-based algorithms <ref> (Aha, Kibler, & Albert 1991) </ref>. The distance measure used in RISE is a combination of Euclidean distance for numeric attributes, and a simplified version of Stanfill and Waltz's value difference metric for symbolic attributes (Stanfill & Waltz 1986).
Reference: <author> Breiman, L. </author> <title> Bagging predictors. Machine Learning. </title> <publisher> In press. </publisher>
Reference: <author> Catlett, J. </author> <year> 1991. </year> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> Ph.D. Dissertation, </type> <institution> Basser Department of Computer Science, University of Syd-ney, </institution> <address> Sydney, Australia. </address>
Reference-contexts: This is particularly likely in noisy domains, where it has been observed to lead to serious performance degradation in the case of C4.5 <ref> (Catlett 1991) </ref>. To avoid this, the implementation used in RISE also limits the number of times the window is grown to a prespecified maximum (5 by default). <p> diabetes 71.62.5 70.62.7 74.42.1 73.63.3 72.82.6 Annealing 97.50.9 98.01.0 93.61.6 96.11.6 96.51.1 Chess 98.40.6 98.40.7 94.50.5 95.20.6 96.60.9 Hypothyroid 97.90.2 97.50.5 97.00.3 97.50.3 97.90.4 Splice junctions 92.50.8 92.80.7 95.00.7 94.60.7 94.70.6 Mushroom 100.00.0 100.00.0 98.90.1 99.50.3 99.80.1 and there are seven possible classes, corresponding to states of the shuttle's radiators <ref> (Catlett 1991) </ref>. The goal is to predict these states with very high accuracy (99-99.9%), using rules that can be taught to a human operator. the number of examples for RISE, RISE with partitioning (using e max = 100), and RISE with windowing, on a log-log scale.
Reference: <author> Chan, P. K., and Stolfo, S. J. </author> <year> 1995a. </year> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 90-98. </pages> <address> Tahoe City, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: On the splice junctions dataset, the success of applying partitioning to RISE using a simple combination scheme contrasts with the results obtained by Chan and Stolfo for general-to-specific learners <ref> (Chan & Stolfo 1995a) </ref>. In general, the best partition size should be determined by experimentation on the specific database RISE is being applied to, starting with smaller (and therefore faster) values.
Reference: <author> Chan, P. K., and Stolfo, S. J. </author> <year> 1995b. </year> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 39-44. </pages> <address> Montreal, Canada: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Thus rules with high apparent accuracy are favored only if they also have high statistical support (i.e., if that apparent accuracy is not simply the result of a small sample). Partitioning In the partitioning speedup approach <ref> (Chan & Stolfo 1995b) </ref>, the training data is divided into a number of disjoint subsets, and the learning algorithm is applied to each in turn. The results of each run are combined in some fashion, either at learning or at classification time. <p> The second method was found to achieve consistently better results, and was therefore adopted. More sophisticated combination methods based on Bayesian theory are currently being studied, but have so far yielded inferior results. Many other combination schemes are possible (e.g., <ref> (Chan & Stolfo 1995b) </ref>). Windowing Windowing is applied to RISE in a fashion similar to C4.5's (Quinlan 1993), and proceeds as follows. Initially, only 2 p e examples randomly extracted from the training set are used for learning. <p> Its superiority over the commonly-used method of windowing is particularly apparent in the case of noisy data. Directions for future research include testing and developing more sophisticated methods of combining the outputs of the individual partitions (e.g., <ref> (Chan & Stolfo 1995b) </ref>), automating the selection of partition size, and testing partitioning on a larger variety of larger databases. Acknowledgments This work was partly supported by a JNICT/PRAXIS XXI scholarship. The author is grateful to all those who provided the datasets used in the empirical study.
Reference: <author> DeGroot, M. H. </author> <year> 1986. </year> <title> Probability and Statistics. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, 2nd edition. </publisher>
Reference-contexts: (including C4.5RULES and CN2) on 30 databases from the UCI repository (Murphy & Aha 1995), RISE was found to be more accurate than each of the other algorithms in about two-thirds of the databases, in each case with a confidence of 98% or better according to a Wilcoxon signed-ranks test <ref> (DeGroot 1986) </ref>, and had the highest average accuracy and highest rank. RISE's running time, like that of previous algorithms, is quadratic in the number of examples, and thus the question arises of whether it is possible to reduce this time to linear without compromising accuracy.
Reference: <author> Domingos, P. </author> <year> 1995. </year> <title> Rule induction and instance-based learning: A unified approach. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1226-1232. </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Domingos, P. </author> <title> Unifying instance-based and rule-based induction. Machine Learning. </title> <publisher> In press. </publisher>
Reference: <author> Holte, R. C.; Acker, L. E.; and Porter, B. W. </author> <year> 1989. </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> 813-818. </pages> <address> De-troit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This has several advantages, among them the ability to detect with confidence a higher level of detail in the databases, and a reduction of sensitivity to the fragmentation (Pagallo & Haussler 1990) and small disjuncts problems <ref> (Holte, Acker, & Porter 1989) </ref>.
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1995. </year> <title> UCI repository of machine learning databases. Machine-readable data repository, </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA. </address>
Reference-contexts: In a study comparing RISE with several induction algorithms (including C4.5RULES and CN2) on 30 databases from the UCI repository <ref> (Murphy & Aha 1995) </ref>, RISE was found to be more accurate than each of the other algorithms in about two-thirds of the databases, in each case with a confidence of 98% or better according to a Wilcoxon signed-ranks test (DeGroot 1986), and had the highest average accuracy and highest rank. <p> Empirical Evaluation The two speedup methods were tested on seven of the UCI repository's largest databases <ref> (Murphy & Aha 1995) </ref> (in increasing order of size: credit screening (Australian), Pima diabetes, annealing, chess endgames (kr-vs-kp), hypothyroid, splice junctions, and mushroom). Of these, at least one (Pima diabetes) is thought to be quite noisy, and at least two (chess and mushroom) aer known to be almost entirely noise-free.
Reference: <author> Niblett, T. </author> <year> 1987. </year> <title> Constructing decision trees in noisy domains. </title> <booktitle> In Proceedings of the Second Euro-pean Working Session on Learning, </booktitle> <pages> 67-78. </pages> <address> Bled, Yu-goslavia: Sigma. </address>
Reference-contexts: When two or more rules are equally close to a test example, the rule that was most accurate on the training set wins. So as to not unduly favor more specific rules, the Laplace-corrected accuracy is used <ref> (Niblett 1987) </ref>: LAcc (R) = N corr (R) + 1 N won (R) + C where R is any rule, C is the number of classes, N won (R) is the total number or examples won by R, N corr (R) is the number of examples among those that R correctly
Reference: <author> Pagallo, G., and Haussler, D. </author> <year> 1990. </year> <title> Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 71-99. </pages>
Reference-contexts: This has several advantages, among them the ability to detect with confidence a higher level of detail in the databases, and a reduction of sensitivity to the fragmentation <ref> (Pagallo & Haussler 1990) </ref> and small disjuncts problems (Holte, Acker, & Porter 1989).
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: More sophisticated combination methods based on Bayesian theory are currently being studied, but have so far yielded inferior results. Many other combination schemes are possible (e.g., (Chan & Stolfo 1995b)). Windowing Windowing is applied to RISE in a fashion similar to C4.5's <ref> (Quinlan 1993) </ref>, and proceeds as follows. Initially, only 2 p e examples randomly extracted from the training set are used for learning.
Reference: <author> Stanfill, C., and Waltz, D. </author> <year> 1986. </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM 29 </journal> <pages> 1213-1228. </pages>
Reference-contexts: Thus RISE's behavior is in many ways similar to that of nearest-neighbor or instance-based algorithms (Aha, Kibler, & Albert 1991). The distance measure used in RISE is a combination of Euclidean distance for numeric attributes, and a simplified version of Stanfill and Waltz's value difference metric for symbolic attributes <ref> (Stanfill & Waltz 1986) </ref>. When two or more rules are equally close to a test example, the rule that was most accurate on the training set wins.
Reference: <author> Wolpert, D. </author> <year> 1992. </year> <title> Stacked generalization. </title> <booktitle> Neural Networks 5 </booktitle> <pages> 241-259. </pages>
References-found: 15

