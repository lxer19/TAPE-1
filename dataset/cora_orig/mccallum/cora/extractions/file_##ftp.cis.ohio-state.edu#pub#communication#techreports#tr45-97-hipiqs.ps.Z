URL: file://ftp.cis.ohio-state.edu/pub/communication/techreports/tr45-97-hipiqs.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~sivaram/publications.html
Root-URL: 
Title: HIPIQS: A High-Performance Switch Architecture using Input Queuing  
Author: Rajeev Sivaram, Craig B. Stunkel, and Dhabaleswar K. Panda 
Abstract: Technical Report OSU-CISRC-10/97-TR45 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Allen, P. T. Gaughan, D. E. Schimmel, and S. Yalamanchali. Ariadne: </author> <title> A Fault Tolerant Router for Multicomputers. </title> <booktitle> In Proceedings of the ACM Annual International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] </ref> to local area and wide area networks [31, 17, 14, 6]. Current generation switches have typically been built keeping particular application domains in mind.
Reference: [2] <author> ATM Forum. </author> <title> ATM User-Network Interface Specification, </title> <note> Version 3.1, </note> <month> September </month> <year> 1994. </year>
Reference-contexts: On the other hand, network switches place greater emphasis on throughput while tolerating latencies that are higher by orders of magnitude. Furthermore, many of these switches are built for fixed size packets (such as cells in ATM networks <ref> [2] </ref>). An important design decision involves the queuing of blocked packets in the buffers of a switch. Two major queuing organizations have been proposed in the literature: input queuing and output queuing [29, 12]. In input queuing, packet queues are maintained at input buffers.
Reference: [3] <author> N. J. Boden, D. Cohen, and et al. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 29-35, </pages> <month> Feb </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] </ref> to local area and wide area networks [31, 17, 14, 6]. Current generation switches have typically been built keeping particular application domains in mind.
Reference: [4] <author> G. A. Boughton. </author> <title> Arctic routing chip. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> volume 853, </volume> <pages> pages 310-317. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <booktitle> (Proc. First Int. Workshop on Parallel Computer Routing & Communication, </booktitle> <month> May '94). </month>
Reference-contexts: Instead, an arriving packet may use any of the buffers, usually the one that has the least number of packets in it. Such a scheme has been used for PaRC [11], Arctic <ref> [4] </ref>, and HAL's PRC [19] switch chips. In these switches, k buffers are maintained at every input port (where k is the number of input and output ports in the switch) and an arriving packet may use any of these k buffers. <p> The crossbar complexity of such a scheme is higher than those of the DAMQ or cross-point schemes <ref> [4] </ref> as each buffer is an input to the crossbar. <p> The DAMQ approach adds complexity of the required central arbitration. The cost of this arbitration varies with its optimality and with the time constraints in which it operates. It must be noted that several contemporary switches such as Arctic <ref> [4] </ref>, HAL's PRC [30], and IBM's Prizma [6], already incur crossbar complexity similar to HIPIQS (approximately k 3 f ). Furthermore, the HIPIQS architecture has significant potential as a basis for multicast.
Reference: [5] <author> Tzi-cker Chiueh and Srinidhi Varadarajan. </author> <title> Design and Evaluation of a DRAM-based Shared Memory ATM Switch. </title> <booktitle> In Proceedings of ACM Sigmetrics '97 Conference, </booktitle> <pages> pages 248-259, </pages> <year> 1997. </year>
Reference-contexts: In particular, a form of output queuing that uses a shared central buffer has been shown to achieve extremely good performance and such an approach is currently being used in a number of current day switches <ref> [27, 6, 5] </ref>. In such a switch, multiple input and output ports may want to access this central buffer concurrently. We therefore need some mechanism to match the input and output bandwidths through the switch.
Reference: [6] <author> W. E. Denzel, A. P. J. Engbersen, and I. Iliadis. </author> <title> Flexible shared-buffer switch for ATM at Gb/s rates. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27(4) </volume> <pages> 611-624, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] to local area and wide area networks <ref> [31, 17, 14, 6] </ref>. Current generation switches have typically been built keeping particular application domains in mind. For example, parallel system interconnects place heavy emphasis on reducing latency and on accommodating a range of packet sizes. <p> In particular, a form of output queuing that uses a shared central buffer has been shown to achieve extremely good performance and such an approach is currently being used in a number of current day switches <ref> [27, 6, 5] </ref>. In such a switch, multiple input and output ports may want to access this central buffer concurrently. We therefore need some mechanism to match the input and output bandwidths through the switch. <p> Such an approach has been used in the IBM Prizma switch <ref> [6] </ref>. The primary drawback of this scheme is that when it is used in systems with varying packet sizes, the solution leads to extensive memory fragmentation or large crossbars. <p> The DAMQ approach adds complexity of the required central arbitration. The cost of this arbitration varies with its optimality and with the time constraints in which it operates. It must be noted that several contemporary switches such as Arctic [4], HAL's PRC [30], and IBM's Prizma <ref> [6] </ref>, already incur crossbar complexity similar to HIPIQS (approximately k 3 f ). Furthermore, the HIPIQS architecture has significant potential as a basis for multicast.
Reference: [7] <author> J. Duato, S. Yalamanchili, and L. Ni. </author> <title> Interconnection Networks: An Engineering Approach. </title> <publisher> The IEEE Computer Society Press, </publisher> <year> 1997. </year>
Reference-contexts: In addition to meeting the possibly diverse demands of various interconnects, this architecture must handle small packets without such fragmentation and loss of bandwidth. Such a switch should provide low latency communication by making use of cut-through switching <ref> [7] </ref> and simple distributed arbitration. Furthermore, it must be capable of delivering high throughput (close to 100%). The switch must also be capable of handling a wide range of message sizes efficiently.
Reference: [8] <author> Mike Galles. Spider: </author> <title> A High-Speed Network Interconnect. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 34-39, </pages> <month> Jan-uary/February </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] </ref> to local area and wide area networks [31, 17, 14, 6]. Current generation switches have typically been built keeping particular application domains in mind. <p> Current switches such as the SGI Spider <ref> [8] </ref> and Tiny Tera [17] use such a queuing scheme coupled with centralized arbiters. Input queuing approaches may also use multiple buffers per input port. One example of such a buffering scheme has separate buffers for every output port.
Reference: [9] <author> David Garcia and William Watson. </author> <title> Servernet II. </title> <booktitle> In Proceedings of the 2nd Parallel Computer Routing and Communication Workshop, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] </ref> to local area and wide area networks [31, 17, 14, 6]. Current generation switches have typically been built keeping particular application domains in mind. <p> If the destination output port for the packet at the head of the queue is busy, the packets behind it are subjected to head of line (HOL) blocking delays. Such a scheme has been shown to have a maximum throughput of around 60% [12]. The switches of Servernet <ref> [10, 9] </ref>, Intel Paragon, and the CM5 [15] use such a queuing scheme. To overcome this HOL blocking, schemes with multiple input queues within the same input buffer have been proposed [29]. Typically, there is one queue corresponding to every output port in every input buffer.
Reference: [10] <author> R. Horst. </author> <title> ServerNet Deadlock Avoidance and Fractahedral Topologies. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <pages> pages 274-280, </pages> <year> 1996. </year> <month> 20 </month>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] </ref> to local area and wide area networks [31, 17, 14, 6]. Current generation switches have typically been built keeping particular application domains in mind. <p> If the destination output port for the packet at the head of the queue is busy, the packets behind it are subjected to head of line (HOL) blocking delays. Such a scheme has been shown to have a maximum throughput of around 60% [12]. The switches of Servernet <ref> [10, 9] </ref>, Intel Paragon, and the CM5 [15] use such a queuing scheme. To overcome this HOL blocking, schemes with multiple input queues within the same input buffer have been proposed [29]. Typically, there is one queue corresponding to every output port in every input buffer.
Reference: [11] <author> Christopher F. Joerg and Andrew Boughton. </author> <title> The Monsoon Interconnection Network. </title> <booktitle> In Proceedings of the International Conference on Computer Design, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Instead, an arriving packet may use any of the buffers, usually the one that has the least number of packets in it. Such a scheme has been used for PaRC <ref> [11] </ref>, Arctic [4], and HAL's PRC [19] switch chips. In these switches, k buffers are maintained at every input port (where k is the number of input and output ports in the switch) and an arriving packet may use any of these k buffers. <p> More interestingly, the performance of HIPIQS very nearly matches the performance of output queuing for a significant range of message sizes. Furthermore, HIPIQS also does better than the published result of [29]. Thus, HIPIQS performs much better than the other input queued architectures presented in the literature <ref> [29, 11] </ref> As mentioned above, this establishes that the performance of HIPIQS is close to the best achievable using either input or output queuing. * The performance of HIPIQS depends on the size of the messages as a fraction of the input buffer size.
Reference: [12] <author> M. Karol, M. Hluchyj, and S. Morgan. </author> <title> Input versus Output Queuing on a Space-Division Packet Switch. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 35(12) </volume> <pages> 1347-1356, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: Furthermore, many of these switches are built for fixed size packets (such as cells in ATM networks [2]). An important design decision involves the queuing of blocked packets in the buffers of a switch. Two major queuing organizations have been proposed in the literature: input queuing and output queuing <ref> [29, 12] </ref>. In input queuing, packet queues are maintained at input buffers. The packets in an input queue may be destined for any of the switch's output ports. However, the packets in a given input queue all arrive from a particular input port of the switch. <p> Furthermore, these queues are in dedicated buffers associated with every output port or within a single buffer shared by all output ports. It has been shown that output queuing performs extremely well <ref> [12] </ref>, and much better than input queuing. In particular, a form of output queuing that uses a shared central buffer has been shown to achieve extremely good performance and such an approach is currently being used in a number of current day switches [27, 6, 5]. <p> If the destination output port for the packet at the head of the queue is busy, the packets behind it are subjected to head of line (HOL) blocking delays. Such a scheme has been shown to have a maximum throughput of around 60% <ref> [12] </ref>. The switches of Servernet [10, 9], Intel Paragon, and the CM5 [15] use such a queuing scheme. To overcome this HOL blocking, schemes with multiple input queues within the same input buffer have been proposed [29]. <p> As can be seen from Fig. 7 the architecture that uses input queuing with a single queue (labeled `FIFO') saturates well before the applied load reaches 0.60. This is consistent with previous research results <ref> [12] </ref>. Since the performance of the single FIFO queue approach is considerably worse than the performance of both HIPIQS and output queuing, we do not consider its performance in the remaining part of this section.
Reference: [13] <author> M. Katevenis, P. Vatsolaki, and A. Efthymiou. </author> <title> Pipelined memory shared buffer for VLSI switches. </title> <booktitle> In Proc. ACM SIGCOMM Conference, </booktitle> <pages> pages 39-48, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Furthermore, it must be capable of delivering high throughput (close to 100%). The switch must also be capable of handling a wide range of message sizes efficiently. In this paper, we present the architecture of such a switch|HIPIQS (HIgh-Performance Input-Queued Switch)|which uses input queuing, pipelined multi-queue input buffers <ref> [13] </ref>, cut-through switching, and simple distributed arbitration, to achieve very high performance (low latency as well as high throughput). <p> Input queuing approaches may also use multiple buffers per input port. One example of such a buffering scheme has separate buffers for every output port. Such a scheme is referred to as cross-point buffering <ref> [13] </ref>, and is equivalent to the SAFC (Statically Allocated, Fully Connected) scheme studied in [29]. A cross-point buffering scheme can theoretically achieve 100% throughput. <p> A similar principle applies to the reads by the output ports. This scheme achieves a performance similar to the `wide memory' approach described above, but may result in a simpler implementation. Such a scheme is described in <ref> [13] </ref>. The main problem with these single module approaches occurs when packets are smaller than a chunk. <p> The architecture extends the functionality of the DAMQ scheme [29] by allowing multiple packets to be transmitted simultaneously from an input buffer. To achieve such simultaneous packet transmissions from an input buffer, the architecture uses efficient, pipelined access to a multi-bank memory <ref> [13] </ref>. An outcome of this ability to transmit multiple packets from an input buffer is that a simple and distributed arbitration scheme can be used which reduces complexity as well as improves efficiency. The architecture also incorporates certain elements of cross-point buffering [13] to improve performance. <p> uses efficient, pipelined access to a multi-bank memory <ref> [13] </ref>. An outcome of this ability to transmit multiple packets from an input buffer is that a simple and distributed arbitration scheme can be used which reduces complexity as well as improves efficiency. The architecture also incorporates certain elements of cross-point buffering [13] to improve performance. We first present the basic idea behind the architecture and the overall organization of the switch. We then describe the structure of each of the input buffers in the switch. <p> In the context of output queuing with shared (centralized) buffers, we have already seen that a buffer with a single read port can satisfy the requests of k output ports concurrently by allowing ports to read in k-flit chunks or by pipelining the concurrent reads through a multi-bank buffer <ref> [13] </ref>. The same principle can be applied to make an input buffer with a DAMQ organization behave like a buffer with multiple read ports. This would eliminate the necessity for output ports to remain idle if all packets destined for them are in buffers that are already being read from. <p> This helps reduce the amount of fragmented space in the buffers. More importantly, it almost eliminates the loss of bandwidth due to multiple output ports accessing packets which are smaller than a chunk. An architecture with cross-point buffering <ref> [13] </ref> also has certain advantages. Most importantly, since every input port-output port pair has a dedicated buffer, an output port can immediately begin transferring data from a corresponding dedicated buffer as soon as it has decided which input port it is to read from next. <p> In this section we use the term output queuing to refer 12 to architectures with a shared central buffer, in particular, a shared buffer consisting of a single module <ref> [27, 13] </ref>. The previously proposed input queuing schemes perform worse than output queuing [29]. By establishing that HIPIQS performs almost as well as output queuing, we show that HIPIQS performs better than previously proposed input queuing schemes.
Reference: [14] <author> Manolis G. H. Katevenis, Panagiota Vatsolaki, Dimitrios Serpanos, and Evangelos Markatos. </author> <title> ATLAS I: A Single-Chip ATM Switch for NOWs. </title> <booktitle> In Proceedings of the Workshop on Communication and Architectural Support for Network-based Parallel Computing (CANPC '97), </booktitle> <month> February </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] to local area and wide area networks <ref> [31, 17, 14, 6] </ref>. Current generation switches have typically been built keeping particular application domains in mind. For example, parallel system interconnects place heavy emphasis on reducing latency and on accommodating a range of packet sizes.
Reference: [15] <author> C. E. Leiserson et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <year> 1992. </year>
Reference-contexts: We then perform detailed simulations to evaluate the performance of HIPIQS in the context of a bidirectional Multistage Interconnection Network (BMIN) based parallel system (such as the IBM SP [27], CM5 <ref> [15] </ref>, and the Meiko CS2 [18]). After comparing HIPIQS with a traditional FIFO based input queuing architecture and a central shared-buffer-based output queuing architecture, we establish that HIPIQS achieves performance very close to that of output queuing. <p> Such a scheme has been shown to have a maximum throughput of around 60% [12]. The switches of Servernet [10, 9], Intel Paragon, and the CM5 <ref> [15] </ref> use such a queuing scheme. To overcome this HOL blocking, schemes with multiple input queues within the same input buffer have been proposed [29]. Typically, there is one queue corresponding to every output port in every input buffer.
Reference: [16] <author> Evangelos P. Markatos and Manolis G. H. Katevenis. Telegraphos: </author> <title> High-Performance Networking for Parallel Processing on Workstation Clusters. </title> <booktitle> In Proceedings of the 2nd IEEE International Symposium on High Performance Computer Architecture (HPCA-2), </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] </ref> to local area and wide area networks [31, 17, 14, 6]. Current generation switches have typically been built keeping particular application domains in mind.
Reference: [17] <author> Nick McKeown, Martin Izzard, Adisak Mekkittikul, William Ellersick, and Mark Horowitz. </author> <title> Tiny Tera: A Packet Switch Core. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 26-33, </pages> <month> January/February </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] to local area and wide area networks <ref> [31, 17, 14, 6] </ref>. Current generation switches have typically been built keeping particular application domains in mind. For example, parallel system interconnects place heavy emphasis on reducing latency and on accommodating a range of packet sizes. <p> Typically, there is one queue corresponding to every output port in every input buffer. Thus, every queue has packets that have arrived from a given input port and that are all destined for the same output port. This scheme is also referred to as virtual output queuing <ref> [17] </ref> because of this. The space for these multiple queues may be allocated statically, wherein, the buffer space is divided among the input queues (according to some distribution) apriori. <p> Current switches such as the SGI Spider [8] and Tiny Tera <ref> [17] </ref> use such a queuing scheme coupled with centralized arbiters. Input queuing approaches may also use multiple buffers per input port. One example of such a buffering scheme has separate buffers for every output port.
Reference: [18] <author> Meiko Limited. </author> <title> Meiko CS-2 System Overview, </title> <year> 1994. </year>
Reference-contexts: We then perform detailed simulations to evaluate the performance of HIPIQS in the context of a bidirectional Multistage Interconnection Network (BMIN) based parallel system (such as the IBM SP [27], CM5 [15], and the Meiko CS2 <ref> [18] </ref>). After comparing HIPIQS with a traditional FIFO based input queuing architecture and a central shared-buffer-based output queuing architecture, we establish that HIPIQS achieves performance very close to that of output queuing.
Reference: [19] <author> Albert Mu, Jeff Larson, Raghu Sastry, Thomas Wicki, and Winfried W. Wilcke. </author> <title> A 9.6 GigaByte/s Throughput Plesiochronous Routing Chip. </title> <booktitle> In Proceedings of COMPCON '96, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Instead, an arriving packet may use any of the buffers, usually the one that has the least number of packets in it. Such a scheme has been used for PaRC [11], Arctic [4], and HAL's PRC <ref> [19] </ref> switch chips. In these switches, k buffers are maintained at every input port (where k is the number of input and output ports in the switch) and an arriving packet may use any of these k buffers.
Reference: [20] <author> D. K. Panda, D. Basak, D. Dai, R. Kesavan, R. Sivaram, M. Banikazemi, and V. Moorthy. </author> <title> Simulation of Modern Parallel Systems: A CSIM-based approach. </title> <booktitle> In Proceedings of the 1997 Winter Simulation Conference (WSC'97), </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: We first describe our basic methodology and our base configuration. We then examine the effect of various system parameters on the performance of HIPIQS in greater detail. 5.1 Experiments We carried out experiments using a C++/CSIM based simulation testbed <ref> [20] </ref> which models cut-through routing on a flit by flit basis. A bidirectional Multistage Interconnection Network (BMIN) based parallel system was assumed for our experiments and various synthetic workloads were used to evaluate the influence of the different system parameters on performance.
Reference: [21] <author> Craig Partridge. </author> <title> Gigabit Networking. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: The latter approach performs better, is easier to implement, and is the most prevalent <ref> [21] </ref>. We will therefore restrict the following discussion to this approach. Since all output queues exist in a single shared buffer, at any given time, all input ports could be contending to write to the same buffer, while all output ports could be contending to read from the same buffer.
Reference: [22] <author> Timothy M. Pinkston, Yungho Choi, and Mongkol Raksapatcharawong. </author> <booktitle> Architecture and Optoelectric Implementation of the WARRP Router. In Proceedings of Hot Interconnects V, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] </ref> to local area and wide area networks [31, 17, 14, 6]. Current generation switches have typically been built keeping particular application domains in mind.
Reference: [23] <author> R. Sivaram, D. K. Panda, and C. B. Stunkel. </author> <title> Efficient Broadcast and Multicast on Multistage Interconnection Networks using Multiport Encoding. </title> <booktitle> In Proceedings of the 8th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 36-45, </pages> <month> Oct </month> <year> 1996. </year>
Reference-contexts: In this paper we have focussed on point to point (unicast) message communication. Multicasting of packets which have multiple destination ports at a given switch can also be supported in HIPIQS with architectural enhancements similar to the ones proposed in <ref> [23, 25] </ref>. However, the details of multicast implementation are beyond the scope of the current paper and hence are not included. 4 Qualitative Comparison of Alternative Switch Architectures HIPIQS is targeted for switching environments that require high throughput even for very small packets.
Reference: [24] <author> C. B. Stunkel, D. Shea, D. G. Grice, P. H. Hochschild, and M. Tsao. </author> <title> The SP1 High Performance Switch. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <pages> pages 150-157, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] </ref> to local area and wide area networks [31, 17, 14, 6]. Current generation switches have typically been built keeping particular application domains in mind.
Reference: [25] <author> C. B. Stunkel, R. Sivaram, and D. K. Panda. </author> <title> Implementing Multidestination Worms in Switch-Based Parallel Systems: Architectural Alternatives and their Impact. </title> <booktitle> In Proceedings of the 24th IEEE/ACM Annual International Symposium on Computer Architecture (ISCA-24), </booktitle> <pages> pages 50-61, </pages> <month> June </month> <year> 1997. </year> <month> 21 </month>
Reference-contexts: In this paper we have focussed on point to point (unicast) message communication. Multicasting of packets which have multiple destination ports at a given switch can also be supported in HIPIQS with architectural enhancements similar to the ones proposed in <ref> [23, 25] </ref>. However, the details of multicast implementation are beyond the scope of the current paper and hence are not included. 4 Qualitative Comparison of Alternative Switch Architectures HIPIQS is targeted for switching environments that require high throughput even for very small packets.
Reference: [26] <author> Craig B. </author> <title> Stunkel. </title> <booktitle> Challenges in the Design of Contemporary Routers. In Proceedings of the 2nd Parallel Computer Routing and Communication Workshop, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: It can also be observed that the bandwidth of links used for interconnection has continued to increase in the recent years <ref> [26] </ref>. This increase in link bandwidth has caused a further increase in the width of the data paths within output-queued switches with shared central buffers. <p> The 1996 version of the IBM SP switches contain a total of 5 KBytes of storage with 2 bytes/flit. This corresponds to 320 flit buffers for an 8-port input buffered switch with equal total storage. Recognizing improvements in VLSI technology and the trend towards using larger buffers in switches <ref> [26] </ref>, we assume a default buffer size of 640 flits for the switches with input buffers and an equivalent amount of space for the central buffered switch.
Reference: [27] <author> Craig B. Stunkel, D. G. Shea, B. Abali, et al. </author> <title> The SP2 High-Performance Switch. </title> <journal> IBM System Journal, </journal> <volume> 34(2) </volume> <pages> 185-204, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] </ref> to local area and wide area networks [31, 17, 14, 6]. Current generation switches have typically been built keeping particular application domains in mind. <p> In particular, a form of output queuing that uses a shared central buffer has been shown to achieve extremely good performance and such an approach is currently being used in a number of current day switches <ref> [27, 6, 5] </ref>. In such a switch, multiple input and output ports may want to access this central buffer concurrently. We therefore need some mechanism to match the input and output bandwidths through the switch. <p> We therefore need some mechanism to match the input and output bandwidths through the switch. A practical approach to match input and output bandwidths through a switch with a shared central buffer is to use wider data paths within the switch <ref> [27] </ref>. Input and output bandwidths are matched by scheduling one write and one read to/from the shared buffer every cycle. <p> We then perform detailed simulations to evaluate the performance of HIPIQS in the context of a bidirectional Multistage Interconnection Network (BMIN) based parallel system (such as the IBM SP <ref> [27] </ref>, CM5 [15], and the Meiko CS2 [18]). After comparing HIPIQS with a traditional FIFO based input queuing architecture and a central shared-buffer-based output queuing architecture, we establish that HIPIQS achieves performance very close to that of output queuing. <p> This requires the input ports to buffer k flits before writing and output ports to serialize the transmission of the k-flit chunk after reading. Such an approach is used by the IBM SP2 switch <ref> [27] </ref>. Another solution that eliminates the requirement for such a `wide memory' is to split the memory module into multiple banks. <p> In this section we use the term output queuing to refer 12 to architectures with a shared central buffer, in particular, a shared buffer consisting of a single module <ref> [27, 13] </ref>. The previously proposed input queuing schemes perform worse than output queuing [29]. By establishing that HIPIQS performs almost as well as output queuing, we show that HIPIQS performs better than previously proposed input queuing schemes.
Reference: [28] <author> Yuval Tamir and Hsin-Chou Chi. </author> <title> Symmetric Crossbar Arbiters for VLSI Communication Switches. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 13-27, </pages> <year> 1993. </year>
Reference-contexts: Furthermore, since 3 no more than one output port can read from a given input buffer at a time, this scheme requires a fairly complex, centralized arbitration scheme to determine a conflict-free assignment of output ports to input buffers that have packets for them <ref> [28] </ref>. Current switches such as the SGI Spider [8] and Tiny Tera [17] use such a queuing scheme coupled with centralized arbiters. Input queuing approaches may also use multiple buffers per input port. One example of such a buffering scheme has separate buffers for every output port.
Reference: [29] <author> Yuval Tamir and Gregory L. Frazier. </author> <title> Dynamically-Allocated Multi-Queue Buffers for VLSI Communication Switches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(6) </volume> <pages> 725-737, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Furthermore, many of these switches are built for fixed size packets (such as cells in ATM networks [2]). An important design decision involves the queuing of blocked packets in the buffers of a switch. Two major queuing organizations have been proposed in the literature: input queuing and output queuing <ref> [29, 12] </ref>. In input queuing, packet queues are maintained at input buffers. The packets in an input queue may be destined for any of the switch's output ports. However, the packets in a given input queue all arrive from a particular input port of the switch. <p> The switches of Servernet [10, 9], Intel Paragon, and the CM5 [15] use such a queuing scheme. To overcome this HOL blocking, schemes with multiple input queues within the same input buffer have been proposed <ref> [29] </ref>. Typically, there is one queue corresponding to every output port in every input buffer. Thus, every queue has packets that have arrived from a given input port and that are all destined for the same output port. <p> An alternative is to dynamically allocate space among the queues according to the proportion of the traffic that is intended for the corresponding destination output ports. Such a scheme is referred to as DAMQ <ref> [29] </ref> (dynamically allocated multi-queue). Under the DAMQ scheme, packets at the head of the individual (FIFO) queues are eligible for transmission. However, only one of these packets may be transmitted from the buffer at any given time. Thus, there is a single data output per input buffer. <p> Input queuing approaches may also use multiple buffers per input port. One example of such a buffering scheme has separate buffers for every output port. Such a scheme is referred to as cross-point buffering [13], and is equivalent to the SAFC (Statically Allocated, Fully Connected) scheme studied in <ref> [29] </ref>. A cross-point buffering scheme can theoretically achieve 100% throughput. However, the main problem with cross-point buffering is that the space associated with an input port is not shared dynamically: traffic in which some output ports are more frequently used than others will be adversely affected. <p> The architecture extends the functionality of the DAMQ scheme <ref> [29] </ref> by allowing multiple packets to be transmitted simultaneously from an input buffer. To achieve such simultaneous packet transmissions from an input buffer, the architecture uses efficient, pipelined access to a multi-bank memory [13]. <p> packets, but such a scheme would be very complex, requiring maintenance of lists of partially filled modules and then a matching of a packet's size with the entries in such a list (which would also increase latency significantly). 5 3.1 Basic Idea One of the limiting factors of the DAMQ <ref> [29] </ref> performance is that each of its (input) buffers has a single read port. Because of this, no more than one output port can read from a buffer at any given time. <p> In this section we use the term output queuing to refer 12 to architectures with a shared central buffer, in particular, a shared buffer consisting of a single module [27, 13]. The previously proposed input queuing schemes perform worse than output queuing <ref> [29] </ref>. By establishing that HIPIQS performs almost as well as output queuing, we show that HIPIQS performs better than previously proposed input queuing schemes. Furthermore, this also demonstrates that the performance of HIPIQS is close to the best achievable by either input or output queuing. <p> This is consistent with previous research results [12]. Since the performance of the single FIFO queue approach is considerably worse than the performance of both HIPIQS and output queuing, we do not consider its performance in the remaining part of this section. The published results of <ref> [29] </ref> show that the DAMQ scheme has a saturation throughput of about 0.76 for packet sizes that are about 20% of the input buffer size. For similar packet sizes, HIPIQS achieves a saturation throughput of over 0.90. <p> More interestingly, the performance of HIPIQS very nearly matches the performance of output queuing for a significant range of message sizes. Furthermore, HIPIQS also does better than the published result of <ref> [29] </ref>. <p> More interestingly, the performance of HIPIQS very nearly matches the performance of output queuing for a significant range of message sizes. Furthermore, HIPIQS also does better than the published result of [29]. Thus, HIPIQS performs much better than the other input queued architectures presented in the literature <ref> [29, 11] </ref> As mentioned above, this establishes that the performance of HIPIQS is close to the best achievable using either input or output queuing. * The performance of HIPIQS depends on the size of the messages as a fraction of the input buffer size.
Reference: [30] <author> W.-D. Weber, S. Gold, P. Helland, T. Shimizu, T. Wicki, and W. Wilcke. </author> <title> The Mercury interconnect architecture: A cost-effective infrastructure for high-performance servers. </title> <booktitle> In Proc. 24th Ann. Int. Symp. on Computer Architecture, </booktitle> <pages> pages 98-107, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations <ref> [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] </ref> to local area and wide area networks [31, 17, 14, 6]. Current generation switches have typically been built keeping particular application domains in mind. <p> The DAMQ approach adds complexity of the required central arbitration. The cost of this arbitration varies with its optimality and with the time constraints in which it operates. It must be noted that several contemporary switches such as Arctic [4], HAL's PRC <ref> [30] </ref>, and IBM's Prizma [6], already incur crossbar complexity similar to HIPIQS (approximately k 3 f ). Furthermore, the HIPIQS architecture has significant potential as a basis for multicast.
Reference: [31] <author> Mingyao Yang and Lionel M. Ni. </author> <title> Design of Scalable and Multicast Capable Cut-Through Switches for High-Speed LANs. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 324-332, </pages> <month> August </month> <year> 1997. </year> <month> 22 </month>
Reference-contexts: 1 Introduction Switch-based interconnects are used for a large number of applications. Such applications vary from low-latency interconnects for parallel systems based on networks of workstations [3, 10, 9, 24, 27, 8, 30, 16, 22, 1] to local area and wide area networks <ref> [31, 17, 14, 6] </ref>. Current generation switches have typically been built keeping particular application domains in mind. For example, parallel system interconnects place heavy emphasis on reducing latency and on accommodating a range of packet sizes.
References-found: 31

