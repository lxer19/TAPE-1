URL: http://www.cs.Helsinki.FI/research/cosco/Articles/aistat97.ps.gz
Refering-URL: http://www.cwi.nl/~pdg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: 26,  
Title: Comparing Predictive Inference Methods for Discrete Domains  
Author: Petri Kontkanen, Petri Myllymaki, Tomi Silander, and Henry Tirri Peter Grunwald 
Date: January 1997)  
Address: (Ft. Lauderdale, USA,  P.O.Box  FIN-00014 University of Helsinki, Finland  P.O. Box 94079, NL-1090 GB Amsterdam, The Netherlands  
Affiliation: Artificial Intelligence and Statistics  Complex Systems Computation Group (CoSCo)  Department of Computer Science  CWI, Department of Algorithms and Architectures  
Note: Pp. 311-318 in Proceedings of the Sixth International Workshop on  
Abstract: Predictive inference is seen here as the process of determining the predictive distribution of a discrete variable, given a data set of training examples and the values for the other problem domain variables. We consider three approaches for computing this predictive distribution, and assume that the joint probability distribution for the variables belongs to a set of distributions determined by a set of parametric models. In the simplest case, the predictive distribution is computed by using the model with the maximum a posteriori (MAP) posterior probability. In the evidence approach, the predictive distribution is obtained by averaging over all the individual models in the model family. In the third case, we define the predictive distribution by using Rissanen's new definition of stochastic complexity. Our experiments performed with the family of Naive Bayes models suggest that when using all the data available, the stochastic complexity approach produces the most accurate predictions in the log-score sense. However, when the amount of available training data is decreased, the evidence approach clearly outperforms the two other approaches. The MAP predictive distribution is clearly inferior in the log-score sense to the two more sophisticated approaches, but for the 0/1-score the MAP approach may still in some cases produce the best results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: prediction formula is as follows: P ev (Y = kjI; D) / P ev ( ~ d [k]; D) / P ev (D) h k + k P K m Y f kil + kil P n i : The P ev formula can be derived using the results in <ref> [1] </ref>.
Reference: [2] <author> M.H. </author> <title> DeGroot. Optimal statistical decisions. </title> <publisher> McGraw-Hill, </publisher> <year> 1970. </year>
Reference-contexts: Since the family of Dirichlet densities is conjugate (see e.g. <ref> [2] </ref>) to the family of multinomials, i.e. the functional form of parameter distribution remains invariant in the prior-to-posterior transformation, we assume that the prior distributions of the parameters are from this family.
Reference: [3] <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Comparing Bayesian model class selection criteria by discrete finite mixtures. </title> <editor> In D. Dowe, K. Korb, and J. Oliver, editors, </editor> <booktitle> Information, Statistics and Induction in Science (Proceedings of the ISIS'96 Conference), </booktitle> <pages> pages 364-374, </pages> <address> Melbourne, Australia, </address> <month> August </month> <year> 1996. </year> <title> World Scientific, </title> <publisher> Singapore. </publisher>
Reference-contexts: For this missing data case, the three predictive distributions described here can only be solved analytically by summing over all the possible instantiations of the missing data, which are exponential in number. Fortunately, there exists computationally efficient methods for approximating this exponential sum (see e.g., the discussion in <ref> [3] </ref>), so the three methods can be also applied in the approximative sense in the general finite mixture case. It is interesting to see whether the empirical results obtained here apply also for the finite mixture model family.
Reference: [4] <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Constructing Bayesian finite mixture models by the EM algorithm. </title> <type> Technical Report C-1996-9, </type> <institution> University of Helsinki, Department of Computer Science, </institution> <month> February </month> <year> 1996. </year>
Reference: [5] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: Consequently, the logarithm of evidence can be seen as an approximation of stochastic complexity (SC), the shortest possible codelength for the data as defined in <ref> [5] </ref>. Rissanen has recently [6] introduced an alternative coding scheme, which in some cases produces much shorter codes than the evidence approach, while retaining the code length approximately the same for the fl URL: http://www.cs.Helsinki.FI/research/cosco/ 311 other cases. <p> The resulting predictive distribution is P ev (Y = kjI; D) = P K = P ( ~ d [k]; Djfi)P (fi)dfi: P K R P ( ~ d [k 0 ]; Djfi)P (fi)dfi: : (6) 2.3 The stochastic complexity predictive distribution Rissanen <ref> [5, 6] </ref> has introduced the notion of stochastic complexity of a data set D relative to a class of models M to be the code length of D when it is encoded using the shortest code obtainable with the help of the class M.
Reference: [6] <author> J. Rissanen. </author> <title> Fisher information and stochastic complexity. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(1) </volume> <pages> 40-47, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Consequently, the logarithm of evidence can be seen as an approximation of stochastic complexity (SC), the shortest possible codelength for the data as defined in [5]. Rissanen has recently <ref> [6] </ref> introduced an alternative coding scheme, which in some cases produces much shorter codes than the evidence approach, while retaining the code length approximately the same for the fl URL: http://www.cs.Helsinki.FI/research/cosco/ 311 other cases. <p> The resulting predictive distribution is P ev (Y = kjI; D) = P K = P ( ~ d [k]; Djfi)P (fi)dfi: P K R P ( ~ d [k 0 ]; Djfi)P (fi)dfi: : (6) 2.3 The stochastic complexity predictive distribution Rissanen <ref> [5, 6] </ref> has introduced the notion of stochastic complexity of a data set D relative to a class of models M to be the code length of D when it is encoded using the shortest code obtainable with the help of the class M. <p> This motivates the use of P sc for prediction. In terms of formulas, log P ev (D) has served as the original definition of the stochastic complexity. Recently, however, Rissanen <ref> [6] </ref> has shown that there exists a code that is itself not dependent on any prior distributions of parameters and which in general yields even shorter codelengths than the code with lengths log P ev (D).
Reference: [7] <author> H. Tirri, P. Kontkanen, and P. Myllymaki. </author> <title> Probabilistic instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 507-515. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year> <month> 318 </month>
Reference-contexts: Interestingly enough, although not the purpose of this paper, our empirical results show that the relatively simple Naive Bayes model used obtains very good prediction accuracy when compared to results obtained by alternative techniques (see the results referenced in <ref> [7] </ref>). Consequently, it would be interesting to see how the three prediction methods perform when used in conjunction with the more complex finite mixture model family. These research issues will be addressed in our future work.
References-found: 7

