URL: http://http.cs.berkeley.edu/~tzuyi/cs263/project.ps
Refering-URL: http://http.cs.berkeley.edu/~tzuyi/cs263/project.html
Root-URL: 
Title: Language Support for Mixed Parallelism  
Author: Tzu-Yi Chen 
Date: July 16, 1996  
Abstract: Traditionally, parallel languages have been either purely task parallel or purely data parallel. However, because of evidence showing that several applications and algorithms could benefit from the use of both forms of parallelism (i.e. from mixed parallelism), several groups are looking into providing support for mixed parallelism. Although this support can come in several forms, this paper focusses on presenting an overview of some of the current work regarding language support for mixed parallelism. The languages discussed are NESL, UC, OPUS, and a few proposed extensions to HPF. Each language is subjectively analyzed with regards to overall potential, implementation, ease-of-use, and flexibility. Examples and descriptions of algorithms and applications that could benefit from support for mixed parallelism are also given.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> NESL home page. </institution> <note> url: http://www.cs.cmu.edu/~scandal/nesl.html. </note>
Reference-contexts: In [20] this is done by translating the user's algorithm into ANSI C with MPI calls, and linking this code with an MPI (Message Passing Interface) library. Many applications that have been coded in NESL can be found on their web page <ref> [1] </ref>. A few examples are: * fft * primes 3 More information on NESL can be found on their homepage. See [1]. 3 * sparse matrix vector multiplication [11] * planar convex-hull [20] 2.1.2 UC UC 4 is a language being developed jointly at Caltech and UCLA. <p> Many applications that have been coded in NESL can be found on their web page <ref> [1] </ref>. A few examples are: * fft * primes 3 More information on NESL can be found on their homepage. See [1]. 3 * sparse matrix vector multiplication [11] * planar convex-hull [20] 2.1.2 UC UC 4 is a language being developed jointly at Caltech and UCLA. Inspired by set-based languages such as SETL, UC began as an attempt to add data-parallel constructs to C.
Reference: [2] <institution> OPUS home page. </institution> <note> url: http://meru.uwyo.edu/~haines/proj/opus.html. </note>
Reference-contexts: OPUS is an extension to HPF which provides the "support necessary for coordinating the parallel execution, communication, and synchronization of [codes used in multidisciplinary optimization applications]" [23]. A multidisciplinary optimization 5 More information on OPUS can be found on their homepage. See <ref> [2] </ref>. 5 application takes codes from different disciplines and integrates them in a complex system where the separate codes can generally execute asynchronously, needing only to communicate to share data. Hence the main challenge in implementing OPUS was to determine the protocol for sharing data between two data-parallel codes.
Reference: [3] <institution> UC home page. </institution> <note> url: http://may.cs.ucla.edu/projects/uc/. </note>
Reference-contexts: Applications that have been coded in UC include: * fft [17] * dense gaussian elimination with pivoting [7] * simulation of mosquito control [7] * diffusion aggregation in fluid flow [7] 4 More information on UC can be found on their homepage. See <ref> [3] </ref>. 4 2.1.3 Task Parallel Extensions to HPF HPF (High Performance Fortran) is a data parallel language developed for use in scientific computing. Proposals have been made suggesting task parallel extensions to HPF. Two such proposals are mentioned in this section.
Reference: [4] <institution> ZPL home page. </institution> <note> url: http://www.cs.washington.edu/research/projects/zpl. 12 </note>
Reference-contexts: Channels provide a means for two tasks to communicate with one another, and are responsible for the conversion 6 More information on ZPL can be found on their homepage. See <ref> [4] </ref>. 7 between potentially different data distributions between the two tasks. These were obviously meant to deal with multidisciplinary optimization applications as described in Section 2.1.4. 3 Language Analysis This section attempts the ambitious task of comparing the languages discussed in Section 2.1.
Reference: [5] <author> B. Avalani, A. Choudhary, I. Foster, and R. Krishnaiyer. </author> <title> Integrating task and data parallelism using parallel I/O techniques. </title> <booktitle> In Proceedings of an International Workshop on Parallel Processing, </booktitle> <month> Dec </month> <year> 1994. </year> <note> url: ftp://erc.cat.syr.edu/ece/choudhary/PASSION/task data.ps.Z. </note>
Reference-contexts: Papers have described mixed parallelism in the following terms: "an integrated task and data parallel system requires data parallel tasks to communicate and synchronize with each other" <ref> [5] </ref>, and "In a purely data parallel execution, the tasks in the task graph are executed one at a time using all the processors for each. In mixed parallelism, each task is spread over a subset of processors" [12]. <p> Another group has written a paper presenting a model for letting data parallel tasks communicate with one another through a mechanism called channels <ref> [5] </ref>. Channels provide a means for two tasks to communicate with one another, and are responsible for the conversion 6 More information on ZPL can be found on their homepage. See [4]. 7 between potentially different data distributions between the two tasks.
Reference: [6] <author> R. Bagrodia and V. Austel. </author> <title> UC User Manual, </title> <type> Version 1.4. </type> <institution> University of California at Los Angeles, </institution> <month> Aug </month> <year> 1992. </year>
Reference: [7] <author> R. Bagrodia, K. M. Chandy, and M. Dhagat. </author> <title> UC | a set-based language for data-parallel programming. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 28(2) </volume> <pages> 186-201, </pages> <year> 1995. </year>
Reference-contexts: Language support for mixed parallelism has been added gradually to the language so that a programmer has been able to specify the synchronization of a set of statements at ever increasing levels of granularity. <ref> [7, 17] </ref>. Original support for mixed parallelism was limited to enclosing function calls that could be executed in parallel in par or arb constructs. A function inside par can be executed on each element of a set specified by an index in parallel. <p> The addition of send and receive (implemented using channels) permits explicit control over communication. Applications that have been coded in UC include: * fft [17] * dense gaussian elimination with pivoting <ref> [7] </ref> * simulation of mosquito control [7] * diffusion aggregation in fluid flow [7] 4 More information on UC can be found on their homepage. See [3]. 4 2.1.3 Task Parallel Extensions to HPF HPF (High Performance Fortran) is a data parallel language developed for use in scientific computing. <p> The addition of send and receive (implemented using channels) permits explicit control over communication. Applications that have been coded in UC include: * fft [17] * dense gaussian elimination with pivoting <ref> [7] </ref> * simulation of mosquito control [7] * diffusion aggregation in fluid flow [7] 4 More information on UC can be found on their homepage. See [3]. 4 2.1.3 Task Parallel Extensions to HPF HPF (High Performance Fortran) is a data parallel language developed for use in scientific computing. <p> The addition of send and receive (implemented using channels) permits explicit control over communication. Applications that have been coded in UC include: * fft [17] * dense gaussian elimination with pivoting <ref> [7] </ref> * simulation of mosquito control [7] * diffusion aggregation in fluid flow [7] 4 More information on UC can be found on their homepage. See [3]. 4 2.1.3 Task Parallel Extensions to HPF HPF (High Performance Fortran) is a data parallel language developed for use in scientific computing. Proposals have been made suggesting task parallel extensions to HPF. <p> was expected to be efficient across multiple architectures, with different inputs, or with a different target number of solutions to search for [16]. 4.3 Simulation of Physical Processes The simulation of physical processes is a broad category describing a wide variety of applications including: * diffusion aggregation in fluid flow. <ref> [7] </ref> * climate modeling. [14, 12, 15] * solar radiation model, model of the gulf stream dynamics. [15] * multilevel models in a single discipline: linear, Euler, and Navier-Stokes models in aerodynamics [13].
Reference: [8] <author> P. Banerjee, J. A. Chandy, M. Gupta, J. G. Holm, A. Lain, D. J. Palermo, S. Ra-maswamy, and E. Su. </author> <title> The PARADIGM compiler for distributed-memory message passing multicomputers. </title> <booktitle> In First International Workshop on Parallel Processing, </booktitle> <address> Bangalore, India, </address> <month> Dec </month> <year> 1994. </year>
Reference-contexts: There are many projects related to developing compilers that can exploit mixed parallelism. Two are the Paradigm project being done at the University of Illinois at Urbana-Champaign addresses mixed parallelism <ref> [8] </ref>, and the iWarp compiler developed at CMU [25].
Reference: [9] <author> G. E. Blelloch. NESL: </author> <title> A nested data-parallel language (3.1). </title> <type> Technical Report CMU-CS-95-170, </type> <institution> Carnegie Mellon University, </institution> <month> Sep </month> <year> 1995. </year> <note> url: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-95-170.html. </note>
Reference: [10] <author> G. E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 39(3), </volume> <month> Mar </month> <year> 1996. </year> <note> url: http://www.cs.cmu/~scandal/cacm.html. </note>
Reference-contexts: Nested data-parallel constructs permit the nesting of such parallel calls. Different from many other parallel languages, in NESL the functions applied can be parallel functions. The ability to nest such functions gives support for mixed parallelism <ref> [10] </ref>. A simple example is quicksort written in NESL in [11]. In this program, each element is compared with a pivot element in data-parallel fashion and sorted into one of three sets depending on whether the element is greater than, equal to, or less than the pivot element.
Reference: [11] <author> G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1), </volume> <month> apr </month> <year> 1994. </year> <note> url: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/nesl-ppopp93. </note>
Reference-contexts: Nested data-parallel constructs permit the nesting of such parallel calls. Different from many other parallel languages, in NESL the functions applied can be parallel functions. The ability to nest such functions gives support for mixed parallelism [10]. A simple example is quicksort written in NESL in <ref> [11] </ref>. In this program, each element is compared with a pivot element in data-parallel fashion and sorted into one of three sets depending on whether the element is greater than, equal to, or less than the pivot element. <p> In terms of implementation, the original NESL compiler compiled NESL code to an intermediate vector code (VCODE) format. The vector instructions in this language-independent VCODE format are then mapped to a library of low-level, architecture specific, vector functions <ref> [11] </ref>. More recent work, directed towards creating an efficient implementation for irregular divide and conquer algorithms, uses a method based on asynchronous processor groups to reduce communication and a run-time load-balancing system to cope with dynamic data distributions. <p> Many applications that have been coded in NESL can be found on their web page [1]. A few examples are: * fft * primes 3 More information on NESL can be found on their homepage. See [1]. 3 * sparse matrix vector multiplication <ref> [11] </ref> * planar convex-hull [20] 2.1.2 UC UC 4 is a language being developed jointly at Caltech and UCLA. Inspired by set-based languages such as SETL, UC began as an attempt to add data-parallel constructs to C.
Reference: [12] <author> S. Chakrabarti, J. Demmel, and K. Yelick. </author> <title> Modeling the benefits of mixed data and task parallelism. </title> <booktitle> In 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1995. </year> <note> url: http://www.cs.ucsb.edu/Conferences/SPAA-PPOPP95. </note>
Reference-contexts: In mixed parallelism, each task is spread over a subset of processors" <ref> [12] </ref>. Examples of mixed parallelism include algorithms that create and search combinatorial search trees | where the algorithm is primarily data parallel yet can use task parallelism to implement load balancing across the processors. <p> However, this section also briefly discusses compilers in Section 2.2.1 and non-language-specific work in Subsection 2.2.2. 1 These last two are called switched parallelism in <ref> [12] </ref>. 2 Of course, to some extent, all analysis on the topics of flexibility and ease-of-use are subjective. You are encouraged to refer to the original papers in order to form your own opinions. 2 2.1 Languages Two types of languages with support for mixed parallelism are currently being developed. <p> One such language is Advanced ZPL, a mixed parallel language being developed at the Univeristy of Washington. Advanced ZPL, not yet fully implemented, will support switched parallelism as defined in <ref> [12] </ref>. <p> a few general types of applications which are thought of as potentially benefitting from support for mixed parallelism. 4.1 Divide and Conquer Algorithms Divide and conquer algorithms are used in many applications, including those for finding eigenvalues of a symmetric matrix, quicksort, and the Barnes-Hut algorithm for doing N-body simulations <ref> [12, 20] </ref>. Fundamentally, at each level of the algorithm, data-parallel code is executed before the algorithm recurses, calling the same data-parallel code on some number of 7 Are these two facts related? Maybe. 9 disjoint subproblems in parallel. <p> efficient across multiple architectures, with different inputs, or with a different target number of solutions to search for [16]. 4.3 Simulation of Physical Processes The simulation of physical processes is a broad category describing a wide variety of applications including: * diffusion aggregation in fluid flow. [7] * climate modeling. <ref> [14, 12, 15] </ref> * solar radiation model, model of the gulf stream dynamics. [15] * multilevel models in a single discipline: linear, Euler, and Navier-Stokes models in aerodynamics [13]. In [23], multidisciplinary optimization applications are specifically noted as examples of codes that exhibit multiple levels of parallelism. <p> However, compiler support for locating and exploiting this type of parallelism is more likely (given the current compiler research regarding support for mixed parallelism). In addition, completely general mixed parallelism is probably not necessary. <ref> [12] </ref> notes that the simpler strategy of switched parallelism achieves most of the efficiency of mixed parallelism. In this case, at any given time all the processors will be executing either data parallel code or task parallel code. This use of mixed parallelism also seems worth supporting.
Reference: [13] <author> K. M. Chandy, I. T. Foster, K. Kennedy, C. Koelbel, and C.-W. Tseng. </author> <title> Integrated support for task and data parallelism. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 8(2) </volume> <pages> 80-98, </pages> <year> 1994. </year>
Reference-contexts: In addition, many parallel languages do not directly support the ability to consider data owned by different tasks as part of a single data structure. <ref> [13] </ref> Traditionally, data parallel languages have supported exactly that. The most common definition of data parallelism is that it refers to a programming model where a program consists of a series of operations applied to all, or almost all, the elements of a large data structure. <p> Two such proposals are mentioned in this section. One suggests adding the ability to make Fortran-M (a task parallel language consisting of extensions to Fortran 77) calls from HPF, the other suggests adding the ability to make MPI (Message Passing Interface) calls from HPF <ref> [19, 13, 18] </ref>. Both are still just proposals, although research into the actual implementation is on-going. Some of the implementation problems forseen are discussed in section 3. <p> As the code in Figure 2 shows, it should eventually be possible to run separate processes on submachines consisting of virtual processors |useful if the processes themselves are parallel programs <ref> [13] </ref>. The second permits a data parallel computation to execute a section that is not inherently data parallel in a task parallel way if this is faster than executing the section serially. <p> The proposal to combine task-parallel Fortran-M and data-parallel HPF, and potentially any proposal to combine two existing parallel languages, has problems raised but not resolved in <ref> [13] </ref>. One such problem is the sharing of data between data-parallel programs, each of which may have a different data layout. Therefore data distribution conversion has to be handled. <p> Therefore data distribution conversion has to be handled. OPUS solves this problem with SDAs, we are still waiting to see how the two proposed extensions to HPF implement their solutions. <ref> [13] </ref> discusses these difficulties, and others, more thoroughly. OPUS, which is the only implemented language of this second variety, is also the least general 7 in that support for using mixed parallelism to do anything except handling multidisciplinary optimization applications is limited. So both approaches have their limitations. <p> processes is a broad category describing a wide variety of applications including: * diffusion aggregation in fluid flow. [7] * climate modeling. [14, 12, 15] * solar radiation model, model of the gulf stream dynamics. [15] * multilevel models in a single discipline: linear, Euler, and Navier-Stokes models in aerodynamics <ref> [13] </ref>. In [23], multidisciplinary optimization applications are specifically noted as examples of codes that exhibit multiple levels of parallelism.
Reference: [14] <author> K. M. Chandy, R. Manohar, B. L. Massingill, and D. I. Meiron. </author> <title> Integrating task and data parallelism with the group communication archetype. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <pages> pages 724-733, </pages> <month> Apr </month> <year> 1995. </year>
Reference-contexts: One group at Caltech is focussing on developing archetypes, "abstractions which embody common features shared by parallel applications within a domain" <ref> [14] </ref>. Their goal is the integration of task and data parallelism in a "language-independent and environment-independent manner" [14]. Specifically, [22] describes a mechanism which permits co-ordination of SPMD programs. <p> One group at Caltech is focussing on developing archetypes, "abstractions which embody common features shared by parallel applications within a domain" <ref> [14] </ref>. Their goal is the integration of task and data parallelism in a "language-independent and environment-independent manner" [14]. Specifically, [22] describes a mechanism which permits co-ordination of SPMD programs. Although the functionality of this particular mechanism is limited as the SPMD programs can only communicate at entry and exit points, other abstractions with other mechanisms are possible. <p> efficient across multiple architectures, with different inputs, or with a different target number of solutions to search for [16]. 4.3 Simulation of Physical Processes The simulation of physical processes is a broad category describing a wide variety of applications including: * diffusion aggregation in fluid flow. [7] * climate modeling. <ref> [14, 12, 15] </ref> * solar radiation model, model of the gulf stream dynamics. [15] * multilevel models in a single discipline: linear, Euler, and Navier-Stokes models in aerodynamics [13]. In [23], multidisciplinary optimization applications are specifically noted as examples of codes that exhibit multiple levels of parallelism.
Reference: [15] <author> B. Chapman, P. Mehrotra, J. V. Rosedale, and H. Zima. </author> <title> A software architecture for multidisciplinary applications: Integrating task and data parallelism. </title> <type> Technical Report 94-18, </type> <institution> ICASE, </institution> <year> 1994. </year> <note> url: http://www.icase.edu/docs/library/itrs.html. </note>
Reference-contexts: Data parallel statements are simply HPF programs. SDAs are used "to provide persistent shared 'objects' for communication and synchronization between large grained parallel tasks, at a much higher level than simple communication channels transferring bytes between tasks." <ref> [15] </ref> A set of tasks interacts by creating a SDA object and making the object accessible to all tasks in the set. The SDA, created explicitly by the programmer, acts as a data repository to which only one task at a time has access. <p> An example of an application that performs simultaneous optimization of the aerodynamic and structural design of an aircraft configuration as it would be expressed in OPUS can be found in <ref> [15] </ref>. 6 2.1.5 Other Languages There are other languages with support for mixed parallelism that this paper does not discuss. One such language is Advanced ZPL, a mixed parallel language being developed at the Univeristy of Washington. <p> efficient across multiple architectures, with different inputs, or with a different target number of solutions to search for [16]. 4.3 Simulation of Physical Processes The simulation of physical processes is a broad category describing a wide variety of applications including: * diffusion aggregation in fluid flow. [7] * climate modeling. <ref> [14, 12, 15] </ref> * solar radiation model, model of the gulf stream dynamics. [15] * multilevel models in a single discipline: linear, Euler, and Navier-Stokes models in aerodynamics [13]. In [23], multidisciplinary optimization applications are specifically noted as examples of codes that exhibit multiple levels of parallelism. <p> of solutions to search for [16]. 4.3 Simulation of Physical Processes The simulation of physical processes is a broad category describing a wide variety of applications including: * diffusion aggregation in fluid flow. [7] * climate modeling. [14, 12, 15] * solar radiation model, model of the gulf stream dynamics. <ref> [15] </ref> * multilevel models in a single discipline: linear, Euler, and Navier-Stokes models in aerodynamics [13]. In [23], multidisciplinary optimization applications are specifically noted as examples of codes that exhibit multiple levels of parallelism.
Reference: [16] <author> L. A. Crowl, M. E. Crovella, T. J. LeBlanc, and M. L. Scott. </author> <title> The advantages of multiple parallelizations in combinatorial search. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21 </volume> <pages> 110-123, </pages> <year> 1994. </year>
Reference-contexts: task parallelism and pure data parallelism have their shortcomings, previous examination of combinatorial search algorithms found that both types of parallelism needed to be considered if an application was expected to be efficient across multiple architectures, with different inputs, or with a different target number of solutions to search for <ref> [16] </ref>. 4.3 Simulation of Physical Processes The simulation of physical processes is a broad category describing a wide variety of applications including: * diffusion aggregation in fluid flow. [7] * climate modeling. [14, 12, 15] * solar radiation model, model of the gulf stream dynamics. [15] * multilevel models in a
Reference: [17] <author> M. Dhagat, R. Bagrodia, and M. Chandy. </author> <title> Integrating task and data parallelism in UC. </title> <booktitle> In 1995 International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 29-36, </pages> <month> Aug </month> <year> 1995. </year>
Reference-contexts: Language support for mixed parallelism has been added gradually to the language so that a programmer has been able to specify the synchronization of a set of statements at ever increasing levels of granularity. <ref> [7, 17] </ref>. Original support for mixed parallelism was limited to enclosing function calls that could be executed in parallel in par or arb constructs. A function inside par can be executed on each element of a set specified by an index in parallel. <p> Statements inside an arb stmts construct, different from the arb construct because no set index is specified) can be executed asynchronously. The addition of send and receive (implemented using channels) permits explicit control over communication. Applications that have been coded in UC include: * fft <ref> [17] </ref> * dense gaussian elimination with pivoting [7] * simulation of mosquito control [7] * diffusion aggregation in fluid flow [7] 4 More information on UC can be found on their homepage.
Reference: [18] <author> I. Foster. </author> <title> An HPF binding for MPI. draft document, </title> <address> url = ftp://ftp.mcs.anl.gov/pub/nexus/reports/hpfmpi.ps, </address> <month> Oct </month> <year> 1995. </year>
Reference-contexts: Two such proposals are mentioned in this section. One suggests adding the ability to make Fortran-M (a task parallel language consisting of extensions to Fortran 77) calls from HPF, the other suggests adding the ability to make MPI (Message Passing Interface) calls from HPF <ref> [19, 13, 18] </ref>. Both are still just proposals, although research into the actual implementation is on-going. Some of the implementation problems forseen are discussed in section 3. <p> These tasks can then call MPI functions to exchange data with other tasks. Although still just a proposal, <ref> [18] </ref> gives sample code displaying how a 2-D FFT procedure and a 2-block Poisson solver are expected to look when coded in such a language. 2.1.4 OPUS OPUS 5 is a language developed by the Institute for Computer Applications at the NASA Langley Research Center.
Reference: [19] <author> I. T. Foster, B. Avalani, A. Choudhary, and M. Xu. </author> <title> A compilation system that integrates High Performance Fortran and Fortran M. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Two such proposals are mentioned in this section. One suggests adding the ability to make Fortran-M (a task parallel language consisting of extensions to Fortran 77) calls from HPF, the other suggests adding the ability to make MPI (Message Passing Interface) calls from HPF <ref> [19, 13, 18] </ref>. Both are still just proposals, although research into the actual implementation is on-going. Some of the implementation problems forseen are discussed in section 3.
Reference: [20] <author> J. C. Hardwick. </author> <title> An efficient implementation of nested data parallelism for irregular divide-and-conquer algorithms. </title> <booktitle> In First International Workshop on High-Level Programming Models and Supportive Environments, </booktitle> <month> Apr </month> <year> 1996. </year>
Reference-contexts: More recent work, directed towards creating an efficient implementation for irregular divide and conquer algorithms, uses a method based on asynchronous processor groups to reduce communication and a run-time load-balancing system to cope with dynamic data distributions. In <ref> [20] </ref> this is done by translating the user's algorithm into ANSI C with MPI calls, and linking this code with an MPI (Message Passing Interface) library. Many applications that have been coded in NESL can be found on their web page [1]. <p> Many applications that have been coded in NESL can be found on their web page [1]. A few examples are: * fft * primes 3 More information on NESL can be found on their homepage. See [1]. 3 * sparse matrix vector multiplication [11] * planar convex-hull <ref> [20] </ref> 2.1.2 UC UC 4 is a language being developed jointly at Caltech and UCLA. Inspired by set-based languages such as SETL, UC began as an attempt to add data-parallel constructs to C. <p> Extensions to data-parallel languages were motivated by multidisciplinary optimization algorithms. UC has been extended by increasing language support for mixed parallelism as more algorithms are found which could use it. On-going research in NESL concerns support for irregular divide-and-conquer algorithms <ref> [20] </ref>. In addition, all the languages are trying to gain widespread use. <p> a few general types of applications which are thought of as potentially benefitting from support for mixed parallelism. 4.1 Divide and Conquer Algorithms Divide and conquer algorithms are used in many applications, including those for finding eigenvalues of a symmetric matrix, quicksort, and the Barnes-Hut algorithm for doing N-body simulations <ref> [12, 20] </ref>. Fundamentally, at each level of the algorithm, data-parallel code is executed before the algorithm recurses, calling the same data-parallel code on some number of 7 Are these two facts related? Maybe. 9 disjoint subproblems in parallel.
Reference: [21] <author> C. Lin and L. Snyder. ZPL: </author> <title> an array sublanguage. </title> <type> Technical report, </type> <institution> Univeristy of Washington, </institution> <month> Oct </month> <year> 1993. </year> <note> url: http://www.cs.washington.edu/research/projects/zpl/papers/abstracts/zpl.html. </note>
Reference: [22] <author> B. L. Massingill. </author> <title> Integrating task and data parallelism. </title> <type> Master's thesis, </type> <institution> California Institute of Technology, </institution> <year> 1993. </year> <note> url: http://www.cs.caltech.edu/~berna/tr/cs-tr-93-01.ps. </note>
Reference-contexts: One group at Caltech is focussing on developing archetypes, "abstractions which embody common features shared by parallel applications within a domain" [14]. Their goal is the integration of task and data parallelism in a "language-independent and environment-independent manner" [14]. Specifically, <ref> [22] </ref> describes a mechanism which permits co-ordination of SPMD programs. Although the functionality of this particular mechanism is limited as the SPMD programs can only communicate at entry and exit points, other abstractions with other mechanisms are possible.
Reference: [23] <author> P. Mehrotra and M. Haines. </author> <title> An overview of the Opus language and runtime system. </title> <booktitle> In Proceedings of the 7th LCPC Workshop, </booktitle> <address> Ithaca, NY, </address> <month> Aug </month> <year> 1994. </year> <note> url: http://www.icase.edu/docs/library/itrs.html. </note>
Reference-contexts: OPUS is an extension to HPF which provides the "support necessary for coordinating the parallel execution, communication, and synchronization of [codes used in multidisciplinary optimization applications]" <ref> [23] </ref>. A multidisciplinary optimization 5 More information on OPUS can be found on their homepage. See [2]. 5 application takes codes from different disciplines and integrates them in a complex system where the separate codes can generally execute asynchronously, needing only to communicate to share data. <p> In <ref> [23] </ref>, multidisciplinary optimization applications are specifically noted as examples of codes that exhibit multiple levels of parallelism.
Reference: [24] <author> S. Prakash, M. Dhagat, and R. Bagrodia. </author> <title> Synchronization issues in data-parallel languages. </title> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <month> Aug </month> <year> 1993. </year> <note> url: http://may.cs.ucla.edu/manu/www/papers/lcpc.ps. </note>
Reference: [25] <author> J. Subhlok, J. M. Stichnoth, D. R. O'Hallaron, and T. Gross. </author> <title> Exploiting task and data parallelism on a multicomputer. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 13-22, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <note> ACM. url: http://www-cgi.cs.cmu.edu/afs/cs/project/iwarp/archive/fx-papers/ppopp93.ps. 14 </note>
Reference-contexts: There are many projects related to developing compilers that can exploit mixed parallelism. Two are the Paradigm project being done at the University of Illinois at Urbana-Champaign addresses mixed parallelism [8], and the iWarp compiler developed at CMU <ref> [25] </ref>.
References-found: 25

