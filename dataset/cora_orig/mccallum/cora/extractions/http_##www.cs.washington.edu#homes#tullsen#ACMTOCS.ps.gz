URL: http://www.cs.washington.edu/homes/tullsen/ACMTOCS.ps.gz
Refering-URL: http://www.cs.washington.edu/research/arch/prefetch-bus.html
Root-URL: 
Title: Effective Cache Prefetching on Bus-Based Multiprocessors  
Author: Dean M. Tullsen and Susan J. Eggers 
Affiliation: University of Washington  
Abstract: Compiler-directed cache prefetching has the potential to hide much of the high memory latency seen by current and future high-performance processors. However, prefetching is not without costs, particularly on a multiprocessor. Prefetching can negatively affect bus utilization, overall cache miss rates, memory latencies and data sharing. We simulate the effects of a compiler-directed prefetching algorithm, running on a range of bus-based multiprocessors. We show that, despite a high memory latency, this architecture does not necessarily support prefetching well, in some cases actually causing performance degradations. We pinpoint several problems with prefetching on a shared memory architecture (additional conflict misses, no reduction in the data sharing traffic and associated latencies, a multiprocessor's greater sensitivity to memory utilization and the sensitivity of the cache hit rate to prefetch distance) and measure their effect on performance. We then solve those problems through architectural techniques and heuristics for prefetching that could be easily incorporated into a compiler: 1) victim caching, which eliminates most of the cache conflict misses caused by prefetching in a direct-mapped cache, 2) special prefetch algorithms for shared data, which significantly improve the ability of our basic prefetching algorithm to prefetch invalidation misses, and 3) compiler-based shared data restructuring, which eliminates many of the invalidation misses the basic prefetching algorithm doesn't predict. The combined effect of these improvements is to make prefetching effective over a much wider range of memory architectures. keywords: cache prefetching, bus-based multiprocessor, cache misses, prefetching strategies, parallel programs, false sharing, memory latency
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Although the need to make processors tolerant of high memory latency is much more severe in multiprocessors than in uniprocessors, most other studies of cache prefetching have concentrated on uniprocessor architectures <ref> [1, 6, 5, 23, 3] </ref>. DASH [18] has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests.
Reference: [2] <author> T.-F. Chen. </author> <title> Data prefetching for high-performance processors. </title> <type> Technical Report No. UW TR-93-07-01 (Ph.D. thesis), </type> <institution> University of Washington, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Chen <ref> [2] </ref> showed that a victim cache matched the performance of a 2-way set-associative cache in the context of hardware prefetching.
Reference: [3] <author> T.-F. Chen and J.-L. Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-61, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Although the need to make processors tolerant of high memory latency is much more severe in multiprocessors than in uniprocessors, most other studies of cache prefetching have concentrated on uniprocessor architectures <ref> [1, 6, 5, 23, 3] </ref>. DASH [18] has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests.
Reference: [4] <author> T.-F. Chen and J.-L. Baer. </author> <title> A performance study of software and hardware data prefetching schemes. </title> <booktitle> In 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 223-232, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Mowry, et al. report overheads for their uniprocessor compiler algorithm as typically less than 15% (in increased instruction count; the impact on total execution time is typically about half that), and Mowry and Gupta's programmer-directed scheme experienced overheads between 1% and 8% of total execution time. Chen and Baer's <ref> [4] </ref> implementation of Mowry, et al.'s compiler algorithm on a multiprocessor (including two of our benchmarks, Mp3d and Water) experienced prefetch instruction overhead of 2-4% of total execution time.
Reference: [5] <author> W.Y. Chen, R.A. Bringmann, S.A. Mahlke, R.E. Hank, and J.E. Sicolo. </author> <title> An efficient architecture for loop based data preloading. </title> <booktitle> In 25th International Symposium on Microarchitecture, </booktitle> <pages> pages 92-101, </pages> <month> December </month> <year> 1992. </year> <month> 27 </month>
Reference-contexts: Although the need to make processors tolerant of high memory latency is much more severe in multiprocessors than in uniprocessors, most other studies of cache prefetching have concentrated on uniprocessor architectures <ref> [1, 6, 5, 23, 3] </ref>. DASH [18] has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests.
Reference: [6] <author> W.Y. Chen, S.A. Mahlke, P.P. Chang, and W.W. Hwu. </author> <title> Data access microarchitectures for superscalar processors with compiler-assisted data prefetching. </title> <booktitle> In 24th International Symposium on Microarchitecture, </booktitle> <pages> pages 69-73, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Although the need to make processors tolerant of high memory latency is much more severe in multiprocessors than in uniprocessors, most other studies of cache prefetching have concentrated on uniprocessor architectures <ref> [1, 6, 5, 23, 3] </ref>. DASH [18] has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests.
Reference: [7] <author> S. Devadas and A.R. </author> <title> Newton. Topological optimization of multiple level array logic. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <pages> pages 915-941, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Until specified otherwise, however, we will prefetch in shared mode. 3.2 Workload The address traces were generated with MPTrace [12] on a Sequent Symmetry [19], running the following coarse-grained, explicitly parallel applications, all written in C (see Table 1). Topopt <ref> [7] </ref> performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. Pverify [20] determines whether two boolean circuits are functionally identical. Statistics on the amount of shared data for these programs can be found in [10]. LocusRoute is a commercial quality VLSI standard cell router.
Reference: [8] <author> M. Dubois, J. Skeppstedt, L. Ricciulli, K. Ramamurthy, and P. Stenstrom. </author> <title> The detection and elimination of useless misses in multiprocessors. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 88-97, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We record a false sharing miss if an invalidation miss is caused by a write from another processor to a word in the local cache line that the local processor has not accessed 2 . Table 4 2 Dubois, et al.'s definition of false sharing <ref> [8] </ref>, in that it calculates false sharing over the lifetime of a cache line, is more accurate 19 0.70 0.80 0.90 1.00 0 8 16 24 32 Data Bus Latency 0.70 0.80 0.90 1.00 0 8 16 24 32 Data Bus Latency 0.70 0.80 0.90 1.00 0 8 16 24 32
Reference: [9] <author> S.J. Eggers. </author> <title> Simulation analysis of data sharing in shared memory multiprocessors. </title> <type> Technical Report No. UCB/CSD 89/501 (Ph.D. thesis), </type> <institution> University of California, Berkeley, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: The last two columns show the percent of the data references that are reads, and the percent that are to private data for each benchmark. 3.3 Multiprocessor Simulations After the prefetch accesses were added, the traces were run through Charlie <ref> [9] </ref>, a multiprocessor simulator, that was modified to handle prefetching, lockup-free caches, a split-transaction bus protocol and victim caches.
Reference: [10] <author> S.J. Eggers. </author> <title> Simplicity versus accuracy in a model of cache coherency overhead. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(8) </volume> <pages> 893-906, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Topopt [7] performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. Pverify [20] determines whether two boolean circuits are functionally identical. Statistics on the amount of shared data for these programs can be found in <ref> [10] </ref>. LocusRoute is a commercial quality VLSI standard cell router. Mp3d solves a problem involving particle flow at extremely low density. Water evaluates the forces and potentials in a system of water molecules in liquid state.
Reference: [11] <author> S.J. Eggers and T.E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 377-381, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: We show results for a 32-byte cache line; previous work <ref> [28, 11] </ref> demonstrates that false sharing goes up significantly with larger block sizes. In [14] and [15], an algorithm is presented for restructuring shared data to reduce false sharing.
Reference: [12] <author> S.J. Eggers, D.R. Keppel, E.J. Koldinger, and H.M. Levy. </author> <title> Techniques for inline tracing on a shared-memory multiprocessor. </title> <booktitle> In Proceedings of the 1990 ACM Sigmetrics, </booktitle> <pages> pages 37-47, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The latter is referred to as an exclusive prefetch. Our simulations support both types of prefetches, as in Mowry and Gupta. Until specified otherwise, however, we will prefetch in shared mode. 3.2 Workload The address traces were generated with MPTrace <ref> [12] </ref> on a Sequent Symmetry [19], running the following coarse-grained, explicitly parallel applications, all written in C (see Table 1). Topopt [7] performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. Pverify [20] determines whether two boolean circuits are functionally identical.
Reference: [13] <author> J.L. Hennessy and N.P. Jouppi. </author> <title> Computer technology and architecture: An evolving interaction. </title> <journal> IEEE Computer, </journal> <volume> 24(9) </volume> <pages> 18-29, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Several factors contribute to the increasing need for processors to tolerate high memory latencies, particularly in multiprocessor systems. Certainly the widening gap in speed between CPUs and memory increases memory latencies in uniprocessors and multiprocessors alike <ref> [13] </ref>. Fast processors also increase contention in multiprocessors, lengthening the actual latency seen by CPUs, because of CPU queuing for the interconnect. Second, parallel workloads exhibit more interconnect operations, caused by data sharing among the processors, resulting in more This research was supported by ONR Grant No.
Reference: [14] <author> T.E. Jeremiassen and S.J. Eggers. </author> <title> Computing per-process summary side-effect information. </title> <booktitle> In Fifth International Workshop on Languages and Compilers for Parallel Computing, Lecture Notes on Computer Science 757, </booktitle> <pages> pages 175-191, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: We show results for a 32-byte cache line; previous work [28, 11] demonstrates that false sharing goes up significantly with larger block sizes. In <ref> [14] </ref> and [15], an algorithm is presented for restructuring shared data to reduce false sharing. While the technique has promise for improving overall performance, for the purpose of this study we are only interested in whether doing so makes prefetching more viable.
Reference: [15] <author> T.E. Jeremiassen and S.J. Eggers. </author> <title> Static analysis of barrier synchronization in explicitly parallel programs. </title> <booktitle> In International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: We show results for a 32-byte cache line; previous work [28, 11] demonstrates that false sharing goes up significantly with larger block sizes. In [14] and <ref> [15] </ref>, an algorithm is presented for restructuring shared data to reduce false sharing. While the technique has promise for improving overall performance, for the purpose of this study we are only interested in whether doing so makes prefetching more viable.
Reference: [16] <author> N.P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The 4-cycle latency corresponds to a transfer of 64 bits across the bus every CPU cycle. Also, in section 6, we add victim caches <ref> [16] </ref> to the architecture. A hit in the victim cache takes 4 cycles longer than a hit in the main cache, but is much less than a memory access and saves at least one bus operation. 4 Basic Prefetching We simulate each memory architecture with a basic prefetching algorithm. <p> In order to see the effect of an alternate cache organization on the magnitude of the prefetch-induced conflicts, we here simulate the same configuration with the addition of a small (8 entry) fully-associative victim cache <ref> [16] </ref>. 14 AA AAAA AA AA AA AAAA AA AAAA AA AAAAAA AAA P R F P P E . v i c t P . v i c t top src data AAA AAA AAA AAA AAA AAA AAA AAA AAA AAA AAA AAA P R F P P E
Reference: [17] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: In the best case, the data arrives at the cache before it is needed by the CPU, and the CPU sees its load as a hit. Lockup-free caches <ref> [17, 21, 25, 27] </ref>, which allow the CPU to continue execution during the prefetch, hide the prefetch latency from the CPU. In this paper we address the issue of prefetching in bus-based, shared memory multiprocessors.
Reference: [18] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: Logic overhead and performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> January </month> <year> 1993. </year> <month> 28 </month>
Reference-contexts: Although the need to make processors tolerant of high memory latency is much more severe in multiprocessors than in uniprocessors, most other studies of cache prefetching have concentrated on uniprocessor architectures [1, 6, 5, 23, 3]. DASH <ref> [18] </ref> has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests. A noteworthy exception is the work by Mowry and Gupta [22], in which simulations were driven by three parallel programs, providing analysis of potential speedups with programmer directed cache prefetching.
Reference: [19] <author> R. Lovett and S. Thakkar. </author> <title> The symmetry multiprocessor system. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 303-310, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The latter is referred to as an exclusive prefetch. Our simulations support both types of prefetches, as in Mowry and Gupta. Until specified otherwise, however, we will prefetch in shared mode. 3.2 Workload The address traces were generated with MPTrace [12] on a Sequent Symmetry <ref> [19] </ref>, running the following coarse-grained, explicitly parallel applications, all written in C (see Table 1). Topopt [7] performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. Pverify [20] determines whether two boolean circuits are functionally identical.
Reference: [20] <author> H-K. T. Ma, S. Devadas, R. Wei, and A. Sangiovanni-Vincentelli. </author> <title> Logic verification algorithms and their parallel implementation. </title> <booktitle> In 24th Design Automation Conference, </booktitle> <pages> pages 283-290, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Topopt [7] performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm. Pverify <ref> [20] </ref> determines whether two boolean circuits are functionally identical. Statistics on the amount of shared data for these programs can be found in [10]. LocusRoute is a commercial quality VLSI standard cell router. Mp3d solves a problem involving particle flow at extremely low density.
Reference: [21] <author> Motorola. </author> <title> MC88100 RISC Microprocessor User's Manual. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: In the best case, the data arrives at the cache before it is needed by the CPU, and the CPU sees its load as a hit. Lockup-free caches <ref> [17, 21, 25, 27] </ref>, which allow the CPU to continue execution during the prefetch, hide the prefetch latency from the CPU. In this paper we address the issue of prefetching in bus-based, shared memory multiprocessors.
Reference: [22] <author> T.C. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: DASH [18] has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests. A noteworthy exception is the work by Mowry and Gupta <ref> [22] </ref>, in which simulations were driven by three parallel programs, providing analysis of potential speedups with programmer directed cache prefetching. <p> With our baseline algorithm, we strive to emulate a compiler-based algorithm (of which Mowry et al.[23] is the best example) rather than a programmer-directed approach (such as Mowry and Gupta <ref> [22] </ref>), because we feel that prefetching will predominantly be the domain of compilers rather than programmers. Mowry and Gupta show that prefetching inserted by a programmer intimately familiar with the application can be effective, but such a methodology does not necessarily indicate what compiler-directed prefetching would do. <p> If the compiler can recognize a read-modify-write pattern over a short span of instructions, it can issue an exclusive prefetch for the leading read miss. Mowry and Gupta <ref> [22] </ref> take advantage of this in their programmer-directed prefetching study. Therefore, in the RDEX prefetching strategy, we modify the WREX algorithm to also do exclusive prefetching when a read miss is followed by a write to the same word within 100 instructions.
Reference: [23] <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Although the need to make processors tolerant of high memory latency is much more severe in multiprocessors than in uniprocessors, most other studies of cache prefetching have concentrated on uniprocessor architectures <ref> [1, 6, 5, 23, 3] </ref>. DASH [18] has hardware support for cache prefetching, but to date they have only published the results of micro-benchmark throughput tests. <p> The penalty for being a few cycles early is also small, because the chances of losing the data before its use are slight over that period. Mowry, et al. <ref> [23] </ref> also studied prefetch distance, noting that only one of their programs degraded with 13 0.80 0.90 1.00 0 8 16 24 32 Data Bus Latency 0.80 0.90 1.00 0 8 16 24 32 Data Bus Latency 0.80 0.90 1.00 0 8 16 24 32 Data Bus Latency 0.80 0.90 1.00 <p> These are prefetched in addition to all prefetches identified by PREF. This strategy, labeled PWS, increases the prefetching instruction overhead (but it still is less than 4%), but improves our coverage of invalidation misses. A compiler algorithm could obtain the same effect by using Mowry, et al.'s <ref> [23] </ref> algorithm, but assuming a much smaller cache size when dealing with data known to be write shared. 18 AAA AAAAAA AAA AAA AAA AA AA P R F W AAAAAA AAA AAA AAA AAA AAA N P E P S top src data non-sharing, not prefetched AAA AAA AAA AAA
Reference: [24] <author> M.S. Papamarcos and J.H. Patel. </author> <title> A low-overhead coherence solution for multiprocessors with private cache memories. </title> <booktitle> In 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 348-354, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: For all of the simulations presented here they are 32 KBytes, with a 32 byte block size. 1 Our simulations include both private and shared data, in order to include the effects of interference between the two in the cache. The cache coherency scheme is the Illinois coherency protocol <ref> [24] </ref>, an invalidation-based protocol. Its most important feature for our purposes is that it has a private-clean state for exclusive prefetches. We simulate a 16-deep buffer to hold pending prefetches on each processor, which is sufficiently large to almost always prevent the processor from stalling because the buffer is full.
Reference: [25] <author> C. Scheurich and M. Dubois. </author> <title> Lockup-free caches in high-performance multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11(1) </volume> <pages> 25-36, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In the best case, the data arrives at the cache before it is needed by the CPU, and the CPU sees its load as a hit. Lockup-free caches <ref> [17, 21, 25, 27] </ref>, which allow the CPU to continue execution during the prefetch, hide the prefetch latency from the CPU. In this paper we address the issue of prefetching in bus-based, shared memory multiprocessors.
Reference: [26] <author> J.P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: LocusRoute is a commercial quality VLSI standard cell router. Mp3d solves a problem involving particle flow at extremely low density. Water evaluates the forces and potentials in a system of water molecules in liquid state. The latter three are part of the Stanford SPLASH benchmarks <ref> [26] </ref> which, in contrast to the other two applications, have been optimized by the programmers for processor locality.
Reference: [27] <author> G.S. Sohi and M. Franklin. </author> <title> High-bandwidth data memory systems for superscalar processor. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 53-62, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In the best case, the data arrives at the cache before it is needed by the CPU, and the CPU sees its load as a hit. Lockup-free caches <ref> [17, 21, 25, 27] </ref>, which allow the CPU to continue execution during the prefetch, hide the prefetch latency from the CPU. In this paper we address the issue of prefetching in bus-based, shared memory multiprocessors.
Reference: [28] <author> J. Torrellas, M.S. Lam, and J.L. Hennessy. </author> <title> Shared data placement optimizations to reduce multiprocessor cache miss rates. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 266-270, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: We show results for a 32-byte cache line; previous work <ref> [28, 11] </ref> demonstrates that false sharing goes up significantly with larger block sizes. In [14] and [15], an algorithm is presented for restructuring shared data to reduce false sharing.
Reference: [29] <author> D.M. Tullsen and S.J. Eggers. </author> <title> Limitations of cache prefetching on a bus-based multiprocessor. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 278-288, </pages> <month> May </month> <year> 1993. </year> <month> 29 </month>
Reference-contexts: Section 8 shows the effect of combining these techniques, and the conclusions appear in section 9. 2 2 Related Work This work builds on a previous study <ref> [29] </ref> in which we pinpointed the problems with prefetching on a shared memory machine (additional conflict misses, data sharing traffic, a multiprocessor's greater sensitivity to memory utilization and determining the best prefetch distance) and measured their effect on performance.
References-found: 29

