URL: http://www.cs.princeton.edu/~ristad/papers/pu-495-95.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/papers/pu-495-95.html
Root-URL: http://www.cs.princeton.edu
Title: A Natural Law of Succession  
Author: Eric Sven Ristad 
Keyword: multinomial prediction, stochastic complexity, Laplace's law of succession.  
Abstract: Research Report CS-TR-495-95 May 1995; Revised July 1995 Abstract Consider the problem of multinomial estimation. You are given an alphabet of distinct symbols and are told the frequency with which each symbol occurred in the past. On the basis of this information alone, you must now estimate the symbol probabilities. In this report, we present a new solution to this fundamental problem in statistics and demonstrate that our solution outperforms standard approaches, both in theory and in practice. 1 This paper was first presented on May 15, 1995 at Johns Hopkins University. It 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baclawski, K., Rota, G.-C., and Billey, S. </author> <title> An Introduction to the Theory of Probability, 2nd preliminary ed. </title> <publisher> MIT, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Even the staunchest advocates of Bayesian reasoning feel compelled to give stern warnings against the use of Laplace's law [12, 14, 19, 20]. Apparently the risk of using Laplace's law has become so great that it may no longer be taught to undergraduates, even at MIT <ref> [1, 7] </ref>.
Reference: [2] <author> Bell, T. C., Cleary, J. G., and Witten, I. H. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: At that point, Lidstone's law would predict the lone remaining novel symbol with probability =((1 + )2 32 1), ie., as a practical impossibility. In contrast, 3 The latter probability, p ( Qjfn i g; n), is the "escape probability" of the text compression community <ref> [2] </ref> and the "backoff probability" of the speech recognition community [25]. See our discussion in section 5 below. 7 the uniform cardinality law would predict that novel symbol with probability 2 32 =(2 32 + 2), ie., as a virtual certainty. <p> In fact, the authors of methods A-B go so far as to claim that "there can be no theoretical justification for choosing any particular escape [probability] as the optimal one." <ref> [2, p.145] </ref> Fortunately, our work provides a clear theoretical justification: pick the method that most closely approximates the uniform cardinality law. In most cases, n q, and then method D most closely approximates the uniform cardinality law. <p> In contrast, methods A-D do not achieve the same level of confidence any time during the 5,220 years of recorded history. 15 5.2 Calgary Corpus Our second test is the Calgary data compression corpus, which includes a wide range of ASCII as well as non-ASCII files <ref> [2] </ref>. 7 Our prediction results are summarized in the following table 1. All prediction results are in whole bytes, rounded up. Again, the uniform cardinality law is the winner, with the uniform subsets law in second place. Laplace's law is the worst of the eight estimators.
Reference: [3] <author> Carnap, R. </author> <title> The Continuum of Inductive Methods. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1952. </year>
Reference-contexts: This may immediately be seen by rewriting (2) as (3) with the substitution = n=(n + k) [14]. p (ijfn i g; n) = n i + (1 ) k Johnson's perspective was taken up by R. Carnap in his widely-cited work on inductive reasoning <ref> [3] </ref>. This class of probability estimates is also prominent in the information theory literature [6, 11, 27, 28] and the statistical language modeling community [23].
Reference: [4] <author> Chaitin, G. </author> <title> On the length of programs for computing finite binary sequences. </title> <booktitle> JACM 13 (1966), </booktitle> <pages> 547-569. </pages>
Reference-contexts: Instead of estimating parameter values, let us impose constraints on strings so that simple strings are more probable than complex ones. Our approach is principally inspired by the 4 theory of stochastic complexity [37, 38, 39] and the related theory of algorithmic complexity <ref> [4, 26, 40] </ref>. The most important constraint on a string arises from its definition. A string is, by definition, a sequence of discrete symbols drawn from a finite alphabet with replacement.
Reference: [5] <author> Cleary, J., and Witten, I. </author> <title> Data compression using adaptive coding and partial string matching. </title> <journal> IEEE Trans. Comm. COM-32, </journal> <volume> 4 (1984), </volume> <pages> 396-402. </pages>
Reference-contexts: The four most widely used multinomial estimators in the text compression community, affectionately dubbed "methods A-D," are summarized in the following table Method p (ijfn i g; n) p ( Qjfn i g; n) A <ref> [5] </ref> n i =(n + 1) 1=(n + 1) C [32] n i =(n + q) q=(n + q) 2 )=n q=2n where r : = jfi : n i &gt; 1gj is the number of symbols that have occurred at least twice. <p> Method B provides the worst 5 Method B assigns probability (n i 1)=n to symbols that have occurred at least twice, and then assigns the remaining q=n probability uniformly to all symbols that have occurred less than two times <ref> [5] </ref>. There are exactly k q novel symbols, so the total probability assigned to novel symbols by method B is ((k q)=(k r))(q=n). 12 approximation to the uniform cardinality law because it assigns such low prob-ability to symbols that have occurred exactly once.
Reference: [6] <author> Cover, T. </author> <title> Admissibility properties of Gilbert's encoding for unknown source probabilities. </title> <journal> IEEE Trans. Inform. Theory 18, </journal> <volume> 1 (1972), </volume> <pages> 216-217. </pages>
Reference-contexts: Carnap in his widely-cited work on inductive reasoning [3]. This class of probability estimates is also prominent in the information theory literature <ref> [6, 11, 27, 28] </ref> and the statistical language modeling community [23].
Reference: [7] <author> Drake, A. W. </author> <title> Fundamentals of Applied Probability Theory. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1967. </year>
Reference-contexts: Even the staunchest advocates of Bayesian reasoning feel compelled to give stern warnings against the use of Laplace's law [12, 14, 19, 20]. Apparently the risk of using Laplace's law has become so great that it may no longer be taught to undergraduates, even at MIT <ref> [1, 7] </ref>.
Reference: [8] <author> Feller, W. </author> <title> An Introduction to Probability Theory and Its Applications, </title> <editor> 3rd ed., </editor> <volume> vol. 1. </volume> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: According to Laplace's law of succession, the probability that the next number is i is only 1=(2 33 1). The widely proclaimed absurdity of these results has led many influential statisticians to reject Bayesian inference altogether <ref> [8, 9, 22, 41] </ref>. Even the staunchest advocates of Bayesian reasoning feel compelled to give stern warnings against the use of Laplace's law [12, 14, 19, 20].
Reference: [9] <author> Fisher, R. A. </author> <title> Statistical Methods for Research Workers, </title> <editor> 14th ed. </editor> <publisher> Oliver and Boyd, Edinburgh, </publisher> <year> 1970. </year>
Reference-contexts: According to Laplace's law of succession, the probability that the next number is i is only 1=(2 33 1). The widely proclaimed absurdity of these results has led many influential statisticians to reject Bayesian inference altogether <ref> [8, 9, 22, 41] </ref>. Even the staunchest advocates of Bayesian reasoning feel compelled to give stern warnings against the use of Laplace's law [12, 14, 19, 20].
Reference: [10] <author> Francis, W. N., and Kucera, H. </author> <title> Frequency analysis of English usage: lexicon and grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1982. </year>
Reference-contexts: All prediction scores are relative to the empirical entropy. 5.3 Natural Language Corpora Our third and final empirical test is the King James Bible [15] and Brown Corpus <ref> [10] </ref>, two large natural language corpora that are widely used in the computational linguistics community.
Reference: [11] <author> Gilbert, E. </author> <title> Coding based on inaccurate source probabilities. </title> <journal> IEEE Trans. Inform. Theory 17, </journal> <volume> 3 (1971), </volume> <pages> 304-314. 19 </pages>
Reference-contexts: Carnap in his widely-cited work on inductive reasoning [3]. This class of probability estimates is also prominent in the information theory literature <ref> [6, 11, 27, 28] </ref> and the statistical language modeling community [23].
Reference: [12] <author> Good, I. J. </author> <title> Probability and the Weighting of Evidence. </title> <address> Charles Griffin & Co., London, </address> <year> 1950. </year>
Reference-contexts: The widely proclaimed absurdity of these results has led many influential statisticians to reject Bayesian inference altogether [8, 9, 22, 41]. Even the staunchest advocates of Bayesian reasoning feel compelled to give stern warnings against the use of Laplace's law <ref> [12, 14, 19, 20] </ref>. Apparently the risk of using Laplace's law has become so great that it may no longer be taught to undergraduates, even at MIT [1, 7].
Reference: [13] <author> Good, I. J. </author> <title> The population frequencies of species and the estimation of population parameters. </title> <booktitle> Biometrika 40 (1953), </booktitle> <pages> 237-264. </pages>
Reference-contexts: We conclude by comparing the empirical performance of the natural law to four estimators employed by the text compression community. The natural law gives the best overall predictions, by a 1 wide margin. In an appendix, we compare the natural law to two variants of the classic Good-Turing estimate <ref> [13] </ref>. 2 Parameter Estimation The problem of predicting symbols from their observed frequencies is traditionally viewed as a problem in parameter estimation. Under this view, each symbol i is associated with a hidden parameter p i whose value represents its probability of occurrence.
Reference: [14] <author> Good, I. J. </author> <title> The Estimation of Probabilities. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1965. </year>
Reference-contexts: The widely proclaimed absurdity of these results has led many influential statisticians to reject Bayesian inference altogether [8, 9, 22, 41]. Even the staunchest advocates of Bayesian reasoning feel compelled to give stern warnings against the use of Laplace's law <ref> [12, 14, 19, 20] </ref>. Apparently the risk of using Laplace's law has become so great that it may no longer be taught to undergraduates, even at MIT [1, 7]. <p> These estimates were later explained by W.E. Johnson [24] as a linear interpolation of the maximum likelihood estimate n i =n and the uniform prior 1=k. This may immediately be seen by rewriting (2) as (3) with the substitution = n=(n + k) <ref> [14] </ref>. p (ijfn i g; n) = n i + (1 ) k Johnson's perspective was taken up by R. Carnap in his widely-cited work on inductive reasoning [3]. <p> The special case = 1=2 has even been given its own name, namely, the Jeffreys-Perks law of succession <ref> [14] </ref>. Empirical investigation shows that the optimal value for ranges from 1 for object code files to 1=32 for files containing English prose. The value of the flattening parameter represents the statistician's prior knowledge. It must be chosen before making any observations.
Reference: [15] <author> Gutenberg, P., Ed. </author> <title> Project Gutenberg Edition of the King James Bible, </title> <type> 10 ed. </type> <institution> Illinois Benedictine College, Lisle, IL, </institution> <year> 1992. </year> <note> 2nd version. </note>
Reference-contexts: All prediction scores are relative to the empirical entropy. 5.3 Natural Language Corpora Our third and final empirical test is the King James Bible <ref> [15] </ref> and Brown Corpus [10], two large natural language corpora that are widely used in the computational linguistics community.
Reference: [16] <author> H. Ney, U. Essen, R. K. </author> <title> On the estimation of small probabilities by leaving-one-out. </title> <journal> IEEE Trans. </journal> <volume> PAMI 17, 12 (1995), </volume> <pages> 1202-1212. </pages>
Reference: [17] <author> Hardy, G. </author> <title> Correspondence. Insurance Record (1889). </title> <journal> Reprinted in Trans. Fac. Actuaries, </journal> <volume> 8, </volume> <year> 1920. </year>
Reference-contexts: Hardy <ref> [17] </ref> and G.J. Lidstone [31] at the turn of the century. These estimates were later explained by W.E. Johnson [24] as a linear interpolation of the maximum likelihood estimate n i =n and the uniform prior 1=k.
Reference: [18] <author> Howard, P., and Vitter, J. </author> <title> Practical implementations of arithmetic coding. In Image and Text Compression, </title> <editor> J. Storer, Ed. </editor> <publisher> Kluwer Academic, Norwell, </publisher> <address> MA, </address> <year> 1992, </year> <pages> pp. 85-112. </pages>
Reference: [19] <author> Jaynes, E. T. </author> <booktitle> Probability Theory in Science and Engineering, vol. 4 of Colloquium Lectures in Pure and Applied Science. </booktitle> <publisher> Socony Mobil Oil Company, </publisher> <address> Dallas, TX, </address> <year> 1958. </year>
Reference-contexts: The widely proclaimed absurdity of these results has led many influential statisticians to reject Bayesian inference altogether [8, 9, 22, 41]. Even the staunchest advocates of Bayesian reasoning feel compelled to give stern warnings against the use of Laplace's law <ref> [12, 14, 19, 20] </ref>. Apparently the risk of using Laplace's law has become so great that it may no longer be taught to undergraduates, even at MIT [1, 7].
Reference: [20] <author> Jaynes, E. T. </author> <title> Prior probabilities. </title> <journal> IEEE Trans. Systems Science and Cybernetics SSC-4 (1968), </journal> <pages> 227-241. </pages>
Reference-contexts: The widely proclaimed absurdity of these results has led many influential statisticians to reject Bayesian inference altogether [8, 9, 22, 41]. Even the staunchest advocates of Bayesian reasoning feel compelled to give stern warnings against the use of Laplace's law <ref> [12, 14, 19, 20] </ref>. Apparently the risk of using Laplace's law has become so great that it may no longer be taught to undergraduates, even at MIT [1, 7].
Reference: [21] <author> Jeffreys, H. </author> <title> An invariant form for the prior probability in estimation problems. </title> <journal> Proc. Roy. Soc. (London) A 186 (1946), </journal> <pages> 453-461. </pages>
Reference-contexts: diverse set of reasons 2 Immediately after calculating the probability of the sun rising, Laplace notes that "this number would be incomparably greater for one who, perceiving . . . the principle regulating days and seasons, sees that nothing at the present moment can check the sun's course." [30, p.11] <ref> [21, 34, 29] </ref>. The special case = 1=2 has even been given its own name, namely, the Jeffreys-Perks law of succession [14]. Empirical investigation shows that the optimal value for ranges from 1 for object code files to 1=32 for files containing English prose.
Reference: [22] <author> Jeffreys, H. </author> <title> Theory of Probability, 2nd ed. </title> <publisher> Oxford at the Clarendon Press, </publisher> <address> London, </address> <year> 1948. </year>
Reference-contexts: According to Laplace's law of succession, the probability that the next number is i is only 1=(2 33 1). The widely proclaimed absurdity of these results has led many influential statisticians to reject Bayesian inference altogether <ref> [8, 9, 22, 41] </ref>. Even the staunchest advocates of Bayesian reasoning feel compelled to give stern warnings against the use of Laplace's law [12, 14, 19, 20].
Reference: [23] <author> Jelinek, F., Mercer, R. L., and Roukos, S. </author> <title> Principles of lexical language modeling for speech recognition. </title> <booktitle> In Advances in Speech Signal Processing, </booktitle> <editor> S. Furui and M. M. Sondhi, Eds. </editor> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1992, </year> <journal> ch. </journal> <volume> 21, </volume> <pages> pp. 651-700. </pages>
Reference-contexts: Carnap in his widely-cited work on inductive reasoning [3]. This class of probability estimates is also prominent in the information theory literature [6, 11, 27, 28] and the statistical language modeling community <ref> [23] </ref>.
Reference: [24] <author> Johnson, W. E. </author> <title> Probability: deductive and inductive problems. </title> <booktitle> Mind 41 (1932), </booktitle> <pages> 421-423. </pages> <note> Appendix to. </note>
Reference-contexts: Hardy [17] and G.J. Lidstone [31] at the turn of the century. These estimates were later explained by W.E. Johnson <ref> [24] </ref> as a linear interpolation of the maximum likelihood estimate n i =n and the uniform prior 1=k.
Reference: [25] <author> Katz, S. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Trans. ASSP 35 (1987), </journal> <pages> 400-401. </pages>
Reference-contexts: In contrast, 3 The latter probability, p ( Qjfn i g; n), is the "escape probability" of the text compression community [2] and the "backoff probability" of the speech recognition community <ref> [25] </ref>. See our discussion in section 5 below. 7 the uniform cardinality law would predict that novel symbol with probability 2 32 =(2 32 + 2), ie., as a virtual certainty.
Reference: [26] <author> Kolmogorov, A. N. </author> <title> Three approaches for defining the concept of `information quantity'. </title> <booktitle> Problems of Information Transmission 1 (1965), </booktitle> <pages> 4-7. 20 </pages>
Reference-contexts: Instead of estimating parameter values, let us impose constraints on strings so that simple strings are more probable than complex ones. Our approach is principally inspired by the 4 theory of stochastic complexity [37, 38, 39] and the related theory of algorithmic complexity <ref> [4, 26, 40] </ref>. The most important constraint on a string arises from its definition. A string is, by definition, a sequence of discrete symbols drawn from a finite alphabet with replacement.
Reference: [27] <author> Krichevskii, R. </author> <title> The connection between redundancy and reliability of information about the source. Probl. </title> <journal> Inform. Trans. </journal> <volume> 4, 3 (1968), </volume> <pages> 48-57. </pages>
Reference-contexts: Carnap in his widely-cited work on inductive reasoning [3]. This class of probability estimates is also prominent in the information theory literature <ref> [6, 11, 27, 28] </ref> and the statistical language modeling community [23].
Reference: [28] <author> Krichevskii, R. </author> <title> Optimal source-coding based on observation. Probl. </title> <journal> Inform. Trans. </journal> <volume> 11, 1 (1975), </volume> <pages> 37-42. </pages>
Reference-contexts: Carnap in his widely-cited work on inductive reasoning [3]. This class of probability estimates is also prominent in the information theory literature <ref> [6, 11, 27, 28] </ref> and the statistical language modeling community [23].
Reference: [29] <author> Krichevskii, R. E., and Trofimov, V. K. </author> <title> The performance of universal coding. </title> <journal> IEEE Trans. Inform. Theory IT-27, </journal> <volume> 2 (1981), </volume> <pages> 199-207. </pages>
Reference-contexts: diverse set of reasons 2 Immediately after calculating the probability of the sun rising, Laplace notes that "this number would be incomparably greater for one who, perceiving . . . the principle regulating days and seasons, sees that nothing at the present moment can check the sun's course." [30, p.11] <ref> [21, 34, 29] </ref>. The special case = 1=2 has even been given its own name, namely, the Jeffreys-Perks law of succession [14]. Empirical investigation shows that the optimal value for ranges from 1 for object code files to 1=32 for files containing English prose.
Reference: [30] <author> Laplace, P.-S. </author> <title> Philosophical Essay on Probabilities. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1995. </year> <title> Translated by A.I. Dale from the 5th French edition of 1825. </title>
Reference-contexts: for a diverse set of reasons 2 Immediately after calculating the probability of the sun rising, Laplace notes that "this number would be incomparably greater for one who, perceiving . . . the principle regulating days and seasons, sees that nothing at the present moment can check the sun's course." <ref> [30, p.11] </ref> [21, 34, 29]. The special case = 1=2 has even been given its own name, namely, the Jeffreys-Perks law of succession [14]. Empirical investigation shows that the optimal value for ranges from 1 for object code files to 1=32 for files containing English prose.
Reference: [31] <author> Lidstone, G. </author> <title> Note on the general case of the Bayes-Laplace formula for inductive or a posteriori probabilities. </title> <journal> Trans. Fac. Actuar. </journal> <volume> 8 (1920), </volume> <pages> 182-192. </pages>
Reference-contexts: Hardy [17] and G.J. Lidstone <ref> [31] </ref> at the turn of the century. These estimates were later explained by W.E. Johnson [24] as a linear interpolation of the maximum likelihood estimate n i =n and the uniform prior 1=k.
Reference: [32] <author> Moffat, A. </author> <title> Implementing the PPM data compresion scheme. </title> <journal> IEEE Trans. Communications 38, </journal> <volume> 11 (1990), </volume> <pages> 1917-1921. </pages>
Reference-contexts: The four most widely used multinomial estimators in the text compression community, affectionately dubbed "methods A-D," are summarized in the following table Method p (ijfn i g; n) p ( Qjfn i g; n) A [5] n i =(n + 1) 1=(n + 1) C <ref> [32] </ref> n i =(n + q) q=(n + q) 2 )=n q=2n where r : = jfi : n i &gt; 1gj is the number of symbols that have occurred at least twice.
Reference: [33] <author> Ney, H., and Essen, U. </author> <title> Estimating `small' probabilities by leaving-one-out. </title> <booktitle> In Eurospeech '93 (Berlin, </booktitle> <month> September 21-23 </month> <year> 1993), </year> <title> vol. </title> <booktitle> 3, ESCA, </booktitle> <pages> pp. 2239-2242. </pages>
Reference: [34] <author> Perks, W. </author> <title> Some observations on inverse probability, including a new indifference rule. </title> <journal> J. Inst. Actuar. </journal> <volume> 73 (1947), </volume> <pages> 285-312. </pages>
Reference-contexts: diverse set of reasons 2 Immediately after calculating the probability of the sun rising, Laplace notes that "this number would be incomparably greater for one who, perceiving . . . the principle regulating days and seasons, sees that nothing at the present moment can check the sun's course." [30, p.11] <ref> [21, 34, 29] </ref>. The special case = 1=2 has even been given its own name, namely, the Jeffreys-Perks law of succession [14]. Empirical investigation shows that the optimal value for ranges from 1 for object code files to 1=32 for files containing English prose.
Reference: [35] <author> Rissanen, J. </author> <title> A universal data compression system. </title> <journal> IEEE Trans. Inform. Theory IT-29, </journal> <volume> 5 (1983), </volume> <pages> 656-664. </pages>
Reference-contexts: Consequently, the uniform subsets law also enjoys a significant advantage over Laplace's law and Lidstone's law. Finally, we note that our analysis applies equally well to finite memory models whose state transition probabilities are estimated using either Laplace's law <ref> [35, 36] </ref> or the Jeffreys-Perks law [42, 43]. Unless all the alphabet symbols are observed in all the model states an incredibly unnatural situation the total probability assigned to the possible strings by such models rapidly approaches zero as the strings get longer.
Reference: [36] <author> Rissanen, J. </author> <title> Complexity of strings in the class of Markov sources. </title> <journal> IEEE Trans. Inform. Theory IT-32, </journal> <volume> 4 (1986), </volume> <pages> 526-532. </pages>
Reference-contexts: Consequently, the uniform subsets law also enjoys a significant advantage over Laplace's law and Lidstone's law. Finally, we note that our analysis applies equally well to finite memory models whose state transition probabilities are estimated using either Laplace's law <ref> [35, 36] </ref> or the Jeffreys-Perks law [42, 43]. Unless all the alphabet symbols are observed in all the model states an incredibly unnatural situation the total probability assigned to the possible strings by such models rapidly approaches zero as the strings get longer.
Reference: [37] <author> Rissanen, J. </author> <title> Stochastic complexity and modeling. </title> <journal> Annals of Statistics 14 (1986), </journal> <pages> 1080-1100. </pages>
Reference-contexts: Instead of estimating parameter values, let us impose constraints on strings so that simple strings are more probable than complex ones. Our approach is principally inspired by the 4 theory of stochastic complexity <ref> [37, 38, 39] </ref> and the related theory of algorithmic complexity [4, 26, 40]. The most important constraint on a string arises from its definition. A string is, by definition, a sequence of discrete symbols drawn from a finite alphabet with replacement.
Reference: [38] <author> Rissanen, J. </author> <title> Stochastic complexity. </title> <journal> J. Royal Statistical Society, Series B 49, </journal> <volume> 3 (1987), </volume> <pages> 223-239 and 252-265. </pages>
Reference-contexts: Instead of estimating parameter values, let us impose constraints on strings so that simple strings are more probable than complex ones. Our approach is principally inspired by the 4 theory of stochastic complexity <ref> [37, 38, 39] </ref> and the related theory of algorithmic complexity [4, 26, 40]. The most important constraint on a string arises from its definition. A string is, by definition, a sequence of discrete symbols drawn from a finite alphabet with replacement.
Reference: [39] <author> Rissanen, J. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Teaneck, NJ, </address> <year> 1989. </year>
Reference-contexts: Instead of estimating parameter values, let us impose constraints on strings so that simple strings are more probable than complex ones. Our approach is principally inspired by the 4 theory of stochastic complexity <ref> [37, 38, 39] </ref> and the related theory of algorithmic complexity [4, 26, 40]. The most important constraint on a string arises from its definition. A string is, by definition, a sequence of discrete symbols drawn from a finite alphabet with replacement.
Reference: [40] <author> Solomonoff, R. </author> <title> A formal theory of inductive inference, parts I and II. </title> <booktitle> Information and Control 7 (1964), </booktitle> <pages> 1-22, 224-254. </pages>
Reference-contexts: Instead of estimating parameter values, let us impose constraints on strings so that simple strings are more probable than complex ones. Our approach is principally inspired by the 4 theory of stochastic complexity [37, 38, 39] and the related theory of algorithmic complexity <ref> [4, 26, 40] </ref>. The most important constraint on a string arises from its definition. A string is, by definition, a sequence of discrete symbols drawn from a finite alphabet with replacement.
Reference: [41] <author> Wald, A. </author> <title> Sequential Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1947. </year>
Reference-contexts: According to Laplace's law of succession, the probability that the next number is i is only 1=(2 33 1). The widely proclaimed absurdity of these results has led many influential statisticians to reject Bayesian inference altogether <ref> [8, 9, 22, 41] </ref>. Even the staunchest advocates of Bayesian reasoning feel compelled to give stern warnings against the use of Laplace's law [12, 14, 19, 20].
Reference: [42] <author> Weinberger, M., Rissanen, J., and Feder, M. </author> <title> A universal finite memory source. </title> <journal> IEEE Trans. Inform. Theory 41, </journal> <volume> 3 (1995), </volume> <pages> 643-652. 21 </pages>
Reference-contexts: Consequently, the uniform subsets law also enjoys a significant advantage over Laplace's law and Lidstone's law. Finally, we note that our analysis applies equally well to finite memory models whose state transition probabilities are estimated using either Laplace's law [35, 36] or the Jeffreys-Perks law <ref> [42, 43] </ref>. Unless all the alphabet symbols are observed in all the model states an incredibly unnatural situation the total probability assigned to the possible strings by such models rapidly approaches zero as the strings get longer.
Reference: [43] <author> Willems, F. M. J., Shtarkov, Y. M., and Tjalkens, T. J. </author> <title> The context-tree weighting method: basic properties. </title> <journal> IEEE Trans. Inform. Theory 41, </journal> <volume> 3 (1995), </volume> <pages> 653-664. </pages>
Reference-contexts: Consequently, the uniform subsets law also enjoys a significant advantage over Laplace's law and Lidstone's law. Finally, we note that our analysis applies equally well to finite memory models whose state transition probabilities are estimated using either Laplace's law [35, 36] or the Jeffreys-Perks law <ref> [42, 43] </ref>. Unless all the alphabet symbols are observed in all the model states an incredibly unnatural situation the total probability assigned to the possible strings by such models rapidly approaches zero as the strings get longer.
References-found: 43

