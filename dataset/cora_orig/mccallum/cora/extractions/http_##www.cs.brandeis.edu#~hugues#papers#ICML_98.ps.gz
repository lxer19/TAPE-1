URL: http://www.cs.brandeis.edu/~hugues/papers/ICML_98.ps.gz
Refering-URL: http://www.cs.brandeis.edu/~hugues/publications.html
Root-URL: http://www.cs.brandeis.edu
Email: hugues@cs.brandeis.edu  pollack@cs.brandeis.edu  
Title: Coevolutionary Learning: a Case Study  
Author: Hugues Juille Jordan B. Pollack 
Address: Waltham, Massachusetts 02254-9110  Waltham, Massachusetts 02254-9110  
Affiliation: Computer Science Department Brandeis University  Computer Science Department Brandeis University  
Abstract: Coevolutionary learning, which involves the embedding of adaptive learning agents in a fitness environment that dynamically responds to their progress, is a potential solution for many technological chicken and egg problems. However, several impediments have to be overcome in order for coevolutionary learning to achieve continuous progress in the long term. This paper presents some of those problems and proposes a framework to address them. This presentation is illustrated with a case study: the evolution of CA rules. Our application of coevolutionary learning resulted in a very significant improvement for that problem compared to the best known results. 
Abstract-found: 1
Intro-found: 1
Reference: [Andre et al., 1996] <author> Andre, D., Bennett III, F. H., & Koza, J. R. </author> <year> (1996). </year> <title> Evolution of intricate long-distance communication signals in cellular automata using genetic programming. </title> <booktitle> In Proceedings of the Fifth Artificial Life Conference, </booktitle> <pages> pp. 16-18. </pages>
Reference-contexts: The main purpose of this work was to develop a particle-based methodology for the analysis of the complex behaviors exhibited by CAs. The GKL and Das rules are human-written while the Andre-Bennett-Koza (ABK) rule has been discovered using the Genetic Programming paradigm <ref> [Andre et al., 1996] </ref>.
Reference: [Capcarrere et al., 1996] <author> Capcarrere, M. S., Sipper, M., & Tomassini, M. </author> <year> (1996). </year> <title> Two-state, r=1 cellular automaton that classifies density. </title> <journal> Physical Review Letters, </journal> <volume> 77(24) </volume> <pages> 4969-4971. </pages>
Reference-contexts: More recently, [Paredis, 1997] describes a coevolutionary approach to search the space of rules and shows the difficulty of coevolving consistently two populations towards continuous improvement. <ref> [Capcarrere et al., 1996] </ref> also reports that by changing the specification of the convergence pattern, a two-state, r = 1 CA exists that can perfectly solve the density problem in dN=2e time steps.
Reference: [Cliff & Miller, 1995] <author> Cliff, D. & Miller, G. F. </author> <year> (1995). </year> <title> Tracking the red queen: Measurements of adaptive progress in co-evolutionary simulations. </title> <booktitle> In Third European Conference on Artificial Life, </booktitle> <volume> LNCS 929, </volume> <pages> pp. 200-218. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Figure 3 describes a run using this definition of the fitness. Two kind of behaviors can be observed in this experiment. In a first stage, the two populations exhibit a cyclic behavior. It is a consequence of the Red Queen effect <ref> [Cliff & Miller, 1995] </ref>: fitness landscapes are changing as a result of agents of each population adapting in response to the evolution of members of the other population. The evaluation of individuals' performance in this changing environment makes continuous progress difficult.
Reference: [Das et al., 1994] <author> Das, R., Mitchell, M., & Crutch-field, J. P. </author> <year> (1994). </year> <title> A genetic algorithm discovers particle-based computation in cellular automata. In Parallel Problem Solving from Nature III, </title> <publisher> LNCS 866, </publisher> <pages> pp. 344-353. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The Gacs-Kurdyumov-Levin (GKL) rule was designed in 1978 for a different goal than solving the c = 1=2 task [Mitchell et al., 1994]. However, for a while it provided the best known performance. [Mitchell et al., 1994] and <ref> [Das et al., 1994] </ref> used Genetic Algorithms (GAs) to explore the space of rules. The main purpose of this work was to develop a particle-based methodology for the analysis of the complex behaviors exhibited by CAs.
Reference: [Epstein, 1994] <author> Epstein, S. L. </author> <year> (1994). </year> <title> Toward an ideal trainer. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 251-277. </pages>
Reference-contexts: The idea of introducing a pressure towards adaptability as the central heuristic for search is not new. Schmidhuber [Schmidhuber, 1995] proposed the Incremental Self-Improvement system in which adaptability is the measure that is optimized. The concept of an ideal trainer is also discussed in <ref> [Epstein, 1994] </ref> in the context of game learning.
Reference: [Hillis, 1992] <author> Hillis, W. D. </author> <year> (1992). </year> <title> Co-evolving parasites improve simulated evolution as an optimization procedure. </title> <editor> In Langton, C. et al. (Eds.), </editor> <booktitle> Artificial Life II, </booktitle> <pages> pp. 313-324. </pages> <publisher> Addison Wesley. </publisher>
Reference-contexts: An intuitive argument to support this hypothesis is presented in [Mitchell et al., 1993]. It is also believed that the most difficult ICs are those with density close to 0:5. 3 Models for Coevolutionary Search The idea of using coevolution in search was introduced by <ref> [Hillis, 1992] </ref>. In coevolution, individuals are evaluated with respect to other individuals instead of a fixed environment (or landscape). As a result, agents adapt in response to other agents' behavior.
Reference: [Juille & Pollack, 1996] <author> Juille, H. & Pollack, J. B. </author> <year> (1996). </year> <title> Co-evolving intertwined spirals. </title> <booktitle> In Proceedings of the Fifth Annual Conference on Evolutionary Programming, </booktitle> <pages> pp. 461-468. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Usually they maintain diversity in the population in order to avoid premature convergence. [Mahfoud, 1995] presents different niching techniques that achieve this goal. Resource sharing, first introduced in [Rosin & Belew, 1995], is a technique that we successfully used in the past <ref> [Juille & Pollack, 1996] </ref>. Resource sharing implements a coverage-based heuristic by giving a higher payoff to problems that few individuals can solve.
Reference: [Land & Belew, 1995] <author> Land, M. & Belew, R. K. </author> <year> (1995). </year> <title> No perfect two-state cellular automata for density classification exists. </title> <journal> Physical Review Letters, </journal> <volume> 74(25) </volume> <pages> 1548-1550. </pages>
Reference-contexts: The task c = 1=2 is known to be difficult. In particular, it has been proven that no rule exists that results in the CA relaxing to the correct state for all possible ICs <ref> [Land & Belew, 1995] </ref>. Indeed, the density is a global property of the initial configuration while individual cells of the CA have access to local information only.
Reference: [Mahfoud, 1995] <author> Mahfoud, S. W. </author> <year> (1995). </year> <title> Niching Methods for Genetic Algorithms. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign. </institution> <note> IlliGAL Report No. 95001. </note>
Reference-contexts: Then, low density ICs exploit those rules and overcome the entire population. A similar experiment is described in [Paredis, 1997]. 3.3 Resource Sharing and Mediocre Stable States Several techniques have been designed to improve evolutionary search. Usually they maintain diversity in the population in order to avoid premature convergence. <ref> [Mahfoud, 1995] </ref> presents different niching techniques that achieve this goal. Resource sharing, first introduced in [Rosin & Belew, 1995], is a technique that we successfully used in the past [Juille & Pollack, 1996].
Reference: [Mitchell et al., 1994] <author> Mitchell, M., Crutchfield, J. P., & Hraber, P. T. </author> <year> (1994). </year> <title> Evolving cellular automata to perform computations: Mechanisms and impediments. </title> <journal> Physica D, </journal> <volume> 75 </volume> <pages> 361-391. </pages>
Reference-contexts: Following <ref> [Mitchell et al., 1994] </ref>, c denotes the threshold for the classification task (here, c = 1=2), denotes the density of 1's in a configuration and o denotes the density of 1's in the initial configuration. Figure 1 presents three examples of the space-time evolution of a CA. <p> The Gacs-Kurdyumov-Levin (GKL) rule was designed in 1978 for a different goal than solving the c = 1=2 task <ref> [Mitchell et al., 1994] </ref>. However, for a while it provided the best known performance. [Mitchell et al., 1994] and [Das et al., 1994] used Genetic Algorithms (GAs) to explore the space of rules. <p> The Gacs-Kurdyumov-Levin (GKL) rule was designed in 1978 for a different goal than solving the c = 1=2 task <ref> [Mitchell et al., 1994] </ref>. However, for a while it provided the best known performance. [Mitchell et al., 1994] and [Das et al., 1994] used Genetic Algorithms (GAs) to explore the space of rules. The main purpose of this work was to develop a particle-based methodology for the analysis of the complex behaviors exhibited by CAs. <p> In the following sections, those modes of interaction are described experimentally using the c = 1=2 task in order to stress the different issues related to coevolutionary learning. For the experiments presented in this section, we used an implementation of Genetic Algorithms similar to the one described in <ref> [Mitchell et al., 1994] </ref>. Each rule is coded on a binary string of length 2 2flr+1 = 128. One-point crossover is used with a 2% bit mutation probability. The population size is n R = 200 for rules and n IC = 200 for ICs.
Reference: [Mitchell et al., 1993] <author> Mitchell, M., Hraber, P. T., & Crutchfield, J. P. </author> <year> (1993). </year> <title> Revisiting the edge of chaos: Evolving cellular automata to perform computations. </title> <journal> Complex Systems, </journal> <volume> 7 </volume> <pages> 89-130. </pages>
Reference-contexts: For the c = 1=2 task, it is believed that the best rules are in the domain of the rule space with density close to 0:5. An intuitive argument to support this hypothesis is presented in <ref> [Mitchell et al., 1993] </ref>. It is also believed that the most difficult ICs are those with density close to 0:5. 3 Models for Coevolutionary Search The idea of using coevolution in search was introduced by [Hillis, 1992].
Reference: [Paredis, 1996] <author> Paredis, J. </author> <year> (1996). </year> <title> Coevolutionary computation. </title> <booktitle> Artificial Life, </booktitle> <pages> 2(4). </pages>
Reference-contexts: For instance, a noisy evaluation of the fitness can force exploration in a cooperative model, and an evaluation of individuals with respect to a set of opponents extracted from previous generations can limit the cyclic behavior observed in competitive models (e.g., see the life-time fitness evaluation technique described in <ref> [Paredis, 1996] </ref> or the "hall of fame" method presented in [Rosin, 1997]). <p> The new contribution of this work is the idea of maintaining a gradient for search as one of the underlying heuristics. In the literature, different approaches have been proposed to address the issues associated with the Red Queen effect <ref> [Paredis, 1996, Rosin, 1997] </ref>. However, to our knowledge, explicit methods to force progress and to prevent mediocre stable states in the context of evolutionary search have never been tried. The idea of introducing a pressure towards adaptability as the central heuristic for search is not new.
Reference: [Paredis, 1997] <author> Paredis, J. </author> <year> (1997). </year> <title> Coevolving cellular automata: Be aware of the red queen! In Back, </title> <editor> T. (Ed.), </editor> <booktitle> Proceedings of the Seventh International Conference on Genetic Algorithms, </booktitle> <pages> pp. 393-400. </pages> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: The main purpose of this work was to develop a particle-based methodology for the analysis of the complex behaviors exhibited by CAs. The GKL and Das rules are human-written while the Andre-Bennett-Koza (ABK) rule has been discovered using the Genetic Programming paradigm [Andre et al., 1996]. More recently, <ref> [Paredis, 1997] </ref> describes a coevolutionary approach to search the space of rules and shows the difficulty of coevolving consistently two populations towards continuous improvement. [Capcarrere et al., 1996] also reports that by changing the specification of the convergence pattern, a two-state, r = 1 CA exists that can perfectly solve the <p> Then, low density ICs exploit those rules and overcome the entire population. A similar experiment is described in <ref> [Paredis, 1997] </ref>. 3.3 Resource Sharing and Mediocre Stable States Several techniques have been designed to improve evolutionary search. Usually they maintain diversity in the population in order to avoid premature convergence. [Mahfoud, 1995] presents different niching techniques that achieve this goal.
Reference: [Pollack et al., 1996] <author> Pollack, J. B., Blair, A., & Land, M. </author> <year> (1996). </year> <title> Coevolution of a backgammon player. </title> <editor> In Langton, C. (Ed.), </editor> <booktitle> Proceedings of Artificial Life V. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: However, this is a mediocre stable state in the sense that evolved rules have poor performance with respect to the c = 1=2 task and there is no pressure towards improvement. The concept of mediocre stable states is also discussed in <ref> [Pollack et al., 1996] </ref>. 3.4 Discussion We have described different models for the coevolution of two populations. Some of the fundamental impediments to coevolutionary learning have been identified along with some of the reasons why continuous progress is difficult to achieve.
Reference: [Rosin, 1997] <author> Rosin, C. D. </author> <year> (1997). </year> <title> Coevolutionary Search Among Adversaries. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego. </address>
Reference-contexts: force exploration in a cooperative model, and an evaluation of individuals with respect to a set of opponents extracted from previous generations can limit the cyclic behavior observed in competitive models (e.g., see the life-time fitness evaluation technique described in [Paredis, 1996] or the "hall of fame" method presented in <ref> [Rosin, 1997] </ref>). However, those mechanisms usually fail to address entirely the fundamental issues discussed previously. 4 Coevolving the "Ideal" Trainer 4.1 Presentation of our Approach From the analysis of the experiments presented in section 3 at least two reasons seem to prevent continuous progress in coevolutionary search. <p> In the future, our goal is to eliminate some of those ex plicit components by introducing some heuristics that automatically identify problems that are appropriate for the current set of learners. The work of Rosin <ref> [Rosin, 1997] </ref> already describes some methods to address this issue. 4.2 Discussion As stated previously, the coevolutionary learning framework introduces a pressure towards adaptability. <p> The new contribution of this work is the idea of maintaining a gradient for search as one of the underlying heuristics. In the literature, different approaches have been proposed to address the issues associated with the Red Queen effect <ref> [Paredis, 1996, Rosin, 1997] </ref>. However, to our knowledge, explicit methods to force progress and to prevent mediocre stable states in the context of evolutionary search have never been tried. The idea of introducing a pressure towards adaptability as the central heuristic for search is not new.
Reference: [Rosin & Belew, 1995] <author> Rosin, C. D. & Belew, R. K. </author> <year> (1995). </year> <title> Methods for competitive co-evolution: Finding opponents worth beating. </title> <editor> In Eshelman, L. J. (Ed.), </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <address> San Mateo, Califor-nia. </address> <publisher> Morgan Kauffmann. </publisher>
Reference-contexts: Usually they maintain diversity in the population in order to avoid premature convergence. [Mahfoud, 1995] presents different niching techniques that achieve this goal. Resource sharing, first introduced in <ref> [Rosin & Belew, 1995] </ref>, is a technique that we successfully used in the past [Juille & Pollack, 1996]. Resource sharing implements a coverage-based heuristic by giving a higher payoff to problems that few individuals can solve.
Reference: [Schmidhuber, 1995] <author> Schmidhuber, J. </author> <year> (1995). </year> <title> Discovering solutions with low kolmogorov complexity and high generalization capability. </title> <editor> In Prieditis, A. & Russell, S. (Eds.), </editor> <booktitle> Machine Learning: Proceedings of the twelfth International Conference, </booktitle> <pages> pp. 188-196. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, to our knowledge, explicit methods to force progress and to prevent mediocre stable states in the context of evolutionary search have never been tried. The idea of introducing a pressure towards adaptability as the central heuristic for search is not new. Schmidhuber <ref> [Schmidhuber, 1995] </ref> proposed the Incremental Self-Improvement system in which adaptability is the measure that is optimized. The concept of an ideal trainer is also discussed in [Epstein, 1994] in the context of game learning.
References-found: 17

