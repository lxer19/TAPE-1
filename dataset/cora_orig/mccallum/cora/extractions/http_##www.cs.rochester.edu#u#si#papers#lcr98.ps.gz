URL: http://www.cs.rochester.edu/u/si/papers/lcr98.ps.gz
Refering-URL: http://www.cs.rochester.edu/stats/oldmonths/1998.06/docs-name.html
Root-URL: 
Email: fsi,sandhyag@cs.rochester.edu  
Title: Compiler and Run-Time Support for Adaptive Load Balancing in Software Distributed Shared Memory Systems  
Author: Sotiris Ioannidis and Sandhya Dwarkadas 
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: Networks of workstations offer inexpensive and highly available high performance computing environments. A critical issue for achieving good performance in any parallel system is load balancing, even more so in workstation environments where the machines might be shared among many users. In this paper, we present and evaluate a system that combines compiler and run-time support to achieve load balancing dynamically on software distributed shared memory programs. We use information provided by the compiler to help the run-time system distribute the work of the parallel loops, not only according to the relative power of the processors, but also in such a way as to minimize communication and page sharing.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. P. Amarasinghe, J. M. Anderson, M. S. Lam, and C. W. Tseng. </author> <title> The SUIF compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the 7th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: Our target run-time system is TreadMarks [2], along with the extensions for prefetching and consistency/communication avoidance described in [6]. We implemented the necessary compiler extensions in the SUIF <ref> [1] </ref> compiler framework. Our experimental environment consists of eight DEC AlphaServer 2100 4/233 computers, each with four 21064A processors operating at 233 MHz. Preliminary results show that our system is able to adapt to changes in load, with performance within 20% of ideal.
Reference: 2. <author> C. Amza, A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. TreadMarks: </author> <title> Shared memory computing on networks of workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: The result is a system that adapts both to changes in access patterns as well as to changes in computational power, resulting in reduced execution time. Our target run-time system is TreadMarks <ref> [2] </ref>, along with the extensions for prefetching and consistency/communication avoidance described in [6]. We implemented the necessary compiler extensions in the SUIF [1] compiler framework. Our experimental environment consists of eight DEC AlphaServer 2100 4/233 computers, each with four 21064A processors operating at 233 MHz. <p> Section 3 presents some preliminary results. Section 4 describes related work. Finally, we present our conclusions and discuss on-going work in Section 5. 2 Design and Implementation We first provide some background on TreadMarks <ref> [2] </ref>, the run-time system we used in our implementation. We then describe the compiler support followed by the run-time support necessary for load balancing. 2.1 The Base Software DSM Library TreadMarks [2] is an SDSM system built at Rice University. <p> conclusions and discuss on-going work in Section 5. 2 Design and Implementation We first provide some background on TreadMarks <ref> [2] </ref>, the run-time system we used in our implementation. We then describe the compiler support followed by the run-time support necessary for load balancing. 2.1 The Base Software DSM Library TreadMarks [2] is an SDSM system built at Rice University. It is an efficient user-level SDSM system that runs on commonly available Unix systems. Tread-Marks provides parallel programming primitives similar to those used in hardware shared memory machines, namely, process creation, shared memory allocation, and lock and barrier synchronization.
Reference: 3. <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related information in distributed shared memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-243, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The system supports a release consistent (RC) memory model [10], requiring the programmer to use explicit synchronization to ensure that changes to shared data become visible. TreadMarks uses a lazy invalidate [14] version of RC and a multiple-writer protocol <ref> [3] </ref> to reduce the overhead involved in implementing the shared memory abstraction. The virtual memory hardware is used to detect accesses to shared memory. Consequently, the consistency unit is a virtual memory page. The multiple-writer protocol reduces the effects of false sharing with such a large consistency unit.
Reference: 4. <author> Michal Cierniak, Wei Li, and Mohammed Javeed Zaki. </author> <title> Loop scheduling for heterogeneity. </title> <booktitle> In Fourth International Symposium on High Performance Distributed Computing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: They present four possible policies for changing k, an exponential adaptive mechanism, a linear adaptive mechanism, a conservative adaptive mechanism, and a greedy adaptive mechanism. In <ref> [4] </ref>, Cierniak et al. study loop scheduling in heterogeneous environments with respect to programs, processors and the interconnection networks.
Reference: 5. <author> A.L. Cox, S. Dwarkadas, H. Lu, and W. Zwaenepoel. </author> <title> Evaluating the performance of software distributed shared memory as a target for parallelizing compilers. </title> <booktitle> In Proceedings of the 11th International Parallel Processing Symposium, </booktitle> <pages> pages 474-482, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Clusters of workstations, whether uniprocessors or symmetric multiprocessors (SMPs), offer cost-effective and highly available parallel computing environments. Software distributed shared memory (SDSM) provides a shared memory abstraction on a distributed memory machine, with the advantage of ease-of-use. Previous work <ref> [5] </ref> has shown that an SDSM run-time can prove to be an effective target for a parallelizing compiler. The advantages of using an SDSM system include reduced complexity at compile-time, and the ability to combine compile-time and run-time information to achieve better performance ([6, 18]).
Reference: 6. <author> S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the 7th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The result is a system that adapts both to changes in access patterns as well as to changes in computational power, resulting in reduced execution time. Our target run-time system is TreadMarks [2], along with the extensions for prefetching and consistency/communication avoidance described in <ref> [6] </ref>. We implemented the necessary compiler extensions in the SUIF [1] compiler framework. Our experimental environment consists of eight DEC AlphaServer 2100 4/233 computers, each with four 21064A processors operating at 233 MHz. <p> Depending on the kind of data sharing between parallel tasks, we follow different strategies of load redistribution in case of imbalance. We will discuss these strategies further in Section 2.3. Prefetching The access pattern information can also used to prefetch data <ref> [6] </ref>. The Tread-Marks library offers prefetching calls. These calls, given a range of addresses, prefetch the data contained in the pages in that range, and provide appropriate (read/write) permissions on the page.
Reference: 7. <author> D. L. Eage and J. Zahorjan. </author> <title> Adaptive guided self-scheduling. </title> <type> Technical Report 92-01-01, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: There are several variations, including self-scheduling [23], fixed-size chunking [15], guided self-scheduling [22], and adaptive guided self-scheduling <ref> [7] </ref>. Markatos and Le Blanc in [20], argue that locality management is more important than load balancing in thread assignment. They introduce a policy they call Memory-Conscious Scheduling that assigns threads to processors whose local memory holds most of the data the thread will access.
Reference: 8. <author> Guy Edjlali, Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Data parallel programming in an adaptive environment. </title> <booktitle> In Internation Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: To compensate for load imbalance, they introduce periodic re-mapping, or re-mapping at predetermined points of the execution, and dynamic re-mapping, in which they determine if repartitioning is required at every time step. In the context of dynamically changing environments, Edjlali et al. in <ref> [8] </ref> or Kaddoura in [13] present a run-time approach for handling such environments. Before each parallel section of the program they check if there is a need to re Fig. 8.
Reference: 9. <author> V. W. Freeh, D. K. Lowenthal, and G. R. Andrews. </author> <title> Distributed filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 201-213, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The authors argue that depending on the application and system parameters each of those schemes can be more suitable than the others. The system that seem most related to ours is Adapt, presented in [17]. Adapt is implemented in concert with the Distributed Filaments software kernel <ref> [9] </ref>, a DSM system. It monitors communication and page faults, and dynamically modifies loop boundaries so that the processes access data that are local if possible. Adapt is able to extract the access patterns by inspecting the patterns of the page faults.
Reference: 10. <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: It is an efficient user-level SDSM system that runs on commonly available Unix systems. Tread-Marks provides parallel programming primitives similar to those used in hardware shared memory machines, namely, process creation, shared memory allocation, and lock and barrier synchronization. The system supports a release consistent (RC) memory model <ref> [10] </ref>, requiring the programmer to use explicit synchronization to ensure that changes to shared data become visible. TreadMarks uses a lazy invalidate [14] version of RC and a multiple-writer protocol [3] to reduce the overhead involved in implementing the shared memory abstraction. <p> A diff is a run-length encoding of the modifications made to a page, generated by comparing the page to a copy saved prior to the modifications (called a twin). With the lazy invalidate protocol, a process invalidates, at the time of an acquire synchronization operation <ref> [10] </ref>, those pages for which it has received notice of modifications by other processors.
Reference: 11. <author> The SUIF Group. </author> <title> An overview of the suif compiler system. </title>
Reference-contexts: On a subsequent page fault, the process fetches the diffs necessary to update its copy. 2.2 Compile-Time Support for Load Balancing For the source-to-source translation from a sequential program to a parallel program using TreadMarks, we use the Stanford University Intermediate Format (SUIF) <ref> [11] </ref> compiler. The SUIF system is organized as a set of compiler passes built on top of a kernel that defines the intermediate format. The passes are implemented as separate programs that typically perform a single analysis or transformation and then write the results out to a file.
Reference: 12. <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Access pattern extraction In order to generate access pattern summaries, our SUIF pass walks through the program looking for accesses to shared memory (identified using the sh prefix). A regular section <ref> [12] </ref> is then created for each such shared access. Regular section descriptors (RSDs) concisely represent the array accesses in a loop nest. The RSDs represent the accessed data as linear expressions of the upper and lower loop bounds along each dimension, and include stride information.
Reference: 13. <author> Maher Kaddoura. </author> <title> Load balancing for regular data-parallel applications on workstation network. </title> <booktitle> In Communication and Architecture Support for Network-Based Parallel Computing, </booktitle> <pages> pages 173-183, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: To compensate for load imbalance, they introduce periodic re-mapping, or re-mapping at predetermined points of the execution, and dynamic re-mapping, in which they determine if repartitioning is required at every time step. In the context of dynamically changing environments, Edjlali et al. in [8] or Kaddoura in <ref> [13] </ref> present a run-time approach for handling such environments. Before each parallel section of the program they check if there is a need to re Fig. 8.
Reference: 14. <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The system supports a release consistent (RC) memory model [10], requiring the programmer to use explicit synchronization to ensure that changes to shared data become visible. TreadMarks uses a lazy invalidate <ref> [14] </ref> version of RC and a multiple-writer protocol [3] to reduce the overhead involved in implementing the shared memory abstraction. The virtual memory hardware is used to detect accesses to shared memory. Consequently, the consistency unit is a virtual memory page.
Reference: 15. <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> In Transactions on Computer Systems, </journal> <month> October </month> <year> 1985. </year>
Reference-contexts: Comparison of the running times of the applications using our load balanc-ing algorithm on 8 loaded processors, compared to there performance on 7 load free processors. finished its assigned portion, more work is obtained from this queue. There are several variations, including self-scheduling [23], fixed-size chunking <ref> [15] </ref>, guided self-scheduling [22], and adaptive guided self-scheduling [7]. Markatos and Le Blanc in [20], argue that locality management is more important than load balancing in thread assignment.
Reference: 16. <author> M. Litzkow and M. Solomon. </author> <title> Supporting checkpointing and process migration outside the unix kernel. </title> <booktitle> In Usenix Winter Conference, </booktitle> <year> 1992. </year>
Reference-contexts: It can only recognize two patterns: nearest-neighbor and broadcast, this limits its flexibility. In our system we use the compiler to extract the access patterns and provides them to the run-time system, making our approach more general and flexible. Finally there are systems like Condor <ref> [16] </ref>, that support transparent migration of processes from one workstation to another. However, such systems don't support parallel programs efficiently. Fig. 9. Running times of the three different implementations of Shallow, in seconds. The manual parallelization takes into account data placement in order to avoid page sharing.
Reference: 17. <author> David K. Lowenthal and Gregory R. Andrews. </author> <title> An adaptive approach to data placement. </title>
Reference-contexts: The authors argue that depending on the application and system parameters each of those schemes can be more suitable than the others. The system that seem most related to ours is Adapt, presented in <ref> [17] </ref>. Adapt is implemented in concert with the Distributed Filaments software kernel [9], a DSM system. It monitors communication and page faults, and dynamically modifies loop boundaries so that the processes access data that are local if possible.
Reference: 18. <author> H. Lu, A.L. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> Software distributed shared memory support for irregular applications. </title> <booktitle> In Proceedings of the 6th Symposium on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 48-56, </pages> <month> June </month> <year> 1996. </year>
Reference: 19. <author> Evangelos P. Markatos and Thomas J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory mu ltiprocessors. </title> <journal> IEEETPDS, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Their results show that the looser the interconnection network the more important the locality management. Based on the observation that the locality of the data that a loop accesses is very important, affinity scheduling was introduced in <ref> [19] </ref>. The loop iterations are divided over all the processors equally in local queues. When a processor is idle, it removes 1/k of the iterations in its local work queue and executes them. K is a parameter of their algorithm which they define as P in most of their experiments. <p> If a processor's work queue is empty, it finds the most loaded processor and it removes 1/P of the iterations in that processor's works queue and executes them, where P is the number of processors. Building on <ref> [19] </ref>, Yan et al. in [24], suggest adaptive affinity scheduling. Their algorithm is similar to affinity scheduling but their runtime system can Fig. 7.
Reference: 20. <author> Evangelos P. Markatos and Thomas J. LeBlanc. </author> <title> Load balancing versus locality management in shared-memory mult iprocessors. </title> <booktitle> PROC of the 1992 ICPP, </booktitle> <pages> pages I:258-267, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: These cut-offs were heuristically determined on the basis of our experimental platform, and are a function of the amount of computation and any extra communication. Load Balancing vs. Locality Management Previous work <ref> [20] </ref> has shown that locality management is at least as important as load balancing. This is even more so in software DSM where the processors are not tightly coupled, making communication expensive. Consequently, we need to avoid unnecessary movement of data and at the same time minimize page sharing. <p> There are several variations, including self-scheduling [23], fixed-size chunking [15], guided self-scheduling [22], and adaptive guided self-scheduling [7]. Markatos and Le Blanc in <ref> [20] </ref>, argue that locality management is more important than load balancing in thread assignment. They introduce a policy they call Memory-Conscious Scheduling that assigns threads to processors whose local memory holds most of the data the thread will access.
Reference: 21. <author> Bongki Moon and Joel Saltz. </author> <title> Adaptive runtime support for direct simulation monte carlo methods on distributed memory architectures. </title> <booktitle> In Salable High Performance Computing Comference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Their results indicate that taking into account the relative computation power as well as any heterogeneity in the loop format while doing the loop distribution improves the overall performance of the application. Similarly, Moon and Saltz <ref> [21] </ref> also looked at applications with irregular access patterns. To compensate for load imbalance, they introduce periodic re-mapping, or re-mapping at predetermined points of the execution, and dynamic re-mapping, in which they determine if repartitioning is required at every time step.
Reference: 22. <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: a practical scheduling scheme for parallel supercomputers. </title> <journal> In Transactions on Computers, </journal> <month> September </month> <year> 1992. </year>
Reference-contexts: Comparison of the running times of the applications using our load balanc-ing algorithm on 8 loaded processors, compared to there performance on 7 load free processors. finished its assigned portion, more work is obtained from this queue. There are several variations, including self-scheduling [23], fixed-size chunking [15], guided self-scheduling <ref> [22] </ref>, and adaptive guided self-scheduling [7]. Markatos and Le Blanc in [20], argue that locality management is more important than load balancing in thread assignment. They introduce a policy they call Memory-Conscious Scheduling that assigns threads to processors whose local memory holds most of the data the thread will access.
Reference: 23. <author> P. Tang and P. C. Yew. </author> <title> Processor self-scheduling: A practical scheduling scheme for parallel computers. </title> <booktitle> In International Conference On Parallel Processing, </booktitle> <month> Augoust </month> <year> 1986. </year>
Reference-contexts: Once a processor has Fig. 6. Comparison of the running times of the applications using our load balanc-ing algorithm on 8 loaded processors, compared to there performance on 7 load free processors. finished its assigned portion, more work is obtained from this queue. There are several variations, including self-scheduling <ref> [23] </ref>, fixed-size chunking [15], guided self-scheduling [22], and adaptive guided self-scheduling [7]. Markatos and Le Blanc in [20], argue that locality management is more important than load balancing in thread assignment.
Reference: 24. <author> Yong Yan, Canming Jin, and Xiaodong Zhang. </author> <title> Adaptively scheduling parallel loops in distributed shared-memory systems. </title> <journal> In Transactions on parallel and sitributed systems, </journal> <volume> volume 8, </volume> <month> January </month> <year> 1997. </year>
Reference-contexts: If a processor's work queue is empty, it finds the most loaded processor and it removes 1/P of the iterations in that processor's works queue and executes them, where P is the number of processors. Building on [19], Yan et al. in <ref> [24] </ref>, suggest adaptive affinity scheduling. Their algorithm is similar to affinity scheduling but their runtime system can Fig. 7.
Reference: 25. <author> Mohammed Javeed Zaki, Wei Li, and Srinivasan Parthasarathy. </author> <title> Customized dynamic load balancing for a network of workstations. </title> <type> Technical Report 602, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: This is similar to our approach. However their approach deals with message passing programs. A discussion on global vs. local and distributed vs. centralized strategies for load balancing is presented in <ref> [25] </ref>. Based on the information they use to make load balancing decisions they can be divided into local and global. Distributed and centralized refers to whether the load balancer is one master processor or distributed among the processors.
References-found: 25

