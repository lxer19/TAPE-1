URL: file://ftp.cis.ohio-state.edu/pub/communication/techreports/tr45-96-rel-barrier.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~panda/wormhole_pub.html
Root-URL: 
Title: Reliable Hardware Barrier Synchronization Schemes  
Abstract: Rajeev Sivaram, Craig B. Stunkel and Dhabaleswar K. Panda Technical Report OSU-CISRC-9/96-TR45 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. B. Andrews, C. J. Beckmann, and D. K.Poulsen. </author> <title> Notification and multicast networks for synchronization and coherence. </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 15 </volume> <pages> 332-350, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The hardware modifications and protocols are equally applicable to both direct and indirect (switch-based) networks. We do not focus on the parallel application performance improvement possible via network hardware barrier operations. Several papers have adequately addressed this topic <ref> [3, 1, 20, 21] </ref> and our hardware scheme should be similar in performance to other tree-based hardware combining schemes. However, we do provide a potential implementation, and we compare the barrier-sync latency of this implementation to software-based schemes.
Reference: [2] <author> ATM Forum. </author> <title> ATM User-Network Interface Specification, </title> <note> Version 3.1, </note> <month> September </month> <year> 1994. </year>
Reference-contexts: This setup can be accomplished in various ways. Special "circuit setup" packets may be sent to every switch involved in the associated multicast/combine tree, in a manner similar to ATM virtual path/channel setup <ref> [2] </ref>.
Reference: [3] <author> T. S. Axelrod. </author> <title> Effects of Synchronization Barriers on Multiprocessor Performance. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 129-140, </pages> <year> 1986. </year>
Reference-contexts: The hardware modifications and protocols are equally applicable to both direct and indirect (switch-based) networks. We do not focus on the parallel application performance improvement possible via network hardware barrier operations. Several papers have adequately addressed this topic <ref> [3, 1, 20, 21] </ref> and our hardware scheme should be similar in performance to other tree-based hardware combining schemes. However, we do provide a potential implementation, and we compare the barrier-sync latency of this implementation to software-based schemes.
Reference: [4] <author> V. Bala, J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.-T. Ho, S. Kipnis, and M. Snir. </author> <title> CCL: A portable and tunable collective communication library for scalable parallel computers. </title> <journal> IEEE Trans. on Parallel and Distributed Computing, </journal> <volume> 6(2) </volume> <pages> 154-164, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: The IBM SP2, the Intel Paragon, and most networks of workstations implement a totally software-based messaging approach: a total data-less exchange is performed by sending several phases of messages via the system message-transmission routines <ref> [4, 26] </ref>. Each of these messages incurs a significant software startup overhead, and the number of messages sent is proportional to log 2 N , where N is the number of processes in the group.
Reference: [5] <author> C. J. Beckmann and C. D. Polychronopoulos. </author> <title> Broadcast Networks for Fast Synchronization. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages I:220-224, </pages> <year> 1991. </year>
Reference-contexts: Many other dedicated hardware schemes have been proposed [21]. A compromise is to provide hardware support for barrier-sync within the existing data network <ref> [5, 8, 16, 20] </ref>. The T3E [20] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessor|the T3D [12]|maintained a separate network for this purpose.
Reference: [6] <author> W. E. Cohen, H. G. Dietz, and J. B. Sponaugle. </author> <title> Dynamic barrier architecture for multi--mode fine-grain parallelism using conventional processors. </title> <booktitle> In Int. Conf. on Parallel Processing, </booktitle> <pages> pages I-93-96, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: This software methodology is easily portable to new systems, but results in a high latency for barrier operations. To address high latency, several machines <ref> [6, 11, 12, 13] </ref> have implemented dedicated hardware networks to perform barrier-sync and some other important collective communication operations. This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations.
Reference: [7] <author> W. J. Dally. </author> <title> Performance analysis of k-ary n-cube interconnection networks. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(6) </volume> <pages> 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We assume that each switch in an interconnection network connects k flit-wide input ports to k flit-wide output ports via a k fi k crossbar. For assessing performance, we assume wormhole routing and flit-wide links, where a flit is the sub-packet unit upon which network flow-control is performed <ref> [7] </ref>. Each input port contains a FIFO that can buffer 16 flits. A switch modified for packlet multicast and combining is shown in Figure 2. One additional input and output have been added to the internal crossbar, providing paths to and from a new function: the barrier unit.
Reference: [8] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer: Designing an MIMD Shared Memory Parallel Computer. </title> <journal> IEEE Transaction on Computers, </journal> <volume> C-32(2):175-189, </volume> <month> Feb </month> <year> 1983. </year>
Reference-contexts: Many other dedicated hardware schemes have been proposed [21]. A compromise is to provide hardware support for barrier-sync within the existing data network <ref> [5, 8, 16, 20] </ref>. The T3E [20] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessor|the T3D [12]|maintained a separate network for this purpose. <p> Note this is a much simpler requirement than supporting general packet multicast [14, 17, 22] because of the small fixed-size of the packlet, and it is simpler than supporting general combining functions <ref> [8] </ref>.
Reference: [9] <author> C. Huang and P. K. McKinley. </author> <title> Communication issues in parallel computing across ATM networks. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(4) </volume> <pages> 73-86, </pages> <month> Winter </month> <year> 1994. </year>
Reference-contexts: In Section 5 we compare latencies for several different barrier implementations. 2 Architectural requirements Barrier-sync is equivalent to a data-less total exchange|every process in the group must be able to observe when every other process has arrived at the barrier. There are several ways of implementing this total exchange <ref> [9] </ref>. Number each processing node, or node, from 0 to N 1, where N is the number of nodes in the barrier-sync group. We will refer to data-less message packets as packlets. When N is a power of 2, packlet exchanges can be performed along successive "dimensions" in the group.
Reference: [10] <author> Y. Huang and P. K. McKinley. </author> <title> Efficient collection operations with ATM network interface support. </title> <booktitle> In Proc. Int. Conf. on Parallel Processing, </booktitle> <pages> pages I-34-43, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: The T3E [20] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessor|the T3D [12]|maintained a separate network for this purpose. A related strategy is to provide extra support at the network interface boundary to avoid the large software overheads for software multicast trees <ref> [24, 10] </ref>. To our knowledge, the fault-tolerant operation of barrier-sync has not been specifically addressed in the literature, and this is the prime focus of our paper. We describe inexpensive support that can be added to network switches for improving barrier-sync while recovering from lost or corrupted messages.
Reference: [11] <author> H. Ishihata, T. Horie, and T. Shimizu. </author> <title> Architecture for the AP1000 highly parallel computer. </title> <journal> Fujitsu Scientific and Technical J., </journal> <volume> 29(1) </volume> <pages> 6-14, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: This software methodology is easily portable to new systems, but results in a high latency for barrier operations. To address high latency, several machines <ref> [6, 11, 12, 13] </ref> have implemented dedicated hardware networks to perform barrier-sync and some other important collective communication operations. This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations.
Reference: [12] <author> R. E. Kessler and J. L. Schwarzmeier. </author> <title> Cray T3D: A new dimension for Cray Research. </title> <booktitle> In Proc. 38th IEEE Int. Computer Conf., </booktitle> <pages> pages 176-182, </pages> <month> Spring </month> <year> 1993. </year>
Reference-contexts: This software methodology is easily portable to new systems, but results in a high latency for barrier operations. To address high latency, several machines <ref> [6, 11, 12, 13] </ref> have implemented dedicated hardware networks to perform barrier-sync and some other important collective communication operations. This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations.
Reference: [13] <author> C. E. Leiserson et al. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proc. 1992 Symp. Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: This software methodology is easily portable to new systems, but results in a high latency for barrier operations. To address high latency, several machines <ref> [6, 11, 12, 13] </ref> have implemented dedicated hardware networks to perform barrier-sync and some other important collective communication operations. This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations.
Reference: [14] <author> X. Lin and L. M. Ni. </author> <title> Deadlock-free Multicast Wormhole Routing in Multicomputer Networks. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 116-124, </pages> <year> 1991. </year>
Reference-contexts: In this section we describe an architecture which can perform these combining and multicast functions only for packlets. Note this is a much simpler requirement than supporting general packet multicast <ref> [14, 17, 22] </ref> because of the small fixed-size of the packlet, and it is simpler than supporting general combining functions [8]. <p> This setup can be accomplished in various ways. Special "circuit setup" packets may be sent to every switch involved in the associated multicast/combine tree, in a manner similar to ATM virtual path/channel setup [2]. Alternately, if the switches already support a means of general multicast <ref> [14, 17, 18] </ref>, a multicast packet could be used to carry the "barrier tag", and the SRAM barrier state pointed to by the barrier tag would then store the identity of the calculated local output ports of the multicast packet.
Reference: [15] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Barriers provide a means of ensuring that no process in a group of processes advances beyond a particular point in a computation, until all have arrived at that point <ref> [15] </ref>. The barrier is useful for bringing the group of processes to a known global state before proceeding to a new phase of computation. The efficiency of barrier-sync affects how small the grain-size of tasks in an application can become without significantly degrading performance.
Reference: [16] <author> D. K. Panda. </author> <title> Fast Barrier Synchronization in Wormhole k-ary n-cube Networks with Multidestination Worms. </title> <booktitle> In International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 200-209, </pages> <year> 1995. </year>
Reference-contexts: Many other dedicated hardware schemes have been proposed [21]. A compromise is to provide hardware support for barrier-sync within the existing data network <ref> [5, 8, 16, 20] </ref>. The T3E [20] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessor|the T3D [12]|maintained a separate network for this purpose.
Reference: [17] <author> D. K. Panda, S. Singal, and P. Prabhakaran. </author> <title> Multidestination Message Passing Mechanism Conforming to Base Wormhole Routing Scheme. </title> <booktitle> In Proceedings of the Parallel Computer Routing and Communication Workshop, </booktitle> <pages> pages 131-145, </pages> <year> 1994. </year>
Reference-contexts: In this section we describe an architecture which can perform these combining and multicast functions only for packlets. Note this is a much simpler requirement than supporting general packet multicast <ref> [14, 17, 22] </ref> because of the small fixed-size of the packlet, and it is simpler than supporting general combining functions [8]. <p> This setup can be accomplished in various ways. Special "circuit setup" packets may be sent to every switch involved in the associated multicast/combine tree, in a manner similar to ATM virtual path/channel setup [2]. Alternately, if the switches already support a means of general multicast <ref> [14, 17, 18] </ref>, a multicast packet could be used to carry the "barrier tag", and the SRAM barrier state pointed to by the barrier tag would then store the identity of the calculated local output ports of the multicast packet.
Reference: [18] <author> D. K. Panda and R. Sivaram. </author> <title> Fast Broadcast and Multicast in Wormhole Multistage Networks with Multidestination Worms. </title> <type> Technical Report OSU-CISRC-4/95-TR21, </type> <institution> Dept. of Computer and Information Science, The Ohio State University, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: This setup can be accomplished in various ways. Special "circuit setup" packets may be sent to every switch involved in the associated multicast/combine tree, in a manner similar to ATM virtual path/channel setup [2]. Alternately, if the switches already support a means of general multicast <ref> [14, 17, 18] </ref>, a multicast packet could be used to carry the "barrier tag", and the SRAM barrier state pointed to by the barrier tag would then store the identity of the calculated local output ports of the multicast packet.
Reference: [19] <author> I. D. Scherson and C.-H. Chien. </author> <title> Least common ancestor networks. </title> <booktitle> In Proc. 7th Int. Parallel Processing Symp., </booktitle> <pages> pages 507-513, </pages> <year> 1993. </year> <month> 22 </month>
Reference-contexts: Figure 5 illustrates a bidirectional MIN <ref> [19] </ref> network in which these fields have been initialized for a barrier group. This setup can be accomplished in various ways. Special "circuit setup" packets may be sent to every switch involved in the associated multicast/combine tree, in a manner similar to ATM virtual path/channel setup [2].
Reference: [20] <author> S. L. Scott. </author> <title> Synchronization and communication in the T3E multiprocessor. </title> <booktitle> In ASPLOS-VII, </booktitle> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: Many other dedicated hardware schemes have been proposed [21]. A compromise is to provide hardware support for barrier-sync within the existing data network <ref> [5, 8, 16, 20] </ref>. The T3E [20] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessor|the T3D [12]|maintained a separate network for this purpose. <p> Many other dedicated hardware schemes have been proposed [21]. A compromise is to provide hardware support for barrier-sync within the existing data network [5, 8, 16, 20]. The T3E <ref> [20] </ref> is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessor|the T3D [12]|maintained a separate network for this purpose. A related strategy is to provide extra support at the network interface boundary to avoid the large software overheads for software multicast trees [24, 10]. <p> The hardware modifications and protocols are equally applicable to both direct and indirect (switch-based) networks. We do not focus on the parallel application performance improvement possible via network hardware barrier operations. Several papers have adequately addressed this topic <ref> [3, 1, 20, 21] </ref> and our hardware scheme should be similar in performance to other tree-based hardware combining schemes. However, we do provide a potential implementation, and we compare the barrier-sync latency of this implementation to software-based schemes. <p> Note this is a much simpler requirement than supporting general packet multicast [14, 17, 22] because of the small fixed-size of the packlet, and it is simpler than supporting general combining functions [8]. The Cray T3E <ref> [20] </ref> network already performs packlet multicast (called eureka) and applies a combination of eureka and packlet combining hardware to perform barrier-sync. 2 2.1 Packlet and switch architecture description The packlet, though carrying no data to be replicated during multicast and no data to be combined during combining, nevertheless contains several fields:
Reference: [21] <author> S. Shang and K. Hwang. </author> <title> Distributed hardwired barrier synchronization for scalable multiprocessor clusters. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 6(6) </volume> <pages> 591-605, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations. Many other dedicated hardware schemes have been proposed <ref> [21] </ref>. A compromise is to provide hardware support for barrier-sync within the existing data network [5, 8, 16, 20]. The T3E [20] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessor|the T3D [12]|maintained a separate network for this purpose. <p> The hardware modifications and protocols are equally applicable to both direct and indirect (switch-based) networks. We do not focus on the parallel application performance improvement possible via network hardware barrier operations. Several papers have adequately addressed this topic <ref> [3, 1, 20, 21] </ref> and our hardware scheme should be similar in performance to other tree-based hardware combining schemes. However, we do provide a potential implementation, and we compare the barrier-sync latency of this implementation to software-based schemes.
Reference: [22] <author> R. Sivaram, D. K. Panda, and C. B. Stunkel. </author> <title> Efficient Broadcast and Multicast on Multistage Interconnection Networks using Multiport Encoding. </title> <booktitle> In Proceedings of the Symposium on Parallel and Distributed Processing, </booktitle> <month> Oct </month> <year> 1996. </year> <note> to be presented. </note>
Reference-contexts: In this section we describe an architecture which can perform these combining and multicast functions only for packlets. Note this is a much simpler requirement than supporting general packet multicast <ref> [14, 17, 22] </ref> because of the small fixed-size of the packlet, and it is simpler than supporting general combining functions [8].
Reference: [23] <author> C. B. Stunkel, D. G. Shea, B. Abali, et al. </author> <title> The SP2 high-performance switch. </title> <journal> IBM System Journal, </journal> <volume> 34(2) </volume> <pages> 185-204, </pages> <year> 1995. </year>
Reference-contexts: The barrier unit stores and processes barrier information for up to B concurrent barriers in the system. To simplify the diagram, we have not shown any flow-control signals; our simulations in Section 5 assume a token-based flow-control similar to that used in the IBM SP2 <ref> [23] </ref>. 2.2 The barrier unit The barrier unit, detailed in Figure 3, contains a B-entry SRAM. Entry i in the SRAM stores the current barrier state for the barrier group i, and is addressed by packlets with barrier tag i. <p> The protocols described in Section 3 accomplish this in cooperation with switch barrier units. We do not consider permanent faults, because these faults are typically handled by system reconfiguration software in commercial systems <ref> [23] </ref>.
Reference: [24] <author> K. Verstoep, K. Langendoen, and H. Bal. </author> <title> Efficient Reliable Multicast on Myrinet. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1996. </year>
Reference-contexts: The T3E [20] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessor|the T3D [12]|maintained a separate network for this purpose. A related strategy is to provide extra support at the network interface boundary to avoid the large software overheads for software multicast trees <ref> [24, 10] </ref>. To our knowledge, the fault-tolerant operation of barrier-sync has not been specifically addressed in the literature, and this is the prime focus of our paper. We describe inexpensive support that can be added to network switches for improving barrier-sync while recovering from lost or corrupted messages. <p> that they have reached the barrier. 4.2 Software Scheme with Network Interface Support (UMin-NI) It has been noted that a completely software based scheme results in very high latency because every message reception and transmission has to go through a number of software layers before being processed by the root <ref> [24] </ref>. This situation can be improved if a processing unit at the network interface can be programmed to handle barrier related messages. Under this scheme we consider the binomial tree-based barrier synchronization scheme described in the previous section but with the addition of a smart network interface.
Reference: [25] <author> H. Xu, Y.-D. Gui, and L. M. Ni. </author> <title> Optimal Software Multicast in Wormhole-Routed Multistage Networks. </title> <booktitle> In Proceedings of the Supercomputing Conference, </booktitle> <pages> pages 703-712, </pages> <year> 1994. </year>
Reference-contexts: The barrier latency depends on the times at which the participating processors reach their respective barriers. Finally, we present an analytical estimate of the best and worst case performance of each of the schemes for a bidi-MIN system using turnaround routing <ref> [25] </ref>. 4.1 Completely Software Based Scheme (UMin) Various methods for software multicast on bidi-MINs have been proposed. The U-Min algorithm [25] uses a contention free binomial tree for performing multicast with the source node as root. <p> Finally, we present an analytical estimate of the best and worst case performance of each of the schemes for a bidi-MIN system using turnaround routing <ref> [25] </ref>. 4.1 Completely Software Based Scheme (UMin) Various methods for software multicast on bidi-MINs have been proposed. The U-Min algorithm [25] uses a contention free binomial tree for performing multicast with the source node as root. The multicast proceeds by having intermediate destinations act as secondary sources in succeeding phases of the multicast 4 . 4 As described in Sec. 2, efficient synchronization can be performed using packlet exchange. <p> Once the root of the binomial tree learns that all participants have reached the barrier, it informs everyone that they can proceed from the barrier by sending a multicast (using the binomial tree). Under the first scheme, we use the binomial tree constructed using the U-Min algorithm <ref> [25] </ref> to perform software barrier synchronization. Under this approach the worst case barrier performance occurs when the process that arrives at the barrier last is a leaf node in the binomial tree that is farthest from the root. <p> We assume that the barriers are being performed on a bidirectional MIN network consisting of N = k n nodes interconnected by n stages of bidirectional k fi k switches. The unicast messages use turnaround routing <ref> [25] </ref>, i.e. to travel between a source and a given destination, they travel up to the least common ancestor stage of the MIN and then turn around to travel to the destination.
Reference: [26] <author> H. Xu, P. K. McKinley, and L. Ni. </author> <title> Efficient Implementation of Barrier Synchronization in Wormhole-routed Hypercube Multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 172-184, </pages> <year> 1992. </year> <month> 23 </month>
Reference-contexts: The IBM SP2, the Intel Paragon, and most networks of workstations implement a totally software-based messaging approach: a total data-less exchange is performed by sending several phases of messages via the system message-transmission routines <ref> [4, 26] </ref>. Each of these messages incurs a significant software startup overhead, and the number of messages sent is proportional to log 2 N , where N is the number of processes in the group. <p> However, this method is only applicable for barriers where the participant set size is a power of two. We therefore do not consider this scheme. 11 A binomial tree constructed for multicast can also be used for barrier synchronization <ref> [26] </ref>. The basic idea is to use the reverse binomial tree to collect information about the participants that have reached the barrier.
References-found: 26

