URL: http://www.merl.com/reports/TR96-03a/TR96-03a.ps.gz
Refering-URL: http://www.merl.com/reports/TR96-03a/index.html
Root-URL: 
Title: Combining Trigram-based and Feature-based Methods for Context-Sensitive Spelling Correction  
Author: Andrew R. Golding and Yves Schabes 
Address: 201 Broadway Cambridge, MA 02139  Cruz, CA,  1996 201 Broadway, Cambridge, Massachusetts 02139  
Affiliation: Mitsubishi Electric Research Laboratories  Santa  Mitsubishi Electric Information Technology Center America,  
Note: MERL A MITSUBISHI ELECTRIC RESEARCH LABORATORY  In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,  Copyright c  
Pubnum: TR-96-03a  
Email: fgolding, schabesg@merl.com  
Date: May 1996  1996, pages 71-78.  
Web: http://www.merl.com  
Abstract: This paper addresses the problem of correcting spelling errors that result in valid, though unintended words (such as peace and piece, or quiet and quite) and also the problem of correcting particular word usage errors (such as amount and number, or among and between). Such corrections require contextual information and are not handled by conventional spelling programs such as Unix spell. First, we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context. This method uses a small number of parameters compared to previous methods based on word trigrams. However, it is effectively unable to distinguish among words that have the same part of speech. For this case, an alternative feature-based method called Bayes performs better; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints. A hybrid method called Tribayes is then introduced that combines the best of the previous two methods. The improvement in performance of Tribayes over its components is verified experimentally. Tribayes is also compared with the grammar checker in Microsoft Word, and is found to have substantially higher performance. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Information Technology Center America; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Information Technology Center America. All rights reserved. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Church, Kenneth Ward. </author> <year> 1988. </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143, </pages> <address> Austin, TX. </address>
Reference: <author> DeRose, S.J. </author> <year> 1988. </year> <title> Grammatical category disam biguation by statistical optimization. </title> <journal> Computational Linguistics, </journal> <volume> 14 </volume> <pages> 31-39. </pages>
Reference: <editor> Flexner, S. B., editor. </editor> <year> 1983. </year> <title> Random House Unabridged Dictionary. Random House, </title> <address> New York. </address> <note> Second edition. </note>
Reference-contexts: All methods were run on a collection of 18 confusion sets, which were largely taken from the list of "Words Commonly Confused" in the back of Random House <ref> (Flexner, 1983) </ref>. The confusion sets were selected on the basis of being frequently-occurring in Brown, and representing a variety of types of errors, including homophone confusions (e.g., fpeace; pieceg) and grammatical mistakes (e.g., famong ; betweeng).
Reference: <author> Gale, William A., Kenneth W. Church, and David Yarowsky. </author> <year> 1993. </year> <title> A method for disambiguating word senses in a large corpus. </title> <journal> Computers and the Humanities, </journal> <volume> 26 </volume> <pages> 415-439. </pages>
Reference-contexts: In addition, huge word-trigram tables need to be available at run time. Moreover, word trigrams are ineffective at capturing long-distance properties such as discourse topic and tense. Feature-based approaches, such as Bayesian classifiers <ref> (Gale, Church, and Yarowsky, 1993) </ref>, decision lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of success for the problem of context-sensitive spelling correction. <p> In this case, a more effective approach is to learn features that characterize the different contexts in which each word tends to occur. A number of feature-based methods have been proposed, including Bayesian classifiers <ref> (Gale, Church, and Yarowsky, 1993) </ref>, decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).
Reference: <author> Golding, Andrew R. and Dan Roth. </author> <year> 1996. </year> <title> Apply ing Winnow to context-sensitive spelling correction. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the 13th International Conference, </booktitle> <address> Bari, Italy. </address> <note> To appear. </note>
Reference-contexts: A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm <ref> (Golding and Roth, 1996) </ref>. We adopt the Bayesian hybrid method, which we will call Bayes, having experimented with each of the methods and found Bayes to be among the best-performing for the task at hand.
Reference: <author> Golding, Andrew R. </author> <year> 1995. </year> <title> A Bayesian hybrid method for context-sensitive spelling correction. </title> <booktitle> In Proceedings of the Third Workshop on Very Large Corpora, </booktitle> <pages> pages 39-53, </pages> <address> Boston, MA. </address>
Reference-contexts: In addition, huge word-trigram tables need to be available at run time. Moreover, word trigrams are ineffective at capturing long-distance properties such as discourse topic and tense. Feature-based approaches, such as Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), and Bayesian hybrids <ref> (Golding, 1995) </ref>, have had varying degrees of success for the problem of context-sensitive spelling correction. <p> In the latter case, it is reduced to simply guessing whichever word in the confusion set is the most common representative of its part-of-speech class. We consider an alternative method, Bayes, a Bayesian hybrid method <ref> (Golding, 1995) </ref>, for the case where the words have the same part of speech. <p> In this case, a more effective approach is to learn features that characterize the different contexts in which each word tends to occur. A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids <ref> (Golding, 1995) </ref>, and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996). We adopt the Bayesian hybrid method, which we will call Bayes, having experimented with each of the methods and found Bayes to be among the best-performing for the task at hand. <p> We adopt the Bayesian hybrid method, which we will call Bayes, having experimented with each of the methods and found Bayes to be among the best-performing for the task at hand. This method has been described elsewhere <ref> (Golding, 1995) </ref> and so will only be briefly reviewed here; however, the version used here uses an improved smoothing technique, which is mentioned briefly below. 2 In the experiments reported here, the trigram method was run using the tag inventory derived from the Brown corpus, except that a handful of common
Reference: <author> Kukich, Karen. </author> <year> 1992. </year> <title> Techniques for automatically correcting words in text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <month> December. </month>
Reference-contexts: Recent studies of actual observed spelling errors have estimated that overall, errors resulting in valid words account for anywhere from 25% to over 50% of the errors, depending on the application <ref> (Kukich, 1992) </ref>. We will use the term context-sensitive spelling correction to refer to the task of fixing spelling errors that result in valid words, such as: (1) fl Can I have a peace of cake? where peace was typed when piece was intended.
Reference: <author> Kucera, H. and W. N. Francis. </author> <year> 1967. </year> <title> Computa tional Analysis of Present-Day American English. </title> <publisher> Brown University Press, </publisher> <address> Providence, RI. </address>
Reference-contexts: The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. We trained each method on 80% (randomly selected) of the Brown corpus <ref> (Kucera and Francis, 1967) </ref> and tested it on the remaining 20%. All methods were run on a collection of 18 confusion sets, which were largely taken from the list of "Words Commonly Confused" in the back of Random House (Flexner, 1983).
Reference: <author> Mays, Eric, Fred J. Damerau, and Robert L. Mercer. </author> <year> 1991. </year> <title> Context based spelling correction. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 27(5) </volume> <pages> 517-522. </pages>
Reference-contexts: The task is to predict, given an occurrence of a word in one of the confusion sets, which word in the set was actually intended. Previous work on context-sensitive spelling correction and related lexical disambiguation tasks has its limitations. Word-trigram methods <ref> (Mays, Dam-erau, and Mercer, 1991) </ref> require an extremely large body of text to train the word-trigram model; even with extensive training sets, the problem of sparse data is often acute. In addition, huge word-trigram tables need to be available at run time.
Reference: <author> Peterson, James L. </author> <year> 1986. </year> <title> A note on undetected typing errors. </title> <journal> Communications of the ACM, </journal> <volume> 29(7) </volume> <pages> 633-637, </pages> <month> July. </month>
Reference-contexts: One analysis has shown that up to 15% of spelling errors that result from elementary typographical errors (character insertion, deletion, or transposition) yield another valid word in the language <ref> (Peterson, 1986) </ref>. These errors remain undetected by traditional spelling checkers. In addition to typographical errors, words that can be easily confused with each other (for instance, the homophones peace and piece) also remain undetected.
Reference: <author> Yarowsky, David. </author> <year> 1994. </year> <title> Decision lists for lexi cal ambiguity resolution: Application to accent restoration in Spanish and French. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 88-95, </pages> <address> Las Cruces, NM. </address> <month> MERL-TR-96-03a 78 May </month> <year> 1996 </year>
Reference-contexts: In addition, huge word-trigram tables need to be available at run time. Moreover, word trigrams are ineffective at capturing long-distance properties such as discourse topic and tense. Feature-based approaches, such as Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists <ref> (Yarowsky, 1994) </ref>, and Bayesian hybrids (Golding, 1995), have had varying degrees of success for the problem of context-sensitive spelling correction. <p> In this case, a more effective approach is to learn features that characterize the different contexts in which each word tends to occur. A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists <ref> (Yarowsky, 1994) </ref>, Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).
References-found: 11

