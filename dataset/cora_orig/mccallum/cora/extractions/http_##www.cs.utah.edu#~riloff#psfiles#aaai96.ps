URL: http://www.cs.utah.edu/~riloff/psfiles/aaai96.ps
Refering-URL: http://www.cs.utah.edu/~riloff/publications.html
Root-URL: 
Email: riloff@cs.utah.edu  
Title: Automatically Generating Extraction Patterns from Untagged Text extraction patterns using an annotated training corpus. CRYSTAL
Author: Ellen Riloff ) and Hastings (Hastings Lytinen 
Keyword: Motivation Generating Extraction Patterns from Tagged Text  
Address: Salt Lake City, UT 84112  
Affiliation: Department of Computer Science University of Utah  
Note: In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), pp. 1044-1049  Related work CRYSTAL (Soderland et al. 1995) also generates  1994) also  
Abstract: Many corpus-based natural language processing systems rely on text corpora that have been manually annotated with syntactic or semantic tags. In particular, all previous dictionary construction systems for information extraction have used an annotated training corpus or some form of annotated input. We have developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text. AutoSlog-TS is based on the AutoSlog system, which generated extraction patterns using annotated text and a set of heuristic rules. By adapting Au-toSlog and combining it with statistical techniques, we eliminated its dependency on tagged text. In experiments with the MUC-4 terrorism domain, AutoSlog-TS created a dictionary of extraction patterns that performed comparably to a dictionary created by Au-toSlog, using only preclassified texts as input. The vast amount of text becoming available on-line offers new possibilities for conquering the knowledge-engineering bottleneck lurking underneath most natural language processing (NLP) systems. Most corpus-based systems rely on a text corpus that has been manually tagged in some way. For example, the Brown corpus (Francis & Kucera 1982) and the Penn Treebank corpus (Marcus, Santorini, & Marcinkiewicz 1993) are widely used because they have been manually annotated with part-of-speech and syntactic bracketing information. Part-of-speech tagging and syntactic bracketing are relatively general in nature, so these corpora can be used by different natural language processing systems and for different domains. But some corpus-based systems rely on a text corpus that has been manually tagged in a domain-specific or task-specific manner. For example, corpus-based approaches to information extraction generally rely on special domain-specific text annotations. Consequently, the manual tagging effort is considerably less cost effective because the annotated corpus is useful for only one type of NLP system and for only one domain. Corpus-based approaches to information extraction have demonstrated a significant time savings over conventional hand-coding methods (Riloff 1993). But the time required to annotate a training corpus is a nontrivial expense. To further reduce this knowledge-engineering bottleneck, we have developed a system called AutoSlog-TS that generates extraction patterns using untagged text. AutoSlog-TS needs only a pre-classified corpus of relevant and irrelevant texts. Nothing inside the texts needs to be tagged in any way. In the last few years, several systems have been developed to generate patterns for information extraction automatically. All of the previous systems depend on manually tagged training data of some sort. One of the first dictionary construction systems was AutoSlog (Riloff 1993), which requires tagged noun phrases in the form of annotated text or text with associated answer keys. PALKA (Kim & Moldovan 1993) is similar in spirit to AutoSlog, but requires manually defined frames (including keywords), a semantic hierarchy, and an associated lexicon. Competing hypotheses are resolved by referring to manually encoded answer keys, if available, or by asking a user. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cardie, C. </author> <year> 1993. </year> <title> A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 798-803. </pages> <publisher> AAAI Press/The MIT Press. </publisher>
Reference: <author> Francis, W., and Kucera, H. </author> <year> 1982. </year> <title> Frequency Analysis of English Usage. </title> <address> Boston, MA: </address> <publisher> Houghton Mi*in. </publisher>
Reference: <author> Hastings, P., and Lytinen, S. </author> <year> 1994. </year> <title> The Ups and Downs of Lexical Acquisition. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 754-759. </pages> <publisher> AAAI Press/The MIT Press. </publisher>
Reference: <author> Huffman, S. </author> <year> 1996. </year> <title> Learning information extraction patterns from examples. </title> <editor> In Wermter, S.; Riloff, E.; and Scheler, G., eds., </editor> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Kim, J., and Moldovan, D. </author> <year> 1993. </year> <title> Acquisition of Semantic Patterns for Information Extraction from Corpora. </title> <booktitle> In Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications, </booktitle> <pages> 171-176. </pages> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Lehnert, W. </author> <year> 1991. </year> <title> Symbolic/Subsymbolic Sentence Analysis: Exploiting the Best of Two Worlds. </title> <editor> In Barnden, J., and Pollack, J., eds., </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <volume> Vol. 1. </volume> <publisher> Ablex Publishers, </publisher> <address> Nor-wood, NJ. </address> <pages> 135-164. </pages>
Reference: <author> Marcus, M.; Santorini, B.; and Marcinkiewicz, M. </author> <year> 1993. </year> <title> Building a Large Annotated Corpus of English: The Penn Treebank. </title> <booktitle> Computational Linguistics 19(2) </booktitle> <pages> 313-330. </pages> <booktitle> MUC-4 Proceedings. 1992. Proceedings of the Fourth Message Understanding Conference (MUC-4). </booktitle> <address> San Ma-teo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Riloff, E., and Shoen, J. </author> <year> 1995. </year> <title> Automatically Acquiring Conceptual Patterns Without an Annotated Corpus. </title> <booktitle> In Proceedings of the Third Workshop on Very Large Corpora, </booktitle> <pages> 148-161. </pages>
Reference-contexts: A user only needs to provide sample texts (relevant and irrelevant), and spend some time filtering and labeling the resulting extraction patterns. Fast dictionary construction also opens the door for IE technology to support other tasks, such as text classification <ref> (Riloff & Shoen 1995) </ref>. Finally, AutoSlog-TS represents a new approach to exploiting on-line text corpora for domain-specific knowledge acquisition by squeezing preclassi-fied texts for all they're worth. Acknowledgments This research was funded by NSF grant MIP-9023174 and NSF grant IRI-9509820.
Reference: <author> Riloff, E. </author> <year> 1993. </year> <title> Automatically Constructing a Dictionary for Information Extraction Tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 811-816. </pages> <publisher> AAAI Press/The MIT Press. </publisher>
Reference-contexts: Therefore a person must manually inspect each extraction pattern and decide which ones should be accepted and which ones should be rejected. This manual filtering process is typically very fast. In experiments with the MUC-4 terrorism domain, it took a user only 5 hours to review 1237 extraction patterns <ref> (Riloff 1993) </ref>. Although this manual filtering process is part of the knowledge-engineering cycle, generating the annotated training corpus is a much more substantial bottleneck. Generating Extraction Patterns from Untagged Text To tag or not to tag? Generating an annotated training corpus is a significant undertaking, both in time and difficulty. <p> F Perp .62 .27 .38 .53 .30 .38 Victim .63 .33 .43 .62 .39 .48 Target .67 .33 .44 .58 .39 .47 Total .64 .31 .42 .58 .36 .44 Table 3: Comparative Results The AutoSlog precision results are substantially lower than those generated by the MUC-4 scoring program <ref> (Riloff 1993) </ref>. There are several reasons for the difference. For one, the current experiments were done with a debilitated version of CIRCUS that did not process conjunctions or semantic features.
Reference: <author> Riloff, E. </author> <year> 1996. </year> <title> An Empirical Study of Automated Dictionary Construction for Information Extraction in Three Domains. </title> <journal> Artificial Intelligence. </journal> <volume> Vol. 85. </volume> <pages> Forthcoming. </pages>
Reference-contexts: Generating Extraction Patterns from Untagged Text To tag or not to tag? Generating an annotated training corpus is a significant undertaking, both in time and difficulty. Previous experiments with AutoSlog suggested that it took a user about 8 hours to annotate 160 texts <ref> (Riloff 1996) </ref>. Therefore it would take roughly a week to construct a training corpus of 1000 texts. Committing a domain expert to a knowledge-engineering project for a week is prohibitive for most short-term applications. Furthermore, the annotation task is deceptively complex. For AutoSlog, the user must annotate relevant noun phrases. <p> We rank the extraction patterns according to the formula: relevance rate fl log 2 (f requency), unless the relevance rate is 0.5 in which case the function returns zero because the pattern is negatively correlated 3 See <ref> (Riloff 1996) </ref> for a more detailed explanation. with the domain (assuming the corpus is 50% relevant). This formula promotes patterns that have a high relevance rate or a high frequency.
Reference: <author> Soderland, S.; Fisher, D.; Aseltine, J.; and Lehnert, W. </author> <year> 1995. </year> <title> CRYSTAL: Inducing a conceptual dictionary. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1314-1319. </pages>
References-found: 11

