URL: http://vibes.cs.uiuc.edu/Publications/Papers/whpc94.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/publications.htm
Root-URL: http://www.cs.uiuc.edu
Email: E-mail: fmendes,reedg@cs.uiuc.edu  
Title: Performance Stability and Prediction  
Author: Celso L. Mendes Daniel A. Reed 
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: This paper presents experimental data from our research on stability of parallel programs and cross-machine performance prediction on multicomputers. We characterize program behavior by an execution graph, obtained from running an instrumented version of the program. We assess program stability using time perturbations, and analyze the resulting execution graphs with an approximation of a graph comparison metric based on subgraph isomorphism. On programs with stable behavior, we predict performance across different systems by transforming the observed execution trace; this trace transformation adjusts timestamps of events according to the architectural parameters of the systems under study, assuming the same partial event order on both systems. This technique allows us to predict performance also for future, hypothetical systems. We provide results showing that the technique is viable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adve, V. S., and Vernon, M. K. </author> <title> The influence of random delays on parallel execution times. </title> <booktitle> In Proceedings of 1993 ACM SIGMETRICS (San Diego, </booktitle> <month> May </month> <year> 1993), </year> <pages> pp. 61-73. </pages>
Reference-contexts: Meanwhile, methods typically used on parallel machines either depend excessively on a particular programming environment (as in [2]), or employ stochastic models [4], now known to be unnecessarily complex <ref> [1] </ref>. Perturbation analysis has also been studied in the context of instrumentation intrusion [5, 9]. There, however, perturbation models are developed as a tool to compensate perturbations, and approximate the original program behavior when no instrumentation is used.
Reference: [2] <author> Clement, M. J., and Quinn, M. J. </author> <title> Analytical performance prediction on multicomputers. </title> <booktitle> In Supercomputing'93 (Portland, </booktitle> <month> November </month> <year> 1993), </year> <pages> pp. 886-894. </pages>
Reference-contexts: Meanwhile, methods typically used on parallel machines either depend excessively on a particular programming environment (as in <ref> [2] </ref>), or employ stochastic models [4], now known to be unnecessarily complex [1]. Perturbation analysis has also been studied in the context of instrumentation intrusion [5, 9]. There, however, perturbation models are developed as a tool to compensate perturbations, and approximate the original program behavior when no instrumentation is used.
Reference: [3] <author> Lyon, G., Snelick, R., and Kacker, R. </author> <title> Synthetic-perturbation tuning of MIMD programs. </title> <type> Tech. Rep. NISTIR 5131, </type> <institution> National Institute of Standards and Technology, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Perturbation analysis has also been studied in the context of instrumentation intrusion [5, 9]. There, however, perturbation models are developed as a tool to compensate perturbations, and approximate the original program behavior when no instrumentation is used. Lyon et al <ref> [3] </ref> developed performance analysis by inserting synthetic perturbations in a program, and measuring the effect on global performance.
Reference: [4] <author> Mak, V. W., and Lundstrom, S. F. </author> <title> Predicting performance of parallel computations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1, </journal> <month> 3 (July </month> <year> 1990), </year> <pages> 257-270. </pages>
Reference-contexts: Meanwhile, methods typically used on parallel machines either depend excessively on a particular programming environment (as in [2]), or employ stochastic models <ref> [4] </ref>, now known to be unnecessarily complex [1]. Perturbation analysis has also been studied in the context of instrumentation intrusion [5, 9]. There, however, perturbation models are developed as a tool to compensate perturbations, and approximate the original program behavior when no instrumentation is used.
Reference: [5] <author> Malony, A. D. </author> <title> Performance Observability. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Meanwhile, methods typically used on parallel machines either depend excessively on a particular programming environment (as in [2]), or employ stochastic models [4], now known to be unnecessarily complex [1]. Perturbation analysis has also been studied in the context of instrumentation intrusion <ref> [5, 9] </ref>. There, however, perturbation models are developed as a tool to compensate perturbations, and approximate the original program behavior when no instrumentation is used. Lyon et al [3] developed performance analysis by inserting synthetic perturbations in a program, and measuring the effect on global performance.
Reference: [6] <author> Mendes, C. L. </author> <title> Performance prediction by trace transformation. </title> <booktitle> In Fifth Brazilian Symposium on Computer Architecture (Florianopolis, </booktitle> <month> September </month> <year> 1993). </year>
Reference-contexts: For programs with stable behavior, we use prediction models and machine architectural parameters to perform cross-machine performance prediction. We presented the basic features of our methodology in <ref> [6] </ref>. In the present work, we show examples of how to apply the methodology. We also extend the applicability of the prediction model to the problem of predicting performance of a given program on future systems, which are charaterized by specific architectural improvements over currently existing systems. <p> If H 1 and H 2 have degree s, we say that the distance d between G 1 and G 2 is d = n s, and that their relative distance d 0 is d 0 = d=n. We have shown in <ref> [6] </ref> that d is a metric in the set of graphs of degree n, and thus it can be used to provide a quantitative measure of similarity between execution graphs. <p> One possible approach is to perform a pairwise comparison of graph nodes corresponding to the same processor on the two graphs. We look for the maximum common subsequence between the two sequences of events from each trace (see <ref> [6] </ref> for more details). With this approximation, it is possible to compare similar graphs in almost linear time. 3.2 Time Perturbation Experiment We illustrate our technique for stability characterization with two examples. <p> On the other hand, the distance between corresponding execution graphs allows a concrete measurement of such variations. The exact relative distance between the graphs can not be smaller than the obtained approximate value of 0.04. 3.3 Time Perturbations with More Processors As we observed in <ref> [6] </ref>, any program instability must be caused by unordered message receipts. Table 5 indicates that, with four processors, most execution events are of the computation type; communication events account for nearly 3 percent only of the total event count. <p> In this section, we summarize the main features of the models we use for the prediction, and apply those models to perform cross-machine predictions on two parallel programs. 4.1 Prediction Model We have described our prediction model in <ref> [6] </ref>. Basically, we transform computation and communication intervals observed in the original execution on the basic system, using architectural parameters that capture the characteristics of the basic and new systems. <p> the processor speed as by the physical network latency. * b is the per byte transfer cost, which depends on processor speed, when n is small (the send duration is governed by buffer copying), or on network bandwidth, for n large. 12 Message Receive: We use our same model from <ref> [6] </ref> to transform the duration of a receive operation. The receive benchmark shows the duration of the receive on the iPSC/860 when the underlying message has already arrived. We can approximate that duration with the following function: T receive = c + dn where n is the message length. <p> Thus, both c and d depend only on processor speed. We must observe the same causality relations described in <ref> [6] </ref>, and use them in the prediction of every receive duration in the program. 5.2 Prediction Experiments We executed instrumented versions of the FFT and GE-Col programs on three different configurations of the iPSC/860: four, sixteen and sixty-four processors.
Reference: [7] <author> Mohamed, A. G., Fox, G. C., von Laszewski, G., Parashar, M., Haupt, T., Mills, K., Lu, Y., Lin, N., and Yeh, N. </author> <title> Application benchmark set for Fortran-D and High Performance Fortran. </title> <type> Tech. Rep. </type> <institution> SCCS-327, Northeast Parallel Architectures Center, </institution> <year> 1992. </year>
Reference-contexts: From the observed trace on the basic system, we create a predicted trace, by adjusting timestamps for each event, assuming the same event order on both systems. 7 4.2 Test Scenario To demonstrate the applicability of our prediction methodology, we selected two stable programs from the HPF/Fortran-D benchmark suite <ref> [7] </ref>: a Fast Fourier Transform (FFT) and a column-scattered version of Gaussian elimination (GE-Col). Both programs passed our stability tests, and are convenient candidates for evaluation of our trace transformation technique. Any error in the predictions must be due to the transformation phase only.
Reference: [8] <author> Saavedra-Barrera, R. H., Smith, A. J., and Miya, E. </author> <title> Performance prediction by benchmark and machine characterization. </title> <journal> IEEE Transactions on Computers 38, </journal> <month> 12 (December </month> <year> 1989), </year> <pages> 1659-1679. </pages>
Reference-contexts: The main restriction to the extension of methods used on sequential machines (like in <ref> [8] </ref>) to the parallel case is the relatively large semantic gap between high level parallel program code and compiled code. Meanwhile, methods typically used on parallel machines either depend excessively on a particular programming environment (as in [2]), or employ stochastic models [4], now known to be unnecessarily complex [1].
Reference: [9] <author> Sarukkai, S. R., and Malony, A. D. </author> <title> Perturbation analysis of high level instrumentation for SPMD programs. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (San Diego, </booktitle> <month> May </month> <year> 1993), </year> <pages> pp. 44-53. 15 </pages>
Reference-contexts: Meanwhile, methods typically used on parallel machines either depend excessively on a particular programming environment (as in [2]), or employ stochastic models [4], now known to be unnecessarily complex [1]. Perturbation analysis has also been studied in the context of instrumentation intrusion <ref> [5, 9] </ref>. There, however, perturbation models are developed as a tool to compensate perturbations, and approximate the original program behavior when no instrumentation is used. Lyon et al [3] developed performance analysis by inserting synthetic perturbations in a program, and measuring the effect on global performance.
References-found: 9

