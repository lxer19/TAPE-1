URL: http://www.cs.umn.edu/Users/dept/users/gini/8551/arml.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/gini/8551/
Root-URL: http://www.cs.umn.edu
Title: Learning from History for Behavior-Based Mobile Robots in Non-Stationary Conditions  
Author: FRANCOIS MICHAUD AND MAJA J. MATARI C Editor: Henry Hexmoor and Maja J. Mataric 
Keyword: Multi-robot learning, history-based learning, non-stationary conditions, self-evaluation  
Address: (Quebec Canada) J1K 2R1  Los Angeles, CA 90089-0781  
Affiliation: Department of Electrical and Computer Engineering, Universite de Sherbrooke, Sherbrooke  Computer Science Department, University of Southern California,  
Note: 1-29 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: michaudf@gel.usherb.ca, mataric@cs.usc.edu  
Phone: 1  2  
Abstract: Learning in the mobile robot domain is a very challenging task, especially in nonstationary conditions. The behavior-based approach has proven to be useful in making mobile robots work in real-world situations. Since the behaviors are responsible for managing the interactions between the robots and its environment, observing their use can be exploited to model these interactions. In our approach, the robot is initially given a set of "behavior-producing" modules to choose from, and the algorithm provides a memory-based approach to dynamically adapt the selection of these behaviors according to the history of their use. The approach is validated using a vision- and sonar-based Pioneer I robot in non-stationary conditions, in the context of a multi-robot foraging task. Results show the effectiveness of the approach in taking advantage of any regularities experienced in the world, leading to fast and adaptable specialization for the learning robot. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E. </author> <year> (1988). </year> <title> The Dynamic Structure of Everyday Life. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference-contexts: Discussion The fundamental idea behind the presented algorithm is that the performance of situated or embedded systems is based on their ability to cope with and to exploit the dynamics of interactions with their environment <ref> (Agre, 1988, Brooks, 1991) </ref>. Since we decided to use behaviors for controlling the robot, it was most appropriate to model these interactions using a representation mechanism consistent with the behavior-based framework.
Reference: <author> Albus, J. S. </author> <year> (1991). </year> <title> Outline for a theory of intelligence. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> 21(3) </volume> <pages> 473-509. </pages>
Reference-contexts: This decreases the variation of E (eval) as an option is tried frequently, making it harder to modify it and to change strategies if the option becomes in fact less useful. In those conditions, forgetting may be as important as learning <ref> (Albus, 1991) </ref>. Forgetting paths may lead to exploring some of the choices again and discover changes in the world. Node deletion also serves to regulate memory use.
Reference: <author> Asada, M., Uchibe, E., Noda, S., Tawaratsumida, S. & Hosoda, K. </author> <year> (1994). </year> <title> Coordination of multiple behaviors acquired by a vision-based reinforcement learning, </title> <booktitle> Proc. IEEE/RSJ/GI Int'l Conf. on Intelligent Robots and Systems, </booktitle> <address> Munich, Germany. </address>
Reference-contexts: Many robot learning algorithms use this methodology, mostly by learning to associate percepts with actions to learn a behavior (Floreano & Mondada, 1996, del R. Millan, 1996, Mahadevan & Connell, 1992), or to associate percepts with behaviors (Mataric, 1994b, Mataric, 1997), or both <ref> (Dorigo & Colombetti, 1994, Asada et al., 1994) </ref>. The objective in using behaviors is to try to minimize the learner's state space, and maximize learning at each trial (Mataric, 1994a).
Reference: <author> Brooks, R. A. </author> <year> (1986). </year> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, RA-2(1):14-23. </journal>
Reference-contexts: This implies that learning must take place while underlying situations and conditions (like obstacle avoidance, navigation, tasks) are being managed in real-time. Behavior-based systems <ref> (Brooks, 1986, Maes, 1989, Mataric, 1992b) </ref> have been praised for their robustness and simplicity of construction. They have been shown to be effective in various domains of mobile robot control, allowing the systems to adapt to the dynamics of real-world environments. <p> Note that a behavior can be activated (selected), but may not have direct control of the robot; the actual control of the robot's effectors is based on the behavior's rules that generate actions and on the overall subsumption organization of the control system <ref> (Brooks, 1986) </ref> (see Section 3). In general, a behavior is considered to be "in use" only when it controls the actions of the robot. <p> In addition, garbage collection freezes the commands sent to the robot for 1 second, making it possible to hit an object or to miss a target during that period. The overall subsumption <ref> (Brooks, 1986) </ref> organization of the behaviors used in our experiments is shown in Figure 3. The behaviors control the velocity and the rotation of the robot, based on sonar readings and visual inputs.
Reference: <author> Brooks, R. A. </author> <year> (1991). </year> <title> Intelligence without representation. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 139-159. </pages>
Reference-contexts: Since we decided to use behaviors for controlling the robot, it was most appropriate to model these interactions using a representation mechanism consistent with the behavior-based framework. Doing so also preserves two key strengths of the behavior-based approach, situatedness and emergence <ref> (Brooks, 1991) </ref>, but also adds a cognitive aspect of the system, allowing it to model its interactions. In our approach, behaviors can be considered to be implicit short-term models of the world, simultaneously capturing the different factors involved in the decision process.
Reference: <author> Brooks, R. A. </author> <year> (1996). </year> <title> MARS: Multiple Agency Reactivity System (Technical Report IS Robotics). </title> <address> Cambridge, MA. </address> <institution> del R. Millan, J. </institution> <year> (1996). </year> <title> Rapid, safe, and incremental learning of navigation strategies. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics|Part B: Cybernetics, </journal> <volume> 26(3) </volume> <pages> 408-420. </pages>
Reference-contexts: Experimental setup and task description Our experiments were performed on a Real World Interface Pioneer I mobile robot (shown on the right of Figure 2) equipped with seven sonars and a Fast Track Vision System. The robot is programmed using MARS (Multiple Agency Reactivity System) <ref> (Brooks, 1996) </ref>, a Lisp-based language for programming multiple concurrent processes and behaviors. The robot on the left in Figure 2 is an IS Robotics R1 robot used in the multi-robot experiments, equipped with infra-red and contact sensors.
Reference: <author> Dorigo, M. & Colombetti, M. </author> <year> (1994). </year> <title> Robot shaping: Developing autonomous agents through learning. </title> <journal> Artificial Intelligence, </journal> <volume> 71(4) </volume> <pages> 321-370. </pages> <note> 29 Floreano, </note> <author> D. & Mondada, F. </author> <year> (1996). </year> <title> Evolution of homing navigation in a real mobile robot. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics|Part B: Cybernetics, </journal> 26(3):396-407. 
Reference-contexts: Many robot learning algorithms use this methodology, mostly by learning to associate percepts with actions to learn a behavior (Floreano & Mondada, 1996, del R. Millan, 1996, Mahadevan & Connell, 1992), or to associate percepts with behaviors (Mataric, 1994b, Mataric, 1997), or both <ref> (Dorigo & Colombetti, 1994, Asada et al., 1994) </ref>. The objective in using behaviors is to try to minimize the learner's state space, and maximize learning at each trial (Mataric, 1994a).
Reference: <author> Fontan, M. S. & Mataric, M. J. </author> <year> (1996). </year> <title> A study of territoriality: the role of critical mass in adaptive task division. </title> <editor> In P. Maes, M. J. Mataric, J.-A. Meyer, J. Pollack & S. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proc. 4th Int'l Conf. on Simulation of Adaptive Behavior, </booktitle> <address> Cape Cod: </address> <publisher> The MIT Press. </publisher>
Reference: <author> Goldberg, D. & Mataric, M. J. </author> <year> (1997). </year> <title> Interference as a tool for designing and evaluating multi-robot controllers. </title> <booktitle> Proc. National Conf. on Artificial Intelligence (AAAI) (pp. </booktitle> <pages> 637-642), </pages> <address> Providence, RI. </address>
Reference-contexts: In the case of robots using a decentralized and distributed framework without explicit communication, it becomes difficult to design an optimization formula that would efficiently represent all possible situations. When communication between robots is used, different group policies or arbitration schemes can be pre-programmed <ref> (Goldberg & Mataric, 1997, Fontan & Mataric, 1996, Mataric, 1994a) </ref>, but the question of knowing which policy to apply according to different factors (like group size and group density) remains.
Reference: <author> Kaelbling, L. P., Littman, M. L. & Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285. </pages>
Reference-contexts: Finally, a general conclusion from the robotics and control applications is that it has proven necessary to supplement the fundamental learning algorithm with additional pre-programmed knowledge in order to make a real system work <ref> (Kaelbling et al., 1996, Mahadevan & Kaelbling, 1996) </ref>. <p> This method bypasses complete autonomy (by giving up tabula rasa learning techniques <ref> (Kaelbling et al., 1996) </ref>) in favor of incorporating bias that accelerates the learning process and then allowing to further refine and alter that initial policy over time, 2 as the dynamics change. <p> Such systems are commonly composed of a collection of "behavior-producing" modules that map environment states into low-level actions, and a gating mechanism that decides, based on the state of the environment, which behavior's action should be switched through and executed <ref> (Kaelbling et al., 1996) </ref>. Many robot learning algorithms use this methodology, mostly by learning to associate percepts with actions to learn a behavior (Floreano & Mondada, 1996, del R. <p> of behavior use Observe 32 8 Follow-side 56 40 Turn-randomly 24 9 Rest 26 13 The results obtained using these environmental configurations show that the different components of the learning algorithm try to establish a compromise between exploration (learning to adapt to noise and changes in the environment) and exploitation <ref> (Kaelbling et al., 1996) </ref> of a stable behavior selection strategy. The evaluation function characterizes the current situation and the past experiences. The tree representation captures sequences of behavior use in a compact fashion to make a decision based on past experiences. <p> Reinforcement learning is defined as the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment, without need 26 ing to specify how the task is to be achieved <ref> (Kaelbling et al., 1996) </ref>. Our work deals with three important issues in reinforcement learning: incomplete and imperfect perception, learning by using prior knowledge, and multi-agent learning (Mahadevan & Kaelbling, 1996). <p> Our work deals with three important issues in reinforcement learning: incomplete and imperfect perception, learning by using prior knowledge, and multi-agent learning <ref> (Mahadevan & Kaelbling, 1996) </ref>. <p> This mechanism is successfully applied to learning to drive in a virtual highway environment. Globally, the U-Tree approach learns a variable-width window that serves simultaneously as a model of the environment and a finite-memory policy <ref> (Kaelbling et al., 1996) </ref>. Comparatively, our algorithm learns a finite-memory strategy of behaviors use and selection, since it uses behaviors as an abstraction. In addition to applying a different performance measure, our approach differs from McCallum's in the selection criteria.
Reference: <author> Maes, P. </author> <year> (1989). </year> <title> The dynamics of action selection. </title> <booktitle> Proc. Int'l Joint Conf. on Artificial Intelligence (IJCAI) (pp. </booktitle> <pages> 991-997), </pages> <address> Detroit, MI. </address>
Reference: <author> Maes, P. & Brooks, R. A. </author> <year> (1990). </year> <title> Learning to coordinate behaviors. </title> <booktitle> Proc. Nat'l Conf. on Artificial Intelligence (AAAI), </booktitle> <volume> Vol. </volume> <pages> 2 (pp. 796-802). </pages>
Reference-contexts: In our experiments, both tasks used the same set of alternative-behaviors, but subset of alternative-behaviors can be specified to limit the search for distinct tasks. Previous approaches to learning behavior selection <ref> (Maes & Brooks, 1990, Mahadevan & Connell, 1992, Mataric, 1997) </ref> used sensory inputs as the selection criterion. In contrast, our approach introduces the use of the stored history within the behavior-based framework, making it possible to learn regularities in the robot's interactions with its environment which are difficult to pre-program.
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1992). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 311-365. </pages>
Reference: <author> Mahadevan, S. & Kaelbling, L. P. </author> <year> (1996). </year> <title> The NSF workshop on reinforcement learning: Summary and observations. </title> <journal> AI Magazine. </journal>
Reference-contexts: Finally, a general conclusion from the robotics and control applications is that it has proven necessary to supplement the fundamental learning algorithm with additional pre-programmed knowledge in order to make a real system work <ref> (Kaelbling et al., 1996, Mahadevan & Kaelbling, 1996) </ref>. <p> Our work deals with three important issues in reinforcement learning: incomplete and imperfect perception, learning by using prior knowledge, and multi-agent learning <ref> (Mahadevan & Kaelbling, 1996) </ref>.
Reference: <author> Mataric, M. J. </author> <year> (1992a). </year> <title> Integration of representation into goal-driven behavior-based robots. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 8(3) </volume> <pages> 46-54. </pages>
Reference-contexts: The computational complexity of this approach is much greater than ours. An approach to using behaviors as the underlying representation for constructing a model within a behavior-based system was done by <ref> (Mataric, 1992a) </ref>. In this work, the navigating mobile robot associated landmarks in the environment with specific behaviors, and connected those behaviors into a network whose topology was isomorphic to the explored environment.
Reference: <author> Mataric, M. J. </author> <year> (1992b). </year> <title> Behavior-based systems: Key properties and implications. </title> <booktitle> Proc. IEEE Int'l Conf. on Robotics and Automation, Workshop on Architectures for Intelligent Control Systems (pp. </booktitle> <pages> 46-54), </pages> <address> Nice, France. </address>
Reference: <author> Mataric, M. J. </author> <year> (1994a). </year> <title> Interaction and intelligent behavior (MIT AI Lab AI-TR 1495). </title> <institution> Mas-sachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference-contexts: Millan, 1996, Mahadevan & Connell, 1992), or to associate percepts with behaviors (Mataric, 1994b, Mataric, 1997), or both (Dorigo & Colombetti, 1994, Asada et al., 1994). The objective in using behaviors is to try to minimize the learner's state space, and maximize learning at each trial <ref> (Mataric, 1994a) </ref>. Initial knowledge for a learning robot can take the form of a set of behaviors, defining its skills for handling the situations encountered in its environment and for accomplishing its goals. Learning is then localized at the gating mechanism.
Reference: <author> Mataric, M. J. </author> <year> (1994b). </year> <title> Reward functions for accelerated learning. </title> <editor> In W. W. Cohen & H. Hirsh, editors, </editor> <booktitle> Proc. 11th Int'l Conf. on Machine Learning (pp. </booktitle> <pages> 181-189), </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kauffman Publishers. </publisher>
Reference-contexts: Many robot learning algorithms use this methodology, mostly by learning to associate percepts with actions to learn a behavior (Floreano & Mondada, 1996, del R. Millan, 1996, Mahadevan & Connell, 1992), or to associate percepts with behaviors <ref> (Mataric, 1994b, Mataric, 1997) </ref>, or both (Dorigo & Colombetti, 1994, Asada et al., 1994). The objective in using behaviors is to try to minimize the learner's state space, and maximize learning at each trial (Mataric, 1994a).
Reference: <author> Mataric, M. J. </author> <year> (1997). </year> <title> Reinforcement learning in the multi-robot domain. </title> <booktitle> Autonomous Robots, </booktitle> <pages> 4(1). </pages>
Reference-contexts: We describe only the results obtained for the Searching Task because they best illustrate the properties of the learning algorithm. A complete description of all of the experimental results can be found in <ref> (Michaud & Mataric, 1997) </ref>. 4.1. Static environment conditions Three different environmental configurations were used for the experiments in static conditions. The first one used three blocks at the corners of the pen, with the home region in the fourth corner. <p> In the case of robots using a decentralized and distributed framework without explicit communication, it becomes difficult to design an optimization formula that would efficiently represent all possible situations. When communication between robots is used, different group policies or arbitration schemes can be pre-programmed <ref> (Goldberg & Mataric, 1997, Fontan & Mataric, 1996, Mataric, 1994a) </ref>, but the question of knowing which policy to apply according to different factors (like group size and group density) remains.
Reference: <author> McCallum, A. K. </author> <year> (1996a). </year> <title> Hidden state and reinforcement learning with instance-based state identification. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics|Part B: Cybernetics, </journal> <volume> 26(3) </volume> <pages> 464-473. </pages>
Reference-contexts: Learning is then localized at the gating mechanism. To the extent of they can be anticipated, initial associations between percepts and behaviors can also be programmed in. However, many real-world tasks involve partial observability, i.e., the state of the environment is incompletely known from the current, immediate percepts <ref> (McCallum, 1996a, McCallum, 1996c) </ref>. One interesting solution to this problem is to use history, i.e., to explicitly take into consideration the time sequence of observations. History information can be learned form percepts and actions (McCallum, 1996b), but for large or continuous spaces it also requires extensive training periods. <p> His approach partitions the state space from raw sensory experiences, and uses memory of features to augment the agent's perceptual inputs. This way, history information allows the representation to be interpreted in the context of the flawed state space <ref> (McCallum, 1996a) </ref>, i.e., to bias this representation according to perceptual constraints. The raw-experience-representing "instances" are encoded in a U-tree, starting from the root node and following the branches labeled by a feature that matches its observations.
Reference: <author> McCallum, A. K. </author> <year> (1996b). </year> <title> Learning to use selective attention and short-term memory in sequential tasks. </title> <editor> In P. Maes, M. J. Mataric, J.-A. Meyer, J. Pollack & S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proc. 4th Int'l Conf. on Simulation of Adaptive Behavior (pp. </booktitle> <pages> 315-324), </pages> <address> Cape Cod: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: One interesting solution to this problem is to use history, i.e., to explicitly take into consideration the time sequence of observations. History information can be learned form percepts and actions <ref> (McCallum, 1996b) </ref>, but for large or continuous spaces it also requires extensive training periods. <p> The behaviors stored the landmark type (similar to our node-type) and compass direction, and spread activation to dynamically direct the robot toward a goal landmark, in a simple form of behavior selection. Representing history information using a tree representation was inspired by the work of <ref> (McCallum, 1996b, McCallum, 1996c) </ref>, concerned with learning for agents with physical and computational restrictions. His approach partitions the state space from raw sensory experiences, and uses memory of features to augment the agent's perceptual inputs.
Reference: <author> McCallum, A. K. </author> <year> (1996c). </year> <title> Reinforcement learning with selective perception and hidden state. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, USA. </institution>
Reference: <author> Michaud, F. </author> <year> (1996). </year> <title> Nouvelle architecture unifiee de controle intelligent par selection inten-tionnelle de comportements. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, Universite de Sherbrooke, Quebec, Canada. </institution>
Reference-contexts: This results in continuous adaptation of what is being "optimized" in the behavior of the robot, without requiring extensive training periods to model changes in nonstationary conditions. Finally, the development of our algorithm was derived from a general control architecture for intelligent agent <ref> (Michaud et al., 1996) </ref> based on dynamic selection of behaviors. The notion of observation of behavior use, called Behavior Exploitation, is used to make the system monitor the proper satisfaction of its "intentions", as expressed by the selected behaviors.
Reference: <author> Michaud, F. </author> <year> (1997). </year> <title> Adaptability by behavior selection and observation for mobile robots. </title> <editor> In R. Roy, P. Chawdry and P. Pants, editors, </editor> <booktitle> Soft Computing in Engineering Design and Manufacturing. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We describe only the results obtained for the Searching Task because they best illustrate the properties of the learning algorithm. A complete description of all of the experimental results can be found in <ref> (Michaud & Mataric, 1997) </ref>. 4.1. Static environment conditions Three different environmental configurations were used for the experiments in static conditions. The first one used three blocks at the corners of the pen, with the home region in the fourth corner. <p> The approach described in the current paper demonstrates that observing behavior use can also be a rich source of information for learning a representation of the robot's interactions with its operating environment and for evaluating its performance, information that can be useful for behavior selection <ref> (Michaud, 1997) </ref>. 7. Conclusion Learning in dynamic and unpredictable environments, as in the multi-robot domain, is a very challenging problem. The goal of our approach is to enable a robot to learn and utilize the interaction dynamics with its environment from self-evaluation of its behavior.
Reference: <author> Michaud, F. & Mataric, M. J. </author> <year> (1997). </year> <title> A history-based learning approach for adaptive robot behavior selection (Technical Report CS-97-192). </title> <institution> Computer Science Department, Volen Center for Complex System, Brandeis University, </institution> <address> Waltham, MA, USA. </address>
Reference-contexts: We describe only the results obtained for the Searching Task because they best illustrate the properties of the learning algorithm. A complete description of all of the experimental results can be found in <ref> (Michaud & Mataric, 1997) </ref>. 4.1. Static environment conditions Three different environmental configurations were used for the experiments in static conditions. The first one used three blocks at the corners of the pen, with the home region in the fourth corner. <p> The approach described in the current paper demonstrates that observing behavior use can also be a rich source of information for learning a representation of the robot's interactions with its operating environment and for evaluating its performance, information that can be useful for behavior selection <ref> (Michaud, 1997) </ref>. 7. Conclusion Learning in dynamic and unpredictable environments, as in the multi-robot domain, is a very challenging problem. The goal of our approach is to enable a robot to learn and utilize the interaction dynamics with its environment from self-evaluation of its behavior.
Reference: <author> Michaud, F., Lachiver, G. & Dinh, C. T. L. </author> <year> (1996). </year> <title> A new control architecture combining reactivity, deliberation and motivation for situated autonomous agent. </title> <editor> In P. Maes, M. J. Mataric, J.-A. Meyer, J. Pollack & S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proc. 4th Int'l Conf. on Simulation of Adaptive Behavior (pp. </booktitle> <pages> 245-254), </pages> <address> Cape Cod: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: This results in continuous adaptation of what is being "optimized" in the behavior of the robot, without requiring extensive training periods to model changes in nonstationary conditions. Finally, the development of our algorithm was derived from a general control architecture for intelligent agent <ref> (Michaud et al., 1996) </ref> based on dynamic selection of behaviors. The notion of observation of behavior use, called Behavior Exploitation, is used to make the system monitor the proper satisfaction of its "intentions", as expressed by the selected behaviors.
Reference: <author> Ram, A. & Santamaria, J. C. </author> <year> (1993). </year> <title> Multistrategy learning in reactive control systems for autonomous robotic navigation. </title> <journal> Informatica, </journal> <volume> 17(4) </volume> <pages> 347-369. </pages>
Reference-contexts: This approach in a compromise between model-free behavior-based reinforcement learning (e.g., for behavior selection) and full model-based work. A related approach, learning from observed behavior activation (but not behavior use), was explored in the case-based reasoning framework in work by <ref> (Ram & Santamaria, 1993) </ref>. The approach learns a behavior activation strategy based on history over a time window from inputs and from past behavior activation. A case-based algorithm is used to retrieve similar cases, adapt the activation of behaviors, and learn new associations or adapt existing cases.
References-found: 27

