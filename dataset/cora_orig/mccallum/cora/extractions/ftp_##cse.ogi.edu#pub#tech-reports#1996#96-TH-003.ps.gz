URL: ftp://cse.ogi.edu/pub/tech-reports/1996/96-TH-003.ps.gz
Refering-URL: ftp://cse.ogi.edu/pub/tech-reports/README.html
Root-URL: http://www.cse.ogi.edu
Title: Explicit N-Best Formant Features for Segment-Based Speech Recognition  
Author: Philipp Schmid 
Degree: A dissertation submitted to the faculty of the Oregon Graduate Institute of Science Technology in partial fulfillment of the requirements for the degree Doctor of Philosophy in Computer Science and Engineering  
Date: 1990  October 1996  
Address: Bern, Switzerland,  
Affiliation: Lizentiat, University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Allen, J. </author> <title> How Do Humans Process and Recognize Speech? IEEE Transactions on Speech and Audio Processing, </title> <journal> vol. </journal> <volume> 2, 4 (1994), </volume> <pages> 567-577. </pages>
Reference: [2] <author> Atal, B., and Hanauer, S. </author> <title> Speech Analysis and Synthesis by Linear Prediction of the Speech Wave. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 50, 2 (1971), </volume> <pages> 637-655. </pages>
Reference-contexts: After describing the algorithm in Sections 5.1 and 5.2 we will evaluate the quality of the proposed algorithm. 5.1 Finding Elementary Tracks There are three popular mechanisms for generating formant candidates for a given sonorant frame: 1. computing the complex roots of a linear predictor polynomial <ref> [2] </ref>, 2. peak picking of a short-time spectral representation [77], 3. analysis by synthesis [66]. Recently, Welling and Ney [89] proposed a formant estimation method based on digital resonators (see Section 8.2.1 for a possible application of this idea in the context of this work).
Reference: [3] <author> Austin, S., Zavaliagkos, G., Makhou, J., and Schwartz, R. </author> <title> Speech Recognition using Segmental Neural Nets. </title> <booktitle> In Proceedings of ICASSP (1992), </booktitle> <pages> pp. 625-628. </pages>
Reference: [4] <author> Barnard, E. </author> <title> Optimization for Training Neural Net. </title> <journal> IEEE Transactions on Neural Network, </journal> <volume> vol. 3, 2 (1992), </volume> <pages> 232-240. </pages>
Reference: [5] <author> Barnard, E., and Cole, R. A. </author> <title> A Neural-Net Training Program based on Conjugate-Gradient Optimization. </title> <type> Tech. Rep. CSE 89-014, </type> <institution> Oregon Graduate Center, </institution> <year> 1989. </year>
Reference: [6] <author> Bishop, C. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: In their SESM system, Leung et al. [54] use neural networks to classify the segment hypotheses. Because standard neural networks (e.g., MLP) estimate posterior probabilities <ref> [6] </ref>, equation 2.2 gets rewritten as: A fl = argmax A;S | -z - phonetic classif ication p (SjX ) segmentation (2.3) Let fb 1 ; b 2 ; :::; b K g be the set of boundaries, generated by the segmentation mechanism, within the hypothesized segment s i .
Reference: [7] <author> Bladon, A., Hendon, C., and Pickering, J. </author> <title> Towards an Auditory Theory of Speaker Normalization . Language Communication, </title> <booktitle> vol. 4 (1984), </booktitle> <pages> 59-69. </pages>
Reference-contexts: This feature was only used in conjunction with the line-segment baseline features, since an interpretation in the context of Legendre polynomials is not directly available. Formant Amplitude and Bandwidth It has been argued by Bladon et al. <ref> [7] </ref>, among others, that the formant locations are not sufficient for vowel perception. They argue that capturing the spectral shape is essential in describing the nature of the vowel sound.
Reference: [8] <author> Brugnara, F., DeMori, R., Giuliani, D., and Omologo, M. </author> <title> Improved Connected Digit Recognition using Spectral Variation Functions. </title> <booktitle> In Proceedings of ICSLP (1992), </booktitle> <pages> pp. 627-630. </pages>
Reference: [9] <author> Bush, M., and Kopec, G. E. </author> <title> Network-Based Connected Digit Recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 35, 10 (1987), </volume> <pages> 1401-1413. </pages>
Reference-contexts: SEGMENTATION 33 transition from the retroflex /r/ into the front vowel /iy/ starts at time 140ms and ends at 180ms. In contrast, the /s/-/iy/ boundary falls in the time range of 360ms to 370ms. A popular approach to segmentation, initially proposed by Bush and Kopec <ref> [9] </ref>, is to consider all possible segmentations. While this is theoretically the optimal solution, it has been acknowledged that the computational requirements make an implementation thereof impractical at this time [22]. Therefore, a subset of the potential boundary locations is generally selected for further processing.
Reference: [10] <author> Calvert, D. </author> <title> Descriptive Phonetics. </title> <publisher> Thieme Inc., </publisher> <year> 1986. </year>
Reference: [11] <author> Carlson, R., and Glass, J. </author> <title> Vowel Classification Based on Analysis-by-Synthesis. </title> <booktitle> In Proceedings of ICSLP (1992), </booktitle> <pages> pp. 575-578. </pages> <note> 93 CHAPTER 8. CONCLUSIONS AND FUTURE WORK 94 </note>
Reference-contexts: Carlson and Glass <ref> [11] </ref> report a 62.5% classification accuracy using averaged Bark spectral vectors and an MLP classifier. They also describe a formant representation based on an analysis-by-synthesis procedure. Averaging the formant locations over segment thirds (3x3 = 9 features) they achieve 56.6% accuracy.
Reference: [12] <author> Chen, S., and Wang, Y. </author> <title> Vector Quantization of Pitch Information in Mandarin Speech. </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 38, 9 (1990), </volume> <pages> 1317-1320. </pages>
Reference-contexts: We chose the Legendre polynomials because they constitute an orthonormal basis set, which results in more robust parameter estimations. The third-order polynomials in Table 6.2 are taken from <ref> [12] </ref>, where M is number of formant trajectory data points fy 1 ; : : : ; y M g available for the parameter estimation.
Reference: [13] <author> Chigier, B., and Brennan, R. </author> <title> Broad Class Network Generation Using a Combination of Rules and Statistics for Speaker Independent Continuous Speech. </title> <booktitle> In Proceedings of ICASSP (1988), </booktitle> <pages> pp. 449-452. </pages>
Reference-contexts: SEGMENTATION 42 phonetic search. As with the formant tracker described in Chapter 5, the true measure of the quality of a segment structure, such as the segment lattice described in this chapter, is reflected by the success of the phonetic recognition. As an approximation, Chigier and Brennan <ref> [13] </ref> use statistics of insertions, deletions, correct segments, and depth to assess the performance of segmentation algorithms. Depth is defined as the total number of segments hypothesized by the algorithm divided by the total number of segments in the hand-labeled transcriptions. <p> Table 3.3: Lattice segment alignment statistics. The dendrogram numbers were computed by Chigier and Brennan. Alignment Statistics System Insertions Deletions Correct Depth Baseline 2.48% 1.60% 98.3% 6.0 + Stop Insertions 2.53% 0.98% 98.9% 6.6 + Augmentation 1.22% 1.08% 98.9% 7.0 Chigier & Brennan <ref> [13] </ref> 11.00% 6.60% 93.3% 1.8 Dendrogram 2.20% 5.80% 94.2% 4.5 The performance numbers by Chigier and Brennan for their rule set and the original dendrogram (computed on a different test set) are provided as a reference.
Reference: [14] <author> Chigier, B., and Leung, H. </author> <title> The Effects of Signal Representations, Phonetic Classification Techniques, and the Telephone Network. </title> <booktitle> In Proceedings of ICSLP (1992), </booktitle> <pages> pp. 97-100. </pages>
Reference-contexts: The best result of 76% is achieved by using 82 acoustic attributes as features to a MLP. Chigier et al. <ref> [14] </ref> and Leung et al. [52] also experimented with various signal representations and classifier technologies. Their best result of 78.0% was obtained using PLP features [33] and a neural network classifier.
Reference: [15] <author> Cole, R., and Muthusamy, Y. </author> <title> Perceptual Studies on Vowels Excised from Continuous Speech. </title> <booktitle> In Proceedings of ICSLP (1992), </booktitle> <pages> pp. 1091-1094. </pages>
Reference-contexts: vowels tend to have on- and offglides of similar, short duration, whereas the lax vowels tend to have relatively long offglides. (Note: We will define F1 interval features in our classification experiments in Section 6.1.) A series of perceptual experiments using the TIMIT database were conducted by Cole et al. <ref> [15] </ref>. They report 54.8% correct vowel identification for the 16 TIMIT vowels when played to subjects in isolation. The identification rate improved to 65.9% when acoustic context was provided to the listener. A related experiment testing speaker normalization effects showed a small but significant increase in listener-labeler agreement.
Reference: [16] <author> Cole, R., Roginski, K., and Fanty, M. </author> <title> English Alphabet Recognition with Telephone Speech. </title> <booktitle> In Proceedings of Eurospeech (1991), </booktitle> <pages> pp. 342-345. </pages>
Reference: [17] <author> Cole, R., Rudnicky, A., Zue, V., and Reddy, D. </author> <title> Speech as Patterns on Paper. In Perception and Production of Fluent Speech,, </title> <editor> R. A. Cole, Ed. </editor> <publisher> Lawrence Erlbaum Assoc., </publisher> <year> 1980, </year> <pages> pp. 3-50. </pages>
Reference-contexts: Spectrogram reading experiments have shown <ref> [17, 20, 92] </ref> that an expert spectrogram reader is capable of locating essentially all segments (97% in continuous speech and 100% for isolated words) found by phoneticians who had access to the acoustics along with the spectrogram.
Reference: [18] <author> Cole, R., Stern, R., and Lasry, M. </author> <title> Performing Fine Phonetic Distinctions: Templates versus Features. In Variability and Invariance in Speech Processes, </title> <editor> J. Perkel and D. Klatt, Eds. </editor> <publisher> Lawrence Erlbaum Assocs., </publisher> <year> 1986, </year> <pages> pp. 325-359. </pages>
Reference: [19] <author> Cole, R., Yan, Y., Mak, B., Fanty, M., and T.Bailey. </author> <title> The Contribution of Consonants versus Vowels to Word Recognition in Fluent Speech. </title> <booktitle> In Proceedings of ICASSP (1996), </booktitle> <pages> pp. 853-856. </pages>
Reference-contexts: The identification rate improved to 65.9% when acoustic context was provided to the listener. A related experiment testing speaker normalization effects showed a small but significant increase in listener-labeler agreement. This study did not test consonants. In a similar series of experiments reported by Cole et al. <ref> [19] </ref>, phonetically balanced TIMIT sentences were altered by replacing the acoustics of either the vowel or consonant segments with white noise. Subjects were then asked to identify the words contained in the utterance.
Reference: [20] <author> Cole, R., and Zue, V. </author> <title> Speech as Eyes See it. In Attention and Performance VIII, </title> <editor> R. Nickerson, Ed. </editor> <publisher> Lawrence Erlbaum Assoc., </publisher> <address> Hillsdale, NJ, </address> <year> 1980, </year> <pages> pp. 475-494. </pages>
Reference-contexts: Spectrogram reading experiments have shown <ref> [17, 20, 92] </ref> that an expert spectrogram reader is capable of locating essentially all segments (97% in continuous speech and 100% for isolated words) found by phoneticians who had access to the acoustics along with the spectrogram.
Reference: [21] <author> Digalakis, V. </author> <title> Segment-based Stochastic Models of Spectral Dynamics for Continuous Speech Recognition. </title> <type> PhD thesis, </type> <institution> Boston University, </institution> <address> Boston, MA, </address> <year> 1992. </year>
Reference-contexts: The knowledge of the location of segment boundaries allows for the use of powerful intra-and supra-segmental features. It is a well-documented fact (see, e.g., <ref> [21] </ref>) that there is a high degree of correlation among parameters of speech frames of a phonetic segment, both in frequency and in time. <p> A state-space dynamical system is used by Digalakis <ref> [21] </ref> to model the trajectories of acoustic attributes (Mel-frequency cepstral coefficients). The model parameters are estimated using the EM algorithm. The likelihood score estimation for each phoneme is based on the innovation process given by the Kalman filtering. Similarly, Goldenthal models the trajectories with a non-parametric function [31].
Reference: [22] <author> Digalakis, V., Ostendorf, M., and Rohlicek, J. </author> <title> Fast Algorithms for Phone Classification and Recognition Using Segment-Based Models. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 40, 12 (1992), </volume> <pages> 2885-2896. </pages>
Reference-contexts: A popular approach to segmentation, initially proposed by Bush and Kopec [9], is to consider all possible segmentations. While this is theoretically the optimal solution, it has been acknowledged that the computational requirements make an implementation thereof impractical at this time <ref> [22] </ref>. Therefore, a subset of the potential boundary locations is generally selected for further processing. In this work, such a segmentation is called an Acoustic Segmentation (AS). In the context of the dendrogram algorithm [28], these segments are called seed regions.
Reference: [23] <author> Digalakis, V., Rohlicek, J., and Ostendorf, M. </author> <title> ML Estimation of a Stochastic Linear System with the EM Algorithm and its Application to Speech Recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 1, 4 (1993), </volume> <pages> 431-442. </pages> <note> CHAPTER 8. CONCLUSIONS AND FUTURE WORK 95 </note>
Reference-contexts: Chigier et al. [14] and Leung et al. [52] also experimented with various signal representations and classifier technologies. Their best result of 78.0% was obtained using PLP features [33] and a neural network classifier. Furthermore, on male-only training and test sets, Digalakis <ref> [23] </ref> achieved 73.9% using his Dynamic System Models. 2.5 Phonetic Recognition Experiments Lee initially used the TIMIT [46] database for his phonetic recognition experiments using SPHINX [50]. Since then, many researchers have used the same task to compare the performance of their own systems.
Reference: [24] <author> Flammia, G., Dalsgaard, P., Andersen, O., and Lindberg, B. </author> <title> Segment based Variable Frame Rate Speech Analysis and Recognition using a Spectral Variation Function. </title> <booktitle> In Proceedings of ICSLP (1992), </booktitle> <pages> pp. 983-986. </pages>
Reference: [25] <author> Forney Jr, G. D. </author> <title> The Viterbi Algorithm. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 61 (1973), </volume> <pages> 268-277. </pages>
Reference-contexts: In segment-based systems, the search is constrained by the segment hypotheses S i 2 S as follows : A fl = argmax A p (AjX ) = argmax A S i 2S In practice, the summation in Equation 2.1 is often replaced by the Viterbi algorithm <ref> [25] </ref> which finds the most likely joint segmentation and classification path: A fl = argmax A;S The basic equation 2.2 can be further decomposed depending on the type of classifier used in the segment classification step. <p> presence of a phonetic boundary) 2 ; this explains the factor 1 2 in front of the sum. 7.2 Implementation Issues The search for the optimal path (in the sense of Equation 7.2) through the segment lattice is implemented as a dynamic programming algorithm, similar to the Viterbi search algorithm <ref> [25] </ref> used in most speech recognition systems. The fundamental algorithm is modified to be used in a segment-based framework in the following way: instead of advancing the search horizon one frame at a time, the search hypotheses are updated at each boundary b k .
Reference: [26] <author> Fukunaga, K. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: The matrix A defines the distance metric used. Popular choices are A = I for the Euclidian distance and A = Cov 1 (the inverse of the feature covariance matrix) for the Mahalanobis distance <ref> [26] </ref>. In the past, we have experimented with other distance metrics, such as the Spectral Variation Function and a context-dependent extension thereof. Appendix A contains CHAPTER 3. SEGMENTATION 35 an overview and discussion of those alternatives.
Reference: [27] <author> Gish, H., and Ng, K. </author> <title> A Segmental Speech Model with Applications to Word Spotting. </title> <booktitle> In Proceedings of ICASSP (1993), </booktitle> <pages> pp. 447-450. </pages>
Reference-contexts: This approach is used as the baseline method in this work. It was also used by Leung in his thesis work on TIMIT segment classification [51]. Gish and Ng <ref> [27] </ref> augmented this basic approach by using a quadratic polynomial to model the temporal evolution of the cepstral coefficients c (n): c (n) = b 1 + b 2 n + b 3 n 2 + e (n) for n = 1; : : : ; N; e (n) ~ N
Reference: [28] <author> Glass, J. </author> <title> Finding Acoustic Regularities in Speech: Applications to Phonetic Recognition. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1988. </year>
Reference-contexts: Therefore, a subset of the potential boundary locations is generally selected for further processing. In this work, such a segmentation is called an Acoustic Segmentation (AS). In the context of the dendrogram algorithm <ref> [28] </ref>, these segments are called seed regions. Most of the segmentation methods rely on some sort of distance measure to detect spectral changes which would indicate the presence of a phonetic boundary between two adjacent frames (see Appendix A for a discussion of commonly used distance metrics). <p> SEGMENTATION 34 level building stage. MIT's SUMMIT system [94] uses the average spectral distance to merge adjacent regions into a hierarchical structure called a dendrogram <ref> [28] </ref> (see Section 2.1.1 for a summary). The segmentation algorithm developed in this research is similar to the dendrogram algorithm .
Reference: [29] <author> Glass, J. </author> <title> Personal Communications, </title> <publisher> MIT, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: Experiment G used the combination of formant and MFCC features. The context-independent, unigram recognition result obtained by Glass et al. <ref> [29] </ref> is provided as a point of reference. Table 7.3: Phonetic recognition results for various classifiers using a unigram language model. "T" and "M" refer to the training segmentations obtained from either TIMIT or our automatic process respectively (see Section 4.3). <p> 64.5% 61.1% 63.2% 60.1% C MFC Vowel/Consonant T 63.5% 59.6% 62.4% 58.5% D MFC Vowel/Cons T+M 64.9% 61.0% 63.1% 59.4% E Formant-Vowel/ Cons T 65.5% 60.6% 62.5% 57.1% F Formant-Vowel/ Cons T+M 65.5 % 61.1% 63.2% 58.4% G Fmt/MFCC-Vowel/ Cons T+M 66.8 % 62.0% 64.5% 59.7% H Glass et al. <ref> [29] </ref> 61.8% As can be seen from Table 7.3, the phonetic recognition results are slightly worse at this point than the state-of-the-art performance (H). Overall, the performance on the NIST-Core test set is lower by about 1:5 to 2 percentage points.
Reference: [30] <author> Glass, J., Chang, J., and McCandless, M. </author> <title> A Probabilistic Framework for Feature-Based Speech Recognition. </title> <booktitle> In Proceedings of ICSLP (1996), </booktitle> <pages> pp. 2277-2280. </pages>
Reference-contexts: He points out that the REPN [75] system achieves 64.7% if the same scoring mechanism would be used. His system, a continuous variable duration HMM (CVDHMM) with context clustering of quasi-triphonic model states, uses a trigram language model to further improve on the recognition performance. Additionally, Glass et al. <ref> [30] </ref> achieved 69.5% recognition accuracy using anti-phones in a segment-based recognition framework. Note that the results reported in Table 2.4 are established using context-dependent phoneme models in combination with either bigram or trigram language models. <p> We proposed to use the broad category information of the acoustic segments used in the construction of the segment lattice to calculate boundary probabilities p (b k ). As Glass et al. <ref> [30] </ref> have CHAPTER 7. PHONETIC RECOGNITION 82 pointed out, our approach (along with most others described in Chapter 2) is not considering the probability of all the competing hypotheses when computing the best-scoring path through the segment lattice.
Reference: [31] <author> Goldenthal, W. </author> <title> Statistical Trajectory Models for Phonetic Recognition. </title> <type> PhD thesis, </type> <institution> Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, </institution> <year> 1994. </year>
Reference-contexts: The model parameters are estimated using the EM algorithm. The likelihood score estimation for each phoneme is based on the innovation process given by the Kalman filtering. Similarly, Goldenthal models the trajectories with a non-parametric function <ref> [31] </ref>. He defines tracks, which describe the temporal evolution of acoustic attributes over a segment, as a sequence of M state vectors T . <p> It should be noted at this point that in CHAPTER 2. RELATED WORK 23 equation 2.3 posterior probabilities are multiplied, which does not correspond to the usual independence assumption. In his Ph.D. work <ref> [31] </ref>, Goldenthal uses Gaussian segment classifiers, which estimate likelihoods. <p> It should be noted that even though all of the results reported below were established using the TIMIT database, the training and test sets along with the set of recognized phonemes might differ from researcher to researcher. Consult <ref> [31] </ref> for a good overview of these differences. Most of the early segment classification experiments focused on vowel classification. This is generally considered to be the more difficult task compared to consonant classification. Table 2.2 summarizes the vowel classification results obtained by several researchers. CHAPTER 2. <p> This result will serve as a point of reference for our vowel classification experiments using explicit formant features (see Chapter 4). They report further improvements to 65.6% when providing the MLP classifier with (explicit) gender information. Even better results are reported by Goldenthal <ref> [31] </ref>. Using the track representation and a Gaussian classifier, he achieves 66.6% correct classification, and 68.9% when using gender-specific models, where the gender in unknown during testing. Vowels, via the formant structure, carry the most speaker-dependent characteristics which tends to make the classification more difficult. <p> Since then, many researchers have used the same task to compare the performance of their own systems. See Goldenthal's Ph.D. thesis <ref> [31] </ref> for a good overview of the best results achieved. He points out the differences in training and test sets used by various sites. We summarize the results in Table 2.4 without repeating the differences here. CHAPTER 2. <p> CHAPTER 7. PHONETIC RECOGNITION 76 All of our recognition experiments will be in a context-independent mode, since all of our classifiers model context-independent units (phonemes). See Goldenthal <ref> [31] </ref> for a discussion of implementation issues related to context-dependent recognition. The selection of the appropriate segment classifier in the case of specialized version for vowels and consonants is guided by the information attached to each segment by the lattice-generation algorithm.
Reference: [32] <author> Hampshire, J., and Pearlmutter, B. </author> <title> Equivalence Proofs for Multi-Layer Perceptron Classifiers and the Bayesian Discriminant Function. </title> <booktitle> In Proceedings of the 1990 Connectionist Models Summer School (San Mateo, </booktitle> <address> CA, </address> <year> 1990), </year> <editor> D. Touretzky, J. Elman, T. Seijnowski, and G. Hinton, Ed., </editor> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 157-172. </pages>
Reference-contexts: In theory, the outputs of the MLP classifiers are posterior probabilities <ref> [32] </ref> and should be comparable, but in practice the assumptions regarding size of training data and classifier resources are not met and hence the outputs are not true posterior probabilities. Future research into the rescaling of MLP outputs is needed to solve this problem properly.
Reference: [33] <author> Hermanski, H. </author> <title> Perceptual Linear Predictive (PLP) analysis of speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 87, 4 (1990), </volume> <pages> 1738-1752. </pages>
Reference-contexts: The best result of 76% is achieved by using 82 acoustic attributes as features to a MLP. Chigier et al. [14] and Leung et al. [52] also experimented with various signal representations and classifier technologies. Their best result of 78.0% was obtained using PLP features <ref> [33] </ref> and a neural network classifier. Furthermore, on male-only training and test sets, Digalakis [23] achieved 73.9% using his Dynamic System Models. 2.5 Phonetic Recognition Experiments Lee initially used the TIMIT [46] database for his phonetic recognition experiments using SPHINX [50].
Reference: [34] <author> Hermansky, H., and Cox, L. </author> <title> Perceptual Linear Predictive (PLP) Analysis-Resynthesis Technique. </title> <booktitle> In Proceedings of Eurospeech (1991), </booktitle> <pages> pp. 415-418. </pages>
Reference-contexts: The success of this approach depends critically on the performance of the formant tracker, the algorithm which extracts formant frequency information (from now on called "formants"). Estimating the formants based on short-term spectral analysis is straightforward if the local information is pronounced (e.g. <ref> [34] </ref>). However, in practice we often have to deal with a flat short-time spectrum, where the information regarding the location of the formants can only be reconstructed from context by "tracking" the formants. Taking a global view allows the algorithm to compensate for incomplete or ambiguous local information.
Reference: [35] <author> Hetherington, I. L., Phillips, M., Glass, J., and Zue, V. </author> <title> A* Word Network Search for Continuous Speech Recognition. </title> <booktitle> In Proceedings of Eurospeech (1993), </booktitle> <pages> pp. 1553-1536. </pages>
Reference-contexts: The SUMMIT system uses a frame-synchronous Viterbi search in the first pass of a two pass search. The second-pass A* search uses the estimates from the initial Viterbi search to estimate the remaining costs needed to run the A* search efficiently <ref> [35] </ref>. The SESM system and the system described in this thesis (SWISS) advance the search not frame-synchronously, but rather from boundary hypothesis to boundary hypothesis, hence a boundary-synchronous Viterbi search.
Reference: [36] <author> Hu, Z., J.Schalkwyk, Barnard, E., and Cole, R. </author> <title> Speech Recognition using Syllable-like Units. </title> <booktitle> In Proceedings of ICSLP (1996), </booktitle> <pages> pp. 1117-1120. </pages> <note> CHAPTER 8. CONCLUSIONS AND FUTURE WORK 96 </note>
Reference: [37] <author> Jones, M., and Woodland, P. C. </author> <title> Using Relative Duration in Large Vocabulary Speech Recognition. </title> <booktitle> In Proceedings of Eurospeech (1993), </booktitle> <pages> pp. 311-314. </pages>
Reference-contexts: Relative Durations as Classification Features It has been observed in the literature that the speaking rate is an important source of recognition errors. Normalization schemes for speaking rate in form of post-processing algorithms have been proposed <ref> [37] </ref>. Having access to the structure of the segment lattice would allow for the direct incorporation of relative duration information into the classification process. Training on Errors As suggested in Section 4.4, training the segment classifiers on false positives should result in a better phonetic recognition performance.
Reference: [38] <author> Kass, M., Witkin, A., and Terzopoulos, D. Snakes: </author> <title> Active Contour Models. </title> <booktitle> International Journal of Computer Vision (1987), </booktitle> <pages> 321-331. </pages>
Reference-contexts: tracking algorithm is to optimize a cost function E that incorporates aspects of closeness to the data (explaining the most energy) and regularity of the formant tracks: E = E F ormant + E Smoothness : The above functional is minimized by using an algorithm proposed by Kass et al. <ref> [38] </ref>. To avoid formant merges by the regularization algorithm, peak enhancement algorithms are applied where needed.
Reference: [39] <author> Klatt, D. </author> <title> Review of ARPA Speech Understanding Project. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 62, suppl. 1 (1977), </volume> <pages> 1345-1366. </pages>
Reference: [40] <author> Klatt, D. </author> <title> Prediction of Perceived Phonetic Distance from Critical-Band Spectra: a First Step. </title> <booktitle> In Proceedings of ICASSP (1982), </booktitle> <pages> pp. 1278-1281. </pages>
Reference-contexts: Additionally, we also included formant amplitude and formant bandwidth in our experiments, despite the fact that Klatt has concluded from his experiments <ref> [40] </ref> that these features are less important than formant location. 61 CHAPTER 6. EXPLICIT FORMANT FEATURES 62 The classification performance including these additional features in combination with either basic approximation (line segments, Legendre polynomials) are summarized in Tables 6.1 and 6.3.
Reference: [41] <author> Kopec, G. </author> <title> A Family of Formant Trackers Based on Hidden Markov Models. </title> <booktitle> In Proceedings of ICASSP (1986), IEEE, </booktitle> <pages> pp. 1225-1228. </pages>
Reference-contexts: The second algorithm (Section 2.2.2), proposed by Laprie et al. [47], is closely related to the formant tracker described in this thesis. Early work includes formant trackers proposed by McCandless [61] and Kopec <ref> [41, 42] </ref>. McCandless' formant tracking algorithm uses the peaks of the linear prediction spectrum [60] as initial formant candidates. In order to have a good seeding for the tracking hypothesis, the algorithm starts in the middle of a voiced segment and works outwards (towards the boundaries) by applying editing functions.
Reference: [42] <author> Kopec, G. </author> <title> Formant Tracking Using Hidden Markov Models and Vector Quantization. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 34, 4 (1986), </volume> <pages> 709-729. </pages>
Reference-contexts: The second algorithm (Section 2.2.2), proposed by Laprie et al. [47], is closely related to the formant tracker described in this thesis. Early work includes formant trackers proposed by McCandless [61] and Kopec <ref> [41, 42] </ref>. McCandless' formant tracking algorithm uses the peaks of the linear prediction spectrum [60] as initial formant candidates. In order to have a good seeding for the tracking hypothesis, the algorithm starts in the middle of a voiced segment and works outwards (towards the boundaries) by applying editing functions. <p> However, he believes that Kopec's <ref> [42] </ref> implementation of a global constraint that formants be continuous is not the best solution, since a global constraint tends to be too weak in sonorant regions and too strong across vowel-consonant boundaries.
Reference: [43] <author> Kreidler, C. </author> <title> The Pronunciation of English: a Course Book in Phonology. </title> <publisher> Basil Blackwell Inc., </publisher> <year> 1989. </year>
Reference: [44] <author> Krishnan, S., and Rao, P. V. S. </author> <title> Segmental Phoneme Recognition using Piecewise Linear Regression. </title> <booktitle> In Proceedings of ICASSP (1994), </booktitle> <pages> pp. 49-52. </pages>
Reference-contexts: The literature review in Chapter 2 explains some of the methods used by other researchers to approximate the trajectories of cepstral or spectral features. In this work, we use the Piecewise Linear Regression (PLR) algorithm proposed by Krishnan and Rao <ref> [44] </ref> to approximate each formant track by three line segments. The PLR algorithm, summarized in Figure 6.1 generally converges to the optimal solution within 3 to 5 iterations. In practice, we have found the computational load to be negligible.
Reference: [45] <author> Ladefoged, P. </author> <title> A Course in Phonetics, third edition ed. </title> <publisher> Harcourt Brace Jovanovich College Publishers, </publisher> <year> 1993. </year>
Reference: [46] <author> Lamel, L., R.Kassel, and Seneff, S. </author> <title> Speech Database Development: Design and Analysis of the Acoustic-Phonetic Corpus. </title> <booktitle> In Proc. DARPA Speech Recognition Workshop (1986), </booktitle> <pages> pp. 100-109. </pages>
Reference-contexts: Their best result of 78.0% was obtained using PLP features [33] and a neural network classifier. Furthermore, on male-only training and test sets, Digalakis [23] achieved 73.9% using his Dynamic System Models. 2.5 Phonetic Recognition Experiments Lee initially used the TIMIT <ref> [46] </ref> database for his phonetic recognition experiments using SPHINX [50]. Since then, many researchers have used the same task to compare the performance of their own systems. See Goldenthal's Ph.D. thesis [31] for a good overview of the best results achieved. <p> Chapter 4 Segment Classification In this thesis, we report on phoneme classification experiments using the TIMIT <ref> [46] </ref> database. A series of classifiers with increasing complexity, from a baseline system using static features to separate vowel and consonant classifiers using explicit formant features, are described.
Reference: [47] <author> Laprie, Y. </author> <title> Optimum Spectral Peak Teack Interpretation in Terms of Formants. </title> <booktitle> In Proceedings of ICSLP (1990), </booktitle> <pages> pp. 1261-1264. </pages>
Reference-contexts: The first (Section 2.2.1) is the state-of-the-art formant tracker proposed by Talkin [83] in the mid 1980's and implemented as part of the commercial ESPS toolkit [84]. The second algorithm (Section 2.2.2), proposed by Laprie et al. <ref> [47] </ref>, is closely related to the formant tracker described in this thesis. Early work includes formant trackers proposed by McCandless [61] and Kopec [41, 42]. McCandless' formant tracking algorithm uses the peaks of the linear prediction spectrum [60] as initial formant candidates. <p> He further outlines improvements to the stationarity measure and the initial signal processing stage to better cope with female voices and formant merges. 2.2.2 CRIN / INRIA 2-Pass Formant Tracker The main objective for the formant tracker proposed by Laprie et al. <ref> [47] </ref> is to find the set of formant locations that explains the most energy. The algorithm was later extended to add a smoothness term to the objective function [49]. The basic algorithm uses five processing steps.
Reference: [48] <author> Laprie, Y. </author> <title> A New Paradigm for Reliable Automatic Formant Tracking. </title> <booktitle> In Proceedings of ICASSP (1992), </booktitle> <pages> pp. 201-204. </pages>
Reference-contexts: Repeat steps 3 and 4 for all sub-segments until the end of the sonorant region is reached. Generally, formant-tracking algorithms try to find a good trade-off between maximizing the amount of energy "explained" (sum of formant amplitudes) by a given interpretation and some sort of smoothness constraint (e.g. <ref> [48] </ref>, [83]). Our goal is to find consistent interpretations of the formant information as represented by the elementary CHAPTER 5. ROBUST, N -BEST FORMANT TRACKING 56 Initialization Rules 1.
Reference: [49] <author> Laprie, Y., and Berger, M. </author> <title> A New Paradigm for Reliable Automatic Formant Tracking. </title> <booktitle> In Proceedings of ICASSP (1994), </booktitle> <pages> pp. 201-204. </pages>
Reference-contexts: The algorithm was later extended to add a smoothness term to the objective function <ref> [49] </ref>. The basic algorithm uses five processing steps. First, elementary tracks (formants or parts thereof ) are identified in a process similar to the first stage of the formant tracker described in the thesis (Chapter 5). <p> Laprie et al. report encouraging results of their formant tracking algorithm using visual inspection of the formant tracks overlayed on a wideband spectrogram for stop-vowel tokens for 4 male speakers. They also report that the algorithm runs in a few times realtime on a moderately powerful workstation <ref> [49] </ref>. CHAPTER 2. RELATED WORK 27 2.3 Perceptual Experiments and Formant Theory In this section we briefly sketch some results reported in the literature describing vowel perceptual experiments and theories of (vowel) perception.
Reference: [50] <author> Lee, K. </author> <title> Large Vocabulary Speaker-Independent Continuous Speech Recognition: The SPHINX System. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <year> 1988. </year> <note> CHAPTER 8. CONCLUSIONS AND FUTURE WORK 97 </note>
Reference-contexts: Furthermore, on male-only training and test sets, Digalakis [23] achieved 73.9% using his Dynamic System Models. 2.5 Phonetic Recognition Experiments Lee initially used the TIMIT [46] database for his phonetic recognition experiments using SPHINX <ref> [50] </ref>. Since then, many researchers have used the same task to compare the performance of their own systems. See Goldenthal's Ph.D. thesis [31] for a good overview of the best results achieved. He points out the differences in training and test sets used by various sites. <p> (OGI-Dev2) 81 34 115 920 MIT Dev Test (MIT-Dev) 34 16 50 400 NIST Training Set (NIST-Train) 326 136 462 3696 NIST Final Test Set (NIST-Core) 16 8 24 192 The TIMIT labels were reduced to a set of 40 categories (Table 4.2), the 39 phonetic categories suggested by Lee <ref> [50] </ref> and a separate voiced closure class. This division into voiced and voiceless closures was made to reduce the within-category variance. Additionally, the nature of the preceding closure might be used as a feature in a dedicated stop classifier. <p> It has been observed numerous times that the accuracy of phonetic recognition improves when going from context-independent to context-dependent models. In the context of this research, context-dependent modelling could be achieved the "traditional" way <ref> [50] </ref> by creating triphonic models. Alternatively, the broad phonetic labeling of the segment lattice can be used to create (intermediate) broad-phonetic tri-phones (e.g. [closure]-/b/-[vowel]).
Reference: [51] <author> Leung, H. </author> <title> The Use of Artificial Neural Networks for Phonetic Recognition. </title> <type> PhD thesis, </type> <institution> Dept. of Electrical Engineering, Massachusetts Institute of Technology, </institution> <year> 1989. </year>
Reference-contexts: The mean values of the parameters over each third are computed and used as classification features (eventually in combination with the segment duration). This approach is used as the baseline method in this work. It was also used by Leung in his thesis work on TIMIT segment classification <ref> [51] </ref>. <p> RELATED WORK 30 comparison. Table 2.3: Overview: Phonetic Segment Classification Results Segment Classification Results : Phonemes Researcher Feature Set Accuracy Chigier et al. MLP & PLP 78.0% Goldenthal Tracks 76.8% Digalakis Dynamic System Models 73.9% Leung Acoustic Attributes 76.0% Leung in his Ph.D. thesis <ref> [51] </ref> experimented with k-Nearest-Neighbor and Gaussian classifiers as well as neural networks in the context of phonetic segment classification. He reports achieving 72% correct phoneme classification on TIMIT for 38 phonemes [53] using the synchrony envelopes and mean-rate responses of Seneff's auditory model [80] along with the segment duration. <p> The classifier architecture used in this research is the Multilayer Perceptron (MLP), trained with a conjugent-gradient optimization algorithm ([4, 5]). We believe that the discriminant nature of this classifier is a major advantage over other classification methods. In his Ph.D. thesis, Leung <ref> [51] </ref> compared various classifier technologies on a phonetic segment classification task and found MLPs to perform the best. In our work, we have noticed a discrepancy between the training and testing conditions for these classifiers. <p> The classification features are similar to the ones proposed by Leung <ref> [51] </ref>. They are static in nature: the average MFCC, energy and zero crossings of the segment thirds and selected frames to the left and right of the current segment (Figure 4.1). The dimension of the input feature vector for the neural network classifier is 113. Baseline Features 1.
Reference: [52] <author> Leung, H., Chigier, B., and Glass, J. </author> <title> A Comparative Study of Signal Representations and Classification Techniques for Speech Recognition. </title> <booktitle> In Proceedings of ICASSP (1994), </booktitle> <pages> pp. 680-683. </pages>
Reference-contexts: The best result of 76% is achieved by using 82 acoustic attributes as features to a MLP. Chigier et al. [14] and Leung et al. <ref> [52] </ref> also experimented with various signal representations and classifier technologies. Their best result of 78.0% was obtained using PLP features [33] and a neural network classifier.
Reference: [53] <author> Leung, H., Glass, J., Phillips, M., and Zue, V. </author> <title> Detection and Classification of Phonemes Using Context-Independent Error Back-Propagation. </title> <booktitle> In Proceedings of ICSLP (1990), </booktitle> <pages> pp. 1061-1064. </pages>
Reference-contexts: He reports achieving 72% correct phoneme classification on TIMIT for 38 phonemes <ref> [53] </ref> using the synchrony envelopes and mean-rate responses of Seneff's auditory model [80] along with the segment duration. The best result of 76% is achieved by using 82 acoustic attributes as features to a MLP.
Reference: [54] <author> Leung, H., Hetherington, I., and Zue, V. </author> <title> Speech Recognition Using Stochastic Explicit-Segment Modeling. </title> <booktitle> In Proceedings of Eurospeech (1991), </booktitle> <pages> pp. 212-215. </pages>
Reference-contexts: In their SESM system, Leung et al. <ref> [54] </ref> use neural networks to classify the segment hypotheses. <p> The term p (SjX) or alternatively p (S) is generally approximated by a global constant. This constant is used to control the insertion and deletion rates of a recognizer 1 . One notable exception are Leung et al. <ref> [54] </ref>. They propose to estimate the probability of a segment by multiplying the probabilities of the external and internal boundaries of a segment. They estimate the boundary probabilities using a separately trained MLP classifier. 1 Insertions and deletions factor into the recognition accuracy score. See Section 7.3 for details.
Reference: [55] <author> Leung, H., Hetherington, I., and Zue, V. </author> <title> Speech Recognition using Stochastic Segment Neural Networks. </title> <booktitle> In Proceedings of ICASSP (1992), </booktitle> <pages> pp. 613-616. </pages>
Reference-contexts: Then the probability of the segment s i is the product of probabilities p (b k jX ) that those internal boundaries are not true boundaries. Hence p (SjX) = i N Y Y p (b k jX ) In <ref> [55] </ref>, Leung et al. extended this basic approach to context-dependent phone models and broad category boundary classifications. It should be noted at this point that in CHAPTER 2. RELATED WORK 23 equation 2.3 posterior probabilities are multiplied, which does not correspond to the usual independence assumption. <p> Therefore, Equation 7.2 is simplified to find the most likely joint segmentation and labeling: A fl = argmax A;S i Because our segment classifiers (MLP) estimate posterior probabilities, we can expand Equation 7.3 as follows (analogous to the expansion of Leung et al. <ref> [55] </ref>): A fl = argmax A;S i | -z - phonetic classification p (S i jX) segmentation : (7.4) The first term in Equation 7.4 represents the segment classification task.
Reference: [56] <author> Levinson, S. </author> <title> Speech Recognition Technology: a Critique. In Voice Communication Between Humans and Machines (Washington D.C., 1994), </title> <publisher> National Academy Press, </publisher> <pages> pp. 159-164. </pages>
Reference: [57] <author> Lindblom, B., and Studdert-Kennedy, M. </author> <title> On the Role of Formant Transitions in Vowel Recognition. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 42 (1967), </volume> <pages> 830-843. </pages>
Reference-contexts: RELATED WORK 28 Studdert-Kennedy <ref> [57] </ref> addressed the target undershoot problem by demonstrating that perceptual boundaries shifted as a function of the syllable duration (in a CVC context) and the direction of the F2 transition. In related research in the early 1960's, Peterson and Lehiste [69] report that vowels differ in their intrinsic durations.
Reference: [58] <author> Lippmann, R. </author> <title> Speech Perception by Humans and Machines. </title> <booktitle> In Proceedings of the Workshop on the Auditory Basis of Speech Perception (1996), </booktitle> <editor> W. Ainsworth and S. Greenberg, Eds., </editor> <publisher> Keele University Press, </publisher> <pages> pp. 309-316. </pages>
Reference: [59] <author> Ljolje, A. </author> <title> High Accuracy Phone Recognition using Context Clustering and Quasi-Triphonic Models. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 8, 1 (1994), </volume> <pages> 129-151. </pages>
Reference-contexts: Phonetic Recognition Results System Accuracy (39 Phones) (47 Phones) HMM - Lamel & Gauvin 69.1% SUMMIT - Phillips & Glass 68.5% Anti-Phones Glass et al. 69.5% STM Goldenthal 69.5% REPN - Robinson 73.9% 64.7% CVDHMM - Ljolje 69.4% Lately, Ljolje <ref> [59] </ref> has reported a recognition accuracy of 69.4% on a 49 phone set and a slightly different scoring of silence and closure segments. He points out that the REPN [75] system achieves 64.7% if the same scoring mechanism would be used.
Reference: [60] <author> Makhoul, J. </author> <title> Linear Prediction: A Tutorial Review. </title> <journal> Proc. IEEE, </journal> <volume> vol. 63, 2 (1975), </volume> <pages> 561-580. </pages>
Reference-contexts: The second algorithm (Section 2.2.2), proposed by Laprie et al. [47], is closely related to the formant tracker described in this thesis. Early work includes formant trackers proposed by McCandless [61] and Kopec [41, 42]. McCandless' formant tracking algorithm uses the peaks of the linear prediction spectrum <ref> [60] </ref> as initial formant candidates. In order to have a good seeding for the tracking hypothesis, the algorithm starts in the middle of a voiced segment and works outwards (towards the boundaries) by applying editing functions.
Reference: [61] <author> McCandless, S. </author> <title> An Algorithm for Automatic Formant Extraction Using Linear Prediction Spectra. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 22 (1974), </volume> <pages> 135-141. </pages>
Reference-contexts: The second algorithm (Section 2.2.2), proposed by Laprie et al. [47], is closely related to the formant tracker described in this thesis. Early work includes formant trackers proposed by McCandless <ref> [61] </ref> and Kopec [41, 42]. McCandless' formant tracking algorithm uses the peaks of the linear prediction spectrum [60] as initial formant candidates.
Reference: [62] <author> Meng, H. </author> <title> The Use of Distinctive Features for Automatic Speech Recognition. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1991. </year> <note> CHAPTER 8. CONCLUSIONS AND FUTURE WORK 98 </note>
Reference-contexts: Feature Set Accuracy Meng Auditory Model 64.5% Carlson & Glass Formants from Analysis-by-Synthesis 62.6% + gender information 65.6% Goldenthal Tracks of cepstral features 66.6% + gender-specific models 68.9% Using the outputs of an auditory model, Meng [63] achieved 64.5% on a 13-vowel classification task and 65.5% on a 16-vowel task <ref> [62] </ref>. Carlson and Glass [11] report a 62.5% classification accuracy using averaged Bark spectral vectors and an MLP classifier. They also describe a formant representation based on an analysis-by-synthesis procedure. Averaging the formant locations over segment thirds (3x3 = 9 features) they achieve 56.6% accuracy.
Reference: [63] <author> Meng, H., and Zue, V. </author> <title> Signal Representation Comparison for Phonetic Classification. </title> <booktitle> In Proceedings of ICASSP (1991), </booktitle> <pages> pp. 285-288. </pages>
Reference-contexts: WORK 29 Table 2.2: Overview: Vowel Classification Results Segment Classification Results : Vowels Researcher Feature Set Accuracy Meng Auditory Model 64.5% Carlson & Glass Formants from Analysis-by-Synthesis 62.6% + gender information 65.6% Goldenthal Tracks of cepstral features 66.6% + gender-specific models 68.9% Using the outputs of an auditory model, Meng <ref> [63] </ref> achieved 64.5% on a 13-vowel classification task and 65.5% on a 16-vowel task [62]. Carlson and Glass [11] report a 62.5% classification accuracy using averaged Bark spectral vectors and an MLP classifier. They also describe a formant representation based on an analysis-by-synthesis procedure.
Reference: [64] <author> Mermelstein, P., and Davis, S. </author> <title> Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 23, vol. 1 (1975), </volume> <pages> 67-72. </pages>
Reference: [65] <author> Miller, J. </author> <title> Auditory-Perceptual Interpretation of the Vowel. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 85, 5 (1989), </volume> <pages> 2114-2134. </pages>
Reference-contexts: However, this model of formant targets has two major problems which Strange terms "speaker normalization" and "target undershoot". The formant-ratio theory has been proposed to solve the speaker normalization problem (see Miller <ref> [65] </ref> for a historical review of formant-ratio theories). More recent work has been focusing on psychophysically motivated transformations of formants and fundamental frequencies in an attempt to create a feature space (mostly in the F1 versus F2 plane) with less overlap among the vowel categories for all ages and genders.
Reference: [66] <author> Olive, J. </author> <title> Automatic Formant Tracking by a Newton-Raphson Technique. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 50 (1971), </volume> <pages> 661-670. </pages>
Reference-contexts: Therefore, in his system, he modulates the transition cost, which determines the importance of the continuity constraint, by an estimate of the signal's stationarity. He further points out that there are three basic methods of generating formant peak candidates: analysis-by-synthesis <ref> [66] </ref>, peak picking in a smoothed spectrum obtained by LPC analysis (with cepstral smoothing), and solving for the root of a linear predictor polynomial. Specifically, he chose to use the complex roots of the denominator polynomial of the z transform of a linear predictor as the source of formant candidates. <p> will evaluate the quality of the proposed algorithm. 5.1 Finding Elementary Tracks There are three popular mechanisms for generating formant candidates for a given sonorant frame: 1. computing the complex roots of a linear predictor polynomial [2], 2. peak picking of a short-time spectral representation [77], 3. analysis by synthesis <ref> [66] </ref>. Recently, Welling and Ney [89] proposed a formant estimation method based on digital resonators (see Section 8.2.1 for a possible application of this idea in the context of this work).
Reference: [67] <author> Oppenheim, A., and Schafer, R. </author> <title> Discrete-Time Signal Processing. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Repeat Step 2 until either the segment boundary locations are no longer chang ing or until the regression error is below a threshold. though these features are redundant with one another in a cascaded formulation of vowel production <ref> [67] </ref>, we find them to contribute independently in our system. 6.2 Piecewise-Linear Regression A variety of methods to capture the dynamic shape of trajectories in the parameter space have been proposed in the past.
Reference: [68] <author> Ostendorf, M., and Roukos, S. </author> <title> A Stochastic Segment Model for Phoneme-Based Continuous Speech Recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 37, 12 (1989), </volume> <pages> 1857-1869. </pages>
Reference-contexts: The resulting error E = S G is used to estimate the likelihood of segment S being phoneme ff using a Gaussian classifier. A slightly different approach is taken by Ostendorf and Roukos <ref> [68] </ref>: instead of modeling the observed variable length trajectories X of length L and dimension k, they first transform X into a fixed-length representation Y of length M using a linear time-warping function T : Y = X T L : CHAPTER 2. <p> The above basic equation 2.4 is further extended to include transition components of the acoustic score. Similarly, Ostendorf and Roukos <ref> [68] </ref> decompose equation 2.2 into the following terms, where Y is the fixed-length model of X : A fl = argmax A | -z - likelihood estimation p (A) language model L |-z duration model C |-z insertion penalty (2.5) The reader should note that in this case the segmentation is
Reference: [69] <author> Peterson, G., and Lehiste, I. </author> <title> Duration of Syllable Nuclei in English. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 30 (1960), </volume> <pages> 693-703. </pages>
Reference-contexts: RELATED WORK 28 Studdert-Kennedy [57] addressed the target undershoot problem by demonstrating that perceptual boundaries shifted as a function of the syllable duration (in a CVC context) and the direction of the F2 transition. In related research in the early 1960's, Peterson and Lehiste <ref> [69] </ref> report that vowels differ in their intrinsic durations. They also noticed systematic differences in the relative durations of onglides (formant transitions into the syllable nucleus), offglides, and quasi-steady-state portions of CVC syllables for tense and lax vowels. <p> The pitch track of each utterance is smoothed by a 5-point median filter. The pitch feature is defined as the the averaged pitch estimates over the middle 80% of the segment. Onglides and Offglides (F1 Intervals) As suggested by Peterson and Lehiste <ref> [69] </ref>, symmetry and duration of the onglides (formant transition into the vowel nucleus) and offglides of F1 in particular are important features for the tense/lax vowel separation.
Reference: [70] <author> Phillips, M. </author> <title> Automatic Discovery of Acoustic Measurements of Acoustic Classification. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 84, Suppl. </volume> <month> 1 </month> <year> (1988), </year> <month> 216. </month>
Reference-contexts: Using block-diagonal correlation matrices makes this approach equivalent to an HMM with a constrained state sequence. The uniqueness of the SUMMIT classification mechanism is not the classifier used (Gaussian classifier), but the features: a set of generalized algorithms <ref> [70] </ref>. These algorithms together with their associated free parameters form a search space. The parameters can be adjusted to optimize the segment classification performance. An example of such an algorithm is the computation of the spectral center of gravity with the lower and upper frequency edges as the free parameters.
Reference: [71] <author> Pinker, S. </author> <title> The Language Instinct. </title> <publisher> Morrow, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference: [72] <author> Poritz, A. </author> <title> Hidden Markov Model: a Guided Tour. </title> <booktitle> In Proceedings of ICASSP (88), </booktitle> <pages> pp. 7-13. </pages>
Reference: [73] <author> Rabiner, L. </author> <title> A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. In Readings in Speech Recognition, </title> <editor> A. Waibel and K. F. Lee, Eds. </editor> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <year> 1990, </year> <pages> pp. 267-296. </pages>
Reference: [74] <author> Rabiner, L., and Schafer, R. </author> <title> Digital Signal Processing of Speech Signals. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1978. </year>
Reference: [75] <author> Robinson, T. </author> <title> Several Improvements to a Recurrent Error Propagation Phone Recognition System. </title> <type> Tech. Rep. </type> <institution> CUED/TINFENG/TR.82, Cambridge University Engineering Dept., </institution> <year> 1991. </year>
Reference-contexts: He points out that the REPN <ref> [75] </ref> system achieves 64.7% if the same scoring mechanism would be used. His system, a continuous variable duration HMM (CVDHMM) with context clustering of quasi-triphonic model states, uses a trigram language model to further improve on the recognition performance.
Reference: [76] <author> Rummelhart, D., and McClelland, J. </author> <title> Parallel Distributed Processing: </title> <journal> Exploration in the Microstructure of Cognition, </journal> <volume> vol. 1. </volume> <publisher> MIT Press, </publisher> <year> 1987. </year> <note> CHAPTER 8. CONCLUSIONS AND FUTURE WORK 99 </note>
Reference: [77] <author> Schafer, R., and Rabiner, L. </author> <title> System for Automatic Formant Analysis of Voiced Speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 57, </volume> <month> 634-648 </month> <year> (1970). </year>
Reference-contexts: Sections 5.1 and 5.2 we will evaluate the quality of the proposed algorithm. 5.1 Finding Elementary Tracks There are three popular mechanisms for generating formant candidates for a given sonorant frame: 1. computing the complex roots of a linear predictor polynomial [2], 2. peak picking of a short-time spectral representation <ref> [77] </ref>, 3. analysis by synthesis [66]. Recently, Welling and Ney [89] proposed a formant estimation method based on digital resonators (see Section 8.2.1 for a possible application of this idea in the context of this work).
Reference: [78] <author> Schmid, P., and Barnard, E. </author> <title> Robust, N-best Formant Tracking. </title> <booktitle> In Proceedings of Eurospeech (1995), </booktitle> <pages> pp. 737-740. </pages>
Reference-contexts: The real tests lie in the application of the tracking algorithm in segment classification and phonetic recognition described in Chapters 6 and 7. In a preliminary study <ref> [78] </ref>, we evaluated the formant tracker on 10 test utterances from the TIMIT database containing 148 sonorant segments (vowels, liquids and glides) for a total of 101 sonorant regions.
Reference: [79] <author> Schmid, P., Cole, R., Fanty, M., Bourlard, H., and Haessen, M. </author> <title> Real-Time, Neural Network-Based, French Alphabet Recognition with Telephone Speech. </title> <booktitle> In Proceedings of Eurospeech (1993), </booktitle> <pages> pp. 1723-1726. </pages>
Reference: [80] <author> Seneff, S. </author> <title> A Joint Synchrony/Mean-Rate Model of Auditory Speech Processing. </title> <journal> Journal of Phonetics, </journal> <volume> vol. 16 (1988), </volume> <pages> 55-76. </pages>
Reference-contexts: He reports achieving 72% correct phoneme classification on TIMIT for 38 phonemes [53] using the synchrony envelopes and mean-rate responses of Seneff's auditory model <ref> [80] </ref> along with the segment duration. The best result of 76% is achieved by using 82 acoustic attributes as features to a MLP. Chigier et al. [14] and Leung et al. [52] also experimented with various signal representations and classifier technologies.
Reference: [81] <author> Stern, P., Eskenazi, M., and Memmi, D. </author> <title> An expert system for speech spectrogram reading. </title> <booktitle> In Proceedings of ICASSP (1986), </booktitle> <pages> pp. 1193-1196. </pages>
Reference-contexts: In addition to the methods described below, there is another line of research focusing on using explicit, knowledge-based features in a rule-based (expert) system framework (see for example <ref> [81] </ref> and [95]). We believe that a combination of features motivated by speech knowledge and statistical classifiers will ultimately yield the best performance.
Reference: [82] <author> Strange, W. </author> <title> Evolving Theories of Vowel Perception. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> vol. 85, 5 (1989), </volume> <pages> 2081-2087. </pages>
Reference-contexts: The purpose of the meeting was to contrast the prevelant theories and to outline open questions and future work. The article by Strange <ref> [82] </ref> provides a good overview of the major theories of vowel perception. The classic textbook model of vowel perception describes the vowel characteristics in terms of formant targets extracted from the spectral cross section of the steady-state portions of the vowel.
Reference: [83] <author> Talkin, D. </author> <title> Speech Formant Trajectory Estimation Using Dynamic Programming with Modulated Transition Costs. </title> <journal> AT&T Internal Memo MH 11222 2924 2D-410, AT&T, </journal> <year> 1987. </year>
Reference-contexts: Note that not all the active hypotheses are updated at every search increment. 2.2 Formant Tracking Algorithms In this section we review two formant tracking algorithms in some detail. The first (Section 2.2.1) is the state-of-the-art formant tracker proposed by Talkin <ref> [83] </ref> in the mid 1980's and implemented as part of the commercial ESPS toolkit [84]. The second algorithm (Section 2.2.2), proposed by Laprie et al. [47], is closely related to the formant tracker described in this thesis. Early work includes formant trackers proposed by McCandless [61] and Kopec [41, 42]. <p> He describes both single-formant and multi-formant models and concludes from experiments on the a digit database that there is no performance advantage from simultaneously tracking multiple formants using his models. 2.2.1 ESPS Formant Tracker In his report, Talkin acknowledges the importance of incorporating non-local constraints into the formant tracking algorithm <ref> [83] </ref>. However, he believes that Kopec's [42] implementation of a global constraint that formants be continuous is not the best solution, since a global constraint tends to be too weak in sonorant regions and too strong across vowel-consonant boundaries. <p> The tracking algorithm is only applied to sonorant regions hence avoiding tracking problems across sonorant / obstruent boundaries as reported by Talkin <ref> [83] </ref>. The formant tracker uses a two-pass algorithm, similar to the work done by Laprie et al. ([47, 48]). The first pass finds individual formant tracks or parts thereof, called elementary tracks (Section 5.1). <p> Repeat steps 3 and 4 for all sub-segments until the end of the sonorant region is reached. Generally, formant-tracking algorithms try to find a good trade-off between maximizing the amount of energy "explained" (sum of formant amplitudes) by a given interpretation and some sort of smoothness constraint (e.g. [48], <ref> [83] </ref>). Our goal is to find consistent interpretations of the formant information as represented by the elementary CHAPTER 5. ROBUST, N -BEST FORMANT TRACKING 56 Initialization Rules 1.
Reference: [84] <author> Talkin, D. </author> <title> ESPS Manual. </title> <institution> Entropic Research Lab., Inc., </institution> <year> 1993. </year>
Reference-contexts: The first (Section 2.2.1) is the state-of-the-art formant tracker proposed by Talkin [83] in the mid 1980's and implemented as part of the commercial ESPS toolkit <ref> [84] </ref>. The second algorithm (Section 2.2.2), proposed by Laprie et al. [47], is closely related to the formant tracker described in this thesis. Early work includes formant trackers proposed by McCandless [61] and Kopec [41, 42].
Reference: [85] <author> Tebelskis, J., and Waibel, A. </author> <title> Performance Through Consistency: MS-TDNN's for Large Vocabulary Continuous Speech Recognition. </title> <booktitle> In Advances in Neural Information Processing 5 (1992), </booktitle> <editor> J. C. S. Hanson and L. Giles, Eds., </editor> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <pages> pp. 696-701. </pages> <note> [86] van Vuuren, </note> <author> S. </author> <title> Pitch Detection. </title> <type> Tech. Rep. </type> <note> (to be published), </note> <institution> Dept. of Computer Science and Engineering, Oregon Graduate Institute of Science and Technology, </institution> <address> Port-land, OR, </address> <year> 1996. </year>
Reference-contexts: The effect is mainly due to a difference in the size of the training sets: OGI-Train contains half of the training material of NIST-Train. 4.3 Training and Testing on Machine-Generated Segments One of the keys to successful classifier design is the matching of training and testing/deployment conditions <ref> [85] </ref>. To this point, the features for the segment classifiers were extracted with the knowledge of the true locations of the phonetic segment boundaries. However, when CHAPTER 4. SEGMENT CLASSIFICATION 48 using the classifiers in a phonetic recognition system, we do not have access to the hand-labeled segmentations.
Reference: [87] <author> Voice Information Associates. </author> <title> Automatic Speech Recognition: A Study of the World-Wide Market, </title> <year> 1995. </year>
Reference: [88] <author> Weinstein, C., McCandless, S., Modshein, L., and Zue, V. </author> <title> A System for Acoustic-Phonetic Analysis of Continuous Speech. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 23, 1 (1975), </volume> <pages> 314-327. </pages>
Reference-contexts: Additionally, Weinstein et al. <ref> [88] </ref> use features based on spectrogram reading experience to classify consonant segments (nasals, frica-tives, and stops). <p> CONCLUSIONS AND FUTURE WORK 92 might improve the N -best performance. Such schemes as introducing "Not-Phoneme /xx/" units into the recognition vocabulary might help in suppressing high output levels for incorrect formant interpretations. Segmentation of Sonorants based on Formant Information Weinstein et al. <ref> [88] </ref> described a two-stage method for segmenting the speech: the first stage segments the speech into one of four broad acoustic categories: vowel-like, volume dip within vowel-like sound, fricative-like, and stops. The information of the formant tracking are then used to further segment the vowel-like sound regions.
Reference: [89] <author> Welling, L., and Ney, H. </author> <title> A Model for Efficient Formant Estimation. </title> <booktitle> In Proceedings of ICASSP (1996), </booktitle> <pages> pp. 797-800. </pages> <note> CHAPTER 8. CONCLUSIONS AND FUTURE WORK 100 </note>
Reference-contexts: Recently, Welling and Ney <ref> [89] </ref> proposed a formant estimation method based on digital resonators (see Section 8.2.1 for a possible application of this idea in the context of this work). <p> F3) the most. Possible Solutions Recently, Welling and Ney proposed a formant estimation method based on digital resonators <ref> [89] </ref>. The dynamic programming algorithm reliably finds exactly k formant candidates per frame (by placing k resonators along the frequency axis). The drawback of this method is that there is not direct estimation of the correct number of formants present in the spectrum.
Reference: [90] <author> Wolf, J., and Woods, W. </author> <title> The HWIM speech understanding system. </title> <booktitle> In Proceedings of ICASSP (1977), </booktitle> <pages> pp. 784-787. </pages>
Reference: [91] <author> Wooters, C., and Morgan, N. </author> <title> Acoustic Subword Models in the Berkeley Restaurant Project. </title> <booktitle> In Proceedings of ICSLP (1992), </booktitle> <pages> pp. 1551-1554. </pages>
Reference-contexts: This is required because the search algorithm will have to account for exactly one phonetic 1 There is the possibility of using the acoustic segments directly for recognition similar as in <ref> [91] </ref>. In this work however, we will use phonemes as the sub-word units for recognition. CHAPTER 3. SEGMENTATION 39 identity for each speech frame (no overlap of phonetic segments and no frames without a covering phonetic segment).
Reference: [92] <author> Zue, V., and Cole, R. </author> <title> Experiments on spectrogram reading. </title> <booktitle> In Proceedings of ICASSP (1979), </booktitle> <pages> pp. 116-119. </pages>
Reference-contexts: Spectrogram reading experiments have shown <ref> [17, 20, 92] </ref> that an expert spectrogram reader is capable of locating essentially all segments (97% in continuous speech and 100% for isolated words) found by phoneticians who had access to the acoustics along with the spectrogram.
Reference: [93] <author> Zue, V., Glass, J., Goodine, D., Leung, H., Phillips, M., Polifroni, J., and Seneff, S. </author> <title> Recent Progress on the SUMMIT System. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop (1990), </booktitle> <publisher> Morgan Kaufmann, Inc., </publisher> <pages> pp. 1-11. </pages>
Reference: [94] <author> Zue, V., Glass, J., and Seneff, S. </author> <title> The MIT SUMMIT Speech Recognition System: A Progress Report. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop (1989), </booktitle> <pages> pp. 179-189. </pages>
Reference-contexts: In practice, this problem is generally solved by setting a low detection threshold and accepting an oversegmentation of the utterance, followed by a grouping or CHAPTER 3. SEGMENTATION 34 level building stage. MIT's SUMMIT system <ref> [94] </ref> uses the average spectral distance to merge adjacent regions into a hierarchical structure called a dendrogram [28] (see Section 2.1.1 for a summary). The segmentation algorithm developed in this research is similar to the dendrogram algorithm .
Reference: [95] <author> Zue, V., and Lamel, L. </author> <title> An Expert Spectrogram Reader: A Knowledge-Based Approach to Speech Recognition. </title> <booktitle> In Proceedings of ICASSP (1986), </booktitle> <pages> pp. 1197-1200. </pages>
Reference-contexts: In addition to the methods described below, there is another line of research focusing on using explicit, knowledge-based features in a rule-based (expert) system framework (see for example [81] and <ref> [95] </ref>). We believe that a combination of features motivated by speech knowledge and statistical classifiers will ultimately yield the best performance.
References-found: 94

