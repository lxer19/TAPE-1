URL: ftp://ftp.cs.ust.hk/pub/dyyeung/paper/yeung.nips95.ps.gz
Refering-URL: http://www.cs.duke.edu/~mlittman/topics/routing-page.html
Root-URL: 
Email: fpmchoi,dyyeungg@cs.ust.hk  
Title: Predictive Q-Routing: A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control  
Author: Samuel P.M. Choi, Dit-Yan Yeung 
Keyword: Q-routing in terms of both learning speed and adaptability.  
Address: Hong Kong  
Affiliation: Department of Computer Science Hong Kong University of Science and Technology Clear Water Bay, Kowloon,  
Abstract: In this paper, we propose a memory-based Q-learning algorithm called predictive Q-routing (PQ-routing) for adaptive traffic control. We attempt to address two problems encountered in Q-routing (Boyan & Littman, 1994), namely, the inability to fine-tune routing policies under low network load and the inability to learn new optimal policies under decreasing load conditions. Unlike other memory-based reinforcement learning algorithms in which memory is used to keep past experiences to increase learning speed, PQ-routing keeps the best experiences learned and reuses them by predicting the traffic trend. The effectiveness of PQ-routing has been verified under various network topologies and traffic conditions. Simulation results show that PQ-routing is superior to 
Abstract-found: 1
Intro-found: 1
Reference: <author> J.A. </author> <title> Boyan & M.L. Littman (1994). Packet routing in dynamically changing networks: a reinforcement learning approach. </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> 671-678. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: Finding the optimal routing policy in such a distributed manner is very difficult. Moreover, since the environment is non-stationary, the optimal policy varies with time as a result of changes in network traffic and topology. In <ref> (Boyan & Littman, 1994) </ref>, a distributed adaptive traffic control scheme based on reinforcement learning (RL), called Q-routing, is proposed for the routing of packets in networks with dynamically changing traffic and topology. <p> In so doing, Q-routing is expected to learn to avoid congestion along popular paths. Although Q-routing is able to alleviate congestion along popular paths by routing some traffic over other (possibly longer) paths, two problems are reported in <ref> (Boyan & Littman, 1994) </ref>. First, Q-routing is not always able to find the shortest paths under low network load. <p> Unless Q-routing continues to explore, the shortest path cannot be chosen again even though the network load has returned to a very low level. However, as mentioned in <ref> (Boyan & Littman, 1994) </ref>, random exploration may have very negative effects on congestion, since packets sent along a suboptimal path tend to increase queue delays, slowing down all the packets passing through this path.
Reference: <author> M. Littman & J. </author> <title> Boyan (1993). A distributed reinforcement learning scheme for network routing. </title> <booktitle> Proceedings of the First International Workshop on Applications of Neural Networks to Telecommunications, </booktitle> <pages> 45-51. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, New Jersey. </address>
Reference: <author> A.W. Moore & C.G. </author> <title> Atkeson (1993). Memory-based reinforcement learning: efficient computation with prioritized sweeping. </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> 263-270. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: It is this very reason which led us to develop the algorithm presented in this paper. 3 PREDICTIVE Q-ROUTING A memory-based Q-learning algorithm called predictive Q-routing (PQ-routing) is proposed here for adaptive traffic control. Unlike Dyna (Peng & Williams, 1993) and prioritized sweeping <ref> (Moore & Atkeson, 1993) </ref> in which memory is used to keep past experiences to increase learning speed, PQ-routing keeps the best experiences (best Q-values) learned and reuses them by predicting the traffic trend. The idea is as follows.
Reference: <author> A.W. Moore & C.G. </author> <title> Atkeson (1993). Prioritized sweeping: reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130. </pages>
Reference-contexts: It is this very reason which led us to develop the algorithm presented in this paper. 3 PREDICTIVE Q-ROUTING A memory-based Q-learning algorithm called predictive Q-routing (PQ-routing) is proposed here for adaptive traffic control. Unlike Dyna (Peng & Williams, 1993) and prioritized sweeping <ref> (Moore & Atkeson, 1993) </ref> in which memory is used to keep past experiences to increase learning speed, PQ-routing keeps the best experiences (best Q-values) learned and reuses them by predicting the traffic trend. The idea is as follows.
Reference: <author> J. Peng & R.J. </author> <title> Williams (1993). Efficient learning and planning within the Dyna framework. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1 </volume> <pages> 437-454. </pages>
Reference-contexts: An algorithm which automatically adapts between exploration and exploitation is therefore necessary. It is this very reason which led us to develop the algorithm presented in this paper. 3 PREDICTIVE Q-ROUTING A memory-based Q-learning algorithm called predictive Q-routing (PQ-routing) is proposed here for adaptive traffic control. Unlike Dyna <ref> (Peng & Williams, 1993) </ref> and prioritized sweeping (Moore & Atkeson, 1993) in which memory is used to keep past experiences to increase learning speed, PQ-routing keeps the best experiences (best Q-values) learned and reuses them by predicting the traffic trend. The idea is as follows.
Reference: <author> S. </author> <title> Thrun (1992). The role of exploration in learning control. In Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, D.A. White & D.A. </title> <editor> Sofge (eds). </editor> <publisher> Van Nostrand Reinhold, </publisher> <address> New York. </address>
Reference-contexts: Moreover, exploration is not done at no cost. This dilemma is well known in the RL community and has been studied by some researchers, e.g. <ref> (Thrun, 1992) </ref>. One possibility is to divide learning into an exploration phase and an exploitation phase. The simplest exploration strategy is random exploration, in which actions are selected randomly without taking the reinforcement feedback into consideration.
Reference: <author> C.J.C.H. </author> <title> Watkins (1989). Learning from delayed rewards. </title> <type> PhD Thesis, </type> <institution> University of Cambridge, </institution> <address> England. </address>
Reference-contexts: In (Boyan & Littman, 1994), a distributed adaptive traffic control scheme based on reinforcement learning (RL), called Q-routing, is proposed for the routing of packets in networks with dynamically changing traffic and topology. Q-routing is a variant of Q-learning <ref> (Watkins, 1989) </ref>, which is an incremental (or asynchronous) version of dynamic programming for solving multistage decision problems.
References-found: 7

