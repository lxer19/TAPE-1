URL: ftp://ftp.aic.nrl.navy.mil/pub/papers/1997/AIC-97-015.ps
Refering-URL: http://www.aic.nrl.navy.mil/~schultz/papers.html
Root-URL: 
Email: moriarty@isi.edu  schultz@aic.nrl.navy.mil  gref@aic.nrl.navy.mil  
Title: Reinforcement Learning through Evolutionary Computation  
Author: David E. Moriarty Alan C. Schultz John J. Grefenstette 
Address: 4676 Admiralty Way, Marina del Rey, CA 90292  DC 20375-5337  
Affiliation: University of Southern California, Information Sciences Institute  Navy Center for Applied Research in Artificial Intelligence Naval Research Laboratory, Washington  
Abstract: This article characterizes the evolutionary algorithm approach to reinforcement learning in relation to the more standard, temporal difference methods. We describe several research issues in reinforcement learning and discuss similarities and differences in how they are addressed by the two methods. A short survey of evolutionary reinforcement learning systems and their successful applications is also presented.
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, C. W. </author> <year> (1989). </year> <title> Learning to control an inverted pendulum using neural networks. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 9, </volume> <pages> 31-37. </pages>
Reference-contexts: Small populations are used to discourage different, competing neural network "species" from forming within the population. Whitley et al. (1993) argue that speciation leads to competing conventions and produces poor offspring when two dissimilar networks are recombined. Whitley et al. (1993) compared GENITOR to the Adaptive Heuristic Critic <ref> (Anderson, 1989, AHC) </ref>, which uses the TD method of reinforcement learning. In several different versions of the common benchmark of balancing an pole on a cart, GENITOR was found to be comparable to the AHC in both learning rate and generalization.
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title>
Reference: <editor> In Gabriel, M., & Moore, J. W. (Eds.), </editor> <title> Learning and Computational Neuroscience. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Boyan, J. A., & Moore, A. W. </author> <year> (1995). </year> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In Tesauro, G., Touretzky, D., & Leen, T. K. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> 21 Chrisman, </note> <author> L. </author> <year> (1992). </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 183-188 San Jose, CA. </address>
Reference-contexts: Generalization in TD methods has produced very mixed results. In some large-scale problems, approximating the value function works well (e.g., Tesauro, 1994), while in some simple toy problems it fails <ref> (Boyan & Moore, 1995) </ref>. One reason that TD methods can have trouble with generalization is because it makes frequent policy updates based on the results of single decisions (immediate reinforcement + estimated value of next state). <p> We predict that this feature makes the EA approach more amenable to generalization through function approximators. We conducted an empirical test of this hypothesis by applying the SANE EA system (described in section 5.4) to the two problems presented in <ref> (Boyan & Moore, 1995) </ref>. Each decision policy (represented by a neural network) in the population was allowed to move the agent for 50 time steps. The agent was only "reset" after it reached the goal as in (Boyan & Moore, 1995). <p> SANE EA system (described in section 5.4) to the two problems presented in <ref> (Boyan & Moore, 1995) </ref>. Each decision policy (represented by a neural network) in the population was allowed to move the agent for 50 time steps. The agent was only "reset" after it reached the goal as in (Boyan & Moore, 1995). The total penalties acquired by the agent over 50 time steps determined the fitness of a decision policy. 13 (a) (b) actions: up, down, right, or left. Each action moves the agent 0.05 + noise from a Gaussian distribution with a 0.01 standard deviation.
Reference: <author> Cobb, H. G., & Grefenstette, J. J. </author> <year> (1993). </year> <title> Genetic algorithms for tracking changing environments. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms (ICGA 93), </booktitle> <pages> pp. 84-91. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dayan, P., & Sejnowski, T. J. </author> <year> (1996). </year> <title> Exploration bonuses and dual control. </title> <journal> Machine Learning, </journal> <volume> 25, </volume> <pages> 5-22. </pages>
Reference: <author> Fogel, L. J., Owens, A. J., & Walsh, M. J. </author> <year> (1966). </year> <title> Artificial Intelligence through Simulated Evolution. </title> <publisher> Wiley Publishing, </publisher> <address> New York. </address>
Reference-contexts: We do not restrict evolutionary algorithm reinforcement learning to a specific evolutionary algorithm. Methods from genetic algorithms (Holland, 1975; Goldberg, 1989), evolutionary programming <ref> (Fogel, Owens, & Walsh, 1966) </ref>, genetic programming (Koza, 1992), or evolutionary strategies (Rechenberg, 1964) could all be used in this framework to form effective decision-making agents. However, in this paper we do restrict our attention to those implementations of evolutionary algorithms that solve reinforcement learning problems.
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: Maintaining effective levels of population diversity is thus an important feature of almost all EA approaches. This is in contrast to the function optimization EA, where convergence is less problematic and often encouraged <ref> (Goldberg, 1989) </ref>. 4.2 Perceptual Aliasing Probably the most problematic research issue facing reinforcement learning methods today concerns policy decisions from limited sensory information. In real world situations, the decision-making agent often will not have access to complete information on the state of its world.
Reference: <author> Grefenstette, J. J. </author> <year> (1996). </year> <title> Genetic learning for adaptation in autonomous robots. </title> <booktitle> In Robotics and Manufacturing: Recent Trends in Research and Applications, </booktitle> <volume> Volume 6, </volume> <pages> pp. 265-270. </pages> <publisher> ASME Press, </publisher> <address> New York. </address>
Reference-contexts: SAMUEL took a human-written rule set that could reach the goal within a limited time without hitting an obstacle only 70 percent of the time, and after 50 generations was able to obtain a 93.5 percent success rate. In <ref> (Schultz & Grefenstette, 1996) </ref>, the robot learned to herd a second robot to a "pasture". In this task, the learning system used the range and bearing to the second robot, the heading of the second robot, and the range and bearing to the goal, as its input sensors. <p> SAMUEL was given an initial, human-designed rule set with a performance of 27 percent, and after 250 generations was able to move the second robot to the goal 86 percent of the time. In <ref> (Grefenstette, 1996) </ref> the SAMUEL EA system is combined with case-based learning to address the adaptation problem. In this approach, called anytime learning, the learning agent interacts both with the external environment and with an internal simulation.
Reference: <author> Grefenstette, J. J., Ramsey, C. L., & Schultz, A. C. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 355-381. </pages>
Reference-contexts: EA methods have been successful using a variety of function approximators including neural networks (Whitley, Dominic, Das, & Anderson, 1993; Moriarty & Miikkulainen, 1996a), symbolic rule sets <ref> (Grefenstette et al., 1990) </ref>, and Lisp S-expressions (Koza, 1992). Generalization in TD methods has produced very mixed results. In some large-scale problems, approximating the value function works well (e.g., Tesauro, 1994), while in some simple toy problems it fails (Boyan & Moore, 1995). <p> Future work may resolve many of the difficulties in the LCS and allow it to achieve the combined benefits of the two approaches. Future work may also uncover more efficient combinations of EA and TD methods. 5.2 SAMUEL SAMUEL <ref> (Grefenstette et al., 1990) </ref> is a system that learns to solve sequential decision problems through trial and error. Using a hybrid algorithm that combines Darwinian and Lamarckian evolution with aspects of temporal difference reinforcement learning, SAMUEL searches the space of decision policies for sets of condition-action rules.
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, </title> <booktitle> Control and Artificial Intelligence. </booktitle> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <booktitle> In Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. 2. </volume> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference-contexts: Notable exceptions are the work of Dayan and Sejnowski (1996) and Cobb and Grefenstette (1993) 15 An LCS uses an evolutionary algorithm to evolve symbolic if-then rules called classifiers that map sensory input to an appropriate action. Figure 8 outlines Holland's LCS framework <ref> (Holland, 1986) </ref>. When sensory input is received, it is posted on the message list. If the left hand side of a classifier matches a message on the message list, its right hand side is posted on the message list. <p> Because of memory and complexity issues, normally only a subset of the matching classifiers are allowed to post their messages to the message list. Determining which classifiers should activate is typically handled by a bidding system known as Holland's bucket brigade algorithm <ref> (Holland, 1986) </ref>. In the bucket brigade algorithm, classifiers maintain a strength measure and use their strength to bid against other classifiers to post their messages. Classifiers are thus in direct competition with each other to post their messages.
Reference: <author> Holland, J. H. </author> <year> (1987). </year> <title> Genetic algorithms and classifier systems: </title> <booktitle> Foundations and future directions. In Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <pages> pp. </pages> <address> 82-89 Hills-dale, New Jersey. </address>
Reference: <author> Holland, J. H., & Reitman, J. S. </author> <year> (1978). </year> <title> Cognitive systems based on adaptive algorithms. In Pattern-directed Inference Systems. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Kaelbling, L. P., Littman, M. L., & Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 237-285. </pages>
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: We do not restrict evolutionary algorithm reinforcement learning to a specific evolutionary algorithm. Methods from genetic algorithms (Holland, 1975; Goldberg, 1989), evolutionary programming (Fogel, Owens, & Walsh, 1966), genetic programming <ref> (Koza, 1992) </ref>, or evolutionary strategies (Rechenberg, 1964) could all be used in this framework to form effective decision-making agents. However, in this paper we do restrict our attention to those implementations of evolutionary algorithms that solve reinforcement learning problems. <p> EA methods have been successful using a variety of function approximators including neural networks (Whitley, Dominic, Das, & Anderson, 1993; Moriarty & Miikkulainen, 1996a), symbolic rule sets (Grefenstette et al., 1990), and Lisp S-expressions <ref> (Koza, 1992) </ref>. Generalization in TD methods has produced very mixed results. In some large-scale problems, approximating the value function works well (e.g., Tesauro, 1994), while in some simple toy problems it fails (Boyan & Moore, 1995).
Reference: <author> Lee, K.-F., & Mahajan, S. </author> <year> (1990). </year> <title> The development of a world class Othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 43, </volume> <pages> 21-36. </pages>
Reference-contexts: SANE was tested in a game tree search in Othello using the evaluation function from the former world champion program Bill <ref> (Lee & Mahajan, 1990) </ref>. Tested against a full-width minimax search, SANE significantly improved the play of Bill, while examining only a subset of the board positions. The second application was obstacle avoidance in a robot arm (Moriarty & Miikkulainen, 1996b).
Reference: <author> Lin, L.-J., & Mitchell, T. M. </author> <year> (1992). </year> <title> Memory approaches to reinforcement learning in non-Markovian domains. </title> <type> Tech. rep. </type> <institution> CMU-CS-92-138, Carnegie Mellon University, School of Computer Science. </institution>
Reference: <author> McCallum, A. K. </author> <year> (1995). </year> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> Ph.D. thesis, </type> <institution> The University of Rochester. </institution>
Reference: <author> Moriarty, D. E., & Miikkulainen, R. </author> <year> (1994). </year> <title> Evolving neural networks to focus minimax search. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pp. </pages> <address> 1371-1377 Seattle, WA. </address> <publisher> MIT Press. 22 Moriarty, </publisher> <editor> D. E., & Miikkulainen, R. </editor> <year> (1996a). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22, </volume> <pages> 11-32. </pages>
Reference-contexts: In other words, SANE effectively performs a problem reduction search on the space of neural networks. SANE has been shown effective in two different large scale problems. In the first problem, SANE evolved neural networks to direct or focus a minimax game-tree search <ref> (Moriarty & Miikkulainen, 1994) </ref>. By selecting which moves should be evaluated from a given game situation, SANE guides the 20 search away from misinformation in the search tree and towards the most effective moves.
Reference: <author> Moriarty, D. E., & Miikkulainen, R. </author> <year> (1996b). </year> <title> Evolving obstacle avoidance behavior in a robot arm. </title> <booktitle> In From Animals to Animats: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior (SAB-96), </booktitle> <pages> pp. </pages> <address> 468-475 Cape Cod, MA. </address>
Reference-contexts: Tested against a full-width minimax search, SANE significantly improved the play of Bill, while examining only a subset of the board positions. The second application was obstacle avoidance in a robot arm <ref> (Moriarty & Miikkulainen, 1996b) </ref>. Most approaches for learning robot arm control learn hand-eye coordination through supervised training methods where examples of correct behavior are explicitly given. Unfortunately in domains with obstacles where the arm must make several intermediate joint rotations before reaching the target, generating training examples is extremely difficult.
Reference: <author> Moriarty, D. E., & Miikkulainen, R. </author> <year> (1998). </year> <title> Forming neural networks through efficient and adaptive co-evolution. Evolutionary Computation, 5. </title> <publisher> (In Press). </publisher>
Reference: <author> Rechenberg, I. </author> <year> (1964). </year> <title> Cybernetic solution path of an experimental problem. In Library Translation 1122. </title> <institution> Royal Aircraft Establishment, Farnborough, Hants, </institution> <month> Aug. </month> <year> 1965. </year>
Reference-contexts: We do not restrict evolutionary algorithm reinforcement learning to a specific evolutionary algorithm. Methods from genetic algorithms (Holland, 1975; Goldberg, 1989), evolutionary programming (Fogel, Owens, & Walsh, 1966), genetic programming (Koza, 1992), or evolutionary strategies <ref> (Rechenberg, 1964) </ref> could all be used in this framework to form effective decision-making agents. However, in this paper we do restrict our attention to those implementations of evolutionary algorithms that solve reinforcement learning problems. Evolutionary algorithms are global search techniques patterned after Darwin's theory of evolution by natural selection.
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> Ph.D. thesis, </type> <institution> The University of Texas at Austin. </institution>
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal, </journal> <volume> 3, </volume> <pages> 210-229. </pages>
Reference-contexts: Sutton (1988) noted that the bucket brigade bid passing strategy bears a strong relation to the theory of temporal differences. Both follow from Samuel's observation that steps in a sequence should be evaluated and adjusted based on near successors rather than on the final outcome <ref> (Samuel, 1959) </ref>. The bucket brigade updates a given classifier's strength based on the strength of the classifiers that fire as a direct result of its activation.
Reference: <author> Schultz, A. C. </author> <year> (1994). </year> <title> Learning robot behaviors using genetic algorithms. </title> <booktitle> In Intelligent Automation and Soft Computing: Trends in Research, Development, and Applications, </booktitle> <pages> pp. 607-612. </pages> <publisher> TSI Press, </publisher> <address> Albuquerque. </address>
Reference-contexts: In this application of SAMUEL, learning is performed under simulation, reflecting the fact that during the initial phases of learning, controlling a real system can be expensive or dangerous. Learned behaviors are then tested on the on-line system. In <ref> (Schultz, 1994) </ref>, SAMUEL is used to learn collision avoidance and local navigation behaviors for a Nomad 200 mobile robot. The sensors available to the learning task were five sonars, five infrared sensors, and the range and bearing to the goal, and the current speed of the vehicle.
Reference: <author> Schultz, A. C., & Grefenstette, J. J. </author> <year> (1992). </year> <title> Using a genetic algorithm to learn behaviors for autonomous vehicles. </title> <booktitle> In Proceedings of the AiAA Guidance, Navigation, and Control Conference Hilton Head, </booktitle> <address> SC. </address>
Reference: <author> Schultz, A. C., & Grefenstette, J. J. </author> <year> (1996). </year> <title> Robo-shepherd: Learning complex robotic behaviors. </title> <booktitle> In Robotics and Manufacturing: Recent Trends in Research and Applications, </booktitle> <volume> Volume 6, </volume> <pages> pp. 763-768. </pages> <publisher> ASME Press, </publisher> <address> New York. </address>
Reference-contexts: SAMUEL took a human-written rule set that could reach the goal within a limited time without hitting an obstacle only 70 percent of the time, and after 50 generations was able to obtain a 93.5 percent success rate. In <ref> (Schultz & Grefenstette, 1996) </ref>, the robot learned to herd a second robot to a "pasture". In this task, the learning system used the range and bearing to the second robot, the heading of the second robot, and the range and bearing to the goal, as its input sensors.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temproal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: methods do not require extensive domain information, they should benefit from any additional a priori knowledge that can be provided in the form of more informative sensory input or additional reinforcement. 2.2 Temporal Difference Reinforcement Learning The most common approach for solving reinforcement learning problems is the temporal difference method <ref> (Sutton, 1988) </ref>. In TD learning, an evaluation or value function maintains predictions of current and future rewards. More specifically, the value function predicts the expected return from the environment given the current state description and the current decision policy.
Reference: <author> Tesauro, G. </author> <year> (1994). </year> <title> TD-Gammon achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6 (2), </volume> <pages> 215-219. </pages>
Reference-contexts: Generalization in TD methods has produced very mixed results. In some large-scale problems, approximating the value function works well <ref> (e.g., Tesauro, 1994) </ref>, while in some simple toy problems it fails (Boyan & Moore, 1995). One reason that TD methods can have trouble with generalization is because it makes frequent policy updates based on the results of single decisions (immediate reinforcement + estimated value of next state).
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. thesis, </type> <institution> University of Cambridge, </institution> <address> England. </address>
Reference: <author> Watkins, C. J. C. H., & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3), </volume> <pages> 279-292. </pages>
Reference: <author> Whitley, D. </author> <year> (1989). </year> <title> The GENITOR algorithm and selective pressure. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pp. </pages> <address> 116-121 San Mateo, CA. </address> <publisher> Morgan Kauf-man. </publisher>
Reference: <author> Whitley, D., & Kauth, J. </author> <year> (1988). </year> <title> GENITOR: A different genetic algorithm. </title> <booktitle> In Proceedings of the Rocky Mountain Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 118-130 Denver, </address> <publisher> CO. </publisher>
Reference: <author> Whitley, D., Dominic, S., Das, R., & Anderson, C. W. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neurocontrol problems. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 259-284. </pages>
Reference: <author> Wilson, S. W. </author> <year> (1994). </year> <title> ZCS: A zeroth level classifier system. </title> <journal> Evolutionary Computation, </journal> <volume> 2 (1), </volume> <pages> 1-18. </pages>
Reference: <author> Wilson, S. W. </author> <year> (1995). </year> <title> Classifier fitness based on accuracy. </title> <journal> Evolutionary Computation, </journal> <volume> 3 (2). </volume>
Reference-contexts: Such classifiers are the most responsible for the good behavior of the LCS and should be selected for by evolution. Recent work, however, has suggested that a separation of classifier strength and fitness may be more appropriate to build smaller, but important niches <ref> (Wilson, 1995) </ref>. Unfortunately, the progress of the LCS has been somewhat disappointing. Despite the long history of work in LCS, there are few successful applications. Wilson and Goldberg (1989) provide a historical perspective of the LCS and discuss many problems that have prevented its practical implementations.
Reference: <author> Wilson, S. W., & Goldberg, D. E. </author> <year> (1989). </year> <title> A critical review of classifier systems. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms San Mateo, </booktitle> <address> CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 23 </pages>
References-found: 38

