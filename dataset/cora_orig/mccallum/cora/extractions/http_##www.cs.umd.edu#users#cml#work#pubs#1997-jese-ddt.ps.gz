URL: http://www.cs.umd.edu/users/cml/work/pubs/1997-jese-ddt.ps.gz
Refering-URL: http://www.cs.umd.edu/users/cml/work/pubs/
Root-URL: 
Title: Repeatable Software Engineering Experiments for Comparing Defect-Detection Techniques  
Author: Christopher M. Lott, H. Dieter Rombach 
Keyword: code reading by stepwise abstraction, functional testing, structural testing, controlled experiments, empirical software engineering.  
Note: To appear in Journal of Empirical Software Engineering,  Also with the  
Address: 445 South Street, MCC 1H-331B, Morristown NJ 07960, USA  Sauerwiesen 6, 67661 Kaiserslautern, Germany  67653 Kaiserslautern, Germany.  
Affiliation: Bell Communications Research  Fraunhofer Institute for Experimental Software Engineering  Department of Computer Science, University of Kaiserslautern,  
Email: lott@bellcore.com  rombach@iese.fhg.de  
Date: Spring 1997  
Abstract: Techniques for detecting defects in source code are fundamental to the success of any software development approach. A software development organization therefore needs to understand the utility of techniques such as reading or testing in its own environment. Controlled experiments have proven to be an effective means for evaluating software engineering techniques and gaining the necessary understanding about their utility. This paper presents a characterization scheme for controlled experiments that evaluate defect-detection techniques. The characterization scheme permits the comparison of results from similar experiments and establishes a context for cross-experiment analysis of those results. The characterization scheme is used to structure a detailed survey of four experiments that compared reading and testing techniques for detecting defects in source code. We encourage educators, researchers, and practition ers to use the characterization scheme in order to develop and conduct further instances of this class of experiments. By repeating this experiment we expect the software engineering community will gain quantitative insights about the utility of defect-detection techniques in different environments. 
Abstract-found: 1
Intro-found: 1
Reference: [ABC85] <author> E. Aronson, M. Brewer, and J. M. Carl-smith. </author> <title> Experimentation in social psychology. </title> <editor> In G. Lindzey and E. Aron-son, editors, </editor> <booktitle> Handbook of social psychology, </booktitle> <volume> Vol. 1, </volume> <editor> 3rd ed. </editor> <address> New York: </address> <publisher> Random House, </publisher> <year> 1985. </year>
Reference-contexts: We prefer to avoid the term treatments as used by some researchers (see e.g., [VP95]) because in psychological and medical experiments, the experimenters apply some treatment such as medication to subjects and measure the subject's responses (see e.g., <ref> [ABC85] </ref>). In contrast, the class of experiments discussed here requires subjects to apply some techniques to some objects, and then to measure the responses them To appear in Journal of Empirical Software Engineering, Spring 1997 10 selves. Thus the subject's ability to apply the technique may obscure the technique's power.
Reference: [BBH93] <author> Lionel C. Briand, Victor R. Basili, and Christopher J. Hetmanski. </author> <title> Developing interpretable models with optimized set reduction for identifying high-risk software components. </title> <note> IEEE To appear in Journal of Empirical Software Engineering, Spring 1997 31 Transactions on Software Engineering, 19(11):10281044, </note> <month> November </month> <year> 1993. </year>
Reference-contexts: Other approaches. A number of pattern-recognition approaches have been applied to software engineering data sets. These include classification trees [SP88] and optimized set reduction <ref> [BBH93] </ref>. These approaches make no demands or assumptions about the distribution of the data sets, but generally require a large number of data points before they are helpful.
Reference: [BCR94] <author> Victor R. Basili, Gianluigi Caldiera, and H. Dieter Rombach. </author> <title> Goal Question Metric Paradigm. </title> <editor> In John J. Marciniak, editor, </editor> <booktitle> Encyclopedia of Software Engineering, </booktitle> <volume> volume 1, </volume> <pages> pages 528532. </pages> <publisher> John Wiley & Sons, </publisher> <year> 1994. </year>
Reference-contexts: We suggest using the GQM Paradigm to support the processes of stating goals, refining goals in an operational way into metrics, and interpreting the resulting data. The GQM Paradigm supports the definition of goals and their refinement into concrete metrics <ref> [BR88, BCR94, DHL96] </ref>. The idea behind the GQM Paradigm is that measurement should be based on goals. By stating goals explicitly, all data collection and interpretation activities are based on a clearly documented rationale. Goal template.
Reference: [BEM96] <author> Lionel Briand, Khaled El Emam, and Sandro Morasca. </author> <title> On the application of measurement theory in software engineering. </title> <journal> Journal of Empirical Software Engineering, </journal> <volume> 1(1), </volume> <year> 1996. </year>
Reference-contexts: We discuss briefly significance level, power analysis, parametric analysis procedures, nonparametric analysis procedures, and other pattern-recognition approaches. See <ref> [BEM96] </ref> for a comprehensive discussion of data analysis procedures in the software engineering domain. Significance level. An important issue is the choice of significance level. Common practice dictates rejecting the null hypothesis when the significance level is 0.05 [BHH78, p. 109]. <p> Even if the assumptions are violated, analysis procedures such as the t test become more conservative; i.e., they will not make a type I error, but may lead the experimenter to make a type II error <ref> [BEM96] </ref>. Nonparametric analyses. If the data lie on an ordinal scale, randomization was not part of the experiment, or a normal distribution for the population is badly violated due to many outliers, then a nonpara-metric analysis procedure is an appropriate choice.
Reference: [BHH78] <author> G. E. P. Box, W. G. Hunter, and J. S. Hunter. </author> <title> Statistics for Experimenters. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: In the previous example, the corresponding null hypothesis would be technique A requires the same amount of time as technique B for accomplishing task T. These null hypotheses will be tested by applying an inferential statistical analysis procedure to the data collected during an experiment <ref> [BHH78] </ref>. That analysis will yield a probability value for the possibility that the results are due to chance (i.e., whether the null hypothesis must be rejected or accepted). <p> See [BEM96] for a comprehensive discussion of data analysis procedures in the software engineering domain. Significance level. An important issue is the choice of significance level. Common practice dictates rejecting the null hypothesis when the significance level is 0.05 <ref> [BHH78, p. 109] </ref>. If multiple hypotheses will be tested simultaneously, a lower value must be used. On the other hand, research in the social sciences will sometimes use a value of 0.10, depending on the design. Regardless of the value, it should be chosen before the experiment is conducted. <p> Some good news for experi To appear in Journal of Empirical Software Engineering, Spring 1997 14 menters is that parametric techniques are robust to nonnormality under certain conditions, most importantly randomization <ref> [BHH78, pp. 46ff. and p. 104] </ref>. Even if the assumptions are violated, analysis procedures such as the t test become more conservative; i.e., they will not make a type I error, but may lead the experimenter to make a type II error [BEM96]. Nonparametric analyses.
Reference: [BR88] <author> Victor R. Basili and H. Dieter Rom-bach. </author> <title> The TAME Project: Towards improvementoriented software environments. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-14(6):758773, </volume> <month> June </month> <year> 1988. </year>
Reference-contexts: We suggest using the GQM Paradigm to support the processes of stating goals, refining goals in an operational way into metrics, and interpreting the resulting data. The GQM Paradigm supports the definition of goals and their refinement into concrete metrics <ref> [BR88, BCR94, DHL96] </ref>. The idea behind the GQM Paradigm is that measurement should be based on goals. By stating goals explicitly, all data collection and interpretation activities are based on a clearly documented rationale. Goal template.
Reference: [Bro80] <author> Ruven E. Brooks. </author> <title> Studying programmer behavior experimentally: The problems of proper methodology. </title> <journal> Communications of the ACM, </journal> <volume> 23(4):207213, </volume> <month> April </month> <year> 1980. </year>
Reference-contexts: For example, a tightly controlled experiment may use very small pieces of code and undergraduate students to attain high internal validity. However, using toy code modules and inexperienced subjects sacrifices external validity, because the results will not necessarily hold in an environment where professionals test large software systems <ref> [Bro80] </ref>. These tradeoffs demonstrate the value and importance of developing an explicit statement of experiment goals. Scope of an experiment.
Reference: [BS87] <author> Victor R. Basili and Richard W. Selby. </author> <title> Comparing the effectiveness of software testing techniques. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 13(12):12781296, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Further, the experi menter will want to balance the choice of faults of various types. We classify faults using two orthogonal facets as discussed in <ref> [BS87] </ref>. Facet one (omission, commission) describes whether the fault was due to the absence of necessary code or the presence of incorrect code. Facet two (initialization, computation, control, interface, data, cosmetic) describes the nature of the fault in more detail; i.e., what code construct was written incorrectly. <p> that the results were somewhat disappointing: these experienced subjects largely ignored unconventional input cases in favor of conventional ones, overlooked many revealed failures, and isolated on average only one-third of the known faults. 3.3 Basili & Selby (1987) Basili & Selby developed a controlled experiment that compared three defect-detection techniques <ref> [Sel85, BS87] </ref>. They conducted their experiment three times. The first two repetitions used a total of 42 advanced students, but those results are not presented here. The discussion below focuses on the last of the three repetitions, when professionals were used. Goal. <p> S9 P3 P1 P4 S10 P4 P3 P1 S19 P1 P4 P3 Junior S20 P3 P1 P4 S21 P1 P4 P3 S32 P4 P3 P1 c fl1987 IEEE. Used by permission. Table 5: Basili & Selby's experimental design <ref> [BS87] </ref> Experimental design. Table 5 summarizes Basili & Selby's experimental design. The subject's experience was considered an independent variable and was characterized in terms of years of professional experience and academic background. A random match of subjects, techniques, programs, and experience levels was made. <p> This was the motivation for the extension done in the Kamsties & Lott experiment. To appear in Journal of Empirical Software Engineering, Spring 1997 21 3.4 Kamsties & Lott (1995) Kamsties & Lott extended the design and techniques originally used by Basili & Selby <ref> [BS87] </ref> and conducted the resulting experiment twice [KL95a]. The extension consisted of a step of fault isolation following failure detection. Next we present an overview of the Kamsties & Lott experiment according to the characterization scheme of Table 1. First the four goals used from that experiment are presented.
Reference: [BSH86] <author> Victor R. Basili, Richard W. Selby, and David H. Hutchens. </author> <title> Experimentation in software engineering. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(7):733743, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: Special emphasis is placed on an experiment that we conducted at the University of Kaiserslautern. Finally, Section 4 draws some conclusions and identifies future work. 2 A Characterization Scheme for Experiments Many have called for an infrastructure for experimental software engineering <ref> [Cur80, BSH86, FPG94] </ref>. The characterization scheme of this section is a contribution towards achieving that goal for experiments that evaluate source-code verification and validation techniques. The characterization scheme draws on work that appeared in [PR94] and is summarized in Table 1. <p> parts, namely the goals and hypotheses that motivate an experiment, the plan for conducting the experiment, the procedures used during the experiment, and finally the results. 2.1 Goals, Hypotheses, and Theories A statement of goals determines what an experiment should accomplish, and thereby assists in designing and conducting the experiment <ref> [BSH86] </ref>. <p> Used by permission. Table 2: Scope of empirical studies <ref> [BSH86] </ref> results should be useful). For example, specialized case studies (narrow scope) may offer results that, although limited to a specific context, are extremely useful for that context [Lee89, Gla95]. Basili et al. [BSH86] classified the scope of empirical studies using repeatability and organizational context as criteria, as reprinted in Table <p> Used by permission. Table 2: Scope of empirical studies <ref> [BSH86] </ref> results should be useful). For example, specialized case studies (narrow scope) may offer results that, although limited to a specific context, are extremely useful for that context [Lee89, Gla95]. Basili et al. [BSH86] classified the scope of empirical studies using repeatability and organizational context as criteria, as reprinted in Table 2. The upper-left box in the table (single project) might also be called a case study; one team (or even a single subject) participates in a single project (exercise).
Reference: [BW84] <author> Victor R. Basili and David M. Weiss. </author> <title> A methodology for collecting valid software engineering data. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(6):728738, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: After the subjects have completed the exercises, we recommend performing detailed interviews of subjects to validate their data (see also <ref> [BW84] </ref> for a discussion of the dangers of not conducting interviews).
Reference: [BW85] <author> Victor R. Basili and David M. Weiss. </author> <title> Evaluating software development by analysis of changes: Some data from the software engineering laboratory. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11(2):157168, </volume> <month> February </month> <year> 1985. </year>
Reference-contexts: Special emphasis is placed on an experiment that we conducted at the University of Kaiserslautern. As in previous work (see, e.g., <ref> [BW85] </ref>), this section demonstrates the difficulty of comparing experiments in software engineering and drawing meaningful conclusions from the results. 3.1 Hetzel (1976) Hetzel performed a controlled experiment that compared three defect-detection techniques [Het76]. To appear in Journal of Empirical Software Engineering, Spring 1997 16 Goal.
Reference: [Coh88] <author> J. Cohen. </author> <title> Statistical power analysis for the behavioral sciences. </title> <publisher> Lawrence Erl-baum Associates, </publisher> <year> 1988. </year>
Reference-contexts: The power of a technique is commonly expressed in a power table. For a given analysis technique, a power table relates the four factors sensitivity (power level), effect size, significance level, and number of observations (i.e., subjects). These tables can be found in references such as <ref> [Coh88] </ref>. Larger samples lead to greater powers for a given analysis technique. An especially difficult part of power analysis is estimating the effect size that is expected in the experiment.
Reference: [CS66] <author> Donald T. Campbell and Julian C. Stanley. </author> <title> Experimental and Quasi-Experimental Designs for Research. </title> <publisher> Houghton Mifflin, </publisher> <address> Boston, </address> <year> 1966. </year> <note> ISBN 0-395-30787-2. </note>
Reference-contexts: Internal validity. Internal validity refers to the extent to which causality is established as a credible explanation for the relationship between the presumed causes and measured responses [JSK91]. Campbell and Stanley identify eight threats to internal validity <ref> [CS66, pp. 56] </ref>. One of the most important threats in software engineering experiments is called a selection effect. An example selection effect is a subject who has a natural ability for the experimental task due to many years of experience. <p> The experimental design explains how this assignment is done, and thereby controls factors that will permit causality to be inferred <ref> [CS66] </ref>. The design is fundamental to internal validity, and determines in large part the external va 1 The term blocked has a highly specific meaning in experimental design, and differs from the usage in Table 2. <p> The amount of time that they required was recorded. The subjects who performed the walk-through/inspection method took the materials with them, prepared on their own, and reported their time. The subsequent collection session was limited to 90 minutes. 3 See <ref> [CS66, p. 15] </ref> for a discussion of artifacts due to regression to the mean and other problems that arise when trying to correct for differences in ability by grouping subjects based on extreme values on pretests. To appear in Journal of Empirical Software Engineering, Spring 1997 19 Results.
Reference: [Cur80] <author> Bill Curtis. </author> <booktitle> Measurement and experimentation in software engineering. Proceedings of the IEEE, </booktitle> <volume> 68(9):1144 1157, </volume> <month> September </month> <year> 1980. </year>
Reference-contexts: Special emphasis is placed on an experiment that we conducted at the University of Kaiserslautern. Finally, Section 4 draws some conclusions and identifies future work. 2 A Characterization Scheme for Experiments Many have called for an infrastructure for experimental software engineering <ref> [Cur80, BSH86, FPG94] </ref>. The characterization scheme of this section is a contribution towards achieving that goal for experiments that evaluate source-code verification and validation techniques. The characterization scheme draws on work that appeared in [PR94] and is summarized in Table 1.
Reference: [DHL96] <author> Christiane Differding, Barbara Hoisl, and Christopher M. Lott. </author> <title> Technology package for the Goal Question Metric Paradigm. </title> <type> Technical Report 281-96, </type> <institution> Department of Computer Science, University of Kaiserslautern, </institution> <address> 67653 Kais-erslautern, Germany, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: We suggest using the GQM Paradigm to support the processes of stating goals, refining goals in an operational way into metrics, and interpreting the resulting data. The GQM Paradigm supports the definition of goals and their refinement into concrete metrics <ref> [BR88, BCR94, DHL96] </ref>. The idea behind the GQM Paradigm is that measurement should be based on goals. By stating goals explicitly, all data collection and interpretation activities are based on a clearly documented rationale. Goal template.
Reference: [Fag76] <author> M. E. Fagan. </author> <title> Design and code inspections to reduce errors in program To appear in Journal of Empirical Software Engineering, Spring 1997 32 development. </title> <journal> IBM Systems Journal, </journal> <volume> 15(3):219248, </volume> <year> 1976. </year>
Reference-contexts: Example verification approaches include walk-throughs (subjects walk through the code, executing it on paper), structured inspections (subjects detect defects on their own and hold meetings to collect the defects) <ref> [Fag76, PVB95] </ref>, and code reading by stepwise abstraction (subjects write their own specification of the code and compare it with the official specification) [LMW79]. The code reading approach used in several experiments surveyed in this paper may be considerably more rigorous than either a walk-through or code inspection. Functional testing.
Reference: [FPG94] <author> Norman Fenton, Shari Lawrence Pfleeger, and Robert L. Glass. </author> <title> Science and substance: A challenge to software engineering. </title> <journal> IEEE Software, </journal> <volume> 11(4):8695, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: Special emphasis is placed on an experiment that we conducted at the University of Kaiserslautern. Finally, Section 4 draws some conclusions and identifies future work. 2 A Characterization Scheme for Experiments Many have called for an infrastructure for experimental software engineering <ref> [Cur80, BSH86, FPG94] </ref>. The characterization scheme of this section is a contribution towards achieving that goal for experiments that evaluate source-code verification and validation techniques. The characterization scheme draws on work that appeared in [PR94] and is summarized in Table 1.
Reference: [GHM87] <author> J. D. Gannon, R. B. Hamlet, and H. D. Mills. </author> <title> Theory of modules. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 13(7):820829, </volume> <month> July </month> <year> 1987. </year>
Reference-contexts: Verification techniques are primarily applied by personnel in an off-line manner. Examples of these techniques include formal proofs based on Hoare or Mills semantics <ref> [Hoa69, GHM87] </ref>, semi-formal techniques such as reading by stepwise abstraction for design and code documents [LMW79], active design reviews [PW85], and scenario-based reading for requirements documents [PVB95]. Validation techniques are applied on-line by running a software system with a set of test cases as inputs.
Reference: [Gla95] <author> Robert L. Glass. </author> <title> Pilot studies: What, why and how. </title> <booktitle> The Software Practitioner, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: Used by permission. Table 2: Scope of empirical studies [BSH86] results should be useful). For example, specialized case studies (narrow scope) may offer results that, although limited to a specific context, are extremely useful for that context <ref> [Lee89, Gla95] </ref>. Basili et al. [BSH86] classified the scope of empirical studies using repeatability and organizational context as criteria, as reprinted in Table 2.
Reference: [Het76] <author> William C. Hetzel. </author> <title> An Experimental Analysis of Program Verification Methods. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <year> 1976. </year>
Reference-contexts: As in previous work (see, e.g., [BW85]), this section demonstrates the difficulty of comparing experiments in software engineering and drawing meaningful conclusions from the results. 3.1 Hetzel (1976) Hetzel performed a controlled experiment that compared three defect-detection techniques <ref> [Het76] </ref>. To appear in Journal of Empirical Software Engineering, Spring 1997 16 Goal. The template presented in Section 2.1.1 was used to construct a goal for this experiment. <p> Hetzel. Used by permission. Table 3: Hetzel's experimental design <ref> [Het76] </ref> contrast to some experiments, the subjects were partially acquainted with the object before beginning the exercises, but they had not practiced the testing tasks. Batch processing of jobs was used; turnaround time averaged 15 minutes. Monitors submitted jobs and fetched output.
Reference: [Hoa69] <author> C. A. R. Hoare. </author> <title> An axiomatic basis for computer programming. </title> <journal> Communications of the ACM, 12(10):576580, </journal> <volume> 583, </volume> <month> October </month> <year> 1969. </year>
Reference-contexts: Verification techniques are primarily applied by personnel in an off-line manner. Examples of these techniques include formal proofs based on Hoare or Mills semantics <ref> [Hoa69, GHM87] </ref>, semi-formal techniques such as reading by stepwise abstraction for design and code documents [LMW79], active design reviews [PW85], and scenario-based reading for requirements documents [PVB95]. Validation techniques are applied on-line by running a software system with a set of test cases as inputs.
Reference: [How78] <author> William E. Howden. </author> <title> An evaluation of the effectiveness of symbolic testing. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 8(4):381398, </volume> <month> July / August </month> <year> 1978. </year>
Reference-contexts: Examples of these techniques include functional testing, in which the specification is primarily used to develop test cases [How80, Mye78], and structural testing, in which the source code is primarily used to develop test cases <ref> [How78, Mar94] </ref>. The primary argument for using both off-line verification techniques and on-line validation techniques is the need to capture defects in requirements, design, and code artifacts as early as possible.
Reference: [How80] <author> William E. Howden. </author> <title> Functional program testing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6:162169, </volume> <month> March </month> <year> 1980. </year>
Reference-contexts: Validation techniques are applied on-line by running a software system with a set of test cases as inputs. Examples of these techniques include functional testing, in which the specification is primarily used to develop test cases <ref> [How80, Mye78] </ref>, and structural testing, in which the source code is primarily used to develop test cases [How78, Mar94]. The primary argument for using both off-line verification techniques and on-line validation techniques is the need to capture defects in requirements, design, and code artifacts as early as possible.
Reference: [Hum95] <author> Watts H. Humphrey. </author> <title> A Discipline for Software Engineering. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: Therefore, an effective approach for convincing professional developers and university students of the utility of off-line techniques is needed. We recommend using the same principle underlying Watts Humphrey's Personal Software Process <ref> [Hum95] </ref>, namely asking developers to apply and measure the techniques themselves. This type of experience may be gained by conducting low-risk projects (the approach advocated by the Personal Software Process) or by taking part in repeatable software engineering experiments (the approach advocated in this paper).
Reference: [IEE83] <institution> Institute of Electrical and Electronics Engineers. Standard Glossary of Software Engineering Terminology, </institution> <year> 1983. </year>
Reference-contexts: a given development phase satisfy the conditions imposed at the start of that phase, and validation techniques evaluate a system or component during or at the end of the 1 To appear in Journal of Empirical Software Engineering, Spring 1997 2 development process to determine whether it satisfies specified requirements <ref> [IEE83] </ref>. Verification techniques are primarily applied by personnel in an off-line manner.
Reference: [JSK91] <author> Charles M. Judd, Eliot R. Smith, and Louise H. Kidder. </author> <title> Research Methods in Social Relations. Holt, Rinehart and Winston, </title> <booktitle> sixth edition, </booktitle> <year> 1991. </year>
Reference-contexts: To support testing such statements using inferential statistical methods, these statements are eventually formulated as null hypotheses, and the original statement is called the alternative hypothesis <ref> [JSK91] </ref>. A null hypothesis simply states that no difference exists between some set of data. In the previous example, the corresponding null hypothesis would be technique A requires the same amount of time as technique B for accomplishing task T. <p> Both correlation and causality can be tested, but the value of a controlled experiment is its power in establishing causality. Construct validity. Construct validity refers to the degree to which a given measure accurately characterizes some construct under study <ref> [JSK91] </ref>. <p> The experimentalist must never forget that all measures are imperfect, and must apply sound reasoning when choosing measures. Internal validity. Internal validity refers to the extent to which causality is established as a credible explanation for the relationship between the presumed causes and measured responses <ref> [JSK91] </ref>. Campbell and Stanley identify eight threats to internal validity [CS66, pp. 56]. One of the most important threats in software engineering experiments is called a selection effect. An example selection effect is a subject who has a natural ability for the experimental task due to many years of experience. <p> This freedom should ensure that the internal validity of the experiment is not threatened by stresses on the subjects that are invisible to the researchers. External validity. External validity refers to the confidence with which the results can be generalized beyond the experiment <ref> [JSK91] </ref>. External validity is affected not only by the design but also by choices made for the objects chosen and the subjects who participate. <p> For example, a null hypothesis may state that the different objects or subjects used in the experiment have no effect on the values of the dependent variables. Two errors are possible when an experimenter considers whether to accept or reject a null hypothesis <ref> [JSK91, pp. 396 ff.] </ref> The first error (called a Type I error) consists of rejecting the null hypothesis although it was in fact true. For example, an experimenter may erroneously state that a technique helps subjects detect defects more rapidly, although it in fact did not. <p> Selection. Ideally, the subjects would be a random sample of the population of computer science professionals. However, attaining a random sample is difficult. Instead, subject groups are commonly an accidental sample of this population <ref> [JSK91] </ref>, meaning that they are selected based on their participation in university or industrial courses. However, it is important not to ask for volunteers to avoid the problems of self-selection. Instead, all course participants should be used. Experience. <p> Ethical issues. Many ethical issues that arise in psychological experiments using human beings are happily absent from this class of experiments. These include involving people without their consent, deceiving subjects, invading the privacy of subjects, or withholding benefits from control groups <ref> [JSK91, Chapter 20] </ref>. <p> However, student subjects are commonly required to participate in experiments as a condition of passing a course, a practice that is considered acceptable provided that no undue coercion is present, and that the procedures provide an educational benefit to the subject (see <ref> [JSK91, pp. 491 492] </ref> for a detailed discussion). Further, it is important that quantitative results are not used when determining course grades, just the fact that the student did or did not participate.
Reference: [KL95a] <author> Erik Kamsties and Christopher M. Lott. </author> <title> An empirical evaluation of three defect-detection techniques. </title> <editor> In W. Schafer and P. Botella, editors, </editor> <booktitle> Proceedings of the Fifth European Software Engineering Conference, </booktitle> <pages> pages 362383. </pages> <note> Lecture Notes in Computer Science Nr. 989, SpringerVerlag, </note> <month> September </month> <year> 1995. </year>
Reference-contexts: To appear in Journal of Empirical Software Engineering, Spring 1997 21 3.4 Kamsties & Lott (1995) Kamsties & Lott extended the design and techniques originally used by Basili & Selby [BS87] and conducted the resulting experiment twice <ref> [KL95a] </ref>. The extension consisted of a step of fault isolation following failure detection. Next we present an overview of the Kamsties & Lott experiment according to the characterization scheme of Table 1. First the four goals used from that experiment are presented. <p> testing (FT) testing (ST) Pgm. 1 (day 1) groups 1, 2 groups 3, 4 groups 5, 6 Pgm. 2 (day 2) groups 3, 5 groups 1, 6 groups 2, 4 Pgm. 3 (day 3) groups 4, 6 groups 2, 5 groups 1, 3 Table 8: Kamsties & Lott's experimental design <ref> [KL95a] </ref> To appear in Journal of Empirical Software Engineering, Spring 1997 26 D.3: The order in which subjects apply the techniques has no effect on the results; i.e., no learn ing effects. D.4: The subjects have no effect on the results; i.e., all subjects perform similarly (no selection ef fects). <p> A nonparametric correlation test was used to evaluate the correlations of subject factors with the results. Reuse guidelines. If the design is reused unchanged, the analyses can proceed identically to those documented in <ref> [KL95a] </ref>. Dramatic changes to the design, especially if the randomization step cannot be performed, may prevent the use of ANOVA; nonparametric statistics such as the Kruskal-Wallis test can then be used. 3.4.3 Procedures The Kamsties & Lott experiment consisted of three phases, namely training, experiment, and feedback. Phase 1: Training.
Reference: [KL95b] <author> Erik Kamsties and Christopher M. Lott. </author> <title> An empirical evaluation of three defect-detection techniques. </title> <type> Technical Report ISERN 9502, </type> <institution> Department of Computer Science, University of Kaiserslautern, </institution> <address> 67653 Kaiserslautern, Germany, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: All analyses and interpretations are presented in detail in <ref> [KL95b] </ref>. No significant differences were observed among the techniques with respect to the percentage of failures observed (i.e., hypothesis D.1 could not be rejected with respect to effectiveness of observing failures).
Reference: [Lai95] <author> Oliver Laitenberger. </author> <title> Perspective-based reading technique, validation and research in future. Student project (Pro-jektarbeit), </title> <institution> Department of Computer Science, University of Kaiserslautern, </institution> <address> 67653 Kaiserslautern, Germany, </address> <year> 1995. </year>
Reference-contexts: The experimenter would also be well advised to consider the nature of the faults encountered in that context, and to select faults that are most similar to the organization's characteristic fault profile. See <ref> [Lai95] </ref> for an experimental design that addresses these issues. Subjects. Some 27 university students participated in the first run of the experiment, and 23 students participated in the second. The two runs are examples of in vitro experiments. Data collection and validation procedures. <p> We note that experiments for other technologies have been similarly packaged; see for example, the comparisons of different inspection techniques <ref> [VWV93, Lai95, PVB95] </ref>. Future directions include performing more repetitions in order to analyze different variation factors and to strengthen the credibility of the existing results. Conducting experiments in software engineering To appear in Journal of Empirical Software Engineering, Spring 1997 30 has benefits for students, professionals, and the community.
Reference: [Lee89] <author> Allen S. Lee. </author> <title> A scientific methodology for MIS case studies. </title> <journal> MIS Quarterly, </journal> <pages> pages 3350, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: Used by permission. Table 2: Scope of empirical studies [BSH86] results should be useful). For example, specialized case studies (narrow scope) may offer results that, although limited to a specific context, are extremely useful for that context <ref> [Lee89, Gla95] </ref>. Basili et al. [BSH86] classified the scope of empirical studies using repeatability and organizational context as criteria, as reprinted in Table 2.
Reference: [LMW79] <author> Richard C. Linger, Harlan D. Mills, and Bernard I. </author> <title> Witt. Structured Programming: Theory and Practice. To appear in Journal of Empirical Software Engineering, </title> <publisher> Spring 1997 33 Addison-Wesley Publishing Company, </publisher> <year> 1979. </year>
Reference-contexts: Verification techniques are primarily applied by personnel in an off-line manner. Examples of these techniques include formal proofs based on Hoare or Mills semantics [Hoa69, GHM87], semi-formal techniques such as reading by stepwise abstraction for design and code documents <ref> [LMW79] </ref>, active design reviews [PW85], and scenario-based reading for requirements documents [PVB95]. Validation techniques are applied on-line by running a software system with a set of test cases as inputs. <p> approaches include walk-throughs (subjects walk through the code, executing it on paper), structured inspections (subjects detect defects on their own and hold meetings to collect the defects) [Fag76, PVB95], and code reading by stepwise abstraction (subjects write their own specification of the code and compare it with the official specification) <ref> [LMW79] </ref>. The code reading approach used in several experiments surveyed in this paper may be considerably more rigorous than either a walk-through or code inspection. Functional testing. Also known as black box testing, this validation technique is characterized by using primarily the specification to develop test cases. <p> In step 1, subjects receive printed source code but do not see the specification. They read the source code and write their own speci To appear in Journal of Empirical Software Engineering, Spring 1997 27 fication of the code based on the technique of reading by stepwise abstraction <ref> [LMW79] </ref>. Subjects identify prime subprograms, write a specification for the subprogram as formally as possible, group subprograms and their specifications together, and repeat the process until they have abstracted all of the source code.
Reference: [Mar94] <author> Brian Marick. </author> <title> The Craft of Software Testing. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: Examples of these techniques include functional testing, in which the specification is primarily used to develop test cases [How80, Mye78], and structural testing, in which the source code is primarily used to develop test cases <ref> [How78, Mar94] </ref>. The primary argument for using both off-line verification techniques and on-line validation techniques is the need to capture defects in requirements, design, and code artifacts as early as possible. <p> Structural testing (ST). In step 1, subjects receive printed source code but do not see the specification. They try to construct test cases that will achieve 100% coverage of all branches, multiple conditions, loops, and relational operators as measured by the Generic Coverage Tool <ref> [Mar94] </ref>. For example, 100% coverage of a multiple condition using a single logical and operator means that all four combinations of true and false must be tested, and 100% coverage of a loop means that it must be executed zero, one, and many time (s).
Reference: [MDW + 95] <author> James Miller, John Daly, Murray Wood, Marc Roper, and Andrew Brooks. </author> <title> Statistical power and its sub-components missing and misunderstood concepts in software engineering empirical research. </title> <type> Technical Report RR/95/192, </type> <institution> Department of Computer Science, University of Strath-clyde, </institution> <address> Livingstone Tower, Richmond Street, Glasgow G1 1XH, Scotland, </address> <year> 1995. </year> <note> http://www.cs.strath.ac.uk/CS/- Research/EFOCS/Research-Reports/- EFoCS-15-95.ps.Z. </note>
Reference-contexts: These tables can be found in references such as [Coh88]. Larger samples lead to greater powers for a given analysis technique. An especially difficult part of power analysis is estimating the effect size that is expected in the experiment. See also <ref> [MDW + 95] </ref> for a discussion of power analysis in software engineering experiments. 2.2.6 Data collection and validation procedures The subjects will collect most of the values for the dependent variables during the experiment using data-collection forms.
Reference: [Mye78] <author> Glenford J. Myers. </author> <title> A controlled experiment in program testing and code walkthroughs / inspections. </title> <journal> Communications of the ACM, </journal> <volume> 21(9):760768, </volume> <month> September </month> <year> 1978. </year>
Reference-contexts: Validation techniques are applied on-line by running a software system with a set of test cases as inputs. Examples of these techniques include functional testing, in which the specification is primarily used to develop test cases <ref> [How80, Mye78] </ref>, and structural testing, in which the source code is primarily used to develop test cases [How78, Mar94]. The primary argument for using both off-line verification techniques and on-line validation techniques is the need to capture defects in requirements, design, and code artifacts as early as possible. <p> Hetzel also noted that the subjects observed only about 50% of the revealed failures, and that the separation of best and worst performers was a factor between 2 and 3. 3.2 Myers (1978) Myers performed a study that compared three defect-detection techniques <ref> [Mye78] </ref>. Goal. The template presented in Section 2.1.1 was used to construct a goal for this experiment. <p> Group A: Group B: Group C: Functional test Structural test Walk./inspec. PL/I rating: 1.5 PL/I rating: 2.1 PL/I rating: 2.4 W/I rating: 0.2 W/I rating: 0.3 W/I rating: 0.6 Table 4: Myers' experimental design <ref> [Mye78] </ref> Subjects. The subjects were 59 software developers who participated in a course held for IBM employees. Data collection procedures. Data was collected via data-collection forms. Data analysis procedures. Myers used nonpara-metric methods (specifically Kruskal-Wallis) and correlation tests to analyze the data. Experimental procedure.
Reference: [Mye79] <author> Glenford J. Myers. </author> <title> The Art of Software Testing. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Many different coverage criteria may be selected; one example is 100% statement coverage, meaning that each statement is executed at least once <ref> [Mye79] </ref>.
Reference: [Pfl95a] <author> Shari Lawrence Pfleeger. </author> <title> Experimental design and analysis in software engineering, part 3: Types of experimental design. </title> <booktitle> ACM SIGSOFT Software Engineering Notes, </booktitle> <address> 20(2):1416, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: An independent variable is commonly called a factor in the terminology of experimental design, and the values that each factor can assume are called levels. Factors (actually the levels) are commonly manipulated in an experiment by crossing them <ref> [Pfl95a] </ref>. A complete crossing of some number of factors is called a full factorial design. A full-factorial design tests all possible combinations of factors. This seemingly ideal situation permits the experimenter to measure interaction effects between the various independent variables.
Reference: [Pfl95b] <author> Shari Lawrence Pfleeger. </author> <title> Experimental design and analysis in software engineering, part 4: Choosing an experimental design. </title> <booktitle> ACM SIGSOFT Software Engineering Notes, </booktitle> <address> 20(3):1315, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: There are further tradeoffs between a full-factorial and an efficient experimental design. For further information, see for example <ref> [Pfl95b] </ref>. Null hypotheses. The design determines a minimum set of null hypotheses regarding external and internal validity of the experiment. The hypotheses concerning external validity correspond directly to the testable hypotheses derived from the goals; the rest check for threats to internal validity.
Reference: [PR94] <author> Jenny Preece and H. Dieter Rom-bach. </author> <title> A taxonomy for combining software engineering and human-computer interaction measurement approaches: Towards a common framework. </title> <journal> International Journal of HumanComputer Studies, </journal> <volume> 41:553583, </volume> <year> 1994. </year>
Reference-contexts: The characterization scheme of this section is a contribution towards achieving that goal for experiments that evaluate source-code verification and validation techniques. The characterization scheme draws on work that appeared in <ref> [PR94] </ref> and is summarized in Table 1. The characterization scheme permits the comparison of results from similar experiments and establishes a context for cross-experiment analyses. <p> For example, experiments that investigate human-computer interaction issues such as the usability of a test-support tool may require the subjects to be videotaped during their exercises to permit additional analyses <ref> [PR94] </ref>. 2.3.3 Phase 3: Analysis, interpretation, and feedback To improve the value of the experiment as a educational exercise, an interpretation session should be scheduled for reporting the results to the subjects as soon as possible following the experiment.
Reference: [PSV95] <author> Adam A. Porter, Harvey Siy, and Lawrence G. Votta. </author> <title> A survey of software inspections. </title> <type> Technical Report CS-TR-3552, </type> <institution> UMIACS-TR-95-104, Department of Computer Science, University of Maryland, College Park, Maryland 20742, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: Reading techniques. These verification techniques are used to detect defects in code without using the computer. Software practitioners use many different off-line reading techniques. Notable differences among the techniques include the use of individuals versus teams, the availability of tool support, and the use of meetings <ref> [PSV95] </ref>.
Reference: [PVB95] <author> Adam A. Porter, Lawrence G. Votta, and Victor R. Basili. </author> <title> Comparing detection methods for software requirements inspections: A replicated experiment. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(6):563575, </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: Examples of these techniques include formal proofs based on Hoare or Mills semantics [Hoa69, GHM87], semi-formal techniques such as reading by stepwise abstraction for design and code documents [LMW79], active design reviews [PW85], and scenario-based reading for requirements documents <ref> [PVB95] </ref>. Validation techniques are applied on-line by running a software system with a set of test cases as inputs. <p> Example verification approaches include walk-throughs (subjects walk through the code, executing it on paper), structured inspections (subjects detect defects on their own and hold meetings to collect the defects) <ref> [Fag76, PVB95] </ref>, and code reading by stepwise abstraction (subjects write their own specification of the code and compare it with the official specification) [LMW79]. The code reading approach used in several experiments surveyed in this paper may be considerably more rigorous than either a walk-through or code inspection. Functional testing. <p> We note that experiments for other technologies have been similarly packaged; see for example, the comparisons of different inspection techniques <ref> [VWV93, Lai95, PVB95] </ref>. Future directions include performing more repetitions in order to analyze different variation factors and to strengthen the credibility of the existing results. Conducting experiments in software engineering To appear in Journal of Empirical Software Engineering, Spring 1997 30 has benefits for students, professionals, and the community.
Reference: [PW85] <author> David Lorge Parnas and David M. Weiss. </author> <title> Active design reviews: </title> <booktitle> principles and practices. In Proceedings of the Eighth International Conference on Software Engineering, </booktitle> <pages> pages 132136. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> August </month> <year> 1985. </year>
Reference-contexts: Verification techniques are primarily applied by personnel in an off-line manner. Examples of these techniques include formal proofs based on Hoare or Mills semantics [Hoa69, GHM87], semi-formal techniques such as reading by stepwise abstraction for design and code documents [LMW79], active design reviews <ref> [PW85] </ref>, and scenario-based reading for requirements documents [PVB95]. Validation techniques are applied on-line by running a software system with a set of test cases as inputs.
Reference: [RBS92] <author> H. Dieter Rombach, Victor R. Basili, and Richard W. Selby, </author> <title> editors. Experimental Software Engineering Issues: A critical assessment and future directions. </title> <booktitle> Lecture Notes in Computer Science Nr. 706, </booktitle> <address> SpringerVerlag, </address> <year> 1992. </year> <note> To appear in Journal of Empirical Software Engineering, Spring 1997 34 </note>
Reference-contexts: We therefore recommend that repeatable experiments be adopted as a standard part of both software engineering education and technology transfer programs. We strongly believe that researchers and practitioners need to view software engineering as an experimental discipline <ref> [RBS92] </ref>. Techniques such as defect-detection techniques that are fundamental to software development need to be understood thoroughly, and such an understanding can only be gained via experimentation. Other fundamental techniques that need to be thoroughly understood include various design methodologies and different styles of documentation.
Reference: [Sel85] <author> Richard W. Selby. </author> <title> Evaluations of Software Technologies: Testing, CLEAN-ROOM, and Metrics. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Maryland, College Park, MD 20742, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: that the results were somewhat disappointing: these experienced subjects largely ignored unconventional input cases in favor of conventional ones, overlooked many revealed failures, and isolated on average only one-third of the known faults. 3.3 Basili & Selby (1987) Basili & Selby developed a controlled experiment that compared three defect-detection techniques <ref> [Sel85, BS87] </ref>. They conducted their experiment three times. The first two repetitions used a total of 42 advanced students, but those results are not presented here. The discussion below focuses on the last of the three repetitions, when professionals were used. Goal.
Reference: [SP88] <author> Richard W. Selby and Adam A. Porter. </author> <title> Learning from examples: generation and evaluation of decision trees for software resource analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(12):17431757, </volume> <month> December </month> <year> 1988. </year>
Reference-contexts: Other approaches. A number of pattern-recognition approaches have been applied to software engineering data sets. These include classification trees <ref> [SP88] </ref> and optimized set reduction [BBH93]. These approaches make no demands or assumptions about the distribution of the data sets, but generally require a large number of data points before they are helpful.
Reference: [Spe81] <author> Paul E. Spector. </author> <title> Research Designs. Sage University Paper series on Quantitative Applications in the Social Sciences, </title> <booktitle> series no. </booktitle> <pages> 07-023. </pages> <publisher> Sage Publications, </publisher> <address> Beverly Hills, </address> <year> 1981. </year>
Reference-contexts: maximum internal and external validity, a controlled experiment (as characterized by the lower right-hand corner of Table 2) will be required. 1 2.2.2 Experimental designs We use the term experiment to mean a controlled investigation in which some random assignment of subjects to levels of independent variables has been done <ref> [Spe81] </ref>. The experimental design explains how this assignment is done, and thereby controls factors that will permit causality to be inferred [CS66].
Reference: [VP95] <author> Lawrence G. Votta and Adam Porter. </author> <title> Experimental software engineering: A report on the state of the art. </title> <booktitle> In Proceedings of the Seventeenth International Conference on Software Engineering, </booktitle> <pages> pages 277279. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1995. </year>
Reference-contexts: Correlation versus causality. A correlation between two events merely states that the two events have been known to occur together. However, event A is said to cause event B only if a certain set of conditions holds <ref> [VP95] </ref>. First, there must be some non-spurious association between the two events. Second, event A must happen before event B (temporal precedence). <p> We prefer to avoid the term treatments as used by some researchers (see e.g., <ref> [VP95] </ref>) because in psychological and medical experiments, the experimenters apply some treatment such as medication to subjects and measure the subject's responses (see e.g., [ABC85]). <p> In the software engineering domain, these two types of experimental subjects have been classified as performing experiments in vitro To appear in Journal of Empirical Software Engineering, Spring 1997 12 (i.e., with students) versus in vivo (i.e., with professionals) <ref> [VP95] </ref>. The argument is that experiments should be piloted in universities to detect problems in the design and procedures at a relatively low cost. A more costly study involving professionals can then be performed with the knowledge that all experimental materials have been rigorously tested.
Reference: [VWV93] <author> Scott A. Vander Wiel and Lawrence G. Votta. </author> <title> Assessing software designs using capture-recapture methods. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(11):10451054, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: We note that experiments for other technologies have been similarly packaged; see for example, the comparisons of different inspection techniques <ref> [VWV93, Lai95, PVB95] </ref>. Future directions include performing more repetitions in order to analyze different variation factors and to strengthen the credibility of the existing results. Conducting experiments in software engineering To appear in Journal of Empirical Software Engineering, Spring 1997 30 has benefits for students, professionals, and the community.
Reference: [ZEWH95] <author> Stuart H. Zweben, Stephen H. Ed-wards, Bruce W. Weide, and Joseph E. Hollingsworth. </author> <title> The effects of layering and encapsulation on software development cost and quality. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(3):200208, </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: Selecting an appropriate analysis procedure to evaluate the results of an experiment can be a challenging task. In the case of software engineering experiments, small data sets and significant restrictions on those already limited data are common <ref> [ZEWH95] </ref>. The data analysis procedure should ideally be selected when the experiment is planned, because the choice of data analysis procedures is dependent on the experimental design as well as the collected data. We discuss briefly significance level, power analysis, parametric analysis procedures, nonparametric analysis procedures, and other pattern-recognition approaches.
References-found: 48

