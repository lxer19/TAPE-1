URL: http://www.cs.ucsb.edu/~cfu/papers/IPPS96.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/RAPID.html
Root-URL: http://www.cs.ucsb.edu
Title: Efficient Run-time Support for Irregular Task Computations with Mixed Granularities  
Author: Cong Fu and Tao Yang 
Web: http://www.cs.ucsb.edu/f~cfu,~tyangg  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: Many irregular scientific computing problems can be modeled by directed acyclic task graphs (DAGs). In this paper, we present an efficient run-time system for executing general asynchronous DAG schedules on distributed memory machines. Our solution tightly integrates the run-time scheme with a fast communication mechanism to eliminate unnecessary overhead in message buffering and copying, and takes advantage of task dependence properties to ensure the correctness of execution. We demonstrate the applications of this scheme in sparse LU and Cholesky factorizations for which actual speedups have been hard to obtain in the literature because parallelism in these problems is irregular and limited. Our experiments on Meiko CS-2 show the promising results of our approach in exploiting irregular task parallelism with mixed granularities. 
Abstract-found: 1
Intro-found: 1
Reference: [BCL + 95] <author> E. A. Brewer, F. T. Chong, L. T. Liu, S. D. Sharma, and J. Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 42-53, </pages> <year> 1995. </year>
Reference-contexts: Additionally, a careful network polling is required for efficiently implementing active messages as demonstrated in [CSBS95]. In the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code <ref> [BCL + 95] </ref>. The implementation in [CSBS95] takes advantage of commutativity in triangular solving and as soon as one data item arrives at the destination, an arithmetic operation can be performed.
Reference: [BF93] <author> Richard L. Burden and J. Douglas Faires. </author> <title> Numerical Analysis. </title> <booktitle> PWS, fifth edition, </booktitle> <year> 1993. </year>
Reference-contexts: The boundary conditions usually make the first row of matrices strictly diagonally dominant. For such matrices, it can be shown [Cai95] that stable solutions can be obtained by LU without pivoting. The proof in [Cai95] is based on a modification to Theorem 6.19 in <ref> [BF93] </ref>. It should be noted that parallel sparse LU with global pivoting is more complicated. For a dense matrix, the BLAS-3 level LU factorization without pivoting is shown in Figure 4.
Reference: [Cai95] <author> W. Cai. </author> <type> Personal Communication, </type> <year> 1995. </year>
Reference-contexts: Many of algebraic systems arising from the discretization of diffusion and convection problems in fluid dynamics [PT83] are nonsymmetric and diagonally dominant. The boundary conditions usually make the first row of matrices strictly diagonally dominant. For such matrices, it can be shown <ref> [Cai95] </ref> that stable solutions can be obtained by LU without pivoting. The proof in [Cai95] is based on a modification to Theorem 6.19 in [BF93]. It should be noted that parallel sparse LU with global pivoting is more complicated. <p> The boundary conditions usually make the first row of matrices strictly diagonally dominant. For such matrices, it can be shown <ref> [Cai95] </ref> that stable solutions can be obtained by LU without pivoting. The proof in [Cai95] is based on a modification to Theorem 6.19 in [BF93]. It should be noted that parallel sparse LU with global pivoting is more complicated. For a dense matrix, the BLAS-3 level LU factorization without pivoting is shown in Figure 4.
Reference: [CF87] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? The Value of Renaming for Parallelism Detection and Storage Allocation. </title> <booktitle> In Proc. of International Conf. on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: With the presence of anti and output dependence, run-time synchronization becomes more complicated in order to ensure the correctness of execution. In functional languages, the single assignment" principle has been used for data flow computation [Sar89]. Renaming techniques <ref> [CF87] </ref> can eliminate the output and anti dependence in transforming a DDG to a single assignment" graph. The advantage of such principle is that dependence enforcement in task execution can be simplified to a certain degree. The disadvantage is that memory usage and data copying overhead increase accordingly.
Reference: [CR92] <author> Y-C. Chung and S. Ranka. </author> <title> Applications and Performance Analysis of a Compile-time Optimization Approach for List Scheduling Algorithms on Distributed Memory Multiprocessor. </title> <booktitle> In Proc. of Supercomputing '92, </booktitle> <year> 1992. </year>
Reference-contexts: Algorithms for the static scheduling of DAGs have been extensively studied in the literature, e.g. <ref> [CR92, KB88, Sar89, YG92] </ref>. Dynamic scheduling which is adaptive to run-time change of dependence structures and weights is more attractive [Pol88], but it is still difficult to balance its benefits and the run-time control overhead in executing tasks with mixed grains on message-passing machines.
Reference: [CRY94] <author> S. Chakrabarti, A. Ranade, and K. Yelick. </author> <title> Randomized Load Balancing for Tree-structured Computation. </title> <booktitle> In Proc. of Scalable High-Performance Computing Conference, </booktitle> <pages> pages 666-673, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: The automatic parallelization of such problems on distributed memory machines is extremely difficult and presents a great challenge. Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations <ref> [CRY94, LB94, GJY95] </ref>. For iterative irregular problems in which communication and computation phases alternate, the CHAOS/PARTI system [DUSH94] has used the inspector/executor approach to exploit irregular parallelism at each iteration of the computation phase.
Reference: [CSBS95] <author> F. T. Chong, S. D. Sharma, E. A. Brewer, and J. Saltz. </author> <title> Multiprocessor Runtime Support for Fine-Grained Irregular DAGs. In Toward Teraflop Computing and New Grand Challenge Applications., </title> <address> New York, 1995. </address> <publisher> Nova Science Publishers. </publisher>
Reference-contexts: However the run-time overhead for task execution is still large and there is still a great deal of room for obtaining better absolute performance. Recently the work by <ref> [CSBS95] </ref> demonstrates that using both effective DAG scheduling and low-overhead communication mechanisms, scalable performance can be obtained on fine-grained DAGs for solving sparse triangular systems. In this paper, we present an efficient run-time support system for executing general DAG computations with mixed granular-ities. <p> The overhead is tolerable with coarse grain computation, but the lesson learned from the PYRROS project is that these techniques are not sufficient to make sending/receiving overhead low enough to efficiently execute task schedules arising from sparse matrix computation with mixed granularities. Active messages [vECGS92] have been used in <ref> [CSBS95] </ref> for executing fine-grained triangular solving DAGs. We find that active messages offer more functionality than what is needed to support the DAG schedule execution, thus with the disadvantage of imposing extra overhead. Additionally, a careful network polling is required for efficiently implementing active messages as demonstrated in [CSBS95]. <p> used in <ref> [CSBS95] </ref> for executing fine-grained triangular solving DAGs. We find that active messages offer more functionality than what is needed to support the DAG schedule execution, thus with the disadvantage of imposing extra overhead. Additionally, a careful network polling is required for efficiently implementing active messages as demonstrated in [CSBS95]. In the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code [BCL + 95]. The implementation in [CSBS95] takes advantage of commutativity in triangular solving and as soon as one data item arrives at the destination, an arithmetic operation can <p> Additionally, a careful network polling is required for efficiently implementing active messages as demonstrated in <ref> [CSBS95] </ref>. In the situation of task parallelism with mixed granularities, it is not easy to decide where to insert the polling code [BCL + 95]. The implementation in [CSBS95] takes advantage of commutativity in triangular solving and as soon as one data item arrives at the destination, an arithmetic operation can be performed.
Reference: [DUSH94] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication Optimizations for Irregular Scientific Computations on Distributed Memory Architectures . Journal of Parallel and Distributed Computing, </title> <booktitle> 22(3) </booktitle> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations [CRY94, LB94, GJY95]. For iterative irregular problems in which communication and computation phases alternate, the CHAOS/PARTI system <ref> [DUSH94] </ref> has used the inspector/executor approach to exploit irregular parallelism at each iteration of the computation phase. The key idea of this approach is to identify data accessing patterns at run-time and optimize data distribution and communication for the rest of computation.
Reference: [FY95] <author> C. Fu and T. Yang. </author> <title> Run-time Techniques for Exploiting Irregular Task Parallelismon Distributed Memory Architectures. </title> <type> Technical Report TRCS95-21, </type> <institution> Dept. of Computer Science, UCSB, </institution> <year> 1995. </year> <note> http://www.cs.ucsb.edu/Research/rapid sweb/RAPID.html. </note>
Reference-contexts: If the resulting graph contains true data dependence only, then this graph is a task graph. Otherwise other transformations are necessary to remove the remaining anti and output dependence <ref> [FY95] </ref>. We call this kind of task graph a dependence-complete task graph. Formally a task graph G is dependence-complete if it satisfies the following four properties: * DA1: A task only uses distinct data items. <p> For example, the sparse LU graphs discussed in Section 5.1 are dependence-complete. One disadvantage of a dependence-complete task graph (or DDG) is that it cannot model parallelism arising from commutative operations. Cholesky factorization is one typical example. In <ref> [FY95] </ref> we have proposed an approach that uses a dependence-incomplete task graph to model the Cholesky factorization before graph scheduling. <p> Then the destination processor will not have the correct copy it needs. Fortunately, in the following theorems, we can show that if a task graph is dependence-complete, the above situations will never happen. The proofs can be found in <ref> [FY95] </ref>. Theorem 1 No receiver site inconsistency. For any dependence-complete graph, if a task T y defines m and sends it to task T x , then before T x reads it, no other task will overwrite it. Theorem 2 No sender site inconsistency. <p> An important property of sparse Cholesky factorization is that all the operations for updating the same block commute. Parallelism from this property is called fan-in parallelism in the literature [HNP91]. Fan-out parallelism in Cholesky factorization is naturally included in the task graph model as submatrix multicasting edges. In <ref> [FY95] </ref> we have given a method that exploits parallelism from commuting operations so that better parallel performance can be achieved. Matrix #tasks #edges Floating ops MFLOPS B15 67,621 177,179 157,398,044 5.89 Table 3: Statistics of B15 and B24 for Sparse Cholesky.
Reference: [GJY95] <author> A. Gerasoulis, J. Jiao, and T. Yang. </author> <title> Scheduling of Structured and Unstructured Computation . In Dominique Sotteau D. </title> <editor> Frank Hsu, Arnold Rosenberg, editor, </editor> <title> Interconnections Networks and Mappings and Scheduling Parallel Computation. </title> <journal> American Math. Society, </journal> <year> 1995. </year>
Reference-contexts: The automatic parallelization of such problems on distributed memory machines is extremely difficult and presents a great challenge. Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations <ref> [CRY94, LB94, GJY95] </ref>. For iterative irregular problems in which communication and computation phases alternate, the CHAOS/PARTI system [DUSH94] has used the inspector/executor approach to exploit irregular parallelism at each iteration of the computation phase. <p> In a more complicated iterative computation, each iteration may involve task parallelism with irregular dependence structures. For example, in the adaptive n-body simulation using the fast multipole method, parallelism at each time step can be modeled as a directed acyclic task graph (DAG) <ref> [GJY95] </ref>. A DAG schedule can be re-used for a number of iterations before rescheduling since particles movement is slow. Then good speedups are obtained on nCUBE-2 for large n-body simulation problems after applying PYRROS DAG scheduling techniques [YG92]. <p> Static scheduling can tolerate a certain degree of run-time weight variation <ref> [GJY95, YFGS95] </ref> and its run-time control mechanism is relatively simple. <p> This is partly because of the limited parallelism in these problems. Another important reason is that the predetermined schedule does not match actual run-time situation very well because task computation weights vary at run-time. The performance becomes even more sensitive when task granularities are small <ref> [GJY95] </ref>. If we look at the distribution of the sizes of supernodes (after blocking) as shown in Table 4, we can see that for both matrices, there are a lot of blocks with sizes less than 6 fi 6.
Reference: [GY93] <author> A. Gerasoulis and T. Yang. </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs . IEEE Transactions on Parallel and Distributed Systems, </title> <booktitle> 4(6) </booktitle> <pages> 686-701, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: However compared to large n-body simulations, efficient execution of DAG schedules for sparse matrix problems is more challenging because the partitioned graphs contain both coarse and fine grained tasks while in the DAGs arising from large n-body simulations, most of the tasks are typically coarse-grained <ref> [GY93] </ref>.
Reference: [HNP91] <author> M. Heath, E. Ng, and B. Peyton. </author> <title> Parallel Algorithms for Sparse Linear Systems . SIAM Review, </title> <booktitle> 33(3) </booktitle> <pages> 420-460, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The task graph operates on those N 2 submatrices. Given a sparse matrix, we first use the multiple minimum degree algorithm to decide an ordering and then calculate fill-ins. After that we use supernode partitioning <ref> [HNP91] </ref> so that every basic task operation involves only dense matrix or vector operations, which can be implemented using BLAS and LAPACK routines. We also partition supernodes into smaller blocks. And for those supernodes smaller than the block size used, they will remain unchanged. <p> We will examine the performance of general task scheduling and executing techniques applied to this problem. An important property of sparse Cholesky factorization is that all the operations for updating the same block commute. Parallelism from this property is called fan-in parallelism in the literature <ref> [HNP91] </ref>. Fan-out parallelism in Cholesky factorization is naturally included in the task graph model as submatrix multicasting edges. In [FY95] we have given a method that exploits parallelism from commuting operations so that better parallel performance can be achieved.
Reference: [Kar91] <author> N. Karmarkar. </author> <title> A New Parallel Architecture for Sparse Matrix Computation Based on Finite Project Geometries . In Proc. </title> <booktitle> of Supercomputing '91, </booktitle> <pages> pages 358-369, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: In solving nonlinear equations using iterative methods, sparse matrix factorization usually dominates the computation time at each iteration step. The topology of iteration matrices usually remain the same but the numerical values in these matrices change at each step <ref> [Kar91] </ref>. Efficient paral-lelization of sparse factorization requires certain compilation cost, but the optimized solution can be used for many iterations. Sparse matrix factorizations can be modeled as DAGs [Sch93].
Reference: [KB88] <author> S.J. Kim and J.C. Browne. </author> <title> A General Approach to Mapping of Parallel Computation upon Multiprocessor Architectures, </title> . <booktitle> In Proc. of Inter. Conf. on Parallel Processing, </booktitle> <pages> pages 318-328, </pages> <year> 1988. </year>
Reference-contexts: Algorithms for the static scheduling of DAGs have been extensively studied in the literature, e.g. <ref> [CR92, KB88, Sar89, YG92] </ref>. Dynamic scheduling which is adaptive to run-time change of dependence structures and weights is more attractive [Pol88], but it is still difficult to balance its benefits and the run-time control overhead in executing tasks with mixed grains on message-passing machines.
Reference: [LB94] <author> A. Lain and P. Banerjee. </author> <title> Techniques to Overlap Computation and Communication in Irregular Iterative Applications. </title> <booktitle> In Proc. of ACM Inter. Conf. on Supercomputing, </booktitle> <pages> pages 236-245, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The automatic parallelization of such problems on distributed memory machines is extremely difficult and presents a great challenge. Automatic scheduling and load balancing techniques are useful in exploiting irregular parallelism in unstructured computations <ref> [CRY94, LB94, GJY95] </ref>. For iterative irregular problems in which communication and computation phases alternate, the CHAOS/PARTI system [DUSH94] has used the inspector/executor approach to exploit irregular parallelism at each iteration of the computation phase.
Reference: [Pol88] <author> C. D. </author> <title> Polychronopoulos. </title> <publisher> Parallel Programming and Compilers . Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Algorithms for the static scheduling of DAGs have been extensively studied in the literature, e.g. [CR92, KB88, Sar89, YG92]. Dynamic scheduling which is adaptive to run-time change of dependence structures and weights is more attractive <ref> [Pol88] </ref>, but it is still difficult to balance its benefits and the run-time control overhead in executing tasks with mixed grains on message-passing machines. Static scheduling can tolerate a certain degree of run-time weight variation [GJY95, YFGS95] and its run-time control mechanism is relatively simple.
Reference: [PT83] <author> R. Peyret and T. D. Taylor. </author> <title> Computational Methods for Fluid Flow . Springer-Verlag, </title> <year> 1983. </year>
Reference-contexts: LU without pivoting can be used for positive definite or strictly diagonally dominant matrices. Many of algebraic systems arising from the discretization of diffusion and convection problems in fluid dynamics <ref> [PT83] </ref> are nonsymmetric and diagonally dominant. The boundary conditions usually make the first row of matrices strictly diagonally dominant. For such matrices, it can be shown [Cai95] that stable solutions can be obtained by LU without pivoting.
Reference: [Rot92] <author> E. Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization . PhD thesis, </title> <institution> Stanford University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: We demonstrate the applications of our techniques for sparse matrix factorizations. It has been very difficult to obtain actual speedups by hand-made code for these problems, not to mention the automatically parallelized code. Recently impressive performance has been obtained in sparse Cholesky factorization <ref> [Rot92, RS94] </ref>. Our experiments will show that our automatically scheduled code delivers acceptable performance. The paper is organized as follows. Section 2 describes the task computation model. Section 3 identifies critical issues in developing an efficient run-time system. <p> As far as we know, this is the best performance achieved by automatically scheduled code. We will discuss the overhead of the run-time execution in Section 5.3. 5.2 Sparse Cholesky factorization Sequential and parallel sparse Cholesky factorization algorithms have been studied extensively in <ref> [Rot92, RS94] </ref>. They show that the supernode-based approach can deliver good performance in both shared and distributed memory machines. We will examine the performance of general task scheduling and executing techniques applied to this problem. <p> For BCSSTK15, 78% of the blocks are of size 1 fi 1, which makes the corresponding tasks just a few floating point operations. In order to improve the performance further, our future work is to consider techniques of increasing supernode sizes, e.g., supernode amalgamation <ref> [Rot92] </ref>. Dynamic scheduling will be more adaptive to run-time weight variation, but it also increases run-time overhead significantly. 6 Conclusions In this paper, we have presented an efficient run-time scheme for supporting task computations and shown its applications in irregular sparse LU and Cholesky factorization.
Reference: [RS94] <author> E. Rothberg and R. Schreiber. </author> <title> Improved Load Distribution in Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proc. of Supercomputing'94, </booktitle> <pages> pages 783-792, </pages> <year> 1994. </year>
Reference-contexts: We demonstrate the applications of our techniques for sparse matrix factorizations. It has been very difficult to obtain actual speedups by hand-made code for these problems, not to mention the automatically parallelized code. Recently impressive performance has been obtained in sparse Cholesky factorization <ref> [Rot92, RS94] </ref>. Our experiments will show that our automatically scheduled code delivers acceptable performance. The paper is organized as follows. Section 2 describes the task computation model. Section 3 identifies critical issues in developing an efficient run-time system. <p> As far as we know, this is the best performance achieved by automatically scheduled code. We will discuss the overhead of the run-time execution in Section 5.3. 5.2 Sparse Cholesky factorization Sequential and parallel sparse Cholesky factorization algorithms have been studied extensively in <ref> [Rot92, RS94] </ref>. They show that the supernode-based approach can deliver good performance in both shared and distributed memory machines. We will examine the performance of general task scheduling and executing techniques applied to this problem. <p> The performance of Sparse Cholesky factorization without exploring the commutativity is shown in Figure 7. The speedups on 32 processors are 9:6 for B15 and 12 for B24. Therefore, processor utilization efficiency are 30% and 37:5% respectively. In <ref> [RS94] </ref>, efficiency of 25% has been reported for B15 on 64 processors (Intel Paragon) after improving the load balancing strategy, which indicates parallelism in these problems is limited.
Reference: [Sar89] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors . MIT Press, </title> <year> 1989. </year>
Reference-contexts: Algorithms for the static scheduling of DAGs have been extensively studied in the literature, e.g. <ref> [CR92, KB88, Sar89, YG92] </ref>. Dynamic scheduling which is adaptive to run-time change of dependence structures and weights is more attractive [Pol88], but it is still difficult to balance its benefits and the run-time control overhead in executing tasks with mixed grains on message-passing machines. <p> With the presence of anti and output dependence, run-time synchronization becomes more complicated in order to ensure the correctness of execution. In functional languages, the single assignment" principle has been used for data flow computation <ref> [Sar89] </ref>. Renaming techniques [CF87] can eliminate the output and anti dependence in transforming a DDG to a single assignment" graph. The advantage of such principle is that dependence enforcement in task execution can be simplified to a certain degree.
Reference: [Sch93] <author> R. Schreiber. </author> <title> Scalability of Sparse Direct Solvers, </title> <booktitle> volume 56 of Graph Theory and Sparse Matrix Computation, </booktitle> <pages> pages 191-209. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: The topology of iteration matrices usually remain the same but the numerical values in these matrices change at each step [Kar91]. Efficient paral-lelization of sparse factorization requires certain compilation cost, but the optimized solution can be used for many iterations. Sparse matrix factorizations can be modeled as DAGs <ref> [Sch93] </ref>. However compared to large n-body simulations, efficient execution of DAG schedules for sparse matrix problems is more challenging because the partitioned graphs contain both coarse and fine grained tasks while in the DAGs arising from large n-body simulations, most of the tasks are typically coarse-grained [GY93].
Reference: [SS95] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In Proc. of 9th International Parallel Processing Symposium, </booktitle> <pages> pages 140-149, </pages> <year> 1995. </year>
Reference-contexts: RMA can be implemented in modern multi-processor architectures such as Cray-T3D and Meiko CS-2 <ref> [SS95] </ref>. With RMA, a processor can write to memory of any other processor if a remote address is given. RMA allows passing data directly from one source location to a destination location, without any copying, packing/unpacking and buffering. <p> We have implemented our system on the Meiko CS-2 which provides Direct Memory Access (DMA) as the major way to access non-local memory. Each node of the Meiko CS-2 is also equipped with a DMA co-processor for handling communication. It takes the main processor 9 microseconds <ref> [SS95] </ref> to dispatch a remote memory access descriptor to the co-processor. <p> The single-node LINPACK performance we have tested is 8:04MFLOPS for double-precision floating operations 1 . The communication peak bandwidth of Meiko CS-2 is 40MBytes/Sec. The effective bandwidth has been reported as 39MB/Sec and this is obtained through a ping-pong test <ref> [SS95] </ref>. In practice, a message is sent only once at a time. We have measured the effective bandwidth of a single-message sending or direct memory access. When the sizes of messages vary from 1K to 10K, the bandwidth is about 10 25MBytes/Sec.
Reference: [vECGS92] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: The overhead is tolerable with coarse grain computation, but the lesson learned from the PYRROS project is that these techniques are not sufficient to make sending/receiving overhead low enough to efficiently execute task schedules arising from sparse matrix computation with mixed granularities. Active messages <ref> [vECGS92] </ref> have been used in [CSBS95] for executing fine-grained triangular solving DAGs. We find that active messages offer more functionality than what is needed to support the DAG schedule execution, thus with the disadvantage of imposing extra overhead.
Reference: [VNS92] <author> S. Venugopal, V. Naik, and J. Saltz. </author> <title> Performance of Distributed Sparse Cholesky Factorization with Pre-scheduling. </title> <booktitle> In Proc. of Supercomputing'92, </booktitle> <pages> pages 52-61, </pages> <address> Minneapolis, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: In <ref> [VNS92] </ref>, static task and communication scheduling is used for sparse Cholesky factorization, and they found that the pre-scheduling improves the performance of distributed factorization by 30% to 40%.
Reference: [YFGS95] <author> T. Yang, C. Fu, A. Gerasoulis, and V. Sarkar. </author> <title> Mapping Iterative Task Graphs on Distributed-memory Machines . In Proc. </title> <booktitle> of 24th Inter. Conf. on Parallel Processing, </booktitle> <pages> pages 151-158, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Static scheduling can tolerate a certain degree of run-time weight variation <ref> [GJY95, YFGS95] </ref> and its run-time control mechanism is relatively simple.
Reference: [YG92] <author> T. Yang and A. Gerasoulis. </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors . In Proc. </title> <booktitle> of 6th ACM Inter. Confer. on Supercomputing, </booktitle> <pages> pages 428-437, </pages> <year> 1992. </year>
Reference-contexts: A DAG schedule can be re-used for a number of iterations before rescheduling since particles movement is slow. Then good speedups are obtained on nCUBE-2 for large n-body simulation problems after applying PYRROS DAG scheduling techniques <ref> [YG92] </ref>. Such results demonstrate that graph scheduling can effectively exploit irregular task parallelism to balance loads and overlap computation with communication. In solving nonlinear equations using iterative methods, sparse matrix factorization usually dominates the computation time at each iteration step. <p> The early work in the PYRROS system <ref> [YG92] </ref> provides a complete framework for general task computation; however, its run-time support system has significant overheads in managing message buffers and copying data between user and system space, which prevents PYRROS from obtaining good performance in executing sparse matrix computations with mixed granularities. <p> Algorithms for the static scheduling of DAGs have been extensively studied in the literature, e.g. <ref> [CR92, KB88, Sar89, YG92] </ref>. Dynamic scheduling which is adaptive to run-time change of dependence structures and weights is more attractive [Pol88], but it is still difficult to balance its benefits and the run-time control overhead in executing tasks with mixed grains on message-passing machines. <p> Optimizations have to be performed to eliminate redundant messages. Correspondingly, at the destination only one receive operation is needed to pull out data from the network. Subsequent receives for this data item should be re-directed to read from the local memory instead. PYRROS <ref> [YG92] </ref> uses a buffered message-passing mechanism and aggregates messages as much as possible.
References-found: 26

