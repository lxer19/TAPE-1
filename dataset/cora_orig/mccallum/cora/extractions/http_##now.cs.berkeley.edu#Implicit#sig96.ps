URL: http://now.cs.berkeley.edu/Implicit/sig96.ps
Refering-URL: http://now.cs.berkeley.edu/Implicit/
Root-URL: 
Email: cullerg@CS.Berkeley.EDU  
Title: Effective Distributed Scheduling of Parallel Workloads  
Author: Andrea C. Dusseau, Remzi H. Arpaci, and David E. Culler fdusseau, remzi, 
Affiliation: Computer Science Division University of California, Berkeley  
Abstract: We present a distributed algorithm for time-sharing parallel workloads that is competitive with coscheduling. Implicit scheduling allows each local scheduler in the system to make independent decisions that dynamically coordinate the scheduling of cooperating processes across processors. Of particular importance is the blocking algorithm which decides the action of a process waiting for a communication or synchronization event to complete. Through simulation of bulk-synchronous parallel applications, we find that a simple two-phase fixed-spin blocking algorithm performs well; a two-phase adaptive algorithm that gathers run-time data on barrier wait-times performs slightly better. Our results hold for a range of machine parameters and parallel program characteristics. These findings are in direct contrast to the literature that states explicit coscheduling is necessary for fine-grained programs. We show that the choice of the local scheduler is crucial, with a priority-based scheduler performing two to three times better than a round-robin scheduler. Overall, we find that the performance of implicit scheduling is near that of coscheduling (+/- 35%), without the requirement of explicit, global coordination. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. H. Arpaci, D. E. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: When jobs are coscheduled, increasing network latency simply increases the two-phase fixed-spin is used instead of coscheduling. Each process spins for the context-switch time (200 s) before blocking on its communication and synchronization events. System Latency Source (s) Cray T3D 2 <ref> [1] </ref> TMC CM-5 6 [41] Intel Paragon 6 [9] FDDI 6 [28] Myrinet 11 [9] Fore ATM 33 [40] Switched Ethernet 52 [23] LattisCell ATM 104 [23] Table 1: Latency on Current Networks. Measured latencies on modern MPP and workstation networks. amount of time processes spin-wait for communication to complete.
Reference: [2] <author> R. H. Arpaci, A. C. Dusseau, A. M. Vahdat, L. T. Liu, T. E. Anderson, and D. A. Patterson. </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations. </title> <booktitle> In Proceedings of ACM SIGMETRICS'95/PERFORMANCE'95 Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 267-278, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: If interactive jobs are scheduled in the same round-robin manner as coscheduled parallel jobs, then users experience excessive delays. Preserving interactive response time requires raising the priority of interactive jobs; however, this perturbs the scheduling of parallel jobs, drastically reducing the performance of frequently communicating parallel applications <ref> [2, 4, 26] </ref>. Coscheduling also requires that processes busy-wait during I/O, wasting processor cycles and reducing throughput. Local scheduling parallel jobs, where the existing operating system on each processor schedules processes independently, has a number of potential structural and performance advantages over coscheduling. <p> For these reasons, we are motivated to investigate whether or not local schedulers can be used to effectively schedule parallel workloads. Several previous researchers have compared the performance of coscheduling to approaches using the local sched-ulers, and have unanimously concluded that local scheduling is insufficient for fine-grained parallel applications <ref> [2, 8, 15, 16, 19] </ref>. The poor performance of local scheduling occurs because cooperating processes are not scheduled simultaneously across processors; as a result, when a process attempts to communicate or synchronize with another process, the other may not be scheduled. <p> Previous work comparing the performance of coschedul-ing to local scheduling has unanimously concluded that explicit coscheduling is necessary for fine-grained parallel applications, since local scheduling did not coordinate the communication phases across processes <ref> [2, 8, 15] </ref>. However, many of these studies did not thoroughly investigate the impact of different blocking algorithms, local schedulers, or programming models. Most studies have considered only spin-waiting or immediate blocking; those considering two-phase blocking have not thoroughly investigated the effect of different fixed-spin times.
Reference: [3] <author> K. Arvind. </author> <title> Probabalistic Clock Synchronization in Distributed Systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(5) </volume> <pages> 474-87, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The worst-case performance is now only 15% slower than when coscheduled, compared to 30% when timer skew exists. The performance of the most fine-grain computations (i.e., g=100 s) is now identical to coscheduling. There are many algorithms that perform fine-grained clock synchronization in a distributed system <ref> [3, 34] </ref>. Whether or not the implementation cost of synchronizing the machines is worth the performance benefits remains to be seen. 7.2 Round-Robin Scheduling One particularly important component to local scheduling is the local scheduling algorithm running on each node in the system.
Reference: [4] <author> M. J. Atallah, C. L. Black, D. C. Marinescu, H. J. Siegel, and T. L. Casavant. </author> <title> Models and Algorithms for Co-scheduling Compute-Intensive Tasks on a Network of Workstations. </title> <journal> Journal of Parallel and Distrbuted Computing, </journal> <volume> 16 </volume> <pages> 319-327, </pages> <year> 1992. </year>
Reference-contexts: If interactive jobs are scheduled in the same round-robin manner as coscheduled parallel jobs, then users experience excessive delays. Preserving interactive response time requires raising the priority of interactive jobs; however, this perturbs the scheduling of parallel jobs, drastically reducing the performance of frequently communicating parallel applications <ref> [2, 4, 26] </ref>. Coscheduling also requires that processes busy-wait during I/O, wasting processor cycles and reducing throughput. Local scheduling parallel jobs, where the existing operating system on each processor schedules processes independently, has a number of potential structural and performance advantages over coscheduling.
Reference: [5] <author> R. Chandra, S. Devine, B. Verghese, A. Gupta, and M. Rosen-blum. </author> <title> Scheduling and Page Migration for Multiprocessor Compute Servers. </title> <booktitle> In International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput.
Reference: [6] <author> J. Chapin, M. Rosenblum, S. Devine, T. Lahiri, D. Teodosiu, and A. Gupta. Hive: </author> <title> Fault Containment for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 12-25, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: First, local scheduling exists on every machine, requiring no additional implementation. Since each scheduler is independent, the system trivially provides fault-containment: a parallel job running on a subset of machines is not affected by the failure of others <ref> [6] </ref>. Second, time-sharing priority-based local schedulers have been carefully tuned for both interactive and I/O-intensive processes [13, 18, 25]. To improve throughput, jobs waiting on I/O relinquish the processor; to improve response time, jobs that block frequently are given higher priority when they have computation.
Reference: [7] <author> S.-H. Chiang, R. K. Mansharamani, and M. K. Vernon. </author> <title> Use of Application Characteristics and Limited Preemption for Run-To-Completion Parallel Processor Scheduling Policies. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 34-44, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput.
Reference: [8] <author> M. Crovella, P. Das, C. Dubnicki, T. LeBlanc, and E. Markatos. </author> <title> Multiprogramming on Multiprocessors. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 590-597, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: For these reasons, we are motivated to investigate whether or not local schedulers can be used to effectively schedule parallel workloads. Several previous researchers have compared the performance of coscheduling to approaches using the local sched-ulers, and have unanimously concluded that local scheduling is insufficient for fine-grained parallel applications <ref> [2, 8, 15, 16, 19] </ref>. The poor performance of local scheduling occurs because cooperating processes are not scheduled simultaneously across processors; as a result, when a process attempts to communicate or synchronize with another process, the other may not be scheduled. <p> Previous work comparing the performance of coschedul-ing to local scheduling has unanimously concluded that explicit coscheduling is necessary for fine-grained parallel applications, since local scheduling did not coordinate the communication phases across processes <ref> [2, 8, 15] </ref>. However, many of these studies did not thoroughly investigate the impact of different blocking algorithms, local schedulers, or programming models. Most studies have considered only spin-waiting or immediate blocking; those considering two-phase blocking have not thoroughly investigated the effect of different fixed-spin times. <p> We can also see that our cosched-uled environment is well-tuned; i.e., no idle time exists and the time spent context-switching is negligible, due to the long time quantum (500 ms) relative to the context-switch cost (200 s). 4 Local Scheduling with Immediate Blocking In this section, we verify previous studies <ref> [8, 15] </ref> that showed coscheduling is superior to local scheduling with immediate blocking for fine-grained parallel jobs.
Reference: [9] <author> D. Culler, L. T. Liu, R. Martin, and C. Yoshikawa. </author> <title> LogP Performance Assessment of Fast Network Interfaces. </title> <booktitle> IEEE Micro, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: When jobs are coscheduled, increasing network latency simply increases the two-phase fixed-spin is used instead of coscheduling. Each process spins for the context-switch time (200 s) before blocking on its communication and synchronization events. System Latency Source (s) Cray T3D 2 [1] TMC CM-5 6 [41] Intel Paragon 6 <ref> [9] </ref> FDDI 6 [28] Myrinet 11 [9] Fore ATM 33 [40] Switched Ethernet 52 [23] LattisCell ATM 104 [23] Table 1: Latency on Current Networks. Measured latencies on modern MPP and workstation networks. amount of time processes spin-wait for communication to complete. <p> Each process spins for the context-switch time (200 s) before blocking on its communication and synchronization events. System Latency Source (s) Cray T3D 2 [1] TMC CM-5 6 [41] Intel Paragon 6 <ref> [9] </ref> FDDI 6 [28] Myrinet 11 [9] Fore ATM 33 [40] Switched Ethernet 52 [23] LattisCell ATM 104 [23] Table 1: Latency on Current Networks. Measured latencies on modern MPP and workstation networks. amount of time processes spin-wait for communication to complete.
Reference: [10] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural Requirements of Parallel Scientific Applications with Explicit Communication. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: It has been our experience, as well as the experience of others <ref> [10, 11, 12, 20, 38] </ref>, that the structure of many parallel applications is bulk-synchronous; i.e., phases of purely local computation alternate with phases of inter-processor communication. Figure 1 demonstrates the programming model that we use in this study.
Reference: [11] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1979. </year>
Reference-contexts: It has been our experience, as well as the experience of others <ref> [10, 11, 12, 20, 38] </ref>, that the structure of many parallel applications is bulk-synchronous; i.e., phases of purely local computation alternate with phases of inter-processor communication. Figure 1 demonstrates the programming model that we use in this study.
Reference: [12] <author> A. Dusseau, D. Culler, K. Schauser, and R. Martin. </author> <title> Fast Parallel Sorting under LogP: Experience with the CM-5. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1996. To Appear. </note>
Reference-contexts: It has been our experience, as well as the experience of others <ref> [10, 11, 12, 20, 38] </ref>, that the structure of many parallel applications is bulk-synchronous; i.e., phases of purely local computation alternate with phases of inter-processor communication. Figure 1 demonstrates the programming model that we use in this study.
Reference: [13] <author> S. Evans, K. Clarke, D. Singleton, and B. Smaalders. </author> <title> Optimizing Unix Resource Scheduling for User Interaction. </title> <booktitle> In 1993 Summer Usenix, </booktitle> <pages> pages 205-218. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: Since each scheduler is independent, the system trivially provides fault-containment: a parallel job running on a subset of machines is not affected by the failure of others [6]. Second, time-sharing priority-based local schedulers have been carefully tuned for both interactive and I/O-intensive processes <ref> [13, 18, 25] </ref>. To improve throughput, jobs waiting on I/O relinquish the processor; to improve response time, jobs that block frequently are given higher priority when they have computation. For these reasons, we are motivated to investigate whether or not local schedulers can be used to effectively schedule parallel workloads.
Reference: [14] <author> D. G. Feitelson and L. Rudolph. </author> <title> Distributed Hierarchical Control for Parallel Processing. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The benefit of coscheduling is that cooperating processes behave as if they were running in a dedicated environment and can spin-wait during communication and synchronization. Coscheduling is not well-suited to emerging processing environments and workloads. First, fault-tolerant, scalable coscheduling is non-trivial to design and implement <ref> [14] </ref>. Coscheduling has traditionally been used in tightly-coupled parallel processing environments that are viewed as a single system, where the failure model is the entire machine fails if one processor dies. This view is not practical in a distributed setting such as a network of workstations.
Reference: [15] <author> D. G. Feitelson and L. Rudolph. </author> <title> Gang Scheduling Performance Benefits for Fine-Grained Synchronization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(4) </volume> <pages> 306-318, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: For these reasons, we are motivated to investigate whether or not local schedulers can be used to effectively schedule parallel workloads. Several previous researchers have compared the performance of coscheduling to approaches using the local sched-ulers, and have unanimously concluded that local scheduling is insufficient for fine-grained parallel applications <ref> [2, 8, 15, 16, 19] </ref>. The poor performance of local scheduling occurs because cooperating processes are not scheduled simultaneously across processors; as a result, when a process attempts to communicate or synchronize with another process, the other may not be scheduled. <p> Previous work comparing the performance of coschedul-ing to local scheduling has unanimously concluded that explicit coscheduling is necessary for fine-grained parallel applications, since local scheduling did not coordinate the communication phases across processes <ref> [2, 8, 15] </ref>. However, many of these studies did not thoroughly investigate the impact of different blocking algorithms, local schedulers, or programming models. Most studies have considered only spin-waiting or immediate blocking; those considering two-phase blocking have not thoroughly investigated the effect of different fixed-spin times. <p> We can also see that our cosched-uled environment is well-tuned; i.e., no idle time exists and the time spent context-switching is negligible, due to the long time quantum (500 ms) relative to the context-switch cost (200 s). 4 Local Scheduling with Immediate Blocking In this section, we verify previous studies <ref> [8, 15] </ref> that showed coscheduling is superior to local scheduling with immediate blocking for fine-grained parallel jobs.
Reference: [16] <author> D. G. Feitelson and L. Rudolph. </author> <title> Coscheduling Based on Run-Time Identification of Activity Working Sets. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 23(2) </volume> <pages> 136-160, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: For these reasons, we are motivated to investigate whether or not local schedulers can be used to effectively schedule parallel workloads. Several previous researchers have compared the performance of coscheduling to approaches using the local sched-ulers, and have unanimously concluded that local scheduling is insufficient for fine-grained parallel applications <ref> [2, 8, 15, 16, 19] </ref>. The poor performance of local scheduling occurs because cooperating processes are not scheduled simultaneously across processors; as a result, when a process attempts to communicate or synchronize with another process, the other may not be scheduled. <p> Furthermore, most previous work has considered simplified local schedulers, such as round-robin, rather than the priority-based schedulers used in commercial systems. In a paper exploring the merits of runtime identification of cooperating processes <ref> [16] </ref>, the performance of local scheduling with two-phase blocking is examined. However, little improvement is seen due to the use of a short spin-time relative to the context-switch cost.
Reference: [17] <author> D. Ghosal, G. Serazzi, and S. K. Tripathi. </author> <title> The Processor Working Set and Its Use in Scheduling Multiprocessor Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(5) </volume> <pages> 443-453, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput.
Reference: [18] <author> B. Goodheart and J. Cox. </author> <title> The Magic Garden Explained: The Internals of UNIX System V Release 4. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: Since each scheduler is independent, the system trivially provides fault-containment: a parallel job running on a subset of machines is not affected by the failure of others [6]. Second, time-sharing priority-based local schedulers have been carefully tuned for both interactive and I/O-intensive processes <ref> [13, 18, 25] </ref>. To improve throughput, jobs waiting on I/O relinquish the processor; to improve response time, jobs that block frequently are given higher priority when they have computation. For these reasons, we are motivated to investigate whether or not local schedulers can be used to effectively schedule parallel workloads. <p> Thus, to obtain realistic results, we constructed the local scheduling component of the simulator to closely match that of the Unix System V Release 4 (SVR4) <ref> [18] </ref> scheduler in both functionality and structure; in fact, significant portions of our code are copied directly from Solaris 2.4 source. The objective of a time-sharing scheduler is to schedule each process fairly and efficiently. This goal is realized in SVR4 with a dynamic priority allocation scheme.
Reference: [19] <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications. </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-131, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: For these reasons, we are motivated to investigate whether or not local schedulers can be used to effectively schedule parallel workloads. Several previous researchers have compared the performance of coscheduling to approaches using the local sched-ulers, and have unanimously concluded that local scheduling is insufficient for fine-grained parallel applications <ref> [2, 8, 15, 16, 19] </ref>. The poor performance of local scheduling occurs because cooperating processes are not scheduled simultaneously across processors; as a result, when a process attempts to communicate or synchronize with another process, the other may not be scheduled. <p> Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput. <p> However, little improvement is seen due to the use of a short spin-time relative to the context-switch cost. In an environment much different than ours, the effect of different fixed-spin times is quantified for shared-memory lock synchronization <ref> [19] </ref>; with local scheduling, they find spinning for the context-switch cost gives the higher utilization than other fixed-spin times, but that coscheduling results in the best utilization. A small number of studies have examined the interaction between multiprogramming and adaptive two-phase synchronization algorithms. <p> We will see that spinning for at least twice the context-switch time dynamically coordinates communicating processes, resulting in performance near that of coscheduling. 5.1 Two-Phase Fixed-Spin Previous research has shown that two-phase fixed-spin algorithms with a spin-time equal to the context-switch cost is beneficial when acquiring shared-memory locks <ref> [19, 22] </ref>. The intuition is that spinning is advantageous when waiting for a lock that will be released soon, since spin-waiting saves the processor the cost of switching to another process. However, if the wait time is long, blocking and switching to another process accomplishes more work.
Reference: [20] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0. </title> <type> Draft, </type> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: It has been our experience, as well as the experience of others <ref> [10, 11, 12, 20, 38] </ref>, that the structure of many parallel applications is bulk-synchronous; i.e., phases of purely local computation alternate with phases of inter-processor communication. Figure 1 demonstrates the programming model that we use in this study.
Reference: [21] <author> M. D. Hill, J. R. Larus, S. K. Reinhardt, and D. A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-18, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Note that because we do not currently model the overhead of sending messages, the broadcast is free to the root process. shared memory <ref> [21] </ref>. A read operation is a request-response message pair, constructed by sending a request to the processor containing the desired memory location; the destination process reads the local data and sends the response back to the requester.
Reference: [22] <author> A. Karlin, K. Li, M. Manasse, and S. Owicki. </author> <title> Empirical studies of competitive spinning for a shared-memory multiprocessor. </title> <booktitle> In Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: A small number of studies have examined the interaction between multiprogramming and adaptive two-phase synchronization algorithms. Karlin et. al. <ref> [22] </ref> show that several adaptive algorithms perform better than fixed-spin algorithms for shared-memory lock synchronization. However, appropriate adaptive blocking techniques for barriers are drastically different than those for locks, due to the global cost of blocking at barriers. <p> We will see that spinning for at least twice the context-switch time dynamically coordinates communicating processes, resulting in performance near that of coscheduling. 5.1 Two-Phase Fixed-Spin Previous research has shown that two-phase fixed-spin algorithms with a spin-time equal to the context-switch cost is beneficial when acquiring shared-memory locks <ref> [19, 22] </ref>. The intuition is that spinning is advantageous when waiting for a lock that will be released soon, since spin-waiting saves the processor the cost of switching to another process. However, if the wait time is long, blocking and switching to another process accomplishes more work. <p> Our heuristic is to record each of the wait-times and remove the 4 The adaptive two-phase algorithm that we described differs significantly from the algorithms, such as a random walk, described in <ref> [22] </ref> that worked well for lock synchronization. We found that these algorithms are not effective for barriers because they learn to immediately block, since spinning for any amount of time is not beneficial until the processes are coordinated.
Reference: [23] <author> K. Keeton, T. Anderson, and D. Patterson. </author> <title> LogP Quantified: The Case for Low-Overhead Local Area Networks. </title> <booktitle> In Proceedings of Hot Interconnects III, </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Each process spins for the context-switch time (200 s) before blocking on its communication and synchronization events. System Latency Source (s) Cray T3D 2 [1] TMC CM-5 6 [41] Intel Paragon 6 [9] FDDI 6 [28] Myrinet 11 [9] Fore ATM 33 [40] Switched Ethernet 52 <ref> [23] </ref> LattisCell ATM 104 [23] Table 1: Latency on Current Networks. Measured latencies on modern MPP and workstation networks. amount of time processes spin-wait for communication to complete. Few context-switches occur when processes are coscheduled; therefore, little benefit is gained by reducing their cost. <p> System Latency Source (s) Cray T3D 2 [1] TMC CM-5 6 [41] Intel Paragon 6 [9] FDDI 6 [28] Myrinet 11 [9] Fore ATM 33 [40] Switched Ethernet 52 <ref> [23] </ref> LattisCell ATM 104 [23] Table 1: Latency on Current Networks. Measured latencies on modern MPP and workstation networks. amount of time processes spin-wait for communication to complete. Few context-switches occur when processes are coscheduled; therefore, little benefit is gained by reducing their cost.
Reference: [24] <author> L. I. Kontothanassis and R. W. Wisniewski. </author> <title> Using Scheduler Information to Achieve Optimal Barrier Synchronization Performance. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Previous research applying adaptive algorithms to barrier synchronization has assumed a radically different programming model from ours, in which only threads from the same parallel job may execute on the same processor <ref> [24, 42] </ref>. Finally, in another time-sharing approach currently under development, communicating processes are dynamically coscheduled by using message arrivals to trigger scheduling events [39]. However, the authors do not explore algorithms for releasing the processor on the sending side.
Reference: [25] <author> S. J. Le*er, M. K. McKusick, M. J. Karels, and J. S. Quarter-man. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: Since each scheduler is independent, the system trivially provides fault-containment: a parallel job running on a subset of machines is not affected by the failure of others [6]. Second, time-sharing priority-based local schedulers have been carefully tuned for both interactive and I/O-intensive processes <ref> [13, 18, 25] </ref>. To improve throughput, jobs waiting on I/O relinquish the processor; to improve response time, jobs that block frequently are given higher priority when they have computation. For these reasons, we are motivated to investigate whether or not local schedulers can be used to effectively schedule parallel workloads.
Reference: [26] <author> S. T. Leutenegger and X.-H. Sun. </author> <title> Distributed Computing Feasibility in a Non-Dedicated Homogenous Distributed System. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 143-152, </pages> <year> 1993. </year>
Reference-contexts: If interactive jobs are scheduled in the same round-robin manner as coscheduled parallel jobs, then users experience excessive delays. Preserving interactive response time requires raising the priority of interactive jobs; however, this perturbs the scheduling of parallel jobs, drastically reducing the performance of frequently communicating parallel applications <ref> [2, 4, 26] </ref>. Coscheduling also requires that processes busy-wait during I/O, wasting processor cycles and reducing throughput. Local scheduling parallel jobs, where the existing operating system on each processor schedules processes independently, has a number of potential structural and performance advantages over coscheduling.
Reference: [27] <author> S. T. Leutenegger and M. K. Vernon. </author> <title> The Performance of Multi-programmed Multiprocessor Scheduling Policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput.
Reference: [28] <author> R. P. Martin. HPAM: </author> <title> An Active Message Layer for a Network of Workstations. </title> <booktitle> In Proceedings of Hot Interconnects II, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Each process spins for the context-switch time (200 s) before blocking on its communication and synchronization events. System Latency Source (s) Cray T3D 2 [1] TMC CM-5 6 [41] Intel Paragon 6 [9] FDDI 6 <ref> [28] </ref> Myrinet 11 [9] Fore ATM 33 [40] Switched Ethernet 52 [23] LattisCell ATM 104 [23] Table 1: Latency on Current Networks. Measured latencies on modern MPP and workstation networks. amount of time processes spin-wait for communication to complete.
Reference: [29] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A Dynamic Processor Allocation Policy for Multi-programmed Shared-Memory Multiprocessors. </title> <journal> In ACM Transactions on Computer Systems, </journal> <volume> volume 11, </volume> <pages> pages 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput.
Reference: [30] <author> C. McCann and J. Zahorjan. </author> <title> Processor Allocation Policies for Message-Passing Parallel Computers. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 19-32, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput.
Reference: [31] <author> C. McCann and J. Zahorjan. </author> <title> Scheduling Memory Constrained Jobs on Distributed Memory Parallel Computers. </title> <booktitle> In Proceedings of ACM SIGMETRICS'95/PERFORMANCE'95 Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 208-219, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput.
Reference: [32] <author> L. McVoy and C. Staelin. lmbench: </author> <title> Portable Tools for Performance Analysis. </title> <booktitle> In Proceedings of the 1996 Winter USENIX, </booktitle> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Table 1 shows that latencies vary between 2 and 100 s in current MPP and workstation networks. Similarly, context-switch costs can vary between 10 and 200 s, depending upon whether or not cache effects are taken into account <ref> [32] </ref>. Modeling the cost of a context-switch to the application requires considering the impact of memory; therefore, we examine a moderate and a high context-switch cost: 50 and 200 s, respectively. immediate blocking versus coscheduling over a set of realistic context-switch costs, network latencies, computation granu-larities, and load-imbalances.
Reference: [33] <author> V. K. Naik, S. K. Setia, and M. S. Squillante. </author> <title> Performance Analysis of Job Scheduling Policies in Parallel Supercomputing Environments. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 824-833, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput.
Reference: [34] <author> A. Olson and K. G. Shin. </author> <title> Fault-tolerant Clock Synchronization in Large Multicomputer Systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(9) </volume> <pages> 912-923, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The worst-case performance is now only 15% slower than when coscheduled, compared to 30% when timer skew exists. The performance of the most fine-grain computations (i.e., g=100 s) is now identical to coscheduling. There are many algorithms that perform fine-grained clock synchronization in a distributed system <ref> [3, 34] </ref>. Whether or not the implementation cost of synchronizing the machines is worth the performance benefits remains to be seen. 7.2 Round-Robin Scheduling One particularly important component to local scheduling is the local scheduling algorithm running on each node in the system.
Reference: [35] <author> J. K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In Third International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: We evaluate the performance of implicit scheduling relative to a zero-overhead implementation of coscheduling <ref> [35] </ref>. 0 This work is supported in part by the Advanced Research Project Agency (F30602-95-C-0014), the National Science Foundation (CDA 94-01156 and CCR-9210260), and the California State MICRO Program. Dusseau is also supported by an Intel Foundation Graduate Fellowship. <p> In this paper, we consider the problem after jobs have been al located to a set of processors and focus on how to time-share between competing jobs on the same processor. In 1982, Ousterhout introduced the idea of coschedul-ing <ref> [35] </ref>, in which cooperating processes from a single job are simultaneously scheduled across processors, giving each job the impression that it is running on a dedicated machine.
Reference: [36] <author> V. G. Peris, M. S. Squillante, and V. K. Naik. </author> <title> Analysis of the Impact of Memory in Distributed Parallel Processing Sytems. </title> <booktitle> In 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 5-18, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput.
Reference: [37] <author> K. C. Sevcik. </author> <title> Characterizations of Parallelism in Applications and their Use in Scheduling. </title> <booktitle> In Proceedings of the 1989 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Parallel scheduling is composed of at least two interdependent steps: the allocation of processes to processors (space-sharing) and the scheduling of the processes over time (time-sharing). A large number of studies have focused on the processor allocation step of parallel job scheduling <ref> [5, 7, 17, 19, 27, 29, 30, 31, 33, 36, 37] </ref>; fewer have investigated the second step. We believe that a mixed approach, utilizing both space-sharing and time-sharing, is required to maintain interactive response times and high throughput.
Reference: [38] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: It has been our experience, as well as the experience of others <ref> [10, 11, 12, 20, 38] </ref>, that the structure of many parallel applications is bulk-synchronous; i.e., phases of purely local computation alternate with phases of inter-processor communication. Figure 1 demonstrates the programming model that we use in this study.
Reference: [39] <author> P. G. Sobalvarro and W. E. Weihl. </author> <title> Demand-based Coscheduling of Parallel Jobs on Multiprogrammed Multiprocessors. </title> <booktitle> In Proceedings of the IPPS '95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pages 63-75, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Finally, in another time-sharing approach currently under development, communicating processes are dynamically coscheduled by using message arrivals to trigger scheduling events <ref> [39] </ref>. However, the authors do not explore algorithms for releasing the processor on the sending side.
Reference: [40] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 40-51, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Each process spins for the context-switch time (200 s) before blocking on its communication and synchronization events. System Latency Source (s) Cray T3D 2 [1] TMC CM-5 6 [41] Intel Paragon 6 [9] FDDI 6 [28] Myrinet 11 [9] Fore ATM 33 <ref> [40] </ref> Switched Ethernet 52 [23] LattisCell ATM 104 [23] Table 1: Latency on Current Networks. Measured latencies on modern MPP and workstation networks. amount of time processes spin-wait for communication to complete. Few context-switches occur when processes are coscheduled; therefore, little benefit is gained by reducing their cost.
Reference: [41] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: For example, the pairwise communication events can be synchronous or asynchronous sends and receives on a distributed-memory machine or remote reads and writes in a shared address space. In this study, we focus on read operations, such as those built on top of active messages <ref> [41] </ref> or cooperative 1 Our barrier is implemented assuming no hardware support. A process reaching the barrier sends a message to the root process. When the root handles the message, it increments a counter; once the counter reaches P , the root broadcasts a barrier-completion message. <p> When jobs are coscheduled, increasing network latency simply increases the two-phase fixed-spin is used instead of coscheduling. Each process spins for the context-switch time (200 s) before blocking on its communication and synchronization events. System Latency Source (s) Cray T3D 2 [1] TMC CM-5 6 <ref> [41] </ref> Intel Paragon 6 [9] FDDI 6 [28] Myrinet 11 [9] Fore ATM 33 [40] Switched Ethernet 52 [23] LattisCell ATM 104 [23] Table 1: Latency on Current Networks. Measured latencies on modern MPP and workstation networks. amount of time processes spin-wait for communication to complete.
Reference: [42] <author> R. W. Wisniewski, L. I. Kontothanassis, and M. L. Scott. </author> <title> High Performance Synchronization Algorithms for Multiprogrammed Multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 199-206, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Previous research applying adaptive algorithms to barrier synchronization has assumed a radically different programming model from ours, in which only threads from the same parallel job may execute on the same processor <ref> [24, 42] </ref>. Finally, in another time-sharing approach currently under development, communicating processes are dynamically coscheduled by using message arrivals to trigger scheduling events [39]. However, the authors do not explore algorithms for releasing the processor on the sending side.
Reference: [43] <author> J. Zahorjan and E. D. Lazowska. </author> <title> Spinning Versus Blocking in Parallel Systems with Uncertainty. </title> <booktitle> In Proceedings of the IFIP International Seminar on Performance of Distributed and Parallel Systems, </booktitle> <pages> pages 455-472, </pages> <month> Dec. </month> <year> 1988. </year>
Reference-contexts: Unfortunately, selecting the spin-time for events such as barriers in a multiprogrammed environment is non-trivial <ref> [43] </ref>. An adaptive two-phase algorithm that adjusts spin-time according to run-time measurements of waiting intervals is beneficial under these conditions.
References-found: 43

