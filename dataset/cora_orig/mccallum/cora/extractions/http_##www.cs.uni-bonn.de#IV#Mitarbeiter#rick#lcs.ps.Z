URL: http://www.cs.uni-bonn.de/IV/Mitarbeiter/rick/lcs.ps.Z
Refering-URL: http://www.cs.uni-bonn.de/IV/Mitarbeiter/rick/welcome.html
Root-URL: http://cs.uni-bonn.de
Keyword: Longest Common Subsequence Problem  
Note: New Algorithms for the  Claus Rick  
Abstract: RESEARCH REPORT Insititut f ur Informatik der Universit at Bonn Report No. 85123-CS 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, D. S. Hirschberg, J. D. Ullman: </author> <title> Bounds on the Complexity of the Longest Common Subsequence Problem, </title> <journal> Journal ACM, </journal> <volume> Vol. 23, </volume> <month> Jan. </month> <year> 1976, </year> <pages> 1-12. </pages>
Reference-contexts: A sequence C = c 1 c 2 : : : c l is called a subsequence of A if C can be obtained from A by deleting a number of (not necessarily consecutive) symbols from A, that is it exists a mapping F : <ref> [1; 2; : : :; l] </ref> ! [1; 2; : : :; m] having the two properties 1. <p> C = c 1 c 2 : : : c l is called a subsequence of A if C can be obtained from A by deleting a number of (not necessarily consecutive) symbols from A, that is it exists a mapping F : [1; 2; : : :; l] ! <ref> [1; 2; : : :; m] </ref> having the two properties 1. <p> Algorithms that use linear space are discussed in [9, 4, 14]. Table 1 gives a chronological survey of algorithms for the LCS Problem. Lower bounds on the complexity of the LCS Problem can be found in <ref> [1, 10, 22] </ref>. <p> Definition 3.1 Given a sequence B = b 1 b 2 : : : b n over some alphabet = foe 1 ; : : : ; oe s g define the function CLOSEST as follows: CLOSEST : [oe 1 ; : : : ; oe s ] fi <ref> [1; : : :; n + 1] </ref> ! [1; : : :; n + 1] CLOSEST [oe i ; j] := ( minffj 0 j j b j 0 = oe i g [ fn + 1gg if j = 1; 2; : : :; n When s is fixed and <p> b 1 b 2 : : : b n over some alphabet = foe 1 ; : : : ; oe s g define the function CLOSEST as follows: CLOSEST : [oe 1 ; : : : ; oe s ] fi <ref> [1; : : :; n + 1] </ref> ! [1; : : :; n + 1] CLOSEST [oe i ; j] := ( minffj 0 j j b j 0 = oe i g [ fn + 1gg if j = 1; 2; : : :; n When s is fixed and small in comparison to n, like for example <p> Output: A LCS C = c 1 c 2 : : : c p . Method: (1) T hresh [0] := 0; for k := 1 to m do T hresh [k] := n + 1 od; (2) for i := 1 to m do j := CLOSEST <ref> [a i ; 1] </ref>; (* first match in row i *) k := 1; (3) while j 6= n + 1 do if j &lt; T hresh [k] then temp := T hresh [k]; T hresh [k] := j; (4) record new minimal match (i; j); j := CLOSEST [a i
Reference: [2] <author> A. Apostolico: </author> <title> Improving the Worst-Case Performance of the Hunt-Szymanski Strategy for the Longest Common Subsequence of Two Strings, </title> <journal> Information Processing Letters 23, </journal> <year> 1986, </year> <pages> 63-69. </pages>
Reference-contexts: A sequence C = c 1 c 2 : : : c l is called a subsequence of A if C can be obtained from A by deleting a number of (not necessarily consecutive) symbols from A, that is it exists a mapping F : <ref> [1; 2; : : :; l] </ref> ! [1; 2; : : :; m] having the two properties 1. <p> C = c 1 c 2 : : : c l is called a subsequence of A if C can be obtained from A by deleting a number of (not necessarily consecutive) symbols from A, that is it exists a mapping F : [1; 2; : : :; l] ! <ref> [1; 2; : : :; m] </ref> having the two properties 1.
Reference: [3] <author> A. Apostolico, C. Guerra: </author> <title> The Longest Common Subsequence Problem Revisited, </title> <address> Algo-rithmica, </address> <year> 1987, </year> <pages> 315-336. </pages>
Reference-contexts: An additional O (n log s) term has to be added for both methods for a standard preprocessing phase. Later, both algorithms have been refined by Hsu and Du [12] and by Apostolico and Guerra <ref> [3] </ref> to take time O (pm log n p + pm) and O (pm log (minfs; m; 2n m g)) respectively. They perform well when a LCS is expected to be short. <p> There is also an algorithm from Nakatsu et al. [17] that has running time O (n (m p)) and that can be used if longest common subsequences of great length are expected. Other algorithms are from Chin and Poon [5] (O (ns + minfpm; dsg)) and Apostolico and Guerra <ref> [3] </ref> (O (m log n + d log ( 2mn d ))). Here d denotes the number of minimal matches (see Section 2 for a definition). Algorithms that use linear space are discussed in [9, 4, 14]. Table 1 gives a chronological survey of algorithms for the LCS Problem. <p> O (n (m p)) [17] 6 1984 Hsu, Du O (pm log ( n 7 1986 Myers O (n (n p)) [16] 8 1987 Apostolico, Guerra O (pm log (minfs; m; 2n m g)) <ref> [3] </ref> d )) 9 1990 Chin, Poon O (ns + minfds; pmg) [5] 10 1990 Wu, Manber, Myers O (n (m p)) [23] 11 1992 Apostolico et al. O (n (m p)) [4] Table 1: Algorithms for the LCS Problem. <p> It is presented in Section 4. In Section 2 the paradigm of computing minimal matches is reviewed. Section 3 presents previous algorithms based on this paradigm, namely the algorithm by Hunt and Szymanski [13] and a refined one by Apostolico and Guerra <ref> [3] </ref>. Then in Section 4 first the new O (ns + minfpm; p (n p)g) algorithm will be developed. Subsequently another O (ns + minfpm; dsg) algorithm is given which essentially does the same as the algorithm by Chin and Poon [5], but in a rather different manner. <p> First we are only interested in computing minimal matches but the algorithm scans all occurring matches. And second a binary search is used for each match considered to identify its rank. The key observation made by Apostolico/Guerra <ref> [3] </ref> is that what we really need to know to compute a minimal match is the first occurrence of a symbol in B to the right of a certain position. <p> Fortunately it is possible to store the same information in a vector of size n + 1 that can be computed in time fi (n). But then time O (log s) is needed to retrieve a value CLOSEST [oe i ; j] from the compact representation <ref> [3] </ref>. The Hunt/Szymanski approach can now be restated as 10 Algorithm 2 (Apostolico/Guerra [3], 1987) Input: Sequences A = a 1 a 2 : : : a m and B = b 1 b 2 : : : b n ; m n. <p> But then time O (log s) is needed to retrieve a value CLOSEST [oe i ; j] from the compact representation <ref> [3] </ref>. The Hunt/Szymanski approach can now be restated as 10 Algorithm 2 (Apostolico/Guerra [3], 1987) Input: Sequences A = a 1 a 2 : : : a m and B = b 1 b 2 : : : b n ; m n. Output: A LCS C = c 1 c 2 : : : c p . <p> Therefore this number constitutes a good criterion for the comparison of the efficiency of these algorithms that is independent of the actual implementation. Six algorithms have been investigated with respect to this characteristic value, namely 1. A revised version of the Hunt/Szymanski algorithm developed by Apostolico/Guerra <ref> [3] </ref> that has running time O (ns + pm). This algorithm is efficient for short longest common subsequences. 2. The algorithm by Nakatsu et al. [17] which has running time O (n (m p)) and thus should be preferred if long longest common subsequences are expected. 3.
Reference: [4] <author> A. Apostolico, S. Browne, C. Guerra: </author> <title> Fast linear-space computations of longest common subsequences, </title> <booktitle> Theoretical Computer Science 92, </booktitle> <year> 1992, </year> <pages> 3-17. </pages>
Reference-contexts: Here d denotes the number of minimal matches (see Section 2 for a definition). Algorithms that use linear space are discussed in <ref> [9, 4, 14] </ref>. Table 1 gives a chronological survey of algorithms for the LCS Problem. Lower bounds on the complexity of the LCS Problem can be found in [1, 10, 22]. <p> O (n (m p)) <ref> [4] </ref> Table 1: Algorithms for the LCS Problem. Time O (n log s) has to be added to algorithms 2, 3, 6 and 8 for a preprocessing phase. likely to occur. The later might be the case when searching a data base for entries similar to a query.
Reference: [5] <author> F. Y. L. Chin, C. K. Poon: </author> <title> A Fast Algorithm for Computing Longest Common Subsequences of Small Alphabet Size, </title> <journal> Journal of Information Processing, </journal> <volume> Vol. 13, No. 4, </volume> <year> 1990, </year> <pages> 463-469. </pages>
Reference-contexts: There is also an algorithm from Nakatsu et al. [17] that has running time O (n (m p)) and that can be used if longest common subsequences of great length are expected. Other algorithms are from Chin and Poon <ref> [5] </ref> (O (ns + minfpm; dsg)) and Apostolico and Guerra [3] (O (m log n + d log ( 2mn d ))). Here d denotes the number of minimal matches (see Section 2 for a definition). Algorithms that use linear space are discussed in [9, 4, 14]. <p> O (n (m p)) [17] 6 1984 Hsu, Du O (pm log ( n 7 1986 Myers O (n (n p)) [16] 8 1987 Apostolico, Guerra O (pm log (minfs; m; 2n m g)) [3] d )) 9 1990 Chin, Poon O (ns + minfds; pmg) <ref> [5] </ref> 10 1990 Wu, Manber, Myers O (n (m p)) [23] 11 1992 Apostolico et al. O (n (m p)) [4] Table 1: Algorithms for the LCS Problem. <p> Then in Section 4 first the new O (ns + minfpm; p (n p)g) algorithm will be developed. Subsequently another O (ns + minfpm; dsg) algorithm is given which essentially does the same as the algorithm by Chin and Poon <ref> [5] </ref>, but in a rather different manner. Some experimental results comparing various algorithms for the LCS Problem are presented in Section 5. <p> From the upper bound on the number d of minimal matches (d p (m + n 2p + 1)) proved in Lemma 4.2 we can conclude that the algorithm presented by Chin/Poon <ref> [5] </ref> which has time complexity O (ns + minfds; pmg) should also exhibit a good performance in both situations. However, the alphabet size has to be reasonably small for this algorithm. <p> Thus we have time complexity O (ns + minfds; pmg) for Algorithm 4. 2 Remark: It is not by chance that the time bound of Algorithm 4 coincides exactly with the time bound given by Chin/Poon <ref> [5] </ref> for their algorithm. Indeed both algorithms check exactly the same T hresh-values. But they differ considerably with respect to the order in which this is done and to the strategy that is used to determine the T hresh-values that have to be checked. <p> This algorithm is efficient for short longest common subsequences. 2. The algorithm by Nakatsu et al. [17] which has running time O (n (m p)) and thus should be preferred if long longest common subsequences are expected. 3. The algorithm by Chin/Poon <ref> [5] </ref> which has a time complexity of O (ns + minfds; pmg). 4. Algorithm 4 which has the same time complexity of O (ns + minfds; pmg). 5. Algorithm 3 which has time complexity O (ns + minfpm; p (n p)g). 6. <p> The time bound obtained for the resulting algorithm is O (ns + minfpm; p (n p)g). An upper bound on the number d of minimal matches (d p (m + n 2p + 1)), a key value for the LCS Problem, implied that the algorithm devised by Chin/Poon <ref> [5] </ref> which has running time O (ns+minfpm; dsg) should be efficient for both long and short longest common subsequences, too.
Reference: [6] <author> F. Y. L. Chin, C. K. Poon: </author> <title> Performance Analysis of Some Simple Heuristics for Computing Longest Common Subsequences, </title> <journal> Algorithmica, </journal> <volume> Vol. 12, </volume> <year> 1994, </year> <pages> 293-311. </pages>
Reference: [7] <author> T. H. Cormen, C. E. Leiserson, R. L. Rivest: </author> <title> Introduction to Algorithms, </title> <publisher> MIT Press / Mc Graw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: They observed that the following recursion holds for L i;j (a detailed proof can be found in <ref> [7] </ref>).
Reference: [8] <author> D. Eppstein, Z. Galil, R. Giancarlo, G. F. </author> <title> Italiano: Sparse Dynamic Programming I: Linear Cost Functions, </title> <journal> Journal of the ACM, </journal> <volume> Vol. 39, No. 3, </volume> <year> 1992, </year> <pages> 519-545. </pages>
Reference: [9] <author> D. S. Hirschberg: </author> <title> A Linear Space Algorithm for Computing Maximal Common Subsequences, </title> <journal> Comm. ACM, </journal> <volume> Vol. 18, </volume> <month> June </month> <year> 1975, </year> <pages> 341-343. </pages>
Reference-contexts: In order to reconstruct a LCS we can save a pointer with each cell L [i; j] that indicates which of the terms L [i 1; j 1] + 1; L [i 1; j]; L [i; j 1] was used to define L [i; j]. It was shown in <ref> [9] </ref> that linear space suffices to compute both the length of a LCS and a LCS while maintaining the time bound. Example: Let = fa; b; c; dg be an alphabet and let A = abcdbb and B = cbacbaaba be two input sequences over . <p> Here d denotes the number of minimal matches (see Section 2 for a definition). Algorithms that use linear space are discussed in <ref> [9, 4, 14] </ref>. Table 1 gives a chronological survey of algorithms for the LCS Problem. Lower bounds on the complexity of the LCS Problem can be found in [1, 10, 22].
Reference: [10] <author> D. S. Hirschberg: </author> <title> An Information-Theoretic Lower Bound for the Longest Common Subsequence Problem, </title> <journal> Information Processing Letters, </journal> <volume> Vol. 7, </volume> <month> Jan. </month> <year> 1977, </year> <pages> 40-41. </pages>
Reference-contexts: Algorithms that use linear space are discussed in [9, 4, 14]. Table 1 gives a chronological survey of algorithms for the LCS Problem. Lower bounds on the complexity of the LCS Problem can be found in <ref> [1, 10, 22] </ref>.
Reference: [11] <author> D. S. Hirschberg: </author> <title> Algorithms for the Longest Common Subsequence Problem, </title> <journal> Journal ACM, </journal> <volume> Vol. 24, </volume> <month> Oct. </month> <year> 1977, </year> <pages> 664-675. </pages>
Reference-contexts: This approach will be reviewed in Section 2 since it is the basis for the new algorithms presented in this paper, too. The first algorithms using this approach have been invented by Hirschberg <ref> [11] </ref> and Hunt/Szymanski [13] with processing time O (pn) and O (m+r log p) respectively. An additional O (n log s) term has to be added for both methods for a standard preprocessing phase. <p> Year Author (s) Time Ref. 1 1974 Wagner, Fischer O (mn) [21] 2 1977 Hunt, Szymanski O (m + r log p) [13] 3 1977 Hirschberg O (pn) <ref> [11] </ref> O ((m p)p log n) 4 1980 Masek, Paterson O (n 2 = log n) [15] 5 1982 Nakatsu et al. <p> For a fixed k compute all T i;k for 1 i m, then advance to k + 1. This is the approach used by Hirschberg <ref> [11] </ref> that considers the matches class by class. 2. For a fixed i compute all T i;k for appropriate k, then advance to i + 1. This is the approach used by Hunt/Szymanski [13] that considers matches row by row in the L matrix. <p> Indeed both algorithms check exactly the same T hresh-values. But they differ considerably with respect to the order in which this is done and to the strategy that is used to determine the T hresh-values that have to be checked. While Chin/Poon, in analogy to Hirschberg <ref> [11] </ref>, proceed class by class and construct minimal matches in C k+1 from matches in C k Algorithm 4 proceeds row by row in a Hunt/Szymanski strategy looking back to some of the previously determined minimal matches. Moreover the two algorithms use very different data structures.
Reference: [12] <author> W. J. Hsu, M. W. Du: </author> <title> New Algorithms for the LCS Problem, </title> <journal> Journal of Computer and System Sciences 29, </journal> <year> 1984, </year> <pages> 133-152. </pages>
Reference-contexts: An additional O (n log s) term has to be added for both methods for a standard preprocessing phase. Later, both algorithms have been refined by Hsu and Du <ref> [12] </ref> and by Apostolico and Guerra [3] to take time O (pm log n p + pm) and O (pm log (minfs; m; 2n m g)) respectively. They perform well when a LCS is expected to be short.
Reference: [13] <author> J. W. Hunt, T. G. Szymanski: </author> <title> A Fast Algorithm for Computing Longest Common Subsequences, </title> <journal> Comm. ACM, </journal> <volume> Vol. 20, </volume> <month> May </month> <year> 1977, </year> <pages> 350-353. </pages>
Reference-contexts: This approach will be reviewed in Section 2 since it is the basis for the new algorithms presented in this paper, too. The first algorithms using this approach have been invented by Hirschberg [11] and Hunt/Szymanski <ref> [13] </ref> with processing time O (pn) and O (m+r log p) respectively. An additional O (n log s) term has to be added for both methods for a standard preprocessing phase. <p> Year Author (s) Time Ref. 1 1974 Wagner, Fischer O (mn) [21] 2 1977 Hunt, Szymanski O (m + r log p) <ref> [13] </ref> 3 1977 Hirschberg O (pn) [11] O ((m p)p log n) 4 1980 Masek, Paterson O (n 2 = log n) [15] 5 1982 Nakatsu et al. <p> It is presented in Section 4. In Section 2 the paradigm of computing minimal matches is reviewed. Section 3 presents previous algorithms based on this paradigm, namely the algorithm by Hunt and Szymanski <ref> [13] </ref> and a refined one by Apostolico and Guerra [3]. Then in Section 4 first the new O (ns + minfpm; p (n p)g) algorithm will be developed. <p> This is the approach used by Hirschberg [11] that considers the matches class by class. 2. For a fixed i compute all T i;k for appropriate k, then advance to i + 1. This is the approach used by Hunt/Szymanski <ref> [13] </ref> that considers matches row by row in the L matrix. It is reviewed in the next paragraph. 3. <p> To reconstruct a LCS we save additional pointers to the last minimal match found for each class C k which enables us to identify a generating match for each newly found match. This allows the dynamic construction of linked lists representing longest common subsequences. Algorithm 1 (Hunt/Szymanski <ref> [13] </ref>, 1977) Input: Sequences A = a 1 a 2 : : : a m and B = b 1 b 2 : : : b n ; m n. Output: A LCS C = c 1 c 2 : : : c p .
Reference: [14] <author> S. K. Kumar, C. P. Rangan: </author> <title> A Linear Space Algorithm for the LCS Problem, </title> <journal> Acta Informatica 24, </journal> <year> 1987, </year> <pages> 353-363. </pages>
Reference-contexts: Here d denotes the number of minimal matches (see Section 2 for a definition). Algorithms that use linear space are discussed in <ref> [9, 4, 14] </ref>. Table 1 gives a chronological survey of algorithms for the LCS Problem. Lower bounds on the complexity of the LCS Problem can be found in [1, 10, 22].
Reference: [15] <author> W. J. Masek, M. S. Paterson: </author> <title> A Faster Algorithm Computing String Edit Distances, </title> <journal> Journal of Computer and System Sciences 20, </journal> <year> 1980, </year> <pages> 18-31. </pages>
Reference-contexts: Example: Let = fa; b; c; dg be an alphabet and let A = abcdbb and B = cbacbaaba be two input sequences over . The above code computes the L-Matrix shown in Figure 1. The asymptotically fastest general solution takes time O (n 2 = log n) <ref> [15] </ref> and uses the "Four Russians" trick. A lot of algorithms have been developed that, although not improving the general O (mn) time bound of the dynamic programming approach, exhibit a much better performance by specializing on certain classes of pairs of sequences. <p> Year Author (s) Time Ref. 1 1974 Wagner, Fischer O (mn) [21] 2 1977 Hunt, Szymanski O (m + r log p) [13] 3 1977 Hirschberg O (pn) [11] O ((m p)p log n) 4 1980 Masek, Paterson O (n 2 = log n) <ref> [15] </ref> 5 1982 Nakatsu et al.
Reference: [16] <author> E. W. Myers: </author> <title> An O(N D) Difference Algorithm and Its Variations, </title> <journal> Algorithmica, </journal> <year> 1986, </year> <pages> 251-266. </pages>
Reference-contexts: 2 2 2 2 2 2 1 2 2 2 3 3 3 3 3 0 1 2 3 4 5 6 7 8 9 0 2 b 4 d 6 b B and regions with identical L i;j value are separated through contours. was first suggested independently by Myers <ref> [16] </ref> and Ukkonen [20]. It takes time O (n (n p)) and was later improved by Wu et al. [23] to O (n (m p)). Thus, these algorithms suit with a class of sequences where the length p of a LCS is expected to be long. <p> O (n (m p)) [17] 6 1984 Hsu, Du O (pm log ( n 7 1986 Myers O (n (n p)) <ref> [16] </ref> 8 1987 Apostolico, Guerra O (pm log (minfs; m; 2n m g)) [3] d )) 9 1990 Chin, Poon O (ns + minfds; pmg) [5] 10 1990 Wu, Manber, Myers O (n (m p)) [23] 11 1992 Apostolico et al. <p> It seems that the most efficient algorithms specialized on long longest common subsequences are those based on the paradigm of computing shortest paths, e. g. the algorithms by Myers <ref> [16] </ref> and by Wu et al. [23] the latter of which was also included in the comparison of running times. The results are shown in Table 3. jLCSj Apostolico/ Guerra O (pm) Nakatsu et al.
Reference: [17] <author> N. Nakatsu, Y. Kambayashi, S. Yajima: </author> <title> A Longest Common Subsequence Algorithm Suitable for Similar Text Strings, </title> <journal> Acta Informatica 18, </journal> <year> 1982, </year> <pages> 171-179. </pages>
Reference-contexts: They perform well when a LCS is expected to be short. There is also an algorithm from Nakatsu et al. <ref> [17] </ref> that has running time O (n (m p)) and that can be used if longest common subsequences of great length are expected. <p> For a fixed alphabet of size s there is still a big gap between the linear lower bound (ns) and the worst case upper bounds of the various algorithms. 1.3 Mission statement As pointed out in <ref> [17] </ref> we have to select one of the algorithms a priori depending on the kind of sequences we wish to compare. <p> O (n (m p)) <ref> [17] </ref> 6 1984 Hsu, Du O (pm log ( n 7 1986 Myers O (n (n p)) [16] 8 1987 Apostolico, Guerra O (pm log (minfs; m; 2n m g)) [3] d )) 9 1990 Chin, Poon O (ns + minfds; pmg) [5] 10 1990 Wu, Manber, Myers O (n (m <p> that is use the order T 1;1 , T 2;2 , T 3;3 ; : : :, T 2;1 , T 3;2 , T 4;3 ; : : :, T 3;1 , T 4;2 , T 5;3 ; : : : This is the approach used by Nakatsu et al. <ref> [17] </ref>. In addition to the different orders of computation different methods for determining the minimum on the right hand side of the recursion, i.e. finding a minimal match, may be used. <p> Six algorithms have been investigated with respect to this characteristic value, namely 1. A revised version of the Hunt/Szymanski algorithm developed by Apostolico/Guerra [3] that has running time O (ns + pm). This algorithm is efficient for short longest common subsequences. 2. The algorithm by Nakatsu et al. <ref> [17] </ref> which has running time O (n (m p)) and thus should be preferred if long longest common subsequences are expected. 3. The algorithm by Chin/Poon [5] which has a time complexity of O (ns + minfds; pmg). 4.
Reference: [18] <author> M. Paterson, V. Dancik: </author> <title> Longest Common Subsequences, </title> <booktitle> Proceedings of the 19th Intern. Symp. on Mathematical Foundations of Computer Science, Vol. 841 of LNCS, </booktitle> <year> 1994, </year> <pages> 127-142. </pages>
Reference: [19] <author> D. Sankoff, J. B. Kruskal (Ed.): </author> <title> Time Warps, String Edits and Macromolecules: The Theory and Practice of Sequence Comparison, </title> <publisher> Addison-Wesley: </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: As a special case L m;n is the length of a LCS between A and B. 1.2 Previous results The first algorithm to solve the LCS Problem has been a dynamic programming approach that was discovered by several different scientist independently <ref> [19, 21] </ref>. They observed that the following recursion holds for L i;j (a detailed proof can be found in [7]). <p> Moreover, when dealing with sequences of approximately equal length consisting of symbols drawn from a small alphabet in a more or less uniform manner the length of a LCS can be expected to lie in the range between 1 3 m and 2 3 m, depending on the alphabet size <ref> [19] </ref>. In this case none of the the above specialized algorithms seems to be well-suited.
Reference: [20] <author> E. Ukkonen: </author> <title> Algorithms for Approximate String Matching, </title> <journal> Inform. and Control 64, </journal> <year> 1985, </year> <pages> 100-118. </pages>
Reference-contexts: 2 2 2 1 2 2 2 3 3 3 3 3 0 1 2 3 4 5 6 7 8 9 0 2 b 4 d 6 b B and regions with identical L i;j value are separated through contours. was first suggested independently by Myers [16] and Ukkonen <ref> [20] </ref>. It takes time O (n (n p)) and was later improved by Wu et al. [23] to O (n (m p)). Thus, these algorithms suit with a class of sequences where the length p of a LCS is expected to be long.
Reference: [21] <author> R. A. Wagner, M. J. Fischer: </author> <title> The String to String Correction Problem, </title> <journal> Journal ACM, </journal> <volume> 21, </volume> <year> 1974, </year> <pages> 168-173. </pages>
Reference-contexts: As a special case L m;n is the length of a LCS between A and B. 1.2 Previous results The first algorithm to solve the LCS Problem has been a dynamic programming approach that was discovered by several different scientist independently <ref> [19, 21] </ref>. They observed that the following recursion holds for L i;j (a detailed proof can be found in [7]). <p> Year Author (s) Time Ref. 1 1974 Wagner, Fischer O (mn) <ref> [21] </ref> 2 1977 Hunt, Szymanski O (m + r log p) [13] 3 1977 Hirschberg O (pn) [11] O ((m p)p log n) 4 1980 Masek, Paterson O (n 2 = log n) [15] 5 1982 Nakatsu et al.
Reference: [22] <author> C. K. Wong, A. K. Chandra: </author> <title> Bounds For the String Editing Problem, </title> <journal> Journal ACM, </journal> <volume> Vol. 23, </volume> <month> Jan. </month> <year> 1976, </year> <pages> 13-16. </pages>
Reference-contexts: Algorithms that use linear space are discussed in [9, 4, 14]. Table 1 gives a chronological survey of algorithms for the LCS Problem. Lower bounds on the complexity of the LCS Problem can be found in <ref> [1, 10, 22] </ref>.
Reference: [23] <author> S. Wu, U. Manber, G. Myers, W. Miller: </author> <title> An O(N P ) Sequence Comparison Algorithm, </title> <journal> Information Processing Letters 35, </journal> <year> 1990, </year> <pages> 317-323. 30 </pages>
Reference-contexts: It takes time O (n (n p)) and was later improved by Wu et al. <ref> [23] </ref> to O (n (m p)). Thus, these algorithms suit with a class of sequences where the length p of a LCS is expected to be long. A lot more algorithms use the paradigm of computing a longest and strictly increasing sequence of (minimal) matches. <p> 1984 Hsu, Du O (pm log ( n 7 1986 Myers O (n (n p)) [16] 8 1987 Apostolico, Guerra O (pm log (minfs; m; 2n m g)) [3] d )) 9 1990 Chin, Poon O (ns + minfds; pmg) [5] 10 1990 Wu, Manber, Myers O (n (m p)) <ref> [23] </ref> 11 1992 Apostolico et al. O (n (m p)) [4] Table 1: Algorithms for the LCS Problem. Time O (n log s) has to be added to algorithms 2, 3, 6 and 8 for a preprocessing phase. likely to occur. <p> It seems that the most efficient algorithms specialized on long longest common subsequences are those based on the paradigm of computing shortest paths, e. g. the algorithms by Myers [16] and by Wu et al. <ref> [23] </ref> the latter of which was also included in the comparison of running times. The results are shown in Table 3. jLCSj Apostolico/ Guerra O (pm) Nakatsu et al.
References-found: 23

