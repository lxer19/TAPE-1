URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-94-16.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Combining Control and Data Parallelism: Data Parallel Extensions to the Mentat Programming Language  
Author: Emily A. West 
Abstract: Computer Science Report No. CS-94-16 May 18, 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.S. Almasi and A. Gottlieb, </author> <title> Highly Parallel Computing, </title> <publisher> Benjamin-Cummings Publishing Co. Inc., </publisher> <address> Redwood City, CA,1994. </address>
Reference-contexts: The aggregate method in Figure 4.4 exhibits a local communication pattern. The reader will recall that possible communication patterns are NONE, PRED, SUCC, NS, EW, 4PT, and 8PT. Each pattern must have an associated radius. The communication pattern of the example 4 6 Slave [0] Slave <ref> [1] </ref> Slave [2] Slave [3] 0 0 7 3 1 Legend: guard row data row a) b) Distributed data parallel object with guard regions at each processor. 2 4 5 7 41 stencil_ave () operation is 4PT with a radius of two.
Reference: [2] <author> F. Bodin et al., </author> <title> Distributed pC++: Basic Ideas for an Object Parallel Language, </title> <booktitle> Proceedings Object-Oriented Numerics Conference, </booktitle> <month> April 25-27, </month> <year> 1993, </year> <title> Sunriver, </title> <booktitle> Oregon, </booktitle> <pages> pp. 1-24. </pages>
Reference-contexts: Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere [7, 8, 17]. 2.3 Related Work Dataparallel C [10, 11, 18, 19], pC++ <ref> [2, 15] </ref> C** [14] Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work. We have also developed some new ideas. C** and pC++ are based on C++. <p> At that time, this assumption will be re-examined. 15 is the approach we have used in creating our data parallel extensions to the MPL. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 11, 14, 15, 18, 19] </ref>. However, we have extended the notion to encompass subset parallelism. of an element-centered approach to data parallel class definition. Data parallel mentat classes are designated by pre-pending the keywords dataparallel mentat in front of the C++ keyword, class. <p> The aggregate method in Figure 4.4 exhibits a local communication pattern. The reader will recall that possible communication patterns are NONE, PRED, SUCC, NS, EW, 4PT, and 8PT. Each pattern must have an associated radius. The communication pattern of the example 4 6 Slave [0] Slave [1] Slave <ref> [2] </ref> Slave [3] 0 0 7 3 1 Legend: guard row data row a) b) Distributed data parallel object with guard regions at each processor. 2 4 5 7 41 stencil_ave () operation is 4PT with a radius of two.
Reference: [3] <author> B. Chapman, P. Mehrotra, and H. Zima, </author> <title> Programming in Vienna Fortran, </title> <journal> Scientific Programming, </journal> <volume> Vol. 1, No. 1, </volume> <month> Aug. </month> <year> 1992, </year> <pages> pp. 31-50. </pages>
Reference-contexts: Methods of the data set may not be applied to any subset. Fortran D Fortran D is a data parallel extension of Fortran developed by Kennedy, Fox, and others [5]. It has many similarities to Vienna Fortran <ref> [3] </ref>, which was simultaneously and independently developed. The basic idea in Fortran D is that data movement is costly and should be avoided. The programmer defines data structures (arrays) that are to be distributed to the processors. <p> The reader will recall that possible communication patterns are NONE, PRED, SUCC, NS, EW, 4PT, and 8PT. Each pattern must have an associated radius. The communication pattern of the example 4 6 Slave [0] Slave [1] Slave [2] Slave <ref> [3] </ref> 0 0 7 3 1 Legend: guard row data row a) b) Distributed data parallel object with guard regions at each processor. 2 4 5 7 41 stencil_ave () operation is 4PT with a radius of two. The pattern type, 4PT, is determined by the actual directions referenced.
Reference: [4] <author> A.A. Chien and W.J. Dally, </author> <title> Concurrent Aggregates (CA), </title> <booktitle> Proceedings of the Second ACM Sig-plan Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> March, 1990, Seattle, </address> <publisher> Wash-ington, </publisher> <pages> pp. 187-196, </pages> . 
Reference-contexts: The language provides mechanisms allowing the programmer to exploit memory locality, abstractions for implementing synchronous operations on distributed data structures, and mechanisms to express massive parallelism. The model is based on Concurrent Aggregates <ref> [4] </ref> and allows them to de-couple the structure of the data from the element type. A collection, essentially a data set, is defined with the C++ class mechanism. Element types are defined in separate classes. Operations are applied concurrently to all elements of the collection.
Reference: [5] <editor> G.C. Fox et al., </editor> <title> Fortran D Language Specifications, </title> <type> Technical Report SCCS 42c, </type> <institution> NPAC, Syra-cuse University, Syracuse, NY. </institution>
Reference-contexts: Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere [7, 8, 17]. 2.3 Related Work Dataparallel C [10, 11, 18, 19], pC++ [2, 15] C** [14] Fortran D <ref> [5] </ref> Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work. We have also developed some new ideas. C** and pC++ are based on C++. <p> Mechanism is provided which 11 allows the definition of subsets, however, these subsets must have their own explicitly defined methods. Methods of the data set may not be applied to any subset. Fortran D Fortran D is a data parallel extension of Fortran developed by Kennedy, Fox, and others <ref> [5] </ref>. It has many similarities to Vienna Fortran [3], which was simultaneously and independently developed. The basic idea in Fortran D is that data movement is costly and should be avoided. The programmer defines data structures (arrays) that are to be distributed to the processors.
Reference: [6] <author> A.S. Grimshaw, </author> <title> Easy to Use Object-Oriented Parallel Programming with Mentat, </title> <booktitle> IEEE Computer, </booktitle> <month> May, </month> <year> 1993, </year> <pages> pp. 39-51. </pages>
Reference: [7] <author> A.S. Grimshaw, </author> <title> The Mentat Computation Model - Data-Driven Support for Dynamic Object-Oriented Parallel Processing, </title> <type> Technical Report CS-93-30, </type> <institution> University of Virginia, Computer Science Department, </institution> <address> Charlottesville, VA, </address> <year> 1993. </year>
Reference-contexts: However, the burden of managing the distribution of data rests with the programmer, as does the generation of iteration code to loop over contained elements, and other mind-numbing details. Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere <ref> [7, 8, 17] </ref>. 2.3 Related Work Dataparallel C [10, 11, 18, 19], pC++ [2, 15] C** [14] Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work.
Reference: [8] <author> A.S. Grimshaw, J.B. Weissman, and W.T. Strayer, </author> <title> Portable Run-Time Support for Dynamic Object-Oriented Parallel Processing, </title> <note> submitted to ACM Transactions on Computer Systems, </note> <month> Jul., </month> <year> 1993. </year>
Reference-contexts: However, the burden of managing the distribution of data rests with the programmer, as does the generation of iteration code to loop over contained elements, and other mind-numbing details. Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere <ref> [7, 8, 17] </ref>. 2.3 Related Work Dataparallel C [10, 11, 18, 19], pC++ [2, 15] C** [14] Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work.
Reference: [9] <author> A.S. Grimshaw, E.A. West, and W.R. Pearson, </author> <title> No Pain and Gain! - Experiences with Mentat on Biological Application, </title> <journal> Concurrency: Practice & Experience, </journal> <volume> Vol. 5, No. 4, </volume> <month> Jun., </month> <year> 1993, </year> <pages> pp. 309-328 </pages> . 
Reference: [10] <author> P.J. Hatcher et al., </author> <title> Compiling Data-Parallel Programs for MIMD Architectures, </title> <booktitle> European Workshop on Parallel Computing, </booktitle> <address> March 1992, Barcelona, Spain. </address>
Reference-contexts: Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere [7, 8, 17]. 2.3 Related Work Dataparallel C <ref> [10, 11, 18, 19] </ref>, pC++ [2, 15] C** [14] Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work. We have also developed some new ideas. <p> Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere [7, 8, 17]. 2.3 Related Work Dataparallel C [10, 11, 18, 19], pC++ [2, 15] C** [14] Fortran D [5] Fortran 90 <ref> [10] </ref> and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work. We have also developed some new ideas. C** and pC++ are based on C++.
Reference: [11] <author> P.J. Hatcher et al, </author> <title> Data-Parallel Programming on MIMD Computers, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 2, No. 3, </volume> <pages> pp. 377-383. </pages>
Reference-contexts: Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere [7, 8, 17]. 2.3 Related Work Dataparallel C <ref> [10, 11, 18, 19] </ref>, pC++ [2, 15] C** [14] Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work. We have also developed some new ideas. <p> At that time, this assumption will be re-examined. 15 is the approach we have used in creating our data parallel extensions to the MPL. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 11, 14, 15, 18, 19] </ref>. However, we have extended the notion to encompass subset parallelism. of an element-centered approach to data parallel class definition. Data parallel mentat classes are designated by pre-pending the keywords dataparallel mentat in front of the C++ keyword, class. <p> This approach is problematic, since it seriously limits the problem sizes of applications using our extensions. However, it is not unique, and has previously been used by Hatcher and Quinn <ref> [11] </ref> to ensure proper access to needed data. A solution to this problem entails the support of dynamic redistribution of objects. Such support would eliminate the need for allocating enough space for entire objects at every processor.
Reference: [12] <author> W.D. Hillis, G.L. Steele, Jr., </author> <title> Data Parallel Algorithms, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 29, No. 12, </volume> <year> 1986, </year> <pages> pp. 1170-1183. </pages>
Reference: [13] <author> J.F. Karpovichet al., </author> <title> A Parallel Object-Oriented Framework for Stencil Algorithms, </title> <booktitle> Proceedings of the Second Symposium on High-Performance Distributed Computing, </booktitle> <address> July, 1993, Spokane, WA. </address>
Reference-contexts: The portions of the master and slave class methods generated using transformations 1 through 3 appear in boldface. The same translations apply to overlay and reduction operations. 5. These communication methods are defined in a base class as was done in <ref> [13] </ref>. 1: void master::class_op1 () 2: - 3: // communication - redistribution and guard regions. 4: // operation invocation. 5: // return results to caller. 6: // communication - redistribution. 7: - Generated slave class method. 1: void slave::class_op1 () 2: - 3: // allocate result space 4: // iterate over
Reference: [14] <author> J.R. Larus, B. Richards, and G. Viswanathan, </author> <title> C**: A Large-Grain, Object-Oriented, Data-Parallel Programming Language, </title> <type> Technical Report 1126, </type> <institution> University of Wisconsin, Computer Science Department, Madison, Wisconsin, </institution> <year> 1992. </year>
Reference-contexts: Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere [7, 8, 17]. 2.3 Related Work Dataparallel C [10, 11, 18, 19], pC++ [2, 15] C** <ref> [14] </ref> Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work. We have also developed some new ideas. C** and pC++ are based on C++. <p> At that time, this assumption will be re-examined. 15 is the approach we have used in creating our data parallel extensions to the MPL. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 11, 14, 15, 18, 19] </ref>. However, we have extended the notion to encompass subset parallelism. of an element-centered approach to data parallel class definition. Data parallel mentat classes are designated by pre-pending the keywords dataparallel mentat in front of the C++ keyword, class.
Reference: [15] <author> J.K. Lee and D. Gannon, </author> <title> Object Oriented Parallel Programming Experiments and Results, </title> <booktitle> Proceedings of Supercomputing 91, 1991, </booktitle> <address> Albuquerque, NM, </address> <pages> pp. 273-282. </pages>
Reference-contexts: Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere [7, 8, 17]. 2.3 Related Work Dataparallel C [10, 11, 18, 19], pC++ <ref> [2, 15] </ref> C** [14] Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work. We have also developed some new ideas. C** and pC++ are based on C++. <p> We provide alignment of multiple data sets, dynamic creation of data sets, and allow for the specification of general reduction methods as opposed to predefined reduction methods. pC++ Lee and Gannon <ref> [15] </ref> have developed a distributed collection model and a C++ based language to support the model. The language provides mechanisms allowing the programmer to exploit memory locality, abstractions for implementing synchronous operations on distributed data structures, and mechanisms to express massive parallelism. <p> At that time, this assumption will be re-examined. 15 is the approach we have used in creating our data parallel extensions to the MPL. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 11, 14, 15, 18, 19] </ref>. However, we have extended the notion to encompass subset parallelism. of an element-centered approach to data parallel class definition. Data parallel mentat classes are designated by pre-pending the keywords dataparallel mentat in front of the C++ keyword, class.
Reference: [16] <author> D.B. Loveman, </author> <title> High Performance Fortran, </title> <journal> IEEE Parallel & Distributed Technology: Systems & Applications, </journal> <volume> Vol. 1, No. 1, </volume> <month> Feb., </month> <year> 1993, </year> <pages> pp. 25-42. </pages>
Reference-contexts: Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere [7, 8, 17]. 2.3 Related Work Dataparallel C [10, 11, 18, 19], pC++ [2, 15] C** [14] Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) <ref> [16] </ref> are the languages from which we have borrowed ideas and which are related to our work. We have also developed some new ideas. C** and pC++ are based on C++. Dataparallel C is based on C, but uses some ideas from object-oriented language design. HPFs origins are obvious.
Reference: [17] <author> Mentat Research Group, </author> <title> Mentat 2.5 Programming Language Reference Manual, </title> <type> Technical Report CS-94-05, </type> <institution> University of Virginia, Department of Computer Science, </institution> <address> Charlottesville, VA, </address> <year> 1994. </year>
Reference-contexts: The programmer uses application domain knowledge to specify those object classes that are of sufficient computational complexity to warrant parallel execution. The complex tasks are handled by Mentat. There are two primary components of Mentat: the Mentat Programming Language (MPL) <ref> [17] </ref> and the Mentat run-time system (RTS). The MPL is an object-oriented programming language based on C++ [20] that masks the complexity of the parallel environment from the programmer. The granule of computation is the Mentat class member function. <p> However, the burden of managing the distribution of data rests with the programmer, as does the generation of iteration code to loop over contained elements, and other mind-numbing details. Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere <ref> [7, 8, 17] </ref>. 2.3 Related Work Dataparallel C [10, 11, 18, 19], pC++ [2, 15] C** [14] Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work.
Reference: [18] <author> N. Nedeljkovic and M.J. Quinn, </author> <title> Data-Parallel Programming on a Network of Heterogeneous Workstations, </title> <booktitle> Proceedings of the First Symposium on High-Performance Distributed Computing, </booktitle> <address> Sept., 1992, Syracuse, NY, </address> <pages> pp. 28-36. </pages>
Reference-contexts: Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere [7, 8, 17]. 2.3 Related Work Dataparallel C <ref> [10, 11, 18, 19] </ref>, pC++ [2, 15] C** [14] Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work. We have also developed some new ideas. <p> At that time, this assumption will be re-examined. 15 is the approach we have used in creating our data parallel extensions to the MPL. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 11, 14, 15, 18, 19] </ref>. However, we have extended the notion to encompass subset parallelism. of an element-centered approach to data parallel class definition. Data parallel mentat classes are designated by pre-pending the keywords dataparallel mentat in front of the C++ keyword, class.
Reference: [19] <author> M.J. Quinn and P.J. Hatcher, </author> <title> Data-Parallel Programming on Multicomputers, </title> <journal> IEEE Software, </journal> <month> Sept. </month> <year> 1990, </year> <pages> pp. 69-76. </pages>
Reference-contexts: Details of the computation model (macro-data ow), the Mentat programming language and the Mentat run-time system can be found elsewhere [7, 8, 17]. 2.3 Related Work Dataparallel C <ref> [10, 11, 18, 19] </ref>, pC++ [2, 15] C** [14] Fortran D [5] Fortran 90 [10] and High Performance Fortran (HPF) [16] are the languages from which we have borrowed ideas and which are related to our work. We have also developed some new ideas. <p> HPFs origins are obvious. All of these languages are strictly data parallel languages, and we differ from them all in that we are combining both control and data parallelism within the same language. Dataparallel C In <ref> [19] </ref>, Quinn and Hatcher, the designers of Dataparallel C, show that a strictly SIMD program can be compiled to a distributed memory architecture without sacrificing performance. This entails loosening the synchronization points by synchronizing only when communication between physical processors is necessary. <p> At that time, this assumption will be re-examined. 15 is the approach we have used in creating our data parallel extensions to the MPL. We call this approach element-centered. Fundamentally, this concept is not new to data parallel languages <ref> [2, 11, 14, 15, 18, 19] </ref>. However, we have extended the notion to encompass subset parallelism. of an element-centered approach to data parallel class definition. Data parallel mentat classes are designated by pre-pending the keywords dataparallel mentat in front of the C++ keyword, class. <p> Therefore the target of a data parallel source is a control parallel code which synchronizes cooperating objects at the proper points. Quinn <ref> [19] </ref> has demonstrated that a data parallel program can be converted to run efficiently on a MIMD machine by synchronizing at the proper points.
Reference: [20] <author> B. Stroustrup, </author> <title> The C++ Programming Language, 2nd ed. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1991. </year> <month> 60 </month>
Reference-contexts: The complex tasks are handled by Mentat. There are two primary components of Mentat: the Mentat Programming Language (MPL) [17] and the Mentat run-time system (RTS). The MPL is an object-oriented programming language based on C++ <ref> [20] </ref> that masks the complexity of the parallel environment from the programmer. The granule of computation is the Mentat class member function. Mentat classes consist of contained objects (local and member variables), their procedures, and a thread of control.
Reference: [21] <author> J.B. </author> <title> Weissman and A.S. Grimshaw, Multigranular Scheduling of Data Parallel Programs, </title> <type> Technical Report CS-93-38, </type> <institution> University of Virginia, Department of Computer Science, </institution> <address> Charlottesville, VA, </address> <month> July, </month> <year> 1993. </year>
Reference-contexts: Joint research is now being conducted within the Mentat group to allow the run-time system to combine the compiler hints with information about the current machine architecture. The run-time system will employ heuristic algorithms to automatically handle the decomposition, distribution and alignment of the data parallel object <ref> [21, 22] </ref>. Cooperating with the run-time system in this manner allows us to divorce ourselves from the underlying machine architecture. We believe that the decomposition, distribution and alignment as described in Fortran-D and other data parallel languages (see Section 2.3) are equivalent to our mechanisms of specifying the communication patterns. <p> It is the responsibility of the master to recognize when the data has been unevenly distributed and ensure that communication of data is properly managed in this case. The research to develop the run-time side of this mechanism is currently being conducted within the Mentat research group <ref> [21, 22] </ref>. Overlay Distribution. Administering the invocation of overlay operations is the second responsibility of the master. Upon invocation of an overlay operation on a data parallel object, the master receives the data to be assigned to the elements of the data set.
Reference: [22] <author> J.B. </author> <title> Weissman and A.S. Grimshaw, Network Partitioning of Data Parallel Computations, </title> <booktitle> to appear in Proceedings of the Symposium on High-Performance Distributed Computing (HPDC-3), August, 1994, </booktitle> <address> San Francisco, CA. </address>
Reference-contexts: Joint research is now being conducted within the Mentat group to allow the run-time system to combine the compiler hints with information about the current machine architecture. The run-time system will employ heuristic algorithms to automatically handle the decomposition, distribution and alignment of the data parallel object <ref> [21, 22] </ref>. Cooperating with the run-time system in this manner allows us to divorce ourselves from the underlying machine architecture. We believe that the decomposition, distribution and alignment as described in Fortran-D and other data parallel languages (see Section 2.3) are equivalent to our mechanisms of specifying the communication patterns. <p> It is the responsibility of the master to recognize when the data has been unevenly distributed and ensure that communication of data is properly managed in this case. The research to develop the run-time side of this mechanism is currently being conducted within the Mentat research group <ref> [21, 22] </ref>. Overlay Distribution. Administering the invocation of overlay operations is the second responsibility of the master. Upon invocation of an overlay operation on a data parallel object, the master receives the data to be assigned to the elements of the data set.
References-found: 22

