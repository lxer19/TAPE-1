URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-92-43.ps
Refering-URL: http://www.cs.virginia.edu/~eaw2t/
Root-URL: http://www.cs.virginia.edu
Title: MetaSystems: An Approach Combining Parallel Processing and Heterogeneous Distributed Computing Systems  
Author: Andrew S. Grimshaw, Jon B. Weissman, Emily A. West, Ed Loyot Jr. 
Note: This work was partially funded by NSF grants ASC-9201822 and CDA-8922545-01, NASA grant NGT-50970, and NLM grant LM04964969.  
Abstract: Technical Report No. CS-92-43 December 29, 1992 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Boyle et al., </author> <title> Portable Programs for Parallel Processors, </title> <publisher> Holt, Rinehart and Winston, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The primary objectives in much of the HDCS work are interoperability, sharing, and availability. Unlike our work, high performance is not the objective. Applications portability across parallel architectures is an objective of many projects. Examples include PVM [28], Linda [7], the Argonne P4 macros <ref> [1] </ref>, and Fortran-D [11]. Our effort shares with these and other projects the basic idea of providing a portable virtual machine to the programmer. The primary difference is the level of the abstraction.
Reference: [2] <author> E. Balkovich, S. Lerman, and R.P. Parmelee, </author> <title> Computing in Higher Education: </title> <journal> The Athena Experience, Commun. ACM vol. </journal> <volume> 28, no. 11, </volume> <pages> pp. 1214-1224, </pages> <month> November, </month> <year> 1985. </year>
Reference: [3] <author> A. Beguelin et al., </author> <title> HeNCE: Graphical Development Tools for Network-Based Concurrent Computing, </title> <booktitle> Proceedings SHPCC-92, </booktitle> <pages> pp. 129-136, </pages> <address> Williamsburg, VA, </address> <month> May, </month> <year> 1992. </year>
Reference-contexts: Second, Fortran D supports data parallelism only, not both functional and data parallelism. Third, Fortran D hard-wires support for low-dimensional arrays into the language, while our approach is to allow the user to provide arbitrary distribution functions for arbitrary data structures. Our work differs from Hence <ref> [3] </ref> in several ways. First, Hence is explicitly parallel. The programmer specifies program graph nodes and the data dependencies between them. In Mentat, the program graphs are transparently constructed for the programmer. Second, in Hence, graph nodes are pure functions. Hence essentially implements coarse-grain data ow.
Reference: [4] <author> G. Bernard et al., </author> <title> Primitives for Distributed Computing in a Heterogeneous Local Area Network Environment, </title> <journal> IEEE Trans on Soft. Eng. </journal> <volume> vol. 15, no. 12, </volume> <month> December 89. </month>
Reference: [5] <author> B. N. Bershad, and H. M. Levy, </author> <title> Remote Computation in a Heterogeneous Environment. </title> <type> Tech. Rep. </type> <institution> 87-06-04, Dept. of Computer Science, University of Washington, </institution> <address> Seattle, </address> <month> June, </month> <year> 1987. </year>
Reference: [6] <author> B. N. Bershad, et al., </author> <title> A Remote Procedure Call Facility for Interconnecting Heterogeneous Computer Systems, </title> <journal> IEEE Trans. Software. Eng. SE, </journal> <volume> vol. 13, no. 8, </volume> <pages> pp. 880-894, </pages> <month> August, </month> <year> 1987. </year>
Reference: [7] <author> N. Carriero, D. Gelernter, and T.G. Mattson, </author> <title> Linda in Heterogeneous Computing Environments, </title> <booktitle> Proceedings of WHP 92 Workshop on Heterogeneous Processing, </booktitle> <publisher> IEEE Press, </publisher> <pages> pp. 43-46, </pages> <address> Beverly Hills, CA, </address> <month> March, 25 </month>
Reference-contexts: The primary objectives in much of the HDCS work are interoperability, sharing, and availability. Unlike our work, high performance is not the objective. Applications portability across parallel architectures is an objective of many projects. Examples include PVM [28], Linda <ref> [7] </ref>, the Argonne P4 macros [1], and Fortran-D [11]. Our effort shares with these and other projects the basic idea of providing a portable virtual machine to the programmer. The primary difference is the level of the abstraction.
Reference: [8] <author> T. L. Casavant, and J. G. Kuhl, </author> <title> A Taxonomy of Scheduling in General-Purpose Distributed Computing Systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 14, </volume> <pages> pp. 141-154, </pages> <month> February, </month> <year> 1988. </year>
Reference: [9] <author> A.L.Cheung, and A.P. Reeves, </author> <title> High Performance Computing on a Cluster of Workstations, </title> <booktitle> Proceedings of the First Symposium on High-Performance Distributed Computing, </booktitle> <pages> pp. 152-160, </pages> <address> Syracuse, NY, </address> <month> Sept., </month> <year> 1992. </year>
Reference-contexts: First, as with Fortran D, they are data parallel only, and support only regular low-dimensional arrays. Second, they support processor heterogeneity only in the processor power sense. They cannot accommodate different data representations. Our method of determining the size of the partitions is similar in some respects to <ref> [9] </ref>. However, we dynamically partition the work based on resource availability and problem size, while it appears that their work is targeted toward compile time analysis. 7. Having many more workers than processors is similar to the concept of virtual processors.
Reference: [10] <author> D.L. Eager, E. D. Lazowska, and J. Zahorjan, </author> <title> Adaptive Load Sharing in Homogeneous Distributed Systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 12, </volume> <pages> pp. 662-675, </pages> <month> May, </month> <year> 1986. </year>
Reference: [11] <author> G. C. Fox, et al., </author> <title> Fortran D Language Specifications, </title> <type> Technical Report SCCS 42c, </type> <institution> NPAC, Syracuse University, Syracuse, NY. </institution>
Reference-contexts: The primary objectives in much of the HDCS work are interoperability, sharing, and availability. Unlike our work, high performance is not the objective. Applications portability across parallel architectures is an objective of many projects. Examples include PVM [28], Linda [7], the Argonne P4 macros [1], and Fortran-D <ref> [11] </ref>. Our effort shares with these and other projects the basic idea of providing a portable virtual machine to the programmer. The primary difference is the level of the abstraction. Low-level abstractions such as in [1,7,28] require the programmer to operate at the assembly language level of parallelism. <p> Low-level abstractions such as in [1,7,28] require the programmer to operate at the assembly language level of parallelism. This makes writing parallel programs more difficult. Others [3,11] share our philosophy of providing a higher level language interface in order to simplify applications development. Our work differs from Fortran D <ref> [11] </ref> in several ways. First, Fortran D is portable but is not targeted to a heterogeneous environment. Second, Fortran D supports data parallelism only, not both functional and data parallelism.
Reference: [12] <author> P. B. Gibbond, </author> <title> A Stub Generator for Multi-Language RPC in Heterogeneous Environments, </title> <journal> IEEE Trans. Software. Eng. SE, </journal> <volume> vol. 13, no. 1, </volume> <pages> pp. 77-87, </pages> <month> January, </month> <year> 1987. </year>
Reference: [13] <author> A. S. Grimshaw, </author> <title> Easy to Use Object-Oriented Parallel Programming with Mentat, </title> <note> to appear in IEEE Computer, </note> <month> May, </month> <year> 1993. </year>
Reference: [14] <author> A. S. Grimshaw, E. Loyot Jr., and J. Weissman, </author> <title> Mentat Programming Language (MPL) Reference Manual, </title> <institution> University of Virginia, Computer Science TR 91-32, </institution> <year> 1991. </year>
Reference: [15] <author> A. S. Grimshaw and V. E. Vivas, </author> <title> FALCON: A Distributed Scheduler for MIMD Architectures, </title> <booktitle> Proceedings of the Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pp. 149-163, </pages> <address> Atlanta, GA, </address> <month> March, </month> <year> 1991. </year>
Reference-contexts: The run-time system knows the types of machines involved and the class (type) of the data being transported. It is in the perfect position to selectively and transparently coerce data between representations as required. Second, the run-time system, not the user, is responsible for scheduling <ref> [15] </ref>. The run-time system selects the best host on which to instantiate an object at run-time depending on current system load. It selects the best instance of a particular host type, e.g., hypercube one or hypercube two. <p> At the highest level, we may schedule a program component on a multiprocessor such as the Delta, but within the domain of the multiprocessor, the individual processors must also be scheduled. And so on. The general multi-level scheduling problem is quite difficult. The Mentat scheduler <ref> [15] </ref> implements a dynamic load-sharing policy that is targeted toward homogeneous systems. It places Mentat objects on processors when it receives instantiation requests. The sched-uler handles functional parallelism very well, although it does not consider object-processor affinity during placement 5 .
Reference: [16] <author> A. S. Grimshaw, </author> <title> The Mentat Run-Time System: Support for Medium Grain Parallel Computation, </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pp. 1064-1073, </pages> <address> Charleston, SC., </address> <month> April 9-12, </month> <year> 1990. </year>
Reference: [17] <author> A. S. Grimshaw, D. Mack, and T. Strayer, MMPS: </author> <title> Portable Message Passing Support for Parallel Computing, </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pp. 784-789, </pages> <address> Charleston, SC., </address> <month> April 9-12, </month> <year> 1990. </year>
Reference-contexts: The SGIs are almost twice as fast as the IPCs, W 1 = 2 and W 2 = 1. The testbed uses a portable communication library, MMPS <ref> [17] </ref> based on sockets. The message cost equation for MMPS is: T (n) = 2.65 + 0.0015n.
Reference: [18] <author> A.S. Grimshaw, E. A. West, and W. Pearson, </author> <title> No Pain and Gain! - Experiences with Mentat on Biological Application, </title> <booktitle> Proceedings of the First Symposium on High-Performance Distributed Computing, </booktitle> <pages> pp. 57-66, </pages> <address> Syracuse, NY, </address> <month> Sept., </month> <year> 1992. </year>
Reference: [19] <author> M. Jones, R. F. Rashid, and M. R. Thompson, </author> <title> An Interface Specification Language for Distributed Processing. </title> <booktitle> Proceedings of the 12th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 225-235, </pages> <year> 1985. </year>
Reference: [20] <author> J.H. Morris, et al., Andrew: </author> <title> A distributed personal computing environment, </title> <journal> Communications of the ACM, </journal> <volume> vol. 29, no. 3, </volume> <month> March </month> <year> 1986. </year>
Reference: [21] <author> R. Mirchandaney, D. Towsley, and J. Stankovic, </author> <title> Adaptive Load Sharing in Heterogeneous Distributed Systems, </title> <journal> Journal of Parallel and Distributed Computing, Academic Press, </journal> <volume> no. 9, </volume> <pages> pp. 331-346, </pages> <year> 1990. </year>
Reference: [22] <author> P. Narayan, et al., </author> <title> Portability and Performance: Mentat Applications on Diverse Architectures, </title> <institution> TR-92-22, Department of Computer Science, University of Virginia, </institution> <month> July, </month> <year> 1992. </year>
Reference: [23] <author> N. Nedeljkovic, and M.J. Quinn, </author> <title> Data-Parallel Programming on a Network of Heterogeneous Workstations, </title> <booktitle> Proceedings of the First Symposium on High-Performance Distributed Computing, </booktitle> <pages> pp. 28-36, </pages> <address> Syra-cuse, NY, </address> <month> Sept., </month> <year> 1992. </year>
Reference-contexts: In the case where the amount of computation is data-dependent, this strategy will not be not always perform well; some form of dynamic rebalancing may need to be done as in DataParallel C <ref> [23] </ref>. 3.4 Effect of Partitioning We turn again to the Gaussian elimination example. To test the efficacy of our approach, we have applied the problem partitioning metric by hand to the problem and scheduled the resulting workers with their computed problem sizes using Mentat. <p> Having many more workers than processors is similar to the concept of virtual processors. Load balance is achieved by scheduling virtual processors to physical processors such that each processor has a number of virtual processors proportional to its power and external load. This approach is used in <ref> [23] </ref>, and is best for non-communicating virtual processors, or when the communication pattern is very regular, i.e. stencils. 24 5.0 Summary and Future Work In this paper we have introduced our approach to constructing high-performance, easy-to-use metasys-tems comprised of heterogeneous components.
Reference: [24] <author> D. Notkin., </author> <title> et al.,Heterogeneous Computing Environments: Report on the ACM SIGOPS Workshop on Accommodating Heterogeneity, </title> <journal> Communications of the ACM, </journal> <volume> vol. 30, no. 2, </volume> <pages> pp. 132-140, </pages> <month> February, </month> <year> 1987. </year>
Reference: [25] <author> D. Notkin, et al., </author> <title> Interconnecting Heterogeneous Computer Systems, </title> <journal> Communications of the ACM, </journal> <volume> vol. 31, no. 3, </volume> <pages> pp. 258-273, </pages> <month> March, </month> <year> 1988. </year>
Reference: [26] <author> B. Stroustrup, </author> <title> The C++ Programming Language, 2nd ed. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1991. </year> <month> 26 </month>
Reference-contexts: The complex tasks are handled by Mentat. There are two primary components of Mentat: the Mentat Programming Language (MPL) and the Men-tat run-time system (RTS). The MPL is an object-oriented programming language based on C++ <ref> [26] </ref> that masks the complexity of the parallel environment from the programmer. The granule of computation is the Mentat class member function. Mentat classes consist of contained objects (local and member variables), their procedures, and a thread of control.
Reference: [27] <author> Sun Microsystems. </author> <title> External Data Representation Reference Manual. Sun Microsystems, </title> <month> Jan. </month> <year> 1985. </year>
Reference: [28] <author> V.S. Sunderam, </author> <title> PVM: A framework for parallel distributed computing, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 2(4), </volume> <pages> pp. 315-339, </pages> <month> December, </month> <year> 1990. </year>
Reference-contexts: The primary objectives in much of the HDCS work are interoperability, sharing, and availability. Unlike our work, high performance is not the objective. Applications portability across parallel architectures is an objective of many projects. Examples include PVM <ref> [28] </ref>, Linda [7], the Argonne P4 macros [1], and Fortran-D [11]. Our effort shares with these and other projects the basic idea of providing a portable virtual machine to the programmer. The primary difference is the level of the abstraction. <p> Recently there have been efforts towards combining parallel processing and heterogeneity [3,9,23,28]. Once again, the level of service distinguishes our work from systems such as PVM <ref> [28] </ref>; they enable, rather that support, heterogeneous parallel computing. Our work differs from that of [9,23] in several ways. First, as with Fortran D, they are data parallel only, and support only regular low-dimensional arrays. Second, they support processor heterogeneity only in the processor power sense.
Reference: [29] <author> M.M. Theimer, and B. Hayes, </author> <title> Heterogeneous Process Migration by Recompilation, </title> <booktitle> Proc. 11th Intl. Conference on Distributed Computing Systems, </booktitle> <address> Arlington, TX, </address> <month> May, </month> <year> 1991, </year> <pages> pp. 18-25. </pages>
Reference: [30] <author> J. Weissman, E. A. West, and A. S. Grimshaw, </author> <title> Automatic Problem Decomposition and Scheduling in Heterogeneous Parallel Processing Systems, </title> <institution> University of Virginia, </institution> <note> Computer Science TR 92-33, in progress, </note> <year> 1992. </year>
Reference-contexts: Currently the coercion functions are generated by hand. Ultimately they will be generated by the compiler. We chose to hand generate the coercion functions so that we could experiment with different coercion functions costs and measure the effect on performance before we undertook the effort to modify the compiler. <ref> [30] </ref> We constructed coercion functions with which we could easily simulate a range of coercion costs. We then simulated a network consisting of one, two, four, and eight different processor types using eight Sun IPCs in the testbed. Each IPC was labeled with a pseudo-architecture type. <p> For a balanced load decomposition (our goal), we will assign the same size work region to each node within a class C i . Thus, a single A i is computed per class C i . We are experimenting with several heuristic algorithms for load selection <ref> [30] </ref>. Of particular interest to us is that the algorithms are fast. The general form of the load selection algorithms is described shortly. The algorithms compute the M i for each architecture class. <p> The number of nodes is determined by a procedure find_grain () called for each architecture class. Find_grain determines the maximum number of nodes that results in a feasible assignment. We have developed a heuristic for find_grain () based on binary-search <ref> [30] </ref>. If the granularity is too large (determined by find_grain ()), the algorithm continues to the next strongest class to apply more nodes to the problem, and so on. If not, the algorithm terminates with the nodes previously computed. Only these nodes will be applied to the problem.
Reference: [31] <author> S. A. Yemini, et al., </author> <title> Concert: A High-Level-Language Approach to Heterogeneous Distributed Systems, </title> <booktitle> Proc. 9th IEEE International Conference on Distributed Computing Systems, </booktitle> <address> Newport, CA., </address> <pages> pp. 162-171, </pages> <month> June, </month> <year> 1989 </year>
References-found: 31

