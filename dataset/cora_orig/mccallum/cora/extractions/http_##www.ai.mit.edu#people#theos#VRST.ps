URL: http://www.ai.mit.edu/people/theos/VRST.ps
Refering-URL: http://www.ai.mit.edu/people/theos/theos.html
Root-URL: 
Email: e-mail: theos@ai.mit.edu  
Phone: 2  Tel: (617) 258-0263  
Title: Image-Based View Synthesis by Combining Trilinear Tensors and Learning Techniques  
Author: S. Avidan T. Evgeniou A. Shashua T. Poggio 
Note: Contact Author: Theodoros Evgeniou  Funding for the project described here is provided by MURI and other sources (ARPA, ONR, NSF, NASA)  
Address: Jerusalem 91904 ISRAEL  Cambridge, MA 02139 USA  E25-201, 45 Carleton Street Cambridge, MA 02142 USA  
Affiliation: 1 Institute of Computer Science, The Hebrew University of Jerusalem,  Artificial Intelligence Laboratory, M.I.T  Massachusetts Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Adelson, E.H. and J.R. </author> <title> Bergen The Plenop-tic Function and the Elements of Early Vision In Computational Models of Visual Processing, Chapter 1, </title> <editor> Edited by Michael Landy and J. An-thony Movshon. </editor> <publisher> The MIT Press, </publisher> <address> Cambridgem, Mass. </address> <year> 1991. </year>
Reference-contexts: However, interpolation may produce physically-invalid images. Seitz and Dyer [22] proposed a physically-valid view interpolation method. The method involves recovering the epipolar geometry between the two acquired images and having interpolation done along the rectified epipolar lines. Interpolation can also be performed directly on the plenoptic function <ref> [1] </ref> which represents the amount of light emitted at each point in space as a function of direction. Levoy et al. [16] and Gortler et al. [11] interpolate between a dense set of several thousands of example images to reconstruct a reduced plenop-tic function (under an occlusion-free world assumption).
Reference: [2] <author> E.B. Barrett, P.M. Payton, and G. Gheen. </author> <title> Robust algebraic invariant methods with applications in geometry and imaging. </title> <booktitle> In Proceedings of the SPIE on Remote Sensing, </booktitle> <address> San Diego, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: There are alternative ways of performing reprojection, but if we would like to do it without recovering first a 3D model of the scene, the trilinear tensor generally provides the best results since it is free from singular configurations (see <ref> [2, 23, 26] </ref>). We have described so far the implementation of the reprojection paradigm via the trilinear equations.
Reference: [3] <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In Proceedings of the European Conference on Computer Vision, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Preprocessing (a) Compute dense correspondence (optical flow) between the two model images. We use a coarse-to-fine implementation of [18] described in <ref> [3] </ref>. Interactive tools for improving correspondence [17] can be used as well, although in our experiments optical flow alone was sufficient. (b) Recover the fundamental matrix of the two model images from the correspondences.
Reference: [4] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1431, </pages> <institution> Artificial Intelligence Laboratory, Mas-sachusetts Institute of Technology, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: This is the basis for the QuickTimeVR system [7]. The fixed position constraint can be relaxed by computing the optical flow between the example images and using it to interpolate between the cylinders constructed at different locations (cf. <ref> [4, 5, 6] </ref>) (originally proposed for views, not mosaics, but the principle is the same). However, interpolation may produce physically-invalid images. Seitz and Dyer [22] proposed a physically-valid view interpolation method. <p> This simple technique can be used to control several non-rigid degrees of freedom such as facial expressions, as shown by <ref> [4, 5, 17, 8] </ref>. It can be combined directly with the algebraic technique described earlier to control the position of the virtual camera.
Reference: [5] <author> D. Beymer, and T. Poggio. </author> <title> Image Representations for Visual Learning. </title> <journal> Science, </journal> <volume> 272, </volume> <pages> pages 1905-1909, </pages> <year> 1996. </year>
Reference-contexts: This is the basis for the QuickTimeVR system [7]. The fixed position constraint can be relaxed by computing the optical flow between the example images and using it to interpolate between the cylinders constructed at different locations (cf. <ref> [4, 5, 6] </ref>) (originally proposed for views, not mosaics, but the principle is the same). However, interpolation may produce physically-invalid images. Seitz and Dyer [22] proposed a physically-valid view interpolation method. <p> This idea of generating "virtual" views of an object by using class-specific knowledge has been discussed before (see references in <ref> [5] </ref>). Suppose that we have two views Img ref and Img p of the prototype. We take Img ref to appear in the same pose as Img nov . Img p is a slightly transformed (i.e., rotated) view of Img ref (see diagram in Fig. 7). <p> A multidimensional interpolation technique such as Radial Basis Functions or splines is then used to interpolate the n example pairs (r i ; (S i ; T i )) (see for instance <ref> [5] </ref>). <p> This simple technique can be used to control several non-rigid degrees of freedom such as facial expressions, as shown by <ref> [4, 5, 17, 8] </ref>. It can be combined directly with the algebraic technique described earlier to control the position of the virtual camera. <p> First, our technique needs only two images from close viewpoints to deal with all the rigid degrees of freedom. Second, the number of examples required for the non-rigid degrees of freedom may remain quite low, for appropriate choices of the input parameters (see <ref> [5] </ref> and references therein). Third, it is possible to bypass the curse of dimensionality problem by representing an object as a hierarchy of components. Interpolation networks responsible for each component are thus independent, for instance the mouth separately from the eyes [17, 8].
Reference: [6] <author> S.E. Chen and L. Williams. </author> <title> View interpolation for image synthesis. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 279-288, </pages> <address> Anahiem, CA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: This is the basis for the QuickTimeVR system [7]. The fixed position constraint can be relaxed by computing the optical flow between the example images and using it to interpolate between the cylinders constructed at different locations (cf. <ref> [4, 5, 6] </ref>) (originally proposed for views, not mosaics, but the principle is the same). However, interpolation may produce physically-invalid images. Seitz and Dyer [22] proposed a physically-valid view interpolation method.
Reference: [7] <author> Shenchang Eric Chen. </author> <title> QuickTimeVR an image-based approach to virtual environment navigation. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 29-38, </pages> <year> 1995. </year>
Reference-contexts: The mosaic is mapped to a virtual cylinder that allows the user to look continuously at all directions but not to move. This is the basis for the QuickTimeVR system <ref> [7] </ref>. The fixed position constraint can be relaxed by computing the optical flow between the example images and using it to interpolate between the cylinders constructed at different locations (cf. [4, 5, 6]) (originally proposed for views, not mosaics, but the principle is the same).
Reference: [8] <author> T. </author> <title> Ezzat Example-based analysis and synthesis for images of human faces. </title> <type> M.Eng. Thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: This simple technique can be used to control several non-rigid degrees of freedom such as facial expressions, as shown by <ref> [4, 5, 17, 8] </ref>. It can be combined directly with the algebraic technique described earlier to control the position of the virtual camera. <p> Third, it is possible to bypass the curse of dimensionality problem by representing an object as a hierarchy of components. Interpolation networks responsible for each component are thus independent, for instance the mouth separately from the eyes <ref> [17, 8] </ref>. The main factors likely to set a lower bound on the number of example images needed for a given set of control parameters are therefore simple visibility constraints: every part of the scene to be rendered must be visible in at least two of the example images.
Reference: [9] <author> O.D. Faugeras. </author> <title> What can be seen in three dimensions with an uncalibrated stereo rig? In Proceedings of the European Conference on Computer Vision, </title> <address> pages 563-578, Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: We next consider how to obtain the seed tensor that starts the process. 2.3 The Seed Tensor of Two Views Given two acquired images we can construct a special tensor composed of the elements of the fundamen tal matrix <ref> [9] </ref> that can serve as a seed tensor that starts the chain of tensors, as follows.
Reference: [10] <author> O.D. Faugeras and B. Mourrain. </author> <title> On the geometry and algebra of the point and line correspondences between N images. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <address> Cam-bridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: These constraints first became prominent in [23] and the underlying theory has been studied intensively in <ref> [27, 13, 24, 10, 28, 14, 25] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
Reference: [11] <author> Gortler, S.J., Grzeszczuk, R., Szeliski, R. and Co-hen, M. </author> <booktitle> The Lumigraph In SIGGRAPH, </booktitle> <pages> pages 43 - 54, </pages> <year> 1996. </year>
Reference-contexts: Interpolation can also be performed directly on the plenoptic function [1] which represents the amount of light emitted at each point in space as a function of direction. Levoy et al. [16] and Gortler et al. <ref> [11] </ref> interpolate between a dense set of several thousands of example images to reconstruct a reduced plenop-tic function (under an occlusion-free world assumption). They considerably increase the number of example images to avoid computing optical flow between the model images.
Reference: [12] <author> W.E.L. </author> <title> Grimson. Why stereo vision is not always about 3D reconstruction. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1435, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Depth maps are easily provided for synthetic environments, whereas for real scenes the process is fragile especially under small base-line situations that arise due to the requirement of dense correspondence between the model images/mosaics <ref> [12] </ref>. In this paper we propose a new view-synthesis method that makes use of the recent development of multi-linear matching constraints, known as trilinear-ities, that were first introduced in [23].
Reference: [13] <author> R. </author> <title> Hartley. A linear method for reconstruction from lines and points. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 882-887, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: These constraints first became prominent in [23] and the underlying theory has been studied intensively in <ref> [27, 13, 24, 10, 28, 14, 25] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
Reference: [14] <author> A. </author> <title> Heyden. Reconstruction from image sequences by means of relative depths. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 1058-1063, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: These constraints first became prominent in [23] and the underlying theory has been studied intensively in <ref> [27, 13, 24, 10, 28, 14, 25] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
Reference: [15] <author> S. Laveau and O.D. Faugeras. </author> <title> 3-d scene representation as a collection of images. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <pages> pages 689-691, </pages> <address> Jerusalem, Israel, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The alternative approach, along the lines of this paper, is to reduce the number of acquired (model) images by exploiting the 3D-from-2D geometry of the problem with the aid of corresponding points between the model images. Laveau and Faugeras <ref> [15] </ref> were the first to use the epipolar constraint for view synthesis, allowing them to extrapolate, as well as interpolate, between the example images. <p> to singularities that arise under certain camera motions (like when the virtual camera center is collinear with the centers of the model cameras) and the relation between translational and rotational parameters of the virtual camera and the epipolar constraint is somewhat indirect and hence requires the specification of matching points <ref> [15] </ref>. The singular camera motions can be relaxed by using the depth map of the environment. <p> It is possible to use a simple "projective Z-buffering" procedure for enforcing the constraint that the surface is opaque (cf. <ref> [15] </ref>). If two pixels are reprojected onto the same cell, we simply choose the one close to the epipolar point corresponding to the projection of the new camera center on image 1. The epipolar point can be recovered linearly from the tensor fl jk i [27].
Reference: [16] <author> M. Levoy and P. Hanrahan. </author> <title> Light field rendering. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 31-42, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Interpolation can also be performed directly on the plenoptic function [1] which represents the amount of light emitted at each point in space as a function of direction. Levoy et al. <ref> [16] </ref> and Gortler et al. [11] interpolate between a dense set of several thousands of example images to reconstruct a reduced plenop-tic function (under an occlusion-free world assumption). They considerably increase the number of example images to avoid computing optical flow between the model images.
Reference: [17] <author> S. </author> <title> Lines The photo-realistic synthesis of novel views from example images M.S. </title> <type> Thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Preprocessing (a) Compute dense correspondence (optical flow) between the two model images. We use a coarse-to-fine implementation of [18] described in [3]. Interactive tools for improving correspondence <ref> [17] </ref> can be used as well, although in our experiments optical flow alone was sufficient. (b) Recover the fundamental matrix of the two model images from the correspondences. <p> This simple technique can be used to control several non-rigid degrees of freedom such as facial expressions, as shown by <ref> [4, 5, 17, 8] </ref>. It can be combined directly with the algebraic technique described earlier to control the position of the virtual camera. <p> Third, it is possible to bypass the curse of dimensionality problem by representing an object as a hierarchy of components. Interpolation networks responsible for each component are thus independent, for instance the mouth separately from the eyes <ref> [17, 8] </ref>. The main factors likely to set a lower bound on the number of example images needed for a given set of control parameters are therefore simple visibility constraints: every part of the scene to be rendered must be visible in at least two of the example images.
Reference: [18] <author> B.D. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Proceedings IJCAI, </booktitle> <pages> pages 674-679, </pages> <address> Vancouver, Canada, </address> <year> 1981. </year>
Reference-contexts: Preprocessing (a) Compute dense correspondence (optical flow) between the two model images. We use a coarse-to-fine implementation of <ref> [18] </ref> described in [3]. Interactive tools for improving correspondence [17] can be used as well, although in our experiments optical flow alone was sufficient. (b) Recover the fundamental matrix of the two model images from the correspondences.
Reference: [19] <author> Leonard McMillan and Gary Bishop. </author> <title> Plenoptic modeling: An image-based rendering system. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 39-46, </pages> <year> 1995. </year>
Reference-contexts: The singular camera motions can be relaxed by using the depth map of the environment. McMillan and Bishop <ref> [19] </ref> use a full depth map (3D reconstruction of the camera motion and the environment) together with the epipolar constraint to provide a direct connection between the virtual camera motion and the reprojection engine.
Reference: [20] <author> Torr P.H.S., Zisserman A., and Murray D. </author> <title> Motion clustering using the trilinear constraint over three views. In Workshop on Geometrical Modeling and Invariants for Computer Vision. </title> <publisher> Xidian University Press., </publisher> <year> 1995. </year>
Reference-contexts: Interactive tools for improving correspondence [17] can be used as well, although in our experiments optical flow alone was sufficient. (b) Recover the fundamental matrix of the two model images from the correspondences. We use a robust estimator based on a Monte Carlo technique described in <ref> [20] </ref>. (c) Construct the rank-2 tensor (Eq. 5) from the elements of the fundamental matrix. 2.
Reference: [21] <author> B. Rousso, S. Avidan, A. Shashua, and S. Peleg. </author> <title> Robust recovery of camera rotation from three frames. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> San-Francisco, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: The matrix a j i representing the rotational component of camera motion between the two model views 1,2 can be represented in closed form as a func tion of the tensor ff jk i as described in <ref> [21] </ref>: X = det @ j3 ff 2 + ff 3 j3 j2 1 Y = det @ j3 ff 2 + ff 3 j3 j2 1 Z = det @ j2 ff 2 + ff 3 j3 j2 1 K = det @ j2 ff j3 3 j3 j2 1
Reference: [22] <author> Steven M. Seitz, Charles R. Dyers. </author> <title> View morphing. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 21-30, </pages> <year> 1996. </year>
Reference-contexts: However, interpolation may produce physically-invalid images. Seitz and Dyer <ref> [22] </ref> proposed a physically-valid view interpolation method. The method involves recovering the epipolar geometry between the two acquired images and having interpolation done along the rectified epipolar lines. <p> Methods that estimate 3D structure are very noisy with small baselines. Although our technique implicitly estimates the 3D structure, not doing so in an explicit way means avoiding noisy steps and therefore generating less noisy virtual images. Moreover, morphing techniques, such as <ref> [22] </ref>, require large baselines since they cannot perform extrapolation. The problems with correspondence and occlusions can be solved at the expense of increasing the number of examples.
Reference: [23] <author> A. Shashua. </author> <title> Algebraic functions for recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(8) </volume> <pages> 779-789, </pages> <year> 1995. </year>
Reference-contexts: In this paper we propose a new view-synthesis method that makes use of the recent development of multi-linear matching constraints, known as trilinear-ities, that were first introduced in <ref> [23] </ref>. The trilineari-ties provide a general (not subject to singular camera configurations) warping function from model images to novel synthesized images governed directly by the camera parameters of the virtual camera. <p> These constraints first became prominent in <ref> [23] </ref> and the underlying theory has been studied intensively in [27, 13, 24, 10, 28, 14, 25]. <p> There are alternative ways of performing reprojection, but if we would like to do it without recovering first a 3D model of the scene, the trilinear tensor generally provides the best results since it is free from singular configurations (see <ref> [2, 23, 26] </ref>). We have described so far the implementation of the reprojection paradigm via the trilinear equations.
Reference: [24] <author> A. Shashua and P. Anandan. </author> <title> The generalized tri-linear constraints and the uncertainty tensor. </title> <booktitle> In Proceedings of the ARPA Image Understanding Workshop, </booktitle> <address> Palm Springs, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: These constraints first became prominent in [23] and the underlying theory has been studied intensively in <ref> [27, 13, 24, 10, 28, 14, 25] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
Reference: [25] <author> A. Shashua and S. Avidan. </author> <title> The rank4 constraint in multiple view geometry. </title> <booktitle> In Proceedings of the European Conference on Computer Vision, </booktitle> <address> Cam-bridge, UK, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: These constraints first became prominent in [23] and the underlying theory has been studied intensively in <ref> [27, 13, 24, 10, 28, 14, 25] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
Reference: [26] <author> A. Shashua and S.J. Maybank. </author> <title> Degenerate n point configurations of three views: </title> <type> Do critical surfaces exist? Technical Report TR 96-19, </type> <institution> He-brew University of Jerusalem, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: There are alternative ways of performing reprojection, but if we would like to do it without recovering first a 3D model of the scene, the trilinear tensor generally provides the best results since it is free from singular configurations (see <ref> [2, 23, 26] </ref>). We have described so far the implementation of the reprojection paradigm via the trilinear equations.
Reference: [27] <author> A. Shashua and M. Werman. </author> <title> On the trilinear tensor of three perspective views and its underlying geometry. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: These constraints first became prominent in [23] and the underlying theory has been studied intensively in <ref> [27, 13, 24, 10, 28, 14, 25] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping. <p> If two pixels are reprojected onto the same cell, we simply choose the one close to the epipolar point corresponding to the projection of the new camera center on image 1. The epipolar point can be recovered linearly from the tensor fl jk i <ref> [27] </ref>. Note that we recover the epipolar points only for resolving visibility problems, not for reprojection, thus inaccuracy in the epipolar points would not affect the accuracy of reprojection.
Reference: [28] <author> B. Triggs. </author> <title> Matching constraints and the joint image. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 338-343, </pages> <address> Cam-bridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: These constraints first became prominent in [23] and the underlying theory has been studied intensively in <ref> [27, 13, 24, 10, 28, 14, 25] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
References-found: 28

