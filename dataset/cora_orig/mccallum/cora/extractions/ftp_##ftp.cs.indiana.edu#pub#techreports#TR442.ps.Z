URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR442.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Email: nsundare@cs.indiana.edu gannon@cs.indiana.edu  
Title: Object Template Abstractions for Light-Weight Data-Parallelism  
Author: Neelakantan Sundaresan Dennis Gannon 
Address: 215 Lindley Hall  Bloomington, IN 47405  
Affiliation: Computer Science Department  Indiana University  
Abstract: Data-parallelism is a widely used model for parallel programming. Control structures like parallel DO loops, and data structures like collections have been used to express data-parallelism. In typical implementations, these constructs are 'flat' in the sense that only one data-parallel operation is active at any time. To model applications that can exploit overlap of synchronization and computation in data-parallel tasks, or that have independent but limited data-parallelism, or that depict static or hierarchical nested parallelism, a more dynamic model is required. This paper describes how to combine light-weight thread mechanism with object-oriented methodologies to provide light-weight fl This research was funded in part by: ARPA DABT63-94-C-0029 and Rome Labs Contract AF 30602-92-C-0135 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gul Agha and Christian Callsen. ActorSpace: </author> <title> An Open Distributed Programming Paradigm. </title> <journal> In ACM Sigplan Notices, </journal> <pages> pages 23-32, </pages> <year> 1993. </year> <booktitle> Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOPP), </booktitle> <address> San Diego, California, </address> <month> May 19-22, </month> <year> 1993. </year>
Reference-contexts: Iterates [7], Presto [12] and Coir++ [18] model data-parallelism for shared-memory machines. ActorSpaces <ref> [1] </ref> and CA [4] in actor systems, provide aggregate computations on top of message-driven control models. Gang-scheduling [19] in multi-threaded operating systems also involves a group mechanism.
Reference: [2] <author> Kendall Atkinson. </author> <title> An Introduction to Numerical Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1978. </year>
Reference-contexts: Figure 9 shows the pseudo code for an adaptive volume integration algorithm <ref> [2] </ref> on the SGI/Power-challenge. In this example the threads in the ropes were allowed to migrate freely. The levels of nesting are not known at static time and the decision about whether or not a new rope should be created is made at run time based on the system state.
Reference: [3] <author> Fran~cois Bodin, Peter Beckman, Dennis Gannon, Shelby Yang, Allen Malony, and Bernd Mohr. </author> <title> Implementing a Parallel C++ Runtime System for Scalable Parallel Systems. </title> <booktitle> Proceedings, Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year> <journal> ACM Sigarch and IEEE Computer Society Technical Committees on Supercomputing Applications and Computer Architecture. </journal>
Reference-contexts: Distribution schemes are similar to data-distribution in HPF [9] or pC++ <ref> [3] </ref>. The difference is that the distribution schemes depict both data and control distribution. The distribution scheme objects are passed as additional arguments to the Rope class constructor. The system defines BLOCK and 6 CYCLIC distribution scheme objects. User-specified distribution scheme classes can be passed.
Reference: [4] <author> Andrew Chien and William Dally. </author> <title> Concurrent Aggregrates (CA). </title> <booktitle> In Second ACM Symposium on Principles and Practice of Parallel Programming, SIGPLAN Notices, </booktitle> <volume> volume 25(3), </volume> <pages> pages 187-196, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Iterates [7], Presto [12] and Coir++ [18] model data-parallelism for shared-memory machines. ActorSpaces [1] and CA <ref> [4] </ref> in actor systems, provide aggregate computations on top of message-driven control models. Gang-scheduling [19] in multi-threaded operating systems also involves a group mechanism.
Reference: [5] <author> Ian Foster, Carl Kesselman, Robert Olson, and Steve Tuecke. </author> <title> Nexus: An Interoperability Layer for Parallel and Distributed Systems. </title> <type> Technical report, </type> <institution> California Institute of Technology, Computer Science Department, Pasadena, </institution> <address> CA., </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The scheduling 22 mechanism there involves scheduling and descheduling all the threads in a group at the same time, unlike Coir where threads in a rope are scheduled independent of each other. Other C language-based multithreaded runtime systems like Chant [8] and Nexus <ref> [5] </ref> are also adding data-parallel layers on top.
Reference: [6] <author> William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1994. </year> <title> The example programs from the book are available from ftp://info.mcs.anl.gov/pub/mpi/using. </title> <type> 24 </type>
Reference-contexts: 1 The Coir System The Coir system provides a model for both control and data parallelism. It is implemented as a C++ library. The system is designed to use reusable components like pthread-standard thread libraries [10] and MPI message passing libraries <ref> [6] </ref>. The architecture model subsumes both shared and distributed memory machines. The following paragraphs provide a brief description. For details on the design and implementation of Coir refer to [15, 16, 17]. 1.1 Control Parallelism In Coir, control parallelism is modeled in terms of light-weight user-level threads.
Reference: [7] <author> Dirk Grunwald and Suvas Vajracharya. </author> <title> The Design of an Object-Oriented Runtime System for Integrated Task and Object Parallelism. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Colorado, Boulder, </institution> <address> CO 80309-0430, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Solve (...) f SolveLocalGrid (); if (NeedToRefine (...)) f if (SufficientGranularity (...)) f GridRope newRope (...); newRope.Solve (); g else SequentialSolve (); g g 4 Related Work There are other systems that provide light-weight mechanism for data-parallelism. Iterates <ref> [7] </ref>, Presto [12] and Coir++ [18] model data-parallelism for shared-memory machines. ActorSpaces [1] and CA [4] in actor systems, provide aggregate computations on top of message-driven control models. Gang-scheduling [19] in multi-threaded operating systems also involves a group mechanism.
Reference: [8] <author> Matthew Haines, David Cronk, and Piyush Mehrotra. </author> <title> On the Design of Chant: A Talking Threads Package. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <address> Washington D.C., </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The scheduling 22 mechanism there involves scheduling and descheduling all the threads in a group at the same time, unlike Coir where threads in a rope are scheduled independent of each other. Other C language-based multithreaded runtime systems like Chant <ref> [8] </ref> and Nexus [5] are also adding data-parallel layers on top.
Reference: [9] <author> High Performance Fortran Forum. </author> <title> Draft: High Performance Fortran Language Specification, </title> <note> Version 1.0. also available as technical report CRPC-TR92225 from Center for Research on Parallel Computation, </note> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Distribution schemes are similar to data-distribution in HPF <ref> [9] </ref> or pC++ [3]. The difference is that the distribution schemes depict both data and control distribution. The distribution scheme objects are passed as additional arguments to the Rope class constructor. The system defines BLOCK and 6 CYCLIC distribution scheme objects. User-specified distribution scheme classes can be passed.
Reference: [10] <author> IEEE. </author> <title> Thread Extensions for Portable Operating Systems (Draft 6), </title> <month> February </month> <year> 1992. </year> <month> P1003.4a/6. </month>
Reference-contexts: 1 The Coir System The Coir system provides a model for both control and data parallelism. It is implemented as a C++ library. The system is designed to use reusable components like pthread-standard thread libraries <ref> [10] </ref> and MPI message passing libraries [6]. The architecture model subsumes both shared and distributed memory machines. The following paragraphs provide a brief description.
Reference: [11] <author> Alexander Klaiber and James Frankel. </author> <title> Comparing Data-Parallel and Message-Passing Paradigms. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, Vol. II (Software), </booktitle> <pages> pages 11-20. </pages> <publisher> CRC, Inc., </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: In the C* language, from any parallel context defined by the set of virtual processors participating in the parallel operation, only a subset of these virtual processors participate in a new parallel operation invoked from this context. This can result in 'sparse' contexts <ref> [11] </ref>. Coir defines the notion of caller domain and callee domain. The caller domain represents the domain subset containing the processor-contexts that participate in a rope creation or a rope-specific data 7 parallel method invocation.
Reference: [12] <institution> Kendall Square Research Parallel Programming Guide, </institution> <month> July </month> <year> 1993. </year> <title> Kendall Square Research Corporation, </title> <type> Technical Documentation. </type>
Reference-contexts: Solve (...) f SolveLocalGrid (); if (NeedToRefine (...)) f if (SufficientGranularity (...)) f GridRope newRope (...); newRope.Solve (); g else SequentialSolve (); g g 4 Related Work There are other systems that provide light-weight mechanism for data-parallelism. Iterates [7], Presto <ref> [12] </ref> and Coir++ [18] model data-parallelism for shared-memory machines. ActorSpaces [1] and CA [4] in actor systems, provide aggregate computations on top of message-driven control models. Gang-scheduling [19] in multi-threaded operating systems also involves a group mechanism.
Reference: [13] <author> Peter Shirley. </author> <title> Radiosity via Ray-tracing. </title> <editor> In James Arvo, editor, </editor> <booktitle> Graphics Gems II, chapter VI-4, </booktitle> <pages> pages 306-310. </pages> <publisher> Academic Press, </publisher> <year> 1991. </year>
Reference-contexts: The line consists of mainly solid segments indicating that the communication and computation overlap has successfully taken place. 3.2 Radiosity Through Ray-tracing Ray-tracing is used to compute the radiosity in the preprocessing stage of graphic rendering systems <ref> [13] </ref>. In this example, we assume a system consisting of a fixed set of polygons (triangles) representing parts of objects in a scene. For the sake of simplicity, one of the patches ( the 0th triangle), is counted as the light source.
Reference: [14] <author> Bjarne Stroustrup. </author> <title> The Design and Evolution of C++. </title> <publisher> Addison Wesley Publishing Company, </publisher> <year> 1994. </year>
Reference-contexts: Data-parallelism in Coir is light-weight in the sense that data-parallel tasks are assigned and executed at the level of light-weight threads. Barrier synchronization, reduction, and broadcast operations are also defined to 3 be rope-specific. 2.1 Rope Objects in C++ Rope objects are implemented as constrained container (template) classes <ref> [14] </ref>. The template parameters are constrained to be instantiated by arguments that are derived thread classes. The 'pseudo'-C++ syntax for rope templates is as given below: template &lt;class T : Thread&gt; class RopeTemplate f ... container class is inherited. The constructor takes 4 parameters, the last three being optional.
Reference: [15] <author> Neelakantan Sundaresan. </author> <title> Modeling Control and Dynamic Data-Parallelism in Object-Oriented Languages. </title> <type> PhD thesis, </type> <institution> Computer Science, Indiana University, </institution> <year> 1995. </year> <month> upcoming. </month>
Reference-contexts: The system is designed to use reusable components like pthread-standard thread libraries [10] and MPI message passing libraries [6]. The architecture model subsumes both shared and distributed memory machines. The following paragraphs provide a brief description. For details on the design and implementation of Coir refer to <ref> [15, 16, 17] </ref>. 1.1 Control Parallelism In Coir, control parallelism is modeled in terms of light-weight user-level threads. A thread is basically a sequence of instructions in execution in a program and has its own stack and program counter.
Reference: [16] <author> Neelakantan Sundaresan and Dennis Gannon. </author> <title> A Thread-Model for Supporting Task and Data Parallelism in Object-Oriented Parallel Languages. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 45-49, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The system is designed to use reusable components like pthread-standard thread libraries [10] and MPI message passing libraries [6]. The architecture model subsumes both shared and distributed memory machines. The following paragraphs provide a brief description. For details on the design and implementation of Coir refer to <ref> [15, 16, 17] </ref>. 1.1 Control Parallelism In Coir, control parallelism is modeled in terms of light-weight user-level threads. A thread is basically a sequence of instructions in execution in a program and has its own stack and program counter.
Reference: [17] <author> Neelakantan Sundaresan and Dennis Gannon. </author> <title> Experimental Evaluation of Coir: A System for Control and Data Parallelism. </title> <booktitle> In Seventh International Conference on Parallel and Distributed Computing and Systems, </booktitle> <month> October </month> <year> 1995. </year> <note> to appear. </note>
Reference-contexts: The system is designed to use reusable components like pthread-standard thread libraries [10] and MPI message passing libraries [6]. The architecture model subsumes both shared and distributed memory machines. The following paragraphs provide a brief description. For details on the design and implementation of Coir refer to <ref> [15, 16, 17] </ref>. 1.1 Control Parallelism In Coir, control parallelism is modeled in terms of light-weight user-level threads. A thread is basically a sequence of instructions in execution in a program and has its own stack and program counter.
Reference: [18] <author> Neelakantan Sundaresan and Linda Lee. </author> <title> An Object-Oriented Thread Model for Parallel Numerical Applications. </title> <booktitle> In Proceedings of the Second Annual Object-Oriented Numerics Conference, </booktitle> <month> April </month> <year> 1994. </year> <institution> Sunriver, Oregon. </institution>
Reference-contexts: Solve (...) f SolveLocalGrid (); if (NeedToRefine (...)) f if (SufficientGranularity (...)) f GridRope newRope (...); newRope.Solve (); g else SequentialSolve (); g g 4 Related Work There are other systems that provide light-weight mechanism for data-parallelism. Iterates [7], Presto [12] and Coir++ <ref> [18] </ref> model data-parallelism for shared-memory machines. ActorSpaces [1] and CA [4] in actor systems, provide aggregate computations on top of message-driven control models. Gang-scheduling [19] in multi-threaded operating systems also involves a group mechanism.
Reference: [19] <author> A Tevanian Jr., R Rashid, D Golub, D Black, E Cooper, and M Young. </author> <title> Mach Threads and the UNIX Kernel: The Battle for Control. </title> <booktitle> In Proceedings of the USENIX Summer Conference, </booktitle> <pages> pages 185-198, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Iterates [7], Presto [12] and Coir++ [18] model data-parallelism for shared-memory machines. ActorSpaces [1] and CA [4] in actor systems, provide aggregate computations on top of message-driven control models. Gang-scheduling <ref> [19] </ref> in multi-threaded operating systems also involves a group mechanism. The scheduling 22 mechanism there involves scheduling and descheduling all the threads in a group at the same time, unlike Coir where threads in a rope are scheduled independent of each other.
Reference: [20] <institution> Thinking Machines Corporation. C* Release Notes. Thinking Machines Corporation, </institution> <address> Cambridge, MA, </address> <month> December </month> <year> 1990. </year> <note> Version 6.0 and 6.1. 25 </note>
Reference-contexts: These objects support asynchronous data-parallel operations, and synchronization operations. These objects appear like normal C++ objects, but at the same time follow data-parallel semantics. The model of execution of these objects is more powerful than the HPF or C* <ref> [20] </ref> models. We gave examples of how multithreading can be used to effectively use data-parallelism in three classes of applications flat data-parallel, static nested data-parallel, and hierarchical nested data-parallel.
References-found: 20

