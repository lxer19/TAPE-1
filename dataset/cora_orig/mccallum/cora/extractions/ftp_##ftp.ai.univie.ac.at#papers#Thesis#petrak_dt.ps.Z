URL: ftp://ftp.ai.univie.ac.at/papers/Thesis/petrak_dt.ps.Z
Refering-URL: http://www.ai.univie.ac.at/oefai/aisoc/peace.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Diplomarbeit VIE-CBR2 An Object-Oriented Case-Based Learning System  unter Anleitung von  
Author: Ausgefuhrt o.Prof. Dr. Robert Trappl und Univ.Doz. Dr. Gerhard Widmer von Johann Petrak Stuwerstr. / 
Date: Wien, im April 1995  
Affiliation: am Institut fur Medizinische Kybernetik und Artificial Intelligence der Universitat Wien  
Pubnum: A-1020 Wien  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1989). </year> <title> Incremental, Instance-Based Learning of Independent and Graded Concept Descriptions. </title> <editor> In A. M. Segre (Ed.), </editor> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Los Altos, CA, </address> <pages> pp. 387-391. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Aha reports that BSS outperforms FSS and beam-search outperforms hill-climbing in the domain he used for testing. The IB4 algorithm <ref> (Aha 1989) </ref> performs a feature weight adaption as part of the learning process (see section 3.3.2). 3.3 Aha's Instance-Based Learning Algorithms Aha, Kibler, and Albert (1991) describe a series of incremental learning algorithms named IB1, IB2, and IB3.
Reference: <author> Aha, D. W. </author> <year> (1991). </year> <title> Case-Based Learning Algorithms. </title> <booktitle> In Proceedings of the DARPA Case-Based Reasoning Workshop, </booktitle> <pages> pp. 147-158. </pages> <note> Available as http://www.aic.nrl.navy.mil/~aha/papers/aha-cbr91.ps. </note>
Reference-contexts: There are several different ways to define similarity. One straightforward one is to just take the negative value of a distance metric <ref> (Aha, Kibler, and Albert 1991) </ref>. A variation of this is to define two identical feature vectors to have similarity 1, and two completely different feature vectors to have similarity 0.
Reference: <author> Aha, D. W. </author> <year> (1995). </year> <title> An implementation and experiment with the nested generalized exemplars algorithm. </title> <type> Technical Report AIC-95-003, </type> <institution> Naval Research Laboratory, Navy Center for Applied Research in Artificial Intelligence, </institution> <address> Wash. D.C., Washington, D.C. </address>
Reference: <author> Aha, D. W. & R. L. Bankert. </author> <title> Feature Selection for Case-Based Classification of Cloud Types: An Empirical Comparison. </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-07), </booktitle> <address> Meno Park, CA. </address> <note> AAAI Press. Also available as NCARAI TR: AIC-94-011 http://www.aic.nrl.navy.mil/papers/1994/AIC-94-011.ps.Z. </note>
Reference: <author> Aha, D. W. & R. L. </author> <month> Bankert </month> <year> (1995). </year> <title> A Comparative Evaluation of Sequential Feature Selection Algorithms. </title> <booktitle> In Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, </address> <pages> pp. 1-7. </pages>
Reference: <author> Aha, D. W. & D. </author> <title> Kibler (1989). Noise-Tolerant Instance-Based Learning Algorithms. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <address> Los Altos, CA, </address> <pages> pp. 794-799. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Aha reports that BSS outperforms FSS and beam-search outperforms hill-climbing in the domain he used for testing. The IB4 algorithm <ref> (Aha 1989) </ref> performs a feature weight adaption as part of the learning process (see section 3.3.2). 3.3 Aha's Instance-Based Learning Algorithms Aha, Kibler, and Albert (1991) describe a series of incremental learning algorithms named IB1, IB2, and IB3.
Reference: <author> Aha, D. W., D. Kibler, & M. K. </author> <title> Albert (1991). Instance-Based Learning Algorithms. Machine Learning 6 (1), 37-66. ANSI Committee (1994). Programming Language Common Lisp draft proposed National Standard for Information Systems. Document number X3J13/94-101. </title> <note> Available from ftp://parcftp.xerorx.com/pub/cl. 122 BIBLIOGRAPHY 123 Bergadano, </note> <editor> F. & L. D. Raedt (Eds.) </editor> <year> (1994, </year> <month> April). </month> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <address> Berlin. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: There are several different ways to define similarity. One straightforward one is to just take the negative value of a distance metric <ref> (Aha, Kibler, and Albert 1991) </ref>. A variation of this is to define two identical feature vectors to have similarity 1, and two completely different feature vectors to have similarity 0.
Reference: <author> Biberman, Y. </author> <year> (1994, </year> <month> April). </month> <title> A Context Similarity Measure. </title> <booktitle> See Bergadano and Raedt (1994), </booktitle> <pages> pp. 49-63. </pages>
Reference: <author> Broder, A. </author> <year> (1990, </year> <title> January/February). Strategies for Efficient Incremental Nearest Neighbor. </title> <booktitle> Pattern Recognition 23 (1/2), </booktitle> <pages> 171-178. </pages>
Reference: <author> Chang, C. </author> <year> (1974, </year> <month> November). </month> <title> Finding Prototypes for Nearest Neighbor Classifiers. </title> <journal> IEEE Transactions on Computers C-23 (11), </journal> <pages> 1179-1184. </pages>
Reference: <author> Cohen, W. W. & H. Hirsh (Eds.) </author> <year> (1994). </year> <institution> Rutgers University New Brunswick, </institution> <address> New Jersey: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cost, S. & S. </author> <title> Salzberg (1993). A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features. </title> <booktitle> Machine Learning 10 (1), </booktitle> <pages> 57-78. </pages>
Reference-contexts: assign if one value is missing. two-mv: The similarity to assign if both values. are missing mi '$fsim-vdm &key class set (init-now t) field missing-value (one-mv 0) (two-mv 0) [Macro] Creates a field similarity object for calculation of value similarities according to the modified Stanfill-Waltz difference metric as described by <ref> (Cost and Salzberg 1993) </ref> (section 3.1.2.1). The arguments ftype, missing-value, one-mv, two-mv have the same meaning as with $fsim-standard. field: the field description object (or its global name) that describes the field for which the similarity is to be calculated subsequently.
Reference: <author> Cover, T. M. & P. E. Hart (1967, </author> <month> January). </month> <title> Nearest Neighbor Pattern Classification. </title> <journal> IEEE Transactions on Information Theory IT-13 (1), </journal> <pages> 21-27. </pages>
Reference: <author> Dasarathy, B. V. </author> <year> (1979, </year> <month> October). </month> <title> All You Needed to Know About the Neighbors (But Never Could Figure Out How!) Recognition Under Partial Exposure and Imperfect Supervision. </title> <booktitle> In Proceedings of the International Conference on Cybernetics and Society, </booktitle> <pages> pp. 218-221. </pages> <publisher> IEEE Press. </publisher>
Reference: <author> Dasarathy, B. V. (Ed.) </author> <year> (1991). </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> Los Alamitos, </publisher> <address> California: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Devijver, P. A. & J. </author> <title> Kittler (1980, December). On the Edited Nearest Neighbor Rule. </title> <booktitle> In Proceedings of the 5th International Conference on Pattern Recognition, </booktitle> <address> Los Alamitos, CA, </address> <pages> pp. 72-80. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Dixon, J. K. </author> <year> (1979, </year> <month> October). </month> <title> Pattern Recognition with Partly Missing Data. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics SMC-9 (10), </journal> <pages> 617-621. </pages>
Reference: <author> Dorffner, G. </author> <year> (1991). </year> <title> Konnektionismus - Von Neuronalen Netzwerken zu einer ,,naturlichen" KI. </title> <publisher> Stuttgart: Teubner. </publisher>
Reference: <author> Dudani, S. A. </author> <year> (1976, </year> <month> April). </month> <title> The Distance-Weighted k-Nearest Neighbor Rule. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics SMC-6 (4), </journal> <pages> 325-327. </pages>
Reference-contexts: CASE-BASED LEARNING 27 1970), the reject option (Hellman 1970; Wilcox and Wilson 1980), the distance-weighted k-NN rule <ref> (Dudani 1976) </ref>, fuzzy k-NN rules (Jozwik 1983; Keller, Gray, and Givens, Jr. 1985), various editing rules (Wilson 1972; Chang 1974; Devijver and Kittler 1980), and attempts to reduce computational complexity (Yunck 1976; Niemann and Goppert 1988; Broder 1990).
Reference: <author> Egmont-Petersen, M., J. L. Talmon, J. Brender, & P. </author> <title> McNair (1994). On the quality of neural net classifiers. </title> <booktitle> Artificial Intelligence in Medicine (6), </booktitle> <pages> 359-381. </pages> <note> BIBLIOGRAPHY 124 Feelders, </note> <author> A. & W. </author> <month> Verkooijen </month> <year> (1995, </year> <month> Jan). </month> <title> Which method learns most from the data? Methodological issues in the analysis of comparative studies. </title> <booktitle> In Proceedings of the fifth international workshop on AI and Statistics, </booktitle> <address> Fort Lauderdale, Florida, </address> <pages> pp. 219-225. </pages>
Reference: <author> Fix, E. & J. L. </author> <title> Hodges (1951). Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties. </title> <type> Technical Report Project 21-49-004, Report Number 4, </type> <institution> USAF School of Aviaton Medicine, Randolph Field, Texas. </institution> <note> Reprinted in Dasarathy (1991). </note>
Reference: <author> Fleiss, J. L., J. Cohen, & B. S. </author> <title> Everitt (1969). Large Sample Standard Errors of Kappa and Weighted Kappa. </title> <journal> Psychological Bulletin 72 (5), </journal> <pages> 323-327. </pages>
Reference: <author> Forbes, A. D. </author> <year> (1995, </year> <month> May). </month> <title> Classification-Algorithm Evaluation: Five Performance Measures Based on Confusion Matrices. </title> <note> Journal of Clinical Monitoring 11 (3). </note>
Reference-contexts: . . . . . . . . 3 7 7 7 7 ; i = 1 : : : J m i;j = 1 (2:12) The result of applying a classifier to a test set can be summed in a similar class confusion matrix O = [o i;j ] <ref> (see Forbes 1995 and Egmont-Petersen, Talmon, Brender, and McNair 1994) </ref> that lists the number of times class j was assigned to a case with real class i. This matrix O actually has one additional row o i;J+1 that contains the number of times a class could not be assigned.
Reference: <author> Friedman, J. H. </author> <year> (1994, </year> <title> Nov.). Flexible Metric Nearest Neighbor Classification. </title> <note> Available as http://playfair.stanford.edu/reports/friedman/flexmet1.ps.Z. </note>
Reference-contexts: In actual learning situations, the number of samples in the training set is of course limited. The higher the dimensionality of the feature vector, the larger the volume of a hyper-sphere with fixed diameter and the smaller the probability to find a sample in it <ref> (Friedman 1994) </ref>. Consequently, the accuracy achieved with real-life datasets may be far worse than the theoretical limit of half the Bayes accuracy. Another consequence is that the choice of a metric can be crucial for the performance of the classifier.
Reference: <author> Gao, Q. & M. </author> <month> Li </month> <year> (1989). </year> <title> The Minimum Description Length Principle And Its Application to Online Learning of Handprinted Characters. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <address> Los Altos, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The expected amount of information (in bits) for J class labels c i is: i=1 CHAPTER 2. MACHINE LEARNING 21 Rissanen (1986) presents the minimum description length principle that has been used to select "simple" concept descriptions from a set of possible descriptions <ref> (e.g. Gao and Li 1989) </ref>. The value to be minimized is the sum of the length of the description -log 2 (P (D)) and the length of encoding of data with the help of this description -log 2 (P (T jD)).
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Hartung, J., B. Elpelt, & K.-H. </author> <note> Kl osner (1989). </note> <institution> Statistik - Lehr- und Handbuch der angewandten Statistik. Munchen: Oldenbourg. </institution>
Reference-contexts: The most exact one is to use the Pearson-Clopper values use the F -distribution <ref> (Hartung, Elpelt, and Klosner 1989) </ref>: CI 2 [CI L ; CI U ] (2:18) where CI L (n T ; n; ff) = 2 2 CI U (n T ; n; ff) = 2 2 A formula that uses the standard normal distribution but is corrected for small values is given
Reference: <author> Hastie, T. & R. </author> <month> Tibishirani </month> <year> (1994). </year> <title> Discriminant Adaptive Nearest Neighbor Classification. </title> <type> Technical report, </type> <institution> Dpt. of Statistics, Stanford University, Stanford, CA. </institution> <note> Available from http://playfair.stanford.edu/author.html. </note>
Reference: <author> Hellman, M. </author> <year> (1970, </year> <month> July). </month> <title> The Nearest Neighbor Classification Rule with a Reject Option. </title> <journal> IEEE Transactions on Systems Science and Cybernetics SSC-6 (3), </journal> <pages> 179-185. </pages>
Reference-contexts: One method to estimate misclassification risk is to look at k 2 nearest neighbors and assign a class only if it is the same for all neighbors <ref> (Hellman 1970) </ref>. A less conservative rule is to require agreement of at least k 0 (k + 1)=2 neighbors. 3.1.3.3 Editing Rules Editing rules try to restrict the number of cases that are kept in memory for classification. There are two good reasons for doing this: 1.
Reference: <author> Hutton, L. V. </author> <year> (1992). </year> <title> Using Statistics to Asses The Performance of Neural Network Classifiers. </title> <type> Johns Hopkins APL Technical Digest 13 (2), </type> <pages> 291-299. </pages>
Reference-contexts: Any classifier that performs significantly (in the statistical sense) better than default accuracy can be said to perform at least better than chance. Comparing classifiers in a statistically sound way is not easy, and there are a broad range of methods mentioned in the literature <ref> (see Hutton 1992, and Feelders and Verkooijen 1995) </ref>. Different methods have to be used depending on the following factors, among others: * the number of classifiers to be compared CHAPTER 2.
Reference: <author> Jain, A. K., R. C. Dubes, & C. </author> <month> Chen </month> <year> (1987). </year> <title> Bootstrap techniques for error estimation. </title> <journal> IEEE transactions on pattern analysis and machine intelligence PAMI-9 (5), </journal> <pages> 628-633. </pages>
Reference-contexts: The bootstrap method is therefore considered not to be useful to estimate accuracy for nearest neighbor learning methods <ref> (Jain, Dubes, and Chen 1987) </ref>. Weiss (1991) presents a modified approach for using bootstrap estimators with k-nearest neighbor classifiers.
Reference: <author> John, G. H., R. Kohavi, & K. </author> <month> Pfleger </month> <year> (1994). </year> <title> Irrelevant Features and the Subset Selection Problem. </title> <note> See Cohen and Hirsh (1994). BIBLIOGRAPHY 125 Jozwik, </note> <author> A. </author> <year> (1983, </year> <month> July). </month> <title> A Learning Scheme for a Fuzzy k-NN Rule. </title> <journal> Pattern Recognition Letters 1 (5/6), </journal> <pages> 287-289. </pages>
Reference-contexts: There are two approaches how to find a feature subset for a given training set: filter methods and wrapper methods <ref> (John, Kohavi, and Pfleger 1994) </ref>. Filter methods try to select relevant features without any knowledge of the learning algorithm. Wrapper methods use the learning algorithm repeatedly with different feature subsets on a subset of the original training set CHAPTER 3.
Reference: <author> Kaelbling, L. P. </author> <year> (1994). </year> <title> Associative Reinforcement Learning: Functions in k-DNF. </title> <booktitle> Machine Learning (15), </booktitle> <pages> 279-298. </pages>
Reference: <author> Keller, J. M., M. Gray, & J. Givens, Jr. </author> <year> (1985, </year> <title> July/August). A Fuzzy k-Nearest Neighbor Algorithm. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics SMC-15 (4), </journal> <pages> 580-585. </pages>
Reference: <author> Kibler, D. & P. </author> <title> Langley (1988). Machine Learning as an Experimental Science. </title> <booktitle> Machine Learning 3 (1). </booktitle>
Reference: <editor> Kodratoff, Y. & R. Michalski (Eds.) </editor> <booktitle> (1990). Machine Learning: An Artificial Intelligence Approach, Volume III. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. </title> <note> In IJCAI-95. To appear. </note>
Reference-contexts: Instead of (fl (T ))(v) we will just write fl (T; v) to indicate the class returned for the feature vector v by the classifier that was generated by fl on the set of preclassified cases from T <ref> (the notation has been taken from Kohavi 1995) </ref>. 2.2.2 Training Sets and Test Sets We have mentioned above that the set of cases presented to a learning system during the learning phase is usually called training set. <p> This is because the number of instances in each subset can deviate by one (this is a problem for small sample sizes). Under the condition that the learning system is stable under the perturbations caused by deleting the instances for the test set <ref> (see Kohavi 1995 for a definition of stability) </ref>, the standard deviation can be approximated by: XV = a XV (1 a XV ) (2:8) Stratified cross-validations try to ensure that approximately the same proportions of class values occur in each T i as in the original data set. <p> Weiss (1991) presents a modified approach for using bootstrap estimators with k-nearest neighbor classifiers. A more detailed discussion of accuracy estimation using cross-validation and bootstrap can also be found in <ref> (Kohavi 1995) </ref>. 2.2.5.2 Coverage Sometimes a classifier might not be able to find a classification for a case.
Reference: <author> Kohavi, R. & D. </author> <title> Sommerfield (1995). Feature Subset Selection Using the Wrapper Model: Overfitting and Dynamic Search Space Topology. </title> <note> In KDD-95. Submitted manuscript. </note>
Reference-contexts: Instead of (fl (T ))(v) we will just write fl (T; v) to indicate the class returned for the feature vector v by the classifier that was generated by fl on the set of preclassified cases from T <ref> (the notation has been taken from Kohavi 1995) </ref>. 2.2.2 Training Sets and Test Sets We have mentioned above that the set of cases presented to a learning system during the learning phase is usually called training set. <p> This is because the number of instances in each subset can deviate by one (this is a problem for small sample sizes). Under the condition that the learning system is stable under the perturbations caused by deleting the instances for the test set <ref> (see Kohavi 1995 for a definition of stability) </ref>, the standard deviation can be approximated by: XV = a XV (1 a XV ) (2:8) Stratified cross-validations try to ensure that approximately the same proportions of class values occur in each T i as in the original data set. <p> Weiss (1991) presents a modified approach for using bootstrap estimators with k-nearest neighbor classifiers. A more detailed discussion of accuracy estimation using cross-validation and bootstrap can also be found in <ref> (Kohavi 1995) </ref>. 2.2.5.2 Coverage Sometimes a classifier might not be able to find a classification for a case.
Reference: <author> Kolodner, J. </author> <year> (1993a). </year> <title> Case-Based Reasoning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kolodner, J. L. </author> <year> (1983). </year> <title> Reconstructive Memory: A computer model. </title> <booktitle> Cognitive Science 7 (4). </booktitle>
Reference-contexts: Another difficulty with discrimination networks is to keep the tree balanced when cases are added to case memory. Yet another serious problem are features for which no information is available (missing values). One strategy to overcome the problem of not finding relevant cases are redundant discrimination networks <ref> (Kolodner 1983) </ref>. Several different discrimination networks are processed in parallel. If a missing feature is required with one network, search continues with the remaining networks and the union of all retrieved cases is used for further selection.
Reference: <author> Kolodner, J. L. </author> <year> (1991). </year> <title> Improving Human Decision Making through Case-Based Decision Aiding. </title> <booktitle> AI Magazine (Summer), </booktitle> <pages> 52-68. </pages>
Reference: <author> Kolodner, J. L. (Ed.) </author> <year> (1993b). </year> <title> Case-Based Learning. Kluwer. </title> <booktitle> Reprint from Machine Learning 10(3) (1993). </booktitle>
Reference: <author> Kononenko, I. & I. </author> <title> Bratko (1991). Information-Based Evaluation Criterion for Classifier's Performance. </title> <booktitle> Machine Learning 6 (1), </booktitle> <pages> 67-80. </pages>
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic Programming On the Programming of Computers by Means of Natural Selection. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Lamport, L. </author> <year> (1986). </year> <title> L a T E X- A Document Preparation System. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The program used for extracting this documentation information from the source files is based on Mark Kantrowitz' LISP program user-manual.lisp 1 , but has been changed heavily to produce well-formatted L a T E X <ref> (Lamport 1986) </ref> output and support additional types of documentation information 2 . The components of VIE-CBR2 are listed in the same order as they occur in the source modules. An alphabetical listing is available through the index.
Reference: <author> Lanczos, C. </author> <year> (1964). </year> <title> A Precision Approximation of the Gamma Function. </title> <journal> SIAM Journal on Numerical Analysis ser. B, </journal> <volume> vol. 1, </volume> <pages> 86-96. </pages>
Reference-contexts: THE VIE-CBR2 SYSTEM 67 If b a + 1, a continued fraction is used: P (a; b) = 1 (a) B B @ b + 1a b+ 2a b+ C C A The gamma function (x) is calculated using Lanczos' series approximation <ref> (Lanczos 1964) </ref>: (z + 1) = 2 ) z+ 1 2 ) 2 c 0 + c 1 z+2 + + c N (4:5) The coefficients fl; c 0 : : : c N for the above formula have been taken from (Press, Teukolsky, Vetterling, and Flannery 1992).
Reference: <author> Lee, C. </author> <year> (1994, </year> <month> April). </month> <title> An Instance-Based Learning Method for Databases: An Information Theoretic Approach. </title> <note> See Bergadano and Raedt (1994), pp. 387-390. BIBLIOGRAPHY 126 Light, </note> <author> R. J. </author> <year> (1971). </year> <title> Measures of Response Agreement for Qualitative Data: Some Generalizations and Alternatives. </title> <journal> Psychological Bulletin 76 (5), </journal> <pages> 365-377. </pages>
Reference: <author> Manago, M., K.-D. Althoff, E. Auriol, R. Traph oner, S. Weiss, N. Conruyt, & F. </author> <title> Maurer (1993). Induction and Reasoning from Cases. </title> <note> See Richter, Weiss, Klaus-Dietethoff, and Al (1993). SEKI Report SR93-12 (SFB 314). Two Volumes. </note>
Reference: <author> Michalski, R. S. </author> <year> (1986). </year> <title> Understanding the nature of learning, </title> <note> Chapter 3. </note>
Reference-contexts: Some of the definitions given for "learning" in the literature are: Simon 1984: "Learning denotes changes in the system that are adaptive in the sense that they enable the system to do the same task or tasks drawn from the same population more efficiently the next time." <ref> (cited from Michalski 1986) </ref>. Scott 1983, p.360: "Learning is any process through which a system acquires synthetic a posteriori knowledge." Michalski 1986, p.10: "Learning is making useful changes in our minds." 5 CHAPTER 2.
Reference: <author> Volume II of Michalski, Carbonell, </author> <note> and Mitchell (1986). </note>
Reference: <author> Michalski, R. S., J. G. Carbonell, & T. M. Mitchell (Eds.) </author> <year> (1984). </year> <title> Machine Learning AnArtificial Intelligence Approach. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> Michalski, R. S., J. G. Carbonell, & T. M. Mitchell (Eds.) </author> <year> (1986). </year> <booktitle> Machine Learning AnArtificial Intelligence Approach, Volume II. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Some of the definitions given for "learning" in the literature are: Simon 1984: "Learning denotes changes in the system that are adaptive in the sense that they enable the system to do the same task or tasks drawn from the same population more efficiently the next time." <ref> (cited from Michalski 1986) </ref>. Scott 1983, p.360: "Learning is any process through which a system acquires synthetic a posteriori knowledge." Michalski 1986, p.10: "Learning is making useful changes in our minds." 5 CHAPTER 2.
Reference: <editor> Michie, D., D. J. Spiegelhalter, & C. C. Taylor (Eds.) </editor> <year> (1994). </year> <title> Machine Learning, Neural and Statistical Classification. </title> <booktitle> Ellis Horwood Series in Artificial Intelligence. </booktitle> <address> New York: </address> <publisher> Ellis Horwood. </publisher>
Reference: <author> Minsky, M. </author> <year> (1985). </year> <title> The Society of Mind. </title> <address> New York: </address> <publisher> Simon and Schuster. </publisher>
Reference: <author> Moore, A. W. & M. S. </author> <title> Lee (1994). Efficient Algorithms for Minimizing Cross Validation Error. </title> <booktitle> See Cohen and Hirsh (1994), </booktitle> <pages> pp. 190-198. </pages>
Reference: <author> Mott, S. </author> <year> (1993). </year> <title> Case-Based Reasoning: Market, Applications, and Fit With Other Technologies. </title> <booktitle> Expert Systems With Applications 6, </booktitle> <pages> 97-104. </pages>
Reference: <author> Niemann, H. & R. </author> <month> Goppert </month> <year> (1988, </year> <month> February). </month> <title> An Efficient Branch-and-Bound Nearest Neighbor Classifier. </title> <journal> Pattern Recognition Letters 7 (2), </journal> <pages> 67-72. </pages>
Reference: <author> Norvig, P. </author> <year> (1992). </year> <title> Paradigms of Artificial Intelligence Programming: Case Studies in Common LISP. </title> <address> San Mateo, </address> <publisher> Cal: Morgan Kaufmann. </publisher>
Reference: <author> Papadimitriou, C. H. & J. L. </author> <title> Bentley (1980). A Worst-Case Analysis of Nearest Neighbor Searching by Projection. </title> <booktitle> In Automata Languages and Programming, Volume 85 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 470-482. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Patrick, E. A. & F. P. </author> <title> Fischer (1970). A Generalized k-Nearest Neighbor Rule. </title> <booktitle> Information and Control 16, </booktitle> <pages> 128-152. </pages>
Reference: <author> Petrak, J. </author> <year> (1994). </year> <title> VIE-CBR - Vienna Case-Based Reasoning Tool, Version 1.0: Programmer's and Installation Manual. </title> <type> Technical Report TR-94-34, </type> <institution> Austrian Research Institute for Artificial Intelligence, Vienna. </institution> <note> BIBLIOGRAPHY 127 Petrak, </note> <editor> J., R. Trappl, & J. </editor> <title> F urnkranz (1994). The Possible Contribution of AI to the Avoidance of Crises and Wars: Using CBR Methods with the KOSIMO Database of Conflicts. </title> <type> Technical Report TR-94-32, </type> <institution> Austrian Research Institute for Artificial Intelligence, Vienna. </institution>
Reference-contexts: INTRODUCTION 2 1.1 History of VIE-CBR2 Work on the project "The Possible Contribution of AI Methods to the Avoidance of Conflict and War" 1 (Trappl 1985,1986, and 1992; Trappl and Miksch 1991; Trappl et al. 1993) suggested the application of Case-Based Learning methods to international conflict databases <ref> (Petrak, Trappl, and Furnkranz 1994) </ref>. Since no software meeting our requirements was readily available, VIECBR-1 was designed and implemented in LISP (Petrak 1994). VIE-CBR2 is a complete redesign of VIECBR-1. <p> Since no software meeting our requirements was readily available, VIECBR-1 was designed and implemented in LISP <ref> (Petrak 1994) </ref>. VIE-CBR2 is a complete redesign of VIECBR-1. Like its predecessor VIECBR-1, it provides support not only for the learning algorithms, but also for management of case libraries and easy experimentation with symbolic-valued data.
Reference: <author> Pfetsch, F. R. & P. </author> <title> Billing (1994). </title> <institution> Datenhandbuch nationaler und internationaler Konflikte. Baden-Baden: Nomos Verlagsgesellschaft. </institution>
Reference-contexts: In that case, the get-indexed method can be called with the hypothesis object as parameter. Decision trees that can be generated without the help of a learning algorithm are prioritized decision trees: 3 This example makes use of the KOSIMO database of political conflicts <ref> (Pfetsch and Billing 1994) </ref>. 4 The implementation of case index objects has not been completed at the time of this writing. CHAPTER 4. <p> An earlier version of VIE-CBR2 has been used in the context of the project "The possible Contribution of AI to the Avoidance of Conflict and War" for analysis of CHAPTER 5. CONCLUSION 79 the KOSIMO data base of conflicts <ref> (Pfetsch and Billing 1994) </ref>. These experiments have shown that even if classification accuracy is not much above the baseline accuracy, the information provided by the set of retrieved similar cases can be valuable to the user.
Reference: <author> Press, W. H., S. A. Teukolsky, W. T. Vetterling, & B. P. </author> <title> Flannery (1992). Numerical Recipes in C (2nd ed.). </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: function (x) is calculated using Lanczos' series approximation (Lanczos 1964): (z + 1) = 2 ) z+ 1 2 ) 2 c 0 + c 1 z+2 + + c N (4:5) The coefficients fl; c 0 : : : c N for the above formula have been taken from <ref> (Press, Teukolsky, Vetterling, and Flannery 1992) </ref>.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: T NIL T f7% $FTYPE-SYMB-ORDINAL FT-ORD1 "" (A B C D E) NIL g g f8% $FDESCR F3 "" F3 2 :INFO NIL T NIL T f9% $FTYPE-RATIO FT-RATIO "" NIL 0 0.0 10.0 g g )g (12 NIL NIL) g g T 4.12 Importing and Exporting Data Since C4.5 <ref> (Quinlan 1993) </ref> is an often-used machine learning program and almost all standard machine learning databases have been converted to the format used by this program at some time, an import filter for files in the data format used by C4.5 was added to VIE-CBR2.
Reference: <author> Rachlin, J., S. Kasif, S. Salzberg, & D. W. </author> <title> Aha (1994). Towards a Better Understanding of Memory-Based Reasoning Systems. </title> <note> See Cohen and Hirsh (1994), pp. 242-250. Also available as NCARAI TR: AIC-94-012 http://www.aic.nrl.navy.mil/papers/1994/AIC-94-012.ps.Z. </note>
Reference: <author> Richter, M. M. </author> <year> (1992). </year> <title> Classification and Learning of Similarity Measures. </title> <type> SEKI Report SR-92-18, </type> <institution> FB Informatik, Universitat des Saarlandes, Saarbrucken. </institution>
Reference: <editor> Richter, M. M., S. Weiss, Klaus-Dietethoff, & F. M. Al (Eds.) </editor> <year> (1993). </year> <institution> Germany. Fachbereich Informatik: University of Kaiserslautern. </institution> <type> SEKI Report SR93-12 (SFB 314). Two Volumes. </type>
Reference: <author> Riesbeck, C. K. & R. C. </author> <title> Schank (1989). Inside Case-Based Reasoning. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Rissanen, J. </author> <year> (1986). </year> <title> Stochastic Complexity and Modeling. </title> <journal> The Annals of Statistics 14 (3), </journal> <pages> 1080-1100. </pages>
Reference: <author> Ritter, G. L., H. B. Woodruff, S. R. Lowry, & T. L. </author> <month> Isenhour </month> <year> (1975, </year> <month> November). </month> <title> An Algorithm for a Selective Nearest Neighbor Decision Rule. </title> <journal> IEEE Transactions on Information Theory IT-21 (6), </journal> <pages> 665-669. </pages>
Reference: <editor> Rumelhart, D. E. & J. L. McClelland (Eds.) </editor> <booktitle> (1986). Parallel Distributed Processing, Explorations in the Microstructure of Cognition, </booktitle> <volume> Vol 1: </volume> <booktitle> Foundations and Vol 2: Psychological and Biological Models. </booktitle> <address> Cambidge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Salzberg, S. </author> <year> (1991). </year> <title> A Nearest Hyperrectangle Learning Method. </title> <booktitle> Machine Learning (6), </booktitle> <pages> 251-276. </pages>
Reference-contexts: If their classifications match, the hyperrectangle of the nearest exemplar is extended just enough to include the new instance. This step might cause the extended hyperrectangle to overlap with one or more other rectangles. A detailed pseudocode of the algorithm can be found in <ref> (Salzberg 1991) </ref>. Cost and Salzberg (1993) present a modification of the basic algorithm that uses a modified value distance metric (Stanfill and Waltz 1986) (see section 3.1.2.1). Aha (1995) presents a slightly modified implementation of the original algorithm CHAPTER 3. CASE-BASED LEARNING 38 and some results of its practical application.
Reference: <author> Sankoff, D. & J. B. Kruskal (Eds.) </author> <year> (1983). </year> <title> Time Warps, String Edits and Macromolecules The Theory and Practice of Sequence Comparison. </title> <address> Reading, Mass.: </address> <publisher> Addison-Wesley. BIBLIOGRAPHY 128 Sarle, </publisher> <editor> W. S. </editor> <booktitle> (1994a). Neural Network Implementation in SAS Software. In Proceedings of the Nineteenth Annual SAS Users Group, Cary, NC:. SAS Institute. </booktitle>
Reference-contexts: sets: &(S 1 ; S 2 ) = jS 1 [ S 2 j B B @ X e j 2 S 2 S 1 1 C C (3:13) 3.1.2.3 Sequences A well-known method for calculating the difference between features that have sequences as their value is the Levenshtein distance <ref> (Sankoff and Kruskal 1983) </ref>. The Levenshtein distance between two sequences is the minimal sum of the weights of those operations (insert, delete, modify) that are needed to transform one sequence into the other. CHAPTER 3.
Reference: <author> Sarle, W. S. </author> <year> (1994b). </year> <title> Neural Networks and Statistical Models. </title> <booktitle> In Proceedings of the Nineteenth Annual SAS Users Group, Cary, NC:, </booktitle> <pages> pp. 1538-1550. </pages> <publisher> SAS Institute. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1994). </year> <title> A Conservation Law for Generalization Performance. </title>
Reference: <institution> See Cohen and Hirsh (1994). </institution>
Reference-contexts: Values &gt; 0:5 indicate some deviation from chance. This metric can also be calculated conditioned on the assigned or true class values <ref> (see Egmont-Petersen, Talmon, Brender, and McNair 1994) </ref>. CHAPTER 2. MACHINE LEARNING 20 2.2.6.5 Information Score Kononenko and Bratko (1991) present a performance measure based on information theory, the information score: This measure indicates the degree to which a classifier succeeds in increasing the posterior probability of a correct classification.
Reference: <author> Schank, R. C. </author> <year> (1982). </year> <title> Dynamic Memory A theory of reminding and learning in computers and people. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference: <author> Schult, T. J. </author> <year> (1992). </year> <journal> Werkzeuge fur Fallbasierte Systeme. </journal> <volume> KI (4), </volume> <pages> 52-53. </pages>
Reference: <author> Scott, P. D. </author> <year> (1983). </year> <title> Learning: The Construction of A Posteriori Knowledge Structures. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence AAAI-83, </booktitle> <pages> pp. 359-363. </pages>
Reference: <author> Siegel, S. </author> <year> (1956). </year> <title> Nonparametric Statistics for the Behavioral Sciences. </title> <address> Tokyo: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Simon, H. </author> <year> (1984). </year> <title> Why should machines learn?, </title> <note> Chapter 2. In Michalski, Carbonell, and Mitchell (1984). </note>
Reference: <author> Skalak, D. B. </author> <year> (1994). </year> <title> Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms. </title> <booktitle> See Cohen and Hirsh (1994), </booktitle> <pages> pp. 293-301. </pages>
Reference: <author> Slade, S. </author> <year> (1991). </year> <title> Case-Based Reasoning: a reasearch paradigm. </title> <journal> AI Magazine 12 (1), </journal> <pages> 42-55. </pages>
Reference: <author> Stanfill, C. & D. Waltz (1986, </author> <month> December). </month> <title> Toward Memory-Based Reasoning. </title> <booktitle> Communications of the ACM 29 (12), </booktitle> <pages> 1213-1228. </pages>
Reference-contexts: This step might cause the extended hyperrectangle to overlap with one or more other rectangles. A detailed pseudocode of the algorithm can be found in (Salzberg 1991). Cost and Salzberg (1993) present a modification of the basic algorithm that uses a modified value distance metric <ref> (Stanfill and Waltz 1986) </ref> (see section 3.1.2.1). Aha (1995) presents a slightly modified implementation of the original algorithm CHAPTER 3. CASE-BASED LEARNING 38 and some results of its practical application.
Reference: <author> Steele Jr., G. L. </author> <year> (1990). </year> <title> Common LISP The Language (2nd ed.). Digital. </title>
Reference-contexts: Like its predecessor VIECBR-1, it provides support not only for the learning algorithms, but also for management of case libraries and easy experimentation with symbolic-valued data. In addition, VIE-CBR2 allows for easy extension through a modular and object-oriented implementation using the Common LISP Object System <ref> (Steele Jr. 1990) </ref>. All data structures visible to the user of the system are parameterizable CLOS objects. <p> We describe how to create objects, how to use methods and functions, and explain implementation details. 4.1 Overall Architecture VIE-CBR2 has been developed in LISP with the following conflicting design objectives in mind: * Portability - VIE-CBR2 should run on any CLtL2 <ref> (Steele Jr. 1990) </ref> com patible Common Lisp System. * Modularity: each functional component should be easily locateable and changeable without the need for adaption of other functional components. * Extensibility: the functionality of VIE-CBR2 (especially the reservoir of learning algorithms and their associated data structures) should be easily extensible. * Simplicity: <p> CHAPTER 4. THE VIE-CBR2 SYSTEM 75 4.17 Portability One of the main design objectives for VIE-CBR2 was portability. The code was developed so that it should run without modification on any LISP system that conforms to CLtL2 <ref> (Steele Jr. 1990) </ref> or to the draft ANSI X3J13 standard (ANSI Committee 1994) 7 . Although these two specifications differ in a number of ways, VIE-CBR2 tries to avoid usage of features that are affected by these differences, or provides alternative code for non-ANSI and ANSI conforming implementations.
Reference: <author> Tanimoto, S. L. </author> <year> (1990). </year> <title> The Elements Of Artificial Intelligence Using Common LISP. </title> <address> New York: </address> <publisher> Computer Science Press. </publisher>
Reference: <author> Trappl, R. </author> <year> (1985). </year> <note> AI for Warfare! AI for Peacefare? ECCAI Newsletter 2 (2). </note>
Reference: <author> Trappl, R. </author> <year> (1986). </year> <title> Reducing International Tension through Artificial Intelligence: A Proposal for 3 Projects. </title> <editor> In R. Trappl (Ed.), </editor> <title> Power, Autonomy, Utopia: New Approaches Toward Complex Systems. </title> <address> New York: </address> <publisher> Plenum. BIBLIOGRAPHY 129 Trappl, R. </publisher> <year> (1992). </year> <title> The Role of Artificial Intelligence in the Avoidance of War. </title> <editor> In R. Trappl (Ed.), </editor> <booktitle> Cybernetics and Systems '92, Singapore, </booktitle> <pages> pp. 1667-1672. </pages> <publisher> World Scientific. </publisher>
Reference: <author> Trappl, R. & S. </author> <month> Miksch </month> <year> (1991). </year> <note> Can Artificial Intelligence Contribute to Peacefare? In V. </note> <editor> Marik (Ed.), Aplikace Umele Inteligence AI'91, </editor> <booktitle> Prague, </booktitle> <pages> pp. 21-30. </pages>
Reference: <editor> Trappl, R., S. D. Unseld, & S. </editor> <booktitle> Miksch (1993). Artificial Intelligence und die mogliche Vermeidung von Krisen und Kriegen. </booktitle> <editor> In G. Hanke (Ed.), Informations- und Kommunikationstechnologie fur das neue Europa, </editor> <publisher> Wien, </publisher> <pages> pp. 511-519. ADV. </pages>
Reference: <author> Tversky, A. </author> <year> (1988). </year> <title> Features of Similarity, </title> <journal> pp. </journal> <pages> 290-302. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Uehara, K., M. Tanizawa, & S. </author> <title> Maekawa (1993). PBL: Prototype-Based Learning Algorithm. </title> <note> See Richter, Weiss, Klaus-Dietethoff, and Al (1993). SEKI Report SR93-12 (SFB 314). Two Volumes. </note>
Reference: <author> Weiss, S. M. </author> <year> (1991, </year> <month> March). </month> <title> Small Sample Error Rate Estimation for k-NN Classifiers. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 13 (3), </journal> <pages> 285-289. </pages>
Reference: <author> Weiss, S. M. & I. </author> <month> Kapouleas </month> <year> (1989). </year> <title> An Empirical Comparison of Pattern Recognition, Neural Nets, and Machine Learning Classification Methods. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <address> Los Altos, CA, </address> <pages> pp. 781-787. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wess, S., K.-D. Althoff, & G. </author> <month> Derwand </month> <year> (1993). </year> <title> Improving the Retrieval Step in Case-Based Reasoning. </title> <note> See Richter, Weiss, Klaus-Dietethoff, and Al (1993). SEKI Report SR93-12 (SFB 314). Two Volumes. </note>
Reference-contexts: Building the tree has complexity O (n 2 log (n 2 )) on the average and O (n 2 2 ) in the worst case. Average cost of retrieval can be reduced to O (log 2 (n 2 )) (worst case complexity is O (n 2 )) <ref> (Wess, Althoff, and Derwand 1993) </ref>. Chapter 4 The VIE-CBR2 System In this section we present the VIE-CBR2 system.
Reference: <author> Wettschereck, D. </author> <year> (1994). </year> <title> A Hybrid Nearest-Neighbor and Nearest-Hyperrectangle Algorithm. </title> <editor> In F. Bergadano and L. Raedt (Eds.), </editor> <booktitle> Proceedings of the European Conference on Machine Learning, </booktitle> <address> Berlin, </address> <pages> pp. 323-335. </pages> <publisher> Springer. </publisher>
Reference: <author> Wilcox, L. & D. Wilson (1980, </author> <month> December). </month> <title> A Single Nearest Neighbor Rule with a Reject Option. </title> <booktitle> In Proceedings of the 5th International Conference on Pattern Recognition, </booktitle> <address> Los Alamitos, CA, </address> <pages> pp. 98-102. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Wilson, D. L. </author> <year> (1972, </year> <month> July). </month> <title> Asymptotic Properties of Nearest Neighbor Rules Using Edited Data. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics SMC-2 (4), </journal> <pages> 408-421. </pages> <note> BIBLIOGRAPHY 130 Winston, </note> <author> P. H. & B. K. </author> <title> Horn (1989). </title> <booktitle> LISP. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher> <address> Third Edition. </address>

References-found: 98

