URL: ftp://cns.brown.edu/nin/papers/tradenips.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Email: inna@roberta.eng.tau.ac.il  nin@math.tau.ac.il  moshaiov@eng.tau.ac.il  
Title: Improving Recognition via Reconstruction  
Author: Inna Stainvas Nathan Intrator Amiram Moshaiov 
Address: Tel-Aviv University  Tel-Aviv University  Tel-Aviv University  
Affiliation: Department of Solid Mechanics, Materials, and Structures  Computer Science Department Sackler Faculty of Exact Sciences  Department of Solid Mechanics, Materials, and Structures  
Abstract: Learning a many-parameter model is generally an underconstrained problem that requires additional regularization. We study several information theoretic constraints and show that reconstruction constraints achieve improved performance on original and corrupted inputs. In addition to studying entropy and BCM constraints on the hidden units of a feed-forward architecture, we derive a new constraint that is based on the sum of the entropies of the hidden units. Results are demonstrated on a well known face recognition task in various resolutions and image corruptions.
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S., Cichocki, A., and Yang, H. H. </author> <year> (1996). </year> <title> A new learning algorithm for blind signal separation. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 757-763. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: ICA has been successfully applied for extraction of meaningful features from data (Bell and Sejnowski, 1997; Karhunen et al., 1997). The maximum entropy (ME) approach is formulated as maximization of the entropy of the projected inputs. It is directly connected to the minimum mutual information (MMI) <ref> (Amari et al., 1996) </ref> which minimizes the Kullback-Leibler divergence between the joint and the marginal distribution of the hidden units feature vector. This formulation does not depend on the nonlinear component-wise transformation.
Reference: <author> Atick, J. J. </author> <year> (1992). </year> <title> Could information theory provide an ecological theory of sensory processing? Network, </title> <booktitle> 3 </booktitle> <pages> 213-251. </pages>
Reference: <author> Barlow, H. B. </author> <year> (1989). </year> <title> Unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 295-311. </pages>
Reference-contexts: It should be noted that it has recently been shown that when the distribution does not contain clusters, directions with high kurtosis are found (Blais et al., 1997). Another approach, the so called Independent Component Analysis (ICA), has been proposed by <ref> (Barlow, 1989) </ref> and later formulated by (Comon, 1994; Bell and Sejnowski, 1995). The idea is to find a transformation of the input that leads to an efficient factorial code, namely, code with statistically independent components.
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> An information-maximisation approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7(6) </volume> <pages> 1129-1159. </pages>
Reference-contexts: The normalization is based on the number of outputs of each layer, namely the number of class labels and the number of image pixels. Stainvas et al. 3 Type of h (x; w) constraints Entropy maximization <ref> (Bell and Sejnowski, 1995) </ref> with sigmoidal activation function: I + (1 2wx)(wx) t BCM (Intrator and Cooper, 1992) Equation 4.5 Sum of entropies: Appendix Equation 2 with few types of nonlinear function f (u): Sum of entropies A u 3 Sum of entropies B 2tanh (u) Sum of entropies C 3
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1997). </year> <title> The independent components of natural scenes are edge filters. Vision Research. </title> <publisher> in press. </publisher>
Reference: <author> Bellman, R. E. </author> <year> (1961). </year> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference-contexts: According to this approach, an image is considered to be a vector of pixel intensities and thus, belongs to a very high dimensional space. This leads to difficulties that are widely known as the curse of dimensionality <ref> (Bellman, 1961) </ref> which says that there is hardly ever enough data to robustly train a classifier in high dimensional space. This leads to predictors with high variance and in order to control this variance, innovative bias constraints should be used (Geman, 1992).
Reference: <author> Bienenstock, E. L., N Cooper, L., and Munro, P. W. </author> <year> (1982). </year> <title> Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal Neuroscience, </journal> <volume> 2 </volume> <pages> 32-48. </pages>
Reference: <author> Blais, B. S., Intrator, N., and Cooper, L. N. </author> <year> (1997). </year> <title> Is there any difference between different learning rules on natural scene environment. </title> <type> Preprint. </type>
Reference-contexts: This is especially useful for classification as the prevailing assumption is that inputs belonging to the same class tend to cluster together in input space. It should be noted that it has recently been shown that when the distribution does not contain clusters, directions with high kurtosis are found <ref> (Blais et al., 1997) </ref>. Another approach, the so called Independent Component Analysis (ICA), has been proposed by (Barlow, 1989) and later formulated by (Comon, 1994; Bell and Sejnowski, 1995).
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis, a new concept? Signal Processing, </title> <booktitle> 36 </booktitle> <pages> 287-314. </pages>
Reference: <author> Geman, S. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference-contexts: This leads to predictors with high variance and in order to control this variance, innovative bias constraints should be used <ref> (Geman, 1992) </ref>. One way is to construct efficient low dimensional representations which are sufficient for the classification task. For example, this can be achieved by multiple class constraints (Intrator and Edelman, 1997). <p> We further compare this constraint with other (unsupervised) constraints that are seeking some structure in the data using the framework presented in (Intrator, 1993). 2 Methodology It is well known that Feed-forward Neural Networks which are often many-parameter models, are faced with bias/variance dilemma <ref> (Geman, 1992) </ref>. This dilemma states the tradeoff between the variance and bias fl Corresponding author. Current address: Box 1843, Brown University, Providence RI 02912 Stainvas et al. 2 portion of the error. Variance is reduced by increasing the bias constraints.
Reference: <author> Hansen, L. K. and Salamon, P. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(10) </volume> <pages> 993-1001. </pages>
Reference-contexts: This independence leads to a reduced contribution from the variance portion of the error when ensemble average is used (Raviv and Intrator, 1996). We have been using two types of ensemble classification prediction. The first one is a majority rule over all the experts in the ensemble <ref> (Hansen and Salamon, 1990) </ref>. We call this a classification ensemble. Another rule is based on averaging the real values of the outputs of all the ensemble members and then producing a decision by thresholding. We call this a regression ensemble.
Reference: <author> Intrator, N. </author> <year> (1993). </year> <title> Combining exploratory projection pursuit and projection pursuit regression with application to neural networks. </title> <journal> Neural Computation, </journal> <volume> 5(3) </volume> <pages> 443-455. </pages>
Reference-contexts: In this paper, we continue this line of thought and study the effect of reconstruction constraint on the resulting classifier. We further compare this constraint with other (unsupervised) constraints that are seeking some structure in the data using the framework presented in <ref> (Intrator, 1993) </ref>. 2 Methodology It is well known that Feed-forward Neural Networks which are often many-parameter models, are faced with bias/variance dilemma (Geman, 1992). This dilemma states the tradeoff between the variance and bias fl Corresponding author.
Reference: <author> Intrator, N. and Cooper, L. N. </author> <year> (1992). </year> <title> Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 3-17. </pages>
Reference-contexts: Some of these constraints are related to the information contents of the hidden-unit representation. For example, one might search for a hidden representation which contains the most entropy, or which has certain deviations from Gaussian distribution. If one is interested in multi-modality, a variant of the BCM learning rule <ref> (Intrator and Cooper, 1992) </ref> can be used, while if high kurtosis is of interest, some kurtosis maximization rule can be used, (Oja, 1995). <p> Stainvas et al. 3 Type of h (x; w) constraints Entropy maximization (Bell and Sejnowski, 1995) with sigmoidal activation function: I + (1 2wx)(wx) t BCM <ref> (Intrator and Cooper, 1992) </ref> Equation 4.5 Sum of entropies: Appendix Equation 2 with few types of nonlinear function f (u): Sum of entropies A u 3 Sum of entropies B 2tanh (u) Sum of entropies C 3 4 u 11 + 15 3 u 7 29 4 u 3 Table 1:
Reference: <author> Intrator, N. and Edelman, S. </author> <year> (1997). </year> <title> Learning low dimensional representations of visual object with extensive use of prior knowledge. </title> <note> To appear in Network. </note>
Reference-contexts: This leads to predictors with high variance and in order to control this variance, innovative bias constraints should be used (Geman, 1992). One way is to construct efficient low dimensional representations which are sufficient for the classification task. For example, this can be achieved by multiple class constraints <ref> (Intrator and Edelman, 1997) </ref>. In this paper, we continue this line of thought and study the effect of reconstruction constraint on the resulting classifier.
Reference: <author> Intrator, N., Reisfeld, D., and Yeshurun, Y. </author> <year> (1996). </year> <title> Face recognition using a hybrid supervised/unsupervised neural network. </title> <journal> Pattern Recognition Letters, </journal> <volume> 17 </volume> <pages> 67-76. </pages>
Reference-contexts: This independence leads to a reduced contribution from the variance portion of the error when ensemble average is used <ref> (Raviv and Intrator, 1996) </ref>. We have been using two types of ensemble classification prediction. The first one is a majority rule over all the experts in the ensemble (Hansen and Salamon, 1990). We call this a classification ensemble. <p> We randomly split the data-set to 14 training images and 13 testing images from each person (total of 210 training and 195 testing images.) Preprocessing details and previous results studying effect of background, illumination and comparison with PCA is given in <ref> (Intrator et al., 1996) </ref>. The preprocessing partially removes the variability due to viewpoint, by setting (automatically) the eyes and tip of the mouth to the same position in all images (Tankus et al., 1997).
Reference: <author> Jain, A. K. </author> <year> (1989). </year> <title> Fundamentals of Digital Image Processing. </title> <publisher> Prentice Hall, London. </publisher>
Reference-contexts: The column optimal NN refers to a single best -network. In the Salt and Pepper experiments, either 10% or 20% of the image were corrupted. Blurring with DOG filter: Difference of Gaussians (DOG) filter <ref> (Jain, 1989) </ref>, which produces a Mexican hat type receptive field, is a well known form of image preprocessing and assumed to be present in early mammal vision (center-surround cells). Standard deviations of the on and off center (positive and negative Gaussians) were 1 and 2 respectively.
Reference: <author> Karhunen, J., Hyvarinen, A., Vigardo, R., </author> <title> and E.Oja (1997). Applications of neural blind separation to signal and image processing. </title> <booktitle> to appear in Proc.IEEE 1997 Int. Conf. on Acoustics, Speech and Signal Processing, </booktitle> <month> April 21-24, </month> <year> 1997. </year>
Reference: <author> Kirby, M. and Sirovich, L. </author> <year> (1990). </year> <title> Application of the Karhunen-Loeve procedure for characterization of human faces. </title> <journal> PAMI, </journal> <volume> 12(1) </volume> <pages> 103-108. </pages>
Reference-contexts: Further preprocessing calculated the difference between each image and an average over all the training set, leading to the so called "caricature" images <ref> (Kirby and Sirovich, 1990) </ref>. Three resolutions were used: high - 64 fi 64, intermediate - 32 fi 32 and and low - 16 fi 16 pixels.
Reference: <author> Oja, E. </author> <year> (1995). </year> <title> PCA, </title> <booktitle> ICA, and nonlinear hebbian learning. </booktitle> <pages> pages 89-94. </pages>
Reference-contexts: If one is interested in multi-modality, a variant of the BCM learning rule (Intrator and Cooper, 1992) can be used, while if high kurtosis is of interest, some kurtosis maximization rule can be used, <ref> (Oja, 1995) </ref>. In the context of entropy maximization and independent component analysis (ICA), a recent review (Yang and Amari, 1997) provides a lot of insight and connection to other methods. In contrast, reconstruction constraints have not been used in this context.
Reference: <author> Olshausen, B. A. and Field, D. J. </author> <year> (1996). </year> <title> Natural image statistics and efficient coding. </title> <journal> Network, </journal> <volume> 7 </volume> <pages> 333-339. </pages>
Reference: <author> Poggio, T. and Girosi, F. </author> <year> (1994). </year> <title> A theory of networks for approximation and learning. </title> <journal> A.I. </journal> <note> Memo No.1140,C.B.I.P. Paper No.31, </note> <institution> Massachusetts Institute of technology. </institution>
Reference: <author> Pomerleau, D. A. </author> <year> (1993). </year> <title> Input reconstruction reliablility estimation. </title> <editor> In Giles, C. L., Hanson, S. J., and Cowan, J. D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5. </volume> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In contrast, reconstruction constraints have not been used in this context. They were used for reliability estimation of network familiarity with novel scenes <ref> (Pomerleau, 1993) </ref> for the purpose of car navigation. In this paper, we introduce reconstruction constraints as a bias-imposing mechanism for feed-forward architectures.
Reference: <author> Raviv, Y. and Intrator, N. </author> <year> (1996). </year> <title> Bootstrapping with noise: An effective regularization technique. </title> <journal> Connection Science, Special issue on Combining Estimators, </journal> <volume> 8 </volume> <pages> 356-372. </pages>
Reference-contexts: This independence leads to a reduced contribution from the variance portion of the error when ensemble average is used <ref> (Raviv and Intrator, 1996) </ref>. We have been using two types of ensemble classification prediction. The first one is a majority rule over all the experts in the ensemble (Hansen and Salamon, 1990). We call this a classification ensemble.
Reference: <author> Tankus, A., Yeshurun, Y., and Intrator, N. </author> <year> (1997). </year> <title> Face detection by direct convexity estimation. </title> <booktitle> In Proceedings of the 1st Intl. Conference on Audio- and Video-based Biometric Person Authentication, </booktitle> <address> Switzerland. </address> <publisher> Springer. </publisher>
Reference-contexts: The preprocessing partially removes the variability due to viewpoint, by setting (automatically) the eyes and tip of the mouth to the same position in all images <ref> (Tankus et al., 1997) </ref>. Further preprocessing calculated the difference between each image and an average over all the training set, leading to the so called "caricature" images (Kirby and Sirovich, 1990).
Reference: <author> Turk, M. and Pentland, A. </author> <year> (1991). </year> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3 </volume> <pages> 71-86. </pages>
Reference-contexts: It turns out that by averaging (in either way) over ensemble members that have been trained with different values of , some independence is achieved, leading to a useful collective decision. 2.5 Data-set description and some implementation details We have used the widely known face data-set <ref> (Turk and Pentland, 1991) </ref> for our simulations. While there have been many successful classification approaches to this data, we demonstrate here that when the data is of low resolution, or corrupted either by blur or partial occlusion, classification performance deteriorates dramatically and leave a lot of room for improvement.
Reference: <author> Wahba, G. </author> <year> (1990). </year> <title> Splines Models for Observational Data. </title> <booktitle> Series in Applied Mathematics, </booktitle> <volume> Vol. 59, </volume> <publisher> SIAM, Philadel-phia. </publisher>
Reference: <author> Yang, H. and Amari, S. </author> <year> (1997). </year> <title> Adaptive on-line learning algorithms for blind separation maximum entropy and minimum mutual information. </title> <note> To appear in Neural Computation. </note>
Reference-contexts: In the context of entropy maximization and independent component analysis (ICA), a recent review <ref> (Yang and Amari, 1997) </ref> provides a lot of insight and connection to other methods. In contrast, reconstruction constraints have not been used in this context. They were used for reliability estimation of network familiarity with novel scenes (Pomerleau, 1993) for the purpose of car navigation. <p> It is directly connected to the minimum mutual information (MMI) (Amari et al., 1996) which minimizes the Kullback-Leibler divergence between the joint and the marginal distribution of the hidden units feature vector. This formulation does not depend on the nonlinear component-wise transformation. As is shown in <ref> (Yang and Amari, 1997) </ref>, there can be several nonlinear transformations which may lead to useful ICA. The various constraints applied in this study are described in Table 1.
Reference: <editor> Stainvas et al. </editor> <volume> 7 </volume>
References-found: 28

