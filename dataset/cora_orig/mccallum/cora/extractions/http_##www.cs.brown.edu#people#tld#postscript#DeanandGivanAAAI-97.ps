URL: http://www.cs.brown.edu/people/tld/postscript/DeanandGivanAAAI-97.ps
Refering-URL: http://www.cs.brown.edu/research/ai/publications/
Root-URL: http://www.cs.brown.edu/
Email: ftld,rlgg@cs.brown.edu  
Title: Model Minimization in Markov Decision Processes  
Author: Thomas Dean and Robert Givan 
Address: Box 1910, Providence, RI 02912  
Affiliation: Department of Computer Science Brown University  
Abstract: We use the notion of stochastic bisimulation homogeneity to analyze planning problems represented as Markov decision processes (MDPs). Informally, a partition of the state space for an MDP is said to be homogeneous if for each action, states in the same block have the same probability of being carried to each other block. We provide an algorithm for finding the coarsest homogeneous refinement of any partition of the state space of an MDP. The resulting partition can be used to construct a reduced MDP which is minimal in a well defined sense and can be used to solve the original MDP. Our algorithm is an adaptation of known automata minimization algorithms, and is designed to operate naturally on factored or implicit representations in which the full state space is never explicitly enumerated. We show that simple variations on this algorithm are equivalent or closely similar to several different recently published algorithms for finding optimal solutions to (partially or fully observable) factored Markov decision processes, thereby providing alternative descriptions of the methods and results regarding those algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G.; Sutton, R. S.; and Watkins, C. J. C. H. </author> <year> 1990. </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M., and Moore, J., eds., </editor> <booktitle> Learning and Computational Neuro-science: Foundations of Adaptive Networks. </booktitle> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference-contexts: This basic operation is also fundamental in regression planning and explanation-based learning. Explanation-based learning (EBL) techniques use regression to manipulate sets instead of individual states. Reinforcement learning (RL) is an on-line method for solving MDPs <ref> (Barto, Sutton, & Watkins 1990) </ref>, essentially by incremental, on-line dynamic programming. Dietterich and Flann (1995) note that computing preimages is closely related to the iterative (dynamic programming) step in policy iteration and other standard algorithms for computing optimal policies.
Reference: <author> Bouajjani, A.; Fernandez, J.-C.; Halbwachs, N.; Ray-mond, P.; and Ratel, C. </author> <year> 1992. </year> <title> Minimal state graph generation. </title> <booktitle> Science of Computer Programming 18 </booktitle> <pages> 247-269. </pages>
Reference: <author> Boutilier, C., and Dearden, R. </author> <year> 1994. </year> <title> Using abstractions for decision theoretic planning with time constraints. </title> <booktitle> In Proceedings AAAI-94, </booktitle> <pages> 1016-1022. </pages> <publisher> AAAI. </publisher>
Reference-contexts: Space limitations preclude detailed descriptions of the algorithms and explication of the background necessary to formalize our arguments; hence, the arguments provided in this paper are only sketches of the formal arguments provided in the longer version of this paper. State-Space Abstraction State-space abstraction <ref> (Boutilier & Dearden 1994) </ref> is a means of solving a factored MDP by generating an equivalent reduced MDP by determining with a superficial analysis which fluents' values are necessarily irrelevant to the solution. <p> A particular explicit MDP may have many different factored representations, and state space abstraction performs well only when the representation chosen represents the independence properties of the fluents well, so that the superficial analysis can easily detect which fluents are relevant. The presentation in <ref> (Boutilier & Dearden 1994) </ref> relies on a slightly more expressive factored representation than that presented above to allow the expression of a richer class of independence properties| each action is described by multiple but consistent aspects which apply simultaneously; each aspect is represented just as an action above.
Reference: <author> Boutilier, C., and Poole, D. </author> <year> 1996. </year> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <booktitle> In Proceedings AAAI-96, </booktitle> <pages> 1169-1175. </pages> <publisher> AAAI. </publisher>
Reference-contexts: We suspect that some existing POMDP algorithms can be partially understood in such terms. In particular, we conjecture that the factored POMDP algorithm described in <ref> (Boutilier & Poole 1996) </ref> is asymptotically equivalent to minimizing the underlying MDP and then using Monahan's (1982) POMDP algorithm. Conclusion This paper is primarily concerned with introducing the method of model minimization for MDPs and presenting it as a way of analyzing and understanding existing algorithms.
Reference: <author> Boutilier, C.; Dean, T.; and Hanks, S. </author> <year> 1995. </year> <title> Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> In Proceedings of the Third European Workshop on Planning. </booktitle>
Reference-contexts: In this paper, we refer to the state-transition graph as a model for the underlying dynamics of a planning problem <ref> (Boutilier, Dean, & Hanks 1995) </ref>. A policy is a mapping from states to actions, : Q ! A. <p> t ) where Parents (X) denotes the parents of X in the 2TBN and each of the conditional probability distributions Pr (X i;t+1 jParents (X i;t+1 ); U t ) can be represented as a conditional probability table or as a decision tree which we do in this paper following <ref> (Boutilier, Dearden, & Goldszmidt 1995) </ref>. We enhance the 2TBN representation to include actions and reward functions; the resulting graph is called an influence diagram (Howard & Matheson 1984). three state variables, X = fA; B; Cg, and describes the transition probabilities and rewards for one action.
Reference: <author> Boutilier, C.; Dearden, R.; and Goldszmidt, M. </author> <year> 1995. </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings IJCAI 14, </booktitle> <pages> 1104-1111. </pages> <month> IJCAII. </month>
Reference-contexts: In this paper, we refer to the state-transition graph as a model for the underlying dynamics of a planning problem <ref> (Boutilier, Dean, & Hanks 1995) </ref>. A policy is a mapping from states to actions, : Q ! A. <p> t ) where Parents (X) denotes the parents of X in the 2TBN and each of the conditional probability distributions Pr (X i;t+1 jParents (X i;t+1 ); U t ) can be represented as a conditional probability table or as a decision tree which we do in this paper following <ref> (Boutilier, Dearden, & Goldszmidt 1995) </ref>. We enhance the 2TBN representation to include actions and reward functions; the resulting graph is called an influence diagram (Howard & Matheson 1984). three state variables, X = fA; B; Cg, and describes the transition probabilities and rewards for one action.
Reference: <author> Dean, T., and Kanazawa, K. </author> <year> 1989. </year> <title> A model for reasoning about persistence and causation. </title> <booktitle> Computational Intelligence 5(3) </booktitle> <pages> 142-150. </pages>
Reference-contexts: The state at time t is now represented as a vector X t = hX 1;t ; : : : ; X m;t i where X i;t denotes the ith state variable at time t. A two-stage temporal Bayesian network (2TBN) <ref> (Dean & Kanazawa 1989) </ref> is a directed acyclic graph consisting of two sets of variables fX i;t g and fX i;t+1 g in which directed arcs indicating dependence are allowed from the variables in the first set to variables in the second set and between variables in the second set.
Reference: <author> Dean, T.; Kaelbling, L.; Kirman, J.; and Nicholson, A. </author> <year> 1995. </year> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence 76(1-2):35-74. </journal>
Reference-contexts: In this paper, we refer to the state-transition graph as a model for the underlying dynamics of a planning problem <ref> (Boutilier, Dean, & Hanks 1995) </ref>. A policy is a mapping from states to actions, : Q ! A. <p> The methods of this paper extend directly to account for reachability from an initial state or set of initial states. We are also working on algorithms that use minimization and reachability to extend decomposition and envelope-based techniques such as <ref> (Dean et al. 1995) </ref> to handle factored representations.
Reference: <author> Dietterich, T. G., and Flann, N. S. </author> <year> 1995. </year> <title> Explanation-based learning and reinforcement learning: A unified view. </title> <booktitle> In Proceedings Twelfth International Conference on Machine Learning, </booktitle> <pages> 176-184. </pages>
Reference: <author> Hartmanis, J., and Stearns, R. E. </author> <year> 1966. </year> <title> Algebraic Structure Theory of Sequential Machines. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Howard, R. A., and Matheson, J. E. </author> <year> 1984. </year> <title> Influence diagrams. </title> <editor> In Howard, R. A., and Matheson, J. E., eds., </editor> <booktitle> The Principles and Applications of Decision Analysis. </booktitle> <address> Menlo Park, CA 94025: </address> <institution> Strategic Decisions Group. </institution>
Reference-contexts: We enhance the 2TBN representation to include actions and reward functions; the resulting graph is called an influence diagram <ref> (Howard & Matheson 1984) </ref>. three state variables, X = fA; B; Cg, and describes the transition probabilities and rewards for one action.
Reference: <author> Kemeny, J. G., and Snell, J. L. </author> <year> 1960. </year> <title> Finite Markov Chains. </title> <address> New York: D. </address> <publisher> Van Nostrand. </publisher>
Reference-contexts: In traditional AI planning, we might use an initial partition 1 Stochastic bisimulation homogeneity is closely related to the substitution property for finite automata developed by Hartmanis and Stearns (1966) and the notion of lumpa-bility for Markov chains <ref> (Kemeny & Snell 1960) </ref>. consisting of two blocks of states: those that satisfy the goal and those that do not. In solving an MDP, we distinguish states that differ on the basis of reward.
Reference: <author> Kushmerick, N.; Hanks, S.; and Weld, D. </author> <year> 1995. </year> <title> An algorithm for probabilistic planning. </title> <journal> Artificial Intelligence 76(1-2). </journal>
Reference-contexts: Factored Representations In the remainder of this paper, we make use of Bayesian networks (Pearl 1988) to encode implicit (or factored) representations; however, our methods apply to other factored representations such as probabilistic STRIPS operators <ref> (Kushmerick, Hanks, & Weld 1995) </ref>. Let X = fX 1 ; : : : ; X m g represent the set of state variables. We assume the variables are boolean, and refer to them also as fluents.
Reference: <author> Lee, D., and Yannakakis, M. </author> <year> 1992. </year> <title> Online minimization of transition systems. </title> <booktitle> In Proceedings of 24th Annual ACM Symposium on the Theory of Computing. </booktitle>
Reference: <author> Monahan, G. E. </author> <year> 1982. </year> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <booktitle> Management Science 28(1) </booktitle> <pages> 1-16. </pages>
Reference: <author> Moore, A. W. </author> <year> 1993. </year> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <editor> In Hanson, S. J.; Cowan, J. D.; and Giles, C. L., eds., </editor> <booktitle> Advances in Neural Information Processing 5. </booktitle> <address> San Francisco, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Francisco, Cal-ifornia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the next section, we introduce one particular method of implicit representation which is well suited to MDPs and then use this as a basis for our discussion. Factored Representations In the remainder of this paper, we make use of Bayesian networks <ref> (Pearl 1988) </ref> to encode implicit (or factored) representations; however, our methods apply to other factored representations such as probabilistic STRIPS operators (Kushmerick, Hanks, & Weld 1995). Let X = fX 1 ; : : : ; X m g represent the set of state variables.
Reference: <author> Puterman, M. L. </author> <year> 1994. </year> <title> Markov Decision Processes. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: to their expected value given that you start in that state and act according the given policy: V (p) = R ((p); p) + fl q2Q where fl is the discount rate, 0 fl &lt; 1, and we assume for simplicity that the objective function is expected discounted cumulative reward <ref> (Puterman 1994) </ref>. Let P = fB 1 ; : : :; B n g be a partition of Q. <p> Structured Policy Iteration Policy iteration is a well-known technique for finding an optimal policy for an explicitly represented MDP by evaluating the value at each state of a fixed policy and using those values to compute a locally better policy|iterating this process converges to an optimum policy <ref> (Puterman 1994) </ref>. <p> repeatedly computing the value of each state using the just computed values for neighboring states|iterating this process converges in the infinite limit to the true values, and a stopping criterion can be designed to indicate when the estimated values are good enough to proceed with another step of policy iteration <ref> (Puterman 1994) </ref>. Boutilier et al. (1995) describe variants of policy iteration and successive approximation designed to work on factored MDP representations, called structured policy iteration (SPI) and structured successive approximation (SSA), respectively. These algorithms can both be understood as variants of minimization using a particular non-optimal but adequate split operation.
Reference: <author> Tsitsiklis, J. N., and Van Roy, B. </author> <year> 1996. </year> <title> Feature-based methods for large scale dynamic programming. </title> <booktitle> Machine Learning 22 </booktitle> <pages> 59-94. </pages>
References-found: 19

