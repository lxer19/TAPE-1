URL: http://www.stat.cmu.edu/www/cmu-stats/tr/tr676/tr676.ps
Refering-URL: 
Root-URL: 
Title: Consistency of Posterior Distributions for Neural Networks  
Author: Herbert Lee 
Keyword: Bayesian statistics, Asymptotic consistency, Posterior approximation, Nonpara metric regression, Sieve Asymptotics, Hellinger distance, Bracketing entropy  
Note: The author is indebted to Larry Wasserman for all of his help in this endeavor.  
Date: May 21, 1998  
Abstract: In this paper we show that the posterior distribution for feedforward neural networks is asymptotically consistent. This paper extends earlier results on universal approximation properties of neural networks to the Bayesian setting. The proof of consistency embeds the problem in a density estimation problem, then uses bounds on the bracketing entropy to show that the posterior is consistent over Hellinger neighborhoods. It then relates this result back to the regression setting. We show consistency in both the setting of the number of hidden nodes growing with the sample size, and in the case where the number of hidden nodes is treated as a parameter. Thus we provide a theoretical justification for using neural networks for nonparametric regression in a Bayesian framework. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barron, A., Schervish, M. J., & Wasserman, L. </author> <year> (1998). </year> <title> The consistency of posterior distributions in nonparametric problems. </title> <journal> The Annals of Statistics. </journal> <note> To appear. </note>
Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference: <author> Buntine, W. L. & Weigend, A. S. </author> <year> (1991). </year> <title> Bayesian back-propogation. </title> <journal> Complex Systems, </journal> <volume> 5, </volume> <pages> 603-643. </pages>
Reference: <author> Diaconis, P. & Freedman, D. A. </author> <year> (1986). </year> <title> On the consistency of bayes estimates. </title> <journal> Annals of Statistics, </journal> <volume> 14, </volume> <pages> 1-26. </pages>
Reference-contexts: Then define a size * weak neighborhood of f o to be ff jD W (f; f o ) &lt; *g. It has been shown <ref> (Diaconis & Freedman, 1986) </ref> that having a prior which puts positive mass on weak neighborhoods of f o does not guarantee that the posterior mass of every weak neighborhood of f o will tend to 1.
Reference: <author> Doob, J. L. </author> <year> (1949). </year> <title> Application of the theory of martingales. </title> <booktitle> In Le Calcul des Probabilites et ses Applications, </booktitle> <pages> (pp. 23-27). </pages> <institution> Colloque Internationaux du Centre National de la Recherche Scientifique, Paris. </institution>
Reference: <author> Funahashi, K. </author> <year> (1989). </year> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 183-192. </pages>
Reference-contexts: This theorem shows that theorem 2 is relevant, in that some standard choices of priors and reasonable conditions on the true regression function will satisfy the conditions of the theorem. The proof of this theorem requires some results about the approximation abilities of neural networks. Theorem 10 <ref> (Funahashi, 1989) </ref> Let x 2 X be the independent variables on a compact space X .
Reference: <author> Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. </author> <year> (1995). </year> <title> Bayesian Data Analysis. </title> <publisher> London: Chapman & Hall. </publisher>
Reference: <author> Gilks, W. R., Richardson, S., & Spiegelhalter, D. J. </author> <year> (1996). </year> <title> Markov Chain Monte Carlo in Practice. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> Haussler, D. & Opper, M. </author> <year> (1997). </year> <booktitle> Lecture notes in computer science: Studies in logic and computer science. </booktitle> <editor> In J. Mycielski, G. Rozenberg, & A. Salomaa (eds.), </editor> <title> Metric Entropy and Minimax Risk in Classification, </title> <journal> vol. </journal> <volume> 1261, </volume> <pages> (pp. 212-235). </pages> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Hornik, K., Stinchcombe, M., & White, H. </author> <year> (1989). </year> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 359-366. </pages>
Reference-contexts: If the true regression function, g o (x), is continuous, then for any given * &gt; 0, there exists a neural network regression function, g (x), such that sup jg (x) g o (x)j &lt; * : (21) Theorem 11 <ref> (Hornik et al., 1989) </ref> If is a probability measure on [0; 1] p , then for every * &gt; 0 and every g o such that R o (x)d (x) &lt; 1, there exists a neural network g such that jjg g o jj 2 = (g (x) g o (x))
Reference: <author> Krogh, A. & hertz, J. A. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In J. E. Moody, S. J. Hanson, & R. P. Lippman (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4 . Cambridge, </booktitle> <address> MA: </address> <publisher> MIT. </publisher>
Reference: <author> MacKay, D. J. C. </author> <year> (1992). </year> <title> Bayesian Methods for Adaptive Methods. </title> <type> Ph.D. thesis, </type> <institution> California Institute of Technology. </institution>
Reference: <author> Muller, P. & Insua, D. R. </author> <year> (1995). </year> <title> Issues in Bayesian Analysis of Neural Network Models. </title> <type> Tech. Rep. 95-31, </type> <institution> Duke University. </institution>
Reference: <author> Neal, R. M. </author> <year> (1996). </year> <title> Bayesian Learning for Neural Networks. </title> <address> New York: </address> <publisher> Springer. </publisher>
Reference: <author> Pollard, D. </author> <year> (1991). </year> <title> Bracketing methods in statistics and econometrics. </title> <editor> In W. A. Barnett, J. Pow-ell, & G. E. Tauchen (eds.), </editor> <title> Nonparametric and Semiparametric Methods in Econometrics and Statistics: </title> <booktitle> Proceedings of the Fifth International Symposium in Econometric Theory and Econometrics, </booktitle> <pages> (pp. 337-355). </pages> <address> Cambridge, UK: </address> <publisher> Cambridge University Press. </publisher> <editor> van der Vaart, A. W. & Wellner, J. A. </editor> <year> (1996). </year> <title> Weak Convergence and Empirical Processes. New York: Springer. Consistency of Neural Nets 22 Wasserman, </title> <editor> L. </editor> <year> (1998). </year> <title> Asymptotic properties of nonparametric bayesian procedures. </title> <editor> In D. Dey, P. Mueller, & D. Sinha (eds.), </editor> <title> Practical Nonparametric and Semiparametric Bayesian Statistics. </title> <note> Springer Lecture Notes. To appear. </note>
Reference-contexts: Finally, the bracketing entropy, denoted H [ ] (), is the natural logarithm of the bracketing number. <ref> (Pollard, 1991) </ref> To find the bracketing entropy for a neural network, one can compute the covering number and use it as an upper bound. For now, consider the number of hidden nodes, k, to be fixed.
Reference: <author> Wong, W. H. & Shen, X. </author> <year> (1995). </year> <title> Probability inequalitites for likelihood ratios and convergence rates of sieve mles. </title> <journal> Annals of Statistics, </journal> <volume> 23, </volume> <pages> 339-362. </pages>
Reference-contexts: Then for any fixed constants c, * &gt; 0, and for all sufficiently large n, Z * 0 H [ ] (u) c n* 2 . We use this lemma for showing the conditions of the following theorem: Consistency of Neural Nets 7 Theorem 5 <ref> (Wong & Shen, 1995) </ref> Let R n (f ) = Q n f (x i ;y i ) f o (x i ;y i ) .
References-found: 16

