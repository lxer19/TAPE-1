URL: ftp://ftp.cs.umass.edu/pub/osl/papers/hcw96.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Title: SCORE: A COMPILER REPRESENTATION FOR HETEROGENEOUS SYSTEMS  
Author: Glen E. Weaver, Kathryn S. McKinley, Charles C. Weems -weaver, mckinley, 
Keyword: Compilers, Heterogeneous Processing, Intermediate Representation, Heterogeneous Systems, Compiler Architecture  
Note: (fax)  
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Email: weems-@cs.umass.edu  
Phone: (413) 545-1249  
Abstract: Heterogeneous parallel systems incorporate different models of parallelism within a single machine or across machines and are well suited for many diverse applications [1, 2, 3]. These systems offer a wealth of underutilized resources for achieving high performance, but are more difficult to program than homogeneous systems. Compilers can and should assist in handling this complexity. Previous research has not considered the impact of architectural heterogeneity on the compiler structure [4]. A compiler for heterogeneous systems needs to order transformations based on component architectures, represent diverse architectural features, allow extension to cover new machines, and reuse its transformations and intermediate representation (IR) to target different architectures. A compiler's IR plays a central role in supporting these capabilities. This paper discusses each of these implications in detail and presents an intermediate representation, Score, that meets the needs of heterogeneity. In addition to incorporating nodes to represent a variety of architectural features, Score supports a new bi-level representation of important language constructs and employs a novel approach to defining the interface between transformations and IR nodes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Klietz, A. Malevsky, and K. Chin-Purcell. </author> <title> A case study in metacomputing: Distributed simulations of mixing in turbulent convection. </title> <booktitle> In Workshop on Heterogeneous Processing, </booktitle> <pages> pages 101-106, </pages> <month> April </month> <year> 1993. </year>
Reference: [2] <author> C. Weems, E. Riseman, A. Hanson, and A. Rosenfeld. </author> <title> The DARPA image understanding benchmark for parallel processors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 </volume> <pages> 1-24, </pages> <year> 1991. </year>
Reference: [3] <author> H. Nicholas, G. Giras, V. Hartonas-Garmhausen, M. Kopko, C. Maher, and A. Ropelewski. </author> <title> Distributing the comparison of DNA and protein sequences across heterogeneous supercomputers. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 139-146, </pages> <year> 1991. </year>
Reference: [4] <author> K. S. McKinley, S. Singhai, G. Weaver, and C. Weems. </author> <title> Compiler architectures for heterogeneous systems. </title> <booktitle> In Proceedings of the Eighth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Instead of relying solely on programmers, compilers should automatically handle some of the complexity of tailoring applications for heterogeneous systems. With certain modifications to a compiler's software architecture, it can use transformations 1 to adjust a program to execute efficiently on a heterogeneous machine <ref> [4] </ref>. 1 For brevity, we use the more generic term transformations to refer to traditional scalar optimizations as well as loop and parallelism transformations. 1 Appears in the 1996 Heterogeneous Computing Workshop 2 For a compiler to effectively compile to heterogeneous systems requires a more flexible software architecture than found in <p> use the more generic term transformations to refer to traditional scalar optimizations as well as loop and parallelism transformations. 1 Appears in the 1996 Heterogeneous Computing Workshop 2 For a compiler to effectively compile to heterogeneous systems requires a more flexible software architecture than found in compilers for homogeneous systems <ref> [4] </ref>.
Reference: [5] <author> L. Smarr and C. E. Catlett. </author> <title> Metacomputing. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 45-52, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: These sections must either be parallelized inefficiently or executed sequentially. Heterogeneous sys tems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing <ref> [5, 6, 7, 8, 9] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [8]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [6] <author> L. Turcotte. </author> <title> A survey of software environments for exploiting networked computing resources. </title> <type> Technical Report MSSU-EIRS-ERC-93-2, </type> <institution> NSF Engineering Research Center, Missis-sippi State University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: These sections must either be parallelized inefficiently or executed sequentially. Heterogeneous sys tems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing <ref> [5, 6, 7, 8, 9] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [8]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [7] <author> L. Turcotte. </author> <title> Cluster computing. </title> <editor> In Albert Y. Zomaya, editor, </editor> <booktitle> Parallel and Distributed Computing Handbook, chapter 26. </booktitle> <publisher> McGraw-Hill, </publisher> <month> October </month> <year> 1995. </year>
Reference-contexts: These sections must either be parallelized inefficiently or executed sequentially. Heterogeneous sys tems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing <ref> [5, 6, 7, 8, 9] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [8]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [8] <author> A. Khokhar, V. Prasanna, M. Shaaban, and C. Wang. </author> <title> Heterogeneous computing: Challenges and opportunities. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 18-27, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: These sections must either be parallelized inefficiently or executed sequentially. Heterogeneous sys tems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing <ref> [5, 6, 7, 8, 9] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [8]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask. <p> Heterogeneous sys tems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing [5, 6, 7, 8, 9] is the well-orchestrated use of heterogeneous hardware to execute a single application <ref> [8] </ref>. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask.
Reference: [9] <author> A. Ghafoor and J. Yang. </author> <title> A distributed heterogeneous supercomputing management system. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 78-86, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: These sections must either be parallelized inefficiently or executed sequentially. Heterogeneous sys tems can deliver consistent high performance by incorporating multiple models of parallelism within one machine or across machines, creating a virtual machine. Heterogeneous processing <ref> [5, 6, 7, 8, 9] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [8]. When an application encompasses subtasks that employ different models of parallelism, the application may benefit from using disparate hardware architectures that match the inherent parallelism of each subtask. <p> For example, a compiler needs IR nodes that represent message operations to compile for message-passing machines; the compiler also needs IR nodes for matrix operations to compile for mesh connected SIMD machines. 2.3 Extensible for New Architectural Features Researchers often view heterogeneous processing as an approach to exploiting available resources <ref> [9] </ref>. This view implies that a heterogeneous system will change as new machines become available, and consequently a compiler frequently needs to target new architectures. Similarly, as new architectural features are developed, language extensions will be developed for these features.
Reference: [10] <author> C. Click and M. Paleczny. </author> <title> A simple graph-based intermediate representation. </title> <booktitle> In ACM SIGPLAN Workshopon Intermediate Representations, </booktitle> <address> San Francisco, CA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: Score is based on an intermediate representation by Click <ref> [10, 11] </ref> (see Section 4). As with Click's representation, Score's node set is based on ILOC93 [12] which models operations found in a typical RISC processor. Since both Click's representation and ILOC93 are designed for uniprocessors, Score includes additional nodes to represent operations on parallel architectures such as message passing. <p> We represent each equivalence class by a store value. A store value can also be thought of as a piece of memory. Optimism in the IR Graph: Score will also improve on the PDG representation by removing as many control dependences as possible <ref> [10, 16] </ref>. The idea is that only instructions that affect the program's output need to be executed under their control dependences. <p> In a CFG, edges represent control flow decisions (e.g., conditionals, loops, and jumps), and basic blocks contain sequentially executed code. Tuples and CFG are effective for low-level transformations and are therefore frequently used <ref> [27, 12, 10, 28] </ref>. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations [29, 30, 31, 32, 33, 34, 35, 36]. <p> Thus, at some point high-SUIF must be lowered in order for these constructs to be optimized. Click's IR: Click describes an IR with simple nodes appropriate for representing instructions in a uniprocessor RISC machine (e.g., load, store, and conditional branch) <ref> [10, 11] </ref>. His representation uses SSA form on top of a CFG, but each CFG block is explicitly represented by an IR node, called a region node. Operation nodes in a basic block point to the region node representing that basic block.
Reference: [11] <author> C. Click. </author> <title> Combining Analyses, Combining Optimizations. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1995. </year>
Reference-contexts: Score is based on an intermediate representation by Click <ref> [10, 11] </ref> (see Section 4). As with Click's representation, Score's node set is based on ILOC93 [12] which models operations found in a typical RISC processor. Since both Click's representation and ILOC93 are designed for uniprocessors, Score includes additional nodes to represent operations on parallel architectures such as message passing. <p> Thus, at some point high-SUIF must be lowered in order for these constructs to be optimized. Click's IR: Click describes an IR with simple nodes appropriate for representing instructions in a uniprocessor RISC machine (e.g., load, store, and conditional branch) <ref> [10, 11] </ref>. His representation uses SSA form on top of a CFG, but each CFG block is explicitly represented by an IR node, called a region node. Operation nodes in a basic block point to the region node representing that basic block.
Reference: [12] <author> P. Briggs and T. Harvey. </author> <title> ILOC '93. </title> <type> Technical Report CRPC-TR93323, </type> <institution> Rice University, </institution> <year> 1993. </year>
Reference-contexts: Score is based on an intermediate representation by Click [10, 11] (see Section 4). As with Click's representation, Score's node set is based on ILOC93 <ref> [12] </ref> which models operations found in a typical RISC processor. Since both Click's representation and ILOC93 are designed for uniprocessors, Score includes additional nodes to represent operations on parallel architectures such as message passing. IR Graph: Score is a variant of the program dependence graph (PDG) [13]. <p> In a CFG, edges represent control flow decisions (e.g., conditionals, loops, and jumps), and basic blocks contain sequentially executed code. Tuples and CFG are effective for low-level transformations and are therefore frequently used <ref> [27, 12, 10, 28] </ref>. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations [29, 30, 31, 32, 33, 34, 35, 36]. <p> Click's IR was not designed for heterogeneity, but it's optimism provides new opportunities for reorganizing instructions. Hence, Score is based on Click's IR. Score borrows Click's optimistic approach and uses the same underlying node set for sequential instructions <ref> [12] </ref>. However, Score has some fundamental differences from Click's representation. Besides its support for heterogeneity, Score's graph structure is PDG-based rather than CFG-based, and Score's node set includes parallel operations.
Reference: [13] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Since both Click's representation and ILOC93 are designed for uniprocessors, Score includes additional nodes to represent operations on parallel architectures such as message passing. IR Graph: Score is a variant of the program dependence graph (PDG) <ref> [13] </ref>. Edges represent data and control dependences. The use of these dependences exposes fine-grain parallelism because dependences impose minimal Appears in the 1996 Heterogeneous Computing Workshop 4 constraints on node order. <p> The DDG consists of all the operator nodes and those control nodes which take in data values; if nodes are the only control nodes which input a data value. Region nodes resemble basic blocks, but are not identical. Rather, region nodes collect together operator nodes with identical control conditions <ref> [13] </ref>. The true edge exiting the if control node represents the action taken if the conditional is true. No false edge is shown because the example does not have code after the if-then statement. <p> ASTs represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. Hence, edges in an AST represent syntactic relationships from the source language. Program dependence graphs (PDG) <ref> [13] </ref> connect nodes together with control dependence edges and data dependence edges. Con Appears in the 1996 Heterogeneous Computing Workshop 11 trol dependence edges reflect less of a source program's syntactic structure than the control flow edges in CFGs or the syntax-based edges in ASTs.
Reference: [14] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Data Values: Score also uses Static Single Assignment (SSA) form to increase opportunities for optimization by removing extraneous data dependences <ref> [14] </ref>. In SSA form, a variable use has exactly one definition; every place a scalar variable is defined or two control paths merge, a new variable name is created. Conceptually, pseudo-assignments, called -nodes, are inserted when two control paths merge to combine the values from incoming control paths. <p> In addition, converting to SSA form creates an extraordinary number of new variable names; many of which can and should be removed (Cytron et al. <ref> [14] </ref> suggest using a graph coloring algorithm for removal). Aliases play an important role in the construction of SSA form. When several memory accesses are potentially aliased, they must be treated as identical. Hence, an update to any one of them is the same as an update to them all. <p> Control dependence edges capture precisely the control information needed for high-level transformations [28]. Any of these representations can be put into static single assignment (SSA) form <ref> [14] </ref>. In SSA form, no variable is assigned a value in more than one location. SSA form eliminates anti-dependences and separates a variable's live ranges. Uniform Internal Representation: Ayguade et al. present an IR designed to support both high- and low-level transformations [37].
Reference: [15] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <address> Reading, MA, 1 edition, </address> <year> 1986. </year>
Reference-contexts: Treating them as identical ensures proper program behavior when variables are aliased but diminishes the precision of the SSA graph when the alias relationship is unwarranted. Alias analysis tries to disprove that variables are aliases and thereby improve the precision of the representation <ref> [15] </ref>. Alias analysis can be thought of separating memory accesses into equivalence classes. We represent each equivalence class by a store value. A store value can also be thought of as a piece of memory.
Reference: [16] <author> D. Weise, R. Crew, M. Ernst, and B. Steensgaard. </author> <title> Value dependence graphs: Representation without taxation. </title> <booktitle> In Proceedings of the Twenty-first Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 297-310, </pages> <address> Portland, OR, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: We represent each equivalence class by a store value. A store value can also be thought of as a piece of memory. Optimism in the IR Graph: Score will also improve on the PDG representation by removing as many control dependences as possible <ref> [10, 16] </ref>. The idea is that only instructions that affect the program's output need to be executed under their control dependences.
Reference: [17] <author> D. Bacon, S. Graham, and O. Sharp. </author> <title> Compiler transformations for high-performance computing. </title> <journal> ACM Computing Surveys, </journal> <volume> 28(4) </volume> <pages> 345-420, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: The effect of transformations already applied to the low-level representation would be lost, and so may need to be repeated on the new low-level nodes. Representing Loops: Loops are an important construct for compiler manipulation as evidenced by the plethora of loop transformations <ref> [17] </ref>. Due to the importance of loops, researchers have developed techniques for extracting loops and related information from low-level representations [18]. Using these techniques, our compiler will find loops through analysis rather than through interacting with the compiler's front end.
Reference: [18] <author> M. Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <address> San Francisco, CA, </address> <year> 1992. </year>
Reference-contexts: Representing Loops: Loops are an important construct for compiler manipulation as evidenced by the plethora of loop transformations [17]. Due to the importance of loops, researchers have developed techniques for extracting loops and related information from low-level representations <ref> [18] </ref>. Using these techniques, our compiler will find loops through analysis rather than through interacting with the compiler's front end.
Reference: [19] <author> A. Diwan, J.E.B. Moss, and K.S. McKinley. </author> <title> Simple and effective analysis of statically-typed object-oriented programs. </title> <note> Submitted for Publication, </note> <year> 1995. </year>
Reference-contexts: High-level information about call sites can be used to deduce constraints on method calls and possibly convert the expensive method call to a direct call <ref> [19, 20, 21, 22, 23] </ref>. If the method call remains, the overhead of table lookup provides opportunities for low-level optimizations. Score distinguishes between four different types of calls: direct calls, indirect calls for languages with function pointers, closure calls for languages with nested procedures, and method calls for object-oriented languages. <p> The closure call, which does not apply to C, is also not shown but would be similar to a direct call. Representing Field References: Analysis of object-oriented code benefits from a high-level representation for references to record fields <ref> [19] </ref>. Therefore, Score includes an annotation which holds the necessary high-level information. This annotation will be similar to the one for procedure calls. 3.2.2 Explicit Properties Permitting the addition of new IR nodes and encouraging the reuse of transformations requires two distinct but related forms of extension.
Reference: [20] <author> B. Calder and D. Grunwald. </author> <title> Reducing iindirect function call overhead in C++ programs. </title> <booktitle> In Proceedings of the Twenty-first Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 397-408, </pages> <address> Portland, OR, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: High-level information about call sites can be used to deduce constraints on method calls and possibly convert the expensive method call to a direct call <ref> [19, 20, 21, 22, 23] </ref>. If the method call remains, the overhead of table lookup provides opportunities for low-level optimizations. Score distinguishes between four different types of calls: direct calls, indirect calls for languages with function pointers, closure calls for languages with nested procedures, and method calls for object-oriented languages.
Reference: [21] <author> J. Dean, D. Grove, and C. Chambers. </author> <title> Optimization of object-oriented programs using static class hierarchy analysis. </title> <type> Technical Report 94-12-01, </type> <institution> University of Washington, </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: High-level information about call sites can be used to deduce constraints on method calls and possibly convert the expensive method call to a direct call <ref> [19, 20, 21, 22, 23] </ref>. If the method call remains, the overhead of table lookup provides opportunities for low-level optimizations. Score distinguishes between four different types of calls: direct calls, indirect calls for languages with function pointers, closure calls for languages with nested procedures, and method calls for object-oriented languages.
Reference: [22] <author> M. Fernandez. </author> <title> Simple and effective link-time optimization of modula-3 programs. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: High-level information about call sites can be used to deduce constraints on method calls and possibly convert the expensive method call to a direct call <ref> [19, 20, 21, 22, 23] </ref>. If the method call remains, the overhead of table lookup provides opportunities for low-level optimizations. Score distinguishes between four different types of calls: direct calls, indirect calls for languages with function pointers, closure calls for languages with nested procedures, and method calls for object-oriented languages.
Reference: [23] <author> J. Palsberg and M. Schwartzbach. </author> <title> Object-oriented. </title> <booktitle> In Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <address> Phoneix, AZ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: High-level information about call sites can be used to deduce constraints on method calls and possibly convert the expensive method call to a direct call <ref> [19, 20, 21, 22, 23] </ref>. If the method call remains, the overhead of table lookup provides opportunities for low-level optimizations. Score distinguishes between four different types of calls: direct calls, indirect calls for languages with function pointers, closure calls for languages with nested procedures, and method calls for object-oriented languages.
Reference: [24] <author> J. Burrill. </author> <title> Using the GNU C++ compiler to generate code for a massively parallel simd processor. </title> <type> Technical report, </type> <institution> Amerinex Artificial Intelligence, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: In previous work on computer vision applications, we obtained significant performance improvements by modifying a compiler so that it applies simple optimizations to our image plane data type <ref> [24] </ref>. In separate research, we are investigating an annotation tool for conveying such information to the compiler [25]. 4 RELATED WORK Though no other intermediate representation specifically addresses the needs of heterogeneity, many representations have been designed to improve portability or to expose more parallelism.
Reference: [25] <author> A. Dhagat. ANN: </author> <title> A software tool for annotatin IRIS-Ada graphs. </title> <type> Master's Project, </type> <month> June </month> <year> 1995. </year>
Reference-contexts: In previous work on computer vision applications, we obtained significant performance improvements by modifying a compiler so that it applies simple optimizations to our image plane data type [24]. In separate research, we are investigating an annotation tool for conveying such information to the compiler <ref> [25] </ref>. 4 RELATED WORK Though no other intermediate representation specifically addresses the needs of heterogeneity, many representations have been designed to improve portability or to expose more parallelism.
Reference: [26] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: To improve efficiency, 5 (b) should be strip mined to account for the hardware's vector length, as in 5 (c). useful for heterogeneity. Several well-known representation techniques occur repeatedly in the literature. Linear tuples and Control Flow Graphs (CFG) are popular low-level representations <ref> [26] </ref>. Linear tuples represent a program as a linear list of tuples (i.e., nodes) with dependence information implicit in the ordering of nodes. In a CFG, edges represent control flow decisions (e.g., conditionals, loops, and jumps), and basic blocks contain sequentially executed code. <p> In a CFG, edges represent control flow decisions (e.g., conditionals, loops, and jumps), and basic blocks contain sequentially executed code. Tuples and CFG are effective for low-level transformations and are therefore frequently used [27, 12, 10, 28]. Abstract syntax trees (ASTs) <ref> [26] </ref> are popular because of their convenience for high-level transformations [29, 30, 31, 32, 33, 34, 35, 36]. ASTs represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program.
Reference: [27] <author> R. Stallman. </author> <title> Using and Porting GNU CC. Free Software Foundation, </title> <address> Cambridge, MA, 2.6 edition, </address> <year> 1994. </year> <note> Appears in the 1996 Heterogeneous Computing Workshop 14 </note>
Reference-contexts: In a CFG, edges represent control flow decisions (e.g., conditionals, loops, and jumps), and basic blocks contain sequentially executed code. Tuples and CFG are effective for low-level transformations and are therefore frequently used <ref> [27, 12, 10, 28] </ref>. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations [29, 30, 31, 32, 33, 34, 35, 36].
Reference: [28] <author> D. Berson, R. Gupta, and M. L. Soffa. GURRR: </author> <title> A global unified resource requirements representations. </title> <booktitle> In Workshop on Intermediate Representations, </booktitle> <address> San Francisco, CA, </address> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: In a CFG, edges represent control flow decisions (e.g., conditionals, loops, and jumps), and basic blocks contain sequentially executed code. Tuples and CFG are effective for low-level transformations and are therefore frequently used <ref> [27, 12, 10, 28] </ref>. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations [29, 30, 31, 32, 33, 34, 35, 36]. <p> Con Appears in the 1996 Heterogeneous Computing Workshop 11 trol dependence edges reflect less of a source program's syntactic structure than the control flow edges in CFGs or the syntax-based edges in ASTs. Control dependence edges capture precisely the control information needed for high-level transformations <ref> [28] </ref>. Any of these representations can be put into static single assignment (SSA) form [14]. In SSA form, no variable is assigned a value in more than one location. SSA form eliminates anti-dependences and separates a variable's live ranges. <p> GURRR: Berson et al. use a PDG as the basis of an IR, Global Unified Resource Requirements Representation (GURRR), that supports reordering of (very) low-level optimizations as well as high level transformations <ref> [28] </ref>. GURRR grows out of earlier work in integrating register allocation with instruction scheduling but seeks to include other optimizations such as loop transforms. GURRR is a collection of resource usage information (e.g., registers and functional units) superimposed on an instruction-level PDG.
Reference: [29] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Tuples and CFG are effective for low-level transformations and are therefore frequently used [27, 12, 10, 28]. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations <ref> [29, 30, 31, 32, 33, 34, 35, 36] </ref>. ASTs represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. Hence, edges in an AST represent syntactic relationships from the source language.
Reference: [30] <author> K. Faigin, J. Hoeflinger, D. A. Padua, P. Petersen, and S. Weatherford. </author> <title> The Polaris internal representation. </title> <type> Technical Report CSRD-1317, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Tuples and CFG are effective for low-level transformations and are therefore frequently used [27, 12, 10, 28]. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations <ref> [29, 30, 31, 32, 33, 34, 35, 36] </ref>. ASTs represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. Hence, edges in an AST represent syntactic relationships from the source language.
Reference: [31] <author> F. Bodin, P. Beckman, D. Gannon, J. Gotwals, S. Narayana, S. Srinivas, and B. Winnicka. Sage++: </author> <title> An object-oriented toolkit and class library for building Fortran and C++ restructuring tools. </title> <booktitle> In Second Object-Oriented Numerics Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Tuples and CFG are effective for low-level transformations and are therefore frequently used [27, 12, 10, 28]. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations <ref> [29, 30, 31, 32, 33, 34, 35, 36] </ref>. ASTs represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. Hence, edges in an AST represent syntactic relationships from the source language.
Reference: [32] <author> K. Forester. </author> <title> Iris-ada reference manual. </title> <institution> Arcadia UM-90-07, University of Massachusetts, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Tuples and CFG are effective for low-level transformations and are therefore frequently used [27, 12, 10, 28]. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations <ref> [29, 30, 31, 32, 33, 34, 35, 36] </ref>. ASTs represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. Hence, edges in an AST represent syntactic relationships from the source language.
Reference: [33] <author> P. Tarr, A. Wise, and G. Weaver. IRIS-C++ 1.0 vs. IRIS-Ada 2.0: </author> <title> A comparison of the internal representations. </title> <type> Arcadia Technical Report UM-94-02, </type> <institution> University of Massachusetts, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Tuples and CFG are effective for low-level transformations and are therefore frequently used [27, 12, 10, 28]. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations <ref> [29, 30, 31, 32, 33, 34, 35, 36] </ref>. ASTs represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. Hence, edges in an AST represent syntactic relationships from the source language.
Reference: [34] <author> D. Rosenblum and A. Wolf. </author> <title> Representing semantically analyzed C++ code with reprise. </title> <booktitle> In Proceedings of the USENIX C++ Conference, </booktitle> <pages> pages 119-134, </pages> <address> DC, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Tuples and CFG are effective for low-level transformations and are therefore frequently used [27, 12, 10, 28]. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations <ref> [29, 30, 31, 32, 33, 34, 35, 36] </ref>. ASTs represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. Hence, edges in an AST represent syntactic relationships from the source language.
Reference: [35] <author> Stanford Compiler Group. </author> <title> The SUIF library. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: Tuples and CFG are effective for low-level transformations and are therefore frequently used [27, 12, 10, 28]. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations <ref> [29, 30, 31, 32, 33, 34, 35, 36] </ref>. ASTs represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. Hence, edges in an AST represent syntactic relationships from the source language. <p> Moreover, though they retain enough high-level information for source-to-source transformations, their textual representation leaves control and data dependences implicit. These dependences will have to be maintained separately which complicates the coding of transformations. SUIF: Stanford University Intermediate Format (SUIF) is the AST-based intermediated representation used in the SUIF compiler <ref> [38, 39, 35] </ref>. SUIF includes both high and low-level nodes to enable code generation as well as loop transformations.
Reference: [36] <author> S. Macrakis. </author> <title> The structure of ANDF: Principles and examples. </title> <type> Technical report, </type> <institution> Open Software Foundation Research Institute, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Tuples and CFG are effective for low-level transformations and are therefore frequently used [27, 12, 10, 28]. Abstract syntax trees (ASTs) [26] are popular because of their convenience for high-level transformations <ref> [29, 30, 31, 32, 33, 34, 35, 36] </ref>. ASTs represent a program by treating source constructs as operators and operands, and organizing these nodes into trees where the root is generally a procedure or an entire program. Hence, edges in an AST represent syntactic relationships from the source language.
Reference: [37] <author> E. Ayguade, C. Barrado, J. Labarta, D. Lopez, S. Moreno, D. Padua, and M. Valero. </author> <title> A uniform representation for high-level and instruction-level transformations. </title> <institution> European Center for Parallelism of Barcelona (CEPBA) UPC-CEPBA-95-01, Universitat Politecncia de Catalunya, </institution> <address> Barcelona, Spain, </address> <year> 1995. </year>
Reference-contexts: In SSA form, no variable is assigned a value in more than one location. SSA form eliminates anti-dependences and separates a variable's live ranges. Uniform Internal Representation: Ayguade et al. present an IR designed to support both high- and low-level transformations <ref> [37] </ref>. Their objective is to extend conventional, parallelizing, source-to-source translators so that they generate object code as well as source code without adding a second intermediate representation. To generate object code, these translators must support low-level transformations.
Reference: [38] <author> R. Wilson, R. French, C. Wilson, S. Amarasinghe, J. An-derson, S. Tjiang, S. Liao, C. Tseng, M. Hall, M. Lam, and J. Hennessy. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> SIGPLAN, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: Moreover, though they retain enough high-level information for source-to-source transformations, their textual representation leaves control and data dependences implicit. These dependences will have to be maintained separately which complicates the coding of transformations. SUIF: Stanford University Intermediate Format (SUIF) is the AST-based intermediated representation used in the SUIF compiler <ref> [38, 39, 35] </ref>. SUIF includes both high and low-level nodes to enable code generation as well as loop transformations.
Reference: [39] <author> S. Amarasinghe, J. Anderson, M. Lam, and A. Lim. </author> <title> An overview of a compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Moreover, though they retain enough high-level information for source-to-source transformations, their textual representation leaves control and data dependences implicit. These dependences will have to be maintained separately which complicates the coding of transformations. SUIF: Stanford University Intermediate Format (SUIF) is the AST-based intermediated representation used in the SUIF compiler <ref> [38, 39, 35] </ref>. SUIF includes both high and low-level nodes to enable code generation as well as loop transformations.

References-found: 39

