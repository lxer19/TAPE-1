URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3503.product.units.neural.nets.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: 
Title: Product Unit Learning constructive algorithm is then introduced which adds product units to a network
Author: Laurens R. Leerink a C. Lee Giles b;c Bill G. Horne b Marwan A. Jabri a 
Note: A  
Address: Sydney, Sydney, NSW 2006, Australia  4 Independence Way, Princeton, NJ 08540, USA  College Park, MD 20742, USA  
Affiliation: a SEDAL, Department of Electrical Engineering The University of  b NEC Research Institute,  c UMIACS, University of Maryland,  
Abstract: Product units provide a method of automatically learning the higher-order input combinations required for the efficient synthesis of Boolean logic functions by neural networks. Product units also have a higher information capacity than sigmoidal networks. However, this activation function has not received much attention in the literature. A possible reason for this is that one encounters some problems when using standard backpropagation to train networks containing these units. This report examines these problems, and evaluates the performance of three training algorithms on networks of this type. Empirical results indicate that the error surface of networks containing product units have more local minima than corresponding networks with summation units. For this reason, a combination of local and global training algorithms were found to provide the most reliable convergence. We then investigate how `hints' can be added to the training algorithm. By extracting a common frequency from the input weights, and training this frequency separately, we show that convergence can be accelerated. In order to compare their performance with other transfer functions, product units were implemented as candidate units in the Cascade Correlation (CC) [13] system. Using these candidate units resulted in smaller networks which trained faster than when the any of the standard (three sigmoidal types and one Gaussian) transfer functions were used. This superiority was confirmed when a pool of candidate units of four different nonlinear activation functions were used, which have to compete for addition to the network. Extensive simulations showed that for the problem of implementing random Boolean logic functions, product units are always chosen above any of the other transfer functions.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Abu-Mostafa, </author> <title> "Learning from hints in neural networks," </title> <journal> Journal of Complexity, </journal> <volume> vol. 6, </volume> <editor> p. </editor> <volume> 192, </volume> <year> 1990. </year>
Reference-contexts: Note that for all l rationally independent real numbers x i , the vectors of the form (sin (fl 1 x 1 ); : : : ; sin (fl l x l )), with the fl i 's real, form a dense subset of <ref> [1; 1] </ref> l , so all dichotomies on fx 1 ; : : : ; x l g can be implemented with (1; sin)-nets. where specified the VC dimension. <p> We hypothesize that adding product units to a network makes the error surface sufficiently mountainous so that a global search is required. 4 Training Product Units with `Hints' Abu-Mostafa <ref> [1] </ref> showed that using appropriate `hints' could decrease learning time in neural networks. Hints are information about the system or training data that is known to the observer, but that is embedded in a non-trivial way in the data. <p> During this comparison, all parameters were set to default values, i.e. the weights of the cosine units were random numbers initialized in the range of <ref> [1; +1] </ref>. As discussed earlier, this puts the cosine units at a slight disadvantage as their optimum range is [2; +2]. In terms of epochs required for convergence, Table 5.4.2 displays the results when CC was trained using three types of homogeneous candidate units.
Reference: [2] <author> K. Al-Mashouq and I. Reed, </author> <title> "Including hints in training neural nets," </title> <journal> Neural Computation, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 418-427, </pages> <year> 1991. </year>
Reference-contexts: As the argument of the cos in cos P ijx i =1 w i is already multiplied by and is symmetric, the weights were initialized randomly in the range <ref> [2; 2] </ref>. In fact, learning seems insensitive to the size of the weights, as long as they are large enough. 3.1.2 Local Minima The presence of local minima when training networks with (co)sine units has been reported earlier. <p> BP was not combined with either SA or RSA, the addition of local learning to these algorithms can clearly only worsen performance. For the parity problem, global search algorithms have the following advantages: * The search space is bounded (all weights are initialized in <ref> [2; +2] </ref>) and only weights in this region are generated. * The dimension of search space is low (maximum of 11 weights for the problems examined). * The fraction of the weight space which satisfies the parity problem relative to the total bounded weight space is high (all weights only have <p> Hints are information about the system or training data that is known to the observer, but that is embedded in a non-trivial way in the data. It has also been shown <ref> [2] </ref> that hints may be extracted from the training data, or by using previous experience learned from solving similar problems. 4.1 Product Unit Computation When the calculation of one product unit is examined, as shown before, the output of a product unit can be approximated by a cos unit if the <p> During this comparison, all parameters were set to default values, i.e. the weights of the cosine units were random numbers initialized in the range of [1; +1]. As discussed earlier, this puts the cosine units at a slight disadvantage as their optimum range is <ref> [2; +2] </ref>. In terms of epochs required for convergence, Table 5.4.2 displays the results when CC was trained using three types of homogeneous candidate units. <p> On the other hand, global search methods perform well on small problems, but are impractical for larger networks. Given a network containing product units, there are some atypical heuristics that can be used: (a) weights leading into the product unit have to be initialized in the range <ref> [2; 2] </ref> (b) reinitialization of the weights if convergence is not reached after a certain number of epochs. The representational power of product units have enabled us to solve standard problems with construction of significantly smaller networks than previously reported, using a very simple constructive method.
Reference: [3] <author> D. Amit, </author> <title> Modelling Brain Function. </title> <publisher> Cambridge: Cambridge University Press, </publisher> <year> 1989. </year>
Reference-contexts: Theoretical capacity analysis using methods from statistical mechanics <ref> [17, 3, 20, 12] </ref> provide some insight into this process.
Reference: [4] <editor> J. Anderson and E. Rosenfeld, eds., Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <address> Cambridge: </address> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference: [5] <author> T. Ash, </author> <title> "Dynamic node creation in backpropagation networks," </title> <journal> Connection Science, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 365-375, </pages> <year> 1989. </year>
Reference-contexts: This is because the addition of a new weight adds one degree of freedom, but limits the new solution to an affine subset of the existing weight space <ref> [5] </ref>. When a new hidden unit is added, it is possible to rotate the previous solution so that the local minima is reached. However, as shown in [5] this network will not be of minimal size. <p> of a new weight adds one degree of freedom, but limits the new solution to an affine subset of the existing weight space <ref> [5] </ref>. When a new hidden unit is added, it is possible to rotate the previous solution so that the local minima is reached. However, as shown in [5] this network will not be of minimal size. For this reason a simple constructive approach was implemented which retains the global (RSA) search for all weights during the whole training process. <p> Reinitialize weights and continue training with the BP-RSA combination. * This process is repeated until a maximum number of epochs is reached or the network has grown to the maximum specified size. The method of [8] and <ref> [5] </ref> was also evaluated, in which neurons with small weights were added to a network according to certain criteria. The method outlined above performed better, possibly because of the global search performed by the RSA step. 10 2 N random mappings.
Reference: [6] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth, </author> <title> "Learnability and the vapnik-chervonenkis dimension," </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> vol. 36, no. 4, </volume> <pages> pp. 929-965, </pages> <year> 1989. </year>
Reference-contexts: It is also worth mentioning that the VC dimension is directly related to the generalization ability of the network. The price that has to be paid for having an infinite VC dimension is that the network 3 cannot be guaranteed to generalize <ref> [6] </ref>. As most applications of neural networks require this ability, an infinite VC dimension is not always desired. 3 Learning with Product Units 3.1 Training Problems This section examines the problems that are inherent in training neural networks that contain product units.
Reference: [7] <author> M. Brady, </author> <title> "Guaranteed learning algorithm for network with units having periodic threshold output function," </title> <journal> Neural Computation, </journal> <volume> vol. 2, </volume> <pages> pp. 405-408, </pages> <year> 1990. </year> <month> 15 </month>
Reference-contexts: This fact, results in a finite VC dimension for product units. However, rational independence can be enforced by performing a nonlinear scaling operation on the results of the inner product before applying the cos function. This was done by Brady <ref> [7] </ref> who proposed a learning algorithm for a neural network consisting of only a single sin unit. It was shown that this network could implement any discrete function p : R n ! f0; 1g.
Reference: [8] <author> D. Chen, C. Giles, G. Sun, H. Chen, Y. Lee, and M. Goudreau, </author> <title> "Constructive learning of recurrent neural networks," </title> <booktitle> in 1993 IEEE International Conference on Neural Networks, vol. III, </booktitle> <address> (Piscataway, NJ), </address> <publisher> IEEE Press, </publisher> <year> 1993. </year>
Reference-contexts: Reinitialize weights and continue training with the BP-RSA combination. * This process is repeated until a maximum number of epochs is reached or the network has grown to the maximum specified size. The method of <ref> [8] </ref> and [5] was also evaluated, in which neurons with small weights were added to a network according to certain criteria. The method outlined above performed better, possibly because of the global search performed by the RSA step. 10 2 N random mappings.
Reference: [9] <author> T. </author> <title> Cover, "Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition," </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> vol. 14, </volume> <pages> pp. 326-334, </pages> <year> 1965. </year>
Reference-contexts: 1 Introduction It is well-known that supplementing the inputs to a neural network with higher-order combinations of the inputs both increases the capacity of the network <ref> [9] </ref> and the the ability to learn geometrically invariant properties [19]. <p> Durbin & Rumelhart [11] determined empirically that the information capacity of these units (as measured by their capacity for learning random Boolean patterns) is approximately 3N , compared to 2N for a single threshold logic function <ref> [9] </ref>. As before, N is the number of inputs to the network. The larger capacity is means that the same functions can be implemented by networks containing less units.
Reference: [10] <author> M. Dawson and D. Schopflocher, </author> <title> "Modifying the generalized delta rule to train networks of nonmonotonic processors for pattern classification," </title> <journal> Connection Science, </journal> <volume> vol. 4, </volume> <pages> pp. 19-31, </pages> <year> 1992. </year>
Reference-contexts: This allows fast convergence by the quickprop algorithm. In <ref> [10] </ref> it was shown that networks with Gaussian units train faster and require less units than networks with standard sigmoidal units. This is supported by our results as shown in Figure 4.
Reference: [11] <author> R. Durbin and D. Rumelhart, </author> <title> "Product units: A computationally powerful and biologically plausible extension to backpropagation networks," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 133-142, </pages> <year> 1989. </year>
Reference-contexts: The `pi-sigma' networks (PSNs) of Ghosh & Shin [18] attempt to make use of this fact. A PSN with N summation units and one `pi' output neuron provides a K-th order approximation of a continuous function. However, the product units introduced by Durbin & Rumelhart <ref> [11] </ref> have the advantage that, given an appropriate training algorithm, the units can automatically learn the higher order terms that are required to implement a specific logical function. In these networks the hidden layer units compute the weighted product (instead of the weighted sum) of the inputs. <p> That is, every unit computes N Y x i instead of N X x i w i : (4) An additional advantage of product units is the increased information capacity of these units compared to standard summation networks. Durbin & Rumelhart <ref> [11] </ref> determined empirically that the information capacity of these units (as measured by their capacity for learning random Boolean patterns) is approximately 3N , compared to 2N for a single threshold logic function [9]. As before, N is the number of inputs to the network. <p> In this case a smaller network with a slightly more complex transfer function is preferable to a larger network with a standard (sigmoidal) transfer function. When product units are used to process Boolean inputs, best performance is obtained <ref> [11] </ref> by using values of +1 and 1. <p> If we then attempt to train a network with 6 inputs, 1 `hidden' product unit and a standard summing output unit network on the parity-6 problem shown in <ref> [11] </ref>, it is found that the standard backpropagation algorithm simply does not work. <p> From the architecture and as illustrated by <ref> [11] </ref>, this problem can be solved by one product unit. The question is whether the training algorithms can find a solution. The architecture simulated has N inputs, 1 product unit in the hidden layer and one summation output unit. <p> is a clear indication that BP alone cannot be used as a training algorithm, and that the addition of a global search procedure is essential for training. 3.3.2 All logical functions of 3 inputs As part of this investigation, we attempted to reproduce the capacity of 3N determined empirically by <ref> [11] </ref> and compare that to the performance of standard summation units. This was done by running two sets of simulations. Firstly, one product unit was trained to calculate all (2 2 ) N logical functions of the N input variables. Unfortunately, this is only practical for N 2 f2; 3g.
Reference: [12] <author> A. Engel, H. Kohler, F. Tschepke, and A. Zippelius, </author> <title> "Storage capacity and learning algorithms for two-layer neural networks," </title> <journal> Physical Review A, </journal> <volume> vol. 45, </volume> <pages> pp. 7590-7609, </pages> <year> 1992. </year>
Reference-contexts: Theoretical capacity analysis using methods from statistical mechanics <ref> [17, 3, 20, 12] </ref> provide some insight into this process.
Reference: [13] <author> S. Fahlman and C. Lebiere, </author> <booktitle> "The cascade-correlation learning architecture," in Advances in Neural Information Processing Systems (D. </booktitle> <editor> Touretzky, ed.), </editor> <volume> vol. </volume> <pages> 2, </pages> <address> (San Mateo), </address> <pages> pp. 524-532, </pages> <address> (Denver 1989), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Another class of algorithms start off with a sparse architecture and grows the network to the complexity required by the problem. Several algorithms have been proposed for growing feedforward networks. The `upstart' algorithm of Frean [15] and the `cascade-correlation' algorithm of Fahlman <ref> [13] </ref> are examples of this approach. In this section we will use the experience gained from the experiments described earlier to devise a simple method for adding product units to a three layer network. <p> The hidden layer will consist of product units, while the output layer contains a single sigmoidal unit. 5.1 The Constructive Algorithm Several constructive algorithms ([15], <ref> [13] </ref>, [31]) are based on the following process: * A network is trained on a certain training set. * The current weights in the network are frozen, and a new neuron (or a set of candidate neurons) is added to the network. <p> The tradeoff between rapid learning and a minimal architecture can thus be controlled through one variable, the number of random restarts. 5.4 Using Cosine Candidate Units in Cascade Correlation 5.4.1 Introduction The Cascade Correlation (CC) <ref> [13] </ref> algorithm is a well-known constructive method, and has been applied to both feedforward and recurrent networks.
Reference: [14] <author> B. Flower and M. Jabri, </author> <title> "Single and Dual Transistor Synapses for Analogue VLSI Artificial Neural Networks," </title> <booktitle> in Proceedings of the Fourth Australian Conference on Neural Networks, </booktitle> <pages> pp. 106-109, </pages> <year> 1993. </year>
Reference-contexts: In this case the number of weights in the network is more important than the number of neurons, as weights require a much larger surface area for implementation <ref> [14] </ref>. In this case a smaller network with a slightly more complex transfer function is preferable to a larger network with a standard (sigmoidal) transfer function. When product units are used to process Boolean inputs, best performance is obtained [11] by using values of +1 and 1.
Reference: [15] <author> M. Frean, </author> <title> "The upstart algorithm: A method for constructing and training feedforward neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 2, </volume> <pages> pp. 198-209, </pages> <year> 1990. </year>
Reference-contexts: Another class of algorithms start off with a sparse architecture and grows the network to the complexity required by the problem. Several algorithms have been proposed for growing feedforward networks. The `upstart' algorithm of Frean <ref> [15] </ref> and the `cascade-correlation' algorithm of Fahlman [13] are examples of this approach. In this section we will use the experience gained from the experiments described earlier to devise a simple method for adding product units to a three layer network. <p> The values are the average of 25 simulations, each on a different training set. 5.2 Simulations To evaluate the performance of the method with that of the `upstart' <ref> [15] </ref> and `tiling' [31] algorithms, the constructive product network was trained on two problems also described in these papers namely * The parity problem, for N = 7 : : : 10. * Random Mappings. 5.2.1 The Parity Problem In [15] it was reported that the upstart algorithm required N units <p> the performance of the method with that of the `upstart' <ref> [15] </ref> and `tiling' [31] algorithms, the constructive product network was trained on two problems also described in these papers namely * The parity problem, for N = 7 : : : 10. * Random Mappings. 5.2.1 The Parity Problem In [15] it was reported that the upstart algorithm required N units for all parity N problems, and 1,000 training epochs were sufficient for all values of N except N = 10, which required 10,000. <p> In this case our BP-RSA algorithm settings are modified so that more RSA iterations are performed compared to the BP iterations. 5.2.2 Random Mappings Following <ref> [15] </ref>, in this problem the random mapping problem is defined by assigning each of the 2 N patterns its target f1; +1g with 50% probability. As mentioned in [15], this is a difficult problem, due to the absence of correlations and structure in the input for the network to exploit. <p> our BP-RSA algorithm settings are modified so that more RSA iterations are performed compared to the BP iterations. 5.2.2 Random Mappings Following <ref> [15] </ref>, in this problem the random mapping problem is defined by assigning each of the 2 N patterns its target f1; +1g with 50% probability. As mentioned in [15], this is a difficult problem, due to the absence of correlations and structure in the input for the network to exploit. As in [15, 31] the average of 25 runs were performed, each on a different training set. <p> As mentioned in [15], this is a difficult problem, due to the absence of correlations and structure in the input for the network to exploit. As in <ref> [15, 31] </ref> the average of 25 runs were performed, each on a different training set. The number of units added by the upstart algorithm was approximately 2 N =9. <p> The values are plotted in Figure 2. Note that the values for the tiling and upstart algorithms are approximate and were obtained through inspection from a similar graph in <ref> [15] </ref>. 11 learning; for this specific problem 3 product units were added at 400 epoch intervals. 5.3 Discussion As expected, the higher separating capacity of the product unit enables the construction of networks with less neurons than those produced by the tiling and upstart algorithms.
Reference: [16] <author> A. Gallant and H. White, </author> <title> "There exists a neural network that does not make avoidable mistakes," </title> <booktitle> in IEEE International Conference on Neural Networks, </booktitle> <volume> vol. </volume> <pages> 1, </pages> <address> (New York), </address> <pages> pp. 657-664, </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference-contexts: remainder of this report the terms product unit (s) and cos (ine) unit will be used interchangeably as all the problems examined have Boolean inputs. 2 Cosine Activation Functions Networks with cosine hidden unit activations have been used in the past, and have been shown to have universal approximation properties <ref> [28, 16] </ref>. Gallant & White [16] proved that a single hidden layer feedforward network using a `cosine squasher' transfer function can implement a Fourier network. <p> Gallant & White <ref> [16] </ref> proved that a single hidden layer feedforward network using a `cosine squasher' transfer function can implement a Fourier network.
Reference: [17] <author> E. Gardner, </author> <title> "The space of interactions in neural network models," </title> <journal> Journal of Physics A, </journal> <volume> vol. 21, </volume> <pages> pp. 257-270, </pages> <year> 1988. </year>
Reference-contexts: Theoretical capacity analysis using methods from statistical mechanics <ref> [17, 3, 20, 12] </ref> provide some insight into this process.
Reference: [18] <author> J. Ghosh and Y. Shin, </author> <title> "Efficient higher-order neural networks for function approximation and classification," </title> <journal> International Journal of Neural Systems, </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 323-350, </pages> <year> 1992. </year>
Reference-contexts: However, Redding et al [33] found that in order to implement a certain logical function, in most cases only a few of these higher order terms are required. The `pi-sigma' networks (PSNs) of Ghosh & Shin <ref> [18] </ref> attempt to make use of this fact. A PSN with N summation units and one `pi' output neuron provides a K-th order approximation of a continuous function.
Reference: [19] <author> C. Giles and T. Maxwell, </author> <title> "Learning, invariance, and generalization in high-order neural networks," </title> <journal> Applied Optics, </journal> <volume> vol. 26, no. 23, </volume> <pages> pp. 4972-4978, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction It is well-known that supplementing the inputs to a neural network with higher-order combinations of the inputs both increases the capacity of the network [9] and the the ability to learn geometrically invariant properties <ref> [19] </ref>. Such a network consists of a single layer of higher-order units [19] which compute the function @ w 0 + i X w ij x i x j + i;j;k 1 where is the activation or transfer function. fl Technical Report CS-TR-3503 and UMIACS-TR-95-80, University of Maryland, College Park, MD <p> 1 Introduction It is well-known that supplementing the inputs to a neural network with higher-order combinations of the inputs both increases the capacity of the network [9] and the the ability to learn geometrically invariant properties <ref> [19] </ref>. Such a network consists of a single layer of higher-order units [19] which compute the function @ w 0 + i X w ij x i x j + i;j;k 1 where is the activation or transfer function. fl Technical Report CS-TR-3503 and UMIACS-TR-95-80, University of Maryland, College Park, MD 20742 (1995) 1 However, there is a combinatorial explosion of higher order
Reference: [20] <author> J. Hertz, A. Krogh, and R. Palmer, </author> <title> Introduction to the Theory of Neural Computation. </title> <address> Redwood City, CA: </address> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: We have found two main reasons for this: * Random weight initialization. * The presence of local minima. 3.1.1 Weight Initialization The first step in the backpropagation procedure (see <ref> [20] </ref> p. 120) is to initialize all weights to small random values. The main reason for this is to use the dynamic range of the sigmoid function and it's derivative. For large inputs the sigmoid saturates and the derivative is small, leading to small weight adjustments and slow learning. <p> Theoretical capacity analysis using methods from statistical mechanics <ref> [17, 3, 20, 12] </ref> provide some insight into this process.
Reference: [21] <author> B. </author> <title> Horne, Recurrent neural networks: A functional approach. </title> <type> PhD thesis, </type> <institution> University of New Mexico, Dept. of Electrical and Computer Engineering, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: It has been shown <ref> [21] </ref> that for the neural network implementation of n-input, m-output logic functions (the type of problem examined here) the node-complexity of the fully connected architecture used in CC is O ( q nlog m ).
Reference: [22] <author> M. Jabri, E. Tinker, and L. Leerink, </author> <title> "MUME an environment for multi-net and multi-architectures neural simulation," in Neural Network Simulation Environments (J. </title> <editor> Skrzypek, ed.), </editor> <publisher> Norwell, </publisher> <address> MA: </address> <publisher> Kluwer Academic Publications, </publisher> <year> 1994. </year>
Reference-contexts: This is indeed the case. If a neural network simulation environment is available (in our case the Multi Module Neural Simulation Environment MUME <ref> [22] </ref> was used), the basic functionality of a product unit can be obtained by simply adding the cos function cos ( fl input) (and it's derivative which is required by backpropagation) to the existing list of transfer functions.
Reference: [23] <author> R. Jacobs, </author> <title> "Increased rates of convergence through learning rate adaptation," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 295-307, </pages> <year> 1988. </year>
Reference-contexts: Backpropagation (BP) was used as a benchmark and for using in combination with the other algorithms. It is per definition a local search method. The Jacobs delta-bar-delta learning rate adaptation rule <ref> [23] </ref> was used along with BP to accelerate convergence, with the parameters were set to the following values ( = 0:35; = 0:05 and = 0:90). <p> For the training of the random mapping problem, it was found that the parameter settings of the Jacobs <ref> [23] </ref> delta-bar-delta weight update rule was critical in ensuring rapid convergence. The parameters were set to the following values ( = 0:35; = 0:05 and = 0:90) for N = 7; 8; 9 and = 0:003 for N = 10.
Reference: [24] <author> D. Johnson, C. Aragon, L. McGeoch, and C. Schevon, </author> <title> "Optimization by simulated annealing: An experimental evaluation; part i, graph partitioning," </title> <journal> Operations Research, </journal> <volume> vol. 37, </volume> <pages> pp. 865-891, </pages> <year> 1989. </year> <note> Parts II and III expected to appear in 1990. 16 </note>
Reference: [25] <author> D. Johnson, C. Aragon, L. McGeoch, and C. Schevon, </author> <title> "Optimization by simulated annealing: an experimental evaluation; part ii, graph coloring and number partitioning," </title> <journal> Operations Research, </journal> <volume> vol. 39, no. 3, </volume> <pages> pp. 378-406, </pages> <year> 1991. </year>
Reference-contexts: The BP-RSA combination is in effect equivalent to the `local optimization with random restarts' process discussed by <ref> [25, 26] </ref>, where the local search is this case is performed by the BP algorithm. In [26] it was shown that for certain problems where the error surface was `exceedingly mountainous', multiple random-start local optimization outperformed simulated annealing. <p> This again is in accordance with the results of <ref> [25, 26] </ref> who showed that for certain problems where the error surface was `exceedingly mountainous', multiple random-start local optimization outperformed more sophisticated methods.
Reference: [26] <author> N. Karmarkar and R. Karp, </author> <title> "The differencing method of set partitioning," </title> <type> Tech. Rep. </type> <institution> UCB/CSD 82/113, Computer Science Division, University of California, Berkeley, California, </institution> <year> 1982. </year>
Reference-contexts: The BP-RSA combination is in effect equivalent to the `local optimization with random restarts' process discussed by <ref> [25, 26] </ref>, where the local search is this case is performed by the BP algorithm. In [26] it was shown that for certain problems where the error surface was `exceedingly mountainous', multiple random-start local optimization outperformed simulated annealing. <p> The BP-RSA combination is in effect equivalent to the `local optimization with random restarts' process discussed by [25, 26], where the local search is this case is performed by the BP algorithm. In <ref> [26] </ref> it was shown that for certain problems where the error surface was `exceedingly mountainous', multiple random-start local optimization outperformed simulated annealing. <p> This again is in accordance with the results of <ref> [25, 26] </ref> who showed that for certain problems where the error surface was `exceedingly mountainous', multiple random-start local optimization outperformed more sophisticated methods.
Reference: [27] <author> S. Kirkpatrick, C. G. Jr., , and M. Vecchi, </author> <title> "Optimization by simulated annealing," </title> <journal> Science, </journal> <volume> vol. 220, </volume> <year> 1983. </year> <note> Reprinted in [4]. </note>
Reference: [28] <author> A. Lapedes and R. Farber, </author> <title> "Nonlinear signal processing using neural networks: Prediction and system modelling," </title> <type> Tech. Rep. </type> <institution> LA-UR-87-2662, Los Alamos National Laboratory, </institution> <address> Los Alamos, NM, </address> <year> 1987. </year>
Reference-contexts: remainder of this report the terms product unit (s) and cos (ine) unit will be used interchangeably as all the problems examined have Boolean inputs. 2 Cosine Activation Functions Networks with cosine hidden unit activations have been used in the past, and have been shown to have universal approximation properties <ref> [28, 16] </ref>. Gallant & White [16] proved that a single hidden layer feedforward network using a `cosine squasher' transfer function can implement a Fourier network. <p> In the discussion of their `Generalized Fourier Networks' in which sin hidden layer activation functions were used, Lapedes & Faber <ref> [28] </ref> commented that 4 Further generalizations are possible by considering multilayer networks and different expressions for the transfer function. We point out that using sin's often leads to numerical problems, and nonglobal minima, whereas sigmoids seemed to avoid such problems throughout all our extensive simulations. <p> For some training algorithms proofs are available that specify that, for a given accuracy, the number of neurons to be added is finite. From our simulations (and that of others, see <ref> [28] </ref>) it is clear that networks containing cos transfer functions suffer from serious local-minima problems, something which sigmoids are reasonably insensitive to.
Reference: [29] <author> Y. Le Cun, J. Denker, and S. Solla, </author> <title> "Optimal brain damage," </title> <booktitle> in Advances in Neural Information Processing Systems (D. </booktitle> <editor> Touretzky, ed.), </editor> <volume> vol. </volume> <pages> 2, </pages> <address> (San Mateo), </address> <pages> pp. 598-605, </pages> <address> (Denver 1989), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: One class of network adaptation algorithms start out with a redundant architecture and proceed by pruning away seemingly unimportant weights ([36], <ref> [29] </ref>). Another class of algorithms start off with a sparse architecture and grows the network to the complexity required by the problem. Several algorithms have been proposed for growing feedforward networks. The `upstart' algorithm of Frean [15] and the `cascade-correlation' algorithm of Fahlman [13] are examples of this approach.
Reference: [30] <author> G. L. Maxwell T., G.L Giles and H. Chen, </author> <title> "Nonlinear dynamics of artificial neural systems," in Neural networks for computing (J. Denker, </title> <editor> ed.), </editor> <address> New York: </address> <institution> American Institute of Physics, </institution> <year> 1986. </year>
Reference-contexts: More specifically, the number of weights required to implement a K-th order neuron with N inputs are [32] K X i = N + K Another approach to this problem were `sigma-pi' networks <ref> [35, 30] </ref>, in which each hidden layer unit calculates a certain product (or conjunct) of the inputs.
Reference: [31] <author> M. Mezard and J.-P. Nadal, </author> <title> "Learning in feedforward layered networks: The tiling algorithm," </title> <journal> Journal of Physics A, </journal> <volume> vol. 22, </volume> <pages> pp. 2191-2204, </pages> <year> 1989. </year>
Reference-contexts: The hidden layer will consist of product units, while the output layer contains a single sigmoidal unit. 5.1 The Constructive Algorithm Several constructive algorithms ([15], [13], <ref> [31] </ref>) are based on the following process: * A network is trained on a certain training set. * The current weights in the network are frozen, and a new neuron (or a set of candidate neurons) is added to the network. <p> The values are the average of 25 simulations, each on a different training set. 5.2 Simulations To evaluate the performance of the method with that of the `upstart' [15] and `tiling' <ref> [31] </ref> algorithms, the constructive product network was trained on two problems also described in these papers namely * The parity problem, for N = 7 : : : 10. * Random Mappings. 5.2.1 The Parity Problem In [15] it was reported that the upstart algorithm required N units for all parity <p> As mentioned in [15], this is a difficult problem, due to the absence of correlations and structure in the input for the network to exploit. As in <ref> [15, 31] </ref> the average of 25 runs were performed, each on a different training set. The number of units added by the upstart algorithm was approximately 2 N =9.
Reference: [32] <author> M. Minsky and S. Papert, </author> <title> Perceptrons. </title> <publisher> Cambridge: MIT Press, </publisher> <year> 1969. </year> <note> Partially reprinted in [4]. </note>
Reference-contexts: More specifically, the number of weights required to implement a K-th order neuron with N inputs are <ref> [32] </ref> K X i = N + K Another approach to this problem were `sigma-pi' networks [35, 30], in which each hidden layer unit calculates a certain product (or conjunct) of the inputs.
Reference: [33] <author> N. Redding, A. Kowalczyk, and T. Downs, </author> <title> "A constructive higher order network algorithm that is polynomial-time," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 6, </volume> <editor> p. </editor> <volume> 997, </volume> <year> 1993. </year>
Reference-contexts: This architecture presents another method of constructing higher-order networks, but the problem of the combinatorial explosion of the number of weights still remains if the conjuncts are not hand-coded. However, Redding et al <ref> [33] </ref> found that in order to implement a certain logical function, in most cases only a few of these higher order terms are required. The `pi-sigma' networks (PSNs) of Ghosh & Shin [18] attempt to make use of this fact.
Reference: [34] <author> F. Romeo, </author> <title> Simulated Annealing: Theory and Applications to Layout Problems. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1989. </year> <note> Memorandum UCB/ERL-M89/29. </note>
Reference: [35] <author> D. Rumelhart, G. Hinton, and J. McClelland, </author> <title> "A general framework for parallel distributed processing," in Parallel Distributed Processing (D. </title> <editor> Rumelhart and J. McClelland, eds.), </editor> <volume> vol. 1, ch. 2, </volume> <pages> pp. 45-76, </pages> <address> Cambridge: </address> <publisher> MIT Press, </publisher> <year> 1986. </year> <note> Reprinted in [4]. </note>
Reference-contexts: More specifically, the number of weights required to implement a K-th order neuron with N inputs are [32] K X i = N + K Another approach to this problem were `sigma-pi' networks <ref> [35, 30] </ref>, in which each hidden layer unit calculates a certain product (or conjunct) of the inputs.
Reference: [36] <author> J. Sietsma and R. Dow, </author> <title> "Neural net pruning|why and how," </title> <booktitle> in IEEE International Conference on Neural Networks, </booktitle> <volume> vol. </volume> <pages> 1, </pages> <address> (New York), </address> <pages> pp. 325-333, </pages> <address> (San Diego 1988), </address> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference: [37] <author> E. Sontag, </author> <title> "Feedforward nets for interpolation and classification," </title> <type> nar, </type> <institution> Rutgers University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: It then follows that product units are networks with universal approximation properties. Also relevant are the results of research into the Vapnic-Chervonenkis (VC) dimension of different activation functions. In <ref> [37] </ref> the VC dimension is defined as follows. For a positive integer N , a dichotomy (S ; S + ) on a set S R N is a partition S = S [ S + of S into two disjoint subsets. <p> The VC dimension is then the largest integer l so that at least some set S of cardinality l in R which can be shattered by some f 2 F . In <ref> [37] </ref> it was remarked that If only or are desired to be infinite, one may take the simpler example (x) = sin (x).
Reference: [38] <author> M. White, </author> <title> "A public domain C implemention of the Cascade Correlation algorithm," </title> <institution> (Department of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA), </address> <year> 1993. </year> <month> 17 </month>
Reference-contexts: Each of these candidate units are initialized with a different set of random initial weights, and may also be different nonlinear functions. The public domain version of CC used <ref> [38] </ref> supports four different candidate types; the asymmetric sigmoid, symmetric sigmoid, variable sigmoid and gaussian units. Facilities exist for either selecting one unit type, or training with a pool of different units allowing the construction of hybrid networks.
References-found: 38

