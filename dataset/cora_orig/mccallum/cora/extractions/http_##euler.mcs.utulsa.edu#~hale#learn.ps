URL: http://euler.mcs.utulsa.edu/~hale/learn.ps
Refering-URL: http://euler.mcs.utulsa.edu/~hale/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: sandip@kolkata.mcs.utulsa.edu  
Title: Learning to coordinate without sharing information  
Author: Sandip Sen, Mahendra Sekaran, and John Hale 
Keyword: Content areas: distributed AI, machine learning  
Address: 600 South College Avenue Tulsa, OK 74104-3189  
Affiliation: Department of Mathematical Computer Sciences University of Tulsa  
Abstract: Researchers in the field of Distributed Artificial Intelligence (DAI) have been interested in developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and also demonstrate the benefits of using learned coordination knowledge on similar problems. Similar reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. B. Barto, R. S. Sutton, and C. Watkins. </author> <title> Sequential decision problems and neural networks. </title> <booktitle> In Proceedings of 1989 Conference on Neural Information Processing, </booktitle> <year> 1989. </year>
Reference-contexts: The resultant systems are, therefore, robust and general-purpose. 3 Reinforcement learning In reinforcement learning problems <ref> [1, 21, 36, 42] </ref>, reactive and adaptive agents are given a description of the current state and have to choose the next action from a set of possible actions so as to maximize a scalar reinforcement or feedback received after each action. <p> The learner's environment can be modeled by a discrete time, finite state, Markov decision process that can be represented by a 4-tuple hS; A; P; ri where P : S fi S fi A 7! <ref> [0; 1] </ref> gives the probability of moving from state s 1 to s 2 on performing action a, and r : S fi A 7! &lt; is a scalar reward function.
Reference: [2] <editor> A. H. Bond and L. Gasser. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: In a reinforcement learning scenario, an agent chooses actions based on its perceptions, receives scalar feedbacks based on past actions, and is expected to develop a mapping from perceptions to actions that will maximize feedbacks. Multiagent systems are a particular type of distributed AI system <ref> [2] </ref>, in which autonomous intelligent agents inhabit a world with no global control or globally consistent knowledge. In contrast to cooperative problem solvers [13, 14], agents in multiagent systems are not pre-disposed to help each other out with the all the resources and capabilities that they possess.
Reference: [3] <author> L. Booker, D. Goldberg, and J. Holland. </author> <title> Classifier systems and genetic algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 235-282, </pages> <year> 1989. </year> <month> 12 </month>
Reference-contexts: On the other hand, learning techniques that can be used incrementally to develop problem-solving skills relying on little or no pre-existing domain knowledge <ref> [3, 16, 28] </ref>, can be used by both cooperative and non-cooperative agents. Though the latter form of learning may be more time-consuming, it is generally more robust in the presence of noisy, uncertain, and incomplete information.
Reference: [4] <editor> P. Brazdil, M. Gams, S. Sian, L. Torgo, and W. van de Velde. </editor> <booktitle> Learning in distributed systems and multi-agent environments. In European Working Session on Learning, Lecture Notes in AI, 482, </booktitle> <address> Berlin, March 1991. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: In this paper, we propose to use a class of knowledge-lean machine learning techniques that can be used by multiple agents to acquire coordination knowledge through repeated interactions with other agents. Previous proposals for using learning techniques to coordinate multiple agents have mostly relied on using prior knowledge <ref> [4, 35] </ref>, or on cooperative domains with unrestricted information sharing [31, 33]. Even previous work on using reinforcement learning for coordinating multiple agents [38, 40] have relied on explicit information sharing. We, however, concentrate on systems where agents share no problem-solving knowledge.
Reference: [5] <author> J. G. Carbonell. </author> <title> Derivational analogy: A theory of reconstructive problem solving and expertise acquisition. </title> <editor> In J. Carbonell and T. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1986. </year>
Reference-contexts: In cooperative domains, where agents have approximate models of the behavior of other agents and are willing to reveal information to enable the group perform better as a whole, preexisting domain knowledge can be used inductively <ref> [5, 9, 25] </ref> to improve performance over time. On the other hand, learning techniques that can be used incrementally to develop problem-solving skills relying on little or no pre-existing domain knowledge [3, 16, 28], can be used by both cooperative and non-cooperative agents.
Reference: [6] <author> P. R. Cohen and H. J. Levesque. </author> <title> Persistence, intention, and commitment. </title> <booktitle> In Proceedings of the 1986 Workshop on Reasoning About Actions and Plans, </booktitle> <month> July </month> <year> 1986. </year>
Reference-contexts: of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches <ref> [6, 7, 41] </ref> Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20]. Almost all of the listed work above assume explicit or implicit sharing of information.
Reference: [7] <author> P. R. Cohen and C. R. Perrault. </author> <title> Elements of a plan-based theory of speech acts. </title> <journal> Cognitive Science, </journal> <volume> 3(3) </volume> <pages> 177-212, </pages> <year> 1979. </year>
Reference-contexts: of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches <ref> [6, 7, 41] </ref> Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20]. Almost all of the listed work above assume explicit or implicit sharing of information. <p> Almost all of the listed work above assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results [12], speech acts <ref> [7] </ref>, resource availabilities [34], etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents [17, 18, 27, 44] to aid local decision-making.
Reference: [8] <author> S. E. Conry, R. A. Meyer, and V. R. Lesser. </author> <title> Multistage negotiation in distributed planning. </title> <editor> In A. H. Bond and L. Gasser, editors, </editor> <booktitle> Readings in Distributed Artificial Intelligence, </booktitle> <pages> pages 367-384. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1988. </year>
Reference-contexts: The search for domain-independent coordination mechanisms has yielded some very different, yet effective, classes of coordination schemes. The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting <ref> [8, 34] </ref> * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34,
Reference: [9] <author> G. DeJong and R. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: In cooperative domains, where agents have approximate models of the behavior of other agents and are willing to reveal information to enable the group perform better as a whole, preexisting domain knowledge can be used inductively <ref> [5, 9, 25] </ref> to improve performance over time. On the other hand, learning techniques that can be used incrementally to develop problem-solving skills relying on little or no pre-existing domain knowledge [3, 16, 28], can be used by both cooperative and non-cooperative agents.
Reference: [10] <author> E. H. Durfee. </author> <title> A Unified Approach to Dynamic Coordination: Planning actions and interactions in a distributed problem solving network. </title> <type> PhD thesis, </type> <institution> University of Mas-sachusetts, </institution> <month> Sept. </month> <year> 1987. </year> <note> (Also published as Technical Report 87-84, </note> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, Massachusetts 01003, </address> <month> September </month> <year> 1987.). </year>
Reference-contexts: The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning <ref> [10, 19, 26] </ref> * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20].
Reference: [11] <author> E. H. Durfee. </author> <title> Coordination of Distributed Problem Solvers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Coordination of problem solvers, irrespective of whether they are selfish or cooperative in nature, is a key issue to the design of an effective distributed AI system <ref> [11] </ref>. The search for domain-independent coordination mechanisms has yielded some very different, yet effective, classes of coordination schemes. <p> and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line <ref> [11, 20] </ref>. Almost all of the listed work above assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results [12], speech acts [7], resource availabilities [34], etc. to other agents to facilitate the process of coordination.
Reference: [12] <author> E. H. Durfee and V. R. Lesser. </author> <title> Partial global planning: A coordination framework for distributed hypothesis formation. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(5), </volume> <month> September </month> <year> 1991. </year> <title> (Special Issue on Distributed Sensor Networks). </title>
Reference-contexts: Almost all of the listed work above assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results <ref> [12] </ref>, speech acts [7], resource availabilities [34], etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents [17, 18, 27, 44] to aid local decision-making.
Reference: [13] <author> E. H. Durfee, V. R. Lesser, and D. D. Corkill. </author> <title> Cooperative distributed problem solving. </title> <editor> In A. Barr, P. R. Cohen, and E. A. Feigenbaum, editors, </editor> <booktitle> The Handbook of Artificial Intelligence, volume IV, chapter XVII, </booktitle> <pages> pages 83-137. </pages> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Multiagent systems are a particular type of distributed AI system [2], in which autonomous intelligent agents inhabit a world with no global control or globally consistent knowledge. In contrast to cooperative problem solvers <ref> [13, 14] </ref>, agents in multiagent systems are not pre-disposed to help each other out with the all the resources and capabilities that they possess. These agents may still need to coordinate their activities with others to achieve their own local goals.
Reference: [14] <author> E. H. Durfee, V. R. Lesser, and D. D. Corkill. </author> <title> Trends in cooperative distributed problem solving. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(1) </volume> <pages> 63-83, </pages> <month> Mar. </month> <year> 1989. </year>
Reference-contexts: Multiagent systems are a particular type of distributed AI system [2], in which autonomous intelligent agents inhabit a world with no global control or globally consistent knowledge. In contrast to cooperative problem solvers <ref> [13, 14] </ref>, agents in multiagent systems are not pre-disposed to help each other out with the all the resources and capabilities that they possess. These agents may still need to coordinate their activities with others to achieve their own local goals.
Reference: [15] <author> E. H. Durfee and T. A. Montgomery. </author> <title> Coordination as distributed search in a hierarchical behavior space. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(6) </volume> <pages> 1363-1378, </pages> <month> November/December </month> <year> 1991. </year>
Reference-contexts: The search for domain-independent coordination mechanisms has yielded some very different, yet effective, classes of coordination schemes. The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms <ref> [15, 22, 43] </ref> * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others
Reference: [16] <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year> <month> 13 </month>
Reference-contexts: On the other hand, learning techniques that can be used incrementally to develop problem-solving skills relying on little or no pre-existing domain knowledge <ref> [3, 16, 28] </ref>, can be used by both cooperative and non-cooperative agents. Though the latter form of learning may be more time-consuming, it is generally more robust in the presence of noisy, uncertain, and incomplete information.
Reference: [17] <author> M. S. Fox. </author> <title> An organizational view of distributed systems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 11(1) </volume> <pages> 70-80, </pages> <month> Jan. </month> <year> 1981. </year> <note> (Also published in Readings in Distributed Artificial Intelligence, </note> <editor> Alan H. Bond and Les Gasser, editors, </editor> <address> pages 140-150, </address> <publisher> Morgan Kaufmann, 1988.). </publisher>
Reference-contexts: The search for domain-independent coordination mechanisms has yielded some very different, yet effective, classes of coordination schemes. The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws <ref> [17, 23, 24] </ref> * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20]. <p> [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line <ref> [34, 17, 32] </ref> as coordination structures, others acquire coordination knowledge on-line [11, 20]. Almost all of the listed work above assume explicit or implicit sharing of information. <p> In the explicit form of information sharing, agents communicate partial results [12], speech acts [7], resource availabilities [34], etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents <ref> [17, 18, 27, 44] </ref> to aid local decision-making.
Reference: [18] <author> M. Genesereth, M. Ginsberg, and J. Rosenschein. </author> <title> Cooperation without communications. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 51-57, </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1986. </year>
Reference-contexts: The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations <ref> [18, 20, 27, 44] </ref> * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20]. Almost all of the listed work above assume explicit or implicit sharing of information. <p> In the explicit form of information sharing, agents communicate partial results [12], speech acts [7], resource availabilities [34], etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents <ref> [17, 18, 27, 44] </ref> to aid local decision-making.
Reference: [19] <author> M. Georgeff. </author> <title> Communication and interaction in multi-agent planning. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 125-129, </pages> <address> Washington, D.C., </address> <month> Aug. </month> <year> 1983. </year> <note> (Also published in Readings in Distributed Artificial Intelligence, </note> <editor> Alan H. Bond and Les Gasser, editors, </editor> <address> pages 200-204, </address> <publisher> Morgan Kaufmann, 1988.). </publisher>
Reference-contexts: The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning <ref> [10, 19, 26] </ref> * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20].
Reference: [20] <author> P. J. Gmytrasiewicz, E. H. Durfee, and D. K. Wehe. </author> <title> A decision-theoretic approach to coordinating multiagent interactions. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 62-68, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations <ref> [18, 20, 27, 44] </ref> * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20]. Almost all of the listed work above assume explicit or implicit sharing of information. <p> and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line <ref> [11, 20] </ref>. Almost all of the listed work above assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results [12], speech acts [7], resource availabilities [34], etc. to other agents to facilitate the process of coordination.
Reference: [21] <author> J. H. Holland. </author> <title> Escaping brittleness: the possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. Michalski, J. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, an artificial intelligence approach: Volume II. </booktitle> <publisher> Morgan Kaufman, </publisher> <address> Los Alamos, CA, </address> <year> 1986. </year>
Reference-contexts: The resultant systems are, therefore, robust and general-purpose. 3 Reinforcement learning In reinforcement learning problems <ref> [1, 21, 36, 42] </ref>, reactive and adaptive agents are given a description of the current state and have to choose the next action from a set of possible actions so as to maximize a scalar reinforcement or feedback received after each action. <p> R and a transition to state s 0 , then the corresponding Q value is modified as follows: Q (s; a) (1 fi) Q (s; a) + fi (R + fl max Q (s 0 ; a 0 ): (1) The above update rule is similar in to Holland's bucket-brigade <ref> [21] </ref> and Sutton's temporal-difference [36] learning scheme. 4 Block pushing problem To explore the application of reinforcement learning in multi-agent environments, we designed a problem in which two agents, a 1 and a 2 , are independently assigned to move a block, b, from a starting position, S, to some goal
Reference: [22] <author> M. Klein. </author> <title> Conflict Resolution in Cooperative Design. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: The search for domain-independent coordination mechanisms has yielded some very different, yet effective, classes of coordination schemes. The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms <ref> [15, 22, 43] </ref> * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others
Reference: [23] <author> T. W. Malone. </author> <title> Modeling coordination in organizations and markets. </title> <journal> Management Science, </journal> <volume> 33(10) </volume> <pages> 1317-1332, </pages> <year> 1987. </year> <note> (Also published in Readings in Distributed Artificial Intelligence, </note> <editor> Alan H. Bond and Les Gasser, editors, </editor> <address> pages 151-158, </address> <publisher> Morgan Kaufmann, 1988.). </publisher>
Reference-contexts: The search for domain-independent coordination mechanisms has yielded some very different, yet effective, classes of coordination schemes. The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws <ref> [17, 23, 24] </ref> * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20].
Reference: [24] <author> J. G. March and H. A. Simon. </author> <title> Organizations. </title> <publisher> John Wiley & Sons, </publisher> <year> 1958. </year>
Reference-contexts: The search for domain-independent coordination mechanisms has yielded some very different, yet effective, classes of coordination schemes. The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws <ref> [17, 23, 24] </ref> * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20].
Reference: [25] <author> B. W. Porter, R. Bareiss, and R. C. Holte. </author> <title> Concept learning and heuristic classification in weak-theory domains. </title> <journal> Artificial Intelligence, </journal> <volume> 45(1 & 2), </volume> <year> 1990. </year>
Reference-contexts: In cooperative domains, where agents have approximate models of the behavior of other agents and are willing to reveal information to enable the group perform better as a whole, preexisting domain knowledge can be used inductively <ref> [5, 9, 25] </ref> to improve performance over time. On the other hand, learning techniques that can be used incrementally to develop problem-solving skills relying on little or no pre-existing domain knowledge [3, 16, 28], can be used by both cooperative and non-cooperative agents.
Reference: [26] <author> J. S. Rosenschein. </author> <title> Synchronization of multi-agent plans. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 115-119, </pages> <address> Pittsburgh, Pennsylvania, </address> <month> Aug. </month> <year> 1982. </year> <note> (Also published in Readings in Distributed Artificial Intelligence, </note> <editor> Alan H. Bond and Les Gasser, editors, </editor> <address> pages 187-191, </address> <publisher> Morgan Kaufmann, 1988.). </publisher>
Reference-contexts: The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning <ref> [10, 19, 26] </ref> * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20].
Reference: [27] <author> J. S. Rosenschein. </author> <title> Rational Interaction: Cooperation Among Intelligent Agents. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> October </month> <year> 1985. </year>
Reference-contexts: The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations <ref> [18, 20, 27, 44] </ref> * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20]. Almost all of the listed work above assume explicit or implicit sharing of information. <p> In the explicit form of information sharing, agents communicate partial results [12], speech acts [7], resource availabilities [34], etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents <ref> [17, 18, 27, 44] </ref> to aid local decision-making.
Reference: [28] <author> D. E. Rumelhart and D. Zipser. </author> <title> Feature discovery by competitive learning. </title> <journal> Cognitive Science, </journal> <volume> 9(1), </volume> <year> 1985. </year>
Reference-contexts: On the other hand, learning techniques that can be used incrementally to develop problem-solving skills relying on little or no pre-existing domain knowledge <ref> [3, 16, 28] </ref>, can be used by both cooperative and non-cooperative agents. Though the latter form of learning may be more time-consuming, it is generally more robust in the presence of noisy, uncertain, and incomplete information.
Reference: [29] <author> P. D. Scott. </author> <title> Learning: the construction of a posteriori knowledge structures. </title> <booktitle> In Proceedings of National Conference on Artificial Intelligence, </booktitle> <pages> pages 359-363, </pages> <year> 1983. </year> <month> 14 </month>
Reference-contexts: One of the most effective techniques to dynamically acquire and use knowledge structures for problem solving is based on the concept of experience based learning <ref> [29] </ref>. Researchers in the field of machine learning have investigated a number of schemes for using past experience to improve problem solving behavior [30]. A number of these schemes can be effectively used to aid the problem of coordinating multiple agents inhabiting a common environment.
Reference: [30] <author> J. W. Shavlik and T. G. Dietterich. </author> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: One of the most effective techniques to dynamically acquire and use knowledge structures for problem solving is based on the concept of experience based learning [29]. Researchers in the field of machine learning have investigated a number of schemes for using past experience to improve problem solving behavior <ref> [30] </ref>. A number of these schemes can be effectively used to aid the problem of coordinating multiple agents inhabiting a common environment.
Reference: [31] <author> M. J. Shaw and A. B. Whinston. </author> <title> Learning and adaptation in distributed artificial intelligence systems. </title> <editor> In L. Gasser and M. N. Huhns, editors, </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence. </booktitle> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference-contexts: Previous proposals for using learning techniques to coordinate multiple agents have mostly relied on using prior knowledge [4, 35], or on cooperative domains with unrestricted information sharing <ref> [31, 33] </ref>. Even previous work on using reinforcement learning for coordinating multiple agents [38, 40] have relied on explicit information sharing. We, however, concentrate on systems where agents share no problem-solving knowledge.
Reference: [32] <author> Y. Shoham and M. Tennenholtz. </author> <title> On the synthesis of useful social laws for artificial agent societies (preliminary report). </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 276-281, </pages> <address> San Jose, California, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line <ref> [34, 17, 32] </ref> as coordination structures, others acquire coordination knowledge on-line [11, 20]. Almost all of the listed work above assume explicit or implicit sharing of information.
Reference: [33] <author> S. Sian. </author> <title> Adaptation based on cooperative learning in multi-agent systems. </title> <editor> In Y. De-mazeau and J.-P. Muller, editors, </editor> <booktitle> Decentralize AI, </booktitle> <volume> volume 2, </volume> <pages> pages 257-272. </pages> <publisher> Elsevier Science Publications, </publisher> <year> 1991. </year>
Reference-contexts: Previous proposals for using learning techniques to coordinate multiple agents have mostly relied on using prior knowledge [4, 35], or on cooperative domains with unrestricted information sharing <ref> [31, 33] </ref>. Even previous work on using reinforcement learning for coordinating multiple agents [38, 40] have relied on explicit information sharing. We, however, concentrate on systems where agents share no problem-solving knowledge.
Reference: [34] <author> R. G. Smith. </author> <title> The contract net protocol: High-level communication and control in a distributed problem solver. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(12):1104-1113, </volume> <month> Dec. </month> <year> 1980. </year>
Reference-contexts: The search for domain-independent coordination mechanisms has yielded some very different, yet effective, classes of coordination schemes. The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting <ref> [8, 34] </ref> * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, <p> [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line <ref> [34, 17, 32] </ref> as coordination structures, others acquire coordination knowledge on-line [11, 20]. Almost all of the listed work above assume explicit or implicit sharing of information. <p> Almost all of the listed work above assume explicit or implicit sharing of information. In the explicit form of information sharing, agents communicate partial results [12], speech acts [7], resource availabilities <ref> [34] </ref>, etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents [17, 18, 27, 44] to aid local decision-making.
Reference: [35] <author> T. Sugawara and V. Lesser. </author> <title> On-line learning of coordination plans. </title> <booktitle> In Working Papers of the 12th International Workshop on Distributed Artificial Intelligence, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In this paper, we propose to use a class of knowledge-lean machine learning techniques that can be used by multiple agents to acquire coordination knowledge through repeated interactions with other agents. Previous proposals for using learning techniques to coordinate multiple agents have mostly relied on using prior knowledge <ref> [4, 35] </ref>, or on cooperative domains with unrestricted information sharing [31, 33]. Even previous work on using reinforcement learning for coordinating multiple agents [38, 40] have relied on explicit information sharing. We, however, concentrate on systems where agents share no problem-solving knowledge.
Reference: [36] <author> R. S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts at Amherst, </institution> <year> 1984. </year>
Reference-contexts: The resultant systems are, therefore, robust and general-purpose. 3 Reinforcement learning In reinforcement learning problems <ref> [1, 21, 36, 42] </ref>, reactive and adaptive agents are given a description of the current state and have to choose the next action from a set of possible actions so as to maximize a scalar reinforcement or feedback received after each action. <p> to state s 0 , then the corresponding Q value is modified as follows: Q (s; a) (1 fi) Q (s; a) + fi (R + fl max Q (s 0 ; a 0 ): (1) The above update rule is similar in to Holland's bucket-brigade [21] and Sutton's temporal-difference <ref> [36] </ref> learning scheme. 4 Block pushing problem To explore the application of reinforcement learning in multi-agent environments, we designed a problem in which two agents, a 1 and a 2 , are independently assigned to move a block, b, from a starting position, S, to some goal position, G, following a
Reference: [37] <author> R. S. Sutton. </author> <title> Integrated architecture for learning, planning, and reacting based on approximate dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-225, </pages> <year> 1990. </year>
Reference-contexts: Without an appropriate choice of system parameters, the system may take considerable time to converge, or may not converge at all. We plan to develop mechanisms to incorporate world models to speed up reinforcement learning as proposed by Sutton <ref> [37] </ref>. We are also investigating the application of reinforcement learning for resource-sharing problems involving non-benevolent agents.
Reference: [38] <author> M. Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 330-337, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Previous proposals for using learning techniques to coordinate multiple agents have mostly relied on using prior knowledge [4, 35], or on cooperative domains with unrestricted information sharing [31, 33]. Even previous work on using reinforcement learning for coordinating multiple agents <ref> [38, 40] </ref> have relied on explicit information sharing. We, however, concentrate on systems where agents share no problem-solving knowledge. We show that although each agent is independently using reinforcement learning techniques to optimize its own environmental reward, global coordination between multiple agents can emerge without explicit or implicit information sharing.
Reference: [39] <author> C. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: Various reinforcement learning strategies have been proposed using which agents can can develop a policy to maximize rewards accumulated over time. For our experiments, we use the Q-learning <ref> [39] </ref> algorithm which is designed to find a policy fl that maximizes V fl (s) for all states s 2 S. The decision policy is represented by a function, Q : S fi A 7! &lt;, which estimates long-term discounted rewards for each state-action pair.
Reference: [40] <author> G. Wei. </author> <title> Learning to coordinate actions in multi-agent systems. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 311-316, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Previous proposals for using learning techniques to coordinate multiple agents have mostly relied on using prior knowledge [4, 35], or on cooperative domains with unrestricted information sharing [31, 33]. Even previous work on using reinforcement learning for coordinating multiple agents <ref> [38, 40] </ref> have relied on explicit information sharing. We, however, concentrate on systems where agents share no problem-solving knowledge. We show that although each agent is independently using reinforcement learning techniques to optimize its own environmental reward, global coordination between multiple agents can emerge without explicit or implicit information sharing.
Reference: [41] <author> E. Werner. </author> <title> Cooperating agents: A unified theory of communication and social structure. </title> <editor> In L. Gasser and M. N. Huhns, editors, </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence. </booktitle> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference-contexts: of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches <ref> [6, 7, 41] </ref> Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20]. Almost all of the listed work above assume explicit or implicit sharing of information.
Reference: [42] <author> S. D. Whitehead and D. H. Ballard. </author> <title> Active perception and reinforcement learning. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 179-188, </pages> <year> 1990. </year>
Reference-contexts: The resultant systems are, therefore, robust and general-purpose. 3 Reinforcement learning In reinforcement learning problems <ref> [1, 21, 36, 42] </ref>, reactive and adaptive agents are given a description of the current state and have to choose the next action from a set of possible actions so as to maximize a scalar reinforcement or feedback received after each action.
Reference: [43] <author> M. Yokoo, E. Durfee, T. Ishida, and K. Kuwabara. </author> <title> Distributed constraint satisfaction for formalizing distributed problem solving. </title> <booktitle> In Proceedings of the Twelfth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 614-621, </pages> <year> 1992. </year> <month> 15 </month>
Reference-contexts: The search for domain-independent coordination mechanisms has yielded some very different, yet effective, classes of coordination schemes. The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms <ref> [15, 22, 43] </ref> * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations [18, 20, 27, 44] * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others
Reference: [44] <author> G. Zlotkin and J. S. Rosenschein. </author> <title> Negotiation and conflict resolution in non-cooperative domains. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 100-105, </pages> <month> July </month> <year> 1990. </year> <month> 16 </month>
Reference-contexts: The most influential classes of coordination mechanisms developed to date are the following: * protocols based on contracting [8, 34] * distributed search formalisms [15, 22, 43] * organizational and social laws [17, 23, 24] * multi-agent planning [10, 19, 26] * decision and game theoretic negotiations <ref> [18, 20, 27, 44] </ref> * linguistic approaches [6, 7, 41] Whereas some of these work uses architectures and protocols designed off-line [34, 17, 32] as coordination structures, others acquire coordination knowledge on-line [11, 20]. Almost all of the listed work above assume explicit or implicit sharing of information. <p> In the explicit form of information sharing, agents communicate partial results [12], speech acts [7], resource availabilities [34], etc. to other agents to facilitate the process of coordination. In the implicit form of information sharing, agents use knowledge about the capabilities of other agents <ref> [17, 18, 27, 44] </ref> to aid local decision-making.
References-found: 44

