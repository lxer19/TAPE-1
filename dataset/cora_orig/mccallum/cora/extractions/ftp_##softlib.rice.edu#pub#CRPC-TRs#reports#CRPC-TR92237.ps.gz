URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92237.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: ADIFOR Working Note #6: Structured Second- and Higher-Order Derivatives through Univariate Taylor Series MCS Preprint P296-0392  
Author: by Christian Bischof, George Corliss, and Andreas Griewank 
Abstract: Second- and higher-order derivatives are required by applications in scientific computation, especially for optimization algorithms. The two complementary concepts of interpolating partial derivatives from univariate Taylor series and preaccumulating of "local" derivatives form the mathematical foundations for accurate, efficient computation of second- and higher-order partial derivatives for large codes. We compute derivatives in a fashion that parallelizes well, exploits sparsity or other structure frequently found in Hessian matrices, can compute only selected elements of a Hessian matrix, and computes Hessian fi vector products. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Ham-marling, A. McKenney, and D. Sorensen. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1992. </year> <note> To appear. </note>
Reference: [2] <author> Edward Anderson, Zhaojun Bai, Christian Bischof, James Demmel, Jack Dongarra, Jeremy DuCroz, Anne Greenbaum, Sven Hammarling, Alan McKenney, and Danny Sorensen. LA-PACK: </author> <title> A portable linear algebra library for high-performance computers. </title> <editor> In Joanne Martin, editor, </editor> <booktitle> SUPERCOMPUTING '90, </booktitle> <pages> pages 2-10, </pages> <address> New York, 1990. </address> <publisher> ACM Press. </publisher>
Reference: [3] <author> Brett Averick, Richard G. Carter, and Jorge J. </author> <title> More. The MINPACK-2 test problem collection (preliminary version). </title> <type> Technical Memorandum ANL/MCS-TM-150, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Writing code for the analytic second derivative is a tedious and error-prone process, even for problems of modest code size like those in the Hock and Schittkowski [19] or the MINPACK-2 <ref> [3] </ref> test suites. For applications such as multidisciplinary optimization [27,28] where the code defining the function f may be tens of thousands of lines long, hand-coding even rf is unthinkable.
Reference: [4] <author> W. Baur and V. Strassen. </author> <title> The complexity of partial derivatives. </title> <journal> Theoretical Computer Science, </journal> <volume> 22 </volume> <pages> 317-330, </pages> <year> 1983. </year>
Reference-contexts: recognized, and algorithms may be developed that efficiently utilize higher derivatives. 3 2.2 Cost of Complete Jacobians and Hessians Using the reverse mode of automatic differentiation, one can compute the gradient of a scalar function for no more than five times the arithmetic operations needed to evaluate the function itself <ref> [4] </ref>. However, this result does not extend to the Jacobians of vector functions.
Reference: [5] <author> L. M. Beda, L. N. Korolev, N. V. Sukkikh, and T. S. Frolova. </author> <title> Programs for automatic differentiation for the machine BESM. </title> <type> Technical Report, </type> <institution> Institute for Precise Mechanics and Computation Techniques, Academy of Science, Moscow, USSR, </institution> <year> 1959. </year> <note> (In Russian). </note>
Reference: [6] <author> Martin Berz. </author> <title> Forward algorithms for high orders and many variables with application to beam physics. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 147-156. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Third-, fourth-, and higher-order partial derivatives are required by some algorithms for solving potentially degenerate nonlinear systems or for nested optimization problems. Berz <ref> [6] </ref> and Mich-elotti [22] discussed the problem of beam tracing in the Superconducting Super Collider. In that application, up to m = 10 derivatives in n = 6 variables are needed to describe the physical system.
Reference: [7] <author> Christian Bischof, Alan Carle, George Corliss, Andreas Griewank, and Paul Hovland. </author> <title> Generating derivative codes from Fortran programs. </title> <journal> Scientific Computing, </journal> <note> to appear. ADIFOR Working Note # 1. Also appeared as Preprint MCS-P263-0991, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1991, </year> <note> and as Technical Report 91185, Center for Research in Parallel Computation, </note> <institution> Rice University, Houston, Tex., </institution> <year> 1991. </year>
Reference-contexts: For first derivatives, this partitioning approach can be adopted quite easily, for example, by utilizing the interface of ADIFOR (Automatic Differentiation In FORtran) <ref> [7] </ref> or ADOL-C (Automatic Differentiation Of aLgorithms written in C) [18]. Each pair of independent variables that corresponds to a nonzero entry in the Hessian must occur together in at least one of the groups.
Reference: [8] <author> Christian Bischof, George Corliss, and Andreas Griewank. </author> <title> ADIFOR exception handling. </title> <type> Technical Memorandum ANL/MCS-TM-159, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> January </month> <year> 1992. </year> <note> ADIFOR Working Note # 3. 15 </note>
Reference: [9] <author> Christian Bischof and Paul Hovland. </author> <title> Using ADIFOR to compute dense and sparse Jacobians. </title> <type> Technical Memorandum ANL/MCS-TM-158, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> October </month> <year> 1991. </year> <note> ADIFOR Working Note # 2. </note>
Reference: [10] <author> D. Callahan, K. Cooper, Robert T. Hood, Ken Kennedy, and Linda M. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1988. </year>
Reference-contexts: Thus, we have an exponential complexity reduction in terms of m. 8 Conclusions and Future Research Directions ADIFOR (Automatic Differentiation In FORtran) [7,8,9] is a source translation tool implemented by using the data abstractions and program analysis capabilities of the ParaScope Parallel Programming Environment <ref> [10] </ref>. ADIFOR accepts arbitrary Fortran 77 code defining the computation of a function and writes portable Fortran 77 code for the computation of its first derivatives. Work is in progress to extend ADIFOR to provide second- and higher-order derivatives as described in this paper.
Reference: [11] <author> Y. F. Chang. </author> <title> The conduction-diffusion theory of semiconductor junctions. </title> <journal> Journal of Applied Physics, </journal> <volume> 38(2) </volume> <pages> 534-544, </pages> <year> 1967. </year>
Reference-contexts: Multiplication of two polynomials of degree m (or, equivalently, the convolution of their coefficients) is the central workhorse of all higher-derivative calculations <ref> [11] </ref>, including the multivariate case and the reverse mode. Hence, it is important for the performance of automatic differentiation that the core routine for multiplying two polynomials be implemented with maximal efficiency for each m, much as dense linear algebra computations rely on efficient implementations of the BLAS [2,1].
Reference: [12] <author> John Dennis and Robert B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: Currently, this structuring is done by hand in some applications. Hence, it is important that a tool for computing second derivatives support sparse computations. Software for solving problems in unconstrained optimization (Dennis and Schnabel <ref> [12] </ref>, for example) often views analytic second-derivative information as optional, but desirable. If the user is able to supply code for computing r 2 f, the algorithms often display the quadratic convergence rate of Newton's method, rather than the superlinear rate of the secant method.
Reference: [13] <author> John E. Dennis and Robert B. Schnabel. </author> <title> A view of unconstrained optimization. </title> <editor> In G. L. Nemhauser, editor, </editor> <booktitle> Handbooks in Operations Research and Mathematical Software, </booktitle> <volume> volume 1, </volume> <pages> pages 1-72. </pages> <publisher> Elsevier, </publisher> <year> 1989. </year>
Reference: [14] <author> G. Di Pillo and L. Grippo. </author> <title> An exact penalty method with global convergence properties for nonlinear programming problems. </title> <journal> Math. Programming, </journal> <volume> 36 </volume> <pages> 1-18, </pages> <year> 1986. </year>
Reference-contexts: Another interesting optimization technique that involves selected second-derivative information is the use of merit functions that are both smooth and exact <ref> [14] </ref>. That is, that obtain unconstrained minima exactly at the constraint minimizers. Williamson uses exact Hessian values in a nonlinear programming algorithm [31]. Secant methods for approximating second derivatives have been quite successful in the context of unconstrained optimization.
Reference: [15] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <editor> In M. Iri and K. Tanabe, editors, </editor> <booktitle> Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83-108. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: It is neither symbolic nor based on potentially unstable finite-difference approximations. It propagates values according to the familiar rules of calculus. There are two fundamental modes for propagating derivative values: the forward mode that we use here, and the reverse mode (see <ref> [15] </ref>). One could use the forward mode of automatic differentiation to compute the gradient and the dense Hessian of f by propagating the first- and second-derivative objects strictly in the forward mode [25]. <p> The alternative of reverse mode propagation of adjoint values <ref> [15] </ref> is attractive for computing gradients, but for the highly structured Hessians and higher-order derivatives, the forward mode is satisfactory. 5 4 Interpolating Derivatives from Taylor Series The two central ideas of this paper are described in this section and the next.
Reference: [16] <author> Andreas Griewank. </author> <title> The chain rule revisited in scientific computing. </title> <journal> SIAM News, </journal> <volume> 24, </volume> <month> May & July </month> <year> 1991. </year> <note> No. 3, </note> <editor> p. </editor> <volume> 20 & No. 4, </volume> <editor> p. </editor> <volume> 8. </volume>
Reference: [17] <author> Andreas Griewank and George F. Corliss, </author> <title> editors. Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference: [18] <author> Andreas Griewank, David Juedes, Jay Srinivasan, and Charles Tyner. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <journal> ACM Trans. Math. Software, </journal> <note> to appear. Also appeared as Preprint MCS-P180-1190, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1990. </year>
Reference-contexts: For first derivatives, this partitioning approach can be adopted quite easily, for example, by utilizing the interface of ADIFOR (Automatic Differentiation In FORtran) [7] or ADOL-C (Automatic Differentiation Of aLgorithms written in C) <ref> [18] </ref>. Each pair of independent variables that corresponds to a nonzero entry in the Hessian must occur together in at least one of the groups.
Reference: [19] <author> Willi Hock and Klaus Schittkowski. </author> <title> Test Examples for Nonlinear Programming Codes, </title> <booktitle> volume 187 of Lecture Notes in Economics and Mathematical Systems. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Writing code for the analytic second derivative is a tedious and error-prone process, even for problems of modest code size like those in the Hock and Schittkowski <ref> [19] </ref> or the MINPACK-2 [3] test suites. For applications such as multidisciplinary optimization [27,28] where the code defining the function f may be tens of thousands of lines long, hand-coding even rf is unthinkable.
Reference: [20] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 315-329. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Discussions of automatic differentiation for computing first-order partial derivatives and Taylor coefficients of arbitrary order have appeared in the literature regularly over the past 30 years [5,15, 16,23,24,30]. Juedes <ref> [20] </ref> includes a survey of 29 software packages for automatic differentiation. In this paper, we describe two concepts: 1. interpolation of partial derivatives from an ensemble of Taylor series of single independents, and 2. preaccumulation of "local" derivatives at the statement or scalar function level.
Reference: [21] <author> Harriet Kagiwada, Robert Kalaba, Nima Rasakhoo, and Spingarn Karl. </author> <title> Numerical Derivatives and Nonlinear Analysis, </title> <booktitle> volume 31 of Mathematical Concepts and Methods in Science and Engineering. </booktitle> <publisher> Plenum Press, Inc., </publisher> <address> New York, </address> <year> 1985. </year>
Reference: [22] <author> Leo Michelotti. MXYZPTLK: </author> <title> A C++ hacker's implementation of automatic differentiation. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 218-227. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Third-, fourth-, and higher-order partial derivatives are required by some algorithms for solving potentially degenerate nonlinear systems or for nested optimization problems. Berz [6] and Mich-elotti <ref> [22] </ref> discussed the problem of beam tracing in the Superconducting Super Collider. In that application, up to m = 10 derivatives in n = 6 variables are needed to describe the physical system.
Reference: [23] <author> Ramon E. Moore. </author> <title> Interval Analysis. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1966. </year>
Reference-contexts: A discussion of univariate Taylor series demonstrates that in this restricted context, the computation of derivatives of arbitrarily high order is well understood (see <ref> [23] </ref> or [24], for example). The interpolation scheme described below shows how arbitrary partial derivatives of a function of many independent variables can be efficiently computed from the much simpler univariate Taylor series case. <p> The forward propagation of truncated Taylor series with m terms requires O (m) arithmetic operations and memory accesses for each addition or subtraction operation and (m + 1)(m+2)=2 arithmetic operations for each multiplication, division, or special function evaluation <ref> [23, 24] </ref>. We may use the factor of (m + 1)(m + 2)=2 as a measure of the cost ratio between the forward propagation of m-term series and as a means for evaluating the underlying function. <p> S 0 = Y 0 W 0 = S 0 =T 0 T i = j=0 W i = @ S i j=1 1 12 The appropriate recurrence relations for each operation or elementary function are given by Moore <ref> [23] </ref> or by Rall [24]. Multiplication of two polynomials of degree m (or, equivalently, the convolution of their coefficients) is the central workhorse of all higher-derivative calculations [11], including the multivariate case and the reverse mode.
Reference: [24] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year> <month> 16 </month>
Reference-contexts: A discussion of univariate Taylor series demonstrates that in this restricted context, the computation of derivatives of arbitrarily high order is well understood (see [23] or <ref> [24] </ref>, for example). The interpolation scheme described below shows how arbitrary partial derivatives of a function of many independent variables can be efficiently computed from the much simpler univariate Taylor series case. <p> The forward propagation of truncated Taylor series with m terms requires O (m) arithmetic operations and memory accesses for each addition or subtraction operation and (m + 1)(m+2)=2 arithmetic operations for each multiplication, division, or special function evaluation <ref> [23, 24] </ref>. We may use the factor of (m + 1)(m + 2)=2 as a measure of the cost ratio between the forward propagation of m-term series and as a means for evaluating the underlying function. <p> S 0 = Y 0 W 0 = S 0 =T 0 T i = j=0 W i = @ S i j=1 1 12 The appropriate recurrence relations for each operation or elementary function are given by Moore [23] or by Rall <ref> [24] </ref>. Multiplication of two polynomials of degree m (or, equivalently, the convolution of their coefficients) is the central workhorse of all higher-derivative calculations [11], including the multivariate case and the reverse mode.
Reference: [25] <author> Louis B. Rall. </author> <title> Differentiation in Pascal-SC: Type GRADIENT. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10(2) </volume> <pages> 161-184, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: One could use the forward mode of automatic differentiation to compute the gradient and the dense Hessian of f by propagating the first- and second-derivative objects strictly in the forward mode <ref> [25] </ref>. We describe how this would be done to show that the combination of preaccumulation and interpolation yields much more efficient code. Suppose that u and v are active variables (they depend on values of independent variables).
Reference: [26] <author> Louis B. Rall. </author> <title> Point and interval differentiation arithmetics. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 17-24. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: The interpolation scheme for second-order partial derivatives is a special case of an interpolation scheme for arbitrarily high-order mixed partial derivatives, thus providing a natural growth path for any software tool based on this method. This treatment was inspired by a remark by Rall <ref> [26] </ref>. In the following section, we show how the preaccumulation of local derivatives complements the interpolation scheme, yielding an efficient method for computing second-order partial derivatives. Let us consider the example that we have two independent variables x and y.
Reference: [27] <author> Greg R. Shubin. </author> <title> Obtaining "cheap" optimization gradients from computational aerodynamics codes. </title> <institution> Applied Mathematics and Statistics Technical Report AMS-TR-164, Boeing Computer Services, </institution> <month> June </month> <year> 1991. </year>
Reference: [28] <author> Greg R. Shubin and P. D. Frank. </author> <title> A comparison of two closely-related approaches to aerodynamic design optimization. </title> <editor> In G. S. Dulikravich, editor, </editor> <booktitle> Proceedings of the Third International Conference on Inverse Design Concepts and Optimization in Engineering Sciences, </booktitle> <address> Washington, D.C., </address> <month> Oct. </month> <pages> 23-25, </pages> <year> 1991, 1991. </year>
Reference: [29] <author> Leigh Tesfatsion. </author> <title> Automatic evaluation of higher-order partial derivatives for nonlocal sensitivity analysis. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 157-165. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference: [30] <author> R. E. Wengert. </author> <title> A simple automatic derivative evaluation program. </title> <journal> Comm. ACM, </journal> <volume> 7(8) </volume> <pages> 463-464, </pages> <year> 1964. </year>
Reference: [31] <author> Karen A. Williamson. </author> <title> A Robust Trust Region Algorithm for Nonlinear Programming. </title> <type> PhD thesis, </type> <institution> Rice University, Department of Mathematical Sciences, </institution> <year> 1990. </year> <note> Technical Report TR90-22. </note>
Reference-contexts: Another interesting optimization technique that involves selected second-derivative information is the use of merit functions that are both smooth and exact [14]. That is, that obtain unconstrained minima exactly at the constraint minimizers. Williamson uses exact Hessian values in a nonlinear programming algorithm <ref> [31] </ref>. Secant methods for approximating second derivatives have been quite successful in the context of unconstrained optimization. <p> For applications such as multidisciplinary optimization [27,28] where the code defining the function f may be tens of thousands of lines long, hand-coding even rf is unthinkable. Software for constrained optimization has viewed analytic second-derivative information as so difficult to supply that only Williamson's code <ref> [31] </ref> even provides the opportunity for a knowledgeable user to supply it. The techniques outlined in this paper make the computation of accurate second-derivative information feasible.
References-found: 31

