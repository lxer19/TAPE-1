URL: http://charm.cs.uiuc.edu/manuals/mpi-20.ps.Z
Refering-URL: http://charm.cs.uiuc.edu/manuals/
Root-URL: http://www.cs.uiuc.edu
Title: MPI-2: Extensions to the Message-Passing Interface Message Passing Interface  
Note: This work was supported in part by NSF and DARPA under NSF contract CDA-9115428 and Esprit under project HPC Standards (21111).  
Date: July 18, 1997  
Pubnum: Forum  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: C_ROUTINE (TYPE) /* C code */ void C_ROUTINE (MPI_Fint *ftype) - int count = 5; int lens [2] = -1,1-; MPI_Aint displs [2]; MPI_Datatype types [2], newtype; /* create an absolute datatype for buffer that consists */ /* of count, followed by R (5) */ MPI_Get_address (&count, &displs [0]); displs <ref> [1] </ref> = 0; types [0] = MPI_INT; types [1] = MPI_Type_f2c (*ftype); MPI_Type_create_struct (2, lens, displs, types, &newtype); MPI_Type_commit (&newtype); MPI_Send (MPI_BOTTOM, 1, newtype, 1, 0, MPI_COMM_WORLD); 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 <p> (MPI_Fint *ftype) - int count = 5; int lens [2] = -1,1-; MPI_Aint displs [2]; MPI_Datatype types [2], newtype; /* create an absolute datatype for buffer that consists */ /* of count, followed by R (5) */ MPI_Get_address (&count, &displs [0]); displs <ref> [1] </ref> = 0; types [0] = MPI_INT; types [1] = MPI_Type_f2c (*ftype); MPI_Type_create_struct (2, lens, displs, types, &newtype); MPI_Type_commit (&newtype); MPI_Send (MPI_BOTTOM, 1, newtype, 1, 0, MPI_COMM_WORLD); 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 56 CHAPTER 4. <p> arguments "-gridfile" and "ocean1.grd" in C: char command [] = "ocean"; char *argv [] = -"-gridfile", "ocean1.grd", NULL-; MPI_Comm_spawn (command, argv, ...); or, if not everything is known at compile time: char *command; char **argv; command = "ocean"; argv=(char **)malloc (3 * sizeof (char *)); argv [0] = "-gridfile"; argv <ref> [1] </ref> = "ocean1.grd"; argv [2] = NULL; MPI_Comm_spawn (command, argv, ...); In Fortran: CHARACTER*25 command, argv (3) command = ' ocean ' argv (1) = ' -gridfile ' argv (2) = ' ocean1.grd' argv (3) = ' ' call MPI_COMM_SPAWN (command, argv, ...) 1 3 5 7 9 11 13 15 <p> In C, the MPI COMM SPAWN argument argv differs from the argv argument of main in two respects. First, it is shifted by one element. Specifically, argv [0] of main is provided by the implementation and conventionally contains the name of the program (given by command). argv <ref> [1] </ref> of main corresponds to argv [0] in MPI COMM SPAWN, argv [2] of main to argv [1] of MPI COMM SPAWN, etc. Second, argv of MPI COMM SPAWN must be null-terminated, so that its length can be determined. <p> First, it is shifted by one element. Specifically, argv [0] of main is provided by the implementation and conventionally contains the name of the program (given by command). argv <ref> [1] </ref> of main corresponds to argv [0] in MPI COMM SPAWN, argv [2] of main to argv [1] of MPI COMM SPAWN, etc. Second, argv of MPI COMM SPAWN must be null-terminated, so that its length can be determined. <p> and Fortran To run the program "ocean" with arguments "-gridfile" and "ocean1.grd" and the program "atmos" with argument "atmos.grd" in C: char *array_of_commands [2] = -"ocean", "atmos"-; char **array_of_argv [2]; char *argv0 [] = -"-gridfile", "ocean1.grd", (char *)0-; char *argv1 [] = -"atmos.grd", (char *)0-; array_of_argv [0] = argv0; array_of_argv <ref> [1] </ref> = argv1; MPI_Comm_spawn_multiple (2, array_of_commands, array_of_argv, ...); Here's how you do it in Fortran: CHARACTER*25 commands (2), array_of_argv (2, 3) commands (1) = ' ocean ' array_of_argv (1, 1) = ' -gridfile ' array_of_argv (1, 2) = ' ocean1.grd' array_of_argv (1, 3) = ' ' commands (2) = ' atmos <p> 2: /* do something */ ... default: /* Unexpected message type */ MPI_Abort ( MPI_COMM_WORLD, 1 ); - - Here is the client. #include "mpi.h" int main ( int argc, char **argv ) - MPI_Comm server; double buf [MAX_DATA]; char port_name [MPI_MAX_PORT_NAME]; MPI_Init ( &argc, &argv ); strcpy (port_name, argv <ref> [1] </ref> );/* assume server's name is cmd-line arg */ MPI_Comm_connect ( port_name, MPI_INFO_NULL, 0, MPI_COMM_WORLD, &server ); while (!done) - tag = 2; /* Action to perform */ MPI_Send ( buf, n, MPI_DOUBLE, 0, tag, server ); /* etc */ MPI_Send ( buf, 0, MPI_DOUBLE, 0, 1, server ); MPI_Comm_disconnect ( <p> MPI_Irecv (&lval, 1, MPI_INT, lchild, args-&gt;tag, args-&gt;comm, &req [0]); MPI_Irecv (&rval, 1, MPI_INT, rchild, args-&gt;tag, args-&gt;comm, &req <ref> [1] </ref>); 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 8.3. <p> If combiner is MPI COMBINER VECTOR then Constructor argument C & C++ location Fortran location count i [0] I (1) blocklength i <ref> [1] </ref> I (2) stride i [2] I (3) oldtype d [0] D (1) 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 190 CHAPTER 8. <p> EXTERNAL INTERFACES and ni = 3, na = 0, nd = 1. If combiner is MPI COMBINER HVECTOR INTEGER or MPI COMBINER HVECTOR then Constructor argument C & C++ location Fortran location count i [0] I (1) blocklength i <ref> [1] </ref> I (2) stride a [0] A (1) oldtype d [0] D (1) and ni = 2, na = 1, nd = 1. If combiner is MPI COMBINER INDEXED then Constructor argument C & C++ location Fortran location count i [0] I (1) array of blocklengths i [1] to i [i <p> (1) blocklength i <ref> [1] </ref> I (2) stride a [0] A (1) oldtype d [0] D (1) and ni = 2, na = 1, nd = 1. If combiner is MPI COMBINER INDEXED then Constructor argument C & C++ location Fortran location count i [0] I (1) array of blocklengths i [1] to i [i [0]] I (2) to I (I (1)+1) array of displacements i [i [0]+1] to i [2*i [0]] I (I (1)+2) to I (2*I (1)+1) oldtype d [0] D (1) and ni = 2*count+1, na = 0, nd = 1. <p> If combiner is MPI COMBINER HINDEXED INTEGER or MPI COMBINER HINDEXED then Constructor argument C & C++ location Fortran location count i [0] I (1) array of blocklengths i <ref> [1] </ref> to i [i [0]] I (2) to I (I (1)+1) array of displacements a [0] to a [i [0]-1] A (1) to A (I (1)) oldtype d [0] D (1) and ni = count+1, na = count, nd = 1. <p> If combiner is MPI COMBINER INDEXED BLOCK then Constructor argument C & C++ location Fortran location count i [0] I (1) blocklength i <ref> [1] </ref> I (2) array of displacements i [2] to i [i [0]+1] I (3) to I (I (1)+2) oldtype d [0] D (1) and ni = count+2, na = 0, nd = 1. <p> If combiner is MPI COMBINER STRUCT INTEGER or MPI COMBINER STRUCT then Constructor argument C & C++ location Fortran location count i [0] I (1) array of blocklengths i <ref> [1] </ref> to i [i [0]] I (2) to I (I (1)+1) array of displacements a [0] to a [i [0]-1] A (1) to A (I (1)) array of types d [0] to d [i [0]-1] D (1) to D (I (1)) and ni = count+1, na = count, nd = count. <p> If combiner is MPI COMBINER SUBARRAY then Constructor argument C & C++ location Fortran location ndims i [0] I (1) array of sizes i <ref> [1] </ref> to i [i [0]] I (2) to I (I (1)+1) array of subsizes i [i [0]+1] to i [2*i [0]] I (I (1)+2) to I (2*I (1)+1) array of starts i [2*i [0]+1] to i [3*i [0]] I (2*I (1)+2) to I (3*I (1)+1) order i [3*i [0]+1] I (3*I <p> DECODING A DATATYPE 191 and ni = 3*ndims+2, na = 0, nd = 1. If combiner is MPI COMBINER DARRAY then Constructor argument C & C++ location Fortran location size i [0] I (1) rank i <ref> [1] </ref> I (2) ndims i [2] I (3) array of gsizes i [3] to i [i [2]+2] I (4) to I (I (3)+3) array of distribs i [i [2]+3] to i [2*i [2]+2] I (I (3)+4) to I (2*I (3)+3) array of dargs i [2*i [2]+3] to i [3*i [2]+2] I <p> If combiner is MPI COMBINER F90 INTEGER then Constructor argument C & C++ location Fortran location r i [0] I (1) If combiner is MPI COMBINER RESIZED then Constructor argument C & C++ location Fortran location lb a [0] A (1) extent a <ref> [1] </ref> A (2) oldtype d [0] D (1) and ni = 0, na = 2, nd = 1. Example 8.2 This example shows how a datatype can be decoded. The routine printdatatype prints out the elements of the datatype. <p> The significant optimizations required for efficiency (e.g., grouping [15], collective buffering <ref> [1, 2, 16, 19, 22] </ref>, and disk-directed I/O [13]) can only be implemented if the parallel I/O system provides a high-level interface supporting partitioning of file data among processes and a collective interface supporting complete transfers of global data structures between process memories and files. <p> TRUE=1; MPI_File_open ( MPI_COMM_WORLD, "myfile", MPI_MODE_RDWR, MPI_INFO_NULL, &fh ) ; MPI_File_set_view ( fh, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; /* MPI_File_set_atomicity ( fh, TRUE ) ; Use this to set atomic mode. */ MPI_File_iwrite_at (fh, 10, &a, 1, MPI_INT, &reqs [0]) ; MPI_File_iread_at (fh, 10, &b, 1, MPI_INT, &reqs <ref> [1] </ref>) ; MPI_Waitall (2, reqs, statuses) ; For asynchronous data access operations, MPI specifies that the access occurs at any time between the call to the asynchronous data access routine and the return from the corresponding request complete routine. <p> b; MPI_File_open ( MPI_COMM_WORLD, "myfile", MPI_MODE_RDWR, MPI_INFO_NULL, &fh ) ; MPI_File_set_view ( fh, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; /* MPI_File_set_atomicity ( fh, TRUE ) ; Use this to set atomic mode. */ MPI_File_iwrite_at (fh, 10, &a, 1, MPI_INT, &reqs [0]) ; MPI_File_iread_at (fh, 10, &b, 1, MPI_INT, &reqs <ref> [1] </ref>) ; MPI_Wait (&reqs [0], &status) ; MPI_Wait (&reqs [1], &status) ; If atomic mode is set, either 2 or 4 will be read into b. Again, MPI does not guarantee sequential consistency in nonatomic mode. <p> ; MPI_File_set_view ( fh, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; /* MPI_File_set_atomicity ( fh, TRUE ) ; Use this to set atomic mode. */ MPI_File_iwrite_at (fh, 10, &a, 1, MPI_INT, &reqs [0]) ; MPI_File_iread_at (fh, 10, &b, 1, MPI_INT, &reqs <ref> [1] </ref>) ; MPI_Wait (&reqs [0], &status) ; MPI_Wait (&reqs [1], &status) ; If atomic mode is set, either 2 or 4 will be read into b. Again, MPI does not guarantee sequential consistency in nonatomic mode. <p> hand, the following code fragment: int a = 4, b; MPI_File_open ( MPI_COMM_WORLD, "myfile", MPI_MODE_RDWR, MPI_INFO_NULL, &fh ) ; MPI_File_set_view ( fh, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_iwrite_at (fh, 10, &a, 1, MPI_INT, &reqs [0]) ; MPI_Wait (&reqs [0], &status) ; MPI_File_iread_at (fh, 10, &b, 1, MPI_INT, &reqs <ref> [1] </ref>) ; MPI_Wait (&reqs [1], &status) ; defines the same ordering as: int a = 4, b; MPI_File_open ( MPI_COMM_WORLD, "myfile", MPI_MODE_RDWR, MPI_INFO_NULL, &fh ) ; MPI_File_set_view ( fh, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_write_at (fh, 10, &a, 1, MPI_INT, &status ) ; MPI_File_read_at (fh, 10, &b, 1, MPI_INT, <p> fragment: int a = 4, b; MPI_File_open ( MPI_COMM_WORLD, "myfile", MPI_MODE_RDWR, MPI_INFO_NULL, &fh ) ; MPI_File_set_view ( fh, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_iwrite_at (fh, 10, &a, 1, MPI_INT, &reqs [0]) ; MPI_Wait (&reqs [0], &status) ; MPI_File_iread_at (fh, 10, &b, 1, MPI_INT, &reqs <ref> [1] </ref>) ; MPI_Wait (&reqs [1], &status) ; defines the same ordering as: int a = 4, b; MPI_File_open ( MPI_COMM_WORLD, "myfile", MPI_MODE_RDWR, MPI_INFO_NULL, &fh ) ; MPI_File_set_view ( fh, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_write_at (fh, 10, &a, 1, MPI_INT, &status ) ; MPI_File_read_at (fh, 10, &b, 1, MPI_INT, &status ) ; Since <p> To create the filetypes for each process one could use the following C program: double subarray [100][25]; MPI_Datatype filetype; int sizes [2], subsizes [2], starts [2]; int rank; MPI_Comm_rank (MPI_COMM_WORLD, &rank); sizes [0]=100; sizes <ref> [1] </ref>=100; subsizes [0]=100; subsizes [1]=25; starts [0]=0; starts [1]=rank*subsizes [1]; MPI_Type_create_subarray (2, sizes, subsizes, starts, MPI_ORDER_C, MPI_DOUBLE, &filetype); Or, equivalently in Fortran: double precision subarray (100,25) integer filetype, rank, ierror integer sizes (2), subsizes (2), starts (2) call MPI_COMM_RANK (MPI_COMM_WORLD, rank, ierror) sizes (1)=100 sizes (2)=100 subsizes (1)=100 subsizes (2)=25 starts (1)=0 starts (2)=rank*subsizes (2) call MPI_TYPE_CREATE_SUBARRAY (2, sizes, subsizes,
Reference: [2] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: CODE REAL R (5) INTEGER TYPE, IERR INTEGER (KIND=MPI_ADDRESS_KIND) ADDR ! create an absolute datatype for array R CALL MPI_GET_ADDRESS ( R, ADDR, IERR) CALL MPI_TYPE_CREATE_STRUCT (1, 5, ADDR, MPI_REAL, TYPE, IERR) CALL C_ROUTINE (TYPE) /* C code */ void C_ROUTINE (MPI_Fint *ftype) - int count = 5; int lens <ref> [2] </ref> = -1,1-; MPI_Aint displs [2]; MPI_Datatype types [2], newtype; /* create an absolute datatype for buffer that consists */ /* of count, followed by R (5) */ MPI_Get_address (&count, &displs [0]); displs [1] = 0; types [0] = MPI_INT; types [1] = MPI_Type_f2c (*ftype); MPI_Type_create_struct (2, lens, displs, types, &newtype); <p> TYPE, IERR INTEGER (KIND=MPI_ADDRESS_KIND) ADDR ! create an absolute datatype for array R CALL MPI_GET_ADDRESS ( R, ADDR, IERR) CALL MPI_TYPE_CREATE_STRUCT (1, 5, ADDR, MPI_REAL, TYPE, IERR) CALL C_ROUTINE (TYPE) /* C code */ void C_ROUTINE (MPI_Fint *ftype) - int count = 5; int lens <ref> [2] </ref> = -1,1-; MPI_Aint displs [2]; MPI_Datatype types [2], newtype; /* create an absolute datatype for buffer that consists */ /* of count, followed by R (5) */ MPI_Get_address (&count, &displs [0]); displs [1] = 0; types [0] = MPI_INT; types [1] = MPI_Type_f2c (*ftype); MPI_Type_create_struct (2, lens, displs, types, &newtype); MPI_Type_commit (&newtype); MPI_Send (MPI_BOTTOM, 1, <p> (KIND=MPI_ADDRESS_KIND) ADDR ! create an absolute datatype for array R CALL MPI_GET_ADDRESS ( R, ADDR, IERR) CALL MPI_TYPE_CREATE_STRUCT (1, 5, ADDR, MPI_REAL, TYPE, IERR) CALL C_ROUTINE (TYPE) /* C code */ void C_ROUTINE (MPI_Fint *ftype) - int count = 5; int lens <ref> [2] </ref> = -1,1-; MPI_Aint displs [2]; MPI_Datatype types [2], newtype; /* create an absolute datatype for buffer that consists */ /* of count, followed by R (5) */ MPI_Get_address (&count, &displs [0]); displs [1] = 0; types [0] = MPI_INT; types [1] = MPI_Type_f2c (*ftype); MPI_Type_create_struct (2, lens, displs, types, &newtype); MPI_Type_commit (&newtype); MPI_Send (MPI_BOTTOM, 1, newtype, 1, 0, <p> in C: char command [] = "ocean"; char *argv [] = -"-gridfile", "ocean1.grd", NULL-; MPI_Comm_spawn (command, argv, ...); or, if not everything is known at compile time: char *command; char **argv; command = "ocean"; argv=(char **)malloc (3 * sizeof (char *)); argv [0] = "-gridfile"; argv [1] = "ocean1.grd"; argv <ref> [2] </ref> = NULL; MPI_Comm_spawn (command, argv, ...); In Fortran: CHARACTER*25 command, argv (3) command = ' ocean ' argv (1) = ' -gridfile ' argv (2) = ' ocean1.grd' argv (3) = ' ' call MPI_COMM_SPAWN (command, argv, ...) 1 3 5 7 9 11 13 15 17 19 21 23 <p> First, it is shifted by one element. Specifically, argv [0] of main is provided by the implementation and conventionally contains the name of the program (given by command). argv [1] of main corresponds to argv [0] in MPI COMM SPAWN, argv <ref> [2] </ref> of main to argv [1] of MPI COMM SPAWN, etc. Second, argv of MPI COMM SPAWN must be null-terminated, so that its length can be determined. <p> Error codes are treated as for MPI COMM SPAWN. Example 5.2 Examples of array of argv in C and Fortran To run the program "ocean" with arguments "-gridfile" and "ocean1.grd" and the program "atmos" with argument "atmos.grd" in C: char *array_of_commands <ref> [2] </ref> = -"ocean", "atmos"-; char **array_of_argv [2]; char *argv0 [] = -"-gridfile", "ocean1.grd", (char *)0-; char *argv1 [] = -"atmos.grd", (char *)0-; array_of_argv [0] = argv0; array_of_argv [1] = argv1; MPI_Comm_spawn_multiple (2, array_of_commands, array_of_argv, ...); Here's how you do it in Fortran: CHARACTER*25 commands (2), array_of_argv (2, 3) commands (1) = <p> Error codes are treated as for MPI COMM SPAWN. Example 5.2 Examples of array of argv in C and Fortran To run the program "ocean" with arguments "-gridfile" and "ocean1.grd" and the program "atmos" with argument "atmos.grd" in C: char *array_of_commands <ref> [2] </ref> = -"ocean", "atmos"-; char **array_of_argv [2]; char *argv0 [] = -"-gridfile", "ocean1.grd", (char *)0-; char *argv1 [] = -"atmos.grd", (char *)0-; array_of_argv [0] = argv0; array_of_argv [1] = argv1; MPI_Comm_spawn_multiple (2, array_of_commands, array_of_argv, ...); Here's how you do it in Fortran: CHARACTER*25 commands (2), array_of_argv (2, 3) commands (1) = ' ocean ' array_of_argv (1, 1) <p> args-&gt;valout = valout; args-&gt;request = *request; /* spawn thread to handle request */ /* The availability of the pthread_create call is system dependent */ pthread_create (&thread, NULL, reduce_thread, args); return MPI_SUCCESS; - /* thread code */ void reduce_thread (void *ptr) - int lchild, rchild, parent, lval, rval, val; MPI_Request req <ref> [2] </ref>; ARGS *args; args = (ARGS*)ptr; /* compute left,right child and parent in tree; set to MPI_PROC_NULL if does not exist */ /* code not shown */ ... <p> If combiner is MPI COMBINER VECTOR then Constructor argument C & C++ location Fortran location count i [0] I (1) blocklength i [1] I (2) stride i <ref> [2] </ref> I (3) oldtype d [0] D (1) 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 190 CHAPTER 8. EXTERNAL INTERFACES and ni = 3, na = 0, nd = 1. <p> If combiner is MPI COMBINER INDEXED BLOCK then Constructor argument C & C++ location Fortran location count i [0] I (1) blocklength i [1] I (2) array of displacements i <ref> [2] </ref> to i [i [0]+1] I (3) to I (I (1)+2) oldtype d [0] D (1) and ni = count+2, na = 0, nd = 1. <p> DECODING A DATATYPE 191 and ni = 3*ndims+2, na = 0, nd = 1. If combiner is MPI COMBINER DARRAY then Constructor argument C & C++ location Fortran location size i [0] I (1) rank i [1] I (2) ndims i <ref> [2] </ref> I (3) array of gsizes i [3] to i [i [2]+2] I (4) to I (I (3)+3) array of distribs i [i [2]+3] to i [2*i [2]+2] I (I (3)+4) to I (2*I (3)+3) array of dargs i [2*i [2]+3] to i [3*i [2]+2] I (2*I (3)+4) to I (3*I <p> The significant optimizations required for efficiency (e.g., grouping [15], collective buffering <ref> [1, 2, 16, 19, 22] </ref>, and disk-directed I/O [13]) can only be implemented if the parallel I/O system provides a high-level interface supporting partitioning of file data among processes and a collective interface supporting complete transfers of global data structures between process memories and files. <p> EXAMPLES 269 process 0 has columns 0-24, process 1 has columns 25-49, etc.; see Figure 9.4). To create the filetypes for each process one could use the following C program: double subarray [100][25]; MPI_Datatype filetype; int sizes <ref> [2] </ref>, subsizes [2], starts [2]; int rank; MPI_Comm_rank (MPI_COMM_WORLD, &rank); sizes [0]=100; sizes [1]=100; subsizes [0]=100; subsizes [1]=25; starts [0]=0; starts [1]=rank*subsizes [1]; MPI_Type_create_subarray (2, sizes, subsizes, starts, MPI_ORDER_C, MPI_DOUBLE, &filetype); Or, equivalently in Fortran: double precision subarray (100,25) integer filetype, rank, ierror integer sizes (2), subsizes (2), starts (2) call <p> EXAMPLES 269 process 0 has columns 0-24, process 1 has columns 25-49, etc.; see Figure 9.4). To create the filetypes for each process one could use the following C program: double subarray [100][25]; MPI_Datatype filetype; int sizes <ref> [2] </ref>, subsizes [2], starts [2]; int rank; MPI_Comm_rank (MPI_COMM_WORLD, &rank); sizes [0]=100; sizes [1]=100; subsizes [0]=100; subsizes [1]=25; starts [0]=0; starts [1]=rank*subsizes [1]; MPI_Type_create_subarray (2, sizes, subsizes, starts, MPI_ORDER_C, MPI_DOUBLE, &filetype); Or, equivalently in Fortran: double precision subarray (100,25) integer filetype, rank, ierror integer sizes (2), subsizes (2), starts (2) call MPI_COMM_RANK (MPI_COMM_WORLD, <p> EXAMPLES 269 process 0 has columns 0-24, process 1 has columns 25-49, etc.; see Figure 9.4). To create the filetypes for each process one could use the following C program: double subarray [100][25]; MPI_Datatype filetype; int sizes <ref> [2] </ref>, subsizes [2], starts [2]; int rank; MPI_Comm_rank (MPI_COMM_WORLD, &rank); sizes [0]=100; sizes [1]=100; subsizes [0]=100; subsizes [1]=25; starts [0]=0; starts [1]=rank*subsizes [1]; MPI_Type_create_subarray (2, sizes, subsizes, starts, MPI_ORDER_C, MPI_DOUBLE, &filetype); Or, equivalently in Fortran: double precision subarray (100,25) integer filetype, rank, ierror integer sizes (2), subsizes (2), starts (2) call MPI_COMM_RANK (MPI_COMM_WORLD, rank, ierror)
Reference: [3] <author> Margaret A. Ellis and Bjarne Stroustrup. </author> <title> The Annotated C++ Reference Manual. </title> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: DECODING A DATATYPE 191 and ni = 3*ndims+2, na = 0, nd = 1. If combiner is MPI COMBINER DARRAY then Constructor argument C & C++ location Fortran location size i [0] I (1) rank i [1] I (2) ndims i [2] I (3) array of gsizes i <ref> [3] </ref> to i [i [2]+2] I (4) to I (I (3)+3) array of distribs i [i [2]+3] to i [2*i [2]+2] I (I (3)+4) to I (2*I (3)+3) array of dargs i [2*i [2]+3] to i [3*i [2]+2] I (2*I (3)+4) to I (3*I (3)+3) array of psizes i [3*i [2]+3]
Reference: [4] <author> C++ Forum. </author> <title> Working paper for draft proposed international standard for information systems | programming language C++. </title> <type> Technical report, </type> <institution> American National Standards Institute, </institution> <year> 1995. </year>
Reference: [5] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface standard. </title> <journal> The International Journal of Supercomputer Applications and High Performance Computing, </journal> <volume> 8, </volume> <year> 1994. </year>
Reference-contexts: Introduction to MPI-2 1.1 Background Beginning in March 1995, the MPI Forum began meeting to consider corrections and extensions to the original MPI Standard document <ref> [5] </ref>. The first product of these deliberations was Version 1.1 of the MPI specification, released in June of 1995 (see http://www.mpi-forum.org for official MPI document releases). Since that time, effort has been focused in five types of areas. 1. Further corrections and clarifications for the MPI-1.1 document. 2.
Reference: [6] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface standard (version 1.1). </title> <type> Technical report, </type> <year> 1995. </year> <note> http://www.mpi-forum.org. </note>
Reference-contexts: The datatype passed to the routine must be a committed datatype. The layout of data in memory corresponding to buf, count, datatype is interpreted the same way as in MPI-1 communication functions; see Section 3.12.5 in <ref> [6] </ref>. The data is accessed from those parts of the file specified by the current view (Section 9.3, page 221). The type signature of datatype must match the type signature of some number of contiguous copies of the etype of the current view. <p> For a complete discussion, please refer to the semantics set forth in MPI-1 <ref> [6] </ref>, Section 4.12. Collective file operations are collective over a dup of the communicator used to open the file|this duplicate communicator is implicitly specified via the file handle argument. <p> Declarations (which apply to all MPI member classes) for construction, destruction, copying, assignment, comparison, and mixed-language operability are also provided. To maintain consistency with what has gone before, the binding definitions are given in the same order as given for the C bindings in <ref> [6] </ref>. Except where indicated, all non-static member functions (except for constructors and the assignment operator) of MPI member classes are virtual functions. Rationale. Providing virtual member functions is an important part of design for inheritance.
Reference: [7] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Bob Manchek, and Vaidy Sunderam. </author> <title> PVM: Parallel Virtual Machine|A User's Guide and Tutorial for Network Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: MPI users have asked that the MPI-1 model be extended to allow process creation and management after an MPI application has been started. A major impetus comes from the PVM <ref> [7] </ref> research effort, which has provided a wealth of experience with process management and resource control that illustrates their benefits and potential pitfalls.
Reference: [8] <author> Michael Hennecke. </author> <title> A Fortran 90 interface to MPI version 1.1. </title> <type> Technical Report Internal Report 63/96, </type> <institution> Rechenzentrum, Universitat Karlsruhe, D-76128 Karlsruhe, Germany, </institution> <month> June </month> <year> 1996. </year> <note> Available via world wide web from http://www.uni-karlsruhe.de/~Michael.Hennecke/Publications/#MPI F90. </note>
Reference-contexts: Advice to implementors. In the case where the compiler does not generate errors, nothing needs to be done to the existing interface. In the case where the compiler may generate errors, a set of overloaded functions may be used. See the paper of M. Hennecke <ref> [8] </ref>. Even if the compiler does not generate errors, explicit interfaces for all routines would be useful for detecting errors in the argument list.
Reference: [9] <institution> Institute of Electrical and Electronics Engineers, </institution> <address> New York. </address> <note> IEEE Standard for Binary Floating-Point Arithmetic, ANSI/IEEE Standard 754-1985, </note> <year> 1985. </year>
Reference-contexts: I/O 9.5.2 External Data Representation: "external32" All MPI implementations are required to support the data representation defined in this section. Support of optional datatypes (e.g., MPI INTEGER2) is not required. All floating point values are in big-endian IEEE format <ref> [9] </ref> of the appropriate size. Floating point values are represented by one of three IEEE formats. These are the IEEE "Single," "Double," and "Double Extended" formats, requiring 4, 8 and 16 bytes of storage, respectively. <p> All signed numerals (e.g., MPI INT, MPI REAL) have the sign bit at the most significant bit. MPI COMPLEX and MPI DOUBLE COMPLEX have the sign bit of the real and imaginary parts at the most significant bit of each part. According to IEEE specifications <ref> [9] </ref>, the "NaN" (not a number) is system dependent. It should not be interpreted within MPI as anything other than "NaN." Advice to implementors.
Reference: [10] <author> International Organization for Standardization, </author> <title> Geneva. Information processing | 8-bit single-byte coded graphic character sets | Part 1: Latin alphabet No. </title> <type> 1, </type> <year> 1987. </year>
Reference-contexts: If we define a C procedure like this, void copyIntBuffer ( int *pin, int *pout, int len ) - int i; for (i=0; i&lt;len; ++i) *pout++ = *pin++; - then a call to it in the following code fragment has aliased arguments. int a <ref> [10] </ref>; copyIntBuffer ( a, a+3, 7); Although the C language allows this, such usage of MPI procedures is forbidden unless otherwise specified. Note that Fortran prohibits aliasing of arguments. All MPI functions are first specified in the language-independent notation. <p> Big-endian means most significant byte at lowest address byte. For Fortran LOGICAL and C++ bool, 0 implies false and nonzero implies true. Fortran COMPLEX and DOUBLE COMPLEX are represented by a pair of floating point format values for the real and imaginary components. Characters are in ISO 8859-1 format <ref> [10] </ref>. Wide characters (of type MPI WCHAR) are in Unicode format [23]. All signed numerals (e.g., MPI INT, MPI REAL) have the sign bit at the most significant bit. <p> For the code below, process 1 will read either 0 or 10 integers. If the latter, every element of b will be 5. If nonatomic mode is set, the results of the read are undefined. /* Process 0 */ int i, a <ref> [10] </ref> ; int TRUE = 1; for ( i=0;i&lt;10;i++) MPI_File_open ( MPI_COMM_WORLD, "workfile", MPI_MODE_RDWR | MPI_MODE_CREATE, MPI_INFO_NULL, &fh0 ) ; MPI_File_set_view ( fh0, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_set_atomicity ( fh0, TRUE ) ; MPI_File_write_at (fh0, 0, a, 10, MPI_INT, &status) ; /* MPI_Barrier ( MPI_COMM_WORLD ) ; */ <p> ( i=0;i&lt;10;i++) MPI_File_open ( MPI_COMM_WORLD, "workfile", MPI_MODE_RDWR | MPI_MODE_CREATE, MPI_INFO_NULL, &fh0 ) ; MPI_File_set_view ( fh0, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_set_atomicity ( fh0, TRUE ) ; MPI_File_write_at (fh0, 0, a, 10, MPI_INT, &status) ; /* MPI_Barrier ( MPI_COMM_WORLD ) ; */ /* Process 1 */ int b <ref> [10] </ref> ; int TRUE = 1; MPI_File_open ( MPI_COMM_WORLD, "workfile", MPI_MODE_RDWR | MPI_MODE_CREATE, MPI_INFO_NULL, &fh1 ) ; MPI_File_set_view ( fh1, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_set_atomicity ( fh1, TRUE ) ; /* MPI_Barrier ( MPI_COMM_WORLD ) ; */ MPI_File_read_at (fh1, 0, b, 10, MPI_INT, &status) ; 1 3 5 <p> In the example above, process 0 could use MPI SEND to send a 0 byte message, received by process 1 using MPI RECV. (End of advice to users.) Alternatively, a user can impose consistency with nonatomic mode set: /* Process 0 */ int i, a <ref> [10] </ref> ; a [i] = 5 ; MPI_File_open ( MPI_COMM_WORLD, "workfile", MPI_MODE_RDWR | MPI_MODE_CREATE, MPI_INFO_NULL, &fh0 ) ; MPI_File_set_view ( fh0, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_write_at (fh0, 0, a, 10, MPI_INT, &status ) ; MPI_File_sync ( fh0 ) ; MPI_Barrier ( MPI_COMM_WORLD ) ; MPI_File_sync ( fh0 ) <p> ( MPI_COMM_WORLD, "workfile", MPI_MODE_RDWR | MPI_MODE_CREATE, MPI_INFO_NULL, &fh0 ) ; MPI_File_set_view ( fh0, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_write_at (fh0, 0, a, 10, MPI_INT, &status ) ; MPI_File_sync ( fh0 ) ; MPI_Barrier ( MPI_COMM_WORLD ) ; MPI_File_sync ( fh0 ) ; /* Process 1 */ int b <ref> [10] </ref> ; MPI_File_open ( MPI_COMM_WORLD, "workfile", MPI_MODE_RDWR | MPI_MODE_CREATE, MPI_INFO_NULL, &fh1 ) ; MPI_File_set_view ( fh1, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_sync ( fh1 ) ; MPI_Barrier ( MPI_COMM_WORLD ) ; MPI_File_sync ( fh1 ) ; MPI_File_read_at (fh1, 0, b, 10, MPI_INT, &status ) ; The "sync-barrier-sync" construct is <p> is visible to all processes. (This does not affect process 0 in this example.) The following program represents an erroneous attempt to achieve consistency by eliminating the apparently superfluous second "sync" call for each process. /* ---------------- THIS EXAMPLE IS ERRONEOUS --------------- */ /* Process 0 */ int i, a <ref> [10] </ref> ; a [i] = 5 ; 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 9.6. <p> CONSISTENCY AND SEMANTICS 263 MPI_File_open ( MPI_COMM_WORLD, "workfile", MPI_MODE_RDWR | MPI_MODE_CREATE, MPI_INFO_NULL, &fh0 ) ; MPI_File_set_view ( fh0, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_File_write_at (fh0, 0, a, 10, MPI_INT, &status ) ; MPI_File_sync ( fh0 ) ; MPI_Barrier ( MPI_COMM_WORLD ) ; /* Process 1 */ int b <ref> [10] </ref> ; MPI_File_open ( MPI_COMM_WORLD, "workfile", MPI_MODE_RDWR | MPI_MODE_CREATE, MPI_INFO_NULL, &fh1 ) ; MPI_File_set_view ( fh1, 0, MPI_INT, MPI_INT, "native", MPI_INFO_NULL ) ; MPI_Barrier ( MPI_COMM_WORLD ) ; MPI_File_sync ( fh1 ) ; MPI_File_read_at (fh1, 0, b, 10, MPI_INT, &status ) ; /* ---------------- THIS EXAMPLE IS ERRONEOUS --------------- */ The
Reference: [11] <author> International Organization for Standardization, </author> <title> Geneva. Information technology | Portable Operating System Interface (POSIX) | Part 1: System Application Program Interface (API) [C Language], </title> <journal> December 1996. </journal> <volume> 301 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 302 BIBLIOGRAPHY </volume>
Reference-contexts: MPI may be implemented in environments where threads are not supported or perform poorly. Therefore, it is not required that all MPI implementations fulfill all the requirements specified in this section. This section generally assumes a thread package similar to POSIX threads <ref> [11] </ref>, but the syntax and semantics of thread calls are not specified here | these are beyond the scope of this document. 8.7.1 General In a thread-compliant implementation, an MPI process is a process that may be multi-threaded. <p> FILE MANIPULATION 213 The modes MPI MODE RDONLY, MPI MODE RDWR, MPI MODE WRONLY, MPI MODE CREATE, and MPI MODE EXCL have identical semantics to their POSIX counterparts <ref> [11] </ref>. Exactly one of MPI MODE RDONLY, MPI MODE RDWR, or MPI MODE WRONLY, must be specified. It is erroneous to specify MPI MODE CREATE or MPI MODE EXCL in conjunction with MPI MODE RDONLY; it is erroneous to specify MPI MODE SEQUENTIAL together with MPI MODE RDWR.
Reference: [12] <author> Charles H. Koelbel, David B. Loveman, Robert S. Schreiber, Guy L. Steele Jr., and Mary E. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: 1 ; : : : ; start ndims2 g; Subarray (1; fsize ndims1 g; fsubsize ndims1 g; fstart ndims1 g; oldtype)) For an example use of MPI TYPE CREATE SUBARRAY in the context of I/O see Section 9.9.2. 4.14.5 Distributed Array Datatype Constructor The distributed array type constructor supports HPF-like <ref> [12] </ref> data distributions. However, unlike in HPF, the storage order may be specified for C arrays as well as for Fortran arrays. Advice to users. One can create an HPF-like file view using this type constructor as follows.
Reference: [13] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: The significant optimizations required for efficiency (e.g., grouping [15], collective buffering [1, 2, 16, 19, 22], and disk-directed I/O <ref> [13] </ref>) can only be implemented if the parallel I/O system provides a high-level interface supporting partitioning of file data among processes and a collective interface supporting complete transfers of global data structures between process memories and files.
Reference: [14] <author> S. J. Le*et, R. S. Fabry, W. N. Joy, P. Lapsley, S. Miller, and C. Torek. </author> <title> An advanced 4.4BSD interprocess communication tutorial, Unix programmer's supplementary documents (PSD) 21. </title> <type> Technical report, </type> <institution> Computer Systems Research Group, Depertment of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <year> 1993. </year> <note> Also available at http://www.netbsd.org/Documentation/lite2/psd/. </note>
Reference-contexts: file descriptor OUT intercomm new intercommunicator (handle) int MPI Comm join (int fd, MPI Comm *intercomm) MPI COMM JOIN (FD, INTERCOMM, IERROR) INTEGER FD, INTERCOMM, IERROR static MPI::Intercomm MPI::Comm::Join (const int fd) MPI COMM JOIN is intended for MPI implementations that exist in an environment supporting the Berkeley Socket interface <ref> [14, 17] </ref>.
Reference: [15] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: The significant optimizations required for efficiency (e.g., grouping <ref> [15] </ref>, collective buffering [1, 2, 16, 19, 22], and disk-directed I/O [13]) can only be implemented if the parallel I/O system provides a high-level interface supporting partitioning of file data among processes and a collective interface supporting complete transfers of global data structures between process memories and files.
Reference: [16] <author> William J. Nitzberg. </author> <title> Collective Parallel I/O. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Oregon, </institution> <month> December </month> <year> 1995. </year> <title> [17] 4.4BSD Programmer's Supplementary Documents (PSD). </title> <publisher> O'Reilly and Associates, </publisher> <year> 1994. </year>
Reference-contexts: The significant optimizations required for efficiency (e.g., grouping [15], collective buffering <ref> [1, 2, 16, 19, 22] </ref>, and disk-directed I/O [13]) can only be implemented if the parallel I/O system provides a high-level interface supporting partitioning of file data among processes and a collective interface supporting complete transfers of global data structures between process memories and files.
Reference: [18] <author> Perry Partow and Dennis Cottel. </author> <title> Scalable Programming Environment. </title> <type> Technical Report 1672, </type> <institution> Naval Command Control and Ocean Surveillance Center (NRAD), </institution> <month> Septem-ber </month> <year> 1994. </year>
Reference: [19] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: The significant optimizations required for efficiency (e.g., grouping [15], collective buffering <ref> [1, 2, 16, 19, 22] </ref>, and disk-directed I/O [13]) can only be implemented if the parallel I/O system provides a high-level interface supporting partitioning of file data among processes and a collective interface supporting complete transfers of global data structures between process memories and files.
Reference: [20] <author> Anthony Skjellum, Nathan E. Doss, and Kishore Viswanathan. </author> <title> Inter-communicator extensions to MPI in the MPIX (MPI eXtension) Library. </title> <type> Technical Report MSU-940722, </type> <institution> Mississippi State University | Dept. of Computer Science, </institution> <month> April </month> <year> 1994. </year> <note> http://www.erc.msstate.edu/mpi/mpix.html. </note>
Reference-contexts: The other communicator constructors, MPI COMM CREATE and MPI COMM SPLIT, currently apply only to intracommunicators. These operations in fact have well-defined semantics for intercommunicators <ref> [20] </ref>. In the following discussions, the two groups in an intercommunicator are called the left and right groups. A process in an intercommunicator is a member of either the left or the right group. <p> To understand how MPI can be extended, we can view most MPI intracommunicator collective operations as fitting one of the following categories (see, for instance, <ref> [20] </ref>): All-To-All All processes contribute to the result. All processes receive the result. * MPI Allgather, MPI Allgatherv 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 150 CHAPTER 7.
Reference: [21] <author> Anthony Skjellum, Ziyang Lu, Purushotham V. Bangalore, and Nathan E. Doss. </author> <title> Explicit parallel programming in C++ based on the message-passing interface (MPI). </title> <editor> In Gregory V. Wilson, editor, </editor> <title> Parallel Programming Using C++, Engineering Computation Series. </title> <publisher> MIT Press, </publisher> <month> July </month> <year> 1996. </year> <note> ISBN 0-262-73118-5. </note>
Reference: [22] <author> Rajeev Thakur and Alok Choudhary. </author> <title> An Extended Two-Phase Method for Accessing Sections of Out-of-Core Arrays. </title> <journal> Scientific Programming, </journal> <volume> 5(4) </volume> <pages> 301-317, </pages> <month> Winter </month> <year> 1996. </year>
Reference-contexts: The significant optimizations required for efficiency (e.g., grouping [15], collective buffering <ref> [1, 2, 16, 19, 22] </ref>, and disk-directed I/O [13]) can only be implemented if the parallel I/O system provides a high-level interface supporting partitioning of file data among processes and a collective interface supporting complete transfers of global data structures between process memories and files.

References-found: 21

