URL: http://polaris.cs.uiuc.edu/reports/1516.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Email: billp@ncsa.uiuc.edu,potteng@csrd.uiuc.edu  billp@ncsa.uiuc.edu,potteng@csrd.uiuc.edu  
Phone: 217-244-0630 fax: 217-244-2909  217-244-0651 fax: 217-244-2909  
Title: Real-time Semantic Indexing on Parallel Computers  
Author: William M. Pottenger and Bruce R. Schatz William M. Pottenger Bruce R. Schatz 
Date: October 20, 1997  
Address: 405 North Mathews, Urbana, Illinois 61801  405 North Mathews, Urbana, Illinois 61801  
Affiliation: University of Illinois at Urbana-Champaign  University of Illinois at Urbana-Champaign  
Note: Digital Library Research Program (DLRP) Center for Supercomputing Research and Development (CSRD) National Center for Supercomputing Applications (NCSA)  Digital Library Research Program (DLRP) National Center for Supercomputing Applications (NCSA)  
Abstract-found: 0
Intro-found: 1
Reference: [AGL + 96] <author> Rafael Asenjo, Eladio Gutierrez, Yuan Lin, David Padua, Bill Pottenger, and Emilio Za-pata. </author> <title> On the Automatic Parallelization of Sparse and Irregular Fortran Codes. </title> <type> Technical Report 1512, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: The process of traversing a list and conditionally removing entries is an operation which occurs in other computer programs as well. For example, in the irregular Fortran benchmark DSMC3D (Discrete Simulation Monte Carlo), a similar operation is performed <ref> [AGL + 96] </ref>. Instead of terms, however, the list contains molecules. Both of these operations can be modeled as coalescing operations in which the list is being reduced in size. In [AGL + 96], we reduce sections of the list in parallel, and the actual recombination of the reduced sections takes <p> For example, in the irregular Fortran benchmark DSMC3D (Discrete Simulation Monte Carlo), a similar operation is performed <ref> [AGL + 96] </ref>. Instead of terms, however, the list contains molecules. Both of these operations can be modeled as coalescing operations in which the list is being reduced in size. In [AGL + 96], we reduce sections of the list in parallel, and the actual recombination of the reduced sections takes place in a dofinal section which follows the doall execution of the doevery portion of the loop.
Reference: [AH90] <author> Zahira Ammarguellat and Luddy Harrison. </author> <title> Automatic Recognition of Induction & Recurrence Relations by Abstract Interpretation. </title> <booktitle> Proceedings of Sigplan 1990, </booktitle> <address> Yorktown Heights, </address> <month> 25(6) </month> <pages> 283-295, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [KBC + 74, CKS78, KS82, PW86, AH90, HP92, GSW, PE95] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [CKS78, KS82]. <p> This approach has the advantage that complex conditional structures occurring in multiply-nested loops pose no particular problem. This approach is, however, inherently limited to the particular patterns programmed into the compiler. 3.2 Compile-time Solution of Recurrences Compile-time solutions include <ref> [AH90] </ref>, in which Harrison and Ammarguellat use abstract interpretation to map each variable assigned in a loop to a symbolic form, and match these against template patterns containing the closed-forms for commonly occurring recurrences. They are able to handle both induction variables and array-based recurrences in singly-nested loops. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [BDE + 96] <author> William Blume, Ramon Doallo, Rudolf Eigenmann, John Grout, Jay Hoeflinger, Thomas Lawrence, Jaejin Lee, David Padua, Yunheung Paek, Bill Pottenger, Lawrence Rauchwerger, and Peng Tu. </author> <title> Parallel Programming with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Here we have conditional expressions guarding updates to scalar variables. This is a classic case of a loop with reduction semantics. Such patterns commonly occur in computationally important loops in a wide range of codes <ref> [BDE + 96] </ref>. Many frameworks have been developed for recognizing parallelism of this nature based on both syntactic and semantic schemes. All of these frameworks have one thing in common, however: the enabling factor underlying the transformation is the associative nature of the operation being performed. <p> The specific techniques used to parallelize output operations of this nature are applicable generally in computer programs that perform output. These techniques can be applied to the automatic parallelization of computer programs in systems such as the Polaris restructurer <ref> [BDE + 96] </ref>. In the following section we introduce the cSpace application and provide an overview of the algorithm to compute Concept Spaces. In section 6, we analyze the parallelism present in cSpace based on the model presented in section 4. <p> Co-occurring terms are ranked in decreasing order of similarity, with the result that more general terms occur lower in the list of co-occurring terms. 5.4 The Implementation cSpace is based upon a collection hierarchy derived from the Polaris Project, a research project investigating the automatic parallelization of Fortran codes <ref> [BDE + 96] </ref>. The collection hierarchy provides an extensive set of templatized data structures including lists, trees, and sets. The data structures are templatized in the sense that they may contain many different types of objects. <p> The specific techniques used to parallelize output are applicable generally in computer programs that perform output. As noted in section 4.6, these techniques can also be applied in the automatic parallelization of computer programs in systems such as the Polaris restructurer <ref> [BDE + 96] </ref>. 7.3.1 Parallel Output Operations As discussed in section 6.2, the output operation is associative but not commutative. As a result, the parallelizing transformation must retain the original non-commuted order of execution.
Reference: [CKS78] <author> S. C. Chen, D. J. Kuck, and A. H. Sameh. </author> <title> Practical Parallel Band Triangular System Solvers. </title> <journal> ACM Trans. on Mathematical Software, </journal> <volume> 4(3) </volume> <pages> 270-277, </pages> <month> Sept., </month> <year> 1978. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [KBC + 74, CKS78, KS82, PW86, AH90, HP92, GSW, PE95] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [CKS78, KS82]. <p> The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., [KBC + 74, CKS78, KS82, PW86, AH90, HP92, GSW, PE95]). Early work included parallel recurrence solvers which were implemented in hardware <ref> [CKS78, KS82] </ref>. More recently, techniques based on powerful symbolic analysis have been employed in the recognition and solution of recurrences statically at compile-time as well as dynamically at run-time. 3.1 Run-time Solution of Recurrences The run-time solution of recurrences also has a fairly rich history. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [CL92] <author> H. Chen and K. J. Lynch. </author> <title> Automatic Construction of Networks of Concepts Characterizing Document Databases. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 22(5) </volume> <pages> 885-902, </pages> <month> September/October </month> <year> 1992. </year> <month> 29 </month>
Reference-contexts: O (T DC) where T is the total number of terms, D is the number of documents in which T occurs, and C is the number of co-occurring terms in D. 5.3 The Similarity Function The similarity computation is based on an asymmetric "Cluster Function" developed by Chen and Lynch <ref> [CL92] </ref>. The authors show that the asymmetric cluster function represents term association better than the popular cosine function.
Reference: [CSN + 96a] <author> H. Chen, B. R. Schatz, T. D. Ng, , and M. Yang. </author> <title> Breaking the Semantic Barrier: A Concept Space Experiment on the Convex Exemplar Parallel Supercomputer. </title> <journal> IEEE Parallel and Distributed Technology (submitted), </journal> <year> 1996. </year>
Reference-contexts: However, the development and implementation of real-time parallel semantic retrieval algorithms is a daunting task. This can be seen from recent results in the computation of Concept Spaces on the Convex Exemplar parallel supercomputer <ref> [CSN + 96a] </ref>. In this study, an input set of approximately 60,000 abstracts each approximately two kilobytes in size required over 10 hours of computation time. There is a sore need for the application of systematic techniques in the development of real-time algorithms. <p> Furthermore, we have decreased the elapsed time needed to compute a Concept Space on a large input set by a factor of over 25 as compared with the results reported in <ref> [CSN + 96a] </ref>. We are entering the information age, and many of the challenges before us are already clear.
Reference: [CSN + 96b] <author> Hsinchun Chen, Bruce Schatz, Tobun Ng, Joanne Martinez, Amy Kirchhoff, and Chienting Lin. </author> <title> A Parallel Computing Approach to Creating Engineering Concept Spaces for Semantic Retrieval: The Illinois Digital Library Initiative Project. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <year> 1996. </year>
Reference-contexts: is ordered, and 1 + qualifies the aggregation operation to mean that one or more objects are being aggregated. 16 5.2 The Algorithm Algorithms to compute Concept Spaces have been under development for several years. cSpace is a parallel C ++ shared-memory implementation based in part on algorithms described in <ref> [Rou96, FBY92, CSN + 96b] </ref>. The computation proceeds in phases. The first phase is symbolic in nature and accounts for less than 5% of the sequential execution time.
Reference: [FBY92] <author> William B. Frakes and Ricardo Baeza-Yates. </author> <title> Information Retrieval Data Structures & Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: is ordered, and 1 + qualifies the aggregation operation to mean that one or more objects are being aggregated. 16 5.2 The Algorithm Algorithms to compute Concept Spaces have been under development for several years. cSpace is a parallel C ++ shared-memory implementation based in part on algorithms described in <ref> [Rou96, FBY92, CSN + 96b] </ref>. The computation proceeds in phases. The first phase is symbolic in nature and accounts for less than 5% of the sequential execution time.
Reference: [FG94] <author> A. Fisher and A. Ghuloum. </author> <title> Parallelizing Complex Scans and Reductions. </title> <booktitle> Proceedings of the SIGPLAN'94 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Fisher and Ghuloum model loop bodies containing reductions and recurrences as composable functions in <ref> [FG94] </ref>. They determine whether, for a given loop, the composition of the function representing the loop body yields a function isomorphic to the original model. From this, a 5 parallel prefix solution of the reduction/recurrence is generated. They are able to handle both scalar and array-based recurrences in singly-nested loops. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). <p> Recognition techniques based on the underlying semantics of reduction operations have been implemented in the Velour vectorizing compiler [JD89]. Similar to the techniques implemented in <ref> [FG94] </ref>, these approaches identify variables which are computed as recurrent associative functions derived from statements in the body of the loop.
Reference: [GPHL90] <author> Mark D. Guzzi, David A. Padua, Jay P. Hoeflinger, and Duncan H. Lawrie. </author> <title> Cedar Fortran and Other Vector and Parallel Fortran Dialects. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(1) </volume> <pages> 37-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: This doevery section of the parallel loop is executed as a dependence-free doall loop <ref> [GPHL90] </ref>.
Reference: [GSW] <author> Michael P. Gerlek, Eric Stoltz, and Michael Wolfe. </author> <title> Beyond Induction Variables: Detecting and Classifying Sequences Using a Demand-driven SSA Form. </title> <note> To appear in TOPLAS. </note>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [KBC + 74, CKS78, KS82, PW86, AH90, HP92, GSW, PE95] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [CKS78, KS82]. <p> Haghighat and Polychronopoulos symbolically execute loops and use finite difference methods in conjunction with interpolation to determine closed-forms for recurrences involving scalar variables [HP92]. Their approach is capable of multiple scalar transformations including induction variable substitution. Other compile-time solutions include <ref> [GSW] </ref> in which Wolfe et al derive relations between variables by matching against cycles in the SSA graph, and then use matrix inversion (among other methods) to determine closed-forms for induction variables. We also treat the solution of generalized induction variables at compile-time in [PE95].
Reference: [Har86] <author> Luddy Harrison. </author> <title> Compiling Lisp for Evaluation on a Tightly Coupled Multiprocessor. </title> <type> Technical Report 565, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> Mar. 20, </month> <year> 1986. </year>
Reference-contexts: Similar to the techniques implemented in [FG94], these approaches identify variables which are computed as recurrent associative functions derived from statements in the body of the loop. Harrison also treats the parallelization of associative inductions, reductions, and recurrences in functional languages in <ref> [Har86] </ref>. 3.4 Commutative Operations In [RD96], Rinard and Diniz present a framework for parallelizing recursive function calls based on commutativity.
Reference: [HP90] <author> John Hennessy and David Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Mor-gan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: The goal of achieving real-time performance in the computation of Concept Spaces is within reach. Typical response times on the order of one second are considered tolerable by computer users in an interactive environment <ref> [HP90] </ref>. We have achieved near real-time performance in the computation of a Concept Space "on-the-fly" for use in an interactive query session.
Reference: [HP92] <author> Mohammad R. Haghighat and Constantine D. Polychronopoulos. </author> <title> Symbolic Program Analysis and Optimization for Parallelizing Compilers. </title> <booktitle> Presented at the 5th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August 3-5, </month> <year> 1992. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [KBC + 74, CKS78, KS82, PW86, AH90, HP92, GSW, PE95] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [CKS78, KS82]. <p> They are able to handle both induction variables and array-based recurrences in singly-nested loops. Haghighat and Polychronopoulos symbolically execute loops and use finite difference methods in conjunction with interpolation to determine closed-forms for recurrences involving scalar variables <ref> [HP92] </ref>. Their approach is capable of multiple scalar transformations including induction variable substitution. Other compile-time solutions include [GSW] in which Wolfe et al derive relations between variables by matching against cycles in the SSA graph, and then use matrix inversion (among other methods) to determine closed-forms for induction variables.
Reference: [IBM88] <author> IBM. </author> <title> Parallel FORTRAN Language and Library Reference, </title> <month> March </month> <year> 1988. </year> <month> 30 </month>
Reference-contexts: To understand these four steps, let's consider the following example: do i = 1; n enddo 11 The following is the parallelized version of this example. The language used in this code is based on IBM's Parallel Fortran <ref> [IBM88] </ref> with extensions which we have added to adapt the language to the special needs of the associative transformation. parallel loop, block i = 1; n private sum p dofirst sum p = 0 doevery sum p = sum p + a (i) enddo dofinal, ordered lock sum = sum +
Reference: [JD89] <author> P. Jouvelot and B. Dehbonei. </author> <title> A Unified Semantic Approach for the Vectorization and Par--allelization of Generalized Reductions. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June 5-9, </month> <year> 1989. </year> <note> ACM. </note>
Reference-contexts: In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). <p> Recognition techniques based on the underlying semantics of reduction operations have been implemented in the Velour vectorizing compiler <ref> [JD89] </ref>. Similar to the techniques implemented in [FG94], these approaches identify variables which are computed as recurrent associative functions derived from statements in the body of the loop.
Reference: [KBC + 74] <author> D. Kuck, P. Budnik, S-C. Chen, Jr. E. Davis, J. Han, P. Kraska, D. Lawrie, Y. Muraoka, R. Strebendt, and R. Towle. </author> <title> Measurements of Parallelism in Ordinary FORTRAN Programs. </title> <journal> Computer, </journal> <volume> 7(1) </volume> <pages> 37-46, </pages> <month> Jan., </month> <year> 1974. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [KBC + 74, CKS78, KS82, PW86, AH90, HP92, GSW, PE95] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [CKS78, KS82].
Reference: [KM74] <author> David Kuck and Yoichi Muraoka. </author> <title> Bounds on the Parallel Evaluation of Arithmetic Expressions Using Associativity and Commutativity. </title> <journal> Acta Informatica, </journal> <volume> 3, Fasc. 3 </volume> <pages> 203-216, </pages> <year> 1974. </year>
Reference-contexts: In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [KS82] <author> David J. Kuck and Richard A. </author> <title> Stokes. The Burroughs Scientific Processor (BSP). </title> <journal> Special Issue on Supersystems, IEEE Trans. on Computers, </journal> <volume> C-31(5):363-376, </volume> <month> May, </month> <year> 1982. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [KBC + 74, CKS78, KS82, PW86, AH90, HP92, GSW, PE95] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [CKS78, KS82]. <p> The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., [KBC + 74, CKS78, KS82, PW86, AH90, HP92, GSW, PE95]). Early work included parallel recurrence solvers which were implemented in hardware <ref> [CKS78, KS82] </ref>. More recently, techniques based on powerful symbolic analysis have been employed in the recognition and solution of recurrences statically at compile-time as well as dynamically at run-time. 3.1 Run-time Solution of Recurrences The run-time solution of recurrences also has a fairly rich history. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [Kuc78] <author> D. J. Kuck. </author> <title> The Structure of Computers and Computations,, volume I. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> NY, </address> <year> 1978. </year>
Reference-contexts: Similarly, Kuck has shown that simple expressions (e.g., right-hand-sides of assignment statements) can be reordered based on combinations of both associativity and commutativity in tree-height reduction <ref> [Kuc78] </ref>. 4 Associativity in Coalescing Loop Operators In this section we introduce the concept of a coalescing loop operator.
Reference: [LGM95] <author> Clifford Lynch and Hector Garcia-Molina. </author> <title> Interoperability, Scaling, and the Digital Libraries Research Agenda: A Report on the May 18-19, </title> <booktitle> 1995 IITA Digital Libraries Workshop. </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The trends in the functionality of the Internet show, for example, that Web searcher functionality is severely 28 limited due to implementations based on indexing algorithms which match only words rather than more abstract concepts <ref> [LGM95] </ref>. The response to these challenges must be met by scientists and engineers with the proper training to do so.
Reference: [PE95] <author> Bill Pottenger and Rudolf Eigenmann. </author> <title> Idiom Recognition in the Polaris Parallelizing Compiler. </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing, Barcelona, Spain, </booktitle> <pages> pages 444-448, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [KBC + 74, CKS78, KS82, PW86, AH90, HP92, GSW, PE95] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [CKS78, KS82]. <p> From this, a 5 parallel prefix solution of the reduction/recurrence is generated. They are able to handle both scalar and array-based recurrences in singly-nested loops. In our previous work <ref> [PE95] </ref> we take a general pattern-matching approach to the recognition and transformation of reductions. This approach has the advantage that complex conditional structures occurring in multiply-nested loops pose no particular problem. <p> Other compile-time solutions include [GSW] in which Wolfe et al derive relations between variables by matching against cycles in the SSA graph, and then use matrix inversion (among other methods) to determine closed-forms for induction variables. We also treat the solution of generalized induction variables at compile-time in <ref> [PE95] </ref>. A general pattern-matching approach is employed in the recognition phase of our induction solution algorithm. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). <p> In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication). For example, in <ref> [PE95] </ref>, recurrence relations are solved using a run-time 6 technique that is based on the associativity of the underlying operator within either a single reduction statement or a group of reduction statements which access the same reduction variable.
Reference: [Pot94] <author> William Morton Pottenger. </author> <title> Induction Variable Substitution and Reduction Recognition in the Polaris Parallelizing Compiler. </title> <type> Master's thesis, </type> <institution> Univ of Illinois at Urbana-Champaign, Cntr for Supercomputing Res & Dev, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [Pot97] <author> William Morton Pottenger. </author> <title> Theory, Techniques, and Experiments in Solving Recurrences in Computer Programs. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., ftp.cs.uiuc.edu/pub/dept/tech reports/1997/UIUCDCS-R-97-1999.ps.Z, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: However, we have determined that indexed data structures such as arrays can also be viewed as conglomerate operands to coalescing loop operators. This point is addressed in more detail in <ref> [Pot97] </ref>. We will now move on to consider coalescing loop operators which are associative in nature. <p> Additional cases can be found in <ref> [Pot97] </ref>.
Reference: [PS96] <author> Bill Pottenger and Bruce Schatz. </author> <title> On the Evaluation of C++ in a Parallel Programming Environment. </title> <type> Technical Report 1506, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> November </month> <year> 1996. </year> <month> 31 </month>
Reference-contexts: Figure 2 portrays speedup and efficiency graphically for the largest data set. Several interesting trends are revealed in Table 1. First, several runs resulted in super-linear speedups. This is an indirect result of the poor performance of multi-threaded dynamic memory allocation in C ++ on the SGI Power Challenge <ref> [PS96] </ref>. The parallel version of cSpace used in these experiments employs a customized memory manager which alleviates much of the overhead associated with multi-threaded dynamic memory allocation. However, this also provides an unexpected benefit in that the overhead of numerous calls to malloc (i.e., operator new) is entirely eliminated.
Reference: [PS97] <author> Bill Pottenger and Bruce Schatz. cSpace: </author> <title> A Parallel C ++ Information Retrieval Benchmark. </title> <type> Technical Report 1511, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1997. </year>
Reference: [PW86] <author> D. Padua and M. Wolfe. </author> <title> Advanced Compiler Optimization for Supercomputers. </title> <journal> CACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December, </month> <year> 1986. </year>
Reference-contexts: The parallel solution of recurrences in Fortran, for example, has been a topic of study for several years (e.g., <ref> [KBC + 74, CKS78, KS82, PW86, AH90, HP92, GSW, PE95] </ref>). Early work included parallel recurrence solvers which were implemented in hardware [CKS78, KS82]. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [RBP + 91] <author> James Rumbaugh, Michael Blaha, William Premerlani, Frederick Eddy, and William Lorensen. </author> <title> Object-Oriented Modeling and Design. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1991. </year>
Reference: [RD96] <author> Martin C. Rinard and Pedro C. Diniz. </author> <title> Commutativity Analysis: A New Analysis Framework for Parallelizing Compilers. </title> <booktitle> In Programming Language Implementation and Design (PLDI), </booktitle> <pages> pages 54-67. </pages> <publisher> ACM, </publisher> <year> 1996. </year>
Reference-contexts: Similar to the techniques implemented in [FG94], these approaches identify variables which are computed as recurrent associative functions derived from statements in the body of the loop. Harrison also treats the parallelization of associative inductions, reductions, and recurrences in functional languages in [Har86]. 3.4 Commutative Operations In <ref> [RD96] </ref>, Rinard and Diniz present a framework for parallelizing recursive function calls based on commutativity.
Reference: [Rou96] <author> Dmitri Roussinov. </author> <title> Personal communication with author, </title> <year> 1996. </year>
Reference-contexts: is ordered, and 1 + qualifies the aggregation operation to mean that one or more objects are being aggregated. 16 5.2 The Algorithm Algorithms to compute Concept Spaces have been under development for several years. cSpace is a parallel C ++ shared-memory implementation based in part on algorithms described in <ref> [Rou96, FBY92, CSN + 96b] </ref>. The computation proceeds in phases. The first phase is symbolic in nature and accounts for less than 5% of the sequential execution time.
Reference: [RP95] <author> Lawrence Rauchwerger and David Padua. </author> <title> The LRPD Test: Speculative Run-Time Par-allelization of Loops with Privatization and Reduction Parallelization. </title> <booktitle> Proceedings of the SIGPLAN'95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Suganuma, Komatsu, and Nakatani, for example, recognize and transform scalar reductions based on the detection of reduction semantics in the data dependence graph [SKN96]. Rauchwerger and Padua test for the presence of privatizable arrays and reduction operations involving arrays in <ref> [RP95] </ref>. By treating individual array elements as scalars and recording at run-time whether a use of a given array element occurs outside the expanded reduction statement (or statements) involving the reduction variable, reduction operations can recognized and executed in parallel. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [SB92] <institution> NRC Computer Science and Telecommunications Board. Computing The Future. National Academy Press, </institution> <address> Washington, D.C., </address> <year> 1992. </year>
Reference-contexts: In light of this fact, The National Research Council report Computing The Future recently called for computer scientists and engineers to broaden the scope of their research efforts to include application areas outside their traditional core areas of study <ref> [SB92] </ref>. This work represents a melding of several fields of computational science: research in parallelizing compilation technology, research in high-performance benchmarking, and research into the parallelization of National Challenge digital library applications.
Reference: [SC96] <author> Bruce Schatz and Hsinchun Chen. </author> <title> Building Large-Scale Digital Libraries. </title> <booktitle> IEEE Computer, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Similarly, there is a growing demand for the application of statistical information retrieval techniques on repositories ranging in size from a few thousand to several hundred thousand items <ref> [SC96] </ref>. In the same vein, it must be noted that the world is heading towards parallel computers and computations. Dr. Richard Wirt, Director of Intel's new Research Lab, emphasized this trend in a recent seminar in which he discussed future directions in microprocessors, multi-processors, and applications [Wir96]. <p> Developed as part of the federal NII Digital Library Initiative (DLI) at the University of Illinois <ref> [SC96] </ref>, cSpace is a hybrid symbolic/numeric application which determines relationships between terms in a collection of documents. The resulting map between terms is designated a Concept Space and is useful in the refinement of queries presented to the collection. <p> The output size for this input data set is 2,665,339 bytes. The second data set consists of 1505 documents and 53,776 terms and is dubbed the "Personal Repository" (PR) data set. This data set is representative of a personal repository <ref> [SC96] </ref>. The document source file is 3,756,384 bytes in size and the output produced is 15,590,146 in size. The third data set consists of 8,787 documents and 710,431 terms. This set is dubbed the "Community Repository" (CR) data set, and is representative of a number of personal repositories.
Reference: [Sch90] <author> Bruce Schatz. </author> <title> Interactive Retrieval in Information Spaces Distributed across a Wide-Area Network. </title> <type> PhD thesis, </type> <institution> University of Arizona Computer Science Department, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Interactive semantic retrieval has long been sought as a goal. The refinement of a query response consisting of 100 to 300 items is considered an important milestone in the interactive retrieval of semantic information <ref> [Sch90] </ref>. Similarly, there is a growing demand for the application of statistical information retrieval techniques on repositories ranging in size from a few thousand to several hundred thousand items [SC96]. In the same vein, it must be noted that the world is heading towards parallel computers and computations. Dr. <p> The total size of this data set is 8,945 terms and the source file containing the documents is 639,483 bytes. This input was chosen specifically because this is the number of documents which is considered manageable in a response to a query to a bibliographic database <ref> [Sch90] </ref>. The output size for this input data set is 2,665,339 bytes. The second data set consists of 1505 documents and 53,776 terms and is dubbed the "Personal Repository" (PR) data set. This data set is representative of a personal repository [SC96].
Reference: [SJCC96] <author> B. Schatz, E. Johnson, P. Cochrane, and H. Chen. </author> <title> Interactive Term Suggestion for Users of Digital Libraries: Using Subject Thesauri and Co-occurrence Lists for Information Retrieval. </title> <booktitle> In 1st International ACM Conference on Digital Libraries, </booktitle> <pages> pages 126-133, </pages> <address> Bethesda, MD, 1996. </address> <publisher> ACM. </publisher>
Reference-contexts: The resulting map between terms is designated a Concept Space and is useful in the refinement of queries presented to the collection. Concept Spaces are used, for example, in interactive query sessions as part of the DLI testbed at the University of Illinois, Urbana-Champaign <ref> [SJCC96] </ref>. Algorithms to perform iterative search refinement which incorporate the computation of Concept Spaces are also under development as part of the Digital Library Research Program (DLRP) at Illinois. 5.1 The Object Model object classes exist in the system: Term, Document, Cooccurrence, and ConceptSpace.
Reference: [SKN96] <author> T. Suganuma, H. Komatsu, and T. Nakatani. </author> <title> Detection and Global Optimization of Reduction Operations. </title> <booktitle> Proceedings of ICS'96, </booktitle> <address> Philadelphia, PA, USA, </address> <month> July </month> <year> 1996. </year> <month> 32 </month>
Reference-contexts: Suganuma, Komatsu, and Nakatani, for example, recognize and transform scalar reductions based on the detection of reduction semantics in the data dependence graph <ref> [SKN96] </ref>. Rauchwerger and Padua test for the presence of privatizable arrays and reduction operations involving arrays in [RP95]. <p> In the transformation phase, mathematical closed-forms for inductions are computed across the iteration spaces of potentially multiply-nested loops enclosing induction sites. 3.3 Associative Operations Associative operations have been the basis for parallelization of reduction operations in both hardware and software systems for many years <ref> [KM74, CKS78, KS82, PW86, JD89, AH90, FG94, Pot94, RP95, PE95, SKN96] </ref>. In most of these cases, the associativity is limited to a single binary operation involving the operator + (addition) or fl (multiplication).
Reference: [Tu95] <author> Peng Tu. </author> <title> Automatic Array Privatization and Demand-Driven Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The following four steps are needed in order to transform a loop based on associativity alone: * Privatization of shared variables * Initialization of private variables * Block loop scheduling * Cross-processor "reduction" Privatization refers to the creation of thread or process-private copies of shared global variables <ref> [Tu95] </ref>. The second step involves the initialization of the newly created private variables. In the third step, the iteration space of the loop is broken into contiguous slices, and each processor executes a slice of the original iteration space.
Reference: [Wir96] <author> Dr. Richard Wirt. </author> <note> Seminar Presented at UIUC, </note> <month> September </month> <year> 1996. </year> <month> 33 </month>
Reference-contexts: In the same vein, it must be noted that the world is heading towards parallel computers and computations. Dr. Richard Wirt, Director of Intel's new Research Lab, emphasized this trend in a recent seminar in which he discussed future directions in microprocessors, multi-processors, and applications <ref> [Wir96] </ref>. This trend is being echoed on the desktop as major providers of personal computer systems integrate multiple processors into workstations. According to Dr. Wirt, the personal computer of the year 2000 will have many times the performance of today's systems.
References-found: 38

