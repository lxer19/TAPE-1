URL: http://www.mli.gmu.edu/~iimam/papers/IAS95.ps
Refering-URL: http://www.mli.gmu.edu/~iimam/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: iimam@aic.gmu.edu  
Title: INTELLIGENT AGENTS FOR MANAGEMENT OF LEARNING: An Introduction and A Case Study for Machine Learning
Author: Ibrahim F. Imam George Mason 
Address: 4400 University Dr. Fairfax, VA 22030  
Affiliation: Center  University  
Abstract: This paper is published in the Proceedings of the First FLAIRS International Workshop on Intelligent Adaptive Systems (IAS-95), pp. 95-106, Melbourne Beach, Florida, April 19, 1995. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baffes, P.T., and Mooney, </author> <title> R . J . , Symbolic Revision of Theories with M-of-N Rules, </title> <booktitle> Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> pp. 69-75, </pages> <address> Harpers Ferry, WV, </address> <month> May 26-29, </month> <year> 1993. </year>
Reference-contexts: An equally attractive example is agents that modify the knowledge to accomplish a given task. Examples of such tasks include: 1) performing a decision making process (Michalski & Imam, 1994); 2) improving the performance of the learned knowledge <ref> (Baffes & Mooney, 1993) </ref>. There is not much work on the development of parametric agents. The paper mainly focuses on the first category of intelligent agents that manage the learning process. A simple case study is introduced to explain the method.
Reference: <author> Boden, M.A., </author> <title> Agents and Creativity, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 37, No. 7, </volume> <pages> pp. 117-121, </pages> <month> July, </month> <year> 1994. </year> <title> Brodley, C . E . , Addressing the Selection Superiority Problem: Automatic Algorithm/Model Class Selection, </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 17-24, </pages> <year> 1993. </year>
Reference: <author> Guha, R.V., and Lenat, </author> <title> D.B, Enabling Agents to Work Together, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 37, No. 7, </volume> <pages> pp. 127-142, </pages> <month> July, </month> <year> 1994. </year> <title> Michalski, R . S . , AQVAL/1-Computer Implementation of a Variable-Valued Logic System VL1 and Examples of its Application to Pattern Recognition, </title> <booktitle> Proceedings of the First International Joint Conference on Pattern Recognition, </booktitle> <pages> (pp. 3-17), </pages> <address> Washington, DC, </address> <month> October 30-November 1, </month> <year> 1973. </year> <title> Michalski, R . S . , A Theory and Methodology of Inductive Learning, </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 20, </volume> <pages> (pp. 111-116), </pages> <year> 1983. </year>
Reference-contexts: Determining the specialty of an intelligent becomes more difficult when many agents must interact together. Since each agent is specialized in performing one or more different task from the others, some knowledge should be shared to allow mutual interaction <ref> (Guha & Lenat, 1994) </ref>. This knowledge is considered equivalent to common sense knowledge. Determining the knowledge to be shared between different (a group or all) agents increases the specialty of an agent and the expectation that it will use such knowledge whenever needed. 2.2.2. <p> One way of doing this is to develop a control mechanism (Riecken, 1994) that interacts with all agents (e.g. Blackboard architecture). Another way is to allow all agents to interact with each other, which requires increasing the understandability of agents communication with each other <ref> (Guha & Lenat, 1994) </ref>. 3. Managing the Learning systems by Intelligent Agents This section describes the parametric intelligent agent and the proposed methodology for building such an agent. It provides also an analysis of its specialty, growingness, and usability. Finally, the section shows a simple case study. 3.1.
Reference: <author> Michalski, R.S., Mozetic, I., Hong, J. and Lavrac, </author> <title> N . , The MultiPurpose Incremental Learning System AQ15 and Its Testing Application to Three Medical Domains, </title> <booktitle> Proceedings of AAAI-86, </booktitle> <pages> (pp. 1041-1045), </pages> <address> Philadelphia, PA, </address> <year> 1986. </year> <title> Minsky, M ., A Conversation with Marvin Minsky about Agents, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 37, No. 7, </volume> <pages> pp. 23-29, </pages> <month> July, </month> <year> 1994. </year>
Reference-contexts: The paper mainly focuses on the first category of intelligent agents that manage the learning process. A simple case study is introduced to explain the method. The case study is concerned with developing an intelligent parametric agent for the learning system AQ15 <ref> (Michalski, et al, 1986) </ref>. The intelligent agent gain its primarily knowledge from experimental runs. About 162 experiments were executed on randomly selected datasets. A subset of these datasets are selected and used to drive the primarly knowledge of the intelligent agent. <p> Description of the Method This section presents a simple case study of a parametric intelligent agent that manages the learning system AQ15 <ref> (Michalski, et al, 1986) </ref>. The specialty of this parametric intelligent agent is to determine the best combination of parameters for producing decision rules with high accuracy for a given problem.
Reference: <author> Mitchell, M., </author> <title> Analogy-Making as Perception, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1993. </year>
Reference-contexts: MCS uses a set of heuristic rules that describe the biases of learning algorithms and learning representations to solve the given problem. These rules represent an information guide in generating the hybrid tree. An example of an intelligent agent that has different capabilities is Copycat <ref> (Mitchell, 1993) </ref>. Copycat was used to discover new analogies in alphabetic letter strings. Copycat consists of different agents that compete with one another to find the strongest analogy. <p> It can also be defined as changing or extending the specialty of an agent. Subsequently, growingness may negatively affect the usability of intelligent agents. It is very difficult to develop intelligent agents capable of growing. The closest example of an intelligent agent that can grow is the system Copycat <ref> (Mitchell, 1993) </ref> which was described by Boden (1994) as a system that uses many independent descriptors in trying to interpret a given analogy and to find a new but similar one. The descriptors are applied in parallel with many hidden complexities such as different probabilistic variations and others.
Reference: <author> Mitchell, T., Caruana, R., Freitag, D., McDermott, J., and Zabowski, </author> <title> D . , Experience with a Learning Personal Assistant, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 37, No. </volume> <pages> 7 , pp. 81-91, </pages> <month> July, </month> <year> 1994. </year> <title> Morik, K . , Balanced Cooperative Modeling in Machine Learning: A Multistrategy Approach Vol. IV, </title> <editor> Michalski, R.S. & Tecuci, G. </editor> <booktitle> (Eds.), </booktitle> <pages> pp. 259-318, </pages> <publisher> Morgan Kaufmann Pubs., </publisher> <address> San Francisco, CA, </address> <year> 1994. </year>
Reference-contexts: The first category is concerned with using the learning systems directly in assisting the users. An example of such an agent is CAP (Calendar APprentice) <ref> (Mitchell, et al, 1994) </ref> which assists the user in scheduling his/her calendar using his/her own scheduling preferences. CAP is an intelligent agent that learns rules from a set of examples that describe previous meetings.
Reference: <author> Norman, D.A., </author> <title> How Might People Interact with Agents, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 37, No. 7, </volume> <pages> pp. 68-71, </pages> <month> July, </month> <year> 1994. </year> <title> Pazzani, M .J., Learning Fault Diagnosis Heuristics from Device Descriptions, </title> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. 3, </volume> <editor> Kodratoff, Y. & Michalski, R.S. (Eds.), pp.214-234, </editor> <publisher> Morgan Kufmann Pub., </publisher> <year> 1990. </year>
Reference: <author> Rendell, L., and Seshu, </author> <title> R . , Learning Hard Concept through Constructive Induction: Framework and Rationale, </title> <journal> Journal of Computational Intelligence, </journal> <volume> No. </volume> <pages> 6, </pages> <note> 1990 Thrun, S.B., </note> <author> Mitchell, T. & Cheng, J . , (Eds.) </author> <title> The MONKs problems: A Performance Comparison of Different Learning Algorithms, </title> <type> Technical Report, </type> <institution> Carnegie Mellon University, </institution> <month> October, </month> <year> 1991. </year>
Reference: <author> Wnek, J., and Michalski, </author> <title> R . S . , Hypothesis-Driven Constructive Induction in AQ17-HCI: </title>
References-found: 9

