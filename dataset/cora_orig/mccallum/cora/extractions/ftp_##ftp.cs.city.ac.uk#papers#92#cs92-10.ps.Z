URL: ftp://ftp.cs.city.ac.uk/papers/92/cs92-10.ps.Z
Refering-URL: http://www.cs.umd.edu/~keleher/bib/dsmbiblio/node8.html
Root-URL: 
Title: Angel: a proposed multiprocessor operating system kernel  initial motivations for Angel and subsequent detailed design|  
Author: T. Wilkinson, T. Stiemerling and P. Osmon A. Saulsbury and P. Kelly 
Note: This document describes  
Address: Northampton Square, London EC1V 0HB, UK.  180 Queens Gate, London SW7 2BZ, UK.  
Affiliation: Computer Science Department, City University,  Department of Computing, Imperial College,  
Abstract: We describe an operating system design for multiprocessor systems called Angel, based on a single, coherent, uniform virtual address space. This unifies naming and interprocess communication in both shared and distributed memory multiprocessors by using distributed shared memory techniques when shared memory is not already provided by the hardware. The design is motivated by analysis of our earlier operating system implementation, based on message passing, and we show how the uniform address space attempts to solve problems with that approach. In particular, we consider the use of client-server cross-mapping to optimise interprocess communications, as used in Bershad et al.'s lightweight RPC. we will review and may modify many of the details described as the design progresses. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Winterbottom and T. Wilkinson, "MESHIX: </author> <title> a UNIX like operating system for distributed machines," </title> <booktitle> in UKUUG Summer Conference Proceedings, </booktitle> <pages> pp. 237-246, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: 1 Introduction This paper describes an experimental operating system being developed at City University and Imperial College. This operating system builds on experience gained with constructing the Meshix operating system <ref> [1, 2] </ref> at City, and the implementation of a distributed shared memory server on top of that system [3]. <p> Topsy comprises single board computers interconnected by the MeshNet high performance (12 Mbytes/sec/channel) mesh topology network. MeshNet is implemented as a set of two ASIC's designed to support the message passing primitives used in the Meshix operating system. For further details, see <ref> [1, 2] </ref>. This hardware platform is less than ideal for supporting a distributed shared memory. Performance will be very disappointing compared with localised shared memory hardware, unless memory to memory communications latency less than or of the order of a microsecond can be achieved for small pages.
Reference: [2] <author> P. Osmon, T. Stiemerling, A. Valsamidis, A. Whitcroft, T. Wilkinson, and N. Williams, </author> <title> "The Topsy Project: a position paper," </title> <booktitle> in Proceedings of the PARLE'92 Conference, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: 1 Introduction This paper describes an experimental operating system being developed at City University and Imperial College. This operating system builds on experience gained with constructing the Meshix operating system <ref> [1, 2] </ref> at City, and the implementation of a distributed shared memory server on top of that system [3]. <p> Topsy comprises single board computers interconnected by the MeshNet high performance (12 Mbytes/sec/channel) mesh topology network. MeshNet is implemented as a set of two ASIC's designed to support the message passing primitives used in the Meshix operating system. For further details, see <ref> [1, 2] </ref>. This hardware platform is less than ideal for supporting a distributed shared memory. Performance will be very disappointing compared with localised shared memory hardware, unless memory to memory communications latency less than or of the order of a microsecond can be achieved for small pages.
Reference: [3] <author> A. Saulsbury and T. Stiemerling, </author> <title> "A DVSM server for Meshix," </title> <type> Tech. Rep. </type> <institution> TCU/CS/1992/7, City University Computer Science Department, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: 1 Introduction This paper describes an experimental operating system being developed at City University and Imperial College. This operating system builds on experience gained with constructing the Meshix operating system [1, 2] at City, and the implementation of a distributed shared memory server on top of that system <ref> [3] </ref>. A number of current operating systems share two main features: a micro-kernel implementation in which all possible operating system services have been moved out of the kernel into user-level processes, and the provision of (lightweight) threads to implement these servers efficiently.
Reference: [4] <author> S. Mullender, G. van Rossum, A. Tanenbaum, R. van Renesse, and H. van Staveren, </author> <title> "Amoeba: a distributed operating system for the 1990's," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 44-53, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: A number of current operating systems share two main features: a micro-kernel implementation in which all possible operating system services have been moved out of the kernel into user-level processes, and the provision of (lightweight) threads to implement these servers efficiently. Typical examples are Amoeba <ref> [4] </ref>, Mach [5], Chorus [6], and also Meshix. In these systems the term "process" is used to define a domain of protection, where inter-process communication (IPC) generally occurs using message-passing, while a process may be composed of many threads sharing the same address space and communicating using shared memory.
Reference: [5] <author> R. Rashid, </author> <title> "From RIG to Accent to Mach: the evolution of a network operating system," </title> <booktitle> in Proceedings of the ACM/IEEE Computer Society Fall Joint Conference, </booktitle> <month> November </month> <year> 1986. </year>
Reference-contexts: A number of current operating systems share two main features: a micro-kernel implementation in which all possible operating system services have been moved out of the kernel into user-level processes, and the provision of (lightweight) threads to implement these servers efficiently. Typical examples are Amoeba [4], Mach <ref> [5] </ref>, Chorus [6], and also Meshix. In these systems the term "process" is used to define a domain of protection, where inter-process communication (IPC) generally occurs using message-passing, while a process may be composed of many threads sharing the same address space and communicating using shared memory.
Reference: [6] <author> M. Rozier and L. Martins, </author> <title> Distributed Operating Systems: Theory and Practice, </title> <booktitle> Nato ASI Series, vol. F28, ch. The Chorus distributed operating system: some design issues, </booktitle> <pages> pp. 261-287. </pages> <publisher> Springer Verlag, </publisher> <year> 1987. </year>
Reference-contexts: A number of current operating systems share two main features: a micro-kernel implementation in which all possible operating system services have been moved out of the kernel into user-level processes, and the provision of (lightweight) threads to implement these servers efficiently. Typical examples are Amoeba [4], Mach [5], Chorus <ref> [6] </ref>, and also Meshix. In these systems the term "process" is used to define a domain of protection, where inter-process communication (IPC) generally occurs using message-passing, while a process may be composed of many threads sharing the same address space and communicating using shared memory.
Reference: [7] <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy, </author> <title> "Lightweight remote procedure call," </title> <journal> ACM Operating Systems Review, </journal> <volume> vol. 23, </volume> <pages> pp. 102-113, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: A crucial issue here has been improving the efficiency of cross-domain remote procedure calls (RPC) on the same machine <ref> [7] </ref>. In systems with no physically shared memory, such as a network of workstations or a distributed memory multiprocessor (eg Topsy [8]), the execution of the threads in a process is restricted to one workstation or node. <p> The classical approach to improving RPC performance, the lightweight RPC optimisation developed for the DEC Firefly system <ref> [7] </ref>, requires non-trivial modification to the operating system's structure while applying only to the local case. * As processor cache architectures become more complex the costs in time and complexity of changing the virtual-to-physical address map become more serious. <p> Despite operating in a highly distributed environment, Bershad et al. <ref> [7] </ref> observed that most RPC's are in fact local (95%-99%), not remote, and furthermore that the total parameter/result size is normally small (&lt; 200 bytes).
Reference: [8] <author> P. Winterbottom and P. Osmon, "Topsy: </author> <title> an extensible UNIX multicomputer," </title> <booktitle> in Proceedings of UK IT90 Conference, </booktitle> <publisher> (Southampton University), </publisher> <pages> pp. 164-176, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: A crucial issue here has been improving the efficiency of cross-domain remote procedure calls (RPC) on the same machine [7]. In systems with no physically shared memory, such as a network of workstations or a distributed memory multiprocessor (eg Topsy <ref> [8] </ref>), the execution of the threads in a process is restricted to one workstation or node. <p> We might consider software at this level to be an application program such as a DBMS, or the file system, shell and library calls to emulate a particular operating system. 8 4.1 The hardware layer The hardware platform presently available to support Angel is the Topsy architecture <ref> [8] </ref>, developed at City University over the past four years. Topsy comprises single board computers interconnected by the MeshNet high performance (12 Mbytes/sec/channel) mesh topology network. MeshNet is implemented as a set of two ASIC's designed to support the message passing primitives used in the Meshix operating system.
Reference: [9] <author> M.-C. Tam, J. Smith, and D. Farber, </author> <title> "A taxonomy-based comparison of several distributed shared memory systems," </title> <journal> ACM Operating Systems Review, </journal> <volume> vol. 24, </volume> <pages> pp. 40-67, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: This restriction, and the desire to provide a shared memory programming environment on distributed memory systems (in preference to using message-passing), has led to various implementations of distributed shared memory (DSM) <ref> [9] </ref> following the approach pioneered by Li [10]. We propose to integrate DSM mechanisms into the Angel kernel so that the entire system's address space is unified under a single DSM region which handles the caching of code, migration of data and coherency of distributed data structures.
Reference: [10] <author> K. Li and P. Hudak, </author> <title> "Memory coherence in shared virtual memory systems," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 7, </volume> <pages> pp. 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: This restriction, and the desire to provide a shared memory programming environment on distributed memory systems (in preference to using message-passing), has led to various implementations of distributed shared memory (DSM) [9] following the approach pioneered by Li <ref> [10] </ref>. We propose to integrate DSM mechanisms into the Angel kernel so that the entire system's address space is unified under a single DSM region which handles the caching of code, migration of data and coherency of distributed data structures.
Reference: [11] <author> T. Wilkinson, T. Stiemerling, P. Osmon, A. Saulsbury, and P. Kelly, "Angel: </author> <title> a proposed multiprocessor operating system," </title> <booktitle> in European Workshops on Parallel Computing 92, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: Then we describe how the sub-components of Angel, the micro-kernel and system services, are structured and interact to provide this model. Finally we present a plan for further work, and summarise the main points of the paper. An extended abstract of this paper has been published <ref> [11] </ref>. 2 Lessons from earlier work A DSM system has been prototyped on Meshix as an external pager (using a similar approach to the implementation on Mach [12]).
Reference: [12] <author> A. Forin, J. Barrera, M. Young, and R. Rashid, </author> <title> "Design, implementation, and performance evaluation of a distributed shared memory server for Mach," </title> <type> Tech. Rep. </type> <institution> CMU-CS-88-165, Carnegie-Mellon University Computer Science Department, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: An extended abstract of this paper has been published [11]. 2 Lessons from earlier work A DSM system has been prototyped on Meshix as an external pager (using a similar approach to the implementation on Mach <ref> [12] </ref>). The following observations have been made during this work, and also concerning Meshix in general: * Monolithic Unix implementations can have superior IPC performance compared with more modular message-passing designs such as Meshix.
Reference: [13] <author> M. Schroeder and M. Burrows, </author> <title> "Performance of Firefly RPC," </title> <booktitle> in Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: The cost of this implementation of IPC compared with a highly-tuned non-local mechanism (e.g. Schroeder and Burrows <ref> [13] </ref>) is higher, but in principle only because a large region is transferred across the network instead of a message containing just the data required. The presence of multiple threads will result in page contention unless care is taken to ensure that different threads' parameter/result stacks lie in disjoint pages.
Reference: [14] <author> J. Mogul and A. Borg, </author> <title> "The effect of context switches on cache performance," </title> <booktitle> in International Conference on Archictectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 75-85, </pages> <year> 1991. </year>
Reference-contexts: Context switching carries a hidden cost in loading data from the new context into the cache, and this is often large compared to other costs of context switching (see Mogel and Borg <ref> [14] </ref>). These issues will be examined more closely in a subsequent document [15]. 2.8 Aliasing in the address space In a single uniform address space, a virtual address may refer to one physical address on one node, and a different address on a different node.
Reference: [15] <author> P. Kelly, </author> <title> "Coherent address space avoids both cache flushing and tags." </title> <note> Working Paper, </note> <month> January </month> <year> 1992. </year>
Reference-contexts: Context switching carries a hidden cost in loading data from the new context into the cache, and this is often large compared to other costs of context switching (see Mogel and Borg [14]). These issues will be examined more closely in a subsequent document <ref> [15] </ref>. 2.8 Aliasing in the address space In a single uniform address space, a virtual address may refer to one physical address on one node, and a different address on a different node. <p> Within a single processor, there are advantages available in the management of virtual caches <ref> [15] </ref>. When coding parallel applications, shared objects are guaranteed to appear at the same address to all cooperating processes, wherever they are located.
Reference: [16] <author> P. Teller, </author> <title> "Translation-lookaside buffer consistency," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 26-36, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: An example of such a problem is ensuring coherency of TLBs in a shared-memory cluster (see <ref> [16] </ref> for a discussion of this area). 3 Angel operating system overview The remainder of this paper presents the Angel operating system currently being designed specifically to address the problems outlined above.
Reference: [17] <author> T. Kilburn, R. Payne, and D. Howarth, </author> <title> "The Atlas Supervisor," </title> <booktitle> in Proceedings of the 1961 Eastern Joint Computer Conference, </booktitle> <volume> vol. 20, </volume> <pages> pp. 279-294, </pages> <publisher> AFIPS, </publisher> <year> 1961. </year>
Reference-contexts: The terms object, process and capability are defined below, after a discussion of the single address space. 3.1 Single coherent shared virtual address space Single-level storage was introduced in Atlas <ref> [17] </ref>, and extended to include the filesystem in Multics [18]. In both these systems, each process has a distinct, inconsistent address space: a given virtual address in one process may refer to a quite different object in a different process.
Reference: [18] <author> E. Organick, </author> <title> The MULTICS system: an examination of it's structure. </title> <publisher> The MIT Press, </publisher> <year> 1971. </year>
Reference-contexts: The terms object, process and capability are defined below, after a discussion of the single address space. 3.1 Single coherent shared virtual address space Single-level storage was introduced in Atlas [17], and extended to include the filesystem in Multics <ref> [18] </ref>. In both these systems, each process has a distinct, inconsistent address space: a given virtual address in one process may refer to a quite different object in a different process.
Reference: [19] <author> D. Redell, </author> <title> Naming and protection in extendible operating systems. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <month> Novem--ber </month> <year> 1974. </year>
Reference-contexts: This was proposed by Redell <ref> [19] </ref> and has been implemented in IBM's system 38 [20]. The main reason is to ease sharing of objects between processes, and in a parallel system this is a particularly important consideration. This has led to more recent single address space operating systems such and Clouds [21] and Psyche [22].
Reference: [20] <author> R. French, R. Collins, and L. </author> <title> Loen, IBM System/38 Technical Developments, ch. </title> <booktitle> System/38 machine storage management, </booktitle> <pages> pp. 63-66. </pages> <institution> IBM General Systems Division, </institution> <year> 1978. </year>
Reference-contexts: This was proposed by Redell [19] and has been implemented in IBM's system 38 <ref> [20] </ref>. The main reason is to ease sharing of objects between processes, and in a parallel system this is a particularly important consideration. This has led to more recent single address space operating systems such and Clouds [21] and Psyche [22].
Reference: [21] <author> P. Dasgupta, R. LeBlanc, and W. Appelbe, </author> <title> "The Clouds distributed operating system: functional description, implementation details and related works," </title> <booktitle> in International Conference on Distributed Computing Systems, </booktitle> <year> 1988. </year>
Reference-contexts: The main reason is to ease sharing of objects between processes, and in a parallel system this is a particularly important consideration. This has led to more recent single address space operating systems such and Clouds <ref> [21] </ref> and Psyche [22].
Reference: [22] <author> M. Scott, T. LeBlanc, and B. Marsh, </author> <title> "Implementation issues for the Psyche multiprocessor operating system," </title> <booktitle> in Proceedings of the Usenix Workshop on Distributed and Multiprocessor Systems, </booktitle> <pages> pp. 227-236, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The main reason is to ease sharing of objects between processes, and in a parallel system this is a particularly important consideration. This has led to more recent single address space operating systems such and Clouds [21] and Psyche <ref> [22] </ref>. In Angel we extend the consistent address space to span multiple PEs in a distributed-memory system, maintaining coherence where necessary using DSM techniques. 1 We assume the emergence of 64-bit processors to make this viewpoint feasible. 6 There are several reasons to expect benefits from this decision.
Reference: [23] <author> B. Martin, C. Pedersen, and J. Bedford-Roberts, </author> <title> "An object-based taxonomy for distributed computing systems," </title> <journal> IEEE Computer, </journal> <volume> vol. 24, </volume> <pages> pp. 17-27, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Before it can be used, the object must be mapped into a process's address space by resolving one of its capabilities. All objects are persistent, once created an object does not depend upon the existence of its creator <ref> [23] </ref>. This means that as long as the object is referenced it will remain present in the object space.
Reference: [24] <author> P. Bishop, </author> <title> Computer systems with a very large address space and garbage collection. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <month> May </month> <year> 1977. </year> <month> 16 </month>
Reference-contexts: Objects can be explicitly deleted, but if a process dies before removing an object only it referenced, this object must be cleared away, to retrieve the virtual address space it occupies, and to save backing store (see Bishop's thesis <ref> [24] </ref>). 3.3 Processes A process 2 is a locus of control and permissions. A process comes into existence when it is created by its parent process, and disappears when it voluntarily dies or is killed by another process with appropriate access rights.
References-found: 24

