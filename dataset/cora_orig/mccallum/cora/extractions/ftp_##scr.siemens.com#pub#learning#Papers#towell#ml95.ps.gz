URL: ftp://scr.siemens.com/pub/learning/Papers/towell/ml95.ps.gz
Refering-URL: http://www.cs.jhu.edu/~weiss/papers.html
Root-URL: 
Email: ftowell,ellen,gupta,beng@scr.siemens.com  
Title: Learning Collection Fusion Strategies for Information Retrieval  
Author: Geoffrey Towell Ellen M. Voorhees Narendra K. Gupta Ben Johnson-Laird 
Address: 755 College Road East Princeton, NJ 08540  
Affiliation: Siemens Corporate Research  
Date: July 1995  
Note: Appears in Proceedings of the Twelfth Annual Machine Learning Conference, Lake Tahoe,  
Abstract: In this paper we describe an Information Retrieval problem called collection fusion. The collection fusion problem is to maximize the number of relevant natural language documents retrieved given: a natural language query, multiple collections of documents, and a fixed total number of documents to retrieve. We describe two algorithms that use past queries to learn collection fusion strategies. Tests of these algorithms on a corpus of 742,000 documents indicate that they can learn good fusion strategies. Moreover, the strategies learned by our methods are consistently superior to those learned by a standard learning algorithm.
Abstract-found: 1
Intro-found: 1
Reference: <author> Atlas, L., Cole, R., Muthusamy, Y., Lippman, A., Connor, J., Park, D., El-Sharkawi, M., & Marks, R. J. </author> <year> (1990). </year> <title> A peerformance comparison of trained multi-layer perceptrons and trained classification trees. </title> <booktitle> Proceedings of IEEE, </booktitle> <volume> 78, </volume> <pages> 1614-1619. </pages>
Reference: <author> Buckley, C. </author> <year> (1985). </year> <title> Implementation of the SMART Information Retrieval System. </title> <type> (Technical Report 85-686), </type> <address> Ithaca, New York: </address> <institution> Computer Science Department, Cor-nell University. </institution>
Reference-contexts: We use the SMART <ref> (Buckley, 1985) </ref> system as the IR system underlying all of our tests. That is, SMART is used to create the document and query vectors and to score the relationship between documents and queries.
Reference: <author> Callan, J. P., Lu, Z., & Croft, W. B. </author> <year> (1995). </year> <title> Searching distributed collections with inference networks. </title> <booktitle> Proceedings 1995 ACM SIGIR Conference on Research and Development in Information Retrieval. </booktitle>
Reference: <author> Dietterich, T. G., Hild, H., & Bakiri, G. </author> <year> (1990). </year> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 24-31). </pages> <address> Austin, TX. </address>
Reference: <author> Gupta, N. & Voorhees, E. M. </author> <year> (1994). </year> <title> Genetic Algorithms for Learning Models of Information Servers. </title> <type> (Technical Report SCR-94-TR-501): </type> <institution> Siemens Corporate Research, Inc. </institution>
Reference-contexts: We also evaluated genetic algorithms on this problem <ref> (Gupta & Voorhees, 1994) </ref>. We do not report these results because genetic algorithms did not match the ability of neural networks. The strategy based upon neural networks is essentially a straw man against which we will evaluate our systems.
Reference: <author> Harman, D. K. </author> <year> (1993). </year> <booktitle> The first Text REtrieval Conference (TREC-1), </booktitle> <address> Rockville, MD, U.S.A, </address> <month> 4-6 November, </month> <year> 1992. </year> <booktitle> Information Processing and Management, </booktitle> <volume> 29(4), </volume> <pages> 411-414. </pages>
Reference-contexts: In practice, F I Q is not known and so must be approximated. 2.2 THE TREC DOCUMENT COLLECTION All of the retrieval runs use the approximately 742,000 documents and 200 queries and relevance assessments in the TREC collection <ref> (Harman, 1993) </ref>. This collection is a standard in the IR community. It consists of five sub-collections that are of different sizes, cover diverse topics, have different retrieval characteristics, and come from different sources. The sources of the documents are: the A.P. newswire (referred to hereafter as AP), U.S.
Reference: <author> Moffat, A. & Zobel, J. </author> <year> (1994). </year> <title> Information retrieval for large document collections. </title> <booktitle> Proceedings of the Third Text REtrieval Conference (TREC-3). </booktitle> <publisher> In press. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructure of cognition. Volume 1: Foundations. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The only parameter affecting the performance of query clustering is L. Testing indicated that L = 100 is as effective or better than other values (Voorhees et al., 1995). 3.3 NEURAL NETWORKS To apply neural networks (NN) to this task, we trained feed-forward networks <ref> (Rumelhart et al., 1986) </ref> to learn the optimal fusion for a given number of documents to be retrieved. We did this by creating networks with one output unit per collection. (An alternative would be to train one network per collection, i.e., create five networks each with a single output unit.
Reference: <author> Salton, G. & Buckley, C. </author> <year> (1988). </year> <title> Term weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24, </volume> <pages> 513-523. </pages>
Reference-contexts: In SMART, we used: document vectors as described, query vectors as described except that the frequency of each word was divided by the number of documents in which it occurred in the given collection, and the cosine similarity measure. This combination has proven effective <ref> (Salton & Buckley, 1988) </ref>. For three collections, labeled A, B, and C, the positions of the rel-evant documents in the 10 most highly ranked documents for a given query, i.e., F I Q (10), is given in the following table.
Reference: <author> Salton, G. & McGill, M. J. </author> <year> (1983). </year> <title> Introduction to Modern Information Retrieval. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Many IR systems use statistical methods to assign a match score to each document in the collection rather than attempting to understand the text <ref> (Salton & McGill, 1983) </ref>. Scores depend upon factors including, but not limited to: words in the query, words in the document, word frequencies in the document collection, the particular scoring function. Changes to any of these items affect scores.
Reference: <author> Salton, G., Wong, A., & Yang, C. S. </author> <year> (1975). </year> <title> A vector space model for automatic indexing. </title> <journal> Communications of the ACM, </journal> <volume> 18(11), </volume> <pages> 613-620. </pages>
Reference-contexts: Instructions for obtaining the TREC collection, and a list of the 99 queries we used, are included as an appendix. 2.3 UNDERLYING IR SYSTEM Throughout the remainder of this paper, natural language is not used directly. Rather, we use the vector space model of IR <ref> (Salton et al., 1975) </ref>.
Reference: <author> Shavlik, J. W., Mooney, R. J., & Towell, G. G. </author> <year> (1991). </year> <title> Symbolic and neural net learning algorithms: An empirical comparison. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 111-143. </pages>
Reference: <author> Voorhees, E. M., Gupta, N. K., & Johnson-Laird, B. </author> <year> (1994). </year> <title> The collection fusion problem. </title> <booktitle> Proceedings of the Third Text REtrieval Conference (TREC-3). </booktitle> <publisher> In press. </publisher>
Reference-contexts: We also evaluated genetic algorithms on this problem <ref> (Gupta & Voorhees, 1994) </ref>. We do not report these results because genetic algorithms did not match the ability of neural networks. The strategy based upon neural networks is essentially a straw man against which we will evaluate our systems.
Reference: <author> Voorhees, E. M., Gupta, N. K., & Johnson-Laird, B. </author> <year> (1995). </year> <title> Learning collection fusion strategies. </title> <booktitle> Proceedings 1995 ACM SIGIR Conference on Research and Development in Information Retrieval. </booktitle>
Reference-contexts: This total ordering is used simply to determine the order in which documents presented to a user. Tests revealed that k = 8 performed as well or better than other values <ref> (Voorhees et al., 1995) </ref>. Hence, all tests reported in the next section use this setting. 3.2 QUERY CLUSTERING The second fusion strategy, query clustering (QC), does not form an explicit model of a collection's relevant document distribution. <p> The only parameter affecting the performance of query clustering is L. Testing indicated that L = 100 is as effective or better than other values <ref> (Voorhees et al., 1995) </ref>. 3.3 NEURAL NETWORKS To apply neural networks (NN) to this task, we trained feed-forward networks (Rumelhart et al., 1986) to learn the optimal fusion for a given number of documents to be retrieved.
References-found: 14

