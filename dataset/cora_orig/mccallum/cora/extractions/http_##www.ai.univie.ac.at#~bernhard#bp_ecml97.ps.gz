URL: http://www.ai.univie.ac.at/~bernhard/bp_ecml97.ps.gz
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/biblio_ora?sort_by_author=yes&tailor=1&loc=0&format=ml/ml&keyword=Publications&keyword=WWW_ML&relop=/
Root-URL: 
Email: email: bernhard@cs.waikato.ac.nz  
Title: Compression-based Pruning of Decision Lists  
Author: Bernhard Pfahringer 
Address: New Zealand  
Affiliation: Department of Computer Science, University of Waikato,  
Abstract: We define a formula for estimating the coding costs of decision lists for propositional domains. This formula allows for multiple classes and both categorical and numerical attributes. For artificial domains the formula performs quite satisfactory, whereas results are rather mixed and inconclusive for natural domains. Further experiments lead to a principled simplification of the original formula which is robust in both artificial and natural domains. Simple hill-climbing search for the most compressive decision list significantly reduces the complexity of a given decision list while not impeding and sometimes even improving its predictive accuracy.
Abstract-found: 1
Intro-found: 1
Reference: <author> Cohen W.W.: </author> <title> Fast Effective Rule Induction, </title> <editor> in Prieditis A. and Russell S.(eds.), </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML95), </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <address> San Francisco, 115-123, </address> <year> 1995. </year>
Reference-contexts: It is interesting to note that most practical systems using some MDL-based formula either internally weigh the two summands differently (Quinlan 93a) or even allow the user to specify weights explicitly <ref> (Cohen 95, Oliveira & Sangiovanni-Vincentelli 95) </ref>. Fig. 2. Lymphgraphy: average error rates for various duplication factors.
Reference: <author> Forsyth R.S., Clarke D.D., Wright R.L.: </author> <title> Overfitting Revisited: An Information-Theoretic Approach to Simplifying Discrimination Trees, </title> <journal> JETAI Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 6,3, </volume> <year> 1994. </year>
Reference: <author> Furnkranz J.: </author> <title> Pruning Algorithms for Rule Learning, </title> <journal> Osterreichisches Forschungsin-stitut fur Artificial Intelligence, Wien, </journal> <volume> TR-96-07, </volume> <year> 1996. </year>
Reference: <author> Georgeff M.P., Wallace C.S.: </author> <title> A General Selection Criterion for Inductive Inference, </title> <booktitle> in O'Shea T.(ed.), Proceedings of the Sixth European Conference on Artificial Intelligence (ECAI-84), </booktitle> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction The Minimum Description Length (MDL) Principle (Rissanen 86), also called the Minimum Message Length (MML) Principle <ref> (Georgeff & Wallace 84) </ref>, has been successfully applied in Machine Learning for a broad variety of problems (for a just a few selected papers see Quinlan & Rivest 89, Quinlan 93a, Forsyth et al. 94, Pfahringer 95a, Muggleton et al. 92).
Reference: <author> Kohavi R.: </author> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs, </title> <institution> Computer Science Dept., Stanford University, Stanford, CA 94305, USA, </institution> <type> PhD Dissertation, </type> <year> 1995. </year>
Reference-contexts: Instead of relying on the probably overly simple MDL1 formula of figure 4, one could try to estimate the appropriate value for f dup of equation 9 for a given domain. Cross validation in a kind of wrapper approach <ref> (Kohavi 95) </ref> should be the right tool for this task. Comparison to other pruning methods which are not based on a variation of the MDL principle are also necessary. A good starting point for this endeavour should be the work described in Furnkranz 96.
Reference: <author> Kononenko I.: </author> <title> On Biases in Estimating the Multi-Valued Attributes, </title> <editor> in Mellish C.S.(ed.), </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.1034-1040, </address> <year> 1995. </year>
Reference-contexts: To reiterate, its improvements are a better estimate based on the finite-length property of all encoded strings <ref> (as proposed in Kononenko 95) </ref>, and its extended applicability.
Reference: <author> Kramer S.: </author> <title> Structural Regression Trees, </title> <booktitle> in Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Cambridge, MA, pp.812-819, </address> <year> 1996. </year>
Reference-contexts: Another promising direction should be the construction of a similar formula applicable to the prediction of continuous class values. This seems to be a prevalent problem in practise and has therefore got some attention in the Machine Learning community recently <ref> (Kramer 96, Quinlan 93b, Weiss & Indurkhya 95) </ref>. Instead of relying on the probably overly simple MDL1 formula of figure 4, one could try to estimate the appropriate value for f dup of equation 9 for a given domain.
Reference: <author> Merz C.J., </author> <title> Murphy P.M.: UCI Repository of machine learning databases, </title> <institution> University of California, Department of Information and Computer Science, </institution> <address> Irvine, CA, </address> <year> 1996. </year> <note> [http://www.ics.uci.edu/ mlearn/MLRepository.html] Muggleton S., </note> <author> Srinivasan A., Bain M.: </author> <title> Compression, Significance, and Accuracy, </title> <editor> in Sleeman D. and Edwards P.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Ninth International Workshop (ML92), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.338-347, </address> <year> 1992. </year>
Reference-contexts: And except for Parity5-5 at the 20% class noise level error rates are never dramatically worse, usually their difference is not statistically significant (as judged by a paired t-test). 4.2 Natural Domains For experiments involving more natural domains we have chosen a few standard databases available from the UCI repository <ref> (Merz & Murphy 96) </ref>. These selected databases range from small to mid-size with regard to number of examples. Some databases comprise solely categorical attributes, whereas others comprise solely numerical attributes, and finally there are a few comprising both categorical and numerical attributes.
Reference: <author> Oliveira A., Sangiovanni-Vincentelli A.: </author> <title> Inferring Reduced Ordered Decision Graphs of Minimal Description Length, </title> <editor> in Prieditis A. and Russell S.(eds.), </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML95), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1995. </year>
Reference-contexts: It is interesting to note that most practical systems using some MDL-based formula either internally weigh the two summands differently (Quinlan 93a) or even allow the user to specify weights explicitly <ref> (Cohen 95, Oliveira & Sangiovanni-Vincentelli 95) </ref>. Fig. 2. Lymphgraphy: average error rates for various duplication factors.
Reference: <author> Pfahringer B.: </author> <title> Practical Uses of the Minimum Description Length Principle in Induc--tive Learning, </title> <institution> Institut fur Med.Kybernetik u. AI, Technische Universitat Wien, Dissertation, </institution> <year> 1995. </year>
Reference-contexts: But recently also some problems with the MDL principle have been discovered (Quinlan 94, Quinlan 95). The problematic formula described in these papers is used by the C4.5rules system for pruning the rule-sets of each class separately. Despite these theoretical concerns, in our own experimental experience <ref> (see e.g. results reported in Pfahringer 95a) </ref> we have found C4.5rules to be a rather robust and reliable learner. The predictive accuracy of the induced ordered rule-sets (i.e. decision lists) is usually quite good, yet the rule-sets appear to be overly complex.
Reference: <author> Pfahringer B.: </author> <title> A New MDL Measure for Robust Rule Induction (Extended Abstract), </title> <editor> in Lavrac N. and Wrobel S.(eds.), </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-95, </address> <publisher> Springer, </publisher> <address> Berlin Heidelberg New York, pp.331-334, </address> <year> 1995. </year>
Reference-contexts: But recently also some problems with the MDL principle have been discovered (Quinlan 94, Quinlan 95). The problematic formula described in these papers is used by the C4.5rules system for pruning the rule-sets of each class separately. Despite these theoretical concerns, in our own experimental experience <ref> (see e.g. results reported in Pfahringer 95a) </ref> we have found C4.5rules to be a rather robust and reliable learner. The predictive accuracy of the induced ordered rule-sets (i.e. decision lists) is usually quite good, yet the rule-sets appear to be overly complex.
Reference: <author> Quinlan, J.R.: </author> <title> Simplifying decision trees. Proc Workshop on Knowledge Acquisition for Knowledge-based Systems, Banff, </title> <editor> Canada.(1986) Quinlan J.R., Rivest R.L.: </editor> <title> Inferring Decision Trees Using the Minimum Description Length Principle, </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference: <author> Quinlan, J.R.: C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: It is interesting to note that most practical systems using some MDL-based formula either internally weigh the two summands differently <ref> (Quinlan 93a) </ref> or even allow the user to specify weights explicitly (Cohen 95, Oliveira & Sangiovanni-Vincentelli 95). Fig. 2. Lymphgraphy: average error rates for various duplication factors.
Reference: <author> Quinlan J.R.: </author> <title> Combining Instance-Based and Model-Based Learning, </title> <booktitle> in Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.236-243, </address> <year> 1993. </year>
Reference-contexts: It is interesting to note that most practical systems using some MDL-based formula either internally weigh the two summands differently <ref> (Quinlan 93a) </ref> or even allow the user to specify weights explicitly (Cohen 95, Oliveira & Sangiovanni-Vincentelli 95). Fig. 2. Lymphgraphy: average error rates for various duplication factors.
Reference: <author> Quinlan J.R.: </author> <title> The Minimum Description Length Principle and Categorical Theories, in Cohen W.W. and Hirsh H.(eds.), Machine Learning, </title> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, pp.233-241, </address> <year> 1994. </year>
Reference-contexts: But recently also some problems with the MDL principle have been discovered <ref> (Quinlan 94, Quinlan 95) </ref>. The problematic formula described in these papers is used by the C4.5rules system for pruning the rule-sets of each class separately.
Reference: <author> Quinlan J.R.: </author> <title> MDL and Categorical Theories (Continued), </title> <editor> in Prieditis A. and Russell S.(eds.), </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML95), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1995. </year>
Reference: <author> Rissanen J.: </author> <title> Stochastic Complexity and Modeling, </title> <journal> in The Annals of Statistics, </journal> <volume> 14(3),p.1080-1100, </volume> <year> 1986. </year>
Reference-contexts: 1 Introduction The Minimum Description Length (MDL) Principle <ref> (Rissanen 86) </ref>, also called the Minimum Message Length (MML) Principle (Georgeff & Wallace 84), has been successfully applied in Machine Learning for a broad variety of problems (for a just a few selected papers see Quinlan & Rivest 89, Quinlan 93a, Forsyth et al. 94, Pfahringer 95a, Muggleton et al. 92).
Reference: <author> Weiss S.M., Indurkhya N.: </author> <title> Rule-based Machine Learning Methods for Functional Prediction, </title> <journal> Journal of Artificial Intelligence Research 3 (1995), </journal> <year> 1995. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
References-found: 18

