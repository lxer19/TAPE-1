URL: ftp://ftp.cs.toronto.edu/pub/stergios/art.ps.gz
Refering-URL: http://www.cs.toronto.edu/~stergios/publications.html
Root-URL: http://www.cs.toronto.edu
Title: on Workstation Clusters and Network-based  Parallel Application Scheduling on Networks of Workstations  
Author: Stergios V. Anastasiadis and Kenneth C. Sevcik 
Address: Canada  
Affiliation: Computer Systems Research Institute University of Toronto  
Date: June 1997  
Note: To appear in JPDC, Special Issue  
Pubnum: Computing,  
Abstract: Parallel applications can be executed using the idle computing capacity of workstation clusters. However, it remains unclear how to most effectively schedule the processors among different applications. Processor scheduling algorithms that were successful for shared-memory machines have proven to be inadequate for distributed memory environments due to the high costs of remote memory accesses and redistributing data. We investigate how knowledge of system load and application characteristics can be used in scheduling decisions. We propose a new algorithm based on adaptive equipartitioning, which, by properly exploiting both the information types above, performs better than other non-preemptive scheduling rules, and nearly as well as idealized versions of preemptive rules (with free preemption). We conclude that the new algorithm is suitable for use in scheduling parallel applications on networks of workstations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anastasiadis, S. </author> <title> Parallel application scheduling on networks of workstations. </title> <type> Master's thesis, Technical Report CSRI-342, </type> <institution> Department of Computer Science, University of Toronto, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: Application Characteristics Maximum Parallelism p max Execution Time on one Processor T (1) Execution Time on p &gt; 0 Processors T (p) Load Parameters Number of Waiting Jobs Number of Running Jobs to now is given by Anastasiadis <ref> [1] </ref>. 3.1 Dynamic Equipartition Policy In the Dynamic Equipartition (DYN-EQUI) policy, the processors are dynamically partitioned as equally as possible among the applications in the system. Special provisions are taken so that no application is given more processors that it can use.
Reference: [2] <author> Arpaci, R. H., Dusseau, A. C., Vahdat, A. M., Liu, L. T., Anderson, T. E., and Patterson, D. A. </author> <title> The interaction of parallel and sequential workloads on a network of workstations. </title> <booktitle> In ACM 29 SIGMETRICS/PERFORMANCE Joint Conf. on Measurement and Modeling of Computer Systems (May 1995), </booktitle> <pages> pp. 267-278. </pages>
Reference-contexts: In this study the parallel applications are kept apart from the sequential workload in the system. (The interaction between sequential and parallel jobs has been studied by Arpaci, et al. <ref> [2] </ref>). We assume that each application is assigned a number of processors between one and a maximum number that can be used effectively, p max . In reality, time constraints, memory requirements or even debugging procedures may entail minimum allocation limits for some applications though.
Reference: [3] <author> Carriero, N., Freeman, E., Gelenter, D., and Kaminsky, D. </author> <title> Adaptive parallelism and piranha. </title> <booktitle> Computer 28, </booktitle> <month> 1 (Jan. </month> <year> 1995), </year> <pages> 40-49. </pages>
Reference-contexts: This involves considerable overhead, which can outweigh the benefits of the processor reallocation itself, as has been demonstrated both analytically and experimentally [6, 18]. Sophisticated run-time systems are required to do dynamic data redistribution automatically, and keep it from being an additional burden on the application writer <ref> [3, 8, 9] </ref>. Recently, it has been shown by Feitelson and Nitzberg [10] that application characteristics, such as execution time on a given number of processors, can be estimated. These estimates can be exploited to achieve improved scheduling.
Reference: [4] <author> Chiang, S.-H., Mansharamani, R. K., and Vernon, M. K. </author> <title> Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems (May 1994), </booktitle> <pages> pp. 33-44. </pages>
Reference-contexts: The main objective of the scheduler is the minimization of the mean response time provided by the system, since the rationale for parallel processing is the fast execution of time-consuming applications. Mean response time has been the primary performance metric in most similar studies <ref> [4, 15, 16, 22, 25] </ref>. However, there are a few other studies that use either the throughput or the power (ratio of throughput to response time), instead, in order to capture the system performance [18, 20]. <p> Variations of this policy (under several different names) have been evaluated in many comparative studies of multiprocessor scheduling policies. The Dynamic Equipartition based rules have generally been found to yield excellent performance in shared-memory environments (where the overhead of changing processor allocations can be kept low) <ref> [4, 12, 15, 16, 25] </ref>. 3.2 Adaptive Policies Adaptive policies assign a number of processors to each application when it is initiated, and the processors are freed only when the application completes. <p> When processors are released, jobs in the queue are activated in order by allocating each a number of processors equal to the minimum of their maximum parallelism and the number of idle processors <ref> [4, 14] </ref>. When SDF is used with a fixed maximum allocation limit, it is called SDF-Max. <p> When SDF is used with a fixed maximum allocation limit, it is called SDF-Max. Such a fixed maximum allocation limit, independent of the application characteristics, was shown to be very helpful for non-preemptive policies to keep applications from being allocated too many processors when system load is high <ref> [4] </ref>. In addition to SDF as defined above, combinations of SDF with the previously defined adaptive policies can be introduced. For example, ASP can keep the waiting jobs in non-decreasing order 7 of their total demand. <p> In addition, large jobs account for 1 8 of jobs in the workload with small jobs accounting for the other 7 8 . The generation of the W values is done by using a 2-stage hyper exponential distribution, as in other simulation studies for the simulated workload <ref> [4] </ref>. The corre sponding mean value is 13.76 and the coefficient of variation is 3.5. <p> In particular, for good speedup (WK1), the policies SDF, SDF-Max=2, SDF-Max=6 saturate at load 90%. For poor speedup (WK3), even a load of 50% makes the mean response time of SDF-Max=6 and SDF infinite. The excellent performance reported for SDF in a previous study <ref> [4] </ref> is illustrated there only for good speedup jobs (WK1) and load 70%. Our experiments do not contradict this specific result. In figure 3, notice that the higher the system load, the smaller the Max parameter must be to obtain the best performance. <p> Thus, the desired properties of the adaptive policies are orthogonal to those of SDF. Therefore, we have a clear advantage by combining these different features, and not treating them as separate algorithms with incompatible characteristics as has been done previously <ref> [4] </ref>. An important observation from figure 6 (a) is the change in the relative performance among AEP (1), ASP (1) or AP1 (1) when compared to that of AEP, ASP and AP1.
Reference: [5] <author> Devarakonda, M. V., and Iyer, R. K. </author> <title> Predictability of process resource usage: A measurement-based study on unix. </title> <journal> IEEE Trans. Software Engineering 15, </journal> <month> 12 (Dec. </month> <year> 1989), </year> <pages> 1579-1586. </pages>
Reference-contexts: Furthermore, the predictive power of the estimates can be improved by using more sophisticated job behavior models than just the mean of previous runs <ref> [5, 18, 24, 26] </ref>. In particular, Wu [26] has shown that the execution time functions of real applications, with varying sizes and structures, can be represented accurately using the model introduced by Sevcik [24].
Reference: [6] <author> Dussa, K., Carlson, B., Dowdy, L., and Park, K.-H. </author> <title> Dynamic partitioning in a transputer environment. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems (May 1990), </booktitle> <pages> pp. 203-213. </pages>
Reference-contexts: In distributed systems, the applications must dynamically redistribute their data structures. This involves considerable overhead, which can outweigh the benefits of the processor reallocation itself, as has been demonstrated both analytically and experimentally <ref> [6, 18] </ref>. Sophisticated run-time systems are required to do dynamic data redistribution automatically, and keep it from being an additional burden on the application writer [3, 8, 9].
Reference: [7] <author> Dusseau, A. C., Arpaci, R. H., and Culler, D. E. </author> <title> Effective distributed scheduling of parallel workloads. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems (May 1996), </booktitle> <pages> pp. 25-36. </pages>
Reference-contexts: It is the responsibility of the appli-cation to determine how the allocated partition will be used and whether multiple threads will be interleaved on individual processors or not. The program developer makes the appropriate decision according to the computation, synchronization and communication needs of the application <ref> [7, 22] </ref>. The scheduling decisions are taken on a single node, where a central queue of job requests is maintained.
Reference: [8] <author> Edjlali, G., Agrawal, G., Sussman, A., and Saltz, J. </author> <title> Data parallel programming in an adaptive environment. </title> <booktitle> In Internat. Parallel Processing Symposium (Apr. </booktitle> <year> 1995), </year> <pages> pp. 827-832. </pages>
Reference-contexts: This involves considerable overhead, which can outweigh the benefits of the processor reallocation itself, as has been demonstrated both analytically and experimentally [6, 18]. Sophisticated run-time systems are required to do dynamic data redistribution automatically, and keep it from being an additional burden on the application writer <ref> [3, 8, 9] </ref>. Recently, it has been shown by Feitelson and Nitzberg [10] that application characteristics, such as execution time on a given number of processors, can be estimated. These estimates can be exploited to achieve improved scheduling.
Reference: [9] <author> Feeley, M. J., Bershad, B. N., Chase, J. S., and Levy, H. M. </author> <title> Dynamic node reconfiguration in a parallel-distributed environment. </title> <booktitle> In ACM Symp. Principles and Practice of Parallel Programming (1991), </booktitle> <pages> pp. 114-121. </pages>
Reference-contexts: This involves considerable overhead, which can outweigh the benefits of the processor reallocation itself, as has been demonstrated both analytically and experimentally [6, 18]. Sophisticated run-time systems are required to do dynamic data redistribution automatically, and keep it from being an additional burden on the application writer <ref> [3, 8, 9] </ref>. Recently, it has been shown by Feitelson and Nitzberg [10] that application characteristics, such as execution time on a given number of processors, can be estimated. These estimates can be exploited to achieve improved scheduling.
Reference: [10] <author> Feitelson, D. G., and Nitzberg, B. </author> <title> Job characteristics of a production parallel scientific workload on the nasa ames ipsc/860. </title> <booktitle> In IPPS '95 Workshop on Job Scheduling Strategies for Parallel Processing (Apr. </booktitle> <year> 1995), </year> <pages> pp. 215-227. 30 </pages>
Reference-contexts: Sophisticated run-time systems are required to do dynamic data redistribution automatically, and keep it from being an additional burden on the application writer [3, 8, 9]. Recently, it has been shown by Feitelson and Nitzberg <ref> [10] </ref> that application characteristics, such as execution time on a given number of processors, can be estimated. These estimates can be exploited to achieve improved scheduling. <p> The computational work W represents the minimum total service demand of the job. (fi and ff are typically smaller than W [26].) To obtain realistic workload parameters, we use the statistics gathered on a 128-node iPSC/860 hypercube message-passing system at NASA Ames by Feitelson 13 and Nitzberg <ref> [10] </ref>. We calculated the total demand of the jobs of each partition size, by multiplying the partition size with the corresponding mean job runtime. <p> These values correspond to 12%, 50% and 100% of the 32 processors in the system model. Since there has been no reliable study of the actual maximum parallelism distribution of real applications, we assume it to be uniform, like the application partition size distribution observed by Feitelson and Nitzberg <ref> [10] </ref>. 14 Job Speedup The fundamental job characteristic that remains to be defined is the speedup function, S (p), where p is the number of processors.
Reference: [11] <author> Ibaraki, T., and Katoh, N. </author> <title> Resource Allocation Problems: Algorithmic Approaches. </title> <booktitle> Series in the Foundations of Computing. </booktitle> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Several algorithms that solve the problem in polynomial time are presented by Ibaraki and Katoh <ref> [11] </ref>. For our experiments, 8 we implemented a simpler algorithm, which allocates the available processors one at a time to the waiting job that achieves the largest decrease in its response time. We use the name Differential Allocation Policy (DIF) to describe this aspect of allocation policies.
Reference: [12] <author> Leutenegger, S. T., and Vernon, M. K. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems (May 1990), </booktitle> <pages> pp. 226-236. </pages>
Reference-contexts: Variations of this policy (under several different names) have been evaluated in many comparative studies of multiprocessor scheduling policies. The Dynamic Equipartition based rules have generally been found to yield excellent performance in shared-memory environments (where the overhead of changing processor allocations can be kept low) <ref> [4, 12, 15, 16, 25] </ref>. 3.2 Adaptive Policies Adaptive policies assign a number of processors to each application when it is initiated, and the processors are freed only when the application completes.
Reference: [13] <author> Leuze, M. R., Dowdy, L. W., and Park, K.-H. </author> <title> Multiprogramming a distributed-memory multiprocessor. </title> <journal> Concurrency: Practice and Experience 1, </journal> <month> 1 (Sept. </month> <year> 1989), </year> <pages> 19-33. </pages>
Reference-contexts: We consider a collection of interconnected nodes, each consisting of a processor and a local memory. The nodes are assumed identical, and communicate with each other with negligible latency differences. Different applications can run on separate partitions without degradation in performance due to contention in the communication medium <ref> [13] </ref>. This assumption may not be valid in the case of a heavily loaded Ethernet-based installation, but is acceptable in the context of ATM interconnects, which are expected to be prevalent soon. We assume that the operating system can support two-level scheduling.
Reference: [14] <author> Majumdar, S., Eager, D. L., and Bunt, R. B. </author> <title> Scheduling in multiprogrammed parallel systems. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems (May 1988), </booktitle> <pages> pp. 104-113. </pages>
Reference-contexts: When processors are released, jobs in the queue are activated in order by allocating each a number of processors equal to the minimum of their maximum parallelism and the number of idle processors <ref> [4, 14] </ref>. When SDF is used with a fixed maximum allocation limit, it is called SDF-Max.
Reference: [15] <author> McCann, C., Vaswani, R., and Zahorjan, J. </author> <title> A dynamic processor allocation policy for mul-tiprogrammed shared-memory multiprocessors. </title> <journal> ACM Trans. Computer Systems 11, </journal> <month> 2 (May </month> <year> 1993), </year> <pages> 146-178. </pages>
Reference-contexts: The main objective of the scheduler is the minimization of the mean response time provided by the system, since the rationale for parallel processing is the fast execution of time-consuming applications. Mean response time has been the primary performance metric in most similar studies <ref> [4, 15, 16, 22, 25] </ref>. However, there are a few other studies that use either the throughput or the power (ratio of throughput to response time), instead, in order to capture the system performance [18, 20]. <p> Variations of this policy (under several different names) have been evaluated in many comparative studies of multiprocessor scheduling policies. The Dynamic Equipartition based rules have generally been found to yield excellent performance in shared-memory environments (where the overhead of changing processor allocations can be kept low) <ref> [4, 12, 15, 16, 25] </ref>. 3.2 Adaptive Policies Adaptive policies assign a number of processors to each application when it is initiated, and the processors are freed only when the application completes.
Reference: [16] <author> McCann, C., and Zahorjan, J. </author> <title> Processor scheduling in shared memory multiprocessors. </title> <type> Tech. Rep. </type> <institution> 89-09-17, Computer Science Department, University of Washington at Seattle, </institution> <year> 1989. </year>
Reference-contexts: The main objective of the scheduler is the minimization of the mean response time provided by the system, since the rationale for parallel processing is the fast execution of time-consuming applications. Mean response time has been the primary performance metric in most similar studies <ref> [4, 15, 16, 22, 25] </ref>. However, there are a few other studies that use either the throughput or the power (ratio of throughput to response time), instead, in order to capture the system performance [18, 20]. <p> Variations of this policy (under several different names) have been evaluated in many comparative studies of multiprocessor scheduling policies. The Dynamic Equipartition based rules have generally been found to yield excellent performance in shared-memory environments (where the overhead of changing processor allocations can be kept low) <ref> [4, 12, 15, 16, 25] </ref>. 3.2 Adaptive Policies Adaptive policies assign a number of processors to each application when it is initiated, and the processors are freed only when the application completes. <p> The pure adaptive policies use only the maximum parallelism of each application in doing processor allocation, while the above composite policies require also the total demand of each application. 3.4 Differential Allocation Policies McCann and Zahorjan <ref> [16] </ref> introduce the Run-To-Completion policy, where each released processor is allocated to the waiting job for which the expected reduction in elapsed execution time is greatest, ensuring that as many jobs as possible are activated. Wu [26] extends the Run-To-Completion policy with a Shortest Demand First queue (RTC-SDF ). <p> Wu [26] extends the Run-To-Completion policy with a Shortest Demand First queue (RTC-SDF ). In general, use of the execution time function for each application allows the formulation of a non-linear discrete constrained optimization problem with the objective of minimizing the total execution time of the waiting jobs <ref> [16, 18, 24, 26] </ref>. <p> As we noted in section 3, the greedy approach of dispatching as many waiting jobs as possible is not intuitively the best <ref> [16, 26] </ref>. As will be shown below, there is a considerable performance improvement by replacing the greedy ASP with AEP. In figure 7, we have depicted the normalized Response Time of all three combinations, ASP (2), AP1 (2) and AEP (2).
Reference: [17] <author> Nguyen, T. D., Vaswani, R., and Zahorjan, J. </author> <title> Using runtime measured workload characteristics in parallel processor scheduling. In IPPS'96 Workshop on Job Scheduling Strategies for Parallel Processing (Apr. </title> <booktitle> 1996), </booktitle> <pages> pp. 93-104. </pages>
Reference-contexts: But both SDF and DIF need some form of execution time information in order to be realized. The approximation method proposed by Nguyen et al. <ref> [17] </ref> for shared-memory machines, where the best partition size of an application is determined by sample executions on different numbers of processors, induces overhead which can become non-negligible in distributed-memory systems.
Reference: [18] <author> Park, K.-H., and Dowdy, L. W. </author> <title> Dynamic partitioning of multiprocessor systems. </title> <booktitle> Internat. J. Parallel Programming 18, 2 (1989), </booktitle> <pages> 91-120. </pages>
Reference-contexts: In distributed systems, the applications must dynamically redistribute their data structures. This involves considerable overhead, which can outweigh the benefits of the processor reallocation itself, as has been demonstrated both analytically and experimentally <ref> [6, 18] </ref>. Sophisticated run-time systems are required to do dynamic data redistribution automatically, and keep it from being an additional burden on the application writer [3, 8, 9]. <p> Furthermore, the predictive power of the estimates can be improved by using more sophisticated job behavior models than just the mean of previous runs <ref> [5, 18, 24, 26] </ref>. In particular, Wu [26] has shown that the execution time functions of real applications, with varying sizes and structures, can be represented accurately using the model introduced by Sevcik [24]. <p> Mean response time has been the primary performance metric in most similar studies [4, 15, 16, 22, 25]. However, there are a few other studies that use either the throughput or the power (ratio of throughput to response time), instead, in order to capture the system performance <ref> [18, 20] </ref>. The load parameters and application characteristics used by the scheduler in the present study are given in table I. 3 The Scheduling Policies In this section, we describe in detail some processor allocation algorithms demonstrated to perform well in previous studies. <p> Wu [26] extends the Run-To-Completion policy with a Shortest Demand First queue (RTC-SDF ). In general, use of the execution time function for each application allows the formulation of a non-linear discrete constrained optimization problem with the objective of minimizing the total execution time of the waiting jobs <ref> [16, 18, 24, 26] </ref>.
Reference: [19] <author> Parsons, E. W., and Sevcik, K. C. </author> <title> Coordinated allocation of memory and processors in multiprocessors. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems (June 1996), </booktitle> <pages> pp. 56-67. 31 </pages>
Reference: [20] <author> Rosti, E., Smirni, E., Dowdy, L., Serazzi, G., and Carlson, B. M. </author> <title> Robust partitioning policies of multiprocessor systems. Performance Evaluation 19, </title> <month> 2-3 </month> <year> (1994), </year> <pages> 141-165. </pages>
Reference-contexts: Mean response time has been the primary performance metric in most similar studies [4, 15, 16, 22, 25]. However, there are a few other studies that use either the throughput or the power (ratio of throughput to response time), instead, in order to capture the system performance <ref> [18, 20] </ref>. The load parameters and application characteristics used by the scheduler in the present study are given in table I. 3 The Scheduling Policies In this section, we describe in detail some processor allocation algorithms demonstrated to perform well in previous studies. <p> At low loads, a job is usually allocated a number of processors close to its maximum parallelism, while at high loads the partition sizes tend to be smaller. Adaptive Policy 1 Rosti et al. <ref> [20] </ref> introduce Adaptive Policy 1 (AP1) specifically for distributed memory systems. The target partition size at a given time is equal to the total number of processors in the system divided by the number of waiting jobs [20]. <p> Adaptive Policy 1 Rosti et al. <ref> [20] </ref> introduce Adaptive Policy 1 (AP1) specifically for distributed memory systems. The target partition size at a given time is equal to the total number of processors in the system divided by the number of waiting jobs [20]. If no processors are available when a job arrives, it joins a FIFO queue. Otherwise, it is allocated a number of processors equal to the minimum of its maximum parallelism, the target partition size, and the number of available processors. <p> Each workload incorporates three of these job types, as they are defined by the respective values of . The results for the pure speedup types are shown in figure 2. These curves are intentionally similar to the speedup curves that were used in the experiments of Rosti, et al. <ref> [20] </ref> and which were derived from a real application for several different inputs. However, in our study, we have three different maximum parallelism values f4; 16; 64g, instead of the one (16) that was used by Rosti, et al. [20]. 16 5.3 The Arrival Process The offered load of a multiprocessor <p> curves that were used in the experiments of Rosti, et al. <ref> [20] </ref> and which were derived from a real application for several different inputs. However, in our study, we have three different maximum parallelism values f4; 16; 64g, instead of the one (16) that was used by Rosti, et al. [20]. 16 5.3 The Arrival Process The offered load of a multiprocessor system with P servers is defined as follows : Load = E (T (1)) P fi M ean Interarrival T ime (9) where E (T (1)) is the mean total execution time of the jobs on one processor [23]. <p> In the article that introduces AP1, power, which is defined as the ratio of throughput to response time, is used as the performance measure <ref> [20] </ref>. 22 10% 30% 50% 70% 90% Load 0.0 1.0 2.0 3.0 o r m R T i m ASP (1) AP1 (1) AEP (1) Waiting Running (a) Load 0 10 20 e a n S i z e WK4 right.
Reference: [21] <author> Setia, S., and Tripathi, S. </author> <title> A comparative analysis of static processor partitioning policies for parallel computers. </title> <booktitle> In Internat. Workshop on Modeling and Simulation of Computer and Telecommunication Systems (MASCOTS) (Jan. </booktitle> <year> 1993), </year> <pages> pp. 283-286. </pages>
Reference-contexts: Adaptive Static Partitioning Setia and Tripathi <ref> [21] </ref> introduce the Adaptive Static Partitioning (ASP) policy. If no processors are available when a job arrives, it joins a FIFO queue. Otherwise, it is allocated the minimum of its maximum parallelism and the number of available processors.
Reference: [22] <author> Setia, S. K., Squillante, M. S., and Tripathi, S. K. </author> <title> Processor scheduling on multiprogrammed distributed memory parallel computers. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems (1993), </booktitle> <pages> pp. 158-170. </pages>
Reference-contexts: It is the responsibility of the appli-cation to determine how the allocated partition will be used and whether multiple threads will be interleaved on individual processors or not. The program developer makes the appropriate decision according to the computation, synchronization and communication needs of the application <ref> [7, 22] </ref>. The scheduling decisions are taken on a single node, where a central queue of job requests is maintained. <p> The main objective of the scheduler is the minimization of the mean response time provided by the system, since the rationale for parallel processing is the fast execution of time-consuming applications. Mean response time has been the primary performance metric in most similar studies <ref> [4, 15, 16, 22, 25] </ref>. However, there are a few other studies that use either the throughput or the power (ratio of throughput to response time), instead, in order to capture the system performance [18, 20].
Reference: [23] <author> Sevcik, K. C. </author> <title> Characterizations of parallelism in applications and their use in scheduling. </title> <booktitle> In ACM SIGMETRICS Conf. Measurement and Modeling of Computer Systems (May 1989), </booktitle> <pages> pp. 171-180. </pages>
Reference-contexts: [20]. 16 5.3 The Arrival Process The offered load of a multiprocessor system with P servers is defined as follows : Load = E (T (1)) P fi M ean Interarrival T ime (9) where E (T (1)) is the mean total execution time of the jobs on one processor <ref> [23] </ref>. In the case of the jobs that have been used in our study, we have: E (T (1)) = E (W ) + E (fi) + E (ff): We already know that E (W ) = 13:76.
Reference: [24] <author> Sevcik, K. C. </author> <title> Application scheduling and processor allocation in multiprogrammed parallel processing systems. Performance Evaluation 19, </title> <month> 2-3 </month> <year> (1994), </year> <pages> 107-140. </pages>
Reference-contexts: Furthermore, the predictive power of the estimates can be improved by using more sophisticated job behavior models than just the mean of previous runs <ref> [5, 18, 24, 26] </ref>. In particular, Wu [26] has shown that the execution time functions of real applications, with varying sizes and structures, can be represented accurately using the model introduced by Sevcik [24]. <p> In particular, Wu [26] has shown that the execution time functions of real applications, with varying sizes and structures, can be represented accurately using the model introduced by Sevcik <ref> [24] </ref>. A least-squares approximation method is applied based on the runtimes of the application for (a few) different numbers of 2 processors. In this paper, we investigate how different levels of information about the application characteristics and the system load can be used to improve the processor allocation decisions. <p> In scheduling jobs with perfect speedup, average response time is minimized by activating jobs one at a time in SDF order and giving each one all P processors <ref> [24] </ref>. The arriving jobs are kept in the queue ordered according to non-decreasing total demand, that is, the execution time of the job on one processor. <p> Wu [26] extends the Run-To-Completion policy with a Shortest Demand First queue (RTC-SDF ). In general, use of the execution time function for each application allows the formulation of a non-linear discrete constrained optimization problem with the objective of minimizing the total execution time of the waiting jobs <ref> [16, 18, 24, 26] </ref>. <p> The computational requirements of the scheduling algorithms are assumed to be negligible. This is valid for the number of nodes typically involved in workstation clusters. 5.2 Workload Parameters A wide range of representative applications can be modeled by utilizing the execution time function form introduced by Sevcik <ref> [24] </ref>: T (p) = OE (p) p The parameter p is the number of processors allocated to the job and W the essential computational work. <p> Actually, the essential computation W determines the potential of a job to reduce its response time when given additional processors. The optimization procedure is not affected by the value of ff, since ff does not participate in the derivative of the execution time function <ref> [24] </ref>. Furthermore fi is important but in typical workloads is much less than W [26]. Thus the ordering of the jobs according to the runtime on a single processor (W + fi + ff) separates the jobs into groups with similar potential for processor exploitation.
Reference: [25] <author> Tucker, A., and Gupta, A. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In 12th ACM Symp. Operating System Principles (Dec. </booktitle> <year> 1989), </year> <pages> pp. 159-166. </pages>
Reference-contexts: The main objective of the scheduler is the minimization of the mean response time provided by the system, since the rationale for parallel processing is the fast execution of time-consuming applications. Mean response time has been the primary performance metric in most similar studies <ref> [4, 15, 16, 22, 25] </ref>. However, there are a few other studies that use either the throughput or the power (ratio of throughput to response time), instead, in order to capture the system performance [18, 20]. <p> Special provisions are taken so that no application is given more processors that it can use. When the number of processors allocated to an application changes, the application adjusts the number of running processes accordingly <ref> [25] </ref>. Variations of this policy (under several different names) have been evaluated in many comparative studies of multiprocessor scheduling policies. <p> Variations of this policy (under several different names) have been evaluated in many comparative studies of multiprocessor scheduling policies. The Dynamic Equipartition based rules have generally been found to yield excellent performance in shared-memory environments (where the overhead of changing processor allocations can be kept low) <ref> [4, 12, 15, 16, 25] </ref>. 3.2 Adaptive Policies Adaptive policies assign a number of processors to each application when it is initiated, and the processors are freed only when the application completes.
Reference: [26] <author> Wu, C.-S. </author> <title> Processor scheduling in multiprogrammed shared memory numa multiprocessors. </title> <type> Master's thesis, Technical Report CSRI-341, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: Furthermore, the predictive power of the estimates can be improved by using more sophisticated job behavior models than just the mean of previous runs <ref> [5, 18, 24, 26] </ref>. In particular, Wu [26] has shown that the execution time functions of real applications, with varying sizes and structures, can be represented accurately using the model introduced by Sevcik [24]. <p> Furthermore, the predictive power of the estimates can be improved by using more sophisticated job behavior models than just the mean of previous runs [5, 18, 24, 26]. In particular, Wu <ref> [26] </ref> has shown that the execution time functions of real applications, with varying sizes and structures, can be represented accurately using the model introduced by Sevcik [24]. A least-squares approximation method is applied based on the runtimes of the application for (a few) different numbers of 2 processors. <p> Wu <ref> [26] </ref> extends the Run-To-Completion policy with a Shortest Demand First queue (RTC-SDF ). <p> Wu [26] extends the Run-To-Completion policy with a Shortest Demand First queue (RTC-SDF ). In general, use of the execution time function for each application allows the formulation of a non-linear discrete constrained optimization problem with the objective of minimizing the total execution time of the waiting jobs <ref> [16, 18, 24, 26] </ref>. <p> It is evident that, by choosing different values for the parameters OE (); W; fi and ff, we obtain descriptions of jobs with different characteristics and inherent structures. Imbalance and Essential Work The parameter OE (p) has been taken equal to one, since the real measurements conducted by Wu <ref> [26] </ref> indicate that it can be treated as constant typically with a value in the range 1:1 to 1:2. The computational work W represents the minimum total service demand of the job. (fi and ff are typically smaller than W [26].) To obtain realistic workload parameters, we use the statistics gathered <p> equal to one, since the real measurements conducted by Wu <ref> [26] </ref> indicate that it can be treated as constant typically with a value in the range 1:1 to 1:2. The computational work W represents the minimum total service demand of the job. (fi and ff are typically smaller than W [26].) To obtain realistic workload parameters, we use the statistics gathered on a 128-node iPSC/860 hypercube message-passing system at NASA Ames by Feitelson 13 and Nitzberg [10]. We calculated the total demand of the jobs of each partition size, by multiplying the partition size with the corresponding mean job runtime. <p> As we noted in section 3, the greedy approach of dispatching as many waiting jobs as possible is not intuitively the best <ref> [16, 26] </ref>. As will be shown below, there is a considerable performance improvement by replacing the greedy ASP with AEP. In figure 7, we have depicted the normalized Response Time of all three combinations, ASP (2), AP1 (2) and AEP (2). <p> The optimization procedure is not affected by the value of ff, since ff does not participate in the derivative of the execution time function [24]. Furthermore fi is important but in typical workloads is much less than W <ref> [26] </ref>. Thus the ordering of the jobs according to the runtime on a single processor (W + fi + ff) separates the jobs into groups with similar potential for processor exploitation. <p> In addition, the enhanced adaptive policies have the advantage of the Shortest Demand First discipline, which minimizes the expected waiting time. Dynamic algorithms could also benefit from using load and job specific information <ref> [26] </ref>. However, the estimation of the execution time function for a dynamic policy is not as straightforward as in the case of static partitioning, due to the continuous change in the number of processors.
Reference: [27] <author> Zhou, S., Zheng, X., Wang, J., and Delisle, P. </author> <title> Utopia: a load sharing facility for large, heterogeneous distributed computer systems. </title> <journal> Software-Practice and Experience 23, </journal> <month> 12 (Dec. </month> <year> 1993), </year> <pages> 1305-1336. </pages>
Reference-contexts: The scheduling decisions are taken on a single node, where a central queue of job requests is maintained. Centralized algorithms that can take advantage of the clustering properties in small or large scale distributed systems have been proven successful in recent resource management environments <ref> [27] </ref> without restricting the system scalability. In this study the parallel applications are kept apart from the sequential workload in the system. (The interaction between sequential and parallel jobs has been studied by Arpaci, et al. [2]).
References-found: 27

