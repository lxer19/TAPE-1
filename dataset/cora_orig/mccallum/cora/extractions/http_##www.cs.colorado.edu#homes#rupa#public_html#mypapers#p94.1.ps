URL: http://www.cs.colorado.edu/homes/rupa/public_html/mypapers/p94.1.ps
Refering-URL: http://www.cs.colorado.edu/~rupa/mypapers.html
Root-URL: http://www.cs.colorado.edu
Title: A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction  
Author: Sreerupa Das and Michael C. Mozer 
Address: Boulder, CO 80309-0430  
Affiliation: Department of Computer Science University of Colorado  
Abstract: Although recurrent neural nets have been moderately successful in learning to emulate finite-state machines (FSMs), the continuous internal state dynamics of a neural net are not well matched to the discrete behavior of an FSM. We describe an architecture, called DOLCE, that allows discrete states to evolve in a net as learning progresses. dolce consists of a standard recurrent neural net trained by gradient descent and an adaptive clustering technique that quantizes the state space. dolce is based on the assumption that a finite set of discrete internal states is required for the task, and that the actual network state belongs to this set but has been corrupted by noise due to inaccuracy in the weights. dolce learns to recover the discrete state with maximum a posteriori probability from the noisy state. Simulations show that dolce leads to a significant improvement in generalization performance over earlier neural net approaches to FSM induction.
Abstract-found: 1
Intro-found: 1
Reference: <author> S. Das & R. Das. </author> <title> (1991) Induction of discrete state-machine by stabilizing a continuous recurrent network using clustering. </title> <booktitle> Computer Science and Informatics 21(2) </booktitle> <pages> 35-40. </pages> <note> Special Issue on Neural Computing. </note>
Reference: <author> J.L. Elman. </author> <title> (1990) Finding structure in time. </title> <booktitle> Cognitive Science 14 </booktitle> <pages> 179-212. </pages>
Reference: <author> E. Forgy. </author> <title> (1965) Cluster analysis of multivariate data: efficiency versus interpretability of classifications. </title> <type> Biometrics 21 </type> <pages> 768-780. </pages>
Reference: <author> M.C. Mozer & J.D Bachrach. </author> <title> (1990) Discovering the structure of a reactive environment by exploration. </title> <booktitle> Neural Computation 2(4) </booktitle> <pages> 447-457. </pages>
Reference: <author> C. McMillan, M.C. Mozer, & P. Smolensky. </author> <title> (1992) Rule induction through integrated symbolic and subsymbolic processing. </title> <editor> In J.E. Moody, S.J. Hanson, & R.P. Lippmann (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> 969-976. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> C.L. Giles, D. Chen, C.B. Miller, H.H. Chen, G.Z. Sun, & Y.C. Lee. </author> <title> (1992) Learning and extracting finite state automata with second-order recurrent neural network. </title> <booktitle> Neural Computation 4(3) </booktitle> <pages> 393-405. </pages>
Reference: <author> H. Schutze. </author> <title> (1993) Word space. </title> <editor> In S.J. Hanson, J.D. Cowan, & C.L. Giles (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> 895-902. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> M. Tomita. </author> <title> (1982) Dynamic construction of finite-state automata from examples using hill-climbing. </title> <booktitle> Proceedings of the Fourth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 105-108. </pages>
Reference: <author> G. Towell & J. Shavlik. </author> <title> (1992) Interpretion of artificial neural networks: mapping knowledge-based neural networks into rules. </title> <editor> In J.E. Moody, S.J. Hanson, & R.P. Lipp-mann (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> 977-984. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> R.L. Watrous & G.M. Kuhn. </author> <title> (1992) Induction of finite state languages using second-order recurrent networks. </title> <editor> In J.E. Moody, S.J. Hanson, & R.P. Lippmann (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> 969-976. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Z. Zeng, R. Goodman, & P. Smyth. </author> <title> (1993) Learning finite state machines with self-clustering recurrent networks. </title> <booktitle> Neural Computation 5(6) </booktitle> <pages> 976-990. </pages>
Reference-contexts: In a second approach, quantization is enforced during training by mapping the the hidden state at each time step to the nearest corner of a [0; 1] n hypercube <ref> (Zeng, Goodman, & Smyth, 1993) </ref>. Each of these approaches has its limitations. In the first approach, because learning does not consider the latter quantization, the hidden activity patterns that result from learning may not lie in natural clusters.
References-found: 11

