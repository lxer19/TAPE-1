URL: http://www.eecs.berkeley.edu/~ananth/DMCCR.ps
Refering-URL: http://www.eecs.berkeley.edu/~ananth/
Root-URL: 
Title: The Common Randomness Capacity of a Pair of Independent Discrete Memoryless Channels  
Author: S. Venkatesan V. Anantharam zx 
Keyword: Common randomness capacity, generating randomness from noise, interactive communication.  
Address: Berkeley.  Cory Hall,  Berkeley, Berkeley, CA 94720.  
Affiliation: Cornell University and U.C. Berkeley. Univ. of California,  Dept. of EECS, U.C.  
Note: Research supported by NSF IRI 9005849, IRI 9310670, NCR 9422513, and the AT&T Foundation.  Address all correspondence to the second author: 570  
Date: 13 August 1995  
Abstract: We study the following problem: two agents Alice and Bob are connected to each other by independent discrete memoryless channels. They wish to generate common randomness by communicating interactively over the two channels. Assuming that Alice and Bob are allowed access to independent external random sources at rates (in bits per step of communication) of H A and H B , respectively, we show that they can generate common randomness at a rate of max f min [H A + H(W jQ); I(P ; V )] + min [H B + H(V jP ); I(Q; W )] g bits per step. Here, V is the channel from Alice to Bob, and W is the channel from Bob to Alice. The maximum is over all probability distributions P and Q on the input alphabets of V and W respectively. We also prove a strong converse which establishes the above rate as the highest attainable in this situation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Ahlswede. </author> <title> Elimination of correlation in random codes for arbitrarily varying channels. </title> <journal> Z. Wahrsch. Verw. Gebiete, </journal> <volume> 33 </volume> <pages> 159-175, </pages> <year> 1978. </year>
Reference: [2] <author> R. Ahlswede and I. Csiszar. </author> <title> Common randomness in information theory and cryptography part I: Secret sharing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 39(No. 4), </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 Introduction As pointed out by Ahlswede and Csiszar in <ref> [2] </ref>, there are several situations in which common randomness available to communicating agents plays a significant role. <p> Finally, in the theory of communication complexity, it is known that common randomness can significantly reduce the amount of inter-processor communication required to perform certain computations in a distributed setting ([8], [11]). For these and other reasons, Ahlswede and Csiszar <ref> [2] </ref> proposed a systematic study of the role of common randomness in information theory. Now, in a situation where the communicating agents only have access to independent random sources, they must set up common randomness by exchanging the outputs of their respective sources. <p> In this context, the problem is one of secret sharing, i.e., generating common randomness at two terminals without giving information about it to an eavesdropper. This has recently been addressed by Maurer ([9], [10]), and Ahlswede and Csiszar <ref> [2] </ref>. In the "channel-type" model introduced in [2], the two terminals are connected by a discrete memoryless channel with one input and two outputs. One terminal governs the input, while the outputs are seen by the other terminal and the wiretapper, respectively. <p> In this context, the problem is one of secret sharing, i.e., generating common randomness at two terminals without giving information about it to an eavesdropper. This has recently been addressed by Maurer ([9], [10]), and Ahlswede and Csiszar <ref> [2] </ref>. In the "channel-type" model introduced in [2], the two terminals are connected by a discrete memoryless channel with one input and two outputs. One terminal governs the input, while the outputs are seen by the other terminal and the wiretapper, respectively. <p> In contrast, no secrecy constraints are imposed in our model, i.e., the random outputs generated by the two agents need not be kept secret from any eavesdroppers. However, the results proved here are not implied by those of <ref> [2] </ref>, because both channels here are allowed to be noisy and constrained in capacity, and no restrictions are imposed on the allowed use of these two channels. 2 Statement of problem and result 2.1 Preliminaries A discrete memoryless channel (DMC) with input alphabet Z, output alphabet ^ Z, and transition probability
Reference: [3] <author> R. Ahlswede and G. Dueck. </author> <title> Identification in the presence of feedback - a discovery of new capacity formulas. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 35(No. 1), </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: For the simple special case of a DMC with perfect instantaneous feedback, the common randomness capacity was obtained by Ahlswede and Dueck in <ref> [3] </ref>, as an auxiliary result in the proof of their identification theorem. <p> In fact, in all the identification problems studied in <ref> [3] </ref> and [4], it turns out that the (second-order) identification capacity equals the (first-order) common randomness capacity. <p> These are slight generalizations of the results of Ahlswede and Dueck <ref> [3] </ref> for DMCs with feedback, mentioned in the Introduction.
Reference: [4] <author> R. Ahlswede and G. Dueck. </author> <title> Identification via channels. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 35(No. 1), </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: 1 Introduction As pointed out by Ahlswede and Csiszar in [2], there are several situations in which common randomness available to communicating agents plays a significant role. For example, in the theory of identification via noisy channels ([3], <ref> [4] </ref>, [5]), the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up. <p> In fact, in all the identification problems studied in [3] and <ref> [4] </ref>, it turns out that the (second-order) identification capacity equals the (first-order) common randomness capacity. These results were an important motivation for our study of the common randomness capacity in the general case, where both agents can play an "active" role and both channels are allowed to be noisy.
Reference: [5] <author> R. Ahlswede and B. Verboven. </author> <title> On identification via multiway channels with feedback. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 37(No. 5), </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: 1 Introduction As pointed out by Ahlswede and Csiszar in [2], there are several situations in which common randomness available to communicating agents plays a significant role. For example, in the theory of identification via noisy channels ([3], [4], <ref> [5] </ref>), the maximum achievable identification rate is essentially determined by the amount of common randomness that the transmitter and receiver can set up.
Reference: [6] <author> I. Csiszar and J. Korner. </author> <title> Information Theory: Coding Theorems for Discrete Memoryless Systems. </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: W (Z; ^ Z) will denote the set of all DMCs with input alphabet Z and output alphabet ^ Z. The notation for all standard information-theoretic quantities is that of <ref> [6] </ref>. All logarithms and exponentials will be to the base two. <p> Here, E (R; P; U ) = min h + i E (R; P; U ) is a continuous function of R and P , which is positive if R &lt; I (P ; U ) and zero otherwise. Proof: Standard. See Theorem 5.2 on p. 165 of <ref> [6] </ref>. 2 Definition 3.2 Let c 2 Z t and C ^ Z t . <p> Now, by condition (a) in the definition of E, we have j ^ P (^x)P V (^x)j = p for all ^x. It follows by the continuity of the entropy function (see, e.g., Lemma 2.7 on p. 33 of <ref> [6] </ref>) that, for all large n, jH ( ^ P ) H (P V )j j ^ X j (= n) log ( n=): (27) Similarly, by condition (b) in the definition of E, j ^ Q t (^y) Q t W (^y)j = p all ^y, and p p 15
Reference: [7] <author> I. Csiszar and P. Narayan. </author> <title> The capacity of the arbitrarily varying channel revisited: Positivity constraints. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 34 </volume> <pages> 181-193, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Common randomness available to transmitter and receiver also allows them to use random codes for data transmission, which can be far superior to deterministic codes in certain situations, e.g., with arbitrarily varying channels ([1], <ref> [7] </ref>). Finally, in the theory of communication complexity, it is known that common randomness can significantly reduce the amount of inter-processor communication required to perform certain computations in a distributed setting ([8], [11]).
Reference: [8] <author> L. Lovasz. </author> <title> Communication complexity: A survey. </title> <editor> In B.H. Korte et al., editors, </editor> <title> Paths, Flows and VLSI layout. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference: [9] <author> U.M. Maurer. </author> <title> Perfect cryptographic security from partially independent channels. </title> <booktitle> Proc. of the 23rd Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1991. </year>
Reference: [10] <author> U.M. Maurer. </author> <title> Secret key agreement by public discussion from common information. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 39(No. 3), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: In this context, the problem is one of secret sharing, i.e., generating common randomness at two terminals without giving information about it to an eavesdropper. This has recently been addressed by Maurer ([9], <ref> [10] </ref>), and Ahlswede and Csiszar [2]. In the "channel-type" model introduced in [2], the two terminals are connected by a discrete memoryless channel with one input and two outputs. One terminal governs the input, while the outputs are seen by the other terminal and the wiretapper, respectively.
Reference: [11] <author> A. Orlitsky and A. El Gamal. </author> <title> Communication complexity. </title> <editor> In Y. Abu-Mostafa, editor, </editor> <booktitle> Complexity in Information Theory. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Finally, in the theory of communication complexity, it is known that common randomness can significantly reduce the amount of inter-processor communication required to perform certain computations in a distributed setting ([8], <ref> [11] </ref>). For these and other reasons, Ahlswede and Csiszar [2] proposed a systematic study of the role of common randomness in information theory.
Reference: [12] <author> S. Venkatesan and V. Anantharam. </author> <title> The common randomness capacity of a pair of independent binary symmetric channels. </title> <type> Technical Report UCB/ERL M95/68, </type> <institution> Electronics Research Laboratory, Univ. of California, Berkeley, </institution> <month> August </month> <year> 1995. </year> <month> 21 </month>
Reference-contexts: (q) = (1 h (p)) + (1 h (q)) : It is somewhat surprising that it is possible to generate common randomness at a rate of 1 bit per step in all these cases. 8 In the binary symmetric case, a much simpler proof of Theorem 2.1 is given in <ref> [12] </ref>. This proof can easily be extended to the case where V and W are symmetric DMCs. 2) Suppose H W = 0, i.e., W is completely noiseless.
References-found: 12

