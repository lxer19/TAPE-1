URL: ftp://ftp.speech.sri.com/pub/people/francois/neuralcomp-1.ps.gz
Refering-URL: http://www.speech.sri.com/people/francois/publications.html
Root-URL: 
Title: Relating Real-Time Backpropagation and Backpropagation-Through-Time: An Application of Flow Graph  
Author: Interreciprocity. Fran~coise Beaufays and Eric A. Wan 
Date: 1994  
Note: Neural Computation,  
Abstract: We show that signal flow graph theory provides a simple way to relate two popular algorithms used for adapting dynamic neural networks, real-time backpropagation and backpropagation-through-time. Starting with the flow graph for real-time backpropagation, we use a simple transposition to produce a second graph. The new graph is shown to be interreciprocal with the original and to correspond to the backpropagation-through-time algorithm. Interreciprocity provides a theoretical argument to verify that both flow graphs implement the same overall weight update. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bordewijk, J. L. </author> <year> 1956. </year> <title> Inter-reciprocity applied to electrical networks. </title> <journal> Appl. Sci. Res. </journal> <volume> 6B, </volume> <pages> 1-74. </pages>
Reference-contexts: Definition 1 Two flow graphs, F and ~ F, are said to be the transpose of each other iff their transmittance matrices are transposed, i.e., ~ T j;k = T k;j 8 j; k: (18) Definition 2 <ref> (Bordewijk, 1956) </ref>: Two flow graphs, F and ~ F, are said to be interreciprocal iff X ( ~ Y k X k Y k ~ X k ) = 0: (19) We can now state the following theorem: Theorem 1 Transposed flow graphs are interreciprocal. 8 Proof: Let F be a
Reference: <author> Bryson, A. E.Jr. and Ho, Y. </author> <year> 1969. </year> <title> Applied Optimal Control, chapter 2. </title> <publisher> Blaisdell Publishing Co., </publisher> <address> New York. </address>
Reference: <author> Hertz, J. A., Krogh, A., and Palmer, R. G. </author> <year> 1991. </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading. </address>
Reference: <author> Kailath, T. </author> <year> 1980. </year> <title> Linear Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: This work was sponsored by EPRI under contract RP8010-13. 1 variety of engineering disciplines, such as the reciprocity of emitting and receiving antennas in electromagnetism (Ramo et al., 1984), the relationship between controller and observer canonical forms in control theory <ref> (Kailath, 1980) </ref>, and the duality between decimation in time and decimation in frequency formulations of the FFT algorithm in signal processing (Oppenheim and Schafer, 1989). The transposed flow graph is shown to correspond directly to the BPTT algorithm.
Reference: <author> Le Cun, Y. </author> <year> 1988. </year> <title> A Theoretical Framework for Back-Propagation. </title> <booktitle> Proceedings of the 1988 Connectionist Models Summer School. </booktitle> <editor> Editors: Touretzky, D., Hinton, G. and Sejnowski, T., </editor> <publisher> Morgan Kaufmann. </publisher> <address> San Mateo, CA. </address> <pages> 21-28. </pages>
Reference: <author> Ljung L. </author> <year> 1987. </year> <title> System Identification: theory for the user. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Pineda, F. J. </author> <year> 1987. </year> <title> Generalization of Back-Propagation to Recurrent Neural Networks. </title> <journal> IEEE Trans on neural networks, special issue on recurrent networks. </journal>
Reference: <author> Plumer, E. S. </author> <year> 1993. </year> <title> Time-Optimal Terminal Control Using Neural Networks. </title> <booktitle> Proceedings of the IEEE International Conference on Neural Networks. </booktitle> <address> San Francisco, CA. 1926-1931. </address> <note> 9 Ramo, </note> <author> S., Whinnery, J. R. and Van Duzer, T. </author> <year> 1984. </year> <title> Fields and waves in communication electronics. Second Edition. </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Rumelhart, D. E. and McClelland, J. L. </author> <year> 1986. </year> <title> Parallel Distributed Processing. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: output vector x (k) is a function of the recurrent and external inputs, and of the adaptive weights w of the network: x (k) = N (x (k 1); r (k 1); w): (1) r (k1) x (k) N The neural network N is most generally a feedforward multilayer architecture <ref> (Rumelhart et al., 1986) </ref>. If N has only a single layer of neurons, the structure of Figure 1 represents a completely recurrent network (Williams and Zipser, 1989; Pineda, 1987). Any connectionist architecture with feedback units can, in fact, be represented in this standard format (Piche, 1993).
Reference: <author> Tellegen, B. D. H. </author> <year> 1952. </year> <title> A general network theorem, with applications. </title> <journal> Philips Res. Rep. </journal> <volume> 7. </volume> <pages> 259-269. </pages>
Reference: <author> Werbos, P. </author> <year> 1990. </year> <title> Backpropagation Through Time: What It Does and How to Do It. </title> <booktitle> Proc. IEEE, special issue on neural networks 2. </booktitle> <pages> 1550-1560. </pages>
Reference: <author> White, S. A. </author> <year> 1975. </year> <title> An Adaptive Recursive Digital Filter. </title> <booktitle> Proc. 9th Asilomar Conf. Circuits Syst. Comput.. </booktitle> <pages> 21. </pages>
Reference-contexts: It is easy to verify that most scalar operations in calculus, such as the chain rule, also hold in the vectorial case. 2 The linear equivalent of the RTBP algorithm was first introduced in the context of Infinite Impulse Response (IIR) filter adaptation <ref> (White, 1975) </ref>. 3 1:0 fl rec (1) fl rec (k) fl rec (K) e T (1) e T (k) e T (K) J (2) J (k) J (k+1) J (K1) Equations 10 and 11 can be illustrated by a flow graph (see Figure 2).
Reference: <author> Williams, R. J. and Zipser, D. </author> <year> 1989. </year> <title> A Learning algorithm for continually running fully recurrent neural networks. </title> <booktitle> Neural Computation 1(2). </booktitle> <pages> 270-280. 10 </pages>
References-found: 13

