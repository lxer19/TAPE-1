URL: http://www.cs.ucsb.edu/conferences/java98/papers/titanium.ps
Refering-URL: http://www.cs.ucsb.edu/conferences/java98/program.html
Root-URL: http://www.cs.ucsb.edu
Title: Titanium: A High-Performance Java Dialect  
Author: Kathy Yelick, Luigi Semenzato, Geoff Pike, Carleton Miyamoto, Ben Liblit, Arvind Krishnamurthy, Paul Hilfinger, Susan Graham, David Gay, Phil Colella, and Alex Aiken 
Affiliation: Computer Science Division University of California at Berkeley and Lawrence Berkeley National Laboratory  
Abstract: Titanium is a language and system for high-performance parallel scientific computing. Titanium uses Java as its base, thereby leveraging the advantages of that language and allowing us to focus attention on parallel computing issues. The main additions to Java are immutable classes, multidimensional arrays, an explicitly parallel SPMD model of computation with a global address space, and zone-based memory management. We discuss these features and our design approach, and report progress on the development of Titanium, including our current driving application: a three-dimensional adaptive mesh refinement parallel Poisson solver.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aiken and D. Gay. </author> <title> Barrier Inference. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <pages> pages 342-354, </pages> <month> January </month> <year> 1998. </year>
Reference-contexts: If processes begin execution of e with identical values for each variable in V , and all processes terminate, then all processes execute the same sequence of global synchronization operations and end with identical values for each variable in V . More details are given by Aiken and Gay <ref> [1] </ref>. Local and global references. The storage associated with a Titanium process is called a region. Each object is contained within a single region. Local variables and objects created by new are contained in the region of the process that allocates them. <p> Points are tuples of integers and domains are sets of points. The following code fragment shows how a multi-dimensional array can be constructed. Point&lt;2&gt; l = <ref> [1, 1] </ref>; Point&lt;2&gt; u = [10, 20]; RectDomain&lt;2&gt; r = [l : u]; double [2d] A = new double [r]; The (two-dimensional) points l and u are declared and initialized.
Reference: [2] <author> A. Aiken and D. Gay. </author> <title> Memory Management with Explicit Regions. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1998. </year> <note> to appear. </note>
Reference-contexts: A preliminary study of this style of memory management on sequential C programs found that zones were faster than malloc/free and conservative garbage collection in most cases <ref> [2] </ref>. This study used a C compiler modified to perform reference counting on all pointers into zones.
Reference: [3] <author> W.W. Carlson and J.M. Draper. </author> <title> Distributed Data Access in AC. </title> <booktitle> In Proceedings of the 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOPP), </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The current plan is to translate Java-AD into standard Java + MPI calls, This approach prevents optimizations like those we are implementing in Titanium. Split-C. The parallel execution model and global address space support in Titanium are closely related to Split-C [5] and AC <ref> [3] </ref>. Titanium shares a common communication layer with Split-C on distributed memory machines, which we have extended as part of the Titanium project to run on shared memory machines.
Reference: [4] <author> Bryan Carpenter, Guansong Zhan, Geoffrey Fox, Yuhong Wen, and Xinying Li. HPJava: </author> <title> Data Parallel Extensions to Java. </title> <note> /http://www.npac.syr.edu/projects/pcrc/July97/doc.html, December 1997. </note>
Reference-contexts: Also, Titanium has two static domain types, general domain and rectangular domain. FIDIL has only a general domain type, thus making it harder to optimize code that uses the more common rectangular kind. Java-AD. The HPJava project includes Java-AD, an extension of Java for SPMD-style programming <ref> [4] </ref>. The main new feature of Java-AD is multidimensional distributed arrays, similar to HPF arrays. Java-AD also offers an interface to MPI for explicit communication. The current plan is to translate Java-AD into standard Java + MPI calls, This approach prevents optimizations like those we are implementing in Titanium. Split-C.
Reference: [5] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The current plan is to translate Java-AD into standard Java + MPI calls, This approach prevents optimizations like those we are implementing in Titanium. Split-C. The parallel execution model and global address space support in Titanium are closely related to Split-C <ref> [5] </ref> and AC [3]. Titanium shares a common communication layer with Split-C on distributed memory machines, which we have extended as part of the Titanium project to run on shared memory machines.
Reference: [6] <author> Paul N. Hilfinger and Phillip Colella. FIDIL: </author> <title> A Language for Scientific Programming. </title> <editor> In Robert Grossman, editor, </editor> <booktitle> Symbolic Computing: Applications to Scientific Computing, Frontiers in Applied Mathematics, chapter 5, </booktitle> <pages> pages 97-138. </pages> <publisher> SIAM, </publisher> <year> 1989. </year> <month> 12 </month>
Reference-contexts: Furthermore, with a global address space the compiler can optimize remote accesses with the same techniques used for the local memory hierarchy. FIDIL. The multidimensional array support in Titanium is strongly influenced by FIDIL maps and domains <ref> [6, 11] </ref>. Titanium, however, sacrifices expressiveness for performance. Titanium arrays may only be rectangular, where FIDIL maps have arbitrary shapes. Also, Titanium has two static domain types, general domain and rectangular domain.
Reference: [7] <author> A. Krishnamurthy and K. Yelick. </author> <title> Analyses and Optimizations for Shared Address Space Pro--grams. </title> <journal> Journal of Parallel and Distributed Computation, </journal> <year> 1996. </year>
Reference-contexts: Krishnamurthy and Yelick have shown that languages can provide the stronger model of sequential consistency through static analysis, even when the languages execute on hardware with weaker semantics <ref> [7] </ref>. We are exploring the use of this analysis, which requires good aliasing and synchronization information, in the context of Titanium, but our current consistency model does not rely on such analysis.
Reference: [8] <author> Leslie Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The most intuitive semantics of such shared accesses is sequential consistency, 5 which states that memory operations appear to take effect in some total order that is consistent with each processor's program order <ref> [8] </ref>. On machines with hundreds to thousands of cycles of memory latency for a local or remote memory access, sequential consistency is expensive, and most machine designers have opted for one of the weaker consistency models, such as processor consistency or release consistency.
Reference: [9] <author> N. K. Madsen. </author> <title> Divergence Preserving Discrete Surface Integral Methods for Maxwell's Curl Equations Using Non-Orthogonal Unstructured Grids. </title> <type> Technical Report 92.04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: AMR3D is the first large Titanium program, and it is interesting to note that the global synchronization analysis (Section 3.2) helped uncover a few bugs during its development. 6.2 EM3D EM3D is the computational kernel from an application that models the propagation of electro-magnetic waves through objects in three dimensions <ref> [9] </ref>. A preprocessing step casts the problem into a simple computation on an irregular bipartite graph containing nodes that represent electric and magnetic field values.
Reference: [10] <author> Steven L. Scott. </author> <title> Synchronization and Communication in the T3E Multiprocessor. </title> <booktitle> In Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1996. </year>
Reference-contexts: MPI, however, has a lower raw performance than a global address space. On a Cray T3E, MPI achieves a bandwidth of about 120 MB/sec, while a global address space achieves a bandwidth of about 330 MB/sec <ref> [10] </ref>. Furthermore, with a global address space the compiler can optimize remote accesses with the same techniques used for the local memory hierarchy. FIDIL. The multidimensional array support in Titanium is strongly influenced by FIDIL maps and domains [6, 11]. Titanium, however, sacrifices expressiveness for performance. <p> Points are tuples of integers and domains are sets of points. The following code fragment shows how a multi-dimensional array can be constructed. Point&lt;2&gt; l = [1, 1]; Point&lt;2&gt; u = <ref> [10, 20] </ref>; RectDomain&lt;2&gt; r = [l : u]; double [2d] A = new double [r]; The (two-dimensional) points l and u are declared and initialized. Then the rectangular domain r is initialized to be the set of all points in the rectangle with corners l and u.
Reference: [11] <author> Luigi Semenzato. </author> <title> A Virtual Machine for Partial Differential Equations. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: Furthermore, with a global address space the compiler can optimize remote accesses with the same techniques used for the local memory hierarchy. FIDIL. The multidimensional array support in Titanium is strongly influenced by FIDIL maps and domains <ref> [6, 11] </ref>. Titanium, however, sacrifices expressiveness for performance. Titanium arrays may only be rectangular, where FIDIL maps have arbitrary shapes. Also, Titanium has two static domain types, general domain and rectangular domain.
Reference: [12] <author> D. Stoutamire. </author> <title> Portable, Modular Expression of Locality. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1997. </year>
Reference-contexts: This study used a C compiler modified to perform reference counting on all pointers into zones. In a related study, Stoutamire obtained a 13% speedup in an AMR code by using zones to improve data locality <ref> [12] </ref>. 3.4 Arrays, points, and domains Titanium arrays, which are distinct from Java arrays, are multi-dimensional. They are constructed using domains, which specify their index sets, and are indexed by points, instead of explicit lists of integers as in Fortran.
Reference: [13] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <month> 13 </month>
Reference-contexts: The last column in the table below shows the percent increase in running time of the Titanium array version relative to the C or C++/Fortran version. 7.2 Parallel Performance Titanium runs on top of a standard Posix thread library on SMPs and on Active Message layer <ref> [13] </ref> on distributed memory multiprocessors and networks of workstations. Figure 4 shows the speedup of EM3D and AMR3D on an 8-way SUN Enterprise SMP for a fixed problem size. The performance shows that the overhead for our parallel runtime library are minimal.
References-found: 13

