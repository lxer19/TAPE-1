URL: http://www.cs.ucsd.edu/users/mihir/papers/inr.ps.gz
Refering-URL: http://www.cs.ucsd.edu/users/mihir/papers/complexity-papers.html
Root-URL: http://www.cs.ucsd.edu
Email: E-mail: mihir@cs.ucsd.edu  e-mail: madhu@watson.ibm.com.  
Address: San Diego, 9500 Gilman Drive, La Jolla, CA 92093.  P.O. Box 218, Yorktown Heights, NY 10598, USA.  
Affiliation: Department of Computer Science Engineering, Mail Code 0114, University of California at  Research Division, IBM T.J. Watson Research Center,  
Date: 1994.  15, 1994  
Note: Appears in Proceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM,  March  
Abstract: Improved Non-Approximability Results Abstract We indicate strong non-approximability factors for central problems: N 1=4 for Max Clique; N 1=10 for Chromatic Number; and 66=65 for Max 3SAT. Underlying the Max Clique result is a proof system in which the verifier examines only three "free bits" to attain an error of 1=2. Underlying the Chromatic Number result is a reduction from Max Clique which is more efficient than previous ones. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, O. Goldreich, J. H -astad and R. Peralta. </author> <title> Simple constructions of almost k-wise independent random variables. </title> <booktitle> Proceedings of the Thirty First Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: However they satisfy a weaker form of independence which will suffice. We first introduce this notion and show how the second moment analysis can still be applied here. Our notion seems to be slightly weaker than some weak forms of independence that have been used in the literature <ref> [21, 1] </ref> and incomparable to some of the others [18, 23]. The definition that follows is given only for the special case of boolean random variables, but is easily generalized. We let [N ] = f1; : : :; N g.
Reference: [2] <author> S. Arora, C. Lund, R. Motwani, M. Sudan and M. Szegedy. </author> <title> Proof verification and intractability of approximation problems. </title> <booktitle> Proceedings of the Thirty Third Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: The first twenty years following the NP-hardness discovery brought little understanding of why this is so. Today we know a lot more. The results of <ref> [12, 3, 2] </ref> indicated the existence of a constant ff &gt; 0 for which N ff factor approximations are unlikely to be achievable. Recent work [7, 13] has been able to show that ff 1=15. <p> See Section 4. 1.3 History and explanations Non-approximability results based on PCPs begin with [12]. They have since been proved under many different assumptions. These include: e P 6= N e P [12]; P 6= NP <ref> [3, 2] </ref>; NP 6= coRP [25, 7, 13]; and N e P 6= coR e P [7]. 2 Since our focus is on the factor g (N ) shown hard rather than on the assumption, we will talk of a problem being hard to approximate within a certain factor; it is <p> The connection described above led researchers to focus on reducing the (expected) number of bits queried in the PCP. Extending [4, 5, 12, 3] it was shown by <ref> [2] </ref> that N e P is in PCP av [ polylog (n); q ] for a value q which, although not specified in the paper, has been said by the authors to be around 10 4 . Thus they could show that approximating Max Clique within N 0:0001 is hard. <p> A value of fi = 1=121 was obtained in [7]. This was improved to fi = 1=71 in [13]. The first non-approximability result for Max 3SAT was that of <ref> [2] </ref>. Assuming P 6= NP they showed that there exists a constant fl &gt; 0 such that no polynomial time algorithm can approximate Max 3SAT within 1 + fl.
Reference: [3] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs: a new characterization of NP. </title> <booktitle> Proceedings of the Thirty Third Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: The first twenty years following the NP-hardness discovery brought little understanding of why this is so. Today we know a lot more. The results of <ref> [12, 3, 2] </ref> indicated the existence of a constant ff &gt; 0 for which N ff factor approximations are unlikely to be achievable. Recent work [7, 13] has been able to show that ff 1=15. <p> For more information see Section 2. The theorem that follows thus says, roughly, that free bits and randomness need to be expended at rates of 3 and polylog (n), respectively, for every 1=2 factor reduction in error. Theorem 1.4 N e P FPCP <ref> [ polylog (n); 3 ] </ref>. If the randomness is required to be logarithmic, we will use slightly more than one more free bit. <p> See Section 4. 1.3 History and explanations Non-approximability results based on PCPs begin with [12]. They have since been proved under many different assumptions. These include: e P 6= N e P [12]; P 6= NP <ref> [3, 2] </ref>; NP 6= coRP [25, 7, 13]; and N e P 6= coR e P [7]. 2 Since our focus is on the factor g (N ) shown hard rather than on the assumption, we will talk of a problem being hard to approximate within a certain factor; it is <p> The connection described above led researchers to focus on reducing the (expected) number of bits queried in the PCP. Extending <ref> [4, 5, 12, 3] </ref> it was shown by [2] that N e P is in PCP av [ polylog (n); q ] for a value q which, although not specified in the paper, has been said by the authors to be around 10 4 . <p> Theorem 3.2 is derived by applying what follows to this system. 3.2 Protocol We now extend the task of the provers P 0 ; : : :; P p1 so as to make the verifier's task easier, using the idea of recursive proof checking <ref> [3] </ref>. Notice that the circuit C may be assumed wlog to be an algebraic circuit over Z 2 .
Reference: [4] <author> L. Babai, L. Fortnow and C. Lund. </author> <title> Non-deterministic Exponential time has two-prover interactive protocols. </title> <booktitle> Proceedings of the Thirty First Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: The connection described above led researchers to focus on reducing the (expected) number of bits queried in the PCP. Extending <ref> [4, 5, 12, 3] </ref> it was shown by [2] that N e P is in PCP av [ polylog (n); q ] for a value q which, although not specified in the paper, has been said by the authors to be around 10 4 .
Reference: [5] <author> L. Babai, L. Fortnow, L. Levin, and M. Szegedy. </author> <title> Checking computations in poly-logarithmic time. </title> <booktitle> Proceedings of the Twenty Third Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1991. </year>
Reference-contexts: The connection described above led researchers to focus on reducing the (expected) number of bits queried in the PCP. Extending <ref> [4, 5, 12, 3] </ref> it was shown by [2] that N e P is in PCP av [ polylog (n); q ] for a value q which, although not specified in the paper, has been said by the authors to be around 10 4 .
Reference: [6] <author> D. Beaver and J. Feigenbaum. </author> <title> Hiding instances in multioracle queries. </title> <booktitle> Proceedings of the Seventh Annual Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science Vol. </booktitle> <volume> 415, </volume> <publisher> Springer Verlag, </publisher> <year> 1990. </year> <month> 15 </month>
Reference-contexts: We start with the proof of Lemma 3.6. Proof of Lemma 3.6: Suppose g fl (ff) 6= 0. Let I [y] be the event that g (y + ff) g (y) = 0. Our starting point is the self-corrector of <ref> [6, 10] </ref> which shows that in this case the probability of I [y] when y is chosen uniformly at random from Z l 2 2 is at most 0:2.
Reference: [7] <author> M. Bellare, S. Goldwasser, C. Lund and A. Russell. </author> <title> Efficient probabilistically check-able proofs and applications to approximation. </title> <booktitle> Proceedings of the Twenty Fifth Annual Symposium on the Theory of Computing, ACM, 1993. See also Errata sheet in Proceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: Today we know a lot more. The results of [12, 3, 2] indicated the existence of a constant ff &gt; 0 for which N ff factor approximations are unlikely to be achievable. Recent work <ref> [7, 13] </ref> has been able to show that ff 1=15. One of the most basic goals in computational complexity theory is to find the exact complexity of problems. <p> We know that 1=15 ff true 1 o (1). But where in this range does ff true lie? Our approach has been to try to improve the lower bound. That is, continuing the work of <ref> [7, 13] </ref>, we wanted to increase the value of the constant ff &gt; 0 for which a factor N ff non-approximability result could be shown. We were faced with the difficulty that probabilistic proof checking techniques seemed to have been pushed to their limits. <p> See Section 4. 1.3 History and explanations Non-approximability results based on PCPs begin with [12]. They have since been proved under many different assumptions. These include: e P 6= N e P [12]; P 6= NP [3, 2]; NP 6= coRP <ref> [25, 7, 13] </ref>; and N e P 6= coR e P [7]. 2 Since our focus is on the factor g (N ) shown hard rather than on the assumption, we will talk of a problem being hard to approximate within a certain factor; it is understood that we mean that <p> They have since been proved under many different assumptions. These include: e P 6= N e P [12]; P 6= NP [3, 2]; NP 6= coRP [25, 7, 13]; and N e P 6= coR e P <ref> [7] </ref>. 2 Since our focus is on the factor g (N ) shown hard rather than on the assumption, we will talk of a problem being hard to approximate within a certain factor; it is understood that we mean that no polynomial time approximation algorithm achieving this factor exists under one <p> Until the work of [13], it has been expressed in terms of query bits, and we'll begin by discussing this part of the literature. Our discussion is in terms of PCP av [ r; q ], the average case version of PCP defined by <ref> [7] </ref> to equal the class of languages which can be recognized with error 1=2 by a PCP which uses r = r (n) random bits and makes a number of queries whose expectation is a constant strictly less than q. <p> Thus they could show that approximating Max Clique within N 0:0001 is hard. The problem of reducing q was first tackled in earnest in <ref> [7] </ref>, and they showed that N e P is in PCP av [ polylog (n); 24 ]. It followed that approximating Max Clique within N 1=25 is hard, a pretty substantial improvement. Amongst their technical contributions are a simplified framework for proof checking and the idea of reusing query bits. <p> They then observed that of the 24 query bits used in the proof system of <ref> [7] </ref> only 14 were free; that is, N e P is in FPCP av [ polylog (n); 14 ]. <p> The first non-approximability result for Chromatic Number was that of Lund and Yannakakis [20]. They showed that there is a constant fi &gt; 0 such that approximating the chromatic number of a graph within N fi is hard. A value of fi = 1=121 was obtained in <ref> [7] </ref>. This was improved to fi = 1=71 in [13]. The first non-approximability result for Max 3SAT was that of [2]. Assuming P 6= NP they showed that there exists a constant fl &gt; 0 such that no polynomial time algorithm can approximate Max 3SAT within 1 + fl. <p> The first non-approximability result for Max 3SAT was that of [2]. Assuming P 6= NP they showed that there exists a constant fl &gt; 0 such that no polynomial time algorithm can approximate Max 3SAT within 1 + fl. Next it was shown by <ref> [7] </ref> that assuming e P 6= N e P, no polynomial time algorithm can approximate Max 3SAT within 1 + 1=93. <p> Section 3). We also perform various optimizations following in the framework of <ref> [7] </ref>. In the latter category, we "weight" differently the basic tests; we modify their "placements;" we factor into the analysis a better analysis of the linearity tests, due to [10] which, although known before [7], seems to have been forgotten by them; we even reduce |to 13, from the 15 in <p> Section 3). We also perform various optimizations following in the framework of <ref> [7] </ref>. In the latter category, we "weight" differently the basic tests; we modify their "placements;" we factor into the analysis a better analysis of the linearity tests, due to [10] which, although known before [7], seems to have been forgotten by them; we even reduce |to 13, from the 15 in [7]| the number of 3SAT clauses needed to write the quadratic test. A very skimpy description follows. <p> In the latter category, we "weight" differently the basic tests; we modify their "placements;" we factor into the analysis a better analysis of the linearity tests, due to [10] which, although known before <ref> [7] </ref>, seems to have been forgotten by them; we even reduce |to 13, from the 15 in [7]| the number of 3SAT clauses needed to write the quadratic test. A very skimpy description follows. We first use the [19, 14] proof system to get a canonical two-prover proof system as discussed in Section 3.1.
Reference: [8] <author> M. Ben-Or, S. Goldwasser, J. Kilian and A. Wigderson. </author> <title> Multi-Prover interactive proofs: How to remove intractability assumptions. </title> <booktitle> Proceedings of the Twentieth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1988. </year>
Reference-contexts: We'll also discuss verifiers who talk to a collection of p = p (n) provers rather than to a proof string <ref> [8] </ref>. Definitions for such things are standard. If is a probability space then ! R denotes the operation of selecting an element at random according to , and Pr ! [] is the corresponding probability.
Reference: [9] <author> P. Berman and G. Schnitger. </author> <title> On the complexity of approximating the independent set problem. </title> <booktitle> Information and Computation 96, </booktitle> <month> 77-94 </month> <year> (1992). </year>
Reference-contexts: This is a slightly tighter version of the original connection of [12] which can be viewed as derived in two alternative ways: either apply the randomized graph products of <ref> [9] </ref> to the construction of [12], or apply [25]. The connection described above led researchers to focus on reducing the (expected) number of bits queried in the PCP.
Reference: [10] <author> M. Blum, M. Luby and R. Rubinfeld. </author> <title> Self-testing/correcting with applications to numerical problems. </title> <booktitle> Proceedings of the Twenty Second Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1990. </year>
Reference-contexts: We start with the proof of Lemma 3.6. Proof of Lemma 3.6: Suppose g fl (ff) 6= 0. Let I [y] be the event that g (y + ff) g (y) = 0. Our starting point is the self-corrector of <ref> [6, 10] </ref> which shows that in this case the probability of I [y] when y is chosen uniformly at random from Z l 2 2 is at most 0:2. <p> For x; x 0 2 Z t 2 , let I [x; x 0 ] be the event that h (x) + h (x 0 ) = h (x + x 0 ). The linearity test analysis of <ref> [10] </ref> implies that if x and x 0 are drawn randomly and independently from Z t 2 then I [x; x 0 ] occurs with probability at most 7=9. <p> Section 3). We also perform various optimizations following in the framework of [7]. In the latter category, we "weight" differently the basic tests; we modify their "placements;" we factor into the analysis a better analysis of the linearity tests, due to <ref> [10] </ref> which, although known before [7], seems to have been forgotten by them; we even reduce |to 13, from the 15 in [7]| the number of 3SAT clauses needed to write the quadratic test. A very skimpy description follows.
Reference: [11] <author> R. Boppana and M. Hald orsson. </author> <title> Approximating maximum independent sets by excluding subgraphs. </title> <journal> BIT, </journal> <volume> Vol. 32, No. 2, </volume> <year> 1992. </year>
Reference-contexts: Unfortunately it is NP-hard [16], and attention since this discovery has thus focused on approximation algorithms. Yet the best known ones can approximate the max clique size of an N node graph only to within a factor of N 1o (1) <ref> [11] </ref>, scarcely better than the trivial factor of N . The first twenty years following the NP-hardness discovery brought little understanding of why this is so. Today we know a lot more.
Reference: [12] <author> U. Feige, S. Goldwasser, L. Lov asz, S. Safra, and M. Szegedy. </author> <title> Approximating clique is almost NP-complete. </title> <booktitle> Proceedings of the Thirty Second Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: The first twenty years following the NP-hardness discovery brought little understanding of why this is so. Today we know a lot more. The results of <ref> [12, 3, 2] </ref> indicated the existence of a constant ff &gt; 0 for which N ff factor approximations are unlikely to be achievable. Recent work [7, 13] has been able to show that ff 1=15. <p> We feel that proof systems of a different nature are required to do better. Our chromatic number reduction Following [20], non-approximability results for Chromatic Number have been obtained by "reduction" from the Max Clique problem on the specific set of graphs constructed by the Max Clique reduction of <ref> [12] </ref>. For the purpose of discussing our contribution the problem can be abstracted as follows. <p> Theorem 1.2 follows. Our reduction is an extension of the one of [17], staying within the same framework but using linear algebra and coding theory techniques to implement "shifts" in a different way. See Section 4. 1.3 History and explanations Non-approximability results based on PCPs begin with <ref> [12] </ref>. They have since been proved under many different assumptions. These include: e P 6= N e P [12]; P 6= NP [3, 2]; NP 6= coRP [25, 7, 13]; and N e P 6= coR e P [7]. 2 Since our focus is on the factor g (N ) shown <p> See Section 4. 1.3 History and explanations Non-approximability results based on PCPs begin with <ref> [12] </ref>. They have since been proved under many different assumptions. These include: e P 6= N e P [12]; P 6= NP [3, 2]; NP 6= coRP [25, 7, 13]; and N e P 6= coR e P [7]. 2 Since our focus is on the factor g (N ) shown hard rather than on the assumption, we will talk of a problem being hard to approximate within a <p> The basic connection of PCPs to Max Clique is due to <ref> [12] </ref>. Until the work of [13], it has been expressed in terms of query bits, and we'll begin by discussing this part of the literature. <p> The connection, which we state without proof, is that if N e P is in PCP av [ polylog (n); q ] then approximating Max Clique within N ff is hard for ff = 1=(1 + q). This is a slightly tighter version of the original connection of <ref> [12] </ref> which can be viewed as derived in two alternative ways: either apply the randomized graph products of [9] to the construction of [12], or apply [25]. The connection described above led researchers to focus on reducing the (expected) number of bits queried in the PCP. <p> This is a slightly tighter version of the original connection of <ref> [12] </ref> which can be viewed as derived in two alternative ways: either apply the randomized graph products of [9] to the construction of [12], or apply [25]. The connection described above led researchers to focus on reducing the (expected) number of bits queried in the PCP. <p> The connection described above led researchers to focus on reducing the (expected) number of bits queried in the PCP. Extending <ref> [4, 5, 12, 3] </ref> it was shown by [2] that N e P is in PCP av [ polylog (n); q ] for a value q which, although not specified in the paper, has been said by the authors to be around 10 4 . <p> We continue to consider the 3 The observation of [13], and the reason it is the free bits rather than the query bits that are relevant, can be explained another way to a reader familiar with the construction of <ref> [12] </ref>. Recall that each node of the graph built by the latter encodes a computation of the verifier, and only nodes corresponding to accepting computations need be in the graph.
Reference: [13] <author> U. Feige and J. Kilian. </author> <title> Two prover protocols Low error at affordable rates. </title> <booktitle> Proceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: Today we know a lot more. The results of [12, 3, 2] indicated the existence of a constant ff &gt; 0 for which N ff factor approximations are unlikely to be achievable. Recent work <ref> [7, 13] </ref> has been able to show that ff 1=15. One of the most basic goals in computational complexity theory is to find the exact complexity of problems. <p> We know that 1=15 ff true 1 o (1). But where in this range does ff true lie? Our approach has been to try to improve the lower bound. That is, continuing the work of <ref> [7, 13] </ref>, we wanted to increase the value of the constant ff &gt; 0 for which a factor N ff non-approximability result could be shown. We were faced with the difficulty that probabilistic proof checking techniques seemed to have been pushed to their limits. <p> How could one go further? As we will see, the key was the build proof systems which are efficient not under "standard" proof checking measures, but under a new one suggested by <ref> [13] </ref>. Our proof systems have enabled us to obtain results strong enough to be surprising. For the first time, we feel that the gap between upper and lower bounds may be bridged. <p> later by [17] is slightly less efficient, achieving a = 6 and b = 5.) Applying this and Theorem 1.4 we can conclude that approximating Chromatic Number within N 1 16 o (1) is hard, which is already better than the best previous hardness factor of N 1=71 due to <ref> [13] </ref>. However, we have the following improvement in the reduction. Theorem 1.5 There is a (a; b)-chromatic number reduction achieving a = 1 and b = 3. Theorem 1.2 follows. <p> See Section 4. 1.3 History and explanations Non-approximability results based on PCPs begin with [12]. They have since been proved under many different assumptions. These include: e P 6= N e P [12]; P 6= NP [3, 2]; NP 6= coRP <ref> [25, 7, 13] </ref>; and N e P 6= coR e P [7]. 2 Since our focus is on the factor g (N ) shown hard rather than on the assumption, we will talk of a problem being hard to approximate within a certain factor; it is understood that we mean that <p> The basic connection of PCPs to Max Clique is due to [12]. Until the work of <ref> [13] </ref>, it has been expressed in terms of query bits, and we'll begin by discussing this part of the literature. <p> N e P 6= ZP e P) is equivalent to NP 6= coRP (resp. N e P 6= coR e P). To reduce below 24 the number of queried bits in a PCP seemed hard. The door to better results was opened by Feige and Kilian <ref> [13] </ref>. <p> It followed that approximating Max Clique within N 1=15 is hard. 3 It was indicated in <ref> [13] </ref> that better results could probably be obtained by optimizing proof checking systems with the new parameter in mind. Our results show that they were right. The first non-approximability result for Chromatic Number was that of Lund and Yannakakis [20]. <p> They showed that there is a constant fi &gt; 0 such that approximating the chromatic number of a graph within N fi is hard. A value of fi = 1=121 was obtained in [7]. This was improved to fi = 1=71 in <ref> [13] </ref>. The first non-approximability result for Max 3SAT was that of [2]. Assuming P 6= NP they showed that there exists a constant fl &gt; 0 such that no polynomial time algorithm can approximate Max 3SAT within 1 + fl. <p> Next it was shown by [7] that assuming e P 6= N e P, no polynomial time algorithm can approximate Max 3SAT within 1 + 1=93. The assumption was improved to P 6= NP in <ref> [13] </ref>. 2 Definitions A PCP is defined by a verifier V who has access to the input x of length n, a random string R of length r = r (n), and a proof string which has a length l = l (n) assumed wlog to be a power of 2. <p> Traditionally the important efficiency measures have been the number of queries q and the amount of randomness r. Instead of q, we focus on the number of free bits f . We continue to consider the 3 The observation of <ref> [13] </ref>, and the reason it is the free bits rather than the query bits that are relevant, can be explained another way to a reader familiar with the construction of [12]. <p> Applying the transformation that follows to this system will yield Theorem 3.1. In the proof system of <ref> [13] </ref> the answers of the second prover are not determined by those of the first. However, we observe that any p-prover proof system can be converted into a p + 1 prover canonical proof system. Applying this to the system of [13] we can conclude that there is a polynomial () <p> In the proof system of <ref> [13] </ref> the answers of the second prover are not determined by those of the first. However, we observe that any p-prover proof system can be converted into a p + 1 prover canonical proof system. Applying this to the system of [13] we can conclude that there is a polynomial () such that for any constant * &gt; 0 there is a canonical proof system with p = 3, r 1 (n) = (1=*) log n, 8 error *, and l = O (r 1 (n)). <p> Underlying our result is again a new measure of proof checking efficiency |different, however, from the free bit measure used above| but we don't have space to discuss it. Acknowledgments We thank Uri Feige and Joe Kilian for sending us a preliminary version of their work <ref> [13] </ref>; Oded Goldreich for pointing out the non-approximability factors under the P 6= NP assumption that were mentioned in Section 1.1; Marcos Kiwi for comments on an early draft; Don Coppersmith for helpful discussions. Work done while the first author was at the IBM T.J. Watson Research Center, New York.
Reference: [14] <author> U. Feige and L. Lov asz. </author> <title> Two-prover one round proof systems: Their power and their problems. </title> <booktitle> Proceedings of the Twenty Fourth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1992. </year>
Reference-contexts: They then observed that of the 24 query bits used in the proof system of [7] only 14 were free; that is, N e P is in FPCP av <ref> [ polylog (n); 14 ] </ref>. It followed that approximating Max Clique within N 1=15 is hard. 3 It was indicated in [13] that better results could probably be obtained by optimizing proof checking systems with the new parameter in mind. Our results show that they were right. <p> Our transformation applies to any canonical proof system. The above theorems are obtained by plugging in specific canonical proof systems as we now describe. The only property missing to make the proof system of <ref> [19, 14] </ref> canonical is that the second prover's answers are not expressible as a projection of those of the first. This is simple to fix. <p> A very skimpy description follows. We first use the <ref> [19, 14] </ref> proof system to get a canonical two-prover proof system as discussed in Section 3.1.
Reference: [15] <author> N. Kahale. </author> <title> On the second eigenvalue and linear expansion of regular graphs. </title> <booktitle> Proceedings of the Thirty Third Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: For example, for any constant ffi &gt; 0, assuming NP 6= coRP we can show that N 1 5 ffi factor approximation is impossible; and assuming just P 6= NP we can use the constructions of <ref> [15] </ref> to show that N 1 6 ffi factor approximation is impossible. The technical underpinning of the above result is a new proof system which will be discussed in Section 1.2. We turn now to the chromatic number problem. Denote by (H) the chromatic number of a graph H.
Reference: [16] <author> R. Karp. </author> <title> Reducibility among combinatorial problems. Complexity of Computer Computations, </title> <editor> Miller and Thatcher (eds.), </editor> <publisher> Plenum Press, </publisher> <address> New York (1972). </address>
Reference-contexts: 1 Introduction Max Clique is amongst the most important combinatorial optimization problems. Unfortunately it is NP-hard <ref> [16] </ref>, and attention since this discovery has thus focused on approximation algorithms. Yet the best known ones can approximate the max clique size of an N node graph only to within a factor of N 1o (1) [11], scarcely better than the trivial factor of N .
Reference: [17] <author> S. Khanna, N. Linial and S. Safra. </author> <title> On the hardness of approximating the chromatic number. </title> <booktitle> Proceedings of the Second Israel Symposium on Theory and Computing Systems, </booktitle> <year> 1993. </year>
Reference-contexts: Thus the problem is to design (a; b)-chromatic number reductions with a; b as small as possible. The reduction of [20] achieves a = 1 and b = 5. (A simple reduction supplied later by <ref> [17] </ref> is slightly less efficient, achieving a = 6 and b = 5.) Applying this and Theorem 1.4 we can conclude that approximating Chromatic Number within N 1 16 o (1) is hard, which is already better than the best previous hardness factor of N 1=71 due to [13]. <p> However, we have the following improvement in the reduction. Theorem 1.5 There is a (a; b)-chromatic number reduction achieving a = 1 and b = 3. Theorem 1.2 follows. Our reduction is an extension of the one of <ref> [17] </ref>, staying within the same framework but using linear algebra and coding theory techniques to implement "shifts" in a different way. See Section 4. 1.3 History and explanations Non-approximability results based on PCPs begin with [12]. They have since been proved under many different assumptions. <p> Thus the lemma is true for c 1 = 729=2. 4 Chromatic Number We present the proof of Theorem 1.5. It will be helpful, although not necessary, if the reader is familiar with the construction and proof of <ref> [17] </ref> which we extend. Let Q = 2 q . Let F = GF (2 q ) be the finite field of size Q = 2 q . Let V = F 3 and regard it as a 3 dimensional vector space over F . <p> Let !(H fl ) denote the size of a minimum clique cover of H fl . Given Lemmas 4.2 and 4.3, an argument analogous to that in <ref> [17] </ref> implies that (1) !(G) = R implies !(H fl ) = jV j = Q 3 (2) !(G) R=g implies !(H fl ) jV jg = Q 3 g. Let H be the complement of H fl . It is well known that (H) = !(H fl ).
Reference: [18] <author> D. Koller and N. Megiddo. </author> <title> Constructing small sample spaces satisfying given constraints. </title> <booktitle> Proceedings of the Twenty Fifth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1993. </year>
Reference-contexts: We first introduce this notion and show how the second moment analysis can still be applied here. Our notion seems to be slightly weaker than some weak forms of independence that have been used in the literature [21, 1] and incomparable to some of the others <ref> [18, 23] </ref>. The definition that follows is given only for the special case of boolean random variables, but is easily generalized. We let [N ] = f1; : : :; N g.
Reference: [19] <author> D. Lapidot and A. Shamir. </author> <title> Fully Parallelized Multi-prover protocols for NEXP-time. </title> <booktitle> Proceedings of the Thirty Second Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: Our transformation applies to any canonical proof system. The above theorems are obtained by plugging in specific canonical proof systems as we now describe. The only property missing to make the proof system of <ref> [19, 14] </ref> canonical is that the second prover's answers are not expressible as a projection of those of the first. This is simple to fix. <p> A very skimpy description follows. We first use the <ref> [19, 14] </ref> proof system to get a canonical two-prover proof system as discussed in Section 3.1.
Reference: [20] <author> C. Lund and M. Yannakakis. </author> <title> On the hardness of approximating minimization problems. </title> <booktitle> Proceedings of the Twenty Fifth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1993. </year>
Reference-contexts: Thus we get Theorem 1.1. We can show that that many aspects of our constructions and analysis are tight. We feel that proof systems of a different nature are required to do better. Our chromatic number reduction Following <ref> [20] </ref>, non-approximability results for Chromatic Number have been obtained by "reduction" from the Max Clique problem on the specific set of graphs constructed by the Max Clique reduction of [12]. For the purpose of discussing our contribution the problem can be abstracted as follows. <p> Then there is no polynomial time algorithm to approximate the Chromatic Number within N fio (1) for fi = 1=(a + bf ). Thus the problem is to design (a; b)-chromatic number reductions with a; b as small as possible. The reduction of <ref> [20] </ref> achieves a = 1 and b = 5. (A simple reduction supplied later by [17] is slightly less efficient, achieving a = 6 and b = 5.) Applying this and Theorem 1.4 we can conclude that approximating Chromatic Number within N 1 16 o (1) is hard, which is already <p> Our results show that they were right. The first non-approximability result for Chromatic Number was that of Lund and Yannakakis <ref> [20] </ref>. They showed that there is a constant fi &gt; 0 such that approximating the chromatic number of a graph within N fi is hard. A value of fi = 1=121 was obtained in [7]. This was improved to fi = 1=71 in [13].
Reference: [21] <author> J. Naor and M. Naor. </author> <title> Small bias probability spaces: efficient constructions and applica-tions. </title> <booktitle> Proceedings of the Twenty Second Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1990. </year>
Reference-contexts: However they satisfy a weaker form of independence which will suffice. We first introduce this notion and show how the second moment analysis can still be applied here. Our notion seems to be slightly weaker than some weak forms of independence that have been used in the literature <ref> [21, 1] </ref> and incomparable to some of the others [18, 23]. The definition that follows is given only for the special case of boolean random variables, but is easily generalized. We let [N ] = f1; : : :; N g.
Reference: [22] <author> C. Papadimitriou and M. Yannakakis. </author> <title> Optimization, approximation, and complexity classes. </title> <booktitle> Proceedings of the Twentieth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1988. </year>
Reference-contexts: Max 3SAT is the problem of determining the maximum number of simultaneously satisfiable clauses in a 3cnf-formula. It is a canonical Max SNP complete problem <ref> [22] </ref>. An algorithm approximates it within 1 + fl if it produces a value which is at least 1=(1 + fl) times optimal, and at most optimal. An algorithm due to [24] achieves fl = 1=3. Theorem 1.3 Suppose N e P 6= e P.
Reference: [23] <author> L. Schulman. </author> <title> Sample spaces uniform on neighborhoods. </title> <booktitle> Proceedings of the Twenty Fourth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1992. </year>
Reference-contexts: We first introduce this notion and show how the second moment analysis can still be applied here. Our notion seems to be slightly weaker than some weak forms of independence that have been used in the literature [21, 1] and incomparable to some of the others <ref> [18, 23] </ref>. The definition that follows is given only for the special case of boolean random variables, but is easily generalized. We let [N ] = f1; : : :; N g.
Reference: [24] <author> M. Yannakakis. </author> <title> On the approximation of maximum satisfiability. This SODA not yet defined! </title> . 
Reference-contexts: It is a canonical Max SNP complete problem [22]. An algorithm approximates it within 1 + fl if it produces a value which is at least 1=(1 + fl) times optimal, and at most optimal. An algorithm due to <ref> [24] </ref> achieves fl = 1=3. Theorem 1.3 Suppose N e P 6= e P. Then there is no polynomial time algorithm to approximate Max 3SAT within 1 + 1=65. The factor decreases to 1 + 1=73 if the assumption is just P 6= NP. <p> Thus they could show that approximating Max Clique within N 0:0001 is hard. The problem of reducing q was first tackled in earnest in [7], and they showed that N e P is in PCP av <ref> [ polylog (n); 24 ] </ref>. It followed that approximating Max Clique within N 1=25 is hard, a pretty substantial improvement. Amongst their technical contributions are a simplified framework for proof checking and the idea of reusing query bits. We use both. 2 Note NP 6= coRP (resp.
Reference: [25] <author> D. Zuckerman. </author> <title> NP-Complete Problems have a version that is hard to Approximate. </title> <booktitle> Proceedings of the Eighth Annual Conference on Structure in Complexity Theory, IEEE, </booktitle> <year> 1993. </year> <month> 17 </month>
Reference-contexts: See Section 4. 1.3 History and explanations Non-approximability results based on PCPs begin with [12]. They have since been proved under many different assumptions. These include: e P 6= N e P [12]; P 6= NP [3, 2]; NP 6= coRP <ref> [25, 7, 13] </ref>; and N e P 6= coR e P [7]. 2 Since our focus is on the factor g (N ) shown hard rather than on the assumption, we will talk of a problem being hard to approximate within a certain factor; it is understood that we mean that <p> This is a slightly tighter version of the original connection of [12] which can be viewed as derived in two alternative ways: either apply the randomized graph products of [9] to the construction of [12], or apply <ref> [25] </ref>. The connection described above led researchers to focus on reducing the (expected) number of bits queried in the PCP.
References-found: 25

