URL: ftp://ftp.cse.unsw.edu.au/pub/users/andrewt/publications/1996/127.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/publications/1996/SCSE_publications.html
Root-URL: 
Email: timm@cse.unsw.edu.au;  
Title: Assessing Responses to Situated Cognition  
Author: Tim Menzies 
Date: September 17, 1996  
Web: http://www.cse.unsw.edu.au/~timm  
Address: Sydney, Australia, 2052  
Affiliation: Dept. of Artifical Intelligence, School of Computer Science and Engineering, The University of New South Wales,  
Abstract: Situated cognition (SC) claims that knowledge is mostly context-dependent and that symbolic descriptions elicited prior to direct experience are less important than functional units developed via direct experience with the current problem. If this were true, then we would need to modify the knowledge modeling approaches of KA which assume that re-using old symbolic descriptions are a productivity tool for new applications. There are numerous tools which, if added to conventional knowledge modeling, could be said to handle SC (e.g. machine learning, abduction, verification & validation tools, repertory grids, certain frameworks for decision support systems, expert critiquing systems, and ripple-down-rules). However, we require an experiment to assess the effectiveness of these tools as a response to SC. 
Abstract-found: 1
Intro-found: 1
Reference: [Ackoff, 1967] <author> Ackoff, R. </author> <year> (1967). </year> <booktitle> Management Misinformation Systems. Management Science, </booktitle> <pages> pages 319-331. </pages>
Reference-contexts: DSS theory believes that management decision making is not inhibited by a lack of information. Rather, it is confused by an excess of irrelevant information <ref> [Ackoff, 1967] </ref>. Modern decision-support systems (DSS) aim to filter useless information to deliver relevant information (a subset of all information) to the manager. Simon originally characterised decision making as a three stage process: intelligence (scanning environment), design (develop alternative courses), and choice (selection of alternative) [Simon, 1960].
Reference: [Agnew et al., 1993] <author> Agnew, N., Ford, K., & Hayes, P. </author> <year> (1993). </year> <title> Expertise in Context: Personally Constructed, </title> <journal> Socially elected, and Reality-Relevant? International Journal of Expert Systems, </journal> <volume> 7. </volume>
Reference-contexts: Note that the chosen premises may radically influence the conclusions reached. Agnew, Ford & Hayes offer their summary of contemporary thinking in the history, philosophy and sociology of science as: Expert-knowledge is comprised of context-dependent, personally constructed, highly functional but fallible abstractions <ref> [Agnew et al., 1993] </ref>. Easterbrook [Easterbrook, 1991] argues that it is undesirable to demand that knowledge bases are con sistent. This insistence that expertise must be consistent and rational imposes restrictions of the knowledge acquired.
Reference: [Agre, 1990] <author> Agre, P. </author> <year> (1990). </year> <note> Book Review: </note> <author> Lcy A. Scuhman, </author> <title> Plans and Situated Actions: The Problems of Human-Machine Communication. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 369-384. </pages>
Reference-contexts: Such an explicit expression of current beliefs may prompt further investigation and model revision; i.e. writing down models of "truths" can cause "truth" to change. Decision support tools are discussed later (x4.6). Suchman <ref> [Suchman, 1987, Suchman, 1993, Agre, 1990] </ref> argues that real-world planning systems have to model their environment as well as their own goals. <p> work together to maintain a shared understanding of what is going on between the tow of them and the copier: : : Far from executing a fully operational plan for effecting a fixed goal, the photocopier users continually reinterpreted their situation and based their various actions on their evolving interpretations <ref> [Agre, 1990] </ref>. If weak SC was false, then we should see that using knowledge does not change that knowledge; i.e. knowledge maintenance for static domains should terminate when it arrives at "truth". Compton [Compton et al., 1989] reports studies that documented the changes made to models of biochemistry diagnosis systems.
Reference: [Agre, 1993] <author> Agre, P. </author> <year> (1993). </year> <title> The Symbolic WorldView: Reply to Vera and Simon. </title> <journal> Cognitive Science, </journal> <volume> 17 </volume> <pages> 61-69. </pages>
Reference-contexts: They describe as "preposterous" [Vera & Simon, 1993a, p95] a claim by Agre that "nobody has described a system capable of intelligent action at all- and that nothing of the sort is going to happen soon" <ref> [Agre, 1993, p69] </ref>. We suspect that they would also object to McDermott's lament about "skimpy results so far" (x3.3). Vera & Simon argue that the physical symbol system hypothesis (PSSH) [Newell & Simon, 1972] has been a fruitful paradigm which can reproduce many known behaviours of experts.
Reference: [Bachant & McDermott, 1984] <author> Bachant, J. & McDermott, J. </author> <year> (1984). </year> <title> R1 Revisited: Four Years in the Trenches. </title> <journal> AI Magazine, </journal> <pages> pages 21-32. </pages>
Reference-contexts: Further, if the situation is as bad as suggested above, then how is it that we have so many seemingly successful expert systems (e.g. MYCIN [Yu et al., 1979], CASNET [Weiss et al., 1978], PROSPECTOR [Campbell et al., 1982, Duda et al., 1985], XCON <ref> [Bachant & McDermott, 1984] </ref>, VT [Marcus et al., 1987], PIGE [Menzies et al., 1992])? This kind of argument is the basis of Vera & Simon's criticisms of SC [Vera & Simon, 1993b, Vera & Simon, 1993a, Vera & Simon, 1993c].
Reference: [Benjamins, 1995] <author> Benjamins, V. </author> <year> (1995). </year> <title> Problem-Solving Methods for Diagnosis and their Role in Knowledge Acquisition. </title> <journal> International Journal of Expert Systems: Research & Applications, </journal> <volume> 8(2) </volume> <pages> 93-120. </pages>
Reference-contexts: Note that the KADS diagnosis model continues to change at a rapid rate (e.g. <ref> [Benjamins, 1995] </ref>). Knowledge developed in one context may not be usefully reusable in another. Corbridge et. al. report a study in which subjects had to extract knowledge from an expert dialogue using a variety of abstract pattern tools [Corbridge et al., 1995].
Reference: [Birnbaum, 1991] <author> Birnbaum, L. </author> <year> (1991). </year> <title> Rigor Mortis: A Response to Nilsson's 'Logic and Artificial Intelligence'. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 57-77. </pages>
Reference-contexts: Searle takes a similar stand, claiming that the only device that can replicate human intelligence is another human [Searle, 1980, Searle, 1982, Searle, 1995] since only humans can share the same context. Birnbaum stresses "the role of a concrete case in reasoning" <ref> [Birnbaum, 1991, p58] </ref> and how logical AI cannot correctly handle such specifics, particularly when we have a specific conflicting belief. Relativity, Heisenburg's uncertainty principle, the indeterminacy of quantum mechanics and Godel's theorem demonstrate hard limits to the complete expression of truth. <p> However, this endorsement does not necessarily imply an endorsement of strong SC. Like Vera & Simon in x4.1.3, this paper argues that symbolic systems are still a useful paradigm. It is not necessarily true what (e.g.) Birnbaum <ref> [Birnbaum, 1991] </ref> and McDermott [McDermott, 1987] argue; i.e. that the obvious alternative to logical AI is some type of procedural/functional semantics (i.e. strong SC).
Reference: [Boose et al., 1992] <author> Boose, J., Bradshaw, J., Koszareck, J., & Shema, D. </author> <year> (1992). </year> <title> Knowledge Acquisition Techniques for Group Decision Support. </title> <editor> In Gaines, B., Musen, M., & Boose, J., (Eds.), </editor> <booktitle> Proceedings of the 7th Knowledge Acquisition for Knowledge-Based Systems Workshop, </booktitle> <pages> pages 2.1-2.22. </pages>
Reference-contexts: More specifically, managers need to seek out problems, solve them, then install some monitoring routine to check that the fix works. A taxonomy of tasks used in that process is shown in Figure 8. Other DSS workers have a similar view. Boose, Bradshaw, Koszaek, and Shema (BBKS) <ref> [Boose et al., 1992] </ref> discuss DSS architectures suitable for groups. The BBKS system lacks an execution module for the generated models as part of the group decision support environment. Boose et. al. assume that once the group's mode is elicited, it will be subsequently exported into an executable form. <p> Portions of the BBKS and the Brookes' models overlap. The BBKS system lets groups manipulate their group model, its interrelationships, and the group's criteria for selecting the best alternative. BBKS stress that: The process of generating and scoring alternatives are at the heart of most decision processes. <ref> [Boose et al., 1992] </ref> That is, more important than representing and executing a model is an ability to assess a model. Note that the BE ST operator of HT4 directly implements this alternative generation, assessment, and selection procedure.
Reference: [Bradshaw et al., 1991] <author> Bradshaw, J., Ford, K., & Adams-Webber, J. </author> <year> (1991). </year> <title> Knowledge Representation of Knowledge Acquisition: A Three-Schemata Approach. </title> <booktitle> In 6th AAAI-Sponsored Banff Knowledge Acquisition for Knowledge-Based Systems Workshop, </booktitle> <month> ,October 6-11 </month> <year> 1991, </year> <title> Banff, </title> <address> Canada, pages 4.1 - 4.25. </address>
Reference-contexts: Researchers into decision support tools make a case something like weak SC. They argue that human "knowledge" appears in some social context and that context can effect the generated "knowledge". Phillips [Phillips, 1984] and Bradshaw et. al. <ref> [Bradshaw et al., 1991] </ref> characterise model construction as a communal process that generates symbolic descriptions that explicate a community's understand of a problem. If the community changes then the explicit record of the communities shared understanding also changes; i.e. "truth" is socially constructed.
Reference: [Bredeweg, 1992] <author> Bredeweg, B. </author> <year> (1992). </year> <title> Expertise in Qualitative Prediction of Behaviour. </title> <type> PhD thesis, </type> <institution> University of Amsterdam. </institution>
Reference-contexts: Further, different interpretations exist of the same problem solving method. For example: Model % disorders identified % knowledge fragments identified 1 (Epistemological) 50 28 2 (KADS) 55 34 3 (no model) 75 41 * The problem solving method proposed by Bredeweg <ref> [Bredeweg, 1992] </ref> for prediction via qualitative reasoning is different to the qualitative prediction problem solving method proposed by Tansley & Hayball [Tansley & Hayball, 1993]. * The KADS problem solving methods for diagnosis [Wielinga et al., 1992] is very different to the assumption-space exploration model proposed by the model-based diagnosis community
Reference: [Breuker, 1994] <author> Breuker, J. </author> <year> (1994). </year> <title> Components of Problem Solving and Types of Problems. </title> <booktitle> In 8th European Knowledge Acquisition Workshop, EKAW '94, </booktitle> <pages> pages 118-136. </pages>
Reference-contexts: An interesting feature of abduction is that it both a validation and an inference engine. It maps exactly into Clancey's characterisation of expert systems as devices that build a system-specific model (SSM) or Breuker's component's of solutions <ref> [Breuker, 1994, Clancey, 1992] </ref>.
Reference: [Brookes, 1986] <author> Brookes, C. </author> <year> (1986). </year> <title> Requirements Elicitation for Knowledge Based Decision Support Systems. </title> <type> Technical Report 11, </type> <institution> Information Systems, University of New South Wales. </institution>
Reference-contexts: Simon originally characterised decision making as a three stage process: intelligence (scanning environment), design (develop alternative courses), and choice (selection of alternative) [Simon, 1960]. Our preferred definition of a decision-support system is based on Brookes <ref> [Brookes, 1986] </ref>' who developed it from Simon's and Mintzberg's model [Mintzberg, 1975]. The goal of a DSS is management comfort, i.e. a subjective impression that all problems are known and under control.
Reference: [Buchanan & Smith, 1989] <author> Buchanan, B. & Smith, R. </author> <year> (1989). </year> <title> Fundamentals of Expert Systems. </title> <editor> In A. Barr, P. C. & Feigenbaum, E., (Eds.), </editor> <booktitle> The Handbook of Artificial Intelligence, </booktitle> <volume> Volume 4, volume 4, </volume> <pages> pages 149-192. </pages> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Despite careful attempts to generalise principles of knowledge acquisition, (e.g. [Stefik et al., 1982]), expert systems construction remained a somewhat hit-and-miss process. By the end of the 1980s, it was recognised that our design concepts for knowledge-based systems were incomplete <ref> [Buchanan & Smith, 1989] </ref>. For example, Steels [Steels, 1994] cites an example where an expert could not solve a problem over the phone but, as soon as they walked into the room where the trouble was, could solve it instantly.
Reference: [Bylander et al., 1991] <author> Bylander, T., Allemang, D., M.C. Tanner, M., & Josephson, J. </author> <year> (1991). </year> <title> The Computational Complexity of Abduction. </title> <journal> Artificial Intelligence, </journal> <volume> 49 </volume> <pages> 25-60. </pages>
Reference: [Campbell et al., 1982] <author> Campbell, A., Hollister, V., Duda, R., & Hart, P. </author> <year> (1982). </year> <title> Recognition of a Hidden Material Deposit by and Artificially Intelligent Program. </title> <journal> Science, </journal> <volume> 217 </volume> <pages> 927-929. </pages>
Reference-contexts: Further, if the situation is as bad as suggested above, then how is it that we have so many seemingly successful expert systems (e.g. MYCIN [Yu et al., 1979], CASNET [Weiss et al., 1978], PROSPECTOR <ref> [Campbell et al., 1982, Duda et al., 1985] </ref>, XCON [Bachant & McDermott, 1984], VT [Marcus et al., 1987], PIGE [Menzies et al., 1992])? This kind of argument is the basis of Vera & Simon's criticisms of SC [Vera & Simon, 1993b, Vera & Simon, 1993a, Vera & Simon, 1993c].
Reference: [Catlett, 1991] <author> Catlett, J. </author> <year> (1991). </year> <title> Inductive learning from subsets or Disposal of excess training data considered harmful. </title> <booktitle> In Australian Workshop on Knowledge Acqusition for Knowledge-Based Systems, Pokolbin, </booktitle> <pages> pages 53-67. </pages>
Reference-contexts: That is, they believe that the model will never be finished/correct [Compton, 1994]. Experiments in machine learning endorse the proposition that any version of a model can be improved after more experience. Machine learning programs input training data to generate a model. Catlett's research <ref> [Catlett, 1991] </ref> explored the following area.
Reference: [Charniak & McDermott, 1987] <author> Charniak, E. & McDermott, D. </author> <year> (1987). </year> <title> Introduction to Artificial Intelligence. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Coming from McDermott, this is a telling criticism since, prior to this article, he was one of the leading proponents of that logical school <ref> [Charniak & McDermott, 1987] </ref>. With the benefit of a little historical hindsight, we can defeat some of McDermott's 1987 arguments. Mc-Dermott repeatedly uses Forbus' 1984 Qualitative Process Theory (QPT) [Forbus, 1984] as an worthy example of an algorithmic/non-logical system.
Reference: [Clancey, 1985] <author> Clancey, W. </author> <year> (1985). </year> <title> Heuristic Classification. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 289-350. </pages>
Reference-contexts: A method will be described below to assess these techniques (x5.2). 4.1 Response: Ignore it 4.1.1 Not a KA Problem Clancey cautions [Clancey, 1989a] that we should not confuse pragmatic discussions about techniques for knowledge acquisition (e.g. <ref> [Clancey, 1985, Clancey, 1989b, Clancey, 1992] </ref>) with discussions about the basic nature of human intelligence (e.g. [Clancey, 1987, Clancey, 1991, Clancey, 1993]).
Reference: [Clancey, 1987] <author> Clancey, W. </author> <year> (1987). </year> <title> Book Review of Winograd & Flores, Understanding Computers and Cognition: A New Foundation for Design. </title> <journal> Artificial Intelligence, </journal> <volume> 31 </volume> <pages> 233-250. </pages>
Reference-contexts: Clancey <ref> [Clancey, 1987, Clancey, 1991] </ref> and Winograd & Flores [Winograd & Flores, 1987] argue that it is a mistake to confuse the symbolic descriptions which humans use to co-ordinate their activities and reflect about their actions (i.e. language) with how humans might generate their minute-to-minute behaviour. <p> Every action is an interpretation of the current situation, based on the entire history of our interactions. On some sense every action is automatically an inductive, adjusted process <ref> [Clancey, 1987, p238] </ref>. Researchers into decision support tools make a case something like weak SC. They argue that human "knowledge" appears in some social context and that context can effect the generated "knowledge". <p> be described below to assess these techniques (x5.2). 4.1 Response: Ignore it 4.1.1 Not a KA Problem Clancey cautions [Clancey, 1989a] that we should not confuse pragmatic discussions about techniques for knowledge acquisition (e.g. [Clancey, 1985, Clancey, 1989b, Clancey, 1992]) with discussions about the basic nature of human intelligence (e.g. <ref> [Clancey, 1987, Clancey, 1991, Clancey, 1993] </ref>). <p> Further, just because a system based on explicit symbolic descriptions works, this says nothing about the best way to build and maintain those symbolic descriptions. Clancey acknowledges the role of symbolic descriptions in working systems <ref> [Clancey, 1987, p278] </ref> [Clancey, 1992]. Symbolic descriptions, Clancey argues, are useful to planning about the future and reflecting on action rather than immediately reacting to a new situation. Human reasoning is immensely more successful by our ability to simulate what might happen, to visualize possible outcomes and prepare for them. <p> Human reasoning is immensely more successful by our ability to simulate what might happen, to visualize possible outcomes and prepare for them. We do this by reflecting, saying what we expect, and responding to what we say <ref> [Clancey, 1987, p247] </ref>. However, Clancey's symbolic descriptions are not as fixed as those in Ontolingua [Gruber, 1993] or the inference layer of KADS. It remains to explain how (the symbolic descriptions) develop: : : . Most learning programs grammatically describe how representations accumulate within a fixed language.
Reference: [Clancey, 1989a] <author> Clancey, W. </author> <year> (1989a). </year> <title> The knowledge level reinterpreted: Modeling how systems interact. </title> <journal> Machine Learning, </journal> 4(3/4):285-293. 
Reference-contexts: A method will be described below to assess these techniques (x5.2). 4.1 Response: Ignore it 4.1.1 Not a KA Problem Clancey cautions <ref> [Clancey, 1989a] </ref> that we should not confuse pragmatic discussions about techniques for knowledge acquisition (e.g. [Clancey, 1985, Clancey, 1989b, Clancey, 1992]) with discussions about the basic nature of human intelligence (e.g. [Clancey, 1987, Clancey, 1991, Clancey, 1993]).
Reference: [Clancey, 1989b] <author> Clancey, W. </author> <year> (1989b). </year> <title> Viewing Knowledge Bases as Qualitative Models. </title> <journal> IEEE Expert, </journal> <pages> pages 9-23. </pages>
Reference-contexts: A method will be described below to assess these techniques (x5.2). 4.1 Response: Ignore it 4.1.1 Not a KA Problem Clancey cautions [Clancey, 1989a] that we should not confuse pragmatic discussions about techniques for knowledge acquisition (e.g. <ref> [Clancey, 1985, Clancey, 1989b, Clancey, 1992] </ref>) with discussions about the basic nature of human intelligence (e.g. [Clancey, 1987, Clancey, 1991, Clancey, 1993]).
Reference: [Clancey, 1991] <author> Clancey, W. </author> <year> (1991). </year> <title> Book Review of Israel Rosenfield, The Invention of Memory: A New View of the Brain. </title> <journal> Artificial Intelligence, </journal> <volume> 50 </volume> <pages> 241-284. </pages>
Reference-contexts: Clancey <ref> [Clancey, 1987, Clancey, 1991] </ref> and Winograd & Flores [Winograd & Flores, 1987] argue that it is a mistake to confuse the symbolic descriptions which humans use to co-ordinate their activities and reflect about their actions (i.e. language) with how humans might generate their minute-to-minute behaviour. <p> be described below to assess these techniques (x5.2). 4.1 Response: Ignore it 4.1.1 Not a KA Problem Clancey cautions [Clancey, 1989a] that we should not confuse pragmatic discussions about techniques for knowledge acquisition (e.g. [Clancey, 1985, Clancey, 1989b, Clancey, 1992]) with discussions about the basic nature of human intelligence (e.g. <ref> [Clancey, 1987, Clancey, 1991, Clancey, 1993] </ref>). <p> It remains to explain how (the symbolic descriptions) develop: : : . Most learning programs grammatically describe how representations accumulate within a fixed language. They don't explain how representations are created, or more generally, the evolution of new routines not described by the given grammar <ref> [Clancey, 1991, p279] </ref>. Knowledge acquisition is the key point that is ignored by Vera & Simon.
Reference: [Clancey, 1992] <author> Clancey, W. </author> <year> (1992). </year> <title> Model Construction Operators. </title> <journal> Artificial Intelligence, </journal> <volume> 53 </volume> <pages> 1-115. </pages>
Reference-contexts: There is a difference between PSCM (hereafter, KL A ) and KL B , a KL-modeling variant which groups together a set of authors who argue for basically the same technique; i.e. Clancey's model construction operators <ref> [Clancey, 1992] </ref>, Steels' components of expertise [Steels, 1990], Chandrasekaran's task analysis, SPARK/ BURN/ FIREFIGHTER (SBF) [Marques et al., 1992] and KADS [Wielinga et al., 1992]. The fundamental premise of KL B is that a knowledge base should be divided into domain-specific facts and domain-independent PSMs. <p> A method will be described below to assess these techniques (x5.2). 4.1 Response: Ignore it 4.1.1 Not a KA Problem Clancey cautions [Clancey, 1989a] that we should not confuse pragmatic discussions about techniques for knowledge acquisition (e.g. <ref> [Clancey, 1985, Clancey, 1989b, Clancey, 1992] </ref>) with discussions about the basic nature of human intelligence (e.g. [Clancey, 1987, Clancey, 1991, Clancey, 1993]). <p> Further, just because a system based on explicit symbolic descriptions works, this says nothing about the best way to build and maintain those symbolic descriptions. Clancey acknowledges the role of symbolic descriptions in working systems [Clancey, 1987, p278] <ref> [Clancey, 1992] </ref>. Symbolic descriptions, Clancey argues, are useful to planning about the future and reflecting on action rather than immediately reacting to a new situation. Human reasoning is immensely more successful by our ability to simulate what might happen, to visualize possible outcomes and prepare for them. <p> An interesting feature of abduction is that it both a validation and an inference engine. It maps exactly into Clancey's characterisation of expert systems as devices that build a system-specific model (SSM) or Breuker's component's of solutions <ref> [Breuker, 1994, Clancey, 1992] </ref>.
Reference: [Clancey, 1993] <author> Clancey, W. </author> <year> (1993). </year> <title> Situated Action: A Neuropsychological Interpretation (Response to Vera and Simon). </title> <journal> Cognitive Science, </journal> <volume> 17 </volume> <pages> 87-116. </pages>
Reference-contexts: Rather, says Clancey, these structures are created on-the-fly as posthoc symbolic justifications of a process which is not symbolic: The neural structures and processes that coordinate perception and action are created during activity, not retrieved and rotely applied, merely reconstructed, or calculated via stored rules and pattern descriptions <ref> [Clancey, 1993, p94] </ref>. Clancey's view is not resolved merely by declaring that knowledge representations are approximate surrogate models of reality (e.g. as proposed by [Davis et al., 1993]). <p> be described below to assess these techniques (x5.2). 4.1 Response: Ignore it 4.1.1 Not a KA Problem Clancey cautions [Clancey, 1989a] that we should not confuse pragmatic discussions about techniques for knowledge acquisition (e.g. [Clancey, 1985, Clancey, 1989b, Clancey, 1992]) with discussions about the basic nature of human intelligence (e.g. <ref> [Clancey, 1987, Clancey, 1991, Clancey, 1993] </ref>). <p> Vera & Simon argue that the physical symbol system hypothesis (PSSH) [Newell & Simon, 1972] has been a fruitful paradigm which can reproduce many known behaviours of experts. They decline to reject PSSH for strong SC since, if they adopted, e.g. Clancey's situated paradigm <ref> [Clancey, 1993] </ref>, then they are unclear on what predictions can be made and what experiments can be performed. That is, they argue that SC is unfalsifiable and unscientific 3 .
Reference: [Clancey, 1996] <author> Clancey, W. </author> <year> (1996). </year> <type> Personal communcaition. </type>
Reference-contexts: Clancey prefers to reserve discussions on SC for the creation of human-equivalent robots which react to real-world situations since, he says, SC argues that human activity is not strictly mediated by inference over descriptions nor is activity a compiled result of such inference <ref> [Clancey, 1996] </ref>. Clancey's remarks notwithstanding, this paper argues that SC has a significant impact on KA. If weak SC is true, then we cannot expect to reuse old symbolic descriptions of ontologies or PSMs as a productivity tool for some current application.
Reference: [Compton, 1994] <author> Compton, P. </author> <year> (1994). </year> <title> Personal communication. regarding the status of the PIERS system. </title>
Reference-contexts: Significantly, the user-group sponsoring the project have created a permanent line item in their budget for maintenance. They anticipate that routinely every day, an expert will review the generated diagnoses and change some of the KB. That is, they believe that the model will never be finished/correct <ref> [Compton, 1994] </ref>. Experiments in machine learning endorse the proposition that any version of a model can be improved after more experience. Machine learning programs input training data to generate a model. Catlett's research [Catlett, 1991] explored the following area.
Reference: [Compton et al., 1989] <author> Compton, P., Horn, K., Quinlan, J., & Lazarus, L. </author> <year> (1989). </year> <title> Maintaining an Expert System. </title> <editor> In Quinlan, J., (Ed.), </editor> <booktitle> Applications of Expert Systems, </booktitle> <pages> pages 366-385. </pages> <publisher> Addison Wesley. </publisher>
Reference-contexts: If weak SC was false, then we should see that using knowledge does not change that knowledge; i.e. knowledge maintenance for static domains should terminate when it arrives at "truth". Compton <ref> [Compton et al., 1989] </ref> reports studies that documented the changes made to models of biochemistry diagnosis systems. The Garvan ES-1, expert system was developed using a traditional iterative prototyping knowledge engineering methodology. Rules that began as simple modular chunks of knowledge evolved into very complicated and confusing knowledge (e.g. <p> Despite this, the Garvan ES-1 expert system never reached a logical termination point, despite years of maintenance. There was always one more major insight into the domain, one more major conceptual error, and one more significant addition <ref> [Compton et al., 1989] </ref>. A graph of the size of that knowledge base versus time (Figure 5) is consistent with either a linear growth curve or a logarithmic curve. <p> A KA methodology that acknowledges SC must offer details about creating and changing a knowledge base. A review of the KA literature suggests that most of the effort is in knowledge analysis and not knowledge maintenance (exceptions: <ref> [de Brug et al., 1986, Compton et al., 1989] </ref>).
Reference: [Compton & Jansen, 1990] <author> Compton, P. & Jansen, R. </author> <year> (1990). </year> <title> A Philosophical Basis for Knowledge Acquisition. </title> <journal> Knowledge Acquisition, </journal> <volume> 2 </volume> <pages> 241-257. </pages>
Reference-contexts: Compton argues that his process of "patching in the context of error" is a more realistic KA approach than assuming that a human analyst will behave in a perfectly rational way to create some initial correct design <ref> [Compton & Jansen, 1990] </ref>. 5 Assessing Responses to Situated Cognition Weak SC suggests that, as far as possible, the symbolic structures inside an expert system must be changeable. Any representational system assumes certain primitives which can't be changed.
Reference: [Console & Torasso, 1991] <author> Console, L. & Torasso, P. </author> <year> (1991). </year> <title> A Spectrum of Definitions of Model-Based Diagnosis. </title> <journal> Computa--tional Intelligence, </journal> <volume> 7 </volume> <pages> 133-141. </pages>
Reference-contexts: Note that the BE ST operator of HT4 directly implements this alternative generation, assessment, and selection procedure. Further, abduction can be used for other DSS tasks such as diagnosis <ref> [Console & Torasso, 1991] </ref> and monitoring 5 . 4.7 Response: Ripple-Down-Rules KL B (x2) typically assumes that prior to building a system, an extensive analysis stage develops a design for the system. Compton reports experiments with a completely reversed approach. In ripple-down-rules (RDR), there is no analysis period.
Reference: [Corbridge et al., 1995] <author> Corbridge, C., Major, N., & Shadbolt, N. </author> <year> (1995). </year> <title> Models Exposed: An Empirical Study. In Proceedings of the 9th AAAI-Sponsored Banff Knowledge Acquisition for Knowledge Based Systems. </title>
Reference-contexts: Knowledge developed in one context may not be usefully reusable in another. Corbridge et. al. report a study in which subjects had to extract knowledge from an expert dialogue using a variety of abstract pattern tools <ref> [Corbridge et al., 1995] </ref>. In that study, subjects were supplied with transcripts of a doctor interviewing a patient. From the transcripts, it was possible to extract 20 respiratory disorders and a total of 304 "knowledge fragments" (e.g. identification of routine tests, non-routine tests, relevant parameters, or complaints).
Reference: [Craw et al., 1994] <author> Craw, S., Sleeman, D., Boswell, R., & Carbonara, L. </author> <year> (1994). </year> <title> Is knowledge refinement different from theory revision? In Wrobel, </title> <editor> S., (Ed.), </editor> <booktitle> Proceedings of the MLNet Familiarization Workshop on Theory Revision and Restructuring in Machine Learning (ECML-94), </booktitle> <pages> pages 32-34. </pages>
Reference-contexts: This can be done using automatic tools (e.g. explanation-based generalisation [van Harmelen & Bundy, 1988]) or semi-automatic tools where the user's opinions are used as part of the theory refinement loop. Heuristic KB refinement (e.g. KRUST <ref> [Craw et al., 1994, Palmer & Craw, 1995] </ref> and expert critiquing systems (x4.4)) are a kind of "machine learning" algorithm in which domain-specific principles are used to fault a KB and assist a human in fixing the faults.
Reference: [Crawford et al., 1992] <author> Crawford, J., Farquhar, A., & Kuipers, B. </author> <year> (1992). </year> <title> QPC: A Compiler from Physical Models into Qualitative Differential Equations. </title> <editor> In Faltings, B. & Struss, P., (Eds.), </editor> <booktitle> Recent Advances in Qualitative Physics. </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: McDermott originally demanded in 1984 that Forbus record the logical axioms underlying QPT. However, in 1987, McDermott comments that ": : : the task, seemingly so feasible (is) actually impossible" [McDermott, 1987, p152]. Note that, 12 years later, QPT was later implemented via a compilation into QSIM <ref> [Crawford et al., 1992] </ref>. QSIM was a special-purpose theorem-prover built by Kuipers in 1986 for processing qualitative differential equations [Kuipers, 1986]. In 1993, Kuipers [Kuipers, 1993, p134] acknowledged is a textbook application of Mackworth's constraint-logic system [Mackworth, 1977, Mackworth, 1992]. That is, QPT was an instantiation of a logic-based system.
Reference: [Davis et al., 1993] <author> Davis, R., Shrobe, H., & Szolovits, P. </author> <year> (1993). </year> <title> What is a Knowledge Representation? AI Magazine, pages 17-33. </title> <editor> [de Brug et al., 1986] de Brug, A. V., Bachant, J., & McDermott, J. </editor> <year> (1986). </year> <title> The Taming of R1. </title> <journal> IEEE Expert, </journal> <pages> pages 33-39. </pages>
Reference-contexts: Clancey's view is not resolved merely by declaring that knowledge representations are approximate surrogate models of reality (e.g. as proposed by <ref> [Davis et al., 1993] </ref>). Rather, Clancey believes that symbolic structures are not only approximations of human knowledge but also that human knowledge changes as a result of applying it. Every action is an interpretation of the current situation, based on the entire history of our interactions.
Reference: [DeKleer, 1986] <author> DeKleer, J. </author> <year> (1986). </year> <title> An Assumption-Based TMS. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 163-196. </pages>
Reference-contexts: A model-level conflict detection facility such as abductive validation requires knowledge of how terms are combined. 4 The connection of HT4 to DeKleer's ATMS system <ref> [DeKleer, 1986] </ref> is explored elsewhere [Menzies, 1996a] 4.4 Response: Expert Critiquing Systems Silverman [Silverman, 1990, Silverman, 1992] advises that attached to an expert system should be an expert critiquing system which he defines as: : : : programs that first cause their user to maximise the falsifiability of their statements and
Reference: [Dreyfus, 1979] <author> Dreyfus, H. </author> <year> (1979). </year> <title> What Computers Can't D: A Critique of Artifical Reason. </title> <publisher> Freeman. </publisher>
Reference-contexts: Our preferred response to SC includes a PSM customisation tool (x5.1). 3 Situated Cognition 3.1 About the SC Premise Dreyfus argues that the context-dependent nature of human knowledge makes it fundamentally impossible to reproduce in symbolic descriptions <ref> [Dreyfus, 1979] </ref>. Searle takes a similar stand, claiming that the only device that can replicate human intelligence is another human [Searle, 1980, Searle, 1982, Searle, 1995] since only humans can share the same context.
Reference: [Duda et al., 1985] <author> Duda, R., Hart, P., & Reboh, R. </author> <year> (1985). </year> <title> Letter to the Editor. </title> <journal> Artificial Intelligence, </journal> <volume> 26 </volume> <pages> 359-360. </pages>
Reference-contexts: Further, if the situation is as bad as suggested above, then how is it that we have so many seemingly successful expert systems (e.g. MYCIN [Yu et al., 1979], CASNET [Weiss et al., 1978], PROSPECTOR <ref> [Campbell et al., 1982, Duda et al., 1985] </ref>, XCON [Bachant & McDermott, 1984], VT [Marcus et al., 1987], PIGE [Menzies et al., 1992])? This kind of argument is the basis of Vera & Simon's criticisms of SC [Vera & Simon, 1993b, Vera & Simon, 1993a, Vera & Simon, 1993c].
Reference: [Easterbrook, 1991] <author> Easterbrook, S. </author> <year> (1991). </year> <title> Handling conflicts between domain descriptions with computer-supported negotiation. </title> <journal> Knowledge Acquisition, </journal> <volume> 3 </volume> <pages> 255-289. </pages>
Reference-contexts: Note that the chosen premises may radically influence the conclusions reached. Agnew, Ford & Hayes offer their summary of contemporary thinking in the history, philosophy and sociology of science as: Expert-knowledge is comprised of context-dependent, personally constructed, highly functional but fallible abstractions [Agnew et al., 1993]. Easterbrook <ref> [Easterbrook, 1991] </ref> argues that it is undesirable to demand that knowledge bases are con sistent. This insistence that expertise must be consistent and rational imposes restrictions of the knowledge acquired. <p> This insistence that expertise must be consistent and rational imposes restrictions of the knowledge acquired. The knowledge acquisition process becomes not so much the modeling of the expert's behaviour, but the synthesis of a domain model which need not resemble any mental model used by the expert <ref> [Easterbrook, 1991, p264] </ref>. The experience with expert systems is that the process of building consensus between individuals or creating an explicit record of it in a knowledge base introduces biases/errors. Silverman cautions that systematic biases in expert preferences may result in incorrect/incomplete knowledge bases (x4.4).
Reference: [Eshghi, 1993] <author> Eshghi, K. </author> <year> (1993). </year> <title> A Tractable Class of Abductive Problems. </title> <booktitle> In IJCAI '93, </booktitle> <volume> volume 1, </volume> <pages> pages 3-8. </pages>
Reference: [Falkenhainer, 1990] <author> Falkenhainer, B. </author> <year> (1990). </year> <title> Abduction as Similarity-Driven Explanation. </title> <editor> In O'Rourke, P., (Ed.), </editor> <booktitle> Working Notes of the 1990 Spring Symposium on Automated Abduction, </booktitle> <pages> pages 135-139. </pages>
Reference-contexts: abductive framework; e.g. intelligent decision support systems (x4.6 & [Menzies, 1995a]), diagrammatic reasoning [Menzies & Compton, 1994], single-user knowledge acquisition, and multiple-expert knowledge acquisition [Menzies, 1995c], certain interesting features of human cognition [Menzies, 1995d]. natural-language processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning <ref> [Falkenhainer, 1990] </ref>, financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994], case-based reasoning [Leake, 1993], expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, verification and validation (x4.2 & [Menzies, 1996a, Menzies, 1996b]).
Reference: [Feldman et al., 1989] <author> Feldman, B., Compton, P., & Smythe, G. </author> <year> (1989). </year> <title> Hypothesis Testing: an Appropriate Task for Knowledge-Based Systems. </title> <booktitle> In 4th AAAI-Sponsored Knowledge Acquisition for Knowledge-based Systems Workshop Banff, </booktitle> <address> Canada. </address>
Reference-contexts: Our preferred validation approach is for the input-output test pairs to be generated totally separately to the current model; e.g. from real-world observations of the entity being modeled in the KB. Based on work by Feldman & Compton <ref> [Feldman et al., 1989] </ref>, a general validation framework based on the HT4 abductive inference engine has been developed. Elsewhere [Menzies, 1996a], we have given an overview of abductive research [O'Rourke, 1990,Bylander et al., 1991,Eshghi, 1993,Selman & Levesque, 1990,Poole, 1990b,Menzies, 1996a]. <p> Using an earlier version of HT4 (which they called QMOD and we call HT1) Feldman & Compton reported that only 69% of the known observations could be explained by Smythe '89n <ref> [Feldman et al., 1989] </ref>. In our re-work of that study, and-vertex processing and multiple causes processing was added, thus allowing the processing of more of the known observations. With those changes, HT4 found that only 55% of the observations were explicable [Menzies, 1996b]. <p> With those changes, HT4 found that only 55% of the observations were explicable [Menzies, 1996b]. When these errors were shown to Smythe, he found them novel and exciting <ref> [Feldman et al., 1989] </ref>; i.e. the domain expert found that these errors were significant. This is both a disturbing and exciting finding.
Reference: [Forbus, 1984] <author> Forbus, K. </author> <year> (1984). </year> <title> Qualitative Process Theory. </title> <journal> Artificial Intelligence, </journal> <volume> 24 </volume> <pages> 85-168. </pages>
Reference-contexts: With the benefit of a little historical hindsight, we can defeat some of McDermott's 1987 arguments. Mc-Dermott repeatedly uses Forbus' 1984 Qualitative Process Theory (QPT) <ref> [Forbus, 1984] </ref> as an worthy example of an algorithmic/non-logical system. McDermott originally demanded in 1984 that Forbus record the logical axioms underlying QPT. However, in 1987, McDermott comments that ": : : the task, seemingly so feasible (is) actually impossible" [McDermott, 1987, p152].
Reference: [Gaines & Shaw, 1989] <author> Gaines, B. & Shaw, M. </author> <year> (1989). </year> <title> Comparing the Conceptual Systems of Experts. </title> <booktitle> In IJCAI '89, </booktitle> <pages> pages 633-638. </pages>
Reference-contexts: The conceptual systems of different experts are explicated and compared using a technique called entity-attribute grid elicitation <ref> [Gaines & Shaw, 1989] </ref>. Experts are asked to identify dimensions along which items from the domain can be distinguished. The two extreme ends of these dimensions are recorded left and right of a grid. New items from the domain are categorised along these dimensions. <p> Differences between the conceptual views of different experts can be identified (e.g. their categorisations are different). Gaines & Shaw describe automatic tools for generating plots representing the proximity of different expert's conceptual systems <ref> [Gaines & Shaw, 1989] </ref>. Gaines & Shaw focuses on identifying and resolving conflicts in the meaning of individual terms, not on conflicts in the semantics of the models built using combinations of those terms.
Reference: [Ginsberg, 1990] <author> Ginsberg, A. </author> <year> (1990). </year> <title> Theory Reduction, Theory Revision, </title> <booktitle> and Retranslation. In AAAI '90, </booktitle> <pages> pages 777-782. </pages>
Reference-contexts: If such a test suite of behaviour is missing, then non-monotonic reasoning techniques can be used to explore the dependency graph between KB literals to find sets of input literals which will exercise the entire knowledge <ref> [Ginsberg, 1990, Zlatereva, 1992] </ref>. However, an expert still has to decide what output is appropriate for each generated input. This can introduce a circularity in the testing procedure.
Reference: [Gruber, 1993] <author> Gruber, T. </author> <year> (1993). </year> <title> A Translation Approach to Portable Ontology Specifications. </title> <journal> Knowledge Acquisition, </journal> <volume> 5(2) </volume> <pages> 199-220. </pages>
Reference-contexts: Available from http:// www.cse.unsw.edu.au/ ~timm/pub/ docs/papersonly.html 2 Brief Notes on Knowledge Modeling This section is a brief review of knowledge modeling. For more information, see the Related Work section of [Wielinga et al., 1992] and [Menzies, 1995b]. See also the ontology literature (e.g. <ref> [Gruber, 1993] </ref>) which assumes that declarative descriptions of portions of old expert systems are useful for building new applications. In Newell's KL approach, intelligence is modeled as a search for appropriate operators that convert some current state to a goal state. <p> We do this by reflecting, saying what we expect, and responding to what we say [Clancey, 1987, p247]. However, Clancey's symbolic descriptions are not as fixed as those in Ontolingua <ref> [Gruber, 1993] </ref> or the inference layer of KADS. It remains to explain how (the symbolic descriptions) develop: : : . Most learning programs grammatically describe how representations accumulate within a fixed language.
Reference: [Hamscher, 1990] <author> Hamscher, W. </author> <year> (1990). </year> <title> Explaining Unexpected Financial Results. </title> <editor> In O'Rourke, P., (Ed.), </editor> <booktitle> AAAI Spring Symposium on Automated Abduction, </booktitle> <pages> pages 96-100. </pages>
Reference-contexts: decision support systems (x4.6 & [Menzies, 1995a]), diagrammatic reasoning [Menzies & Compton, 1994], single-user knowledge acquisition, and multiple-expert knowledge acquisition [Menzies, 1995c], certain interesting features of human cognition [Menzies, 1995d]. natural-language processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning <ref> [Hamscher, 1990] </ref>, machine learning [Hirata, 1994], case-based reasoning [Leake, 1993], expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, verification and validation (x4.2 & [Menzies, 1996a, Menzies, 1996b]).
Reference: [Hamscher et al., 1992] <author> Hamscher, W., Console, L., & DeKleer, J. </author> <year> (1992). </year> <title> Readings in Model-Based Diagnosis. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1992] for prediction via qualitative reasoning is different to the qualitative prediction problem solving method proposed by Tansley & Hayball [Tansley & Hayball, 1993]. * The KADS problem solving methods for diagnosis [Wielinga et al., 1992] is very different to the assumption-space exploration model proposed by the model-based diagnosis community <ref> [Hamscher et al., 1992] </ref>. Note that the KADS diagnosis model continues to change at a rapid rate (e.g. [Benjamins, 1995]). Knowledge developed in one context may not be usefully reusable in another.
Reference: [Hirata, 1994] <author> Hirata, K. </author> <year> (1994). </year> <title> A Classification of Abduction: Abduction for Logic Programming. </title> <booktitle> In Proceedings of the Fourteenth International Machine Learning Workshop, </booktitle> <address> ML-14, </address> <note> page 16. Also in Machine Intelligence 14 (to appear). </note>
Reference-contexts: & [Menzies, 1995a]), diagrammatic reasoning [Menzies & Compton, 1994], single-user knowledge acquisition, and multiple-expert knowledge acquisition [Menzies, 1995c], certain interesting features of human cognition [Menzies, 1995d]. natural-language processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning <ref> [Hirata, 1994] </ref>, case-based reasoning [Leake, 1993], expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, verification and validation (x4.2 & [Menzies, 1996a, Menzies, 1996b]).
Reference: [Kohavi et al., 1996] <author> Kohavi, R., Sommerfield, D., & Dougherty, J. </author> <year> (1996). </year> <title> Data Minining using MLC++: A Machine Learning Library in C++. </title> <booktitle> In Tools with AI 1996. </booktitle>
Reference-contexts: If numerous examples are available (say, hundreds to thousands), then empirical inductive techniques such as mathematical regression, genetic algorithms, neural nets or other techniques (e.g. nearest-neighbor algorithms, decision-tree generation, simple Bayesian reasoning <ref> [Kohavi et al., 1996] </ref>) can propose a new theory. These techniques have not come to replace standard knowledge acquisition for several reasons. Firstly, most naturally-occurring domains are data-poor. Automatic empirical inductive generalisation in such data-poor domains is an unreliable technique.
Reference: [Kuhn, 1962] <author> Kuhn, T. </author> <year> (1962). </year> <title> The Structure of Scientific Revolutions. </title> <publisher> Cambridge Press. </publisher>
Reference-contexts: Many twentieth century thinkers have therefore adopted a relativist knowledge position. Kuhn notes that data is not interpreted neutrally, but (in the usual case) processed in terms of some dominant intellectual paradigm <ref> [Kuhn, 1962] </ref>. Popper [Popper, 1963] argues that, ultimately, we cannot prove the "truth" of anything since "proofs" must terminate on premises. If we request proofs of premises, then we potentially recurse forever. Hence, on purely pragmatic grounds, people are forced into an acceptance of certain premises.
Reference: [Kuipers, 1986] <author> Kuipers, B. </author> <year> (1986). </year> <title> Qualitative Simulation. </title> <journal> Artificial Intelligence, </journal> <volume> 29 </volume> <pages> 229-338. </pages>
Reference-contexts: Note that, 12 years later, QPT was later implemented via a compilation into QSIM [Crawford et al., 1992]. QSIM was a special-purpose theorem-prover built by Kuipers in 1986 for processing qualitative differential equations <ref> [Kuipers, 1986] </ref>. In 1993, Kuipers [Kuipers, 1993, p134] acknowledged is a textbook application of Mackworth's constraint-logic system [Mackworth, 1977, Mackworth, 1992]. That is, QPT was an instantiation of a logic-based system. However, when it was first developed, this was not known.
Reference: [Kuipers, 1993] <author> Kuipers, B. </author> <year> (1993). </year> <title> Qualitative Simulation: then and now. </title> <journal> Artificial Intelligence, </journal> <volume> 59 </volume> <pages> 133-140. </pages>
Reference-contexts: Note that, 12 years later, QPT was later implemented via a compilation into QSIM [Crawford et al., 1992]. QSIM was a special-purpose theorem-prover built by Kuipers in 1986 for processing qualitative differential equations [Kuipers, 1986]. In 1993, Kuipers <ref> [Kuipers, 1993, p134] </ref> acknowledged is a textbook application of Mackworth's constraint-logic system [Mackworth, 1977, Mackworth, 1992]. That is, QPT was an instantiation of a logic-based system. However, when it was first developed, this was not known.
Reference: [Larkin et al., 1980] <author> Larkin, J., McDermott, J., Simon, D., & Simon, H. </author> <year> (1980). </year> <title> Expert and Novice Performance in Solving Physics Problems. </title> <journal> Science, </journal> <volume> 208 </volume> <pages> 1335-1342. </pages>
Reference-contexts: Instead of focusing on reusing old knowledge, KA SC-style should focus on how we build and change models. That is, expertise is not a function of using a large library of old knowledge as argued in <ref> [Larkin et al., 1980] </ref> and favoured by the KL B approach. Rather, expertise is the ability to quickly adapt old models to new situations. A KA methodology that acknowledges SC must offer details about creating and changing a knowledge base.
Reference: [Leake, 1993] <author> Leake, D. </author> <year> (1993). </year> <title> Focusing Construction and Selection of Abductive Hypotheses. </title> <booktitle> In IJCAI '93, </booktitle> <pages> pages 24-29. </pages>
Reference-contexts: reasoning [Menzies & Compton, 1994], single-user knowledge acquisition, and multiple-expert knowledge acquisition [Menzies, 1995c], certain interesting features of human cognition [Menzies, 1995d]. natural-language processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994], case-based reasoning <ref> [Leake, 1993] </ref>, expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, verification and validation (x4.2 & [Menzies, 1996a, Menzies, 1996b]). Further, abduction handles certain hard and interesting cases; such as the processing of indeterminate, under-specified, globally inconsistent, poorly measured theories.
Reference: [Linster, 1992] <author> Linster, M. </author> <year> (1992). </year> <title> A review of Sisyphus 91 and 92: Models of Problem-Solving Knowledge. </title> <editor> In Aussenac, N., Boy, G., Gaines, B., Linser, M., Ganascia, J.-G., & Kordratoff, Y., (Eds.), </editor> <booktitle> Knowledge Acquisition for Knowledge-Based Systems, </booktitle> <pages> pages 159-182. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Both MIP and AIP contribute edges to a knowledge base dependency graph. This knowledge base is subsequently evaluated via our validation process (x4.2). We would declare MIP or AIP to be satisfactory if it can generate competent systems from the same specification (e.g. one of the Sisyphus projects <ref> [Linster, 1992] </ref>). Note that we can assess competence via the HT4 abductive validation algorithm. Our validation process also lets us identify good edges (x4.2). Further, we can declare a BE ST operator to be good if it was exercised in the generation of good edges.
Reference: [Linster & Musen, 1992] <author> Linster, M. & Musen, M. </author> <year> (1992). </year> <title> Use of KADS to Create a Conceptual Model of the ONCOCIN task. </title> <journal> Knowledge Acquisition, </journal> <volume> 4 </volume> <pages> 55-88. </pages>
Reference-contexts: While there is some overlap, the lists are different. Also, the number and nature of the problem solving methods is not fixed. Often when a domain is analysed using KL B , a new problem solving method is required <ref> [Linster & Musen, 1992] </ref>. Further, different interpretations exist of the same problem solving method.
Reference: [Mackworth, 1977] <author> Mackworth, A. </author> <year> (1977). </year> <title> Consistency in Networks of Relations. </title> <journal> Artificial Intelligence, </journal> <volume> 8 </volume> <pages> 99-118. </pages>
Reference-contexts: QSIM was a special-purpose theorem-prover built by Kuipers in 1986 for processing qualitative differential equations [Kuipers, 1986]. In 1993, Kuipers [Kuipers, 1993, p134] acknowledged is a textbook application of Mackworth's constraint-logic system <ref> [Mackworth, 1977, Mackworth, 1992] </ref>. That is, QPT was an instantiation of a logic-based system. However, when it was first developed, this was not known. The lesson of the QPT story is that logical/symbolic descriptions could handle seemingly functional semantics.
Reference: [Mackworth, 1992] <author> Mackworth, A. </author> <year> (1992). </year> <title> The Logic of Constraint Satisfaction. </title> <journal> Artificial Intelligence, </journal> <volume> 58 </volume> <pages> 3-20. </pages>
Reference-contexts: QSIM was a special-purpose theorem-prover built by Kuipers in 1986 for processing qualitative differential equations [Kuipers, 1986]. In 1993, Kuipers [Kuipers, 1993, p134] acknowledged is a textbook application of Mackworth's constraint-logic system <ref> [Mackworth, 1977, Mackworth, 1992] </ref>. That is, QPT was an instantiation of a logic-based system. However, when it was first developed, this was not known. The lesson of the QPT story is that logical/symbolic descriptions could handle seemingly functional semantics.
Reference: [Marcus et al., 1987] <author> Marcus, S., Stout, J., & McDermott, J. </author> <year> (1987). </year> <title> VT: An Expert Elevator Designer That Uses Knowledge-Based Backtracking. </title> <journal> AI Magazine, </journal> <pages> pages 41-58. </pages>
Reference-contexts: Further, if the situation is as bad as suggested above, then how is it that we have so many seemingly successful expert systems (e.g. MYCIN [Yu et al., 1979], CASNET [Weiss et al., 1978], PROSPECTOR [Campbell et al., 1982, Duda et al., 1985], XCON [Bachant & McDermott, 1984], VT <ref> [Marcus et al., 1987] </ref>, PIGE [Menzies et al., 1992])? This kind of argument is the basis of Vera & Simon's criticisms of SC [Vera & Simon, 1993b, Vera & Simon, 1993a, Vera & Simon, 1993c].
Reference: [Marques et al., 1992] <author> Marques, D., Dallemagne, G., Kliner, G., McDermott, J., & Tung, D. </author> <year> (1992). </year> <title> Easy Programming: Empowering People to Build Their Own Applications. </title> <journal> IEEE Expert, </journal> <pages> pages 16-29. </pages>
Reference-contexts: Clancey's model construction operators [Clancey, 1992], Steels' components of expertise [Steels, 1990], Chandrasekaran's task analysis, SPARK/ BURN/ FIREFIGHTER (SBF) <ref> [Marques et al., 1992] </ref> and KADS [Wielinga et al., 1992]. The fundamental premise of KL B is that a knowledge base should be divided into domain-specific facts and domain-independent PSMs. <p> There is evidence for this elsewhere. For example, between the various camps of KL B researchers, there is little agreement on the internal details. Contrast the list of "reusable" problem solving methods from KADS [Wielinga et al., 1992] and SBF <ref> [Marques et al., 1992] </ref> (termed "knowledge sources' and "mechanism" respectively). While there is some overlap, the lists are different. Also, the number and nature of the problem solving methods is not fixed. <p> In the nine applications studied by Marques et. al., development times changed from one to 17 days (using SBF) to 63 to 250 days (without using SBF) <ref> [Marques et al., 1992] </ref>.
Reference: [McDermott, 1987] <author> McDermott, D. </author> <year> (1987). </year> <title> A Critique of Pure Reason. </title> <journal> Computational Intelligence, </journal> <volume> 3 </volume> <pages> 151-160. </pages>
Reference-contexts: However, this endorsement does not necessarily imply an endorsement of strong SC. Like Vera & Simon in x4.1.3, this paper argues that symbolic systems are still a useful paradigm. It is not necessarily true what (e.g.) Birnbaum [Birnbaum, 1991] and McDermott <ref> [McDermott, 1987] </ref> argue; i.e. that the obvious alternative to logical AI is some type of procedural/functional semantics (i.e. strong SC). <p> McDermott's motivation for a move away from symbols is based on his view that there has been "skimpy results so far and : : : it is going to be very difficult to do much better in the future" <ref> [McDermott, 1987, p151] </ref> 1 . Coming from McDermott, this is a telling criticism since, prior to this article, he was one of the leading proponents of that logical school [Charniak & McDermott, 1987]. With the benefit of a little historical hindsight, we can defeat some of McDermott's 1987 arguments. <p> Mc-Dermott repeatedly uses Forbus' 1984 Qualitative Process Theory (QPT) [Forbus, 1984] as an worthy example of an algorithmic/non-logical system. McDermott originally demanded in 1984 that Forbus record the logical axioms underlying QPT. However, in 1987, McDermott comments that ": : : the task, seemingly so feasible (is) actually impossible" <ref> [McDermott, 1987, p152] </ref>. Note that, 12 years later, QPT was later implemented via a compilation into QSIM [Crawford et al., 1992]. QSIM was a special-purpose theorem-prover built by Kuipers in 1986 for processing qualitative differential equations [Kuipers, 1986].
Reference: [Menzies, 1995a] <author> Menzies, T. </author> <year> (1995a). </year> <title> Applications of Abduction #1: Intelligent Decision Support Systems. </title> <type> Technical Report TR95-16, </type> <institution> Department of Software Development, Monash University. </institution>
Reference-contexts: As evidence of this, we can express the details of a wide range of KBS tasks in this abductive framework; e.g. intelligent decision support systems (x4.6 & <ref> [Menzies, 1995a] </ref>), diagrammatic reasoning [Menzies & Compton, 1994], single-user knowledge acquisition, and multiple-expert knowledge acquisition [Menzies, 1995c], certain interesting features of human cognition [Menzies, 1995d]. natural-language processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994],
Reference: [Menzies, 1995b] <author> Menzies, T. </author> <year> (1995b). </year> <title> Limits to Knowledge Level-B Modeling (and KADS). </title> <booktitle> In Proceedings of AI '95, </booktitle> <address> Australia. World-Scientific. </address>
Reference-contexts: Available from http:// www.cse.unsw.edu.au/ ~timm/pub/ docs/papersonly.html 2 Brief Notes on Knowledge Modeling This section is a brief review of knowledge modeling. For more information, see the Related Work section of [Wielinga et al., 1992] and <ref> [Menzies, 1995b] </ref>. See also the ontology literature (e.g. [Gruber, 1993]) which assumes that declarative descriptions of portions of old expert systems are useful for building new applications. In Newell's KL approach, intelligence is modeled as a search for appropriate operators that convert some current state to a goal state.
Reference: [Menzies, 1995c] <author> Menzies, T. </author> <year> (1995c). </year> <title> Principles for Generalised Testing of Knowledge Bases. </title> <type> PhD thesis, </type> <institution> University of New South Wales. </institution>
Reference-contexts: As evidence of this, we can express the details of a wide range of KBS tasks in this abductive framework; e.g. intelligent decision support systems (x4.6 & [Menzies, 1995a]), diagrammatic reasoning [Menzies & Compton, 1994], single-user knowledge acquisition, and multiple-expert knowledge acquisition <ref> [Menzies, 1995c] </ref>, certain interesting features of human cognition [Menzies, 1995d]. natural-language processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994], case-based reasoning [Leake, 1993], expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning,
Reference: [Menzies, 1995d] <author> Menzies, T. </author> <year> (1995d). </year> <title> Situated Semantics is a Side-Effect of the Computational Complexity of Abduction. </title> <booktitle> In Australian Cognitive Science Society, 3rd Conference. </booktitle>
Reference-contexts: As evidence of this, we can express the details of a wide range of KBS tasks in this abductive framework; e.g. intelligent decision support systems (x4.6 & [Menzies, 1995a]), diagrammatic reasoning [Menzies & Compton, 1994], single-user knowledge acquisition, and multiple-expert knowledge acquisition [Menzies, 1995c], certain interesting features of human cognition <ref> [Menzies, 1995d] </ref>. natural-language processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994], case-based reasoning [Leake, 1993], expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, verification and validation
Reference: [Menzies, 1996b] <author> Menzies, T. </author> <year> (1996b). </year> <title> On the Practicality of Abductive Validation. </title> <booktitle> In ECAI '96. </booktitle>
Reference-contexts: In our re-work of that study, and-vertex processing and multiple causes processing was added, thus allowing the processing of more of the known observations. With those changes, HT4 found that only 55% of the observations were explicable <ref> [Menzies, 1996b] </ref>. When these errors were shown to Smythe, he found them novel and exciting [Feldman et al., 1989]; i.e. the domain expert found that these errors were significant. This is both a disturbing and exciting finding. <p> processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994], case-based reasoning [Leake, 1993], expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, verification and validation (x4.2 & <ref> [Menzies, 1996a, Menzies, 1996b] </ref>). Further, abduction handles certain hard and interesting cases; such as the processing of indeterminate, under-specified, globally inconsistent, poorly measured theories. Inferencing over such theories implies making assumptions and handling mutually exclusive assumptions in different worlds.
Reference: [Menzies, 1996a] <author> Menzies, T. </author> <month> (September, </month> <year> 1996a). </year> <title> Applications of Abduction: Knowledge Level Modeling. </title> <journal> International Journal of Human Computer Studies. </journal>
Reference-contexts: Based on work by Feldman & Compton [Feldman et al., 1989], a general validation framework based on the HT4 abductive inference engine has been developed. Elsewhere <ref> [Menzies, 1996a] </ref>, we have given an overview of abductive research [O'Rourke, 1990,Bylander et al., 1991,Eshghi, 1993,Selman & Levesque, 1990,Poole, 1990b,Menzies, 1996a]. Here, we offer an approximate characterisation of abduction as the search for consistent subsets of some background theory that are relevant for achieving some goal. <p> A model-level conflict detection facility such as abductive validation requires knowledge of how terms are combined. 4 The connection of HT4 to DeKleer's ATMS system [DeKleer, 1986] is explored elsewhere <ref> [Menzies, 1996a] </ref> 4.4 Response: Expert Critiquing Systems Silverman [Silverman, 1990, Silverman, 1992] advises that attached to an expert system should be an expert critiquing system which he defines as: : : : programs that first cause their user to maximise the falsifiability of their statements and then proceed to check to <p> processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994], case-based reasoning [Leake, 1993], expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, verification and validation (x4.2 & <ref> [Menzies, 1996a, Menzies, 1996b] </ref>). Further, abduction handles certain hard and interesting cases; such as the processing of indeterminate, under-specified, globally inconsistent, poorly measured theories. Inferencing over such theories implies making assumptions and handling mutually exclusive assumptions in different worlds.
Reference: [Menzies et al., 1992] <author> Menzies, T., Black, J., Fleming, J., & Dean, M. </author> <year> (1992). </year> <title> An Expert System for Raising Pigs. </title> <booktitle> In The first Conference on Practical Applications of Prolog. </booktitle>
Reference-contexts: MYCIN [Yu et al., 1979], CASNET [Weiss et al., 1978], PROSPECTOR [Campbell et al., 1982, Duda et al., 1985], XCON [Bachant & McDermott, 1984], VT [Marcus et al., 1987], PIGE <ref> [Menzies et al., 1992] </ref>)? This kind of argument is the basis of Vera & Simon's criticisms of SC [Vera & Simon, 1993b, Vera & Simon, 1993a, Vera & Simon, 1993c].
Reference: [Menzies & Compton, 1994] <author> Menzies, T. & Compton, P. </author> <year> (1994). </year> <title> A Precise Semantics for Vague Diagrams, </title> <address> pages 149-156. </address> <publisher> World Scientific. </publisher>
Reference-contexts: As evidence of this, we can express the details of a wide range of KBS tasks in this abductive framework; e.g. intelligent decision support systems (x4.6 & [Menzies, 1995a]), diagrammatic reasoning <ref> [Menzies & Compton, 1994] </ref>, single-user knowledge acquisition, and multiple-expert knowledge acquisition [Menzies, 1995c], certain interesting features of human cognition [Menzies, 1995d]. natural-language processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994], case-based reasoning [Leake, 1993],
Reference: [Menzies & Haynes, 1994] <author> Menzies, T. & Haynes, P. </author> <year> (1994). </year> <title> The Methodologies of Methodologies; or, Evaluating Current Methodologies: Why and How. </title> <booktitle> In Tools Pacific '94, </booktitle> <pages> pages 83-92. </pages> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Current KA practice has not acknowledged SC since, if it did, there would be more work in knowledge maintenance. 2 Elsewhere <ref> [Menzies & Haynes, 1994] </ref>, certain inflated claims of reuse [Stark, 1993] from the object-oriented community have been criticised. 4.1.2 Evidence for SC Not Conclusive It could be argued that the evidence for weak SC is not convincing.
Reference: [Menzies & Compton, 1995] <author> Menzies, T. J. & Compton, P. </author> <year> (1995). </year> <title> The (Extensive) Implications of Evaluation on the Development of Knowledge-Based Systems. In Proceedings of the 9th AAAI-Sponsored Banff Knowledge Acquisition for Knowledge Based Systems. </title>
Reference-contexts: It has been argued previously <ref> [Menzies & Compton, 1995] </ref> that such potentially inaccurate models must be tested, lest they generate inappropriate output for certain circumstances. Testing can only demonstrate the presence of bugs (never their absence) and so must be repeated whenever new data is available.
Reference: [Michalski, 1993] <author> Michalski, R. </author> <year> (1993). </year> <title> Toward a Unified Theory of Learning: Multistrategy Task-adaptive Learning. </title> <editor> In Buchanan, B. G. & Wilkin, D. C., (Eds.), </editor> <title> Readings in Knowledge Acquisition and Learning: Automatic Construction and Improvement of Expert System. </title> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Machine learning (ML) techniques can fully or partially automate the creation or the fixing of a specification. Given some input facts, some goals, and some prior knowledge, then ML can use induction, analogy, deduction, or heuristic techniques to generate a revision to the prior knowledge <ref> [Michalski, 1993] </ref>. If numerous examples are available (say, hundreds to thousands), then empirical inductive techniques such as mathematical regression, genetic algorithms, neural nets or other techniques (e.g. nearest-neighbor algorithms, decision-tree generation, simple Bayesian reasoning [Kohavi et al., 1996]) can propose a new theory.
Reference: [Mintzberg, 1975] <author> Mintzberg, H. </author> <year> (1975). </year> <title> The Manager's Job: Folklore and Fact. </title> <booktitle> Harvard Business Review, </booktitle> <pages> pages 29-61. </pages>
Reference-contexts: Simon originally characterised decision making as a three stage process: intelligence (scanning environment), design (develop alternative courses), and choice (selection of alternative) [Simon, 1960]. Our preferred definition of a decision-support system is based on Brookes [Brookes, 1986]' who developed it from Simon's and Mintzberg's model <ref> [Mintzberg, 1975] </ref>. The goal of a DSS is management comfort, i.e. a subjective impression that all problems are known and under control. More specifically, managers need to seek out problems, solve them, then install some monitoring routine to check that the fix works.
Reference: [Muggleton, 1991] <author> Muggleton, S. </author> <year> (1991). </year> <title> Inductive Logic Programming. </title> <journal> New Generation Computing, </journal> <volume> 8 </volume> <pages> 295-318. </pages>
Reference-contexts: Further, most empirical inductive generalisation machine learning algorithms (e.g. the C4.5 decision tree of Figure 6) make no attempt to preserve current beliefs (exception: inductive logic programming <ref> [Muggleton, 1991] </ref>). It may be unacceptable to permit a learning algorithm to scribble all over a knowledge base, particularly those portions which the user has some commitment to.
Reference: [Newell, 1982] <author> Newell, A. </author> <year> (1982). </year> <title> The Knowledge Level. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 87-127. </pages>
Reference-contexts: When implemented, this KL is built on top of a symbol-level containing data structures, algorithms, etc. However, to a KL agent, these sub-cognitive symbol-level constructs are the tools used "sub-consciously" as it performs its KL processing <ref> [Newell, 1982] </ref>. Newell's subsequent exploration of the KL lead to a general rule-based language called SOAR [Rosen-bloom et al., 1993] which was the basis for the problem-space computational model (PSCM) [Yost & Newell, 1989]. Programming SOAR using the PSCM involves the consideration of multiple, nested problem spaces.
Reference: [Newell, 1993] <author> Newell, A. </author> <year> (1993). </year> <title> Reflections on the Knowledge Level. </title> <journal> Artificial Intelligence, </journal> <volume> 59 </volume> <pages> 31-38. </pages>
Reference-contexts: Programming SOAR using the PSCM involves the consideration of multiple, nested problem spaces. Whenever a "don't know what to do" state is reached, a new problem space is forked to solve that problem. Newell concluded that the PSCM was the bridge between SOAR and true KL modeling <ref> [Newell et al., 1991, Newell, 1993] </ref>. There is a difference between PSCM (hereafter, KL A ) and KL B , a KL-modeling variant which groups together a set of authors who argue for basically the same technique; i.e.
Reference: [Newell & Simon, 1972] <author> Newell, A. & Simon, H. </author> <year> (1972). </year> <title> Human Problem Solving. </title> <publisher> Prentice-Hall Englewood Cliffs, </publisher> <address> N.J. </address>
Reference-contexts: We suspect that they would also object to McDermott's lament about "skimpy results so far" (x3.3). Vera & Simon argue that the physical symbol system hypothesis (PSSH) <ref> [Newell & Simon, 1972] </ref> has been a fruitful paradigm which can reproduce many known behaviours of experts. They decline to reject PSSH for strong SC since, if they adopted, e.g.
Reference: [Newell et al., 1991] <author> Newell, A., Yost, G., Laird, J., Rosenbloom, P., & Altmann, E. </author> <year> (1991). </year> <title> Formulating the Problem Space Computational Model. </title> <editor> In Rosenbloom, P., Laird, J., & Newell, A., (Eds.), </editor> <booktitle> The Soar Papers, </booktitle> <volume> volume 2, </volume> <pages> pages 1321-1359. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Programming SOAR using the PSCM involves the consideration of multiple, nested problem spaces. Whenever a "don't know what to do" state is reached, a new problem space is forked to solve that problem. Newell concluded that the PSCM was the bridge between SOAR and true KL modeling <ref> [Newell et al., 1991, Newell, 1993] </ref>. There is a difference between PSCM (hereafter, KL A ) and KL B , a KL-modeling variant which groups together a set of authors who argue for basically the same technique; i.e.
Reference: [Ng & Mooney, 1990] <author> Ng, H. & Mooney, R. </author> <year> (1990). </year> <title> The Role of Coherence in Constructing and Evaluating Abductive Explanations. </title> <booktitle> In Working Notes of the 1990 Spring Symposium on Automated Abduction, </booktitle> <volume> volume TR 90-32, </volume> <pages> pages 13-17. </pages>
Reference-contexts: we can express the details of a wide range of KBS tasks in this abductive framework; e.g. intelligent decision support systems (x4.6 & [Menzies, 1995a]), diagrammatic reasoning [Menzies & Compton, 1994], single-user knowledge acquisition, and multiple-expert knowledge acquisition [Menzies, 1995c], certain interesting features of human cognition [Menzies, 1995d]. natural-language processing <ref> [Ng & Mooney, 1990] </ref>, design [Poole, 1990a], visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994], case-based reasoning [Leake, 1993], expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, verification and validation (x4.2 & [Menzies, 1996a,
Reference: [O'Rourke, 1990] <author> O'Rourke, P. </author> <year> (1990). </year> <title> Working Notes of the 1990 Spring Symposium on Automated Abduction. </title> <type> Technical Report 90-32, </type> <institution> University of California, </institution> <address> Irvine, CA. </address> <month> September 27, </month> <year> 1990. </year>
Reference: [Palmer & Craw, 1995] <author> Palmer, G. & Craw, S. </author> <year> (1995). </year> <title> Utilising Explanation to Assist the Refinement of Knowledge-Based Systems. </title> <booktitle> In EUROVAV-95: The Third European Symposium on Validation and Verification of Knowledge-Based Systems, </booktitle> <pages> pages 201-211. </pages>
Reference-contexts: This can be done using automatic tools (e.g. explanation-based generalisation [van Harmelen & Bundy, 1988]) or semi-automatic tools where the user's opinions are used as part of the theory refinement loop. Heuristic KB refinement (e.g. KRUST <ref> [Craw et al., 1994, Palmer & Craw, 1995] </ref> and expert critiquing systems (x4.4)) are a kind of "machine learning" algorithm in which domain-specific principles are used to fault a KB and assist a human in fixing the faults.
Reference: [Patil et al., 1981] <author> Patil, R., Szolovitis, P., & Schwartz, W. </author> <year> (1981). </year> <title> Causal Understanding of Patient Illness in Medical Diagnosis. </title> <booktitle> In IJCAI '81, </booktitle> <pages> pages 893-899. </pages>
Reference-contexts: For example, the PIERS system at St. Vincent's Hospital, Sydney, models 20% of human biochemistry sufficiently well to make diagnoses that are 99% accurate [Preston et al., 1993]. RDR has succeeded in domains where previous attempts, based on much higher-level constructs, never made it out of the prototype stage <ref> [Patil et al., 1981] </ref>. Further, while large expert systems are notoriously hard to maintain [de Brug et al., 1986], the no-model approach of RDR has never encountered maintenance problems.
Reference: [Phillips, 1984] <author> Phillips, L. </author> <year> (1984). </year> <title> A Theory of Requisite Decision Models. </title> <journal> Acta Psychologica, </journal> <volume> 56 </volume> <pages> 29-48. </pages>
Reference-contexts: On some sense every action is automatically an inductive, adjusted process [Clancey, 1987, p238]. Researchers into decision support tools make a case something like weak SC. They argue that human "knowledge" appears in some social context and that context can effect the generated "knowledge". Phillips <ref> [Phillips, 1984] </ref> and Bradshaw et. al. [Bradshaw et al., 1991] characterise model construction as a communal process that generates symbolic descriptions that explicate a community's understand of a problem. If the community changes then the explicit record of the communities shared understanding also changes; i.e. "truth" is socially constructed.
Reference: [Poole, 1990a] <author> Poole, D. </author> <year> (1990a). </year> <title> Hypo-Deductive Reasoning for Abduction, Default Reasoning, and Design. </title> <editor> In O'Rourke, P., (Ed.), </editor> <booktitle> Working Notes of the 1990 Spring Symposium on Automated Abduction., </booktitle> <volume> volume TR 90-32, </volume> <pages> pages 106-110. </pages>
Reference-contexts: of a wide range of KBS tasks in this abductive framework; e.g. intelligent decision support systems (x4.6 & [Menzies, 1995a]), diagrammatic reasoning [Menzies & Compton, 1994], single-user knowledge acquisition, and multiple-expert knowledge acquisition [Menzies, 1995c], certain interesting features of human cognition [Menzies, 1995d]. natural-language processing [Ng & Mooney, 1990], design <ref> [Poole, 1990a] </ref>, visual pattern recognition [Poole, 1990c], analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994], case-based reasoning [Leake, 1993], expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, verification and validation (x4.2 & [Menzies, 1996a, Menzies, 1996b]).
Reference: [Poole, 1990b] <author> Poole, D. </author> <year> (1990b). </year> <title> A Methodology for Using a Default and Abductive Reasoning System. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 5 </volume> <pages> 521-548. </pages>
Reference: [Poole, 1990c] <author> Poole, D. </author> <year> (1990c). </year> <title> A Methodology for Using a Default and Abductive Reasoning System. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 5 </volume> <pages> 521-548. </pages>
Reference-contexts: KBS tasks in this abductive framework; e.g. intelligent decision support systems (x4.6 & [Menzies, 1995a]), diagrammatic reasoning [Menzies & Compton, 1994], single-user knowledge acquisition, and multiple-expert knowledge acquisition [Menzies, 1995c], certain interesting features of human cognition [Menzies, 1995d]. natural-language processing [Ng & Mooney, 1990], design [Poole, 1990a], visual pattern recognition <ref> [Poole, 1990c] </ref>, analogical reasoning [Falkenhainer, 1990], financial reasoning [Hamscher, 1990], machine learning [Hirata, 1994], case-based reasoning [Leake, 1993], expert critiquing systems (x4.4), prediction, classification, explanation, tutoring, qualitative reasoning, planning, monitoring, set-covering diagnosis, consistency-based diagnosis, verification and validation (x4.2 & [Menzies, 1996a, Menzies, 1996b]).
Reference: [Popper, 1963] <author> Popper, K. </author> <year> (1963). </year> <title> Conjectures and Refutations,. </title> <publisher> Routledge and Kegan Paul. </publisher>
Reference-contexts: Many twentieth century thinkers have therefore adopted a relativist knowledge position. Kuhn notes that data is not interpreted neutrally, but (in the usual case) processed in terms of some dominant intellectual paradigm [Kuhn, 1962]. Popper <ref> [Popper, 1963] </ref> argues that, ultimately, we cannot prove the "truth" of anything since "proofs" must terminate on premises. If we request proofs of premises, then we potentially recurse forever. Hence, on purely pragmatic grounds, people are forced into an acceptance of certain premises.
Reference: [Preece, 1992] <author> Preece, A. </author> <year> (1992). </year> <title> Principles and Practice in Verifying Rule-based Systems. </title> <journal> The Knowledge Engineering Review, </journal> <volume> 7 </volume> <pages> 115-141. </pages>
Reference-contexts: That is, testing is an essential, on-going process through-out the lifetime of a knowledge base. Preece and Zlatereva describe test programs based on the logical structure of rule-based expert systems. Preece's verification tools detect anomalies in those structures <ref> [Preece, 1992] </ref> while Zlatereva's validation tools analyse that structure to generate a test suite which will exercise all parts of the rule-base [Zlatereva & Preece, 1994].
Reference: [Preece & Shinghal, 1992] <author> Preece, A. & Shinghal, R. </author> <year> (1992). </year> <title> Verifying Knowledge Bases by Anomaly Detection: An Experience Report. </title> <booktitle> In ECAI '92. </booktitle>
Reference-contexts: The experience with expert systems is that the process of building consensus between individuals or creating an explicit record of it in a knowledge base introduces biases/errors. Silverman cautions that systematic biases in expert preferences may result in incorrect/incomplete knowledge bases (x4.4). Preece & Shinghal <ref> [Preece & Shinghal, 1992] </ref> document five fielded expert systems that contain numerous logical anomalies (see Figure 1). These expert systems still work, apparently because in the context of their day-today use, the anomalous logic is never exercised. <p> Verification tools search for syntactic anomalies within a knowledge base such as tautologies, redundancies, and circularities in the dependency graph of literals in a knowledge base <ref> [Preece & Shinghal, 1992] </ref>. Many of Preece's verification tools can be mapped into a graph-theoretic analysis of the dependency graph D of literals in a KB used in HT4 (e.g. Figure 7.A). For example, a test for "unreachable conclusions" can be converted into the following graph-theoretic process.
Reference: [Preston et al., 1993] <author> Preston, P., Edwards, G., & Compton, P. </author> <year> (1993). </year> <title> A 1600 Rule Expert System Without Knowledge Engineers. </title> <editor> In Leibowitz, J., (Ed.), </editor> <booktitle> Second World Congress on Expert Systems. </booktitle>
Reference-contexts: Even if we can approach "the truth", it seems it may take years to do so. Garvan ES-1 was decommissioned before enough data could be collected to test if the growth curve was linear or logarithmic. Compton is monitoring the maintenance of PIERS <ref> [Preston et al., 1993] </ref>, a much larger system (which is version 2 of the above diagnosis system ). A y = x 0:5 growth in KB size has been noted in that system. Significantly, the user-group sponsoring the project have created a permanent line item in their budget for maintenance. <p> Yet this low-level model-less approach has produced large working expert systems in routine daily use. For example, the PIERS system at St. Vincent's Hospital, Sydney, models 20% of human biochemistry sufficiently well to make diagnoses that are 99% accurate <ref> [Preston et al., 1993] </ref>. RDR has succeeded in domains where previous attempts, based on much higher-level constructs, never made it out of the prototype stage [Patil et al., 1981].
Reference: [Quinlan, 1986] <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: Catlett's research [Catlett, 1991] explored the following area. Given a large amount of training data, is it necessary to use it all? That is, after a certain number of examples, is further experience (i.e. training data) superfluous? To test this, Catlett used C4.5 <ref> [Quinlan, 1986] </ref> to generate 20 decision trees for eleven machine learning problems using either (i) all the training cases or (ii) half the cases (randomly selected). Each generated tree was assessed using the test cases.
Reference: [Rosenbloom et al., 1993] <author> Rosenbloom, P., Laird, J., & Newell, A. </author> <year> (1993). </year> <title> The SOAR Papers. </title> <publisher> The MIT Press. </publisher>
Reference: [Runkel, 1995] <author> Runkel, J. </author> <year> (1995). </year> <title> Analyzing Tasks to Build Reusable Model-Based Tools. </title> <booktitle> In Proceedings of the 9th AAAI--Sponsored Banff Knowledge Acquisition for Knowledge-Based Systems Workshop Banff, </booktitle> <address> Canada. </address>
Reference-contexts: Runkel reports large amounts of verbatim reuse using a toolkit of problem solving strategies that separated search control from other domain knowledge <ref> [Runkel, 1995] </ref>. Marques et. al. report significantly reduced development times for expert systems using the 13 mechanisms in the SBF toolkit (eliminate, schedule, present, monitor, transform-01, transform-02, compare-01, compare-02, translate-01, translate-02, classify, select, dialog-mgr).
Reference: [Searle, 1980] <author> Searle, J. </author> <year> (1980). </year> <title> Minds, Brain, and Programs. </title> <journal> The Behavioral and Brain Sciences, </journal> <volume> 3 </volume> <pages> 417-457. </pages>
Reference-contexts: Searle takes a similar stand, claiming that the only device that can replicate human intelligence is another human <ref> [Searle, 1980, Searle, 1982, Searle, 1995] </ref> since only humans can share the same context. Birnbaum stresses "the role of a concrete case in reasoning" [Birnbaum, 1991, p58] and how logical AI cannot correctly handle such specifics, particularly when we have a specific conflicting belief.
Reference: [Searle, 1982] <author> Searle, J. </author> <year> (1982). </year> <title> The Myth of the Computer. </title> <publisher> The New York Review of Books, </publisher> <pages> pages 3-6. </pages> <month> April 29. </month>
Reference-contexts: Searle takes a similar stand, claiming that the only device that can replicate human intelligence is another human <ref> [Searle, 1980, Searle, 1982, Searle, 1995] </ref> since only humans can share the same context. Birnbaum stresses "the role of a concrete case in reasoning" [Birnbaum, 1991, p58] and how logical AI cannot correctly handle such specifics, particularly when we have a specific conflicting belief.
Reference: [Searle, 1995] <author> Searle, J. </author> <year> (1995). </year> <title> 'The Mystery of Consciousness': An Exchange. </title> <publisher> The New York Review of Books, </publisher> <pages> pages 83-84. </pages>
Reference-contexts: Searle takes a similar stand, claiming that the only device that can replicate human intelligence is another human <ref> [Searle, 1980, Searle, 1982, Searle, 1995] </ref> since only humans can share the same context. Birnbaum stresses "the role of a concrete case in reasoning" [Birnbaum, 1991, p58] and how logical AI cannot correctly handle such specifics, particularly when we have a specific conflicting belief.
Reference: [Selman & Levesque, 1990] <author> Selman, B. & Levesque, H. </author> <year> (1990). </year> <title> Abductive and Default Reasoning: a Computational Core. </title> <booktitle> In AAAI '90, </booktitle> <pages> pages 343-348. </pages>
Reference: [Shaw, 1988] <author> Shaw, M. </author> <year> (1988). </year> <title> Validation in a Knowledge Acquisition System with Multiple Experts. </title> <booktitle> In Proceedings of the International Conference on Fifth Generation Computer Systems, </booktitle> <pages> pages 1259-1266. </pages>
Reference-contexts: 5 0 4 5 Missing rule errors 0 16 0 17 0 Circularities in reasoning errors 0 0 0 20 0 Shaw reports an experiment where a group of geological experts built models for the same domain, then reviewed each other's KBs as well as their own twelve weeks later <ref> [Shaw, 1988] </ref>. Note the two context changes: from expert to expert and also a change of twelve weeks. For the twelve week self-review study, it was found that an expert's understandability and agreement with their own knowledge was less than total (see Figure 2.A).
Reference: [Shaw & Gaines, 1992] <author> Shaw, M. & Gaines, B. </author> <year> (1992). </year> <title> Repgrid-net: Combining Conceptual Modeling with Electronic Mail to Provide Decision Support. </title> <booktitle> In 7th Banff Knowledge Acquisition for Knowledge-Based Systems Workshop, </booktitle> <address> Banff, Canada, pages 24.1-24.18. </address>
Reference-contexts: The two extreme ends of these dimensions are recorded left and right of a grid. New items from the domain are categorised along these dimensions. This may elicit new dimensions of comparisons from the expert which will cause the grid to grow (see <ref> [Shaw & Gaines, 1992] </ref> for a sample of such grids). Once the dimensions stabilize, and a representative sample of items from the domain have been categorised, then the major distinctions and terminology of a domain has been defined.
Reference: [Silverman, 1990] <author> Silverman, B. </author> <year> (1990). </year> <title> Critiquing Human Judgmet Using Knowledge-Acquisition Systems. </title> <journal> AI Magazine, </journal> <pages> pages 60-79. </pages>
Reference-contexts: A model-level conflict detection facility such as abductive validation requires knowledge of how terms are combined. 4 The connection of HT4 to DeKleer's ATMS system [DeKleer, 1986] is explored elsewhere [Menzies, 1996a] 4.4 Response: Expert Critiquing Systems Silverman <ref> [Silverman, 1990, Silverman, 1992] </ref> advises that attached to an expert system should be an expert critiquing system which he defines as: : : : programs that first cause their user to maximise the falsifiability of their statements and then proceed to check to see if errors exist.
Reference: [Silverman, 1992] <author> Silverman, B. </author> <year> (1992). </year> <title> Survey of Expert Critiquing Systems: Practical and Theoretical Frontiers. </title> <journal> Communications of the ACM, </journal> <volume> 35 </volume> <pages> 106-127. </pages>
Reference-contexts: A model-level conflict detection facility such as abductive validation requires knowledge of how terms are combined. 4 The connection of HT4 to DeKleer's ATMS system [DeKleer, 1986] is explored elsewhere [Menzies, 1996a] 4.4 Response: Expert Critiquing Systems Silverman <ref> [Silverman, 1990, Silverman, 1992] </ref> advises that attached to an expert system should be an expert critiquing system which he defines as: : : : programs that first cause their user to maximise the falsifiability of their statements and then proceed to check to see if errors exist. <p> A good critic program doubts and traps its user into revealing his or her errors. It then attempts to help the user make the necessary repairs <ref> [Silverman, 1992] </ref>. Silverman divides an expert critiquing system into (i) a deep model which can generate behaviour; (ii) a differential analyser which compares the generated behaviour with the expected behaviour; and (iii) a dialogue generator that explains the errors and assists in correcting them. Dialogue generators are very domain-specific.
Reference: [Simon, 1960] <author> Simon, H. </author> <year> (1960). </year> <title> The New Science of Management Decision. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: Modern decision-support systems (DSS) aim to filter useless information to deliver relevant information (a subset of all information) to the manager. Simon originally characterised decision making as a three stage process: intelligence (scanning environment), design (develop alternative courses), and choice (selection of alternative) <ref> [Simon, 1960] </ref>. Our preferred definition of a decision-support system is based on Brookes [Brookes, 1986]' who developed it from Simon's and Mintzberg's model [Mintzberg, 1975]. The goal of a DSS is management comfort, i.e. a subjective impression that all problems are known and under control.
Reference: [Smythe, 1989] <author> Smythe, G. </author> <year> (1989). </year> <note> Brain-hypothalmus, Pituitary and the Endocrine Pancreas. The Endocrine Pancreas. </note>
Reference-contexts: Note that this procedure corresponds to answering the following question: "how much of the known behaviour of X can be reproduced by out model of X?". This abductive validation was applied to Smythe '89, a model of glucose regulation published in an international, refereed journal <ref> [Smythe, 1989] </ref>. Using an earlier version of HT4 (which they called QMOD and we call HT1) Feldman & Compton reported that only 69% of the known observations could be explained by Smythe '89n [Feldman et al., 1989].
Reference: [Stark, 1993] <author> Stark, M. </author> <year> (1993). </year> <title> Impacts of Object-Oriented Technologies: Seven Years of Software Engineering. </title> <journal> J. Systems Software, </journal> <volume> 23 </volume> <pages> 163-169. </pages>
Reference-contexts: Current KA practice has not acknowledged SC since, if it did, there would be more work in knowledge maintenance. 2 Elsewhere [Menzies & Haynes, 1994], certain inflated claims of reuse <ref> [Stark, 1993] </ref> from the object-oriented community have been criticised. 4.1.2 Evidence for SC Not Conclusive It could be argued that the evidence for weak SC is not convincing.
Reference: [Steels, 1990] <author> Steels, L. </author> <year> (1990). </year> <title> Components of Expertise. </title> <journal> AI Magazine, </journal> <volume> 11 </volume> <pages> 29-49. </pages>
Reference-contexts: There is a difference between PSCM (hereafter, KL A ) and KL B , a KL-modeling variant which groups together a set of authors who argue for basically the same technique; i.e. Clancey's model construction operators [Clancey, 1992], Steels' components of expertise <ref> [Steels, 1990] </ref>, Chandrasekaran's task analysis, SPARK/ BURN/ FIREFIGHTER (SBF) [Marques et al., 1992] and KADS [Wielinga et al., 1992]. The fundamental premise of KL B is that a knowledge base should be divided into domain-specific facts and domain-independent PSMs.
Reference: [Steels, 1994] <author> Steels, L. </author> <year> (1994). </year> <title> How Can we Make Further Progress in Knowledge Acquisition? In Mizoguchi, </title> <editor> R., Motoda, H., Boose, J., Gaines, B., & Compton, P., (Eds.), </editor> <booktitle> Proceedings of the Third Japanese Knowledge Acquisition for Knowledge-Based Systems Workshop, </booktitle> <volume> JKAW '94, </volume> <pages> pages 65-71. </pages>
Reference-contexts: Despite careful attempts to generalise principles of knowledge acquisition, (e.g. [Stefik et al., 1982]), expert systems construction remained a somewhat hit-and-miss process. By the end of the 1980s, it was recognised that our design concepts for knowledge-based systems were incomplete [Buchanan & Smith, 1989]. For example, Steels <ref> [Steels, 1994] </ref> cites an example where an expert could not solve a problem over the phone but, as soon as they walked into the room where the trouble was, could solve it instantly.
Reference: [Stefik et al., 1982] <author> Stefik, M., Aikins, J., Balzer, R., Benoit, J., Birnhaum, L., Hayes-Roth, F., & Sacerdoti, E. </author> <year> (1982). </year> <title> The Organisation of Expert Systems, A Tutorial. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 135-127. </pages>
Reference-contexts: Subsequently, a empirical experiment for evaluating SC will be defined (x5.2) descriptions. Despite careful attempts to generalise principles of knowledge acquisition, (e.g. <ref> [Stefik et al., 1982] </ref>), expert systems construction remained a somewhat hit-and-miss process. By the end of the 1980s, it was recognised that our design concepts for knowledge-based systems were incomplete [Buchanan & Smith, 1989].
Reference: [Suchman, 1987] <author> Suchman, L. </author> <year> (1987). </year> <title> Book Review of Winograd & Flores, Understanding Computers and Cognition: A New Foundation for Design. </title> <journal> Artificial Intelligence, </journal> <volume> 31 </volume> <pages> 227-232. </pages>
Reference-contexts: Such an explicit expression of current beliefs may prompt further investigation and model revision; i.e. writing down models of "truths" can cause "truth" to change. Decision support tools are discussed later (x4.6). Suchman <ref> [Suchman, 1987, Suchman, 1993, Agre, 1990] </ref> argues that real-world planning systems have to model their environment as well as their own goals.
Reference: [Suchman, 1993] <author> Suchman, L. </author> <year> (1993). </year> <title> Response to Vera and Simon's Situated Action: A Symbolic Interpretation. </title> <journal> Cognitive Science, </journal> <volume> 17 </volume> <pages> 71-75. </pages>
Reference-contexts: Such an explicit expression of current beliefs may prompt further investigation and model revision; i.e. writing down models of "truths" can cause "truth" to change. Decision support tools are discussed later (x4.6). Suchman <ref> [Suchman, 1987, Suchman, 1993, Agre, 1990] </ref> argues that real-world planning systems have to model their environment as well as their own goals.
Reference: [Tansley & Hayball, 1993] <author> Tansley, D. & Hayball, C. </author> <year> (1993). </year> <title> Knowledge-Based Systems Analysis and Design. </title> <publisher> Prentice-Hall. </publisher> <editor> [van Harmelen & Bundy, 1988] van Harmelen, F. & Bundy, A. </editor> <year> (1988). </year> <title> Explanation-Based Generalisation = Partial Evaluation. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> pages 401-412. </pages>
Reference-contexts: Model % disorders identified % knowledge fragments identified 1 (Epistemological) 50 28 2 (KADS) 55 34 3 (no model) 75 41 * The problem solving method proposed by Bredeweg [Bredeweg, 1992] for prediction via qualitative reasoning is different to the qualitative prediction problem solving method proposed by Tansley & Hayball <ref> [Tansley & Hayball, 1993] </ref>. * The KADS problem solving methods for diagnosis [Wielinga et al., 1992] is very different to the assumption-space exploration model proposed by the model-based diagnosis community [Hamscher et al., 1992]. Note that the KADS diagnosis model continues to change at a rapid rate (e.g. [Benjamins, 1995]).
Reference: [Vera & Simon, 1993a] <author> Vera, A. & Simon, H. </author> <year> (1993a). </year> <title> Situated Action: A Response to Reviewers. </title> <journal> Cognitive Science, </journal> <volume> 17 </volume> <pages> 77-86. </pages>
Reference-contexts: They describe as "preposterous" <ref> [Vera & Simon, 1993a, p95] </ref> a claim by Agre that "nobody has described a system capable of intelligent action at all- and that nothing of the sort is going to happen soon" [Agre, 1993, p69].
Reference: [Vera & Simon, 1993b] <author> Vera, A. & Simon, H. </author> <year> (1993b). </year> <title> Situated Action: A Symbolic Interpretation. </title> <journal> Cognitive Science, </journal> <volume> 17 </volume> <pages> 7-48. </pages>
Reference: [Vera & Simon, 1993c] <author> Vera, A. & Simon, H. </author> <year> (1993c). </year> <title> Situated Action: Reply to William Clancey. </title> <journal> Cognitive Science, </journal> <volume> 17 </volume> <pages> 117-133. </pages>
Reference: [Weiss et al., 1978] <author> Weiss, S., Kulikowski, C., & Amarel, S. </author> <year> (1978). </year> <title> A Model-Based Method for Computer-Aided Medical Decision-Making. </title> <journal> Artificial Intelligence, </journal> <volume> 11. </volume>
Reference-contexts: Further, if the situation is as bad as suggested above, then how is it that we have so many seemingly successful expert systems (e.g. MYCIN [Yu et al., 1979], CASNET <ref> [Weiss et al., 1978] </ref>, PROSPECTOR [Campbell et al., 1982, Duda et al., 1985], XCON [Bachant & McDermott, 1984], VT [Marcus et al., 1987], PIGE [Menzies et al., 1992])? This kind of argument is the basis of Vera & Simon's criticisms of SC [Vera & Simon, 1993b, Vera & Simon, 1993a, Vera
Reference: [Wielinga et al., 1992] <author> Wielinga, B., Schreiber, A., & Breuker, J. </author> <year> (1992). </year> <title> KADS: a Modeling Approach to Knowledge Engineering. </title> <journal> Knowledge Acquisition, </journal> <volume> 4 </volume> <pages> 1-162. </pages>
Reference-contexts: Available from http:// www.cse.unsw.edu.au/ ~timm/pub/ docs/papersonly.html 2 Brief Notes on Knowledge Modeling This section is a brief review of knowledge modeling. For more information, see the Related Work section of <ref> [Wielinga et al., 1992] </ref> and [Menzies, 1995b]. See also the ontology literature (e.g. [Gruber, 1993]) which assumes that declarative descriptions of portions of old expert systems are useful for building new applications. <p> Clancey's model construction operators [Clancey, 1992], Steels' components of expertise [Steels, 1990], Chandrasekaran's task analysis, SPARK/ BURN/ FIREFIGHTER (SBF) [Marques et al., 1992] and KADS <ref> [Wielinga et al., 1992] </ref>. The fundamental premise of KL B is that a knowledge base should be divided into domain-specific facts and domain-independent PSMs. <p> There is evidence for this elsewhere. For example, between the various camps of KL B researchers, there is little agreement on the internal details. Contrast the list of "reusable" problem solving methods from KADS <ref> [Wielinga et al., 1992] </ref> and SBF [Marques et al., 1992] (termed "knowledge sources' and "mechanism" respectively). While there is some overlap, the lists are different. Also, the number and nature of the problem solving methods is not fixed. <p> 2 (KADS) 55 34 3 (no model) 75 41 * The problem solving method proposed by Bredeweg [Bredeweg, 1992] for prediction via qualitative reasoning is different to the qualitative prediction problem solving method proposed by Tansley & Hayball [Tansley & Hayball, 1993]. * The KADS problem solving methods for diagnosis <ref> [Wielinga et al., 1992] </ref> is very different to the assumption-space exploration model proposed by the model-based diagnosis community [Hamscher et al., 1992]. Note that the KADS diagnosis model continues to change at a rapid rate (e.g. [Benjamins, 1995]). Knowledge developed in one context may not be usefully reusable in another. <p> By some measures, KL B is a successful paradigm. For example, Wielinga et. al. report that, as of 1992, KADS has been used in some 40-to 50 KBS projects, 17 of which are described in published papers <ref> [Wielinga et al., 1992] </ref>. Further, if the situation is as bad as suggested above, then how is it that we have so many seemingly successful expert systems (e.g. <p> Let us characterise two opposing KA processes: 1. An analysis intensive process (AIP) where most of the knowledge base is found via an analysis that precedes building the executing system; e.g. KADS <ref> [Wielinga et al., 1992] </ref>. In an AIP system built in the HT4 framework, most of the edges of the theory and all of the PSM would be added before the system has processed IN put, OU T put pairs. 2.
Reference: [Winograd & Flores, 1987] <author> Winograd, T. & Flores, F. </author> <year> (1987). </year> <title> On Understanding Computers and Cognition: A New Foundation for Design: A respose to the reviews. </title> <journal> Artificial Intelligence, </journal> <volume> 31 </volume> <pages> 250-261. </pages>
Reference-contexts: Clancey [Clancey, 1987, Clancey, 1991] and Winograd & Flores <ref> [Winograd & Flores, 1987] </ref> argue that it is a mistake to confuse the symbolic descriptions which humans use to co-ordinate their activities and reflect about their actions (i.e. language) with how humans might generate their minute-to-minute behaviour.
Reference: [Yost & Newell, 1989] <author> Yost, G. & Newell, A. </author> <year> (1989). </year> <title> A Problem Space Approach to Expert System Specification. </title> <booktitle> In IJCAI '89, </booktitle> <pages> pages 621-627. </pages>
Reference-contexts: Newell's subsequent exploration of the KL lead to a general rule-based language called SOAR [Rosen-bloom et al., 1993] which was the basis for the problem-space computational model (PSCM) <ref> [Yost & Newell, 1989] </ref>. Programming SOAR using the PSCM involves the consideration of multiple, nested problem spaces. Whenever a "don't know what to do" state is reached, a new problem space is forked to solve that problem. <p> PSMs are only implicit in KL A . The observation that a PSCM system is performing (e.g.) classification is a user-interpretation of a lower-level inference (operator selection over a problem space traversal) <ref> [Yost & Newell, 1989] </ref>. In KL B , PSMs specify the data structures required for each method. In KL B , once a PSM is initially specified, it is assumed to be set in stone for the life of the project.
Reference: [Yu et al., 1979] <author> Yu, V., Fagan, L., Wraith, S., Clancey, W., Scott, A., Hanigan, J., Blum, R., Buchanan, B., & Cohen, S. </author> <year> (1979). </year> <title> Antimicrobial Selection by a Computer: a Blinded Evaluation by Infectious Disease Experts. </title> <journal> Journal of American Medical Association, </journal> <volume> 242 </volume> <pages> 1279-1282. </pages>
Reference-contexts: Further, if the situation is as bad as suggested above, then how is it that we have so many seemingly successful expert systems (e.g. MYCIN <ref> [Yu et al., 1979] </ref>, CASNET [Weiss et al., 1978], PROSPECTOR [Campbell et al., 1982, Duda et al., 1985], XCON [Bachant & McDermott, 1984], VT [Marcus et al., 1987], PIGE [Menzies et al., 1992])? This kind of argument is the basis of Vera & Simon's criticisms of SC [Vera & Simon, 1993b,
Reference: [Zlatereva, 1992] <author> Zlatereva, N. </author> <year> (1992). </year> <title> Truth Mainteance Systems and Their Application for Verifying Expert System Knowledge Bases. </title> <journal> Artificial Intelligence Review, </journal> <volume> 6. </volume>
Reference-contexts: If such a test suite of behaviour is missing, then non-monotonic reasoning techniques can be used to explore the dependency graph between KB literals to find sets of input literals which will exercise the entire knowledge <ref> [Ginsberg, 1990, Zlatereva, 1992] </ref>. However, an expert still has to decide what output is appropriate for each generated input. This can introduce a circularity in the testing procedure.
Reference: [Zlatereva & Preece, 1994] <author> Zlatereva, N. & Preece, A. </author> <year> (1994). </year> <title> State of the Art in Automated Validation of Knowledge-Based Systems. </title> <journal> Expert Systems with Applications, </journal> <volume> 7 </volume> <pages> 151-167. </pages> <note> Some of the Menzies papers can be found at http:// www.cse.unsw.edu.au/ ~timm/pub/ docs/papersonly.html. </note>
Reference-contexts: Preece and Zlatereva describe test programs based on the logical structure of rule-based expert systems. Preece's verification tools detect anomalies in those structures [Preece, 1992] while Zlatereva's validation tools analyse that structure to generate a test suite which will exercise all parts of the rule-base <ref> [Zlatereva & Preece, 1994] </ref>. Verification tools search for syntactic anomalies within a knowledge base such as tautologies, redundancies, and circularities in the dependency graph of literals in a knowledge base [Preece & Shinghal, 1992].
References-found: 119

