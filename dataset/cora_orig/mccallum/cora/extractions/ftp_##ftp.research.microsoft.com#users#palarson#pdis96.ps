URL: ftp://ftp.research.microsoft.com/users/palarson/pdis96.ps
Refering-URL: http://www.research.microsoft.com/~palarson/
Root-URL: http://www.research.microsoft.com
Title: Building Regression Cost Models for Multidatabase Systems  
Author: Qiang Zhu Per -Ake Larson 
Keyword: multidatabase system, global query optimization, cost model, cost estimation, multiple regression  
Address: Dearborn, MI 48128 Waterloo, Canada N2L 3G1  
Affiliation: Department of Comp. and Inf. Science Department of Computer Science University Michigan Dearborn University of Waterloo  
Abstract: A major challenge for performing global query optimization in a multidatabase system (MDBS) is the lack of cost models for local database systems at the global level. In this paper we present a statistical procedure based on multiple regression analysis for building cost models for local database systems in an MDBS. Explanatory variables that can be included in a regression model are identified and a mixed forward and backward method for selecting significant explanatory variables is presented. Measures for developing useful regression cost models, such as removing outliers, eliminating multicollinearity, validating regression model assumptions, and checking significance of regression models, are discussed. Experimental results demonstrate that the presented statistical procedure can develop useful local cost models in an MDBS. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. F. Arnold. </author> <title> The Theory of Linear Models and Multivariate Analysis. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1981. </year>
Reference-contexts: However, there is some evidence that non-normality usually does not distort the conclusions too seriously [12] . In general, the F -test under the normality assump tion is asymptotically (i.e., with sufficiently large samples) valid when the error terms are not normally distributed <ref> [1] </ref> . Therefore, F -test is adopted in our application to test the significance of a regression model although the error terms may not follow the normality assumption.
Reference: [2] <author> S. Chatterjee and B. Price. </author> <title> Regression Analysis by Example, 2nd Ed. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: However, evaluating all possible models may not be practically feasible when the number of variables is large. To reduce the amount of computation, two types of selection procedures have been proposed <ref> [2] </ref> : the forward selection procedure and the backward elimination procedure. The forward selection procedure starts with a model containing no variables, i.e., only a constant term, and introduces explanatory variables into the regression model one at a time. <p> If H A is concluded for the absolute residuals and fitted values, the assumption of equal variances is violated. If the assumption of equal variances is violated, the estimates given by the corresponding regression model will not have the maximum precision <ref> [2] </ref> . Since the estimation precision requirement is not high for query optimization, the violation of this assumption can be tolerated to a certain degree. However, if the assumption of equal variances is severely violated, account should be taken of this in fitting the model.
Reference: [3] <author> W. Du, R. Krishnamurthy, and M. C. Shan. </author> <title> Query optimization in heterogeneous DBMS. </title> <booktitle> In Proc. of VLDB, </booktitle> <pages> pp 277-91, </pages> <year> 1992. </year>
Reference-contexts: To perform global query optimization, methods to derive approximate cost models for an autonomous local DBS are required. This issue has attracted a number of researchers recently. In <ref> [3] </ref>, Du et al. proposed a calibration method to deduce necessary local cost parameters. The idea is to construct a special local synthetic calibrating database and then run a set of special queries against this database.
Reference: [4] <author> M. Jarke and J. Koch. </author> <title> Query optimization in database systems. </title> <journal> Computing Surveys, </journal> <volume> 16(2) </volume> <pages> 111-152, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Local implementation information, such as index tree structures and index clustering ratio, is not available. The basic model captures the major performance behavior of queries in a query class. In fact, the basic model is based on some existing cost models <ref> [4; 10] </ref> for a DBMS.
Reference: [5] <author> R. J. Lipton and J. F. Naughton. </author> <title> Practical selectivity estimation through adaptive sampling. </title> <booktitle> In Proc. of SIGMOD, </booktitle> <pages> pp 1-11, </pages> <year> 1990. </year>
Reference-contexts: Although a number of sampling techniques have been applied to query optimization in the literature <ref> [5; 8; 11] </ref> , all of them perform data sampling (i.e., sampling data from databases) instead of query sampling (i.e., sampling queries from a query class). The query sampling method overcomes several shortcomings of Du et al.'s calibration method [14] .
Reference: [6] <author> H. Lu, B.-C. Ooi, and C.-H. Goh. </author> <title> On global mul-tidatabase query optimization. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 21(4) </volume> <pages> 6-11, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: In [15, 16], Zhu and Larson proposed a fuzzy optimization method to solve the problem. The idea is to build a fuzzy cost model based on experts' knowledge, experience and guesses about local DBSs and perform query optimization based on the fuzzy cost model. In <ref> [6, 13] </ref>, Lu and Zhu discussed issues for employing dynamic (adaptive) query optimization techniques based on information available at run time in an MDBS. The idea of the query sampling method that we proposed in [14] is as follows.
Reference: [7] <author> J. Neter, W. Wasserman, and M. H. Kutner. </author> <title> Applied Linear Statistical Models, </title> <editor> 3rd Ed. Richard D. Irwin, </editor> <publisher> Inc., </publisher> <year> 1990. </year>
Reference-contexts: In a (standardized) residual plot, an outlier is usu ally four or more standard deviations from zero <ref> [7] </ref> . Therefore, an observation whose residual exceeds a certain amount of standard deviations D, such as D = 4, can be considered as an outlier and be removed. <p> Since the uncorrelation assumption is rarely violated in our application, it is not checked by our regression analysis program. For the normality assumption, many studies have shown that regression analysis is robust to it <ref> [7; 9] </ref> ; that is, the technique will give usable results even if this assumption is not satisfied. In fact, the normality assumption is not required to obtain the point estimates of b B i 's, b Y and s. <p> Assuming that a regression model is proper to fit sample observations, the sampled residuals should reflect the assumptions on the error terms. We can, therefore, use the sampled residuals to check the as-sumptions. There are two ways in which the sampled residuals can be used to check the assumptions <ref> [7; 9] </ref> : residual plots and statistical tests. The former is subjective, while the latter is objective. Since we try to develop a program to test assumption 2 automatically, we employ the latter. <p> A good regression model is evidenced by a small standard er ror of estimation and a high coefficient of multiple determination. The significance of the developed model can be further tested by using the F -test <ref> [7; 9] </ref> . The F -test was derived under the normality assumption. However, there is some evidence that non-normality usually does not distort the conclusions too seriously [12] .
Reference: [8] <author> F. Olken and D. Rotem. </author> <title> Simple random sampling from relational databases. </title> <booktitle> In Proc. of 12th VLDB, </booktitle> <pages> pp 160-9, </pages> <year> 1986. </year>
Reference-contexts: Although a number of sampling techniques have been applied to query optimization in the literature <ref> [5; 8; 11] </ref> , all of them perform data sampling (i.e., sampling data from databases) instead of query sampling (i.e., sampling queries from a query class). The query sampling method overcomes several shortcomings of Du et al.'s calibration method [14] .
Reference: [9] <author> R. C. Pfaffenberger and J. H. Patterson. </author> <title> Statistical Methods for Business and Economics. </title> <editor> Richard D. Irwin, </editor> <publisher> Inc., </publisher> <year> 1987. </year>
Reference-contexts: Since the uncorrelation assumption is rarely violated in our application, it is not checked by our regression analysis program. For the normality assumption, many studies have shown that regression analysis is robust to it <ref> [7; 9] </ref> ; that is, the technique will give usable results even if this assumption is not satisfied. In fact, the normality assumption is not required to obtain the point estimates of b B i 's, b Y and s. <p> Assuming that a regression model is proper to fit sample observations, the sampled residuals should reflect the assumptions on the error terms. We can, therefore, use the sampled residuals to check the as-sumptions. There are two ways in which the sampled residuals can be used to check the assumptions <ref> [7; 9] </ref> : residual plots and statistical tests. The former is subjective, while the latter is objective. Since we try to develop a program to test assumption 2 automatically, we employ the latter. <p> In this case, the absolute values of the residuals usually have a significant correlation with the fitted values of the response variable. A simple test for the correlation between two random variables u and w when the bivariate distribution is unknown is to use Spearman's rank correlation coefficient <ref> [9; 12] </ref> , which is defined as r s = 1 6 i=1 where r (u i ) and r (w i ) are the ranks of the values u i and w i of u and w, respectively. <p> The decision rule at the significance level ff is: If 1ff=2 r s ff=2 , conclude H 0 . If r s &lt; 1ff=2 or r s &gt; ff=2 , conclude H A . The critical values ff=2 = 1ff=2 can be found in <ref> [9] </ref>. If H A is concluded for the absolute residuals and fitted values, the assumption of equal variances is violated. If the assumption of equal variances is violated, the estimates given by the corresponding regression model will not have the maximum precision [2] . <p> A good regression model is evidenced by a small standard er ror of estimation and a high coefficient of multiple determination. The significance of the developed model can be further tested by using the F -test <ref> [7; 9] </ref> . The F -test was derived under the normality assumption. However, there is some evidence that non-normality usually does not distort the conclusions too seriously [12] .
Reference: [10] <author> P. G. Selinger et al. </author> <title> Access path selection in relational database management systems. </title> <booktitle> In Proc. of ACM SIGMOD, </booktitle> <pages> pp 23-34, </pages> <year> 1979. </year>
Reference-contexts: Local implementation information, such as index tree structures and index clustering ratio, is not available. The basic model captures the major performance behavior of queries in a query class. In fact, the basic model is based on some existing cost models <ref> [4; 10] </ref> for a DBMS.
Reference: [11] <author> G. P. Shapiro and C. Connel. </author> <title> Accurate estimation of the number of tuples satisfying a condition. </title> <booktitle> In Proc. of SIGMOD, </booktitle> <pages> pp 256-76, </pages> <year> 1984. </year>
Reference-contexts: Although a number of sampling techniques have been applied to query optimization in the literature <ref> [5; 8; 11] </ref> , all of them perform data sampling (i.e., sampling data from databases) instead of query sampling (i.e., sampling queries from a query class). The query sampling method overcomes several shortcomings of Du et al.'s calibration method [14] .
Reference: [12] <author> G. W. Snedecor and W. G. Cochran. </author> <title> Statistical Methods, 6th Ed. </title> <institution> The Iowa State university Press, </institution> <year> 1967. </year>
Reference-contexts: In this case, the absolute values of the residuals usually have a significant correlation with the fitted values of the response variable. A simple test for the correlation between two random variables u and w when the bivariate distribution is unknown is to use Spearman's rank correlation coefficient <ref> [9; 12] </ref> , which is defined as r s = 1 6 i=1 where r (u i ) and r (w i ) are the ranks of the values u i and w i of u and w, respectively. <p> The significance of the developed model can be further tested by using the F -test [7; 9] . The F -test was derived under the normality assumption. However, there is some evidence that non-normality usually does not distort the conclusions too seriously <ref> [12] </ref> . In general, the F -test under the normality assump tion is asymptotically (i.e., with sufficiently large samples) valid when the error terms are not normally distributed [1] .
Reference: [13] <author> Qiang Zhu. </author> <title> Query optimization in multidatabase systems. </title> <booktitle> In Proc. of the 1992 IBM CAS Conf., vol.II, </booktitle> <pages> pp 111-27, </pages> <address> Toronto, Canada, </address> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: In [15, 16], Zhu and Larson proposed a fuzzy optimization method to solve the problem. The idea is to build a fuzzy cost model based on experts' knowledge, experience and guesses about local DBSs and perform query optimization based on the fuzzy cost model. In <ref> [6, 13] </ref>, Lu and Zhu discussed issues for employing dynamic (adaptive) query optimization techniques based on information available at run time in an MDBS. The idea of the query sampling method that we proposed in [14] is as follows.
Reference: [14] <author> Qiang Zhu and P. A. Larson. </author> <title> A query sampling method for estimating local cost parameters in a multidatabase system. </title> <booktitle> In Proc. of the 10th IEEE Int'l Conf. on Data Eng., </booktitle> <pages> pp 144-53, </pages> <address> Houston, Texas, </address> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: The idea is to construct a special local synthetic calibrating database and then run a set of special queries against this database. Cost metrics for the queries are used to deduce the coefficients in the cost formulas for the access methods supported by the underlying local database system. In <ref> [14] </ref>, Zhu and Larson presented a query sampling method to tackle this issue. The idea of this method will be reviewed below. In [15, 16], Zhu and Larson proposed a fuzzy optimization method to solve the problem. <p> In [6, 13], Lu and Zhu discussed issues for employing dynamic (adaptive) query optimization techniques based on information available at run time in an MDBS. The idea of the query sampling method that we proposed in <ref> [14] </ref> is as follows. The first step is to group all possible queries for a local database 1 into more homogeneous classes so that the costs of queries in each class can be estimated by the same formula. <p> The query sampling method overcomes several shortcomings of Du et al.'s calibration method <ref> [14] </ref> . However, the statistical procedure for deriving cost estimation formulas in [14] was oversimplified. In this paper, an improved statistical procedure is presented. The formulas are automatically determined based on observed sampling costs. More explanatory variables in a formula are considered. <p> The query sampling method overcomes several shortcomings of Du et al.'s calibration method <ref> [14] </ref> . However, the statistical procedure for deriving cost estimation formulas in [14] was oversimplified. In this paper, an improved statistical procedure is presented. The formulas are automatically determined based on observed sampling costs. More explanatory variables in a formula are considered. A series of measures for ensuring useful formulas are adopted. The rest of this paper is organized as follows.
Reference: [15] <author> Qiang Zhu and P. A. Larson. </author> <title> Establishing a fuzzy cost model for query optimization in a multidata-base system. </title> <booktitle> In Proc. of the 27th IEEE/ACM Hawaii Int'l Conf. on Sys. Sci., </booktitle> <pages> pp 263-72, </pages> <address> Maui, Hawaii, </address> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: In [14], Zhu and Larson presented a query sampling method to tackle this issue. The idea of this method will be reviewed below. In <ref> [15, 16] </ref>, Zhu and Larson proposed a fuzzy optimization method to solve the problem. The idea is to build a fuzzy cost model based on experts' knowledge, experience and guesses about local DBSs and perform query optimization based on the fuzzy cost model.
Reference: [16] <author> Qiang Zhu and P. A. Larson. </author> <title> Query optimization using fuzzy set theory for a multidatabase system. </title> <booktitle> In Proc. of the 1993 IBM CAS Conf., </booktitle> <pages> pp 848-59, </pages> <address> Toronto, Canada, </address> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: In [14], Zhu and Larson presented a query sampling method to tackle this issue. The idea of this method will be reviewed below. In <ref> [15, 16] </ref>, Zhu and Larson proposed a fuzzy optimization method to solve the problem. The idea is to build a fuzzy cost model based on experts' knowledge, experience and guesses about local DBSs and perform query optimization based on the fuzzy cost model.
References-found: 16

