URL: http://polaris.cs.uiuc.edu/reports/1385.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: An Efficient Algorithm for the Run-Time Parallelization of Doacross Loops 1  
Author: Ding-Kai Chen David A. Oesterreich Josep Torrellas, and Pen-Chung Yew 
Keyword: Fortran loop parallelization, run-time parallelization, shared-memory multiprocessors, inspector-executor, application characterization, experimental analysis.  
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development and Computer Science Department University of Illinois at Urbana-Champaign,  
Abstract: While automatic parallelization of loops is generally based on compile-time analysis of data dependences, sometimes the data dependences can not be determined at compile time. This often occurs, for example, when arrays are accessed via subscripted subscripts. In these cases, it is necessary to use run-time parallelization algorithms. In this paper, we present and evaluate a new run-time parallelization algorithm based on an inspector-executor pair. The scheme, called CYT, handles all types of data dependences in the loop without requiring any special architectural support. Furthermore, compared to an older scheme as general as the new algorithm, the latter speeds up execution by reducing the amount of interprocessor communication required. The new algorithm is applied to the Perfect Club codes and to parameterized loops, all running on the 32-CPU Cedar shared-memory multiprocessor. Although most loops with subscripted subscripts in the Perfect Club codes are highly parallel, their small size prevents good speedups. However, the larger, parameterized loops deliver speedups of up to 14 if the inspector is not reused and up to 27 if it is. Furthermore, the new algorithm consistently outperforms the older scheme, sometimes by as much as 37 times. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Loop-level parallelism is an important source of speedup in multiprocessors that exploit medium-and course-grain parallelism. To parallelize a loop, it is necessary to identify the data dependence relations between loop iterations via dependence analysis <ref> [1] </ref>. If the loop does not have any dependences across iterations, then it can run in parallel. Otherwise, we have to run it serially or, if we want to exploit some parallelism, insert proper synchronization to ensure that the memory accesses are performed in the correct sequence.
Reference: [2] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: However, the algorithm needs to be extended to parallelize the many types of more complicated loop organizations found in real codes. In this section, we describe these extensions. To determine what extensions to the algorithm are necessary, we carefully examined the Perfect Club applications <ref> [2, 4] </ref>. We isolated all loops that contain assignments to arrays that are accessed in the same loop via subscripted subscripts. These are the loops that need our algorithm to be parallelized. The results of our measurements are shown in Tables 2 and 3.
Reference: [3] <author> D.-K. Chen and P.-C. Yew. </author> <title> A Scheme for Effective Execution of Irregular DOACROSS Loops. </title> <booktitle> In Proceedings of the 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 285-292, </pages> <month> August </month> <year> 1992. </year> <note> Also available as CSRD tech report No. 1192. </note>
Reference-contexts: Some doacross loops can be transformed into fully parallel loops. For example, if the difference in iteration number between dependent iterations (called dependence distance) is known at compile time, we can use techniques like loop-alignment, loop skewing, or dependence uniformization <ref> [3, 9, 15, 17] </ref>. Then, the loop can be run in parallel. Unfortunately, many times it is not possible to determine the dependence distance at compile time. <p> Table 1: Sample dependence types and parallelization methods. Dependence Example Method to Parallelize the Loop Type f (i) g (i) Uniform Distance 2i+3 2i+1 Loop Alignment [9], Loop Skewing [17] (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization <ref> [3, 15] </ref> (if loop level&gt; 1) Unknown Distance B (i) C (i) Run-Time Parallelization Without loss of generality, we use the loop in Figure 1-(b) as our main example, where we assume that the values of arrays B and C are not available until run-time.
Reference: [4] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1992. </year> <month> 31 </month>
Reference-contexts: However, the algorithm needs to be extended to parallelize the many types of more complicated loop organizations found in real codes. In this section, we describe these extensions. To determine what extensions to the algorithm are necessary, we carefully examined the Perfect Club applications <ref> [2, 4] </ref>. We isolated all loops that contain assignments to arrays that are accessed in the same loop via subscripted subscripts. These are the loops that need our algorithm to be parallelized. The results of our measurements are shown in Tables 2 and 3. <p> We use synthetic loops and parallelized versions of the Perfect Club codes, all running on a scalable shared-memory multiprocessor. In the following, we describe the workloads and the machine used. 5.1 Workloads For our first set of experiments, we use parallel versions of the Perfect Club applications <ref> [4] </ref>. The versions studied run with 32 processors and were parallelized using a parallelizing compiler and then by hand. Of course, the loops where arrays are accessed through subscripted subscripts could not be parallelized. A second set of experiments use the synthetic loop shown in Figure 16. <p> These parameters are shown in Table 6. The first three columns list the application, routine, and number <ref> [4] </ref> of the loop. Column 4 shows what fraction of the execution time of the parallel application is accounted for by the loop. Recall that the applications have been parallelized [4]. <p> These parameters are shown in Table 6. The first three columns list the application, routine, and number <ref> [4] </ref> of the loop. Column 4 shows what fraction of the execution time of the parallel application is accounted for by the loop. Recall that the applications have been parallelized [4]. Consequently, since the loops in Table 6 could not be parallelized by the compiler or by hand, they account for a significant fraction of the execution time in some of the parallel applications.
Reference: [5] <author> V. Krothapalli and P. Sadayappan. </author> <title> An Approach to Synchronization of Parallel Computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 573-581, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Much previous research 1 A shorter version of this paper was presented at Supercomputing '94. 2 Currently with Silicon Graphics Inc., Mountain View, California. 3 Currently with Advanced Micro Devices, Austin, Texas. 4 Currently with University of Minnesota, Minneapolis, Minnesota. 1 has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18] </ref>. The main differences among the schemes proposed are the types of dependence patterns that are handled and the required system or architectural support. The key to success in these schemes is to minimize the time spent on the analysis of dependences and on synchronizing processes. <p> To do so, the scheme requires more than one key field per array element. This scheme, however, still requires as much communication as Zhu-Yew's. Other, slightly less general schemes have been proposed <ref> [5, 10] </ref>. They restrict the problem in three ways. First, no two elements of a given array used as index (for example B in Figure 1-(b)) can have the same values. This makes the scheduling of the parallel iterations simpler.
Reference: [6] <author> D. Kuck et al. </author> <title> The Cedar System and an Initial Performance Study. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 213-224, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In our experiments, the size of the array A is equal to the iteration count N . Finally, to compute speedups, we use as a reference the sequential version of the corresponding loop. 5.2 The Cedar Multiprocessor The applications and parameterized loops are executed on Cedar <ref> [6] </ref>. Cedar is a 32-processor scalable shared-memory multiprocessor designed at the Center for Supercomputing Research and Development of the University of Illinois. The machine has four clusters of eight processors each. Each cluster is a bus-based Alliant FX/8 with 32 Mbytes of memory shared by the processors in the cluster. <p> Synchronization Overhead There are two types of synchronization operations in these algorithms, namely compare&stores and barriers (Figure 6). The compare&stores can be implemented with simple lock and unlock primitives. In Cedar, however, we implement them with the Cedar sync synchronization primitive <ref> [6] </ref>. In the following, we compare the number of compare&store and barrier operations in both algorithms. base loop and r = 1 or 4 references per iteration. The experiments use 8 and 32 processors and run the mostly-serial loop (Figure 18-(a)) and the mostly-parallel loop (Figure 18-(b)).
Reference: [7] <author> S.-T. Leung and J. Zahorjan. </author> <title> Improving the Performance of Runtime Parallelization. </title> <booktitle> In 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Much previous research 1 A shorter version of this paper was presented at Supercomputing '94. 2 Currently with Silicon Graphics Inc., Mountain View, California. 3 Currently with Advanced Micro Devices, Austin, Texas. 4 Currently with University of Minnesota, Minneapolis, Minnesota. 1 has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18] </ref>. The main differences among the schemes proposed are the types of dependence patterns that are handled and the required system or architectural support. The key to success in these schemes is to minimize the time spent on the analysis of dependences and on synchronizing processes. <p> Finally, if the first restriction is not true, then they require dynamic allocation of shared-memory. Unfortunately, this can be slow and complicated. Schemes for loops without output dependences have been proposed by Saltz et al [14] and Leung and Zahorjan <ref> [7] </ref>. This assumption simplifies the problem a bit. The emphasis of the work of Saltz et al is on scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition.
Reference: [8] <author> S. Midkiff and D. Padua. </author> <title> Compiler Algorithms for Synchronization. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12), </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Much previous research 1 A shorter version of this paper was presented at Supercomputing '94. 2 Currently with Silicon Graphics Inc., Mountain View, California. 3 Currently with Advanced Micro Devices, Austin, Texas. 4 Currently with University of Minnesota, Minneapolis, Minnesota. 1 has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18] </ref>. The main differences among the schemes proposed are the types of dependence patterns that are handled and the required system or architectural support. The key to success in these schemes is to minimize the time spent on the analysis of dependences and on synchronizing processes. <p> END IF END DOALL END REPEAT 4 is that it does not treat input dependences (read after read) differently than the other dependences. Therefore, it serializes consecutive reads to the same array element by different iterations. A second scheme, proposed by Midkiff and Padua <ref> [8] </ref>, improved Zhu-Yew's scheme by allowing concurrent reads to the same array element by several iterations. To do so, the scheme requires more than one key field per array element. This scheme, however, still requires as much communication as Zhu-Yew's. Other, slightly less general schemes have been proposed [5, 10]. <p> Obviously, the reads could be performed in parallel. However, the CYT algorithm forces the serial execution order on these reads. While it is possible to include in the CYT algorithm a solution similar to Midkiff and Padua's algorithm <ref> [8] </ref> to allow the concurrentization of the reads, it would introduce overhead. Since this optimization introduces overhead even if it is not used, more analysis is required to determine whether or not it should be included.
Reference: [9] <author> D. A. Padua. </author> <title> Multiprocessors: Discussion of Some Theoretical and Practical Problems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: Some doacross loops can be transformed into fully parallel loops. For example, if the difference in iteration number between dependent iterations (called dependence distance) is known at compile time, we can use techniques like loop-alignment, loop skewing, or dependence uniformization <ref> [3, 9, 15, 17] </ref>. Then, the loop can be run in parallel. Unfortunately, many times it is not possible to determine the dependence distance at compile time. <p> Therefore, to parallelize this loop, we need to use a run-time parallelization scheme. Table 1: Sample dependence types and parallelization methods. Dependence Example Method to Parallelize the Loop Type f (i) g (i) Uniform Distance 2i+3 2i+1 Loop Alignment <ref> [9] </ref>, Loop Skewing [17] (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization [3, 15] (if loop level&gt; 1) Unknown Distance B (i) C (i) Run-Time Parallelization Without loss of generality, we use the loop in Figure 1-(b) as our main example, where we assume that the values of arrays
Reference: [10] <author> C. Polychronopoulos. </author> <title> Advanced Loop Optimizations for Parallel Computers. </title> <booktitle> In Lecture Notes in Computer Science No. 297: Proceedings of the First International Conference on Supercomputing, </booktitle> <pages> pages 255-277, </pages> <year> 1987. </year> <editor> E. Houstis, T. Papatheodorou, and C. Polychronopoulos, Editors. </editor> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: To do so, the scheme requires more than one key field per array element. This scheme, however, still requires as much communication as Zhu-Yew's. Other, slightly less general schemes have been proposed <ref> [5, 10] </ref>. They restrict the problem in three ways. First, no two elements of a given array used as index (for example B in Figure 1-(b)) can have the same values. This makes the scheduling of the parallel iterations simpler.
Reference: [11] <author> R. Ponnusamy, J. Saltz, and A. Choudhary. </author> <title> Runtime Compilation Techniques for Data Partitioning and Communication Schedule Reuse. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 361-370, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition. Their recent work focuses on run-time compilation techniques for FORALL loops with only output dependences for reduction operations on distributed-memory computers <ref> [11] </ref>. There are similar issues, but the context is different. Another scheme, proposed by Rauchwerger and Padua [12] uses the inspector to simply detect whether or not the loop is fully parallel. If the inspector determines that the loop is not fully parallel, then the loop is executed serially.
Reference: [12] <author> L. Rauchwerger and D. Padua. </author> <title> The PRIVATIZING DOALL Test: A Run-Time Technique for DOALL Loop Identification and Array Privatization. </title> <booktitle> In Proceedings of the 1994 International Conference on Supercomputing, </booktitle> <pages> pages 33-43, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Their recent work focuses on run-time compilation techniques for FORALL loops with only output dependences for reduction operations on distributed-memory computers [11]. There are similar issues, but the context is different. Another scheme, proposed by Rauchwerger and Padua <ref> [12] </ref> uses the inspector to simply detect whether or not the loop is fully parallel. If the inspector determines that the loop is not fully parallel, then the loop is executed serially.
Reference: [13] <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Parallelization. </title> <booktitle> In Proceedings of the SIGPLAN 1995 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: If the inspector determines that the loop is not fully parallel, then the loop is executed serially. These same authors have extended the algorithm to speculatively execute the loop as if it were fully parallel <ref> [13] </ref>. If a dependence is found, the work is undone and the loop executed serially. <p> However, the CYT algorithm as it currently is has at least two shortcomings. The first one is that, as in all other inspector-executor algorithms, dependences are necessarily analyzed before execution. For cases in which the loop is fully parallel, this is unnecessary overhead compared to speculative algorithms such as <ref> [13] </ref>. A second disadvantage is that, as it is, the algorithm does not treat input dependences (Read-after-Read) differently than the other dependences. Therefore, it serializes consecutive reads to the same target array element. 4 Advanced Features of the CYT Algorithm The base algorithm described so far can parallelize simple loops.
Reference: [14] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-Time Parallelization and Scheduling of Loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-611, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Much previous research 1 A shorter version of this paper was presented at Supercomputing '94. 2 Currently with Silicon Graphics Inc., Mountain View, California. 3 Currently with Advanced Micro Devices, Austin, Texas. 4 Currently with University of Minnesota, Minneapolis, Minnesota. 1 has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18] </ref>. The main differences among the schemes proposed are the types of dependence patterns that are handled and the required system or architectural support. The key to success in these schemes is to minimize the time spent on the analysis of dependences and on synchronizing processes. <p> The values of arrays B and C are assumed to remain constant during the execution of one invocation of the loop, although they are allowed to change outside the loop. In general, run-time parallelization schemes have two stages, namely inspector and executor <ref> [14, 18] </ref>. Both are performed at runtime. The inspector determines the dependence relations among the data accesses. The executor uses this information to execute the iterations in parallel in an order that preserves the dependences. <p> Second, these schemes require a serial pre-processing loop, which can reduce speedup. Finally, if the first restriction is not true, then they require dynamic allocation of shared-memory. Unfortunately, this can be slow and complicated. Schemes for loops without output dependences have been proposed by Saltz et al <ref> [14] </ref> and Leung and Zahorjan [7]. This assumption simplifies the problem a bit. The emphasis of the work of Saltz et al is on scheduling issues. <p> A second one is that the ticket table is built with little interprocessor communication. Finally, as indicated above, iterations with dependencies may be partially overlapped, unlike in [18] and <ref> [14] </ref>. However, the CYT algorithm as it currently is has at least two shortcomings. The first one is that, as in all other inspector-executor algorithms, dependences are necessarily analyzed before execution.
Reference: [15] <author> T. H. Tzen and L. Ni. </author> <title> Dependence Uniformization: A Loop Parallelization Technique. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(5), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Some doacross loops can be transformed into fully parallel loops. For example, if the difference in iteration number between dependent iterations (called dependence distance) is known at compile time, we can use techniques like loop-alignment, loop skewing, or dependence uniformization <ref> [3, 9, 15, 17] </ref>. Then, the loop can be run in parallel. Unfortunately, many times it is not possible to determine the dependence distance at compile time. <p> Table 1: Sample dependence types and parallelization methods. Dependence Example Method to Parallelize the Loop Type f (i) g (i) Uniform Distance 2i+3 2i+1 Loop Alignment [9], Loop Skewing [17] (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization <ref> [3, 15] </ref> (if loop level&gt; 1) Unknown Distance B (i) C (i) Run-Time Parallelization Without loss of generality, we use the loop in Figure 1-(b) as our main example, where we assume that the values of arrays B and C are not available until run-time.
Reference: [16] <author> M. J. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <type> Ph.D. dissertation, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: In order to determine if there is any loop-carried dependence <ref> [16] </ref> between statements S 1 and S 2 and, if so, compute its dependence distance, we have to solve the following integer equation f (i) = g (i 0 ) and evaluate the distance (i 0 i) within the constraints of the loop limits. <p> In the other applications, however, these loops rarely appear. Overall, these loops account for 20% of the loops considered. No extension to the CYT algorithm to eliminate this problem is proposed. Accumulations Loops that perform accumulations cannot be parallelized unless some transformations are performed first <ref> [16] </ref>. Some of the loops with subscripted subscripts in our applications perform accumulations. They are classified under Accumulate in Table 2. From the table, we see that these loops account for 17% of the loops with subscripted subscripts.
Reference: [17] <author> M. J. Wolfe. </author> <title> Loop Skewing: The Wavefront Method Revisited. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 4(15) </volume> <pages> 883-892, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Some doacross loops can be transformed into fully parallel loops. For example, if the difference in iteration number between dependent iterations (called dependence distance) is known at compile time, we can use techniques like loop-alignment, loop skewing, or dependence uniformization <ref> [3, 9, 15, 17] </ref>. Then, the loop can be run in parallel. Unfortunately, many times it is not possible to determine the dependence distance at compile time. <p> Therefore, to parallelize this loop, we need to use a run-time parallelization scheme. Table 1: Sample dependence types and parallelization methods. Dependence Example Method to Parallelize the Loop Type f (i) g (i) Uniform Distance 2i+3 2i+1 Loop Alignment [9], Loop Skewing <ref> [17] </ref> (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization [3, 15] (if loop level&gt; 1) Unknown Distance B (i) C (i) Run-Time Parallelization Without loss of generality, we use the loop in Figure 1-(b) as our main example, where we assume that the values of arrays B and C
Reference: [18] <author> C. Q. Zhu and P. C. Yew. </author> <title> A Scheme to Enforce Data Dependence on Large Multiprocessor Systems. </title> <journal> In IEEE Transactions on Software Engineering, </journal> <pages> pages 726-739, </pages> <month> June </month> <year> 1987. </year> <month> 32 </month>
Reference-contexts: Much previous research 1 A shorter version of this paper was presented at Supercomputing '94. 2 Currently with Silicon Graphics Inc., Mountain View, California. 3 Currently with Advanced Micro Devices, Austin, Texas. 4 Currently with University of Minnesota, Minneapolis, Minnesota. 1 has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18] </ref>. The main differences among the schemes proposed are the types of dependence patterns that are handled and the required system or architectural support. The key to success in these schemes is to minimize the time spent on the analysis of dependences and on synchronizing processes. <p> In this paper, we describe and evaluate a new algorithm for the run-time parallelization of doacross loops. Our scheme handles all types of dependence patterns without requiring any special architectural support. Furthermore, compared to an older scheme as general as ours <ref> [18] </ref>, the new algorithm speeds up execution by significantly reducing the amount of interprocessor communication required. To evaluate the algorithm, we apply it to the Perfect Club codes and parameterized loops, all running on the 32-CPU shared-memory Cedar multiprocessor. <p> The values of arrays B and C are assumed to remain constant during the execution of one invocation of the loop, although they are allowed to change outside the loop. In general, run-time parallelization schemes have two stages, namely inspector and executor <ref> [14, 18] </ref>. Both are performed at runtime. The inspector determines the dependence relations among the data accesses. The executor uses this information to execute the iterations in parallel in an order that preserves the dependences. <p> Finally, note that, if arrays B and C do not change between invocations of the loop, the inspector results can be reused across loop invocations. 2.2 Previous Work Several run-time parallelization schemes have been proposed. The first one was proposed by Zhu and Yew <ref> [18] </ref>. Their scheme is general enough to handle any dependence pattern. Given a loop like that in Figure 1-(b), the compiler transforms it into the code shown in Figure 2. <p> Unfortunately, it requires many memory accesses. Indeed, for each iteration of the repeat loop, each unexecuted iteration requires at least 3 fl r memory accesses, where r is the number of array references with subscripted subscripts per iteration <ref> [18] </ref>. A second problem of Zhu-Yew's algorithm 3 Done (1:N)= .FALSE. <p> We call the algorithm CYT from Chen, Yew, and Torrellas. 3.1 Description of the Algorithm Although Zhu-Yew's algorithm can handle all types of dependences <ref> [18] </ref>, it has two limitations. First, the inspector is not reused across invocations of the same loop, even if the dependences do not change. This is because the inspector and the executor are tightly connected. Second, the execution of iterations with dependences cannot be overlapped even partially. <p> A second one is that the ticket table is built with little interprocessor communication. Finally, as indicated above, iterations with dependencies may be partially overlapped, unlike in <ref> [18] </ref> and [14]. However, the CYT algorithm as it currently is has at least two shortcomings. The first one is that, as in all other inspector-executor algorithms, dependences are necessarily analyzed before execution.
References-found: 18

