URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/IEEESP.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/
Root-URL: http://www.cs.umn.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations. </title> <publisher> The John Hopkins University Press, </publisher> <year> 1983. </year>
Reference-contexts: Traditionally the singular value decomposition (SVD) is used to compute the low-rank least squares solution <ref> [1] </ref>. However, in real time applications, it is expensive to update the SVD. Recently, a low rank, eigensubspace RLS algorithm has been proposed in [2] using a Schur-type decomposition of the input correlation matrix. <p> exponential weighting matrix given by fl (n) = diag i j When fl = I, the LS solution can be expressed in terms of the SVD as h LS (n) = X y (n)d (n) = V (n) y (n)U T (n)d (n): (3.8) Here, () y denotes the pseudoinverse <ref> [1] </ref>. When X (n) is close to rank deficient, the least squares solution is ill conditioned due to the inversion of the small singular values in y (n).
Reference: [2] <author> P. Strobach, </author> <title> "Fast recursive eigensubspace adaptive filters," </title> <booktitle> in Proc. ICASSP, </booktitle> <address> (Detroit), </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Traditionally the singular value decomposition (SVD) is used to compute the low-rank least squares solution [1]. However, in real time applications, it is expensive to update the SVD. Recently, a low rank, eigensubspace RLS algorithm has been proposed in <ref> [2] </ref> using a Schur-type decomposition of the input correlation matrix. This algorithm requires O (rN ) flops, where r is the effective rank of the input correlation matrix. However, the algorithm requires the knowledge of this rank r. Hence, it is not suitable to applications where r varies with time. <p> Hence, it is not suitable to applications where r varies with time. Rank-revealing QR (RRQR) decompositions may also be used to solve the least-squares (LS) problem [3]. These algorithms can track the rank and therefore do not suffer from the disadvantage of <ref> [2] </ref>. The computational complexity of this approach is O (N 2 ). <p> The LS problem was also solved by using a truncated ULV decomposition to approximate the data matrix [4]. Although the computational expense, O (N 2 ), of this algorithm is greater than that of <ref> [2] </ref>, this method offers the advantage that it is able to track rank changes. Furthermore, the quality of the approximations to the singular value subspaces that it produces does not depend on the magnitude of the gap in the singular values. <p> The figures show the performance of the GULV-LMS algorithm, the plain ULV-LMS, plain LMS, traditional RLS, QRD-RLS, GULV-RLS and the Schur pseudoinverse based least squares method discussed in <ref> [2] </ref>. The learning curves show the improved performance of the GULV-LMS algorithm. It can be seen from these figures that breaking up the subspaces corresponding to the larger singular values into sub clusters using the GULV algorithm further reduces the condition numbers of these clusters. <p> This results in the improved performance of the GULV based RLS and LMS algorithms. On the other hand, the algorithm of <ref> [2] </ref> has a starting value of the rank set to 2. Therefore its behavior is similar to that of the GULV based algorithms initially.
Reference: [3] <author> T. F. Chan and P. C. Hansen, </author> <title> "Some applications of the rank revealing QR factorization," </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> vol. 13, </volume> <pages> pp. 727-741, </pages> <year> 1992. </year>
Reference-contexts: However, the algorithm requires the knowledge of this rank r. Hence, it is not suitable to applications where r varies with time. Rank-revealing QR (RRQR) decompositions may also be used to solve the least-squares (LS) problem <ref> [3] </ref>. These algorithms can track the rank and therefore do not suffer from the disadvantage of [2]. The computational complexity of this approach is O (N 2 ).
Reference: [4] <author> R. D. Fierro and J. R. Bunch, </author> <title> "Bounding the subspaces from rank revealing two-sided orthogonal decompositions." </title> <note> Preprint (to appear in SIAM Matrix Anal. Appl.). </note>
Reference-contexts: Rank-revealing QR (RRQR) decompositions may also be used to solve the least-squares (LS) problem [3]. These algorithms can track the rank and therefore do not suffer from the disadvantage of [2]. The computational complexity of this approach is O (N 2 ). However, it has been shown <ref> [4] </ref> that the quality of approximation of the singular value subspaces using RRQR (and hence the closeness of the truncated RRQR decomposition to the truncated SVD) depends on the gap between the singular values. <p> The LS problem was also solved by using a truncated ULV decomposition to approximate the data matrix <ref> [4] </ref>. Although the computational expense, O (N 2 ), of this algorithm is greater than that of [2], this method offers the advantage that it is able to track rank changes. <p> This method is easily updated when new data arrives without making any a priori assumptions about the overall distribution of the singular values. Each ULV update requires only O (N 2 ) operations. An analysis of the ULV algorithm was also performed <ref> [4, 11] </ref>. It was shown in [4] that the "noise" subspace (the subspace corresponding to the cluster of small singular values) is close to the corresponding SVD subspace. The analysis of [11] also shows that the ULV subspaces are only slightly 2 more sensitive to perturbations. <p> This method is easily updated when new data arrives without making any a priori assumptions about the overall distribution of the singular values. Each ULV update requires only O (N 2 ) operations. An analysis of the ULV algorithm was also performed [4, 11]. It was shown in <ref> [4] </ref> that the "noise" subspace (the subspace corresponding to the cluster of small singular values) is close to the corresponding SVD subspace. The analysis of [11] also shows that the ULV subspaces are only slightly 2 more sensitive to perturbations. <p> 6 4 H E 3 7 5 V r V 0 (2.5) and the SVD of A be given by A = U 1 U 2 U ? 2 6 4 0 nr 3 7 5 V 1 V 2 : (2.6) The following theorem due to Fierro and Bunch <ref> [4] </ref> shows that as the off-diagonal block H decreases, the ULV subspaces converge to their SVD counterparts. Theorem 1 (Fierro & Bunch) Let A have the ULV in (2.5) and the SVD in (2.6). Assume k k = k k 2 .
Reference: [5] <author> C. E. Davila, </author> <title> "Recursive total least squares algorithm for adaptive filtering," </title> <booktitle> in Proc. ICASSP, </booktitle> <pages> pp. 1853-1856, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Recently, some subspace updating techniques have been suggested <ref> [5] </ref>-[9]. A Kalman filter was used to update the eigenvector corresponding to the smallest eigenvalue in [5]. However it was not suggested how to modify the algorithm in case of multiple eigenvalues corresponding to noise. In [6], a fast eigen-decomposition algorithm which replaced the noise and signal eigenvalues by their corresponding average values was proposed.
Reference: [6] <author> K. B. Yu, </author> <title> "Recursive updating the eigenvalue decomposition of a covariance matrix," </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> vol. 39, </volume> <pages> pp. 1136-1145, </pages> <year> 1991. </year>
Reference-contexts: Recently, some subspace updating techniques have been suggested [5]-[9]. A Kalman filter was used to update the eigenvector corresponding to the smallest eigenvalue in [5]. However it was not suggested how to modify the algorithm in case of multiple eigenvalues corresponding to noise. In <ref> [6] </ref>, a fast eigen-decomposition algorithm which replaced the noise and signal eigenvalues by their corresponding average values was proposed. This technique could work well if the exact eigenvalues could be grouped together in two tight clusters. In [7] and [8], the averaging technique of [6] is used. <p> In <ref> [6] </ref>, a fast eigen-decomposition algorithm which replaced the noise and signal eigenvalues by their corresponding average values was proposed. This technique could work well if the exact eigenvalues could be grouped together in two tight clusters. In [7] and [8], the averaging technique of [6] is used. However, the SVD is updated instead of the eigenvalue decomposition. This reduces the condition numbers to their square roots and increases numerical accuracy. Again the assumption that the singular values could be grouped into two tight clusters is made.
Reference: [7] <author> E. M. Dowling and R. D. DeGroat, </author> <title> "Recursive total least squares adaptive filtering," </title> <booktitle> in Proceedings of the SPIE Conf. on Adaptive Signal Proc., </booktitle> <volume> vol. 1565, </volume> <pages> pp. 35-46, SPIE, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: In [6], a fast eigen-decomposition algorithm which replaced the noise and signal eigenvalues by their corresponding average values was proposed. This technique could work well if the exact eigenvalues could be grouped together in two tight clusters. In <ref> [7] </ref> and [8], the averaging technique of [6] is used. However, the SVD is updated instead of the eigenvalue decomposition. This reduces the condition numbers to their square roots and increases numerical accuracy. Again the assumption that the singular values could be grouped into two tight clusters is made.
Reference: [8] <author> R. D. DeGroat, </author> <title> "Noniterative subspace tracking," </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> vol. 40, </volume> <pages> pp. 571-577, </pages> <year> 1992. </year>
Reference-contexts: In [6], a fast eigen-decomposition algorithm which replaced the noise and signal eigenvalues by their corresponding average values was proposed. This technique could work well if the exact eigenvalues could be grouped together in two tight clusters. In [7] and <ref> [8] </ref>, the averaging technique of [6] is used. However, the SVD is updated instead of the eigenvalue decomposition. This reduces the condition numbers to their square roots and increases numerical accuracy. Again the assumption that the singular values could be grouped into two tight clusters is made.
Reference: [9] <author> D. L. Boley and K. T. Sutherland, </author> <title> "Recursive total least squares : An alternative to the discrete kalman filter," </title> <type> Tech. Rep. TR 93-92, </type> <institution> Dept. of Comp. Sci., Univ. of Minn., </institution> <year> 1993. </year>
Reference-contexts: In practice all these tolerances may not be available. Therefore, this heuristic cannot be used in the GULV algorithm that we describe next. A second heuristic has been proposed in <ref> [9] </ref>. This heuristic decides that a gap in the singular values exists if s 2 &gt; d 2 (f 2 + b 2 ) where f is the Frobenius norm of the trailing part [E; F]. The parameter b, called Zero Tolerance, is selected by the user. <p> Though it is a reasonable assumption that the rank usually does not change by more than one, the problem still exists if the rank is underestimated. Figure 2 shows the rank tracking behavior of the ULV algorithm using the heuristics of [10], <ref> [9] </ref> and that proposed in this paper. The input initially consisted of two complex sinusoids each having an amplitude of 0.1. The background noise was white with a variance of 10 12 . Therefore, there are initially two large singular values with magnitudes on the order of 0.01 each. <p> Now the larger group of singular values is f1; 0:01; 0:01g. The figure shows that after some time, the ULV algorithm using the proposed heuristic and that of [10] converged to a rank of three. However, the ULV algorithm using the heuristic of <ref> [9] </ref> converges to one. This is because, the heuristic initially under estimated the rank as two.
Reference: [10] <author> G. W. Stewart, </author> <title> "An updating algorithm for subspace tracking," </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> vol. 40, </volume> <pages> pp. 1535-1541, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Again the assumption that the singular values could be grouped into two tight clusters is made. In normal signal scenarios and in particular for the application targeted in this paper, this assumption is generally not valid. The ULV decomposition was first introduced by Stewart <ref> [10] </ref>, to break the eigenspace of the input correlation matrix R N , where N is the length of the impulse response of the adaptive filter, into two subspaces, one corresponding to the cluster of largest singular values and the other corresponding to the smaller singular values or noise subspace. <p> As mentioned earlier, the drawback of the SVD is that it requires O (N 3 ) flops. Here, we review the ULV decomposition of Stewart <ref> [10] </ref>. <p> Though it is a reasonable assumption that the rank usually does not change by more than one, the problem still exists if the rank is underestimated. Figure 2 shows the rank tracking behavior of the ULV algorithm using the heuristics of <ref> [10] </ref>, [9] and that proposed in this paper. The input initially consisted of two complex sinusoids each having an amplitude of 0.1. The background noise was white with a variance of 10 12 . Therefore, there are initially two large singular values with magnitudes on the order of 0.01 each. <p> Next a complex exponential of unit amplitude is added to the input. Now the larger group of singular values is f1; 0:01; 0:01g. The figure shows that after some time, the ULV algorithm using the proposed heuristic and that of <ref> [10] </ref> converged to a rank of three. However, the ULV algorithm using the heuristic of [9] converges to one. This is because, the heuristic initially under estimated the rank as two. <p> The tracking performance of the ULV decomposition using this heuristic is shown in Fig. 2. Note that if we replace df by the user specified tolerance in our heuristic, we obtain the heuristic of <ref> [10] </ref>. 2.3 Quality of Subspaces Consider the orthogonal projector onto a subspace S, P S .
Reference: [11] <author> R. D. Fierro, </author> <title> "Perturbation analysis for two-sided (or complete) orthogonal decompositions." </title> <type> Preprint. </type>
Reference-contexts: This method is easily updated when new data arrives without making any a priori assumptions about the overall distribution of the singular values. Each ULV update requires only O (N 2 ) operations. An analysis of the ULV algorithm was also performed <ref> [4, 11] </ref>. It was shown in [4] that the "noise" subspace (the subspace corresponding to the cluster of small singular values) is close to the corresponding SVD subspace. The analysis of [11] also shows that the ULV subspaces are only slightly 2 more sensitive to perturbations. <p> An analysis of the ULV algorithm was also performed [4, 11]. It was shown in [4] that the "noise" subspace (the subspace corresponding to the cluster of small singular values) is close to the corresponding SVD subspace. The analysis of <ref> [11] </ref> also shows that the ULV subspaces are only slightly 2 more sensitive to perturbations. These analyses show that the ULV algorithm can be used in many situations where SVD was the only available alternative to date. We use the ULV decomposition to develop a low rank recursive-least-squares (RLS) algorithm. <p> ); R (V r )) oe 2 2 * sin OE (R ( ~ U r ); R (U r )) oe min ( ~ L r ) kEk 2 * : These results indicate that the ULV subspaces are only slightly more sensitive to perturbations than the singular subspaces <ref> [11] </ref>. The above theorems provide us with bounds on the distance between ULV subspaces and the SVD subspaces. They also provide us with bounds on the angle between the subspaces obtained by performing a ULV decomposition on the actual data matrix and a perturbed data matrix.
Reference: [12] <author> S. Haykin, </author> <title> Adaptive Filter Theory. </title> <address> Engelwood Cliffs, N.J.: </address> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Even though, the dominant subspace contains strong signal components, its condition number might still be large. Now, recall that the convergence speed of the LMS algorithm depends inversely on the condition number of the input autocorrelation matrix (the ratio of its maximum to minimum eigenvalue) <ref> [12, 13] </ref>. Thus, a low rank LMS algorithm which uses the ULV decomposition would still have a poor convergence performance. We therefore develop a generalization of the ULV algorithm to track several well conditioned subspaces of the input correlation matrix. <p> response d (n) and that estimated by the adaptive filter ^ d (n) can be written as The RLS algorithm tries to recursively solve the weighted LS problem min n X ni je (i)j 2 : (3.4) By rewriting (3.4), we find that the RLS algorithm solves the following problem <ref> [12] </ref>. min kfl 1=2 (n)(X (n)h (n) d (n))k 2 : (3.5) In (3.5), d (n) = [d (1); d (2); ; d (n)] T is the desired response vector, X (n) is the input data matrix given by h i T 10 and fl (n) is the n fi n <p> In fact, it has been shown that [18] kU 1 r V T r k 1 k Thus, unlike traditional RLS algorithm, which is asymptotically unbiased <ref> [12] </ref>, the ULV-RLS algorithm has a small bias. However, the magnitude of this bias depends on the closeness of the ULV subspace, V r , to its corresponding singular subspace. This closeness, in turn, depends on the magnitude of the off diagonal matrix H (See Theorem 1). <p> As noted in the introduction, the convergence of the LMS algorithm depends on the condition number of the input autocorrelation matrix, O (R x ) <ref> [19, 12] </ref>, where E fi fl : fi fl . When all the eigenvalues of the input correlation matrix are equal, i.e., the condition number O (R x ) = 1, the algorithm converges fastest. <p> This implies that the slow converging subspace projections (usually the ones with lower signal energy) should be assigned large step sizes. Note that the average time constant o av of the learning curve <ref> [12] </ref> is o av 1 2 av , where av is the average eigenvalue of the input correlation matrix or the average input power. Therefore, the step size for coefficients in a subspace should be made inversely proportional to the average energy in that subspace. <p> This error equation is of the form derived for the LMS algorithm <ref> [12] </ref> and can be rewritten as E [ ~ *(n + 1)] = (I M (fl + E)) n+1 E [ ~ * 0 ] + (I (M (fl + E)) n )(I M (fl + E)) 1 F l h fl : (7.27) Thus, if the step-size for each cluster <p> (n) g T (n)y (n)) 2 ]: (7.33) However, as the transformation is unitary, it can be written as, E [J (n)] = E [(d (n) h T (n)x (n)) 2 ] = oe 2 The above equation can be re-written in terms of the weight error vector *(n) as <ref> [12] </ref> E [J (n)] = J min + E [* T (n)R*(n)]: (7.35) In the above equation, J min denotes the minimum MSE achieved by the optimum weight vector.
Reference: [13] <author> D. F. Marshall, W. K. Jenkins, and J. J. Murphy, </author> <title> "The use of orthogonal transforms for improving performance of adaptive filters," </title> <journal> IEEE Trans. Circuits Syst., </journal> <volume> vol. 36, </volume> <pages> pp. 474-483, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Even though, the dominant subspace contains strong signal components, its condition number might still be large. Now, recall that the convergence speed of the LMS algorithm depends inversely on the condition number of the input autocorrelation matrix (the ratio of its maximum to minimum eigenvalue) <ref> [12, 13] </ref>. Thus, a low rank LMS algorithm which uses the ULV decomposition would still have a poor convergence performance. We therefore develop a generalization of the ULV algorithm to track several well conditioned subspaces of the input correlation matrix.
Reference: [14] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations. </title> <address> Baltimore, MD: </address> <publisher> Johns Hopkins University Press, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference-contexts: The first three procedures are designed to allow easy updating of the ULV decomposition as new rows are appended. Each basic procedure costs O (N 2 ) operations and consists of a sequence of plane (Givens) rotations <ref> [14] </ref>. Pre-multiplication by a plane rotation operates on the rows of the matrix while post-multiplication operates on its columns. By using a sequence of such rotations in a very special order, we can annihilate desired entries while filling in as few zero entries as possible.
Reference: [15] <author> N. J. Higham, </author> <title> "A survey of condition number estimators for triangular matrices," </title> <journal> SIAM Rev, </journal> <pages> pp. 575-596, </pages> <year> 1987. </year>
Reference-contexts: These are computed using a condition number estimator <ref> [15] </ref>. * Deflate One: Deflate the ULV decomposition by one (i.e., apply transformation and decrement the rank index by one so that the smallest singular value in the leading r fi r part of L is "moved" to the trailing rows).
Reference: [16] <author> G. W. Stewart, </author> <title> "Updating a rank revealing ULV decomposition," </title> <type> Tech. Rep. </type> <institution> UMIACS-TR-91-39 and CS-TR 2627, Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: The choice of the heuristic is extremely important as it is used to cluster the singular values and hence obtain the correct singular subspaces. To our knowledge three heuristics (including the one used in this paper) have been proposed in literature. The heuristic proposed in <ref> [16] </ref> estimates the smallest singular value, s of C and compares it with a user specified tolerance. This tolerance, provided by the user, is usually based on some knowledge of the eigenvalue distribution. The choice of the tolerance is important. <p> Thus, when updating an inner rank boundary, the value s r i+1 of 16 the minimum singular value of the next cluster is available at no extra cost. Therefore this approach would require only O (N 2 ) flops more than the heuristic given in <ref> [16] </ref> in order to obtain the smallest singular value of L. This extra complexity can be avoided if an estimate of this singular value (e.g., an estimate of the noise power) is provided by the user. The main advantage of this heuristic over that proposed in [16] is that it avoids <p> the heuristic given in <ref> [16] </ref> in order to obtain the smallest singular value of L. This extra complexity can be avoided if an estimate of this singular value (e.g., an estimate of the noise power) is provided by the user. The main advantage of this heuristic over that proposed in [16] is that it avoids the need for the user to provide the different tolerances needed to check if a gap exists at each rank boundary. * Generalized Update: This procedure encompasses the entire process.
Reference: [17] <author> P. A. Wedin, </author> <title> "Perturbation bounds in connection with singular value decomposition," </title> <journal> BIT, </journal> <volume> vol. 12, </volume> <pages> pp. 99-111, </pages> <year> 1973. </year>
Reference-contexts: In particular, we effectively have obtained the singular subspaces for the matrix. If, furthermore, there exist ff and ffi such that oe min ( ~ L) ff + ffi and oe r+1 ff; (2.9) the above theorem reduces to the perturbation bounds for singular subspaces obtained by Wedin <ref> [17] </ref>. We therefore, obtain the perturbation bound for singular subspaces as a special case of the ULV bound.
Reference: [18] <author> R. D. Fierro and P. C. Hansen, </author> <title> "Accuracy of TSVD solutions computed from rank revealing decompositions." </title> <type> Preprint. </type>
Reference-contexts: 2 (n)U ? (n) 2 6 4 0 0 nk 3 7 5 V 1 (n)V 2 (n) = U 1 (n) r (n)V T The solution to the modified LS problem is then obtained as h MLS (n) = X y r (n)U T It has been recently suggested, <ref> [18] </ref>, that the rank r approximate of X (n), discussed above, be replaced by a matrix obtained using a truncated ULV decomposition. The main motivations behind the use of the ULV decomposition is its lower computational expense, O (N 2 ) as compared to the SVD O (N 3 ). <p> The main motivations behind the use of the ULV decomposition is its lower computational expense, O (N 2 ) as compared to the SVD O (N 3 ). Thus, the modified minimum norm solution can be computed as <ref> [18] </ref> h ULVLS (n) = V r (n)L 1 r (n)U T It was however not suggested how the algorithm is to be modified in case the data matrix X (n) grows as new data becomes available. Clearly, in such a case storing U (n) is not a viable option. <p> In fact, it has been shown that <ref> [18] </ref> kU 1 r V T r k 1 k Thus, unlike traditional RLS algorithm, which is asymptotically unbiased [12], the ULV-RLS algorithm has a small bias. However, the magnitude of this bias depends on the closeness of the ULV subspace, V r , to its corresponding singular subspace. <p> Now, the magnitude of 1 n F (n) depends on the norm of the off diagonal matrix H (see <ref> [18] </ref>, (4.33) and Theorem 1). This norm can be made arbitrarily small using extra refinements. Furthermore,the magnitude of the third term in the RHS of (4.39) is O ( 1 n ).
Reference: [19] <author> B. Widrow and S. D. Stearns, </author> <title> Adaptive Signal Processing. </title> <address> Engelwood Cliffs, N.J.: </address> <publisher> Prentice Hall, </publisher> <year> 1985. </year>
Reference-contexts: As noted in the introduction, the convergence of the LMS algorithm depends on the condition number of the input autocorrelation matrix, O (R x ) <ref> [19, 12] </ref>, where E fi fl : fi fl . When all the eigenvalues of the input correlation matrix are equal, i.e., the condition number O (R x ) = 1, the algorithm converges fastest. <p> E [ ~ *(n + 1)] = (I M (fl + E)) n+1 E [ ~ * 0 ] + (I (M (fl + E)) n )(I M (fl + E)) 1 F l h fl : (7.27) Thus, if the step-size for each cluster m k satisfies the condition <ref> [19] </ref>, [20] 0 &lt; m k &lt; max ( i k + j i k ) where j i k is the perturbation in the corresponding eigenvalue due to the perturbation matrix E, we get E [ ~ * 1 ] = (I M (fl + E)) 1 F l h <p> The proof of (7.37) is straightforward and we omit it for lack of space. 24 7.3 Discussion The condition on m k for convergence, (7.32), is similar to that obtained in <ref> [19] </ref>, [20].
Reference: [20] <author> G. Ungerboeck, </author> <title> "Theory on the speed of convergence in adaptive equalizers for digital communication," </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 16, </volume> <pages> pp. 546-555, </pages> <year> 1972. </year>
Reference-contexts: [ ~ *(n + 1)] = (I M (fl + E)) n+1 E [ ~ * 0 ] + (I (M (fl + E)) n )(I M (fl + E)) 1 F l h fl : (7.27) Thus, if the step-size for each cluster m k satisfies the condition [19], <ref> [20] </ref> 0 &lt; m k &lt; max ( i k + j i k ) where j i k is the perturbation in the corresponding eigenvalue due to the perturbation matrix E, we get E [ ~ * 1 ] = (I M (fl + E)) 1 F l h fl <p> The proof of (7.37) is straightforward and we omit it for lack of space. 24 7.3 Discussion The condition on m k for convergence, (7.32), is similar to that obtained in [19], <ref> [20] </ref>.
Reference: [21] <author> G. W. Stewart and J. Sun, </author> <title> Matrix Perturbation Theory. </title> <address> San Diego, USA: </address> <publisher> Academic Press Inc., </publisher> <year> 1990. </year>
Reference-contexts: Also as pointed out earlier, in case l = p, F l = 0. Thus, E [ ~ * 1 ] = 0. Therefore, for this case, the algorithm converges in mean to the optimum weight vector h fl . Using the inequality (Weyl's Thm., p. 203 of <ref> [21] </ref>), i (A + B) i (A) + kBk 2 ; (7.30) max ( i k + j i k ) max ( i k ) + kEk 2 : (7.31) Thus for convergence it is sufficient to choose m k as, 0 &lt; m k max ( i k )
Reference: [22] <author> S. Hosur, </author> <title> Recursive Matrix Factorization Algorithms in Adaptive Filtering and Mobile Communications. </title> <type> PhD thesis, </type> <institution> University of Minnesota, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: The misadjustment error is the excess MSE after the adaptive filter has converged, E [J ex (1)]. It is show in <ref> [22] </ref> that E [J ex (1)] is given by E [J ex (1)] 2 Tr (Mfl) where fl is the diagonal eigenvalue matrix of the input correlation matrix R x .
Reference: [23] <author> J.S.Goldstein, M.A.Ingram, E.J.Holder, and R. Smith, </author> <title> "Adaptive subspace selection using subband decompositions for sensor array processing," </title> <booktitle> in Proc. ICASSP, </booktitle> <volume> vol. IV, </volume> <pages> pp. 281-284, </pages> <month> April </month> <year> 1994. </year> <title> 29 Noise at -160 dB. Curves are averages of 20 runs. </title> <type> 30 </type>
Reference-contexts: Sec. 8). As discussed above, adapting only in the l dominant subspaces corresponds to adaptively estimating the solution to the modified Wiener equation (7.9). This implies that there is an inherent "noise cleaning" associated with such an approach. It is noted <ref> [23] </ref> that as we slowly increase the number of nonzero m k 's corresponding to the subspaces containing significant signal strengths, the MSE decreases until the desired signals are contained in these spaces. A further increase would only increase the MSE due to the inclusion of the noise eigenvalues.
References-found: 23

