URL: http://net.cs.utexas.edu/users/grib/IJCAI95.ps
Refering-URL: http://www.cs.utexas.edu/users/grib/
Root-URL: 
Email: osiris@cs.utexas.edu  grib@arlut.utexas.edu  kuipers@cs.utexas.edu  
Phone: (512) 471-9563  (512) 471-9563  (512) 471-9561 (voice), (512) 471-8885 (fax)  
Title: Real-time tracking of complex objects  
Author: Robert L. Browning William S. Gribble Benjamin Kuipers 
Address: Austin, Austin, Texas 78712 USA.  Texas 78712 USA.  Austin, Austin, Texas 78712 USA.  
Affiliation: Dept. of Computer Sciences, University of Texas at  Dept. of Electrical and Computer Engineering, University of Texas at Austin, Austin,  Dept. of Computer Sciences, University of Texas at  
Date: January 23, 1995  
Abstract: This paper presents a strategy for real-time visual identification and tracking of complex objects which does not rely on specialized image-processing hardware. In this system perceptual schemas represent objects as a graph of primitive features. Distributed software agents identify and track these features, using variable-geometry image subwindows of limited size. Active control of imaging parameters and selective processing makes simultaneous real-time tracking of many primitive features tractable. Perceptual schemas operate independently from the tracking of primitive features, so that real-time tracking of a set of image features is not hurt by latency in recognition of the object that those features make up. The architecture allows semantically significant features to be tracked with limited expenditure of computational resources, and allows the visual computation to be distributed across a network of processors. Early experiments are described which demonstrate the usefulness of this formulation. Keywords: Active vision, vision architectures, object recognition. This paper has not already been accepted by and is not currently under review for a journal or another conference. Nor will it be submitted for such during IJCAI's review period. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ronald C. Arkin and Douglas MacKenzie. </author> <title> Temporal coordination of per ceptual algorithms for mobile robot navigation. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 10(3) </volume> <pages> 276-286, </pages> <year> 1994. </year>
Reference-contexts: In the following two sections, we briefly describe each of these levels and then discuss some of the implications for scalable distributed processing. 2.1 Perceptual Schemas A perceptual schema can be regarded as a virtual sensor tuned to respond to a particular landmark, spatial property, or event of interest <ref> [1] </ref>. A robot may have many different perceptual schemas, any number of which could be active at a given time, depending on the robot's state and current goals.
Reference: [2] <author> Kevin J. Bradshaw, Phillip F. McLauchlan, Ian D. Reid, and David W Murray. </author> <title> Saccade and pursuit on an active head/eye platform. </title> <journal> Image and Vision Computing, </journal> <volume> 12(3) </volume> <pages> 155-163, </pages> <year> 1994. </year> <month> 13 </month>
Reference-contexts: A number of such chips supported by a network of several Mb/sec (comparable to workstation network interfaces) is capable of tracking primitive features numbering into the hundreds. Distributed and parallel processing are not new ideas in the computer vision community <ref> [11, 8, 7, 2] </ref>. However, most efforts in multiprocessor vision have taken an approach which parallelizes a solution to a particular visual problem 6 rather than exploiting the natural parallelism of visual tasks.
Reference: [3] <author> Rafael C. Gonzalez and Richard E. Woods. </author> <title> Digital Image Processing. </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Since only a tiny image buffer is processed and computationally inexpensive methods are used, vertical line tracking computations are very fast on typical computing hardware. Processing steps are: Edge detection. The Sobel edge operator <ref> [3] </ref> is applied to get an estimate of the image intensity gradient. The Sobel operator was chosen for its computational simplicity and somewhat improved noise performance relative to simpler methods. 9 10 Vertical segment detection. A degenerate Hough transform [3] is applied to the gradient estimate image. <p> Processing steps are: Edge detection. The Sobel edge operator <ref> [3] </ref> is applied to get an estimate of the image intensity gradient. The Sobel operator was chosen for its computational simplicity and somewhat improved noise performance relative to simpler methods. 9 10 Vertical segment detection. A degenerate Hough transform [3] is applied to the gradient estimate image. The accumulator array is parameterized by horizontal and vertical position of short vertical line segments. Gradient direction and strength information are used to determine whether or not a particular pixel votes for a vertical line segment at each location.
Reference: [4] <author> B. J. Kuipers. </author> <title> A frame for frames: representing knowledge for recognition. </title> <editor> In D. G. Bobrow and A. Collins, editors, </editor> <booktitle> Representation and Understanding, </booktitle> <pages> pages 151-184. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Confidence in a proper match increases as more primitive features and their spatial relationships are matched with the model. The process of incrementally matching the model against features of the image stream can be thought of as a model-directed exploration of the image stream <ref> [4] </ref>. For example, consider a perceptual schema tuned to detect doors in an office-like environment.
Reference: [5] <author> B. J. Kuipers and Tod Levitt. </author> <title> Navigation and mapping in large scale space. </title> <journal> AI Magazine, </journal> <volume> 9(2) </volume> <pages> 25-43, </pages> <year> 1988. </year> <booktitle> Reprinted in Advances in Spatial Reasoning, </booktitle> <volume> Volume 2, </volume> <editor> Su-shing Chen (Ed.), </editor> <address> Norwood NJ: </address> <publisher> Ablex Publishing, </publisher> <year> 1990. </year>
Reference-contexts: Location of traversable spaces. As a complement to obstacle detection, the robot must be able to find spaces that it can navigate without obstruction. Place recognition. The Spatial Semantic Hierarchy (SSH) framework for robot spatial reasoning <ref> [5] </ref> characterizes places by a measure of "distinc-tiveness." A robot vision system should notice when its current neighborhood is a distinctive space such as an intersection of hallways, a concave or convex wall corner, etc. <p> Such a model could be represented by a simple relational graph, or by a map of distinctive points and interconnections following the Spatial Semantic Hierarchy <ref> [5] </ref>. An initial hypothesis of "doorness" for a region of image space can be made upon successful location of a single model feature, 4 such as a vertical line. The model of a door predicts that there will be an intersecting line at the top of a vertical line.
Reference: [6] <author> Wan Yik Lee. </author> <title> Abstract mapping senses and a heterogeneous control based on landmark vector sense. </title> <type> unpublished paper, </type> <month> August </month> <year> 1992. </year>
Reference-contexts: The tracking of landmarks fixed in world space allows the robot to infer its own motion and provides useful feedback to motion control laws <ref> [6] </ref>. Obstacle detection. A mobile robot must be able to detect objects that might impede its progress. Also, active (possibly hostile) agents such 2 as moving graduate students and floor polishers should be detected and avoided. Location of traversable spaces.
Reference: [7] <author> M. Mirmehdi and T. J. Ellis. </author> <title> Parallel approach to tracking edge segments in dynamic scenes. </title> <journal> Image and Vision Computing, </journal> <volume> 11(1) </volume> <pages> 35-47, </pages> <year> 1993. </year>
Reference-contexts: A number of such chips supported by a network of several Mb/sec (comparable to workstation network interfaces) is capable of tracking primitive features numbering into the hundreds. Distributed and parallel processing are not new ideas in the computer vision community <ref> [11, 8, 7, 2] </ref>. However, most efforts in multiprocessor vision have taken an approach which parallelizes a solution to a particular visual problem 6 rather than exploiting the natural parallelism of visual tasks.
Reference: [8] <author> Nikolaos P. Papanikolopoulos and Pradeep K. Khosla. </author> <title> Adaptive robotic visual tracking: Theory and experiments. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> 38(3) </volume> <pages> 429-445, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: A number of such chips supported by a network of several Mb/sec (comparable to workstation network interfaces) is capable of tracking primitive features numbering into the hundreds. Distributed and parallel processing are not new ideas in the computer vision community <ref> [11, 8, 7, 2] </ref>. However, most efforts in multiprocessor vision have taken an approach which parallelizes a solution to a particular visual problem 6 rather than exploiting the natural parallelism of visual tasks.
Reference: [9] <author> D. Pierce and B. Kuipers. </author> <title> Learning to explore and build maps. </title> <booktitle> In Proc. 12th National Conf. on Artificial Intelligence (AAAI-94). </booktitle> <publisher> AAAI/MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: It has been shown that characterization of such places allows the robot to build useful maps of its environment <ref> [9] </ref>. Identification of regularities. By exploring the relation between its actions and its sensory input, the robot should be able to identify domain-specific regularities that provide useful perceptual features [9]. For example, vertical edges are important features in many environments, while indoor office environments also include many horizontal edges. <p> It has been shown that characterization of such places allows the robot to build useful maps of its environment <ref> [9] </ref>. Identification of regularities. By exploring the relation between its actions and its sensory input, the robot should be able to identify domain-specific regularities that provide useful perceptual features [9]. For example, vertical edges are important features in many environments, while indoor office environments also include many horizontal edges.
Reference: [10] <author> Shimon Ullman. </author> <title> Visual routines. </title> <journal> Cognition, </journal> <volume> 18 </volume> <pages> 97-159, </pages> <year> 1984. </year>
Reference-contexts: Due to its sequential nature, this process could take several image-frame times to complete, which amounts to a significant fraction of a second. Humans perform quite well in tasks requiring recognition of these types of spatial relationships, and latencies of several hundred milliseconds are not unusual or particularly problematic <ref> [10] </ref>. A human can easily identify a door even if it is moving through the perceived image. Clearly, the process of recognizing the door is somewhat independent from the real-time task of tracking its position in the image.
Reference: [11] <author> P. H. Welch and D. C. Wood. </author> <title> Image tracking in real-time: a transputer emulation of some early mammalian vision processes. </title> <journal> Image and Vision Computing, </journal> <volume> 11(4) </volume> <pages> 221-228, </pages> <year> 1993. </year> <month> 14 </month>
Reference-contexts: A number of such chips supported by a network of several Mb/sec (comparable to workstation network interfaces) is capable of tracking primitive features numbering into the hundreds. Distributed and parallel processing are not new ideas in the computer vision community <ref> [11, 8, 7, 2] </ref>. However, most efforts in multiprocessor vision have taken an approach which parallelizes a solution to a particular visual problem 6 rather than exploiting the natural parallelism of visual tasks.
References-found: 11

