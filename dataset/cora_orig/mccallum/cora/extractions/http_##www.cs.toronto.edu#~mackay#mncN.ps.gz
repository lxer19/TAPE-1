URL: http://www.cs.toronto.edu/~mackay/mncN.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mackay/README.html
Root-URL: http://www.cs.toronto.edu
Title: Good Error-Correcting Codes based on Very Sparse Matrices  
Author: David J.C. MacKay 
Keyword: Error-correction codes, iterative probabilistic decoding, Shannon limit, low-complexity decoding.  
Note: To appear in IEEE Transactions on Information Theory  Submitted Jun 9 1997; accepted subject to revisions  revised version completed and accepted for publication  
Address: Cambridge, CB3 0HE. United Kindom.  
Affiliation: Cavendish Laboratory,  
Email: mackay@mrao.cam.ac.uk  
Date: January 1999.  March 1 1998;  July 27, 1998.  
Abstract: We study two families of error-correcting codes defined in terms of very sparse matrices. `MN' (MacKay-Neal) codes are recently invented, and `Gallager codes' were first investigated in 1962, but appear to have been largely forgotten, in spite of their excellent properties. The decoding of both codes can be tackled with a practical sum-product algorithm. We prove that these codes are `very good', in that sequences of codes exist which, when optimally decoded, achieve information rates up to the Shannon limit. This result holds not only for the binary symmetric channel but also for any channel with symmetric stationary ergodic noise. We give experimental results for binary symmetric channels and Gaussian channels demonstrating that practical performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance of Gallager codes is almost as close to the Shannon limit as that of Turbo codes. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Ahlswede and G. Dueck. </author> <title> Good codes can be produced by a few permutations. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 28(3) </volume> <pages> 430-443, </pages> <year> 1982. </year>
Reference-contexts: Shannon's proof was non-constructive and employed random codes for which there is no practical encoding or decoding algorithm. Since 1948, it has been proved that there exist very good cyclic codes (non-constructively) [45], and that very good codes with a short description in terms of permutations can be produced <ref> [1] </ref>; and an explicit algebraic construction of very good codes for 1 certain channels was given in 1982 [19]. <p> M such that Ax = z mod 2 Binary vector w weight of vector x x 0 ; equivalently, the number of columns of A that might be linearly dependent Integer p 00 Probability that random walk on M -dimensional hypercube returns to starting corner on step r Real 2 <ref> [0; 1] </ref> q 00 Upper bound for p (r) 00 Real k; l; m; n indices running from 1 to K; L; M; N. Integer j index running from 1 to M=2 in the eigenvalue expansion of p 00 . <p> <ref> [0; 1] </ref> q 00 Upper bound for p (r) 00 Real k; l; m; n indices running from 1 to K; L; M; N. Integer j index running from 1 to M=2 in the eigenvalue expansion of p 00 . Integer j=M Real 2 [0; 1=2] OE w=L Real 2 [0; 1] f Density of x Real f n Noise density Real f s Source density Real r Number of steps in random walk on M dimensional hyper cube. r = wt. r=M = OEt.
Reference: 2. <author> S. V. B. Aiyer, M. Niranjan, and F. Fallside. </author> <title> A theoretical investigation into the performance of the Hopfield model. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 204-215, </pages> <year> 1990. </year>
Reference-contexts: Prior work related to this concept is found in [65]. It is possible that further benefits may be obtained by applying sum-product concepts in statistical physics or to other optimization problems where mean field methods have been found useful <ref> [30, 2] </ref>. Decoding algorithms. We conjecture that as we get closer to the Shannon limit, the decoding problem gets harder. But we don't understand what aspects of the problem determine the practical 39 limits of our present decoding algorithms.
Reference: 3. <author> R. J. Anderson. </author> <title> Searching for the optimum correlation attack. </title> <editor> In B. Preneel, editor, </editor> <booktitle> Fast Software Encryption (Proceedings of 1994 K.U. Leuven Workshop on Cryptographic Algorithms), Lecture Notes in Computer Science, </booktitle> <pages> pages 179-195. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: It seems unlikely that there is a practical rate 1=4 interleaved code that can communicate reliably under these conditions. Cryptanalysis. This work grew out of an interest in a problem in cryptanalysis <ref> [3] </ref>, the inference of the state of a linear feedback shift register given its noisy output sequence, which is also equivalent to the decoding of a cyclic code. The free energy minimization algorithm was found to be an improvement over Meier and Staffelbach's algorithm in [38].
Reference: 4. <author> S. Andreassen, M. Woldbye, B. Falck, and S. Andersen. </author> <title> MUNIN | a causal probabilistic network for the interpretation of electromyographic findings. </title> <booktitle> In Proc. of the 10th National Conf. on AI, AAAI: </booktitle> <address> Menlo Park CA., </address> <pages> pages 121-123, </pages> <year> 1987. </year>
Reference-contexts: However, it is interesting to implement the decoding algorithm that would be appropriate if there were no cycles, on the assumption that the errors introduced might be relatively small. This approach of ignoring cycles has been used in the artificial intelligence literature <ref> [4] </ref> but is now frowned upon because it produces inaccurate probabilities (D. Spiegelhalter, personal communication). However, for our problem the end-product is a decoding; the marginal probabilities are not required if the decoding is correct.
Reference: 5. <author> L. R. Bahl, J. Cocke, F. Jelinek, and J. Raviv. </author> <title> Optimal decoding of linear codes for minimizing symbol error rate. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-20:284-287, </volume> <year> 1974. </year>
Reference-contexts: The probabilities for z m having its observed value given either x l = 0 or x l = 1 can then be found efficiently by use of the forward-backward algorithm <ref> [7, 67, 5] </ref>. A particularly convenient implementation of this method uses forward and backward passes in which products of the differences ffiq ml j q 0 ml q 1 ml are computed.
Reference: 6. <author> G. Battail. </author> <title> We can think of good codes, and even decode them. </title> <editor> In P. Camion, P. Charpin, and S. Harari, editors, </editor> <booktitle> Eurocode '92. </booktitle> <address> Udine, Italy, </address> <month> 26-30 October, </month> <booktitle> number 339 in CISM Courses and Lectures, </booktitle> <pages> pages 353-368. </pages> <publisher> Springer, Wien, </publisher> <year> 1993. </year>
Reference-contexts: It seems to be widely believed that while almost any random linear code is good, codes with structure that allows practical coding are likely to be bad [45], [15]. Battail expresses an alternative view, however, that `we can think of good codes, and we can decode them' <ref> [6] </ref>. This statement is supported by the results of the present paper. In this paper we study the theoretical and practical properties of two code families.
Reference: 7. <author> L. E. Baum and T. Petrie. </author> <title> Statistical inference for probabilistic functions of finite-state Markov chains. </title> <journal> Ann. Math. Stat., </journal> <volume> 37 </volume> <pages> 1559-1563, </pages> <year> 1966. </year> <month> 52 </month>
Reference-contexts: The probabilities for z m having its observed value given either x l = 0 or x l = 1 can then be found efficiently by use of the forward-backward algorithm <ref> [7, 67, 5] </ref>. A particularly convenient implementation of this method uses forward and backward passes in which products of the differences ffiq ml j q 0 ml q 1 ml are computed.
Reference: 8. <author> F. Bein. </author> <title> Construction of telephone networks. </title> <journal> Notices Amer. Math. Soc., </journal> <volume> 36, </volume> <month> Jan </month> <year> 1989. </year>
Reference-contexts: Constructions. By introducing constructions 2A and 2B, we pushed the performance of Gallager codes a little closer to capacity. Are there further useful changes we could make to the code construction? We are currently investigating the possibility of systematic construction of matrices A whose corresponding graphs have large girth <ref> [44, 8, 35] </ref>. In this paper we have mainly considered regular low density matrices, that is, matrices in which the weight per column is constant and the weight per row is constant, or nearly constant. <p> We also evaluate a numerical bound that is useful for all r. 46 F.1 Bound that is tight for r AE M q 00 =2 = 2 M j=0 M ! 2j r 2 M j=0 j! M <ref> (89) </ref> 1 X 1 e ( 2r At equation (89) we have used the inequality (1 a) r e ar . <p> We also evaluate a numerical bound that is useful for all r. 46 F.1 Bound that is tight for r AE M q 00 =2 = 2 M j=0 M ! 2j r 2 M j=0 j! M <ref> (89) </ref> 1 X 1 e ( 2r At equation (89) we have used the inequality (1 a) r e ar .
Reference: 9. <author> E. R. Berlekamp. </author> <title> Algebraic Coding Theory. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: For BCH codes it was assumed that more than t errors cannot be corrected, as specified in the (n; k; t) description of the code. In principle, it may be possible in some cases to make a BCH decoder that corrects more than t errors, but according to Berlekamp <ref> [9] </ref>, "little is known about: : : how to go about finding the solutions" and "if there are more than t + 1 errors then the situation gets very complicated very quickly." All relevant BCH codes listed in [56] are included [block sizes up to 1023]. 43 D Decoding by free
Reference: 10. <author> E. R. Berlekamp, R. J. McEliece, and H. C. A. van Tilborg. </author> <title> On the intractability of certain coding problems. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 24(3) </volume> <pages> 384-386, </pages> <year> 1978. </year>
Reference-contexts: any of these codes, and it is known that the general linear decoding problem (find the maximum likelihood source vector s in the equation G T s+n = r mod 2, where G is a generator matrix, n is a noise vector, and r is the received vector) is NP-complete <ref> [10] </ref>. Convolutional codes (which can be viewed as block codes with memory) can approach the Shannon limit as their constraint length increases but the complexity of their best known decoding algorithms grows exponentially with the constraint length. <p> The optimal decoder returns the message s that maximizes the posterior probability P (sjr; G) = P (rjG) It is often not practical to implement the optimal decoder; indeed the general decoding problem is known to be NP-complete <ref> [10] </ref>. If the prior probability of s is assumed uniform, and the probability of n is assumed to be independent of s (c.f. definition 6), then it is convenient to introduce the (N K) fi N parity check matrix, H, which in systematic form is [PjI NK ].
Reference: 11. <author> C. Berrou and A. Glavieux. </author> <title> Near optimum error correcting coding and decoding: </title> <journal> Turbo-codes. IEEE Transactions on Communications, </journal> <volume> 44 </volume> <pages> 1261-1271, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: McEliece.) The curves labelled Turbo show the performance of the rate 1/2 Turbo code described in <ref> [12, 11] </ref> and the rate 1/4 code reported in [21]. Gallager codes: From left to right the codes had the following parameters (N,K,R). Panel (a): (65389; 32621; 0:499) (1B); (19839; 9839; 0:496) (1B); (29331; 19331; 0:659) (1B). <p> McEliece, personal communication). This system can only be decoded using expensive special purpose hardware, and the details of the code are unpublished outside JPL [61]. 7.1 Comparison with Turbo Codes We heard about Turbo codes <ref> [12, 11] </ref>, which outperform Gallager's codes in terms of E b =N 0 , towards the end of this work. There are some similarities between the codes. The Turbo decoding algorithm may be viewed as a sum-product algorithm ([69, 68, 46]).
Reference: 12. <author> C. Berrou, A. Glavieux, and P. Thitimajshima. </author> <title> Near Shannon limit error-correcting coding and decoding: </title> <booktitle> Turbo-codes. In Proc. 1993 IEEE International Conference on Communications, </booktitle> <address> Geneva, Switzerland, </address> <pages> pages 1064-1070, </pages> <year> 1993. </year>
Reference-contexts: McEliece.) The curves labelled Turbo show the performance of the rate 1/2 Turbo code described in <ref> [12, 11] </ref> and the rate 1/4 code reported in [21]. Gallager codes: From left to right the codes had the following parameters (N,K,R). Panel (a): (65389; 32621; 0:499) (1B); (19839; 9839; 0:496) (1B); (29331; 19331; 0:659) (1B). <p> McEliece, personal communication). This system can only be decoded using expensive special purpose hardware, and the details of the code are unpublished outside JPL [61]. 7.1 Comparison with Turbo Codes We heard about Turbo codes <ref> [12, 11] </ref>, which outperform Gallager's codes in terms of E b =N 0 , towards the end of this work. There are some similarities between the codes. The Turbo decoding algorithm may be viewed as a sum-product algorithm ([69, 68, 46]). <p> For the codes presented here, this is about 800 operations. This is not at all excessive when compared with textbook codes | the constraint length 7 convolutional code used by Voyager requires 256 operations per decoded bit. The Turbo codes of <ref> [12] </ref> require about 3800 operations per decoded bit (B.J. Frey, personal communication). Strictly, a constant number of iterations (taken above to be 20) is not sufficient to achieve negligible probability of error for any blocklength [26].
Reference: 13. <author> N. H. </author> <title> Bingham. Fluctuation theory for the Ehrenfest urn. </title> <booktitle> Advances in Applied Probability, </booktitle> <volume> 23 </volume> <pages> 598-611, </pages> <year> 1991. </year>
Reference-contexts: We use the result from [31] that p 00 = 2 M j=0 M ! 2j r See <ref> [13, 20, 32, 34, 62] </ref> for further information about this random walk, which is also known as the Ehrenfest Urn model. Equation (86) is an eigenvalue expansion, where the eigenvalues of the Markov process are labeled by j = 0 : : : M and have value i M .
Reference: 14. <author> V. Chepyzhov and B. Smeets. </author> <title> On a fast correlation attack on certain stream ciphers. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 547 </volume> <pages> 176-185, </pages> <year> 1991. </year>
Reference: 15. <author> J. T. Coffey and R. M. Goodman. </author> <title> Any code of which we cannot think is good. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36(6) </volume> <pages> 1453-1461, </pages> <year> 1990. </year>
Reference-contexts: It seems to be widely believed that while almost any random linear code is good, codes with structure that allows practical coding are likely to be bad [45], <ref> [15] </ref>. Battail expresses an alternative view, however, that `we can think of good codes, and we can decode them' [6]. This statement is supported by the results of the present paper. In this paper we study the theoretical and practical properties of two code families.
Reference: 16. <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: 1 log 2 P (xjN ) fi fi For example, a memoryless binary symmetric channel's noise has mean entropy H n = H 2 (f n ), where f n is the density of the noise; the proof of this statement, by the law of large numbers, is well known <ref> [16] </ref>. We will prove that the codes presented in this paper are good codes not only for the binary symmetric channel but also for a wide class of channels with memory. <p> We consider the probability of error of a typical set decoder <ref> [16] </ref>, averaging it over all very sparse random matrices A. <p> If this symmetry property is satisfied then it is evident that the decoding problem is equivalent to the decoding of a symmetric stationary ergodic binary channel (definition 6). 1 Our definition of a `symmetric' channel differs from that of Cover and Thomas <ref> [16] </ref>. For them, a channel is `symmetric' if the rows of p (yjx) are permutations of each other and the columns are permutations of each other; a channel is `weakly symmetric' if the rows of p (yjx) are permutations of each other and the column sums are equal.
Reference: 17. <author> M. C. Davey and D. J. C. MacKay. </author> <title> Low density parity check codes over GF(q). </title> <journal> IEEE Communications Letters, </journal> <month> June </month> <year> 1998. </year>
Reference-contexts: Our results show that Gallager codes over GF (4) and GF (8) perform better than comparable Gallager codes over GF (2) in the case of the binary symmetric channel and the Gaussian channel <ref> [17] </ref>. Constructions. By introducing constructions 2A and 2B, we pushed the performance of Gallager codes a little closer to capacity.
Reference: 18. <author> M. C. Davey and D. J. C. MacKay. </author> <title> Low density parity check codes over GF(q). </title> <booktitle> In Proceedings of the 1998 IEEE Information Theory Workshop. IEEE, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: Their results 38 indicate that significant enhancements in performance can be obtained. We have applied this idea to Gallager codes over GF (8) and have discovered an irregular Gallager code with block length 24,000 bits whose performance equals that of the best Turbo codes <ref> [18] </ref>. The choice of construction of Gallager code remains a productive area for further research. Bursty channels and fading channels.
Reference: 19. <author> P. Delsarte and P. Piret. </author> <title> Algebraic constructions of Shannon codes for regular channels. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 28(4) </volume> <pages> 593-599, </pages> <year> 1982. </year>
Reference-contexts: Since 1948, it has been proved that there exist very good cyclic codes (non-constructively) [45], and that very good codes with a short description in terms of permutations can be produced [1]; and an explicit algebraic construction of very good codes for 1 certain channels was given in 1982 <ref> [19] </ref>.
Reference: 20. <author> P. Diaconis, R.L. Graham, and J.A. Morrison. </author> <title> Asymptotic analysis of a random walk on a hypercube with many dimensions. Random Structures and Algorithms, </title> <booktitle> 1 </booktitle> <pages> 51-72, </pages> <year> 1990. </year>
Reference-contexts: We use the result from [31] that p 00 = 2 M j=0 M ! 2j r See <ref> [13, 20, 32, 34, 62] </ref> for further information about this random walk, which is also known as the Ehrenfest Urn model. Equation (86) is an eigenvalue expansion, where the eigenvalues of the Markov process are labeled by j = 0 : : : M and have value i M .
Reference: 21. <author> D. Divsilar and F. Pollara. </author> <title> On the design of Turbo codes. </title> <type> Technical Report TDA 42-123, </type> <institution> Jet Propulsion Laboratory, Pasadena, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: McEliece.) The curves labelled Turbo show the performance of the rate 1/2 Turbo code described in [12, 11] and the rate 1/4 code reported in <ref> [21] </ref>. Gallager codes: From left to right the codes had the following parameters (N,K,R). Panel (a): (65389; 32621; 0:499) (1B); (19839; 9839; 0:496) (1B); (29331; 19331; 0:659) (1B). <p> However, Turbo code researchers have found similar tweaks to the sum-product algorithm are helpful <ref> [21] </ref>.] The encoding and decoding software and the parity check matrices used in this paper are available from http://wol.ra.phy.cam.ac.uk/mackay/codes/. 7.3 Descriptive Complexity The descriptive complexity of these codes is much smaller than the descriptive complexity of arbitrary linear codes, which is N K bits.
Reference: 22. <author> G.-L. Feng and T. R. N. Rao. </author> <title> Decoding algebraic-geometric codes up to the designed minimum distance. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(1) </volume> <pages> 37-45, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Goppa's algebraic geometry codes, reviewed in [66], appear to be both practical and good (with practical decoding proven possible up to the Gilbert bound), but we believe that the literature has not established whether they are very good. The best practical decoding algorithm that is known for these codes <ref> [22] </ref> appears to be prohibitively costly (N 3 ) to implement, and algebraic geometry codes do not appear to be destined for practical use.
Reference: 23. <author> G. D. Forney, Jr. </author> <title> Concatenated Codes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1966. </year>
Reference-contexts: Forney proved that there do exist very good `concatenated' codes that are practical <ref> [23] </ref>; but the proof was also non-constructive [45]. When it comes to practical, constructive codes, constructions have been demonstrated of codes based on concatenation that are good, though not very good, but most known practical codes are asymptotically bad [45]. <p> In 1963, the N 2 cost in memory for explicit storage of the generator matrix would have been unattainable, so computational resources were (temporarily) a problem. R.G. Gallager (personal communication) has suggested that Gallager codes were generally forgotten because it was assumed that concatenated codes <ref> [23] </ref> were superior for practical purposes. 7.8 Future work Generalization to q-ary alphabets. Gallager and MN codes can also be defined over q-ary alphabets consisting of the elements of GF (q).
Reference: 24. <author> B. J. Frey. </author> <title> Graphical Models for Machine Learning and Digital Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA., </address> <year> 1998. </year> <note> See http://www.cs.utoronto.ca/frey. </note>
Reference-contexts: In this work, we have rediscovered a method of Gallager [27]. See <ref> [68, 24, 46] </ref> for further discussion of the sum-product algorithm. We refer to the elements z m corresponding to each row m = 1 : : : M of A as checks. <p> After each vertical step we prevented all the probabilities from going greater than 1 10 10 or less than 10 10 . [One could view the `update schedule', i.e., the order in which the quantities q and r are updated, as an adjustable aspect of the algorithm <ref> [24] </ref>; we have not explored this option. We have briefly examined two modifications of the algorithm, making the prior probabilities more extreme if a decoding has not emerged, and making the propagated probabilities more (or less) extreme, but we have not found any useful improvement in performance.
Reference: 25. <author> P. Gacs. </author> <title> Reliable computation with cellular automata. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 32(1) </volume> <pages> 15-78, </pages> <year> 1986. </year>
Reference: 26. <author> R. G. Gallager. </author> <title> Low density parity check codes. </title> <journal> IRE Trans. Info. Theory, </journal> <volume> IT-8:21-28, </volume> <month> Jan </month> <year> 1962. </year>
Reference-contexts: This statement is supported by the results of the present paper. In this paper we study the theoretical and practical properties of two code families. Gallager's low-density parity-check codes are defined in terms of a very sparse random parity check matrix <ref> [26, 27, 41] </ref>. `MN codes' are also defined in terms of very sparse random matrices, and were first presented in [40]. (MN stands for MacKay-Neal; MacKay and Neal generalized MN codes to Gallager codes, then realised that they had rediscovered Gallager's work.) MN codes are unconventional in that redundancy can be <p> In section 3 we present a `sum-product' decoding algorithm for Gallager codes and MN codes, first used by Gallager <ref> [26] </ref>. We give an analysis of the decoding algorithm in section 3.3. These results lead us to conjecture that there exist Gallager and MN codes which are not only good but which also achieve error rates approaching zero at a non-zero information rate when decoded using a practical algorithm. <p> Previous work on low density parity check codes has already established some good properties of Gallager codes. Gallager <ref> [26, 27] </ref> proved that his codes have good distance properties. Zyablov and Pinsker [73] proved that Gallager codes are good and gave a practical decoder, but only for communication rates substantially below the Gilbert bound. <p> Our approach in terms of an ideal 6 decoder allows us to prove that the codes are good not only for the binary symmetric channel but also for arbitrary ergodic symmetric channel models; we also prove that Gallager codes are very good, a result not explicitly proven in <ref> [26, 27, 73] </ref>. 2.1 Ensembles of very sparse matrices The properties that we prove depend on the ensemble of matrices A that is averaged over. We find it easiest to prove the desired properties by weakening the ensemble of matrices from that described in section 1.3. <p> Gallager codes, as Gallager showed <ref> [26] </ref> and we will show later, can in practice be decoded beyond their minimum distance. 2.3 Proof of theorem 1 Consider the problem, given A and z, of inferring x, where Ax = z mod 2, and x has probability distribution P (x) with mean entropy H x . <p> We define p z (o ) to be the probability that z m = 1. Starting from p z (0) = 0, we can use the recurrence relation: p z (o + 1) = p z (o )(1 f ) + (1 p z (o ))f (44) to obtain <ref> [26] </ref>: p z (o ) = 2 1 (1 2f ) o : (45) We use this result to obtain tighter bounds on the achievable communication rate. Bound for constrained matrices A. <p> Another way of constructing regular Gallager codes is to build the matrix A from non-overlapping random permutation matrices as shown in figure 7 (c,d). Figure 7 (c) shows the construction used by Gallager <ref> [26] </ref>. <p> The Turbo codes of [12] require about 3800 operations per decoded bit (B.J. Frey, personal communication). Strictly, a constant number of iterations (taken above to be 20) is not sufficient to achieve negligible probability of error for any blocklength <ref> [26] </ref>. <p> system, if appropriately controlled, would allow the users dynamically to change their rate of communication by changing their densities f u without changing their encoder. 7.7 Conundrum: why were Gallager codes forgotten? Why was Gallager's work mostly forgotten by the information theory community? There are very few citations of Gallager's <ref> [26, 27] </ref> work on low-density parity-check codes. A search on BIDS returns the following citations: [14, 25, 28, 63, 44, 49, 48, 54, 53, 55, 59, 64, 71, 72].
Reference: 27. <author> R. G. Gallager. </author> <title> Low Density Parity Check Codes. Number 21 in Research monograph series. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1963. </year>
Reference-contexts: This statement is supported by the results of the present paper. In this paper we study the theoretical and practical properties of two code families. Gallager's low-density parity-check codes are defined in terms of a very sparse random parity check matrix <ref> [26, 27, 41] </ref>. `MN codes' are also defined in terms of very sparse random matrices, and were first presented in [40]. (MN stands for MacKay-Neal; MacKay and Neal generalized MN codes to Gallager codes, then realised that they had rediscovered Gallager's work.) MN codes are unconventional in that redundancy can be <p> Previous work on low density parity check codes has already established some good properties of Gallager codes. Gallager <ref> [26, 27] </ref> proved that his codes have good distance properties. Zyablov and Pinsker [73] proved that Gallager codes are good and gave a practical decoder, but only for communication rates substantially below the Gilbert bound. <p> Our approach in terms of an ideal 6 decoder allows us to prove that the codes are good not only for the binary symmetric channel but also for arbitrary ergodic symmetric channel models; we also prove that Gallager codes are very good, a result not explicitly proven in <ref> [26, 27, 73] </ref>. 2.1 Ensembles of very sparse matrices The properties that we prove depend on the ensemble of matrices A that is averaged over. We find it easiest to prove the desired properties by weakening the ensemble of matrices from that described in section 1.3. <p> In this work, we have rediscovered a method of Gallager <ref> [27] </ref>. See [68, 24, 46] for further discussion of the sum-product algorithm. We refer to the elements z m corresponding to each row m = 1 : : : M of A as checks. <p> Another related algorithm is the variational free energy minimization decoder [37]. We describe the application of this decoder to Gallager and MN codes in appendix D. Its performance is not as good as the sum-product decoder's. 3.3 Analysis of decoding algorithms We analyse a simple decoding algorithm, following Gallager <ref> [27] </ref> and Meier and Staffelbach [47]. (The same algorithm has been used by Spielman [60].) We also study the sum-product decoder in the limit of large N using Monte Carlo methods. <p> Successful decoding will only occur if the average entropy of a bit decreases to zero as the number of iterations increases. We have simulated the iteration of an infinite belief network by Monte Carlo methods | a technique first used by Gallager <ref> [27] </ref>. Imagine a network of radius I (the total number of iterations) centred on one bit. Our aim is to compute the conditional entropy of the central bit x given the state z of all checks out to radius I. <p> Our random code constructions are not identical to Gallager's, and we ran the decoder for more iterations (up to 500), but the results for construction 1A appear much the same as those in figure 6.7 of his book <ref> [27] </ref>. 5 Pictorial demonstration of Gallager codes Figures 12-15 illustrate visually the conditions under which Gallager's low density parity check codes can give reliable communication over binary symmetric channels and Gaussian channels. <p> system, if appropriately controlled, would allow the users dynamically to change their rate of communication by changing their densities f u without changing their encoder. 7.7 Conundrum: why were Gallager codes forgotten? Why was Gallager's work mostly forgotten by the information theory community? There are very few citations of Gallager's <ref> [26, 27] </ref> work on low-density parity-check codes. A search on BIDS returns the following citations: [14, 25, 28, 63, 44, 49, 48, 54, 53, 55, 59, 64, 71, 72].
Reference: 28. <author> R. G. Gallager. </author> <title> Finding parity in a simple broadcast network. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 34(2) </volume> <pages> 176-180, </pages> <year> 1988. </year>
Reference: 29. <author> S. W. Golomb, R. E. Peile, and R. A. Scholtz. </author> <title> Basic Concepts in Information Theory and Coding: The Adventures of Secret Agent 00111. </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Panel (a) shows codes with rates between about 1=2 and 2=3; panel (b) shows codes with rates between 1=4 and 1=3. Textbook codes: The curve labelled (7,1/2) shows the performance of a rate 1/2 convolutional code with constraint length 7, known as the de facto standard for satellite communications <ref> [29] </ref>. The curve (7,1/2)C shows the performance of the concatenated code composed of the same convolutional code and a Reed-Solomon code. <p> This expectation is shared by Golomb, Peile and Scholtz, who state that `the optimal code for a given set of channel conditions may not resemble the optimal code for another' <ref> [29, p. 369] </ref>. But theoretically, the same encoder family can be used for any channel | all that needs to be changed is the decoding algorithm. The practical performance of Gallager's 1962 codes, using Gallager's 1962 decoding algorithm, would have broken practical coding records up until 1993.
Reference: 30. <author> J. J. Hopfield and D. W. Tank. </author> <title> Neural computation of decisions in optimization problems. </title> <journal> Biological Cybernetics, </journal> <volume> 52 </volume> <pages> 1-25, </pages> <year> 1985. </year>
Reference-contexts: Prior work related to this concept is found in [65]. It is possible that further benefits may be obtained by applying sum-product concepts in statistical physics or to other optimization problems where mean field methods have been found useful <ref> [30, 2] </ref>. Decoding algorithms. We conjecture that as we get closer to the Shannon limit, the decoding problem gets harder. But we don't understand what aspects of the problem determine the practical 39 limits of our present decoding algorithms.
Reference: 31. <author> M. Kac. </author> <title> Random walk and the theory of Brownian motion. </title> <journal> Amer. Math. Monthly, </journal> <volume> 54 </volume> <pages> 369-391, </pages> <year> 1947. </year>
Reference-contexts: F Bounds on random walk's return probability We derive several upper bounds on the probability p (r) 00 that the random walk on the M -dimensional hypercube returns to its starting corner on the rth step. We use the result from <ref> [31] </ref> that p 00 = 2 M j=0 M ! 2j r See [13, 20, 32, 34, 62] for further information about this random walk, which is also known as the Ehrenfest Urn model.
Reference: 32. <author> S. Karlin, B. Lindqvist, and Y-C Yao. </author> <title> Markov chains on hypercubes: Spectral representations and several majorization relations. Random Structures and Algorithms, </title> <booktitle> 4 </booktitle> <pages> 1-36, </pages> <year> 1993. </year> <month> 53 </month>
Reference-contexts: We use the result from [31] that p 00 = 2 M j=0 M ! 2j r See <ref> [13, 20, 32, 34, 62] </ref> for further information about this random walk, which is also known as the Ehrenfest Urn model. Equation (86) is an eigenvalue expansion, where the eigenvalues of the Markov process are labeled by j = 0 : : : M and have value i M .
Reference: 33. <author> S. L. Lauritzen and D. J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 50 </volume> <pages> 157-224, </pages> <year> 1988. </year>
Reference-contexts: In section 3.3.2 we analyse the flow of information up this network. 3 Practical Decoding by the Sum-product Algorithm We have developed a `sum-product decoder', also known as a `belief propagation decoder' <ref> [51, 33] </ref> for the decoding problem Ax = z mod 2. In this work, we have rediscovered a method of Gallager [27]. See [68, 24, 46] for further discussion of the sum-product algorithm.
Reference: 34. <author> G. Letac and L. Takacs. </author> <title> Random walk on the m-dimensional cube. </title> <journal> J. reine angew. Math., </journal> <volume> 310 </volume> <pages> 187-195, </pages> <year> 1979. </year>
Reference-contexts: We use the result from [31] that p 00 = 2 M j=0 M ! 2j r See <ref> [13, 20, 32, 34, 62] </ref> for further information about this random walk, which is also known as the Ehrenfest Urn model. Equation (86) is an eigenvalue expansion, where the eigenvalues of the Markov process are labeled by j = 0 : : : M and have value i M .
Reference: 35. <author> A. Lubotsky. </author> <title> Ramanujan graphs. </title> <journal> Combinatorica, </journal> <volume> 8, </volume> <year> 1988. </year>
Reference-contexts: Constructions. By introducing constructions 2A and 2B, we pushed the performance of Gallager codes a little closer to capacity. Are there further useful changes we could make to the code construction? We are currently investigating the possibility of systematic construction of matrices A whose corresponding graphs have large girth <ref> [44, 8, 35] </ref>. In this paper we have mainly considered regular low density matrices, that is, matrices in which the weight per column is constant and the weight per row is constant, or nearly constant.
Reference: 36. <author> M. G. Luby, M. Mitzenmacher, M. A. Shokrollahi, and D. A. Spielman. </author> <title> Improved low-density parity-check codes using irregular graphs and belief propagation. </title> <note> Submitted to ISIT98, </note> <year> 1998. </year>
Reference-contexts: There is a way out of this dilemma, however: we obtained better performance by using slightly irregular matrices with weight two and weight three columns (see figure 9); Luby, Mitzenmacher, Shokrollahi and Spielman <ref> [36] </ref> have recently extended this idea, investigating highly irregular Gallager codes. Their results 38 indicate that significant enhancements in performance can be obtained.
Reference: 37. <author> D. J. C. MacKay. </author> <title> Free energy minimization algorithm for decoding and cryptanalysis. </title> <journal> Electronics Letters, </journal> <volume> 31(6) </volume> <pages> 446-447, </pages> <year> 1995. </year>
Reference-contexts: Another related algorithm is the variational free energy minimization decoder <ref> [37] </ref>. We describe the application of this decoder to Gallager and MN codes in appendix D. <p> about: : : how to go about finding the solutions" and "if there are more than t + 1 errors then the situation gets very complicated very quickly." All relevant BCH codes listed in [56] are included [block sizes up to 1023]. 43 D Decoding by free energy minimization MacKay <ref> [38, 37] </ref> derived a continuous optimization algorithm for solving the discrete decoding problem Ax + y = z mod 2 (70) where A is a given binary M fiL matrix and z is a received vector of length M .
Reference: 38. <author> D. J. C. MacKay. </author> <title> A free energy minimization framework for inference problems in modulo 2 arithmetic. </title> <editor> In B. Preneel, editor, </editor> <booktitle> Fast Software Encryption (Proceedings of 1994 K.U. Leuven Workshop on Cryptographic Algorithms), number 1008 in Lecture Notes in Computer Science, </booktitle> <pages> pages 179-195. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: The free energy minimization algorithm was found to be an improvement over Meier and Staffelbach's algorithm in <ref> [38] </ref>. We anticipate that the sum-product decoder might perform even better on these cryptanalysis problems. We are at present investigating this possibility. Statistical Physics. <p> about: : : how to go about finding the solutions" and "if there are more than t + 1 errors then the situation gets very complicated very quickly." All relevant BCH codes listed in [56] are included [block sizes up to 1023]. 43 D Decoding by free energy minimization MacKay <ref> [38, 37] </ref> derived a continuous optimization algorithm for solving the discrete decoding problem Ax + y = z mod 2 (70) where A is a given binary M fiL matrix and z is a received vector of length M .
Reference: 39. <author> D. J. C. MacKay. </author> <title> Iterative probabilistic decoding of low density parity check codes. Animations available on world wide web, </title> <note> 1997. http://wol.ra.phy.cam.ac.uk/mackay/codes/gifs/. </note>
Reference-contexts: These demonstrations may be viewed as animations on the world wide web <ref> [39] </ref>. 5.1 Encoding matrix is a 10000fi20000 matrix with three 1s per column. The high density of the generator matrix is illustrated in (b) and (c) by showing the change in the transmitted vector when one of the 10000 source bits is altered.
Reference: 40. <author> D. J. C. MacKay and R. M. Neal. </author> <title> Good codes based on very sparse matrices. In Colin Boyd, editor, Cryptography and Coding. </title> <booktitle> 5th IMA Conference, number 1025 in Lecture Notes in Computer Science, </booktitle> <pages> pages 100-111. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: Gallager's low-density parity-check codes are defined in terms of a very sparse random parity check matrix [26, 27, 41]. `MN codes' are also defined in terms of very sparse random matrices, and were first presented in <ref> [40] </ref>. (MN stands for MacKay-Neal; MacKay and Neal generalized MN codes to Gallager codes, then realised that they had rediscovered Gallager's work.) MN codes are unconventional in that redundancy can be incorporated in the transmitted codewords not only by using a K fi N generator matrix with transmitted block length N <p> The novel idea behind MN codes is that we can constrain the source vectors to be sparse and exploit this unconventional form of redundancy in the decoder <ref> [40] </ref>. We will discuss properties and possible applications of MN codes in section 6. <p> Conventionally the code is systematic, so the first K transmitted bits are the K source bits. The (N K) extra bits are parity check bits, which produce redundancy in the transmitted vector t. This redundancy is exploited by the decoding algorithm to infer the noise vector n. MN codes <ref> [40] </ref> are based on a different approach. We first assume that the source may itself be redundant, having f s , the expected density of s, less than 0.5. Consecutive source symbols are independent and identically distributed. <p> Of these, it seems that the only author who pursued the practical implementation of Gallager codes (and variations on them) was Tanner [63]. An independent rediscovery of Gallager's work has been made by Wiberg [69, 68]. We regret that we initially misunderstood Gallager's work: in <ref> [40] </ref>, we incorrectly asserted that Gallager's codes were `bad' owing to a confusion with their duals, low density generator matrix codes, which are bad; we also confused the decoding algorithms of Gallager and Meier and Staffelbach.
Reference: 41. <author> D. J. C. MacKay and R. M. Neal. </author> <title> Near Shannon limit performance of low density parity check codes. </title> <journal> Electronics Letters, </journal> <volume> 32(18) </volume> <pages> 1645-1646, </pages> <month> August </month> <year> 1996. </year> <journal> Reprinted Electronics Letters, </journal> 33(6):457-458, March 1997. 
Reference-contexts: This statement is supported by the results of the present paper. In this paper we study the theoretical and practical properties of two code families. Gallager's low-density parity-check codes are defined in terms of a very sparse random parity check matrix <ref> [26, 27, 41] </ref>. `MN codes' are also defined in terms of very sparse random matrices, and were first presented in [40]. (MN stands for MacKay-Neal; MacKay and Neal generalized MN codes to Gallager codes, then realised that they had rediscovered Gallager's work.) MN codes are unconventional in that redundancy can be
Reference: 42. <author> D. J. C. MacKay, S. T. Wilson, and M. C. Davey. </author> <title> Comparison of constructions of irregular Gallager codes. </title> <note> in preparation, </note> <month> June </month> <year> 1998. </year>
Reference-contexts: We are currently investigating the performance of low density parity check codes which can be encoded in linear time <ref> [42] </ref>. Decoding involves approximately 6N t floating point multiplies per iteration (assuming a model of computation where the cost of elementary operations does not grow with N ), so the total number of operations per decoded bit (assuming 20 iterations) is about 120t=R, independent of block length.
Reference: 43. <author> F. J. MacWilliams and N. J. A. Sloane. </author> <title> The theory of error-correcting codes. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1977. </year>
Reference-contexts: The Gilbert bound GV (f n ) is GV (f n ) = 1 H 2 (2f n ) f n &lt; 1=4 : (6) This is the maximum rate at which one can communicate with a code which satisfies the Gilbert-Varshamov minimum distance bound, assuming bounded distance decoding <ref> [43] </ref>. <p> distance of a code may be viewed as a convenient measure of how `good' it is, but in fact it is not possible to distinguish between good and very good codes by their minimum distance, and bounded distance decoders are well known to be unable to achieve the Shannon limit <ref> [43] </ref>. We have proved that Gallager and MN codes can (when optimally decoded) achieve capacity.
Reference: 44. <author> G. A. Margulis. </author> <title> Explicit constructions of graphs without short cycles and low-density codes. </title> <journal> Combina-torica, </journal> <volume> 2(1) </volume> <pages> 71-78, </pages> <year> 1982. </year>
Reference-contexts: Constructions. By introducing constructions 2A and 2B, we pushed the performance of Gallager codes a little closer to capacity. Are there further useful changes we could make to the code construction? We are currently investigating the possibility of systematic construction of matrices A whose corresponding graphs have large girth <ref> [44, 8, 35] </ref>. In this paper we have mainly considered regular low density matrices, that is, matrices in which the weight per column is constant and the weight per row is constant, or nearly constant.
Reference: 45. <author> R. J. </author> <title> McEliece. The Theory of Information and Coding: A Mathematical Framework for Communication. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1977. </year>
Reference-contexts: Shannon's proof was non-constructive and employed random codes for which there is no practical encoding or decoding algorithm. Since 1948, it has been proved that there exist very good cyclic codes (non-constructively) <ref> [45] </ref>, and that very good codes with a short description in terms of permutations can be produced [1]; and an explicit algebraic construction of very good codes for 1 certain channels was given in 1982 [19]. <p> Forney proved that there do exist very good `concatenated' codes that are practical [23]; but the proof was also non-constructive <ref> [45] </ref>. When it comes to practical, constructive codes, constructions have been demonstrated of codes based on concatenation that are good, though not very good, but most known practical codes are asymptotically bad [45]. <p> that there do exist very good `concatenated' codes that are practical [23]; but the proof was also non-constructive <ref> [45] </ref>. When it comes to practical, constructive codes, constructions have been demonstrated of codes based on concatenation that are good, though not very good, but most known practical codes are asymptotically bad [45]. Goppa's algebraic geometry codes, reviewed in [66], appear to be both practical and good (with practical decoding proven possible up to the Gilbert bound), but we believe that the literature has not established whether they are very good. <p> It seems to be widely believed that while almost any random linear code is good, codes with structure that allows practical coding are likely to be bad <ref> [45] </ref>, [15]. Battail expresses an alternative view, however, that `we can think of good codes, and we can decode them' [6]. This statement is supported by the results of the present paper. In this paper we study the theoretical and practical properties of two code families.
Reference: 46. <author> R. J. McEliece, D. J. C. MacKay, and J.-F. Cheng. </author> <title> Turbo decoding as an instance of Pearl's `belief propagation' algorithm. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 16(2) </volume> <pages> 140-152, </pages> <year> 1998. </year>
Reference-contexts: In this work, we have rediscovered a method of Gallager [27]. See <ref> [68, 24, 46] </ref> for further discussion of the sum-product algorithm. We refer to the elements z m corresponding to each row m = 1 : : : M of A as checks.
Reference: 47. <author> W. Meier and O. Staffelbach. </author> <title> Fast correlation attacks on certain stream ciphers. </title> <journal> J. Cryptology, </journal> <volume> 1 </volume> <pages> 159-176, </pages> <year> 1989. </year>
Reference-contexts: When there is a failure, the partial decoding ^ x may serve as a useful starting point for another decoding algorithm <ref> [47] </ref>. We note in passing the difference between this decoding procedure and the widespread practice in the Turbo code community, where the decoding algorithm is run for a fixed number of iterations (irrespective of whether the decoder finds a consistent state at some earlier time). <p> The alternative practice mentioned above blurs this distinction between undetected and detected errors, which seems bad science. And in engineering practice, it would seem preferable for the detected errors to be labelled as erasures if practically possible. 3.2 Relationship to other algorithms Meier and Staffelbach <ref> [47] </ref> implemented an algorithm similar to this sum-product decoder, also studied by Mihaljevic and Golic [49, 50]. <p> We describe the application of this decoder to Gallager and MN codes in appendix D. Its performance is not as good as the sum-product decoder's. 3.3 Analysis of decoding algorithms We analyse a simple decoding algorithm, following Gallager [27] and Meier and Staffelbach <ref> [47] </ref>. (The same algorithm has been used by Spielman [60].) We also study the sum-product decoder in the limit of large N using Monte Carlo methods.
Reference: 48. <author> M. J. Mihaljevic and J. D. Golic. </author> <title> A comparison of cryptanalytic principles based on iterative error-correction. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 547 </volume> <pages> 527-531, </pages> <year> 1991. </year>
Reference: 49. <author> M. J. Mihaljevic and J. D. Golic. </author> <title> A fast iterative algorithm for a shift register initial state reconstruction given the noisy output sequence. </title> <booktitle> In Advances in Cryptology AUSCRYPT'90, </booktitle> <volume> volume 453, </volume> <pages> pages 165-175. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: And in engineering practice, it would seem preferable for the detected errors to be labelled as erasures if practically possible. 3.2 Relationship to other algorithms Meier and Staffelbach [47] implemented an algorithm similar to this sum-product decoder, also studied by Mihaljevic and Golic <ref> [49, 50] </ref>.
Reference: 50. <author> M. J. Mihaljevic and J. D. Golic. </author> <title> Convergence of a Bayesian iterative error-correction procedure on a noisy shift register sequence. </title> <booktitle> In Advances in Cryptology - EUROCRYPT 92, </booktitle> <volume> volume 658, </volume> <pages> pages 124-137. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: And in engineering practice, it would seem preferable for the detected errors to be labelled as erasures if practically possible. 3.2 Relationship to other algorithms Meier and Staffelbach [47] implemented an algorithm similar to this sum-product decoder, also studied by Mihaljevic and Golic <ref> [49, 50] </ref>.
Reference: 51. <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1988. </year>
Reference-contexts: In section 3.3.2 we analyse the flow of information up this network. 3 Practical Decoding by the Sum-product Algorithm We have developed a `sum-product decoder', also known as a `belief propagation decoder' <ref> [51, 33] </ref> for the decoding problem Ax = z mod 2. In this work, we have rediscovered a method of Gallager [27]. See [68, 24, 46] for further discussion of the sum-product algorithm. <p> We think of the set of bits x and checks z as making up a `belief network', also known as a `Bayesian network', `causal network', or `influence diagram' <ref> [51] </ref>, in which every bit x l is the parent of t checks z m , and each check z m is the child of t r bits (figure 5). The network of checks and bits form a bipartite graph: bits only connect to checks, and vice versa. <p> We aim, given the observed checks, to compute the marginal posterior probabilities P (x l = 1jz; A) for each l. Algorithms for the computation of such marginal probabilities in belief networks are found in <ref> [51] </ref>. These computations are expected to be intractable for the belief network corresponding to our problem Ax = z mod 2 because its topology contains many cycles. <p> The algorithm often gives useful results after a number of iterations much greater than the number at which it could be affected by the presence of cycles. 17 3.1 The algorithm We have implemented the following algorithm (for background reading see <ref> [51] </ref>). The algorithm is appropriate for a binary channel model in which the noise bits are independent | for example, the memoryless binary symmetric channel, or the Gaussian channel with binary inputs and real outputs (the connection to real-output channels is explained in appendix B). <p> The algorithm would produce the exact posterior probabilities of all the bits after a fixed number of iterations if the bipartite graph defined by the matrix A contained no cycles <ref> [51] </ref>. Initialization. Let p 0 l = P (x l = 0) (the prior probability that bit x l is 0), and let p 1 l = P (x l = l .
Reference: 52. <author> W. W. Peterson and E. J. Weldon, Jr. </author> <title> Error-Correcting Codes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, 2nd edition, </address> <year> 1972. </year>
Reference-contexts: We found that the results were best for t = 3 and became steadily worse as t increased. We compare Gallager codes with t = 3 with BCH codes, which are described in <ref> [52] </ref> as "the best known constructive codes" for memoryless noisy channels, and with Reed-Muller (RM) codes. These are multiple random error correcting codes that can be characterized by three parameters (n; k; t).
Reference: 53. <author> N. Pippenger. </author> <title> The expected capacity of concentrators. </title> <journal> SIAM Journal on Discrete Mathematics, </journal> <volume> 4(1) </volume> <pages> 121-129, </pages> <year> 1991. </year>
Reference: 54. <author> N. Pippenger, G. D. Stamoulis, and J. N. Tsitsiklis. </author> <title> On a lower bound for the redundancy of reliable networks with noisy gates. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(3) </volume> <pages> 639-643, </pages> <year> 1991. </year>
Reference: 55. <author> B. Radosavljevic, E. Arikan, and B. Hajek. </author> <title> Sequential-decoding of low-density parity-check codes by adaptive reordering of parity checks. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(6) </volume> <pages> 1833-1839, </pages> <year> 1992. </year>
Reference: 56. <author> T. R. N. Rao and E. Fujiwara. </author> <title> Error-control Coding for Computer Systems. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: BCH decoder that corrects more than t errors, but according to Berlekamp [9], "little is known about: : : how to go about finding the solutions" and "if there are more than t + 1 errors then the situation gets very complicated very quickly." All relevant BCH codes listed in <ref> [56] </ref> are included [block sizes up to 1023]. 43 D Decoding by free energy minimization MacKay [38, 37] derived a continuous optimization algorithm for solving the discrete decoding problem Ax + y = z mod 2 (70) where A is a given binary M fiL matrix and z is a received
Reference: 57. <author> J. Rissanen and G. G. Langdon. </author> <title> Arithmetic coding. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 23 </volume> <pages> 149-162, </pages> <year> 1979. </year>
Reference-contexts: We first assume that the source may itself be redundant, having f s , the expected density of s, less than 0.5. Consecutive source symbols are independent and identically distributed. Redundant sources of this type can be produced from other sources by using a variation on arithmetic coding <ref> [70, 57] </ref>; one simply reverses the role of encoder and decoder in a standard arithmetic coder based on a model corresponding to the sparse messages (see appendix H). Now given that the source is already redundant, we are no longer constrained to have N &gt; K. <p> H Arithmetic coding for creation of sparse sources A redundant source having density less than 0.5, with consecutive source symbols that are inde pendent and identically distributed, can be produced from a dense source by using a variation on arithmetic coding <ref> [70, 57] </ref>; one simply reverses the role of encoder and decoder in a standard arith metic coder based on a model corresponding to the sparse messages. The following pseudocode gives an algorithm for this task, but ignores issues of initialization and termination.
Reference: 58. <author> C. E. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell Sys. Tech. J., </journal> <volume> 27 </volume> <pages> 379-423, 623-656, </pages> <year> 1948. </year>
Reference-contexts: 1 Introduction For a glossary of symbols used in this paper, please see appendix A. 1.1 Background In 1948, Shannon <ref> [58] </ref> proved that for any channel there exist families of block codes that achieve arbitrarily small probability of error at any communication rate up to the capacity of the channel. We will refer to such code families as `very good' codes.
Reference: 59. <author> A. M. Slinko. </author> <title> Design of experiments to detect nonnegligible variables in a linear model. </title> <journal> Cybernetics, </journal> <volume> 27(3) </volume> <pages> 433-442, </pages> <year> 1991. </year>
Reference: 60. <author> D. A. Spielman. </author> <title> Linear-time encodable and decodable error-correcting codes. </title> <note> To appear in IEEE Transactions on Information Theory, </note> <year> 1996. </year>
Reference-contexts: Its performance is not as good as the sum-product decoder's. 3.3 Analysis of decoding algorithms We analyse a simple decoding algorithm, following Gallager [27] and Meier and Staffelbach [47]. (The same algorithm has been used by Spielman <ref> [60] </ref>.) We also study the sum-product decoder in the limit of large N using Monte Carlo methods.
Reference: 61. <author> L. Swanson. </author> <title> A new code for Galileo. </title> <booktitle> In Proc. 1988 IEEE International Symposium Information Theory, </booktitle> <pages> pages 94-95, </pages> <year> 1988? </year>
Reference-contexts: McEliece, personal communication). This system can only be decoded using expensive special purpose hardware, and the details of the code are unpublished outside JPL <ref> [61] </ref>. 7.1 Comparison with Turbo Codes We heard about Turbo codes [12, 11], which outperform Gallager's codes in terms of E b =N 0 , towards the end of this work. There are some similarities between the codes.
Reference: 62. <author> L. Takacs. </author> <title> On an urn problem of Paul and Tatiana Ehrenfest. </title> <journal> Math. Proc. Camb. Phil. Soc., </journal> <volume> 86 </volume> <pages> 127-130, </pages> <year> 1979. </year>
Reference-contexts: We use the result from [31] that p 00 = 2 M j=0 M ! 2j r See <ref> [13, 20, 32, 34, 62] </ref> for further information about this random walk, which is also known as the Ehrenfest Urn model. Equation (86) is an eigenvalue expansion, where the eigenvalues of the Markov process are labeled by j = 0 : : : M and have value i M .
Reference: 63. <author> R. M. Tanner. </author> <title> A recursive approach to low complexity codes. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 27(5) </volume> <pages> 533-547, </pages> <year> 1981. </year>
Reference-contexts: A search on BIDS returns the following citations: [14, 25, 28, 63, 44, 49, 48, 54, 53, 55, 59, 64, 71, 72]. Of these, it seems that the only author who pursued the practical implementation of Gallager codes (and variations on them) was Tanner <ref> [63] </ref>. An independent rediscovery of Gallager's work has been made by Wiberg [69, 68].
Reference: 64. <author> C. Thommesen. </author> <title> Error-correcting capabilities of concatenated codes with MDS outer codes on memoryless channels with maximum-likelihood decoding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 33(5) </volume> <pages> 632-640, </pages> <year> 1987. </year>
Reference: 65. <author> D. J. Thouless, P. W. Anderson, and R. G. Palmer. </author> <title> Solutions of `solvable models of a spin glass'. </title> <journal> Philosophical Magazine, </journal> <volume> 35(3) </volume> <pages> 593-601, </pages> <year> 1977. </year>
Reference-contexts: A difference between the algorithms is that the free energy minimization algorithm (also known as `mean field theory') shows spontaneous symmetry breaking, whereas the sum-product algorithm only breaks symmetry if the energy function itself breaks symmetry. Prior work related to this concept is found in <ref> [65] </ref>. It is possible that further benefits may be obtained by applying sum-product concepts in statistical physics or to other optimization problems where mean field methods have been found useful [30, 2]. Decoding algorithms. We conjecture that as we get closer to the Shannon limit, the decoding problem gets harder.
Reference: 66. <author> M. A. Tsfasman. </author> <title> Algebraic-geometric codes and asymptotic problems. </title> <journal> Discrete Applied Mathematics, </journal> <volume> 33(1-3):241-256, </volume> <year> 1991. </year>
Reference-contexts: When it comes to practical, constructive codes, constructions have been demonstrated of codes based on concatenation that are good, though not very good, but most known practical codes are asymptotically bad [45]. Goppa's algebraic geometry codes, reviewed in <ref> [66] </ref>, appear to be both practical and good (with practical decoding proven possible up to the Gilbert bound), but we believe that the literature has not established whether they are very good.
Reference: 67. <author> A. J. </author> <title> Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-13:260-269, </volume> <year> 1967. </year>
Reference-contexts: The probabilities for z m having its observed value given either x l = 0 or x l = 1 can then be found efficiently by use of the forward-backward algorithm <ref> [7, 67, 5] </ref>. A particularly convenient implementation of this method uses forward and backward passes in which products of the differences ffiq ml j q 0 ml q 1 ml are computed. <p> the constraint that only the defined binary inputs are allowed, then the capacity is reduced to: C Binary = C B = H (Y ) H (Y jX) (66) = dy P (y) log P (y) + dyP (yjx = x 0 ) log P (yjx = x 0 ); <ref> (67) </ref> where P (y) = 2 2oe 2 e (yx) 2 =(2oe 2 ) + e (y+x) 2 =(2oe 2 ) ; (68) which may be evaluated numerically.
Reference: 68. <author> N. Wiberg. </author> <title> Codes and Decoding on General Graphs. </title> <type> PhD thesis, </type> <institution> Dept. of Electrical Engineering, Linkoping, Sweden, </institution> <year> 1996. </year> <booktitle> Linkoping studies in Science and Technology. </booktitle> <volume> Dissertation No. </volume> <pages> 440. </pages>
Reference-contexts: In this work, we have rediscovered a method of Gallager [27]. See <ref> [68, 24, 46] </ref> for further discussion of the sum-product algorithm. We refer to the elements z m corresponding to each row m = 1 : : : M of A as checks. <p> Of these, it seems that the only author who pursued the practical implementation of Gallager codes (and variations on them) was Tanner [63]. An independent rediscovery of Gallager's work has been made by Wiberg <ref> [69, 68] </ref>. We regret that we initially misunderstood Gallager's work: in [40], we incorrectly asserted that Gallager's codes were `bad' owing to a confusion with their duals, low density generator matrix codes, which are bad; we also confused the decoding algorithms of Gallager and Meier and Staffelbach.
Reference: 69. <author> N. Wiberg, H.-A. Loeliger, and R. Kotter. </author> <title> Codes and iterative decoding on general graphs. </title> <journal> European Transactions on Telecommunications, </journal> <volume> 6 </volume> <pages> 513-525, </pages> <year> 1995. </year>
Reference-contexts: Of these, it seems that the only author who pursued the practical implementation of Gallager codes (and variations on them) was Tanner [63]. An independent rediscovery of Gallager's work has been made by Wiberg <ref> [69, 68] </ref>. We regret that we initially misunderstood Gallager's work: in [40], we incorrectly asserted that Gallager's codes were `bad' owing to a confusion with their duals, low density generator matrix codes, which are bad; we also confused the decoding algorithms of Gallager and Meier and Staffelbach.
Reference: 70. <author> I. H. Witten, R. M. Neal, and J. G. Cleary. </author> <title> Arithmetic coding for data compression. </title> <journal> Communications of the ACM, </journal> <volume> 30(6) </volume> <pages> 520-540, </pages> <year> 1987. </year>
Reference-contexts: We first assume that the source may itself be redundant, having f s , the expected density of s, less than 0.5. Consecutive source symbols are independent and identically distributed. Redundant sources of this type can be produced from other sources by using a variation on arithmetic coding <ref> [70, 57] </ref>; one simply reverses the role of encoder and decoder in a standard arithmetic coder based on a model corresponding to the sparse messages (see appendix H). Now given that the source is already redundant, we are no longer constrained to have N &gt; K. <p> H Arithmetic coding for creation of sparse sources A redundant source having density less than 0.5, with consecutive source symbols that are inde pendent and identically distributed, can be produced from a dense source by using a variation on arithmetic coding <ref> [70, 57] </ref>; one simply reverses the role of encoder and decoder in a standard arith metic coder based on a model corresponding to the sparse messages. The following pseudocode gives an algorithm for this task, but ignores issues of initialization and termination.
Reference: 71. <author> G. Zemor and G. D. Cohen. </author> <title> The threshold probability of a code. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 41(2) </volume> <pages> 469-477, </pages> <year> 1995. </year>
Reference: 72. <author> M. Zivkovic. </author> <title> On two probabilistic decoding algorithms for binary linear codes. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(6) </volume> <pages> 1707-1716, </pages> <year> 1991. </year>

References-found: 72

