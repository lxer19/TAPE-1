URL: ftp://www.cs.rutgers.edu/pub/technical-reports/lcsr-tr-192.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: 
Title: A Comprehensive Approach to Parallel Data Flow Analysis  
Author: Yong-fong Lee Barbara G. Ryder 
Keyword: Definitions problem and the structural characteristics of the program flow graphs that affect algorithm performance. Keywords. Data flow analysis, parallel algorithms, parallel data flow analysis.  
Address: New Brunswick, NJ 08903  
Affiliation: Department of Computer Science Rutgers University  
Abstract: We present a comprehensive approach to performing data flow analysis in parallel. We identify three types of parallelism inherent in the data flow solution process: independent-problem parallelism, separate-unit parallelism and algorithmic parallelism; and describe a unified framework to exploit them. Our investigations of typical Fortran programs reveal an abundance of the last two types of parallelism. In particular, we illustrate the exploitation of algorithmic parallelism in the design of our parallel hybrid data flow analysis algorithms. We report on the empirical performance of the parallel hybrid algorithm for the Reaching 
Abstract-found: 1
Intro-found: 1
Reference: [ABC + 88] <author> Frances Allen, Michael Burke, Philippe Charles, Ron Cytron, and Jeanne Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 617-640, </pages> <year> 1988. </year>
Reference-contexts: By speeding up the data flow solution process, we can compute more precise interprocedural data flow information, previously too time-consuming to calculate, and thus enable an optimizing compiler to perform more aggressive optimizations. Parallelizing compilers <ref> [ABC + 88, Pol88, ZBG88, Wol89] </ref>, like conventional optimizing compilers, collect data flow information for a source program and use this information to detect potential parallelism, determine an appropriate grain size, and then transform the program into a functionally equivalent program with a higher degree of parallelism.
Reference: [AC76] <author> Frances E. Allen and John Cocke. </author> <title> A program data flow analysis procedure. </title> <journal> Communications of the ACM, </journal> <volume> 19(3) </volume> <pages> 137-147, </pages> <year> 1976. </year>
Reference-contexts: Because of the computational complexity, we attempted to find approximation algorithms that can do well at relatively small cost on flow graphs from real programs. Here we present our forward algorithm for region partition, which is based on the Allen-Cocke interval-finding algorithm <ref> [AC76] </ref>. 11 The algorithm is shown in Figure 7, where a region is simply represented by a set of nodes. Since it forms regions by proceeding along the direction of execution flow on flow graph edges, it is called the forward algorithm.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: An earlier version of this paper appears in Proceedings of the 6th ACM International Conference on Supercom puting, pp. 236-247, Washington, D.C., July 1992. y Email addresses: flee,ryderg@cs.rutgers.edu 1 <ref> [ASU86] </ref>. This information can be approximated at varied levels of precision; its utility varies directly with its precision. Computation of precise data flow information needs to account for data flow effects across procedure boundaries. <p> , and That is, access to every region internal node in N 1 fhg must go through the region head node; every region is single-entry [Hec77]. 2.2 Data Flow Analysis Data flow analysis is a process of collecting information about the way variables are defined and used in a program <ref> [ASU86] </ref>. It is performed under the common assumption that all execution paths in the program are actually feasible (i.e., traversable on some program execution). Barth terms this assumption precise up to symbolic execution [Bar78]. <p> Data flow analysis algorithms use a directed graph representation of the program. In intraprocedural analysis, a procedure is abstracted as a control flow graph, in which a node represents a basic block, 2 and an edge represents a possible control transfer from one basic block to another <ref> [ASU86] </ref>. In interprocedural analysis, a program is abstracted as a call graph, in which a node represents a procedure and an edge, a possible procedure call [Hec77]. Note that call graphs are multigraphs, for a procedure can call itself or another procedure more than once. <p> For these problems, data flow information is propagated in an opposite direction to that specified by the control flow graph edges. Solutions to intraprocedural problems are used by optimizing compilers for safe program transformations such as copy propagation, common subex-pression elimination and dead-code elimination <ref> [ASU86] </ref>. A downward exposed definition d of variable x at control flow graph node v reaches the top of node w, if there is a path from v to w such that x is not redefined along the path. Such a path is called a definition-clear path for x.
Reference: [Ban79] <author> John Banning. </author> <title> An efficient way to find the side effects of procedural calls and the aliases of variables. </title> <booktitle> In Conference Record of the 6th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 29-41, </pages> <month> January </month> <year> 1979. </year>
Reference-contexts: The interprocedural side effect analysis problems include MOD, REF, KILL and USE. They are solved on the call graph. 3 MOD asks what variables may be modified (or defined) by the execution of a statement <ref> [Ban79, CK84, Bur90] </ref>. REF asks what variables may be used (or referenced) by the execution of a statement [Bar78, CKT86b, CK87, Bur90]. 4 KILL asks what variables must be defined (or killed) by the execution of a statement [Cal88]. <p> For a procedure, Aliasing asks for the set of variable pairs (x; y) such that x and 10 y may be aliases in some invocation of that procedure 6 <ref> [Ban79, CK89] </ref>. Must-Aliasing asks for the set of variable pairs (x; y) such that x and y must be aliases in all invocations of that procedure. MOD is dependent on Aliasing, and KILL on Must-Aliasing. REACH is dependent on MOD, KILL and Aliasing. <p> The well-studied decomposition of MOD is another example to illustrate problem dependence. Burke's decomposition is shown in Figure 3 [Bur90]. Banning first isolated the effect of aliases on the problem from other effects <ref> [Ban79] </ref>.
Reference: [Bar78] <author> Jeffrey M. Barth. </author> <title> A practical interprocedural data flow analysis algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 21(9) </volume> <pages> 724-736, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: It is performed under the common assumption that all execution paths in the program are actually feasible (i.e., traversable on some program execution). Barth terms this assumption precise up to symbolic execution <ref> [Bar78] </ref>. <p> They are solved on the call graph. 3 MOD asks what variables may be modified (or defined) by the execution of a statement [Ban79, CK84, Bur90]. REF asks what variables may be used (or referenced) by the execution of a statement <ref> [Bar78, CKT86b, CK87, Bur90] </ref>. 4 KILL asks what variables must be defined (or killed) by the execution of a statement [Cal88]. USE asks what variables may be used (or referenced) before being redefined by the execution of a statement [Cal88]. All these problems are complicated by chains of procedure calls.
Reference: [BC86] <author> Michael Burke and Ron Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 162-175. </pages> <publisher> ACM Press, </publisher> <address> June 1986. Palo Alto, California. </address>
Reference-contexts: 1 Introduction 1.1 Motivation Data flow analysis is a compile-time analysis technique that gathers information about the flow of data in the program. Data flow information is essential in program development environments [CKT86a], testing [RW85, OW91], debugging [Wei84], software maintenance [Ryd89], program integration [HPR89, YHR90], program parallelization <ref> [BC86, PW86] </ref>, and compiler optimization fl This research was supported, in part, by National Science Foundation grants CCR-8920078 and CCR-9023628-1/5. An earlier version of this paper appears in Proceedings of the 6th ACM International Conference on Supercom puting, pp. 236-247, Washington, D.C., July 1992. y Email addresses: flee,ryderg@cs.rutgers.edu 1 [ASU86].
Reference: [BS90] <editor> David Barnard and David Skillicorn, editors. </editor> <booktitle> Proceedings of the Workshop on Parallel Compilation, </booktitle> <address> Kingston, Ontario, Canada, </address> <month> May </month> <year> 1990. </year> <institution> Queen's University. </institution>
Reference-contexts: Our work is aimed at facilitating efficient static analysis on parallel machines; thus we are adding to the body of work addressing issues of compilation performed in parallel <ref> [SWJ + 88, GZZ89, Gaf90, BS90] </ref>. Interprocedural data flow analysis is rarely performed currently in sequential compilers, in part because of its expected cost.
Reference: [Bur90] <author> Michael Burke. </author> <title> An interval-based approach to exhaustive and incremental interprocedural data-flow analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 341-395, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The interprocedural side effect analysis problems include MOD, REF, KILL and USE. They are solved on the call graph. 3 MOD asks what variables may be modified (or defined) by the execution of a statement <ref> [Ban79, CK84, Bur90] </ref>. REF asks what variables may be used (or referenced) by the execution of a statement [Bar78, CKT86b, CK87, Bur90]. 4 KILL asks what variables must be defined (or killed) by the execution of a statement [Cal88]. <p> They are solved on the call graph. 3 MOD asks what variables may be modified (or defined) by the execution of a statement [Ban79, CK84, Bur90]. REF asks what variables may be used (or referenced) by the execution of a statement <ref> [Bar78, CKT86b, CK87, Bur90] </ref>. 4 KILL asks what variables must be defined (or killed) by the execution of a statement [Cal88]. USE asks what variables may be used (or referenced) before being redefined by the execution of a statement [Cal88]. All these problems are complicated by chains of procedure calls. <p> Two dummy tasks start and finish of no cost are added to make a critical path always begin at start and end at finish. The well-studied decomposition of MOD is another example to illustrate problem dependence. Burke's decomposition is shown in Figure 3 <ref> [Bur90] </ref>. Banning first isolated the effect of aliases on the problem from other effects [Ban79].
Reference: [Cal88] <author> David Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 47-56, </pages> <month> June </month> <year> 1988. </year> <month> 32 </month>
Reference-contexts: REF asks what variables may be used (or referenced) by the execution of a statement [Bar78, CKT86b, CK87, Bur90]. 4 KILL asks what variables must be defined (or killed) by the execution of a statement <ref> [Cal88] </ref>. USE asks what variables may be used (or referenced) before being redefined by the execution of a statement [Cal88]. All these problems are complicated by chains of procedure calls. MOD and REF are flow-insensitive; KILL and USE are flow-sensitive. <p> may be used (or referenced) by the execution of a statement [Bar78, CKT86b, CK87, Bur90]. 4 KILL asks what variables must be defined (or killed) by the execution of a statement <ref> [Cal88] </ref>. USE asks what variables may be used (or referenced) before being redefined by the execution of a statement [Cal88]. All these problems are complicated by chains of procedure calls. MOD and REF are flow-insensitive; KILL and USE are flow-sensitive. Solving flow-insensitive problems does not require the knowledge of control flow within procedures, whereas solving flow sensitive problems does.
Reference: [CCH + 88] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel pro-gramming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <year> 1988. </year>
Reference-contexts: These optimizations are normally aimed at reducing nonlocal data accesses in computer systems with distributed memories [HKT91] or hierarchical shared memories [GV91]. Furthermore, there are interactive parallel program development tools, such as PTOOL [HHLS90] 3 and ParaScope <ref> [CCH + 88] </ref>, designed to help the user parallelize sequential programs and debug par-allel programs. It is required that these tools have as precise data flow information as possible, in order to assist the user in parallelizing, debugging, and testing programs.
Reference: [CCKT86] <author> David Callahan, Keith Cooper, Ken Kennedy, and Linda Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 152-161, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Lack of precise data flow information limits the tools that use it to ensure the correctness of program transformations. For example, an optimizing compiler cannot perform aggressive optimizations such as interprocedural constant propagation <ref> [CCKT86] </ref> and interprocedural register allocation [SO90]. A parallel programming tool cannot aid users as effectively in parallelizing their programs because of too many spurious data dependences [HHLS90].
Reference: [CK84] <author> Keith D. Cooper and Ken Kennedy. </author> <title> Efficient computation of flow insensitive interprocedural summary information. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction. </booktitle> <publisher> ACM Press, </publisher> <address> June 1984. Montreal, Canada. </address>
Reference-contexts: The interprocedural side effect analysis problems include MOD, REF, KILL and USE. They are solved on the call graph. 3 MOD asks what variables may be modified (or defined) by the execution of a statement <ref> [Ban79, CK84, Bur90] </ref>. REF asks what variables may be used (or referenced) by the execution of a statement [Bar78, CKT86b, CK87, Bur90]. 4 KILL asks what variables must be defined (or killed) by the execution of a statement [Cal88]. <p> Banning first isolated the effect of aliases on the problem from other effects [Ban79]. To improve the efficiency of Banning's technique, Cooper and Kennedy further separated the other effects into two parts: one is due to global variables and the other is due to formal parameters in the program <ref> [CK84] </ref>. (Interested readers should consult the three papers cited here for details.) To better exploit independent-problem parallelism, we can compile a comprehensive set of data flow problems and construct the complete problem dependence graph for them.
Reference: [CK87] <author> Keith Cooper and Ken Kennedy. </author> <title> Complexity of interprocedural side-effect analysis. </title> <institution> Computer Science Department Technical Report TR87-61, Rice University, </institution> <month> October </month> <year> 1987. </year>
Reference-contexts: They are solved on the call graph. 3 MOD asks what variables may be modified (or defined) by the execution of a statement [Ban79, CK84, Bur90]. REF asks what variables may be used (or referenced) by the execution of a statement <ref> [Bar78, CKT86b, CK87, Bur90] </ref>. 4 KILL asks what variables must be defined (or killed) by the execution of a statement [Cal88]. USE asks what variables may be used (or referenced) before being redefined by the execution of a statement [Cal88]. All these problems are complicated by chains of procedure calls.
Reference: [CK88] <author> David Callahan and Ken Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <pages> pages 151-170, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The detection of parallelism is not the main issue in compiling explicitly parallel programs; nevertheless, compilers for such languages need even more precise and larger amount of data flow information to perform optimizations and further concurrentization <ref> [CK88, MP90] </ref>. These optimizations are normally aimed at reducing nonlocal data accesses in computer systems with distributed memories [HKT91] or hierarchical shared memories [GV91].
Reference: [CK89] <author> Keith Cooper and Ken Kennedy. </author> <title> Fast interprocedural alias analysis. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 49-59. </pages> <publisher> ACM Press, </publisher> <month> January </month> <year> 1989. </year> <institution> Austin, Texas. </institution>
Reference-contexts: For a procedure, Aliasing asks for the set of variable pairs (x; y) such that x and 10 y may be aliases in some invocation of that procedure 6 <ref> [Ban79, CK89] </ref>. Must-Aliasing asks for the set of variable pairs (x; y) such that x and y must be aliases in all invocations of that procedure. MOD is dependent on Aliasing, and KILL on Must-Aliasing. REACH is dependent on MOD, KILL and Aliasing.
Reference: [CKPK90] <author> George Cybenko, Lyle Kipp, Lynn Pointer, and David Kuck. </author> <title> Supercomputer performance evaluation and the perfect benchmarks. </title> <booktitle> In Proceedings of 1990 International Conference on Supercomputing, </booktitle> <pages> pages 254-266, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We used the 13 Fortran programs from the Perfect Benchmarks as representatives <ref> [CKPK90] </ref>. Table 1 gives the size statistics of these programs. The size of a procedure is defined to be the number of nodes in its control flow graph and is denoted by n. All the programs except ti and lw have a moderate to high degree of separate-unit parallelism.
Reference: [CKT86a] <author> Keith Cooper, Ken Kennedy, and Linda Torczon. </author> <title> The impact of interprocedural analysis and optimization in the R n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction 1.1 Motivation Data flow analysis is a compile-time analysis technique that gathers information about the flow of data in the program. Data flow information is essential in program development environments <ref> [CKT86a] </ref>, testing [RW85, OW91], debugging [Wei84], software maintenance [Ryd89], program integration [HPR89, YHR90], program parallelization [BC86, PW86], and compiler optimization fl This research was supported, in part, by National Science Foundation grants CCR-8920078 and CCR-9023628-1/5.
Reference: [CKT86b] <author> Keith D. Cooper, Ken Kennedy, and Linda Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 58-67. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1986. </year>
Reference-contexts: They are solved on the call graph. 3 MOD asks what variables may be modified (or defined) by the execution of a statement [Ban79, CK84, Bur90]. REF asks what variables may be used (or referenced) by the execution of a statement <ref> [Bar78, CKT86b, CK87, Bur90] </ref>. 4 KILL asks what variables must be defined (or killed) by the execution of a statement [Cal88]. USE asks what variables may be used (or referenced) before being redefined by the execution of a statement [Cal88]. All these problems are complicated by chains of procedure calls. <p> Intuitively, it is iterative within 3 Similarly, backward interprocedural problems are solved on the reverse call graph. 4 However, Cooper et al. use USE to denote our REF <ref> [CKT86b] </ref>. 5 The method of Graham and Wegman can handle irreducible flow graphs [GW76]. In practice, node splitting can be used to transform an irreducible flow graph into an equivalent, reducible one [Hec77]. 7 components and elimination-like in its propagation on the strong component condensation.
Reference: [DG87] <author> Jack J. Dongarra and Eric Grosse. </author> <title> Distribution of mathematical software via electronic mail. </title> <journal> Communications of the ACM, </journal> <volume> 30 </volume> <pages> 403-407, </pages> <year> 1987. </year>
Reference-contexts: By contrast, program cs has 22 control flow graphs of size more than 100 (the largest control flow graph has 380 nodes and 587 edges). To evaluate parallel hybrid algorithms for intraprocedural data flow analysis, we additionally used five netlib libraries 12 <ref> [DG87] </ref>. These libraries are listed in Table 2. We chose procedures from both application programs and libraries because their procedures might have different characteris tics. A procedure is Small if and only if the size of its control flow graph is less than 20.
Reference: [Gaf90] <author> Neal M. Gafter. </author> <title> Parallel Incremental Compilation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: Our work is aimed at facilitating efficient static analysis on parallel machines; thus we are adding to the body of work addressing issues of compilation performed in parallel <ref> [SWJ + 88, GZZ89, Gaf90, BS90] </ref>. Interprocedural data flow analysis is rarely performed currently in sequential compilers, in part because of its expected cost.
Reference: [GPS90] <author> Rajiv Gupta, Lori Pollock, and Mary Lou Soffa. </author> <title> Parallelizing data flow analysis. </title> <booktitle> In Proceedings of the Workshop on Parallel Compilation, </booktitle> <address> Kingston, Ontario, Canada, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: This problem, in principle, could be alleviated by use of parallel data flow algorithms. Although parallel machines are increasingly available, current algorithms for data flow analysis are sequential. Recently, some researchers have designed parallel data flow algorithms <ref> [Zob90, LMR90, GPS90, KGS91] </ref>; preliminary empirical results were reported in [Zob90], [LMR91] and [LR92]. Although these results are encouraging, no one has proposed a comprehensive approach to performing data flow analysis in parallel.
Reference: [GV91] <author> Elana Granston and Alexander Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 854-865, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: These optimizations are normally aimed at reducing nonlocal data accesses in computer systems with distributed memories [HKT91] or hierarchical shared memories <ref> [GV91] </ref>. Furthermore, there are interactive parallel program development tools, such as PTOOL [HHLS90] 3 and ParaScope [CCH + 88], designed to help the user parallelize sequential programs and debug par-allel programs.
Reference: [GW76] <author> Susan Graham and Mark Wegman. </author> <title> A fast and usually linear algorithm for global flow analysis. </title> <journal> Journal of the ACM, </journal> <volume> 23(1) </volume> <pages> 172-202, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: Intuitively, it is iterative within 3 Similarly, backward interprocedural problems are solved on the reverse call graph. 4 However, Cooper et al. use USE to denote our REF [CKT86b]. 5 The method of Graham and Wegman can handle irreducible flow graphs <ref> [GW76] </ref>. In practice, node splitting can be used to transform an irreducible flow graph into an equivalent, reducible one [Hec77]. 7 components and elimination-like in its propagation on the strong component condensation. It can handle irreducible flow graphs by application of fixed-point iteration within components [MR90].
Reference: [GZZ89] <author> Thomas Gross, Angelika Zobel, and Markus Zolg. </author> <title> Parallel compilation for a parallel machine. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 91-100. </pages> <publisher> ACM Press, </publisher> <address> June 1989. Portland, Oregan. </address>
Reference-contexts: Our work is aimed at facilitating efficient static analysis on parallel machines; thus we are adding to the body of work addressing issues of compilation performed in parallel <ref> [SWJ + 88, GZZ89, Gaf90, BS90] </ref>. Interprocedural data flow analysis is rarely performed currently in sequential compilers, in part because of its expected cost. <p> They can take hours to compile programs of reasonable size <ref> [GZZ89] </ref>. Although the parallel code generated may be quite efficient, these long compilation times must be shortened to make the use of these compilers practical.
Reference: [Hec77] <author> Matthew S. Hecht. </author> <title> Flow Analysis of Computer Programs. </title> <publisher> Elsevier North-Holland, </publisher> <address> Amsterdam, Netherlands, </address> <year> 1977. </year>
Reference-contexts: is some i k such that * v i = h, * v i+1 ; : : : ; v k are in N 1 , and That is, access to every region internal node in N 1 fhg must go through the region head node; every region is single-entry <ref> [Hec77] </ref>. 2.2 Data Flow Analysis Data flow analysis is a process of collecting information about the way variables are defined and used in a program [ASU86]. It is performed under the common assumption that all execution paths in the program are actually feasible (i.e., traversable on some program execution). <p> =&lt; G; L; F; M; &gt;, where G is a flow graph (N; E; ), L is usually a meet semilattice, F is a space of monotone functions mapping L into L, M is a mapping of edges E of G into F , and is an element of L <ref> [Hec77, MR91] </ref>. Intuitively, L is a lattice of data flow solutions, M the assignment of transition functions to nodes or edges, and the entry solution at ; F is usually not represented explicitly. <p> In interprocedural analysis, a program is abstracted as a call graph, in which a node represents a procedure and an edge, a possible procedure call <ref> [Hec77] </ref>. Note that call graphs are multigraphs, for a procedure can call itself or another procedure more than once. Consequently, the number of edges in a call graph is the number of call sites in its corresponding program. <p> We assume the flow graphs for a program have been generated before we begin to solve data flow problems. A flow graph is irreducible if and only if it contains a subgraph as shown in Figure 1, and is reducible otherwise <ref> [Hec77] </ref>. The root and nodes a; b and c are distinct except that and a may be the same; there are node-disjoint paths among them as the figure indicates. Intuitively, an irreducible flow graph contains a cycle with multiple entry nodes. <p> There are three families of general-purpose solution procedures for data flow analysis: iterative, elimination and hybrid. An iterative algorithm is based on fixed-point iteration. It starts with a safe, initial solution, and then proceeds to get the maximum fixed point for the equations <ref> [Hec77] </ref>. Although iterative algorithms do not rely on the flow graph being reducible, they are normally not as efficient as elimination or hybrid algorithms. An elimination algorithm has two phases and is conceptually similar to Gaussian elimination [RP86]. <p> In practice, node splitting can be used to transform an irreducible flow graph into an equivalent, reducible one <ref> [Hec77] </ref>. 7 components and elimination-like in its propagation on the strong component condensation. It can handle irreducible flow graphs by application of fixed-point iteration within components [MR90]. In addition to the above general-purpose algorithms, there is a family of data flow solution procedures called the partitioned variable technique (PVT) [Zad84].
Reference: [HHLS90] <author> L.A. Henderson, R.E. Hiromoto, O.M. Lubeck, </author> <title> and M.L. Simmons. On the use of diagnostic dependence-analysis tools in parallel programming: Experiences using PTOOL. </title> <journal> The Journal of Supercomputing, </journal> <volume> 4(1) </volume> <pages> 83-96, </pages> <month> March </month> <year> 1990. </year> <month> 33 </month>
Reference-contexts: For example, an optimizing compiler cannot perform aggressive optimizations such as interprocedural constant propagation [CCKT86] and interprocedural register allocation [SO90]. A parallel programming tool cannot aid users as effectively in parallelizing their programs because of too many spurious data dependences <ref> [HHLS90] </ref>. Furthermore, without such information a debugger cannot focus one's attention on a reasonably small portion of code where errors may exist, especially when multiple procedures are involved [Wei84]. This problem, in principle, could be alleviated by use of parallel data flow algorithms. <p> These optimizations are normally aimed at reducing nonlocal data accesses in computer systems with distributed memories [HKT91] or hierarchical shared memories [GV91]. Furthermore, there are interactive parallel program development tools, such as PTOOL <ref> [HHLS90] </ref> 3 and ParaScope [CCH + 88], designed to help the user parallelize sequential programs and debug par-allel programs. It is required that these tools have as precise data flow information as possible, in order to assist the user in parallelizing, debugging, and testing programs.
Reference: [HKT91] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 86-100, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: These optimizations are normally aimed at reducing nonlocal data accesses in computer systems with distributed memories <ref> [HKT91] </ref> or hierarchical shared memories [GV91]. Furthermore, there are interactive parallel program development tools, such as PTOOL [HHLS90] 3 and ParaScope [CCH + 88], designed to help the user parallelize sequential programs and debug par-allel programs.
Reference: [Hoa78] <author> C.A.R. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-677, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: A parallel machine P = fP 1 ; P 2 ; : : : ; P p g consists of p processors. Our computation model is based on Hoare's Communicating Sequential Processes (CSP) <ref> [Hoa78] </ref>. We will formulate the parallel execution of data flow analysis as a task system. Intuitively, a task system is a parallel composition of asynchronous communicating tasks. A task is executed sequentially and will run to completion without interruption.
Reference: [HPR89] <author> Susan Horwitz, Jan Prins, and Thomas Reps. </author> <title> Integrating non-interfering versions of programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(3) </volume> <pages> 345-387, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: 1 Introduction 1.1 Motivation Data flow analysis is a compile-time analysis technique that gathers information about the flow of data in the program. Data flow information is essential in program development environments [CKT86a], testing [RW85, OW91], debugging [Wei84], software maintenance [Ryd89], program integration <ref> [HPR89, YHR90] </ref>, program parallelization [BC86, PW86], and compiler optimization fl This research was supported, in part, by National Science Foundation grants CCR-8920078 and CCR-9023628-1/5.
Reference: [KGS91] <author> Robert Kramer, Rajiv Gupta, and Mary Lou Soffa. </author> <title> The combining DAG: A technique for parallel data flow analysis. </title> <type> Technical Report 91-8, </type> <institution> University of Pittsburgh, </institution> <address> Pittsburgh, PA., </address> <month> March </month> <year> 1991. </year>
Reference-contexts: This problem, in principle, could be alleviated by use of parallel data flow algorithms. Although parallel machines are increasingly available, current algorithms for data flow analysis are sequential. Recently, some researchers have designed parallel data flow algorithms <ref> [Zob90, LMR90, GPS90, KGS91] </ref>; preliminary empirical results were reported in [Zob90], [LMR91] and [LR92]. Although these results are encouraging, no one has proposed a comprehensive approach to performing data flow analysis in parallel.
Reference: [Lee92] <author> Yong-fong Lee. </author> <title> Performing Data Flow Analysis in Parallel. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rutgers University, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Straightforward parallelization of an elimination algorithm can simultaneously summarize (and propagate) data flow information in independent intervals; however, this approach offers no control over the flow graph partitioning [Zob90]. For improving performance, we can partition a flow graph into regions instead of intervals <ref> [Lee92] </ref>. We can refine a large interval into smaller regions to improve parallelism and load balancing, and cluster small intervals into a larger region to improve load balancing. Too small intervals usually offer parallelism that is too fine-grained to be effectively exploited in practice. <p> For a specific flow graph, fewer regions are more likely to create fewer inter-region edges; therefore, communication and synchronization costs can be reduced too. The region partition problem is in general provably N P-hard <ref> [Lee92] </ref>. Because of the computational complexity, we attempted to find approximation algorithms that can do well at relatively small cost on flow graphs from real programs. <p> We also look at structural properties of flow graphs that influence their suitability for parallel hybrid algorithms. 11 The other and more complicated bottom-up algorithm, can be found in <ref> [Lee92] </ref>. 19 Input: flow graph G and size limit S. Output: a list of regions RP that partitions G. Data Structures: 1. H a set of head nodes. 2. R a single region. 3.
Reference: [LK78] <author> J.K. Lenstra and A.H.G. Rinnooy Kan. </author> <title> Complexity of scheduling under precedence constraints. </title> <journal> Operations Research, </journal> <volume> 26(1) </volume> <pages> 22-35, </pages> <year> 1978. </year>
Reference-contexts: A condensed flow graph. (b). The induced task dependence graph. c Since mapping has strong interaction with scheduling when we want to achieve the goal, it is usually viewed as a subproblem of scheduling. Several scheduling problems subject to precedence constraints have been shown to be N P-complete <ref> [LK78] </ref>. The usual approach to scheduling is to devise a set of heuristics to achieve a suboptimal but good schedule with a reasonable amount of effort. We propose two heuristics for scheduling our task systems. Mapping heuristic M1.
Reference: [LMR90] <author> Yong-fong Lee, Thomas J. Marlowe, and Barbara G. Ryder. </author> <title> Performing data flow analysis in parallel. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 942-951, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: This problem, in principle, could be alleviated by use of parallel data flow algorithms. Although parallel machines are increasingly available, current algorithms for data flow analysis are sequential. Recently, some researchers have designed parallel data flow algorithms <ref> [Zob90, LMR90, GPS90, KGS91] </ref>; preliminary empirical results were reported in [Zob90], [LMR91] and [LR92]. Although these results are encouraging, no one has proposed a comprehensive approach to performing data flow analysis in parallel.
Reference: [LMR91] <author> Yong-fong Lee, Thomas J. Marlowe, and Barbara G. Ryder. </author> <title> Experiences with a parallel algorithm for data flow analysis. </title> <journal> The Journal of Supercomputing, </journal> <volume> 5(2) </volume> <pages> 163-188, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: This problem, in principle, could be alleviated by use of parallel data flow algorithms. Although parallel machines are increasingly available, current algorithms for data flow analysis are sequential. Recently, some researchers have designed parallel data flow algorithms [Zob90, LMR90, GPS90, KGS91]; preliminary empirical results were reported in [Zob90], <ref> [LMR91] </ref> and [LR92]. Although these results are encouraging, no one has proposed a comprehensive approach to performing data flow analysis in parallel. <p> we build a prototype of the parallel data flow analyzer and experiment with different combinations of input programs and data flow problems, can we gain experience and understanding of the issues. 4 Parallel Hybrid Algorithms 4.1 Design We have designed a family of parallel hybrid algorithms for data flow analysis <ref> [LMR91] </ref> based on the general-purpose, hybrid algorithms [MR90]. The phases of a sequential hybrid algorithm are: 9 A flow-sensitive problem normally contains a subproblem of considering control flow within procedures. We can also exploit separate-unit parallelism in solving the subproblem. 15 1. Flow graph condensation. <p> Procedures for other applications or written in other languages such as C may have quite different characteristics. 6 Implementation and Findings We have reported our preliminary experiments on the parallel hybrid algorithm for Reaching Definitions in <ref> [LMR91] </ref>.
Reference: [LMSS91] <author> Jon Loeliger, Robert Metzger, Mark Seligman, and Sean Stroud. </author> <title> Pointer target tracking an empirical study. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 14-22, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Computation of precise data flow information needs to account for data flow effects across procedure boundaries. Since this kind of interprocedural analysis can be very time-consuming, it is often either not performed or performed approximately. An example of approximation is provided by Loeliger et al. in <ref> [LMSS91] </ref>. To facilitate vector-ization and parallelization of C programs, they perform an analysis called pointer target tracking that determines the range of memory locations potentially pointed to by a pointer during execution. This analysis is performed not only within individual procedures, but also across procedure boundaries.
Reference: [LR92] <author> Yong-fong Lee and Barbara G. Ryder. </author> <title> Parallel hybrid data flow algorithms: A case study. </title> <booktitle> In Conference Record of 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <publisher> Yale University, </publisher> <pages> pages 183-190, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Although parallel machines are increasingly available, current algorithms for data flow analysis are sequential. Recently, some researchers have designed parallel data flow algorithms [Zob90, LMR90, GPS90, KGS91]; preliminary empirical results were reported in [Zob90], [LMR91] and <ref> [LR92] </ref>. Although these results are encouraging, no one has proposed a comprehensive approach to performing data flow analysis in parallel.
Reference: [MP90] <author> S.P. Midkiff and D.A. Padua. </author> <title> Issues in the optimization of parallel programs. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, Vol.II, </booktitle> <pages> pages 105-113. </pages> <publisher> The Penn State University Press, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: The detection of parallelism is not the main issue in compiling explicitly parallel programs; nevertheless, compilers for such languages need even more precise and larger amount of data flow information to perform optimizations and further concurrentization <ref> [CK88, MP90] </ref>. These optimizations are normally aimed at reducing nonlocal data accesses in computer systems with distributed memories [HKT91] or hierarchical shared memories [GV91].
Reference: [MR90] <author> Thomas J. Marlowe and Barbara G. Ryder. </author> <title> An efficient hybrid algorithm for incremental data flow analysis. </title> <booktitle> In Conference Record of the 17th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 184-196. </pages> <publisher> ACM Press, </publisher> <address> January 1990. San Francisco, California. </address>
Reference-contexts: In practice, node splitting can be used to transform an irreducible flow graph into an equivalent, reducible one [Hec77]. 7 components and elimination-like in its propagation on the strong component condensation. It can handle irreducible flow graphs by application of fixed-point iteration within components <ref> [MR90] </ref>. In addition to the above general-purpose algorithms, there is a family of data flow solution procedures called the partitioned variable technique (PVT) [Zad84]. A PVT algorithm partitions a data flow problem by variables and finds the relevant information for a single variable, one at one time. <p> data flow analyzer and experiment with different combinations of input programs and data flow problems, can we gain experience and understanding of the issues. 4 Parallel Hybrid Algorithms 4.1 Design We have designed a family of parallel hybrid algorithms for data flow analysis [LMR91] based on the general-purpose, hybrid algorithms <ref> [MR90] </ref>. The phases of a sequential hybrid algorithm are: 9 A flow-sensitive problem normally contains a subproblem of considering control flow within procedures. We can also exploit separate-unit parallelism in solving the subproblem. 15 1. Flow graph condensation.
Reference: [MR91] <author> Thomas J. Marlowe and Barbara G. Ryder. </author> <title> Properties of data flow frameworks: A unified model. </title> <journal> Acta Informatica, </journal> <volume> 28(2) </volume> <pages> 121-164, </pages> <year> 1991. </year>
Reference-contexts: =&lt; G; L; F; M; &gt;, where G is a flow graph (N; E; ), L is usually a meet semilattice, F is a space of monotone functions mapping L into L, M is a mapping of edges E of G into F , and is an element of L <ref> [Hec77, MR91] </ref>. Intuitively, L is a lattice of data flow solutions, M the assignment of transition functions to nodes or edges, and the entry solution at ; F is usually not represented explicitly.
Reference: [Mye92] <author> Ware Myers. </author> <booktitle> Special report on Supercomputing '91. IEEE Computer, </booktitle> <pages> pages 87-90, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: At the ACM/IEEE Supercomputing'91 Conference, Alliant Computer Systems introduced the Campus/800, which has 800 processors. They claimed that the Campus/800 is the first system to support both distributed 8 and shared memory. They expected the system made it easier to program and more efficient for real-world applications <ref> [Mye92] </ref>; however, recent economic developments have dimmed the hopes for this system. 2.4 Computation Model We choose MIMD machines to be our machine model, because they are more appropriate for us to discover massive parallelism in data flow analysis.
Reference: [OW91] <author> T.J. Ostrand and E. Weyuker. </author> <title> Data flow based test adequecy analysis for languages with pointers. </title> <booktitle> In Proceedings of the 1991 Symposium on Software Testing, Analysis and Verification (TAV4), </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction 1.1 Motivation Data flow analysis is a compile-time analysis technique that gathers information about the flow of data in the program. Data flow information is essential in program development environments [CKT86a], testing <ref> [RW85, OW91] </ref>, debugging [Wei84], software maintenance [Ryd89], program integration [HPR89, YHR90], program parallelization [BC86, PW86], and compiler optimization fl This research was supported, in part, by National Science Foundation grants CCR-8920078 and CCR-9023628-1/5.
Reference: [Pol88] <author> Constantine D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: By speeding up the data flow solution process, we can compute more precise interprocedural data flow information, previously too time-consuming to calculate, and thus enable an optimizing compiler to perform more aggressive optimizations. Parallelizing compilers <ref> [ABC + 88, Pol88, ZBG88, Wol89] </ref>, like conventional optimizing compilers, collect data flow information for a source program and use this information to detect potential parallelism, determine an appropriate grain size, and then transform the program into a functionally equivalent program with a higher degree of parallelism.
Reference: [PW86] <author> David Padua and Michael Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: 1 Introduction 1.1 Motivation Data flow analysis is a compile-time analysis technique that gathers information about the flow of data in the program. Data flow information is essential in program development environments [CKT86a], testing [RW85, OW91], debugging [Wei84], software maintenance [Ryd89], program integration [HPR89, YHR90], program parallelization <ref> [BC86, PW86] </ref>, and compiler optimization fl This research was supported, in part, by National Science Foundation grants CCR-8920078 and CCR-9023628-1/5. An earlier version of this paper appears in Proceedings of the 6th ACM International Conference on Supercom puting, pp. 236-247, Washington, D.C., July 1992. y Email addresses: flee,ryderg@cs.rutgers.edu 1 [ASU86].
Reference: [RP86] <author> Barbara G. Ryder and Marvin C. Paull. </author> <title> Elimination algorithms for data flow analysis. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(3) </volume> <pages> 277-316, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Intuitively, L is a lattice of data flow solutions, M the assignment of transition functions to nodes or edges, and the entry solution at ; F is usually not represented explicitly. Often M can be thought of as specifying a system of jN j or jEj equations <ref> [RP86] </ref>. 2.2.1 Program Representation A program consists of one or more procedures; it is specified by a pair, Q = (S; A 1 ), where S is a set of procedures fA 1 ; A 2 ; : : : ; A b g, and A 1 is the main procedure. <p> Although iterative algorithms do not rely on the flow graph being reducible, they are normally not as efficient as elimination or hybrid algorithms. An elimination algorithm has two phases and is conceptually similar to Gaussian elimination <ref> [RP86] </ref>. Most elimination algorithms require the flow graph to be reducible. 5 In the elimination phase, the flow graph is partitioned into intervals (or regions), data flow information local to an interval is summarized, and an interval is condensed into a node.
Reference: [RW85] <author> Sandra Rapps and Elaine J. Weyuker. </author> <title> Selecting software test data using data flow information. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(4):367-375, </volume> <year> 1985. </year> <month> 34 </month>
Reference-contexts: 1 Introduction 1.1 Motivation Data flow analysis is a compile-time analysis technique that gathers information about the flow of data in the program. Data flow information is essential in program development environments [CKT86a], testing <ref> [RW85, OW91] </ref>, debugging [Wei84], software maintenance [Ryd89], program integration [HPR89, YHR90], program parallelization [BC86, PW86], and compiler optimization fl This research was supported, in part, by National Science Foundation grants CCR-8920078 and CCR-9023628-1/5.
Reference: [Ryd89] <author> Barbara G. Ryder. </author> <title> ISMM: Incremental software maintenance manager. </title> <booktitle> In Proceedings of the IEEE Computer Society Conference on Software Maintenance, </booktitle> <pages> pages 142-164. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1989. </year> <title> Miami, </title> <address> Florida. </address>
Reference-contexts: 1 Introduction 1.1 Motivation Data flow analysis is a compile-time analysis technique that gathers information about the flow of data in the program. Data flow information is essential in program development environments [CKT86a], testing [RW85, OW91], debugging [Wei84], software maintenance <ref> [Ryd89] </ref>, program integration [HPR89, YHR90], program parallelization [BC86, PW86], and compiler optimization fl This research was supported, in part, by National Science Foundation grants CCR-8920078 and CCR-9023628-1/5.
Reference: [Sar89] <author> Vivek Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <booktitle> Research Monographs in Parallel and Distributed Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: In the literature, the computation model is also called the macro-dataflow model <ref> [Sar89] </ref>.
Reference: [SO90] <author> Vatsa Santhanam and Daryl Odnert. </author> <title> Register allocation across procedure and module boundaries. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 28-39, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Lack of precise data flow information limits the tools that use it to ensure the correctness of program transformations. For example, an optimizing compiler cannot perform aggressive optimizations such as interprocedural constant propagation [CCKT86] and interprocedural register allocation <ref> [SO90] </ref>. A parallel programming tool cannot aid users as effectively in parallelizing their programs because of too many spurious data dependences [HHLS90]. Furthermore, without such information a debugger cannot focus one's attention on a reasonably small portion of code where errors may exist, especially when multiple procedures are involved [Wei84].
Reference: [SWJ + 88] <author> V. Seshadri, D.B. Wortman, M.D. Junkin, S. Weber, C.P. Yu, and I. </author> <title> Small. Semantic analysis in a concurrent compiler. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 233-240. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1988. </year> <institution> Atlanta, Georgia. </institution>
Reference-contexts: Our work is aimed at facilitating efficient static analysis on parallel machines; thus we are adding to the body of work addressing issues of compilation performed in parallel <ref> [SWJ + 88, GZZ89, Gaf90, BS90] </ref>. Interprocedural data flow analysis is rarely performed currently in sequential compilers, in part because of its expected cost.
Reference: [van89] <author> Andre M. van Tilborg. </author> <title> Panel on future directions in parallel computer architecture. </title> <journal> Computer Architecture News, </journal> <volume> 17(4) </volume> <pages> 3-22, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: In distributed-memory machines, such as the Intel iPSC/2 and the NCUBE/10, processors with their own local memories exchange information by explicitly sending and receiving messages. The overhead of message passing is a main issue for these machines. It has been observed <ref> [van89] </ref> that the distinction between these two families of MIMD machines is disappearing. For shared-memory machines, mechanisms such as a well-organized hierarchical memory system are introduced to hide the latency of memory access. In the future, synchronization may be handled more efficiently and combined with the memory access.
Reference: [Wei84] <author> Mark Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: 1 Introduction 1.1 Motivation Data flow analysis is a compile-time analysis technique that gathers information about the flow of data in the program. Data flow information is essential in program development environments [CKT86a], testing [RW85, OW91], debugging <ref> [Wei84] </ref>, software maintenance [Ryd89], program integration [HPR89, YHR90], program parallelization [BC86, PW86], and compiler optimization fl This research was supported, in part, by National Science Foundation grants CCR-8920078 and CCR-9023628-1/5. <p> A parallel programming tool cannot aid users as effectively in parallelizing their programs because of too many spurious data dependences [HHLS90]. Furthermore, without such information a debugger cannot focus one's attention on a reasonably small portion of code where errors may exist, especially when multiple procedures are involved <ref> [Wei84] </ref>. This problem, in principle, could be alleviated by use of parallel data flow algorithms. Although parallel machines are increasingly available, current algorithms for data flow analysis are sequential.
Reference: [Wol89] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: By speeding up the data flow solution process, we can compute more precise interprocedural data flow information, previously too time-consuming to calculate, and thus enable an optimizing compiler to perform more aggressive optimizations. Parallelizing compilers <ref> [ABC + 88, Pol88, ZBG88, Wol89] </ref>, like conventional optimizing compilers, collect data flow information for a source program and use this information to detect potential parallelism, determine an appropriate grain size, and then transform the program into a functionally equivalent program with a higher degree of parallelism.
Reference: [YHR90] <author> Wuu Yang, Susan Horwitz, and Thomas Reps. </author> <title> A program integration algorithm that accomodates semantics preserving transformations. </title> <booktitle> In Proceedings of the Fourth ACM SIGSOFT Symposium on Software Development Environments, </booktitle> <pages> pages 133-143. </pages> <publisher> ACM Press, </publisher> <month> December </month> <year> 1990. </year> <institution> Irvine, California. </institution>
Reference-contexts: 1 Introduction 1.1 Motivation Data flow analysis is a compile-time analysis technique that gathers information about the flow of data in the program. Data flow information is essential in program development environments [CKT86a], testing [RW85, OW91], debugging [Wei84], software maintenance [Ryd89], program integration <ref> [HPR89, YHR90] </ref>, program parallelization [BC86, PW86], and compiler optimization fl This research was supported, in part, by National Science Foundation grants CCR-8920078 and CCR-9023628-1/5.
Reference: [Zad84] <author> F.K. Zadeck. </author> <title> Incremental data flow analysis in a structured program editor. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 132-143. </pages> <publisher> ACM Press, </publisher> <address> June 1984. Montreal, Canada. </address>
Reference-contexts: It can handle irreducible flow graphs by application of fixed-point iteration within components [MR90]. In addition to the above general-purpose algorithms, there is a family of data flow solution procedures called the partitioned variable technique (PVT) <ref> [Zad84] </ref>. A PVT algorithm partitions a data flow problem by variables and finds the relevant information for a single variable, one at one time.
Reference: [ZBG88] <author> Zima, Bast, and Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6(1) </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: By speeding up the data flow solution process, we can compute more precise interprocedural data flow information, previously too time-consuming to calculate, and thus enable an optimizing compiler to perform more aggressive optimizations. Parallelizing compilers <ref> [ABC + 88, Pol88, ZBG88, Wol89] </ref>, like conventional optimizing compilers, collect data flow information for a source program and use this information to detect potential parallelism, determine an appropriate grain size, and then transform the program into a functionally equivalent program with a higher degree of parallelism.
Reference: [Zob90] <author> Angelika Zobel. </author> <title> Parallel interval analysis of data flow equations. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, Vol.II, </booktitle> <pages> pages 9-16. </pages> <publisher> The Penn State University Press, </publisher> <month> August </month> <year> 1990. </year> <month> 35 </month>
Reference-contexts: This problem, in principle, could be alleviated by use of parallel data flow algorithms. Although parallel machines are increasingly available, current algorithms for data flow analysis are sequential. Recently, some researchers have designed parallel data flow algorithms <ref> [Zob90, LMR90, GPS90, KGS91] </ref>; preliminary empirical results were reported in [Zob90], [LMR91] and [LR92]. Although these results are encouraging, no one has proposed a comprehensive approach to performing data flow analysis in parallel. <p> This problem, in principle, could be alleviated by use of parallel data flow algorithms. Although parallel machines are increasingly available, current algorithms for data flow analysis are sequential. Recently, some researchers have designed parallel data flow algorithms [Zob90, LMR90, GPS90, KGS91]; preliminary empirical results were reported in <ref> [Zob90] </ref>, [LMR91] and [LR92]. Although these results are encouraging, no one has proposed a comprehensive approach to performing data flow analysis in parallel. <p> Although iterative algorithms are simple in implementation, they do not allow as effective parallelization as elimination and hybrid algorithms. Straightforward parallelization of an elimination algorithm can simultaneously summarize (and propagate) data flow information in independent intervals; however, this approach offers no control over the flow graph partitioning <ref> [Zob90] </ref>. For improving performance, we can partition a flow graph into regions instead of intervals [Lee92]. We can refine a large interval into smaller regions to improve parallelism and load balancing, and cluster small intervals into a larger region to improve load balancing.
References-found: 56

