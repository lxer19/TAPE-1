URL: http://www.graphics.stanford.edu/projects/flashg/papers/dfb_sync.ps
Refering-URL: http://www.graphics.stanford.edu/projects/flashg/papers/
Root-URL: http://www.cs.stanford.edu
Title: Synchronization for a Multi-Port Frame Buffer on a Mesh-Connected Multicomputer  
Author: Bin Wei, Gordon Stoll, Douglas W. Clark, Edward W. Felten, and Kai Li Patrick Hanrahan 
Keyword: Key Words and Phrases: Parallel rendering, mesh routing network, multicomputers, multi-port frame buffer, synchronization.  
Affiliation: Princeton University  Stanford University  
Abstract: Parallel rendering on multicomputers involves the par-allelization of geometry processing, rasterization and composition. A known approach to support the back end of parallel rendering on multicomputers is to connect a multi-port frame buffer directly to the multicomputer routing network to take advantage of the aggregate bandwidth available on the network. However, a multi-port frame buffer design raises the question of how to synchronize the processors with the frame buffer in order to perform global control operations. The challenge is to provide a simple and efficient synchronization algorithm that requires minimal hardware support. This paper describes a software-based solution to the synchronization problem for a multi-port frame buffer on the Paragon mesh routing network. Simulations on the Paragon multicomputer show that our algorithm is indeed efficient. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Kurt Akeley. </author> <title> RealityEngine Graphics. </title> <booktitle> In Proceedings of SIGGRAPH '93. </booktitle> <address> (Anaheim, California, </address> <month> August 1-6, </month> <year> 1993), </year> <pages> pp. 109-116. </pages>
Reference-contexts: Related Work Multi-port frame buffers are used in many graphics machines. In the PixelFlow [19] machine, commands are issued by a parallel host computer which also generates frame synchronization commands. In pipeline-structured graphics machines <ref> [1, 10, 12] </ref>, control operations can be inserted serially into the graphics pipeline. These machines have special hardware support for synchronizing parallel pipeline stages. In some parallel graphics systems, such as that of the NCUBE/ten [2], general-purpose processors are associated with each port of the distributed frame buffer.
Reference: [2] <author> Robert Benner. </author> <title> Parallel Graphics Algorithms on a 1024-Processor Hypercube. </title> <booktitle> In The Fourth Conference on Hypercubes, Concurrent Computers, and Applications. </booktitle> <address> (Monterey, California, March 6-8, </address> <year> 1989), </year> <pages> pp. 133-140. </pages>
Reference-contexts: In pipeline-structured graphics machines [1, 10, 12], control operations can be inserted serially into the graphics pipeline. These machines have special hardware support for synchronizing parallel pipeline stages. In some parallel graphics systems, such as that of the NCUBE/ten <ref> [2] </ref>, general-purpose processors are associated with each port of the distributed frame buffer. These processors can participate in ordinary global barrier operation, making frame synchronization relatively easy. Using general-purpose processors, however, adds cost and complexity to the frame buffer design.
Reference: [3] <author> Matthias Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathan Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputers. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture. </booktitle> <address> (Chicago, Illinois, </address> <month> April 18-21, </month> <year> 1994), </year> <pages> pp. 142-153. </pages>
Reference-contexts: That paper however did not address the synchronization issue. 2 Design Overview We have designed a multi-port frame buffer for the Intel Paragon routing backplane. The frame buffer can be used in either the Paragon multicomputer or the SHRIMP multicomputer <ref> [3] </ref>, which uses the Paragon backplane to connect commodity PCs. Our design goal is to provide a frame buffer capable of sustaining the data rate produced by parallel rendering. We also aim to put into the frame buffer some functions to help with parallel rendering. <p> The unit of communication from processors to the frame buffer is a horizontal span made up of a contiguous sequence of pixels. For a machine which has effective support for small packet transfer such as SHRIMP <ref> [3] </ref>, one span per packet is efficient. For machines with high sending overhead, small packets are concatenated before sent to the frame buffer.
Reference: [4] <author> Avaika Networks Corporation. </author> <title> HiPPI Frame Buffer. Data Sheet, </title> <year> 1994. </year>
Reference-contexts: The synchronization overhead must be small compared to the time required to send a frame. Because there is no single channel that all data goes through, synchronization with multi-port frame buffers is quite different from the single-port case <ref> [4, 14] </ref> where the ordering of data through the port can always be known. The challenge is to provide a simple and efficient synchronization algorithm that requires minimal hardware support.
Reference: [5] <author> Brooktree Corporation. </author> <title> RAMDACs-Workstation Graphics. Section 5., Graphics and Imaging Product Databook, </title> <year> 1993. </year>
Reference-contexts: Each subframe is implemented on a separate printed circuit board which can be plugged into the Paragon routing backplane as if it were a Paragon compute node. One of the boards includes the output control logic and the true-color RAMDAC <ref> [5] </ref> which drives a standard 24-bit RGB display with a resolution of 1280 fi 1024 pixels at a 72Hz refresh rate. Compute nodes can send data to the frame buffer just as they do to other processors.
Reference: [6] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview. </title> <publisher> Intel Corporation, </publisher> <year> 1991. </year>
Reference-contexts: On today's multi-computers, frame buffers are normally attached through fl Department of Computer Science, Princeton NJ 88544. [bw,gws,doug,felten,li]@cs.princeton.edu y 127 Center for Integrated Systems, Stanford CA 94305. hanrahan@cs.stanford.edu I/O channels, and HiPPI frame buffers are commonly used <ref> [6, 7, 13, 17, 21] </ref>. The bandwidth available to the frame buffer thus is limited by the HiPPI channel itself, as well as by the I/O node which controls the HiPPI channel. For parallel rendering on existing multicomputers, this is a serious problem [8, 9, 11, 18, 23].
Reference: [7] <institution> Thinking Machines Corporation. </institution> <type> CM5 Technical Summary. </type> <institution> Thinking Machines Corporation, </institution> <year> 1993. </year>
Reference-contexts: On today's multi-computers, frame buffers are normally attached through fl Department of Computer Science, Princeton NJ 88544. [bw,gws,doug,felten,li]@cs.princeton.edu y 127 Center for Integrated Systems, Stanford CA 94305. hanrahan@cs.stanford.edu I/O channels, and HiPPI frame buffers are commonly used <ref> [6, 7, 13, 17, 21] </ref>. The bandwidth available to the frame buffer thus is limited by the HiPPI channel itself, as well as by the I/O node which controls the HiPPI channel. For parallel rendering on existing multicomputers, this is a serious problem [8, 9, 11, 18, 23].
Reference: [8] <author> Michael Cox. </author> <title> Algorithms for Parallel Rendering. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The bandwidth available to the frame buffer thus is limited by the HiPPI channel itself, as well as by the I/O node which controls the HiPPI channel. For parallel rendering on existing multicomputers, this is a serious problem <ref> [8, 9, 11, 18, 23] </ref>. Connecting a frame buffer directly to the multicom-puter's routing network can achieve better bandwidth. Scalability can be improved by connecting the frame buffer to the network using multiple ports.
Reference: [9] <author> Thomas Crockett. </author> <title> Design Considerations for Parallel Graphics Libraries. In Proceedings of Intel Supercomputer Users Group. </title> <address> (San Diego, California, </address> <month> June 26-29, </month> <year> 1994), </year> <pages> pp. 3-14. </pages>
Reference-contexts: The bandwidth available to the frame buffer thus is limited by the HiPPI channel itself, as well as by the I/O node which controls the HiPPI channel. For parallel rendering on existing multicomputers, this is a serious problem <ref> [8, 9, 11, 18, 23] </ref>. Connecting a frame buffer directly to the multicom-puter's routing network can achieve better bandwidth. Scalability can be improved by connecting the frame buffer to the network using multiple ports.
Reference: [10] <author> Michael Deering and Scott Nelson. Leo: </author> <title> A System for Cost Effective 3D Shaded Graphics. </title> <booktitle> In Proceedings of SIGGRAPH '93. </booktitle> <address> (Anaheim, California, </address> <month> August 1-6, </month> <year> 1993), </year> <pages> pp. 101-108. </pages>
Reference-contexts: Related Work Multi-port frame buffers are used in many graphics machines. In the PixelFlow [19] machine, commands are issued by a parallel host computer which also generates frame synchronization commands. In pipeline-structured graphics machines <ref> [1, 10, 12] </ref>, control operations can be inserted serially into the graphics pipeline. These machines have special hardware support for synchronizing parallel pipeline stages. In some parallel graphics systems, such as that of the NCUBE/ten [2], general-purpose processors are associated with each port of the distributed frame buffer.
Reference: [11] <author> David Ellsworth. </author> <title> A New Algorithm for Interactive Graphics on Multicomputers. </title> <journal> Computer Graphics and Applications, </journal> <month> July </month> <year> 1994, </year> <pages> pp. 33-40. </pages>
Reference-contexts: The bandwidth available to the frame buffer thus is limited by the HiPPI channel itself, as well as by the I/O node which controls the HiPPI channel. For parallel rendering on existing multicomputers, this is a serious problem <ref> [8, 9, 11, 18, 23] </ref>. Connecting a frame buffer directly to the multicom-puter's routing network can achieve better bandwidth. Scalability can be improved by connecting the frame buffer to the network using multiple ports.
Reference: [12] <author> Chandlee Harrell and Farhad Fouladi. </author> <title> Graphics Ren--dering Architecture for a High Performance Desktop Workstation. </title> <booktitle> In Proceedings of SIGGRAPH '93. </booktitle> <address> (Anaheim, California, </address> <month> August 1-6, </month> <year> 1993), </year> <pages> pp. 93-100. </pages>
Reference-contexts: Related Work Multi-port frame buffers are used in many graphics machines. In the PixelFlow [19] machine, commands are issued by a parallel host computer which also generates frame synchronization commands. In pipeline-structured graphics machines <ref> [1, 10, 12] </ref>, control operations can be inserted serially into the graphics pipeline. These machines have special hardware support for synchronizing parallel pipeline stages. In some parallel graphics systems, such as that of the NCUBE/ten [2], general-purpose processors are associated with each port of the distributed frame buffer.
Reference: [13] <author> CRAY Research Inc. </author> <title> T3D System Architecture Overview. </title> <year> 1993. </year>
Reference-contexts: On today's multi-computers, frame buffers are normally attached through fl Department of Computer Science, Princeton NJ 88544. [bw,gws,doug,felten,li]@cs.princeton.edu y 127 Center for Integrated Systems, Stanford CA 94305. hanrahan@cs.stanford.edu I/O channels, and HiPPI frame buffers are commonly used <ref> [6, 7, 13, 17, 21] </ref>. The bandwidth available to the frame buffer thus is limited by the HiPPI channel itself, as well as by the I/O node which controls the HiPPI channel. For parallel rendering on existing multicomputers, this is a serious problem [8, 9, 11, 18, 23].
Reference: [14] <author> Vineet Kumar. </author> <title> A Host Interface Architecture for HiPPI. </title> <booktitle> In Scalable High Performance Computing Conference. </booktitle> <address> (Knoxville, Tennessee, </address> <month> May 23-25, </month> <year> 1994), </year> <pages> pp. 142-149. </pages>
Reference-contexts: The synchronization overhead must be small compared to the time required to send a frame. Because there is no single channel that all data goes through, synchronization with multi-port frame buffers is quite different from the single-port case <ref> [4, 14] </ref> where the ordering of data through the port can always be known. The challenge is to provide a simple and efficient synchronization algorithm that requires minimal hardware support.
Reference: [15] <author> Kai Li, Jeffrey F. Naughton, and James S. Plank. </author> <title> An Efficient Checkpointing Method for Multicomputers with Wormhole Routing. </title> <journal> Intl. J. of Parallel Programming, </journal> <volume> 20(3) </volume> <pages> 159-180, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The key point is to prevent any blocked pre-sync packet from remaining in the network when any synchronization related message enters the frame buffer. We use a synchronization algorithm which depends on the knowledge of synchronization between compute nodes together with the network sweeping technique <ref> [15] </ref>. 4 The Synchronization Algorithm In this section, we present a synchronization algorithm by considering the frame buffer hardware, the software /* pre-sync data transfer */ synchronization of all compute nodes if (node x-coord == 0) f send pixel flush message to each frame buffer port if (node y-coord &gt; 0)
Reference: [16] <author> Eric Lawrence McDonald. </author> <title> A Video Controller and Distributed Frame Buffer for the J-Mchine. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <year> 1995. </year>
Reference-contexts: In this case, the price is the lack of generality of SIMD designs. Instead of distributed control, the frame buffer in the J-Machine uses a centralized video controller <ref> [16] </ref>. Thus synchronization between processing elements|message driven processors (MDPs)|and the frame buffer is handled by the video controller. The MDPs halt at a barrier synchronization until they receive a notice from the video controller.
Reference: [17] <author> Meiko. </author> <title> CS-2 Product Description. Meiko Computing Surface, </title> <year> 1993. </year>
Reference-contexts: On today's multi-computers, frame buffers are normally attached through fl Department of Computer Science, Princeton NJ 88544. [bw,gws,doug,felten,li]@cs.princeton.edu y 127 Center for Integrated Systems, Stanford CA 94305. hanrahan@cs.stanford.edu I/O channels, and HiPPI frame buffers are commonly used <ref> [6, 7, 13, 17, 21] </ref>. The bandwidth available to the frame buffer thus is limited by the HiPPI channel itself, as well as by the I/O node which controls the HiPPI channel. For parallel rendering on existing multicomputers, this is a serious problem [8, 9, 11, 18, 23].
Reference: [18] <author> S. Molnar and H. Fuchs. </author> <title> Advanced Raster Graphics Architecture. Chapter 18, Computer Graphics and Practice, Second Edition. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1990. </year>
Reference-contexts: The bandwidth available to the frame buffer thus is limited by the HiPPI channel itself, as well as by the I/O node which controls the HiPPI channel. For parallel rendering on existing multicomputers, this is a serious problem <ref> [8, 9, 11, 18, 23] </ref>. Connecting a frame buffer directly to the multicom-puter's routing network can achieve better bandwidth. Scalability can be improved by connecting the frame buffer to the network using multiple ports.
Reference: [19] <author> Steven Monlnar, J. Eyles, and J. Poulton. Pixelflow: </author> <title> High-Speed Rendering Using Image Composition. </title> <booktitle> In Proceedings of SIGGRAPH '92. </booktitle> <address> (Chicago, Illinois, </address> <month> July 26-31, </month> <year> 1992), </year> <pages> pp. 231-240. </pages>
Reference-contexts: We have simulated our algorithm on a 512-node Paragon multicomputer and showed that the overhead of our algorithm is small. Related Work Multi-port frame buffers are used in many graphics machines. In the PixelFlow <ref> [19] </ref> machine, commands are issued by a parallel host computer which also generates frame synchronization commands. In pipeline-structured graphics machines [1, 10, 12], control operations can be inserted serially into the graphics pipeline. These machines have special hardware support for synchronizing parallel pipeline stages.
Reference: [20] <author> Gordon Stoll, Bin Wei, Douglas Clark, Edward Felten, Kai Li, and Patrick Hanrahan. </author> <title> Evaluating Multi-Port Frame Buffer Designs for a Mesh-Connected Multicomputer. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture. </booktitle> <address> (Santa Margherita Ligure, Italy, </address> <month> June 22-24, </month> <year> 1995), </year> <pages> pp. 96-105. </pages>
Reference-contexts: Thus synchronization between processing elements|message driven processors (MDPs)|and the frame buffer is handled by the video controller. The MDPs halt at a barrier synchronization until they receive a notice from the video controller. A companion paper <ref> [20] </ref> reports our evaluations of design alternatives of multi-port frame buffers including how many network ports the frame buffer should receive input from, whether to perform Z-buffering in hardware in the frame buffer or in software on the computing nodes and whether the multi-port frame buffer architecture design alternatives are scalable. <p> This is accounted for by the run-to-run variance we observed; the negative values are less than zero by a statistically insignificant amount. When we applied the synchronization algorithm to the process of redistribution and software Z-buffering for several scenes <ref> [20] </ref>, the overhead was invisible in almost all configurations. We can deduce that the synchronization algorithm would not cause network congestion in real rendering applications.
Reference: [21] <author> C. B. Stunkel, D. G. Shea, B. Abali, M. M. Denneau, P. H. Hochschild, D. J. Joseph, B. J. Nathanson, M. Tsao, and P. R. Varker. </author> <booktitle> Architecture and Implementation of Vulcan. In Proceedings of the 8th International Parallel Processing Symposium. </booktitle> <address> (Cancun, Mexico, </address> <month> April 26-29, </month> <year> 1994), </year> <pages> pp. 268-274. </pages>
Reference-contexts: On today's multi-computers, frame buffers are normally attached through fl Department of Computer Science, Princeton NJ 88544. [bw,gws,doug,felten,li]@cs.princeton.edu y 127 Center for Integrated Systems, Stanford CA 94305. hanrahan@cs.stanford.edu I/O channels, and HiPPI frame buffers are commonly used <ref> [6, 7, 13, 17, 21] </ref>. The bandwidth available to the frame buffer thus is limited by the HiPPI channel itself, as well as by the I/O node which controls the HiPPI channel. For parallel rendering on existing multicomputers, this is a serious problem [8, 9, 11, 18, 23].
Reference: [22] <author> Roger Traylor and Dave Dunning. </author> <title> Routing Chip Set for Intel Paragon Parallel Supercomputer. </title> <booktitle> In Proceedings of Hot Chips '92 Symposium. (Standford, </booktitle> <address> California, </address> <month> August 9-11, </month> <year> 1992), </year> <note> [slide] 7.1.1-7.1.3. </note>
Reference-contexts: Hardware Z-buffering is implemented in the frame buffer to support hidden surface removal. The Paragon Mesh Routing Network An Intel Paragon multicomputer consists of a two-dimensional array of processing nodes connected by a routing network. Each point on the routing mesh contains an iMRC chip <ref> [22] </ref>; the iMRCs collectively allow messages to be routed between any two points in the mesh. Each iMRC has four bidirectional ports for connecting to neighboring iMRCs, and one bidirectional port for connecting to a compute node. The network uses wormhole routing, so it has very little buffering capability. <p> We do not consider layouts in which frame-buffer ports are mixed in with the processor ports, because this would require significant rewriting of the Paragon's operating system. To interface with the Paragon routing network, we use the Intel network interface chip (NIC) <ref> [22] </ref> that the Paragon compute nodes use. The frame buffer hardware is designed to accept data at the maximum network bandwidth. The frame buffer ports implement disjoint subframes which collectively cover the entire display space.
Reference: [23] <author> Scott Whitman. </author> <title> Multiprocessor Methods for Computer Graphics Rendering. </title> <type> AK Peters, Wellesley, </type> <year> 1992. </year>
Reference-contexts: The bandwidth available to the frame buffer thus is limited by the HiPPI channel itself, as well as by the I/O node which controls the HiPPI channel. For parallel rendering on existing multicomputers, this is a serious problem <ref> [8, 9, 11, 18, 23] </ref>. Connecting a frame buffer directly to the multicom-puter's routing network can achieve better bandwidth. Scalability can be improved by connecting the frame buffer to the network using multiple ports.
References-found: 23

