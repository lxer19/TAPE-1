URL: http://www.cs.toronto.edu/~mackay/pred.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: mackay@mrao.cam.ac.uk  
Title: Bayesian Non-linear Modelling for the Prediction Competition  
Author: David J.C. MacKay 
Note: An entry using the ARD model won the competition by a significant margin.  
Address: Cambridge, CB3 0HE. United Kingdom  
Affiliation: Cavendish Laboratory,  
Abstract: The 1993 energy prediction competition involved the prediction of a series of building energy loads from a series of environmental input variables. Non-linear regression using `neural networks' is a popular technique for such modeling tasks. Since it is not obvious how large a time-window of inputs is appropriate, or what preprocessing of inputs is best, this can be viewed as a regression problem in which there are many possible input variables, some of which may actually be irrelevant to the prediction of the output variable. Because a finite data set will show random correlations between the irrelevant inputs and the output, any conventional neural network (even with reg-ularisation or `weight decay') will not set the coefficients for these junk inputs to zero. Thus the irrelevant variables will hurt the model's performance. The Automatic Relevance Determination (ARD) model puts a prior over the regression parameters which embodies the concept of relevance. This is done in a simple and `soft' way by introducing multiple regularisation constants, one associated with each input. Using Bayesian methods, the regularisation constants for junk inputs are automatically inferred to be large, preventing those inputs from causing significant overfitting. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Box, G. and Tiao, G. </author> <year> (1973). </year> <title> Bayesian inference in statistical analysis, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Method A large number of neural nets were trained using the ARD model, for each of the prediction problems. The data seemed to include some substantial glitches. Because I had not yet developed an automatic Bayesian noise model that anticipates outliers (though this certainly could be done <ref> (Box and Tiao 1973) </ref>), I omitted by hand those data points which gave large residuals relative to the 1 The quantity equivalent to fl is fl c = k c Trace c (A 1 ), where the trace is over the parameters in class c, and k c is the number
Reference: <author> Breiman, L. </author> <year> (1992). </year> <title> Stacked regressions, </title> <type> Technical Report 367, </type> <institution> Dept. of Stat., Univ. of Cal. Berkeley. </institution>
Reference-contexts: The size of the committee was chosen so as to minimise the validation error of the mean predictions. This method of selecting committee size has also been described under the name `stacked generalization' <ref> (Breiman 1992) </ref>. In all cases, a committee was found that performed significantly better on the validation set than any individual model. The predictions and residuals are shown in figures 1-3. There are local trends in the testing data which the models were unable to predict.
Reference: <author> MacKay, D. </author> <year> (1992). </year> <title> A practical Bayesian framework for backpropagation networks, </title> <booktitle> Neural Computation 4(3): </booktitle> <pages> 448-472. </pages>
Reference-contexts: 1 Overview of Bayesian modeling methods A practical Bayesian framework for adaptive data modeling has been described in <ref> (MacKay 1992) </ref>. In this framework, the overall aim is to develop probabilistic models that are well matched to the data, and make optimal predictions with those models. Neural network learning, for example, is interpreted as an inference of the most probable parameters for a model, given the training data. <p> Bayesian inference for neural nets can be implemented numerically by a deterministic method involving Gaussian approximations, the `evidence' framework <ref> (MacKay 1992) </ref>, or by Monte Carlo methods (Neal 1993). The former framework is used here. Neural networks for regression A supervised neural network is a non-linear parameterised mapping from an input x to an output y = y (x; w). Here, the parameters of the net are denoted by w. <p> I summarise here the method for the case of a single regularisa-tion constant ff. As shown in <ref> (MacKay 1992) </ref>, the maximum evidence ff satisfies the following self-consistent equation: 1=ff = i i =fl (8) where w MP is the parameter vector which minimises the objective function M = fiE D + ffE W and fl is the `number of well-determined parameters', given by fl = kffTrace (A 1 <p> For large problems these calculations can be performed more efficiently (Skilling 1993). Automatic Relevance Determination The automatic relevance determination (ARD) model (MacKay and Neal 1994) is a Bayesian model which can be implemented with the methods described in <ref> (MacKay 1992) </ref>. Consider a regression problem in which there are many input variables, some of which are actually irrelevant to the prediction of the output variable.
Reference: <author> MacKay, D. and Neal, R. </author> <year> (1994). </year> <title> Automatic relevance determination for neural networks, </title> <note> Technical Report in preparation, </note> <institution> Cambridge University. </institution> <note> 15 Neal, </note> <author> R. </author> <year> (1993). </year> <title> Bayesian learning via stochastic dynamics, </title> <editor> in C. Giles, S. Han--son and J. Cowan (eds), </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 475-482. </pages>
Reference-contexts: In my work I have evaluated this matrix explicitly; this does not take a significant time if the number of parameters is small (a few hundred). For large problems these calculations can be performed more efficiently (Skilling 1993). Automatic Relevance Determination The automatic relevance determination (ARD) model <ref> (MacKay and Neal 1994) </ref> is a Bayesian model which can be implemented with the methods described in (MacKay 1992). Consider a regression problem in which there are many input variables, some of which are actually irrelevant to the prediction of the output variable.
Reference: <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors, </title> <booktitle> Nature 323: </booktitle> <pages> 533-536. </pages>
Reference-contexts: by minimising an error function, e.g., E D (w) = 2 m i t i y i (x (m) ; w) : (2) This function is minimised using some optimisation method that makes use of the gradient of E D , which can be evaluated using `backpropagation' (the chain rule) <ref> (Rumelhart, Hinton and Williams 1986) </ref>. Often, regularisation or `weight decay' is included, modifying the objective function to: M (w) = fiE D + ffE W (3) where E W = 1 2 i w 2 i .
Reference: <author> Skilling, J. </author> <year> (1993). </year> <title> Bayesian numerical analysis, </title> <editor> in W. G. Jr. and P. Milonni (eds), </editor> <title> Physics and Probability, C.U.P., </title> <address> Cambridge. </address> <month> 16 </month>
Reference-contexts: In my work I have evaluated this matrix explicitly; this does not take a significant time if the number of parameters is small (a few hundred). For large problems these calculations can be performed more efficiently <ref> (Skilling 1993) </ref>. Automatic Relevance Determination The automatic relevance determination (ARD) model (MacKay and Neal 1994) is a Bayesian model which can be implemented with the methods described in (MacKay 1992).
References-found: 6

