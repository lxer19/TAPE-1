URL: http://robotics.stanford.edu/users/ronnyk/teza.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: WRAPPERS FOR PERFORMANCE ENHANCEMENT AND OBLIVIOUS DECISION GRAPHS  
Author: Ron Kohavi 
Degree: a dissertation submitted to the department of computer science and the committee on graduate studies of stanford university in partial fulfillment of the requirements for the degree of doctor of philosophy By  
Date: September 1995  
Abstract-found: 0
Intro-found: 1
Reference: <editor> It is somewhat disturbing to have one's work both unreferenced and dismissed as invalid. |C. L. Mallows, </editor> <booktitle> discussion in Miller (1984) AAA (1992), Tenth National Conference on Artificial Intelligence, </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Unsupervised learning, exemplified by clustering methods (Duda & Hart 1973, Krish-naiah & Kanal 1982, Cheeseman et al. 1988), deals with discovering structure in unlabelled instances. Regression problems deal with learning a function mapping from unlabelled instances to a real-valued label <ref> (Breiman, Friedman, Olshen & Stone 1984, Draper & Smith 1981) </ref>. 1.2 Motivation The three most important motivating factors for supervised classification learning are: data mining, overcoming the knowledge acquisition bottleneck, and improving upon expert performance. CHAPTER 1. INTRODUCTION 4 dataset (described later). CHAPTER 1.
Reference: <author> Aha, D. W. </author> <year> (1992), </year> <title> "Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms", </title> <journal> International Journal of Man-Machine Studies 36(1), </journal> <pages> 267-287. </pages>
Reference: <author> Aha, D. W. & Bankert, R. L. </author> <year> (1994), </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison, </title> <booktitle> in "Working Notes of the AAAI-94 Workshop on Case-Based Reasoning", </booktitle> <pages> pp. 106-112. </pages>
Reference: <author> Aha, D. W. & Bankert, R. L. </author> <year> (1995), </year> <title> A comparative evaluation of sequential feature selection algorithms, </title> <editor> in D. Fisher & H. Lenz, eds, </editor> <booktitle> "Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics", </booktitle> <address> Ft. Lauderdale, FL, </address> <pages> pp. 1-7. </pages>
Reference-contexts: The work with the wrapper approach led to a large experiment to see which accuracy estimation method should be used (Kohavi 1995b). Recently, work on OODGs resumed with an experimental comparison of discretization methods to allow experiments on real datasets <ref> (Dougherty, Kohavi & Sahami 1995) </ref>, and a top-down induction algorithm (Kohavi & Li 1995). Chapter 6 combines most of the work CHAPTER 1. <p> Real-world domains are useful because they come from real-world problems that we do not always understand and are therefore actual problems on which we would like to improve performance. All real-world datasets used are from the UC Irvine repository <ref> (Murphy & Aha 1995) </ref>, which contains over 100 datasets mostly contributed by researchers in the field of machine learning 2 The real-world datasets were chosen based on the following criteria: dataset size, reasonable encoding, comprehensibility, non-triviality, and age. <p> To choose a set of datasets, we looked at the learning curves for C4.5 and Naive-Bayes for most of the supervised classification datasets at the UC Irvine repository <ref> (Murphy & Aha 1995) </ref> that contained more than 500 instances (about 25 such datasets). To ensure little variance, we chose datasets with at least 500 instances for testing. <p> In practice, we have not seen a phase transition using the C4.5 or Naive-Bayes algorithms on any of the datasets in the UC Irvine repository <ref> (Murphy & Aha 1995) </ref>; however, if they do occur between datasets of sizes m m=k and m, they are likely to cause the learning curves to cross, and hence the wrong model will be selected. <p> The algorithm starts from a decision tree and converts it to an OODG using the minimum description length principle (Rissanen 1986, Rissanen 1978). Their algorithm performed extremely well on many artificial domains but rather poorly on the real-domains from UC Irvine repository <ref> (Murphy & Aha 1995) </ref>. General decision graphs were investigated by Oliver, Dowe & Wallace (1992) and Oliver (1993). The algorithms construct decision graphs top-down, by doing a hill-climbing search through the space of graphs, estimating the usefulness of each graph by Wallace's MMLP (minimum message length principle).
Reference: <author> Aha, D. W., Kibler, D. & Albert, M. K. </author> <year> (1991), </year> <title> "Instance-based learning algorithms", </title> <booktitle> Machine Learning 6(1), </booktitle> <pages> 37-66. </pages>
Reference: <author> Akers, S. B. </author> <year> (1978), </year> <title> "Binary decision diagrams", </title> <journal> IEEE Transactions on Computers C-27(6), </journal> <pages> 509-516. </pages>
Reference: <author> Almuallim, H. & Dietterich, T. G. </author> <year> (1991), </year> <title> Learning with many irrelevant features, </title> <booktitle> in "Ninth National Conference on Artificial Intelligence", </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 547-552. </pages>
Reference-contexts: The main disadvantage of the filter approach is that it totally ignores the effects of the selected CHAPTER 4. WRAPPERS 86 feature subset on the performance of the induction algorithm. We now review some existing algorithms that fall into the filter approach. The FOCUS Algorithm The FOCUS algorithm <ref> (Almuallim & Dietterich 1991, Almuallim & Dietterich 1994) </ref>, originally defined for noise-free Boolean domains, exhaustively examines all subsets of features, selecting the minimal subset of features that is sufficient to determine the label value for all instances in the training set.
Reference: <author> Almuallim, H. & Dietterich, T. G. </author> <year> (1992a), </year> <title> Efficient algorithms for identifying relevant features, </title> <booktitle> in "Proceedings of the Ninth Canadian Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 38-45. </pages> <note> 249 BIBLIOGRAPHY 250 Almuallim, </note> <author> H. & Dietterich, T. G. </author> <year> (1992b), </year> <title> On learning more concepts, </title> <booktitle> in ML- (1992), </booktitle> <pages> pp. 11-19. </pages>
Reference: <author> Almuallim, H. & Dietterich, T. G. </author> <year> (1994), </year> <title> "Learning boolean concepts in the presence of many irrelevant features", </title> <booktitle> Artificial Intelligence 69(1-2), </booktitle> <pages> 279-306. </pages>
Reference: <author> Anderson, J. R. & Matessa, M. </author> <year> (1992), </year> <title> "Explorations of an incremental, bayesian algorithm for categorization", </title> <booktitle> Machine Learning 9, </booktitle> <pages> 275-308. </pages>
Reference: <author> Andrews, D. F., Bickel, P. J., Hampel, F. R., Huber, P. J., Rogers, W. H. & Tukey, J. W. </author> <year> (1972), </year> <title> Robust Estimates of Location, </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: Some studies show that for heavy tails, trimmed means are better, and the variance of the estimates is not much larger than the ordinary mean even if the underlying distribution is Gaussian <ref> (Andrews, Bickel, Hampel, Huber, Rogers & Tukey 1972) </ref>. In the following experiment, we evaluate the use of a trimmed mean. We experimented with two-fold and ten-fold cross-validation for both C4.5 and Naive-Bayes using an ff = 10% trimmed mean.
Reference: <author> Angluin, D. </author> <year> (1992), </year> <title> Computational learning theory: Survey and selected bibliography, </title> <booktitle> in "Proceedings of the 24th Annual ACM Symposium on the Theory of Computing", </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 351-369. </pages>
Reference: <author> Anscombe, F. J. </author> <year> (1967), </year> <title> "Topics in the investigation of linear relations fitted by the method of least squares", </title> <journal> Journal of the Royal Statistical Society B 29, </journal> <pages> 1-52. </pages>
Reference: <author> Babcock, C. </author> <year> (1994), </year> <title> "Parallel processing mines retail data", </title> <type> ComputerWorld. </type> <month> 26 Sep </month> <year> 1994. </year>
Reference-contexts: Fayyad et al. (to appear) give some examples of massive datasets created recently: 1. Wal-Mart, a U.S. retailer, created a database that handles over 20 million transactions a day <ref> (Babcock 1994) </ref>. 2. Mobil Oil Corporation, is developing a data warehouse capable of storing over 100 terabytes of data related to oil exploration (Harrison 1993). 3.
Reference: <author> Bahl, L. R., Brown, P. F., de Souza, P. V. & Mercer, R. L. </author> <year> (1989), </year> <title> "A tree-based statistical language model for natural language speech recognition", </title> <booktitle> IEEE Transactions on Acoustics, Speech, and Signal Processing 37(7), </booktitle> <pages> 1001-1008. </pages>
Reference: <author> Bai, C. </author> <year> (1988), </year> <title> Asymptotic properties of some samples reuse methods for prediction and classification, </title> <type> PhD thesis, </type> <institution> Univesrity of California, </institution> <address> San Diego. </address>
Reference: <author> Bailey, T. L. & Elkan, C. </author> <year> (1993), </year> <title> Estimating the accuracy of learned concepts, </title> <booktitle> in "Proceedings of International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 895-900. </pages>
Reference: <author> Barrington, D. A. </author> <year> (1989), </year> <title> "Bounded-width polynomial-size branching programs recognize exactly those languages in NC 1 ", Journal of Computer and System Sciences 38(1), </title> <type> 150-164. </type>
Reference-contexts: Two important theorems tell us that an algorithm in SPACE (S (n)) for S (n) log n has a branching program complexity of at most c S (n) for some constant c (Masek 1976), and that constant-width branching programs are very powerful, being able to accept all NC 1 languages <ref> (Barrington 1989) </ref>. Krause & Waack (1991) studied decision graphs of linear depth and gave exponential lower bounds on several graph accessibility problems. Meinel, Krause & Waack (1988) showed the equivalence of read-once decision graphs with a log n-space bounded eraser Turing machine, which has a special read-once-only input tape.
Reference: <author> Ben-Bassat, M. </author> <year> (1982), </year> <title> Use of distance measures, information measures and error bounds in feature evaluation, </title> <editor> in P. R. Krishnaiah & L. N. Kanal, eds, </editor> <booktitle> "Handbook of Statistics", </booktitle> <volume> Vol. 2, </volume> <publisher> North-Holland Publishing Company, </publisher> <pages> pp. 773-791. </pages> <note> BIBLIOGRAPHY 251 Berliner, </note> <author> H. </author> <year> (1981), </year> <title> The B fl tree search algorithm: A best-first proof procedure, </title> <editor> in B. Web-ber & N. Nilsson, eds, </editor> <booktitle> "Readings in Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 79-87. </pages>
Reference: <author> Binder, K. & Heerman, D. W. </author> <year> (1988), </year> <title> Monte Carlo Simulation in Statistical Physics : an Introduction, </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The cross-validation estimate commonly used chooses a single split of the data into k folds, thus approximating complete k-fold cross-validation with k estimates having disjoint test sets. Executing cross-validation multiple times, each time with a different split into the k folds, can viewed as a Monte-Carlo estimation <ref> (Binder & Heerman 1988) </ref> to complete k-fold cross-validation, which is usually too expensive to run. 3 Repeating cross-validation multiple times will not change the bias inherent in the method but it might change the variance of the estimates. 3 One reviewer asked if we ever tried running complete cross-validation to show
Reference: <author> Blum, A. L. & Rivest, R. L. </author> <year> (1992), </year> <title> "Training a 3-node neural network is NP-complete", </title> <booktitle> Neural Networks 5, </booktitle> <pages> 117-127. </pages>
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D. & Warmuth, M. K. </author> <year> (1987), </year> <title> "Occam's razor", </title> <journal> Information Processing Letters 24, </journal> <pages> 377-380. </pages>
Reference-contexts: Preference bias This bias places a preference ordering on hypotheses. Many times the preference ordering is defined by how the search through the space of hypotheses is conducted. Most preference biases attempt to minimize some measure of syntactic complexity, following Occam's Razor principle of preferring simpler hypotheses <ref> (Blumer, Ehrenfeucht, Haussler & Warmuth 1987) </ref>. Most decision tree induction algorithms restrict the hypothesis space to the space of finite trees that conduct threshold splits on continuous features and equality or subset splits on discrete features.
Reference: <author> Boddy, M. & Dean, T. </author> <year> (1989), </year> <title> Solving time-dependent planning problems, </title> <editor> in N. S. Srid-haran, ed., </editor> <booktitle> "Proceedings of the Eleventh International Joint Conference on Artificial Intelligence", </booktitle> <volume> Vol. 2, </volume> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 979-984. </pages>
Reference-contexts: Best-first search usually terminates upon reaching the goal. Our problem is an optimization problem, so the search can be stopped at any point and the best solution found so far can be returned (theoretically improving over time), thus making it an anytime algorithm <ref> (Boddy & Dean 1989) </ref>. In practice, we must stop the run at some stage, and we use what we call a stale search: if we have not found a new best node in the last k expansions, we terminate the search. <p> Decreasing the stale parameter from five to two will speed all runs significantly, with little loss of accuracy. The running times for HOODG-Entropy are much faster: 47 minutes for DNA, and under three minutes for all the other datasets. One can use HOODG-Middle as an anytime algorithm <ref> (Boddy & Dean 1989) </ref> and let it run as long as time permits. The initial guess is quite good, but may improve as in the soybean-large dataset. 6 Speed is always relative.
Reference: <author> Bollig, B. & Wegener, I. </author> <year> (1994), </year> <title> Improving the variable ordering of OBDDs is NP-complete, </title> <type> Technical Report 542, </type> <institution> Universitat Dortmund. </institution>
Reference: <author> Boole, G. </author> <title> (1854), An investigation of the laws of thought, on which are founded the theories of logic and probabilities, </title> <publisher> London, Walton and Maberly; Macmillan and Co. Reprinted by Dover Books, </publisher> <address> New York, </address> <year> 1954. </year>
Reference: <author> Boppana, R. B., & Sipser, M. </author> <year> (1990), </year> <title> The complexity of finite functions, </title> <editor> in J. v. Leeuwen, ed., </editor> <booktitle> "Handbook of Theoretical Computer Science", </booktitle> <publisher> MIT Press, </publisher> <pages> chapter 14, pp. 758-804. </pages>
Reference-contexts: In the computer science theory community, binary decision graphs have been called branching programs, and studied extensively in the hope of separating some complexity classes and for studying the amount of space needed to compute various functions <ref> (Boppana, & Sipser 1990) </ref>.
Reference: <author> Brachman, R. J. </author> <year> (1987), </year> <title> "The myth of the one true logic", </title> <booktitle> Computational Intelligence 3, </booktitle> <pages> 168-172. </pages> <note> Response to Drew McDermott's critique of pure reason. </note>
Reference: <author> Brazdil, P., Gama, J. & Henery, B. </author> <year> (1994), </year> <title> Characterizing the applicability of classification algorithms using meta-level learning, </title> <editor> in F. Bergadano & L. D. Raedt, eds, </editor> <booktitle> "Proceedings of the European Conference on Machine Learning". </booktitle>
Reference: <author> Breiman, L. </author> <year> (1994a), </year> <institution> Bagging predictors, Technical Report Statistics Department, University of California at Berkeley. </institution> <note> BIBLIOGRAPHY 252 Breiman, </note> <author> L. </author> <year> (1994b), </year> <title> Heuristics of instability in model selection, </title> <institution> Technical Report Statistics Department, University of California at Berkeley. </institution>
Reference: <author> Breiman, L. & Spector, P. </author> <year> (1992), </year> <title> "Submodel selection and evaluation in regression. the x-random case", </title> <journal> International Statistical Review 60(3), </journal> <pages> 291-319. </pages>
Reference-contexts: An induction algorithm overfits the dataset if it models the given data too well and its CHAPTER 4. WRAPPERS 115 predictions are poor. An example of an over-specialized hypothesis, or classifier, is a lookup table on all the features. Overfitting is closely related to the bias-variance tradeoff <ref> (Geman et al. 1992, Breiman et al. 1984) </ref>: if the algorithm fits the data too well, the variance term is large, and hence the overall error is increased.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Unsupervised learning, exemplified by clustering methods (Duda & Hart 1973, Krish-naiah & Kanal 1982, Cheeseman et al. 1988), deals with discovering structure in unlabelled instances. Regression problems deal with learning a function mapping from unlabelled instances to a real-valued label <ref> (Breiman, Friedman, Olshen & Stone 1984, Draper & Smith 1981) </ref>. 1.2 Motivation The three most important motivating factors for supervised classification learning are: data mining, overcoming the knowledge acquisition bottleneck, and improving upon expert performance. CHAPTER 1. INTRODUCTION 4 dataset (described later). CHAPTER 1. <p> CART <ref> (Breiman et al. 1984) </ref> and C4.5 (Quinlan 1993) are decision tree inducers that build decision tree classifiers. We now formally define the symbols used. Appendix F contains a summary of these symbols. Let the domain of feature X i be Dom (X i ). <p> After the test is chosen, the instances are split according to the test, and the subproblems are solved recursively. C4.5 uses gain ratio, a variant of mutual information, as the feature selection measure; other measures have been proposed, such as the Gini index <ref> (Breiman et al. 1984) </ref>, C-separators (Fayyad & Irani 1992), distance-based measures (De Mantaras 1991), and Relief (Kononenko 1995b). <p> In one case (the corral database described below), this had a significant impact on the resulting tree: although the root split was incorrect, it was replaced by one of the children. CART <ref> (Breiman et al. 1984) </ref> also builds decision trees top-down and prunes them, but the pruning mechanism is drastically different. Pruning in CART is done using ten-fold stratified cross-validation, and hence the algorithm runs an order of magnitude slower. <p> While an unbiased method is important to estimate future performance, the large variations in the results may deem an unbiased method inferior to a biased one with lower variance. When accuracy estimation is used for model selection, such as selecting the right amount of pruning for a decision tree <ref> (Breiman et al. 1984) </ref> or early stopping for neural nets (Finnoff, Hergert & Zimmermann 1993), we are interested in the difference between two classifiers, and the variance may be even more important because if the bias for the two models is equal, they cancel out. 3.4.1 The Bias In this section, <p> Because the wrapper approach optimizes any desired function, such costs can easily be taken into account in the evaluation function. 4.10.2 Parameter Tuning Parameter tuning is a widely-studied problem in statistics. CART <ref> (Breiman et al. 1984) </ref> is a prime example of the automatic setting of a parameter (the cost-complexity parameter) in decision tree induction. CART uses stratified ten-fold cross-validation to set this parameter.
Reference: <author> Breitbart, Y., Hunt III, H. B. & Rosenkrantz, D. </author> <year> (1991), </year> <title> The size of binary decision diagrams representing boolean functions, </title> <note> Submitted. </note>
Reference: <author> Bryant, R. E. </author> <year> (1986), </year> <title> "Graph-based algorithms for boolean function manipulation", </title> <journal> IEEE Transactions on Computers C-35(8), </journal> <pages> 677-691. </pages>
Reference-contexts: OODGs retain most of the advantages of decision trees, while overcoming the replication problem mentioned above; the induction algorithm avoids the inherent fragmentation in recursive partitioning. OODGs are similar to Ordered Binary Decision Diagrams (OBDDs) <ref> (Bryant 1986) </ref>, which have been used in the engineering community to represent state-graph models of systems, allowing verification of finite-state systems with up to 10 120 states. 2 The chapter is organized as follows. In Section 6.2, we formally define OODGs and some variants. <p> The oblivious and read-once restrictions together are equivalent to defining a total ordering on the features, and hence binary OODGs are isomorphic to Ordered Binary Decision Diagrams (OBDDs) <ref> (Bryant 1986) </ref>, which will be discussed further in the related work 3 The fi notation indicates upper and lower asymptotic bounds up to constant factors. For example, m could be n=2. CHAPTER 6. OBLIVIOUS READ-ONCE DECISION GRAPHS 153 section (Section 6.7 on page 182).
Reference: <author> Bryant, R. E. </author> <year> (1992), </year> <title> "Symbolic boolean manipulation with ordered binary-decision diagrams", </title> <journal> ACM Computing Surveys 24(3), </journal> <pages> 293-318. </pages>
Reference: <author> Buchanan, B. & Smith, R. </author> <year> (1988), </year> <title> "Fundamentals of expert systems", </title> <booktitle> Annual Review of Computer Science. </booktitle>
Reference-contexts: The ability to extract interesting information and understand the data is of vital importance. The field of data mining is now growing rapidly with the increased need for smart data warehouses. CHAPTER 1. INTRODUCTION 6 1.2.2 Knowledge Acquisition Bottleneck For knowledge itself is power. |Francis Bacon (1561-1626) Expert systems <ref> (Feigenbaum, McCorduck & Nii 1988, Buchanan & Smith 1988) </ref> solve problems that are normally solved by human experts. To solve expert-level problems, expert systems need to build a large knowledge base, a task usually assigned to a knowledge engineer.
Reference: <author> Buntine, W. </author> <year> (1992), </year> <title> "Learning classification trees", </title> <journal> Statistics and Computing 2, </journal> <pages> 63-73. </pages>
Reference: <author> Burch, J. R., Clarke, E. M. & Long, D. E. </author> <year> (1991), </year> <title> Representing circuits more efficiently in symbolic model checking, </title> <booktitle> in "Proceedings of the 28th ACM/IEEE Design Automation Conference", </booktitle> <pages> pp. 403-407. </pages>
Reference: <author> Burch, J. R., Clarke, E. M., McMillan, K. L., Dill, D. L. & Hwang, L. J. </author> <year> (1990), </year> <title> Symbolic model checking: 10 20 states and beyond, </title> <booktitle> in "Fifth Annual IEEE Symposium on Logic in Computer Science.", </booktitle> <publisher> IEEE Comput. Soc. Press, </publisher> <pages> pp. 428-439. </pages>
Reference: <author> Cardie, C. </author> <year> (1993), </year> <title> Using decision trees to improve case-based learning, </title> <booktitle> in "Proceedings of the Tenth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 25-32. </pages>
Reference: <author> Caruana, R. & Freitag, D. </author> <year> (1994), </year> <title> Greedy attribute selection, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Mor-gan Kaufmann Publishers, Inc. BIBLIOGRAPHY 253 Caruana, </publisher> <editor> R. A. </editor> <year> (1993), </year> <title> Multitask learning: A knowledge-based source of inductive bias, </title> <booktitle> in "Proceedings of the Tenth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 41-48. </pages>
Reference: <author> Catlett, J. </author> <year> (1991a), </year> <title> Megainduction: machine learning on very large databases, </title> <type> PhD thesis, </type> <institution> Univeristy of Sydney. </institution> <note> Available from http://www.research.att.com/orgs/ssr/people/catlett/phd.ps.Z. </note>
Reference: <author> Catlett, J. </author> <year> (1991b), </year> <title> On changing continuous attributes into ordered discrete attributes, </title> <editor> in Y. Kodratoff, ed., </editor> <booktitle> "Proceedings of the European Working Session on Learning", </booktitle> <address> Berlin, Germany: </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 164-178. </pages>
Reference: <author> Cendrowska, J. </author> <year> (1987), </year> <title> "PRISM: an algorithm for inducing modular rules", </title> <journal> International Journal of Man-Machine Studies 27, </journal> <pages> 349-370. </pages>
Reference-contexts: Although it was very accurate, the tree was large, obscure, and the chess master was in a "total blackout." The tree structure and existing algorithms for inducing decision trees suffer from some well-known problems, most notably the replication problem and the fragmentation problem <ref> (Cendrowska 1987, Pagallo & Haussler 1990) </ref>. The replication problem is exemplified by the concept (A ^ B) _ (C ^ D) shown in CHAPTER 6. OBLIVIOUS READ-ONCE DECISION GRAPHS 143 grey nodes are isomorphic. CHAPTER 6.
Reference: <author> Cestnik, B. </author> <year> (1990), </year> <title> Estimating probabilities: A crucial task in machine learning, </title> <editor> in L. </editor> <address> C. </address>
Reference: <editor> Aiello, ed., </editor> <booktitle> "Proceedings of the ninth European Conference on Artificial Intelligence", </booktitle> <pages> pp. 147-149. </pages>
Reference: <author> Chakravarty, S. </author> <year> (1993), </year> <title> "A characterization of binary decision diagrams", </title> <journal> IEEE Transactions on Computers 42(2), </journal> <pages> 129-137. </pages>
Reference: <author> Cheeseman et al. </author> <year> (1988), </year> <title> AutoClass: a bayesian classification system, </title> <booktitle> in "Proceedings of the Fifth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 54-64. </pages> <note> Also appears in Readings in Machine Learning by Shavlik and Dietterich. </note>
Reference: <author> Chou, P. A. </author> <year> (1988), </year> <title> Application of information theory to pattern recognition and the design of decision trees and trellises, </title> <type> PhD thesis, </type> <institution> Stanford University. </institution>
Reference: <author> Chou, P. A. </author> <year> (1991), </year> <title> "Optimal partitioning for classification and regression trees", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 13(4), </journal> <pages> 340-354. </pages>
Reference: <author> Clark, P. & Boswell, R. </author> <year> (1991), </year> <title> Rule induction with CN2: Some recent improvements, </title> <editor> in Y. Kodratoff, ed., </editor> <booktitle> "Proceedings of the fifth European conference (EWSL-91)", </booktitle> <publisher> Springer Verlag, </publisher> <pages> pp. 151-163. </pages> <note> BIBLIOGRAPHY 254 Clark, </note> <author> P. & Matwin, S. </author> <year> (1993), </year> <title> Using qualitative models to guide inductive learning, </title> <booktitle> in "Proceedings of the Tenth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 45-56. </pages>
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989), </year> <title> "The CN2 induction algorithm", </title> <booktitle> Machine Learning 3(4), </booktitle> <pages> 261-283. </pages>
Reference-contexts: C4.5 The C4.5 algorithm with default parameter settings. C4.5-AP C4.5 with automatic parameter tuning as described in Section 4.9 on page 119. C4.5-FSS C4.5 with a backwards best-first search wrapper with compound operators (Sec tion 4.6). C4.5-rules C4.5 in rule generation mode Quinlan (1993). CN2 The CN2 V6.1 algorithm <ref> (Clark & Niblett 1989, Clark & Boswell 1991) </ref>, which induces decision rules. HOODG The HOODG-Middle algorithm described in Chapter 6 on page 141. IB1,IB4 The instance based algorithms described in Aha (1992) (version from 3/9/94).
Reference: <author> Cormen, T. H., Leiserson, C. E. & Rivest, R. L. </author> <year> (1990), </year> <title> Introduction to Algorithms, </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: We now describe the data structures that allow fast incremental operations on DTMs. The underlying data structure that we use is a universal hash table <ref> (Cormen, Leiserson & Rivest 1990) </ref>.
Reference: <author> Cortes, C., Jackel, L. D., Solla, S. A., Vapnik, V. & Denker, J. S. </author> <year> (1994), </year> <title> Learning curves: Asymptotic values and rate of convergence, </title> <editor> in J. D. Cowan, G. Tesauro & J. Alspector, eds, </editor> <booktitle> "Advances in Neural Information Processing Systems", </booktitle> <volume> Vol. 6, </volume> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 327-334. </pages>
Reference: <author> Cover, T. M. & Campenhout, J. M. V. </author> <year> (1977), </year> <title> "On the possible orderings in the measurement selection problem", </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics SMC-7(9), </journal> <pages> 657-661. </pages>
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1993), </year> <title> Learning symbolic rules using artificial neural networks, </title> <booktitle> in "Proceedings of the Tenth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 73-80. </pages>
Reference-contexts: The smallest decision trees for most symmetric functions|functions which yield the same value for all permutations of the input features|are exponentially-sized. Symmetric functions, such as m-of-n, are known to occur in medical domains (Spackman 1988) and were useful in converting neural-networks to decision rules <ref> (Towell & Shavlik 1993, Craven & Shavlik 1993) </ref>.
Reference: <author> Crawford, S. L. </author> <year> (1989), </year> <title> "Extensions to the CART algorithm", </title> <journal> International Journal of Man-Machine Studies 31, </journal> <pages> 197-217. </pages>
Reference: <author> Dasarathy, B. V. </author> <year> (1990), </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California. </address>
Reference: <author> De Mantaras, R. L. </author> <year> (1991), </year> <title> "A distance-based attribute selection measure for decision tree induction", </title> <booktitle> Machine Learning 6, </booktitle> <pages> 81-92. </pages> <editor> desJardins, M. </editor> <year> (1994), </year> <title> How to be a good graduate student and advisor, </title> <note> Available from marie@erg.sri.com. </note>
Reference-contexts: C4.5 uses gain ratio, a variant of mutual information, as the feature selection measure; other measures have been proposed, such as the Gini index (Breiman et al. 1984), C-separators (Fayyad & Irani 1992), distance-based measures <ref> (De Mantaras 1991) </ref>, and Relief (Kononenko 1995b).
Reference: <author> Devijver, P. A. & Kittler, J. </author> <year> (1982), </year> <title> Pattern Recognition: A Statistical Approach, Prentice-Hall International. BIBLIOGRAPHY 255 Dietterich, </title> <editor> T. G. </editor> <year> (1986), </year> <title> "Learning at the knowledge level", </title> <booktitle> Machine Learning 1(3), </booktitle> <pages> 287-315. </pages> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: WRAPPERS 124 Points below the 45 degree line indicate that C4.5-AP is outperforming C4.5. 4.10.1 Feature Subset Selection The pattern recognition literature <ref> (Devijver & Kittler 1982, Kittler 1986, Ben-Bassat 1982) </ref>, statistics literature (Draper & Smith 1981, Miller 1984, Miller 1990, Neter et al. 1990), and recent machine learning papers (Almuallim & Dietterich 1991, Almuallim & Dietterich 1994, Kira & Rendell 1992a, Kira & Rendell 1992b, Kononenko 1994) consist of many such measures for
Reference: <author> Dietterich, T. G. & Shavlik, J. W., </author> <booktitle> eds (1990), Readings in Machine Learning, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: INTRODUCTION 2 the topic of this dissertation. Empirical learning is "accomplished by reasoning from externally supplied examples to produce general rules" <ref> (Dietterich & Shavlik 1990, p. 1) </ref>. In the inductive learning domain, we work on supervised classification learning problems. Figure 1.1 shows the learning hierarchy just described and related areas.
Reference: <author> Dietterich, T., Hild, H. & Bakiri, G. </author> <year> (1995), </year> <title> "A comparison of id3 and backpropagation for english text-to-speech mapping", </title> <booktitle> Machine Learning 18(1), </booktitle> <pages> 51-80. </pages>
Reference: <author> Doak, J. </author> <year> (1992), </year> <title> An evaluation of feature selection methods and their application to computer security, </title> <type> Technical Report CSE-92-18, </type> <institution> University of California at Davis. </institution>
Reference: <author> Dougherty, J., Kohavi, R. & Sahami, M. </author> <year> (1995), </year> <title> Supervised and unsupervised discretization of continuous features, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Twelfth International Conference", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: The work with the wrapper approach led to a large experiment to see which accuracy estimation method should be used (Kohavi 1995b). Recently, work on OODGs resumed with an experimental comparison of discretization methods to allow experiments on real datasets <ref> (Dougherty, Kohavi & Sahami 1995) </ref>, and a top-down induction algorithm (Kohavi & Li 1995). Chapter 6 combines most of the work CHAPTER 1.
Reference: <author> Draper, N. R. & Smith, H. </author> <year> (1981), </year> <title> Applied Regression Analysis, 2nd edn, </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973), </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley. </publisher>
Reference: <author> Dvorak, V. </author> <year> (1992), </year> <title> An optimization technique for ordered (binary) decision diagrams, </title> <editor> in P. Dewilde & J. Vandewalle, eds, </editor> <booktitle> "Compeuro Proceedings. Computer Systems and Software Engineering", </booktitle> <publisher> IEEE Comput. Soc. Press, </publisher> <pages> pp. 1-4. </pages>
Reference: <author> Efron, B. </author> <year> (1979), </year> <title> "Bootstrap methods: another look at the jacknife", </title> <booktitle> Anals of Statistics 7(1), </booktitle> <pages> 1-26. </pages>
Reference-contexts: Leave-one-out on such a dataset with 50% of the labels for each class and a majority inducer (the best possible inducer) would still predict 0% accuracy. 3.2.4 Bootstrap The bootstrap family was introduced by <ref> (Efron 1979) </ref> and is fully described in Efron & Tibshirani (1993). Mammen (1992) studies bootstrap for non-parametric curve estimation and linear models. Given a dataset of size m, a bootstrap sample is created by sampling m instances uniformly from the data (with replacement).
Reference: <author> Efron, B. </author> <year> (1983), </year> <title> "Estimating the error rate of a prediction rule: improvement on cross-validation", </title> <journal> Journal of the American Statistical Association 78(382), </journal> <pages> 316-330. </pages>
Reference-contexts: For non-parametric estimation, the .632 bootstrap was claimed to be the best <ref> (Efron 1983) </ref>. CHAPTER 3. ACCURACY ESTIMATION 46 The variance of the estimate can be determined by computing the variance of the estimates for the samples. One of the main questions in using bootstrap is how many bootstrap samples to use. <p> Because only .632 of the instances in the dataset are used for training, the *0 bootstrap estimate is closely related to two-fold cross-validation and Efron suggests that 2-CV might be similarly corrected with the resubstitution accuracy <ref> (Efron 1983) </ref>. The assumption made by bootstrap is basically the same as that of cross-validation, i.e., stability of the algorithm on the dataset: the "bootstrap world" should closely approximate the real world. In many cases, it is obvious that the bootstrap worlds are inappropriate.
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1991), </year> <title> "Statistical data analysis in the computer age", </title> <booktitle> Science 253, </booktitle> <pages> 390-395. </pages>
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1993), </year> <title> An Introduction to the Bootstrap, </title> <publisher> Chapman & Hall. </publisher>
Reference-contexts: Regrettably, many researchers in the field claim the significance of their results by increasing k until the difference, as small as it it, is made significant. To compute the variance, we recommend using the percentile method <ref> (Efron & Tibshirani 1993, pp. 168-176) </ref>. A 1 2ff confidence interval is defined by taking the ff and 1 ff quantiles of the estimates. This procedure requires that k be large, say at least 50, so CHAPTER 3.
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1995), </year> <title> Cross-validation and the bootstrap: Estimating the error rate of a prediction rule, </title> <type> Technical Report 477, </type> <institution> Stanford University. </institution> <note> BIBLIOGRAPHY 256 Ergun, </note> <author> F., Kumar, S. R. & Rubinfeld, R. </author> <year> (1995), </year> <title> On learning bounded-width branching programs, </title> <booktitle> in "Proceedings of the Eighth Annual Conference on Computational Learning Theory", ACM, </booktitle> <publisher> Inc, </publisher> <pages> pp. 361-368. </pages>
Reference-contexts: They have recently proposed a modified rule, called the 632+ rule <ref> (Efron & Tibshirani 1995) </ref>, which attempts to correct for the bias exhibited in Figures 3.10 and 3.11 by changing the .632 factor based on the * 0 estimated accuracy. The new rule also works for perfect memorizers and random data, where the .632 bootstrap fails.
Reference: <author> Esposito, F., Malerba, D. & Semeraro, G. </author> <year> (1995a), </year> <title> A further study of pruning methods in decision tree induction, </title> <editor> in D. Fisher & H. Lenz, eds, </editor> <booktitle> "Proceedings of the fifth International Workshop on Artificial Intelligence and Statistics", </booktitle> <pages> pp. 211-218. </pages>
Reference: <author> Esposito, F., Malerba, D. & Semeraro, G. </author> <year> (1995b), </year> <title> Simplifying decision trees by pruning and grafting: New results, </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> "Machine Learning: ECML-95 (Proc. European Conf. on Machine Learning, 1995)", Lecture Notes in Artificial Intelligence 914, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <pages> pp. 287-290. </pages>
Reference: <author> Fayyad, U. M. </author> <year> (1991), </year> <title> On the induction of decision trees for multiple concept learning, </title> <type> PhD thesis, </type> <institution> EECS Dept, Michigan University. </institution>
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1992), </year> <title> The attribute selection problem in decision tree generation, </title> <booktitle> in AAA (1992), </booktitle> <pages> pp. 104-110. </pages>
Reference-contexts: After the test is chosen, the instances are split according to the test, and the subproblems are solved recursively. C4.5 uses gain ratio, a variant of mutual information, as the feature selection measure; other measures have been proposed, such as the Gini index (Breiman et al. 1984), C-separators <ref> (Fayyad & Irani 1992) </ref>, distance-based measures (De Mantaras 1991), and Relief (Kononenko 1995b).
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1993), </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning, </title> <booktitle> in "Proceedings of the 13th International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 1022-1027. </pages>
Reference: <author> Fayyad, U. M., Piatetsky-Shapiro, G. & Smyth, P. </author> <title> (to appear), From data mining to knowledge discovery: An overview, </title> <editor> in U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth & R. Uthurusamy, eds, </editor> <booktitle> "Advances in Knowledge Discovery and Data Mining", </booktitle> <publisher> AAAI/MIT Press, </publisher> <address> chapter 1. </address>
Reference: <author> Fayyad, U. M., Weir, N. & Djorgovski, S. </author> <year> (1993), </year> <title> SKICAT: a machine learning system for automated cataloging of large scale sky surveys, </title> <booktitle> in "Proceedings of the Tenth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 112-119. </pages> <note> Longer version to appear in Advances in Knowledge Discovery and Data Mining, </note> <editor> Fayyad, Piatetsky-Shapiro, Smyth, </editor> <publisher> and Uthurusamy (Eds.). </publisher>
Reference: <author> Feigenbaum, E. A. </author> <year> (1977), </year> <booktitle> The art of artificial intelligence: Themes and case studies of knowledge engineering, in "Proceedings of the 5th International Joint Conference on Artificial Intelligence", </booktitle> <pages> pp. 1014-1029. </pages> <note> BIBLIOGRAPHY 257 Feigenbaum, </note> <author> E. A. </author> <year> (1988), </year> <title> Knowledge processing: From file servers to knowledge servers, </title> <editor> in J. R. Quinlan, ed., </editor> <booktitle> "Applications of Expert Systems", </booktitle> <volume> Vol. 2, </volume> <publisher> Turing Institute Press, </publisher> <pages> chapter 1, pp. 3-11. </pages>
Reference: <author> Feigenbaum, E. A., McCorduck, P. & Nii, H. P. </author> <year> (1988), </year> <title> The Rise of the Expert Company, Times Books. </title>
Reference-contexts: The ability to extract interesting information and understand the data is of vital importance. The field of data mining is now growing rapidly with the increased need for smart data warehouses. CHAPTER 1. INTRODUCTION 6 1.2.2 Knowledge Acquisition Bottleneck For knowledge itself is power. |Francis Bacon (1561-1626) Expert systems <ref> (Feigenbaum, McCorduck & Nii 1988, Buchanan & Smith 1988) </ref> solve problems that are normally solved by human experts. To solve expert-level problems, expert systems need to build a large knowledge base, a task usually assigned to a knowledge engineer.
Reference: <author> Finnoff, W., Hergert, F. & Zimmermann, H. G. </author> <year> (1993), </year> <title> Improving model selection by nonconvergent methods, </title> <booktitle> in "Neural Networks", </booktitle> <volume> Vol. 6, </volume> <pages> pp. 771-783. </pages>
Reference-contexts: When accuracy estimation is used for model selection, such as selecting the right amount of pruning for a decision tree (Breiman et al. 1984) or early stopping for neural nets <ref> (Finnoff, Hergert & Zimmermann 1993) </ref>, we are interested in the difference between two classifiers, and the variance may be even more important because if the bias for the two models is equal, they cancel out. 3.4.1 The Bias In this section, we investigate the bias of cross-validation and the .632 bootstrap.
Reference: <author> Fisher, R. A. </author> <year> (1936), </year> <title> "The use of multiple measurements in taxonomic problems", </title> <journal> Annals of Eugenics 7(1), </journal> <pages> 179-188. </pages>
Reference-contexts: If the training set and test set are formed by a split of an original dataset, then an over-represented class in one subset will be under-represented in the other. To demonstrate the issue, we simulated a 2/3, 1/3 split of Fisher's famous iris dataset <ref> (Fisher 1936) </ref> and used a majority inducer that builds a classifier predicting the prevalent class in the training set. The iris dataset describes iris plants using four continuous features, and the task is to classify each instance (an iris) as Iris Setosa, Iris Versicolour, or Iris Virginica. <p> Breiman (1994b) discusses the instability of some induction algorithms. While some inducers are likely to be inherently more stable, the following example shows that one must also take into account the dataset and the actual perturbations. Example 3.1 (Failure of leave-one-out) Fisher's iris dataset <ref> (Fisher 1936) </ref> contains 50 instances of each class, leading one to expect that a majority inducer should have an accuracy of about 33%. However, the combination of this dataset with a majority inducer is unstable for the small perturbations performed by leave-one-out.
Reference: <author> Freund, Y. </author> <year> (1990), </year> <title> Boosting a weak learning algorithm by majority, </title> <booktitle> in "Proceedings of the Third Annual Workshop on Computational Learning Theory", </booktitle> <pages> pp. 202-216. </pages> <note> To appear in Information and Computation. </note>
Reference: <author> Freund, Y. & Schapire, R. E. </author> <year> (1995), </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting, </title> <booktitle> in "Proceedings of the Second European Conference on Computational Learning Theory", </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 23-37. </pages>
Reference: <author> Friedman, S. J. & Suppowit, K. J. </author> <year> (1987), </year> <title> Finding the optimal variable ordering for binary decision diagrams, </title> <booktitle> in "Proceedings of the 24th ACM/IEEE Design Automation Conference", </booktitle> <pages> pp. 348-355. </pages>
Reference: <author> Friedman, S. J. & Suppowit, K. J. </author> <year> (1990), </year> <title> "Finding the optimal variable ordering for binary decision diagrams", </title> <journal> IEEE Transactions on Computers 39(5), </journal> <pages> 710-713. </pages>
Reference: <author> Fujita, M., Matsunaga, Y. & Kakuda, T. </author> <year> (1991), </year> <title> On variable ordering of binary decision diagrams for the application of multilevel logic synthesis, </title> <booktitle> in "Proceedings of the European Conference on Design Automation", </booktitle> <publisher> IEEE Computing Press, </publisher> <pages> pp. 50-54. </pages>
Reference: <author> Furnival, G. M. & Wilson, R. W. </author> <year> (1974), </year> <title> "Regression by leaps and bounds", </title> <type> Technometrics 16(4), </type> <pages> 499-511. </pages>
Reference: <author> Gaines, B. R. </author> <year> (1991), </year> <title> The trade-off between knowledge and data in knowledge acquisition, </title> <editor> in G. Piatetsky-Shapiro & W. Frawley, eds, </editor> <title> "Knowledge Discovery in Databases", </title> <publisher> MIT Press, </publisher> <pages> chapter 29, pp. 491-505. </pages> <note> BIBLIOGRAPHY 258 Garey, </note> <author> M. R. </author> <year> (1972), </year> <title> "Optimal binary identification procedures", </title> <journal> Siam Journal on Applied Mathematics 23, </journal> <pages> 173-186. </pages>
Reference: <author> Garey, M. R. & Johnson, D. S. </author> <year> (1979), </year> <title> Computers and Intractability: a Guide to the Theory of NP-completeness, </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: To prove that the problem is NP-hard, we show a reduction from k-colorability (chromatic number). Graph k-colorability is defined as follows <ref> (Garey & Johnson 1979, p. 191) </ref>.
Reference: <author> Gavalda, R. & Guijarro, D. </author> <year> (1995), </year> <title> Learning ordered binary decision diagrams, </title> <type> Technical Report NC-TR-95-050, </type> <institution> Universitat Politecnica de Catalunya, Barcelona, Spain. </institution> <type> NeuroCOLT Technical Report Series. </type>
Reference: <author> Geisser, S. </author> <year> (1975), </year> <title> "The predictive sample reuse method with applications", </title> <journal> Journal of the American Statistical Association 70(350), </journal> <pages> 320-328. </pages>
Reference-contexts: Complete cross-validation is the average of all m possibilities for choosing m=k instances out of m, but it is usually too expensive <ref> (Geisser 1975) </ref>. Except for leave-one-out (m-fold cross-validation), which is always complete, k-fold cross-validation is estimating complete k-fold cross-validation using a single split of the data into the folds. Repeating cross-validation multiple times using different splits into folds provides a better Monte-Carlo estimate to the complete cross-validation at an added cost.
Reference: <author> Geman, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> "Neural networks and the bias/variance dilemma", </title> <booktitle> Neural Computation 4, </booktitle> <pages> 1-48. </pages>
Reference-contexts: Taking expectation with respect to the training set D (i.e., averaging over all possible training sets of the given size), yields E D (I (D; ~x) E [y j ~x]) i which can be decomposed as follows <ref> (Geman, Bienenstock & Doursat 1992) </ref>: E D I (D; ~x) E [y j ~x] # Subtract and addE D [I (D; ~x)] = E D (I (D; ~x) E D [I (D; ~x)]) + (E D [I (D; ~x)] E [y j ~x]) # = E D I (D; ~x) E <p> An induction algorithm overfits the dataset if it models the given data too well and its CHAPTER 4. WRAPPERS 115 predictions are poor. An example of an over-specialized hypothesis, or classifier, is a lookup table on all the features. Overfitting is closely related to the bias-variance tradeoff <ref> (Geman et al. 1992, Breiman et al. 1984) </ref>: if the algorithm fits the data too well, the variance term is large, and hence the overall error is increased.
Reference: <author> Gennari, J. H., Langley, P. & Fisher, D. </author> <year> (1989), </year> <title> "Models of incremental concept formation", </title> <booktitle> Artificial Intelligence 40, </booktitle> <pages> 11-61. </pages>
Reference: <author> Ginsberg, M. L. </author> <year> (1993), </year> <booktitle> Essentials of Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Glick, N. </author> <year> (1978), </year> <title> "Additive estimators for probabilities of correct classification", </title> <booktitle> Pattern Recognition 10, </booktitle> <pages> 211-222. </pages>
Reference: <author> Goldberg, D. E. </author> <year> (1989), </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning, </title> <publisher> Addison-Wesley Publishing Company, Inc. </publisher>
Reference: <author> Gong, G. </author> <year> (1982), </year> <title> Cross validation, the jackknife, and the bootstrap: excess error estimation in forward logistic regression, </title> <type> PhD thesis, </type> <institution> Stanford University. 3781-1982G. </institution>
Reference: <author> Good, I. J. </author> <year> (1965), </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods, </title> <publisher> M.I.T. Press. </publisher>
Reference-contexts: In case of zero occurrences for a label value and a feature value, we use the :5=m as the probability, where m is the number of instances. Other approaches are possible, such as using Laplace's law of succession or using a beta prior <ref> (Good 1965, Cestnik 1990) </ref>. In these approaches, the probability for n successes after N trials is estimated at (n + a)=(N + a + b), where a and b are the parameters of the beta function.
Reference: <author> Greiner, R. </author> <year> (1992), </year> <title> Probabilistic hill climbing : Theory and applications, </title> <editor> in J. Glasgow & R. Hadley, eds, </editor> <booktitle> "Proceedings of the Ninth Canadian Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 60-67. </pages>
Reference: <author> Hancock, T. R. </author> <year> (1989), </year> <title> On the difficulty of finding small consistent decision trees, </title> <type> Unpublished Manuscript, </type> <institution> Harvard University. </institution> <note> BIBLIOGRAPHY 259 Hardy, </note> <author> G. H. & Wright, E. M. </author> <year> (1979), </year> <title> An introduction to the theory of numbers, </title> <booktitle> 5th edn, </booktitle> <publisher> Oxford: claredon Press. </publisher>
Reference: <author> Harrison, D. </author> <year> (1993), </year> <title> "Backing up", </title> <journal> Network Computing pp. </journal> <pages> 98-104. </pages> <month> 15 Oct. </month>
Reference-contexts: Wal-Mart, a U.S. retailer, created a database that handles over 20 million transactions a day (Babcock 1994). 2. Mobil Oil Corporation, is developing a data warehouse capable of storing over 100 terabytes of data related to oil exploration <ref> (Harrison 1993) </ref>. 3. The NASA Earth Observing System (EOS) of orbiting satellites and other spaceborne instruments is projected to generate on the order of 50 gigabytes of remotely sensed image data per hour when operational in the late 1990s and into the next century (Way & Smith 1991). 4.
Reference: <author> Hartmann, C. R. P., Varshney, P. K., Mehrotra, K. G. & Gerberich, C. L. </author> <year> (1982), </year> <title> "Application of information theory to the construction of efficient decision trees", </title> <journal> IEEE Transactions on information theory IT-28(4), </journal> <pages> 565-577. </pages>
Reference: <author> Haussler, D. </author> <year> (1988), </year> <title> "Quantifying inductive bias: AI learning algorithms and valiant's learning framework", </title> <booktitle> Artificial Intelligence 36(2), </booktitle> <pages> 177-221. </pages>
Reference-contexts: The features can be extended to other types, such as linear (but not continuous) and tree-structured <ref> (Haussler 1988) </ref>, but this issue is orthogonal to this dissertation; algorithms that support these types could be used with methods described here. We assume a flat-file format, where the instances contain a fixed number of features.
Reference: <author> Haussler, D. </author> <year> (1992), </year> <title> "Decision theoretic generalizations of the PAC model for neural net and other learning applications", </title> <booktitle> Information and Computation 100(1), </booktitle> <pages> 78-150. </pages>
Reference: <author> Haussler, D., Kearns, M., Seung, H. S. & Tishby, N. </author> <year> (1994), </year> <title> Rigorous learning curve bounds from statistical mechanics, </title> <booktitle> in "Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory", </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 76-87. </pages>
Reference: <author> Hertz, J. A., Krogh, A. & Thorbergsson, G. I. </author> <year> (1989), </year> <title> "Phase transitions in simple learning", </title> <journal> Journal of Physics A 22(12), </journal> <pages> 2133-2150. </pages>
Reference: <author> Hertz, J., Krogh, A. & Palmer, R. G. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison Wesley. </publisher>
Reference: <author> Hoaglin, D. C., Mosteller, F. & Tukey, J. W. </author> <year> (1983), </year> <title> Understanding Robust and Exploratory Data Analysis, </title> <publisher> John Wiley & Sons, Inc. </publisher>
Reference: <author> Hoeffding, W. </author> <year> (1963), </year> <title> "Probability inequalities for sums of bounded random variables", </title> <journal> Journal of the American Statistical Association 58, </journal> <pages> 13-30. </pages>
Reference-contexts: The race ends when there is a winner, or when all n steps in the leave-one-out cross-validation have been executed. The confidence interval is defined according to Hoeffding's formula <ref> (Hoeffding 1963) </ref>: Pr fi fi fi &gt; * &lt; 2e 2m* 2 =B 2 where b f (s) is the average of m evaluations and B bounds the possible spread of point values.
Reference: <author> Holland, J. H. </author> <year> (1992), </year> <title> Adaptation in natural and artificial systems : an introductory analysis with applications to biology, control, </title> <booktitle> and artificial intelligence, </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Looking at the search, we have seen that one general area of the search space is explored heavily when it is found to be good. It might be worthwhile to introduce some diversity into the search, following the genetic algorithm and genetic programming approaches <ref> (Holland 1992, Goldberg 1989, Koza 1992) </ref>. The problem has been abstracted as search with probabilistic estimates (Section 4.8), but we have not done experiments in an attempt to understand the tradeoff between the quality of the estimates and the search size.
Reference: <author> Holte, R. C. </author> <year> (1993), </year> <title> "Very simple classification rules perform well on most commonly used datasets", </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-90. </pages>
Reference-contexts: An experimental comparison between different discretization methods was done in Dougherty et al. (1995) and, for the datasets tested, discretization by mutual information turned out to be superior to uniform binning, a class-blind unsupervised discretization method, and to Holte's 1R discretization method, which is supervised <ref> (Holte 1993) </ref>. The discretization method used in this dissertation is based on minimizing mutual information, originally presented in Catlett (1991b) and later refined in Fayyad & Irani (1993). The method is implemented in MLC ++ (Kohavi et al. 1994).
Reference: <author> Hyafil, L. & Rivest, R. L. </author> <year> (1976), </year> <title> "Constructing optimal binary decision trees is NP-complete", </title> <note> Information Processing Letters 5(1), 15-17. BIBLIOGRAPHY 260 Imam, </note> <author> I. F. </author> <year> (1995), </year> <title> Driving Task-Oriented Decision Structures from Decision Rules, </title> <type> PhD thesis, </type> <institution> School of Information Technology and Engineering, George Mason Univeristy. </institution>
Reference-contexts: For example, decision tree induction algorithms usually attempt to find a small tree that fits the data well, yet finding the optimal binary decision tree is NP-hard <ref> (Hyafil & Rivest 1976, Hancock 1989) </ref>. For neural-networks, the problem is even harder; the problem of loading a three-node neural network with a training set is NP-hard if the nodes compute linear threshold functions (Judd 1988, Blum & Rivest 1992). <p> Finding the minimal representation using other common structures is also difficult in the worst case, yet heuristics seem to work well in practice. It is known that finding an optimal binary decision tree is NP-hard <ref> (Hyafil & Rivest 1976, Hancock 1989) </ref>. For neural-networks, the problem of loading a three-node neural network with a training set is NP-hard if the nodes compute linear threshold functions (Judd 1988, Blum & Rivest 1992). Given the hardness results, we use a simple greedy heuristic to assign the IPs.
Reference: <author> Ishiura, N., Sawada, H. & Yajima, S. </author> <year> (1991), </year> <title> Minimization of binary decision diagrams based on exchanges of variables, </title> <booktitle> in "IEEE International Conference on Computer-Aided Design. Digest of Technical Papers", </booktitle> <publisher> IEEE Comput. Soc. Press, </publisher> <pages> pp. 472-475. </pages>
Reference: <author> Jain, A. K., Dubes, R. C. & Chen, C. </author> <year> (1987), </year> <title> "Bootstrap techniques for error estimation", </title> <journal> IEEE transactions on pattern analysis and machine intelligence PAMI-9(5), </journal> <pages> 628-633. </pages>
Reference: <author> Jelinek, F. </author> <year> (1985), </year> <title> The development of an experimental discrete dictation recognizer, </title> <booktitle> in "Proceedings of the IEEE", </booktitle> <volume> Vol. 73, </volume> <pages> pp. 1616-1624. </pages>
Reference-contexts: This is something that is commonly done for the N-gram model used in speech recognition: the 2-gram model is used to smooth the 3-gram model for combinations that did not appear in the training set <ref> (Jelinek 1985) </ref>. Similar smoothing was done by Buntine (1992) for regular (non-oblivious) decision trees. 6.9 Summary We have described some properties of oblivious read-once decision graphs (OODGs). Given an ordering on the features, every function has a unique OODG implementing it, which makes the hypothesis space structured and non-redundant.
Reference: <author> John, G. H. </author> <year> (1994), </year> <title> Cross-validated C4.5: Using error estimation for automatic parameter selection, </title> <type> Technical Report STAN-CS-TN-94-12, </type> <institution> Computer Science Department, Stanford University. </institution> <note> Available at ftp://starry.Stanford.EDU/pub/gjohn/papers/cvc45.ps. </note>
Reference-contexts: John (1994) reports preliminary results on using cross-validation and exhaustive search to set the m parameter in C4.5. CHAPTER 4. WRAPPERS 127 4.10.3 The Wrapper Approach Since the introduction of the wrapper approach, we have seen it used in a few papers that reference our original paper <ref> (John et al. 1994) </ref>. Langley & Sage (1994a) used the wrapper approach to select features for Naive-Bayes (but without discretization) and Langley & Sage (1994b) used it to select features for a nearest-neighbor algorithm.
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages>
Reference-contexts: John (1994) reports preliminary results on using cross-validation and exhaustive search to set the m parameter in C4.5. CHAPTER 4. WRAPPERS 127 4.10.3 The Wrapper Approach Since the introduction of the wrapper approach, we have seen it used in a few papers that reference our original paper <ref> (John et al. 1994) </ref>. Langley & Sage (1994a) used the wrapper approach to select features for Naive-Bayes (but without discretization) and Langley & Sage (1994b) used it to select features for a nearest-neighbor algorithm.
Reference: <author> Judd, S. </author> <year> (1988), </year> <title> "On the complexity of loading shallow neural networks", </title> <journal> Journal of Complexity 4, </journal> <pages> 177-192. </pages>
Reference-contexts: For neural-networks, the problem is even harder; the problem of loading a three-node neural network with a training set is NP-hard if the nodes compute linear threshold functions <ref> (Judd 1988, Blum & Rivest 1992) </ref>. Because most induction problems are NP-hard and heuristics are used, we define an optimal feature subset with respect to the induction algorithm. The problem of feature subset selection is then reduced to the problem of finding an optimal subset. <p> It is known that finding an optimal binary decision tree is NP-hard (Hyafil & Rivest 1976, Hancock 1989). For neural-networks, the problem of loading a three-node neural network with a training set is NP-hard if the nodes compute linear threshold functions <ref> (Judd 1988, Blum & Rivest 1992) </ref>. Given the hardness results, we use a simple greedy heuristic to assign the IPs. The greedy strategy starts creating projection sets (branching nodes) from projections having the greatest number of known destinations, and then proceeds to projections with fewer known destinations.
Reference: <author> Kadie, C. M. </author> <year> (1995), </year> <title> Seer: Maximum Likelihood Regression for Learning-Speed Curves, </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign. </institution>
Reference: <author> Kadie, C. M. & Wilkins, D. C. </author> <title> (to appear), "Learning-speed curves: Their use in induction of classification expert systems", </title> <journal> International Journal of Knowledge Acquisition. </journal>
Reference: <author> Kaelbling, L. P. </author> <year> (1993), </year> <title> Learning in Embedded Systems, </title> <publisher> MIT Press. </publisher>
Reference-contexts: Increasing the number of runs shrinks the confidence interval for the mean, but requires more time. The tradeoff between more accurate estimates and more extensive exploration of the search space is referred to as the exploration versus exploitation problem <ref> (Kaelbling 1993) </ref> and leads to the following abstract search problem. Definition 4.7 (Search with Probabilistic Estimates) Let S be a state space with operators between states.
Reference: <author> Kira, K. & Rendell, L. A. </author> <year> (1992a), </year> <title> The feature selection problem: Traditional methods and a new algorithm, </title> <booktitle> in AAA (1992), </booktitle> <pages> pp. 129-134. </pages>
Reference-contexts: Given only the SSN, any induction algorithm is expected to generalize very poorly. The Relief Algorithm The Relief algorithm <ref> (Kira & Rendell 1992a, Kira & Rendell 1992b, Kononenko 1994) </ref> assigns a "relevance" weight to each feature, which is meant to denote the relevance of the feature to the target concept. Relief is a randomized algorithm. <p> The Relief algorithm finds all weakly relevant features: Relief does not help with redundant features. If most of the given features are relevant to the concept, it would select most of them even though only a fraction are necessary for concept description <ref> (Kira & Rendell 1992a, page 133) </ref>. In real domains, many features have high correlations with the label, and thus many are (weakly) relevant, and will not be removed by Relief. <p> In the simple parity example used 2 This is true even if SSN is encoded in ` binary features as long as more than ` other features are required to determine the diagnosis. CHAPTER 4. WRAPPERS 87 in <ref> (Kira & Rendell 1992a, Kira & Rendell 1992b) </ref>, there were only strongly relevant and irrelevant features, so Relief found the strongly relevant features most of the time. While nearest-neighbors are not hurt much by weakly relevant features, Naive-Bayes is affected.
Reference: <author> Kira, K. & Rendell, L. A. </author> <year> (1992b), </year> <title> A practical approach to feature selection, in ML- (1992). BIBLIOGRAPHY 261 Kittler, </title> <editor> J. </editor> <year> (1978), </year> <title> Une generalisation de quelques algorithms sous-optimaux de recherche d'ensembles d'attributs, </title> <booktitle> in "Proc. </booktitle> <institution> Congres Reconnaissance des Formes et Traitement des Images". </institution>
Reference: <author> Kittler, J. </author> <year> (1986), </year> <title> Feature Selection and Extraction, </title> <publisher> Academic Press, Inc, </publisher> <pages> chapter 3, pp. 59-83. </pages>
Reference-contexts: Note that feature subset selection chooses a set of features from existing features, and does not construct new ones; there is no feature extraction or construction <ref> (Kittler 1986, Rendell & Seshu 1990) </ref>. From a purely theoretical standpoint, the question is not of much interest. The optimal Bayes rule is monotonic, i.e., adding features cannot decrease the accuracy, and hence restricting the induction algorithm to a subset of features is never advised.
Reference: <author> Knoke, J. D. </author> <year> (1986), </year> <title> "The robust estimation of classification error rates", </title> <journal> Computers and Mathematics with Applications 12A(2), </journal> <pages> 253-260. </pages>
Reference: <author> Knuth, D. E. </author> <year> (1973), </year> <booktitle> The Art of Computer Programming, fundamental algorithms, </booktitle> <volume> Vol. 1, </volume> <booktitle> 2nd edn, </booktitle> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1994a), </year> <title> Bottom-up induction of oblivious, read-once decision graphs, </title> <editor> in F. Bergadano & L. D. Raedt, eds, </editor> <booktitle> "Proceedings of the European Conference on Machine Learning", </booktitle> <pages> pp. 154-169. </pages>
Reference: <author> Kohavi, R. </author> <year> (1994b), </year> <title> Bottom-up induction of oblivious, read-once decision graphs : strengths and limitations, </title> <booktitle> in "Twelfth National Conference on Artificial Intelligence", </booktitle> <pages> pp. 613-618. </pages>
Reference: <author> Kohavi, R. </author> <year> (1994c), </year> <title> Feature subset selection as search with probabilistic estimates, </title> <booktitle> in "AAAI Fall Symposium on Relevance", </booktitle> <pages> pp. 122-126. </pages>
Reference-contexts: In John, Kohavi & Pfleger (1994) the wrapper approach was introduced as a method for feature subset selection that will work with any induction algorithm. For about a year most of the effort was concentrated on the problem of feature subset selection <ref> (Kohavi 1994c, Kohavi & Sommerfield 1995a) </ref>. The results on feature subset selection using the wrapper approach were generalized into general optimization of parameters (Kohavi & John 1995) and specialized into subset selection for decision tables (Kohavi 1995a).
Reference: <author> Kohavi, R. </author> <year> (1994d), </year> <title> A third dimension to rough sets, </title> <booktitle> in "Third International Workshop on Rough Sets and Soft Computing", </booktitle> <pages> pp. 244-251. </pages> <note> Also appeared in Soft Computing by Lin and Wildberger. </note>
Reference: <author> Kohavi, R. </author> <year> (1995a), </year> <title> The power of decision tables, </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> "Proceedings of the European Conference on Machine Learning", Lecture Notes in Artificial Intelligence 914, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <pages> pp. 174-189. </pages>
Reference-contexts: The results on feature subset selection using the wrapper approach were generalized into general optimization of parameters (Kohavi & John 1995) and specialized into subset selection for decision tables <ref> (Kohavi 1995a) </ref>. Rough sets were related to decision tables and to OODGs (Kohavi & Frasca 1994, Kohavi 1994d). The work with the wrapper approach led to a large experiment to see which accuracy estimation method should be used (Kohavi 1995b). <p> In this dissertation, we give comprehensibility an important weight. 3. It is compact. While related to comprehensibility, one does not imply the other. A perceptron (see below) might be a compact classifier, yet given an instance, it may be hard to understand the labelling process. Alternatively, a decision table <ref> (Kohavi 1995a) </ref> (also see Chapter 5 on page 130) may be very large, yet labelling each instance is trivial: simply look it up in the table. <p> Repeating this for every case squeezes the data almost dry." The use of incremental induction algorithms, however, allows cross-validation in time that is independent of the number of folds <ref> (Kohavi 1995a, Moore & Lee 1994, Utgoff 1994) </ref>. With such algorithms, leave-one-out takes exactly the same time as ten-fold cross-validation; is it clear that leave-one-out should be preferred? While leave-one-out is almost unbiased (Lachenbruch 1967, Glick 1978, Efron 1983), it has high variance, leading to unreliable estimates. <p> The other unexpected result in this dissertation is the power of decision tables. In the original paper on decision tables <ref> (Kohavi 1995a) </ref>, their power was noted, but they failed for domains with continuous features. In this dissertation, the data has been discretized, and IDTM appears to be a truly powerful algorithm for many real-world datasets.
Reference: <author> Kohavi, R. </author> <year> (1995b), </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> "Proceedings of the 14th International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Rough sets were related to decision tables and to OODGs (Kohavi & Frasca 1994, Kohavi 1994d). The work with the wrapper approach led to a large experiment to see which accuracy estimation method should be used <ref> (Kohavi 1995b) </ref>. Recently, work on OODGs resumed with an experimental comparison of discretization methods to allow experiments on real datasets (Dougherty, Kohavi & Sahami 1995), and a top-down induction algorithm (Kohavi & Li 1995). Chapter 6 combines most of the work CHAPTER 1.
Reference: <author> Kohavi, R. & Frasca, B. </author> <year> (1994), </year> <title> Useful feature subsets and rough set reducts, </title> <booktitle> in "Third International Workshop on Rough Sets and Soft Computing", </booktitle> <pages> pp. 310-317. </pages> <note> Also appeared in Soft Computing by Lin and Wildberger. BIBLIOGRAPHY 262 Kohavi, </note> <author> R. & John, G. </author> <year> (1995), </year> <title> Automatic parameter selection by minimizing estimated error, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Twelfth International Conference", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: The results on feature subset selection using the wrapper approach were generalized into general optimization of parameters (Kohavi & John 1995) and specialized into subset selection for decision tables (Kohavi 1995a). Rough sets were related to decision tables and to OODGs <ref> (Kohavi & Frasca 1994, Kohavi 1994d) </ref>. The work with the wrapper approach led to a large experiment to see which accuracy estimation method should be used (Kohavi 1995b). <p> (~x) is same for all label values. / Pr (X 1 = x 1 ; : : : ; X n = x n j Y = y) Pr (Y = y) by independence = i=1 The version of Naive-Bayes we use in our experiments was implemented in MLC ++ <ref> (Kohavi et al. 1994) </ref>. The probabilities for nominal features are estimated from data. The probabilities for continuous features are assumed to be coming from a Gaussian distribution, and we estimate the mean and standard deviation from the data. <p> The discretization method used in this dissertation is based on minimizing mutual information, originally presented in Catlett (1991b) and later refined in Fayyad & Irani (1993). The method is implemented in MLC ++ <ref> (Kohavi et al. 1994) </ref>. This supervised discretization algorithm uses the mutual information of the class partitions and the partitions formed by different thresholds to select the discretization boundaries. The notation below closely follows the notation of Fayyad & Irani (1993).
Reference: <author> Kohavi, R. & Li, C.-H. </author> <year> (1995), </year> <title> Oblivious decision trees, graphs, and top-down pruning, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> "Proceedings of the 14th International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: For about a year most of the effort was concentrated on the problem of feature subset selection (Kohavi 1994c, Kohavi & Sommerfield 1995a). The results on feature subset selection using the wrapper approach were generalized into general optimization of parameters <ref> (Kohavi & John 1995) </ref> and specialized into subset selection for decision tables (Kohavi 1995a). Rough sets were related to decision tables and to OODGs (Kohavi & Frasca 1994, Kohavi 1994d). <p> The work with the wrapper approach led to a large experiment to see which accuracy estimation method should be used (Kohavi 1995b). Recently, work on OODGs resumed with an experimental comparison of discretization methods to allow experiments on real datasets <ref> (Dougherty, Kohavi & Sahami 1995) </ref>, and a top-down induction algorithm (Kohavi & Li 1995). Chapter 6 combines most of the work CHAPTER 1. <p> Recently, work on OODGs resumed with an experimental comparison of discretization methods to allow experiments on real datasets (Dougherty, Kohavi & Sahami 1995), and a top-down induction algorithm <ref> (Kohavi & Li 1995) </ref>. Chapter 6 combines most of the work CHAPTER 1.
Reference: <author> Kohavi, R. & Sommerfield, D. </author> <year> (1995a), </year> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology, </title> <booktitle> in "The First International Conference on Knowledge Discovery and Data Mining". </booktitle>
Reference-contexts: The results on feature subset selection using the wrapper approach were generalized into general optimization of parameters (Kohavi & John 1995) and specialized into subset selection for decision tables <ref> (Kohavi 1995a) </ref>. Rough sets were related to decision tables and to OODGs (Kohavi & Frasca 1994, Kohavi 1994d). The work with the wrapper approach led to a large experiment to see which accuracy estimation method should be used (Kohavi 1995b). <p> In this dissertation, we give comprehensibility an important weight. 3. It is compact. While related to comprehensibility, one does not imply the other. A perceptron (see below) might be a compact classifier, yet given an instance, it may be hard to understand the labelling process. Alternatively, a decision table <ref> (Kohavi 1995a) </ref> (also see Chapter 5 on page 130) may be very large, yet labelling each instance is trivial: simply look it up in the table. <p> Repeating this for every case squeezes the data almost dry." The use of incremental induction algorithms, however, allows cross-validation in time that is independent of the number of folds <ref> (Kohavi 1995a, Moore & Lee 1994, Utgoff 1994) </ref>. With such algorithms, leave-one-out takes exactly the same time as ten-fold cross-validation; is it clear that leave-one-out should be preferred? While leave-one-out is almost unbiased (Lachenbruch 1967, Glick 1978, Efron 1983), it has high variance, leading to unreliable estimates. <p> The other unexpected result in this dissertation is the power of decision tables. In the original paper on decision tables <ref> (Kohavi 1995a) </ref>, their power was noted, but they failed for domains with continuous features. In this dissertation, the data has been discretized, and IDTM appears to be a truly powerful algorithm for many real-world datasets.
Reference: <author> Kohavi, R. & Sommerfield, D. </author> <year> (1995b), </year> <note> MLC++ utilities, Available in http://robotics.stanford.edu/users/ronnyk/mlc.html. </note>
Reference-contexts: Rough sets were related to decision tables and to OODGs (Kohavi & Frasca 1994, Kohavi 1994d). The work with the wrapper approach led to a large experiment to see which accuracy estimation method should be used <ref> (Kohavi 1995b) </ref>. Recently, work on OODGs resumed with an experimental comparison of discretization methods to allow experiments on real datasets (Dougherty, Kohavi & Sahami 1995), and a top-down induction algorithm (Kohavi & Li 1995). Chapter 6 combines most of the work CHAPTER 1.
Reference: <author> Kohavi, R., John, G., Long, R., Manley, D. & Pfleger, K. </author> <year> (1994), </year> <title> MLC++: A machine learning library in C++, </title> <booktitle> in "Tools with Artificial Intelligence", </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 740-743. </pages> <note> Available at ftp://starry.Stanford.EDU/pub/ronnyk/mlc/toolsmlc.ps. </note>
Reference-contexts: The results on feature subset selection using the wrapper approach were generalized into general optimization of parameters (Kohavi & John 1995) and specialized into subset selection for decision tables (Kohavi 1995a). Rough sets were related to decision tables and to OODGs <ref> (Kohavi & Frasca 1994, Kohavi 1994d) </ref>. The work with the wrapper approach led to a large experiment to see which accuracy estimation method should be used (Kohavi 1995b). <p> (~x) is same for all label values. / Pr (X 1 = x 1 ; : : : ; X n = x n j Y = y) Pr (Y = y) by independence = i=1 The version of Naive-Bayes we use in our experiments was implemented in MLC ++ <ref> (Kohavi et al. 1994) </ref>. The probabilities for nominal features are estimated from data. The probabilities for continuous features are assumed to be coming from a Gaussian distribution, and we estimate the mean and standard deviation from the data. <p> The discretization method used in this dissertation is based on minimizing mutual information, originally presented in Catlett (1991b) and later refined in Fayyad & Irani (1993). The method is implemented in MLC ++ <ref> (Kohavi et al. 1994) </ref>. This supervised discretization algorithm uses the mutual information of the class partitions and the partitions formed by different thresholds to select the discretization boundaries. The notation below closely follows the notation of Fayyad & Irani (1993).
Reference: <author> Kononenko, I. </author> <year> (1993), </year> <title> "Inductive and bayesian learning in medical diagnosis", </title> <booktitle> Applied Artificial Intelligence 7, </booktitle> <pages> 317-337. </pages>
Reference: <author> Kononenko, I. </author> <year> (1994), </year> <title> Estimating attributes: Analysis and extensions of Relief, </title> <editor> in F. Bergadano & L. D. Raedt, eds, </editor> <booktitle> "Proceedings of the European Conference on Machine Learning". </booktitle>
Reference: <author> Kononenko, I. </author> <year> (1995a), </year> <title> A counter example to the stronger version of the binary tree hypothesis, </title> <booktitle> in "ECML-95 workshop on Statistics, machine learning, and knowledge discovery in databases", </booktitle> <pages> pp. 31-36. </pages>
Reference: <author> Kononenko, I. </author> <year> (1995b), </year> <title> On biases in estimating multi-valued attributes, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> "Proceedings of the 14th International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 1034-1040. </pages>
Reference-contexts: C4.5 uses gain ratio, a variant of mutual information, as the feature selection measure; other measures have been proposed, such as the Gini index (Breiman et al. 1984), C-separators (Fayyad & Irani 1992), distance-based measures (De Mantaras 1991), and Relief <ref> (Kononenko 1995b) </ref>. C4.5 prunes by using the upper bound of a confidence interval on the resubstitution error as the error estimate; since nodes with fewer instances have a wider confidence interval, they are removed if the difference in error between them and their parents is not significant.
Reference: <author> Koutsofios, E. & North, S. C. </author> <year> (1994), </year> <title> Drawing graphs with dot, </title> <note> Available by anonymous ftp from research.att.com:dist/drawdag/dotdoc.ps.Z. BIBLIOGRAPHY 263 Koza, J. </note> <year> (1992), </year> <title> Genetic Programming : On the Programming of Computers by Means of Natural Selection, </title> <publisher> MIT Press. </publisher>
Reference: <author> Krause, M. & Waack, S. </author> <year> (1991), </year> <title> "On oblivious branching programs of linear length", </title> <booktitle> Information and Computation 94, </booktitle> <pages> 232-249. </pages>
Reference: <author> Krishnaiah, P. R. & Kanal, L. N. </author> <year> (1982), </year> <title> Classification, Pattern Recognition, and Reduction in Dimensionality, </title> <publisher> Amsterdam: North Holland. </publisher>
Reference: <author> Krogh, A. & Vedelsby, J. </author> <year> (1995), </year> <title> Neural network ensembles, cross validation, and active learning, </title> <booktitle> in "Advances in Neural Information Processing Systems", </booktitle> <volume> Vol. 7, </volume> <publisher> MIT Press. </publisher>
Reference: <author> Kwok, S. W. & Carter, C. </author> <year> (1990), </year> <title> Multiple decision trees, </title> <editor> in R. D. Schachter, T. S. Levitt, L. N. Kanal & J. F. Lemmer, eds, </editor> <booktitle> "Uncertainty in Artificial Intelligence", </booktitle> <publisher> Elsevier Science Publishers, </publisher> <pages> pp. 327-335. </pages>
Reference: <author> Lachenbruch, P. A. </author> <year> (1967), </year> <title> "An almost unbiased method of obtaining confidence intervals for the probability of misclassification in discriminant analysis", </title> <type> Biometrics 23, </type> <pages> 639-645. </pages>
Reference-contexts: With such algorithms, leave-one-out takes exactly the same time as ten-fold cross-validation; is it clear that leave-one-out should be preferred? While leave-one-out is almost unbiased <ref> (Lachenbruch 1967, Glick 1978, Efron 1983) </ref>, it has high variance, leading to unreliable estimates. Recent results, both theoretical and experimental, have shown that it is not CHAPTER 3.
Reference: <author> Lachenbruch, P. A. & Mickey, M. R. </author> <year> (1968), </year> <title> "Estimation of error rates in discriminant analysis", </title> <type> Technometrics 10(1), </type> <pages> 1-11. </pages>
Reference: <author> Langley, P. </author> <year> (1994), </year> <title> Selection of relevant features in machine learning, </title> <booktitle> in "AAAI Fall Symposium on Relevance", </booktitle> <pages> pp. 140-144. </pages>
Reference: <author> Langley, P. </author> <year> (1995), </year> <title> Elements of Machine Learning, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Francisco. </address> <publisher> In press. </publisher>
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994a), </year> <title> Induction of selective bayesian classifiers, </title> <booktitle> in "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Seattle, WA, </address> <pages> pp. 399-406. </pages>
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994b), </year> <title> Oblivious decision trees and abstract cases, </title> <booktitle> in "Working Notes of the AAAI-94 Workshop on Case-Based Reasoning", </booktitle> <publisher> AAAI Press, </publisher> <address> Seattle. </address>
Reference: <author> Langley, P. & Simon, H. A. </author> <title> (to appear), "Applications of machine learning and rule induction", </title> <journal> Communications of the ACM. </journal> <note> BIBLIOGRAPHY 264 Langley, </note> <author> P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An analysis of bayesian classifiers, </title> <booktitle> in "Proceedings of the tenth national conference on artificial intelligence", </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference: <author> Lavrac, N. & Dzeroski, S. </author> <year> (1994), </year> <title> Inductive logic programming : Techniques and Applications, </title> <editor> E. </editor> <publisher> Horwood, </publisher> <address> New York. </address>
Reference-contexts: We assume a flat-file format, where the instances contain a fixed number of features. Inductive logic programming (ILP) techniques <ref> (Lavrac & Dzeroski 1994, Lavrac & Dzeroski 1994) </ref> can deal with variable formats, and techniques described here could conceivably be used in ILP, although they have not been tried. 2.2 Induction Algorithms Following the middle ages, the origin of this word [algorithm] was in doubt, and early linguists attempted to guess
Reference: <author> Lee, C. Y. </author> <year> (1959), </year> <title> "Representation of switching circuits by binary-decision programs", </title> <journal> The Bell System Technical Journal 38(4), </journal> <pages> 985-999. </pages>
Reference: <author> Lenat, D. B. & Feigenbaum, E. A. </author> <year> (1991), </year> <title> "On the thresholds of knowledge", </title> <booktitle> Artificial Intelligence 47, </booktitle> <pages> 185-250. </pages>
Reference-contexts: INTRODUCTION 9 was indeed the generating function, then for zero/one-utility (u (h; f; D) = 1 if h = f and zero otherwise) the accuracy will be high. The no-free-lunch theorems formalize the principle that knowledge facilitates learning, defined in <ref> (Lenat & Feigenbaum 1991) </ref> as "if you don't know very much to begin with, don't expect to learn a lot quickly." The question of interest to researchers in machine learning is how to define the biases of existing algorithms and how to find out when a given bias is appropriate, based
Reference: <author> Linhart, H. & Zucchini, W. </author> <year> (1986), </year> <title> Model Selection, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: Automating this last approach for a large number of parameter settings is the topic of this chapter. In theory, every possible parameter setting creates a different model, so the problem can be viewed as that of model selection <ref> (Linhart & Zucchini 1986) </ref>.
Reference: <author> Lund, C. & Yannakakis, M. </author> <year> (1993), </year> <title> On the hardness of approximating minimization problems, </title> <booktitle> in "ACM Symposium on Theory of Computing". </booktitle>
Reference-contexts: The reduction in the proof is done from graph k-colorability (chromatic number) using only Boolean features. This is a strong negative result because it is known that the chromatic number of a graph cannot be approximated to within any constant multiplicative factor unless P=NP <ref> (Lund & Yannakakis 1993) </ref>. Finding the minimal representation using other common structures is also difficult in the worst case, yet heuristics seem to work well in practice. It is known that finding an optimal binary decision tree is NP-hard (Hyafil & Rivest 1976, Hancock 1989).
Reference: <author> Mallows, C. L. </author> <year> (1973), </year> <title> "Some comments on c p ", Technometrics 15, </title> <type> 661-675. </type>
Reference-contexts: In some cases, measures can be devised that are algorithm specific, and these may be computed efficiently. For example, in linear regression, measures such as Mallow's C p <ref> (Mallows 1973) </ref> and PRESS (Prediction sum of squares) (Neter, Wasserman & Kutner 1990) have been devised so that they do not require running the regression many times, and thus avoid the cross-validation step used in the default wrapper setup. These measures CHAPTER 4. <p> Notable selection measures that do satisfy monotonicity assumption are residual sum of squares (RSS), adjusted R-square, minimum mean residual, Mallow's C p <ref> (Mallows 1973) </ref>, discriminant functions, and distance measures, such as the Bhattacharyya distance and divergence. The PRESS measure (Prediction sum of squares), however, does not obey monotonicity. For monotonic functions, branch and bound techniques can be used CHAPTER 4. WRAPPERS 125 to prune the search space.
Reference: <author> Mammen, E. </author> <year> (1992), </year> <title> When does bootstrap work? : asymptotic results and simulations, </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Marill, T. & Green, D. M. </author> <year> (1963), </year> <title> "On the effectiveness of receptors in recognition systems", </title> <journal> IEEE Transactions on Information Theory 9, </journal> <pages> 11-17. </pages>
Reference: <author> Maron, O. & Moore, A. W. </author> <year> (1994), </year> <title> Hoeffding races: Accelerating model selection search for classification and function approximation, </title> <booktitle> in "Advances in Neural Information Processing Systems", </booktitle> <volume> Vol. 6, </volume> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Given a confidence level, one can determine *, and hence a confidence interval for f fl (s), from the above formula. The paper <ref> (Maron & Moore 1994) </ref>, however, does not discuss any search heuristic, and assumes that a fixed set of models is given by some external source. CHAPTER 4.
Reference: <author> Marquardt, D. W. </author> <year> (1963), </year> <title> "An algorithm for least-squares estimation of nonlinear parameters", </title> <journal> Journal of the Society for Industrial and Applied Mathematics 11, </journal> <pages> 431-441. </pages>
Reference-contexts: The reason for keeping the test sets fixed is to avoid variance that arises from the shu*ing in cross-validation. The curve is fitted using Levenberg Marquardt non-linear least squares with starting values of a = b = 0:5 and ff = 0:01 <ref> (Marquardt 1963, Press, Teukolsky, Vetterling & Flannery 1992) </ref>. These values were determined by looking at a few learning curves; we do not attribute any great significance to the starting point. After the curve is fitted, the accuracy performance is estimated by plugging in the full dataset size. <p> Both models are modified to account for the roughness of the learning curve by a noise term that has a beta distribution. Parameters for the model were found by fitting the curve using the Levenberg-Marquardt nonlinear optimization <ref> (Marquardt 1963, Press et al. 1992) </ref>. 3.8 Future Work Future, n.
Reference: <author> Masek, W. J. </author> <year> (1976), </year> <title> A fast algorithm for the string editing problem and decision graph complexity, </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology. </institution>
Reference-contexts: It can be shown that a read-once decision graph must have at least m (n m + 1) + 2 nodes in the graph because the function must update the count of ones (0 to m) each time a feature is read <ref> (Masek 1976) </ref>. If at any interior level (excluding the top m and bottom log 2 m) there are less than m nodes, then they cannot represent the sum of ones, and the graph cannot compute the correct function. <p> Two important theorems tell us that an algorithm in SPACE (S (n)) for S (n) log n has a branching program complexity of at most c S (n) for some constant c <ref> (Masek 1976) </ref>, and that constant-width branching programs are very powerful, being able to accept all NC 1 languages (Barrington 1989). Krause & Waack (1991) studied decision graphs of linear depth and gave exponential lower bounds on several graph accessibility problems.
Reference: <author> McLachlan, G. J. </author> <year> (1976), </year> <title> "The bias of the apparent error rate in discriminant analysis", </title> <journal> Biometrika 63, </journal> <pages> 239-244. </pages> <note> BIBLIOGRAPHY 265 McLachlan, </note> <author> G. J. </author> <year> (1992), </year> <title> Discriminant Analysis and Statistical Pattern Recognition, </title> <publisher> Wiley-Interscience. </publisher>
Reference-contexts: ACCURACY ESTIMATION 71 Knoke (1986) provided a survey of error estimation of classification rules, but he assumed that classification is based on linear discriminant functions. Because the asymptotic bias of the resubstitution estimate is on the order of 1=m <ref> (McLachlan 1976) </ref> for linear discriminant functions, he concluded that "for large samples, there is no need to look further than the resubstitution estimator when seeking a robust method." Jain et al. (1987) compared the performance of the *0 bootstrap and leave-one-out cross-validation with nearest-neighbor classifiers using artificial data and claimed that
Reference: <author> Mehta, M., Rissanen, J. & Agrawal, R. </author> <year> (1995), </year> <title> MDL-based decision tree pruning, </title> <editor> in U. </editor> <publisher> M. </publisher>
Reference: <editor> Fayyad & R. Uthurusamy, eds, </editor> <booktitle> "Proceedings of the first international conference on knowledge discovery and data mining", </booktitle> <publisher> AAAI Press, </publisher> <pages> pp. 216-221. </pages>
Reference: <author> Meinel, C. </author> <year> (1992), </year> <title> "Branching programs | an efficient data structure for computer-aided circuit design", </title> <booktitle> Bulletin of the European Association For Theoretical Computer Science 46, </booktitle> <pages> 149-170. </pages>
Reference-contexts: that for some functions there exists a polynomially sized decision graph but not a polynomially-sized oblivious decision graph of depth O (n); moreover, there exists a function for which the polynomially-sized decision graph is read-once and there is no read-many polynomially-sized oblivious decision graph that is of depth O (n) <ref> (Meinel 1992) </ref>.
Reference: <author> Meinel, C., Krause, M. & Waack, S. </author> <year> (1988), </year> <title> Separating the eraser turing machine classes L e , N L e , coN L e , and P e , in "Mathematical Foundations of Computer Science", </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Vol 324, </volume> <publisher> Springer-Verlag, </publisher> <pages> pp. 405-409. </pages>
Reference: <author> Michalski, R. S. </author> <year> (1978), </year> <title> A planar geometric model for representing multidimensional discrete spaces and multiple-valued logic functions, </title> <type> Technical Report UIUCDCS-R-78-897, </type> <institution> University of Illinois at Urbaba-Champaign. </institution>
Reference-contexts: We conclude that much of the inductive power indeed comes from feature subset selection, and that for the datasets tested, a small subset of features has high predictive power. Appendix B shows General Logic Diagrams <ref> (Michalski 1978) </ref>, which are two dimensional projections of the projected space, for the target concepts. In Chapter 6, we describe OODGs, oblivious read-once decision graphs, and show that such graph structures have some advantages over decision-trees.
Reference: <author> Michalski, R. S. & Imam, I. F. </author> <year> (1994), </year> <title> Learning problem-oriented decision structures from decision rules: The aqdt-2 system, </title> <editor> in Z. W. Ras & M. Zemankova, eds, </editor> <booktitle> "Proceedings of the 8th International Symposium on Methodology for Intelligent Systems (ISMIS-94)", Lecture Notes in Artificial Intelligence 914, </booktitle> <publisher> Springer Verlag, </publisher> <pages> pp. 416-426. </pages>
Reference: <author> Michalski, R. S., Mozetic, I., Hong, J. & Lavrac, N. </author> <year> (1986), </year> <title> The multipurpose incremental learning system AQ15 and its testing application to three medical domains, </title> <booktitle> in "Proceedings of the Fifth National Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kauf-mann, </publisher> <pages> pp. 1041-1045. </pages>
Reference-contexts: The decision structures are constructed from rules (as opposed to construction from instances in the standard supervised machine learning setting), which are in turn derived by the AQ15 system <ref> (Michalski, Mozetic, Hong & Lavrac 1986) </ref> or one of its variants. The AQDT-2 algorithm selects tests for nodes based on five criteria that measure properties of the decision rules: measuring cost, disjointness of classes, rule importance score, value distribution, and dominance of features.
Reference: <author> Michie, D. </author> <year> (1987), </year> <title> Current developments in expert systems, </title> <editor> in J. R. Quinlan, ed., </editor> <booktitle> "Applications of Expert Systems", </booktitle> <volume> Vol. 1, </volume> <publisher> Turing Institute Press, </publisher> <pages> chapter 8, pp. 137-156. </pages>
Reference: <author> Miller, A. J. </author> <year> (1984), </year> <title> "Selection of subsets of regression variables", </title> <journal> Royal Statistical Society A 147, </journal> <pages> 389-425. </pages>
Reference: <author> Miller, A. J. </author> <year> (1990), </year> <title> Subset Selection in Regression, Chapman and Hall. BIBLIOGRAPHY 266 Minato, </title> <editor> S. </editor> <year> (1992), </year> <title> "Minimum-width method of variable ordering for binary decision diagrams", </title> <journal> IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences E75-A(3), </journal> <pages> 392-399. </pages>
Reference: <author> Minato, S., Ishiura, N. & Yajima, S. </author> <year> (1990), </year> <title> Shared binary decision diagram with attributed edges for efficient boolean function manipulation, </title> <booktitle> in "Proceedings of the 27th ACM/IEEE Design Automation Conference", </booktitle> <pages> pp. 24-28. </pages>
Reference: <author> Minsky, M. L. & Papert, S. </author> <year> (1988), </year> <title> Perceptrons : an Introduction to Computational Geometry, </title> <publisher> MIT Press. Expanded ed. </publisher>
Reference: <author> Mitchell, T. M. </author> <year> (1982), </year> <title> "Generalization as search", </title> <booktitle> Artificial Intelligence 18, </booktitle> <pages> 203-226. </pages>
Reference: <editor> Reprinted in Shavlik and Dietterich (eds.) </editor> <booktitle> Readings in Machine Learning. </booktitle>
Reference: <author> Mitchell, T. M. & Thrun, S. B. </author> <year> (1993), </year> <title> Explanation based learning: A comparison of symbolic and nerual network approaches, </title> <booktitle> in "Proceedings of the Tenth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 197-204. </pages>
Reference: <author> Mitchell, T. M., Keller, R. M. & Kedar-Cabelli, S. T. </author> <year> (1986), </year> <title> "Explanation-based generalization: A unifying view", </title> <booktitle> Machine Learning 1(1), </booktitle> <pages> 47-80. </pages> <booktitle> ML- (1992), Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <publisher> Mor-gan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Induction algorithms that induce comprehensible structures aid in understanding the domain and may constitute new knowledge (Dietterich 1986, Newell 1982). As shown in Figure 1.1, supervised classification learning is a small subset of the machine learning field. Speedup learning, exemplified by Explanation-Based Learning <ref> (Mitchell, Keller & Kedar-Cabelli 1986) </ref>, deals with exploiting knowledge to speed up the efficiency of existing processes (e.g., introduction of macro operators and metalevel control knowledge).
Reference: <author> Mladenic, D. </author> <year> (1995), </year> <title> Automated model selection, </title> <booktitle> in "ECML workshop on Knowledge Level Modeling and Machine Learning". </booktitle>
Reference: <author> Modrzejewski, M. </author> <year> (1993), </year> <title> Feature selection using rough sets theory, </title> <editor> in P. B. Brazdil, ed., </editor> <booktitle> "Proceedings of the European Conference on Machine Learning", </booktitle> <publisher> Springer, </publisher> <pages> pp. 213-226. </pages>
Reference: <author> Moore, A. W. & Lee, M. S. </author> <year> (1994), </year> <title> Efficient algorithms for minimizing cross validation error, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Given a confidence level, one can determine *, and hence a confidence interval for f fl (s), from the above formula. The paper <ref> (Maron & Moore 1994) </ref>, however, does not discuss any search heuristic, and assumes that a fixed set of models is given by some external source. CHAPTER 4.
Reference: <author> Moore, A. W., Hill, D. J. & Johnson, M. P. </author> <year> (1992), </year> <title> An empirical investigation of brute force to choose features, smoothers and function approximators, </title> <editor> in S. Hanson, S. Judd & T. Petsche, eds, </editor> <booktitle> "Computational Learning Theory and Natural Learning Systems Conference", </booktitle> <volume> Vol. 3, </volume> <publisher> MIT Press. BIBLIOGRAPHY 267 Moret, </publisher> <editor> B. M. E. </editor> <year> (1982), </year> <title> "Decision trees and diagrams", </title> <journal> ACM Computing Surveys 14(4), </journal> <pages> 593-623. </pages>
Reference: <author> Mosteller, F. & Tukey, J. W. </author> <year> (1968), </year> <title> Data analysis, including statistics, </title> <editor> in G. Lindzey & E. Aronson, eds, </editor> <booktitle> "Handbook of Social Psychology", </booktitle> <volume> Vol. 2, </volume> <publisher> Addison Wesley. </publisher>
Reference: <author> Muggleton, S. H. </author> <year> (1990), </year> <title> Inductive acquisition of expert knowledge, </title> <publisher> Addison-Wesley, </publisher> <address> Wok-ingham, England. </address>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1995), </year> <note> UCI repository of machine learning databases, http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: Real-world domains are useful because they come from real-world problems that we do not always understand and are therefore actual problems on which we would like to improve performance. All real-world datasets used are from the UC Irvine repository <ref> (Murphy & Aha 1995) </ref>, which contains over 100 datasets mostly contributed by researchers in the field of machine learning 2 The real-world datasets were chosen based on the following criteria: dataset size, reasonable encoding, comprehensibility, non-triviality, and age. <p> To choose a set of datasets, we looked at the learning curves for C4.5 and Naive-Bayes for most of the supervised classification datasets at the UC Irvine repository <ref> (Murphy & Aha 1995) </ref> that contained more than 500 instances (about 25 such datasets). To ensure little variance, we chose datasets with at least 500 instances for testing. <p> In practice, we have not seen a phase transition using the C4.5 or Naive-Bayes algorithms on any of the datasets in the UC Irvine repository <ref> (Murphy & Aha 1995) </ref>; however, if they do occur between datasets of sizes m m=k and m, they are likely to cause the learning curves to cross, and hence the wrong model will be selected. <p> The algorithm starts from a decision tree and converts it to an OODG using the minimum description length principle (Rissanen 1986, Rissanen 1978). Their algorithm performed extremely well on many artificial domains but rather poorly on the real-domains from UC Irvine repository <ref> (Murphy & Aha 1995) </ref>. General decision graphs were investigated by Oliver, Dowe & Wallace (1992) and Oliver (1993). The algorithms construct decision graphs top-down, by doing a hill-climbing search through the space of graphs, estimating the usefulness of each graph by Wallace's MMLP (minimum message length principle).
Reference: <author> Murthy, S. K., Kasif, S. & Salzberg, S. </author> <year> (1994), </year> <title> "A system for the induction of oblique decision trees", </title> <journal> Journal of Artificial Intelligence Research 2, </journal> <pages> 1-33. </pages>
Reference: <author> Murthy, S., Kasif, S., Salzberg, S. & Beigel, R. </author> <year> (1993), </year> <title> OC1: Randomized induction of oblique decision trees, </title> <booktitle> in "Eleventh National Conference on Artificial Intelligence", </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 322-327. </pages>
Reference-contexts: GLOBAL COMPARISON 245 IDTM The IDTM algorithm described in Chapter 5 on page 130. NB Naive-Bayes with entropy discretization. NB-FSS-back Naive-Bayes with entropy discretization and backward best-first search wrapper with compound operators as described in Section 4.5 on page 102. OC1 The OC1 V3.0 algorithm <ref> (Murthy, Kasif, Salzberg & Beigel 1993, Murthy, Kasif & Salzberg 1994) </ref>, which induces decision trees with oblique splits. OneR The 1R induction algorithm described in Holte (1993). The implementation was done in MLC ++ . APPENDIX E. GLOBAL COMPARISON 246 Table E.1: A global comparison for real-world datasets.
Reference: <author> Naeher, S. </author> <year> (1995), </year> <title> LEDA: A Library of Efficient Data Types and Algorithms, </title> <institution> 3.2 edn, Max-Planck-Institut fuer Informatik, </institution> <address> IM Stadtwald, D-66123 Saarbruecken, FRG. </address> <note> Available by anonymous ftp in ftp.mpi-sb.mpg.de. </note>
Reference: <author> Narendra, M. P. & Fukunaga, K. </author> <year> (1977), </year> <title> "A branch and bound algorithm for feature subset selection", </title> <journal> IEEE Transactions on Computers C-26(9), </journal> <pages> 917-922. </pages>
Reference: <author> Neter, J., Wasserman, W. & Kutner, M. H. </author> <year> (1990), </year> <title> Applied Linear Statistical Models, 3rd edn, </title> <type> Irwin: Homewood, </type> <institution> IL. </institution>
Reference-contexts: In some cases, measures can be devised that are algorithm specific, and these may be computed efficiently. For example, in linear regression, measures such as Mallow's C p (Mallows 1973) and PRESS (Prediction sum of squares) <ref> (Neter, Wasserman & Kutner 1990) </ref> have been devised so that they do not require running the regression many times, and thus avoid the cross-validation step used in the default wrapper setup. These measures CHAPTER 4.
Reference: <author> Newell, A. </author> <year> (1982), </year> <title> "The knowledge level", </title> <booktitle> Artificial Intelligence 18, </booktitle> <pages> 87-127. </pages> <note> Originally appeared in the AI Magazine, 2(2), </note> <year> 1981. </year>
Reference: <author> Nilsson, N. J. </author> <year> (1990), </year> <title> The Mathematical Foundations of Learning Machines, </title> <publisher> Morgan Kauf-mann Publishers, </publisher> <address> Inc. </address> <note> Previously published as: Learning Machines, </note> <year> 1965. </year>
Reference: <author> O'Kane, D. </author> <year> (1994), </year> <title> "Learning to classify in large committee machines", </title> <journal> Physical Review E 50(4), </journal> <pages> 3201-3209. </pages> <note> BIBLIOGRAPHY 268 Oliveira, </note> <author> A. L. & Sangiovanni-Vincentelli, A. </author> <year> (1995), </year> <title> Inferring reduced ordered decision graphs of minimum description length, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Twelfth International Conference", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 421-429. </pages>
Reference: <author> Oliver, J., Dowe, D. & Wallace, C. </author> <year> (1992), </year> <title> Inferring decision graphs using the minimum message length principle, </title> <editor> in A. Adams & L. Sterling, eds, </editor> <booktitle> "Proceedings of the 5th Australian Joint Conference on Artificial Intelligence", World Scientific, Singapore, </booktitle> <pages> pp. 361-367. </pages>
Reference: <author> Oliver, J. J. </author> <year> (1993), </year> <title> Decision graphs|an extension of decision trees, </title> <booktitle> in "Proceedings of the fourth International workshop on Artificial Intelligence and Statistics", </booktitle> <pages> pp. 343-350. </pages>
Reference-contexts: DGRAPH <ref> (Oliver 1993) </ref> is an algorithm for building general decision graphs (with no restrictions), using an MDL criterion for splitting and merging nodes.
Reference: <author> Olshen, R. A., Gilpin, E. A., Henning, H., LeWinter, M. L., Collins, D. & Ross, J. </author> <year> (1985), </year> <title> Twelve-month prognosis following myocardial infarction: Classification trees, logistic regression, and stepwise linear discrimination, </title> <editor> in L. L. Cam, R. Olshen & C. Chin-Shiu, eds, </editor> <booktitle> "Proceedings of the Berkeley conference in honor of Jerzy Neyman and Jack Kiefer", </booktitle> <volume> Vol. 1, </volume> <publisher> Wadsworth Advanced Books & Software. </publisher>
Reference: <author> Pagallo, G. & Haussler, D. </author> <year> (1990), </year> <title> "Boolean feature discovery in empirical learning", </title> <booktitle> Machine Learning 5, </booktitle> <pages> 71-99. </pages>
Reference: <author> Pawlak, Z. </author> <year> (1987), </year> <title> "Decision tables | a rough sets approach", </title> <journal> Bull. of EATCS 33, </journal> <pages> 85-96. </pages>
Reference-contexts: The rough sets community has been using the hypothesis space of decision tables for a few years <ref> (Pawlak 1987, Pawlak 1991, Slowinski 1992) </ref>. Researchers in the field of rough sets suggested using the degrees-of-dependency of a feature on the label (called fl) to determine which features should be included in a decision table (Ziarko 1991, Modrzejewski 1993).
Reference: <author> Pawlak, Z. </author> <year> (1991), </year> <title> Rough Sets, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Borrowing some terminology from rough sets <ref> (Pawlak 1991, Slowinski 1992) </ref>, the set of strongly relevant features form the core and any set of features that allow a Bayes classifier to achieve the highest possible accuracy forms a reduct. A reduct can only contain strongly relevant and weakly relevant features. <p> All the algorithms described above assume that the evaluation function is deterministic. Langley (1994) reviewed feature subset selection methods in machine learning and contrasted the wrapper and filter approaches. The theory of rough sets defines notions of relevance that are closely related to the ones defined here <ref> (Pawlak 1991) </ref>. Pawlak (1993) wrote that one of the most important and fundamental notions to the rough sets philosophy is the need to discover redundancy and dependencies between features, and there has been a lot of work on feature subset selection coming from the rough sets community (cf.
Reference: <author> Pawlak, Z. </author> <year> (1993), </year> <title> "Rough sets: present state and the future", </title> <booktitle> Foundations of Computing and Decision Sciences 18(3-4), </booktitle> <pages> 157-166. </pages>
Reference: <author> Pawlak, Z., Wong, S. & Ziarko, W. </author> <year> (1988), </year> <title> "Rough sets: Probabilistic versus deterministic approach", </title> <journal> Internation Journal of Man Machine Studies 29, </journal> <pages> 81-95. </pages>
Reference-contexts: Researchers in the field of rough sets suggested using the degrees-of-dependency of a feature on the label (called fl) to determine which features should be included in a decision table (Ziarko 1991, Modrzejewski 1993). Another suggestion was to use normalized entropy <ref> (Pawlak, Wong & Ziarko 1988) </ref>, which is similar to the information gain measure of ID3 and CART.
Reference: <author> Pazzani, M. J. </author> <year> (1995), </year> <title> Searching for dependencies in bayesian classifiers, </title> <editor> in D. Fisher & H. Lenz, eds, </editor> <booktitle> "Proceedings of the fifth International Workshop on Artificial Intelligence and Statistics", </booktitle> <address> Ft. Lauderdale, FL. </address>
Reference: <author> Perrone, M. </author> <year> (1993), </year> <title> Improving regression estimation: averaging methods for variance reduction with extensions to general convex measure optimization, </title> <type> PhD thesis, </type> <institution> Brown University, Physics Dept. </institution> <note> BIBLIOGRAPHY 269 Pimat, </note> <author> V., Kononenko, I., Janc, T. & Bratko, I. </author> <year> (1989), </year> <title> Medical estimation of automatically induced decision rules, </title> <booktitle> in "Proceedings of the 2nd European conference on Artificial Intelligence in Medicine", </booktitle> <pages> pp. 24-36. </pages>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T. & Flannery, B. P. </author> <year> (1992), </year> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing, second edn, </booktitle> <publisher> Cambridge University Press. </publisher>
Reference: <author> Provan, G. M. & Singh, M. </author> <year> (1995), </year> <title> Learning bayesian networks using feature selection, </title> <editor> in D. Fisher & H. Lenz, eds, </editor> <booktitle> "Proceedings of the fifth International Workshop on Artificial Intelligence and Statistics", </booktitle> <address> Ft. Lauderdale, FL, </address> <pages> pp. 450-456. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986), </year> <title> "Induction of decision trees", </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: GASOIL (an expert system for Hydrocarbon separation and configuration) and BMT (an expert system for configuring of fire-protection equipment in buildings) had 2,800 rules and 30,000 rules, respectively, and were constructed using ID3 <ref> (Quinlan 1986) </ref>, a decision tree induction algorithm. The time to construct MYCIN and XCON was estimated to be an order of magnitude greater than BMT, which Muggleton claimed is the largest expert system in full-time commercial use. Of course, induction cannot replace experts. <p> The Naive-Bayes algorithm is explained below. The specific details are not essential for the rest of the dissertation. The C4.5 algorithm (Quinlan 1993) is a descendent of ID3 <ref> (Quinlan 1986) </ref>, which builds decision trees top-down and prunes them. The tree is constructed by finding the best single-feature test to conduct at the root node of the tree. After the test is chosen, the instances are split according to the test, and the subproblems are solved recursively.
Reference: <author> Quinlan, J. R. </author> <year> (1987), </year> <title> "Simplifyng decision trees", </title> <journal> International Journal of Man-Machine Studies 27, </journal> <pages> 221-234. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1988), </year> <title> An empirical comparison of genetic and decision-tree classifiers, </title> <booktitle> in "Proceedings of the Fifth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 135-141. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1989), </year> <title> Unknown attribute values in induction, </title> <booktitle> in "Proceedings of the Sixth International Machine Learning Workshop", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 164-168. </pages>
Reference-contexts: Another weak point is the fact that missing values are considered distinct values, while research indicates that this is usually the worst possible approach to handling them <ref> (Quinlan 1989) </ref>.
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, California. </address>
Reference-contexts: We test different search engines to search the space of feature subsets and introduce compound operators to speed the search. We conclude with an example of how the wrapper approach can be used to tune parameters for C4.5 <ref> (Quinlan 1993) </ref> and improve its performance on artificial and real-world problems. In Chapter 5, we evaluate the power of feature subset selection as the only inductive process in an inducer. We use a structure called DTM, which is a decision table with a CHAPTER 1. <p> CART (Breiman et al. 1984) and C4.5 <ref> (Quinlan 1993) </ref> are decision tree inducers that build decision tree classifiers. We now formally define the symbols used. Appendix F contains a summary of these symbols. Let the domain of feature X i be Dom (X i ). <p> DEFINITIONS AND METHODOLOGY 20 Quinlan (1993), Breiman et al. (1984), Fayyad (1991), Buntine (1992), and Moret (1982); hence we will not describe them in detail. The Naive-Bayes algorithm is explained below. The specific details are not essential for the rest of the dissertation. The C4.5 algorithm <ref> (Quinlan 1993) </ref> is a descendent of ID3 (Quinlan 1986), which builds decision trees top-down and prunes them. The tree is constructed by finding the best single-feature test to conduct at the root node of the tree. <p> A relatively unknown post processing step in C4.5 replaces a node by one of its children if the accuracy of the child is considered better <ref> (Quinlan 1993, page 39) </ref>. In one case (the corral database described below), this had a significant impact on the resulting tree: although the root split was incorrect, it was replaced by one of the children. <p> [requirement for parameter tuning], which is named the "China Syndrome" because sometimes the only person who is able to make a program run is in China. |Buchanan, 1987 talk [paraphrased in Catlett (1991a)] We now describe an application of the wrapper approach to parameter tuning for the C4.5 induction algorithm <ref> (Quinlan 1993) </ref>. The C4.5-AP algorithm is a wrapper around C4.5 that attempts to automatically tune some parameters in C4.5 that can be adjusted. We chose to automatically set all of the C4.5 tree-building parameters (m,c,g, and s) shown in Table 4.13.
Reference: <author> Quinlan, J. R. </author> <year> (1994), </year> <title> Comparing connectionist and symbolic learning methods, </title> <editor> in S. </editor> <publisher> J. </publisher>
Reference-contexts: The following datasets were also eliminated for this reason: ionosphere, letter, net-talk, segment, tic-tac-toe, and vehicle. Classifiers such as nearest-neighbors and neural-nets are more suited for these parallel tasks <ref> (Quinlan 1994) </ref>, unless the data are preprocessed (see Dietterich, Hild & Bakiri (1995) for an example where using block-encoding on a text-to-speech task drastically improved the performance of ID3 and made it indistinguishable from backpropagation).
Reference: <editor> Hanson, G. A. Drastal & R. L. Rivest, eds, </editor> <title> "Computational Learning Theory and Natural Learning Systems", Vol. I: Constraints and Prospects, </title> <publisher> MIT Press, </publisher> <pages> chapter 15, pp. 445|456. </pages>
Reference: <author> Quinlan, J. R. & Rivest, R. L. </author> <year> (1989), </year> <title> "Inferring decision trees using the minimum description length principle", </title> <booktitle> Information and Computation 80, </booktitle> <pages> 227-248. </pages>
Reference-contexts: Another weak point is the fact that missing values are considered distinct values, while research indicates that this is usually the worst possible approach to handling them <ref> (Quinlan 1989) </ref>.
Reference: <author> Raghavan, V. & Wilkins, D. </author> <year> (1993), </year> <title> Learning -branching programs with queries, </title> <booktitle> in "Proceedings of the Sixth Annual Workshop on Computational Learning Theory", </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <pages> pp. 27-36. </pages> <note> BIBLIOGRAPHY 270 Reinwald, </note> <author> L. T. & Soland, R. M. </author> <year> (1966), </year> <title> "Conversion of limited-entry decision tables to optimal computer programs i: Minimum average processing time", </title> <journal> Journal of the ACM 13(3), </journal> <pages> 339-358. </pages>
Reference: <author> Reinwald, L. T. & Soland, R. M. </author> <year> (1967), </year> <title> "Conversion of limited-entry decision tables to optimal computer programs ii: Minimum storage requirement", </title> <journal> Journal of the ACM 14(4), </journal> <pages> 742-755. </pages>
Reference: <author> Rendell, L. & Seshu, R. </author> <year> (1990), </year> <title> "Learning hard concepts through constructive induction: framework and rationale", </title> <booktitle> Computational Intelligence 6(4), </booktitle> <pages> 247-270. </pages>
Reference: <author> Rice, J. A. </author> <year> (1988), </year> <title> Mathematical Statistics and Data Analysis, </title> <publisher> Wadsworth & Brooks/Cole. </publisher>
Reference: <author> Rissanen, J. </author> <year> (1978), </year> <title> "Modeling by shortest data description", </title> <type> Automatica 14, </type> <pages> 465-471. </pages>
Reference-contexts: This method can then be applied recursively to both of the partitions induced by T min until some stopping condition is achieved, thus creating multiple intervals on the feature X . Fayyad and Irani make use of the Minimum Description Length Principle <ref> (Rissanen 1978, Rissanen 1986, Wallace & Freeman 1987) </ref> to determine a stopping criterion for their recursive discretization strategy.
Reference: <author> Rissanen, J. </author> <year> (1986), </year> <title> "Stochastic complexity and modeling", </title> <journal> Ann. </journal> <volume> Statist 14, </volume> <pages> 1080-1100. </pages>
Reference-contexts: The algorithm starts from a decision tree and converts it to an OODG using the minimum description length principle <ref> (Rissanen 1986, Rissanen 1978) </ref>. Their algorithm performed extremely well on many artificial domains but rather poorly on the real-domains from UC Irvine repository (Murphy & Aha 1995). General decision graphs were investigated by Oliver, Dowe & Wallace (1992) and Oliver (1993).
Reference: <author> Rosenblatt, F. </author> <year> (1958), </year> <title> "The perceptron: A probabilistic model for information storage and organization in the brain", </title> <journal> Psychological Review 65, </journal> <pages> 386-408. </pages>
Reference-contexts: A perceptron, sometimes called a neuron, is a classifier that is usually associated with the perceptron error correcting rule <ref> (Rosenblatt 1958, Nilsson 1990, Minsky & Papert 1988) </ref>. Multi-layer network of perceptrons are called neural networks and are usually associated with the backpropagation rule for learning (Rumelhart, Hinton & Williams 1986, Hertz, Krogh & Palmer 1991).
Reference: <author> Rumelhart, D. E., Hinton, G. E. & Williams, R. J. </author> <year> (1986), </year> <title> Learning Internal Representations by Error Propagation, </title> <publisher> MIT Press, </publisher> <address> chapter 8. </address>
Reference-contexts: A perceptron, sometimes called a neuron, is a classifier that is usually associated with the perceptron error correcting rule (Rosenblatt 1958, Nilsson 1990, Minsky & Papert 1988). Multi-layer network of perceptrons are called neural networks and are usually associated with the backpropagation rule for learning <ref> (Rumelhart, Hinton & Williams 1986, Hertz, Krogh & Palmer 1991) </ref>. A Bayes rule, or a Bayes classifier, is a rule that predicts the most probable class for a given instance, based on the full distribution D (assumed to be known).
Reference: <author> Russell, S. J. & Norvig, P. </author> <year> (1995), </year> <title> Artificial Intelligence: A Modern Approach, </title> <publisher> Prentice Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey 07632. </address>
Reference-contexts: The results, especially on the artificial datasets where we know what the relevant feature are, indicate that the feature subset selection is getting stuck at local maxima too often. The next section deals with improving the search engine. 4.4.2 A best-first Search Engine Best-first search <ref> (Russell & Norvig 1995, Ginsberg 1993) </ref> is a more robust method than hill-climbing. The idea is to select the most promising node we have generated so far that has not already been expanded.
Reference: <author> Schaffer, C. </author> <year> (1993), </year> <title> "Selecting a classification method by cross-validation", </title> <booktitle> Machine Learning 13(1), </booktitle> <pages> 135-143. </pages>
Reference-contexts: If there are only a few models, as is the case when one chooses between three induction algorithms, one can estimate the accuracy of each one and select the one with the highest accuracy <ref> (Schaffer 1993) </ref> or perhaps even find some underlying theory to help predict the best one for a given dataset (see Brazdil, Gama & Henery (1994) for an attempt that was not very successful in finding regularities in the StatLog project).
Reference: <author> Schaffer, C. </author> <year> (1994), </year> <title> A conservation law for generalization performance, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 259-265. </pages>
Reference-contexts: On one dataset, vehicle, the generalization accuracy of the Naive-Bayes algorithm deteriorated by more than 4% as more instances were given. A similar phenomenon was observed on the shuttle dataset (not shown). Such a phenomenon was predicted by Schaffer and Wolpert <ref> (Schaffer 1994, Wolpert 1994b) </ref>, but we were surprised that it was observed on two real-world datasets. Figure 3.3 shows a plot of the "circularity" feature versus label value two in the vehicle dataset. The distribution is approximately bimodal, and thus violates the normality assumption made by Naive-Bayes.
Reference: <author> Schapire, R. E. </author> <year> (1990), </year> <title> "The strength of weak learnability", </title> <booktitle> Machine Learning 5(2), </booktitle> <pages> 197-227. </pages>
Reference: <author> Schumacher, H. & Sevcik, K. C. </author> <year> (1976), </year> <title> "The synthetic approach to decision table conversion", </title> <journal> Communications of the ACM 19(6), </journal> <pages> 343-351. </pages> <note> BIBLIOGRAPHY 271 Shannon, </note> <author> C. E. </author> <year> (1949), </year> <title> "The synthesis of two-terminal switching circuits", </title> <journal> The Bell System Technical Journal 28(1), </journal> <pages> 59-98. </pages>
Reference: <author> Shao, J. </author> <year> (1993), </year> <title> "Linear model selection via cross-validation", </title> <journal> Journal of the American Statistical Association 88(422), </journal> <pages> 486-494. </pages>
Reference: <author> Shapiro, A. & Niblett, T. </author> <year> (1982), </year> <title> Automatic induction of classification rules for a chess endgame, </title> <editor> in M. R. B. Clarke, ed., </editor> <booktitle> "Advances in Computer Chess", 3, </booktitle> <address> Oxford: Perga-mon. </address>
Reference: <author> Shawe-Taylor, J., Anthony, M. & Biggs, N. </author> <year> (1993), </year> <title> "Bounding sample size with the vapnik chervonenkis dimension", </title> <booktitle> Discrete Applied Mathematics 42(1), </booktitle> <pages> 65-73. </pages>
Reference: <author> Siedlecki, W. & Sklansky, J. </author> <year> (1988), </year> <title> "On automatic feature selection", </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence 2(2), </journal> <pages> 197-220. </pages>
Reference-contexts: More recent papers attempt to use AI techniques, such as beam search and bidirectional search <ref> (Siedlecki & Sklansky 1988) </ref>, best-first search (Xu, Yan & Chang 1989), and genetic algorithms (Vafai & De Jong 1992, Vafai & De Jong 1993). All the algorithms described above assume that the evaluation function is deterministic.
Reference: <author> Simon, H. </author> <year> (1983), </year> <title> Why should machines learn?, </title> <editor> in R. S. Michalski, J. G. Carbonell & T. M. Mitchell, eds, </editor> <booktitle> "Machine learning: An Artificial Intelligence Approach", </booktitle> <volume> Vol. 1, </volume> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Singh, M. & Provan, G. M. </author> <year> (1995), </year> <title> A comparison of induction algorithms for selective and non-selective bayesian classifiers, </title> <booktitle> in "Machine Learning: Proceedings of the Twelfth International Conference". </booktitle>
Reference: <author> Slowinski, R. </author> <year> (1992), </year> <title> Intelligent decision support : handbook of applications and advances of the rough sets theory, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Smith, D. E. & Genesereth, M. R. </author> <year> (1985), </year> <title> "Ordering conjunctive queries", </title> <journal> Artificial Intelligence Journal 26(3), </journal> <pages> 171-215. </pages>
Reference: <author> Spackman, A. K. </author> <year> (1988), </year> <title> Learning categorical criteria in biomedical domains, </title> <booktitle> in "Proceedings of the Fifth International Machine Learning Conference", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 36-46. </pages>
Reference-contexts: Such target concepts are common in medical domains where a patient needs to exhibit at least m of a set of n symptoms to be diagnosed with some disease <ref> (Spackman 1988) </ref>. Towell & Shavlik (1993) showed the representation of neural networks closely resembles m-of-n concepts. This concept is extremely hard in terms of feature interactions. Seven features interact here, and most algorithms are unable to identify the correct features. <p> The smallest decision trees for most symmetric functions|functions which yield the same value for all permutations of the input features|are exponentially-sized. Symmetric functions, such as m-of-n, are known to occur in medical domains <ref> (Spackman 1988) </ref> and were useful in converting neural-networks to decision rules (Towell & Shavlik 1993, Craven & Shavlik 1993).
Reference: <author> Spector, P. </author> <year> (1994), </year> <title> An Introduction to S and S-PLUS, </title> <publisher> Duxbury Press. </publisher>
Reference-contexts: With only 50 instances for the breast cancer dataset, each fold contains only five instances, hence the 4 The binning was determined automatically by S-plus <ref> (Spector 1994) </ref>. While these histograms could be made to look more Gaussian by changing the bin sizes, the histograms shown are more informative because they indicate important gaps in the accuracy estimates. CHAPTER 3. ACCURACY ESTIMATION 64 cross-validation (repeated 50 times) CHAPTER 3.
Reference: <author> Staudte, R. G. & Sheather, S. J. </author> <year> (1990), </year> <title> Robust Estimation and Testing, </title> <publisher> John Wiley & Sons, Inc. BIBLIOGRAPHY 272 Stone, </publisher> <editor> C. J. </editor> <year> (1982), </year> <title> "Optimal global rates of convergence for nonparametric regression", </title> <journal> The Annals of Statistics 10(4), </journal> <pages> 1040-1053. </pages>
Reference-contexts: Trimmed means are known to have good properties: they are robust to outliers up to 100ff%, their asymptotic efficiency relative to the untrimmed mean never drops below (1 2ff) 2 , and the standard errors can be estimated easily <ref> (Staudte & Sheather 1990, Rice 1988) </ref>. CHAPTER 3. ACCURACY ESTIMATION 63 with varying times. In order to evaluate this suggestion and similar ones, it is useful to look at the distribution of cross-validation.
Reference: <author> Stone, M. </author> <year> (1974), </year> <title> "Cross-validatory choice and assessment of statistical predictions", </title> <journal> Journal of the Royal Statistical Society B 36, </journal> <pages> 111-147. </pages>
Reference-contexts: Figure 3.25 shows the fit to the ten estimated points for two datasets that generated distinctly different learning curves. 3.7 Related Work I have in mind procedures such as AID, the automatic interactor detector, which guarantees to get significance out of any data whatsoever. |G. A. Barnard <ref> (Stone 1974, Discussion) </ref> Some experimental studies comparing different accuracy estimation methods have been CHAPTER 3. ACCURACY ESTIMATION 70 previously reported, but most of them were on artificial or small datasets. We now describe some of these efforts.
Reference: <author> Stone, M. </author> <year> (1977), </year> <title> "Asymptotics for and against cross-validation", </title> <journal> Biometrika 64(1), </journal> <pages> 29-35. </pages>
Reference-contexts: He also demonstrated the use of cross-validation for estimating the accuracy of a weighted least-squares fit to a satellite dataset with 27 instances. A few years later <ref> (Stone 1977) </ref>, he showed that estimating parameters by leave-one-out cross-validation leads to asymptotically inconsistent estimates in some cases, such as deciding whether to estimate a parameter by the median or the mean.
Reference: <author> Stone, M. </author> <year> (1978), </year> <title> "Cross validation: A review", </title> <journal> Mathematische Operationsforschung und Statistik. Series Statistics 9(1), </journal> <pages> 127-139. </pages>
Reference: <author> Street, W. N., Mangasarian, O. L. & Wolberg, W. H. </author> <year> (1995), </year> <title> An inductive learning approach to prognostic prediction, </title> <booktitle> in "Machine Learning: Proceedings of the Twelfth International Conference". </booktitle>
Reference: <author> Stroustroup, B. </author> <year> (1994), </year> <title> The Design and Evolution of C ++ , Addison-Wesley Publishing Company. </title>
Reference: <author> Takenaga, Y. & Yajima, S. </author> <year> (1993), </year> <title> NP-completeness of minimum binary decision diagram identification, </title> <type> Technical Report COMP 92-99, IEICE. </type>
Reference: <author> Taylor, C., Michie, D. & Spiegalhalter, D. </author> <year> (1994), </year> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Paramount Publishing International. </publisher>
Reference-contexts: The features have been coded to preserve confidentiality. The dataset was first used in Quinlan (1987). There are six continuous features and nine nominal ones. DNA There are 3,186 instances in the StatLog version of the DNA dataset we used <ref> (Taylor et al. 1994) </ref>. The domain is drawn from the field of molecular biology. Splice junctions are points on a DNA sequence at which "superfluous" DNA is removed during protein creation.
Reference: <author> Thrun et al. </author> <year> (1991), </year> <title> The Monk's problems: A performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Thrun, S. & Mitchell, T. M. </author> <year> (1995), </year> <title> Learning one more thing, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> "Proceedings of the 14th International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 1217-1223. </pages>
Reference: <author> Towell, G. G. & Shavlik, J. W. </author> <year> (1993), </year> <title> "Extracting refined rules from knowledge-based neural networks", </title> <booktitle> Machine Learning 13(1), </booktitle> <pages> 71-101. </pages>
Reference-contexts: The smallest decision trees for most symmetric functions|functions which yield the same value for all permutations of the input features|are exponentially-sized. Symmetric functions, such as m-of-n, are known to occur in medical domains (Spackman 1988) and were useful in converting neural-networks to decision rules <ref> (Towell & Shavlik 1993, Craven & Shavlik 1993) </ref>.
Reference: <author> Turney, P. D. </author> <year> (1993), </year> <title> Exploiting context when learning to classify, </title> <editor> in P. B. Brazdil, ed., </editor> <booktitle> "Proceedings of the European Conference on Machine Learning (ECML)", </booktitle> <pages> pp. 402-407. </pages>
Reference: <author> Utgoff, P. E. </author> <year> (1994), </year> <title> An improved algorithm for incremental induction of decision trees, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 318-325. </pages> <note> BIBLIOGRAPHY 273 Utgoff, </note> <author> P. E. </author> <year> (1995), </year> <title> Decision tree induction based on efficient tree restructuring, </title> <type> Technical Report 95-18, </type> <institution> Department of Computer Science, University of Massachusetts. </institution>
Reference: <author> Vafai, H. & De Jong, K. </author> <year> (1992), </year> <title> Genetic algorithms as a tool for feature selection in machine learning, </title> <booktitle> in "Fourth International Conference on Tools with Artificial Intelligence", </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 200-203. </pages>
Reference-contexts: More recent papers attempt to use AI techniques, such as beam search and bidirectional search (Siedlecki & Sklansky 1988), best-first search (Xu, Yan & Chang 1989), and genetic algorithms <ref> (Vafai & De Jong 1992, Vafai & De Jong 1993) </ref>. All the algorithms described above assume that the evaluation function is deterministic. Langley (1994) reviewed feature subset selection methods in machine learning and contrasted the wrapper and filter approaches.
Reference: <author> Vafai, H. & De Jong, K. </author> <year> (1993), </year> <title> Robust feature selection algorithms, </title> <booktitle> in "Fifth International Conference on Tools with Artificial Intelligence", </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 356-363. </pages>
Reference: <author> Valiant, L. G. </author> <year> (1984), </year> <title> "A theory of the learnable", </title> <journal> Communications of the ACM 27, </journal> <note> 1134-1142. </note> <author> van Laarhoven, P. & Aarts, E. </author> <year> (1987), </year> <title> Simulated annealing : Theory and Applications, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: In both versions, the features were selected until there CHAPTER 5. DECISION TABLES 139 were no conflicting instances. Almuallim & Dietterich (1992b) discussed the "Multi-balls" algorithm that has high coverage: for a given sample size, the number of concepts it can learn in Valiant's PAC model <ref> (Valiant 1984, Angluin 1992) </ref> is close to the upper bound of any learning algorithm. DTMs can be viewed as a multi-balls hypothesis space because the centers are equidistant by their definitions. <p> The AQDT-2 algorithm selects tests for nodes based on five criteria that measure properties of the decision rules: measuring cost, disjointness of classes, rule importance score, value distribution, and dominance of features. Results for learning branching programs under Valiant's PAC learning model <ref> (Valiant 1984, Angluin 1992) </ref> are either negative or still open.
Reference: <author> Walker, M. G. </author> <year> (1992), </year> <title> Probability Estimation for Classification Trees and DNA Sequences Analysis, </title> <type> PhD thesis, </type> <institution> Stanford University. STAN-CS-92-1422. </institution>
Reference: <author> Wallace, C. & Freeman, P. </author> <year> (1987), </year> <title> "Estimation and inference by compact coding", </title> <journal> Journal of the Royal Statistical Society B 49, </journal> <pages> 240-265. </pages>
Reference: <author> Wallace, C. & Patrick, J. </author> <year> (1993), </year> <title> "Coding decision trees", </title> <booktitle> Machine Learning 11, </booktitle> <pages> 7-22. </pages>
Reference: <author> Way, J. & Smith, E. A. </author> <year> (1991), </year> <title> "The evolution of synthetic aperture radar systems and their progression to the EOS SAR", </title> <booktitle> IEEE Transactions on Geoscience and Remote Sensing 29(6), </booktitle> <pages> 962-985. </pages>
Reference-contexts: The NASA Earth Observing System (EOS) of orbiting satellites and other spaceborne instruments is projected to generate on the order of 50 gigabytes of remotely sensed image data per hour when operational in the late 1990s and into the next century <ref> (Way & Smith 1991) </ref>. 4. The sky catalog from the Palomar Observatory survey contains billions of entries with raw image data sizes measured in terabytes.
Reference: <author> Wegener, I. </author> <year> (1987), </year> <title> The Complexity of Boolean Functions, </title> <editor> B. G. Tuebner, </editor> <publisher> Stuttgart. </publisher>
Reference: <author> Weiss, S. M. </author> <year> (1991), </year> <title> "Small sample error rate estimation for k-nearest neighbor classifiers", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 13(3), </journal> <pages> 285-289. </pages>
Reference-contexts: et al. (1984, Section 8.7) claimed that, especially for regression, stratification is the preferred method for selecting the right-sized tree; the cross-validation used in CART and in the C4.5 cross-validation utility, xval.sh, is stratified; and Weiss, who has done much work in accuracy estimation, uses stratified cross-validation in his experiments <ref> (Weiss 1991, Weiss & Indurkhya 1994a, Weiss & Indurkhya 1994b) </ref>.
Reference: <author> Weiss, S. M. & Indurkhya, N. </author> <year> (1994a), </year> <title> Decision tree pruning : Biased or optimal, </title> <booktitle> in "Proceedings of the twelfth national conference on artificial intelligence", </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 626-632. </pages> <note> BIBLIOGRAPHY 274 Weiss, </note> <author> S. M. & Indurkhya, N. </author> <year> (1994b), </year> <title> Small sample decision tree pruning, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> "Proceedings of the Eleventh international conference on machine learning", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 335-342. </pages>
Reference: <author> Weiss, S. M. & Kulikowski, C. A. </author> <year> (1991), </year> <title> Computer Systems that Learn, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference-contexts: et al. (1984, Section 8.7) claimed that, especially for regression, stratification is the preferred method for selecting the right-sized tree; the cross-validation used in CART and in the C4.5 cross-validation utility, xval.sh, is stratified; and Weiss, who has done much work in accuracy estimation, uses stratified cross-validation in his experiments <ref> (Weiss 1991, Weiss & Indurkhya 1994a, Weiss & Indurkhya 1994b) </ref>.
Reference: <author> Wettschereck, D. </author> <year> (1994), </year> <title> A Study of Distance-Based Machine Learning Algorithms, </title> <type> PhD thesis, </type> <institution> Oregon State University. </institution>
Reference: <author> Wilson, S. W. </author> <year> (1987), </year> <title> "Classifier systems and the animat problem", </title> <booktitle> Machine Learning. </booktitle>
Reference-contexts: An instance is labelled positive iff the data bit indicated by the numerical interpretation of the address bits is set to one. Although multiplexer-type functions have a small representation in a decision tree, they are known to be very hard to learn using existing top-down recursive partitioning approaches <ref> (Wilson 1987) </ref>. Quinlan (1988) observed that there is no information gain (mutual information is zero) from a split on an address bit at the root because due to symmetry there are an equal number of instances for each class in each child node.
Reference: <author> Wnek, J. & Michalski, R. S. </author> <year> (1994), </year> <title> "Hypothesis-driven constructive induction in AQ17-HCI : A method and experiments", </title> <booktitle> Machine Learning 14(2), </booktitle> <pages> 139-168. </pages>
Reference: <author> Wolberg, W. H. & Mangasarian, O. L. </author> <year> (1990), </year> <title> Multisurface method of pattern separation for medical diagnosis applied to breast cytology, </title> <booktitle> in "Proceedings of the National Academy of Sciences, U.S.A.", </booktitle> <volume> Vol. 87, </volume> <pages> pp. 9193-9196. </pages>
Reference-contexts: CHAPTER 2. DEFINITIONS AND METHODOLOGY 26 We chose to experiment with the following real-world domains: Breast cancer Wisconsin There are 699 instances collected from Dr. Wolberg's clinical cases at the University of Wisconsin <ref> (Wolberg & Mangasarian 1990, Zhang 1992a) </ref>. These were collected over a period of two and a half years, and the problem is to determine whether the tumors were benign or malignant based on data for each cancer patient.
Reference: <author> Wolpert, D. H. </author> <year> (1992a), </year> <title> "On the connection between in-sample testing and generalization error", </title> <booktitle> Complex Systems 6, </booktitle> <pages> 47-94. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1992b), </year> <title> "Stacked generalization", </title> <booktitle> Neural Networks 5, </booktitle> <pages> 241-259. </pages>
Reference-contexts: & Tibshirani (1991) Estimating the accuracy of a classifier induced by a supervised learning algorithm is important not only in order to predict its future performance, but also in order to choose a classifier from a given set (model selection) as in Schaffer (1993), or in order to combine classifiers <ref> (Wolpert 1992b, Breiman 1994a) </ref>. For estimating the final accuracy of a classifier, we would like an estimation method with low bias and low variance.
Reference: <author> Wolpert, D. H. </author> <year> (1994a), </year> <title> Off-training set error and a priori distinctions between learning algorithms, </title> <type> Technical Report SFI TR 94-12-123, </type> <institution> The Sante Fe Institute. </institution>
Reference-contexts: Arrows show the points we chose for accuracy estimation. CHAPTER 3. ACCURACY ESTIMATION 52 assumption made by Naive-Bayes is wrong and becomes worse as enough instances are seen to estimate the mean at 45.5 accurately. and the classifications then become worse. Recently, Wolpert and Schaffer <ref> (Wolpert 1994a, Wolpert 1994b, Schaffer 1994) </ref> have advocated the use of off-training set error for testing.
Reference: <author> Wolpert, D. H. </author> <year> (1994b), </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework, </title> <editor> in D. H. Wolpert, ed., </editor> <title> "The Math-emtatics of Generalization", </title> <publisher> Addison Wesley. </publisher>
Reference-contexts: In this chapter, we explain some of the assumptions made by the different estimation methods and present concrete examples where each method fails. While it is known that no accuracy estimation can be correct all the time <ref> (Wolpert 1994b, Schaffer 1994) </ref>, we are interested in identifying methods that are well suited for the biases and trends in typical real world datasets. For years it was generally assumed that higher folds for cross-validation (up to leave-one-out) would yield better estimates, usually at the expense of longer computation time. <p> We thus believe that this bias is appropriate for problems similar to those we used. The fact that a small subset of relevant features suffices to learn accurate classifiers is one good example that the uniform assumption over target concepts that is made in the no-free-lunch theorems <ref> (Wolpert 1994b, Schaffer 1994) </ref> is irrelevant (pun intended) to the real world. We have shown that the resulting decision tables are very small and use few features, allowing one to concisely display them using General Logic Diagrams (Appendix B).
Reference: <author> Wolpert, D. H. </author> <year> (1995), </year> <title> On bias plus variance, </title> <type> Technical Report SFI TR 95-007, </type> <institution> Santa Fe Institute. </institution>
Reference: <author> Xu, L., Yan, P. & Chang, T. </author> <year> (1989), </year> <title> Best first strategy for feature selection, </title> <booktitle> in "Ninth International Conference on Pattern Recognition", </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 706-708. </pages>
Reference-contexts: More recent papers attempt to use AI techniques, such as beam search and bidirectional search (Siedlecki & Sklansky 1988), best-first search <ref> (Xu, Yan & Chang 1989) </ref>, and genetic algorithms (Vafai & De Jong 1992, Vafai & De Jong 1993). All the algorithms described above assume that the evaluation function is deterministic. Langley (1994) reviewed feature subset selection methods in machine learning and contrasted the wrapper and filter approaches.
Reference: <author> Yan, D. & Mukai, H. </author> <year> (1992), </year> <note> "Stochastic discrete optimization", Siam J. Control and Optimization 30(3), 594-612. BIBLIOGRAPHY 275 Yu, </note> <author> B. & Yuan, B. </author> <year> (1993), </year> <title> "A more efficient branch and bound algorithm for feature selection", </title> <booktitle> Pattern Recognition 26(6), </booktitle> <pages> 883-889. </pages>
Reference: <author> Zhang, J. </author> <year> (1992a), </year> <title> Selecting typical instances in instance-based learning, </title> <booktitle> in "Proceedings of the Ninth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 470-479. </pages>
Reference: <author> Zhang, P. </author> <year> (1992b), </year> <title> "On the distributional properties of model selection criteria", </title> <journal> Journal of the American Statistical Association 87(419), </journal> <pages> 732-737. </pages>
Reference-contexts: On the theoretical side, using leave-one-out cross-validation for model selection of linear models is asymptotically inconsistent: the probability of selecting the model with the best predictive power does not converge to one as the total number of observations approaches infinity <ref> (Zhang 1992b, Shao 1993) </ref>. On the experimental side, Breiman & Spector (1992) and Kohavi (1995a) found that five-fold and ten-fold cross-validation are better for model selection of feature subsets than leave-one-out.
Reference: <author> Zheng, Z. </author> <year> (1995), </year> <title> Constructing nominal x-of-n attributes, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> "Proceedings of the 14th International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 1064-1070. </pages>

References-found: 278

