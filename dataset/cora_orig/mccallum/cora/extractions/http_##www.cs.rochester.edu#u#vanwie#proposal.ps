URL: http://www.cs.rochester.edu/u/vanwie/proposal.ps
Refering-URL: http://www.cs.rochester.edu/stats/oldmonths/1998.04/docs-bytes.html
Root-URL: 
Title: Observation and Cooperation  
Author: by Michael Van Wie Christopher M. Brown James Allen, David Braun, Chris Brown and Lenhart Schubert. 
Degree: Thesis Proposal for the Degree Doctor of Philosophy Supervised by  The thesis committee  
Note: members are:  
Date: 1998  
Address: Rochester, New York  
Affiliation: Department of Computer Science The College Arts and Sciences University of Rochester  
Abstract-found: 0
Intro-found: 1
Reference: [Allen, 1984] <author> James Allen, </author> <title> "Towards a General Theory of Action and Time," </title> <journal> Artificial Intelligence, </journal> <volume> 23:123 - 154, </volume> <year> 1984. </year>
Reference-contexts: To express the rules formally, Pollack uses a form of predicate calculus based on Allen's interval-based temporal logic <ref> [Allen, 1984] </ref>. The following predicates are used in Pollack's formalism: HOLDS (P; t) is true if proposition P holds throughout time interval t. OCCU RS (ff; A; t) is true if agent A performs action ff during time interval t.
Reference: [Allen and Perrault, 1980] <author> J.F. Allen and C.R. Perrault, </author> <title> "Analyzing Intention in Dialogues," </title> <journal> Artificial Intelligence, </journal> <volume> 15(3) </volume> <pages> 143-178, </pages> <year> 1980. </year>
Reference-contexts: Planning and symbolic plan recognition have been successfully applied to dialog understanding. Cohen and Perrault [Cohen and Perrault, 1986] formalize the notion of an utterance as an action intended to bring about a change in the listener's belief; Allen and Perrault <ref> [Allen and Perrault, 1980] </ref> view dialog understanding as an application of plan recognition. Reasoning about the intentions of the speaker leads to a better understanding of what was said.
Reference: [Arkin et al., 1993] <author> R. C. Arkin, T. Balch, and E. Nitz, </author> <title> "Communication of Behavioral State in Muti-agent Retrieval Tasks," </title> <booktitle> Proceedings, IEEE International Conference on Robotics and Automation, </booktitle> <volume> 3, </volume> <year> 1993. </year>
Reference-contexts: Such systems are naturally divided by their application domains. In domains where agents are assumed to be working together toward a common goal, work has focused on the amount and type of communication required to improve performance <ref> [Arkin et al., 1993; Parker, 1993] </ref>. In domains where agents are assumed to be self-interested and possibly hostile, research focuses on designing systems where agents can arrange to form optimal-value coalitions [Ketchpel, 1994; Zlotkin and Rosenschein, 1994; Zlotkin and Rosenschein, 1989]. <p> In domains where agents are assumed to be self-interested and possibly hostile, research focuses on designing systems where agents can arrange to form optimal-value coalitions [Ketchpel, 1994; Zlotkin and Rosenschein, 1994; Zlotkin and Rosenschein, 1989]. Arkin <ref> [Arkin et al., 1993] </ref> and Balch [Balch and Arkin, 1994] attempt to answer the question of whether communication is useful in the shared-goal environment. Arkin examines a simple forage task under varying levels of communication.
Reference: [Asama, 1994] <author> Hajime Asama, </author> <booktitle> "Trends of Distributed Autonomous Robotic Systems," </booktitle> <pages> pages 3 - 8, </pages> <year> 1994. </year>
Reference-contexts: The two most important axes along which cooperative systems differ are communication and organization. By communication, I mean the method the robots use to exchange information; by organization, I mean the top-down or bottom-up structure of the system <ref> [Asama, 1994; Gasser and Huhns, 1989] </ref>. I refer to bottom-up systems, where global behavior emerges from the interactions of many individual autonomous units, as local. <p> Work into systems that decompose the planning process in this way is somewhat reminiscent of parallel algorithms work in the systems community, and many of the same trade-offs exist <ref> [Asama, 1994] </ref>. Research in multi-agent planning typically involves research into efficient methods of goal decomposition and subplan merging. For example, Katz and Rosenschein [Katz and Rosenschein, 1989] use STRIPS action descriptions as a base for their planner. A central supervisor directs the actions of the individual agents.
Reference: [Balch and Arkin, 1994] <author> T. Balch and R. C. Arkin, </author> <title> "Communication in Reactive Multi-agent Robotic Systems," </title> <booktitle> Autonomous Robots, </booktitle> <volume> 1(1) </volume> <pages> 27-53, </pages> <year> 1994. </year>
Reference-contexts: In domains where agents are assumed to be self-interested and possibly hostile, research focuses on designing systems where agents can arrange to form optimal-value coalitions [Ketchpel, 1994; Zlotkin and Rosenschein, 1994; Zlotkin and Rosenschein, 1989]. Arkin [Arkin et al., 1993] and Balch <ref> [Balch and Arkin, 1994] </ref> attempt to answer the question of whether communication is useful in the shared-goal environment. Arkin examines a simple forage task under varying levels of communication. <p> Arkin argues that, because the distance travelled is an average over the number of robots, the fact that steps-to-goal is decreasing is an indication that robots are doing more useful work. In contrast, Balch <ref> [Balch and Arkin, 1994] </ref> describes a situation where communication is not necessarily helpful to cooperating agents. <p> It will be retain the robustness of locally organized systems, place the burden of computating responses to world-states on the agent instead of the programmer, model other agents in order to cooperate effectively, and be able to handle complex problems in the real world. Balch's research <ref> [Balch and Arkin, 1994] </ref> motivates a promising area of research into cooperative robotics, one in which locally organized agents communicate implicitly by observing the world-state. These systems fit into the final area of my characterization of cooperative robotics research, which I have so far left blank.
Reference: [Bratman, 1990] <author> M. Bratman, </author> <title> What is Intention?, </title> <year> 1990. </year>
Reference-contexts: I first give a formal definition of Joint Intentions, then describe its problems and the two most relevant modifications. 3.1 Persistent goals In [Cohen and Levesque, 1990], Cohen, Levesque and Nunes give a logical formalization of individual intention, based in part on a definition of intention given by Bratman <ref> [Bratman, 1990] </ref>. They call their formalization "persistent goals" and derive an operator PGOAL that describes how an agent's intentions are related to its beliefs, its commitments, and its actions.
Reference: [Brooks, 1986] <author> Rodney A. Brooks, </author> <title> "A Robust Layered Control System for a Mobile Robot," </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: divided between those who believe robot control is best achieved through symbolic means, including explicit world representation and logical reasoning [Nilsson, 1984], and those who believe it is best achieved through reactive means, in which robots rely on simple behaviors and intelligence emerges naturally from the interactions among those behaviors <ref> [Brooks, 1986] </ref>. <p> Much of the work on multi-agent planning has been directed at this problem [Ephrati and Rosenschein, 1994; Ephrati et al., 1995a]. Finally, like traditional single-robot planners, cooperative planners find it difficult to adjust to unexpected changes in the world <ref> [Brooks, 1986] </ref>. 2.2 Reactive cooperation Another organizational philosophy is to assume that all robots are created equal, to give them each simple rules that hopefully lead to cooperative behavior. Such approaches are often 9 referred to as reactive. <p> He suggests that it be used as an example lower-bound on the performance of a robust robotic workcell. 2.3 Behavioral cooperation A number of researchers are investigating cooperation based on behavioral robotics (as proposed by Brooks <ref> [Brooks, 1986] </ref>). Brooks advocates dividing control into several conceptually 10 simple behaviors, all of which run in parallel. Each behavior receives its input straight from the sensors, and each has access to the robot's actuators. <p> However, the reliance of hierarchical systems on the single-agent planning literature lead to multi-agent systems that exhibit many of the same problems that are inherent in planning systems in general <ref> [Brooks, 1986] </ref>. In addition, due to partially to the high complexity of a world that includes many active agents, the multi-agent case of the planning problem has representational, combinatorial and implementational issues that have yet to be fully addressed [Mataric, 1995a].
Reference: [Charniak, 1991] <author> Eugene Charniak, </author> <title> "Bayesian Networks Without Tears," </title> <journal> AI Magazine, </journal> <volume> 12(4), </volume> <year> 1991. </year>
Reference-contexts: observed agent's current actions with the current plan to determine the status of that agent's commitment. 23 Huber and Durfee implement joint intentions under a system called UM-PRS [Huber et al., 1994b], mapping plans for plan recognition by a bayes net approach given in [Huber et al., 1994a] (see also <ref> [Charniak, 1991] </ref> for an introduction to bayes nets, or [Pearl, 1988] for a detailed description). They use no vision in their system, assuming that logical predicates can be detected directly by the cooperating agents.
Reference: [Charniak and Goldman, 1991] <author> Eugene Charniak and Robert Goldman, </author> <title> "A Probabilistic Model of Plan Recognition," </title> <booktitle> AAAI, </booktitle> <volume> 1 </volume> <pages> 160-165, </pages> <year> 1991. </year>
Reference-contexts: Kautz himself deals only with systems where knowledge is complete and accurate. A logical formalization of plan recognition in the case where agents may construct invalid plans is provided by Pollack in [Pollack, 1990]. Using a bayesian representation of uncertainty, Charniak <ref> [Charniak and Goldman, 1991] </ref> offers a robust system for plan recognition in the real world. In general, plan recognition can be viewed as another instance of the classification problem. <p> to the bank after picking up its rifle to be planning to cash a check and then go hunting? It seems more intuitive to model plan inference as a probabilistic process, and allow agents to reason about the relative odds of the various models being valid under the current situation <ref> [Charniak and Goldman, 1991] </ref>. Also, the fact that the incremental inference process makes monotonic conclusions based on its limited information seems likely to lead agents to make mistakes from which they cannot recover. <p> Also, the fact that the incremental inference process makes monotonic conclusions based on its limited information seems likely to lead agents to make mistakes from which they cannot recover. Finally, in the real world, plans are rarely ends in themselves; instead, In answer to their own criticisms of Kautz <ref> [Charniak and Goldman, 1991] </ref>, Charniak and Goldman presents a bayesian model of plan recognition. His model takes logical axioms as input and build a bayes net structure with primitive actions at the leaves and the plan description at the root.
Reference: [Cohen et al., 1990] <author> P. Cohen, H. Levesque, and J. H. Nunes, </author> <title> "On Acting Together," </title> <booktitle> Proceedings AAAI-90, </booktitle> <year> 1990. </year>
Reference-contexts: cooperative robotic agents: agents decisions are local, they model the world and each other to arrive at sensible choices of action, they are able to work together efficiently by intuiting each others' internal states. 17 3 Joint Intentions This section describes Joint Intentions, a framework for multi-agent cooperation proposed in <ref> [Cohen et al., 1990] </ref>, and a number of modifications that claim to extend it in useful or necessary ways. <p> Cohen and Levesque use their theory of persistent goals to fashion a theory of collaborative intentions. I will describe their extension (in more detail than I do the PGOAL structure) in section 3.2. 19 3.2 Joint intentions Joint intentions, introduced by Cohen, Levesque and Nunes in <ref> [Cohen et al., 1990] </ref> which extend persistent goals to handle cooperative intentions. Joint intentions are intended to clarify the relationships among beliefs, desires and intentions for multiple agents. <p> It specifies the conditions under which the agents may drop their intention while still believing that the goal is untrue and satisfiable. This definition implies that agents who hold a joint goal will take great pains to coordinate with each other. Among its features are the following (proven in <ref> [Cohen et al., 1990] </ref>): * If x and y have a JPG to achieve p, then x has a PGOAL of p and y has a PGOAL of p. * If x (or y) comes to believe that p is impossible, while the other still believes that p is possible, then <p> For more information on belief-based filtering, see the related work in chapter 4. 5.3 Verification of adopted plans Agents can not in all situations depend on the fact that their recognized plan is valid, even once they have begun cooperating. As Cohen, Levesque and Nunes point out in <ref> [Cohen et al., 1990] </ref>, agents may find their goals untenable and defect from a group action. Unlike their 39 checked against the current world-state. Only B is found consistent; A and C are discarded. consistency.
Reference: [Cohen and Levesque, 1990] <author> P. R. Cohen and H. J. Levesque, </author> <title> "Persistence, Intention and Commitment," Intentions in Communication, </title> <booktitle> 1990. </booktitle> <pages> 56 </pages>
Reference-contexts: I first give a formal definition of Joint Intentions, then describe its problems and the two most relevant modifications. 3.1 Persistent goals In <ref> [Cohen and Levesque, 1990] </ref>, Cohen, Levesque and Nunes give a logical formalization of individual intention, based in part on a definition of intention given by Bratman [Bratman, 1990]. <p> The seemingly odd second condition is necessary because of the following example (quoted from the text of <ref> [Cohen and Levesque, 1990] </ref>): An agent intends to kill his uncle. On the way to his uncle's house, this intention causes him to become so agitated that he loses control of his car and runs over a pedestrian, who happens to be his uncle.
Reference: [Cohen and Perrault, 1986] <author> P. R. Cohen and C. R. Perrault, </author> <title> "Elements of a Plan-based Theory of Speech Acts," </title> <booktitle> Readings in NL Processing, </booktitle> <pages> pages 423-439, </pages> <year> 1986. </year>
Reference-contexts: Recipes are in some way considered minimal sets of actions to achieve goals; agents are rarely assumed to take actions unnecessary to their goals. Planning and symbolic plan recognition have been successfully applied to dialog understanding. Cohen and Perrault <ref> [Cohen and Perrault, 1986] </ref> formalize the notion of an utterance as an action intended to bring about a change in the listener's belief; Allen and Perrault [Allen and Perrault, 1980] view dialog understanding as an application of plan recognition.
Reference: [Colorni et al., 1992] <author> A. Colorni, M. Dorigo, and V. Maniezzo, </author> <title> "Distributed Optimization by Ant Colonies," Toward a Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life, </booktitle> <pages> pages 134 - 142, </pages> <year> 1992. </year>
Reference: [Deneubourg et al., 1992] <author> J. L. Deneubourg, G. Theraulax, and R. </author> <title> Beckers, "Swarm-Made Architectures," Toward a Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First European Conference on Artificial Life, </booktitle> <pages> pages 123 - 133, </pages> <year> 1992. </year>
Reference: [Doty and Aken, 1993] <author> K. L. Doty and R. E. Van Aken, </author> <title> "Swarm Robot Materials Handling Paradigm for a Manufacturing Workcell," </title> <booktitle> Proceedings, IEEE International conference on Robotics And Automation, </booktitle> <volume> 1, </volume> <year> 1993. </year>
Reference-contexts: A good study and evaluation of the reactive approach to cooperation was done by K. L. Doty in <ref> [Doty and Aken, 1993] </ref>. K. L. Doty examines a hypothetical "workcell" where swarms of robots participate in moving partially completed machine parts from one worker-machine to another. In this paper, robots follow strict reactive rules.
Reference: [Durfee and Rosenschein, 1994] <author> Edmund H. Durfee and Jeffrey S. Rosenschein, </author> <title> "Distributed Problem Solving and Multi-Agent Systems: Comparisons and Examples," </title> <booktitle> Thirteenth International Distributed Artificial Intelligence Workshop, </booktitle> <pages> pages 94 - 104, </pages> <year> 1994. </year>
Reference: [Ephrati et al., 1995a] <author> E. Ephrati, M. Pollack, and J. Rosenschein, </author> <title> "A Tractable Heuristic that Maximizes Global Utility through Local Plan Combination," </title> <booktitle> First International Conference on Multi-Agent Systems, </booktitle> <year> 1995. </year>
Reference-contexts: current planning problem. (As another example of the similarity between multi-agent planning and 7 parallel systems, Katz defines a levels relation on the DAG that is basically a barrier condition: all actions at a level n must terminate before any action at a level n + 1 may begin.) Ephrati95 <ref> [Ephrati et al., 1995a] </ref> studies multi-agent planning for cases where local optimization for individual agents does not necessarily lead to a globally optimal solution. He describes a system in which agents create local plans and then pass them to a global coordinator for synthesis. <p> Such interference can lead to extra work (at best) or deadlock (at worst). Much of the work on multi-agent planning has been directed at this problem <ref> [Ephrati and Rosenschein, 1994; Ephrati et al., 1995a] </ref>.
Reference: [Ephrati et al., 1995b] <author> E. Ephrati, M. Pollack, and S. </author> <title> Ur, "Deriving Multi-Agent Coordination through Filtering Strategies," </title> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1995. </year>
Reference: [Ephrati and Rosenschein, 1994] <author> E. Ephrati and J. Rosenschein, </author> <title> "Divide and Conquer in Multi-Agent Planning," </title> <booktitle> Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <month> July-August </month> <year> 1994. </year>
Reference-contexts: Worse, conflicting subgoals are more of a problem in multi-agent systems than in single-agent planners, because interactions between the individual agents can easily lead to deadlock or goal-clobbering <ref> [Ephrati and Rosenschein, 1994; Mataric, 1995b] </ref>. The two most important axes along which cooperative systems differ are communication and organization. By communication, I mean the method the robots use to exchange information; by organization, I mean the top-down or bottom-up structure of the system [Asama, 1994; Gasser and Huhns, 1989]. <p> Furthermore, if the bonus is strictly increasing with a plan's marginal utility, agents will be motivated to generate plans that "achieve more in a shorter time." Ephrati takes a more straightforward approach to subplan combination in <ref> [Ephrati and Rosenschein, 1994] </ref>. It uses a symbolic, STRIPS-like representation for actions and world-states. A central agent divides the global task into subtasks, which are assigned in some way to the robotic agents, who each find solutions for their subgoals. <p> The robotic agents solve their subplans in parallel, leading to a running time of (b=i) l=i , where i is the number of robotic agents. The paper <ref> [Ephrati and Rosenschein, 1994] </ref> concerns itself primarily with the merging process. Each individual robotic agent derives a set of subplans that solve its goal. <p> Second, because central controllers in hierarchical systems must communicate with their slaves, the required bandwidth of such a system increases linearly with the number of slaves [Mataric, 1995b]. A third weakness of hierarchical planning systems is that of goal interference <ref> [Mataric, 1995b; Mataric, 1992; Ephrati and Rosenschein, 1994] </ref>, where one agent may undo or block the actions of another. Such interference can lead to extra work (at best) or deadlock (at worst). <p> Such interference can lead to extra work (at best) or deadlock (at worst). Much of the work on multi-agent planning has been directed at this problem <ref> [Ephrati and Rosenschein, 1994; Ephrati et al., 1995a] </ref>.
Reference: [Ephrati and Rosenschein, 1993] <author> E. Ephrati and J. S. Rosenschein, </author> <title> "Multi-Agent Planning as a Dynamic Search for Social Consensus," </title> <booktitle> Thirteenth International Joint Conference ON Artificial Intelligence, </booktitle> <pages> pages 423 - 429, </pages> <year> 1993. </year>
Reference: [Gasser and Huhns, 1989] <editor> L. Gasser and M. N. Huhns, </editor> <booktitle> "Themes in Distributed Artificial Intelligence Research," Distributed Artificial Intelligence, </booktitle> <volume> 2, </volume> <year> 1989. </year>
Reference-contexts: The two most important axes along which cooperative systems differ are communication and organization. By communication, I mean the method the robots use to exchange information; by organization, I mean the top-down or bottom-up structure of the system <ref> [Asama, 1994; Gasser and Huhns, 1989] </ref>. I refer to bottom-up systems, where global behavior emerges from the interactions of many individual autonomous units, as local.
Reference: [Gmytrasiewicz et al., 1991] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe, </author> <title> "The Utility of Communication in Coordinating Intelligent Agents," </title> <booktitle> AAAI, </booktitle> <volume> 1 </volume> <pages> 166-172, </pages> <year> 1991. </year> <month> 57 </month>
Reference-contexts: For example, one of the agents described by Doty would have difficulty to cooperate with a human being, or with one of Balch's agents. Furthermore, while communication is a useful shorthand between communicating homogeneous agents, it does not relieve agents of the necessity of modeling each other <ref> [Gmytrasiewicz et al., 1991] </ref>. In communicating societies, the burden of modeling is on the message sender - agents must know that the message they are sending will be accepted by its recipient.
Reference: [Gorgeff, 1984] <author> M. P. Gorgeff, </author> <title> "A Theory of Action for Multi-agent Planning," </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 121 - 125, </pages> <year> 1984. </year>
Reference: [Huber and Durfee, 1993] <author> Marcus J. Huber and Edmund H. Durfee, </author> <title> "Observational Uncertainty in Plan Recognition Among Interacting Robots," </title> <booktitle> Working Notes of the IJCAI-93 Workshop on Dynamically Interacting Robots, </booktitle> <year> 1993. </year>
Reference-contexts: They suggest using a probabilistic plan recognizer, similar to their work in <ref> [Huber and Durfee, 1993] </ref> and [Huber and Durfee, 1995a], to deduce the status of the commitments of other agents involved in a joint plan. <p> Charniak and Goldman's system is elegant in that it represents uncertainty and allows intelligent choices among plan alternatives, while retaining the high-level description of plans. Charniak and Goldman's system is used as the basis for Huber and Durfee's observational agent <ref> [Huber and Durfee, 1993; Huber and Durfee, 1995b] </ref>. 4.3 Summary In this section, I have discussed approaches to plan recognition.
Reference: [Huber and Durfee, 1995a] <author> Marcus J. Huber and Edmund H. Durfee, </author> <title> "Deciding When to Commit to Action During Observation-based Coordination," </title> <booktitle> Proceedings, First International Conference on Multi-Agent Systems, </booktitle> <pages> pages 163 - 170, </pages> <year> 1995. </year>
Reference-contexts: Another interesting feature of Tambe's architecture is selective communication, where agents communicate only information with high utility to the completion of the plan. Because such communication is beyond the scope of this thesis, I will not describe its mechanism. 3.4 Joint intentions and observation In <ref> [Huber and Durfee, 1995b; Huber and Durfee, 1995a] </ref>, Huber and Durfee describe a system that attempts to solve the problems of Joint Intentions. <p> They suggest using a probabilistic plan recognizer, similar to their work in [Huber and Durfee, 1993] and <ref> [Huber and Durfee, 1995a] </ref>, to deduce the status of the commitments of other agents involved in a joint plan.
Reference: [Huber and Durfee, 1995b] <author> Marcus J. Huber and Edmund H. Durfee, </author> <title> "On Acting Together: Without Communication," </title> <booktitle> AAAI Spring Symposium on Representing Mental States and Mechanisms, </booktitle> <pages> pages 60 - 71, </pages> <year> 1995. </year>
Reference-contexts: Another interesting feature of Tambe's architecture is selective communication, where agents communicate only information with high utility to the completion of the plan. Because such communication is beyond the scope of this thesis, I will not describe its mechanism. 3.4 Joint intentions and observation In <ref> [Huber and Durfee, 1995b; Huber and Durfee, 1995a] </ref>, Huber and Durfee describe a system that attempts to solve the problems of Joint Intentions. <p> Charniak and Goldman's system is elegant in that it represents uncertainty and allows intelligent choices among plan alternatives, while retaining the high-level description of plans. Charniak and Goldman's system is used as the basis for Huber and Durfee's observational agent <ref> [Huber and Durfee, 1993; Huber and Durfee, 1995b] </ref>. 4.3 Summary In this section, I have discussed approaches to plan recognition. <p> Unlike their 39 checked against the current world-state. Only B is found consistent; A and C are discarded. consistency. Here only one script remains after filtering. joint intentions framework, however, our observational system does not place any obligation on defectors to communicate with their peers. Instead, as in <ref> [Huber and Durfee, 1995b] </ref>, agents are expected to keep track of their collaborators and verify the status of the group goals for themselves. As one might expect, verification is very much like plan recognition. Once a group plan is recognized, it is stored in the observing agent's memory. <p> For example, in a give-and-go plan in RoboCup, the agent in possession of the ball may find it necessary to dribble away from a defender before passing. Such subactions are explicitly accounted for by Huber and Durfee in <ref> [Huber and Durfee, 1995b] </ref>, but that approach is almost certain to place an unwanted burden on the programmer (who may not even be a good choice for deciding what noise to emphasize and what to leave out!).
Reference: [Huber et al., 1994a] <author> Marcus J. Huber, Edmund H. Durfee, and Michael P. Wellman, </author> <title> "The Automated Mapping of Plans for Plan Recognition," </title> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 344 - 351, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: agent need do is compare the observed agent's current actions with the current plan to determine the status of that agent's commitment. 23 Huber and Durfee implement joint intentions under a system called UM-PRS [Huber et al., 1994b], mapping plans for plan recognition by a bayes net approach given in <ref> [Huber et al., 1994a] </ref> (see also [Charniak, 1991] for an introduction to bayes nets, or [Pearl, 1988] for a detailed description). They use no vision in their system, assuming that logical predicates can be detected directly by the cooperating agents. <p> KAs are converted into bayesian belief networks as described in <ref> [Huber et al., 1994a] </ref>. Beliefs about the current behaviors of the external agents are added to the networks as instantiated variables at the leaves of the network, and probabilities are propagated to the root. The root node holds the final decision about the nature of the external agents' commitments.
Reference: [Huber et al., 1994b] <author> Marcus J. Huber, Jaeho Lee, Patrick Kenny, and Edmund H. Durfee, </author> <title> "UM-PRS V2.7 Programmer and User Guide," </title> <institution> The University of Michigan, </institution> <address> 1101 Beal Avenue, Ann Arbor MI 48109, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: All an observing agent need do is compare the observed agent's current actions with the current plan to determine the status of that agent's commitment. 23 Huber and Durfee implement joint intentions under a system called UM-PRS <ref> [Huber et al., 1994b] </ref>, mapping plans for plan recognition by a bayes net approach given in [Huber et al., 1994a] (see also [Charniak, 1991] for an introduction to bayes nets, or [Pearl, 1988] for a detailed description).
Reference: [Jennings, 1995] <author> N. R. Jennings, </author> <title> "Controlling Cooperative Problem Solving in Industrial Multi-agent Systems Using Joint Intentions," </title> <journal> Artificial Intelligence, </journal> <volume> 75(2), </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: Joint intentions are intended to clarify the relationships among beliefs, desires and intentions for multiple agents. They are attractive to computer scientists because they are presented in an implementable framework in fact, in <ref> [Jennings, 1995] </ref>, an implementation of joint intentions is developed for industrial robots. Joint intentions are developed in three levels. First they define weak goals, which specify the conditions under which an agent holds a goal, and the actions it must take if the goal is satisfied or impossible.
Reference: [Kamel and Syed, 1989] <author> M. Kamel and A. Syed, </author> <title> "An Object-Oriented Multiple Agent Planning System," </title> <journal> Distributed Artificial Intelligence, </journal> <volume> 2:259 - 290, </volume> <year> 1989. </year>
Reference: [Katz and Rosenschein, 1989] <author> Matthew J. Katz and Jeffrey S. Rosenschein, </author> <title> "Plans for Multiple Agents," </title> <journal> Distributed Artificial Intelligence, </journal> <volume> 2:198 - 228, </volume> <year> 1989. </year>
Reference-contexts: Research in multi-agent planning typically involves research into efficient methods of goal decomposition and subplan merging. For example, Katz and Rosenschein <ref> [Katz and Rosenschein, 1989] </ref> use STRIPS action descriptions as a base for their planner. A central supervisor directs the actions of the individual agents.
Reference: [Kautz, 1990] <author> Henry Kautz, </author> <title> "A circumscriptive theory of plan recognition," Intentions in Communication, </title> <year> 1990. </year>
Reference-contexts: Plan recognition is the act of reasoning from a set of observed actions to goal, a possible next action, or a complete plan recipe (a sequence of steps for completing a plan). Kautz <ref> [Kautz, 1990] </ref> classifies plan recognition in terms of the role of the observed agent in the recognition process. In intended recognition, the observed agent is assumed to be acting so as to make its intentions obvious. <p> By doing so, agents can not only cooperate in performing unexecuted aspects of an inferred plan, but can also act to repair plans that are somehow damaged. 4.2 A logical formalization of plan recognition In his thesis <ref> [Kautz, 1990] </ref>, Kautz formalizes a model of plan recognition that is based on the circumscription of plan hierarchies. To represent plans and actions, he constructs a hierarchy where a node's children are either components of the node's recipe or alternative, more specialized versions, of the node.
Reference: [Kautz, 1987] <author> Henry A. Kautz, </author> <title> "A Formal Theory of Plan Recognition," </title> <type> *Thesis - TR 215*, </type> <year> 1987. </year>
Reference-contexts: Reasoning about the intentions of the speaker leads to a better understanding of what was said. In the papers discussed in the following sections, Pollack in [Pollack, 1990] and Kautz in <ref> [Kautz, 1987] </ref> both apply their systems to examples from natural language.
Reference: [Ketchpel, 1994] <author> S. Ketchpel, </author> <title> "Forming Coalitions in the Face of Uncertain Rewards," </title> <booktitle> Proceedings of the Twelfth National Conference on Artificial Intelligence, July-August 1994. </booktitle> <pages> 58 </pages>
Reference-contexts: In domains where agents are assumed to be self-interested and possibly hostile, research focuses on designing systems where agents can arrange to form optimal-value coalitions <ref> [Ketchpel, 1994; Zlotkin and Rosenschein, 1994; Zlotkin and Rosenschein, 1989] </ref>. Arkin [Arkin et al., 1993] and Balch [Balch and Arkin, 1994] attempt to answer the question of whether communication is useful in the shared-goal environment. Arkin examines a simple forage task under varying levels of communication.
Reference: [Kuniyoshi et al., 1994] <author> Y. Kuniyoshi, M. Inaba, and H. Inoue, </author> <title> "Learning by Watching: Extracting Reusable Task Knowledge from Visual Observation of Human Performance," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(6):799 - 822, </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: More complex plan structures will, of course, require a better plan ! HMM scheme. 5.5 Action recognition The problem of action recognition is that of assigning actions to agents in order to explain changes in world-state <ref> [Kuniyoshi et al., 1994] </ref>.
Reference: [Lesh and Etzioni, 1995] <author> Neal Lesh and Oren Etzioni, </author> <title> "A Sound and Fast Goal Recognizer," </title> <booktitle> Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Neal Lesh, in <ref> [Lesh and Etzioni, 1995] </ref>, uses a similar graph-oriented filtering approach to plan recongition.
Reference: [Lucarini et al., 1993] <author> G. Lucarini, M. Varioli, R. Cerutti, and G. </author> <title> Sandini, </title> <booktitle> "Cellular Robotics: Simulation and HW Implementation," Proceedings, IEEE Internation Conference on Robotics and Automation, </booktitle> <volume> 3, </volume> <year> 1993. </year>
Reference: [Mataric, 1992] <editor> Maja J. Mataric, </editor> <title> "Behavior-Based Systems: Key Properties and Implications," </title> <booktitle> Proceedings, IEEE International conference on Robotics and Automation, Workshop on Architectures for Intelligent Control Systems, </booktitle> <year> 1992. </year>
Reference-contexts: Second, because central controllers in hierarchical systems must communicate with their slaves, the required bandwidth of such a system increases linearly with the number of slaves [Mataric, 1995b]. A third weakness of hierarchical planning systems is that of goal interference <ref> [Mataric, 1995b; Mataric, 1992; Ephrati and Rosenschein, 1994] </ref>, where one agent may undo or block the actions of another. Such interference can lead to extra work (at best) or deadlock (at worst).
Reference: [Mataric, 1994] <editor> Maja J. Mataric, </editor> <title> "Interaction and Intelligent Behavior," </title> <type> Technical Report, </type> <institution> MIT, </institution> <year> 1994. </year>
Reference-contexts: Finally, it most commonly employs genetic techniques for evolving the agents' comparitively simple control systems." Mataric's work, in contrast, deals with real robots in the real world, confines itself to smaller groups of agents, and uses reinforcement learning to guide the development of group behaviors. In her thesis <ref> [Mataric, 1994] </ref>, Mataric characterizes AI research as lying in the 2D-space with axes representing cognitive and environmental complexity (see figure 2.2). Traditional AI research, she claims, deals with complex agents in simple environments. Behavioral and reactive approaches deal with simple agents in complex environments.
Reference: [Mataric, 1995a] <editor> Maja J. Mataric, </editor> <title> "Designing and Understanding Adaptive Group Behavior," </title> <booktitle> Adaptive Behavior, </booktitle> <pages> pages 51 - 80, </pages> <year> 1995. </year>
Reference-contexts: The programmer provides a structure whereby behaviors can inhibit each others' outputs (or supress their inputs), thus establishing a means of arbitrating conflicts between behaviors. The behavioral approach has attracted several researchers in cooperative robotics. One such approach is taken by Mataric <ref> [Mataric, 1995a] </ref>, who views a society of agents as a single entity. Behaviors are divided across the members of the society. Some reduncancy is, of course, necessary for example, every agent has an obstacle avoidance behavior. <p> In addition, due to partially to the high complexity of a world that includes many active agents, the multi-agent case of the planning problem has representational, combinatorial and implementational issues that have yet to be fully addressed <ref> [Mataric, 1995a] </ref>. Finally, hierarchical systems generally are not robust enough to handle malfunctions in single component agents. Reactive and behavioral systems (those with local structure and no communication) solve many of these problems by synthesizing social organization from the bottom up.
Reference: [Mataric, 1995b] <editor> Maja J. Mataric, </editor> <booktitle> "Issues and Approaches in the Design of Collective Autonomous Agents," Robotics and Autonomous Systems, </booktitle> <volume> 16(2 - 4):321 - 331, </volume> <year> 1995. </year>
Reference-contexts: Worse, conflicting subgoals are more of a problem in multi-agent systems than in single-agent planners, because interactions between the individual agents can easily lead to deadlock or goal-clobbering <ref> [Ephrati and Rosenschein, 1994; Mataric, 1995b] </ref>. The two most important axes along which cooperative systems differ are communication and organization. By communication, I mean the method the robots use to exchange information; by organization, I mean the top-down or bottom-up structure of the system [Asama, 1994; Gasser and Huhns, 1989]. <p> First, while many researchers in hierarchical systems have investigated them as a means of reducing the state-space of a planning problem, Mataric points out that the state-space of such a system must actually grow exponentially to account for the many agents who can act on the world <ref> [Mataric, 1995b] </ref>. Reasoning about other agents is desirable, but difficult in the framework of multi-agent planning. Second, because central controllers in hierarchical systems must communicate with their slaves, the required bandwidth of such a system increases linearly with the number of slaves [Mataric, 1995b]. <p> many agents who can act on the world <ref> [Mataric, 1995b] </ref>. Reasoning about other agents is desirable, but difficult in the framework of multi-agent planning. Second, because central controllers in hierarchical systems must communicate with their slaves, the required bandwidth of such a system increases linearly with the number of slaves [Mataric, 1995b]. A third weakness of hierarchical planning systems is that of goal interference [Mataric, 1995b; Mataric, 1992; Ephrati and Rosenschein, 1994], where one agent may undo or block the actions of another. Such interference can lead to extra work (at best) or deadlock (at worst). <p> Second, because central controllers in hierarchical systems must communicate with their slaves, the required bandwidth of such a system increases linearly with the number of slaves [Mataric, 1995b]. A third weakness of hierarchical planning systems is that of goal interference <ref> [Mataric, 1995b; Mataric, 1992; Ephrati and Rosenschein, 1994] </ref>, where one agent may undo or block the actions of another. Such interference can lead to extra work (at best) or deadlock (at worst). <p> Mataric herself mentions the similarity <ref> [Mataric, 1995b] </ref>, but goes on to differentiate her behavior-based approach from current research in Alife: "However, work in Alife does not typically deal with agents situated in physically realistic worlds. Additionally, it [Alife] usually treats much larger population sizes than the work presented here.
Reference: [Matsubara et al., 1996] <author> Hitoshi Matsubara, Itsuki Noda, and Kazuo Hiraki, </author> <title> "Learning of Cooperative Actions in Multi-Agent Systems: a case study of pass in Soccer," </title> <booktitle> AAAI-96 Spring Symposium on Adaptation, Coevolution and Learning in Multi-agent Systems, </booktitle> <year> 1996. </year>
Reference-contexts: In the RoboCup domain, for example, several neural-net soccer learning systems have been attempted, all with reasonable to good results in improving performance in the limited domain under which they train atwo-on-one situation in the goal-mouth is typical <ref> [Matsubara et al., 1996] </ref>. However good those results have been in improving system behavior in their own domain, teams composed of such agents have consistently been defeated by less sophisticated approaches in RoboCup competition.
Reference: [McCarthy, 1980] <author> John McCarthy, </author> <title> "Circumscription A Form of Non-Monotonic Reasoning," </title> <journal> Artificial Intelligence, </journal> <volume> 13:27 - 39, </volume> <year> 1980. </year>
Reference-contexts: Kautz characterizes c-entailment as a case of McCarthy's circumscription <ref> [McCarthy, 1980] </ref>, and mc-entailment as a case of circumscription with variables.
Reference: [Nilsson, 1984] <author> N. J. Nilsson, </author> <title> "Shakey the Robot," </title> <type> Technical Report 323, </type> <institution> SRI International, Artificial Intelligence Center, </institution> <year> 1984. </year>
Reference-contexts: To date, work in cooperative robotics has been divided between those who believe robot control is best achieved through symbolic means, including explicit world representation and logical reasoning <ref> [Nilsson, 1984] </ref>, and those who believe it is best achieved through reactive means, in which robots rely on simple behaviors and intelligence emerges naturally from the interactions among those behaviors [Brooks, 1986]. <p> Much of the planning tradition stems from the STRIPS system <ref> [Nilsson, 1984] </ref>, and therefore planning systems have many common characteristics.
Reference: [Noda, 1995] <author> Itsuki Noda, </author> <title> "Soccer Server: a simulator for RoboCup," </title> <note> JSAI AI-Symposium 95: Special Session on RoboCup, </note> <month> Dec </month> <year> 1995. </year>
Reference-contexts: This section gives a specific adaptation of the general framework to the RoboCup domain. 6.2.1 RoboCup The RoboCup domain, developed by Itsuki Noda <ref> [Noda, 1995] </ref>, is rapidly becoming a standard testbed for multi-agent robotics. It is a client/server implementation of a computerized soccer match, in which the server enforces the physics and rules of soccer and the players are run by client programs, one client per player.
Reference: [Parker, 1993] <author> L. E. Parker, </author> <title> "Designing Control Laws for Cooperative Agent Teams," </title> <booktitle> Proceedings, IEEE International Conference on Robotics and Automation, </booktitle> <volume> 3, </volume> <year> 1993. </year>
Reference-contexts: Such systems are naturally divided by their application domains. In domains where agents are assumed to be working together toward a common goal, work has focused on the amount and type of communication required to improve performance <ref> [Arkin et al., 1993; Parker, 1993] </ref>. In domains where agents are assumed to be self-interested and possibly hostile, research focuses on designing systems where agents can arrange to form optimal-value coalitions [Ketchpel, 1994; Zlotkin and Rosenschein, 1994; Zlotkin and Rosenschein, 1989].
Reference: [Pearl, 1988] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year> <month> 59 </month>
Reference-contexts: determine the status of that agent's commitment. 23 Huber and Durfee implement joint intentions under a system called UM-PRS [Huber et al., 1994b], mapping plans for plan recognition by a bayes net approach given in [Huber et al., 1994a] (see also [Charniak, 1991] for an introduction to bayes nets, or <ref> [Pearl, 1988] </ref> for a detailed description). They use no vision in their system, assuming that logical predicates can be detected directly by the cooperating agents. In UM-PRS, a Knowledge Area (KA) is an implementation of one of Cohen and Levesque's recipes: it is a procedure for accomplishing a goal.
Reference: [Pollack, 1990] <author> Martha E. Pollack, </author> <title> "Plans as Complex Mental Attitudes," Intentions in Communication, </title> <year> 1990. </year>
Reference-contexts: In real world domains, the latter is the more interesting problem. Kautz himself deals only with systems where knowledge is complete and accurate. A logical formalization of plan recognition in the case where agents may construct invalid plans is provided by Pollack in <ref> [Pollack, 1990] </ref>. Using a bayesian representation of uncertainty, Charniak [Charniak and Goldman, 1991] offers a robust system for plan recognition in the real world. In general, plan recognition can be viewed as another instance of the classification problem. <p> ff has preconditions, which describe the facts that must hold in order for ff to be executable; effects, which describe the transformations ff induces in the world; constraints, which describe restrictions on ff's execution; and possibly a body, a list of subgoals whose realization brings about the realization of ff <ref> [Pollack, 1990] </ref>. Generally, once the actions ff 1 ; : : : ff n are executed, the goal state fl is assumed to hold. Recipes are in some way considered minimal sets of actions to achieve goals; agents are rarely assumed to take actions unnecessary to their goals. <p> Reasoning about the intentions of the speaker leads to a better understanding of what was said. In the papers discussed in the following sections, Pollack in <ref> [Pollack, 1990] </ref> and Kautz in [Kautz, 1987] both apply their systems to examples from natural language. <p> However, dialog understanding is by no means the only domain to which their techniques apply; Kautz's system extends naturally to observation by cooperation, and Pollack's model of planning as a mental attutude is useful to general observational systems as well. 4.1 A formal framework for plans as mental attitudes In <ref> [Pollack, 1990] </ref>, Pollack argues that the traditional model of a plan as a recipe of actions is not enough. Consider the case of an agent A with plan r. <p> Though Woods's objections bring to light some serious flaws in the formal, symbolic methods of plan recognition, I nonetheless prefer symbolic approaches to non-symbolic approaches. I believe that a formal framework (like Pollacks in <ref> [Pollack, 1990] </ref>) is necessary in order to apply plan recognition to the problem of cooperation by observation. Once an agent's plan has been deduced, it is necessary to isolate parts of the plan that remain undone, avoid potential future conflicts, and possibly repair faulty plan elements.
Reference: [Stilwell and Bay, 1993] <author> D. J. Stilwell and J. S. </author> <title> Bay, "Toward the Development of a Material Transport System using Swarms of Ant-like Robots," </title> <booktitle> Proceedings, IEEE International Conference on Robotics and Automation, </booktitle> <volume> 1, </volume> <year> 1993. </year>
Reference-contexts: A representative example of reactive cooperation is given in Stilwell <ref> [Stilwell and Bay, 1993] </ref>. Stilwell investigates the feasibility of having swarms of robots transport relatively heavy loads. Robots act strictly in reaction to the forces that are exerted on them by their load.
Reference: [Stuart, 1985] <author> C. Stuart, </author> <title> "An Implementation of a Multi-agent Plan Synchronizer," </title> <booktitle> Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1031 - 1033, </pages> <year> 1985. </year>
Reference: [Tambe, 1997] <author> Milind Tambe, </author> <title> "Agent Architectures for Flexible, Practical Teamwork," </title> <publisher> AAAI, </publisher> <year> 1997. </year>
Reference-contexts: Agents must rely on communication to coordinate. Section 3.3 describes a paper that attempts solutions to the first two problems; section 3.4 describes a paper that attempts a solution to the third. 3.3 Forming robust joint intentions In <ref> [Tambe, 1997] </ref>, Milind Tambe describes methods for solving two of the problems mentioned in section 3.2. The first problem that of forming joint intentions he solves via communication.
Reference: [Woods, 1990] <author> W. A. Woods, </author> <title> "On Plans and Plan recognition: Comments on Pollack and on Kautz," Intentions in Communication, </title> <year> 1990. </year>
Reference-contexts: Both symbolic and nonsymbolic methods are applicable in the sense that they can be used to derive future actions (and even abstractions of an agent's intentions) from a sequence of observations. However, research to date has focused almost exclusively on symbolic systems. In <ref> [Woods, 1990] </ref>, Woods critiques both Kautz and Pollack. The following are his objections to the papers, along with an analysis of how they apply if the domain is restricted to that of observational cooperation.
Reference: [Zlotkin and Rosenschein, 1989] <author> Gilad Zlotkin and Jeffrey S. Rosenschein, </author> <title> "Negotiation and Task Sharing Among Autonomous Agents in Cooperative Domains," </title> <booktitle> Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 912 - 917, </pages> <year> 1989. </year>
Reference-contexts: In domains where agents are assumed to be self-interested and possibly hostile, research focuses on designing systems where agents can arrange to form optimal-value coalitions <ref> [Ketchpel, 1994; Zlotkin and Rosenschein, 1994; Zlotkin and Rosenschein, 1989] </ref>. Arkin [Arkin et al., 1993] and Balch [Balch and Arkin, 1994] attempt to answer the question of whether communication is useful in the shared-goal environment. Arkin examines a simple forage task under varying levels of communication.
Reference: [Zlotkin and Rosenschein, 1994] <author> Gilad Zlotkin and Jeffrey S. Rosenschein, </author> <title> "Coalition, Cryptography and Stability: Mechanisms for Coalition Formation in Task Oriented Domains," </title> <booktitle> The National Conference on Artificial Intelligence, </booktitle> <pages> pages 432 - 437, </pages> <year> 1994. </year>
Reference-contexts: In domains where agents are assumed to be self-interested and possibly hostile, research focuses on designing systems where agents can arrange to form optimal-value coalitions <ref> [Ketchpel, 1994; Zlotkin and Rosenschein, 1994; Zlotkin and Rosenschein, 1989] </ref>. Arkin [Arkin et al., 1993] and Balch [Balch and Arkin, 1994] attempt to answer the question of whether communication is useful in the shared-goal environment. Arkin examines a simple forage task under varying levels of communication.
References-found: 54

