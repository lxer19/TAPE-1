URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/iasted-pr97.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: keb@comeng.chungnam.ac.kr  tgd@cs.orst.edu  
Title: Probability Estimation via Error-Correcting Output Coding  
Author: Eun Bae Kong Thomas G. Dietterich 
Keyword: Key Words: Machine learning, Probability estimation, Classification, Error-correcting output coding  
Address: Taejon, 305-764, South Korea  Corvallis, OR 97331-3202  
Affiliation: Department of Computer Engineering Chungnam National University  Department of Computer Science Oregon State University  
Note: To Appear: IASTED International Conference: Artificial Intelligence and Soft Computing. Banff, Canada.  
Abstract: Previous research has shown that a technique called error-correcting output coding (ECOC) can dramatically improve the classification accuracy of supervised learning algorithms that learn to classify data points into one of k 2 classes. In this paper, we will extend the technique so that ECOC can also provide class probability information. ECOC is a method of converting k-class supervised learning problem into a large number L of two-class supervised learning problems and then combining the results of these L evaluations. The underlying two-class supervised learning algorithms are assumed to provide L probability estimates. The problem of computing class probabilities is formulated as an over-constrained system of L linear equations. Least squares methods are applied to solve these equations. Accuracy and reliability of the probability estimates are demonstrated.
Abstract-found: 1
Intro-found: 1
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1991). </year> <title> Error-correcting output codes: A general method for improving multiclass inductive learning programs. </title> <booktitle> In Proc. of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 572-577. </pages> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: If the minimum Hamming distance between any pair of codewords is d, then any b (d 1)=2c errors in the individual f j 's can be corrected, because the nearest codeword will be the correct codeword. Dietterich and Bakiri <ref> (Dietterich & Bakiri, 1991, 1995) </ref> have shown that the error-correcting output coding technique works very well with the decision-tree algorithm C4.5.
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1995). </year> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 263-286. </pages>
Reference-contexts: The goal of a learning system is to produce a classification rule which would assign new examples to correct classes. Decision tree methods and artificial neural network approaches are two representative machine learning algorithms. Error-correcting output coding (ECOC) dramatically improves the generalization ability of learning algorithms <ref> (Dietterich & Bakiri, 1995) </ref>. Kong and Dietterich (1995) show that ECOC is a kind of voting among multiple hypotheses and can reduce both bias and variance of the decision-tree learning algorithm. Many classification algorithms are able to supply more useful information than just a class identifier as an output. <p> Dietterich and Bakiri (Dietterich & Bakiri, 1991, 1995) have shown that the error-correcting output coding technique works very well with the decision-tree algorithm C4.5. Kong and Dietterich <ref> (Kong & Dietterich, 1995) </ref> have shown that ECOC is a kind of voting, and can reduce both bias and variance, and the bias correction ability relies on the non-local behavior of the underlying learning algorithms. 2 3 ECOC and Class Probabilities In decision tree induction algorithms, a leaf contains a subset
Reference: <author> Izenman, A. J. </author> <year> (1991). </year> <title> Recent developments in nonparametric density estimation. </title> <journal> Journal of the American Statistical Associaion, </journal> <volume> 86 (413), </volume> <pages> 205-224. </pages>
Reference: <author> Kong, E. B., & Dietterich, T. G. </author> <year> (1995). </year> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In 12th International Conference on Machine Learning, </booktitle> <pages> pp. 313-321. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Dietterich and Bakiri (Dietterich & Bakiri, 1991, 1995) have shown that the error-correcting output coding technique works very well with the decision-tree algorithm C4.5. Kong and Dietterich <ref> (Kong & Dietterich, 1995) </ref> have shown that ECOC is a kind of voting, and can reduce both bias and variance, and the bias correction ability relies on the non-local behavior of the underlying learning algorithms. 2 3 ECOC and Class Probabilities In decision tree induction algorithms, a leaf contains a subset
Reference: <author> Lawson, C. L., & Hanson, R. J. </author> <year> (1974). </year> <title> Solving Least Squares Problems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J. </address>
Reference-contexts: However, if we realize the fact that the equations are not exact but inexact equalities whose right-hand side vector is an estimate, we may solve the equations by the method of least squares <ref> (Lawson & Hanson, 1974) </ref>. The least squares method finds the closest point in a given subspace within a feasible region. 4 Experiments on Overlapping Classes We can test the accuracy of posterior probability estimates by generating overlapping class-conditional probability density functions p (xjc i )'s.
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Decision trees as probabilistic classfiers. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pp. 31-37. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Some of these leaves may contain training examples from more than one class. In order to estimate the reliability of a classification arising from a leaf, Quinlan <ref> (Quinlan, 1987, 1993) </ref> estimates the class probabilities. Let n be the number of training examples in the leaf and e be the number of examples that do not belong to the class designated by the leaf. The class probability is estimated as ne n . <p> Let n be the number of training examples in the leaf and e be the number of examples that do not belong to the class designated by the leaf. The class probability is estimated as ne n . This is called the central estimate <ref> (Quinlan, 1987) </ref>. The outputs of the individual f l (x) can be themselves class probabilities ^ f l (x) = P (f l (x) = 1).
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Program for Empirical Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Estimating probabilities accurately has been studied in machine learning and statistics (Quinlan, 1987; Izenman, 1991; Richard & Lippmann, 1991). In this paper, we will develop methods for estimating such probabilities in ECOC framework using decision-tree learning algorithms such as C4.5 <ref> (Quinlan, 1993) </ref>. We will demonstrate through experiments that the method presents more accurate and reliable posterior probabilities. fl Supported by KOSEF under grant 961-0901-006-2. 1 The remainder of this paper is structured as follows. First, we describe the error-correcting output coding method and summarize the previous results.
Reference: <author> Richard, D. M., & Lippmann, R. P. </author> <year> (1991). </year> <title> Neural network classifiers estimate Bayesian a posteriori probabilities. </title> <journal> Neural Computation, </journal> <volume> 4 (3), </volume> <pages> 461-483. 6 </pages>
References-found: 8

