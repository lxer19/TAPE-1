URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92301-S.ps.gz
Refering-URL: http://wwwipd.ira.uka.de/~hopp/seminar97.html
Root-URL: 
Email: reinhard@rice.edu  
Title: Compiler Support for Machine Independent Parallelization of Irregular Problems  
Author: Reinhard von Hanxleden 
Degree: Thesis Proposal  
Address: Houston, TX 77251  
Affiliation: Department of Computer Science Rice University  
Date: November 1992  
Abstract: The Fortran D group at Rice University aims at providing a machine independent data parallel programming style, in which the applications programmer uses a dialect of sequential Fortran and high level distribution annotations. Extracting parallelism from these applications typically is straightforward, but making efficient use of this parallelism for irregular applications, such as molecular dynamics or unstructured meshes, is a challenge due to the limited compile-time knowledge about data access patterns. It is my thesis that the spatial locality of the underlying problems can be used as a basis of compiler support for parallelizing such applications. Value-based decompositions are an extension of Fortran D to express the spatial locality of an application and to assist the compiler in computing a distribution with both a balanced computational workload and high data access locality. A communication data flow framework detects opportunities to combine messages, move them into less frequently executed code regions, or even eliminate them. Loop flattening is a code transformation to overcome SIMD specific control flow limitations when executing nested loops with varying inner loop bounds, which are typical for irregular problems. 
Abstract-found: 1
Intro-found: 1
Reference: [All70] <author> F. E. Allen. </author> <title> Control flow analysis. </title> <journal> ACM SIGPLAN Notices, </journal> <year> 1970. </year>
Reference-contexts: The W2 compiler [GS90] for the Warp multiprocessor gathers information like the set of definitions reaching a basic block to exploit the fine-grain parallelism offered by the highly pipelined functional units. It is based on interval analysis <ref> [All70, Coc70] </ref> and computes information with array region granularity. Granston and Veidenbaum combine flow and dependence analysis to detect redundant global memory accesses in parallelized and vectorized codes [GV91]. They assume that the program is already annotated with read/write operations.
Reference: [APT90] <author> F. Andre, J. Pazat, and H. Thomas. </author> <title> Pandore: A system to manage data distribution. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore <ref> [APT90] </ref>, Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [Bad87a] <author> S. B. Baden. </author> <title> Run-Time Partitioning of Scientific Continuum Calculations Running on Multiprocessors. </title> <type> PhD thesis, </type> <institution> Lawrence Berkeley Laboratory, University of California, </institution> <year> 1987. </year>
Reference-contexts: The second group comes into play after the data structures have been laid out; these tools try to free the user from dealing with the access properties of the parallel program; i.e., they examine data locality. 5 2.3.1 Tools based on spatial decomposition The Generic Multiprocessor The Generic Multiprocessor (GenMP) <ref> [Bad87a, Bad87b, Bad91] </ref>, aims at providing a machine independent programming environment for a certain class of problems, namely scientific calculations that are spatially localized on a mesh.
Reference: [Bad87b] <author> S. B. Baden. </author> <title> Very large vortex calculations in two dimensions. In Vortex Methods, </title> <booktitle> volume 1360 of Lecture Notes in Mathematics, </booktitle> <address> Los Angeles, CA, May 1987. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The second group comes into play after the data structures have been laid out; these tools try to free the user from dealing with the access properties of the parallel program; i.e., they examine data locality. 5 2.3.1 Tools based on spatial decomposition The Generic Multiprocessor The Generic Multiprocessor (GenMP) <ref> [Bad87a, Bad87b, Bad91] </ref>, aims at providing a machine independent programming environment for a certain class of problems, namely scientific calculations that are spatially localized on a mesh.
Reference: [Bad91] <author> S. B. Baden. </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(1) </volume> <pages> 145-157, </pages> <year> 1991. </year>
Reference-contexts: The second group comes into play after the data structures have been laid out; these tools try to free the user from dealing with the access properties of the parallel program; i.e., they examine data locality. 5 2.3.1 Tools based on spatial decomposition The Generic Multiprocessor The Generic Multiprocessor (GenMP) <ref> [Bad87a, Bad87b, Bad91] </ref>, aims at providing a machine independent programming environment for a certain class of problems, namely scientific calculations that are spatially localized on a mesh.
Reference: [Bad92] <author> S. B. Baden. </author> <title> Lattice parallelism: A parallel programming model for manipulating localized non-uniform scientific data structures. </title> <booktitle> In Intel Super 20 computer University Partners Conference, </booktitle> <address> Timberline Lodge, Mt. Hood, OR, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: The limitation of this approach lies in its specificity towards mesh-based, localized applications, which we try to overcome by using general value-based decompositions as introduced in Section 3.2. Lattice Parallelism Lattice Parallelism (LPar) <ref> [Bad92] </ref> is an SPMD programming model that supports coarse-grained parallelism based on the Fidil language [HC88] and the owner computes rule. It is intended for non-uniform computations that involve partial differential equations and have local structure. <p> It enables very elegant formulations of a limited class of problems and can be seen as a potential user of an implementation of the value-based decompositions proposed in Section 3.2. Citing Baden <ref> [Bad92] </ref>: It is not an implementation-level system, and relies on application libraries or other run time systems to handle data partitioning or to handle machine level optimizations, that could be provided for example by Dino or by For tran D. 2.3.2 Tools based on access patterns The inspector-executor paradigm An important
Reference: [Bal90] <author> V. Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 154-170, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The bulk of the work in this field has treated all variables as scalars, resulting in a very conservative analysis for array variables. More precise methods are based on representations of array subsets, like data access descriptors <ref> [Bal90] </ref> or regular sections [HK91]. The W2 compiler [GS90] for the Warp multiprocessor gathers information like the set of definitions reaching a basic block to exploit the fine-grain parallelism offered by the highly pipelined functional units.
Reference: [BB87] <author> M. J. Berger and S. Bokhari. </author> <title> A partitioning strategy for non-uniform problems on multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):570-580, </volume> <year> 1987. </year>
Reference: [BBHL90] <author> T. Bemmerl, A. Bode, O. Hansen, and T. Ludwig. </author> <title> A testbed for dynamic loadbalancing on distributed memory multiprocessors. </title> <note> PUMA Working Paper 14, </note> <institution> Technical University Munich, Munchen, Germany, </institution> <month> August </month> <year> 1990. </year>
Reference: [BBO + 83] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swami-nathan, and M. Karplus. CHARMM: </author> <title> A program for macromolecular energy, minimization and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4(2) </volume> <pages> 187-217, </pages> <year> 1983. </year>
Reference-contexts: Some of these tradeoffs are listed here. Simplicity: Critical in the development stage, but also for later maintenance. For example, one might argue that the long lasting success of molecular dynamics packages like Gromos [GB88] or Charmm <ref> [BBO + 83] </ref> is at least partially due to the simplicity of their main data structures. These programs use standard arrays with indirect accesses, as opposed to complicated pointer structures with data allocation/deallocation or other data structures that capture spatial locality.
Reference: [BCS91] <author> B. Bagheri, T. W. Clark, and L. R. Scott. </author> <title> IPfortran (a parallel extension of Fortran) reference manual. </title> <institution> Research Report UH/MD-119, Dept. of Mathematics, University of Houston, </institution> <year> 1991. </year>
Reference-contexts: This is accompanied with a comparison between Fortran D and low level message passing code, IPfortran, which still operates in the local name space of message passing code, but provides language constructs for easy access of non-local values <ref> [BCS91] </ref>. The dominating difficulties addressed here are of type 1 and 3 (see Section 1.1).
Reference: [BCZ92] <author> S. Benkner, B. Chapman, and H. Zima. </author> <title> Vienna Fortran 90. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran <ref> [BCZ92] </ref>.
Reference: [Ben75] <author> J. L. Bentley. </author> <title> Multidimensional binary search trees used for associative searching. </title> <journal> Communications of the ACM, </journal> <volume> 18 </volume> <pages> 509-516, </pages> <year> 1975. </year>
Reference-contexts: Although the concept of value-based decompositions was developed with simple arrays as underlying data structures, the thesis will also contain an analysis of how well this concept applies to the more advanced data structures used in hierarchical solvers <ref> [Ben75, FB74, LW82] </ref>. On the practical side, this involves the parallelization of a Molecular Dynamics code (Gromos).
Reference: [BH86] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(n log n) force calculation algorithm. </title> <booktitle> Nature, </booktitle> <year> 1986. </year>
Reference-contexts: However, they enable us to use faster algorithms; in the N -body example, we can replace the nave O (N 2 ) algorithm with hierarchical tree methods, having an O (N log N ) <ref> [BH86] </ref> or even an O (N ) time bound [GR87]. With increasing processor power opening the door to solving scientific problems that were previously impractical to solve ("Grand Challenges"), the relative importance of the already widespread irregular applications is expected to increase even further.
Reference: [BHMS91] <author> M. Bromley, S. Heller, T. McNerney, and G. Steele, Jr. </author> <title> Fortran at ten gigaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The virtual machine model used by CM Fortran <ref> [BHMS91] </ref> can be seen as a typical example of the latter [Chr91], as explained in more detail in Section 3.4. A programmer should not have to make this tradeoff when choosing a compiler, especially in a performance oriented field like scientific parallel computing.
Reference: [Bia91] <author> E. S. Biagioni. </author> <title> Scan Directed Load Balancing. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <year> 1991. </year>
Reference-contexts: The restricted control flow of pure SIMD programming has been addressed by several researchers. General simulators of MIMD semantics on SIMD machines have been implemented by Kuszmaul [Kus86] and Hudak et al. [HM88] on the Connection Machine and by Biagioni <ref> [Bia91] </ref> on the MasPar. These simulations are generally based on graph reduction interpreters for functional languages. Their performance tends to be scalable, but in absolute measures still below the speed of sequential workstations. <p> Scan operations are one efficient way for determining the total workload and its distribution <ref> [Bia91, Ble90] </ref>. On architectures providing an embedded reduction tree, this operation can be done in O (log P ) cycles. 3 Research Plan 3.1 Overview For establishing the thesis stated in Section 1, I am currently pursuing three closely related projects. <p> D is typically small enough to be replicated across processors, so ffi can still be evaluated locally. This kind of mapping is applicable if the data structure, here X, directly reflects spatial locality. Furthermore, a data structure with a fixed density, such as Variable Conductance Diffusion <ref> [Bia91] </ref>, is preferable since in this case the memory requirements per spatial space unit are also fixed. These mapping functions can be further subdivided into several classes, ranging from restrictive but compact and fast to more general but larger and slower. <p> Figure 2 can be represented as a triplet of one-dimensional mapping functions (ffi x ; ffi y ; ffi z ): ffi (i; j; k) = (ffi x (i); ffi y (j); ffi z (k)): In this case, ffi can be encoded by Q + R + S 3 integers <ref> [Bia91] </ref>, which is typically O ( 3 p P ) for a 3-D index space. A hierarchical mapping function, as illustrated in Figure 3, can also be represented as a triplet of mapping functions (ffi x ; ffi y ; ffi z ).
Reference: [Ble90] <author> G. E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Loop flattening [HK92] is one technique to overcome this limitation for loop nests with varying loop bounds, as proposed in Section 3.4. Loop flattening can also be used to process multiple array segments of different lengths per processor, as introduced in Blelloch's V-RAM model <ref> [Ble90] </ref>. Thus it can be viewed as a generalization of substituting direct addressing with indirect addressing as Tomboulian and Pappas did for computing the Mandelbrot set [TP90]. 2.6.4 Fast scan operations The inhomogeneous workload across processors generally associated with irregular problems calls for load balancing. <p> Scan operations are one efficient way for determining the total workload and its distribution <ref> [Bia91, Ble90] </ref>. On architectures providing an embedded reduction tree, this operation can be done in O (log P ) cycles. 3 Research Plan 3.1 Overview For establishing the thesis stated in Section 1, I am currently pursuing three closely related projects.
Reference: [Bok81] <author> S. H. Bokhari. </author> <title> On the mapping problem. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(3):207-214, </volume> <year> 1981. </year>
Reference: [Bra89] <author> T. Braunl. </author> <title> Structured SIMD programing in Parallaxis. </title> <journal> Structured Programming, </journal> <volume> 10(3) </volume> <pages> 121-132, </pages> <year> 1989. </year>
Reference-contexts: However, this kind of loop nest causes special problems for SIMD (Single Instruction, Multiple Data) architectures because of the restricted control flow on these machines <ref> [Bra89] </ref>. If the number of iterations of the inner loops varies from one outer loop iteration to the next, as for example in the code shown in Figure 9, then the restriction to a common program counter makes a nave SIMD implementation inefficient.
Reference: [BS90] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> ICASE Interim Report 13, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Parti The Parti primitives (Parallel Automated Runtime Toolkit at ICASE) are a set of high level communication routines that provide convenient access to off-processor elements of arrays that are accessed (and distributed) irregularly <ref> [BS90, SBW90] </ref>. Parti is first to propose and implement user-defined irregular distributions [MSS + 88] and a hashed cache for nonlocal values [MSMB90]. They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2.
Reference: [BSGM90] <author> H. Berryman, J. Saltz, W. Gropp, and R. Mirchandaney. </author> <title> Krylov methods preconditioned with incompletely factored matrices on the CM-2. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 186-190, </pages> <year> 1990. </year> <month> 21 </month>
Reference: [BT88] <author> Henri E. Bal and Andrew S. Tanenbaum. </author> <title> Distributed programming with shared data. </title> <booktitle> In Proceedings of the IEEE CS 1988 International Conference on Computer Languages, </booktitle> <pages> pages 82-91, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin [CBZ91, KCZ92], Orca <ref> [BT88] </ref>, and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency proto 10 col, which can be lazy or eager, based on invalidations or updates. Munin supports several such protocols, the choice between them for each individual shared variable is guided by access pattern annotations provided by the user.
Reference: [BZ91] <author> Brian N. Bershad and Matthew J. Zekauskas. </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway <ref> [BZ91] </ref>, Munin [CBZ91, KCZ92], Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency proto 10 col, which can be lazy or eager, based on invalidations or updates.
Reference: [CAL + 89] <author> J. Chase, F. Amador, E. Lazowska, H. Levy, and R. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Examples of these systems are Amber <ref> [CAL + 89] </ref>, Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin [CBZ91, KCZ92], Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency proto 10 col, which can be lazy or eager, based on invalidations or updates.
Reference: [CBZ91] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin <ref> [CBZ91, KCZ92] </ref>, Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency proto 10 col, which can be lazy or eager, based on invalidations or updates.
Reference: [CCRS91] <author> C. Chase, A. Cheung, A. Reeves, and M. Smith. </author> <title> Paragon: A parallel programming environment for scientific applications using communication structures. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon <ref> [CCRS91] </ref>, Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [CF89] <author> A. Cox and R. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor: Experiences with Platinum. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin [CBZ91, KCZ92], Orca [BT88], and Platinum <ref> [CF89] </ref>. They preserve sequential semantics by enforcing a consistency proto 10 col, which can be lazy or eager, based on invalidations or updates. Munin supports several such protocols, the choice between them for each individual shared variable is guided by access pattern annotations provided by the user.
Reference: [CG89] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Early work in the field of compiling for distributed memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions [GB91, HA90, RS89]. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda <ref> [CG89] </ref>, Strand [FT90, FO90], and Delirium [LS91] have been defined.
Reference: [CHK + 92] <author> T. W. Clark, R. v. Hanxleden, K. Kennedy, C. Koelbel, and L. R. Scott. </author> <title> Evaluating parallel languages for molecular dynamics computations. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: As described in Appendix A, this project already has led to valuable insights into which of the language concepts well proven for regular problems carry over easily into the irregular world and which concepts have to be 15 modified or extended <ref> [CHK + 92, CHMS92] </ref>. 3.3 Communication Analysis for Irregular Problems One issue related to value-based decompositions, and to irregular decompositions in general, is how to generate the necessary communication for accessing the data distributed this way. We decided to use the Parti communication routines which are described in Section 2.3.2.
Reference: [CHMS92] <author> T. W. Clark, R. v. Hanxleden, J. A. McCammon, and L. R. Scott. </author> <title> Paral-lelization strategies for a molecular dynamics program. In Intel Supercomputer University Partners Conference, </title> <address> Timberline Lodge, Mt. Hood, OR, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: These, however, are not all one-dimensional any more; instead, it is ffi (i; j; k) = ffi x (i; ffi y (j; ffi z (k))): Here ffi can be represented with S 1 + (R 1)S + (Q 1)RS = P 1 integers <ref> [CHMS92] </ref>, see also Section A.4.1. A general mapping function cannot be represented as a simple composition of subfunctions; i.e., we cannot decouple any of the dimensions from each other. <p> As described in Appendix A, this project already has led to valuable insights into which of the language concepts well proven for regular problems carry over easily into the irregular world and which concepts have to be 15 modified or extended <ref> [CHK + 92, CHMS92] </ref>. 3.3 Communication Analysis for Irregular Problems One issue related to value-based decompositions, and to irregular decompositions in general, is how to generate the necessary communication for accessing the data distributed this way. We decided to use the Parti communication routines which are described in Section 2.3.2.
Reference: [Chr91] <author> P. Christy. </author> <title> Virtual processors considered harmful. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The virtual machine model used by CM Fortran [BHMS91] can be seen as a typical example of the latter <ref> [Chr91] </ref>, as explained in more detail in Section 3.4. A programmer should not have to make this tradeoff when choosing a compiler, especially in a performance oriented field like scientific parallel computing.
Reference: [CK88] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Early work in the field of compiling for distributed memory machines focussed on defining frameworks for nonlocal memory accesses <ref> [CK88] </ref> and data distributions [GB91, HA90, RS89]. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. <p> towards the thesis are summarized in Sections 3.2, 3.3, and 3.4, respectively; the Appendices A, B, and C contain more details for each of these. 3.2 Value-Based Decompositions A standard problem in compiling for distributed memory multiprocessors is to determine which processors own which elements of a data array X <ref> [CK88, Fox88, PRV87] </ref>. For simplicity of notation, we assume in the following that X is three-dimensional; the extension to lower or higher dimensions is straightforward.
Reference: [CM90] <author> T. W. Clark and J. A. McCammon. </author> <title> Parallelization of a molecular dynamics non-bonded force algorithm for MIMD architectures. </title> <journal> Computers & Chemistry, </journal> <volume> 14(3) </volume> <pages> 219-224, </pages> <year> 1990. </year>
Reference: [CMS91] <author> T. W. Clark, J. A. McCammon, and L. R. Scott. </author> <title> Parallel molecular dynamics. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Houston, TX, </address> <month> March </month> <year> 1991. </year>
Reference: [Coc70] <author> J. Cocke. </author> <title> Global common subexpression elimination. </title> <journal> ACM SIGPLAN Notices, </journal> <year> 1970. </year> <month> 22 </month>
Reference-contexts: The W2 compiler [GS90] for the Warp multiprocessor gathers information like the set of definitions reaching a basic block to exploit the fine-grain parallelism offered by the highly pipelined functional units. It is based on interval analysis <ref> [All70, Coc70] </ref> and computes information with array region granularity. Granston and Veidenbaum combine flow and dependence analysis to detect redundant global memory accesses in parallelized and vectorized codes [GV91]. They assume that the program is already annotated with read/write operations.
Reference: [Dah90] <author> D. Dahl. </author> <title> Mapping and compiled communication on the connection ma-chine system. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The dataflow framework presented as part of this proposal in Section 3.3 is designed for attacking the second part of the problem, namely enabling the compiler to make good use of these primitives without further advice by the user. The Communication Compiler The Communication Compiler <ref> [Dah90] </ref> is a software facility for scheduling general communications on the Connection Machine. It employs simulated annealing to find a data mapping with as low communication requirements as possible. It uses a recursive routing algorithm to determine an actual communication schedule.
Reference: [DG90] <author> F. Dehne and M. Gastaldo. </author> <title> A note on the load balancing problem for coarse grained hypercube dictionary machines. </title> <booktitle> Parallel Computing, </booktitle> <year> 1990. </year>
Reference: [DMS + 92] <author> R. Das, D. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <booktitle> AIAA-92-0562. In Proceedings of the 30th Aerospace Sciences Meeting. AIAA, </booktitle> <month> January </month> <year> 1992. </year>
Reference: [EHL77] <author> J. Eastwood, R. Hockney, and D. Lawrence. </author> <title> PM3DP the three dimensional periodic particle-particle/ particle-mesh program. </title> <journal> Computer Physics Communications, </journal> <volume> 19 </volume> <pages> 215-261, </pages> <year> 1977. </year>
Reference: [ELZ86] <author> D. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> A comparison of receiver-initiated and sender-initiated adaptive load sharing. Performance Evaluation, </title> <booktitle> 6 </booktitle> <pages> 53-68, </pages> <year> 1986. </year>
Reference-contexts: Here information about the utilization of different processors can be helpful. However, the work done in this area has focussed on thread based parallelism <ref> [ELZ86, Luc88] </ref>, typically even associated with distinct processes, instead of data parallelism. 2.6 The Hardware Some hardware facilities that can be particularly useful for irregular applications are the following. 2.6.1 Low latency Due to the typically very irregular access patterns, message blocking becomes more complicated than for regular applications [SHG92].
Reference: [FB74] <author> R. A. Finkel and J. L. Bentley. </author> <title> Quad trees: A data structure for retrieval on composite keys. </title> <journal> Acta Informatica, </journal> <volume> 4 </volume> <pages> 1-9, </pages> <year> 1974. </year>
Reference-contexts: Although the concept of value-based decompositions was developed with simple arrays as underlying data structures, the thesis will also contain an analysis of how well this concept applies to the more advanced data structures used in hierarchical solvers <ref> [Ben75, FB74, LW82] </ref>. On the practical side, this involves the parallelization of a Molecular Dynamics code (Gromos).
Reference: [FHK + 90] <author> G. C. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year> <note> Revised April, </note> <year> 1991. </year>
Reference-contexts: 1 Introduction The Fortran D group at Rice University aims at providing a "machine independent parallel programming style," in which the applications programmer uses a dialect of sequential Fortran and annotates it with high level distribution information <ref> [FHK + 90] </ref>. From this annotated program, Fortran D compilers will generate codes in different native Fortran dialects for different parallel architectures. The target architectures include both shared and distributed memory architectures and both MIMD and SIMD machines. <p> The following contains a very brief summary of its basic concepts, 8 the complete language is described in detail elsewhere <ref> [FHK + 90] </ref>. Citing Hiranandani et al. [HKT92b]: Fortran D is the first language to provide users with explicit control over data partitioning with both data alignment and distribution specifications. The DECOMPOSITION statement specifies an abstract problem or index domain. <p> Each processor can evaluate ffi locally for all (i; j; k) 2 S. This function is typically hardwired into the compiler and the code it generates <ref> [FHK + 90] </ref>. * For irregular mappings that can be represented in some compact format, for example because they map S into P rectangular subdomains.
Reference: [FJL + 88] <author> G. C. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Multiprocessors. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: Obviously it is the key towards good efficiency, and it is often worth redesigning a preexisting sequential algorithm before taking it as a basis for a parallel implementation. An typical example can be found in the WaTor population simulation <ref> [FJL + 88] </ref>. In WaTor, members of different species breed, move around within a quantized space, and die. To avoid overpopulation, one rule is that whenever two animals move to the same location, one of them has to retreat and try to go elsewhere.
Reference: [FKW86] <author> G. C. Fox, A. Kolowa, and R. Williams. </author> <title> The implementation of a dynamic load balancer. </title> <booktitle> In Proceedings of the Second Hypercube Microprocessors Conference, </booktitle> <pages> pages 114-121, </pages> <address> Knoxville, TN, </address> <month> September </month> <year> 1986. </year>
Reference: [FO90] <author> I. Foster and R. Overbeek. </author> <title> Bilingual parallel programming. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Early work in the field of compiling for distributed memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions [GB91, HA90, RS89]. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand <ref> [FT90, FO90] </ref>, and Delirium [LS91] have been defined.
Reference: [Fox88] <author> G. C. Fox. </author> <title> Domain decomposition in distributed and shared memory environments. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <pages> pages 1042-1073, </pages> <address> Or-lando, FL, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: towards the thesis are summarized in Sections 3.2, 3.3, and 3.4, respectively; the Appendices A, B, and C contain more details for each of these. 3.2 Value-Based Decompositions A standard problem in compiling for distributed memory multiprocessors is to determine which processors own which elements of a data array X <ref> [CK88, Fox88, PRV87] </ref>. For simplicity of notation, we assume in the following that X is three-dimensional; the extension to lower or higher dimensions is straightforward.
Reference: [Fox91] <author> G. Fox. </author> <title> Parallel problem architectures and their implications for portable parallel software systems. </title> <type> CRPC Report CRPC-TR91120, </type> <institution> Center for Research on Parallel Computation, Syracuse University, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: However, with the field of parallel programming slowly maturing, parallel algorithms have been formulated for many important application domains <ref> [Fox91] </ref>.
Reference: [FT90] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: Early work in the field of compiling for distributed memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions [GB91, HA90, RS89]. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand <ref> [FT90, FO90] </ref>, and Delirium [LS91] have been defined.
Reference: [GAY91] <author> E. Gabber, A. Averbuch, and A. Yehudai. </author> <title> Experience with a portable parallelizing Pascal compiler. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C <ref> [GAY91] </ref>, Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [GB88] <author> W. F. van Gunsteren and H. J. C. Berendsen. GROMOS: </author> <title> GROningen MOlecular Simulation software. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year> <month> 23 </month>
Reference-contexts: Some of these tradeoffs are listed here. Simplicity: Critical in the development stage, but also for later maintenance. For example, one might argue that the long lasting success of molecular dynamics packages like Gromos <ref> [GB88] </ref> or Charmm [BBO + 83] is at least partially due to the simplicity of their main data structures. These programs use standard arrays with indirect accesses, as opposed to complicated pointer structures with data allocation/deallocation or other data structures that capture spatial locality.
Reference: [GB91] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Early work in the field of compiling for distributed memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions <ref> [GB91, HA90, RS89] </ref>. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined.
Reference: [GB92] <author> M. Gupta and P. Banerjee. </author> <title> Compile-time estimation of communication costs on multicomputers. </title> <booktitle> In Proceedings of the 6th International Parallel Processing Symposium, </booktitle> <address> Beverly Hills, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 <ref> [GB92] </ref>, Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [Ger90] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Parallelism is expressed by mapping a logical processing Domain onto a spatial processing Domain. LPar supports load balancing and ghost regions, in which each processor stores data within a certain proximity to its own data, similarly to overlap regions <ref> [Ger90] </ref>. It is currently implemented in C++ for the iPSC/860. LPar treats parallelism at a very high level, it manipulates the structure of the data, rather than the data itself.
Reference: [GR87] <author> L. Greengard and V. Rokhlin. </author> <title> A fast algorithm for particle simulation. </title> <journal> Journal of Computational Physics, </journal> <volume> 73(325), </volume> <year> 1987. </year>
Reference-contexts: However, they enable us to use faster algorithms; in the N -body example, we can replace the nave O (N 2 ) algorithm with hierarchical tree methods, having an O (N log N ) [BH86] or even an O (N ) time bound <ref> [GR87] </ref>. With increasing processor power opening the door to solving scientific problems that were previously impractical to solve ("Grand Challenges"), the relative importance of the already widespread irregular applications is expected to increase even further.
Reference: [GS90] <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The bulk of the work in this field has treated all variables as scalars, resulting in a very conservative analysis for array variables. More precise methods are based on representations of array subsets, like data access descriptors [Bal90] or regular sections [HK91]. The W2 compiler <ref> [GS90] </ref> for the Warp multiprocessor gathers information like the set of definitions reaching a basic block to exploit the fine-grain parallelism offered by the highly pipelined functional units. It is based on interval analysis [All70, Coc70] and computes information with array region granularity.
Reference: [GV91] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: It is based on interval analysis [All70, Coc70] and computes information with array region granularity. Granston and Veidenbaum combine flow and dependence analysis to detect redundant global memory accesses in parallelized and vectorized codes <ref> [GV91] </ref>. They assume that the program is already annotated with read/write operations. Their technique tries to eliminate these operations where possible, also across loop nests and in the presence of conditionals.
Reference: [HA90] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Early work in the field of compiling for distributed memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions <ref> [GB91, HA90, RS89] </ref>. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined.
Reference: [Han89] <author> R. v. Hanxleden. </author> <title> Parallelizing dynamic processes. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, The Pennsylvania State University, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: A special case here are recursive mapping functions like the orthogonal recursive bisection shown in Figure 4, which can still be encoded in P 1 integers <ref> [Han89] </ref>. 13 decomposition for P = 64 processors. orthogonal mapping for P = 64 processors. hierarchical mapping for P = 64 processors. bisective mapping for P = 64 processors.
Reference: [HC88] <author> P. N. Hilfinger and P. Colella. FIDIL: </author> <title> A language for scientific programming. </title> <type> Technical report, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1988. </year>
Reference-contexts: The limitation of this approach lies in its specificity towards mesh-based, localized applications, which we try to overcome by using general value-based decompositions as introduced in Section 3.2. Lattice Parallelism Lattice Parallelism (LPar) [Bad92] is an SPMD programming model that supports coarse-grained parallelism based on the Fidil language <ref> [HC88] </ref> and the owner computes rule. It is intended for non-uniform computations that involve partial differential equations and have local structure. It explicitly excludes unstructured calculations such as sparse matrix linear algebra and finite element problems.
Reference: [HHKT91] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <type> Technical Report TR91-169, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Novem-ber </month> <year> 1991. </year>
Reference-contexts: A prototype Fortran D compiler targeting the iPSC/860 has been under development. This compiler has had considerable success with regular problems <ref> [HHKT91, HKK + 91, HKT91, HKT92b] </ref>.
Reference: [Hig92] <institution> Proceedings of the High Performance Fortran Forum, Houston, TX, </institution> <month> Jan-uary </month> <year> 1992. </year>
Reference-contexts: The formation of the High Performance Fortran Forum <ref> [Hig92] </ref>, an ongoing standardization effort for commercial parallel Fortran compilers, is certainly an indication for this progress. High Performance Fortran (HPF) derives many of its underlying for Fortran D, which is the base language chosen for the extensions to be proposed here.
Reference: [HK91] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The bulk of the work in this field has treated all variables as scalars, resulting in a very conservative analysis for array variables. More precise methods are based on representations of array subsets, like data access descriptors [Bal90] or regular sections <ref> [HK91] </ref>. The W2 compiler [GS90] for the Warp multiprocessor gathers information like the set of definitions reaching a basic block to exploit the fine-grain parallelism offered by the highly pipelined functional units. It is based on interval analysis [All70, Coc70] and computes information with array region granularity.
Reference: [HK92] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Relaxing SIMD control flow constraints using loop transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: This can either be emulated using stacks of MASK bits, 11 or it can be implemented directly in an MSIMD machine which contains multiple pro-gram counters. In either case, their proposal is mainly concerned with enabling the concurrent execution of both branches in IF-THEN-ELSE constructs. Loop flattening <ref> [HK92] </ref> is one technique to overcome this limitation for loop nests with varying loop bounds, as proposed in Section 3.4. Loop flattening can also be used to process multiple array segments of different lengths per processor, as introduced in Blelloch's V-RAM model [Ble90]. <p> This is due to the fact that the synchronous execution of instructions forces each processor to either perform the operation or wait in an idle state until all processors have completed the operation. " To overcome this limitation, we propose a transformation which we call loop flattening <ref> [HK92] </ref>. Roughly speaking, loop flattening amounts to lifting the innermost loop body up into the outer, parallel loop by merging the control of the inner loops with the control of the outer loop, as shown in Figure 10. <p> Further details on this can be found in Appendix C. 17 3.4.1 Contributions The development of the loop flattening concept, which is a loop transformation strategy to overcome certain control flow limitations on SIMD architectures <ref> [HK92] </ref>, addresses the problem of providing architecture independent compiler support for parallelizing irregular problems. These limitations emerge within nested loops if the number of iterations of the inner loop varies between different iterations of the outer loop. <p> Loop flattening was designed to ease some particular SIMD restrictions without introducing any overheads; however, it supports a programming style that seems to be preferable on current SIMD machines even when running regular applications <ref> [HK92] </ref>. This rather surprising result suggests that flattened loops make it easier for compilers to derive the information they need for performing certain optimizations, such as pruning out virtual processor layers for individual statements whenever possible.
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: A prototype Fortran D compiler targeting the iPSC/860 has been under development. This compiler has had considerable success with regular problems <ref> [HHKT91, HKK + 91, HKT91, HKT92b] </ref>. <p> So far, the prospective test suite includes programs covering * molecular dynamics (Gromos, see Section A.1), * unstructured meshes [Mav91], and * sparse matrices. Beyond working with these source codes, other applications (whose source codes are not available or not written in Fortran) will be evaluated at least theoretically <ref> [HKK + 91] </ref>. 3.4 MIMD vs. SIMD for Irregular Problems In the process of parallelizing scientific programs, it is common to find loop nests in which the outer loop can run in parallel but the amount of computation in the inner loop varies for different iterations of the outer loop.
Reference: [HKK + 92] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the 24 Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: However, to derive a good communication placement as the one shown in Figure 6 is nontrivial. The approach taken in this thesis is based on a dataflow framework <ref> [HKK + 92] </ref> which bears similarities to classical techniques such as common subexpression elimination, loop invariant code motion, and dead code elimination. <p> Eliminating redundant data movements in the communication schedules is achieved by using a hash table. 3.3.1 Contributions A major part of this thesis is the design and implementation of a data flow framework for compile time reasoning about irregular array access patterns that occur when using value-based decompositions <ref> [HKK + 92] </ref>. The generated code should make effective use of the scatter/gather operations provided by the Parti system (see Section 2.3.2) and 16 maximize message blocking and data reuse, as described in Section B. This mainly addresses Difficulties 2 and 4 in Section 1.1.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: A prototype Fortran D compiler targeting the iPSC/860 has been under development. This compiler has had considerable success with regular problems <ref> [HHKT91, HKK + 91, HKT91, HKT92b] </ref>.
Reference: [HKT92a] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehro-tra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference: [HKT92b] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: A prototype Fortran D compiler targeting the iPSC/860 has been under development. This compiler has had considerable success with regular problems <ref> [HHKT91, HKK + 91, HKT91, HKT92b] </ref>. <p> The following contains a very brief summary of its basic concepts, 8 the complete language is described in detail elsewhere [FHK + 90]. Citing Hiranandani et al. <ref> [HKT92b] </ref>: Fortran D is the first language to provide users with explicit control over data partitioning with both data alignment and distribution specifications. The DECOMPOSITION statement specifies an abstract problem or index domain.
Reference: [HM88] <author> P. Hudak and E. Mohr. </author> <title> Graphinators and the duality of SIMD and MIMD. </title> <booktitle> In Proceedings of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <year> 1988. </year>
Reference-contexts: This may cause additional idling when running irregular problems on SIMD machines instead of MIMD machines. The restricted control flow of pure SIMD programming has been addressed by several researchers. General simulators of MIMD semantics on SIMD machines have been implemented by Kuszmaul [Kus86] and Hudak et al. <ref> [HM88] </ref> on the Connection Machine and by Biagioni [Bia91] on the MasPar. These simulations are generally based on graph reduction interpreters for functional languages. Their performance tends to be scalable, but in absolute measures still below the speed of sequential workstations.
Reference: [Hoa85] <author> C. A. R. Hoare. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice-Hall, </publisher> <address> En-glewood Cliffs, NJ, </address> <year> 1985. </year>
Reference: [HQL + 91] <author> P. Hatcher, M. Quinn, A. Lapadula, B. Seevers, R. Anderson, and R. Jones. </author> <title> Data-parallel programming on MIMD computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 377-383, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C <ref> [HQL + 91, RS87] </ref>, Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [HRB90] <author> S. Horwitz, T. Reps, and D. Binkley. </author> <title> Interprocedural slicing using dependence graphs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(1) </volume> <pages> 26-60, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: A more general representation of the program could be based on basic blocks instead of inner loops. A complication then arising would be the loss of an implicit iteration space for each irregular reference. To determine which references actually constitute a particular portion, one can use slicing <ref> [HRB90] </ref>. (This is in fact already part of our current implementation design, the representation of portions in the fixed format introduced in Section B.1.2 was mainly used for illustration purposes).
Reference: [HS91a] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Load balancing on message passing architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 312-324, </pages> <year> 1991. </year>
Reference: [HS91b] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Parallelizing dynamic processes on message passing architectures. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1991. </year>
Reference-contexts: They can assist in load balancing and in communication, both of which can be particularly tedious and error prone when trying to parallelize an irregular problem efficiently <ref> [HS91b] </ref>. The tools for parallelizing irregular problems can roughly be divided into two groups. The first group of tools provides an easier grip on the physical properties of the problem, i.e., it takes advantage of spatial locality.
Reference: [HS92] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Correctness and determinism of parallel Monte Carlo processes. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 121-132, </pages> <year> 1992. </year>
Reference-contexts: After identifying such a problem, one should reexamine the algorithm, which was originally formulated for sequential computers. In the WaTor case, it turns out that for enforcing the concept of finite space and avoiding overpopulation, there are alternatives to the original rollback rule <ref> [HS92] </ref>. These alternatives are not only more efficient on parallel machines by avoiding costly rollback operations, they also enable deterministic program execution even for different problem decompositions and for varying numbers of processors.
Reference: [HT84] <author> T. Hoshino and K. Takenouchi. </author> <title> Processing of the molecular dynamics model by the parallel computer PAX. </title> <journal> Computer Physics Communications, </journal> <volume> 31(4), </volume> <year> 1984. </year>
Reference: [IFKF90] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar <ref> [IFKF90] </ref>, C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [KCZ92] <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year> <month> 25 </month>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin <ref> [CBZ91, KCZ92] </ref>, Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency proto 10 col, which can be lazy or eager, based on invalidations or updates.
Reference: [KM91] <author> C. Koelbel and P. Mehrotra. </author> <title> Programming data parallel algorithms on dis-tributed memory machines using Kali. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Each dimension of the decomposi tion is distributed in a block, cyclic, or block-cyclic manner or replicated. 2.4.3 Compilation Systems for Irregular Problems Projects that have aimed at least to some degree towards compiler support for paral-lelizing irregular problems are the following. Kali Kali <ref> [KMV90, MV90, KM91] </ref> is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. Programs written for Kali must specify a virtual processor array and assign distributed arrays to BLOCK, CYCLIC, or user-specified decompositions.
Reference: [KMSB90] <author> C. Koelbel, P. Mehrotra, J. Saltz, and S. Berryman. </author> <title> Parallel loops on distributed machines. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: systems to handle data partitioning or to handle machine level optimizations, that could be provided for example by Dino or by For tran D. 2.3.2 Tools based on access patterns The inspector-executor paradigm An important concept for the tools that are based on access patterns is is the inspector-executor paradigm <ref> [KMV90, MSS + 88, KMSB90, WSBH91] </ref>, which was developed to support message blocking even in the 6 presence of indirection arrays. A loop that contains indirect accesses to a distributed array is processed in four steps: 1. <p> Communication is then generated automatically based on the ON clause and data decompositions. An inspector/executor strategy as described in Section 2.3.2 is used for run-time preprocessing of communication for irregularly distributed arrays <ref> [KMSB90] </ref>. Major differences between Kali and the Fortran D compiler include Kali's mandatory ON clauses for parallel loops and Fortran D's support for alignment, collective communication, and dynamic decomposition. ARF Arf is another compiler based on the inspector-executor paradigm.
Reference: [KMT91] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: This approach is similar to the power steering paradigm <ref> [KMT91] </ref> used for loop transformations, where the compiler cannot always pick the best transformation, but it assists the user by (conservatively) testing correctness and performing the actual rewriting work. 2.4.1 Parallel Compilation Systems There have been and still are numerous research projects in the area of compiling for parallel architectures.
Reference: [KMV90] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: systems to handle data partitioning or to handle machine level optimizations, that could be provided for example by Dino or by For tran D. 2.3.2 Tools based on access patterns The inspector-executor paradigm An important concept for the tools that are based on access patterns is is the inspector-executor paradigm <ref> [KMV90, MSS + 88, KMSB90, WSBH91] </ref>, which was developed to support message blocking even in the 6 presence of indirection arrays. A loop that contains indirect accesses to a distributed array is processed in four steps: 1. <p> Each dimension of the decomposi tion is distributed in a block, cyclic, or block-cyclic manner or replicated. 2.4.3 Compilation Systems for Irregular Problems Projects that have aimed at least to some degree towards compiler support for paral-lelizing irregular problems are the following. Kali Kali <ref> [KMV90, MV90, KM91] </ref> is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. Programs written for Kali must specify a virtual processor array and assign distributed arrays to BLOCK, CYCLIC, or user-specified decompositions.
Reference: [KRS92] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy code motion. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference: [KU76] <author> J. Kam and J. Ullman. </author> <title> Global data flow analysis and iterative algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 23(1) </volume> <pages> 159-171, </pages> <month> January </month> <year> 1976. </year>
Reference: [Kus86] <author> B. C. Kuszmaul. </author> <title> Simulating applicative architectures on the Connection Machine. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1986. </year>
Reference-contexts: This may cause additional idling when running irregular problems on SIMD machines instead of MIMD machines. The restricted control flow of pure SIMD programming has been addressed by several researchers. General simulators of MIMD semantics on SIMD machines have been implemented by Kuszmaul <ref> [Kus86] </ref> and Hudak et al. [HM88] on the Connection Machine and by Biagioni [Bia91] on the MasPar. These simulations are generally based on graph reduction interpreters for functional languages. Their performance tends to be scalable, but in absolute measures still below the speed of sequential workstations.
Reference: [LC91] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal <ref> [LC91] </ref>, Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [LH89] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash [LLG + 90], Ivy <ref> [LH89] </ref>, Midway [BZ91], Munin [CBZ91, KCZ92], Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency proto 10 col, which can be lazy or eager, based on invalidations or updates.
Reference: [LLG + 90] <author> D. Lenoski, J. Laudon, K Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds [RAK88], Dash <ref> [LLG + 90] </ref>, Ivy [LH89], Midway [BZ91], Munin [CBZ91, KCZ92], Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency proto 10 col, which can be lazy or eager, based on invalidations or updates.
Reference: [LS91] <author> S. Lucco and O. Sharp. </author> <title> Parallel programming with coordination structures. </title> <booktitle> In Conference Record of the Eighteenth ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: Early work in the field of compiling for distributed memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions [GB91, HA90, RS89]. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium <ref> [LS91] </ref> have been defined.
Reference: [Luc88] <author> B. J. Lucier. </author> <title> Performance evaluation for multiprocessors programmed using monitors. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, volume 16 of SIG-METRICS Performance Evaluation Review, </booktitle> <year> 1988. </year>
Reference-contexts: Here information about the utilization of different processors can be helpful. However, the work done in this area has focussed on thread based parallelism <ref> [ELZ86, Luc88] </ref>, typically even associated with distinct processes, instead of data parallelism. 2.6 The Hardware Some hardware facilities that can be particularly useful for irregular applications are the following. 2.6.1 Low latency Due to the typically very irregular access patterns, message blocking becomes more complicated than for regular applications [SHG92].
Reference: [LW82] <author> G. S. Lueker and D. E. Willard. </author> <title> A data structure for dynamic range queries. </title> <journal> Inf. Proc. Lett., </journal> <volume> 15 </volume> <pages> 209-213, </pages> <year> 1982. </year>
Reference-contexts: Although the concept of value-based decompositions was developed with simple arrays as underlying data structures, the thesis will also contain an analysis of how well this concept applies to the more advanced data structures used in hierarchical solvers <ref> [Ben75, FB74, LW82] </ref>. On the practical side, this involves the parallelization of a Molecular Dynamics code (Gromos).
Reference: [Mas91] <institution> MasPar Computer Corporation, Sunnyvale, CA. MasPar Fortran Reference Manual, </institution> <year> 1991. </year>
Reference: [Mav91] <author> D. Mavriplis. </author> <title> Three dimensional unstructured multigrid for the euler equations. </title> <type> Technical report, </type> <institution> Institute for Computer Application in Science and Engineering, </institution> <year> 1991. </year> <month> 26 </month>
Reference-contexts: So far, the prospective test suite includes programs covering * molecular dynamics (Gromos, see Section A.1), * unstructured meshes <ref> [Mav91] </ref>, and * sparse matrices. Beyond working with these source codes, other applications (whose source codes are not available or not written in Fortran) will be evaluated at least theoretically [HKK + 91]. 3.4 MIMD vs.
Reference: [McC87] <author> J. A. McCammon. </author> <title> Computer-aided molecular design. </title> <booktitle> Science, </booktitle> <address> 238:486--491, </address> <month> October </month> <year> 1987. </year>
Reference: [MH87] <author> J. A. McCammon and Stephen C. Harvey. </author> <title> Dynamics of proteins and nucleic acids. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference: [MPB91] <author> F. Muller-Plathe and D. Brown. </author> <title> Multi-colour algorithms in molecular simulation: Vectorisation and parallelisation of internal forces and constraints. </title> <journal> Computer Physics Communications, </journal> <volume> 64 </volume> <pages> 7-14, </pages> <year> 1991. </year>
Reference: [MSMB90] <author> S. Mirchandaney, J. Saltz, P. Mehrotra, and H. Berryman. </author> <title> A scheme for supporting automatic data migration on multicomputers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Parti is first to propose and implement user-defined irregular distributions [MSS + 88] and a hashed cache for nonlocal values <ref> [MSMB90] </ref>. They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2. Manage the storage of and access to copies of off-processor data, and 3.
Reference: [MSS + 88] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: systems to handle data partitioning or to handle machine level optimizations, that could be provided for example by Dino or by For tran D. 2.3.2 Tools based on access patterns The inspector-executor paradigm An important concept for the tools that are based on access patterns is is the inspector-executor paradigm <ref> [KMV90, MSS + 88, KMSB90, WSBH91] </ref>, which was developed to support message blocking even in the 6 presence of indirection arrays. A loop that contains indirect accesses to a distributed array is processed in four steps: 1. <p> Parti The Parti primitives (Parallel Automated Runtime Toolkit at ICASE) are a set of high level communication routines that provide convenient access to off-processor elements of arrays that are accessed (and distributed) irregularly [BS90, SBW90]. Parti is first to propose and implement user-defined irregular distributions <ref> [MSS + 88] </ref> and a hashed cache for nonlocal values [MSMB90]. They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2. Manage the storage of and access to copies of off-processor data, and 3.
Reference: [MV90] <author> P. Mehrotra and J. Van Rosendale. </author> <title> Programming distributed memory architectures using Kali. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Each dimension of the decomposi tion is distributed in a block, cyclic, or block-cyclic manner or replicated. 2.4.3 Compilation Systems for Irregular Problems Projects that have aimed at least to some degree towards compiler support for paral-lelizing irregular problems are the following. Kali Kali <ref> [KMV90, MV90, KM91] </ref> is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines. Programs written for Kali must specify a virtual processor array and assign distributed arrays to BLOCK, CYCLIC, or user-specified decompositions.
Reference: [PRV87] <author> M. L. Patrick, D. A. Reed, and R. G. Voigt. </author> <title> The impact of domain partitioning on the performance of a shared multiprocessor. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 211-217, </pages> <year> 1987. </year>
Reference-contexts: towards the thesis are summarized in Sections 3.2, 3.3, and 3.4, respectively; the Appendices A, B, and C contain more details for each of these. 3.2 Value-Based Decompositions A standard problem in compiling for distributed memory multiprocessors is to determine which processors own which elements of a data array X <ref> [CK88, Fox88, PRV87] </ref>. For simplicity of notation, we assume in the following that X is three-dimensional; the extension to lower or higher dimensions is straightforward.
Reference: [PT91a] <author> M. Philippsen and W. F. Tichy. </author> <title> Modula-2 fl and its compilation. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: These simulations are generally based on graph reduction interpreters for functional languages. Their performance tends to be scalable, but in absolute measures still below the speed of sequential workstations. Philippsen et al. introduce two variants of a FORALL statement, a synchronous version and an asynchronous one <ref> [PT91a] </ref>. The asynchronous FORALL enables multiple threads of control to coexist. This can either be emulated using stacks of MASK bits, 11 or it can be implemented directly in an MSIMD machine which contains multiple pro-gram counters.
Reference: [PT91b] <author> M. R. S. Pinches and D. J. Tildesley. </author> <title> Large scale molecular dynamics on parallel computers using the link-cell algorithm. </title> <journal> Molecular Simulation, </journal> <volume> 6 </volume> <pages> 51-87, </pages> <year> 1991. </year>
Reference: [RA90] <author> R. Ruhl and M. Annaratone. </author> <title> Parallelization of fortran code on distributed-memory parallel processors. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Nether-lands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen <ref> [RA90] </ref>, P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [RAK88] <author> U. Ramachandran, M. Ahamad, and Y. Khalidi. </author> <title> Unifying synchronization and data transfer in maintaining coherence of distributed shared memory. </title> <type> Technical Report GIT-CS-88/23, </type> <institution> Georgia Institute of Technology, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Examples of these systems are Amber [CAL + 89], Clouds <ref> [RAK88] </ref>, Dash [LLG + 90], Ivy [LH89], Midway [BZ91], Munin [CBZ91, KCZ92], Orca [BT88], and Platinum [CF89]. They preserve sequential semantics by enforcing a consistency proto 10 col, which can be lazy or eager, based on invalidations or updates.
Reference: [RCB77] <author> J. Rycaert, G. Ciccotti, and H. J. C. Berendsen. </author> <title> Numerical integration of the cartesian equations of motion of a system with constraints: Molecular dynamics of n-Alkanes. </title> <journal> Journal of Computational Physics, </journal> <volume> 23 </volume> <pages> 327-341, </pages> <year> 1977. </year>
Reference: [RP89] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau <ref> [RP89] </ref>, Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [RS87] <author> J. Rose and G. Steele, Jr. </author> <title> C fl : An extended C language for data parallel programming. </title> <editor> In L. Kartashev and S. Kartashev, editors, </editor> <booktitle> Proceedings of 27 the Second International Conference on Supercomputing, </booktitle> <address> Santa Clara, CA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C <ref> [HQL + 91, RS87] </ref>, Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [RS89] <author> J. Ramanujam and P. Sadayappan. </author> <title> A methodology for parallelizing programs for multicomputers and complex memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Early work in the field of compiling for distributed memory machines focussed on defining frameworks for nonlocal memory accesses [CK88] and data distributions <ref> [GB91, HA90, RS89] </ref>. For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined.
Reference: [RSW91] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino <ref> [RSW91] </ref>, Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [SBW90] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and runtime compilation. </title> <type> ICASE Report 90-59, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Parti The Parti primitives (Parallel Automated Runtime Toolkit at ICASE) are a set of high level communication routines that provide convenient access to off-processor elements of arrays that are accessed (and distributed) irregularly <ref> [BS90, SBW90] </ref>. Parti is first to propose and implement user-defined irregular distributions [MSS + 88] and a hashed cache for nonlocal values [MSMB90]. They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2.
Reference: [SCMB90] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: They build on the inspector-executor paradigm described above; they 1. Coordinate interprocessor data movement, 2. Manage the storage of and access to copies of off-processor data, and 3. Support a shared name space by building a distributed translation table <ref> [SCMB90] </ref> to store the local address and processor number for each distributed array ele ment. Communicating the right data at the right time and place is a difficult, yet crucial task for parallelizing irregular problems.
Reference: [SH91] <author> J. P. Singh and J. L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Citing from a study about parallelizing different applications (including a molecular dynamics simulation) using the FX/Fortran parallelizing compiler <ref> [SH91] </ref>: It is worth noting that the available directives were sometimes found to be restrictive or incapable of expressing the exact information we wished to convey to the compiler. For example, the smallest entity for which we can tell the compiler not to check for dependences is an entire loop.
Reference: [SHG92] <author> J. P. Singh, J. L. Hennessy, and A. Gupta. </author> <title> Implications of hierarchical N-body methods for multiprocessor architecture. </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: parallelism [ELZ86, Luc88], typically even associated with distinct processes, instead of data parallelism. 2.6 The Hardware Some hardware facilities that can be particularly useful for irregular applications are the following. 2.6.1 Low latency Due to the typically very irregular access patterns, message blocking becomes more complicated than for regular applications <ref> [SHG92] </ref>.
Reference: [SHT + 92] <author> J. P. Singh, C. Holt, T. Totsuka, A. Gupta, and J. L. Hennessy. </author> <title> Load balancing and data locality in hierarchical N-body methods. </title> <type> Technical Report CSL-TR-92-505, </type> <institution> Stanford University, </institution> <month> January </month> <year> 1992. </year>
Reference: [SM90] <author> T. P. Straatsma and J. Andrew McCammon. ARGOS, </author> <title> a vectorized general molecular dynamics program. </title> <journal> Journal of Computational Chemistry, </journal> <volume> II(8):943-951, </volume> <year> 1990. </year>
Reference-contexts: For example, the poor performance of many molecular dynamics codes on vector machines such as the Cray is basically a consequence of low fine-grain locality. In this case, the factors that prevent a code from vectorization are similar to those that cause frequent cache misses <ref> [SM90] </ref>. 2.3 Tools Tools try to resolve the tradeoffs between simplicity and efficiency. They can assist in load balancing and in communication, both of which can be particularly tedious and error prone when trying to parallelize an irregular problem efficiently [HS91b].
Reference: [SM91] <author> J. Shen and J. A. McCammon. </author> <title> Molecular dynamics simulation of Superox-ide interacting with Superoxide Dismutase. </title> <journal> Chemical Physics, </journal> <volume> 158 </volume> <pages> 191-198, </pages> <year> 1991. </year>
Reference: [Soc90] <author> D. Socha. </author> <title> Compiling single-point iterative programs for distributed memory computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot <ref> [SS90, Soc90] </ref>, Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [SPBR91] <author> J. Saltz, S. Petiton, H. Berryman, and A. Rifkin. </author> <title> Performance effects of irregular communication patterns on massively parallel multicomputers. </title> <type> ICASE Report 91-12, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year>
Reference: [SS90] <author> L. Snyder and D. Socha. </author> <title> An algorithm producing balanced partitionings of data arrays. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot <ref> [SS90, Soc90] </ref>, Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [SWW92] <author> R. Sawdayi, G. Wagenbreth, and J. Williamson. MIMDizer: </author> <title> Functional and data decomposition. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> Elsevier, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year> <month> 28 </month>
Reference-contexts: Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer <ref> [SWW92] </ref>, Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [Thi91] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, </note> <year> 1991. </year>
Reference: [TP90] <author> S. Tomboulian and M. Pappas. </author> <title> Indirect addressing and load balancing for faster solutions to the Mandelbrot set on SIMD architectures. </title> <booktitle> In Fron-tiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 443-450, </pages> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: Loop flattening can also be used to process multiple array segments of different lengths per processor, as introduced in Blelloch's V-RAM model [Ble90]. Thus it can be viewed as a generalization of substituting direct addressing with indirect addressing as Tomboulian and Pappas did for computing the Mandelbrot set <ref> [TP90] </ref>. 2.6.4 Fast scan operations The inhomogeneous workload across processors generally associated with irregular problems calls for load balancing. Scan operations are one efficient way for determining the total workload and its distribution [Bia91, Ble90].
Reference: [Tse90] <author> P.-S. Tseng. </author> <title> A parallelizing compiler for distributed memory parallel computers. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: For exploiting coarse-grained functional parallelism, high-level parallel languages such as Linda [CG89], Strand [FT90, FO90], and Delirium [LS91] have been defined. Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al <ref> [Tse90] </ref>, Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb [ZBG88], and Vienna Fortran [BCZ92].
Reference: [WLR90] <author> M. Willebeek-LeMair and A. P. Reeves. </author> <title> Solving nonuniform problems on SIMD computers: Case study on region growing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 135-149, </pages> <year> 1990. </year>
Reference-contexts: As observed in a case study implementing an image processing algorithm on the Massively Parallel Processor <ref> [WLR90, page 143] </ref>: "... the complexity of each iteration in the SIMD environment is dominated by the largest region in the image.
Reference: [WSBH91] <author> J. Wu, J. Saltz, H. Berryman, and S. Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <type> ICASE Report 91-13, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: systems to handle data partitioning or to handle machine level optimizations, that could be provided for example by Dino or by For tran D. 2.3.2 Tools based on access patterns The inspector-executor paradigm An important concept for the tools that are based on access patterns is is the inspector-executor paradigm <ref> [KMV90, MSS + 88, KMSB90, WSBH91] </ref>, which was developed to support message blocking even in the 6 presence of indirection arrays. A loop that contains indirect accesses to a distributed array is processed in four steps: 1.
Reference: [WSHB91] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: ARF Arf is another compiler based on the inspector-executor paradigm. Arf is designed to interface Fortran application programs with the Parti run-time routines described in Section 2.3.2 <ref> [WSHB91] </ref>. It supports BLOCK, CYCLIC, and user-defined irregular decompositions. The goal of Arf is to demonstrate that inspector/executors based on Parti primitives can be automatically generated by the compiler. 2.4.4 Communication analysis Determining communication requirements and satisfying them efficiently is critical for any parallel program.
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: Numerous compilation systems for exploiting fine-grained parallelism have been and are being built, which include Al [Tse90], Aspar [IFKF90], C*/Dataparallel C [HQL + 91, RS87], Crystal [LC91], Dino [RSW91], Id Nouveau [RP89], Mimdizer [SWW92], Oxygen [RA90], P 3 C [GAY91], Pandore [APT90], Parafrase-2 [GB92], Paragon [CCRS91], Spot [SS90, Soc90], Superb <ref> [ZBG88] </ref>, and Vienna Fortran [BCZ92].
References-found: 127

