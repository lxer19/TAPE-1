URL: http://www.pdl.cs.cmu.edu/PDL-FTP/RAID/CMU-CS-95-200.ps
Refering-URL: http://www.pdl.cs.cmu.edu/Publications/publications.html
Root-URL: 
Title: RAIDframe: Rapid prototyping for disk arrays  
Author: Garth Gibson, William V. Courtright II, Mark Holland, Jim Zelenka 
Date: 18 October 1995  
Note: Submitted to the 1996 ACM conference on measurement and modeling (SIGMETRICS-96) http://www.cs.cmu.edu:8001/Web/Groups/PDL  CMU-CS-95-200  
Address: Pittsburgh, Pennsylvania 15213-3890  Pittsburgh, Pennsylvania 15213-3890  
Affiliation: School of Computer Science Carnegie Mellon University  Department of Electrical and Computer Engineering Carnegie Mellon University  
Abstract: The complexity of advanced disk array architectures makes accurate representation necessary, arduous, and error-prone. In this paper, we present RAIDframe, an array framework that separates architectural policy from execution mechanism. RAIDframe facilitations rapid prototyping of new RAID architectures by localizing modifications and providing libraries of existing architectures to extend. In addition, RAIDframe implemented architectures run the same code as a synthetic and trace-driven simulator, as a user-level application managing raw disks, and as a Digital Unix device-driver capable of mounting a filesystem. Evaluation shows that RAIDframe performance is equivalent to less complex array implementations and thance is equivalent to less complex array implementations and that case studies of RAID levels 0, 1, 4, 5, 6, and parity declustering achieve expected performance. The project team is indebted to the generous donations of the member companies of the Parallel Data Laboratory (PDL) Consortium. At the time of this writing, these include: Data General, Digital Equipment, Hewlett-Packard, International Business Machines, Seagate, Storage Technology, and Symbios Logic. Symbios Logic additionally provided a fellowship. The Data Storage Systems Center also provided funding through a grant from the National Science Foundation under grant number ECD-8907068. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the PDL Consortium member companies or the U.S. government. 
Abstract-found: 1
Intro-found: 1
Reference: [Accetta86] <editor> M. Accetta, et. al Mach: </editor> <title> A New Kernel Foundation for Unix Development, </title> <booktitle> Proceedings of the Summer 1986 USENIX Workshop, p. </booktitle> <pages> 93-113, </pages> <year> 1986. </year>
Reference-contexts: The in-kernel implementation additionally allows direct Page 5 of 19 benchmarking of real applications (up to the CPU limitations imposed by RAIDframes software computation of parity). While the out-of-kernel RAIDframe execution environment is a performance compromise in the spirit of microkernel operating system design <ref> [Accetta86] </ref>, experiments run at user-level on attached but unmounted disks execute with little or no dependence on the local operating system. User level execution also offers accurate modeling, by direct execution, of the interactions between application threads and system scheduling [Ganger93].
Reference: [ATC90] <author> Product Description, </author> <title> RAID+ Series Model RX, Array Technology Corporation, </title> <address> Boulder, CO, </address> <year> 1990. </year>
Reference-contexts: These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration <ref> [ATC90, Blaum94, STC94] </ref>, performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94]. Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $5 billion this year and to surpass $13 billion in 1998 [DISK/TREND94]. <p> Next, we present RAID level 1, the most studied redundant array implementation, also known as disk mirroring [Bitton88, Gray90]. Next, we present declus-tered parity, a variant of RAID level 5 that reduces the on-line failure recovery performance penalties and RAID level 6 <ref> [ATC90, STC94] </ref>, a double failure tolerant variant of RAID level 5.
Reference: [Bitton88] <author> D. Bitton and J. Gray, </author> <title> Disk Shadowing, </title> <booktitle> Proceedings of the 14th Conference on Very Large Data Bases, </booktitle> <year> 1988, </year> <pages> pp. 331-338. </pages>
Reference-contexts: Case Studies in Extensibility In this section we present six array architectures implemented in RAIDframe. We begin with three of the well-known RAID levels [Patterson88]: RAID levels 0, 4, and 5. Next, we present RAID level 1, the most studied redundant array implementation, also known as disk mirroring <ref> [Bitton88, Gray90] </ref>. Next, we present declus-tered parity, a variant of RAID level 5 that reduces the on-line failure recovery performance penalties and RAID level 6 [ATC90, STC94], a double failure tolerant variant of RAID level 5. <p> In the latter case (parity failed instead of data), the entire parity computation is suppressed, and the write proceeds as if the array were non-redundant. 3.2 RAID Level 1 The most implemented, most studied, and most optimized redundant disk array architecture is mirroring, also known as RAID level 1 <ref> [Solworth91, Bitton88, Polyzois93, Gray90] </ref>. Basically, two copies of every data unit are kept on two different disks. This architecture doubles storage costs, offers reads a choice of copies, and simplifies updating redundant data and recovering from failures. <p> Specifically, based on the simple performance model given by Patterson, Gibson and Katz [Patterson88], we expect small, fault-free read accesses to achieve the same performance in all architectures except RAID level 1, whose shortest queue discipline should improve throughput and decrease response time <ref> [Chen90, Bitton88] </ref>. For fault-free small writes, we expect minimum average response time for the parity-based architectures to be about twice that of the direct write architectures (RAID levels 0 and 1).
Reference: [Blaum94] <author> M. Blaum, J. Brady, J. Bruck, and J. Menon, Evenodd: </author> <title> An Optimal Scheme for Tolerating Double Disk Failures in RAID Architectures, </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1994, </year> <pages> pp. 245-254. </pages>
Reference-contexts: These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration <ref> [ATC90, Blaum94, STC94] </ref>, performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94]. Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $5 billion this year and to surpass $13 billion in 1998 [DISK/TREND94]. <p> If two disks are simultaneously failed in one array, they make no guarantee that data will not be irretrievably lost. In very large arrays, or in arrays requiring very high reliability, multiple concurrent failures may need to be tolerated <ref> [Blaum94, Burkhard93] </ref>. The most common multiple-failure tolerating array organization, RAID level 6, also known as P+Q, uses two redundant units per stripe, the P unit and the Q unit, to store a double-erasure-correcting Reed-Solomon code computed over the data portion of the stripe.
Reference: [Brown72] <author> D. Brown, R. Gibson, and C. Thorn, </author> <title> Channel and Direct Access Device Architecture, </title> <journal> IBM Systems Journal, </journal> <volume> 11(3), </volume> <pages> pp. 186-199, </pages> <year> 1972. </year>
Reference-contexts: The development of RAIDframes access sequencing abstraction owes much to prior storage systems. A powerful approach for localizing access-specific control is suggested by IBMs channel command words and channel programs <ref> [Brown72] </ref>. Although this model serializes accesses, its success indicates that the range of primitives needed to support storage control is small and may directly correspond to operations provided by lower level devices. Similar abstractions are found in SCSI chip control scripts [NCR91] and network-RAID node-to-node data transfers [Cabrera91, Long94].
Reference: [Burkhard93] <author> W. Burkhard and J. Menon, </author> <title> Disk Array Storage System Reliability, </title> <booktitle> Proceedings of the International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1993, </year> <pages> pp. 432-441. </pages>
Reference-contexts: If two disks are simultaneously failed in one array, they make no guarantee that data will not be irretrievably lost. In very large arrays, or in arrays requiring very high reliability, multiple concurrent failures may need to be tolerated <ref> [Blaum94, Burkhard93] </ref>. The most common multiple-failure tolerating array organization, RAID level 6, also known as P+Q, uses two redundant units per stripe, the P unit and the Q unit, to store a double-erasure-correcting Reed-Solomon code computed over the data portion of the stripe.
Reference: [Cabrera91] <author> L.-F. Cabrera and D. Long, Swift: </author> <title> Using Distributed Disk Striping to Provide High I/O Data Rates, </title> <journal> Computing Systems, </journal> <volume> vol. 4 no. 4, </volume> <year> 1991, </year> <pages> pp. 405-439. </pages>
Reference-contexts: These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID <ref> [Cabrera91, Hartman93, Long94] </ref>. Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $5 billion this year and to surpass $13 billion in 1998 [DISK/TREND94]. In this paper, we are concerned with the process of developing and evaluating a new array architecture. <p> Although this model serializes accesses, its success indicates that the range of primitives needed to support storage control is small and may directly correspond to operations provided by lower level devices. Similar abstractions are found in SCSI chip control scripts [NCR91] and network-RAID node-to-node data transfers <ref> [Cabrera91, Long94] </ref>. A more exible example of a storage control abstraction specifically developed for RAID architectures is the parallel state table approach in TickerTAIP, a distributed implementation of RAID level 5 [Cao93].
Reference: [Cao93] <author> P. Cao, S.B. Lim, S. Venkataraman, and J. Wilkes, </author> <title> The TickerTAIP parallel RAID architecture, </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 52-63. </pages>
Reference-contexts: Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed. These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization <ref> [Cao93, Drapeau94, Menon93] </ref>, multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94]. <p> Similar abstractions are found in SCSI chip control scripts [NCR91] and network-RAID node-to-node data transfers [Cabrera91, Long94]. A more exible example of a storage control abstraction specifically developed for RAID architectures is the parallel state table approach in TickerTAIP, a distributed implementation of RAID level 5 <ref> [Cao93] </ref>. RAIDframe expands on the example of TickerTAIP, simplify the expression of potentially concurrent orderings of operations, by using directed acyclic graphs (DAGs) of primitive actions. We believe that a graphical abstraction has the added benefit of compactly and visually conveying the essential ordering aspects of a RAID architecture.
Reference: [Chen90] <author> P. Chen, et. al., </author> <title> An Evaluation of Redundant Arrays of Disks using an Amdahl 5890, </title> <booktitle> Proceedings of the Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1990, </year> <pages> pp. 74-85. </pages>
Reference-contexts: Specifically, based on the simple performance model given by Patterson, Gibson and Katz [Patterson88], we expect small, fault-free read accesses to achieve the same performance in all architectures except RAID level 1, whose shortest queue discipline should improve throughput and decrease response time <ref> [Chen90, Bitton88] </ref>. For fault-free small writes, we expect minimum average response time for the parity-based architectures to be about twice that of the direct write architectures (RAID levels 0 and 1).
Reference: [Chen90b] <author> P. Chen and D. Patterson, </author> <title> Maximizing Performance in a Striped Disk Array, </title> <booktitle> Proceedings of International Symposium on Computer Architecture, </booktitle> <year> 1990, </year> <pages> pp. 322-331. </pages>
Reference-contexts: In general, ideas for new architectures are derived using back-of-the-envelope analytical models and evaluated by detailed simulation experiments. To maximize evaluation accuracy, simulators are sometimes built with sufficient detail to manipulate actual dataoften by beginning with code from a running storage system <ref> [Chen90b, Lee91, Kistler92] </ref>. In our experience, the specification and implementation of a detailed array simulator, even if it does not closely model a running system, is an arduous task. <p> We began with RaidSim, a 92 file, 13,886 line detailed array simulator derived at Berkeley from an implementation of a functional device driver in the Sprite operating system <ref> [Chen90b, Lee91] </ref>. To this simulator we added new array functions, replaced previously unessential simple mechanisms, and augmented statistics recording, workload generation and debugging functions.
Reference: [Courtright94] <author> William V. Courtright II and Garth A. Gibson, </author> <title> Backward error recovery in redundant disk arrays. </title> <booktitle> Proceedings of the 1994 Computer Measurement Group (CMG) Conference, </booktitle> <volume> Vol. 1, </volume> <month> December 4-9, </month> <year> 1994, </year> <pages> pp 63-74. </pages> <note> Page 18 of 19 </note>
Reference-contexts: While error handling in RAIDframe is not currently independent of architecture, we are extending this error handling modelquiesce, change state, and retryinto a mechanized error recovery system, capable of providing recovery for n-fault-tolerant arrays <ref> [Courtright94] </ref>. The policies implementing a particular array architecture are available to RAIDframes infrastructure as a set of modules are: mapping library: This library contains the layout routines which determine the placement of data and redundancy information in the array.
Reference: [DISK/TREND94] <author> DISK/TREND, Inc. </author> <year> 1994. </year> <title> 1994 DISK/TREND Report: Disk Drive Arrays. 1925 Landings Drive, </title> <institution> Mountain View, Calif., SUM-3. </institution>
Reference-contexts: Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $5 billion this year and to surpass $13 billion in 1998 <ref> [DISK/TREND94] </ref>. In this paper, we are concerned with the process of developing and evaluating a new array architecture. In general, ideas for new architectures are derived using back-of-the-envelope analytical models and evaluated by detailed simulation experiments.
Reference: [Drapeau94] <author> A.Drapeau, K.Shirriff, J. Hartman, E. Miller, S. Seshan, R. Katz, D. Patterson, E. Lee, P. Chen, and G. Gibson, </author> <title> RAID-II: a High-Bandwidth Network File Server, </title> <booktitle> Proceedings the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 234-44, </pages> <year> 1994. </year>
Reference-contexts: Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed. These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization <ref> [Cao93, Drapeau94, Menon93] </ref>, multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94].
Reference: [English92] <author> Robert M. English and Alexander A. Stepanov. Loge: </author> <title> a self-organizing storage device. </title> <booktitle> In Proceedings of the 1992 Winter Usenix Technical Conference, </booktitle> <pages> pages 237251, </pages> <month> January </month> <year> 1992. </year>
Reference: [Ganger93] <author> G. Ganger, and Y. Patt, </author> <title> The Process-Flow Model: Examining I/O Performance from the Systems Point of View, </title> <booktitle> Proceedings of the ACM Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 86-97, </pages> <year> 1993. </year>
Reference-contexts: User level execution also offers accurate modeling, by direct execution, of the interactions between application threads and system scheduling <ref> [Ganger93] </ref>. Relative to an in-kernel implementation, however, user-level RAIDframe has decreased responsiveness to asynchronous events like disk completion and added messaging, system call and context switching overheads.
Reference: [Gibson93] <author> G. Gibson and D. Patterson, </author> <title> Designing Disk Arrays for High Data Reliability, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 17, </volume> <year> 1993, </year> <pages> pp. 4-27. </pages>
Reference-contexts: 1. Introduction Disk arrays are an effective method for increasing I/O system performance [Salem86, Kim86]. By incorporating redundancy into arrays, systems are able to survive disk faults without loss of data or interruption of service <ref> [Lawlor81, Patterson88, Gibson93] </ref>. Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed. <p> In our evaluations, we use a single parity group consisting of fifteen disks on five busses, neglecting the dependent failure mode that would occur should a SCSI bus controller fail <ref> [Gibson93] </ref>. The DAG templates above use the same nomenclature as Figure 2. A Q node computes Reed-Solomon check symbols identified by subscript q.
Reference: [Gray90] <author> G. Gray, B. Horst, and M. Walker, </author> <title> Parity Striping of Disc Arrays: Low-Cost Reliable Storage with Acceptable Throughput, </title> <booktitle> Proceedings of the Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. 148-160. </pages>
Reference-contexts: Case Studies in Extensibility In this section we present six array architectures implemented in RAIDframe. We begin with three of the well-known RAID levels [Patterson88]: RAID levels 0, 4, and 5. Next, we present RAID level 1, the most studied redundant array implementation, also known as disk mirroring <ref> [Bitton88, Gray90] </ref>. Next, we present declus-tered parity, a variant of RAID level 5 that reduces the on-line failure recovery performance penalties and RAID level 6 [ATC90, STC94], a double failure tolerant variant of RAID level 5. <p> In the latter case (parity failed instead of data), the entire parity computation is suppressed, and the write proceeds as if the array were non-redundant. 3.2 RAID Level 1 The most implemented, most studied, and most optimized redundant disk array architecture is mirroring, also known as RAID level 1 <ref> [Solworth91, Bitton88, Polyzois93, Gray90] </ref>. Basically, two copies of every data unit are kept on two different disks. This architecture doubles storage costs, offers reads a choice of copies, and simplifies updating redundant data and recovering from failures.
Reference: [Hartman93] <author> J. Hartman and J. Ousterhout, </author> <title> The Zebra Striped Network File System, </title> <booktitle> Proceedings of the Symposium on Operating System Principles, </booktitle> <year> 1993. </year>
Reference-contexts: These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID <ref> [Cabrera91, Hartman93, Long94] </ref>. Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $5 billion this year and to surpass $13 billion in 1998 [DISK/TREND94]. In this paper, we are concerned with the process of developing and evaluating a new array architecture.
Reference: [Holland92] <author> M. Holland and G. Gibson, </author> <title> Parity Declustering for Continuous Operation in Redundant Disk Arrays, </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992, </year> <pages> pp. 23-25. </pages>
Reference-contexts: These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure <ref> [Holland92, Muntz90] </ref>, and network-based RAID [Cabrera91, Hartman93, Long94]. Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $5 billion this year and to surpass $13 billion in 1998 [DISK/TREND94]. <p> This can greatly complicate the task of modifying the simulator to model another, significantly different, array architecture. As an example of the complexity involved in modifying a simulator, we examined simulator code changes we made in the development of our first new array architecture <ref> [Holland92] </ref>. We began with RaidSim, a 92 file, 13,886 line detailed array simulator derived at Berkeley from an implementation of a functional device driver in the Sprite operating system [Chen90b, Lee91]. <p> Illustrated in Figure 4, their idea has since been extended by mechanisms evaluated by analytical modeling and event-driven simulation <ref> [Holland92, Merchant92, Schwabe94, Ng92b] </ref>. These studies suggest that parity declustering should be able to improve user performance during on-line recovery and to dramatically reduce the duration of on-line reconstruction. <p> In our RAIDframe description of parity declustering, the request DAGs are the same as those used in RAID level 5, as are all other modules except the mapping module. Changes in the mapping module depend on the implementation technique; we have used the internet-published block designs approach <ref> [Holland92] </ref>. Page 10 of 19 3.4 RAID Level 6 RAID levels 4 and 5 and parity declustering are tolerant of a single disk failure. If two disks are simultaneously failed in one array, they make no guarantee that data will not be irretrievably lost.
Reference: [Holland94b] <author> M. Holland, G. Gibson, D. Siewiorek, </author> <title> Architectures and Algorithms for On-line Failure Recovery in Redundant Disk Arrays, </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> vol 2, </volume> <pages> pp. 295-335, </pages> <year> 1994. </year>
Reference-contexts: For a fixed user workload, these additional accesses cause the I/O rate observed at each surviving disk to be increased by approximately 60-80% <ref> [Holland94b] </ref>. The severity of the resulting performance degradation draws into question the use of such arrays in many highly available applications. <p> Table 1lists the distinct DAG templates for RAID level 6 with a guide to when each is used. 3.5 Reconstruction Reconstruction of lost data is implemented in RAIDframe using a disk-oriented algorithm <ref> [Holland94b] </ref>. A single reconstruction thread is logically located in parallel with the RAID execution engine. When invoked, this reconstruction thread issues, through the locking and DAG layers, a low-priority read request for the next unit on each disk required for reconstruction.
Reference: [Kim86] <author> M. Kim, </author> <title> Synchronized Disk Interleaving, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 35 no. 11, </volume> <year> 1986, </year> <pages> pp. 978-988. </pages>
Reference-contexts: 1. Introduction Disk arrays are an effective method for increasing I/O system performance <ref> [Salem86, Kim86] </ref>. By incorporating redundancy into arrays, systems are able to survive disk faults without loss of data or interruption of service [Lawlor81, Patterson88, Gibson93].
Reference: [Kistler92] <author> J. Kistler and M. Satyanarayanan, </author> <title> Disconnected Operation in the Coda File System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 10 no. 1, </volume> <year> 1992, </year> <pages> pp. 3-25. </pages>
Reference-contexts: In general, ideas for new architectures are derived using back-of-the-envelope analytical models and evaluated by detailed simulation experiments. To maximize evaluation accuracy, simulators are sometimes built with sufficient detail to manipulate actual dataoften by beginning with code from a running storage system <ref> [Chen90b, Lee91, Kistler92] </ref>. In our experience, the specification and implementation of a detailed array simulator, even if it does not closely model a running system, is an arduous task.
Reference: [Lawlor81] <author> F. D. Lawlor, </author> <title> Efficient mass storage parity recovery mechanism, </title> <journal> IBM Technical Disclosure Bulletin </journal>
Reference-contexts: 1. Introduction Disk arrays are an effective method for increasing I/O system performance [Salem86, Kim86]. By incorporating redundancy into arrays, systems are able to survive disk faults without loss of data or interruption of service <ref> [Lawlor81, Patterson88, Gibson93] </ref>. Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed.
Reference: [Lee90] <author> E. Lee, </author> <title> Software and Performance Issues in the Implementation of a RAID Prototype, </title> <institution> University of California, </institution> <type> Technical Report UCB/CSD 90/573, </type> <year> 1990. </year>
Reference: [Lee91] <author> E. Lee and R. Katz, </author> <title> Performance Consequences of Parity Placement in Disk Arrays, </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991, </year> <pages> pp. 190-199. </pages>
Reference-contexts: In general, ideas for new architectures are derived using back-of-the-envelope analytical models and evaluated by detailed simulation experiments. To maximize evaluation accuracy, simulators are sometimes built with sufficient detail to manipulate actual dataoften by beginning with code from a running storage system <ref> [Chen90b, Lee91, Kistler92] </ref>. In our experience, the specification and implementation of a detailed array simulator, even if it does not closely model a running system, is an arduous task. <p> We began with RaidSim, a 92 file, 13,886 line detailed array simulator derived at Berkeley from an implementation of a functional device driver in the Sprite operating system <ref> [Chen90b, Lee91] </ref>. To this simulator we added new array functions, replaced previously unessential simple mechanisms, and augmented statistics recording, workload generation and debugging functions. <p> RAID level 5 provides the same fault-tolerance but rotates parity units over all disks to distribute the parity update workload. While many data and parity layouts are possible <ref> [Lee91] </ref>, the layout shown, called left symmetric, allocates data units to disks round-robin over all disks, interposing parity units between data units according to the parity rotation. User requests to a fault-free RAID level 0 are translated directly into a DAG of one access on each disk.
Reference: [Long94] <author> D. Long, B. Montague, and L.-F. Cabrera, Swift/RAID: </author> <title> A Distributed RAID System, </title> <journal> Computing Systems, </journal> <volume> 3(7), </volume> <pages> pp. 333-359, </pages> <year> 1994 </year>
Reference-contexts: These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID <ref> [Cabrera91, Hartman93, Long94] </ref>. Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $5 billion this year and to surpass $13 billion in 1998 [DISK/TREND94]. In this paper, we are concerned with the process of developing and evaluating a new array architecture. <p> Although this model serializes accesses, its success indicates that the range of primitives needed to support storage control is small and may directly correspond to operations provided by lower level devices. Similar abstractions are found in SCSI chip control scripts [NCR91] and network-RAID node-to-node data transfers <ref> [Cabrera91, Long94] </ref>. A more exible example of a storage control abstraction specifically developed for RAID architectures is the parallel state table approach in TickerTAIP, a distributed implementation of RAID level 5 [Cao93].
Reference: [Menon92a] <author> J. Menon and J. Kasson, </author> <title> Methods for Improved Update Performance of Disk Arrays, </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <year> 1992, </year> <pages> pp. 74-83. </pages>
Reference-contexts: Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed. These include designs for emphasizing improved write performance <ref> [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94] </ref>, array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94].
Reference: [Menon93] <author> J. Menon and J. Cortney, </author> <title> The Architecture of a Fault-Tolerant Cached RAID Controller, </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 76-86. </pages> <note> [Merchant92]A. </note> <author> Merchant and P. Yu, </author> <title> Design and Modeling of Clustered RAID, </title> <booktitle> Proceedings of the International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1992, </year> <pages> pp. 140-149. </pages>
Reference-contexts: Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed. These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization <ref> [Cao93, Drapeau94, Menon93] </ref>, multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94].
Reference: [Mogi94] <author> K. Mogi and M. Kitsuregawa, </author> <title> Dynamic Parity Stripe Reorganizations for RAID5 Disk Arrays, </title> <booktitle> Proceedings of the Third International Conference on Parallel and Distributed Information Systems, </booktitle> <year> 1994, </year> <pages> pp. </pages> <note> 17-26 Page 19 of 19 </note>
Reference-contexts: Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed. These include designs for emphasizing improved write performance <ref> [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94] </ref>, array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94].
Reference: [Muntz90] <author> R. Muntz and J. Lui, </author> <title> Performance Analysis of Disk Arrays Under Failure, </title> <booktitle> Proceedings of the Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. 162-173. </pages>
Reference-contexts: These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure <ref> [Holland92, Muntz90] </ref>, and network-based RAID [Cabrera91, Hartman93, Long94]. Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $5 billion this year and to surpass $13 billion in 1998 [DISK/TREND94]. <p> The severity of the resulting performance degradation draws into question the use of such arrays in many highly available applications. To address this on-line failure recovery problem, Muntz and Lui <ref> [Muntz90] </ref> proposed an extension to RAID level 5 called parity declustering which decouples the number of units in a parity stripe from the number of disks in the array, and distributes the contents of each parity stripe.
Reference: [NCR91] <institution> NCR 53C720 SCSI I/O Processor Programmers Guide, NCR Corp., Dayton OH, </institution> <year> 1991. </year>
Reference-contexts: Although this model serializes accesses, its success indicates that the range of primitives needed to support storage control is small and may directly correspond to operations provided by lower level devices. Similar abstractions are found in SCSI chip control scripts <ref> [NCR91] </ref> and network-RAID node-to-node data transfers [Cabrera91, Long94]. A more exible example of a storage control abstraction specifically developed for RAID architectures is the parallel state table approach in TickerTAIP, a distributed implementation of RAID level 5 [Cao93].
Reference: [Ng92b] <author> S. Ng and R. Mattson, </author> <title> Uniform Parity Group Distribution in Disk Arrays, </title> <institution> IBM Research Division Computer Science Research Report RJ 8835 (79217), </institution> <year> 1992. </year>
Reference-contexts: Illustrated in Figure 4, their idea has since been extended by mechanisms evaluated by analytical modeling and event-driven simulation <ref> [Holland92, Merchant92, Schwabe94, Ng92b] </ref>. These studies suggest that parity declustering should be able to improve user performance during on-line recovery and to dramatically reduce the duration of on-line reconstruction.
Reference: [Patterson88] <author> D. Patterson, G. Gibson, and R. Katz, </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the ACM Conference on Management of Data, </booktitle> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: 1. Introduction Disk arrays are an effective method for increasing I/O system performance [Salem86, Kim86]. By incorporating redundancy into arrays, systems are able to survive disk faults without loss of data or interruption of service <ref> [Lawlor81, Patterson88, Gibson93] </ref>. Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed. <p> Case Studies in Extensibility In this section we present six array architectures implemented in RAIDframe. We begin with three of the well-known RAID levels <ref> [Patterson88] </ref>: RAID levels 0, 4, and 5. Next, we present RAID level 1, the most studied redundant array implementation, also known as disk mirroring [Bitton88, Gray90]. <p> Since most of these case study architectures are well known, so is their relative performance. Specifically, based on the simple performance model given by Patterson, Gibson and Katz <ref> [Patterson88] </ref>, we expect small, fault-free read accesses to achieve the same performance in all architectures except RAID level 1, whose shortest queue discipline should improve throughput and decrease response time [Chen90, Bitton88]. <p> Moreover, we expect the ratio of the maximum throughputs achieved in a workload of 100% small, fault-free writes in a 10 disk array to be 1:10/6:10/4:10/2:10 corresponding to RAID level 4, RAID level 6, RAID level 5 (and declustered parity), RAID level 1, and RAID level 0, respectively <ref> [Patterson88] </ref>. bear out these expectations in all three evaluation environments. Moreover, because there is almost no difference in the measurements between in-kernel and user-level RAIDframe performance, array researchers unable or unwilling to port RAIDframes in-kernel implementation to their operating system can be confident of the validity of user-level performance results.
Reference: [Patterson95] <author> R. Hugo Patterson, Garth A. Gibson, Eka Ginting, Daniel Stodolsky, Jim Zelenka, </author> <title> Informed Prefetching and Caching, </title> <booktitle> to appear in Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: To evaluate the overhead involved in RAIDframe, we compare its in-kernel RAID level 0 implementation to a simpler pseudo-device disk striper. This striper was developed independently, based on code from 4.4BSD-Lite, to provide high performance access for a file system research project <ref> [Patterson95] </ref>. Architecture Lines of Code New Files % Code Reuse RAID level 0 34,311 122 parity declustering 2,416 7 93.4 RAID level 5 360 3 99.1 RAID level 4 139 2 99.6 RAID level 6 3,632 7 91.1 RAID level 1 408 2 99.0 Table 2.
Reference: [Polyzois93] <author> C. Polyzois, A. Bhide, and D. Dias, </author> <title> Disk Mirroring with Alternating Deferred Updates, </title> <booktitle> Proceedings of the Conference on Very Large Data Bases, </booktitle> <year> 1993, </year> <pages> pp. 604-617. </pages>
Reference-contexts: Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed. These include designs for emphasizing improved write performance <ref> [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94] </ref>, array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94]. <p> In the latter case (parity failed instead of data), the entire parity computation is suppressed, and the write proceeds as if the array were non-redundant. 3.2 RAID Level 1 The most implemented, most studied, and most optimized redundant disk array architecture is mirroring, also known as RAID level 1 <ref> [Solworth91, Bitton88, Polyzois93, Gray90] </ref>. Basically, two copies of every data unit are kept on two different disks. This architecture doubles storage costs, offers reads a choice of copies, and simplifies updating redundant data and recovering from failures.
Reference: [Ruemmler94] <author> C. Ruemmler and J. Wilkes, </author> <title> An Introduction to Disk Drive Modeling, </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <year> 1994 </year>
Reference: [Schwabe94] <author> E. Schwabe and I. Sutherland, </author> <title> Improved Parity-Declustered Layouts for Disk Arrays, </title> <booktitle> draft submission to the Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1994. </year>
Reference-contexts: Illustrated in Figure 4, their idea has since been extended by mechanisms evaluated by analytical modeling and event-driven simulation <ref> [Holland92, Merchant92, Schwabe94, Ng92b] </ref>. These studies suggest that parity declustering should be able to improve user performance during on-line recovery and to dramatically reduce the duration of on-line reconstruction.
Reference: [Solworth91] <author> J. Solworth and C. Orji, </author> <title> Distorted Mirrors, </title> <booktitle> Proceedings of the International Conference on Parallel and Distributed Information Systems, </booktitle> <year> 1991, </year> <pages> pp. 10-17. </pages>
Reference-contexts: Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed. These include designs for emphasizing improved write performance <ref> [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94] </ref>, array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94]. <p> In the latter case (parity failed instead of data), the entire parity computation is suppressed, and the write proceeds as if the array were non-redundant. 3.2 RAID Level 1 The most implemented, most studied, and most optimized redundant disk array architecture is mirroring, also known as RAID level 1 <ref> [Solworth91, Bitton88, Polyzois93, Gray90] </ref>. Basically, two copies of every data unit are kept on two different disks. This architecture doubles storage costs, offers reads a choice of copies, and simplifies updating redundant data and recovering from failures.
Reference: [STC94] <author> Storage Technology Corporation, </author> <title> Iceberg 9200 Storage System: Introduction, STK Part Number 307406101, Storage Technology Corporation, </title> <type> Corporate Technical Publications, </type> <address> 2270 South 88th Street, Lou-isville, CO 80028 </address>
Reference-contexts: These include designs for emphasizing improved write performance [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94], array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration <ref> [ATC90, Blaum94, STC94] </ref>, performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94]. Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $5 billion this year and to surpass $13 billion in 1998 [DISK/TREND94]. <p> Next, we present RAID level 1, the most studied redundant array implementation, also known as disk mirroring [Bitton88, Gray90]. Next, we present declus-tered parity, a variant of RAID level 5 that reduces the on-line failure recovery performance penalties and RAID level 6 <ref> [ATC90, STC94] </ref>, a double failure tolerant variant of RAID level 5.
Reference: [Stodolsky94] <author> Daniel Stodolsky, Mark Holland, William V. Courtright II, and Garth Gibson, </author> <title> Parity-Logging Disk Arrays, </title> <journal> Transactions on Computer Systems, </journal> <volume> 12(3), </volume> <month> August, </month> <year> 1994, </year> <pages> pp. 206-235. </pages>
Reference-contexts: Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, reliability, availability, capacity, and cost, a significant number of redundant disk architectures have been proposed. These include designs for emphasizing improved write performance <ref> [Menon92a, Mogi94, Polyzois93, Solworth91, Stodolsky94] </ref>, array controller design and organization [Cao93, Drapeau94, Menon93], multiple failure toleration [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Cabrera91, Hartman93, Long94].
References-found: 40

