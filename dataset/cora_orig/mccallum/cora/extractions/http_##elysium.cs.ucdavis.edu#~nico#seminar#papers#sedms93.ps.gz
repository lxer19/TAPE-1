URL: http://elysium.cs.ucdavis.edu/~nico/seminar/papers/sedms93.ps.gz
Refering-URL: http://elysium.cs.ucdavis.edu/~nico/seminar/seminar.html
Root-URL: http://www.cs.ucdavis.edu
Title: Panda: A Portable Platform to Support Parallel Programming Languages  
Author: Raoul Bhoedjang Tim Ruhl Rutger Hofman Koen Langendoen Henri Bal Frans Kaashoek 
Date: June 21, 1993  
Address: Cambridge MA  
Affiliation: Vrije Universiteit Amsterdam Department of Mathematics and Computer Science  MIT Laboratory for Computer Science,  
Abstract: Current parallel programming languages require advanced run-time support to implement communication and data consistency. As such run-time systems are usually layered on top of a specific operating system, they are nonportable. This paper reports on our early experiences with Panda, a portable virtual machine that provides general and flexible support for implementing run-time systems for parallel programming languages. Panda has two interfaces: a Panda interface providing threads, RPC, and totally-ordered group communication, and a system interface which encapsulates machine dependencies by providing machine-independent thread and communication abstractions. We describe the interfaces, our experience with an initial Unix 1 implementation, and the development of a new, portable, and scalable run-time system for the Orca parallel programming language on top of Panda.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, G. D'Souza, K. Johnson, D. Kranz, J. Kubia-towics, K. Hurihara, B-H. Lim, G. Maa, D. Nussbaum, M. Parkin, and D. Yeung. </author> <title> The MIT Alewife Machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report MIT/LCS TM-454, </type> <institution> MIT, </institution> <year> 1991. </year>
Reference-contexts: Implementing the system layer on such systems is easier. We have constructed an initial implementation of Panda for a collection of SPARC-based workstations, running Unix, and connected by a 10 Mbit/s Ethernet. We intend to port Panda in the near future to a T9000-based parallel machine, the Alewife <ref> [1] </ref>, and the CM-5 [25]. Section 2 describes the Panda and system interface in more detail. The machine-independent implementation of the Panda interface is outlined in Section 3. In Section 4, we describe our experience with an initial implementation of the system interface on Unix.
Reference: [2] <author> T.E. Anderson, B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <booktitle> In Proc. of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 95-109. </pages> <publisher> ACM, </publisher> <year> 1991. </year> <month> 14 </month>
Reference-contexts: Pthreads provides all the functionality we need, including priority scheduling, and runs entirely in user-space. User space threads are more efficient than (pure) kernel-based implementations, because thread context switches do not involve trapping to the kernel. However, they suffer from poor integration with virtual memory management and blocking I/O <ref> [2] </ref>. Virtual memory makes performance less predictable: a page fault will block all threads while the missing page is brought in from the disk. Blocking network I/O is a more serious problem. The thread that waits for incoming messages should not block all other threads contained in the same process.
Reference: [3] <author> H.E. Bal and M.F. Kaashoek. </author> <title> Object Distribution in Orca using Compile--time and Run-time Techniques. </title> <booktitle> In Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <address> Washington D.C., </address> <note> 26 September-1 October 1993. To be published. </note>
Reference-contexts: Orca programs consist of processes that communicate solely through shared objects, which are instances of abstract data types. To speed up read accesses to shared objects, such objects may be replicated. The replication strategy is based on a combination of compile-time and run-time techniques <ref> [3] </ref>. The Orca run-time system (RTS) is responsible for keeping replicas in a consistent state. Orca is being re-implemented to obtain a programming system that is portable, efficient, and scalable.
Reference: [4] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A Language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Our experience with the parallel fl This research is supported in part by a PIONIER grant from the Netherlands Organisation for Scientific Research (N.W.O.). 1 Unix is a trademark of Unix Systems Laboratories, Inc. 1 programming language Orca <ref> [4] </ref> on the Amoeba [19, 24] distributed operating system is that this strategy results in a language implementation that is difficult to port to other operating systems. Orca is based on shared objects.
Reference: [5] <author> B. Bershad, M. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway Distributed Shared Memory System. In Computer Conference, </booktitle> <year> 1993. </year>
Reference-contexts: Work on extending Orca and Panda with performance debugging tools is in progress. Unlike PVM and p4, Panda does not support heterogeneity. DSM systems like Munin [9] and Midway <ref> [5] </ref> support parallel programming by providing a shared memory abstraction that hides all message passing from the programmer. Although Panda does not provide such an abstraction by itself, it gives sufficient support to layer a shared memory model on top of it. <p> Although Midway does not rely on MMU manipulation to enforce entry consistency, it does need the MMU to implement stronger memory consistency models (release consistency and processor consistency) <ref> [5] </ref>. Munin and Midway support parallel programming by providing a shared memory abstraction and weak consistency models. We consider this support too low-level for application programming: programmers should not have to annotate their variables or use low-level locking.
Reference: [6] <author> K. P. Birman and T.A. Joseph. </author> <title> Exploiting Virtual Synchrony in Distributed Systems. </title> <booktitle> In Proc. of the 11th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 123-138, </pages> <year> 1987. </year>
Reference-contexts: A data unit is an abstraction of a typed region of memory that can be named, moved, and shared across multiple nodes in a distributed environment. Language-specific objects can be mapped onto ARCADE's data units. Like Orca, ISIS <ref> [6] </ref> is currently being reimplemented for reasons of portability and scalability. The new ISIS system, HORUS [26], has a core interface that provides reliable, causal multicasting (CBCAST). Other services are implemented on top of this interface.
Reference: [7] <author> A.D. Birrell and B.J. Nelson. </author> <title> Implementing Remote Procedure Calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: It provides the following, general abstractions: * threads, * Remote Procedure Call (RPC), * totally-ordered group communication. Experience with similar Amoeba abstractions has shown that efficient implementations of shared objects can be built on top of them [23]. Threads provide a simple, lightweight unit of activity. RPC <ref> [7] </ref> is a general mechanism for high-level point-to-point communication between nodes (and thus for the implementation of remote object invocation). Totally-ordered group communication [16] has been successfully employed in previous Orca run-time systems for keeping replicated objects consistent and for the implementation of a distributed checkpointing algorithm [17]. <p> The code of the group implementation is adapted by these parameters. 3.2 RPC Structure and Protocol The RPC protocol is based on Birrell and Nelson <ref> [7] </ref>. An RPC requires three messages during normal execution: a request, a reply, and an acknowledgement. On some architectures (e.g. a network of T9000 Transputers) reliable message passing is provided already, so the acknowledgement is not necessary.
Reference: [8] <author> R. Butler and E. Lusk. </author> <title> Monitors, Messages, and Clusters: the p4 Parallel Programming System. </title> <journal> Journal of Parallel Computing. </journal> <note> (submitted). </note>
Reference-contexts: Page-based DSM systems, however, often rely on manipulation of the virtual memory management unit, and therefore also suffer from portability problems. Highly portable message passing systems exist, but they provide limited functionality. PVM [22] and p4 <ref> [8] </ref>, for instance, provide low-level message passing, but they support neither threads nor totally-ordered group communication. Instead of relying on page-based DSM, operating systems, or low-level message passing, we have developed a portable virtual machine, called Panda. <p> We consider portable message passing systems (p4, PVM), Distributed Shared Memory systems (Munin and Midway), ARCADE, and ISIS/HORUS. Like Panda, PVM [22] and p4 <ref> [8] </ref> provide portable communication primitives. PVM and p4, however, provide message passing primitives only, and neither provides high-level communication in the form of RPC or totally-ordered group communication. In our experience, RPC and group communication simplify the implementation of complex run-time systems. Neither PVM nor p4 supports lightweight threads.
Reference: [9] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proc. of the 13th Symposium on Operating Systems Principles. ACM, </booktitle> <year> 1991. </year>
Reference-contexts: Unless such a model is very simple, flexible, and lightweight, layering another object model on top of it can be troublesome and inefficient. Higher-level approaches to supporting parallel programming include page-based Distributed Shared Memory (DSM) systems (e.g., Munin <ref> [9] </ref>). Page-based DSM systems, however, often rely on manipulation of the virtual memory management unit, and therefore also suffer from portability problems. Highly portable message passing systems exist, but they provide limited functionality. <p> Work on extending Orca and Panda with performance debugging tools is in progress. Unlike PVM and p4, Panda does not support heterogeneity. DSM systems like Munin <ref> [9] </ref> and Midway [5] support parallel programming by providing a shared memory abstraction that hides all message passing from the programmer. Although Panda does not provide such an abstraction by itself, it gives sufficient support to layer a shared memory model on top of it.
Reference: [10] <author> D.L. Cohn, A. Banerji, </author> <title> M.R. Casey, P.M. Greenawalt, and D.C. Kulka-rni. Basing Micro-kernel Abstractions on High-Level Language Models. </title> <booktitle> In Proc. of the Autumn 1992 OpenForum Technical Conference, </booktitle> <pages> pages 323-336, </pages> <address> Utrecht, Holland, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Munin (and sometimes Midway) needs to manipulate the MMU, while Orca implementations guarantee sequential consistency, which is stronger than all previously mentioned forms of consistency, without MMU manipulation. Thus, layering an Orca run-time system on top of Panda results in a portable implementation of sequential consistency. Like Panda, ARCADE <ref> [10] </ref> supports the implementation of parallel programming languages through high-level abstractions. The ARCADE abstractions, however, are based on language-independent data units rather than communication mechanisms.
Reference: [11] <author> E.C. Cooper and R.P. Draves. </author> <title> C Threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Department of Computer Science, Carnegie Mellon University, Pitts-burgh, </institution> <year> 1988. </year>
Reference-contexts: Threads The thread interface (see Table 1) is based on the Pthreads [15, 18] and C Threads <ref> [11] </ref> interfaces. Since threads are implemented in the system layer (see Figure 1), thread primitives do not have a pan prefix. From experience with Amoeba threads we have learned that a thread package for parallel programming languages should support priority scheduling to handle incoming messages immediately when they arrive.
Reference: [12] <author> P. Dasgupta, R.C. Chen, S. Menon, M. Pearson, R. Ananthanarayanan, U. Ramachandran, M. Ahamad, R. LeBlanc Jr., W. Applebe, J. M. Bernabeu-Auban, P.W. Hutto, M.Y.A. Khalidi, and C. J. Wilkenloh. </author> <title> The Design and Implementation of the Clouds Distributed Operating System. </title> <journal> Computing Systems Journal, </journal> <volume> 3, </volume> <year> 1990. </year>
Reference-contexts: Also, current operating systems generally provide more functionality than is needed or wanted for parallel processing (e.g., virtual memory management). Some modern operating systems, like Clouds <ref> [12] </ref>, support their own object model. Unless such a model is very simple, flexible, and lightweight, layering another object model on top of it can be troublesome and inefficient. Higher-level approaches to supporting parallel programming include page-based Distributed Shared Memory (DSM) systems (e.g., Munin [9]).
Reference: [13] <author> S.E. Deering and D.R. Cheriton. </author> <title> Multicast Routing in Datagram Inter-networks and Extended LANs. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 17(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: We have implemented our threads interface with Pthreads [18], a POSIX-conformant, user-space threads implementation. We have extended the kernels of our SPARC workstations with IP multicast, a kernel extension for multicasting <ref> [13] </ref>. Point-to-point message passing has been implemented on top of UDP [21]. Pthreads provides all the functionality we need, including priority scheduling, and runs entirely in user-space. User space threads are more efficient than (pure) kernel-based implementations, because thread context switches do not involve trapping to the kernel.
Reference: [14] <author> C.A.R. Hoare. </author> <title> Monitors: An Operating System Structuring Concept. </title> <journal> Communications of the ACM, </journal> <volume> 12(10) </volume> <pages> 549-557, </pages> <month> October </month> <year> 1974. </year> <month> 15 </month>
Reference-contexts: Synchronization between threads is based on mutexes and condition variables. Together these provide strong enough semantics to construct monitors <ref> [14] </ref>. The RPC interface (see Table 2) is based on the notion of a service that provides a number of operations. A service is implemented by one or more servers.
Reference: [15] <author> IEEE. </author> <title> Threads Extensions for Portable Operating Systems P1003.4a, </title> <type> draft 6 edition, </type> <month> February </month> <year> 1992. </year>
Reference-contexts: Functions that are part of both interfaces are not prefixed. 2.1 The Panda Interface The Panda interface provides the RPC, totally-ordered group communication, and thread abstractions with which Panda applications can be built. Threads The thread interface (see Table 1) is based on the Pthreads <ref> [15, 18] </ref> and C Threads [11] interfaces. Since threads are implemented in the system layer (see Figure 1), thread primitives do not have a pan prefix.
Reference: [16] <author> M.F. Kaashoek. </author> <title> Group Communication in Distributed Computer Systems. </title> <type> PhD thesis, </type> <institution> Vrije Universiteit Amsterdam, </institution> <year> 1992. </year>
Reference-contexts: Threads provide a simple, lightweight unit of activity. RPC [7] is a general mechanism for high-level point-to-point communication between nodes (and thus for the implementation of remote object invocation). Totally-ordered group communication <ref> [16] </ref> has been successfully employed in previous Orca run-time systems for keeping replicated objects consistent and for the implementation of a distributed checkpointing algorithm [17]. It assures that all members of a group receive all group messages in the same order, which makes many parallel applications easier to implement. <p> This thread calls the registered function for the specified service and operation. This function has three parameters: an operation index number, an input message, and a reply message. Totally-Ordered Group Communication The group abstraction of Panda (see Table 3) supports totally-ordered, closed groups <ref> [16] </ref>. The total ordering assures that every group member receives all group messages in the same order. A group is closed if only its members can send messages to the group. This makes an efficient implementation possible. <p> Therefore, this code is entirely machine-independent. 8 3.1 Group Communication Structure and Protocol The group communication implementation is based on <ref> [16] </ref>, which describes an efficient, totally-ordered, and atomic group communication protocol. Since we are not concerned with fault tolerance, we have implemented this protocol in a non-resilient way, thereby loosing atomicity (all-or-none delivery, even in the presence of processor crashes). <p> It is possible, however, to do synchronous checkpointing on top of totally-ordered group communication without atomicity [17]. Totally-ordered group communication is achieved by having a special member in each group, the sequencer, which assigns a sequence number to each group message. This gives two possibilities for a group send <ref> [16] </ref>. The first method is to send a point-to-point message to the sequencer, and the sequencer then broadcasts the message after filling in the sequence number (the so-called PB method). The second method is to let the sender itself do the broadcast. <p> The latency of an empty group message is 6.7 ms. Since the protocol uses negative acknowledgements, this latency is almost independent of the number of platforms <ref> [16] </ref>.
Reference: [17] <author> M.F. Kaashoek, R. Michiels, H.E. Bal, </author> <title> and A.S. Tanenbaum. Transparent Fault-Tolerance in Parallel Orca Programs. </title> <booktitle> Symposium on Experiences with Distributed and Multiprocessor Systems III, </booktitle> <pages> pages 297-312, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: RPC [7] is a general mechanism for high-level point-to-point communication between nodes (and thus for the implementation of remote object invocation). Totally-ordered group communication [16] has been successfully employed in previous Orca run-time systems for keeping replicated objects consistent and for the implementation of a distributed checkpointing algorithm <ref> [17] </ref>. It assures that all members of a group receive all group messages in the same order, which makes many parallel applications easier to implement. Hardware broadcast mechanisms usually do not guarantee 2 this strong semantics. <p> Since we are not concerned with fault tolerance, we have implemented this protocol in a non-resilient way, thereby loosing atomicity (all-or-none delivery, even in the presence of processor crashes). It is possible, however, to do synchronous checkpointing on top of totally-ordered group communication without atomicity <ref> [17] </ref>. Totally-ordered group communication is achieved by having a special member in each group, the sequencer, which assigns a sequence number to each group message. This gives two possibilities for a group send [16].
Reference: [18] <author> F. Mueller. </author> <title> Implementing POSIX Threads under UNIX: Description of Work in Progress. </title> <booktitle> In Proc. of the 2nd Software Engineering Research Forum, </booktitle> <pages> pages 253-261, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Functions that are part of both interfaces are not prefixed. 2.1 The Panda Interface The Panda interface provides the RPC, totally-ordered group communication, and thread abstractions with which Panda applications can be built. Threads The thread interface (see Table 1) is based on the Pthreads <ref> [15, 18] </ref> and C Threads [11] interfaces. Since threads are implemented in the system layer (see Figure 1), thread primitives do not have a pan prefix. <p> To avoid writing a large amount of software that we expect to be provided by future target platforms, we have used public-domain software for our threads and (unreliable) multicast implementation. We have implemented our threads interface with Pthreads <ref> [18] </ref>, a POSIX-conformant, user-space threads implementation. We have extended the kernels of our SPARC workstations with IP multicast, a kernel extension for multicasting [13]. Point-to-point message passing has been implemented on top of UDP [21]. Pthreads provides all the functionality we need, including priority scheduling, and runs entirely in user-space.
Reference: [19] <author> S.J. Mullender, G. van Rossum, A.S. Tanenbaum, R. van Renesse, and H. van Staveren. </author> <title> Amoeba-A Distributed Operating System for the 1990s. </title> <booktitle> IEEE Computer, </booktitle> <year> 1990. </year>
Reference-contexts: Our experience with the parallel fl This research is supported in part by a PIONIER grant from the Netherlands Organisation for Scientific Research (N.W.O.). 1 Unix is a trademark of Unix Systems Laboratories, Inc. 1 programming language Orca [4] on the Amoeba <ref> [19, 24] </ref> distributed operating system is that this strategy results in a language implementation that is difficult to port to other operating systems. Orca is based on shared objects. As shared objects may be replicated to speed up read accesses, their implementation on distributed memory machines requires advanced run-time support.
Reference: [20] <author> L. Peterson, N. Hutchinson, S. O'Malley, and H. Rao. </author> <title> The x-kernel: A Platform for Accessing Internet Resources. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 23-33, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Panda takes a layering approach towards portability. Although layering is an effective way to abstract from machine-dependencies, it bears with it the danger of poor performance. Thoughtless layering may well result in a loss of information that is essential to achieving good performance <ref> [20] </ref>. Therefore, we have identified Panda's performance-critical parts: threads, message manipulation, and the nature of the underlying network. These performance-critical parts are all implemented in the system layer, where they have access to low-level, operating-specific features. <p> After getting a fragment from a message, some fields in the common header part can be filled in with information that identifies this fragment. At the receiving side, sys message assemble is used to reassemble the original message. These primitives resemble the x-kernel primitives for fragmenting messages <ref> [20] </ref>. 7 A fragment message need not contain copies of the common header and data fields of the original message; pointers may be used instead.
Reference: [21] <author> J. Postel. </author> <title> User Datagram Protocol. Internet Request for Comments RFC768, </title> <month> September </month> <year> 1981. </year>
Reference-contexts: We have implemented our threads interface with Pthreads [18], a POSIX-conformant, user-space threads implementation. We have extended the kernels of our SPARC workstations with IP multicast, a kernel extension for multicasting [13]. Point-to-point message passing has been implemented on top of UDP <ref> [21] </ref>. Pthreads provides all the functionality we need, including priority scheduling, and runs entirely in user-space. User space threads are more efficient than (pure) kernel-based implementations, because thread context switches do not involve trapping to the kernel.
Reference: [22] <author> V. Sunderam. </author> <title> PVM: A Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: Higher-level approaches to supporting parallel programming include page-based Distributed Shared Memory (DSM) systems (e.g., Munin [9]). Page-based DSM systems, however, often rely on manipulation of the virtual memory management unit, and therefore also suffer from portability problems. Highly portable message passing systems exist, but they provide limited functionality. PVM <ref> [22] </ref> and p4 [8], for instance, provide low-level message passing, but they support neither threads nor totally-ordered group communication. Instead of relying on page-based DSM, operating systems, or low-level message passing, we have developed a portable virtual machine, called Panda. <p> We consider portable message passing systems (p4, PVM), Distributed Shared Memory systems (Munin and Midway), ARCADE, and ISIS/HORUS. Like Panda, PVM <ref> [22] </ref> and p4 [8] provide portable communication primitives. PVM and p4, however, provide message passing primitives only, and neither provides high-level communication in the form of RPC or totally-ordered group communication. In our experience, RPC and group communication simplify the implementation of complex run-time systems.
Reference: [23] <author> A. S. Tanenbaum, M. F. Kaashoek, and H. E. Bal. </author> <title> Parallel Programming Using Shared Objects and Broadcasting. </title> <journal> IEEE Computer, </journal> <volume> 25(8) </volume> <pages> 10-19, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Panda, however, does not restrict its users to Orca's object model. It provides the following, general abstractions: * threads, * Remote Procedure Call (RPC), * totally-ordered group communication. Experience with similar Amoeba abstractions has shown that efficient implementations of shared objects can be built on top of them <ref> [23] </ref>. Threads provide a simple, lightweight unit of activity. RPC [7] is a general mechanism for high-level point-to-point communication between nodes (and thus for the implementation of remote object invocation).
Reference: [24] <author> A.S. Tanenbaum, R. van Renesse, H. van Staveren, G.J. Sharp, S.J. Mul-lender, A.J. Jansen, and G. van Rossum. </author> <title> Experiences with the Amoeba Distributed Operating System. </title> <journal> Communications of the ACM, </journal> <volume> 33(2) </volume> <pages> 46-63, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Our experience with the parallel fl This research is supported in part by a PIONIER grant from the Netherlands Organisation for Scientific Research (N.W.O.). 1 Unix is a trademark of Unix Systems Laboratories, Inc. 1 programming language Orca [4] on the Amoeba <ref> [19, 24] </ref> distributed operating system is that this strategy results in a language implementation that is difficult to port to other operating systems. Orca is based on shared objects. As shared objects may be replicated to speed up read accesses, their implementation on distributed memory machines requires advanced run-time support.
Reference: [25] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <year> 1991. </year>
Reference-contexts: We have constructed an initial implementation of Panda for a collection of SPARC-based workstations, running Unix, and connected by a 10 Mbit/s Ethernet. We intend to port Panda in the near future to a T9000-based parallel machine, the Alewife [1], and the CM-5 <ref> [25] </ref>. Section 2 describes the Panda and system interface in more detail. The machine-independent implementation of the Panda interface is outlined in Section 3. In Section 4, we describe our experience with an initial implementation of the system interface on Unix.
Reference: [26] <author> R. van Renesse, K. Birman, R. Cooper, B. Glade, and P. Stephenson. </author> <title> Reliable Multicast between Microkernels. </title> <booktitle> In Proccedings of the USENIX workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 269-283, </pages> <month> April 27-28 </month> <year> 1992. </year> <month> 16 </month>
Reference-contexts: Language-specific objects can be mapped onto ARCADE's data units. Like Orca, ISIS [6] is currently being reimplemented for reasons of portability and scalability. The new ISIS system, HORUS <ref> [26] </ref>, has a core interface that provides reliable, causal multicasting (CBCAST). Other services are implemented on top of this interface.
References-found: 26

