URL: http://www.eecs.uic.edu/~sloan/circulate-ml-submit.ps
Refering-URL: http://www.eecs.uic.edu/~sloan/papers.html
Root-URL: 
Email: E-mail: angluin@cs.yale.edu and krikis@cs.yale.edu  E-mail: sloan@eecs.uic.edu  
Phone: 60607.  
Title: Malicious Omissions and Errors in Answers to Membership Queries  
Author: Dana Angluin Marti~ns Krik~is Robert H. Sloan Gyorgy Turan 
Address: sity, P.O. Box 208285, New Haven, CT 06520.  Chicago, Chicago, IL  
Affiliation: Department of Computer Science, Yale Univer  Dept. of Electrical Eng. and Computer Science,  University of Illinois at  
Note: Submitted to Machine Learning for publication. Partially supported by NSF grant CCR-9213881.  Partially supported by NSF grants CCR-9108753 and CCR-9314258.  851 S. Morgan St. Rm 1120,  
Date: May 1, 1995  
Abstract: We consider two issues in polynomial-time exact learning of concepts using membership and equivalence queries: (1) errors or omissions in answers to membership queries, and (2) learning finite variants of concepts drawn from a learnable class. To study (1), we introduce two new kinds of membership queries: limited membership queries and malicious membership queries. Each is allowed to give incorrect responses on a maliciously chosen set of strings in the domain. Instead of answering correctly about a string, a limited membership query may give a special "I don't know" answer, while a malicious membership query may give the wrong answer. The table-size of the set of strings that receive such incorrect answers is bounded by a new parameter L. Equivalence queries are answered correctly, and learning algorithms are allowed time polynomial in the usual parameters and L. Any class of concepts learnable in polynomial time z Partially supported by NSF grant CCR-9208170, Esprit BRA ILP, Project 6020, and OTKA T014228. Dept. of Math., Stat., and Comp. Sci., 851 S. Morgan St. Rm 322, University of Illinois at Chicago, Chicago, IL 60607, Automata Theory Research Group Hungarian Academy of Sciences, Szeged. E-mail: U11557@uicvm.uic.edu 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John R. Anderson. </author> <title> Cognitive Psychology and Its Implications. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1980. </year>
Reference-contexts: Studies in cognitive psychology indicate that this is the norm; people are typically quite inconsistent in deciding where the precise boundary of a concept lies. (See, e.g., <ref> [1] </ref>.) 1.1 Omissions and Limited Membership Queries This motivated us to introduce the limited membership query.
Reference: [2] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Inform. Comput., </journal> <volume> 75(2) </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: As corollaries of Theorem 7 we have the following. Corollary 5 The class of regular languages, represented by DFA's, is learnable in polynomial time with equivalence and malicious membership queries. Proof: In [10] it was shown that this class of concepts is polynomially closed under finite exceptions. In <ref> [2] </ref> it was shown that it is learnable in polynomial time using membership and equivalence queries. 46 Corollary 6 The class of boolean decision trees is learnable in polynomial time with extended equivalence and malicious membership queries.
Reference: [3] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Proof: We modify Angluin's algorithm for learning monotone DNF from ordinary membership and equivalence queries <ref> [3] </ref>, by using Algorithm Delimit. The complete code of the algorithm is given in Figure 8. Note that the same algorithm works for both the standard and strict models. <p> The subroutine is given in Figure 9. CheckedMQ (x) f If (9 (v; 1) 2 CounterExamples s.t. x v) Return 1 If (9 (v; 0) 2 CounterExamples s.t. x v) Return 0 Return MMQ (x) g As in <ref> [3] </ref> and [7], our algorithm also uses a subroutine Reduce in order to move down in the lattice from a positive counterexample. All the membership queries are done using the subroutine CheckedMQ, which possibly lets the algorithm avoid some incorrect answers. The subroutine Reduce is given in Figure 10. <p> This is done by another very simple subroutine TheFunction, given in Figure 13. As in <ref> [3] </ref>, [7], and Section 4, our algorithm also uses a subroutine Reduce to move down in the lattice from a positive counterexample. Its goal is to reduce the positive counterexample to some point that can be added as a term to the formula h.
Reference: [4] <author> D. Angluin. </author> <title> Exact learning of -DNF formulas with malicious membership queries. </title> <type> Technical Report YALEU/DCS/TR-1020, </type> <institution> Yale University Department of Computer Science, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: For example, there is not yet any algorithm for learning read-once formulas from equivalence and limited or malicious membership queries, even though there is an algorithm for learning read-once formulas &gt;from equivalence and standard membership queries. A start in this direction is made in <ref> [4] </ref>, which gives a randomized polynomial-time algorithm to learn -DNF formulas with equivalence and malicious membership queries. In the model of PAC learning with membership queries, it would be interesting to see whether Baum's algorithm [9] can be modified to tolerate "I don't know" answers.
Reference: [5] <author> D. Angluin and M. Krik~is. </author> <title> Learning with malicious membership queries and exceptions. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 57-66. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: time bound for the algorithm that learns monotone DNF formulas with exceptions and possibly for the one that learns monotone DNF from equivalence and malicious membership queries. 52 9 Comments This work appeared as two separate papers in the Proceedings of the 7th Annual ACM Conference on Computational Learning Theory <ref> [5] </ref>, [23]. Part of it is also available as a technical report [6].
Reference: [6] <author> D. Angluin and M. Krikis. </author> <title> Malicious membership queries and exceptions. </title> <type> Technical Report YALEU/DCS/TR-1019, </type> <institution> Yale University Department of Computer Science, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Part of it is also available as a technical report <ref> [6] </ref>.
Reference: [7] <author> D. Angluin and D. K. </author> <title> Slonim. Randomly fallible teachers: learning monotone DNF with an incomplete membership oracle. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 7-26, </pages> <year> 1994. </year>
Reference-contexts: The "I don't know" answers are determined by independent coin flips the first time each query is made <ref> [7] </ref>. They give a polynomial-time algorithm to learn monotone DNF formulas with high probability in this setting. They also show that a variant of this algorithm can deal with one-sided errors, assuming that no negative point is classified as positive. Goldman and Mathias also consider this model [16]. <p> This algorithm plays the role of the algorithms called "Reduce" in other works on learning monotone DNF <ref> [7] </ref>. We choose a different name because those algorithms output a single monomial, whereas Algorithm Delimit finds a set of points that must include a correct term. <p> The subroutine is given in Figure 9. CheckedMQ (x) f If (9 (v; 1) 2 CounterExamples s.t. x v) Return 1 If (9 (v; 0) 2 CounterExamples s.t. x v) Return 0 Return MMQ (x) g As in [3] and <ref> [7] </ref>, our algorithm also uses a subroutine Reduce in order to move down in the lattice from a positive counterexample. All the membership queries are done using the subroutine CheckedMQ, which possibly lets the algorithm avoid some incorrect answers. The subroutine Reduce is given in Figure 10. <p> This is done by another very simple subroutine TheFunction, given in Figure 13. As in [3], <ref> [7] </ref>, and Section 4, our algorithm also uses a subroutine Reduce to move down in the lattice from a positive counterexample. Its goal is to reduce the positive counterexample to some point that can be added as a term to the formula h. <p> Note that ADDRESSING also causes the incomplete membership query model <ref> [7] </ref> to have an expected exponential blowup over ordinary membership queries when 48 the probability of a ? response is a constant. For constant probability p of ?, the expected number of instances in X 0 answered ? is pm.
Reference: [8] <author> P. Auer and P. M. </author> <title> Long. Simulating access to hidden information while learning. </title> <booktitle> In Proc. of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 263-272. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: If instead of equivalence queries one allows equivalence queries with any set, then such a blowup cannot occur. This follows from a result of Auer and Long <ref> [8] </ref> showing that in this model membership queries can speed up learning by only a constant factor. 6.3 Strict versus Nonstrict Recall that in the nonstrict model the final hypothesis need only agree with the target concept on points x such that LMQ (x) 6= ?, while in the strict model,
Reference: [9] <author> E. Baum. </author> <title> Neural net algorithms that learn in polynomial time from examples and queries. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2 </volume> <pages> 5-19, </pages> <year> 1991. </year> <month> 53 </month>
Reference-contexts: Lang and Baum [19] report that this problem derailed their attempt to apply Baum's algorithm for learning neural nets from examples and membership queries <ref> [9] </ref> to the problem of recognizing hand-written digits. The attempt failed because the membership questions posed by the algorithm were too difficult for people to answer reliably. <p> A start in this direction is made in [4], which gives a randomized polynomial-time algorithm to learn -DNF formulas with equivalence and malicious membership queries. In the model of PAC learning with membership queries, it would be interesting to see whether Baum's algorithm <ref> [9] </ref> can be modified to tolerate "I don't know" answers. Another type of open problems is finding lower bounds for any of the classes of concepts for which we give learning algorithms using equivalence and limited or malicious membership queries.
Reference: [10] <author> R. Board and L. Pitt. </author> <title> On the necessity of Occam algorithms. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 100 </volume> <pages> 157-184, </pages> <year> 1992. </year>
Reference-contexts: This definition differs from a similar earlier definition <ref> [10] </ref> in that we do not require the existence of a polynomial-time algorithm that produces the new concept given the old concept and a list of exceptions. However, for the classes that we consider there are such algorithms. <p> Board and Pitt give an algorithm that takes as input a DFA M and an exception set S, and produces a new DFA for xcpt (M; S) <ref> [10] </ref>. The DFA's size is polynomial in the size of M and S. 35 Example 2 Another example of a class that is polynomially closed under finite exceptions is the class of boolean decision trees. This result is taken from [10] but since the construction is not given there, we sketch <p> S, and produces a new DFA for xcpt (M; S) <ref> [10] </ref>. The DFA's size is polynomial in the size of M and S. 35 Example 2 Another example of a class that is polynomially closed under finite exceptions is the class of boolean decision trees. This result is taken from [10] but since the construction is not given there, we sketch it here. Lemma 9 The class of boolean decision trees is polynomially closed under finite exceptions. Proof: Let T be a decision tree on n variables. Let S be the exception set for T . <p> Note that stronger bounds on the size of the new formula can be obtained by using the result in [25]. We, however, chose to present a simpler argument. Also note that the size bound is insufficient for strong polynomial closure under exception lists as defined in <ref> [10] </ref>. Example 5 As our final example we show that any class that is obtained by adding exception tables to another class is polynomially closed under finite exceptions. Lemma 11 Let (R; Dom; ) be any class of concepts. <p> This concludes the proof of Theorem 7. As corollaries of Theorem 7 we have the following. Corollary 5 The class of regular languages, represented by DFA's, is learnable in polynomial time with equivalence and malicious membership queries. Proof: In <ref> [10] </ref> it was shown that this class of concepts is polynomially closed under finite exceptions.
Reference: [11] <author> N. Bshouty. </author> <title> Exact learning via the monotone theory. </title> <booktitle> In Proc. of the 34th Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 302-311. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1993. </year>
Reference-contexts: Proof: Lemma 9 shows that the class of boolean decision trees is polynomially closed under finite exceptions. In <ref> [11] </ref> it was shown that it is learnable in polynomial time using membership and extended equivalence queries. Corollary 7 The class of monotone DNF formulas with finite exceptions is learnable in polynomial time with equivalence and malicious membership queries.
Reference: [12] <author> William J. Bultman. </author> <title> Topics in the Theory of Machine Learning and Neural Computing. </title> <type> PhD thesis, </type> <institution> University of Illinois at Chicago Mathematics Department, </institution> <year> 1991. </year>
Reference-contexts: He gives a general technique of repeating each query sufficiently often to establish the correct answer with high probability. This yields a uniform transformation of existing query algorithms. The method also works for both of Bultman's models <ref> [12] </ref>. This could be a reasonable model of a situation in which the answers to queries were being transmitted through a medium subject to random independent errors; then the technique of repeating the query is eminently sensible.
Reference: [13] <author> T. Dean, D. Angluin, K. Basye, S. Engelson, L. Kaelbling, E. Kokkevis, and O. Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> pages 208-214. </pages> <publisher> AAAI, </publisher> <year> 1992. </year>
Reference-contexts: A related model is considered by Dean et al. for the case of a robot learning a finite-state map of its environment using faulty sensors and reliable effectors <ref> [13] </ref>. This model assumes that observation errors are independent as long as there is a nonempty action sequence separating the observations.
Reference: [14] <author> M. Frazier, S. Goldman, N. Mishra, and L. Pitt. </author> <title> Learning from a consistently ignorant teacher. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 328-339. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Goldman and Mathias also consider this model [16]. Our current models and results differ in that the omissions and errors are chosen by a malicious adversary instead of a random process, and the rate of incorrect answers that can be tolerated is consequently much lower. Frazier et al. <ref> [14] </ref> have introduced a model of omissions in answers to membership queries, called learning from a consistently ignorant teacher.
Reference: [15] <author> S. A. Goldman, M. J. Kearns, and R. E. Schapire. </author> <title> Exact identification of read-once formulas using fixed points of amplification functions. </title> <journal> SIAM J. Comput., </journal> <volume> 22(4) </volume> <pages> 705-726, </pages> <year> 1993. </year>
Reference-contexts: For example, Goldman, Kearns, and Schapire give polynomial-time algorithms for exactly learning read-once majority formulas and read-once positive NAND formulas of depth O (log n) with high probability using membership queries with high rates of persistent random noise or modest rates of persistent malicious noise <ref> [15] </ref>. As another example, Kushilevitz and Mansour's algorithm that uses membership queries and exactly learns logarithmic-depth decision trees with high probability in polynomial time seems likely to be robust under nontrivial rates of persistent random noise in the answers to queries [18].
Reference: [16] <author> S. A. Goldman and H. D. Mathias. </author> <title> Learning k-term DNF formulas with an incomplete membership oracle. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 77-84. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: They give a polynomial-time algorithm to learn monotone DNF formulas with high probability in this setting. They also show that a variant of this algorithm can deal with one-sided errors, assuming that no negative point is classified as positive. Goldman and Mathias also consider this model <ref> [16] </ref>. Our current models and results differ in that the omissions and errors are chosen by a malicious adversary instead of a random process, and the rate of incorrect answers that can be tolerated is consequently much lower.
Reference: [17] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proc. 25th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 392-401. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Algorithms that use membership queries to estimate probabilities (in the spirit of the statistical queries defined by Kearns <ref> [17] </ref>) are generally not too sensitive to small rates of random persistent errors in the answers to queries.
Reference: [18] <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the Fourier spectrum. </title> <booktitle> In Proc. of the 23rd Symposium on Theory of Computing, </booktitle> <pages> pages 455-464. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: As another example, Kushilevitz and Mansour's algorithm that uses membership queries and exactly learns logarithmic-depth decision trees with high probability in polynomial time seems likely to be robust under nontrivial rates of persistent random noise in the answers to queries <ref> [18] </ref>.
Reference: [19] <author> Kevin J. Lang and Eric B. Baum. </author> <title> Query learning can work poorly when a human oracle is used. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <address> Beijing, </address> <year> 1992. </year>
Reference-contexts: Previous learning algorithms in the equivalence and membership query model are guaranteed to perform well assuming that queries are answered correctly, but there is often no guarantee that the performance of the algorithm will "degrade gracefully" if that assumption is not exactly satisfied. Lang and Baum <ref> [19] </ref> report that this problem derailed their attempt to apply Baum's algorithm for learning neural nets from examples and membership queries [9] to the problem of recognizing hand-written digits. The attempt failed because the membership questions posed by the algorithm were too difficult for people to answer reliably.
Reference: [20] <author> W. Maass and G. Turan. </author> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145, </pages> <year> 1992. </year> <month> 54 </month>
Reference-contexts: Proof: We construct a concept class C that is a variant of ADDRESSING <ref> [20] </ref>. Let the instance space be X = S 2 ` i=0 X i , where the X i 's are disjoint, X 0 = f1; : : : ; `g, and jX i j = m l + 1 for 1 i 2 ` .
Reference: [21] <author> D. Ron and R. Rubinfeld. </author> <title> Learning fallible finite state automata. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 218-227. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: very 6 interesting application of the ideas of error-correcting algorithms, Ron and Rubin--feld use the criterion of PAC-identification with respect to the uniform distribution, and give a polynomial-time randomized algorithm using membership queries to learn DFA's with high rates of random persistent errors in the answers to the membership queries <ref> [21] </ref>. Algorithms that use membership queries to estimate probabilities (in the spirit of the statistical queries defined by Kearns [17]) are generally not too sensitive to small rates of random persistent errors in the answers to queries.
Reference: [22] <author> Y. Sakakibara. </author> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Inform. Proc. Lett., </journal> <volume> 37(5) </volume> <pages> 279-284, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: and can always be made persistent simply by caching and using the first answer for each domain point queried. 1.5 Non-persistent Errors in Queries Sakakibara defines one model of non-persistent errors, in which each answer to a query may be wrong with some probability, and repeated queries constitute independent events <ref> [22] </ref>. He gives a general technique of repeating each query sufficiently often to establish the correct answer with high probability. This yields a uniform transformation of existing query algorithms. The method also works for both of Bultman's models [12].
Reference: [23] <author> R. Sloan and G. Turan. </author> <title> Learning with queries but incomplete information. </title> <booktitle> In Proc. of the 7th Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 237-245. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: bound for the algorithm that learns monotone DNF formulas with exceptions and possibly for the one that learns monotone DNF from equivalence and malicious membership queries. 52 9 Comments This work appeared as two separate papers in the Proceedings of the 7th Annual ACM Conference on Computational Learning Theory [5], <ref> [23] </ref>. Part of it is also available as a technical report [6].
Reference: [24] <author> L. G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the 9th International Joint Conference on Artificial Intelligence, </booktitle> <volume> vol. 1, </volume> <pages> pages 560-566, </pages> <address> Los Angeles, California, </address> <year> 1985. </year> <booktitle> International Joint Committee for Artificial Intelligence. </booktitle>
Reference-contexts: In both cases, the goal is to identify exactly the concept presented by the equivalence oracle. 1.4 Related Work There is a considerable body of literature on errors in examples in the PAC model, starting with the first error-tolerant algorithm in the PAC model, given by Valiant <ref> [24] </ref>.
Reference: [25] <author> Y. Zhuravlev and Y. Kogan. </author> <title> Realization of boolean functions with a small number of zeros by disjunctive normal forms, and related problems. </title> <journal> Soviet Math. Doklady, </journal> <volume> 32 </volume> <pages> 771-775, </pages> <year> 1985. </year> <month> 55 </month>
Reference-contexts: Example 4 By duality it follows that the class of CNF formulas is polynomially closed under finite exceptions. Note that stronger bounds on the size of the new formula can be obtained by using the result in <ref> [25] </ref>. We, however, chose to present a simpler argument. Also note that the size bound is insufficient for strong polynomial closure under exception lists as defined in [10].
References-found: 25

