URL: ftp://ftp.cs.brown.edu/pub/techreports/93/cs93-55.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-93-55.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [ Bar-Shalom and Fortmann, 1988 ] <author> Bar-Shalom, Yaakov and Fortmann, Thomas E. </author> <year> 1988. </year> <title> Tracking and Data Association. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: First, determine the optimal regulator assuming that observation is perfect; this can be done using Howard's policy iteration. Second, design the optimal observer using a model for the observation errors; there is a substantial literature on using Bayesian decision theory to design optimal observers <ref> [ Bar-Shalom and Fortmann, 1988 ] </ref> . Finally, couple the optimal observer to the optimal regulator.
Reference: [ Bellman, 1957 ] <author> Bellman, Richard 1957. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: The research presented in this paper is related to work in the literature on sequential decision making, stochastic control, and reinforcement learning; we review this work in Section 9. 2 Markov Decision Models Following the work on Markov decision processes <ref> [ Bellman, 1957, Bertsekas, 1987 ] </ref> , we model the entire environment as a stochastic automaton. Let S be the finite set of world states; we assume that they can be reliably identified by the agent. <p> Given a state-transition model, a reward function, and a value for fl, it is possible to compute the optimal policy using either the policy iteration algorithm [ Howard, 1960 ] or the value iteration algorithm <ref> [ Bellman, 1957 ] </ref> . We use the policy iteration algorithm because it is guaranteed to converge in a finite number of steps|generally a small number of steps in the domains that we have experimented with|and thus simplifies debugging our computational experiments. The policy iteration algorithm works as follows: 1. <p> The heuristic methods for adding fringe states and pruning are likewise similar in their implementation and intuitive basis. 9 Related Work Our primary interest is in applying the sequential decision making techniques of Bellman <ref> [ Bellman, 1957 ] </ref> and Howard [ Howard, 1960 ] in time-critical applications. Our initial motivation for the methods discussed here came from the `anytime synthetic projection' work of Drummond and Bresina. [ Drummond and Bresina, 1990 ] .
Reference: [ Bertsekas, 1987 ] <author> Bertsekas, Dimitri P. </author> <year> 1987. </year> <title> Dynamic Programming. </title> <publisher> Prentice-Hall, </publisher> <address> En-glewood Cliffs, N.J. </address> <month> 40 </month>
Reference-contexts: The research presented in this paper is related to work in the literature on sequential decision making, stochastic control, and reinforcement learning; we review this work in Section 9. 2 Markov Decision Models Following the work on Markov decision processes <ref> [ Bellman, 1957, Bertsekas, 1987 ] </ref> , we model the entire environment as a stochastic automaton. Let S be the finite set of world states; we assume that they can be reliably identified by the agent.
Reference: [ Boddy, 1991 ] <author> Boddy, </author> <title> Mark 1991. Anytime problem solving using dynamic programming. </title> <booktitle> In Proceedings AAAI-91. AAAI. </booktitle> <pages> 738-743. </pages>
Reference-contexts: Boddy <ref> [ Boddy, 1991 ] </ref> describes solutions to related problems involving dynamic programming. <p> Christiansen and Goldberg [ Christiansen and Goldberg, 1990 ] and Kushmerick, Hanks, and Weld [ Kush-merick et al., 1993 ] also address the problem of planning in stochastic domains. Boddy <ref> [ Boddy, 1991 ] </ref> describes solutions to related problems involving dynamic programming. For an overview of resource-bounded decision making methods, see chapter 8 of the text by Dean and Wellman [ Dean and Wellman, 1991 ] .
Reference: [ Christiansen and Goldberg, 1990 ] <author> Christiansen, Alan and Goldberg, </author> <title> Ken 1990. Robotic manipulation planning with stochastic actions. In DARPA Workshop on Innovative Approaches to Planning, Scheduling and Control. </title> <address> San Diego,California. </address>
Reference-contexts: Hansson and Mayer's BPS (Bayesian Problem Solver) [ Hansson and Mayer, 1989 ] supports general state-space search with decision-theoretic control of inference; it may be that BPS could be used as the basis for envelope extension thus providing more fine-grained decision-theoretic control. Christiansen and Goldberg <ref> [ Christiansen and Goldberg, 1990 ] </ref> and Kushmerick, Hanks, and Weld [ Kush-merick et al., 1993 ] also address the problem of planning in stochastic domains. Boddy [ Boddy, 1991 ] describes solutions to related problems involving dynamic programming.
Reference: [ Dean and Boddy, 1988 ] <author> Dean, Thomas and Boddy, </author> <title> Mark 1988. An analysis of time-dependent planning. </title> <booktitle> In Proceedings AAAI-88. AAAI. </booktitle> <pages> 49-54. </pages>
Reference-contexts: For states outside of the envelope, the policy is defined by a set of reflexes that implement some default behavior for the agent. The algorithm is implemented as an anytime algorithm <ref> [ Dean and Boddy, 1988 ] </ref> , one that can be interrupted at any point during execution to return an answer whose value, at least in certain classes of stochastic processes, improves in expectation as a function of the computation time. <p> We improve on the Drum-mond and Bresina work by providing (i) coherent semantics for goals in stochastic domains, (ii) theoretically sound probabilistic foundations, (iii) and decision-theoretic methods for controlling inference. 39 The approach described in this paper represents a particular instance of time-dependent planning <ref> [ Dean and Boddy, 1988 ] </ref> and borrows from, among others, Horvitz' [ Horvitz, 1988 ] approach to flexible computation. Boddy [ Boddy, 1991 ] describes solutions to related problems involving dynamic programming.
Reference: [ Dean and Wellman, 1991 ] <author> Dean, Thomas and Wellman, </author> <title> Michael 1991. Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: Boddy [ Boddy, 1991 ] describes solutions to related problems involving dynamic programming. For an overview of resource-bounded decision making methods, see chapter 8 of the text by Dean and Wellman <ref> [ Dean and Wellman, 1991 ] </ref> . Acknowledgements Thomas Dean's work was supported in part by a National Science Foundation Presidential Young Investigator Award IRI-8957601, in part by the Advanced Research Projects Agency of the Department of Defense monitored by the Air Force under Contract No.
Reference: [ Drummond and Bresina, 1990 ] <author> Drummond, Mark and Bresina, </author> <title> John 1990. Anytime synthetic projection: Maximizing the probability of goal satisfaction. </title> <booktitle> In Proceedings AAAI-90. AAAI. </booktitle> <pages> 138-144. </pages>
Reference-contexts: In more complicated models, called recurrent-deliberation models, we assume that the agent periodically replans and executes the resulting policy in parallel with planning the next policy. Our approach is motivated by the intuitively appealing work of Drummond and Bresina on `anytime synthetic projection' <ref> [ Drummond and Bresina, 1990 ] </ref> . <p> Our initial motivation for the methods discussed here came from the `anytime synthetic projection' work of Drummond and Bresina. <ref> [ Drummond and Bresina, 1990 ] </ref> .
Reference: [ Fikes et al., 1972 ] <author> Fikes, Richard E.; Hart, Peter E.; and Nilsson, Nils J. </author> <year> 1972. </year> <title> Learning and executing generalized robot plans. </title> <booktitle> Artificial Intelligence 3 </booktitle> <pages> 251-288. </pages>
Reference-contexts: In nondeterministic worlds, planners must address the question of what to do when things do not go as expected. The method of triangle tables <ref> [ Fikes et al., 1972 ] </ref> made plans that could be executed robustly in any circumstance along the nominal trajectory of world states, allowing for certain classes of failures and serendipitous events.
Reference: [ Hansson and Mayer, 1989 ] <author> Hansson, Othar and Mayer, </author> <title> Andrew 1989. Heuristic search as evidential reasoning. </title> <booktitle> In Proceedings of the Fifth Workshop on Uncertainty in AI. </booktitle> <pages> 152-161. </pages>
Reference-contexts: Boddy [ Boddy, 1991 ] describes solutions to related problems involving dynamic programming. Hansson and Mayer's BPS (Bayesian Problem Solver) <ref> [ Hansson and Mayer, 1989 ] </ref> supports general state-space search with decision-theoretic control of inference; it may be that BPS could be used as the basis for envelope extension thus providing more fine-grained decision-theoretic control.
Reference: [ Horvitz, 1988 ] <author> Horvitz, Eric J. </author> <year> 1988. </year> <title> Reasoning under varying and uncertain resource constraints. </title> <booktitle> In Proceedings AAAI-88. AAAI. </booktitle> <pages> 111-116. </pages>
Reference-contexts: Bresina work by providing (i) coherent semantics for goals in stochastic domains, (ii) theoretically sound probabilistic foundations, (iii) and decision-theoretic methods for controlling inference. 39 The approach described in this paper represents a particular instance of time-dependent planning [ Dean and Boddy, 1988 ] and borrows from, among others, Horvitz' <ref> [ Horvitz, 1988 ] </ref> approach to flexible computation. Boddy [ Boddy, 1991 ] describes solutions to related problems involving dynamic programming.
Reference: [ Howard, 1960 ] <author> Howard, Ronald A. </author> <year> 1960. </year> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: Given a state-transition model, a reward function, and a value for fl, it is possible to compute the optimal policy using either the policy iteration algorithm <ref> [ Howard, 1960 ] </ref> or the value iteration algorithm [ Bellman, 1957 ] . <p> The heuristic methods for adding fringe states and pruning are likewise similar in their implementation and intuitive basis. 9 Related Work Our primary interest is in applying the sequential decision making techniques of Bellman [ Bellman, 1957 ] and Howard <ref> [ Howard, 1960 ] </ref> in time-critical applications. Our initial motivation for the methods discussed here came from the `anytime synthetic projection' work of Drummond and Bresina. [ Drummond and Bresina, 1990 ] .
Reference: [ Kemeny and Snell, 1960 ] <author> Kemeny, J. G. and Snell, J. L. </author> <year> 1960. </year> <title> Finite Markov Chains. </title> <address> D. </address> <publisher> Van Nostrand, </publisher> <address> New York. </address>
Reference-contexts: A policy is a mapping from S to A, specifying an action to be taken in each situation. An environment combined with a policy for choosing actions in that environment yields a Markov chain <ref> [ Kemeny and Snell, 1960 ] </ref> . A reward function is a mapping from S to &lt;, specifying the instantaneous reward that the agent derives from being in each state.
Reference: [ Kushmerick et al., 1993 ] <author> Kushmerick, Nicholas; Hanks, Steve; and Weld, </author> <title> Daniel 1993. An algorithm for probabilistic planning. </title> <type> Unpublished Manuscript. </type>
Reference-contexts: We are considering, but have not tried, Kushmerick et al.'s method for generating plausible initial policies <ref> [ Kushmerick et al., 1993 ] </ref> .
Reference: [ Lovejoy, 1991 ] <author> Lovejoy, William S. </author> <year> 1991. </year> <title> Computationally feasible bounds for partially observed markov decision processes. </title> <journal> Operations Research 39(1) </journal> <pages> 162-175. 41 </pages>
Reference-contexts: In the worst case, the number of regions in the partition can grow exponentially in the time horizon and so much of the current research involves approximation algorithms <ref> [ Lovejoy, 1991, White and Scherer, 1989 ] </ref> . Integration into our approach The above discussion was meant to provide some evidence that our basic approach can be extended to take into account uncertainty in obser 38 vation.
Reference: [ Monahan, 1982 ] <author> Monahan, George E. </author> <year> 1982. </year> <title> A survey of partially observable markov deci-sion processes: Theory, models, and algorithms. </title> <booktitle> Management Science 28(1) </booktitle> <pages> 1-16. </pages>
Reference-contexts: In the following, we consider an observer that accounts for all of the past observations and actions. Partially observable Markov decision processes In operations research, Markov decision processes that involve uncertainty in observation are called partially observable. There is a large literature on partially observable Markov decision processes (see <ref> [ Monahan, 1982 ] </ref> for a survey). A standard technique is to convert the partially observable process into a perfectly observable process with states that correspond to distributions over the original states, summarizing the agent's knowledge of its current state given all of its past actions and observations.
Reference: [ Schoppers, 1987 ] <author> Schoppers, Marcel J. </author> <year> 1987. </year> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings IJCAI 10. IJCAII. </booktitle> <pages> 1039-1046. </pages>
Reference-contexts: Many systems (sipe, for example [ Wilkins, 1988 ] ) can monitor for plan "failures" and initiate replanning. Replanning is often too slow to be useful in time-critical domains, however. Schoppers, in his universal plans <ref> [ Schoppers, 1987 ] </ref> , gives a method for generating a reaction for every possible situation that could transpire during plan execution; these plans are robust and fast to execute, but can be very large and expensive to generate. There is an inherent contradiction in all of these approaches.
Reference: [ Smallwood and Sondik, 1973 ] <author> Smallwood, Richard D. and Sondik, Edward J. </author> <year> 1973. </year> <title> The optimal control of partially observable markov processes over a finite horizon. </title> <journal> Operations Research 21 </journal> <pages> 1071-1088. </pages>
Reference-contexts: A number of techniques have been developed that serve to partition the state space into a finite set of regions or consider only a finite, reachable sub-space of the infinite state space. Smallwood and Sondik describe state-space-partitioning algorithms for the finite-horizon <ref> [ Smallwood and Sondik, 1973 ] </ref> and indefinite-duration (infinite-horizon) [ Sondik, 1978 ] cases.
Reference: [ Sondik, 1978 ] <author> Sondik, Edward J. </author> <year> 1978. </year> <title> The optimal control of partially observable markov processes over the infinite horizon: Discounted cost. </title> <journal> Operations Research 26(2) </journal> <pages> 282-304. </pages>
Reference-contexts: Smallwood and Sondik describe state-space-partitioning algorithms for the finite-horizon [ Smallwood and Sondik, 1973 ] and indefinite-duration (infinite-horizon) <ref> [ Sondik, 1978 ] </ref> cases. In the worst case, the number of regions in the partition can grow exponentially in the time horizon and so much of the current research involves approximation algorithms [ Lovejoy, 1991, White and Scherer, 1989 ] .
Reference: [ White and Scherer, 1989 ] <author> White, Chelsea C. III and Scherer, William T. </author> <year> 1989. </year> <title> Solution procedures for partially observed markov decision processes. </title> <journal> Operations Research 37(5) </journal> <pages> 791-797. </pages>
Reference-contexts: In the worst case, the number of regions in the partition can grow exponentially in the time horizon and so much of the current research involves approximation algorithms <ref> [ Lovejoy, 1991, White and Scherer, 1989 ] </ref> . Integration into our approach The above discussion was meant to provide some evidence that our basic approach can be extended to take into account uncertainty in obser 38 vation.
Reference: [ Wilkins, 1988 ] <author> Wilkins, David E. </author> <year> 1988. </year> <title> Practical Planning: Extending the Classical AI Planning Paradigm. </title> <publisher> Morgan-Kaufmann, </publisher> <address> Los Altos, California. </address> <month> 42 </month>
Reference-contexts: It is often the case, however, that an execution error will move the world to a situation that has not been previously considered by the planner. Many systems (sipe, for example <ref> [ Wilkins, 1988 ] </ref> ) can monitor for plan "failures" and initiate replanning. Replanning is often too slow to be useful in time-critical domains, however.
References-found: 21

