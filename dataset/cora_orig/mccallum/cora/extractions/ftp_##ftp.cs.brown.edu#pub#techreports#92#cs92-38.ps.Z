URL: ftp://ftp.cs.brown.edu/pub/techreports/92/cs92-38.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-92-38.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [AAC] <author> A. Aggarwal, B. Alpern, A. K. Chandra, and M. Snir, </author> <title> "A Model for Hierarchical Memory," </title> <booktitle> Proceedings of 19th Annual ACM Symposium on Theory of Computing (May 1987), </booktitle> <pages> 305-314. </pages>
Reference-contexts: on a CRCW PRAM with P processors, is shown in Figure 2b for the special case P = D, although we allow P to be as large as M . 2.2 Parallel multilevel hierarchies The simplest multilevel hierarchy model is the Hierarchical Memory Model (HMM) proposed by Aggarwal et al. <ref> [AAC] </ref>, depicted in Figure 3a. In the HMM f (x) model, access to memory location x takes f (x) time.
Reference: [ACSa] <author> A. Aggarwal, A. Chandra, and M. Snir, </author> <title> "Hierarchical Memory with Block Transfer," </title> <booktitle> Proceedings of 28th Annual IEEE Symposium on Foundations of Computer Science (October 1987), </booktitle> <pages> 204-216. 31 </pages>
Reference-contexts: An elaboration of HMM is the Block Transfer (BT) model of Aggarwal et al. <ref> [ACSa] </ref>, depicted schematically in Figure 3b. Like HMM, it has a cost function f (x), but additionally it simulates the effect of block transfer by allowing the ` + 1 locations x `; . . . ; x to be accessed for cost f (x) + `. <p> The only difference is that in Algorithm 1, we need to add another line right after step (6) to reposition all the buckets into consecutive locations on each virtual memory hierarchy. This repositioning is done on a virtual-hierarchy-by-virtual-hierarchy basis, using the generalized matrix transposition algorithm given in <ref> [ACSa] </ref>. We concentrate in this section on the cost function f (x) = x ff , where 0 &lt; ff &lt; 1. Deterministic algorithms for f (x) = x ff with ff 1 have been reported previously [ViSa]. <p> We need to make one more change to the algorithm for BT hierarchies, but one that is hard to write explicitly. Aggarwal et al. gave an algorithm called the "touch" algorithm <ref> [ACSa] </ref>. This algorithm takes an array of n consecutive records stored at the lowest possible level and passes them through the base memory level in order, using time O (n log log n) for 0 &lt; ff &lt; 1.
Reference: [AgV] <author> A. Aggarwal and J. S. Vitter, </author> <title> "The Input/Output Complexity of Sorting and Related Problems," </title> <journal> Communications of the ACM (September 1988), </journal> <pages> 1116-1127. </pages>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel [GHK, GiS, Jil, Mag, PGK, Uni]. Aggarwal and Vitter did the initial work in the use of parallel block transfer for sorting <ref> [AgV] </ref>. In their model, they considered the parameters N = # records in the file M = # records that can fit in internal memory B = # records per block D = # blocks transferred per I=O (# of disks) where M &lt; N , and 1 DB M=2. <p> Thus, D blocks can be transferred per I/O, as in the <ref> [AgV] </ref> model, but only if no two blocks access the same disk. This assumption is very reasonable in light of the way real systems are constructed. Vitter and Shriver presented a randomized version of distribution sort in the D-disk model using two complementary partitioning techniques. <p> This assumption is very reasonable in light of the way real systems are constructed. Vitter and Shriver presented a randomized version of distribution sort in the D-disk model using two complementary partitioning techniques. Their algorithm meets the I/O lower bound (1) for the more lenient model of <ref> [AgV] </ref>, and thus the algorithm is optimal. The difficulty in implementing a version of distribution sort that works on a set of D parallel disks is making sure that each of the buckets can be read efficiently in parallel. <p> We also improve upon the deterministic Greed Sort algorithm in [NoVb], which was optimal only for the parallel disk models and could not be used optimally for hierarchical memories. The lower bounds are proved in [ViSa] (Theorems 1 and 2) and <ref> [AgV] </ref> (Theorem 3). Theorem 1 In the P-HMM model, the time for sorting is fi N log N log log N !! fi N ff+1 N log N if f (x) = x ff , ff &gt; 0. <p> Thus, we get the recurrence T (N ) = ST N 0 if N M which has solution T (N) = O N log S M = O N log (N=B) ! This is the same bound as was shown to be optimal for the parallel disk model <ref> [AgV] </ref>. We devote the remainder of this section to showing that the algorithm operates with optimal internal processing time as long as the number of processors P M log minfM=B; log Mg= log M . The optimal internal processing time (assuming comparison-based sorting) is ((N=P ) log N ).
Reference: [ACF] <author> B. Alpern, L. Carter, and E. Feig, </author> <title> "Uniform Memory Hierarchies," </title> <booktitle> Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science (October 1990), </booktitle> <pages> 600-608. </pages>
Reference-contexts: A possibly more realistic memory hierarchy is the Uniform Memory Hierarchy (UMH) of Alpern et al. <ref> [ACF] </ref>, depicted in Figure 3c.
Reference: [BFP] <author> M. Blum, R. Floyd, V. Pratt, R. Rivest, and R. E. Tarjan, </author> <title> "Time Bounds for Selection," </title> <editor> J. </editor> <booktitle> Computer and System Sciences 7 (1973), </booktitle> <pages> 448-461. </pages>
Reference-contexts: t := djS 0 j=M e Read M elements in parallel (dM=DBe tracks; the last may be partial) Sort internally R i := the median element if t = 1 return (kth element) s := the median of R 1 ; . . . ; R t using algorithm from <ref> [BFP] </ref> Partition into two sets S 1 and S 2 such that S 1 &lt; s and S 2 s k 1 := jS 1 j if k k 1 Select (S 1 ; k) else Select (S 2 ; k k 1 ) Algorithm 10 [ComputePartitionElements (S)] for each memoryload
Reference: [Col] <author> Richard Cole, </author> <title> "Parallel Merge Sort," </title> <note> SIAM J. Computing 17 (August 1988), 770-785. </note>
Reference-contexts: Proof : To group the records within each bucket together, we sort the records in internal memory. If log M = O (log M=B) then we can sort the M records using Cole's parallel merge sort algorithm <ref> [Col] </ref> in time (M=P ) log M = O ((M=P ) log (M=B)) for any P M . Otherwise, instead of sorting using the keys contained in the records, we sort using the bucket number as the key. We will start by considering only P M= log M .
Reference: [Flo] <author> R. W. Floyd, </author> <title> "Permuting Information in Idealized Two-Level Storage," in Complexity of Computer Computations, </title> <editor> R. Miller and J. Thatcher, ed., </editor> <publisher> Plenum, </publisher> <year> 1972, </year> <pages> 105-109. </pages>
Reference-contexts: In each I/O, D blocks of B records can be transferred simultaneously, as illustrated in Figure 1. Their measure of performance is the number of parallel I/Os required; They ignore internal computation time. This model generalized the initial work on I/O of Floyd <ref> [Flo] </ref> and Hong and Kung [HoK]. Aggarwal and Vitter proved that the average-case and worst-case number of I/Os required for sorting is 1 fi N log (N=B) ! All logarithms in this paper are base 2 logarithms.
Reference: [GHK] <author> G. Gibson, L. Hellerstein, R. M. Karp, R. H. Katz, and D. A. Patterson, </author> <title> "Coding Techniques for Handling Failures in Large Disk Arrays," </title> <editor> U. C. </editor> <address> Berkeley, UCB/CSD 88/477, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did the initial work in the use of parallel block transfer for sorting [AgV].
Reference: [GiS] <author> D. Gifford and A. Spector, </author> <title> "The TWA Reservation System," </title> <booktitle> Communications of the ACM 27 (July 1984), </booktitle> <pages> 650-665. </pages>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did the initial work in the use of parallel block transfer for sorting [AgV].
Reference: [GoS] <author> Mark Goldberg and Thomas Spencer, </author> <title> "Constructing a Maximal Independent Set in Parallel," </title> <journal> SIAM J. Discrete Math 2, </journal> <pages> 322-328. </pages>
Reference-contexts: even the fastest known deterministic parallel algorithm for maximal matching with n items is O (log 2 n) with a quartic number of processors for dense graphs (which we have) [Luba], or O (log 3 n) with a quadratic number of processors [IsS] or n 2 = log n processors <ref> [GoS] </ref>. Since we have n = H 0 , this means that the fastest known algorithm is fi (log 2 H), and still doesn't work since we have only H = fi ((H 0 ) 2 log 2 H 0 ) processors.
Reference: [HoK] <author> J. W. Hong and H. T. Kung, </author> <title> "I/O Complexity: The Red-Blue Pebble Game," </title> <booktitle> Proc. of the 13th Annual ACM Symposium on the Theory of Computing (May 1981), </booktitle> <pages> 326-333. </pages>
Reference-contexts: In each I/O, D blocks of B records can be transferred simultaneously, as illustrated in Figure 1. Their measure of performance is the number of parallel I/Os required; They ignore internal computation time. This model generalized the initial work on I/O of Floyd [Flo] and Hong and Kung <ref> [HoK] </ref>. Aggarwal and Vitter proved that the average-case and worst-case number of I/Os required for sorting is 1 fi N log (N=B) ! All logarithms in this paper are base 2 logarithms.
Reference: [IsS] <author> A. Israeli and Y. Shiloach, </author> <title> "An Improved Parallel Algorithm for Maximal Matching," </title> <booktitle> Information Processing Letters 22 (1986), </booktitle> <pages> 57-60. </pages>
Reference-contexts: Unfortunately, even the fastest known deterministic parallel algorithm for maximal matching with n items is O (log 2 n) with a quartic number of processors for dense graphs (which we have) [Luba], or O (log 3 n) with a quadratic number of processors <ref> [IsS] </ref> or n 2 = log n processors [GoS]. Since we have n = H 0 , this means that the fastest known algorithm is fi (log 2 H), and still doesn't work since we have only H = fi ((H 0 ) 2 log 2 H 0 ) processors.
Reference: [Jil] <author> W. Jilke, </author> <title> "Disk Array Mass Storage Systems: The New Opportunity," </title> <publisher> Amperif Corporation, </publisher> <month> September </month> <year> 1986. </year>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did the initial work in the use of parallel block transfer for sorting [AgV].
Reference: [Lei] <author> T. Leighton, </author> <title> "Tight Bounds on the Complexity of Parallel Sorting," </title> <journal> IEEE Transactions on Computers C-34 (April 1985), </journal> <pages> 344-354, </pages> <booktitle> also appears in Proceedings of the 16th Annual ACM Symposium on Theory of Computing, </booktitle> <month> (April </month> <year> 1983), </year> <pages> 71-80. </pages>
Reference-contexts: The parallel disk version of the algorithm also improves upon our previously published algorithms [NoVa, ViN] in the following ways: 1. The hidden constants in the big-O notation are small. The problem with the Greed Sort algorithm of [NoVb] is that it uses Columnsort <ref> [Lei] </ref> as a subroutine, which introduces at least an additional factor of 4 into the constant of proportionality. 2. The algorithm can operate using only striped write operations.
Reference: [Luba] <author> Michael Luby, </author> <title> "A Simple Parallel Algorithm for the Maximal Independent Set Problem," </title> <journal> SIAM J. Computing 15 (1986), </journal> <pages> 1036-1053. </pages>
Reference-contexts: Unfortunately, even the fastest known deterministic parallel algorithm for maximal matching with n items is O (log 2 n) with a quartic number of processors for dense graphs (which we have) <ref> [Luba] </ref>, or O (log 3 n) with a quadratic number of processors [IsS] or n 2 = log n processors [GoS]. <p> But we know that k bH 0 =3c, so that the constant is at least 1=2. Hence, in expected O (log H 0 ) time, we can match all the vertices on the left. 2 This algorithm can be derandomized in an efficient way using the techniques of Luby <ref> [Luba, Lubb] </ref>. First, notice that we have H processors available instead of only H 0 . When H 0 = p H = log H, that means that H = fi ((H 0 ) 2 log 2 H 0 ).
Reference: [Lubb] <author> Michael G. Luby, </author> <title> "Removing Randomness in a Parallel Computation Without a Processor Penalty," </title> <booktitle> International Computer Science Institute, </booktitle> <address> TR-89-044, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: But we know that k bH 0 =3c, so that the constant is at least 1=2. Hence, in expected O (log H 0 ) time, we can match all the vertices on the left. 2 This algorithm can be derandomized in an efficient way using the techniques of Luby <ref> [Luba, Lubb] </ref>. First, notice that we have H processors available instead of only H 0 . When H 0 = p H = log H, that means that H = fi ((H 0 ) 2 log 2 H 0 ).
Reference: [Mag] <author> N. B. Maginnis, </author> <title> "Store More, Spend Less: Mid-Range Options Around," Com-puterworld (November 16, </title> <booktitle> 1986), </booktitle> <volume> 71. </volume> <pages> 32 </pages>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did the initial work in the use of parallel block transfer for sorting [AgV].
Reference: [NoVa] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Optimal Deterministic Sorting on Parallel Disks," </title> <institution> Department of Computer Science, Brown University, CS-92-08, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: That algorithm was based on merge sort, and was therefore not obviously applicable to finding an optimal deterministic algorithm for parallel memory hierarchies, since there is no known way of using merge sort optimally on even a single memory hierarchy. The companion paper <ref> [NoVa] </ref> described Balance Sort, the first known optimal and deterministic sorting algorithm based on distribution sort. Balance Sort is optimal for parallel disk sorting, both in terms of the number of I/O steps and in terms of the amount of internal processing. <p> This algorithm improves upon the randomized algorithms of Vitter and Shriver [ViSa]. The algorithm applies to P-HMM, P-BT, P-UMH, P-RUMH, and P-SUMH, as well as the parallel disk model. The parallel disk version of the algorithm also improves upon our previously published algorithms <ref> [NoVa, ViN] </ref> in the following ways: 1. The hidden constants in the big-O notation are small. The problem with the Greed Sort algorithm of [NoVb] is that it uses Columnsort [Lei] as a subroutine, which introduces at least an additional factor of 4 into the constant of proportionality. 2.
Reference: [NoVb] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Greed Sort: An Optimal External Sorting Algorithm for Multiple Disks," </title> <institution> Brown University, CS-90-04, </institution> <month> February </month> <year> 1990, </year> <title> also appears in shortened form in "Large-Scale Sorting in Parallel Memories," </title> <booktitle> Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Hilton Head, SC (July 1991), </address> <pages> 29-39. </pages>
Reference-contexts: Merge sort combined with disk striping is deterministic, but the number of I/Os used can be much larger than optimal, by a multiplicative factor of log (M=B). The question posed by Vitter and Shriver was answered in the affirmative by Nodine and Vitter using an algorithm called Greed Sort <ref> [NoVb] </ref>. That algorithm was based on merge sort, and was therefore not obviously applicable to finding an optimal deterministic algorithm for parallel memory hierarchies, since there is no known way of using merge sort optimally on even a single memory hierarchy. <p> In particular we get deterministic (as well as more practical) versions of the optimal randomized algorithms based on [ViSa]and [ViN]. We also improve upon the deterministic Greed Sort algorithm in <ref> [NoVb] </ref>, which was optimal only for the parallel disk models and could not be used optimally for hierarchical memories. The lower bounds are proved in [ViSa] (Theorems 1 and 2) and [AgV] (Theorem 3). <p> The parallel disk version of the algorithm also improves upon our previously published algorithms [NoVa, ViN] in the following ways: 1. The hidden constants in the big-O notation are small. The problem with the Greed Sort algorithm of <ref> [NoVb] </ref> is that it uses Columnsort [Lei] as a subroutine, which introduces at least an additional factor of 4 into the constant of proportionality. 2. The algorithm can operate using only striped write operations.
Reference: [PGK] <author> D. A. Patterson, G. Gibson, and R. H. Katz, </author> <title> "A Case for Redundant Arrays of Inexpensive Disks (RAID)," </title> <booktitle> Proceedings ACM SIGMOD Conference (June 1988), </booktitle> <pages> 109-116. </pages>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did the initial work in the use of parallel block transfer for sorting [AgV].
Reference: [RaR] <author> S. Rajasekaran and J. Reif, </author> <title> "Optimal and Sublogarithmic Time Randomized Parallel Sorting Algorithms," </title> <journal> SIAM J. Computing 18 (1989), </journal> <pages> 594-607. </pages>
Reference-contexts: Otherwise, instead of sorting using the keys contained in the records, we sort using the bucket number as the key. We will start by considering only P M= log M . We will make use of a deterministic algorithm by Rajasekaran and Reif that appears as Lemma 3.1 in <ref> [RaR] </ref>. Their algorithm sorts n elements in the range 1; . . . ; log n in time O (log n) using n= log n processors. We need to consider two cases: Case 1: M=B log M . In this case, we let n = M .
Reference: [Uni] <institution> University of California at Berkeley, "Massive Information Storage, Management, and Use (NSF Institutional Infrastructure Proposal)," </institution> <note> Technical Report No. UCB/CSD 89/493, </note> <month> January </month> <year> 1989. </year>
Reference-contexts: An increasingly popular way to get further speedup is to use many disk drives working in parallel <ref> [GHK, GiS, Jil, Mag, PGK, Uni] </ref>. Aggarwal and Vitter did the initial work in the use of parallel block transfer for sorting [AgV].
Reference: [ViSa] <author> J. S. Vitter and E. A. M. Shriver, </author> <title> "Optimal Disk I/O with Parallel Block Transfer," </title> <booktitle> Proceedings of the 22nd Annual ACM Symposium on Theory of Computing (May 1990), </booktitle> <pages> 159-169. </pages>
Reference-contexts: Vitter and Shriver considered a more realistic D-disk model , in which the secondary storage is partitioned into D physically distinct disk drives <ref> [ViSa] </ref>, as in Figure 2a. (Note that each head of a multi-head drive could count as a distinct disk in this definition, as long as each could operate independently of the other heads on the drive.) In each I/O operation, each of the D disks can simultaneously transfer one block of <p> (as well as more practical) versions of the optimal randomized algorithms based on <ref> [ViSa] </ref>and [ViN]. We also improve upon the deterministic Greed Sort algorithm in [NoVb], which was optimal only for the parallel disk models and could not be used optimally for hierarchical memories. The lower bounds are proved in [ViSa] (Theorems 1 and 2) and [AgV] (Theorem 3). Theorem 1 In the P-HMM model, the time for sorting is fi N log N log log N !! fi N ff+1 N log N if f (x) = x ff , ff &gt; 0. <p> Although the array E = fe j g is listed as being returned from the routine, it is too large to fit into the base memory level, and is therefore implemented as a "global" variable. This algorithm is essentially identical to that given in <ref> [ViSa] </ref>. We show the following lemma, which will lead to an analysis of the effectiveness of the approximate partitioning elements in dividing the records into nearly equal-sized buckets. <p> Theorem 6 The algorithm given above is optimal for all well-behaved cost functions f (x). 20 Proof : This proof is essentially that of the corresponding theorem in <ref> [ViSa] </ref> for their randomized algorithm. Let T M;H (N) denote the average number of I/O steps the sorting algorithm does with an internal memory of size M , where we allow each hierarchy to move simultaneously, in a single I/O step, a record between internal memory and external memory. <p> We concentrate in this section on the cost function f (x) = x ff , where 0 &lt; ff &lt; 1. Deterministic algorithms for f (x) = x ff with ff 1 have been reported previously <ref> [ViSa] </ref>. <p> a solution. 2 We have now completed the proof for Theorem 2. 23 4.5 Sorting in P-UMH and relatives Vitter and Nodine gave upper and lower bound results for P-UMH, P-RUMH, and P-SUMH [ViN], some of which are were based on simulating the randomized P-HMM and P-BT algorithms given in <ref> [ViSa] </ref>. <p> We will likewise use partial striping to group D=D 0 disks together to make D 0 virtual disks, with D 0 = p D= log D. We also use a different method for computing the partitioning elements, described in <ref> [ViSa] </ref>. Finally, we will select S = M fl The procedure for finding the partitioning elements uses as a subroutine Algorithm 9, which computes the kth smallest of n elements in O (n=DB) I/Os. <p> Since there are overall dN=M e memoryloads, we find that the number of I/Os done per call to Balance Disks is O (N=DB). Vitter and Shriver showed that the partition elements can also be computed using O (N=DB) I/Os <ref> [ViSa] </ref>. Thus, we get the recurrence T (N ) = ST N 0 if N M which has solution T (N) = O N log S M = O N log (N=B) ! This is the same bound as was shown to be optimal for the parallel disk model [AgV]. <p> Therefore, the total time is O N log (N=B) ! P ! P as desired. 2 30 6 Conclusions In this report, we have described the first known deterministic algorithm for sorting optimally using parallel hierarchical memories. This algorithm improves upon the randomized algorithms of Vitter and Shriver <ref> [ViSa] </ref>. The algorithm applies to P-HMM, P-BT, P-UMH, P-RUMH, and P-SUMH, as well as the parallel disk model. The parallel disk version of the algorithm also improves upon our previously published algorithms [NoVa, ViN] in the following ways: 1. The hidden constants in the big-O notation are small.
Reference: [ViSb] <author> J. S. Vitter and E. A. M. Shriver, </author> <title> "Algorithms for Parallel Memory II: Hierarchical Multilevel Memories," </title> <institution> Brown University, CS-90-22, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: When N &lt; H 2 , the values of S and G are the same as in the P-HMM case, so the same proof holds as before. 2 As with the P-HMM algorithm, there are at most two additional levels of recursion after N H 2 . <ref> [ViSb] </ref> showed that two lists can be merged in P-BT using the same amount of time as that needed to touch all the elements. We need to make one more change to the algorithm for BT hierarchies, but one that is hard to write explicitly.
Reference: [ViN] <author> J.S. Vitter and Mark H. Nodine, </author> <title> "Large-Scale Sorting in Uniform Memory Hierarchies," Journal of Parallel and Distributed Computing (January 1993), also appears in shortened form in "Large-Scale Sorting in Parallel Memories," </title> <booktitle> Proc. 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Hilton Head, SC (July 1991), </address> <pages> 29-39. 33 </pages>
Reference-contexts: The restriction that allows only one bus to be active at a time is called the SUMH model. A less restrictive model that we proposed earlier <ref> [ViN] </ref> and which corresponds closely to currently used programming constructs, is the RUMH model, in which several buses are allowed to be active simultaneously if they are cooperating on a single logical block move operation. As with two-level hierarchies, multilevel hierarchies can be parallelized. <p> The modified Balance Sort algorithm we describe in the next section gives us optimal deterministic algorithms for all the models we consider. In particular we get deterministic (as well as more practical) versions of the optimal randomized algorithms based on [ViSa]and <ref> [ViN] </ref>. We also improve upon the deterministic Greed Sort algorithm in [NoVb], which was optimal only for the parallel disk models and could not be used optimally for hierarchical memories. The lower bounds are proved in [ViSa] (Theorems 1 and 2) and [AgV] (Theorem 3). <p> The upper bounds are given by a deterministic algorithm based on Balance Sort. The (N=H) log N terms in the lower bounds require the comparison model of computation. Our techniques can also be used to transform the randomized P-UMH algorithms of <ref> [ViN] </ref> into deterministic ones in our model, as we discuss in Section 4.5. 6 Theorem 3 The number of I/Os required for sorting N records in the parallel disk model is fi N log (N=B) ! The upper bound is given by a deterministic algorithm based on Balance Sort, which also <p> T (N) = H Proof : Substituting the solution above into Equation (5) demonstrates that it is a solution. 2 We have now completed the proof for Theorem 2. 23 4.5 Sorting in P-UMH and relatives Vitter and Nodine gave upper and lower bound results for P-UMH, P-RUMH, and P-SUMH <ref> [ViN] </ref>, some of which are were based on simulating the randomized P-HMM and P-BT algorithms given in [ViSa]. <p> This algorithm improves upon the randomized algorithms of Vitter and Shriver [ViSa]. The algorithm applies to P-HMM, P-BT, P-UMH, P-RUMH, and P-SUMH, as well as the parallel disk model. The parallel disk version of the algorithm also improves upon our previously published algorithms <ref> [NoVa, ViN] </ref> in the following ways: 1. The hidden constants in the big-O notation are small. The problem with the Greed Sort algorithm of [NoVb] is that it uses Columnsort [Lei] as a subroutine, which introduces at least an additional factor of 4 into the constant of proportionality. 2.
References-found: 25

