URL: http://cwis.kub.nl/~fdl/general/people/daelem/papers/cogsci93.ps
Refering-URL: http://ilk.kub.nl/~antalb/pubs-time.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: gillis@reks.uia.ac.be, durieux@reks.uia.ac.be  walter@kub.nl, antalb@kub.nl  
Title: Learnability and Markedness: Dutch Stress Assignment  
Author: Steven Gillisy, Walter Daelemansz, Gert Durieuxy, Antal van den Boschz 
Address: Universiteitsplein 1 B-2610 Wilrijk  P.O. Box 90153 NL-5000 LE Tilburg  
Affiliation: yNational Fund for Scientific Research (Belgium) Department of Linguistics, University of Antwerp  zInstitute for Language Technology and AI (ITK) Tilburg University  
Abstract: This paper investigates the computational grounding of learning theories developed within a metrical phonology approach to stress assignment. In current research the Principles and Parameters approach to learning stress is pervasive. We point out some inherent problems associated with this approach in learning the stress system of Dutch. The paper focuses on two specific aspects of the learning task: we empirically investigate the effect of input encodings on learnability, and we examine the possibility of a data-oriented approach as an alternative to the Principles and Parameters approach. We show that a data-oriented similarity-based machine learning technique (Instance-Based Learning), working on phonemic input encodings is able to learn metrical phonology abstractions based on concepts like syllable weight, and that in addition, it is able to extract generalizations which cannot be expressed within a metrical framework. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler, D. and Albert, M. </author> <year> 1991. </year> <title> Instance-Based Learning Algorithms. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 37-66. </pages>
Reference-contexts: For instance, the word nirvana was encoded as follows: Encoding-1 Encoding-2 Syllable Weight Nucleus Coda Antepenultimate 3 I r Penultimate 2 a - Final 2 a - The learning algorithm: Instance-Based Learning Instance-based learning <ref> (IBL, Aha et al., 1991) </ref> is a framework and methodology for incremental supervised machine learning. The distinguishing feature of IBL is the fact that no explicit abstractions are constructed on the basis of the training examples during the training phase.
Reference: <author> Chomsky, N. </author> <year> 1981. </year> <title> Principles and parameters in syntactic theory. </title> <editor> In Hornstein, N. and Lighfoot, D. eds. </editor> <title> Explanations in linguistics: The logical problem of language acquisition. </title> <publisher> London: Longman. </publisher> <pages> 32-75. </pages>
Reference-contexts: They all approach the learning problem from the angle of the `principles and parameters' theory <ref> (Chomsky 1981) </ref>. In this approach the learner comes to the task of language learning equipped with a priori knowledge incorporated in a universal grammar that constrains him to entertain only useful generalizations.
Reference: <author> Dresher, E. and Kaye, J. </author> <year> 1990. </year> <title> A computational learning model for metrical phonology. </title> <journal> Cognition 34 </journal> <pages> 137-195. </pages>
Reference-contexts: In this approach the learner comes to the task of language learning equipped with a priori knowledge incorporated in a universal grammar that constrains him to entertain only useful generalizations. It is assumed that universal grammar specifies a number of parameters relevant to the metrical domain <ref> (see Dresher & Kaye, 1990) </ref>. The computational models add a learning theory to the linguistic notion of universal grammar. <p> The most straightforward way to present stress assignment in Dutch is by reviewing the settings of the relevant metrical parameters <ref> (see Dresher & Kaye, 1990, Trommelen & Zonneveld, 1990) </ref>: P1 The word-tree is strong on the [Left/ Right] Right P2 Feet are [Binary/Unbounded] Binary P3 Feet are built from the [Left/Right] Right P4 Feet are strong on the [Left/Right] Left P5 Feet are quantity sensitive [Yes/No] Yes P6 Feet are quantity
Reference: <author> Gillis, S., W. Daelemans, G. Durieux, and A. van den Bosch. </author> <year> 1992. </year> <title> Exploring Artificial Learning Algorithms. </title> <address> APIL 71. </address>
Reference: <author> Gupta, P. and Touretzky, D. </author> <year> 1991. </year> <title> Connectionist models and linguistic theory: Investigations of stress systems in language. </title> <journal> Unpublished ms. </journal>
Reference-contexts: They dedicate a specialized module to determining if there exist obvious conflicts (such as the ones illustrated above for Dutch). Eventually, a brute force learner is invoked to deal with similar input. They also indicate that the set of parameters will undoubtedly have to be extended <ref> (see also Gupta & Touretzky, 1991) </ref>. But keeping the present set of parameters as sufficient, for the sake of the argument, a number of serious problems turn up when we try to analyze how a learner of Dutch might fix the values of the parameters.
Reference: <author> Nyberg, E. </author> <year> 1992. </year> <title> A non-deterministic, success-driven model of parameter setting in language acquisition. </title> <type> Unpublished PhD, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction Of Decision Trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages>
Reference-contexts: We only experimented with the similarity metric. We extended the metric proposed by Aha et al. (1991) with a technique for assigning a different importance to different features. Our approach to the problem of weighing the relative importance of features is based on the concept of Information Gain <ref> (IG, also used in learning inductive decision trees, Quinlan, 1986) </ref>. Results The algorithm attains an overall success rate of 81.2% for encoding-1 and 87.6% for encoding-2.
Reference: <author> Riesbeck, C. K. and Schank, R.S. </author> <year> 1987. </year> <title> Inside Case-Based Reasoning. </title> <address> Hillsdale: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Rumelhart, D.E., Hinton, G.E. and Williams, R.J. </author> <year> 1986. </year> <title> Learning Internal Representations by Error Propagation. </title> <editor> In D.E. Rumelhart and J.L. McClel-land (eds.), </editor> <booktitle> Parallel Distributed Processing: </booktitle> <volume> Vol. </volume> <pages> 2. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The performance of an IBL classifier crucially depends on the selection of training items to be kept in memory, 3 The same experiments were also performed with the Analogical Modeling algorithm (Skousen, 1989) and with the Backpropagation of Errors algorithm <ref> (Rumelhart et al., 1986) </ref>. A comparative analysis of the results of the three algorithms is reported in Gillis et al. (1992). and the similarity metric used. In the experiment reported in this paper, all training items were "remembered". We only experimented with the similarity metric.
Reference: <author> Skousen, R. </author> <year> 1989. </year> <title> Analogical Modeling of Language. </title> <publisher> Kluwer: Dordrecht. </publisher>
Reference-contexts: As such IBL shares an emphasis on `analogy' in language use with Skousen's theory of Analogical Modeling <ref> (Skousen, 1989) </ref> 3 . The operation of the basic algorithm is quite simple: for each pattern to be assigned a category (test item), it is checked whether this pattern has been encountered in the training set earlier. <p> The performance of an IBL classifier crucially depends on the selection of training items to be kept in memory, 3 The same experiments were also performed with the Analogical Modeling algorithm <ref> (Skousen, 1989) </ref> and with the Backpropagation of Errors algorithm (Rumelhart et al., 1986). A comparative analysis of the results of the three algorithms is reported in Gillis et al. (1992). and the similarity metric used. In the experiment reported in this paper, all training items were "remembered".
Reference: <author> Stanfill, C. and Waltz, D.L. </author> <year> 1986. </year> <title> Toward Memory-based Reasoning. </title> <journal> Communications of the ACM 29 </journal> <pages> 1213-1228. </pages>
Reference: <author> Trommelen, M. and Zonneveld, W. </author> <year> 1990. </year> <title> Stress In English And Dutch: A Comparison. </title> <address> DWPELL 17. </address>
Reference-contexts: The most straightforward way to present stress assignment in Dutch is by reviewing the settings of the relevant metrical parameters <ref> (see Dresher & Kaye, 1990, Trommelen & Zonneveld, 1990) </ref>: P1 The word-tree is strong on the [Left/ Right] Right P2 Feet are [Binary/Unbounded] Binary P3 Feet are built from the [Left/Right] Right P4 Feet are strong on the [Left/Right] Left P5 Feet are quantity sensitive [Yes/No] Yes P6 Feet are quantity
Reference: <author> Weiss, S. and Kulikowski, C. </author> <year> 1991. </year> <title> Computer Systems That Learn. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This computationally very costly method has as its major advantage that it provides the best possible estimate of the true error rate of a learning algorithm <ref> (Weiss & Kulikowski, 1991) </ref>. The data were encoded (i) as strings of syllable weights of the last three syllables of the word (encoding-1), and (ii) using the phonemic information contained in the rhyme projections of the last three syllables (encoding-2).
References-found: 13

