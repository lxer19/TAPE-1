URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1998/tr-98-015.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1998.html
Root-URL: http://www.icsi.berkeley.edu
Title: Incremental Class Learning approach and its application to Handwritten Digit Recognition  
Author: Jacek Mandziuk and Lokendra Shastri 
Note: Senior Fulbright  On leave from the  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  Berkeley.  00-661 Warsaw, Poland.  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  Scholar visiting ICSI and EECS Dept. University of California at  Institute of Mathematics, Warsaw University of Technology, Plac Politechniki 1,  
Pubnum: TR-98-015  
Email: e-mail: mandziuk@alpha.im.pw.edu.pl  
Phone: (510) 643-9153 FAX (510) 643-7684  
Date: June 1998  
Abstract: Incremental Class Learning (ICL) provides a feasible framework for the development of scalable learning systems. Instead of learning a complex problem at once, ICL focuses on learning subproblems incrementally, one at a time | using the results of prior learning for subsequent learning | and then combining the solutions in an appropriate manner. With respect to multi-class classification problems, the ICL approach presented in this paper can be summarized as follows. Initially the system focuses on one category. After it learns this category, it tries to identify a compact subset of features (nodes) in the hidden layers, that are crucial for the recognition of this category. The system then freezes these crucial nodes (features) by fixing their incoming weights. As a result, these features cannot be obliterated in subsequent learning. These frozen features are available during subsequent learning and can serve as parts of weight structures build to recognize other categories. As more categories are learned, the set of features gradually stabilizes and learning a new category requires less effort. Eventually, learning a new category may only involve combining existing features in an appropriate manner. The approach promotes the sharing of learned features among a number of categories and also alleviates the well-known catastrophic interference problem. We present results of applying the ICL approach to the Handwritten Digit Recognition problem, based on a spatio-temporal representation of patterns. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. E. Fahlman and C. Lebiere, </author> <title> "The Cascade-Correlation Learning Architecture", </title> <type> Technical Report: </type> <institution> CMU-CS-90-100, </institution> <year> 1990 </year>
Reference-contexts: There is, however, a significant difference between most constructive approaches and ICL. Typically, constructive methods (e.g. Cascade-Correlation <ref> [1] </ref> or Upstart Algorithm [2]) start with a minimal network and expand its structure by adding new nodes and links in order to minimize the overall network's error. In our approach, the network starts off with all the nodes and links it will ever have. <p> nodes and the winning nodes in the 2hl are updated in the following way: w 2 [i][win s ] := 1 [i] w 2 [i][win s ]) scale [s] ; (12) i = 0; : : : ; H1SIZE 1; s = 1; : : : ; 5; scale = <ref> [1; 2; 2; 3; 3] </ref>; where is a predefined learning rate 3 . The degree of weights change is scaled by coefficient scale based on the "winning position" of the winning node.
Reference: [2] <author> M. Frean, </author> <title> "The upstart algorithm: a method for constructing and training feed-forward neural networks", </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <pages> pp. 198-209, </pages> <year> 1990 </year> <month> 23 </month>
Reference-contexts: There is, however, a significant difference between most constructive approaches and ICL. Typically, constructive methods (e.g. Cascade-Correlation [1] or Upstart Algorithm <ref> [2] </ref>) start with a minimal network and expand its structure by adding new nodes and links in order to minimize the overall network's error. In our approach, the network starts off with all the nodes and links it will ever have. <p> nodes and the winning nodes in the 2hl are updated in the following way: w 2 [i][win s ] := 1 [i] w 2 [i][win s ]) scale [s] ; (12) i = 0; : : : ; H1SIZE 1; s = 1; : : : ; 5; scale = <ref> [1; 2; 2; 3; 3] </ref>; where is a predefined learning rate 3 . The degree of weights change is scaled by coefficient scale based on the "winning position" of the winning node.
Reference: [3] <author> Hou, </author> <title> Digital Document Processing, </title> <editor> J. </editor> <publisher> Willey & Sons, </publisher> <year> 1983 </year>
Reference-contexts: nodes and the winning nodes in the 2hl are updated in the following way: w 2 [i][win s ] := 1 [i] w 2 [i][win s ]) scale [s] ; (12) i = 0; : : : ; H1SIZE 1; s = 1; : : : ; 5; scale = <ref> [1; 2; 2; 3; 3] </ref>; where is a predefined learning rate 3 . The degree of weights change is scaled by coefficient scale based on the "winning position" of the winning node. <p> All links between the m-ol nodes in the CSM and in the RSM and the output layer nodes are equal to ff and fi, respectively. 3.3.1 Preprocessing Size normalization: Patterns, originally of various sizes, were normalized <ref> [3] </ref> by resizing from rectangle to square, sampling with a regular interval, comparing the normalized sum of activations of pixels in the surrounding square with the threshold value (= 0:5) and, if greater or equal than the threshold, setting the corresponding pixel in the resulting image to 1, or setting it
Reference: [4] <author> T. Kohonen, </author> <title> "Self-Organized Formation of Topologically Correct Feature Maps", </title> <journal> Biological Cybernetics, </journal> <volume> 43, </volume> <pages> pp. 59-69, </pages> <year> 1982 </year>
Reference-contexts: In the ICL method, subproblems are not learned independently and the structures learned for solving one subproblem are available for solving subsequent subproblems. Thus, unlike the modular approach, ICL allows considerable sharing of structure across subnetworks. The ICL approach resonates with the notion of competitive learning (cf. <ref> [8, 4] </ref>) and also longlife learning (cf. [11]) wherein learning new tasks becomes relatively easier when the number of tasks that have already been learned increases. The current system implementation does not involve any relearning.
Reference: [5] <author> J. L. McClelland, B. L. McNaughton and R. C. O'Reilly, </author> <title> "Why there are Complementary Learning Systems in the Hippocampus and Neocortex: Insights from the Successes and Failures of Connectionist Models of Learning and Memory", </title> <type> Technical Report: </type> <institution> PDP.CNS.94.1, </institution> <year> 1994 </year>
Reference-contexts: 1 Introduction The catastrophic interference problem <ref> [6, 5, 7] </ref> is one of the greatest impediments in building large, scalable learning systems based on neural networks. <p> The current system implementation does not involve any relearning. However, it is possible to include some form of interleaved learning <ref> [5] </ref> within the ICL paradigm. 3 ICL application to Handwritten Digit Recognition prob lem In this section we present the application of the proposed ICL method to the handwritten digit recognition (HDR) problem.
Reference: [6] <author> M. McCloskey and N. J. Cohen, </author> <title> "Catastrophic interference in connectionist networks: The sequential learning problem", </title> <editor> In G. Bower (Ed.) </editor> <booktitle> The psychology of learning and motivation, </booktitle> <volume> 24, </volume> <pages> pp. 109-165, </pages> <address> 1989, </address> <publisher> Academic Press </publisher>
Reference-contexts: 1 Introduction The catastrophic interference problem <ref> [6, 5, 7] </ref> is one of the greatest impediments in building large, scalable learning systems based on neural networks.
Reference: [7] <author> E. Pessa and M. P. Pennaand, </author> <title> "Catastrophic Interference in Learning Process by Neural Networks", </title> <booktitle> Proc. of the ICANN'94, </booktitle> <address> Sorrento, Italy, </address> <pages> pp. 589-592, </pages> <month> May </month> <year> 1994 </year>
Reference-contexts: 1 Introduction The catastrophic interference problem <ref> [6, 5, 7] </ref> is one of the greatest impediments in building large, scalable learning systems based on neural networks.
Reference: [8] <author> D. Rumelhart and D. Zipser, </author> <title> "Feature Discovery by Competitive Learning", </title> <journal> Cognitive Science, </journal> <volume> 9, </volume> <pages> pp. 75-112, </pages> <year> 1985 </year>
Reference-contexts: In the ICL method, subproblems are not learned independently and the structures learned for solving one subproblem are available for solving subsequent subproblems. Thus, unlike the modular approach, ICL allows considerable sharing of structure across subnetworks. The ICL approach resonates with the notion of competitive learning (cf. <ref> [8, 4] </ref>) and also longlife learning (cf. [11]) wherein learning new tasks becomes relatively easier when the number of tasks that have already been learned increases. The current system implementation does not involve any relearning.
Reference: [9] <author> L. Shastri, </author> <title> "Attribution Learning as a solution to the Catastrophic Interference Problem in Learning with Neural Nets". </title> <type> Working Paper, </type> <institution> International Computer Science Institute. </institution> <month> December </month> <year> 1994 </year>
Reference: [10] <author> L. Shastri and T. Fontaine, </author> <title> "Recognizing Handwritten Digit Strings Using Modular Spatio-temporal Connectionist Networks", </title> <journal> Connection Science, </journal> <volume> 7(3), </volume> <year> 1995, </year> <pages> pp. 211-235. </pages>
Reference-contexts: Consequently, the representational capacity of the whole network is available right from the very start of the learning process (as is the case of backpropagation nets). The ICL method also shares some common features with modular approaches (e.g. <ref> [12, 10] </ref>) in that the problem to be learned is divided into subproblems. In modular approaches, however, the subproblems are learned independently by separate modules, and then the solutions for the subproblems are combined to yield the solution for the initial problem. <p> the window being moved along the pattern. 3 system used to implement the ICL method, and discuss experimental results that provide evidence for the effectiveness of the ICL approach in solving classification problems such as HDR. 3.1 Spatio-temporal representation of patterns In the spatio-temporal representation used in this work (cf. <ref> [10] </ref>), one of the spatial dimensions of the pattern is replaced by the temporal dimension, and the two dimensional static pattern is converted into a sequence of signals produced by sliding a window over the pattern along a direction of scan (see F igure 1). <p> Therefore, the sliding window mechanism allows detection of local features with a much smaller number of links between the input layer and the first (hidden) layer of a network. For a detailed discussion of the advantages of the spatio-temporal approach refer to <ref> [10] </ref>. 3.2 System architecture The system is composed of two modules operating simultaneously and independently on the input data. One module performs scanning of the input pattern along columns and the other one along rows Figure 2.
Reference: [11] <author> S. Thrun and T.M. Mitchell, </author> <title> "Learning One More Thing", </title> <type> Technical Report: </type> <institution> CMU-CS-94-184, </institution> <year> 1994 </year>
Reference-contexts: Thus, unlike the modular approach, ICL allows considerable sharing of structure across subnetworks. The ICL approach resonates with the notion of competitive learning (cf. [8, 4]) and also longlife learning (cf. <ref> [11] </ref>) wherein learning new tasks becomes relatively easier when the number of tasks that have already been learned increases. The current system implementation does not involve any relearning.
Reference: [12] <author> A. Waibel, </author> <title> "Consonant Recognition by Modular Construction of Large Phonemic Time-Delay Neural Networks", </title> <editor> in D.S. Touretzky (ed.), </editor> <booktitle> Advances in Neural Information Processing Systems (NIPS) 1, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 215-223, </pages> <year> 1989 </year> <month> 24 </month>
Reference-contexts: Consequently, the representational capacity of the whole network is available right from the very start of the learning process (as is the case of backpropagation nets). The ICL method also shares some common features with modular approaches (e.g. <ref> [12, 10] </ref>) in that the problem to be learned is divided into subproblems. In modular approaches, however, the subproblems are learned independently by separate modules, and then the solutions for the subproblems are combined to yield the solution for the initial problem.
References-found: 12

