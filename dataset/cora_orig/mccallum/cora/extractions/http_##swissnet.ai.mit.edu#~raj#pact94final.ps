URL: http://swissnet.ai.mit.edu/~raj/pact94final.ps
Refering-URL: http://www-swiss.ai.mit.edu/~raj/index.html
Root-URL: 
Title: Exploiting the Parallelism Exposed by Partial Evaluation  
Author: R. Surati a and A. Berlin b 
Keyword: Keyword Codes: Keywords: Parallel Compilation; Partial Evaluation; Parallel Instruction Scheduling; Fine-grain Parallelism  
Address: 545 Technology Square, Cambridge, MA 02139, USA  3333 Coyote Hill Road, Palo Alto, CA 94304, USA  
Affiliation: a MIT Artificial Intelligence Laboratory,  b Xerox Palo Alto Research Center,  
Abstract: We describe an approach to parallel compilation that seeks to harness the vast amount of fine-grain parallelism that is exposed through partial evaluation of numerically-intensive scientific programs. We have constructed a parallelizing compiler which uses partial evaluation to break down data abstractions and program structure, producing huge basic blocks that contain large amounts of fine-grain parallelism. To utilize this parallelism, we have developed a technique for automatically mapping the fine grain parallelism onto a coarser grain parallel computer architecture. We selectively group the fine-grain operations together so as to adjust the parallelism grain-size to match the inter-processor communication capabilities of the target architecture. On an important scientific problem, code produced by our compiler for the Supercomputer Toolkit parallel computer runs 6.2 times faster on eight processors than on one. For an important class of scientific applications, the coupling of partial evaluation with static scheduling techniques eliminates the need to require programmers to obscure programs by manually exposing the parallelism implicit in a computation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Surati, </author> <title> "A Parallelizing Compiler Based on Partial Evaluation", </title> <institution> MIT Artificial Intelligence Laboratory Technical Report TR-1377, </institution> <month> July </month> <year> 1992 </year>
Reference-contexts: A detailed discussion of the heuristics used by the region-level scheduler is presented in <ref> [1] </ref>. computational load was spread so evenly that the variation in utilization efficiency among the 8 processors was only one percent. The final phase of the compilation process is instruction-level scheduling.
Reference: [2] <author> A. </author> <title> Berlin, "A compilation strategy for numerical programs based on partial evaluation," </title> <institution> MIT Artificial Intelligence Laboratory Technical Report TR-1144, </institution> <address> Cambridge, MA., </address> <month> July </month> <year> 1989. </year>
Reference-contexts: Others have developed special-purpose hardware that parallelizes the 9-body problem by dedicating one processor per planet.[16] Previous work in partial evaluation <ref> [2, 4, 3] </ref> has shown that the 9-body problem contains large amounts of fine-grain parallelism, suggesting that more subtle parallelizations are possible without the need to dedicate one processor to each planet.
Reference: [3] <author> A. Berlin and D. Weise, </author> <title> "Compiling Scientific Code using Partial Evaluation," </title> <note> IEEE Computer December 1990. </note>
Reference-contexts: 1 Introduction Previous work has shown that partial evaluation is good at breaking down data abstraction and exposing underlying fine-grain parallelism in a program <ref> [3] </ref>. We have written a novel compiler which couples partial evaluation with static scheduling techniques to exploit this fine-grain parallelism by automatically mapping it onto a coarse-grain parallel architecture. <p> conditional branches, table lookups, loop iterations, and even some numerical operations may be performed in advance, at compile time, leaving only the underlying numerical operations to be performed at run time Our compiler exposes fine-grain parallelism using a simple partial evaluation strategy based on a symbolic execution technique described in <ref> [4, 3] </ref>. 1 Despite this technique's simplicity, it works well at exposing fine-grain parallelism. <p> Others have developed special-purpose hardware that parallelizes the 9-body problem by dedicating one processor per planet.[16] Previous work in partial evaluation <ref> [2, 4, 3] </ref> has shown that the 9-body problem contains large amounts of fine-grain parallelism, suggesting that more subtle parallelizations are possible without the need to dedicate one processor to each planet.
Reference: [4] <author> A. </author> <title> Berlin, "Partial Evaluation Applied to Numerical Computation," </title> <booktitle> Proc. 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <address> Nice, France, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: conditional branches, table lookups, loop iterations, and even some numerical operations may be performed in advance, at compile time, leaving only the underlying numerical operations to be performed at run time Our compiler exposes fine-grain parallelism using a simple partial evaluation strategy based on a symbolic execution technique described in <ref> [4, 3] </ref>. 1 Despite this technique's simplicity, it works well at exposing fine-grain parallelism. <p> Others have developed special-purpose hardware that parallelizes the 9-body problem by dedicating one processor per planet.[16] Previous work in partial evaluation <ref> [2, 4, 3] </ref> has shown that the 9-body problem contains large amounts of fine-grain parallelism, suggesting that more subtle parallelizations are possible without the need to dedicate one processor to each planet.
Reference: [5] <author> H. Abelson, A. Berlin, J. Katzenelson, W. McAllister, G. Rozas, G.J. Sussman, and J. </author> <title> Wisdom "The Supercomputer Toolkit: A general framework for special-purpose computing", </title> <journal> International Journal of High-Speed Electronics, </journal> <volume> vol. 3, no. 3, </volume> <year> 1992, </year> <pages> pp. 337-361. </pages>
Reference-contexts: The target architecture of our compiler is the Supercomputer Toolkit , a parallel processor consisting of eight independent VLIW processors connected to each other by two shared communication busses <ref> [5] </ref>. Performance measurements of actual compiled programs running on the Supercomputer Toolkit show that the code produced by our compiler for an important astrophysics application [18] runs 6.2 times faster on an eight-processor system than does near-optimal code executing on a single processor.
Reference: [6] <author> H. Kasahara, H. Honda, and S. </author> <title> Narita "Parallel Processing of Near Fine Grain Tasks Using Static Scheduling on OSCAR", </title> <booktitle> Supercomputing 90, </booktitle> <pages> pp 856-864, </pages> <year> 1990 </year>
Reference: [7] <author> B. Kruatrachue and T. Lewis, </author> <title> "Grain Size Determination for Parallel Processing", </title> <journal> IEEE Software, </journal> <volume> Volume 5, No 1, </volume> <month> January </month> <year> 1988 </year>
Reference: [8] <author> B. Shirazi, M. Wang, and G. Pathak, </author> <title> "Analysis and Evaluation of Heuristic Methods for Static Task Scheduling.", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Volume 10, Number 3, </volume> <month> Nov </month> <year> 1990. </year>
Reference: [9] <author> E. Ruf and D. Weise, </author> <title> "Avoiding Redundant Specialization During Partial Evaluation" In Proceedings of the 1991 ACM SIGPLAN Symposium on Partial Evaluationand Semantics-Based Program Manipulation, </title> <address> New Haven, CN. </address> <month> June </month> <year> 1991. </year>
Reference-contexts: in Figure 2, when a value is used by only one instruction, the producer and consumer of that value may be grouped together to form a region, thereby ensuring that the scheduler will not place the 1 More complex partial evaluation strategies that address data-dependent computations may be found in <ref> [9, 11, 10] </ref>. 2 Specifically, one time-step of a 12th-order Stormer integration of the gravity-induced motion of a 9-body solar system. 3 The name region was chosen because we think of the grain size adjustment technique as identifying "regions" of locality within the data-flow graph. <p> Indeed, several Supercomputer Toolkit users have built code generation systems on top of our compiler that automatically generate complete programs, including data-dependent conditionals, invoking the partial evaluator to optimize the data-independent portions of the program. Recent work by Weise, Ruf, and Katz <ref> [9, 10] </ref> describes additional techniques for automating the partial-evaluation process across data-dependent branches. increases the throughput of the computation.
Reference: [10] <author> E. Ruf and D. Weise, </author> <title> "Opportunities for Online Partial Evaluation", </title> <type> Technical Report CSL-TR-92-516, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford, </institution> <address> CA. </address> <year> 1992. </year>
Reference-contexts: in Figure 2, when a value is used by only one instruction, the producer and consumer of that value may be grouped together to form a region, thereby ensuring that the scheduler will not place the 1 More complex partial evaluation strategies that address data-dependent computations may be found in <ref> [9, 11, 10] </ref>. 2 Specifically, one time-step of a 12th-order Stormer integration of the gravity-induced motion of a 9-body solar system. 3 The name region was chosen because we think of the grain size adjustment technique as identifying "regions" of locality within the data-flow graph. <p> Indeed, several Supercomputer Toolkit users have built code generation systems on top of our compiler that automatically generate complete programs, including data-dependent conditionals, invoking the partial evaluator to optimize the data-independent portions of the program. Recent work by Weise, Ruf, and Katz <ref> [9, 10] </ref> describes additional techniques for automating the partial-evaluation process across data-dependent branches. increases the throughput of the computation.
Reference: [11] <author> N. D. Jones, C. K. Gomard and P. Sestoft, </author> <title> Partial Evaluation and Automatic Program Generations Prentice Hall, </title> <year> 1993 </year>
Reference-contexts: in Figure 2, when a value is used by only one instruction, the producer and consumer of that value may be grouped together to form a region, thereby ensuring that the scheduler will not place the 1 More complex partial evaluation strategies that address data-dependent computations may be found in <ref> [9, 11, 10] </ref>. 2 Specifically, one time-step of a 12th-order Stormer integration of the gravity-induced motion of a 9-body solar system. 3 The name region was chosen because we think of the grain size adjustment technique as identifying "regions" of locality within the data-flow graph.
Reference: [12] <author> M. Lam, </author> <title> "A Systolic Array Optimizing Compiler." </title> <institution> Carnegie Mellon Computer Science Department Technical Report CMU-CS-87-187., </institution> <month> May, </month> <year> 1987. </year>
Reference-contexts: list-scheduling using critical-path based heuristics is very effective when the grain size of the instructions is well-matched to inter-processor communication bandwidth, we have found that in the case of limited bandwidth, a grain size adjustment phase is required to make the list-scheduling approach effective. 8 6.2 Software Pipelining Software Pipelining <ref> [12] </ref> optimizes a particular fixed size loop structure such that several iterations of the loop are started on different processors at constant intervals of time.
Reference: [13] <author> J. Ellis, Bulldog: </author> <title> A Compiler for VLIW Architectures, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: If the regions are made too large, communication bandwidth will be under utilized since the operations within a region do not transmit their results. 5 The heuristics used by the region-level scheduler are closely related to list-scheduling <ref> [13] </ref>. A detailed discussion of the heuristics used by the region-level scheduler is presented in [1]. computational load was spread so evenly that the variation in utilization efficiency among the 8 processors was only one percent. The final phase of the compilation process is instruction-level scheduling. <p> An interesting direction for future work would be to add trace-scheduling to our approach, to optimize across the data-dependent branches that occur at basic block boundaries. Most trace-scheduling based compilers use a variant of list-scheduling <ref> [13] </ref> to parallelize operations within an individual basic block.
Reference: [14] <author> J.A. Fisher, </author> <title> "Trace scheduling: A Technique for Global Microcode Compaction." </title> <journal> IEEE Transactions on Computers, </journal> <volume> Number 7, pp.478-490. </volume> <year> 1981. </year>
Reference-contexts: For instance, compilers for vector machines unroll loops to help fill vector registers. Other parallelization techniques include trace-scheduling, software pipelining, vectorizing, as well as static and dynamic scheduling of data-flow graphs. 6.1 Trace Scheduling Compilers that exploit fine-grain parallelism often employ trace-scheduling techniques <ref> [14] </ref> to guess which way a branch will go, allowing computations beyond the branch to occur in parallel with those that precede the branch.
Reference: [15] <author> G. Cybenko, J. Bruner, S. Ho, </author> <title> "Parallel Computing and the Perfect Benchmarks." Center for Supercomputing Research and Development Report 1191., </title> <month> November </month> <year> 1991 </year>
Reference-contexts: Programmer's write and rewrite their code until the parallelizer is able to automatically recognize and utilize the available parallelism. There are many utilities for doing this, some of which are discussed in <ref> [15] </ref>.
Reference: [16] <author> J. Applegate, M. Douglas, Y. Gursel, P. Hunter, C. Seitz, G.J. Sussman, </author> <title> "A Digital Orrery," </title> <journal> IEEE Trans. on Computers, </journal> <month> Sept. </month> <year> 1985. </year>
Reference: [17] <author> J. Miller, "Multischeme: </author> <title> A Parallel Processing System Based on MIT Scheme". </title> <institution> MIT Laboratory For Computer Science technical report no. TR-402. </institution> <month> September, </month> <year> 1987. </year>
Reference-contexts: The applications that have generated the most interest from the scientific community involve various integrations of the N-body gravitational attraction problem. 7 Parallelization of these integrations has been previously studied by Miller <ref> [17] </ref>, who parallelized the program by using futures to manually specify how parallel execution should be attained.
Reference: [18] <author> G. Sussman and J. </author> <title> Wisdom, </title> <booktitle> "Chaotic Evolution of the Solar System",Science, </booktitle> <volume> Volume 257, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: Performance measurements of actual compiled programs running on the Supercomputer Toolkit show that the code produced by our compiler for an important astrophysics application <ref> [18] </ref> runs 6.2 times faster on an eight-processor system than does near-optimal code executing on a single processor. <p> Figure 1 illustrates a parallelism profile analysis of the nine-body gravitational attraction problem of the type discussed in <ref> [18] </ref>. 2 Partial evaluation exposed so much low-level parallelism that in theory, parallel execution could speed up the computation by a factor of 69 over a uniprocessor. 3 Adjusting the Grain Size Searching for an optimal schedule for a program which exploits fine-grain parallelism is both computationally expensive and difficult to <p> would probably be best not to partially-evaluate some of the outermost loops! Our work focuses on achieving efficient parallel execution of the partially-evaluated segments of a program, leaving the decision of which portions of a program should be subjected to this compilation technique up to the programmer. 7 For instance, <ref> [18] </ref> describes results obtained using the Supercomputer Toolkit that prove that the solar system's dynamics are chaotic. 6 Related Work The use of partial evaluation to expose parallelism makes our approach to parallel compilation fundamentally different from the approaches taken by other compilers.
References-found: 18

