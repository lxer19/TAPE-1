URL: http://www.eecs.umich.edu/PPP/MICRO93.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Title: Predictability of Load/Store Instruction Latencies  
Author: Santosh G. Abraham, Rabin A. Sugumar, B. R. Rau, Rajiv Gupta Daniel Windheiser 
Address: 1501 Page Mill Road, Bldg. 3U-7 Ann Arbor, MI 48109-2122 Palo Alto, CA 94304  
Affiliation: EECS Department Hewlett Packard Laboratories The University of Michigan  
Abstract: Due to increasing cache-miss latencies, cache control instructions are being implemented for future systems. We study the memory referencing behavior of individual machine-level instructions using simulations of fully-associative caches under MIN replacement. Our objective is to obtain a deeper understanding of useful program behavior that can be eventually employed at optimizing programs and to motivate architectural features aimed at improving the efficacy of memory hierarchies. Our simulation results show that a very small number of load/store instructions account for a majority of data cache misses. Specifically, fewer than 10 instructions account for half the misses for six out of nine SPEC89 benchmarks. Selectively prefetching data referenced by a small number of instructions identified through profiling can reduce overall miss ratio significantly while only incurring a small number of unnecessary prefetches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Alpha Architecture Handbook Preliminary Edition. Digital Equipment Corporation, Maynard, </institution> <address> MA, </address> <year> 1992. </year>
Reference-contexts: In order to effectively support selective scheduling, systems should have instructions that permit the software to manage the cache, e.g., DEC Alpha <ref> [1] </ref>. We describe a set of source-cache or latency specifiers for loads that specifies the highest level in the memory hierarchy where the data is likely to be found [13].
Reference: [2] <author> G. R. Beck, D. W. L. Yen, and T. L. Anderson. </author> <title> The Cydra 5 mini-supercomputer: architecture and implementation. </title> <journal> The Journal of Supercomputing, </journal> 7(1/2):143-180, 1992. 
Reference-contexts: In each case, we present a general expression and a number for the particular choice of n = 128 and m = 32. First, consider a machine without a data cache representative of VLIW machines such as the Cydrome Cydra 5 <ref> [2, 21] </ref> or Multiflow TRACE family [9]. The ideal execution time assumes a schedule with a memory latency of 20 cycles. Since there is no cache, the cache miss penalty is zero and every reference is satisfied by main memory.
Reference: [3] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proc. of ASPLOS IV, </booktitle> <pages> pages 40-52, </pages> <year> 1991. </year>
Reference-contexts: Callahan and Porterfield [4] investigate the data cache behavior of scientific code using source-level instrumentation. For a suite of regular scientific applications, they show that a few source-level references contribute to most of the misses. In a later paper, Callahan and Porterfield <ref> [3] </ref> present techniques for prefetching to tolerate the latency of instructions that miss. They investigate both complete prefetching where all array accesses are prefetched, and selective prefetching where the compiler decides, based on the size of the cache, what data to prefetch.
Reference: [4] <author> D. Callahan and A. Porterfield. </author> <title> Data cache performance of supercomputer applications. </title> <booktitle> In Supercomputing '90, </booktitle> <pages> pages 564-572, </pages> <year> 1990. </year>
Reference-contexts: Even if load profiling is not feasible for production compiler use, the load profiling results may lead to compiler techniques that generate similar information. Related Work Several researchers have investigated prefetching to reduce cache miss stalls. Callahan and Porterfield <ref> [4] </ref> investigate the data cache behavior of scientific code using source-level instrumentation. For a suite of regular scientific applications, they show that a few source-level references contribute to most of the misses. <p> In this paper, we describe a framework for software cache control that subsumes prefetching. We illustrate the performance improvements obtainable through software cache control using the matrix multiply program. Unlike Callahan and Porterfield <ref> [4] </ref>, we investigate the caching behavior of machine-level instructions for irregular floating-point programs such as spice and integer programs such as gcc. We also evaluate the costs and benefits of prefetching using load/store profiling. Profile guided optimizations play an important role in code optimization.
Reference: [5] <author> P. Chang, S. Mahlke, W. Chen, and W.W. Hwu. </author> <title> Profile-guided automatic inline expansion for C programs. </title> <journal> Software-Practice and Experience, </journal> <volume> 22(5) </volume> <pages> 349-369, </pages> <year> 1992. </year>
Reference-contexts: Profiling has also been used in instruction cache management by McFarling [18] and by Hwu and Chang [11], register allocation by Chow and Hennessy [8], and in-lining by Chang et al <ref> [5] </ref>. This work proposes using load/store instruction profiling for instruction scheduling and cache management. MemSpy [16] is a profiling tool that can be used to identify source-level code segments and data structures with poor memory system performance.
Reference: [6] <author> T-F. Chen and J-L. Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In Proc. of ASPLOS V, </booktitle> <pages> pages 51-61, </pages> <year> 1992. </year>
Reference-contexts: Klaiber and Levy [15] and Mowry et al [20] also investigate selective prefetching using compiler techniques for scientific code. Mowry et al [20] combine prefetching with software pipelining and further investigate the interaction between blocking and prefetching. Chen and Baer <ref> [6] </ref> investigate hardware-based prefetching and moving loads within basic blocks statically to tolerate cache-miss latencies for some SPEC benchmarks. Chen et al [7] reduce the effect of primary cache latency by preloading all memory accesses for the SPEC benchmarks.

Reference: [8] <author> F. Chow and J. Hennessy. </author> <title> Register allocation by priority-based coloring. </title> <booktitle> In Proc. of the 1984 Symp. on Compiler Construction, </booktitle> <pages> pages 222-232, </pages> <year> 1984. </year>
Reference-contexts: Profiling has also been used in instruction cache management by McFarling [18] and by Hwu and Chang [11], register allocation by Chow and Hennessy <ref> [8] </ref>, and in-lining by Chang et al [5]. This work proposes using load/store instruction profiling for instruction scheduling and cache management. MemSpy [16] is a profiling tool that can be used to identify source-level code segments and data structures with poor memory system performance.
Reference: [9] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Pap-worth, and P. K. Rodman. </author> <title> A VLIW architecture for a trace scheduling compiler. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-37(8):967-979, </volume> <year> 1988. </year>
Reference-contexts: In each case, we present a general expression and a number for the particular choice of n = 128 and m = 32. First, consider a machine without a data cache representative of VLIW machines such as the Cydrome Cydra 5 [2, 21] or Multiflow TRACE family <ref> [9] </ref>. The ideal execution time assumes a schedule with a memory latency of 20 cycles. Since there is no cache, the cache miss penalty is zero and every reference is satisfied by main memory.
Reference: [10] <author> J. A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(7):478-490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: We also evaluate the costs and benefits of prefetching using load/store profiling. Profile guided optimizations play an important role in code optimization. Branch profiling has been used in scheduling multiple basic blocks together to increase instruction level parallelism in VLIW and superscalar machines by Fisher <ref> [10] </ref> and Hwu et al [12]. Profiling has also been used in instruction cache management by McFarling [18] and by Hwu and Chang [11], register allocation by Chow and Hennessy [8], and in-lining by Chang et al [5].
Reference: [11] <author> W. W. Hwu and P. P. Chang. </author> <title> Achieving high instruction cache performance with an optimizing compiler. </title> <booktitle> In Proc. of 16th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 242-251, </pages> <year> 1989. </year>
Reference-contexts: Branch profiling has been used in scheduling multiple basic blocks together to increase instruction level parallelism in VLIW and superscalar machines by Fisher [10] and Hwu et al [12]. Profiling has also been used in instruction cache management by McFarling [18] and by Hwu and Chang <ref> [11] </ref>, register allocation by Chow and Hennessy [8], and in-lining by Chang et al [5]. This work proposes using load/store instruction profiling for instruction scheduling and cache management.
Reference: [12] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bringmann, R. G. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lavery. </author> <title> The superblock: an effective technique for vliw and superscalar compilation. </title> <journal> J. of Supercomputing, </journal> 7(1/2):229-248, 1993. 
Reference-contexts: Profile guided optimizations play an important role in code optimization. Branch profiling has been used in scheduling multiple basic blocks together to increase instruction level parallelism in VLIW and superscalar machines by Fisher [10] and Hwu et al <ref> [12] </ref>. Profiling has also been used in instruction cache management by McFarling [18] and by Hwu and Chang [11], register allocation by Chow and Hennessy [8], and in-lining by Chang et al [5]. This work proposes using load/store instruction profiling for instruction scheduling and cache management.
Reference: [13] <author> V. Kathail, M. S. Schlansker, and B. R. Rau. </author> <title> HPL PlayDoh architecture specification: Version 1.0. </title> <type> Technical Report HPL-93-80, </type> <institution> Hewlett-Packard Laboratories, </institution> <year> 1993. </year> <note> in preparation. </note>
Reference-contexts: We describe a set of source-cache or latency specifiers for loads that specifies the highest level in the memory hierarchy where the data is likely to be found <ref> [13] </ref>. After an earlier phase of compilation has inserted the latency specifiers, the instruction scheduler preferentially schedules long-latency loads which tend to miss the cache and all other short-latency loads with the cache-hit latency. In a VLIW processor, the architectural latency of a load is determined using its latency specifier. <p> Superscalar processors can also use the latency specifiers for dynamic scheduling. We describe a set of target cache or level-direction specifiers for load/store instructions that install data at specific levels in the memory hierarchy <ref> [13] </ref>. Through these instructions, the software can manage the contents of caches at various levels in the memory hierarchy and potentially improve cache-hit ratios and/or improve the accuracy of latency specification. In selective scheduling, the compiler must tag each load with a latency specifier. <p> The main disadvantage of an explicitly managed local memory is that the software is forced to make all management decisions. We describe a mixed implicit-explicit managed cache that captures most of the advantages of the two extremes <ref> [13] </ref>. A mixed cache provides software mechanisms to explicitly control the cache as well as simple default hardware policies. The software mechanisms are utilized when the compiler has sufficient knowledge of the program's memory accessing behavior and when significant performance improvements are obtainable through software control. <p> Otherwise, the cache is managed using the default hardware policies. We now describe the load/store instructions in the HPL PlayDoh architecture 2 in the remainder of this section <ref> [13] </ref>. In addition to the standard load/store operations, the PlayDoh architecture provides explicit control over the memory hierarchy and supports prefetching of data to any level in the hierarchy. The PlayDoh memory system consists of the following: main memory, second-level cache, first-level cache and a first-level data prefetch cache.
Reference: [14] <author> D. R. Kerns and S. J. Eggers. </author> <title> Balanced scheduling: Instruction scheduling when memory latency is uncertain. </title> <booktitle> In Proc. of the SIGPLAN '93 Conf. on Prog. Lang. Design and Implementation, </booktitle> <pages> pages 278-289, </pages> <year> 1993. </year>
Reference-contexts: We have not addressed this issue in this paper. The choice of a threshold should be governed by a tradeoff between the cost of prefetches and the benefit arising from a lower miss ratio <ref> [14] </ref>. Lower residual miss ratios reduce cache miss penalties. The cost of prefetches is a function of both the number of prefetches and the amount of instruction level parallelism in the program. In a program with low instruction level parallelism, unnecessary prefetches constrain scheduling decisions and increase critical path lengths.
Reference: [15] <author> A. C. Klaiber and H. M. Levy. </author> <title> Architecture for software controlled data prefetching. </title> <booktitle> In Proc. of 18th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 43-63, </pages> <year> 1991. </year>
Reference-contexts: They investigate both complete prefetching where all array accesses are prefetched, and selective prefetching where the compiler decides, based on the size of the cache, what data to prefetch. McNiven and Davidson [19] evaluate the reductions in memory traffic obtainable through compiler-directed cache management. Klaiber and Levy <ref> [15] </ref> and Mowry et al [20] also investigate selective prefetching using compiler techniques for scientific code. Mowry et al [20] combine prefetching with software pipelining and further investigate the interaction between blocking and prefetching.
Reference: [16] <author> M. Martonosi, A. Gupta, and T. Anderson. Mem-spy: </author> <title> Analyzing memory system bottlenecks in programs. </title> <booktitle> In Proc. ACM SIGMETRICS Conf., </booktitle> <pages> pages 1-12, </pages> <year> 1992. </year>
Reference-contexts: Profiling has also been used in instruction cache management by McFarling [18] and by Hwu and Chang [11], register allocation by Chow and Hennessy [8], and in-lining by Chang et al [5]. This work proposes using load/store instruction profiling for instruction scheduling and cache management. MemSpy <ref> [16] </ref> is a profiling tool that can be used to identify source-level code segments and data structures with poor memory system performance. Our work focuses on opportunities for optimization of cache performance through profiling of machine-level load/store instructions. The rest of the paper is structured as follows.
Reference: [17] <author> R. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger. </author> <title> Evaluation techniques for storage hierarchies. </title> <journal> IBM Systems Journal, </journal> <volume> 9(2) </volume> <pages> 78-117, </pages> <year> 1970. </year>
Reference-contexts: We have confirmed that the general nature of the results apply for two-way set-associative caches [22]. We simulated the caches either for 100 million addresses or till completion of the program. We use the stack processing technique of Mattson et al <ref> [17] </ref> to simulate fully-associative caches under LRU and MIN replacements and we use tree implementations of the stack for efficiency. The stack represents the state of a range of cache sizes, with the top C lines being contained in a cache of size C lines.
Reference: [18] <author> S. McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> In Proc. of ASPLOS III, </booktitle> <year> 1989. </year>
Reference-contexts: Branch profiling has been used in scheduling multiple basic blocks together to increase instruction level parallelism in VLIW and superscalar machines by Fisher [10] and Hwu et al [12]. Profiling has also been used in instruction cache management by McFarling <ref> [18] </ref> and by Hwu and Chang [11], register allocation by Chow and Hennessy [8], and in-lining by Chang et al [5]. This work proposes using load/store instruction profiling for instruction scheduling and cache management.
Reference: [19] <author> D. M. McNiven. </author> <title> Reduction in Main Memory Traffic through the Efficient use of Local Memory. </title> <type> PhD thesis, </type> <institution> University of Illinois, </institution> <year> 1988. </year>
Reference-contexts: They investigate both complete prefetching where all array accesses are prefetched, and selective prefetching where the compiler decides, based on the size of the cache, what data to prefetch. McNiven and Davidson <ref> [19] </ref> evaluate the reductions in memory traffic obtainable through compiler-directed cache management. Klaiber and Levy [15] and Mowry et al [20] also investigate selective prefetching using compiler techniques for scientific code. Mowry et al [20] combine prefetching with software pipelining and further investigate the interaction between blocking and prefetching.
Reference: [20] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proc. of ASPLOS V, </booktitle> <pages> pages 62-73, </pages> <year> 1992. </year>
Reference-contexts: McNiven and Davidson [19] evaluate the reductions in memory traffic obtainable through compiler-directed cache management. Klaiber and Levy [15] and Mowry et al <ref> [20] </ref> also investigate selective prefetching using compiler techniques for scientific code. Mowry et al [20] combine prefetching with software pipelining and further investigate the interaction between blocking and prefetching. <p> McNiven and Davidson [19] evaluate the reductions in memory traffic obtainable through compiler-directed cache management. Klaiber and Levy [15] and Mowry et al <ref> [20] </ref> also investigate selective prefetching using compiler techniques for scientific code. Mowry et al [20] combine prefetching with software pipelining and further investigate the interaction between blocking and prefetching. Chen and Baer [6] investigate hardware-based prefetching and moving loads within basic blocks statically to tolerate cache-miss latencies for some SPEC benchmarks.
Reference: [21] <author> B. R. Rau, D. W. L. Yen, W. Yen, and R. A. Towle. </author> <title> The Cydra 5 departmental supercomputer: Design philosophies, decisions and trade-offs. </title> <journal> IEEE Computer, </journal> <volume> 22(1) </volume> <pages> 12-35, </pages> <year> 1989. </year>
Reference-contexts: In each case, we present a general expression and a number for the particular choice of n = 128 and m = 32. First, consider a machine without a data cache representative of VLIW machines such as the Cydrome Cydra 5 <ref> [2, 21] </ref> or Multiflow TRACE family [9]. The ideal execution time assumes a schedule with a memory latency of 20 cycles. Since there is no cache, the cache miss penalty is zero and every reference is satisfied by main memory.
Reference: [22] <author> R. A. Sugumar and S. G. Abraham. </author> <title> Multi-configuration simulation algorithms for the evaluation of computer architecture designs. </title> <type> Technical Report CSE-TR-173-93, </type> <institution> CSE Division, University of Michi-gan, </institution> <year> 1992. </year>
Reference-contexts: We have confirmed that the general nature of the results apply for two-way set-associative caches <ref> [22] </ref>. We simulated the caches either for 100 million addresses or till completion of the program. We use the stack processing technique of Mattson et al [17] to simulate fully-associative caches under LRU and MIN replacements and we use tree implementations of the stack for efficiency. <p> Cache replacement can be managed using the target cache specifiers described in Section 2 to reduce traffic between levels of the memory hierarchy. Our initial results on using profiling to manage cache contents show a reduction in traffic of less than 10% <ref> [22] </ref>. Compiler transformations such as peeling and loop unrolling can greatly improve the resolution of the profiling data. Intuition gathered from profiling may also be useful in developing compiler heuristics so that, thereafter, profiling is not needed while compiling.
References-found: 21

