URL: http://reality.sgi.com/ronnyk/vote.ps.gz
Refering-URL: http://www.cs.orst.edu/~margindr/ML_RG/fall97-mlrg.html
Root-URL: 
Title: An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants  
Author: ERIC BAUER RON KOHAVI Editors: Philip Chan, Salvatore Stolfo, and David Wolpert 
Keyword: Classification, Boosting, Bagging, Decision Trees, Naive-Bayes, Mean-squared error  
Address: Stanford CA. 94305  2011 N. Shoreline Blvd, Mountain View, CA. 94043  
Affiliation: Computer Science Department, Stanford University  Data Mining and Visualization, Silicon Graphics Inc.  
Note: Machine Learning, vv, 1?? (1998) c 1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands. Submitted  
Email: ebauer@cs.stanford.edu  ronnyk@engr.sgi.com  
Date: Received 3 Oct 1997; Revised 17 June 1998  
Abstract: Methods for voting classification algorithms, such as Bagging and AdaBoost, have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real-world datasets. We review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer (three variants) and a Naive-Bayes inducer. The purpose of the study is to improve our understanding of why and when these algorithms, which use perturbation, reweighting, and combination techniques, affect classification error. We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms. This allowed us to determine that Bagging reduced variance of unstable methods, while boosting methods (AdaBoost and Arc-x4) reduced both the bias and variance of unstable methods but increased the variance for Naive-Bayes, which was very stable. We observed that Arc-x4 behaves differently than AdaBoost if reweighting is used instead of resampling, indicating a fundamental difference. Voting variants, some of which are introduced in this paper, include: pruning versus no pruning, use of probabilistic estimates, weight perturbations (Wagging), and backfitting of data. We found that Bagging improves when probabilistic estimates in conjunction with no-pruning are used, as well as when the data was backfit. We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error. We compare the mean-squared error of voting methods to non-voting methods and show that the voting methods lead to large and significant reductions in the mean-squared errors. Practical problems that arise in implementing boosting algorithms are explored, including numerical instabilities and underflows. We use scatterplots that graphically show how AdaBoost reweights instances, emphasizing not only "hard" areas but also outliers and noise. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K. M. </author> <year> (1996), </year> <title> Learning Probabilistic Relational Concept Descriptions, </title> <type> PhD thesis, </type> <institution> University of California, Irvine. </institution> <note> http://www.ics.uci.edu/~ali. 32 ERIC BAUER AND RON KOHAVI Becker, </note> <author> B., Kohavi, R. & Sommerfield, D. </author> <year> (1997), </year> <title> Visualizing the simple bayesian classifier, in `KDD Workshop on Issues in the Integration of Data Mining and Data Visualization'. </title>
Reference: <author> Bernardo, J. M. & Smith, A. F. </author> <year> (1993), </year> <title> Bayesian Theory, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: We are not aware of anyone who reported any mean-squared errors results for voting algorithms in the past. Good probability estimates are crucial for applications when loss matrices are used <ref> (Bernardo & Smith 1993) </ref>, and the significant differences indicate that p-Bagging is a very promising approach. 8. Boosting Algorithms: AdaBoost and Arc-x4 We now discuss boosting algorithms. First, we explore practical considerations for boosting algorithm implementation, specifically numerical instabilities and underflows.
Reference: <author> Breiman, L. </author> <year> (1994), </year> <title> Heuristics of instability in model selection, </title> <institution> Technical Report Statistics Department, University of California at Berkeley. </institution>
Reference-contexts: For large m, this is about 1 1=e = 63:2%, which means that each bootstrap sample contains only about 63.2% unique instances from the training set. This perturbation causes different classifiers to be built if the inducer is unstable (e.g., neural networks, decision trees) <ref> (Breiman 1994) </ref> and the performance can improve if the induced classifiers are good and not correlated; however, Bagging may slightly degrade the performance of stable algorithms (e.g., k-nearest neighbor) because effectively smaller training sets are used for training each classifier (Breiman 1996b).
Reference: <author> Breiman, L. </author> <year> (1996a), </year> <title> Arcing classifiers, </title> <type> Technical report, </type> <institution> Statistics Department, University of California, Berkeley. </institution> <note> http://www.stat.Berkeley.EDU/users/breiman/. </note>
Reference-contexts: Ali (1996) provides a recent review of related algorithms, and additional recent work can be found in Stolfo (1996). Algorithms that adaptively change the distribution include AdaBoost (Freund & Schapire 1995) and Arc-x4 <ref> (Breiman 1996a) </ref>. Drucker & Cortes (1996) and Quinlan (1996) applied boosting to decision tree induction, observing both that error significantly decreases and that the generalization error does not degrade as more classifiers are combined. <p> Conclusions for Boosting The AdaBoost and Arc-x4 algorithms have different behavior than Bagging, and they also differ themselves. Here are the important observations: 1. On average, AdaBoost and Arc-x4-resample are better than Bagging for our datasets. This confirms previous comparisons <ref> (Breiman 1996a, Quinlan 1996) </ref>. 2. AdaBoost and Arc-x4, however, are not uniformly better than Bagging. There were several cases where the performance of the boosting algorithms degraded compared to the original (non-voted) algorithms.
Reference: <author> Breiman, L. </author> <year> (1996b), </year> <title> `Bagging predictors', </title> <booktitle> Machine Learning 24, </booktitle> <pages> 123-140. </pages>
Reference-contexts: 1. Introduction Methods for voting classification algorithms, such as Bagging and AdaBoost, have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real-world datasets <ref> (Breiman 1996b, Freund & Schapire 1996, Quinlan 1996) </ref>. Voting algorithms can be divided into two types: those that adaptively change the distribution of the training set based on the performance of previous classifiers (as in boosting methods) and those that do not (as in Bagging). <p> if the inducer is unstable (e.g., neural networks, decision trees) (Breiman 1994) and the performance can improve if the induced classifiers are good and not correlated; however, Bagging may slightly degrade the performance of stable algorithms (e.g., k-nearest neighbor) because effectively smaller training sets are used for training each classifier <ref> (Breiman 1996b) </ref>. EMPIRICAL COMPARISON OF BOOSTING, BAGGING, AND VARIANTS 5 Input: training set S of size m, Inducer I, integer T (number of trials). 1. S 0 = S with instance weights assigned to be 1. 2.
Reference: <author> Breiman, L. </author> <year> (1997), </year> <title> Arcing the edge, </title> <type> Technical Report Technical Report 486, </type> <institution> Statistics Department, University of California, Berkeley. </institution> <note> http://www.stat.Berkeley.EDU/users/breiman/. </note>
Reference: <author> Buntine, W. </author> <year> (1992a), </year> <title> `Learning classification trees', </title> <journal> Statistics and Computing 2(2), </journal> <pages> 63-73. </pages>
Reference: <author> Buntine, W. </author> <year> (1992b), </year> <title> A Theory of Learning Classification Rules, </title> <type> PhD thesis, </type> <institution> University of Technology, Sydney, School of Computing Science. </institution>
Reference-contexts: Algorithms that do not adaptively change the distribution include option decision tree algorithms that construct decision trees with multiple options at some nodes <ref> (Buntine 1992b, Buntine 1992a, Kohavi & Kunz 1997) </ref>; averaging path sets, fanned sets, and extended fanned sets as alternatives 2 ERIC BAUER AND RON KOHAVI to pruning (Oliver & Hand 1995); voting trees using different splitting criteria and human intervention (Kwok & Carter 1990); and error-correcting output codes (Dietterich & Bakiri
Reference: <author> Cestnik, B. </author> <year> (1990), </year> <title> Estimating probabilities: A crucial task in machine learning, </title> <editor> in L. C. Aiello, ed., </editor> <booktitle> `Proceedings of the ninth European Conference on Artificial Intelligence', </booktitle> <pages> pp. 147-149. </pages>
Reference-contexts: C i = I (S 0 ) 5. C fl (x) = arg max X 1 (the most often predicted label y) Output: classifier C fl . Irani 1993). Probabilities are estimated using frequency counts with an m-estimate Laplace correction <ref> (Cestnik 1990) </ref> as described in Kohavi, Becker & Sommerfield (1997). The Naive-Bayes classifier is relatively simple but very robust to violations of its independence assumptions.
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1993), </year> <title> Learning symbolic rules using artificial neural networks, </title> <booktitle> in `Proceedings of the Tenth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 73-80. </pages>
Reference: <author> Dietterich, T. G. </author> <title> (in press), `Approximate statistical tests for comparing supervised classification learning algorithms', </title> <booktitle> Neural Computation </booktitle> . 
Reference: <author> Dietterich, T. G. & Bakiri, G. </author> <year> (1991), </year> <title> Error-correcting output codes: A general method for improving multiclass inductive learning programs, </title> <booktitle> in `Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91)', </booktitle> <pages> pp. 572-577. </pages>
Reference-contexts: some nodes (Buntine 1992b, Buntine 1992a, Kohavi & Kunz 1997); averaging path sets, fanned sets, and extended fanned sets as alternatives 2 ERIC BAUER AND RON KOHAVI to pruning (Oliver & Hand 1995); voting trees using different splitting criteria and human intervention (Kwok & Carter 1990); and error-correcting output codes <ref> (Dietterich & Bakiri 1991, Kong & Dietterich 1995) </ref>. Wolpert (1992) discusses "stacking" classifiers into a more complex classifier instead of using the simple uniform weighting scheme of Bagging. Ali (1996) provides a recent review of related algorithms, and additional recent work can be found in Stolfo (1996).
Reference: <author> Domingos, P. </author> <year> (1997), </year> <title> Why does bagging work? a Bayesian account and its implications, </title> <editor> in D. Heckerman, H. Mannila, D. Pregibon & R. Uthurusamy, eds, </editor> <booktitle> `Proceedings of the third international conference on Knowledge Discovery and Data Mining', </booktitle> <publisher> AAAI Press, </publisher> <pages> pp. 155-158. </pages>
Reference-contexts: The Naive-Bayes Inducer The Naive-Bayes Inducer (Good 1965, Duda & Hart 1973, Langley, Iba & Thompson 1992), sometimes called Simple-Bayes <ref> (Domingos & Pazzani 1997) </ref>, builds a simple conditional independence classifier. <p> Irani 1993). Probabilities are estimated using frequency counts with an m-estimate Laplace correction (Cestnik 1990) as described in Kohavi, Becker & Sommerfield (1997). The Naive-Bayes classifier is relatively simple but very robust to violations of its independence assumptions. It performs well for many real-world datasets <ref> (Domingos & Pazzani 1997, Kohavi & Sommerfield 1995) </ref> and is excellent at handling irrelevant attributes (Langley & Sage to appear). 4. The Voting Algorithms The different voting algorithms used are described below.
Reference: <author> Domingos, P. & Pazzani, M. </author> <year> (1997), </year> <title> `Beyond independence: Conditions for the optimality of the simple Bayesian classifier', </title> <booktitle> Machine Learning 29(2/3), </booktitle> <pages> 103-130. </pages>
Reference-contexts: The Naive-Bayes Inducer The Naive-Bayes Inducer (Good 1965, Duda & Hart 1973, Langley, Iba & Thompson 1992), sometimes called Simple-Bayes <ref> (Domingos & Pazzani 1997) </ref>, builds a simple conditional independence classifier. <p> Irani 1993). Probabilities are estimated using frequency counts with an m-estimate Laplace correction (Cestnik 1990) as described in Kohavi, Becker & Sommerfield (1997). The Naive-Bayes classifier is relatively simple but very robust to violations of its independence assumptions. It performs well for many real-world datasets <ref> (Domingos & Pazzani 1997, Kohavi & Sommerfield 1995) </ref> and is excellent at handling irrelevant attributes (Langley & Sage to appear). 4. The Voting Algorithms The different voting algorithms used are described below.
Reference: <author> Drucker, H. & Cortes, C. </author> <year> (1996), </year> <title> Boosting decision trees, </title> <booktitle> in `Advances in Neural Information processing Systems 8', </booktitle> <pages> pp. 479-485. </pages>
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973), </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley. </publisher>
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1993), </year> <title> An Introduction to the Bootstrap, </title> <publisher> Chapman & Hall. </publisher>
Reference-contexts: The generated classifiers are then combined to create a final classifier that is used to classify the test set. 4.1. The Bagging Algorithm The Bagging algorithm (Bootstrap aggregating) by Breiman (1996b) votes classifiers generated by different bootstrap samples (replicates). Figure 1 shows the algorithm. A Bootstrap sample <ref> (Efron & Tibshirani 1993) </ref> is generated by uniformly sampling m instances from the training set with replacement. T bootstrap samples B 1 ; B 2 ; : : : ; B T are generated and a classifier C i is built from each bootstrap sample B i .
Reference: <author> Elkan, C. </author> <year> (1997), </year> <title> Boosting and Naive Bayesian learning, </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego. </address>
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1993), </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning, </title> <booktitle> in `Proceedings of the 13th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 1022-1027. </pages>
Reference: <author> Freund, Y. </author> <year> (1990), </year> <title> Boosting a weak learning algorithm by majority, </title> <booktitle> in `Proceedings of the Third Annual Workshop on Computational Learning Theory', </booktitle> <pages> pp. 202-216. </pages>
Reference: <author> Freund, Y. </author> <year> (1996), </year> <title> `Boosting a weak learning algorithm by majority', </title> <booktitle> Information and Computation 121(2), </booktitle> <pages> 256-285. </pages>
Reference-contexts: After improvements by Freund (1990), recently expanded in Freund (1996), AdaBoost (Adaptive Boosting) was introduced by Freund & Schapire (1995). In our work below, we concentrate on AdaBoost, sometimes called AdaBoost.M1 (e.g., <ref> (Freund & Schapire 1996) </ref>). Like Bagging, the AdaBoost algorithm generates a set of classifiers and votes them. Beyond this, the two algorithms differ substantially. The AdaBoost algorithm, shown in Figure 2, generates the classifiers sequentially, while Bagging can generate them in parallel.
Reference: <author> Freund, Y. & Schapire, R. E. </author> <year> (1995), </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting, </title> <booktitle> in `Proceedings of the Second European Conference on Computational Learning Theory', </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 23-37. </pages> <note> To appear in Journal of Computer and System Sciences. </note>
Reference-contexts: Wolpert (1992) discusses "stacking" classifiers into a more complex classifier instead of using the simple uniform weighting scheme of Bagging. Ali (1996) provides a recent review of related algorithms, and additional recent work can be found in Stolfo (1996). Algorithms that adaptively change the distribution include AdaBoost <ref> (Freund & Schapire 1995) </ref> and Arc-x4 (Breiman 1996a). Drucker & Cortes (1996) and Quinlan (1996) applied boosting to decision tree induction, observing both that error significantly decreases and that the generalization error does not degrade as more classifiers are combined.
Reference: <author> Freund, Y. & Schapire, R. E. </author> <year> (1996), </year> <title> Experiments with a new boosting algorithm, </title> <editor> in L. Saitta, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Thirteenth National Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 148-156. </pages>
Reference-contexts: After improvements by Freund (1990), recently expanded in Freund (1996), AdaBoost (Adaptive Boosting) was introduced by Freund & Schapire (1995). In our work below, we concentrate on AdaBoost, sometimes called AdaBoost.M1 (e.g., <ref> (Freund & Schapire 1996) </ref>). Like Bagging, the AdaBoost algorithm generates a set of classifiers and votes them. Beyond this, the two algorithms differ substantially. The AdaBoost algorithm, shown in Figure 2, generates the classifiers sequentially, while Bagging can generate them in parallel.
Reference: <author> Friedman, J. H. </author> <year> (1997), </year> <title> `On bias, variance, 0/1-loss, and the curse of dimensionality', Data Mining and Knowledge Discovery 1(1), </title> <type> 55-77. </type> <institution> ftp://playfair.stanford.edu/pub/friedman/curse.ps.Z. </institution>
Reference-contexts: The probability estimates generated are usually extreme because the conditional independence assumption is not true in many cases, causing a single factor to affect several attributes whose probabilities are multiplied assuming they are conditionally independent given the label <ref> (Friedman 1997) </ref>. To summarize, we have seen error reductions for the family of decision-tree algorithms when probabilistic estimates were used. The error reductions were larger for the one level decision trees. This reduction was due to a decrease in both bias and variance.
Reference: <author> Geman, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> `Neural networks and the bias/variance dilemma', </title> <booktitle> Neural Computation 4, </booktitle> <pages> 1-48. </pages>
Reference-contexts: A final classifier C fl is built that returns the class predicted by the most classifiers (ties are broken arbitrarily). Unlike AdaBoost, the classifiers are voted equally. 5. The Bias and Variance Decomposition The bias plus variance decomposition <ref> (Geman, Bienenstock & Doursat 1992) </ref> is a powerful tool from sampling theory statistics for analyzing supervised learning scenarios that have quadratic loss functions.
Reference: <author> Good, I. J. </author> <year> (1965), </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods, </title> <publisher> M.I.T. Press. </publisher>
Reference-contexts: Specifically for multi-class problems with continuous attributes, MC4 (1) is usually unable to build a good classifier because the tree consists of a single binary root split with leaves as children. 3.2. The Naive-Bayes Inducer The Naive-Bayes Inducer <ref> (Good 1965, Duda & Hart 1973, Langley, Iba & Thompson 1992) </ref>, sometimes called Simple-Bayes (Domingos & Pazzani 1997), builds a simple conditional independence classifier.
Reference: <author> Holte, R. C. </author> <year> (1993), </year> <title> `Very simple classification rules perform well on most commonly used datasets', </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-90. </pages>
Reference: <author> Iba, W. & Langley, P. </author> <year> (1992), </year> <title> Induction of one-level decision trees, </title> <booktitle> in `Proceedings of the Ninth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 233-240. </pages>
Reference-contexts: Along with the original algorithm, two variants of MC4 were explored: MC4 (1) and MC4 (1)- disc. MC4 (1) limits the tree to a single root split; such a shallow tree is sometimes called a decision stump <ref> (Iba & Langley 1992) </ref>. If the root attribute is nominal, a multi-way split is created with one branch for unknowns. If the root attribute is continuous, a three-way split is created: less than a threshold, greater than a threshold, and unknown.
Reference: <author> Kohavi, R. </author> <year> (1995a), </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> `Proceedings of the 14th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. </pages> <month> 1137-1143. </month> <title> http://robotics.stanford.edu/~ronnyk. EMPIRICAL COMPARISON OF BOOSTING, BAGGING, AND VARIANTS 33 Kohavi, </title> <editor> R. </editor> <year> (1995b), </year> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs, </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Science department. STAN-CS-TR-95-1560, </institution> <note> http://robotics.Stanford.EDU/~ronnyk/teza.ps.Z. </note>
Reference-contexts: Standard deviations of the error estimate from each run were computed as the standard deviation of the three outer runs, assuming they were independent. Although such an assumption is not strictly correct <ref> (Kohavi 1995a, Dietterich in press) </ref>, it is quite reasonable given our circumstances because our training sets are small in size and we only average three values. 6.
Reference: <author> Kohavi, R., Becker, B. & Sommerfield, D. </author> <year> (1997), </year> <title> Improving Simple Bayes, </title> <booktitle> in `The 9th European Conference on Machine Learning, Poster Papers', </booktitle> <pages> pp. 78-87. </pages> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference-contexts: EMPIRICAL COMPARISON OF BOOSTING, BAGGING, AND VARIANTS 3 3.1. The Decision Tree Inducers The basic decision tree inducer we used, called MC4 (MLC ++ C4.5), is a Top-Down Decision Tree (TDDT) induction algorithm implemented in MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1997) </ref>. The algorithm is similar to C4.5 (Quinlan 1993) with the exception that unknowns are regarded as a separate value. The algorithm grows the decision tree following the standard methodology of choosing the best attribute according to the evaluation criterion (gain-ratio). <p> The main reason for choosing this algorithm over C4.5 is our familiarity with it, our ability to modify it for experiments, and its tight integration with multiple model mechanisms within MLC ++ . MC4 is available off the web in source form as part of MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1997) </ref>. Along with the original algorithm, two variants of MC4 were explored: MC4 (1) and MC4 (1)- disc. MC4 (1) limits the tree to a single root split; such a shallow tree is sometimes called a decision stump (Iba & Langley 1992). <p> The probabilities in the above formulas must be estimated from the training set. In our implementation, which is part of MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1997) </ref>, continuous attributes are discretized using entropy discretization (Kohavi & Sahami 1996, Fayyad & 4 ERIC BAUER AND RON KOHAVI Input: training set S, Inducer I, integer T (number of bootstrap samples). 1. for i = 1 to T f 2. <p> Irani 1993). Probabilities are estimated using frequency counts with an m-estimate Laplace correction (Cestnik 1990) as described in Kohavi, Becker & Sommerfield (1997). The Naive-Bayes classifier is relatively simple but very robust to violations of its independence assumptions. It performs well for many real-world datasets <ref> (Domingos & Pazzani 1997, Kohavi & Sommerfield 1995) </ref> and is excellent at handling irrelevant attributes (Langley & Sage to appear). 4. The Voting Algorithms The different voting algorithms used are described below. <p> One solution proposed by Kohavi & Kunz (1997) attempts to build a structured model that has the same affect as Bagging. Ridgeway, Madigan & Richardson (1998) convert a boosted Naive-Bayes to a regular Naive-Bayes, which then allows for visualizations <ref> (Becker, Kohavi & Sommerfield 1997) </ref>. Are there ways to make boosting comprehensible for general models? Craven & Shavlik (1993) built a single decision tree that attempts to make the same classifications as a neural network. Quinlan (1994) notes that there are parallel problems that require testing all attributes.
Reference: <author> Kohavi, R. & Kunz, C. </author> <year> (1997), </year> <title> Option decision trees with majority votes, </title> <editor> in D. Fisher, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Fourteenth International Conference', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 161-169. </pages> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference-contexts: EMPIRICAL COMPARISON OF BOOSTING, BAGGING, AND VARIANTS 3 3.1. The Decision Tree Inducers The basic decision tree inducer we used, called MC4 (MLC ++ C4.5), is a Top-Down Decision Tree (TDDT) induction algorithm implemented in MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1997) </ref>. The algorithm is similar to C4.5 (Quinlan 1993) with the exception that unknowns are regarded as a separate value. The algorithm grows the decision tree following the standard methodology of choosing the best attribute according to the evaluation criterion (gain-ratio). <p> The main reason for choosing this algorithm over C4.5 is our familiarity with it, our ability to modify it for experiments, and its tight integration with multiple model mechanisms within MLC ++ . MC4 is available off the web in source form as part of MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1997) </ref>. Along with the original algorithm, two variants of MC4 were explored: MC4 (1) and MC4 (1)- disc. MC4 (1) limits the tree to a single root split; such a shallow tree is sometimes called a decision stump (Iba & Langley 1992). <p> The probabilities in the above formulas must be estimated from the training set. In our implementation, which is part of MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1997) </ref>, continuous attributes are discretized using entropy discretization (Kohavi & Sahami 1996, Fayyad & 4 ERIC BAUER AND RON KOHAVI Input: training set S, Inducer I, integer T (number of bootstrap samples). 1. for i = 1 to T f 2. <p> Irani 1993). Probabilities are estimated using frequency counts with an m-estimate Laplace correction (Cestnik 1990) as described in Kohavi, Becker & Sommerfield (1997). The Naive-Bayes classifier is relatively simple but very robust to violations of its independence assumptions. It performs well for many real-world datasets <ref> (Domingos & Pazzani 1997, Kohavi & Sommerfield 1995) </ref> and is excellent at handling irrelevant attributes (Langley & Sage to appear). 4. The Voting Algorithms The different voting algorithms used are described below. <p> One solution proposed by Kohavi & Kunz (1997) attempts to build a structured model that has the same affect as Bagging. Ridgeway, Madigan & Richardson (1998) convert a boosted Naive-Bayes to a regular Naive-Bayes, which then allows for visualizations <ref> (Becker, Kohavi & Sommerfield 1997) </ref>. Are there ways to make boosting comprehensible for general models? Craven & Shavlik (1993) built a single decision tree that attempts to make the same classifications as a neural network. Quinlan (1994) notes that there are parallel problems that require testing all attributes.
Reference: <author> Kohavi, R. & Sahami, M. </author> <year> (1996), </year> <title> Error-based and entropy-based discretization of continuous features, </title> <booktitle> in `Proceedings of the Second International Conference on Knowledge Discovery and Data Mining', </booktitle> <pages> pp. 114-119. </pages>
Reference-contexts: If the root attribute is nominal, a multi-way split is created with one branch for unknowns. If the root attribute is continuous, a three-way split is created: less than a threshold, greater than a threshold, and unknown. MC4 (1)-disc first discretizes all the attributes using entropy discretization <ref> (Kohavi & Sahami 1996, Fayyad & Irani 1993) </ref>, thus effectively allowing a root split with multiple thresholds. <p> MC4 (1)-disc is very similar to the 1R classifier of Holte (1993), except that the discretization step is based on entropy, which compared favorably with his 1R discretization in our previous work <ref> (Kohavi & Sahami 1996) </ref>. Both MC4 (1) and MC4 (1)-disc build very weak classifiers, but MC4 (1)-disc is the more powerful of the two.
Reference: <author> Kohavi, R. & Sommerfield, D. </author> <year> (1995), </year> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology, </title> <booktitle> in `The First International Conference on Knowledge Discovery and Data Mining', </booktitle> <pages> pp. 192-197. </pages>
Reference: <author> Kohavi, R., Sommerfield, D. & Dougherty, J. </author> <year> (1997), </year> <title> `Data mining using MLC ++ : A machine learning library in C ++ ', International Journal on Artificial Intelligence Tools 6(4), </title> <address> 537-566. http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: EMPIRICAL COMPARISON OF BOOSTING, BAGGING, AND VARIANTS 3 3.1. The Decision Tree Inducers The basic decision tree inducer we used, called MC4 (MLC ++ C4.5), is a Top-Down Decision Tree (TDDT) induction algorithm implemented in MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1997) </ref>. The algorithm is similar to C4.5 (Quinlan 1993) with the exception that unknowns are regarded as a separate value. The algorithm grows the decision tree following the standard methodology of choosing the best attribute according to the evaluation criterion (gain-ratio). <p> The main reason for choosing this algorithm over C4.5 is our familiarity with it, our ability to modify it for experiments, and its tight integration with multiple model mechanisms within MLC ++ . MC4 is available off the web in source form as part of MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1997) </ref>. Along with the original algorithm, two variants of MC4 were explored: MC4 (1) and MC4 (1)- disc. MC4 (1) limits the tree to a single root split; such a shallow tree is sometimes called a decision stump (Iba & Langley 1992). <p> The probabilities in the above formulas must be estimated from the training set. In our implementation, which is part of MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1997) </ref>, continuous attributes are discretized using entropy discretization (Kohavi & Sahami 1996, Fayyad & 4 ERIC BAUER AND RON KOHAVI Input: training set S, Inducer I, integer T (number of bootstrap samples). 1. for i = 1 to T f 2. <p> Irani 1993). Probabilities are estimated using frequency counts with an m-estimate Laplace correction (Cestnik 1990) as described in Kohavi, Becker & Sommerfield (1997). The Naive-Bayes classifier is relatively simple but very robust to violations of its independence assumptions. It performs well for many real-world datasets <ref> (Domingos & Pazzani 1997, Kohavi & Sommerfield 1995) </ref> and is excellent at handling irrelevant attributes (Langley & Sage to appear). 4. The Voting Algorithms The different voting algorithms used are described below. <p> One solution proposed by Kohavi & Kunz (1997) attempts to build a structured model that has the same affect as Bagging. Ridgeway, Madigan & Richardson (1998) convert a boosted Naive-Bayes to a regular Naive-Bayes, which then allows for visualizations <ref> (Becker, Kohavi & Sommerfield 1997) </ref>. Are there ways to make boosting comprehensible for general models? Craven & Shavlik (1993) built a single decision tree that attempts to make the same classifications as a neural network. Quinlan (1994) notes that there are parallel problems that require testing all attributes.
Reference: <author> Kohavi, R. & Wolpert, D. H. </author> <year> (1996), </year> <title> Bias plus variance decomposition for zero-one loss functions, </title> <editor> in L. Saitta, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Thirteenth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 275-283. </pages> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference-contexts: If the root attribute is nominal, a multi-way split is created with one branch for unknowns. If the root attribute is continuous, a three-way split is created: less than a threshold, greater than a threshold, and unknown. MC4 (1)-disc first discretizes all the attributes using entropy discretization <ref> (Kohavi & Sahami 1996, Fayyad & Irani 1993) </ref>, thus effectively allowing a root split with multiple thresholds. <p> MC4 (1)-disc is very similar to the 1R classifier of Holte (1993), except that the discretization step is based on entropy, which compared favorably with his 1R discretization in our previous work <ref> (Kohavi & Sahami 1996) </ref>. Both MC4 (1) and MC4 (1)-disc build very weak classifiers, but MC4 (1)-disc is the more powerful of the two.
Reference: <author> Kong, E. B. & Dietterich, T. G. </author> <year> (1995), </year> <title> Error-correcting output coding corrects bias and variance, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 313-321. </pages>
Reference: <author> Kwok, S. W. & Carter, C. </author> <year> (1990), </year> <title> Multiple decision trees, </title> <editor> in R. D. Schachter, T. S. Levitt, L. N. Kanal & J. F. Lemmer, eds, </editor> <booktitle> `Uncertainty in Artificial Intelligence', </booktitle> <publisher> Elsevier Science Publishers, </publisher> <pages> pp. 327-335. </pages>
Reference-contexts: that construct decision trees with multiple options at some nodes (Buntine 1992b, Buntine 1992a, Kohavi & Kunz 1997); averaging path sets, fanned sets, and extended fanned sets as alternatives 2 ERIC BAUER AND RON KOHAVI to pruning (Oliver & Hand 1995); voting trees using different splitting criteria and human intervention <ref> (Kwok & Carter 1990) </ref>; and error-correcting output codes (Dietterich & Bakiri 1991, Kong & Dietterich 1995). Wolpert (1992) discusses "stacking" classifiers into a more complex classifier instead of using the simple uniform weighting scheme of Bagging.
Reference: <author> Langley, P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An analysis of Bayesian classifiers, </title> <booktitle> in `Proceedings of the tenth national conference on artificial intelligence', </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference-contexts: Along with the original algorithm, two variants of MC4 were explored: MC4 (1) and MC4 (1)- disc. MC4 (1) limits the tree to a single root split; such a shallow tree is sometimes called a decision stump <ref> (Iba & Langley 1992) </ref>. If the root attribute is nominal, a multi-way split is created with one branch for unknowns. If the root attribute is continuous, a three-way split is created: less than a threshold, greater than a threshold, and unknown.
Reference: <author> Langley, P. & Sage, S. </author> <title> (to appear), Scaling to domains with many irrelevant features, </title> <editor> in R. Greiner, ed., </editor> <booktitle> `Computational Learning Theory and Natural Learning Systems', </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Merz, C. & Murphy, P. </author> <year> (1998), </year> <note> `UCI repository of machine learning databases'. *http://www.ics.uci.edu/~mlearn/MLRepository.html Oates, </note> <author> T. & Jensen, D. </author> <year> (1997), </year> <title> The effects of training set size on decision tree complexity, </title> <editor> in D. Fisher, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Fourteenth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 254-262. </pages>
Reference-contexts: This requires that the test set size be large, which led us to choose only files with at least 1000 instances. Fourteen files satisfying this requirement were found in the UC Irvine repository <ref> (Merz & Murphy 1998) </ref> and are shown in Table 1. 2. There should be room for improving the error for a given training set size.
Reference: <author> Oliver, J. & Hand, D. </author> <year> (1995), </year> <title> On pruning and averaging decision trees, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 430-437. </pages>
Reference-contexts: Algorithms that do not adaptively change the distribution include option decision tree algorithms that construct decision trees with multiple options at some nodes (Buntine 1992b, Buntine 1992a, Kohavi & Kunz 1997); averaging path sets, fanned sets, and extended fanned sets as alternatives 2 ERIC BAUER AND RON KOHAVI to pruning <ref> (Oliver & Hand 1995) </ref>; voting trees using different splitting criteria and human intervention (Kwok & Carter 1990); and error-correcting output codes (Dietterich & Bakiri 1991, Kong & Dietterich 1995). Wolpert (1992) discusses "stacking" classifiers into a more complex classifier instead of using the simple uniform weighting scheme of Bagging.
Reference: <author> Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T. & Brunk, C. </author> <year> (1994), </year> <title> Reducing misclassification costs, </title> <booktitle> in `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For single trees, the variance penalty incurred by using estimates from nodes with a small number of instances may be large and pruning can help <ref> (Pazzani, Merz, Murphy, Ali, Hume & Brunk 1994) </ref>; however, voting methods reduce the variance by voting multiple classifiers, and the bias introduced by pruning may be a limiting factor.
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: EMPIRICAL COMPARISON OF BOOSTING, BAGGING, AND VARIANTS 3 3.1. The Decision Tree Inducers The basic decision tree inducer we used, called MC4 (MLC ++ C4.5), is a Top-Down Decision Tree (TDDT) induction algorithm implemented in MLC ++ (Kohavi, Sommerfield & Dougherty 1997). The algorithm is similar to C4.5 <ref> (Quinlan 1993) </ref> with the exception that unknowns are regarded as a separate value. The algorithm grows the decision tree following the standard methodology of choosing the best attribute according to the evaluation criterion (gain-ratio).
Reference: <author> Quinlan, J. R. </author> <year> (1994), </year> <title> Comparing connectionist and symbolic learning methods, </title> <editor> in S. J. Hanson, G. A. Drastal & R. L. Rivest, eds, </editor> <title> `Computational Learning Theory and Natural Learning Systems', Vol. I: Constraints and Prospects, </title> <publisher> MIT Press, </publisher> <pages> chapter 15, pp. 445|456. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1996), </year> <title> Bagging, boosting, </title> <booktitle> and c4.5, in `Proceedings of the Thirteenth National Conference on Artificial Intelligence', </booktitle> <publisher> AAAI Press and the MIT Press, </publisher> <pages> pp. 725-730. </pages>
Reference-contexts: Our implementations of MC4, MC4 (1), MC4 (1)-disc, and Naive-Bayes support weighted instances, so we have implemented boosting by reweighting, which is a more direct implementation of the theory. Some evidence exists that reweighting works better in practice <ref> (Quinlan 1996) </ref>. Recent work by Schapire, Freund, Bartlett & Lee (1997) suggests one explanation for the success of boosting and for the fact that test set error does not increase when many classifiers are combined as the theoretical model implies.
Reference: <author> Ridgeway, G., Madigan, D. & Richardson, T. </author> <year> (1998), </year> <title> Interpretable boosted naive bayes classification, </title> <booktitle> in `Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining'. </booktitle>
Reference: <author> Schaffer, C. </author> <year> (1994), </year> <title> A conservation law for generalization performance, </title> <booktitle> in `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 259-265. </pages>
Reference-contexts: The actual training set sizes are shown in Table 1. In one surprising case, the segment dataset with MC4 (1), the error increased as the training set size grew. While in theory such behavior must happen for every induction algorithm <ref> (Wolpert 1994, Schaffer 1994) </ref>, this is the first time we have seen it in a real dataset.
Reference: <author> Schapire, R. E. </author> <year> (1990), </year> <title> `The strength of weak learnability', </title> <booktitle> Machine Learning 5(2), </booktitle> <pages> 197-227. </pages>
Reference: <author> Schapire, R. E., Freund, Y., Bartlett, P. & Lee, W. S. </author> <year> (1997), </year> <title> Boosting the margin: A new explanation for the effectiveness of voting methods, </title> <editor> in D. Fisher, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Fourteenth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 322-330. </pages>
Reference-contexts: Recent boosting implementations by Freund, Schapire, and Singer maintain the log of the weights and modify the definition of fi so that a small value (0.5 divided by the number of training examples) is added to the numerator and denominator <ref> (personal communication with Schapire, 1997) </ref>. It seems that the issue deserves careful attention and that boosting experiments with many trials (e.g., 1000 as in Schapire et al. (1997)) require addressing the issue carefully. 8.2. AdaBoost: Error, Bias, and Variance bias and variance.
Reference: <author> Stolfo, S. </author> <year> (1996), </year> <title> Integrating multiple learned models for improving and scaling machine learning algorithms. </title> <booktitle> AAAI Workshop. </booktitle>
Reference: <author> Wolpert, D. H. </author> <year> (1992), </year> <title> `Stacked generalization', </title> <booktitle> Neural Networks 5, </booktitle> <pages> 241-259. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1994), </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework, </title> <editor> in D. H. Wolpert, ed., </editor> <title> `The Mathematics of Generalization', </title> <publisher> Addison Wesley. </publisher>
Reference-contexts: The actual training set sizes are shown in Table 1. In one surprising case, the segment dataset with MC4 (1), the error increased as the training set size grew. While in theory such behavior must happen for every induction algorithm <ref> (Wolpert 1994, Schaffer 1994) </ref>, this is the first time we have seen it in a real dataset.
References-found: 52

