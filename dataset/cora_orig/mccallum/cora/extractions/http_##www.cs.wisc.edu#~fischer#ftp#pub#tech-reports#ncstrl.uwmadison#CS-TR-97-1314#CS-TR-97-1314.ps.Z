URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-97-1314/CS-TR-97-1314.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-97-1314/
Root-URL: http://www.cs.wisc.edu
Email: pmd@cs.wisc.edu  sameeta@microsoft.com  naughton@cs.wisc.edu  raghu@cs.wisc.edu  
Title: Computation of Multidimensional Aggregates  
Author: Prasad M. Deshpande Sameet Agarwal Jeffrey F. Naughton Raghu Ramakrishnan 
Address: Wisconsin, Madision, WI-53706  
Affiliation: Computer Sciences Department U.  
Abstract: At the heart of OLAP or multidimensional data analysis applications is the ability to simultaneously aggregate across many sets of dimensions. Computing multidimensional aggregates is a performance bottleneck for these applications. We explore various schemes for implementing multidimensional aggregation; in particular, the CUBE operator [1] proposed by Gray et al. This operator computes aggregates over all subsets of dimensions specified in the CUBE operation, and is equivalent to the union of a number of standard group-by operations. We show how the structure of CUBE computation can be viewed in terms of a hierarchy of group-by operations, and present a class of sorting-based algorithms that overlap the computation of different group-by operations using the least possible memory for each computation. Our algorithms seek to minimize the number of sorting steps required to compute the many sub-aggregates that comprise a multidimensional aggregate, and make efficient use of the available memory to reduce the number of I/Os. The implementation results show that the method dramatically outperforms the straightforward implementation of multidimensional aggregation as a sequence of SQL group-by operations. Finding a guaranteed optimal strategy within our framework to compute the CUBE operation is likely to be expensive; we prove that a simplified version of the problem is NP-hard. Our algorithm also deals efficiently with the common case where only a subset of the CUBE is to be computed.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jim Gray, Adam Bosworth, Andrew Layman and Hamid Pirahesh, </author> <title> "Data Cube: A Relational Operator Generalizing Group-By, </title> <journal> Cross-Tab and Sub-Totals", </journal> <note> to appear in IEEE transactions on Knowledge and Data Engineering. </note>
Reference-contexts: Recently there has been a lot of research interest in this area. One of the earlier papers is by Gray et al <ref> [1] </ref>, where multidimensional aggregation is formalized and expressed in SQL by a new operator called the CUBE operator. The CUBE operator is the n-dimensional generalization of the group-by operator. <p> This cuboid can be represented as a k + 1 attribute relation by using the special value ALL for the remaining k j attributes <ref> [1] </ref>. The CUBE on attribute set X is defined as the union of cuboids on all subset of attributes of X. The technique for computing a CUBE depends on the aggregation function. In this paper we assume that the function is distributive. Functions like add, min are distributive.
Reference: [2] <author> Venky Harinarayan, Anand Rajaraman and Jeff Ullman, </author> <title> "Implementing Data Cubes Efficiently", </title> <booktitle> ACM SIGMOD, </booktitle> <year> 1996. </year>
Reference-contexts: Indeed, we provide some experimental evidence indicating that its I/O performance is often close to optimal (Section 6.3). Often, because of storage limitations we may not want to precompute all the cuboids. This problem is considered in <ref> [2] </ref> which decides on an optimal set of cuboids to be precomputed given storage constraints. Our algorithm can be easily adapted to compute only a given set of cuboids. The rest of the paper is organized as follows. In Section 2 we present different approaches to computing the CUBE. <p> This may be due to storage considerations or other reasons. For example, in <ref> [2] </ref> the authors consider the problem of deciding which cuboids are to be computed given some amount of disk storage.
Reference: [3] <author> Pendse, Nigel and Richard Creeth, </author> <booktitle> "The OLAP Report",Business Intelligence, </booktitle> <address> London, England, </address> <year> 1995. </year>
Reference-contexts: Further, the cost of precomputation influences how frequently the precomputed data is brought up-to-date. 1.1 What is a CUBE? Tools for multidimensional data analysis comprise a large and fast-growing industry (estimates vary widely, but go as high as $700M for 1995 <ref> [3] </ref>). Recently there has been a lot of research interest in this area. One of the earlier papers is by Gray et al [1], where multidimensional aggregation is formalized and expressed in SQL by a new operator called the CUBE operator.
Reference: [4] <author> Codd E. F., </author> <title> "Providing OLAP: An IT Mandate", </title> <type> Unpublished Manuscript, </type> <institution> E.F. Codd and Associates, </institution> <year> 1993. </year>
Reference-contexts: Speed is a primary goal in this kind of application. Interactive analysis (response time in seconds) is not possible if each of these aggregates is computed "on the fly" at query execution time; so most products for these applications (OLAP <ref> [4] </ref> or Multidimensional Database (MDDB) systems ) precompute the aggregates and run interactive sessions against the precomputed data. Speed is critical for this precomputation as well, since the precomputation time is a lower bound on the recency of the data available to an analysts.
Reference: [5] <author> Richard Finkelstein, </author> <title> "Understanding the Need for On-Line Analytical Servers", Unpublished Manuscript, Performance Computing, Inc. [6] "Designing the Data Warehouse on Relational Databases", </title> <type> Unpublished Manuscript, </type> <institution> Stanford Technology Group, Inc, </institution> <year> 1995. </year>
Reference-contexts: 1 Introduction The group-by operator in SQL is typically used to compute aggregates on a set of attributes. For business data analysis, it is often necessary to aggregate data across many dimensions (attributes) <ref> [5, 7] </ref>. For example, in a retail application, one might have a relation with attributes (Product, Year, Customer, Sales).
Reference: [7] <author> Jay-Louise Weldon, </author> <title> "Managing Multidimensional Data: Harnessing the Power", </title> <type> Unpublished Manuscript, </type> <year> 1995. </year>
Reference-contexts: 1 Introduction The group-by operator in SQL is typically used to compute aggregates on a set of attributes. For business data analysis, it is often necessary to aggregate data across many dimensions (attributes) <ref> [5, 7] </ref>. For example, in a retail application, one might have a relation with attributes (Product, Year, Customer, Sales).
Reference: [8] <author> Robert Epstein, </author> <title> "Techniques for Processing of Aggregates in Relational Database Systems", </title> <institution> Memo UCB/ERL M79/8, E.R.L., College of Engg., U. of California, Berkeley, </institution> <month> Feb </month> <year> 1979. </year>
Reference-contexts: Computing the CUBE requires that we compute all the cuboids that together form the CUBE. The base cuboid has to be computed from the original relation; this can be done using any of the standard group-by techniques like sorting or hashing <ref> [8, 9, 10] </ref>. The other cuboids can be computed from the base cuboid due to the distributive nature of the aggregation. For example, in the retail application relation, sum of sales by (product, customer) can be obtained by using sum of sales by (product, year, customer).
Reference: [9] <author> Goetz Graefe, </author> <title> "Query Evaluation Techniques for Large Databases", </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Computing the CUBE requires that we compute all the cuboids that together form the CUBE. The base cuboid has to be computed from the original relation; this can be done using any of the standard group-by techniques like sorting or hashing <ref> [8, 9, 10] </ref>. The other cuboids can be computed from the base cuboid due to the distributive nature of the aggregation. For example, in the retail application relation, sum of sales by (product, customer) can be obtained by using sum of sales by (product, year, customer). <p> From a base cuboid on k attributes we compute all cuboids on k 1 attributes and using these, all cuboids with k 2 attributes and so on. 4 For many matching tasks, it has been shown that hashing is a reasonable alternative to sorting <ref> [9] </ref>. It may be possible to apply the overlap method for hashing as well. However, we concentrate on Sorting based methods in this paper. 3 The Overlap Method The method we propose for CUBE computation is a sort-based overlap method.
Reference: [10] <author> Ambuj Shatdal and Jeffrey F. Naughton, </author> <title> "Adaptive Parallel Aggregation Algorithms", </title> <booktitle> Proc. of the 1995 ACM-SIGMOD Conference, </booktitle> <address> San Jose, CA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Computing the CUBE requires that we compute all the cuboids that together form the CUBE. The base cuboid has to be computed from the original relation; this can be done using any of the standard group-by techniques like sorting or hashing <ref> [8, 9, 10] </ref>. The other cuboids can be computed from the base cuboid due to the distributive nature of the aggregation. For example, in the retail application relation, sum of sales by (product, customer) can be obtained by using sum of sales by (product, year, customer).
Reference: [11] <author> William Feller, </author> <title> An Introduction to Probability Theory and Its Applications, Vol. I, </title> <publisher> John Wiley & Sons, </publisher> <pages> pp 241; 1957. </pages>
Reference-contexts: If such information is not available, then it can be estimated by using the uniform distribution assumption given the number of distinct values of each attribute in the CUBE and the size of the base cuboid <ref> [11] </ref>.
Reference: [12] <author> Michael R. Garey and David S. Johnson, Computers and Intractability, W. H. Freeman and Company, </author> <note> pp 45-76,65,96,247; 1979. </note>
Reference: [13] <author> Amit Shukla, Prasad M. Deshpande, Jeffrey F. Naughton and Karthik Ramasamy, </author> <title> Storage Estimation for Multidimensional Aggregates in the Presence of Hierarchies, </title> <note> submitted to VLDB '96 </note>
Reference-contexts: Our algorithm would benefit from accurate estimates of the cuboid sizes. Accurate estimation is an interesting research issue and we have some results on it <ref> [13] </ref>. We also need to study how the algorithm performs in the presence of incorrect estimates. The algorithm we have presented is not the only way to compute the CUBE, of course. For example, one interesting approach is to sort the base cuboid in two different sort orders.
Reference: [14] <author> Sunita Sarawagi, Rakesh Agrawal and Ashish Gupta, </author> <title> On Computing the Data Cube, </title> <note> submitted to VLDB '96 </note>
Reference-contexts: The spikes show that the BF allocation may be non-optimal in some cases. 7 Related Work There has been some concurrent work on the problem of computing the CUBE <ref> [14, 15] </ref>. In this section, we compare and contrast our approach to theirs. They have identifed a set of common optimizations that can be applied while computing the CUBE. Specifically, they name these as "Smallest-parent", "Cache- results", "Amortize-scans", "Share-sorts", "Share-partitions".
Reference: [15] <author> Sunita Sarawagi, Rakesh Agarwal and Ashish Gupta, </author> <title> On Computiing the Data Cube, </title> <type> Research Report RJ 10026, </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, California, </address> <year> 1996. </year> <month> 22 </month>
Reference-contexts: The spikes show that the BF allocation may be non-optimal in some cases. 7 Related Work There has been some concurrent work on the problem of computing the CUBE <ref> [14, 15] </ref>. In this section, we compare and contrast our approach to theirs. They have identifed a set of common optimizations that can be applied while computing the CUBE. Specifically, they name these as "Smallest-parent", "Cache- results", "Amortize-scans", "Share-sorts", "Share-partitions".
References-found: 14

