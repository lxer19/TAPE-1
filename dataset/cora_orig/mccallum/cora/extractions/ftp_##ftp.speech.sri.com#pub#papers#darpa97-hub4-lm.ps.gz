URL: ftp://ftp.speech.sri.com/pub/papers/darpa97-hub4-lm.ps.gz
Refering-URL: http://www.speech.sri.com/people/sankar/publications.html
Root-URL: 
Title: Hub4 Language Modeling Using Domain Interpolation and Data Clustering adapting language models to the speech
Author: Fuliang Weng Andreas Stolcke Ananth Sankar 
Note: Two adaptation approaches are also described:  
Address: Menlo Park, California  
Affiliation: Speech Technology And Research Laboratory SRI International  
Abstract: In SRI's language modeling experiments for the Hub4 domain, three basic approaches were pursued: interpolating multiple models estimated from Hub4 and non-Hub4 training data, adapting the language model (LM) to the focus conditions, and adapting the LM to different topic types. In the first approach, we built separate LMs for the closely transcribed Hub4 material (acoustic training transcripts) and the loosely transcribed Hub4 material (LM training data), as well as the North-American Business News (NABN) and Switchboard training data, projected onto the Hub4 vocabulary. By interpolating the probabilities obtained from these models, we obtained a 20% reduction in perplexity and a 1.8% reduction in word error rate, compared to a baseline Hub4-only language model. Finally, we identify the problems and future directions of our work. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. Cowie, L. Guthrie, and J. Guthrie. </author> <title> Lexical disambiguation using simulated annealing. </title> <booktitle> In Proceedings of 1992 DARPA Workshop on Speech and Natural Language, </booktitle> <year> 1992. </year>
Reference-contexts: For example, the word drug could be used to describe abuse by athletes for performance enhancement, or to refer to cases of recreational drug use by sports celebrities. To disambiguate such uses one would probably have to look at higher-order statistics of content words (and not just longer N-grams) <ref> [1] </ref>. However, the use of higher-order statistics could subject the similarity measure to too much noise, given that they are unreliable when collected on small samples.
Reference: 2. <author> L. Gillick et al. </author> <title> Application of large vocabulary continuous speech recognition to topic and speaker identification using telephone speech. </title> <booktitle> In Proceedings of ICASSP-93, </booktitle> <year> 1993. </year>
Reference-contexts: Alternatively, the unigram distance metric can be made even less sensitive to stylistic and grammatical differences by omitting function and other high-frequency words from the similarity assessment. This is common practice in information retrieval (IR), and was also used for LM adaptation [6] and topic identification <ref> [2] </ref>. This approach focuses the similarity measure on semantic aspects and makes it less sensitive to syntactic features. So far, we have experimented with only a few of the many choices on this continuum.
Reference: 3. <author> J. J. Godfrey, E. C. Holliman, and J. McDaniel. </author> <title> SWITCH BOARD: Telephone speech corpus for research and development. </title> <booktitle> In Proceedings ICASSP, </booktitle> <volume> vol. I, </volume> <pages> pp. 517520, </pages> <address> San Fran-cisco, </address> <year> 1992. </year>
Reference-contexts: Using Non-Hub4 Training Data To improve the coverage and robustness of our baseline LM, we used training material from two generally available non-Hub4 databases. The Switchboard corpus <ref> [3] </ref> contains conversational speech collected over the telephone and can supplement the coverage of various spontaneous speech phenomena.
Reference: 4. <author> R. Iyer, M. Ostendorf, and H. Gish. </author> <title> Using out-of-domain data to improve in-domain language models. </title> <type> Technical Report ECE-97-001, </type> <institution> ECE Department, Boston University, </institution> <year> 1997. </year>
Reference-contexts: Related approaches by other researchers have effectively used much smaller amounts of target-specific training data for LM adaptation purposes. For example, [6] reported small but significant improvements by using the 50 closest articles to the segment hypotheses within a story boundary. Similarly, <ref> [4] </ref> reported improvements by interpolating an in-domain LM with out-of-domain articles, which were weighted by a distance metric defining how similar they were to the target domain.
Reference: 5. <author> S. M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 35(3), </volume> <year> 1987. </year>
Reference-contexts: As before, we combine the various data sources through linear interpolation of LMs. Separate backoff LMs <ref> [5] </ref> were estimated for each of the four data sources - Hub4 LM data, Hub4 acoustic data, Switchboard, and NABN restricting N-grams to the Hub4 20,000 word vocabulary. Word probabilities from the individual models were interpolated linearly. Interpolation weights were optimized by minimizing the perplexity on the Hub4 development data.
Reference: 6. <author> S. Sekine, A. Borthwick, and R. Grishman. </author> <title> NYU language modeling experiment for 1996 CSR evaluation. </title> <booktitle> In these proceedings. </booktitle>
Reference-contexts: This results in very large cluster sizes, not allowing the cluster models to be highly specialized, e.g., to a specific subtopic. Related approaches by other researchers have effectively used much smaller amounts of target-specific training data for LM adaptation purposes. For example, <ref> [6] </ref> reported small but significant improvements by using the 50 closest articles to the segment hypotheses within a story boundary. Similarly, [4] reported improvements by interpolating an in-domain LM with out-of-domain articles, which were weighted by a distance metric defining how similar they were to the target domain. <p> Alternatively, the unigram distance metric can be made even less sensitive to stylistic and grammatical differences by omitting function and other high-frequency words from the similarity assessment. This is common practice in information retrieval (IR), and was also used for LM adaptation <ref> [6] </ref> and topic identification [2]. This approach focuses the similarity measure on semantic aspects and makes it less sensitive to syntactic features. So far, we have experimented with only a few of the many choices on this continuum.
Reference: 7. <author> R. Stern. </author> <title> Specification of the 1996 Broadcast News evaluation. </title> <booktitle> In these proceedings. </booktitle>
Reference-contexts: Condition-specific LM The Hub4 data exhibit a variety of acoustic conditions. For the purposes of the evaluation, the data in the Hub4 benchmark were partitioned into seven different focus conditions <ref> [7] </ref>. These conditions are correlated with different speaking styles; for example, condition F0 is planned speech while F1 is spontaneous speech. Since speaking style affects language modeling, we can try to exploit this correlation using condition-specific LMs.
References-found: 7

