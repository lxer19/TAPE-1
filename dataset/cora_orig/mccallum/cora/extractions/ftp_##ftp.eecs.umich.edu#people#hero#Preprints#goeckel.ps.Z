URL: ftp://ftp.eecs.umich.edu/people/hero/Preprints/goeckel.ps.Z
Refering-URL: http://www.eecs.umich.edu/~hero/comm.html
Root-URL: http://www.cs.umich.edu
Title: Data-Recursive Algorithms for Blind Channel Identification in Direct-Sequence Systems  
Author: Dennis L. Goeckel Alfred O. Hero III and Wayne E. Stark Dennis Goeckel Alfred O. Hero III Wayne E. Stark 
Keyword: Blind Channel Identification, Direct-Sequence Spread-Spectrum Systems, Oversam pled ISI Systems  
Address: 1301 Beal 1301 Beal 1301 Beal Ann Arbor, MI 48109-2122 Ann Arbor, MI 48109 -2122 Ann Arbor, MI 48109-2122 Tel:(313) 764-0564 Tel:(313) 763-0390  
Affiliation: c=o Prof. Wayne Stark Univ. of Michigan EECS Univ. of Michigan EECS Univ. of Michigan EECS  
Note: Corresponding Author  Fax:(313) 763-8041  
Pubnum: Tel:(313) 764-4110 Fax:(313) 763-8041 Fax:(313)  
Email: Email: hero@eecs.umich.edu Email: stark@eecs.umich.edu Email: goeckel@eecs.umich.edu  
Date: November 30, 1995  763-8041  
Abstract: Algorithms for performing blind channel identification for a binary phase-shift keyed (BPSK) direct-sequence spread-spectrum (DS/SS) system operating over a fading channel are presented. These algorithms are derived by identifying the DS/SS system as a discrete oversampled system with intersymbol interference. In this setting the spreading code can be viewed as a transmit filter, the knowledge of which can be used to aid in channel identification. An off-line solution to the channel identification problem involves the determination of the eigenvector corresponding to the minimum eigenvalue of a matrix that depends on the correlation statistics of the samples of the received signal. In this paper, the online solution is derived for the case that the transmit filter and propagation channel are unknown and jointly identified. Novel low complexity stochastic gradient algorithms and conjugate gradient algorithms are derived and mean convergence conditions given. Then it is shown how the knowledge of the spreading code can be incorporated to aid in identification. An algorithm is then derived that utilizes trellis searching for joint data and channel identification for oversampled systems. Finally, numerical results in the form of channel estimation error are presented.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Haykin, Ed., </author> <title> Blind Deconvolution, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1994. </year>
Reference: [2] <author> H. Liu, G. Xu, and L. Tong, </author> <title> "A Deterministic Approach to Blind Equalization," </title> <booktitle> Conference Record of the Twenty-Seventh Asilomar Conference on Signals, Systems, and Computers (1993), </booktitle> <pages> pp. 751-755. </pages>
Reference-contexts: This is the motivation for performing channel identification of DS/SS systems at the chip period. Much work has appeared on channel identification/equalization via oversampling the output of the channel <ref> [2, 3, 4, 5, 6, 7, 8, 9] </ref>. The common base of this work is that the oversampled process is cyclostationary, thus allowing for blind channel identification based on only second order statistics. <p> As in much of the previous work, it will be assumed that the channel has a finite impulse response (FIR) and that the time support of the channel response is known. Efficient adaptive algorithms for highly oversampled systems are considered for the mathematically identical algorithms described in <ref> [2, 4, 11] </ref>. The formulation that will be used throughout the paper is similar to the subchannel response matching (SRM) algorithm [4], a block-oriented channel identification scheme which minimizes a sum of pairwise differences between filtered subchannel outputs. <p> The number of computations becomes quadratic in the oversampling rate when the knowledge of the transmit filter is incorporated. Furthermore, these algorithms are capable of tracking changes in the channel unlike the batch or "off-line" algorithms of <ref> [2, 4, 11] </ref> on which the algorithms are based. Convergence analyses are presented to aid in selection of the gain factor in the stochastic gradient algorithm. The convergence issues that arise due to data bit transitions are also discussed. <p> The channel identification algorithms described here accomplish identification of the entire channel by identification of each of the N subchannels. Thus, the DS/SS system fortuitously falls into the setting of channel identification via oversampling as presented in <ref> [2, 4, 11] </ref>. The distinction is that in DS/SS systems the spreading waveform is naturally oversampled and no extra sampling equipment needs to be designed into the receiver. <p> This falls into the category of channel identification problems with known transmit filter; in this case, (2) indicates that the spreading code is the transmit filter. 2.2 Subchannel Response Matching Next the subchannel identification criterion used here and in <ref> [2, 4, 11] </ref> is described.
Reference: [3] <author> L. Baccala and S. Roy, </author> <title> "Time-Domain Blind Channel Identification Algorithms," </title> <booktitle> Proceedings of the 1994 Conference on Information Science and Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 863-867. </pages>
Reference-contexts: This is the motivation for performing channel identification of DS/SS systems at the chip period. Much work has appeared on channel identification/equalization via oversampling the output of the channel <ref> [2, 3, 4, 5, 6, 7, 8, 9] </ref>. The common base of this work is that the oversampled process is cyclostationary, thus allowing for blind channel identification based on only second order statistics. <p> Clearly E 2 = 0 when the channels are identified correctly. Furthermore, if the channels have no common zeroes (which is the well-known identifiability criterion for second order identification <ref> [3] </ref>), E 2 = 0 implies ^ h n 0 (k) = h n 0 (k); ^ h n 1 (k) = h n 1 (k) if the trivial solution ^ h n 0 (k) = 0; ^ h n 1 (k) = 0; 8k, is excluded. <p> S H = S. 2. From (3) and (4), it can be shown that S is non-negative definite. 3. If the channels have no common zeroes, there will be a unique minimum eigenvalue of S and the conjugate of its eigenvector will be the desired channel h <ref> [3] </ref>. From (5), the minimum eigenvalue in a noiseless or noisy system is equal to 0 or (N 1) 2 , respectively, where 2 is the variance of the observation noise per sample. 4.
Reference: [4] <author> S. Schell and D. Smith, </author> <title> "Improved Performance of Blind Equalization Using Prior Knowledge of Transmitter Filter," </title> <booktitle> Conference Record of the 1994 IEEE Military Communications Conference, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 128-132. </pages>
Reference-contexts: This is the motivation for performing channel identification of DS/SS systems at the chip period. Much work has appeared on channel identification/equalization via oversampling the output of the channel <ref> [2, 3, 4, 5, 6, 7, 8, 9] </ref>. The common base of this work is that the oversampled process is cyclostationary, thus allowing for blind channel identification based on only second order statistics. <p> As in much of the previous work, it will be assumed that the channel has a finite impulse response (FIR) and that the time support of the channel response is known. Efficient adaptive algorithms for highly oversampled systems are considered for the mathematically identical algorithms described in <ref> [2, 4, 11] </ref>. The formulation that will be used throughout the paper is similar to the subchannel response matching (SRM) algorithm [4], a block-oriented channel identification scheme which minimizes a sum of pairwise differences between filtered subchannel outputs. <p> Efficient adaptive algorithms for highly oversampled systems are considered for the mathematically identical algorithms described in [2, 4, 11]. The formulation that will be used throughout the paper is similar to the subchannel response matching (SRM) algorithm <ref> [4] </ref>, a block-oriented channel identification scheme which minimizes a sum of pairwise differences between filtered subchannel outputs. The SRM algorithm requires obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix of correlation statistics of the received signal. <p> The number of computations becomes quadratic in the oversampling rate when the knowledge of the transmit filter is incorporated. Furthermore, these algorithms are capable of tracking changes in the channel unlike the batch or "off-line" algorithms of <ref> [2, 4, 11] </ref> on which the algorithms are based. Convergence analyses are presented to aid in selection of the gain factor in the stochastic gradient algorithm. The convergence issues that arise due to data bit transitions are also discussed. <p> The channel identification algorithms described here accomplish identification of the entire channel by identification of each of the N subchannels. Thus, the DS/SS system fortuitously falls into the setting of channel identification via oversampling as presented in <ref> [2, 4, 11] </ref>. The distinction is that in DS/SS systems the spreading waveform is naturally oversampled and no extra sampling equipment needs to be designed into the receiver. <p> This falls into the category of channel identification problems with known transmit filter; in this case, (2) indicates that the spreading code is the transmit filter. 2.2 Subchannel Response Matching Next the subchannel identification criterion used here and in <ref> [2, 4, 11] </ref> is described. <p> Comparisons with the trellis-searching algorithms are made in Figures 6 and 7. 22 Key to the Figure abbreviations: 1. "SG": The standard stochastic gradient algorithm. 2. "Norm": The normalized stochastic gradient algorithm. 3. "SRM": The off-line O (L 3 N 3 ) algorithm of <ref> [4] </ref> where S k is obtained at time k by averaging all of the data through time k to estimate ensemble averages. 4. "Trellis LMS": The adaptive trellis-searching algorithm where channel updating is done with an LMS algorithm with (empirically chosen) gain factor = 0:02. 5. "Trellis Mean": The adaptive trellis-searching
Reference: [5] <author> L. Tong, G. Xu, and T. Kailath, </author> <title> "Blind Identification and Equalization Based on Second-Order Statistics: A Time Domain Approach," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 40, </volume> <pages> pp. 340-349, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: This is the motivation for performing channel identification of DS/SS systems at the chip period. Much work has appeared on channel identification/equalization via oversampling the output of the channel <ref> [2, 3, 4, 5, 6, 7, 8, 9] </ref>. The common base of this work is that the oversampled process is cyclostationary, thus allowing for blind channel identification based on only second order statistics.
Reference: [6] <author> E. Moulines, P. Duhamel, J. Cardoso, and S. Mayrargue, </author> <title> "Subspace Methods for the Blind Identification of Multichannel FIR Filters," </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> Vol. 43, </volume> <pages> pp. 516-525, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: This is the motivation for performing channel identification of DS/SS systems at the chip period. Much work has appeared on channel identification/equalization via oversampling the output of the channel <ref> [2, 3, 4, 5, 6, 7, 8, 9] </ref>. The common base of this work is that the oversampled process is cyclostationary, thus allowing for blind channel identification based on only second order statistics.
Reference: [7] <author> K. Meraim, P. Duhamel, D. Gesbert, P. Loubaton, S. Mayrargue, E. Moulines, and D. Slock, </author> <title> "Prediction Error Methods for Time-Domain Blind Identification of Multichannel FIR Filters," </title> <booktitle> Proceedings of the 1995 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 1968-1971. </pages>
Reference-contexts: This is the motivation for performing channel identification of DS/SS systems at the chip period. Much work has appeared on channel identification/equalization via oversampling the output of the channel <ref> [2, 3, 4, 5, 6, 7, 8, 9] </ref>. The common base of this work is that the oversampled process is cyclostationary, thus allowing for blind channel identification based on only second order statistics.
Reference: [8] <author> G. Giannakis and S. Halford, </author> <title> "Performance Analysis of Blind Equalizers Based on Cyclostationary Statistics," </title> <booktitle> Proceedings of the 1994 Conference on Information Science and Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 873-876. </pages>
Reference-contexts: This is the motivation for performing channel identification of DS/SS systems at the chip period. Much work has appeared on channel identification/equalization via oversampling the output of the channel <ref> [2, 3, 4, 5, 6, 7, 8, 9] </ref>. The common base of this work is that the oversampled process is cyclostationary, thus allowing for blind channel identification based on only second order statistics.
Reference: [9] <author> D. Slock and C. Papadias, </author> <title> "Further Results on Blind Identification and Equalization of Multiple FIR Channels ," Proceedings of the 1995 International Conference on Acoustics, Speech, </title> <booktitle> and Signal Processing, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 1964-1967. </pages>
Reference-contexts: This is the motivation for performing channel identification of DS/SS systems at the chip period. Much work has appeared on channel identification/equalization via oversampling the output of the channel <ref> [2, 3, 4, 5, 6, 7, 8, 9] </ref>. The common base of this work is that the oversampled process is cyclostationary, thus allowing for blind channel identification based on only second order statistics.
Reference: [10] <author> S. Schell, D. Smith, and S. Roy, </author> <title> "Blind Channel Identification using Subchannel Response Matching," </title> <booktitle> Proceedings of the 1994 Conference on Information Science and Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 858-862. </pages>
Reference-contexts: Besides being suboptimal, as information is being lost in such an operation, the known spreading code of the user cannot be incorporated to improve convergence and steady state error performance of the channel identification algorithms <ref> [10] </ref>. As in much of the previous work, it will be assumed that the channel has a finite impulse response (FIR) and that the time support of the channel response is known. <p> However, in <ref> [10] </ref>, it is shown how knowledge of the transmit filter can reduce this dimensionality. In the case where the spreading code is the known transmit filter as in (2), the search space dimension is reduced to N L N + 1. <p> In <ref> [10] </ref>, it was shown that E (^g) = ^g T QSQ H ^g fl where Q, an (L1)N +1 by LN matrix of spreading coefficients, is defined as Q where Q n = [q n (0) q n (1) : : :q n (L 1)] with length LN N + 1 <p> Furthermore, it appears that the gain of the conjugate gradient algorithms over the stochastic gradient algorithms is not as prominent as was shown in the Pisarenko's harmonic retrieval; however, the conjugate gradient algorithm showed markedly faster initial convergence in general. The huge gains shown in <ref> [10] </ref> for the known transmit filter are also not evident for the case that was considered in this paper. In general, the trellis-searching algorithms perform much better than the SRM-based algorithms while still only being linear in the oversampling rate.
Reference: [11] <author> L. Baccala and S. Roy, </author> <title> "A New Blind Time-Domain Channel Identification Method Based on Cyclostationarity," </title> <journal> IEEE Signal Processing Letters, </journal> <volume> Vol. 1, </volume> <pages> pp. 89-91, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: As in much of the previous work, it will be assumed that the channel has a finite impulse response (FIR) and that the time support of the channel response is known. Efficient adaptive algorithms for highly oversampled systems are considered for the mathematically identical algorithms described in <ref> [2, 4, 11] </ref>. The formulation that will be used throughout the paper is similar to the subchannel response matching (SRM) algorithm [4], a block-oriented channel identification scheme which minimizes a sum of pairwise differences between filtered subchannel outputs. <p> The number of computations becomes quadratic in the oversampling rate when the knowledge of the transmit filter is incorporated. Furthermore, these algorithms are capable of tracking changes in the channel unlike the batch or "off-line" algorithms of <ref> [2, 4, 11] </ref> on which the algorithms are based. Convergence analyses are presented to aid in selection of the gain factor in the stochastic gradient algorithm. The convergence issues that arise due to data bit transitions are also discussed. <p> The channel identification algorithms described here accomplish identification of the entire channel by identification of each of the N subchannels. Thus, the DS/SS system fortuitously falls into the setting of channel identification via oversampling as presented in <ref> [2, 4, 11] </ref>. The distinction is that in DS/SS systems the spreading waveform is naturally oversampled and no extra sampling equipment needs to be designed into the receiver. <p> This falls into the category of channel identification problems with known transmit filter; in this case, (2) indicates that the spreading code is the transmit filter. 2.2 Subchannel Response Matching Next the subchannel identification criterion used here and in <ref> [2, 4, 11] </ref> is described. <p> Thus, some sort of differential encoding at the transmitter is required [25, pg. 266]. 20 6.2 Simulation Results The algorithms presented in the previous sections are compared in this section through simu lation. As in <ref> [11] </ref>, the normalized mean squared error at iteration k is defined by NMSE k = fl fl fl k ^ h k k h fl fl fl : The goal is to determine both how the algorithms compare to each other and also how these results compare to results reported on
Reference: [12] <author> P. Thompson, </author> <title> "An Adaptive Spectral Analysis Technique for Unbiased Frequency Estimation in the Presence of White Noise," </title> <booktitle> Conference Record of the Thirteenth Asilomar Conference on Circuits, Systems, and Computers (1979), </booktitle> <pages> pp. 529-533. </pages>
Reference-contexts: The problem of data-recursively obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix is well studied in the literature on Pisarenko's harmonic retrieval method <ref> [12, 13, 14, 15, 16, 17] </ref> and direction-of-arrival estimation [18, 19, 20, 21]. In each case, however, the matrix to be analyzed consisted of the expectation of an outer product of the received vector with itself, which allows a very simple rank 1 matrix update formula. <p> N L). Thus, adaptive algorithms will be sought that have low complexity in N . Historically, algorithms for iteratively solving for the eigenvector corresponding to the minimum eigenvalue of a non-negative matrix have fallen into three classes: stochastic gradient <ref> [12, 14, 15, 20] </ref>, conjugate gradient [13, 23], and Newton's method [16, 17]. It is well known for the harmonic retrieval problem that the latter two algorithms converge significantly faster than the stochastic gradient algorithm [13, 17]. <p> Thus, attention here is restricted to the stochastic gradient and conjugate gradient algorithms. 3.1 Stochastic Gradient Algorithm: Definition Although finding the eigenvector corresponding to the minimum eigenvalue of a matrix corresponds closely to Pisarenko's harmonic retrieval method <ref> [12] </ref> and direction of arrival estimation [18], as noted previously S is not expressible as the expectation of an outer product of the observed vector unless N = 2; hence, the simple data updates of [12, 20] do not apply. As in early work on Pisarenko's harmonic retrieval [12], a simple <p> corresponding to the minimum eigenvalue of a matrix corresponds closely to Pisarenko's harmonic retrieval method [12] and direction of arrival estimation [18], as noted previously S is not expressible as the expectation of an outer product of the observed vector unless N = 2; hence, the simple data updates of <ref> [12, 20] </ref> do not apply. As in early work on Pisarenko's harmonic retrieval [12], a simple stochastic gradient algorithm is considered first. <p> retrieval method <ref> [12] </ref> and direction of arrival estimation [18], as noted previously S is not expressible as the expectation of an outer product of the observed vector unless N = 2; hence, the simple data updates of [12, 20] do not apply. As in early work on Pisarenko's harmonic retrieval [12], a simple stochastic gradient algorithm is considered first. The algorithm updates ^ h k1 , the estimate of h opt at iteration k 1, based on an additional symbol period of data: y 0 (k); y 1 (k); : : : ; y N1 (k). <p> As contrasted with the algorithms of <ref> [12, 16] </ref>, normalization of ^ h k to unity is unnecessary since it is accounted for by the denominator of e k ( ^ h k1 ). <p> Note that this normalized algorithm is similar to the algorithms of <ref> [12, 16] </ref>, but here it has been motivated directly from the first order convergence analysis of the unnormalized stochastic gradient algorithm. 3.2.2 Rank of S k and Convergence Even in a noiseless system, when the empirical estimate S k is used the algorithm will see a loss in rank relative to
Reference: [13] <author> H. Chen, T. Sarkar, S. Dianat, J. Brule, </author> <title> "Adaptive Spectral Estimation by the Conjugate Gradient Method," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 34, </volume> <pages> pp. 272-284, </pages> <month> April </month> <year> 1986. </year> <month> 26 </month>
Reference-contexts: The problem of data-recursively obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix is well studied in the literature on Pisarenko's harmonic retrieval method <ref> [12, 13, 14, 15, 16, 17] </ref> and direction-of-arrival estimation [18, 19, 20, 21]. In each case, however, the matrix to be analyzed consisted of the expectation of an outer product of the received vector with itself, which allows a very simple rank 1 matrix update formula. <p> N L). Thus, adaptive algorithms will be sought that have low complexity in N . Historically, algorithms for iteratively solving for the eigenvector corresponding to the minimum eigenvalue of a non-negative matrix have fallen into three classes: stochastic gradient [12, 14, 15, 20], conjugate gradient <ref> [13, 23] </ref>, and Newton's method [16, 17]. It is well known for the harmonic retrieval problem that the latter two algorithms converge significantly faster than the stochastic gradient algorithm [13, 17]. <p> It is well known for the harmonic retrieval problem that the latter two algorithms converge significantly faster than the stochastic gradient algorithm <ref> [13, 17] </ref>. However, in the DS/SS problem, Newton's method is impractical because it requires the computation of the inverse of the Hessian matrix. <p> The two algorithms considered here are presented in the notation of [23] for a more detailed discussion of the application of these algorithms to harmonic retrieval, see <ref> [13, 23] </ref>. <p> In Chen et. al. <ref> [13] </ref>, b k is chosen such that p H k S fl p k1 = 0, which yields 14 b k = k p k1 k1 S fl : On the other hand, Yang et. al. [23] choose b k such that p H k Hp k1 = 0 where H <p> This yields b k = k p k1 + k 5 e k+1 ( ^ h k )k 2 ^ h k p k1 ] k1 S fl : The algorithms above differ from the algorithms presented in <ref> [13, 23] </ref>; similarly to the stochastic gradient algorithms presented in Section 3.1, they use only the instantaneous data estimates of S fl k to step once per symbol period. <p> The results are averaged over 200 sample trials. Only the data-adaptive version of the conjugate gradient algorithm of Chen et. al. <ref> [13] </ref> is shown as the data-adaptive version of the conjugate gradient algorithm of Yang et. al. [23] was found to have nearly identical performance.
Reference: [14] <author> M. Larimore and R. Calvert, </author> <title> "Convergence Studies of Thompson's Unbiased Adap--tive Spectral Estimator," </title> <booktitle> Conference Record of the Fourteenth Asilomar Conference on Circuits, Systems, and Computers (1980), </booktitle> <pages> pp. 258-262. </pages>
Reference-contexts: The problem of data-recursively obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix is well studied in the literature on Pisarenko's harmonic retrieval method <ref> [12, 13, 14, 15, 16, 17] </ref> and direction-of-arrival estimation [18, 19, 20, 21]. In each case, however, the matrix to be analyzed consisted of the expectation of an outer product of the received vector with itself, which allows a very simple rank 1 matrix update formula. <p> N L). Thus, adaptive algorithms will be sought that have low complexity in N . Historically, algorithms for iteratively solving for the eigenvector corresponding to the minimum eigenvalue of a non-negative matrix have fallen into three classes: stochastic gradient <ref> [12, 14, 15, 20] </ref>, conjugate gradient [13, 23], and Newton's method [16, 17]. It is well known for the harmonic retrieval problem that the latter two algorithms converge significantly faster than the stochastic gradient algorithm [13, 17]. <p> However, it is possible to show that for a noisy system all of the eigenspaces except the one corresponding to the minimum eigenvector are unstable; thus, the algorithm will eventually leave these spaces <ref> [14] </ref>. A less ambitious but desirable goal is to choose such that the algorithm will converge to the correct solution when it is near the minimizing eigenvector. Here a first order convergence analysis is performed, which is equivalent to assuming S can be estimated without error.
Reference: [15] <author> M. Larimore, </author> <title> "Adaptive Convergence of Spectral Estimation Based on Pisarenko Harmonic Retrieval," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 31, </volume> <pages> pp. 955-962, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: The problem of data-recursively obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix is well studied in the literature on Pisarenko's harmonic retrieval method <ref> [12, 13, 14, 15, 16, 17] </ref> and direction-of-arrival estimation [18, 19, 20, 21]. In each case, however, the matrix to be analyzed consisted of the expectation of an outer product of the received vector with itself, which allows a very simple rank 1 matrix update formula. <p> N L). Thus, adaptive algorithms will be sought that have low complexity in N . Historically, algorithms for iteratively solving for the eigenvector corresponding to the minimum eigenvalue of a non-negative matrix have fallen into three classes: stochastic gradient <ref> [12, 14, 15, 20] </ref>, conjugate gradient [13, 23], and Newton's method [16, 17]. It is well known for the harmonic retrieval problem that the latter two algorithms converge significantly faster than the stochastic gradient algorithm [13, 17].
Reference: [16] <author> V. Reddy, B. Egardt, and T. Kailath, </author> <title> "Least Squares Type Algorithm for Adaptive Implementation of Pisarenko's Harmonic Retrieval Method," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 30, </volume> <pages> pp. 399-405, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: The problem of data-recursively obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix is well studied in the literature on Pisarenko's harmonic retrieval method <ref> [12, 13, 14, 15, 16, 17] </ref> and direction-of-arrival estimation [18, 19, 20, 21]. In each case, however, the matrix to be analyzed consisted of the expectation of an outer product of the received vector with itself, which allows a very simple rank 1 matrix update formula. <p> Thus, adaptive algorithms will be sought that have low complexity in N . Historically, algorithms for iteratively solving for the eigenvector corresponding to the minimum eigenvalue of a non-negative matrix have fallen into three classes: stochastic gradient [12, 14, 15, 20], conjugate gradient [13, 23], and Newton's method <ref> [16, 17] </ref>. It is well known for the harmonic retrieval problem that the latter two algorithms converge significantly faster than the stochastic gradient algorithm [13, 17]. However, in the DS/SS problem, Newton's method is impractical because it requires the computation of the inverse of the Hessian matrix. <p> As contrasted with the algorithms of <ref> [12, 16] </ref>, normalization of ^ h k to unity is unnecessary since it is accounted for by the denominator of e k ( ^ h k1 ). <p> Note that this normalized algorithm is similar to the algorithms of <ref> [12, 16] </ref>, but here it has been motivated directly from the first order convergence analysis of the unnormalized stochastic gradient algorithm. 3.2.2 Rank of S k and Convergence Even in a noiseless system, when the empirical estimate S k is used the algorithm will see a loss in rank relative to
Reference: [17] <author> G. Mathew, S. Dasgupta, and V. Reddy, </author> <title> "Improved Newton-Type Algorithm for Adaptive Implementation of Pisarenko's Harmonic Retrieval Method and Its Convergence Analysis," </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> Vol. 42, </volume> <pages> pp. 434-437, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: The problem of data-recursively obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix is well studied in the literature on Pisarenko's harmonic retrieval method <ref> [12, 13, 14, 15, 16, 17] </ref> and direction-of-arrival estimation [18, 19, 20, 21]. In each case, however, the matrix to be analyzed consisted of the expectation of an outer product of the received vector with itself, which allows a very simple rank 1 matrix update formula. <p> Thus, adaptive algorithms will be sought that have low complexity in N . Historically, algorithms for iteratively solving for the eigenvector corresponding to the minimum eigenvalue of a non-negative matrix have fallen into three classes: stochastic gradient [12, 14, 15, 20], conjugate gradient [13, 23], and Newton's method <ref> [16, 17] </ref>. It is well known for the harmonic retrieval problem that the latter two algorithms converge significantly faster than the stochastic gradient algorithm [13, 17]. However, in the DS/SS problem, Newton's method is impractical because it requires the computation of the inverse of the Hessian matrix. <p> It is well known for the harmonic retrieval problem that the latter two algorithms converge significantly faster than the stochastic gradient algorithm <ref> [13, 17] </ref>. However, in the DS/SS problem, Newton's method is impractical because it requires the computation of the inverse of the Hessian matrix.
Reference: [18] <author> R. Schmidt, </author> <title> "Multiple Emitter Location and Signal Parameter Estimation," </title> <journal> IEEE Transactions on Antennas and Propagation, </journal> <volume> Vol. 34, </volume> <pages> pp. 276-280, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: The problem of data-recursively obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix is well studied in the literature on Pisarenko's harmonic retrieval method [12, 13, 14, 15, 16, 17] and direction-of-arrival estimation <ref> [18, 19, 20, 21] </ref>. In each case, however, the matrix to be analyzed consisted of the expectation of an outer product of the received vector with itself, which allows a very simple rank 1 matrix update formula. <p> Thus, attention here is restricted to the stochastic gradient and conjugate gradient algorithms. 3.1 Stochastic Gradient Algorithm: Definition Although finding the eigenvector corresponding to the minimum eigenvalue of a matrix corresponds closely to Pisarenko's harmonic retrieval method [12] and direction of arrival estimation <ref> [18] </ref>, as noted previously S is not expressible as the expectation of an outer product of the observed vector unless N = 2; hence, the simple data updates of [12, 20] do not apply.
Reference: [19] <author> M. Kaveh and A. Barabell, </author> <title> "The Statistical Performance of the MUSIC and the Minimum-Norm Algorithms in Resolving Plane Waves in Noise," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 34, </volume> <pages> pp. 331-341, </pages> <month> April </month> <year> 1986. </year>
Reference-contexts: The problem of data-recursively obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix is well studied in the literature on Pisarenko's harmonic retrieval method [12, 13, 14, 15, 16, 17] and direction-of-arrival estimation <ref> [18, 19, 20, 21] </ref>. In each case, however, the matrix to be analyzed consisted of the expectation of an outer product of the received vector with itself, which allows a very simple rank 1 matrix update formula.
Reference: [20] <author> D. Torrieri and K. Bakhru, </author> <title> "The Recursive Suppression Algorithm for Adaptive Superresolution," </title> <journal> IEEE Transactions on Antennas and Propagation, </journal> <volume> Vol. 40, </volume> <pages> pp. 926-932, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The problem of data-recursively obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix is well studied in the literature on Pisarenko's harmonic retrieval method [12, 13, 14, 15, 16, 17] and direction-of-arrival estimation <ref> [18, 19, 20, 21] </ref>. In each case, however, the matrix to be analyzed consisted of the expectation of an outer product of the received vector with itself, which allows a very simple rank 1 matrix update formula. <p> N L). Thus, adaptive algorithms will be sought that have low complexity in N . Historically, algorithms for iteratively solving for the eigenvector corresponding to the minimum eigenvalue of a non-negative matrix have fallen into three classes: stochastic gradient <ref> [12, 14, 15, 20] </ref>, conjugate gradient [13, 23], and Newton's method [16, 17]. It is well known for the harmonic retrieval problem that the latter two algorithms converge significantly faster than the stochastic gradient algorithm [13, 17]. <p> corresponding to the minimum eigenvalue of a matrix corresponds closely to Pisarenko's harmonic retrieval method [12] and direction of arrival estimation [18], as noted previously S is not expressible as the expectation of an outer product of the observed vector unless N = 2; hence, the simple data updates of <ref> [12, 20] </ref> do not apply. As in early work on Pisarenko's harmonic retrieval [12], a simple stochastic gradient algorithm is considered first.
Reference: [21] <author> J. Yang and M. Kaveh, </author> <title> "Adaptive Eigensubspace Algorithms for Direction or Frequency Estimation and Tracking," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 36, </volume> <pages> pp. 241-251, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The problem of data-recursively obtaining the eigenvector corresponding to the minimum eigenvalue of a matrix is well studied in the literature on Pisarenko's harmonic retrieval method [12, 13, 14, 15, 16, 17] and direction-of-arrival estimation <ref> [18, 19, 20, 21] </ref>. In each case, however, the matrix to be analyzed consisted of the expectation of an outer product of the received vector with itself, which allows a very simple rank 1 matrix update formula.
Reference: [22] <author> N. Seshadri, </author> <title> "Joint Data and Channel Estimation Using Blind Trellis Search Techniques," </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol. 42, </volume> <pages> pp. 1000-1011, </pages> <month> Febru-ary/March/April </month> <year> 1994. </year>
Reference-contexts: Convergence analyses are presented to aid in selection of the gain factor in the stochastic gradient algorithm. The convergence issues that arise due to data bit transitions are also discussed. Finally, a trellis-searching blind channel identification algorithm is described by extending the methods of <ref> [22] </ref> to an oversampled system. In [22], blind channel identification is done by extending the optimal known ISI decoder, the Viterbi Algorithm, to blind identification by allowing each state to retain multiple paths. Each path operates with its own channel estimate, which is updated at each time instant. <p> The convergence issues that arise due to data bit transitions are also discussed. Finally, a trellis-searching blind channel identification algorithm is described by extending the methods of <ref> [22] </ref> to an oversampled system. In [22], blind channel identification is done by extending the optimal known ISI decoder, the Viterbi Algorithm, to blind identification by allowing each state to retain multiple paths. Each path operates with its own channel estimate, which is updated at each time instant. <p> Section 3 develops the new adaptive algorithm for second order identification of oversampled systems, while Section 4 develops adaptive algorithms for a known transmit filter. Section 5 contains the extension of the trellis-searching algorithm of <ref> [22] </ref> to oversampled systems. Finally, numerical results for the presented algorithms are shown in Section 6. 2 Motivation and Problem Formulation 2.1 Mapping of a DS/SS System to a Discrete Oversampled System Assume that binary phase-shift keyed (BPSK) modulation is employed with a data symbol duration of T s . <p> Finally, note that the conjugate gradient algorithms can take advantage of the transmit filter knowledge in an analogous way to the stochastic gradient algorithm. 5 Adaptive Algorithm Using Trellis Searching In this section, a joint data and channel estimation algorithm due to Seshadri <ref> [22] </ref> is extended to oversampled systems. 5.1 Joint Data/Channel Data Estimation It is well known that the maximum likelihood sequence estimator for a discrete channel with known impulse response of duration L symbol periods can be implemented with a Viterbi Algorithm with 2 L1 states, one state for each possible set <p> Two paths enter each of the states at time k. At each state, the path with the highest metric is kept and the other discarded. The algorithm considered in <ref> [22] </ref> extends the above idea to channels where the ISI is unknown. The algorithm is nearly identical to the algorithm described above except that multiple paths (the number denoted M ) are kept for each state at each time. <p> Since the method of <ref> [22] </ref> is able to equalize a channel that is not oversampled, each subchannel could be separately equalized with this algorithm 18 and the results combined (although there would be a sign ambiguity for each subchannel). <p> A method of updating the channel estimate for a given path must now be considered. Although the LMS algorithm could be used just as it was in <ref> [22] </ref>, a different method is proposed here for the following reason. Stochastic gradient algorithms (like LMS) are largely utilized to reduce complexity as has been done in the previous sections here. <p> The only moderately good performance of the trellis-searching algorithm with either channel updating strategy is caused by outstanding performance on certain sample paths and a complete lack of convergence in others. This was an unexpected phenomenon as it was not noted in <ref> [22] </ref>. Finally, the benefit of knowledge of the transmit filter is considered.
Reference: [23] <author> X. Yang, T. Sarkar, and E. Arvas, </author> <title> "A Survery of Conjugate Gradient Algorithms for Solution of Extreme Eigen-Problems of a Symmetric Matrix," </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 37, </volume> <pages> pp. 1550-1555, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: N L). Thus, adaptive algorithms will be sought that have low complexity in N . Historically, algorithms for iteratively solving for the eigenvector corresponding to the minimum eigenvalue of a non-negative matrix have fallen into three classes: stochastic gradient [12, 14, 15, 20], conjugate gradient <ref> [13, 23] </ref>, and Newton's method [16, 17]. It is well known for the harmonic retrieval problem that the latter two algorithms converge significantly faster than the stochastic gradient algorithm [13, 17]. <p> It is illustrated how data-adaptive versions of these algorithms can be implemented in O (L 2 N ) flops for the oversampled blind channel identification problem. The two algorithms considered here are presented in the notation of <ref> [23] </ref> for a more detailed discussion of the application of these algorithms to harmonic retrieval, see [13, 23]. <p> The two algorithms considered here are presented in the notation of [23] for a more detailed discussion of the application of these algorithms to harmonic retrieval, see <ref> [13, 23] </ref>. <p> In Chen et. al. [13], b k is chosen such that p H k S fl p k1 = 0, which yields 14 b k = k p k1 k1 S fl : On the other hand, Yang et. al. <ref> [23] </ref> choose b k such that p H k Hp k1 = 0 where H is the Hessian of the Rayleigh quotient. <p> This yields b k = k p k1 + k 5 e k+1 ( ^ h k )k 2 ^ h k p k1 ] k1 S fl : The algorithms above differ from the algorithms presented in <ref> [13, 23] </ref>; similarly to the stochastic gradient algorithms presented in Section 3.1, they use only the instantaneous data estimates of S fl k to step once per symbol period. <p> The results are averaged over 200 sample trials. Only the data-adaptive version of the conjugate gradient algorithm of Chen et. al. [13] is shown as the data-adaptive version of the conjugate gradient algorithm of Yang et. al. <ref> [23] </ref> was found to have nearly identical performance.
Reference: [24] <author> J. Marsden and A. Tromba, </author> <title> Vector Calculus, </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The key is to recognize that Q fl S fl k Q T ^g k1 = 5E k (^g k1 ) where the gradient in this case is with respect to the elements of ^g k1 . Applying the chain rule for gradients <ref> [24, pg. 134] </ref>, element j of Q fl S fl k Q T ^g k1 is given by 16 @(^g k1 QS k Q H ^g k1 ) = m=0 l=0 T fl @ ^ h k1 [mL + l] @^g k1 [j] N1 X L1 X @( ^ h k1
Reference: [25] <author> J. Proakis, </author> <title> Digital Communications, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Data/Channel Data Estimation It is well known that the maximum likelihood sequence estimator for a discrete channel with known impulse response of duration L symbol periods can be implemented with a Viterbi Algorithm with 2 L1 states, one state for each possible set of the previous L 1 data bits <ref> [25, pg. 610] </ref>. <p> It cannot distinguish between the true bit stream across the true channel and the negative of the true bit stream across the negative of the true channel because they both give the same outputs. Thus, some sort of differential encoding at the transmitter is required <ref> [25, pg. 266] </ref>. 20 6.2 Simulation Results The algorithms presented in the previous sections are compared in this section through simu lation.
Reference: [26] <author> S. Verdu, B. Anderson, and R. Kennedy, </author> <title> "Blind Equalization without Gain Identification," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 39, </volume> <pages> pp. 292-297, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: As mentioned earlier, the algorithms have the drawback that they are only correct to within a complex constant; in other words, they have both an amplitude and phase ambiguity. As discussed in <ref> [26] </ref>, this is not a severe limitation. Furthermore, the algorithms only identify the channel and do not decode the transmitted bits. One possible method of doing so is to run a Viterbi Algorithm concurrent with the channel estimator.
Reference: [27] <author> D. Goeckel, A. Hero III, and W. Stark, </author> <title> "Blind Channel Identification for Direct-Sequence Spread-Spectrum Systems," </title> <booktitle> Conference Record of the 1995 IEEE Military Communications Conference, </booktitle> <pages> pp. </pages> <month> 368-372. </month> <title> 27 28 stochastic gradient algorithm with that when the transmit filter is known, N = 8; L = 3, </title> <type> SNR = 10 dB 29 </type>
Reference-contexts: In particular, the trellis-searching algorithms have higher complexity than the SRM-based algorithms. Since the SRM-based algorithms calculate e k ( ^ h k1 ) at each step, it could be used to implement a multiple starting point iteration to close both the performance and complexity gap <ref> [27] </ref>. Slight extensions, however, will be ignored and only the base algorithms considered. In the first set of numerical results, two channels are considered with zeroes as tabulated below. For both channels, N = 4; L = 5, which yields = 0:14 from the convergence analysis in Section 3.2.
References-found: 27

