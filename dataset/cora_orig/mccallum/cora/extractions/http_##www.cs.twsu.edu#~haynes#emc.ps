URL: http://www.cs.twsu.edu/~haynes/emc.ps
Refering-URL: http://adept.cs.twsu.edu/~thomas/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: [haynes,sandip]@euler.mcs.utulsa.edu  
Title: The Evolution of Multiagent Coordination Strategies  
Author: Thomas Haynes Sandip Sen 
Keyword: Multiagent Coordination, Behavioral Strategies, Evolution of Behavior, Genetic Programming  
Address: Tulsa  
Affiliation: Department of Mathematical Computer Sciences, The University of  
Abstract: The design and development of behavioral strategies to coordinate the actions of multiple agents is a central issue in multiagent systems research. We propose a novel approach of evolving, rather than handcrafting, behavioral strategies. The evolution scheme used is a variant of the Genetic Programming (GP) paradigm. As a proof of principle, we evolve behavioral strategies in the predator-prey domain that has been studied widely in the Distributed Artificial Intelligence community. We use the GP to evolve behavioral strategies for individual agents, as prior literature claims that communication between predators is not necessary for successfully capturing the prey. The evolved strategy, when used by each predator, performs better than all but one of the handcrafted strategies mentioned in literature. We analyze the shortcomings of each of these strategies. The next set of experiments involve co-evolving predators and prey. To our surprise, a simple prey strategy evolves that consistently evades all of the predator strategies. We analyze the implications of the relative successes of evolution in the two sets of experiments and comment on the nature of domains for which GP based evolution is a viable mechanism for generating coordination strategies. We conclude with our design for concurrent evolution of multiple agent strategies in domains where agents need to communicate with each other to successfully solve a common problem. 
Abstract-found: 1
Intro-found: 1
Reference: [ Angeline and Kinnear, Jr., 1996 ] <editor> Peter J. Angeline and Kenneth E. Kinnear, Jr., editors. </editor> <booktitle> Advances in Genetic Programming 2. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference: [ Angeline and Pollack, 1993 ] <author> Peter J. Angeline and Jordan B. Pollack. </author> <title> Competitive environments evolve better solutions for complex tasks. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the 5th International Conference on Genetic Algorithms, ICGA-93, </booktitle> <pages> pages 264-270, </pages> <institution> University of Illinois at Urbana-Champaign, </institution> <address> 17-21 July 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Angeline, 1994 ] <author> Peter John Angeline. </author> <title> Genetic programming and emergent intelligence. </title> <editor> In Kenneth E. Kinnear, Jr., editor, </editor> <booktitle> Advances in Genetic Programming, chapter 4, </booktitle> <pages> pages 75-98. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: A drawback to the GP paradigm is that without better identification and encapsulation of reusable components, strategies which are complex and encapsulate a large amount of knowledge are unlikely to be evolved. The GP community is actively researching this issue <ref> [ Angeline, 1994, Kinnear, Jr., 1994b, Koza, 1994 ] </ref> . Acknowledgments We must thank Larry Stephens for providing us with the thirty test cases he and Matthias Merx used in their work [ Stephens and Merx, 1990 ] .
Reference: [ Benda et al., 1986 ] <author> M. Benda, V. Jagannathan, and R. Dodhiawala. </author> <title> On optimal cooperation of knowledge sources an empirical investigation. </title> <type> Technical Report BCS-G2010-28, </type> <institution> Boeing Advanced Technology Center, Boeing Computing Services, </institution> <address> Seattle, Washington, </address> <month> July </month> <year> 1986. </year>
Reference-contexts: Populations of such structures are evolved to produce increasingly efficient coordination strategies. We have used both single and multiagent domains to evaluate the evolution of behavioral strategies by STGP [ Haynes et al., 1995, Haynes, 1994 ] . We have used the predator-prey pursuit game <ref> [ Benda et al., 1986 ] </ref> to test our hypothesis that useful coordination strategies can be evolved using the STGP paradigm for non-trivial problems. This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. <p> the utility of GP for evolving behavioral strategies. 2 2 The Pursuit Problem The original version of the predator-prey pursuit problem was introduced by Benda, et al. and consisted of four blue (predator) agents trying to capture a red (prey) agent by surrounding it from four directions on a grid-world <ref> [ Benda et al., 1986 ] </ref> . Agent movements were limited to either a horizontal or a vertical step per time unit. The movement of the prey agent was random. No two agents were allowed to occupy the same location.
Reference: [ Bond and Gasser, 1988 ] <editor> Alan H. Bond and Les Gasser, editors. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: In effect, we want to evolve behavioral strategies that guide the actions of agents in a given domain. The identification, design, and implementation of strategies for coordination is a central research issue in the field of Distributed Artificial Intelligence (DAI) <ref> [ Bond and Gasser, 1988 ] </ref> . Current research techniques in developing coordination strategies are mostly off-line mechanisms that use extensive domain knowledge to design from scratch the most appropriate cooperation strategy. It is nearly impossible to identify or even prove the existence of the best coordination strategy.
Reference: [ Davis, 1991 ] <editor> Lawrence Davis, editor. </editor> <booktitle> Handbook of genetic algorithms. </booktitle> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: Though GAs are not guaranteed to find optimal solutions, they still possess some nice provable properties (optimal allocation of trials to substrings, evaluating exponential number of schemas with linear number of string evaluations, etc.), and have been found to be useful in a number of practical applications <ref> [ Davis, 1991 ] </ref> . Koza's work on genetic programming [ Koza, 1992 ] was motivated by the representational constraint in traditional GAs.
Reference: [ Gasser and Rouquette, 1988 ] <author> Les Gasser and Nicolas Rouquette. </author> <title> Representing and using organizational knowledge in distributed AI systems. </title> <booktitle> In Proceedings of the 1988 Distributed AI Workshop, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: The approach undertaken by Gasser et al. allowed for the predators to occupy and maintain a Lieb configuration (each predator occupying a different quadrant, where a quadrant is defined by diagonals intersecting at the location of the prey) while homing in on the prey <ref> [ Gasser and Rouquette, 1988 ] </ref> . This study, as well as the study by Singh on using group intentions for agent coordination [ Singh, 1990 ] , lacked any experimental results which would allow comparison with other reported work.
Reference: [ Gasser et al., 1989 ] <author> Les Gasser, Nicolas Rouquette, Randall W. Hill, and John Lieb. </author> <title> Representing and using organizational knowledge in DAI systems. </title> <editor> In Les Gasser and Michael N. Huhns, editors, </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence, </booktitle> <pages> pages 55-78. </pages> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference: [ Haynes and Sen, 1996 ] <author> Thomas Haynes and Sandip Sen. </author> <title> Evolving behavioral strategies in predators and prey. </title> <editor> In Gerhard Wei and Sandip Sen, editors, </editor> <booktitle> Adaptation and Learning in Multi-Agent Systems, Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 113-126. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: The initial results from the prey learning experiment also prompted us to conduct an experiment in which predators were trained against a prey which moves in a straight line. The best evolved strategy is referred to as Linear STGP. In <ref> [ Haynes and Sen, 1996 ] </ref> we found it to be evident that the Linear STGP strategy is not very general, and only has significant performance when pitted against a Linear prey. 7.3 Analysis of Competitive Co-evolution Some compelling questions arose from these experiments in co-evolution: 1.
Reference: [ Haynes and Sen, 1997a ] <author> Thomas Haynes and Sandip Sen. </author> <title> Crossover operators for evolving a team. </title> <editor> In John R. Koza, Kalyanmoy Deb, Marco Dorigo, David B. Fogel, Max Garzon, Hitoshi Iba, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1997: Proceedings of the Second Annual Conference. </booktitle> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: The developed strategies in this research had implicit communication in that the same program was used to control the four predator agents. We are examining the rise of cooperation strategies without implicit communication <ref> [ Haynes and Sen, 1997a ] </ref> . This is achieved by having each predator agent being controlled by its own program.
Reference: [ Haynes and Sen, 1997b ] <author> Thomas Haynes and Sandip Sen. </author> <title> The influence of a domain's behavioral laws on on-line learning. </title> <booktitle> In AAAI Workshop on On-Line Search, </booktitle> <year> 1997. </year> <note> (accepted for publication). </note>
Reference: [ Haynes and Wainwright, 1995 ] <author> Thomas D. Haynes and Roger L. Wainwright. </author> <title> A simulation of adaptive agents in a hostile environment. </title> <editor> In K. M. George, Janice H. Carroll, Ed Deaton, Dave Oppenheim, and Jim Hightower, editors, </editor> <booktitle> Proceedings of the 1995 ACM Symposium on Applied Computing, </booktitle> <pages> pages 318-323. </pages> <publisher> ACM Press, </publisher> <year> 1995. </year>
Reference-contexts: STGP eliminates certain combinations of operations. Hence it necessarily reduces the size of the search space. In many cases the reduction is a significant factor. The GP system, GPengine, used in this research is an extension of that used in <ref> [ Haynes and Wainwright, 1995 ] </ref> and is written in C. Furthermore, it allows for strong typing as described by Montana.
Reference: [ Haynes et al., 1994 ] <author> Thomas Haynes, Roger Wainwright, and Sandip Sen. </author> <title> Evolving cooperation strategies. </title> <type> Technical Report UTULSA-MCS-94-10, </type> <institution> The University of Tulsa, </institution> <month> December 16, </month> <year> 1994. </year>
Reference-contexts: To compare the algorithms we utilized 30 test cases from Stephens [ Stephens and Merx, 1990 ] , averaged over 26 different initial random seeds. 4 In our previous work <ref> [ Haynes et al., 1994, Haynes et al., 1995 ] </ref> we had allowed the simulation to run for 200 time steps. We found that MNO and MDO did not have the capture rates reported by Korf, who did not report the maximum number of time steps used in his experiments. <p> We found the evolved strategies still ignored other predator locations. In order to test the hypothesis that the prey was learning to escape the predators, we ran experiments where the prey was pitted against our version of Manhattan distance (MD) algorithm <ref> [ Haynes et al., 1994 ] </ref> . The prey was very successful in evading the predators. This was particularly surprising because the algorithm developed by the prey was simple: pick a random direction and move in a straight line in that direction. <p> The Linear prey is so effective because it avoids locality of movement. The greedy strategies <ref> [ Haynes et al., 1994, Korf, 1992 ] </ref> fare well against the prey that stay in a small neighborhood. Locality allows the predators both to surround the prey and to escape any deadlock situations, such as that in Figure 10 (a).
Reference: [ Haynes et al., 1995 ] <author> Thomas Haynes, Roger Wainwright, Sandip Sen, and Dale Schoenefeld. </author> <title> Strongly typed genetic programming in evolving cooperation strategies. </title> <editor> In Larry Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 271-278, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: We can then measure their efficiency and effectiveness by some criteria relevant to the domain. Populations of such structures are evolved to produce increasingly efficient coordination strategies. We have used both single and multiagent domains to evaluate the evolution of behavioral strategies by STGP <ref> [ Haynes et al., 1995, Haynes, 1994 ] </ref> . We have used the predator-prey pursuit game [ Benda et al., 1986 ] to test our hypothesis that useful coordination strategies can be evolved using the STGP paradigm for non-trivial problems. <p> Furthermore, it allows for strong typing as described by Montana. We decided to utilize STGP rather than GP because STGP reduces the solution space to be searched <ref> [ Haynes et al., 1995, Montana, 1995 ] </ref> and it ensures that all parse trees are syntactically correct. 5.2 Encoding of Behavioral Strategies In Korf's implementation of the predator-prey domain, he utilized the same algorithm to control each of the predator agents. <p> To compare the algorithms we utilized 30 test cases from Stephens [ Stephens and Merx, 1990 ] , averaged over 26 different initial random seeds. 4 In our previous work <ref> [ Haynes et al., 1994, Haynes et al., 1995 ] </ref> we had allowed the simulation to run for 200 time steps. We found that MNO and MDO did not have the capture rates reported by Korf, who did not report the maximum number of time steps used in his experiments.
Reference: [ Haynes, 1994 ] <author> Thomas D. Haynes. </author> <title> A simulation of adaptive agents in a hostile environment. </title> <type> Master's thesis, </type> <institution> University of Tulsa, Tulsa, OK., </institution> <month> April </month> <year> 1994. </year> <month> 22 </month>
Reference-contexts: We can then measure their efficiency and effectiveness by some criteria relevant to the domain. Populations of such structures are evolved to produce increasingly efficient coordination strategies. We have used both single and multiagent domains to evaluate the evolution of behavioral strategies by STGP <ref> [ Haynes et al., 1995, Haynes, 1994 ] </ref> . We have used the predator-prey pursuit game [ Benda et al., 1986 ] to test our hypothesis that useful coordination strategies can be evolved using the STGP paradigm for non-trivial problems.
Reference: [ Holland, 1975 ] <author> John H. Holland. </author> <booktitle> Adpatation in Natural and Artificial Systems. </booktitle> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1975. </year>
Reference-contexts: In the following subsections we briefly introduce the genetic programming paradigm, along with its strongly typed variant, and explain how we have used it to evolve coordination strategies. 5.1 Genetic Programming Holland's work on adaptive systems <ref> [ Holland, 1975 ] </ref> produced a class of biologically inspired algorithms known as genetic algorithms (GAs) that can manipulate and develop solutions to optimization, learning, and other types of problems. In order for GAs to be effective, the solution should be represented as n-ary strings.
Reference: [ Kinnear, Jr., 1994a ] <editor> Kenneth E. Kinnear, Jr., editor. </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference: [ Kinnear, Jr., 1994b ] <author> Kenneth E. Kinnear, Jr. </author> <title> Alternatives in automatic function definition: A comparison of performance. </title> <editor> In Kenneth E. Kinnear, Jr., editor, </editor> <booktitle> Advances in Genetic Programming, chapter 6, </booktitle> <pages> pages 119-141. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: A drawback to the GP paradigm is that without better identification and encapsulation of reusable components, strategies which are complex and encapsulate a large amount of knowledge are unlikely to be evolved. The GP community is actively researching this issue <ref> [ Angeline, 1994, Kinnear, Jr., 1994b, Koza, 1994 ] </ref> . Acknowledgments We must thank Larry Stephens for providing us with the thirty test cases he and Matthias Merx used in their work [ Stephens and Merx, 1990 ] .
Reference: [ Korf, 1992 ] <author> Richard E. Korf. </author> <title> A simple solution to pursuit games. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 183-194, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This will introduce significant uncertainty and complexity into the problem. The earlier research into the predator-prey domain involved explicit communication between the predator agents. Korf claimed that such expensive communication was unnecessary, as simple greedy algorithms always lead to capture <ref> [ Korf, 1992 ] </ref> . He further claims that the orthogonal game, a discretization of the continuous world which allows only horizontal and vertical movements, is a poor approximation. <p> In our experiments, we used random training cases per generation. 6 Evolution of Individual Greedy Strategies Korf's basic claim is that predators need not jointly decide on a strategy, but can choose locally optimal moves and still be able to capture the prey consistently <ref> [ Korf, 1992 ] </ref> . We used this philosophy as the basis for our research in this domain. Like Korf, our predators choose their moves individually, i.e., without reasoning about the moves of other predators or without mutually deciding on a set of moves. <p> Furthermore, the predators do not possess any explicit communication skills; two predators cannot communicate to resolve conflicts or negotiate a capture strategy. 6.1 Deterministic Predator Algorithms Korf used two greedy heuristics in his work: Manhattan distance and max norm <ref> [ Korf, 1992 ] </ref> . The Manhattan distance algorithm determines the best move to make based on the sum of the differences of the x and y coordinates of a predator and the prey. <p> In Korf's model of the randomly moving prey, the prey's possible moves are limited to those surrounding cells which are unoccupied by the predators <ref> [ Korf, 1992 ] </ref> . Since we chose to let all agents, prey and predators, move simultaneously, the randomly moving prey is allowed to consider all directions. In Korf's setup, if the prey was surrounded on three sides, it would move to the free cell. <p> The Linear prey is so effective because it avoids locality of movement. The greedy strategies <ref> [ Haynes et al., 1994, Korf, 1992 ] </ref> fare well against the prey that stay in a small neighborhood. Locality allows the predators both to surround the prey and to escape any deadlock situations, such as that in Figure 10 (a). <p> With the MD algorithm, deadlock situations can occur. With the MN algorithm, the predators are not guaranteed to stay in a capture position <ref> [ Korf, 1992 ] </ref> . Since we have shown, with an ordering on movements, the greedy agents are communicating, we would like to see if greedy agents are effective if we do not order the movements. An argument can be made that the ordering of the moves is not natural.
Reference: [ Koza, 1992 ] <author> John R. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Our approach for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed genetic programming (STGP) paradigm [ Montana, 1995 ] , which is an extension of genetic programming (GP) <ref> [ Koza, 1992 ] </ref> . To use the STGP approach for evolving coordination strategies, the strategies are encoded as symbolic expressions (S-expressions) and an evaluation criterion is chosen for evaluating arbitrary S-expressions. <p> Koza's work on genetic programming <ref> [ Koza, 1992 ] </ref> was motivated by the representational constraint in traditional GAs. <p> Hence, closure means any element can be a child node in a parse tree for any other element without having conflicting data types. Koza describes a way to relax the closure constraint using the concept of constrained syntax structures <ref> [ Koza, 1992 ] </ref> . Koza used tree generation routines which only generated legal trees. He also used operations on the parse trees which maintain legal syntactic structures. Montana claims that closure is a serious limitation to genetic programming [ Montana, 1995 ] .
Reference: [ Koza, 1994 ] <author> John R. Koza. </author> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: A drawback to the GP paradigm is that without better identification and encapsulation of reusable components, strategies which are complex and encapsulate a large amount of knowledge are unlikely to be evolved. The GP community is actively researching this issue <ref> [ Angeline, 1994, Kinnear, Jr., 1994b, Koza, 1994 ] </ref> . Acknowledgments We must thank Larry Stephens for providing us with the thirty test cases he and Matthias Merx used in their work [ Stephens and Merx, 1990 ] .
Reference: [ Levy and Rosenschein, 1992 ] <author> Ran Levy and Jeffrey S. Rosenschein. </author> <title> A game theoretic approach to the pursuit problem. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 195-213, </pages> <month> February </month> <year> 1992. </year>
Reference: [ Montana, 1995 ] <author> David J. Montana. </author> <title> Strongly typed genetic programming. </title> <journal> Evolutionary Computation, </journal> <volume> 3(2) </volume> <pages> 199-230, </pages> <year> 1995. </year>
Reference-contexts: Our approach for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed genetic programming (STGP) paradigm <ref> [ Montana, 1995 ] </ref> , which is an extension of genetic programming (GP) [ Koza, 1992 ] . To use the STGP approach for evolving coordination strategies, the strategies are encoded as symbolic expressions (S-expressions) and an evaluation criterion is chosen for evaluating arbitrary S-expressions. <p> Koza used tree generation routines which only generated legal trees. He also used operations on the parse trees which maintain legal syntactic structures. Montana claims that closure is a serious limitation to genetic programming <ref> [ Montana, 1995 ] </ref> . He introduced strongly typed genetic programming (STGP), in which the variables, constants, arguments, and returned values can be of any type [ Montana, 1995 ] . The only restriction is that the data type for each element be specified beforehand. <p> Montana claims that closure is a serious limitation to genetic programming <ref> [ Montana, 1995 ] </ref> . He introduced strongly typed genetic programming (STGP), in which the variables, constants, arguments, and returned values can be of any type [ Montana, 1995 ] . The only restriction is that the data type for each element be specified beforehand. This causes the initialization process and the various genetic operations to only construct syntactically correct trees. <p> Furthermore, it allows for strong typing as described by Montana. We decided to utilize STGP rather than GP because STGP reduces the solution space to be searched <ref> [ Haynes et al., 1995, Montana, 1995 ] </ref> and it ensures that all parse trees are syntactically correct. 5.2 Encoding of Behavioral Strategies In Korf's implementation of the predator-prey domain, he utilized the same algorithm to control each of the predator agents.
Reference: [ Phillips, Jr., 1992 ] <author> John L. Phillips, Jr. </author> <title> How to Think About Statistics. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1992. </year>
Reference-contexts: In an effort to remove any unfair 4 This apparently uncommon number was used for averaging so that we can use a formula for the Wilcoxon matched pair signed-rank test <ref> [ Phillips, Jr., 1992 ] </ref> , which is used to test the significance of the results. 11 IFTE ( &lt;( IFTE ( T, MD ( CellOf ( Prey, H ), CellOf ( Bi,E )), MD ( CellOf ( Prey, N), CellOf ( Bi, H ))), MD ( CellOf ( Prey, N
Reference: [ Reynolds, 1994 ] <author> Craig W. Reynolds. </author> <title> Evolution of obstacle avoidance behaviour:using noise to promote robust solutions. </title> <editor> In Kenneth E. Kinnear, Jr., editor, </editor> <booktitle> Advances in Genetic Programming, chapter 10, </booktitle> <pages> pages 221-241. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The basic premise of co-evolution is that if one population devises a good ploy, then the other population will construct a counter to that ploy. We expect that the populations will see-saw in being dominant. This has been shown in Reynold's work on co-evolution in the game of tag <ref> [ Reynolds, 1994 ] </ref> . In his work, the two opposing agents, from the same population, take turns being the predator and the prey.
Reference: [ Russell and Norvig, 1995 ] <author> Stuart Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: When faced with a set of choices C, a single agent A i is rational if and only if it selects the one which yields the highest expected utility U i;max <ref> [ Russell and Norvig, 1995 ] </ref> .
Reference: [ Singh, 1990 ] <author> Munindar P. Singh. </author> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Working Papers of the 10th International Workshop on Distributed Artificial Intelligence, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: This study, as well as the study by Singh on using group intentions for agent coordination <ref> [ Singh, 1990 ] </ref> , lacked any experimental results which would allow comparison with other reported work. Stephens and Merx performed a series of experiments to demonstrate the relative effectiveness of three different control strategies [ Stephens and Merx, 1990 ] .
Reference: [ Smith, 1980 ] <author> Reid G. Smith. </author> <title> The contract net protocol: High-level communication and control in a distributed problem solver. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(12):1104-1113, </volume> <month> December </month> <year> 1980. </year>
Reference-contexts: In general, we believe that the predator agents must employ either some explicit communication or further domain knowledge to capture the Linear, 1Ply, and Still prey algorithms. A contract net <ref> [ Smith, 1980 ] </ref> based predator algorithm, where agents get assigned different sides of the prey to occupy, should fare well against these prey algorithms.
Reference: [ Stephens and Merx, 1990 ] <author> Larry M. Stephens and Matthias B. Merx. </author> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Proceedings of the 1990 Distributed AI Workshop, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: Stephens and Merx performed a series of experiments to demonstrate the relative effectiveness of three different control strategies <ref> [ Stephens and Merx, 1990 ] </ref> . They defined the local control strategy where a predator broadcasts its position to other predators when it occupies a neighboring location to the prey. Other predator agents then concentrate on occupying the other locations neighboring the prey. <p> To compare the algorithms we utilized 30 test cases from Stephens <ref> [ Stephens and Merx, 1990 ] </ref> , averaged over 26 different initial random seeds. 4 In our previous work [ Haynes et al., 1994, Haynes et al., 1995 ] we had allowed the simulation to run for 200 time steps. <p> The GP community is actively researching this issue [ Angeline, 1994, Kinnear, Jr., 1994b, Koza, 1994 ] . Acknowledgments We must thank Larry Stephens for providing us with the thirty test cases he and Matthias Merx used in their work <ref> [ Stephens and Merx, 1990 ] </ref> . It provided us with an opportunity to compare our work with that of previous researchers. This research was partially supported by NSF Research Initiative Award IRI-9410180. 21
Reference: [ Vidal and Durfee, 1995 ] <author> Jose M. Vidal and Edmund H. Durfee. </author> <title> Recursive agent modeling using limited rationality. </title> <editor> In Victor Lesser, editor, </editor> <booktitle> Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> pages 376-383, </pages> <address> San Francisco, CA, 1995. </address> <publisher> MIT Press. </publisher> <pages> 23 </pages>
References-found: 30

